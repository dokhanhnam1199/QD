{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n[Worse code]\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Heuristic function for TSP using a combination of factors, sparsification, and adaptive thresholding.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n\n    Returns:\n        np.ndarray: Prior indicators (probabilities) of edge inclusion.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Calculate inverse distance\n    inverse_distance = 1.0 / (distance_matrix + 1e-9)  # Add small value to avoid division by zero\n\n    # Calculate node degree preference:  Nodes with lower degree are preferred (avoiding premature saturation).\n    degree_preference = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_preference[i, j] = 1.0  # Initially, all edges are equally preferred\n    row_sums = np.sum(degree_preference > 0, axis=1, keepdims=True)  # Calculate degree\n    degree_penalty = np.clip(1.0 - row_sums / (n - 1), 0.1, 1.0)  # Scale degree preference, clipping to avoid zero\n    for i in range(n):\n        degree_preference[i, :] *= degree_penalty[i, 0]\n    degree_preference = np.nan_to_num(degree_preference) #handles division by 0, though unlikely.\n\n    # Adaptive Sparsification using Percentile Thresholding.\n    threshold_percentile = 50  # Adjust percentile for sparsification. Experiment with different values.\n    thresholds = np.percentile(distance_matrix[distance_matrix > 0], threshold_percentile) #only consider non-zero elements when determining threshold\n\n    sparse_matrix = distance_matrix.copy()\n    sparse_matrix[distance_matrix > thresholds] = np.inf  # effectively sparsifies matrix.\n    sparse_matrix = (sparse_matrix != np.inf) #creates binary sparse representation\n\n    # Combine heuristics, applying sparsification mask\n    heuristic_matrix = inverse_distance * degree_preference * sparse_matrix # Element-wise product of factors\n\n    # Ensure no self-loops.\n    for i in range(n):\n        heuristic_matrix[i, i] = 0.0\n\n    # Normalize heuristic values to create a probability-like matrix.\n    row_sums = heuristic_matrix.sum(axis=1, keepdims=True)\n    heuristic_matrix = np.nan_to_num(heuristic_matrix / row_sums)  # Avoid division by zero\n\n    return heuristic_matrix\n\n[Better code]\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Heuristic function for TSP using a combination of factors and sparsification.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n\n    Returns:\n        np.ndarray: Prior indicators (probabilities) of edge inclusion.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Calculate nearest neighbor heuristic\n    nearest_neighbors = np.zeros((n, n), dtype=float)\n    for i in range(n):\n        distances = distance_matrix[i].copy()\n        distances[i] = np.inf  # Avoid self-loop\n        nearest_neighbor_indices = np.argsort(distances)[:2]  # Consider top 2 nearest neighbors\n        for neighbor_index in nearest_neighbor_indices:\n            nearest_neighbors[i, neighbor_index] = 1.0\n\n    # Calculate inverse distance\n    inverse_distance = 1.0 / (distance_matrix + 1e-9)  # Add small value to avoid division by zero\n\n    # Combine heuristics\n    heuristic_matrix = inverse_distance * (0.5 + 0.5 * nearest_neighbors) #weighted averaging\n\n    # Sparsify the matrix:  Keep only the top k promising edges for each node.\n    k = min(5, n - 1)  # Keep at least top 2 neighbors.  Adjust `k` based on instance size.\n\n    for i in range(n):\n        row = heuristic_matrix[i].copy()  # Use a copy to avoid modifying the original during sorting\n        indices_to_keep = np.argsort(row)[-k:]  # Indices of the k largest values\n        mask = np.zeros(n, dtype=bool)\n        mask[indices_to_keep] = True\n        heuristic_matrix[i, ~mask] = 0  # Zero out less promising edges\n        heuristic_matrix[i,i] = 0 # Remove self loops\n\n    # Normalize heuristic values to create a probability-like matrix.  Normalization also mitigates any extreme value effects\n    row_sums = heuristic_matrix.sum(axis=1, keepdims=True)\n    heuristic_matrix = np.nan_to_num(heuristic_matrix / row_sums)  # Avoid division by zero\n\n    return heuristic_matrix\n\n[Reflection]\nPrioritize nearest neighbors, adaptive k-sparsification, and avoid excessive degree penalties for improved exploration.\n\n\n[Improved code]\nPlease write an improved function `heuristics_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}