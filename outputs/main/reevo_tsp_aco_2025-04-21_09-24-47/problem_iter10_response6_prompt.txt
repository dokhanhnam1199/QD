{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n[Worse code]\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Heuristic function for TSP using a combination of factors, adaptive sparsification,\n    edge combination strategies, and cycle prevention. This version uses\n    relative saving, inverse distance and mst.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n\n    Returns:\n        np.ndarray: Prior indicators (probabilities) of edge inclusion.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # 1. Relative Saving Heuristic\n    saving_matrix = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                saving_matrix[i, j] = distance_matrix[i, 0] + distance_matrix[0, j] - distance_matrix[i, j]\n    # Normalize saving values\n    max_saving = np.max(saving_matrix)\n    if max_saving > 0:\n        saving_matrix = saving_matrix / max_saving\n\n    # 2. Inverse Distance Heuristic\n    inverse_distance = 1.0 / (distance_matrix + 1e-9)\n    # Normalize inverse distance\n    max_inv_dist = np.max(inverse_distance)\n    if max_inv_dist > 0:\n        inverse_distance = inverse_distance / max_inv_dist\n\n    # 3. MST Heuristic (approximation using distances)\n    mst_heuristic = np.zeros((n, n), dtype=float)\n\n    #Prim's algorithm inspired approach\n    start_node = 0\n    visited = {start_node}\n    possible_edges = []\n    for i in range(n):\n        if i != start_node:\n            possible_edges.append((start_node, i, distance_matrix[start_node, i]))\n\n    mst_edges = []\n    while len(visited) < n:\n        min_edge = None\n        min_dist = np.inf\n        for u, v, dist in possible_edges:\n            if (u in visited and v not in visited) or (v in visited and u not in visited):\n                if dist < min_dist:\n                    min_dist = dist\n                    min_edge = (u, v, dist)\n        if min_edge:\n            u, v, _ = min_edge\n            mst_edges.append((u, v))\n            visited.add(u)\n            visited.add(v)\n            new_possible_edges = []\n            for i in range(n):\n              for j in range(n):\n                  if i != j:\n                    if (i,j, distance_matrix[i,j]) not in possible_edges:\n                      new_possible_edges.append((i, j, distance_matrix[i, j]))\n\n            possible_edges = new_possible_edges\n            for i in range(n):\n              if i not in visited:\n                for v_node in visited:\n\n                  possible_edges.append((i,v_node, distance_matrix[i, v_node]))\n            possible_edges = list(set(possible_edges)) # Remove duplicates\n    for u,v in mst_edges:\n        mst_heuristic[u, v] = 1.0\n        mst_heuristic[v, u] = 1.0\n    max_mst = np.max(mst_heuristic)\n    if max_mst > 0:\n        mst_heuristic = mst_heuristic/max_mst\n\n\n    # Combine Heuristics\n    heuristic_matrix = (0.4 * saving_matrix +\n                        0.4 * inverse_distance +\n                        0.2 * mst_heuristic)\n\n    # Adaptive Sparsification (adjust based on n)\n    k = min(max(3, int(np.sqrt(n))), n - 1) # Adjust k based on sqrt(n)\n    for i in range(n):\n        row = heuristic_matrix[i].copy()\n        indices_to_keep = np.argsort(row)[-k:]\n        mask = np.zeros(n, dtype=bool)\n        mask[indices_to_keep] = True\n        heuristic_matrix[i, ~mask] = 0\n        heuristic_matrix[i,i] = 0 # Remove self loops\n\n    # Add small noise to allow exploration\n    noise = np.random.rand(n, n) * 0.01\n    heuristic_matrix = (heuristic_matrix + noise)\n    heuristic_matrix = (heuristic_matrix + heuristic_matrix.T)/2 #ensure Symmetry\n\n    row_sums = heuristic_matrix.sum(axis=1, keepdims=True)\n    heuristic_matrix = np.nan_to_num(heuristic_matrix / (row_sums + 1e-9))\n    return heuristic_matrix\n\n[Better code]\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Heuristic function for TSP using a combination of diverse factors, adaptive sparsification,\n    and edge combination strategies. This version adaptively adjusts the combination\n    weights and sparsification based on problem size and incorporates Clarke-Wright savings\n    and a perturbation factor to encourage exploration.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n\n    Returns:\n        np.ndarray: Prior indicators (probabilities) of edge inclusion.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # 1. Inverse Distance Heuristic\n    inverse_distance = 1.0 / (distance_matrix + 1e-9)\n\n    # 2. Nearest Neighbor Heuristic\n    nearest_neighbors = np.zeros((n, n), dtype=float)\n    for i in range(n):\n        distances = distance_matrix[i].copy()\n        distances[i] = np.inf\n        num_neighbors = min(max(2, int(np.log(n))), n - 1)  # Logarithmic scaling\n        nearest_neighbor_indices = np.argsort(distances)[:num_neighbors]\n        for neighbor_index in nearest_neighbor_indices:\n            nearest_neighbors[i, neighbor_index] = 1.0\n\n    # 3. Minimum Spanning Tree (MST) heuristic (approximation using distances)\n    mst_heuristic = np.zeros((n, n), dtype=float)\n\n    # Prim's algorithm inspired approach\n    start_node = 0\n    visited = {start_node}\n    possible_edges = []\n    for i in range(n):\n        if i != start_node:\n            possible_edges.append((start_node, i, distance_matrix[start_node, i]))\n\n    mst_edges = []\n    while len(visited) < n:\n        min_edge = None\n        min_dist = np.inf\n        for u, v, dist in possible_edges:\n            if (u in visited and v not in visited) or (v in visited and u not in visited):\n                if dist < min_dist:\n                    min_dist = dist\n                    min_edge = (u, v, dist)\n        if min_edge:\n            u, v, _ = min_edge\n            mst_edges.append((u, v))\n            visited.add(u)\n            visited.add(v)\n            new_possible_edges = []\n            for i in range(n):\n              for j in range(n):\n                  if i != j:\n                    if (i,j, distance_matrix[i,j]) not in possible_edges:\n                      new_possible_edges.append((i, j, distance_matrix[i, j]))\n\n            possible_edges = new_possible_edges\n            for i in range(n):\n              if i not in visited:\n                for v_node in visited:\n\n                  possible_edges.append((i,v_node, distance_matrix[i, v_node]))\n            possible_edges = list(set(possible_edges)) # Remove duplicates\n\n    for u,v in mst_edges:\n        mst_heuristic[u, v] = 1.0\n        mst_heuristic[v, u] = 1.0\n\n    # 4. Clarke-Wright Savings Heuristic\n    savings_heuristic = np.zeros((n, n), dtype=float)\n    for i in range(n):\n        for j in range(i + 1, n):\n            savings = distance_matrix[0, i] + distance_matrix[0, j] - distance_matrix[i, j]\n            savings_heuristic[i, j] = savings\n            savings_heuristic[j, i] = savings\n\n    # 5. Perturbation factor to encourage exploration\n    perturbation = np.random.rand(n, n) * 0.05  # Small random values\n\n    # Adaptive weights (tuned based on experimentation)\n    weight_inverse_distance = 0.2\n    weight_nearest_neighbors = 0.3\n    weight_mst = 0.2\n    weight_savings = 0.2\n    weight_perturbation = 0.1\n\n    # Combine Heuristics\n    heuristic_matrix = (weight_inverse_distance * inverse_distance +\n                        weight_nearest_neighbors * nearest_neighbors +\n                        weight_mst * mst_heuristic +\n                        weight_savings * savings_heuristic +\n                        weight_perturbation * perturbation)\n\n    # Adaptive Sparsification (adjust based on n)\n    k = min(max(3, int(np.sqrt(n))), n - 1)  # Adjust k based on sqrt(n)\n    for i in range(n):\n        row = heuristic_matrix[i].copy()\n        indices_to_keep = np.argsort(row)[-k:]\n        mask = np.zeros(n, dtype=bool)\n        mask[indices_to_keep] = True\n        heuristic_matrix[i, ~mask] = 0\n        heuristic_matrix[i, i] = 0 # Remove self loops\n\n    # Normalize Heuristic Matrix\n    row_sums = heuristic_matrix.sum(axis=1, keepdims=True)\n    heuristic_matrix = np.nan_to_num(heuristic_matrix / row_sums)\n\n    return heuristic_matrix\n\n[Reflection]\nDiversify heuristics, tune weights adaptively, and scale sparsification with problem size for improved TSP solutions.\n\n\n[Improved code]\nPlease write an improved function `heuristics_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}