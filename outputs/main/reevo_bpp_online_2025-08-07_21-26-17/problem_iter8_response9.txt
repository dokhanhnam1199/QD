```python
import numpy as np
from typing import Optional

def priority_v2(
    item: float,
    bins_remain_cap: np.ndarray,
    *,
    epsilon: float = 0.02,
    noise_scale: float = 1e-5,
    alpha_fit: float = 10.0,
    target_fit: float = 0.75,
    alpha_waste: float = 12.0,
    waste_target: float = 0.05,
    alpha_used: float = 8.0,
    used_target: float = 0.40,
    random_state: Optional[int] = None,
) -> np.ndarray:
    """
    Compute priority scores for online bin packing.

    Parameters
    ----------
    item : float
        Size of the incoming item.
    bins_remain_cap : np.ndarray
        1‑D array of remaining capacities of the open bins.
    epsilon : float, optional
        Probability of selecting a random bin instead of the deterministic
        score (ε‑greedy exploration).
    noise_scale : float, optional
        Scale of uniform additive noise used to break ties.
    alpha_fit, target_fit : float, optional
        Logistic parameters for the fit‑ratio component.
    alpha_waste, waste_target : float, optional
        Logistic parameters for the waste component.
    alpha_used, used_target : float, optional
        Logistic parameters for the used‑fraction component.
    random_state : int or None, optional
        Random seed for reproducibility.

    Returns
    -------
    np.ndarray
        Priority scores (higher = more desirable). Infeasible bins receive
        ``-np.inf`` so they will never be selected.
    """
    bins_remain_cap = np.asarray(bins_remain_cap, dtype=np.float64)

    # Empty bin list: nothing to score
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)

    feasible = bins_remain_cap >= item
    if not np.any(feasible):
        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)

    # Estimate bin capacity from the maximum remaining capacity
    bin_capacity = float(np.max(bins_remain_cap))
    if bin_capacity <= 0.0:
        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)

    rng = np.random.default_rng(random_state)

    # ------------------------------------------------------------------
    # 1) Fit‑ratio component
    # ------------------------------------------------------------------
    fit_ratio = np.zeros_like(bins_remain_cap, dtype=np.float64)
    np.divide(item, bins_remain_cap, out=fit_ratio, where=feasible)

    # ------------------------------------------------------------------
    # 2) Waste component (leftover after placement)
    # ------------------------------------------------------------------
    waste_norm = np.clip((bins_remain_cap - item) / bin_capacity, 0.0, 1.0)

    # ------------------------------------------------------------------
    # 3) Used‑fraction component
    # ------------------------------------------------------------------
    used_before = (bin_capacity - bins_remain_cap) / bin_capacity
    used_after = np.clip(used_before + item / bin_capacity, 0.0, 1.0)

    # ------------------------------------------------------------------
    # Helper: variance‑based steepness adjustment
    # ------------------------------------------------------------------
    def _var_norm(arr: np.ndarray, mask: np.ndarray) -> float:
        """Return normalized variance (0‑1) of `arr` over indices where `mask`."""
        if np.any(mask):
            var = np.var(arr[mask])
        else:
            var = 0.0
        # Max variance for a [0,1] distribution is 0.25
        return min(var / 0.25, 1.0)

    var_fit = _var_norm(fit_ratio, feasible)
    var_waste = _var_norm(waste_norm, feasible)
    var_used = _var_norm(used_after, feasible)

    # Adjust logistic steepness inversely with variance
    alpha_fit_adj = alpha_fit * (1.0 - var_fit)
    alpha_waste_adj = alpha_waste * (1.0 - var_waste)
    alpha_used_adj = alpha_used * (1.0 - var_used)

    # Prevent zero steepness
    alpha_fit_adj = max(alpha_fit_adj, 1e-3)
    alpha_waste_adj = max(alpha_waste_adj, 1e-3)
    alpha_used_adj = max(alpha_used_adj, 1e-3)

    # ------------------------------------------------------------------
    # Logistic scoring functions
    # ------------------------------------------------------------------
    def _logistic(x: np.ndarray, target: float, alpha: float) -> np.ndarray:
        return 1.0 / (1.0 + np.exp(-alpha * (x - target)))

    fit_score = _logistic(fit_ratio, target_fit, alpha_fit_adj)
    # Waste score is decreasing: use negative alpha
    waste_score = _logistic(waste_norm, waste_target, -alpha_waste_adj)
    used_score = _logistic(used_after, used_target, alpha_used_adj)

    # ------------------------------------------------------------------
    # Combine components multiplicatively
    # ------------------------------------------------------------------
    combined_score = fit_score * waste_score * used_score

    # Add exact‑fit bonus
    exact_fit = feasible & np.isclose(bins_remain_cap - item, 0.0, atol=1e-9)
    combined_score[exact_fit] += 1e6

    # Set infeasible bins to -inf
    combined_score[~feasible] = -np.inf

    # ------------------------------------------------------------------
    # Tie‑breaker: small deterministic offset
    # ------------------------------------------------------------------
    tie_breaker = np.arange(bins_remain_cap.size, dtype=np.float64) * 1e-12
    combined_score += tie_breaker

    # ------------------------------------------------------------------
    # Add random noise
    # ------------------------------------------------------------------
    if noise_scale > 0.0:
        noise = rng.random(bins_remain_cap.size) * noise_scale
        combined_score[feasible] += noise[feasible]

    # ------------------------------------------------------------------
    # ε‑greedy exploration
    # ------------------------------------------------------------------
    if epsilon > 0.0 and rng.random() < epsilon:
        random_scores = np.full_like(combined_score, -np.inf, dtype=np.float64)
        random_scores[feasible] = rng.random(np.count_nonzero(feasible))
        combined_score = random_scores

    return combined_score
```
