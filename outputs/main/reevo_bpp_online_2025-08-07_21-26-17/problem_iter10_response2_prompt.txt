{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    item: float,\n    bins_remain_cap: np.ndarray,\n    *,\n    capacity: float | None = None,\n    epsilon: float = 0.05,\n    noise_scale: float = 1e-6,\n    base_k_fit: float = 12.0,\n    base_k_waste: float = 12.0,\n    base_k_used: float = 10.0,\n    var_max: float = 0.25,\n    target_fit: float = 0.80,\n    waste_target: float = 0.07,\n    used_target: float = 0.40,\n    weight_fit: float = 1.0,\n    weight_waste: float = 1.0,\n    weight_used: float = 1.0,\n    exact_fit_bonus: float = 1e6,\n) -> np.ndarray:\n    \"\"\"\n    Compute a priority score for each open bin in an online bin\u2011packing\n    setting. Higher scores indicate more desirable bins for placing the\n    incoming ``item``. Infeasible bins receive ``-np.inf`` so they are\n    never selected.\n\n    The function combines three sigmoid\u2011based components:\n    * fit\u2011ratio (item / bin capacity),\n    * waste (remaining capacity after placement),\n    * used\u2011fraction (how full the bin already is).\n\n    The steepness of each sigmoid is adapted to the variance of the\n    underlying component over the feasible bins, providing more decisive\n    preferences when the data is concentrated.\n\n    Parameters\n    ----------\n    item : float\n        Size of the incoming item (0 < item \u2264 bin capacity).\n    bins_remain_cap : array_like\n        Remaining capacities of the currently open bins.\n    capacity : float, optional\n        Bin capacity. If ``None`` it is inferred as the maximum observed\n        remaining capacity.\n    epsilon : float, optional\n        Probability of performing pure exploration (random feasible bin).\n    noise_scale : float, optional\n        Scale of a small Gaussian perturbation added to break ties.\n    base_k_fit, base_k_waste, base_k_used : float, optional\n        Base steepness for the three sigmoid components.\n    var_max : float, optional\n        Maximum variance used for normalising the adaptive steepness.\n    target_fit, waste_target, used_target : float, optional\n        Target values for the three components; the sigmoid is centred\n        around each target.\n    weight_fit, weight_waste, weight_used : float, optional\n        Exponential weights applied to each component when combining them.\n    exact_fit_bonus : float, optional\n        Large additive bonus for bins that fit the item exactly.\n\n    Returns\n    -------\n    np.ndarray\n        Priority scores (higher = more desirable). Infeasible bins are\n        marked with ``-np.inf``.\n    \"\"\"\n    # ----------------------------------------------------------------------\n    # Normalise inputs\n    # ----------------------------------------------------------------------\n    bins = np.asarray(bins_remain_cap, dtype=np.float64)\n\n    if bins.size == 0:\n        return np.array([], dtype=np.float64)\n\n    # Feasibility mask\n    feasible = (bins >= item) & (bins >= 0.0)\n\n    # Early exit if nothing fits\n    if not np.any(feasible):\n        return np.full_like(bins, -np.inf, dtype=np.float64)\n\n    # Estimate capacity if not provided\n    if capacity is None:\n        capacity = float(np.max(bins))\n        if capacity <= 0.0:\n            capacity = 1.0  # fallback to avoid division by zero\n\n    # ----------------------------------------------------------------------\n    # Component 1: Fit ratio (item / bin_capacity)\n    # ----------------------------------------------------------------------\n    fit_ratio = np.zeros_like(bins)\n    np.divide(item, bins, out=fit_ratio, where=feasible)\n\n    var_fit = np.var(fit_ratio[feasible]) if np.any(feasible) else 0.0\n    k_fit = base_k_fit * (1.0 - min(var_fit / var_max, 1.0))\n    k_fit = max(k_fit, 1e-3)  # avoid zero steepness\n\n    fit_score = 1.0 / (1.0 + np.exp(-k_fit * (fit_ratio - target_fit)))\n\n    # ----------------------------------------------------------------------\n    # Component 2: Waste (leftover capacity after placement)\n    # ----------------------------------------------------------------------\n    leftover = bins - item\n    waste_norm = np.clip(leftover / capacity, 0.0, 1.0)\n\n    var_waste = np.var(waste_norm[feasible]) if np.any(feasible) else 0.0\n    k_waste = base_k_waste * (1.0 - min(var_waste / var_max, 1.0))\n    k_waste = max(k_waste, 1e-3)\n\n    # Want low waste; sigmoid decreasing with waste\n    waste_score = 1.0 / (1.0 + np.exp(k_waste * (waste_norm - waste_target)))\n\n    # ----------------------------------------------------------------------\n    # Component 3: Used fraction (how full the bin already is)\n    # ----------------------------------------------------------------------\n    used_frac = (capacity - bins) / capacity\n\n    var_used = np.var(used_frac[feasible]) if np.any(feasible) else 0.0\n    k_used = base_k_used * (1.0 - min(var_used / var_max, 1.0))\n    k_used = max(k_used, 1e-3)\n\n    used_score = 1.0 / (1.0 + np.exp(-k_used * (used_frac - used_target)))\n\n    # ----------------------------------------------------------------------\n    # Combine components\n    # ----------------------------------------------------------------------\n    combined = (fit_score ** weight_fit) * \\\n               (waste_score ** weight_waste) * \\\n               (used_score ** weight_used)\n\n    # Exact\u2011fit bonus\n    exact_fit = feasible & np.isclose(leftover, 0.0, atol=1e-9)\n    combined[exact_fit] += exact_fit_bonus\n\n    # Small Gaussian noise for tie\u2011breaking\n    if noise_scale > 0.0:\n        rng = np.random.default_rng()\n        combined += rng.normal(scale=noise_scale, size=combined.shape)\n\n    # ----------------------------------------------------------------------\n    # \u03b5\u2011greedy exploration\n    # ----------------------------------------------------------------------\n    if epsilon > 0.0:\n        rng = np.random.default_rng()\n        if rng.random() < epsilon:\n            rand_scores = rng.random(bins.shape)\n            rand_scores[~feasible] = -np.inf\n            return rand_scores\n\n    # Mask infeasible bins\n    combined[~feasible] = -np.inf\n\n    return combined\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    item: float,\n    bins_remain_cap: np.ndarray,\n    *,\n    epsilon: float = 0.02,\n    noise_scale: float = 1e-5,\n    alpha_fit: float = 10.0,\n    target_fit: float = 0.75,\n    alpha_waste: float = 12.0,\n    waste_target: float = 0.05,\n    tie_breaker_scale: float = 1e-12,\n    random_state: Optional[int] = None,\n) -> np.ndarray:\n    \"\"\"\n    Compute a priority score for each open bin in an online Bin Packing problem.\n\n    The score is higher for bins that:\n      * fit the item well (fit ratio near ``target_fit``),\n      * leave little waste after placement (waste below ``waste_target``),\n      * break ties deterministically by bin index,\n      * optionally explore via random noise or \u03b5\u2011greedy exploration.\n\n    Parameters\n    ----------\n    item : float\n        Size of the incoming item.\n    bins_remain_cap : np.ndarray\n        1\u2011D array of remaining free capacity of each currently open bin.\n    epsilon : float, default 0.02\n        Probability of replacing the deterministic scores with random scores\n        (\u03b5\u2011greedy exploration).\n    noise_scale : float, default 1e-5\n        Scale of uniform additive noise added to each feasible bin's score.\n    alpha_fit : float, default 10.0\n        Logistic steepness for the *fit\u2011ratio* component.\n    target_fit : float, default 0.75\n        Desired fit ratio (item / remaining capacity) where the fit component\n        reaches 0.5.\n    alpha_waste : float, default 12.0\n        Logistic steepness for the *waste* component.\n    waste_target : float, default 0.05\n        Desired waste fraction (relative to bin capacity) where the waste\n        component reaches 0.5.\n    tie_breaker_scale : float, default 1e-12\n        Small multiplier for bin index to break exact ties deterministically.\n    random_state : int or None, default None\n        Seed for the internal RNG (useful for reproducibility).\n\n    Returns\n    -------\n    np.ndarray\n        Priority scores (higher = better) for each bin. Infeasible bins receive\n        ``-np.inf`` so they will never be selected by ``np.argmax``.\n    \"\"\"\n    # Ensure proper NumPy array\n    bins_remain_cap = np.asarray(bins_remain_cap, dtype=np.float64)\n    n_bins = bins_remain_cap.size\n\n    if n_bins == 0:\n        return np.array([], dtype=np.float64)\n\n    # Feasibility mask: can the item fit?\n    feasible = bins_remain_cap >= item\n    if not np.any(feasible):\n        # No feasible bin \u2192 all scores -inf\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    # Estimate static bin capacity (assume all bins share the same capacity)\n    bin_capacity = float(np.max(bins_remain_cap))\n    if bin_capacity <= 0.0:\n        # Defensive: capacity non\u2011positive, treat as infeasible\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    rng = np.random.default_rng(random_state)\n\n    # ------------------------------------------------------------------\n    # 1) Fit\u2011ratio component (item size relative to remaining capacity)\n    # ------------------------------------------------------------------\n    fit_ratio = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    # Compute ratio only for feasible bins to avoid divide\u2011by\u2011zero warnings\n    np.divide(item, bins_remain_cap, out=fit_ratio, where=feasible)\n\n    # Logistic: high when fit_ratio \u2248 target_fit\n    # Clip exponent to avoid overflow\n    exp_fit = np.exp(-np.clip(alpha_fit * (fit_ratio - target_fit), -500, 500))\n    fit_score = 1.0 / (1.0 + exp_fit)\n\n    # ------------------------------------------------------------------\n    # 2) Waste component (leftover capacity after placement)\n    # ------------------------------------------------------------------\n    waste_norm = np.clip((bins_remain_cap - item) / bin_capacity, 0.0, 1.0)\n    # Logistic decreasing: small waste \u2192 high score\n    exp_waste = np.exp(np.clip(alpha_waste * (waste_norm - waste_target), -500, 500))\n    waste_score = 1.0 / (1.0 + exp_waste)\n\n    # ------------------------------------------------------------------\n    # Combine components multiplicatively\n    # ------------------------------------------------------------------\n    combined_score = fit_score * waste_score\n\n    # Infeasible bins must be penalised with -inf\n    combined_score[~feasible] = -np.inf\n\n    # ------------------------------------------------------------------\n    # Deterministic tie\u2011breaker (tiny index\u2011based bias)\n    # ------------------------------------------------------------------\n    combined_score += np.arange(n_bins, dtype=np.float64) * tie_breaker_scale\n\n    # ------------------------------------------------------------------\n    # Add small random noise for stochastic tie\u2011breaking\n    # ------------------------------------------------------------------\n    if noise_scale > 0.0:\n        noise = rng.random(n_bins) * noise_scale\n        combined_score[feasible] += noise[feasible]\n\n    # ------------------------------------------------------------------\n    # \u03b5\u2011greedy exploration: occasionally replace scores with random ranks\n    # ------------------------------------------------------------------\n    if epsilon > 0.0 and rng.random() < epsilon:\n        random_scores = np.full_like(combined_score, -np.inf, dtype=np.float64)\n        random_scores[feasible] = rng.random(np.count_nonzero(feasible))\n        combined_score = random_scores\n\n    return combined_score\n\n[Reflection]\nAvoid costly variance adaptation; use fixed\u2011steepness sigmoids, deterministic index tie\u2011breaker, small noise, \u03b5\u2011greedy exploration.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}