{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n\n    \"\"\"TSP heuristic: Combines inverse distance, shortest paths, and adaptive degree penalty with sparsification.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # 1. Inverse distance\n    inverse_distance = 1 / (distance_matrix + 1e-9)\n\n    # 2. Shortest path estimate (Dijkstra)\n    graph = scipy.sparse.csr_matrix(distance_matrix)\n    shortest_paths = dijkstra(graph, directed=False, indices=range(n))\n\n    # 3. Adaptive Degree Bias\n    degree_penalty = np.ones_like(distance_matrix)\n\n    # Calculate degrees and average degree based on inverse distances\n    degrees = np.sum(inverse_distance, axis=1) - np.diag(inverse_distance)\n    avg_degree = np.mean(degrees)\n\n    # Apply penalty only to edges connected to nodes with above-average degree\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                if degrees[i] > avg_degree:\n                    degree_penalty[i, j] *= max(0.1, avg_degree / (degrees[i] + 1e-9))  # Reduced penalty\n                if degrees[j] > avg_degree:\n                    degree_penalty[i, j] *= max(0.1, avg_degree / (degrees[j] + 1e-9))  # Reduced penalty\n\n    # 4. Combine heuristics: Favor short edges on short paths, penalized by degree\n    heuristic_matrix = inverse_distance * degree_penalty / (shortest_paths + 1e-9)\n\n    # 5. Normalize before sparsification\n    max_heuristic = np.max(heuristic_matrix)\n    if max_heuristic > 0:\n        heuristic_matrix /= max_heuristic\n\n    # 6. Sparsification: Remove weak edges based on percentile\n    threshold = np.percentile(heuristic_matrix[heuristic_matrix > 0], 25)  # More aggressive\n    heuristic_matrix[heuristic_matrix < threshold] = 0\n\n\n    return heuristic_matrix\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"TSP heuristic: Combines inverse distance, degree bias, shortest paths, and introduces controlled randomness with adaptive scaling.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Inverse distance\n    inverse_distance = 1 / (distance_matrix + 1e-9)\n\n    # Degree bias\n    degrees = np.sum(inverse_distance, axis=1)\n    avg_degree = np.mean(degrees)\n    degree_penalty = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty[i, j] = max(0.1, avg_degree / (degrees[i] + degrees[j] + 1e-9)) # Smoother penalty\n            else:\n                degree_penalty[i, j] = 0\n\n    # Shortest path estimate (Dijkstra) - using sparse matrix for efficiency\n    graph = scipy.sparse.csr_matrix(distance_matrix)\n    shortest_paths = dijkstra(graph, directed=False, indices=range(n))\n    shortest_paths_penalty = 1 / (shortest_paths + 1e-9)\n\n    # Introduce controlled randomness - scaled by inverse distance\n    randomness_factor = 0.05  # Adjust for desired exploration\n    random_matrix = np.random.rand(n, n) * randomness_factor * inverse_distance\n\n\n    # Combine heuristics with adaptive scaling\n    alpha = 0.6 # Weight for distance\n    beta = 0.3  # Weight for degree\n    gamma = 0.1 # Weight for shortest path\n\n    heuristic_matrix = alpha * inverse_distance + beta * degree_penalty + gamma * shortest_paths_penalty + random_matrix\n\n    # Sparsification (remove less promising edges early)\n    threshold = np.percentile(heuristic_matrix[heuristic_matrix > 0], 30) # Adjusted percentile\n    heuristic_matrix[heuristic_matrix < threshold] = 0\n\n\n    # Normalize the heuristic matrix\n    max_heuristic = np.max(heuristic_matrix)\n    if max_heuristic > 0:\n        heuristic_matrix /= max_heuristic\n\n    return heuristic_matrix\n\n### Analyze & experience\n- Comparing (1st) vs (17th), we see the top heuristic uses inverse distance, Dijkstra shortest paths, and an adaptive degree bias, sparsification and Normalization. The 17th heuristic also utilizes these features, but incorporates a weighted combination of inverse distance, degree penalty, shortest path, and randomness. The 1st is better, implying the specific combination strategy is superior.\n\nComparing (3rd) vs (4th), the 3rd heuristic introduces controlled randomness if the heuristic matrix is too sparse and applies a final node degree penalty. The 4th heuristic lacks these elements. The 3rd heuristic performs better, suggesting that controlled randomness to ensure connectivity and a final node degree adjustment are beneficial.\n\nComparing (5th) vs (6th), they are exactly the same. It does not give more information to compare.\n\nComparing (19th) vs (20th), they are exactly the same. It does not give more information to compare.\n\nComparing (2nd worst) vs (worst), the 19th utilizes a weighted geometric mean for combining inverse distance and degree penalty, along with sparsification. The 20th is identical, indicating that the geometric mean approach itself isn't necessarily worse; other factors must contribute to the ranking.\n\nOverall: The best heuristics incorporate inverse distance, shortest paths (using Dijkstra), and adaptive degree bias, followed by sparsification and normalization. Introducing randomness can be helpful to escape local optima but needs to be controlled. Weighted combinations of different components and adaptive strategies for sparsification may improve performance. Excessive complexity or poorly weighted combinations can degrade results. Node degree as penalty should apply late stage.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a redefined perspective on \"Current self-reflection\" tailored for improved heuristic design, specifically avoiding the pitfalls of \"Ineffective Self-reflection\":\n\n*   **Keywords:** Adaptive penalties, sparsification thresholds, informed randomness, component normalization, inverse distance, shortest paths, Dijkstra.\n\n*   **Advice:** Systematically combine inverse distance, Dijkstra-based shortest paths, and adaptive high-degree node penalties. Sparsify based on percentile thresholds *before* normalization to emphasize salient edges. Carefully integrate controlled randomness.\n\n*   **Avoid:** Vague statements about \"multiple factors\" or \"different combinations.\" Shun broad statements about \"temperature control based on current heuristic values\".\n\n*   **Explanation:** Focus on concrete mechanisms (Dijkstra, percentile-based sparsification) instead of general concepts. Implement controlled, directed exploration via randomness instead of relying on undirected, random exploration.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}