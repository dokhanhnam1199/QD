{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\nCurrent heuristics:\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"TSP heuristic: Combines inverse distance, adaptive degree bias, and shortest paths with sparsification.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Inverse distance\n    inverse_distance = 1 / (distance_matrix + 1e-9)\n\n    # Shortest path estimate (Dijkstra)\n    graph = scipy.sparse.csr_matrix(distance_matrix)\n    shortest_paths = dijkstra(graph, directed=False, indices=range(n))\n\n    # Adaptive degree bias\n    degree_penalty = np.ones((n, n))\n    degrees = np.sum(inverse_distance, axis=1)\n    avg_degree = np.mean(degrees)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                penalty_i = max(1, avg_degree / (degrees[i] + 1e-9))\n                penalty_j = max(1, avg_degree / (degrees[j] + 1e-9))\n                degree_penalty[i, j] = penalty_i * penalty_j\n            else:\n                degree_penalty[i, j] = 0\n\n    # Combine heuristics\n    heuristic_matrix = inverse_distance * degree_penalty / (shortest_paths + 1e-9)\n\n    # Sparsification\n    threshold = np.percentile(heuristic_matrix[heuristic_matrix > 0], 20)\n    heuristic_matrix[heuristic_matrix < threshold] = 0\n\n    # Normalize\n    max_heuristic = np.max(heuristic_matrix)\n    if max_heuristic > 0:\n        heuristic_matrix /= max_heuristic\n\n    return heuristic_matrix\n\nNow, think outside the box write a mutated function `heuristics_v2` better than current version.\nYou can use some hints below:\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a redefined perspective on \"Current self-reflection\" tailored for improved heuristic design, specifically avoiding the pitfalls of \"Ineffective Self-reflection\":\n\n*   **Keywords:** Adaptive penalties, sparsification thresholds, informed randomness, component normalization, inverse distance, shortest paths, Dijkstra.\n\n*   **Advice:** Systematically combine inverse distance, Dijkstra-based shortest paths, and adaptive high-degree node penalties. Sparsify based on percentile thresholds *before* normalization to emphasize salient edges. Carefully integrate controlled randomness.\n\n*   **Avoid:** Vague statements about \"multiple factors\" or \"different combinations.\" Shun broad statements about \"temperature control based on current heuristic values\".\n\n*   **Explanation:** Focus on concrete mechanisms (Dijkstra, percentile-based sparsification) instead of general concepts. Implement controlled, directed exploration via randomness instead of relying on undirected, random exploration.\n\n\nOutput code only and enclose your code with Python code block: ```python ... ```.\nI'm going to tip $999K for a better solution!"}