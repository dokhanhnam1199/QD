{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"TSP heuristic: shortest paths + adaptive degree bias + sparsification.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # 1. Inverse distance\n    inverse_distance = 1 / (distance_matrix + 1e-9)\n\n    # 2. Shortest path estimate (Dijkstra)\n    graph = scipy.sparse.csr_matrix(distance_matrix)\n    shortest_paths = dijkstra(graph, directed=False, indices=range(n))\n\n    # 3. Degree bias (adaptive penalty)\n    degree_penalty = np.ones((n, n))  # Initialize to 1 for no penalty by default\n    avg_degree = 0\n    degrees = np.sum(inverse_distance, axis=1) - np.diag(inverse_distance)\n    avg_degree = np.mean(degrees)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                # Adaptive degree penalty: only penalize if degree > avg\n                penalty_i = max(1, avg_degree / (degrees[i] + 1e-9))\n                penalty_j = max(1, avg_degree / (degrees[j] + 1e-9))\n                degree_penalty[i, j] = penalty_i * penalty_j\n            else:\n                degree_penalty[i, j] = 0\n\n    # 4. Combine heuristics\n    heuristic_matrix = inverse_distance * degree_penalty / (shortest_paths + 1e-9)\n\n    # 5. Sparsification: Remove edges with low heuristic values\n    threshold = np.percentile(heuristic_matrix[heuristic_matrix > 0], 20) # Keep top 80%\n    heuristic_matrix[heuristic_matrix < threshold] = 0\n\n    # 6. Normalize\n    max_heuristic = np.max(heuristic_matrix)\n    if max_heuristic > 0:  # Avoid division by zero\n        heuristic_matrix /= max_heuristic\n\n    return heuristic_matrix\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines inverse distance with node degree penalty and shortest-path approximation.\n    \"\"\"\n    n = distance_matrix.shape[0]\n\n    # Inverse distance\n    inverse_distance = 1.0 / (distance_matrix + np.eye(n))\n    inverse_distance[np.diag_indices_from(inverse_distance)] = 0.0\n\n    # Node degree penalty (penalize links between popular nodes)\n    node_degrees = np.sum(inverse_distance, axis=0)\n    degree_matrix = np.outer(node_degrees, node_degrees)\n    degree_penalty = 1.0 / (degree_matrix + 1e-9)\n\n    # Heuristic combination\n    heuristic_matrix = inverse_distance * degree_penalty\n\n    return heuristic_matrix\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see (1st) includes sparsification based on a percentile threshold and normalization, while (20th) adds randomness after the core heuristic calculation, and sparsifies based on an absolute threshold;\n(2nd best) vs (second worst), both have similar structures but (2nd) includes Dijkstra for shortest paths and focuses on inverse distance, and (19th) calculates edge centrality and applies randomness; Comparing (1st) vs (2nd), they appear identical; (3rd) vs (4th), (3rd) calculates degree differently using a loop, while (4th) incorporates controlled randomness and adjusts the degree penalty; Comparing (second worst) vs (worst), the degree penalty calculation is different, and (19th) incorporates shortest paths using Floyd-Warshall, whereas (20th) focuses on the inverse distance. Overall: The better heuristics seem to prioritize sparsification based on percentile thresholds, combined with normalization. Adaptive degree penalties, especially when applied to nodes with significantly above-average degrees, also appear beneficial. Adding randomness can be helpful, but it should be controlled and applied judiciously. The shortest path calculation using Dijkstra is consistently present in the better heuristics.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, let's refine that reflection to design better heuristics, focusing on effectiveness and avoiding common pitfalls.\n\n*   **Keywords:** Adaptive Penalties, Multi-Source Information, Controlled Stochasticity, Post-Normalization\n*   **Advice:** Fuse node characteristics (degree) with network structure (shortest paths, distance) adaptively. Experiment with different combination functions (e.g., weighted sums, multiplicative models). Control randomness via temperature parameters.\n*   **Avoid:** Premature normalization, static penalties, isolated component design.\n*   **Explanation:** Combine node-level features and path information. Adaptive penalties based on evolving network state better manage exploration. Combine and *then* normalize to preserve component influence.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}