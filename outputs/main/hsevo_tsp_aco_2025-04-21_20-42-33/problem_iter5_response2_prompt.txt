{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"TSP heuristic: shortest paths + adaptive degree bias + controlled randomness.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # 1. Inverse distance\n    inverse_distance = 1 / (distance_matrix + 1e-9)\n\n    # 2. Shortest path estimate (Dijkstra)\n    graph = scipy.sparse.csr_matrix(distance_matrix)\n    shortest_paths = dijkstra(graph, directed=False, indices=range(n))\n\n    # 3. Degree bias (adaptive penalty)\n    degree_penalty = np.ones((n, n))  # Initialize with ones for no penalty by default\n    avg_degree = 0\n    for i in range(n):\n        degree_i = np.sum(inverse_distance[i, :]) - inverse_distance[i, i]\n        avg_degree += degree_i\n    avg_degree /= n\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_i = np.sum(inverse_distance[i, :]) - inverse_distance[i, i]\n                degree_j = np.sum(inverse_distance[j, :]) - inverse_distance[j, j]\n\n                # Adaptive degree penalty: only penalize if degree > avg\n                if degree_i > avg_degree or degree_j > avg_degree:\n                    penalty_i = max(0.5, avg_degree / (degree_i + 1e-9))  # Reduced penalty\n                    penalty_j = max(0.5, avg_degree / (degree_j + 1e-9))  # Reduced penalty\n                    degree_penalty[i, j] = penalty_i * penalty_j\n            else:\n                degree_penalty[i, j] = 0\n\n    # 4. Combine heuristics (emphasize shortest paths and inverse distance)\n    heuristic_matrix = inverse_distance**1.2 * degree_penalty / (shortest_paths + 1e-9)**0.8\n\n    # 5. Sparsification (remove less promising edges)\n    threshold = np.percentile(heuristic_matrix[heuristic_matrix > 0], 30)  # Keep top 70%\n    heuristic_matrix[heuristic_matrix < threshold] = 0\n\n    # 6. Normalize\n    max_heuristic = np.max(heuristic_matrix)\n    if max_heuristic > 0:\n        heuristic_matrix /= max_heuristic\n\n    # 7. Controlled Randomness (only when no strong signal)\n    if np.sum(heuristic_matrix > 0) < n:  # If too sparse, add some noise\n        random_matrix = np.random.rand(n, n) * 0.1\n        heuristic_matrix += random_matrix\n        max_heuristic = np.max(heuristic_matrix)\n        heuristic_matrix /= max_heuristic\n\n\n    return heuristic_matrix\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"TSP heuristic: shortest paths + adaptive degree bias + controlled randomness.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # 1. Inverse distance\n    inverse_distance = 1 / (distance_matrix + 1e-9)\n\n    # 2. Shortest path estimate (Dijkstra)\n    graph = scipy.sparse.csr_matrix(distance_matrix)\n    shortest_paths = dijkstra(graph, directed=False, indices=range(n))\n\n    # 3. Adaptive degree bias (only penalize high degree nodes)\n    degree_penalty = np.ones_like(distance_matrix)\n    avg_degree = 0\n    for i in range(n):\n        degree_i = np.sum(inverse_distance[i, :]) - inverse_distance[i, i]\n        avg_degree += degree_i\n    avg_degree /= n\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_i = np.sum(inverse_distance[i, :]) - inverse_distance[i, i]\n                degree_j = np.sum(inverse_distance[j, :]) - inverse_distance[j, j]\n\n                # Penalize only if degree is significantly above average\n                if degree_i > 1.5 * avg_degree:\n                    degree_penalty[i, j] *= max(0.5, avg_degree / (degree_i + 1e-9)) # Reduced penalty\n                if degree_j > 1.5 * avg_degree:\n                    degree_penalty[i, j] *= max(0.5, avg_degree / (degree_j + 1e-9)) # Reduced penalty\n            else:\n                degree_penalty[i, j] = 0\n\n    # 4. Combine heuristics: Prioritize inverse distance and shortest paths\n    heuristic_matrix = inverse_distance * degree_penalty / (shortest_paths + 1e-9)\n\n    # 5. Controlled Randomness (only add if heuristic value is non-zero)\n    randomness_factor = 0.01  # Reduced randomness\n    for i in range(n):\n        for j in range(n):\n            if heuristic_matrix[i, j] > 0:\n                heuristic_matrix[i, j] += randomness_factor * np.random.rand()\n\n    # 6. Normalize\n    max_heuristic = np.max(heuristic_matrix)\n    heuristic_matrix /= max_heuristic\n\n    # 7. Sparsify: Remove edges with very low heuristic value\n    threshold = 0.01 #Adjust threshold to control sparcity\n    heuristic_matrix[heuristic_matrix < threshold] = 0\n\n    return heuristic_matrix\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see (1st) includes sparsification based on a percentile threshold and normalization, while (20th) adds randomness after the core heuristic calculation, and sparsifies based on an absolute threshold;\n(2nd best) vs (second worst), both have similar structures but (2nd) includes Dijkstra for shortest paths and focuses on inverse distance, and (19th) calculates edge centrality and applies randomness; Comparing (1st) vs (2nd), they appear identical; (3rd) vs (4th), (3rd) calculates degree differently using a loop, while (4th) incorporates controlled randomness and adjusts the degree penalty; Comparing (second worst) vs (worst), the degree penalty calculation is different, and (19th) incorporates shortest paths using Floyd-Warshall, whereas (20th) focuses on the inverse distance. Overall: The better heuristics seem to prioritize sparsification based on percentile thresholds, combined with normalization. Adaptive degree penalties, especially when applied to nodes with significantly above-average degrees, also appear beneficial. Adding randomness can be helpful, but it should be controlled and applied judiciously. The shortest path calculation using Dijkstra is consistently present in the better heuristics.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, let's refine that reflection to design better heuristics, focusing on effectiveness and avoiding common pitfalls.\n\n*   **Keywords:** Adaptive Penalties, Multi-Source Information, Controlled Stochasticity, Post-Normalization\n*   **Advice:** Fuse node characteristics (degree) with network structure (shortest paths, distance) adaptively. Experiment with different combination functions (e.g., weighted sums, multiplicative models). Control randomness via temperature parameters.\n*   **Avoid:** Premature normalization, static penalties, isolated component design.\n*   **Explanation:** Combine node-level features and path information. Adaptive penalties based on evolving network state better manage exploration. Combine and *then* normalize to preserve component influence.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}