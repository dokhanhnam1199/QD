{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Improved heuristics for TSP based on a combination of distance,\n    node degree centrality, and randomness to encourage exploration.\n\n    Args:\n        distance_matrix (np.ndarray): A square matrix representing the distances\n                                        between cities. distance_matrix[i][j] is the\n                                        distance between city i and city j.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, where each\n                    element represents the heuristic score for including that edge\n                    in a potential TSP tour.  Higher values indicate more promising\n                    edges.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # Calculate node degree centrality (approximation using inverse distance sum)\n    node_centrality = np.sum(1 / (distance_matrix + np.eye(n)), axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                # Base heuristic: inverse distance (shorter distances are better)\n                h = 1 / distance_matrix[i, j]\n\n                # Incorporate node centrality: Preferentially connect to \"important\" nodes\n                # This is normalized so edges between higher degree nodes don't dominate\n\n                h *= (np.sqrt(node_centrality[i] * node_centrality[j])) #Geometric Mean\n                # Small random component to introduce exploration, especially early on\n                h += np.random.rand() * 0.1\n                heuristics[i, j] = h\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Heuristics function for solving the Traveling Salesman Problem (TSP)\n    via stochastic solution sampling. This version incorporates several\n    ideas to improve the quality of the edge priors:\n\n    1.  Inverse distance, as closer cities are more likely to be neighbors\n    2.  Edge Centrality: Edges that lie on shorter paths between many nodes are preferable\n    3.  Nearest Neighbor: Each node is more likely connected with its nearest neighbors\n    4.  A slight global randomization, allowing escaping from local minima.\n\n    Args:\n        distance_matrix (np.ndarray): A 2D numpy array representing the distance\n            matrix between cities. distance_matrix[i][j] is the distance\n            between city i and city j.\n\n    Returns:\n        np.ndarray: A 2D numpy array of the same shape as distance_matrix,\n            representing the prior probabilities (or heuristic scores) for\n            including each edge in a solution.  Higher values indicate a\n            more promising edge.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # 1. Inverse Distance\n    inverse_distance = 1 / (distance_matrix + 1e-9)  # Adding a small value to avoid division by zero\n\n    # 2. Edge Centrality heuristic: approximated via closeness centrality\n\n    # Calculate shortest paths between all pairs of nodes using Floyd-Warshall algorithm, inspired from internet\n    dist = np.copy(distance_matrix)\n    for k in range(n):\n      for i in range(n):\n        for j in range(n):\n          dist[i, j] = min(dist[i, j], dist[i, k] + dist[k, j])\n\n    closeness_centrality = np.zeros((n,n))\n    for i in range(n):\n      for j in range(n):\n        closeness_centrality[i][j] = 1/(dist[i,j] + 1e-9)\n\n\n    # 3. Nearest Neighbor heuristic.  Each node is more likely to be connected to its nearest neighbors.\n    nearest_neighbors = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        # Find the indices of the k-nearest neighbors (excluding itself). Let k be 3\n        k = min(3, n-1) # Handle the edge case that n can be smaller than 3\n\n        neighbors = np.argpartition(distance_matrix[i], k+1)[:k+1]\n        neighbors = neighbors[neighbors != i]\n        if len(neighbors)> 0:\n\n          for j in neighbors:\n            nearest_neighbors[i][j] = 1 # Prioritize to connect to neighbors.\n\n    #4. Incorporate heuristics scores\n    heuristics = inverse_distance + closeness_centrality + nearest_neighbors\n    # Normalize to be between 0 and 1\n    heuristics = (heuristics - np.min(heuristics)) / (np.max(heuristics) - np.min(heuristics) + 1e-9)\n\n    # 5. Add a small amount of randomness to allow exploring different routes\n    randomness = np.random.rand(n, n) * 0.1\n    heuristics += randomness\n    heuristics = np.clip(heuristics, 0, 1)  # Clip values to be within [0, 1]\n\n    return heuristics\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see that the best heuristic incorporates shortest paths using Dijkstra's algorithm to estimate proximity, and penalizes connections between high-degree nodes and edges that are part of a long path. The worst uses a combination of inverse distance, node degree preference, and simulated annealing-inspired perturbation.\n\nComparing (2nd) vs (19th), the second-best heuristic combines inverse distance, node degree bias, and global average distance, boosting edges significantly shorter than average. The 19th considers inverse distance and local density around each node.\n\nComparing (1st) vs (2nd), the first heuristic utilizes Dijkstra's algorithm and shortest path estimations, while the second relies on global average distance. This suggests the shortest path calculations contribute more effectively.\n\nComparing (3rd) vs (4th), the third prioritizes nodes with fewer close neighbors and penalizes long edges to well-connected nodes. The fourth prioritizes shorter edges, penalizes edges on longer paths, adds randomness, and ensures non-zero heuristic values. The more sophisticated degree adjustment seems advantageous.\n\nComparing (second worst) vs (worst), the 19th combines inverse distance with local density. The 20th uses inverse distance, node degree penalties, and simulated annealing-inspired perturbation. The inclusion of randomness doesn't seem to boost its position.\n\nOverall: The top-performing heuristics consider a combination of inverse distance, node degree, and shortest path information or global distance awareness, while penalizing connections between high-degree nodes. The use of shortest paths (via Dijkstra or similar) appears more effective than global averages or local density alone. Randomness, although often included, doesn't guarantee better results. More informed exploration strategies, like those based on node degree or shortest paths, tend to perform better.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, I will help you redefine \"Current Self-Reflection\" to design better heuristics, avoiding pitfalls and focusing on effective strategies. Here's a breakdown to guide the process:\n\n*   **Keywords:** Informed Exploration, Node Degree Penalty, Inverse Distance, Shortest Path Integration, Controlled Randomness, Solution Balancing.\n\n*   **Advice:** Prioritize shortest path information and inverse distance metrics, then gently bias exploration by penalizing high-degree node connections only when necessary. Use randomization sparingly, guided by the quality of the existing solution.\n\n*   **Avoid:** Blindly penalizing high-degree nodes, uncontrolled or excessive randomness, neglecting shortest path information and solution balance.\n\n*   **Explanation:** Combining these factors ensures the heuristic balances exploration with exploitation of good solutions, leverages distance and path information, and avoids getting stuck in local optima due to over-penalization or excessive randomness.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}