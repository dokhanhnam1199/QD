{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"TSP heuristic: Combines inverse distance, adaptive degree bias, and shortest paths with sparsification.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Inverse distance\n    inverse_distance = 1 / (distance_matrix + 1e-9)\n\n    # Shortest path estimate (Dijkstra)\n    graph = scipy.sparse.csr_matrix(distance_matrix)\n    shortest_paths = dijkstra(graph, directed=False, indices=range(n))\n\n    # Adaptive degree bias\n    degree_penalty = np.ones((n, n))\n    degrees = np.sum(inverse_distance, axis=1)\n    avg_degree = np.mean(degrees)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                penalty_i = max(1, avg_degree / (degrees[i] + 1e-9))\n                penalty_j = max(1, avg_degree / (degrees[j] + 1e-9))\n                degree_penalty[i, j] = penalty_i * penalty_j\n            else:\n                degree_penalty[i, j] = 0\n\n    # Combine heuristics\n    heuristic_matrix = inverse_distance * degree_penalty / (shortest_paths + 1e-9)\n\n    # Sparsification\n    threshold = np.percentile(heuristic_matrix[heuristic_matrix > 0], 20)\n    heuristic_matrix[heuristic_matrix < threshold] = 0\n\n    # Normalize\n    max_heuristic = np.max(heuristic_matrix)\n    if max_heuristic > 0:\n        heuristic_matrix /= max_heuristic\n\n    return heuristic_matrix\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n\n    \"\"\"Combines inverse distance, shortest paths, and adaptive degree penalty.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # Inverse distance\n    inverse_distance = 1 / (distance_matrix + 1e-9)\n\n    # Shortest paths (Dijkstra's)\n    shortest_paths = np.zeros((n, n))\n    for i in range(n):\n        dist = np.full(n, np.inf)\n        visited = np.zeros(n, dtype=bool)\n        dist[i] = 0\n        for _ in range(n):\n            u = np.argmin(dist + visited * np.inf)\n            visited[u] = True\n            for v in range(n):\n                if distance_matrix[u, v] > 0 and dist[v] > dist[u] + distance_matrix[u, v]:\n                    dist[v] = dist[u] + distance_matrix[u, v]\n        shortest_paths[i, :] = dist\n\n    # Node degree (inverse distance based)\n    node_degree = np.sum(1 / (distance_matrix + np.eye(n)), axis=1)\n    mean_distance = np.mean(distance_matrix[distance_matrix > 0])\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                h = inverse_distance[i, j]\n\n                # Adaptive degree penalty\n                degree_penalty = 1 / (np.sqrt(node_degree[i] * node_degree[j]) + 1e-9)\n                h *= degree_penalty\n\n                # Favor edges on shorter paths\n                h *= np.exp(-shortest_paths[i, j] / mean_distance)\n\n                heuristics[i, j] = h\n\n    # Normalize\n    heuristics = (heuristics - np.min(heuristics)) / (np.max(heuristics) - np.min(heuristics) + 1e-9)\n\n    # Sparsify based on percentile threshold.  More aggressive sparsification.\n    threshold = np.percentile(heuristics[heuristics > 0], 50)\n    heuristics[heuristics < threshold] = 0\n\n    return heuristics\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), the first heuristic uses sparsification with a fixed percentile (20), while the second uses a higher percentile (75) and adds controlled randomness. The first one performs sparsification before normalization while the second performs sparsification after normalization.\n\nComparing (2nd) vs (3rd), the third heuristic introduces stochasticity based on heuristic values (higher heuristic -> lower temperature) and uses a more aggressive sparsification (30th percentile).  The second heuristic adds randomness with a fixed temperature. The third heuristic makes the temperature adaptive.\n\nComparing (3rd) vs (4th), the fourth heuristic only applies the degree penalty if the degree is above the average and adds a small amount of randomness, ensuring non-zero values. The third heuristic applies a degree penalty regardless of being above average and applies a stochastic temperature scaled by heuristic value.\n\nComparing (4th) vs (5th), the fifth heuristic reduces the penalty applied to nodes with above-average degrees.\n\nComparing (5th) vs (6th), they are identical.\n\nComparing (1st) vs (17th), the adaptive degree penalty in (17th) applies penalty only when both nodes have row sums greater than the mean while first heuristic always applies a degree penalty.\n\nComparing (16th) vs (17th), The adaptive node degree penalty in 17th simply divides the heuristic matrix. In 16th, a pheromone inspired component is incorporated as a weighted sum.\n\nComparing (19th) vs (20th), they are identical. Comparing (1st) vs (20th), (1st) applies adaptive degree bias via geometric means, and sparsifies via percentile, whereas 20th uses weighted sum with degree penalty and a \"node potential\" term.\n\nOverall: The better heuristics tend to combine inverse distance with shortest paths and adaptive degree biases. Sparsification, controlled randomness, and normalization are also important. Adaptive temperature based on heuristic value and using geometric mean seems to perform better than using constant temperature or weighted sums. Penalizing high-degree nodes, encouraging exploration, and using node potential can further improve the heuristic. Applying sparsification earlier seems beneficial.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a refined view of self-reflection for designing better TSP heuristics, focusing on actionability and avoiding common pitfalls:\n\n*   **Keywords:** Component importance, adaptive penalties, sparsification timing, controlled randomness.\n\n*   **Advice:** Quantify the impact of each component (distance, degree, shortest path) during heuristic construction. Track their influence on solution quality to guide adaptive parameter adjustments.\n\n*   **Avoid:** Premature commitment to fixed parameter settings. Avoid overly complex, untestable combinations of components initially.\n\n*   **Explanation:** Continuous monitoring and evaluation are crucial. Instead of assuming a combination works, measure its effect.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}