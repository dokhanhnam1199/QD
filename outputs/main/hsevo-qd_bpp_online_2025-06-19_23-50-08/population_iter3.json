[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    ratios = item / bins_remain_cap\n    log_ratios = np.log(ratios)\n    priorities = -log_ratios\n    return priorities",
    "response_id": 0,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 11.60964047443681,
    "mi": 94.04446327225541,
    "token_count": 47.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Prioritize bins where the item fits well, leaving minimal wasted space.\n    # However, strongly penalize bins where the item *doesn't* fit.\n\n    fit = item <= bins_remain_cap\n    waste = bins_remain_cap - item\n    \n    priorities[~fit] = -np.inf  # Impossible to fit, very low priority\n\n    # Emphasize filling up more capacity (higher fill rate is better), and punish little wasted space to incentivize near-perfect fills, avoid smaller fragments.\n\n    priorities[fit] = (item / bins_remain_cap[fit]) + np.exp(-waste[waste >= 0] * 10) # Adjusted exponent\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 199.7052750908657,
    "mi": 82.52195939041982,
    "token_count": 162.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins considering fill rate, waste, and infeasibility.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    fit = item <= bins_remain_cap\n    waste = bins_remain_cap - item\n\n    priorities[~fit] = -np.inf  # Impossible to fit\n\n    # Fill ratio + waste penalty, adjusted penalty strength for finer control.\n    priorities[fit] = (item / bins_remain_cap[fit]) + np.exp(-waste[waste >= 0] * 5)\n\n    # Refinement: slight bonus for bins that will have remaining capacity close to the average item size.\n    avg_item_size = np.mean(item)  # Average item for online setting\n    remaining_after_fit = bins_remain_cap[fit] - item\n    priorities[fit] += np.exp(-np.abs(remaining_after_fit - avg_item_size) * 2) # Add bonus\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 3.9589150378939015,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 103.72627427729671,
    "mi": 91.14963151192664,
    "token_count": 105.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response1.txt_stdout.txt",
    "code_path": "problem_iter3_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins considering fill rate, waste, and future fit.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    bin_capacity = np.max(bins_remain_cap) # Assuming bins are of same capacity.\n\n    fit = item <= bins_remain_cap\n    waste = bins_remain_cap - item\n\n    priorities[~fit] = -np.inf  # Impossible to fit\n\n    if np.any(fit):\n        # Fill ratio + waste penalty, adjusted penalty strength for finer control.\n        priorities[fit] = (item / bins_remain_cap[fit]) + np.exp(-waste[fit] * 5)\n\n        # Heuristic 1: Prioritize bins with remaining capacity closest to common item sizes to reduce fragmentation\n        # Consider a range of common item sizes (e.g., 1/4, 1/3, 1/2 of bin capacity)\n        common_sizes = np.array([bin_capacity/4, bin_capacity/3, bin_capacity/2])\n        remaining_after_fit = bins_remain_cap[fit] - item\n        size_diffs = np.abs(remaining_after_fit[:, np.newaxis] - common_sizes)\n        min_size_diffs = np.min(size_diffs, axis=1) #Closest match\n        priorities[fit] += np.exp(-min_size_diffs * 3)\n\n        # Heuristic 2: Slightly penalize bins that become nearly full after packing to leave room for small adjustments.\n        nearly_full = (waste[fit] / bin_capacity) < 0.1\n        priorities[fit][nearly_full] -= 0.2  # Slightly demote\n\n        #Heuristic 3: Bonus for bins that have a remaining capacity that can fit at least a certain percentage (e.g., 20%) of the current item. Prevents creating too small waste.\n        can_fit_part = (bins_remain_cap[fit] - item) >= (0.2 * item)\n        priorities[fit][can_fit_part] += 0.1\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 3.9988033506182825,
    "SLOC": 13.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 310.19550008653874,
    "mi": 86.47900540549108,
    "token_count": 187.0,
    "exec_success": true
  }
]