{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins by favoring exact fits, then tightest fits, and finally smallest suitable bins.\n\n    Combines the strict preference for exact fits from Heuristic 3 with the\n    refined Best Fit logic of Heuristic 1, creating a clear preference hierarchy.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    \n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Priority 1: Exact Fits (highest priority)\n    # Assign a high score to bins where remaining capacity exactly matches the item size.\n    exact_fit_mask = (bins_remain_cap == item) & can_fit_mask\n    priorities[exact_fit_mask] = 2.0  # Strict preference for exact fits\n\n    # Mask for bins that can fit but are not exact fits\n    suitable_but_not_exact_mask = can_fit_mask & ~exact_fit_mask\n\n    if np.any(suitable_but_not_exact_mask):\n        suitable_bins_remain_cap = bins_remain_cap[suitable_but_not_exact_mask]\n        \n        # Priority 2: Tightest Fits (second highest priority)\n        # Calculate priority based on how close the remaining capacity is to the item size.\n        # A smaller difference (residual capacity) gets a higher score.\n        # We use 1 / (difference + epsilon) to give higher scores to smaller differences.\n        differences = suitable_bins_remain_cap - item\n        tight_fit_scores = 1.0 / (differences + 1e-9)\n        \n        # Normalize these scores so they are between 0 and 1, and less than 2.0\n        # This ensures exact fits still have the highest priority.\n        if np.max(tight_fit_scores) > 0:\n            normalized_tight_fit_scores = (tight_fit_scores / np.max(tight_fit_scores)) * 0.9 + 0.1 # Scale to 0.1-1.0\n        else:\n            normalized_tight_fit_scores = np.zeros_like(suitable_bins_remain_cap)\n\n        # Assign these refined priorities\n        priorities[suitable_but_not_exact_mask] = normalized_tight_fit_scores\n\n    # Bins where the item doesn't fit retain their default priority of 0.0\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Exact Fit with a refined Best Fit, prioritizing snug fits\n    and then bins that minimize remaining capacity, ensuring clarity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    remaining_capacities_after_packing = suitable_bins_caps - item\n\n    # Assign highest priority to exact fits (tolerance for floating point issues)\n    tolerance = 1e-9\n    exact_fit_mask = np.abs(remaining_capacities_after_packing) < tolerance\n    \n    if np.any(exact_fit_mask):\n        priorities[suitable_bins_mask][exact_fit_mask] = 1.0\n\n    # For non-exact fits, prioritize based on Best Fit principle:\n    # minimize remaining capacity after packing.\n    # Use a score that is inversely proportional to the remaining capacity.\n    # Add a small epsilon to avoid division by zero if remaining_after_fit is 0 (though covered by exact fit).\n    non_exact_fit_mask = ~exact_fit_mask\n    \n    if np.any(non_exact_fit_mask):\n        non_exact_remain_caps = remaining_capacities_after_packing[non_exact_fit_mask]\n        \n        # Higher score for smaller remaining capacity after packing\n        # Use a scaled inverse to keep scores in a reasonable range and prioritize tighter fits.\n        # We scale by the minimum remaining capacity among these non-exact fits.\n        min_remaining_among_non_exact = np.min(non_exact_remain_caps)\n        scaled_scores = 1.0 / (1.0 + (non_exact_remain_caps - min_remaining_among_non_exact))\n\n        # Normalize scores for non-exact fits to be between 0 and 0.9\n        # This ensures exact fits (score 1.0) are always preferred.\n        max_score = np.max(scaled_scores)\n        min_score = np.min(scaled_scores)\n        if max_score > min_score:\n            normalized_scores = 0.9 * (scaled_scores - min_score) / (max_score - min_score)\n        else:\n            normalized_scores = np.full_like(scaled_scores, 0.45) # Neutral score if all are equal\n\n        # Assign these scores to the appropriate bins\n        priorities[suitable_bins_mask][non_exact_fit_mask] = normalized_scores\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Exact fit + graded Best Fit) vs. Heuristic 10 (Basic inverse of remaining capacity without handling exact fits or suitability): Heuristic 1 is superior due to its explicit handling of exact fits and its more nuanced scoring for non-exact fits, leading to more predictable and often better packing.\n\nComparing Heuristic 1 vs. Heuristic 12 (Simple inverse of remaining capacity for fitting bins): Heuristic 1's explicit high priority for exact fits (score 2.0) and graded scoring for non-exact fits (1.0 / diff) provides a more robust \"Best Fit\" approach than Heuristic 12's simple inverse, which might not distinguish between exact fits and other close fits as effectively.\n\nComparing Heuristic 2 (Modified Best Fit with perfect fit prioritization) vs. Heuristic 11 (Balanced Fit, prioritizing moderate remaining capacity): Heuristic 2's explicit prioritization of perfect fits (score 2.0) and then a graded \"least remaining capacity\" for others is a clear hierarchy. Heuristic 11's approach of peaking at a moderate remaining capacity (using `r * exp(-k*r)`) is a different strategy that might perform better in some scenarios by avoiding overly full bins, but lacks the clear initial preference for exact fits that Heuristic 2 provides.\n\nComparing Heuristic 18/19 (Exact fit + scaled Best Fit) vs. Heuristic 1 (Exact fit + graded Best Fit): Heuristics 18/19 use a scaled inverse of the difference for non-exact fits, normalized to a range below the exact fit priority. Heuristic 1 uses a simpler inverse. Heuristics 18/19 offer a more controlled prioritization for non-exact fits, ensuring they are clearly secondary to exact fits.\n\nComparing Heuristic 16 (First Fit simulation) vs. Heuristic 20 (First Fit simulation with minor index bias): Both are attempts to mimic First Fit (FF). Heuristic 16 and 20 assign a high priority to the *first* bin that fits. The minor bias in Heuristic 20 is an artificial way to favor earlier bins, which is inherent in FF's sequential nature. However, a true FF implementation typically involves sequential iteration, not just scoring. These are less sophisticated than \"Best Fit\" variations.\n\nOverall: Heuristics that explicitly prioritize exact fits and then use a graded approach for \"Best Fit\" (like 1, 18, 19) or combine exact fits with a well-defined secondary strategy (like 2, 9) tend to be better. Simple \"Best Fit\" (12, 13) or variations that aim for moderate fill (11, 14, 15) are also reasonable. Heuristics simulating First Fit (16, 20) or basic inverse scoring without refinement (10) are generally less robust.\n- \nHere's a redefinition of self-reflection for designing better heuristics:\n\n*   **Keywords:** Objective-driven, measurable improvement, adaptive strategies, principled design.\n*   **Advice:** Focus on *why* a strategy works. Does it directly address a quantifiable objective (e.g., minimizing waste)? Explore how to *adapt* to different problem characteristics, rather than just mimicking existing ones.\n*   **Avoid:** Overly complex, arbitrary mathematical functions (like `r * exp(-k*r)`) without a clear theoretical or empirical justification for their specific form. Avoid \"simulating\" known heuristics if a more direct, objective-driven approach is possible.\n*   **Explanation:** The goal is to build heuristics that are understood through their objective alignment and demonstrable performance improvements, not through the intricacy of their mathematical formulation or a blind imitation of prior methods.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}