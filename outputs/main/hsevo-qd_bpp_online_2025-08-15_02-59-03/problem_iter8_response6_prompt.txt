{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin.\n    This heuristic prioritizes bins that can fit the item and, among those,\n    favors bins that would leave the least remaining capacity after packing (Best Fit).\n    Additionally, it assigns a higher priority to bins that are \"closer\" to being full,\n    as this might encourage packing smaller items into less utilized bins first,\n    potentially leaving larger capacities for larger items later.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        # Calculate the remaining capacity for suitable bins after packing the item\n        remaining_capacities_after_packing = bins_remain_cap[suitable_bins_mask] - item\n\n        # Find the minimum remaining capacity (Best Fit criterion)\n        min_remaining_capacity = np.min(remaining_capacities_after_packing)\n\n        # Create a mask for bins that achieve the best fit\n        best_fit_mask = suitable_bins_mask & (bins_remain_cap - item == min_remaining_capacity)\n\n        # Assign a base priority to bins that are a best fit\n        priorities[best_fit_mask] = 1.0\n\n        # Introduce a secondary priority: favor bins that are less empty (closer to full)\n        # This can be done by inverting the remaining capacity (before packing) or using the original capacity.\n        # Let's use the original remaining capacity, higher values mean more available space.\n        # We want to prioritize bins that have *less* remaining capacity, so we can invert this.\n        # However, simply inverting might lead to negative priorities if we normalize.\n        # A better approach is to use a value that increases as capacity decreases.\n        # Let's consider bins that are *not* the absolute best fit, but still suitable.\n        # For these bins, we can assign a priority based on how \"tight\" the fit is,\n        # or how much remaining capacity they have.\n        # Let's refine: For bins that are not the absolute best fit, assign a score\n        # proportional to how much *less* remaining capacity they have compared to\n        # the largest available capacity among suitable bins. This encourages using\n        # bins that are already somewhat full.\n\n        suitable_bins_original_capacities = bins_remain_cap[suitable_bins_mask]\n        max_suitable_capacity = np.max(suitable_bins_original_capacities)\n\n        # For bins that are suitable but not the best fit:\n        # Assign a priority inversely proportional to their remaining capacity.\n        # Higher priority for bins with less remaining capacity.\n        # We can scale this to avoid overlapping with the '1.0' priority of best-fit bins.\n        # A simple approach is to assign a priority value between 0 and 1,\n        # where 0.5 might be a good intermediate value.\n        # The priority should be higher for smaller remaining_capacities_after_packing.\n        # Let's map remaining_capacities_after_packing to a value from 0 to 0.5.\n        # Higher priority for smaller remaining_capacities_after_packing.\n\n        non_best_fit_suitable_mask = suitable_bins_mask & ~best_fit_mask\n\n        if np.any(non_best_fit_suitable_mask):\n            non_best_fit_remaining_caps_after = bins_remain_cap[non_best_fit_suitable_mask] - item\n            # Normalize these remaining capacities to be between 0 and 0.5\n            # A simple linear scaling:\n            # If there's only one non-best-fit bin, it gets 0.5.\n            # If there are multiple, map the smallest remaining capacity to 0.5 and largest to 0.\n            min_non_best_fit_rem_cap = np.min(non_best_fit_remaining_caps_after)\n            max_non_best_fit_rem_cap = np.max(non_best_fit_remaining_caps_after)\n\n            if max_non_best_fit_rem_cap == min_non_best_fit_rem_cap:\n                # All non-best-fit bins have the same remaining capacity\n                priorities[non_best_fit_suitable_mask] = 0.5\n            else:\n                # Scale remaining capacities to the range [0, 0.5)\n                scaled_priorities = 0.5 * (1 - (non_best_fit_remaining_caps_after - min_non_best_fit_rem_cap) / (max_non_best_fit_rem_cap - min_non_best_fit_rem_cap))\n                priorities[non_best_fit_suitable_mask] = scaled_priorities\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a hybrid strategy:\n    Prioritize bins that are a \"tight fit\" (minimizing waste), but also consider\n    bins that have a moderate amount of remaining capacity to potentially accommodate\n    future larger items or to avoid creating many small, unusable spaces.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_mask]\n\n    # Strategy 1: Tight fit (Best Fit)\n    # Calculate remaining capacity if item is placed\n    remaining_after_fit = suitable_bins_capacities - item\n    min_remaining = np.min(remaining_after_fit)\n    tight_fit_mask_local = (suitable_bins_capacities - item) == min_remaining\n\n    # Strategy 2: Moderate capacity (to potentially fit larger items later)\n    # This is a bit heuristic, let's consider bins that have capacity\n    # roughly between the item size and the bin capacity, avoiding extreme\n    # both very full and very empty bins for this secondary score.\n    # We can normalize remaining capacities and assign a score.\n    # Let's define a \"good enough\" fit as having remaining capacity that is\n    # not too small (but not necessarily the absolute smallest) and not too large.\n    # A simple approach is to give higher priority to bins with remaining capacity\n    # that is closer to the item size, but not exactly the tightest fit.\n\n    # Calculate a \"wastefulness\" score for each suitable bin. Lower is better.\n    # We want to minimize (bin_capacity - item).\n    waste_score = suitable_bins_capacities - item\n\n    # Normalize waste score to be between 0 and 1 (where 0 is best).\n    # Avoid division by zero if all suitable bins have exactly item size capacity.\n    if np.max(waste_score) > 0:\n        normalized_waste_score = waste_score / np.max(waste_score)\n    else:\n        normalized_waste_score = np.zeros_like(waste_score)\n\n    # Create a composite score: prioritize tight fits, but give some boost\n    # to bins that are not excessively empty.\n    # We can use a function that gives higher values to smaller waste.\n    # For example, 1 / (1 + waste_score) or exp(-waste_score).\n    # Let's try a simpler approach: a weighted sum.\n    # High score for tightest fits, moderate score for other suitable bins.\n\n    # Assign a base score to all suitable bins.\n    base_priority = 0.1 # Small baseline for being suitable\n\n    # Boost the tightest fits.\n    priorities[suitable_bins_mask][tight_fit_mask_local] = 1.0\n\n    # For other suitable bins, assign a priority based on how much space is left.\n    # We want to prefer bins that leave less space, but not *too* little (which is the tight fit).\n    # A score that increases as remaining capacity decreases, but capped.\n    # Let's assign a score that is inversely proportional to the remaining capacity AFTER packing.\n    # We'll scale this to avoid overpowering the 'best fit' score.\n    other_suitable_indices = np.where(suitable_bins_mask)[0][~tight_fit_mask_local]\n    if other_suitable_indices.size > 0:\n        other_capacities = bins_remain_cap[other_suitable_indices] - item\n        # Assign priority based on inverse of remaining capacity (smaller remaining is better)\n        # Add a small epsilon to avoid division by zero if a bin is perfectly filled\n        inverse_capacity_score = 1.0 / (other_capacities + 1e-6)\n        # Normalize this score so it doesn't dominate the '1.0' from tight fit\n        max_inverse_score = np.max(inverse_capacity_score)\n        if max_inverse_score > 0:\n            normalized_inverse_score = inverse_capacity_score / max_inverse_score\n        else:\n            normalized_inverse_score = np.zeros_like(inverse_capacity_score)\n\n        # Combine baseline, tight fit boost, and moderate fit score.\n        # The tightest fit gets 1.0. Other suitable bins get a score based on inverse capacity.\n        # We need to ensure that the 'best fit' priority (1.0) is respected.\n        # If a bin is the tightest fit, its priority is 1.0.\n        # For others, it's a scaled inverse capacity.\n\n        # Recalculate priorities for 'other' suitable bins\n        priorities[other_suitable_indices] = 0.5 * normalized_inverse_score # Scale it down to be less than 1.0\n\n    # Ensure that the best fits truly have the highest priority and that other suitable bins\n    # have a priority higher than 0 but lower than the best fit.\n    # If any bin was marked as a tight fit, its priority is 1.0.\n    # Other suitable bins get their calculated score, which should be < 1.0.\n    # If a bin is suitable but not a tight fit, and not in 'other_suitable_indices' (which shouldn't happen if logic is correct),\n    # it will remain 0.\n    # The logic above correctly assigns 1.0 to tight fits and <1.0 to other suitable bins.\n\n    return priorities\n\n### Analyze & experience\n- *   **Heuristics 1 vs 8 (and 2, 4, 10, 14, 17, 18, 20):** Heuristic 1 (Exact Fit, then Best Fit with penalty) is more direct and explicit about its prioritization. Heuristics 2, 4, 8, 10, 14, 17, 20 use sigmoid or exponential functions to model \"tightness\" or \"moderate fit\". While potentially more nuanced, they are less interpretable and rely on tuned parameters (`scaling_factor`, `steepness`, `k`). Heuristic 1's clear hierarchy of Exact Fit > Best Fit is generally a strong and understandable strategy.\n*   **Heuristics 1 vs 3:** Heuristic 1 uses a distinct highest priority (1.0) for exact fits and then scales other fits. Heuristic 3 uses a higher priority (2.0) for exact fits, which is a clearer way to denote absolute preference. However, Heuristic 1's approach of using 1.0 for exact fits and then scaling others below that is also valid. Heuristic 3's score for non-exact fits (`1.0 / (difference + 1e-9)`) can lead to very high priorities for very small differences, potentially more volatile than Heuristic 1's approach.\n*   **Heuristics 1 vs 5 & 6 (and 9):** Heuristics 5, 6, and 9 introduce a secondary criterion (favoring bins that are less empty). Heuristic 1 prioritizes tightest fits after exact fits. The introduction of secondary criteria adds complexity and tunable parameters (like the scaling for non-best-fit bins) that might not always be beneficial. Heuristic 1's focused approach is simpler.\n*   **Heuristics 1 vs 7 & 9:** Heuristics 7 and 9 attempt to refine Best Fit by penalizing \"too full\" bins or by favoring a peak at a moderate remaining capacity (`r_after * exp(-k*r_after)`). While these are interesting \"outside the box\" ideas, they introduce more parameters (`tolerance`, `k`, `weights`, `tightness_threshold_factor`) and are less straightforward than Heuristic 1's clear Best Fit logic.\n*   **Heuristics 1 vs 13 (First Fit):** Heuristic 1 (Best Fit) is generally considered better than First Fit for minimizing the number of bins, as it makes more informed local decisions. Heuristic 13 is a basic implementation of First Fit and doesn't use priority scores in a way that allows for selection among multiple fitting bins.\n*   **Heuristics 1 vs 15 & 17:** Heuristics 15 and 17 combine exact fit (1.0) with a sigmoid score for tight fits. The sigmoid approach (`1 / (1 + exp(-k * (ratio - 1)))`) aims to penalize larger bins. Heuristic 1's approach is simpler: exact fits first, then pure Best Fit (implicitly, by penalizing bins that cannot fit). The sigmoid adds complexity with the `k` parameter.\n*   **Heuristics 1 vs 16 & 20:** Heuristics 16 and 20 use a multi-component scoring system (tightness, anti-tightness, fullness preference) with weights. This is significantly more complex and parameter-dependent than Heuristic 1.\n*   **Heuristics 1 vs 18:** Heuristic 18 combines exact fit (1.0) with inverse remaining capacity, scaled. It also assigns a low priority to unsuitable bins. Heuristic 1's explicit penalty for unsuitable bins (implicitly by assigning 0 priority) and its clear two-tier Best Fit logic make it more understandable.\n*   **Overall:** Heuristic 1 stands out for its clarity, simplicity, and effective combination of two strong strategies (Exact Fit First and Best Fit). It avoids complex, tunable functions or multi-component scores, making it a robust and easy-to-understand heuristic. The other heuristics introduce complexity (sigmoid, multi-factor scoring, peak functions) often tied to specific tunable parameters or less direct logic.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Simplicity, Robustness, Interpretability, Explicit Penalties, Logical Hierarchy.\n*   **Advice:** Focus on intuitive, rule-based strategies. Define a clear, prioritized sequence of simple objectives. Implement explicit checks and penalties for invalid or suboptimal choices.\n*   **Avoid:** Complex, opaque mathematical functions without empirical validation. Arbitrary weighting or blending of criteria. Mimicking sequential logic through static scores.\n*   **Explanation:** Simpler heuristics are easier to debug, adapt, and understand, leading to more reliable performance across diverse problem instances. Explicitly addressing constraints and undesirable states through penalties makes the heuristic's behavior predictable.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}