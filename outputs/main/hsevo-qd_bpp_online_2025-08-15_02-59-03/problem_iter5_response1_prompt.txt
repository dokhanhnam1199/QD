{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a modified Best Fit strategy.\n    This version prioritizes bins that have just enough space for the item, and among those,\n    prefers bins that will have the least remaining capacity after packing.\n    It also introduces a slight preference for bins that have been used less (i.e., higher remaining capacity)\n    if no \"perfect fit\" is found, to encourage better space utilization over time.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    remaining_capacities_after_packing = suitable_bins_caps - item\n\n    # Identify bins that are a \"perfect fit\" or very close\n    # A small tolerance is introduced to capture bins that are \"almost\" a perfect fit\n    tolerance = 1e-6\n    perfect_fit_mask = np.abs(remaining_capacities_after_packing) < tolerance\n\n    if np.any(perfect_fit_mask):\n        # Among perfect fits, prefer those with the absolute minimum remaining capacity (which is close to zero)\n        # This is essentially Best Fit for perfect fits.\n        min_remaining_cap_among_perfect = np.min(remaining_capacities_after_packing[perfect_fit_mask])\n        best_fit_perfect_mask = (suitable_bins_caps - item == min_remaining_cap_among_perfect)\n\n        # Map back to original indices\n        original_indices_perfect = np.where(suitable_bins_mask)[0][best_fit_perfect_mask]\n        priorities[original_indices_perfect] = 2.0 # Highest priority\n\n    else:\n        # If no perfect fit, fall back to a modified Best Fit strategy.\n        # Prioritize bins that leave the least remaining capacity, but not too much.\n        # We'll give a higher score to bins that have a small remaining capacity after packing,\n        # but not so small that it's completely unusable for future small items.\n        # The score is inversely related to the remaining capacity after packing.\n        # To avoid extreme values, we use a scaled inverse.\n        min_remaining_cap_overall = np.min(remaining_capacities_after_packing)\n        \n        # Assign scores based on how close the remaining capacity is to the minimum\n        # Higher score for bins that are closer to the minimum remaining capacity\n        scores_for_suitable = 1.0 / (1.0 + remaining_capacities_after_packing - min_remaining_cap_overall)\n        \n        # Normalize scores to be between 0 and 1\n        max_score = np.max(scores_for_suitable)\n        min_score = np.min(scores_for_suitable)\n        if max_score > min_score:\n            normalized_scores = (scores_for_suitable - min_score) / (max_score - min_score)\n        else:\n            normalized_scores = np.ones_like(scores_for_suitable) * 0.5 # Assign a neutral score if all are equal\n\n        # Map back to original indices\n        original_indices_suitable = np.where(suitable_bins_mask)[0]\n        priorities[original_indices_suitable] = normalized_scores\n        \n        # Ensure perfect fits (if any were missed by tolerance) still get top priority\n        if np.any(np.abs(remaining_capacities_after_packing) < tolerance):\n            min_remaining_cap_close_to_zero = np.min(remaining_capacities_after_packing[np.abs(remaining_capacities_after_packing) < tolerance])\n            best_fit_close_mask = (suitable_bins_caps - item == min_remaining_cap_close_to_zero)\n            original_indices_close = np.where(suitable_bins_mask)[0][best_fit_close_mask]\n            priorities[original_indices_close] = np.maximum(priorities[original_indices_close], 1.5) # Boost priority\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins based on tight fit, favoring exact fits then closest fits.\n\n    Combines exact fit (priority 1.0) with a tight fit metric using sigmoid\n    on the inverse of remaining capacity relative to item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # If there are no bins that can fit the item, return all zeros\n    if fitting_bins_remain_cap.size == 0:\n        return priorities\n\n    # Prioritize exact fits with the highest score\n    exact_fit_mask_for_fitting = np.abs(fitting_bins_remain_cap - item) < epsilon\n    priorities[can_fit_mask][exact_fit_mask_for_fitting] = 1.0\n\n    # For bins that can fit but are not exact fits, use a sigmoid score\n    # that favors bins with less remaining capacity (tighter fit).\n    # The metric is `item / remaining_capacity`. Closer to 1 is better.\n    # We use `sigmoid(k * (item / remaining_capacity - 1))`\n    # k controls the steepness. We choose a moderate k.\n    # We need to apply this only to bins that are not exact fits.\n    non_exact_fit_mask_for_fitting = ~exact_fit_mask_for_fitting\n\n    if np.any(non_exact_fit_mask_for_fitting):\n        non_exact_fitting_capacities = fitting_bins_remain_cap[non_exact_fit_mask_for_fitting]\n        \n        # Calculate the tightness ratio: item size / remaining capacity.\n        # A higher ratio indicates a tighter fit.\n        tightness_ratio = item / (non_exact_fitting_capacities + epsilon)\n\n        # Sigmoid function to map the tightness ratio to a priority score between 0 and 1.\n        # The formula `1 / (1 + exp(-k * (ratio - 1)))` peaks around ratio=1.\n        # A k=5.0 makes the sigmoid relatively steep, favoring capacities closer to item size.\n        k = 5.0\n        sigmoid_priorities = 1 / (1 + np.exp(-k * (tightness_ratio - 1)))\n        \n        # Assign these priorities to the corresponding original bins\n        priorities[can_fit_mask][non_exact_fit_mask_for_fitting] = sigmoid_priorities\n\n    return priorities\n\n### Analyze & experience\n- *   **Heuristic 1 vs. Heuristic 2:** These two heuristics are identical. The ranking suggests a subtle difference might have been intended, but as provided, they perform the same.\n*   **Heuristic 1 & 2 vs. Heuristic 3:** Heuristics 1 and 2 offer a more nuanced priority system. They specifically target \"perfect fits\" with the highest priority (2.0) and then use a scaled inverse of remaining capacity for other suitable bins. Heuristic 3 simply uses the inverse of remaining capacity, which might lead to very large scores for near-perfect fits and doesn't explicitly prioritize exact fits.\n*   **Heuristic 1 & 2 vs. Heuristic 4:** Heuristic 4 implements a pure Best Fit strategy, assigning a priority of 1.0 only to the bin(s) with the minimum remaining capacity. Heuristics 1 and 2 are more sophisticated by introducing a higher priority for exact fits and a graded priority for other bins, making them potentially better.\n*   **Heuristic 1 & 2 vs. Heuristic 5:** Heuristic 5 calculates `capacity - item` as a base priority and then transforms it. The transformation `min_diff - priorities[priorities != -np.inf]` effectively prioritizes bins with the *smallest* `capacity - item`, which is the Best Fit criterion. However, it lacks the explicit handling of exact fits that Heuristics 1 and 2 have.\n*   **Heuristic 1 & 2 vs. Heuristic 6:** Heuristic 6 explicitly prioritizes exact fits (1.0) and then uses a normalized score for other best-fit bins (0.9 minus a scaled gap). This is very similar in spirit to Heuristics 1 and 2, but Heuristics 1 and 2 use a higher peak priority (2.0 vs. 1.0 for exact fit) and a slightly different scaling for non-exact fits. The initial ranking suggests 1 and 2 are slightly better.\n*   **Heuristic 7 vs. Heuristics 1 & 2:** Heuristic 7 attempts to combine Best Fit with a secondary consideration for bin \"fill ratio.\" However, its implementation is more complex and its secondary criterion might not always be beneficial. Heuristics 1 and 2's direct focus on exact fit and then best fit for remaining capacity seems more robust.\n*   **Heuristic 8 & 9 vs. Heuristics 1 & 2:** Heuristics 8 and 9 introduce a second heuristic focusing on moderate remaining space, adding it as a scaled component to the snug fit score. While this aims for better overall utilization, the specific definition of \"moderate\" (10%-50% of item size) is quite arbitrary and might not generalize well. Heuristics 1 and 2's focused approach on exact/best fit is likely more stable.\n*   **Heuristic 10 vs. Heuristics 1 & 2:** Heuristic 10 implements a First Fit strategy, prioritizing bins with the smallest positive remaining capacity (`1.0 / (difference + epsilon)`). This is a distinct strategy from Best Fit and doesn't explicitly handle exact fits as a special case. Heuristics 1 and 2 are generally more sophisticated.\n*   **Heuristic 11, 14, 17 vs. Heuristics 1 & 2:** These heuristics use a sigmoid function on a scaled remaining capacity. The sigmoid `1 / (1 + exp(-5 * (scaled_capacities - 0.5)))` attempts to give higher priority to bins with remaining capacity closer to the middle of the scale (0.5). They also give a priority of 1.0 to exact fits. While these are interesting, the sigmoid's specific shape and the scaling (`fitted_capacities / np.max(available_capacities)`) might be less direct than Heuristics 1 and 2's explicit Best Fit logic. Heuristics 1 and 2 are ranked higher, suggesting their approach is favored.\n*   **Heuristic 12 & 18 vs. Heuristics 1 & 2:** These heuristics also combine exact fits (1.0) with a sigmoid for other bins, using `item / remaining_capacity`. This is similar to 11/14/17 but uses a different metric for tightness. Again, Heuristics 1 and 2 are ranked above them, indicating a preference for their specific implementation.\n*   **Heuristic 13 vs. Others:** Heuristic 13 simply returns all zeros, making it the worst possible heuristic as it never selects a bin.\n*   **Heuristic 15, 19, 20 vs. Heuristics 1 & 2:** These heuristics attempt to implement First Fit (FF). They give priority to bins that can fit the item and then try to boost the priority of earlier bins. However, FF is inherently a greedy, sequential selection, not typically implemented by assigning numerical priorities to all bins and then picking the max. The approach of assigning a slightly higher score based on index is a weak simulation and less effective than true Best Fit variants like 1 and 2.\n*   **Comparing Worst:** Heuristic 13 (all zeros) is clearly the worst. Heuristics 15/19/20 (FF simulation) are also poor because they don't capture the core FF selection mechanism effectively through priorities alone. Heuristic 3, while simpler than 1/2, is a valid but less refined Best Fit.\n\n**Overall Comparison:** Heuristics 1 and 2 (being identical and ranked highest) represent a refined Best Fit strategy that explicitly prioritizes exact fits. They offer a balanced approach by giving the highest score to perfect matches and a graded score to other suitable bins based on minimizing remaining capacity. The heuristics ranked lower introduce more complex (and sometimes arbitrary) secondary criteria, use different mathematical functions (sigmoid), or attempt to simulate other strategies (FF) in a way that's less effective for priority-based selection.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Objective alignment, iterative refinement, empirical validation, robust design.\n*   **Advice:** Focus on how your heuristic *directly* optimizes the primary objective. If a secondary criterion is needed, ensure it's a logical extension or a robust tie-breaker, not an arbitrary addition. Validate performance against diverse problem instances.\n*   **Avoid:** Relying on \"mimicking\" existing heuristics without understanding their underlying logic. Over-engineering with complex, unproven transformations or prioritizing easily quantifiable but less impactful metrics.\n*   **Explanation:** True self-reflection in heuristic design means dissecting *why* a strategy works, ensuring alignment with the core problem, and building robustness through testing, not just imitation or complexity.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}