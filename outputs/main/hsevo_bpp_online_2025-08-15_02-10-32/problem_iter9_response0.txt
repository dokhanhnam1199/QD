```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Adaptive Best Fit: Prioritizes bins that are a "good fit" based on the item size,
    and then uses a penalty for bins that are "too large" to encourage denser packing.
    This aims for a balance between fitting the current item well and leaving space
    for future items without excessive wasted capacity in any single bin.

    The core idea is to favor bins where the remaining capacity after packing is
    close to the item size, but also penalize bins that have a much larger remaining
    capacity than the item, as these are less efficient for the current item.
    """
    priorities = np.zeros_like(bins_remain_cap)
    suitable_bins_mask = bins_remain_cap >= item

    if not np.any(suitable_bins_mask):
        return priorities

    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]

    # Score 1: "Near Fit" score.
    # This score rewards bins where the remaining capacity *after* packing is small,
    # meaning the fit is close to perfect. We use a negative log to map smaller
    # remaining capacities (better fits) to higher scores.
    # Adding 1e-9 for numerical stability.
    near_fit_score = -np.log(suitable_bins_remain_cap - item + 1e-9)

    # Score 2: "Waste Penalty" score.
    # This score penalizes bins where the remaining capacity *before* packing is
    # significantly larger than the item. This encourages using bins that are
    # "just enough" rather than overly large ones, which might lead to fragmentation.
    # We want to *decrease* the priority of bins with high "excess capacity".
    # `excess_capacity_ratio` = (bin_capacity - item) / bin_capacity
    # A higher ratio means more wasted space relative to the bin's total size.
    # We want to penalize high `excess_capacity_ratio`.
    # A simple penalty: `1 / (excess_capacity_ratio + epsilon)`.
    # Or, consider `item_fraction_in_bin = item / bin_capacity`.
    # We want to prioritize bins where `item_fraction_in_bin` is high.
    # A sigmoid-like function can map `item_fraction_in_bin` to a score between 0 and 1,
    # where higher values mean better fit for utilization.
    # Let's use the ratio of item size to bin's remaining capacity to encourage fuller bins.
    # `fill_ratio = item / suitable_bins_remain_cap` (this is BEFORE packing)
    # This doesn't reflect the state AFTER packing.
    # Let's use `item / bin_capacity` to represent how much of a bin is occupied if we use it.
    # This is `item / (suitable_bins_remain_cap + item)`.
    # We want to maximize this ratio.
    # A sigmoid-like function: `tanh(k * (item / (suitable_bins_remain_cap + item)))`
    # where `k` is a scaling factor. A simpler version is just the ratio itself, scaled.

    # A score based on how much of the bin's *original* capacity is used by the item.
    # This encourages using bins that are better utilized by the current item.
    # Original capacity of suitable bins is `suitable_bins_remain_cap + item`.
    utilization_score = item / (suitable_bins_remain_cap + item + 1e-9) # Ratio of item size to total capacity of the bin if used

    # Combining scores:
    # We want to maximize `near_fit_score` (good fit after packing)
    # and maximize `utilization_score` (bin is well-utilized by this item).
    # A multiplicative combination often works well:
    # priority = near_fit_score * utilization_score
    # This ensures that both conditions must be met to some degree for a high priority.
    # A bin with a perfect fit (high `near_fit_score`) but low `utilization_score` (bin is very large)
    # will be penalized. Similarly, a bin with high `utilization_score` but not a great fit
    # (large `suitable_bins_remain_cap - item`) will also be penalized.

    priorities[suitable_bins_mask] = near_fit_score * utilization_score

    return priorities
```
