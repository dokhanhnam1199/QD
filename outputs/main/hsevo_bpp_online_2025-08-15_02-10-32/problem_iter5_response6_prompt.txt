{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines Best Fit with a penalty for overly large remaining capacities.\n\n    Prioritizes bins that leave the least space after packing (Best Fit),\n    while slightly penalizing bins that have very large capacities initially\n    to encourage using bins that are already somewhat full.\n    \"\"\"\n    epsilon_small = 1e-9\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can fit the item\n    fitting_indices = np.where(bins_remain_cap >= item)[0]\n\n    if fitting_indices.size > 0:\n        # Calculate remaining capacity after fitting the item\n        remaining_after_fit = bins_remain_cap[fitting_indices] - item\n\n        # Calculate a base priority using Best Fit logic: higher for smaller remaining space\n        # This prioritizes bins that will have the least space left.\n        best_fit_priority = -remaining_after_fit\n\n        # Introduce a penalty for bins that are initially \"too empty\".\n        # If a bin's current remaining capacity is much larger than the item, it's less desirable.\n        # Let's define \"too empty\" as having remaining capacity significantly larger than the item.\n        # We can penalize bins where `bins_remain_cap` is, for example, more than 2*item.\n        # This encourages using bins that are already closer to being full.\n        penalty_threshold = item * 2.0\n        penalty = np.zeros_like(fitting_indices, dtype=float)\n\n        # Apply penalty to bins where current remaining capacity is large\n        large_capacity_mask = bins_remain_cap[fitting_indices] > penalty_threshold\n        # The penalty should be such that it reduces the priority.\n        # A simple linear penalty could work: -(bins_remain_cap - penalty_threshold)\n        # Or an inverse relationship: -1.0 / (bins_remain_cap + epsilon_small)\n        # Let's use a score that is low if bins_remain_cap is very large.\n        # We can use a scaled inverse: -(bins_remain_cap[fitting_indices] / np.max(bins_remain_cap[fitting_indices]))\n        # Or simply, a negative value proportional to the capacity itself.\n        # Let's try subtracting a scaled version of the original capacity.\n        # This makes bins with very large capacities have lower scores.\n        \n        # We want to prioritize bins that are already somewhat full.\n        # Using `1.0 / (bins_remain_cap[i] + epsilon_small)` for fitting bins\n        # prioritizes bins with small *initial* remaining capacity.\n        # Let's combine this with the \"best fit\" idea.\n        \n        # Strategy:\n        # 1. Prioritize bins that are \"almost full\" (low initial `bins_remain_cap`).\n        # 2. Among those, pick the one that results in the tightest fit (Best Fit).\n        \n        # Score = (some value based on initial capacity) + (value based on tightest fit)\n        \n        # Let's use the inverse of initial remaining capacity for \"almost full\" preference\n        # and the negative of the difference for \"best fit\" preference.\n        # We can add them, or use one as a primary and the other as a secondary refinement.\n        \n        # Let's prioritize bins that are ALREADY almost full. Small `bins_remain_cap` is good.\n        # Use `1.0 / (bins_remain_cap[i] + epsilon_small)` as the base priority.\n        # This captures the \"almost full\" aspect by favoring bins with low remaining capacity.\n        \n        initial_almost_full_priority = 1.0 / (bins_remain_cap[fitting_indices] + epsilon_small)\n        \n        # Now, within the \"almost full\" bins, let's refine using Best Fit (minimize remaining space).\n        # The \"best fit\" priority is `-remaining_after_fit`.\n        \n        # To combine: We want bins with small `bins_remain_cap` AND small `remaining_after_fit`.\n        # A simple sum or weighted sum could work.\n        # `combined_priority = w1 * initial_almost_full_priority + w2 * best_fit_priority`\n        \n        # Let's try a simpler approach: Prioritize bins that are \"almost full\" (low initial remaining capacity).\n        # If multiple bins are equally \"almost full\" (e.g., same small capacity), then pick the best fit.\n        # But the `1/r` score already heavily favors the smallest `r`.\n        \n        # Let's go with a direct interpretation: \"Almost Full Fit\" means filling bins.\n        # This is achieved by minimizing the space left after packing.\n        # This is precisely Best Fit: maximize `-(bins_remain_cap[i] - item)`.\n        \n        # If we want to emphasize \"almost full\" by penalizing bins that are *too* empty,\n        # we can modify the Best Fit score.\n        # Let's try a score that favors tight fits but penalizes very large initial capacities.\n        \n        # A score that is high when `remaining_after_fit` is small, AND `bins_remain_cap` is not excessively large.\n        \n        # Let's use `-(remaining_after_fit)` as the core Best Fit score.\n        # Then, subtract a penalty if `bins_remain_cap` is very large.\n        \n        # Penalty: if `bins_remain_cap[i] > item * K`, subtract a penalty.\n        # The penalty should be significant enough to push very large bins down.\n        \n        penalty_factor = 1.0 # Controls how much we penalize large capacities\n        penalty_value = 0.0\n        \n        # Penalize bins whose initial remaining capacity is significantly larger than the item size.\n        # A threshold relative to the item size is reasonable.\n        # Let's say we penalize if `bins_remain_cap > item * 3`.\n        threshold_large_capacity = item * 3.0\n        \n        # Calculate penalty for bins where initial capacity is large\n        large_capacity_indices_relative = np.where(bins_remain_cap[fitting_indices] > threshold_large_capacity)[0]\n        \n        if large_capacity_indices_relative.size > 0:\n            # The penalty should reduce the priority.\n            # Subtract a value proportional to how much larger the capacity is.\n            # The scale of this penalty should be comparable to the best_fit_priority range.\n            \n            # Best fit priorities are typically negative values like -0.1, -0.5, -2.0 etc.\n            # If bins_remain_cap is 100 and item is 1, remaining_after_fit is 99, best_fit_priority is -99.\n            # If we penalize for capacity 100, it should be a large negative number.\n            \n            # Let's subtract a value that scales with the extra capacity.\n            # The difference `bins_remain_cap[fitting_indices] - item` gives the remaining space.\n            # The penalty should be for large `bins_remain_cap`.\n            \n            # Let's try a score structure:\n            # Priority = `-(bins_remain_cap[i] - item)`   (Best Fit term)\n            #          - `f(bins_remain_cap[i])`        (Penalty for large initial capacity)\n            \n            # `f(x)` could be `(x / MaxCap) * Scale` or simply `x / Scale` for large x.\n            \n            # Let's use a simpler heuristic: prioritize bins that are \"almost full\" (low initial remaining capacity).\n            # If `bins_remain_cap` is small, priority is high.\n            # If `bins_remain_cap` is large, priority is low.\n            # THEN, among bins with similar \"almost fullness\", pick the best fit.\n            \n            # Score = `1.0 / (bins_remain_cap[i] + epsilon_small)`  (Prioritize small initial capacity)\n            # This is essentially \"First Fit Decreasing\" idea applied to capacities.\n            \n            # Let's try combining Best Fit with a preference for less \"empty\" bins.\n            # Priority = `-(bins_remain_cap[i] - item)` (Best Fit term)\n            # Add a term for how \"full\" the bin is initially. Small `bins_remain_cap` is good.\n            # Add `k * (1.0 / (bins_remain_cap[i] + epsilon_small))`\n            \n            # Let's simplify the objective:\n            # 1. MUST fit: `bins_remain_cap[i] >= item`\n            # 2. Prefer tightest fit: minimize `bins_remain_cap[i] - item`\n            # 3. Prefer bins that are already somewhat full: prefer small `bins_remain_cap[i]`\n            \n            # Consider the score: `priorities[fitting_indices] = -(bins_remain_cap[fitting_indices] - item)`\n            # This is Best Fit. It implicitly favors smaller initial capacities if the item size is fixed.\n            # E.g., if item=3, capacities [4, 5, 10].\n            # Scores: -(4-3)=-1, -(5-3)=-2, -(10-3)=-7.\n            # Picks capacity 4. It has the smallest initial capacity among those that fit and results in least space.\n            \n            # If we want to emphasize \"almost full\" as in \"prefer bins that are already small\",\n            # we can use `1.0 / (bins_remain_cap[i] + epsilon_small)`.\n            \n            # Let's try a composite score:\n            # Score = `(1.0 / (bins_remain_cap[fitting_indices] + epsilon_small))`  (Preference for already \"almost full\" bins)\n            #       `+ 0.5 * (-remaining_after_fit)`                           (Preference for tightest fit)\n            \n            # This prioritizes bins that are initially almost full, and among those, picks the best fit.\n            # The weight `0.5` can be tuned.\n            \n            combined_priority = (1.0 / (bins_remain_cap[fitting_indices] + epsilon_small)) + 0.5 * (-remaining_after_fit)\n            priorities[fitting_indices] = combined_priority\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritizes bins by giving a higher score to bins that fit the item\n    tightly, using an exponential decay based on relative capacity,\n    but also strongly favoring exact fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n    \n    if fitting_bins_capacities.size > 0:\n        \n        exact_fit_mask = fitting_bins_capacities == item\n        \n        \n        relative_capacities = fitting_bins_capacities - item\n        \n        \n        non_exact_fit_mask = ~exact_fit_mask\n        \n        \n        non_exact_fitting_capacities = relative_capacities[non_exact_fit_mask]\n        \n        if non_exact_fitting_capacities.size > 0:\n            \n            mean_relative_capacity = np.mean(non_exact_fitting_capacities)\n            \n            \n            if mean_relative_capacity > 0:\n                \n                priorities[can_fit_mask][non_exact_fit_mask] = np.exp(-non_exact_fitting_capacities / mean_relative_capacity)\n            else:\n                \n                priorities[can_fit_mask][non_exact_fit_mask] = 0.0\n        \n        \n        priorities[can_fit_mask][exact_fit_mask] = 1.0 \n        \n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 vs Heuristic 2: Heuristic 1 uses a log transform and a `tightness_ratio` for its score, while Heuristic 2 uses an inverse remaining capacity and a normalized excess capacity penalty. Heuristic 1's score `best_fit_score * penalty_multiplier` (where `penalty_multiplier = 1.0 / (excess_ratio + 0.2)`) appears more direct in penalizing bins with large excess capacity relative to item size.\n\nComparing Heuristic 3 vs Heuristic 7 (which are identical): Both use `fill_ratio * tightness` as their score, which is `item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`. This score effectively balances the tightness of the fit with how much of the remaining space the item occupies, and it implicitly penalizes bins with large `suitable_bins_remain_cap`.\n\nComparing Heuristic 4 vs Heuristic 9 (which are identical): Both use a sigmoid function based on normalized slack. Heuristic 4 maps `max_slack - slack` to the sigmoid, aiming for higher scores for smaller slack. Heuristic 9 directly uses `differences * scale_factor` as the sigmoid input, achieving a similar goal.\n\nComparing Heuristic 5 vs Heuristic 6 (which are identical): Both prioritize exact fits with a high score, then use inverse remaining capacity for non-exact fits, normalizing these scores. This is a robust approach for prioritizing exact fits.\n\nComparing Heuristic 10 vs Heuristic 11/12: Heuristic 10 uses a product of Best Fit, Excess Capacity Penalty, and Distributional Balancing. Heuristics 11/12 combine Best Fit with a preference for \"almost full\" bins using a weighted sum. Heuristic 11/12's direct combination of `1.0 / (bins_remain_cap + epsilon_small)` and `0.5 * (-remaining_after_fit)` seems more straightforward than Heuristic 10's multiplicative approach with potentially complex interactions.\n\nComparing Heuristic 13/14/15 vs Heuristic 16: Heuristics 13/14/15 prioritize exact fits (score 1.0) and then use an exponential decay `exp(-relative_capacities / mean_relative_capacity)` for non-exact fits. Heuristic 16 prioritizes exact fits with a high score and uses `exp(-scaled_relative_capacities)` for non-exact fits, where `scaled_relative_capacities` is normalized between 0 and 1. Heuristic 16's normalization and conditional scaling might lead to more stable and comparable scores.\n\nComparing Heuristic 17/18 vs Heuristic 19/20: Heuristics 17/18 use a sigmoid `1 / (1 + exp(-scaled_differences))` based on the negative difference for tight fits. Heuristics 19/20 use `exp(remaining_capacities)` scaled and normalized, which seems to amplify the preference for tighter fits directly. The sigmoid in 17/18 might offer smoother control.\n\nOverall: Heuristics focusing on multiplicative combinations of \"tightness\" (like `1/(rem-item)`) and \"fill ratio\" (like `item/rem`), or those that directly penalize large initial remaining capacities, seem most promising for balancing fit quality and overall bin utilization. Exact fit prioritization is also a strong strategy. The complexity of penalties and normalizations can sometimes obscure the intended behavior.\n- \nHere's a redefined approach to self-reflection for heuristic design:\n\n*   **Keywords:** Iterative refinement, empirical validation, component interaction, adaptability.\n*   **Advice:** Focus on how heuristic components *interact* and *evolve* during search. Design for *adaptability* based on problem instance characteristics.\n*   **Avoid:** Over-reliance on fixed mathematical combinations or static penalties.\n*   **Explanation:** Instead of solely prioritizing fixed metrics like \"tightness\" or \"fill ratio,\" continuously evaluate their effectiveness *in conjunction* throughout the heuristic's execution. Tune component weights or logic dynamically, rather than with static formulas.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}