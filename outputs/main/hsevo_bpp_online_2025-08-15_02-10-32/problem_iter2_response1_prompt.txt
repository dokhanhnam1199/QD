{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using sigmoid fit score.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We want to prioritize bins that are \"almost full\" but can still fit the item.\n    # This encourages using existing bins efficiently before opening new ones.\n\n    # Calculate the \"goodness\" of fit for each bin. A higher value means a better fit.\n    # We consider bins where the remaining capacity is just enough or slightly more than the item.\n    # This is a subjective measure to encourage tighter packing.\n\n    # Create a measure of how \"tight\" the fit is.\n    # We want capacity_remaining - item_size to be close to zero.\n    tightness = bins_remain_cap - item\n\n    # Filter out bins that cannot fit the item. Their priority should be zero (or very low).\n    fit_mask = bins_remain_cap >= item\n    valid_tightness = tightness[fit_mask]\n\n    # Normalize the tightness to a range where sigmoid can work well.\n    # If there are no valid bins, this step will be skipped and an all-zero array will be returned.\n    priorities = np.zeros_like(bins_remain_cap)\n\n    if valid_tightness.size > 0:\n        # A simple normalization: scale such that the range of valid tightness is somewhat centered around 0.\n        # This is heuristic and can be tuned. We want positive values for good fits.\n        # A smaller max_tightness means we are more sensitive to \"almost full\" bins.\n        max_tightness = np.max(valid_tightness)\n        min_tightness = np.min(valid_tightness)\n\n        # Avoid division by zero if all valid bins have the same tightness\n        if max_tightness == min_tightness:\n            normalized_tightness = np.zeros_like(valid_tightness)\n        else:\n            # Scale so that the range of valid_tightness is mapped to roughly [-2, 2] or similar,\n            # allowing sigmoid to capture differences.\n            # We want to map the smallest valid tightness (most space left) to a lower sigmoid value,\n            # and the largest valid tightness (tightest fit) to a higher sigmoid value.\n            # So, invert the range for scaling: max_tightness becomes -1, min_tightness becomes 1.\n            # Using -valid_tightness here because we want smaller remaining space (larger negative)\n            # to result in a higher sigmoid value.\n            scaled_tightness = -2 * (valid_tightness - min_tightness) / (max_tightness - min_tightness)\n\n            # Apply sigmoid function.\n            # Sigmoid(x) = 1 / (1 + exp(-x))\n            # This will map scaled_tightness to values between 0 and 1.\n            # We want higher values for tighter fits, so we will scale and shift if needed.\n            # A common approach is to use a scaled and shifted sigmoid.\n            # Let's map tightest fit (min_tightness) to 1 and loosest fit (max_tightness) to 0.\n            # This means we want a higher score when valid_tightness is smaller.\n            # Let's use a shifted and scaled sigmoid.\n            # f(x) = 1 / (1 + exp(-(k * (tightness_max - x))))\n            # where k is a steepness parameter and x is the remaining capacity after fitting.\n\n            # Parameter to control steepness of sigmoid. Higher k means more emphasis on tighter fits.\n            steepness_param = 2.0\n\n            # Calculate the value we pass to sigmoid. We want a larger value for smaller remaining capacity.\n            # So we use -(bins_remain_cap[fit_mask] - item) which means a smaller remaining capacity gives a larger positive value.\n            sigmoid_input = steepness_param * (item - bins_remain_cap[fit_mask])\n\n            # Apply sigmoid. The output will be between 0 and 1.\n            # Higher values for bins that are almost full (but can fit the item).\n            sigmoid_scores = 1 / (1 + np.exp(-sigmoid_input))\n\n            # Map these scores back to the original priority array\n            priorities[fit_mask] = sigmoid_scores\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            priorities[i] = bins_remain_cap[i] - item\n        else:\n            priorities[i] = -1.0\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Exact Fit First) and Heuristic 2 (Best Fit): Heuristic 1 prioritizes exact fits and then falls back to First Fit, while Heuristic 2 strictly implements Best Fit. Heuristic 1's explicit handling of exact fits and fallback logic seems more robust for certain scenarios than a pure Best Fit.\n\nComparing Heuristic 1 and Heuristic 6 (Inverse of Remaining Capacity): Heuristic 1 offers a more sophisticated priority system with distinct cases (exact fit, then first fit). Heuristic 6 simply uses `1 / (remaining_capacity - item)`, which is a form of Best Fit but lacks the specific exact-fit advantage of Heuristic 1.\n\nComparing Heuristic 2 (Best Fit) and Heuristic 8 (Loop-based Best Fit): Heuristic 2 uses vectorized NumPy operations for efficiency, while Heuristic 8 uses a Python loop. Vectorization is generally preferred for performance in numerical computations. The logic is similar.\n\nComparing Heuristic 6 and Heuristic 11 (Identical): Heuristics 6 and 11 are identical. Both implement Best Fit by prioritizing bins with the least remaining capacity using the inverse of `(remaining_capacity - item)`.\n\nComparing Heuristic 7 and Heuristic 10 (Identical): Heuristics 7 and 10 are identical. They use an exponential decay based on the mean relative capacity, aiming to balance fits but potentially being sensitive to outliers.\n\nComparing Heuristic 5 (Sigmoid Fit Score) and Heuristic 12 (Scaled Sigmoid): Heuristic 12 uses a more carefully tuned sigmoid function with scaling parameters based on the range of valid fits. Heuristic 5's sigmoid scaling is fixed, making Heuristic 12 likely more adaptable to different data distributions.\n\nComparing Heuristic 16 (Remaining Capacity) and Heuristic 18 (Identical): Heuristics 16 and 18 simply return the remaining capacity if the item fits, otherwise -1. This is a very basic heuristic and doesn't actively try to optimize packing beyond identifying bins that can fit.\n\nComparing Heuristic 13 (Relative Capacity) and Heuristic 17 (Normalized Exponential Relative Capacity): Heuristic 13 uses a simple ratio `(remaining - item) / remaining`. Heuristic 17 uses an exponential of the relative capacity, normalized across fitting bins. Heuristic 17's approach seems to give stronger preference to tighter fits via exponentiation.\n\nOverall: Heuristics that combine explicit strategies like exact fits with fallback mechanisms (e.g., Heuristic 1) or use well-tuned mathematical transformations (e.g., Heuristic 12's scaled sigmoid) appear more robust. Simple implementations of Best Fit (e.g., Heuristic 6, 11) are effective but less nuanced. Heuristics with simple negative remaining capacity or fixed scaling (e.g., 16, 18, 5) are less sophisticated.\n- \nHere's a refined approach to self-reflection for heuristic design:\n\n*   **Keywords:** Precision, Adaptability, Explainability, Performance.\n*   **Advice:** Focus on mechanisms that allow heuristics to adapt to varying data distributions and problem complexities. Document the reasoning behind each heuristic component for maintainability and debugging.\n*   **Avoid:** \"Black box\" logic or overly rigid rules that don't account for edge cases or evolving data. Avoid introducing complexity without a clear performance or accuracy benefit.\n*   **Explanation:** True self-reflection in heuristic design means understanding *why* a strategy works, its limitations, and how it can be improved. It's about building intelligent, interpretable, and robust solutions, not just fast ones.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}