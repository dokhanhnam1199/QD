[
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for bins with too much excess capacity.\n    Prioritizes bins that are a tight fit, penalizing those with large gaps.\n    This aims for better space utilization by avoiding overly large remaining spaces.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Calculate inverse of remaining capacity (similar to Best Fit)\n    # Adding a small epsilon to avoid division by zero if remaining_cap == item\n    inverse_remaining = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n    \n    # Calculate a penalty for bins with a large gap (excess capacity)\n    # Using a sigmoid-like function to penalize larger gaps more significantly\n    # Normalize remaining capacity to a 0-1 scale for the penalty function\n    max_suitable_cap = np.max(suitable_bins_remain_cap)\n    min_suitable_cap = np.min(suitable_bins_remain_cap)\n    \n    # Avoid division by zero if all suitable bins have the same capacity\n    if max_suitable_cap == min_suitable_cap:\n        normalized_excess = np.zeros_like(suitable_bins_remain_cap)\n    else:\n        # Capacity of the bin relative to the range of suitable capacities\n        # We want to penalize bins with capacity much larger than the item\n        # Focus on the gap: suitable_bins_remain_cap - item\n        excess_capacity = suitable_bins_remain_cap - item\n        normalized_excess = excess_capacity / (max_suitable_cap - item + 1e-9) # Normalize by max possible excess\n        \n    # Apply a penalty: higher penalty for larger normalized excess\n    # A simple inverse of the normalized excess can work as a penalty,\n    # or a more aggressive function like exp(-k * normalized_excess)\n    # Let's use a simple inverse for now, penalizing bins with larger excess\n    # Add a small constant to avoid division by zero for bins that are exact fits after normalization\n    penalty = 1.0 / (normalized_excess + 0.1) \n    \n    # Combine the \"Best Fit\" score with the penalty\n    # We want to favor smaller remaining capacities (high inverse_remaining)\n    # and penalize larger excess capacities (low penalty value, as penalty is 1/(normalized_excess+c))\n    # So, we want to maximize inverse_remaining and minimize penalty\n    # A simple combination: inverse_remaining / penalty (effectively inverse_remaining * (normalized_excess + c))\n    # This gives higher scores to bins that are tight fits AND don't have excessive space after fitting.\n    priorities[suitable_bins_mask] = inverse_remaining * penalty\n    \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.028719585161557,
    "cyclomatic_complexity": 3.0,
    "halstead": 176.46653521143952,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response1.txt_stdout.txt",
    "code_path": "problem_iter2_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit (tightness) with a sigmoid for prioritizing near-exact fits.\n\n    Prioritizes bins that are almost full but can fit the item, using a sigmoid\n    to smooth the preference for tighter fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[fit_mask]\n\n    # Heuristic: Prioritize bins that are \"almost full\" but can still fit the item.\n    # This is inspired by Best Fit, but uses a sigmoid to give a smoother preference\n    # to bins where remaining_capacity - item is small.\n    # The input to the sigmoid is scaled such that tighter fits result in a higher score.\n    # We use -(valid_bins_remain_cap - item) to make smaller remaining space\n    # correspond to larger (less negative) sigmoid inputs.\n\n    # A simple scaling to avoid extreme sigmoid values too quickly.\n    # The range of (bins_remain_cap - item) can vary. Let's normalize it.\n    # For bins that fit, the \"slack\" is valid_bins_remain_cap - item.\n    # We want to prioritize smaller slack.\n    slack = valid_bins_remain_cap - item\n\n    # Normalize slack to be between 0 and 1 for sigmoid input.\n    # If all slack is the same, avoid division by zero.\n    if slack.size > 0:\n        min_slack = np.min(slack)\n        max_slack = np.max(slack)\n\n        if max_slack == min_slack:\n            normalized_slack = np.zeros_like(slack)\n        else:\n            # Map slack to a range where sigmoid can differentiate well.\n            # We want smaller slack to map to a higher priority.\n            # So, map min_slack (tightest fit) to a high sigmoid input,\n            # and max_slack (loosest fit) to a low sigmoid input.\n            # Consider the inverse of slack: 1 / (slack + epsilon) is similar to Best Fit.\n            # Let's use a transformation like: 1 - (slack / max_slack) or similar.\n            # A sigmoid on -(slack) might be good: larger negative means smaller slack.\n            # sigmoid_input = -slack\n            # To control steepness and range, we can use:\n            steepness = 5.0 # Tune this parameter\n            # We want smaller slack to give higher priority.\n            # So, we want a higher value when slack is small.\n            # Transform slack to a value that is higher for smaller slack.\n            # Example: max_slack - slack. Then normalize.\n            transformed_slack = max_slack - slack\n            if max_slack - min_slack > 0:\n                normalized_transformed_slack = transformed_slack / (max_slack - min_slack)\n            else:\n                normalized_transformed_slack = np.zeros_like(slack)\n\n            # Use sigmoid on the transformed slack. High transformed_slack (low original slack)\n            # should map to a high sigmoid output.\n            # We can use `steepness * (normalized_transformed_slack - 0.5)` to center around 0.5.\n            sigmoid_input = steepness * (normalized_transformed_slack - 0.5)\n            priorities[fit_mask] = 1 / (1 + np.exp(-sigmoid_input))\n        \n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 5.0,
    "halstead": 208.89318279048564,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response2.txt_stdout.txt",
    "code_path": "problem_iter2_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Exact Fit First with a scaled Best Fit using exponential decay.\n    Prioritizes exact fits, then bins with the tightest fit, scaled for\n    better discrimination among close fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Exact Fit: Highest priority\n    exact_fit_indices = np.where(bins_remain_cap == item)[0]\n    if len(exact_fit_indices) > 0:\n        priorities[exact_fit_indices] = 1e9  # Very high priority\n        # Optional: Add a slight preference for lower capacity if multiple exact fits exist\n        # priorities[exact_fit_indices] += 1000 - bins_remain_cap[exact_fit_indices] \n        return priorities\n    \n    # Best Fit with exponential scaling for remaining candidates\n    suitable_bins_mask = bins_remain_cap > item\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_mask]\n    \n    if suitable_bins_capacities.size > 0:\n        relative_capacities = suitable_bins_capacities - item\n        # Use exponential decay to strongly prefer tighter fits\n        # Add a small constant to avoid division by zero or extremely large values if relative_capacities are close to 0\n        # Scaling factor to prevent overflow and control the steepness of the decay\n        scale_factor = np.mean(relative_capacities) if np.mean(relative_capacities) > 0 else 1.0\n        exp_priorities = np.exp(-relative_capacities / scale_factor)\n        \n        # Normalize priorities to be between 0 and 1 for the suitable bins\n        sum_exp_priorities = np.sum(exp_priorities)\n        if sum_exp_priorities > 0:\n            priorities[suitable_bins_mask] = exp_priorities / sum_exp_priorities\n        else: # Handle case where all relative capacities are extremely large, leading to exp_priorities being 0\n            priorities[suitable_bins_mask] = 1.0 / len(suitable_bins_capacities)\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 5.0,
    "halstead": 135.93368043019473,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by giving a higher score to bins that fit the item\n    tightly, using an exponential decay based on relative capacity,\n    but also strongly favoring exact fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n    \n    if fitting_bins_capacities.size > 0:\n        \n        exact_fit_mask = fitting_bins_capacities == item\n        \n        \n        relative_capacities = fitting_bins_capacities - item\n        \n        \n        non_exact_fit_mask = ~exact_fit_mask\n        \n        \n        non_exact_fitting_capacities = relative_capacities[non_exact_fit_mask]\n        \n        if non_exact_fitting_capacities.size > 0:\n            \n            mean_relative_capacity = np.mean(non_exact_fitting_capacities)\n            \n            \n            if mean_relative_capacity > 0:\n                \n                priorities[can_fit_mask][non_exact_fit_mask] = np.exp(-non_exact_fitting_capacities / mean_relative_capacity)\n            else:\n                \n                priorities[can_fit_mask][non_exact_fit_mask] = 0.0\n        \n        \n        priorities[can_fit_mask][exact_fit_mask] = 1.0 \n        \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.487435181491823,
    "cyclomatic_complexity": 4.0,
    "halstead": 100.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response4.txt_stdout.txt",
    "code_path": "problem_iter2_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with an exponential preference for tighter fits.\n    Prioritizes bins that can exactly fit the item, then favors bins with\n    less remaining capacity among those that can accommodate the item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if np.any(can_fit_mask):\n        # Calculate the difference for fitting bins\n        diffs = fitting_bins_remain_cap - item\n        \n        # Assign priorities: higher for exact fits, then inverse of remaining capacity (Best Fit)\n        # A small epsilon is added to avoid division by zero and to ensure exact fits have highest priority\n        priorities[can_fit_mask] = np.exp(-diffs) * (1.0 / (fitting_bins_remain_cap + 1e-9))\n        \n        # Further boost exact fits (where diff is zero)\n        exact_fit_mask_for_fitting = (diffs == 0)\n        priorities[can_fit_mask][exact_fit_mask_for_fitting] = 1.0 # Highest priority for exact fits\n        \n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 81.7492568250068,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response5.txt_stdout.txt",
    "code_path": "problem_iter2_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a sigmoid-like preference for tighter fits.\n    Prioritizes bins that leave minimal remaining capacity after packing,\n    but uses a scaled exponential to amplify this preference.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Find bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if not np.any(can_fit_mask):\n        return priorities # No bin can fit the item\n        \n    # Calculate remaining capacity after fitting the item\n    remaining_capacities = fitting_bins_remain_cap - item\n    \n    # Use a scaled exponential to prioritize tighter fits (similar to Heuristic 17 but simpler)\n    # This amplifies the preference for bins with smaller remaining_capacities\n    # Add a small epsilon to avoid division by zero if all remaining capacities are the same\n    epsilon = 1e-8\n    scaled_preference = np.exp(remaining_capacities)\n    \n    # Normalize the preference scores so they sum to 1 for the fitting bins\n    sum_scaled_preference = np.sum(scaled_preference)\n    if sum_scaled_preference > 0:\n        normalized_preference = scaled_preference / sum_scaled_preference\n    else:\n        # If all scaled preferences are zero (e.g., due to very large negative exponents if we used them)\n        # assign equal probability to all fitting bins.\n        normalized_preference = np.ones_like(fitting_bins_remain_cap) / len(fitting_bins_remain_cap)\n\n    priorities[can_fit_mask] = normalized_preference\n    \n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 3.0,
    "halstead": 64.72503367497926,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response6.txt_stdout.txt",
    "code_path": "problem_iter2_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a Sigmoid for nuanced bin prioritization.\n\n    Prioritizes bins that offer a tighter fit using a sigmoid function,\n    favoring bins where the remaining capacity is closer to the item size.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[eligible_bins_mask]\n\n    if eligible_bins_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"tightness\" of the fit for eligible bins.\n    # A smaller difference means a tighter fit.\n    differences = eligible_bins_cap - item\n\n    # Use a sigmoid function to map the differences to a priority score.\n    # A smaller difference (tighter fit) should result in a higher priority.\n    # We want to invert the difference so smaller differences are \"better\".\n    # The sigmoid function will then map these inverted differences to [0, 1].\n    # Scaling factor to control the steepness of the sigmoid.\n    scale_factor = 5.0\n    # Add a small epsilon to avoid division by zero if difference is 0.\n    inverted_differences = 1.0 / (differences + 1e-9)\n    \n    # Shift the scores so that a perfect fit (difference of 0) gets a high score.\n    # Since inverted_differences will be large for small differences,\n    # we can directly apply sigmoid or shift if needed.\n    # Here, a larger inverted_differences (meaning smaller original difference)\n    # should lead to higher priority.\n    \n    # Sigmoid function: 1 / (1 + exp(-x))\n    # We want higher priority for smaller differences.\n    # Let's use exp(-difference) as a base for priority.\n    # A larger value for exp(-difference) means a smaller difference.\n    # Then apply sigmoid to these values.\n    \n    # Option 1: Directly use exp(-difference) and sigmoid\n    # shifted_inverted_differences = -differences * scale_factor\n    # priorities = 1 / (1 + np.exp(-shifted_inverted_differences))\n    \n    # Option 2: Use 1/(difference + epsilon) and sigmoid\n    # The 'fit_scores' from v1 can be interpreted as how much \"room\" is left relative to the item.\n    # A score of 1 means exact fit. We want scores close to 1 to have high priority.\n    # Let's revisit v1 logic for better interpretation:\n    # fit_scores = valid_bins_cap / (valid_bins_cap - item + 1e-9)\n    # A higher fit_score means the bin is less full relative to the item size.\n    # This is NOT what we want for \"tight fit\". We want small (valid_bins_cap - item).\n    \n    # Let's go back to prioritizing smaller differences.\n    # We can directly use the negative difference, scaled, within the sigmoid.\n    # A smaller difference means a more desirable fit.\n    # We want the sigmoid output to be higher for smaller `differences`.\n    # `1 / (1 + exp(-k * difference))` will achieve this: as `difference` decreases, `-k * difference` increases, and sigmoid output increases.\n    \n    scaled_differences = differences * scale_factor\n    priorities = 1 / (1 + np.exp(-scaled_differences))\n\n    # Map priorities back to the original bins_remain_cap array\n    original_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_priorities[eligible_bins_mask] = priorities\n    \n    return original_priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 13.40247307538892,
    "cyclomatic_complexity": 2.0,
    "halstead": 112.37013046707143,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response7.txt_stdout.txt",
    "code_path": "problem_iter2_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins that are an exact fit, then uses an inverse remaining capacity\n    approach for other suitable bins, balancing precision and general effectiveness.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    # Exact fit priority (highest)\n    exact_fit_mask = np.isclose(bins_remain_cap, item)\n    priorities[exact_fit_mask] = 1e9 # Assign a very high priority for exact fits\n    \n    # Best fit for non-exact fits\n    non_exact_suitable_mask = suitable_bins_mask & ~exact_fit_mask\n    \n    if np.any(non_exact_suitable_mask):\n        remaining_capacities_for_non_exact = bins_remain_cap[non_exact_suitable_mask] - item\n        # Using inverse remaining capacity as a measure of \"best fit\"\n        priorities[non_exact_suitable_mask] = 1.0 / (remaining_capacities_for_non_exact + 1e-9)\n        \n    # Normalize priorities for non-exact fits to avoid overpowering exact fits\n    # and to make the best-fit values relative among themselves.\n    # We don't normalize the exact fit priorities as they are intended to be dominant.\n    if np.any(non_exact_suitable_mask):\n        non_exact_priorities = priorities[non_exact_suitable_mask]\n        if np.sum(non_exact_priorities) > 0:\n            priorities[non_exact_suitable_mask] = non_exact_priorities / np.sum(non_exact_priorities)\n\n    # If no bins are suitable, all priorities remain 0.\n    \n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 101.02330072391149,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response8.txt_stdout.txt",
    "code_path": "problem_iter2_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by favoring exact fits and then applying a scaled exponential\n    decay to the remaining capacity, balancing 'best fit' with a preference for\n    tight fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    fitting_bins_caps = bins_remain_cap[can_fit_mask]\n    \n    \n    exact_fit_mask = fitting_bins_caps == item\n    \n    if np.any(exact_fit_mask):\n        priorities[can_fit_mask][exact_fit_mask] = 1e10  \n    \n    \n    non_exact_fitting_bins_caps = fitting_bins_caps[~exact_fit_mask]\n    non_exact_fitting_indices = np.where(can_fit_mask)[0][~exact_fit_mask]\n    \n    if non_exact_fitting_bins_caps.size > 0:\n        \n        relative_capacities = non_exact_fitting_bins_caps - item\n        \n        \n        min_relative_capacity = np.min(relative_capacities)\n        max_relative_capacity = np.max(relative_capacities)\n\n        \n        if max_relative_capacity == min_relative_capacity:\n            scaled_relative_capacities = np.zeros_like(relative_capacities)\n        else:\n            \n            scaled_relative_capacities = (relative_capacities - min_relative_capacity) / (max_relative_capacity - min_relative_capacity)\n        \n        \n        priorities[non_exact_fitting_indices] = np.exp(-scaled_relative_capacities)\n        \n    \n    if np.any(exact_fit_mask):\n        priorities[can_fit_mask][exact_fit_mask] = np.max(priorities[can_fit_mask][~exact_fit_mask]) * 1.1 if np.any(~exact_fit_mask) else 1e10\n        \n    \n    priorities[~can_fit_mask] = 0.0\n    \n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 5.195452732349436,
    "cyclomatic_complexity": 7.0,
    "halstead": 192.7180284437848,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a penalty for overly large remaining capacities.\n\n    Prioritizes bins that leave the least space after packing (Best Fit),\n    while slightly penalizing bins that have very large capacities initially\n    to encourage using bins that are already somewhat full.\n    \"\"\"\n    epsilon_small = 1e-9\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can fit the item\n    fitting_indices = np.where(bins_remain_cap >= item)[0]\n\n    if fitting_indices.size > 0:\n        # Calculate remaining capacity after fitting the item\n        remaining_after_fit = bins_remain_cap[fitting_indices] - item\n\n        # Calculate a base priority using Best Fit logic: higher for smaller remaining space\n        # This prioritizes bins that will have the least space left.\n        best_fit_priority = -remaining_after_fit\n\n        # Introduce a penalty for bins that are initially \"too empty\".\n        # If a bin's current remaining capacity is much larger than the item, it's less desirable.\n        # Let's define \"too empty\" as having remaining capacity significantly larger than the item.\n        # We can penalize bins where `bins_remain_cap` is, for example, more than 2*item.\n        # This encourages using bins that are already closer to being full.\n        penalty_threshold = item * 2.0\n        penalty = np.zeros_like(fitting_indices, dtype=float)\n\n        # Apply penalty to bins where current remaining capacity is large\n        large_capacity_mask = bins_remain_cap[fitting_indices] > penalty_threshold\n        # The penalty should be such that it reduces the priority.\n        # A simple linear penalty could work: -(bins_remain_cap - penalty_threshold)\n        # Or an inverse relationship: -1.0 / (bins_remain_cap + epsilon_small)\n        # Let's use a score that is low if bins_remain_cap is very large.\n        # We can use a scaled inverse: -(bins_remain_cap[fitting_indices] / np.max(bins_remain_cap[fitting_indices]))\n        # Or simply, a negative value proportional to the capacity itself.\n        # Let's try subtracting a scaled version of the original capacity.\n        # This makes bins with very large capacities have lower scores.\n        \n        # We want to prioritize bins that are already somewhat full.\n        # Using `1.0 / (bins_remain_cap[i] + epsilon_small)` for fitting bins\n        # prioritizes bins with small *initial* remaining capacity.\n        # Let's combine this with the \"best fit\" idea.\n        \n        # Strategy:\n        # 1. Prioritize bins that are \"almost full\" (low initial `bins_remain_cap`).\n        # 2. Among those, pick the one that results in the tightest fit (Best Fit).\n        \n        # Score = (some value based on initial capacity) + (value based on tightest fit)\n        \n        # Let's use the inverse of initial remaining capacity for \"almost full\" preference\n        # and the negative of the difference for \"best fit\" preference.\n        # We can add them, or use one as a primary and the other as a secondary refinement.\n        \n        # Let's prioritize bins that are ALREADY almost full. Small `bins_remain_cap` is good.\n        # Use `1.0 / (bins_remain_cap[i] + epsilon_small)` as the base priority.\n        # This captures the \"almost full\" aspect by favoring bins with low remaining capacity.\n        \n        initial_almost_full_priority = 1.0 / (bins_remain_cap[fitting_indices] + epsilon_small)\n        \n        # Now, within the \"almost full\" bins, let's refine using Best Fit (minimize remaining space).\n        # The \"best fit\" priority is `-remaining_after_fit`.\n        \n        # To combine: We want bins with small `bins_remain_cap` AND small `remaining_after_fit`.\n        # A simple sum or weighted sum could work.\n        # `combined_priority = w1 * initial_almost_full_priority + w2 * best_fit_priority`\n        \n        # Let's try a simpler approach: Prioritize bins that are \"almost full\" (low initial remaining capacity).\n        # If multiple bins are equally \"almost full\" (e.g., same small capacity), then pick the best fit.\n        # But the `1/r` score already heavily favors the smallest `r`.\n        \n        # Let's go with a direct interpretation: \"Almost Full Fit\" means filling bins.\n        # This is achieved by minimizing the space left after packing.\n        # This is precisely Best Fit: maximize `-(bins_remain_cap[i] - item)`.\n        \n        # If we want to emphasize \"almost full\" by penalizing bins that are *too* empty,\n        # we can modify the Best Fit score.\n        # Let's try a score that favors tight fits but penalizes very large initial capacities.\n        \n        # A score that is high when `remaining_after_fit` is small, AND `bins_remain_cap` is not excessively large.\n        \n        # Let's use `-(remaining_after_fit)` as the core Best Fit score.\n        # Then, subtract a penalty if `bins_remain_cap` is very large.\n        \n        # Penalty: if `bins_remain_cap[i] > item * K`, subtract a penalty.\n        # The penalty should be significant enough to push very large bins down.\n        \n        penalty_factor = 1.0 # Controls how much we penalize large capacities\n        penalty_value = 0.0\n        \n        # Penalize bins whose initial remaining capacity is significantly larger than the item size.\n        # A threshold relative to the item size is reasonable.\n        # Let's say we penalize if `bins_remain_cap > item * 3`.\n        threshold_large_capacity = item * 3.0\n        \n        # Calculate penalty for bins where initial capacity is large\n        large_capacity_indices_relative = np.where(bins_remain_cap[fitting_indices] > threshold_large_capacity)[0]\n        \n        if large_capacity_indices_relative.size > 0:\n            # The penalty should reduce the priority.\n            # Subtract a value proportional to how much larger the capacity is.\n            # The scale of this penalty should be comparable to the best_fit_priority range.\n            \n            # Best fit priorities are typically negative values like -0.1, -0.5, -2.0 etc.\n            # If bins_remain_cap is 100 and item is 1, remaining_after_fit is 99, best_fit_priority is -99.\n            # If we penalize for capacity 100, it should be a large negative number.\n            \n            # Let's subtract a value that scales with the extra capacity.\n            # The difference `bins_remain_cap[fitting_indices] - item` gives the remaining space.\n            # The penalty should be for large `bins_remain_cap`.\n            \n            # Let's try a score structure:\n            # Priority = `-(bins_remain_cap[i] - item)`   (Best Fit term)\n            #          - `f(bins_remain_cap[i])`        (Penalty for large initial capacity)\n            \n            # `f(x)` could be `(x / MaxCap) * Scale` or simply `x / Scale` for large x.\n            \n            # Let's use a simpler heuristic: prioritize bins that are \"almost full\" (low initial remaining capacity).\n            # If `bins_remain_cap` is small, priority is high.\n            # If `bins_remain_cap` is large, priority is low.\n            # THEN, among bins with similar \"almost fullness\", pick the best fit.\n            \n            # Score = `1.0 / (bins_remain_cap[i] + epsilon_small)`  (Prioritize small initial capacity)\n            # This is essentially \"First Fit Decreasing\" idea applied to capacities.\n            \n            # Let's try combining Best Fit with a preference for less \"empty\" bins.\n            # Priority = `-(bins_remain_cap[i] - item)` (Best Fit term)\n            # Add a term for how \"full\" the bin is initially. Small `bins_remain_cap` is good.\n            # Add `k * (1.0 / (bins_remain_cap[i] + epsilon_small))`\n            \n            # Let's simplify the objective:\n            # 1. MUST fit: `bins_remain_cap[i] >= item`\n            # 2. Prefer tightest fit: minimize `bins_remain_cap[i] - item`\n            # 3. Prefer bins that are already somewhat full: prefer small `bins_remain_cap[i]`\n            \n            # Consider the score: `priorities[fitting_indices] = -(bins_remain_cap[fitting_indices] - item)`\n            # This is Best Fit. It implicitly favors smaller initial capacities if the item size is fixed.\n            # E.g., if item=3, capacities [4, 5, 10].\n            # Scores: -(4-3)=-1, -(5-3)=-2, -(10-3)=-7.\n            # Picks capacity 4. It has the smallest initial capacity among those that fit and results in least space.\n            \n            # If we want to emphasize \"almost full\" as in \"prefer bins that are already small\",\n            # we can use `1.0 / (bins_remain_cap[i] + epsilon_small)`.\n            \n            # Let's try a composite score:\n            # Score = `(1.0 / (bins_remain_cap[fitting_indices] + epsilon_small))`  (Preference for already \"almost full\" bins)\n            #       `+ 0.5 * (-remaining_after_fit)`                           (Preference for tightest fit)\n            \n            # This prioritizes bins that are initially almost full, and among those, picks the best fit.\n            # The weight `0.5` can be tuned.\n            \n            combined_priority = (1.0 / (bins_remain_cap[fitting_indices] + epsilon_small)) + 0.5 * (-remaining_after_fit)\n            priorities[fitting_indices] = combined_priority\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.417630634224167,
    "cyclomatic_complexity": 3.0,
    "halstead": 235.53074858920888,
    "exec_success": true
  }
]