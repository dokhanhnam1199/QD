{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines Best Fit (tightness) with a sigmoid for prioritizing near-exact fits.\n\n    Prioritizes bins that are almost full but can fit the item, using a sigmoid\n    to smooth the preference for tighter fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[fit_mask]\n\n    # Heuristic: Prioritize bins that are \"almost full\" but can still fit the item.\n    # This is inspired by Best Fit, but uses a sigmoid to give a smoother preference\n    # to bins where remaining_capacity - item is small.\n    # The input to the sigmoid is scaled such that tighter fits result in a higher score.\n    # We use -(valid_bins_remain_cap - item) to make smaller remaining space\n    # correspond to larger (less negative) sigmoid inputs.\n\n    # A simple scaling to avoid extreme sigmoid values too quickly.\n    # The range of (bins_remain_cap - item) can vary. Let's normalize it.\n    # For bins that fit, the \"slack\" is valid_bins_remain_cap - item.\n    # We want to prioritize smaller slack.\n    slack = valid_bins_remain_cap - item\n\n    # Normalize slack to be between 0 and 1 for sigmoid input.\n    # If all slack is the same, avoid division by zero.\n    if slack.size > 0:\n        min_slack = np.min(slack)\n        max_slack = np.max(slack)\n\n        if max_slack == min_slack:\n            normalized_slack = np.zeros_like(slack)\n        else:\n            # Map slack to a range where sigmoid can differentiate well.\n            # We want smaller slack to map to a higher priority.\n            # So, map min_slack (tightest fit) to a high sigmoid input,\n            # and max_slack (loosest fit) to a low sigmoid input.\n            # Consider the inverse of slack: 1 / (slack + epsilon) is similar to Best Fit.\n            # Let's use a transformation like: 1 - (slack / max_slack) or similar.\n            # A sigmoid on -(slack) might be good: larger negative means smaller slack.\n            # sigmoid_input = -slack\n            # To control steepness and range, we can use:\n            steepness = 5.0 # Tune this parameter\n            # We want smaller slack to give higher priority.\n            # So, we want a higher value when slack is small.\n            # Transform slack to a value that is higher for smaller slack.\n            # Example: max_slack - slack. Then normalize.\n            transformed_slack = max_slack - slack\n            if max_slack - min_slack > 0:\n                normalized_transformed_slack = transformed_slack / (max_slack - min_slack)\n            else:\n                normalized_transformed_slack = np.zeros_like(slack)\n\n            # Use sigmoid on the transformed slack. High transformed_slack (low original slack)\n            # should map to a high sigmoid output.\n            # We can use `steepness * (normalized_transformed_slack - 0.5)` to center around 0.5.\n            sigmoid_input = steepness * (normalized_transformed_slack - 0.5)\n            priorities[fit_mask] = 1 / (1 + np.exp(-sigmoid_input))\n        \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a dynamic penalty based on the item's size relative\n    to the bin's remaining capacity, and considers the overall distribution of\n    remaining capacities.\n\n    Prioritizes bins that are a tight fit, but also dynamically penalizes bins\n    with excess capacity that is disproportionately large compared to the item size.\n    It also incorporates a factor that encourages using bins that are closer to\n    the average remaining capacity among suitable bins, promoting a more balanced\n    usage of bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # --- Component 1: Best Fit (Tightness) ---\n    # Prioritize bins with less remaining capacity after placing the item.\n    # Add a small epsilon to avoid division by zero if remaining_cap == item.\n    best_fit_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # --- Component 2: Dynamic Excess Capacity Penalty ---\n    # Penalize bins where the remaining capacity (after placing the item) is\n    # significantly larger than the item itself.\n    # We want to penalize `suitable_bins_remain_cap - item` when it's large relative to `item`.\n    # Using a log-based penalty can be more robust than a simple inverse\n    # and less sensitive to extreme outliers in excess capacity.\n    excess_capacity_ratio = (suitable_bins_remain_cap - item) / (item + 1e-9)\n    # Penalize more if the excess ratio is high. A higher penalty value means lower priority.\n    # We use 1 + log(1 + ratio) to ensure positive values and a gradual penalty.\n    # Adding 1 to log argument to handle cases where ratio is 0.\n    excess_penalty = 1.0 / (1.0 + np.log(1.0 + excess_capacity_ratio)) # Lower value for higher penalty\n\n    # --- Component 3: Distributional Balancing ---\n    # Consider how \"central\" a bin's remaining capacity is within the set of suitable bins.\n    # Bins that are closer to the mean remaining capacity of suitable bins might be\n    # preferred to avoid creating too many bins with very large remaining spaces,\n    # or conversely, using up all the slightly-larger-but-still-suitable bins too quickly.\n    if len(suitable_bins_remain_cap) > 1:\n        avg_suitable_cap = np.mean(suitable_bins_remain_cap)\n        # Score based on proximity to the average: higher score for being closer.\n        # Using inverse of absolute difference from average.\n        proximity_to_avg_score = 1.0 / (np.abs(suitable_bins_remain_cap - avg_suitable_cap) + 1e-9)\n    else:\n        # If only one suitable bin, this component has no effect.\n        proximity_to_avg_score = np.ones_like(suitable_bins_remain_cap)\n\n    # --- Combination Strategy ---\n    # We want to maximize best_fit_score and proximity_to_avg_score,\n    # and maximize excess_penalty (which means minimizing the penalty term).\n    # Multiply scores together.\n    # Using weights to balance the contributions of each component. These weights\n    # can be tuned based on empirical performance. For now, we give equal weight conceptually.\n    \n    # The effective priority for suitable bins is the product of their scores.\n    # Higher scores are better.\n    priorities[suitable_bins_mask] = (best_fit_score * excess_penalty * proximity_to_avg_score)\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 vs Heuristic 2: Heuristic 1 uses a log transform and a `tightness_ratio` for its score, while Heuristic 2 uses an inverse remaining capacity and a normalized excess capacity penalty. Heuristic 1's score `best_fit_score * penalty_multiplier` (where `penalty_multiplier = 1.0 / (excess_ratio + 0.2)`) appears more direct in penalizing bins with large excess capacity relative to item size.\n\nComparing Heuristic 3 vs Heuristic 7 (which are identical): Both use `fill_ratio * tightness` as their score, which is `item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`. This score effectively balances the tightness of the fit with how much of the remaining space the item occupies, and it implicitly penalizes bins with large `suitable_bins_remain_cap`.\n\nComparing Heuristic 4 vs Heuristic 9 (which are identical): Both use a sigmoid function based on normalized slack. Heuristic 4 maps `max_slack - slack` to the sigmoid, aiming for higher scores for smaller slack. Heuristic 9 directly uses `differences * scale_factor` as the sigmoid input, achieving a similar goal.\n\nComparing Heuristic 5 vs Heuristic 6 (which are identical): Both prioritize exact fits with a high score, then use inverse remaining capacity for non-exact fits, normalizing these scores. This is a robust approach for prioritizing exact fits.\n\nComparing Heuristic 10 vs Heuristic 11/12: Heuristic 10 uses a product of Best Fit, Excess Capacity Penalty, and Distributional Balancing. Heuristics 11/12 combine Best Fit with a preference for \"almost full\" bins using a weighted sum. Heuristic 11/12's direct combination of `1.0 / (bins_remain_cap + epsilon_small)` and `0.5 * (-remaining_after_fit)` seems more straightforward than Heuristic 10's multiplicative approach with potentially complex interactions.\n\nComparing Heuristic 13/14/15 vs Heuristic 16: Heuristics 13/14/15 prioritize exact fits (score 1.0) and then use an exponential decay `exp(-relative_capacities / mean_relative_capacity)` for non-exact fits. Heuristic 16 prioritizes exact fits with a high score and uses `exp(-scaled_relative_capacities)` for non-exact fits, where `scaled_relative_capacities` is normalized between 0 and 1. Heuristic 16's normalization and conditional scaling might lead to more stable and comparable scores.\n\nComparing Heuristic 17/18 vs Heuristic 19/20: Heuristics 17/18 use a sigmoid `1 / (1 + exp(-scaled_differences))` based on the negative difference for tight fits. Heuristics 19/20 use `exp(remaining_capacities)` scaled and normalized, which seems to amplify the preference for tighter fits directly. The sigmoid in 17/18 might offer smoother control.\n\nOverall: Heuristics focusing on multiplicative combinations of \"tightness\" (like `1/(rem-item)`) and \"fill ratio\" (like `item/rem`), or those that directly penalize large initial remaining capacities, seem most promising for balancing fit quality and overall bin utilization. Exact fit prioritization is also a strong strategy. The complexity of penalties and normalizations can sometimes obscure the intended behavior.\n- \nHere's a redefined approach to self-reflection for heuristic design:\n\n*   **Keywords:** Iterative refinement, empirical validation, component interaction, adaptability.\n*   **Advice:** Focus on how heuristic components *interact* and *evolve* during search. Design for *adaptability* based on problem instance characteristics.\n*   **Avoid:** Over-reliance on fixed mathematical combinations or static penalties.\n*   **Explanation:** Instead of solely prioritizing fixed metrics like \"tightness\" or \"fill ratio,\" continuously evaluate their effectiveness *in conjunction* throughout the heuristic's execution. Tune component weights or logic dynamically, rather than with static formulas.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}