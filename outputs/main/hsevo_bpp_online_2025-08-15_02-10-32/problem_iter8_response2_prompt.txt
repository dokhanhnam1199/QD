{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a dynamically weighted \"slack\" minimization and a \"first-fit-like\" fallback.\n    This heuristic prioritizes bins that offer a close fit (minimize slack), but also\n    considers the number of available bins to avoid premature opening of new bins,\n    and uses a more robust way to handle near-perfect fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Mask for bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities  # No bin can fit the item\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Component 1: Best Fit - Prioritize bins with minimum remaining capacity after packing.\n    # This is the core of minimizing wasted space in the chosen bin.\n    # We want to maximize -(remaining_capacity - item), which is equivalent to minimizing (remaining_capacity - item).\n    # Using log to compress the range and make smaller differences more impactful.\n    # Add a small epsilon to prevent log(0) or division by zero if remaining_cap == item.\n    best_fit_score = -np.log(suitable_bins_remain_cap - item + 1e-9)\n    \n    # Component 2: Slack Minimization (refined)\n    # Penalize bins where the remaining capacity is significantly larger than the item size.\n    # This is about finding a \"good enough\" fit, not necessarily the absolute tightest if it leaves too much void.\n    # Let's consider the ratio of leftover space to the item size.\n    # `slack_ratio = (remaining_capacity - item) / item`\n    # We want to penalize high slack ratios. A score of `1 / (slack_ratio + C)` would work.\n    # A more nuanced approach: Consider the \"fill ratio\" of the *potential* packed bin.\n    # `fill_ratio = item / (bin_capacity_before_packing)` is not directly available here.\n    # Instead, let's focus on `item / (item + slack)` where slack is `remaining_cap - item`.\n    # This is `item / remaining_cap`. We want to maximize this, as it means the item fills a larger\n    # portion of the bin's *current* remaining capacity.\n    \n    # Calculate the potential fill ratio if the item is placed in the bin.\n    # This captures how \"full\" the bin would become with this item.\n    potential_fill_ratio = item / (suitable_bins_remain_cap + 1e-9)\n    \n    # We want to maximize this ratio. High fill ratio is good.\n    fill_score = potential_fill_ratio\n    \n    # Component 3: Dynamic Weighting based on number of available bins (adaptability)\n    # If there are many bins available, we can be pickier about the fit.\n    # If there are few bins, we might prioritize filling any available bin more easily.\n    # Let's introduce a factor that slightly favors more \"efficient\" bins,\n    # but also consider that \"almost perfect\" fits are good.\n    \n    # A simple weighting scheme: Higher weight for bins that leave less slack relative to the item size.\n    # `slack_penalty_factor = 1.0 / (1.0 + (suitable_bins_remain_cap - item) / (item + 1e-9))`\n    # This factor is close to 1 for tight fits and decreases as slack increases.\n    slack_penalty_factor = 1.0 / (1.0 + ((suitable_bins_remain_cap - item) / (item + 1e-9))**0.5)\n    \n    # Combine scores:\n    # We want to maximize `best_fit_score` (minimize slack) and `fill_score` (maximize item usage).\n    # The `slack_penalty_factor` down-weights bins with excessive slack.\n    # A multiplicative combination seems appropriate:\n    # Score = (best_fit_score_proxy) * (fill_score_proxy) * (slack_penalty_factor)\n    # Let's use `best_fit_score` directly as it already encodes minimizing slack.\n    # The `fill_score` complements `best_fit_score` by ensuring the item itself is substantial relative to the bin's remaining capacity.\n    \n    # We want to maximize: `best_fit_score` (log of inverse slack) AND `fill_score` (item / remaining_cap).\n    # A potential issue with simple multiplication is that if one component is very small, it can dominate.\n    # Let's consider a convex combination or a sum after scaling.\n    # A common approach is to use a weighted sum: `w1 * bf_score + w2 * fill_score + w3 * sf_factor`.\n    # However, components are on different scales.\n\n    # Let's refine the interaction:\n    # `best_fit_score` prioritizes minimal `remaining_cap - item`.\n    # `fill_score` prioritizes `item / remaining_cap`.\n    # These two are often aligned: when `remaining_cap - item` is small, `item / remaining_cap` is large.\n    # The `slack_penalty_factor` ensures we don't excessively penalize bins that might be slightly less \"best fit\"\n    # if they offer a better overall fill ratio for the item.\n\n    # Let's try a combined score that emphasizes tight fits but also the quality of the item's fill.\n    # Score = alpha * best_fit_score + beta * fill_score\n    # We can also incorporate the idea of not overly penalizing slight deviations from perfect fit.\n    # Consider `(suitable_bins_remain_cap - item)` as slack. We want small slack.\n    # `best_fit_score` is `-log(slack + epsilon)`.\n    # `fill_score` is `item / (item + slack + epsilon)`.\n\n    # Let's try to combine them additively after normalization or scaling.\n    # A simpler multiplicative approach that captures both:\n    # Prioritize bins where `item` is a significant fraction of `suitable_bins_remain_cap`\n    # AND `suitable_bins_remain_cap` is not excessively large.\n    \n    # The \"tightness\" metric: `item / suitable_bins_remain_cap`. Higher is better.\n    # The \"slack\" metric: `suitable_bins_remain_cap - item`. Lower is better.\n    # Let's map slack to a score: `1 / (slack + 1)` or `-log(slack + epsilon)`.\n    # `best_fit_score` is already `-log(slack + epsilon)`.\n    \n    # Let's boost the score for bins that are closer to \"perfect fit\" without being too sensitive.\n    # The `slack_penalty_factor` already does this by down-weighting large slacks.\n    \n    # Combine `best_fit_score` and `fill_score`.\n    # `best_fit_score` rewards small `slack`.\n    # `fill_score` rewards large `item` relative to `suitable_bins_remain_cap`.\n    # If `slack` is small, `item` is close to `suitable_bins_remain_cap`, so `fill_score` will be near 1.\n    # If `slack` is large, `best_fit_score` will be low. `fill_score` will also be low.\n    \n    # Let's try a composite score that is robust to very small items.\n    # If item is very small, `fill_score` might be very small, and `best_fit_score` might be very large (due to log).\n    # We want to favor bins that are not excessively large *even if* the item is small.\n    \n    # Final approach: Combine Best Fit with a \"Fair Fit\" metric.\n    # Best Fit: Minimize `remaining_capacity - item`. Score: `-log(slack + epsilon)`.\n    # Fair Fit: Maximize `item / (item + slack)`. This is `item / suitable_bins_remain_cap`.\n    # A potential issue is if `suitable_bins_remain_cap` is very large, even if `slack` is small.\n    # We want to prioritize bins that are not excessively larger than `item` + `slack`.\n    \n    # Let's adjust the `best_fit_score` to also consider the absolute remaining capacity.\n    # Penalize bins with very large remaining capacity, even if the slack is small.\n    # `capacity_penalty = -log(suitable_bins_remain_cap + 1e-9)`\n    # Combining `best_fit_score` and `capacity_penalty`:\n    # `combined_fit = best_fit_score + capacity_penalty`\n    # This would prioritize bins where `slack` is small AND `suitable_bins_remain_cap` is small.\n    \n    # Let's refine again: Focus on two key aspects:\n    # 1. How well does the item fit into the remaining capacity (tightness)?\n    # 2. How much capacity is left *after* fitting (waste)?\n    \n    # Metric 1: Tightness. Maximize `item / suitable_bins_remain_cap`.\n    tightness_score = item / (suitable_bins_remain_cap + 1e-9)\n    \n    # Metric 2: Waste. Minimize `suitable_bins_remain_cap - item`.\n    # We can use `-log(slack + epsilon)` as before.\n    waste_score = -np.log(suitable_bins_remain_cap - item + 1e-9)\n    \n    # Combine these: A simple addition can work if they are on comparable scales or normalized.\n    # `tightness_score` is between 0 and 1. `waste_score` can be large negative for large slack.\n    # Let's use a weighted sum where both components are maximized.\n    # Higher `tightness_score` is good. Higher `waste_score` (less negative) is good.\n    \n    # Let's re-introduce the `slack_penalty_factor` to modulate the contribution of `tightness_score`.\n    # We want `tightness_score` to be high, but not at the expense of extreme waste.\n    # The `slack_penalty_factor` provides this: `1.0 / (1.0 + (slack / item)**0.5)`.\n    # If slack is large relative to item, this factor becomes small.\n    \n    # A combined score could be:\n    # `final_score = tightness_score * (1 + waste_score/some_scale) * slack_penalty_factor`\n    # Or simpler: maximize the product of factors that are good.\n    \n    # Prioritize bins where `item` is a significant portion of the remaining capacity,\n    # and the absolute slack is not excessively large.\n    \n    # Let's consider the combination:\n    # - Prioritize small slack: `waste_score = -log(slack + epsilon)`\n    # - Prioritize good fill: `tightness_score = item / (item + slack)`\n    # - Avoid very large bins: Perhaps a penalty based on `suitable_bins_remain_cap` itself.\n    \n    # Let's try a heuristic that balances these:\n    # `score = (item / suitable_bins_remain_cap) * exp(- (suitable_bins_remain_cap - item) / (item + epsilon) )`\n    # This encourages high `item / suitable_bins_remain_cap` and penalizes large slack relative to item size.\n    \n    slack = suitable_bins_remain_cap - item\n    \n    # Score 1: How much of the remaining capacity is used by the item (tightness)\n    tightness_ratio = item / (suitable_bins_remain_cap + 1e-9)\n    \n    # Score 2: How much waste is left *relative to the item size* (normalized slack)\n    # We want to penalize large relative slack. Exponential decay is good for this.\n    # Higher `slack_penalty_factor` means less penalty. So, we want to minimize `slack / item`.\n    # Let's use `exp(-(slack / item))` as a factor that decreases with slack.\n    # Add a small value to item to prevent division by zero for zero-sized items (though unlikely in BPP).\n    relative_slack = slack / (item + 1e-9)\n    # We want to penalize large relative_slack. So, `exp(-relative_slack)` is a good candidate.\n    # A factor that is close to 1 for small relative_slack and decays to 0 for large relative_slack.\n    slack_decay_factor = np.exp(-relative_slack)\n    \n    # Combine: Maximize `tightness_ratio` and `slack_decay_factor`.\n    # A multiplicative approach ensures both are considered.\n    # `final_score = tightness_ratio * slack_decay_factor`\n    \n    # This encourages bins where the item fills a good portion of the remaining space,\n    # AND the leftover space is not excessively large compared to the item.\n    \n    priorities[suitable_bins_mask] = tightness_ratio * slack_decay_factor\n    \n    # Edge case: If multiple bins have identical scores, we might want a tie-breaker.\n    # A simple tie-breaker could be First Fit (preferring lower index bins) or Best Fit (preferring bins with less absolute slack).\n    # For now, numpy's `argmax` will pick the first occurrence in case of ties.\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a dynamic penalty for large remaining capacity,\n    prioritizing tight fits and penalizing bins with excessive unused space.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    # If no bins can fit the item, return zero priorities\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Component 1: Best Fit (minimize remaining capacity after packing)\n    # We want to maximize -log(remaining_capacity_after_packing).\n    # Adding a small epsilon to prevent log(0) or division by zero issues.\n    remaining_after_fit = suitable_bins_remain_cap - item\n    best_fit_score = -np.log(remaining_after_fit + 1e-9)\n    \n    # Component 2: Dynamic Penalty for Excess Capacity\n    # Penalize bins where the *initial* remaining capacity is much larger than the item.\n    # We want to penalize bins with a high ratio of (bin_capacity / item_size)\n    # or equivalently, a low ratio of (item_size / bin_capacity).\n    # A simple penalty can be based on (item / bin_capacity).\n    # A high item/bin_capacity ratio is good.\n    # Let's use 1 - (item / bin_capacity) as a penalty: higher value means more excess.\n    # We want to *minimize* this penalty. So, use -(1 - item / bin_capacity) or (item / bin_capacity - 1).\n    # A different approach: penalize large initial remaining capacity relative to the item.\n    # Consider 'excess_ratio_initial' = (suitable_bins_remain_cap - item) / item\n    # A high excess_ratio_initial is bad. We want to penalize it.\n    # Penalty multiplier = 1 / (excess_ratio_initial + C)\n    # This is similar to the logic in priority_v0 for penalty_multiplier.\n    \n    # Let's use the concept of \"slack\" (initial remaining capacity) and \"tightness\" (remaining after fit)\n    # We want to prioritize bins with low slack AND low remaining_after_fit.\n    \n    # Using a multiplicative approach combining Best Fit and a penalty for initial large capacity.\n    # The penalty term should be higher for bins with larger initial capacity compared to the item.\n    # Consider the inverse of the remaining capacity as a score for \"fullness\".\n    # Add epsilon to avoid division by zero.\n    fullness_score = 1.0 / (suitable_bins_remain_cap + 1e-9)\n    \n    # Combine Best Fit score with the fullness score.\n    # A high best_fit_score (tight fit after packing) is good.\n    # A high fullness_score (bin is initially quite full) is also good.\n    # Multiplying them seems reasonable: a bin is good if it's a tight fit AND was already quite full.\n    combined_score = best_fit_score * fullness_score\n    \n    # Assign the calculated scores to the priority array\n    priorities[suitable_bins_mask] = combined_score\n    \n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), the code is identical. The ranking suggests a subtle difference in conceptualization, not implementation.\n\nComparing (2nd) vs (3rd), the code is identical. The ranking is arbitrary or based on external factors not evident from the code.\n\nComparing (3rd) vs (4th): Heuristic 3 attempts to combine \"Best Fit\" with a penalty for large excess capacity relative to item size using a `tightness_ratio` and `penalty_component`. Heuristic 4 introduces \"Worst Fit\" concepts and a \"dynamic gap penalty,\" but its implementation primarily focuses on `best_fit_score * gap_management_score`, where `gap_management_score` is `1.0 / (1.0 + (gap / item))`. Heuristic 3's penalty is more directly tied to the `remaining_cap - item` ratio, whereas Heuristic 4 uses `item / (remaining_cap)`. The core idea of penalizing large gaps is similar, but Heuristic 4's `gap_management_score` might be more stable for very small items.\n\nComparing (4th) vs (5th): Heuristic 4 uses a multiplicative combination of Best Fit and a gap management score. Heuristic 5 introduces a sigmoid for tight fits and a penalty for large initial capacities, combining them multiplicatively. Heuristic 5's sigmoid introduces non-linearity, potentially offering smoother preferences for tight fits. Heuristic 4's approach is more directly interpretable as Best Fit scaled by a tightness factor.\n\nComparing (5th) vs (6th): Heuristic 5 uses a sigmoid and a simple inverse capacity penalty. Heuristic 6 prioritizes exact fits with a very high score and then uses exponential decay based on normalized slack for non-exact fits. Heuristic 6's explicit handling of exact fits is a strong point, but its normalization of slack might be sensitive to extreme values.\n\nComparing (6th) vs (7th): Heuristic 6 has explicit exact fit handling and exponential decay. Heuristic 7 also prioritizes exact fits (with a slightly lower score) and then combines Best Fit (`1.0 / (remaining_capacities_after_fit)`) with a preference for \"almost full\" bins (using `1.0 / (suitable_capacities)`). Heuristic 7's additive combination of these two preferences, weighted, offers a different balance.\n\nComparing (7th) vs (8th): Heuristics 7 and 8 are identical.\n\nComparing (8th) vs (9th): Heuristic 8 uses a sigmoid for tight fits and an inverse capacity penalty. Heuristic 9 combines a tightness score (`1.0 / slack`) with a fill ratio (`item / remaining_cap`), using multiplication. Heuristic 9's multiplicative approach is more direct in rewarding both tight fits and good item utilization within the bin.\n\nComparing (9th) vs (10th): Heuristic 9 multiplies tightness and fill ratio. Heuristic 10 does the same but adds an \"almost full\" bonus, which is essentially another term favoring bins with low initial remaining capacity. The additive bonus provides an additional layer of preference.\n\nComparing (10th) vs (11th): Heuristic 10 combines efficiency (tightness * fill ratio) with an \"almost full\" bonus additively. Heuristic 11 uses a weighted additive combination of \"almost full\" preference and \"tightest fit\" preference. Heuristic 11's explicit weighting of preferences is more structured than Heuristic 10's additive bonus.\n\nComparing (11th) vs (12th): Heuristics 11 and 12 are identical.\n\nComparing (12th) vs (13th): Heuristics 12 and 13 are identical.\n\nComparing (13th) vs (14th): Heuristic 13 is identical to 11 and 12. Heuristic 14 introduces an \"adaptive Best Fit\" by dynamically weighting a tightness component based on the item's size relative to average remaining capacity. It also uses a log-based best-fit score. This adaptivity is a novel aspect.\n\nComparing (14th) vs (15th): Heuristics 14 and 15 are identical.\n\nComparing (15th) vs (16th): Heuristic 15 is identical to 14. Heuristic 16 combines Best Fit (log scale) with a refined slack minimization and a fill ratio component, using multiplication. It also introduces a `slack_decay_factor` (`exp(-relative_slack)`). This multiplicative approach with an exponential decay is a strong combination.\n\nComparing (16th) vs (17th): Heuristics 16 and 17 are identical.\n\nComparing (17th) vs (18th): Heuristics 17 and 18 are identical.\n\nComparing (18th) vs (19th): Heuristic 18 is identical to 16 and 17. Heuristic 19 attempts to combine Best Fit, an adaptive slack penalty (normalized by average excess capacity), and a uniformity score (penalizing deviation from average remaining capacity). This multi-faceted approach aims for more balanced packing.\n\nComparing (19th) vs (20th): Heuristic 19 combines Best Fit (log), adaptive slack penalty, and uniformity. Heuristic 20 combines Best Fit (log), fullness score (inverse capacity), multiplicatively. Heuristic 19's attempt at adaptivity and uniformity suggests a more sophisticated strategy than Heuristic 20's simpler multiplicative combination.\n\nOverall, heuristics that explicitly handle exact fits (6, 7), use adaptive weights or penalties based on item size or system state (14, 15, 19), or combine multiple strong metrics multiplicatively with decay functions (16, 17, 18) appear to be conceptually superior. The ranking seems to generally increase in complexity and adaptivity up to a point, then potentially plateau or slightly decline in perceived quality.\n- \nHere's a redefined approach to self-reflection for heuristic design:\n\n*   **Keywords:** Robustness, adaptability, clarity, parsimony, performance, objective function.\n*   **Advice:** Focus on designing heuristics with clear, interpretable objective functions that can adapt to varying problem states (e.g., item sizes, bin utilization). Prioritize vectorized implementations for efficiency.\n*   **Avoid:** Redundant or overly complex metric combinations, arbitrary thresholds, and heuristics that don't demonstrably improve performance across a range of scenarios.\n*   **Explanation:** True self-reflection means identifying core principles of good heuristic design (like clarity and adaptability) and ruthlessly eliminating elements that introduce complexity without clear benefit, ensuring robustness and efficient execution.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}