{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines Best Fit (tightness) with a sigmoid for prioritizing near-exact fits.\n\n    Prioritizes bins that are almost full but can fit the item, using a sigmoid\n    to smooth the preference for tighter fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[fit_mask]\n\n    # Heuristic: Prioritize bins that are \"almost full\" but can still fit the item.\n    # This is inspired by Best Fit, but uses a sigmoid to give a smoother preference\n    # to bins where remaining_capacity - item is small.\n    # The input to the sigmoid is scaled such that tighter fits result in a higher score.\n    # We use -(valid_bins_remain_cap - item) to make smaller remaining space\n    # correspond to larger (less negative) sigmoid inputs.\n\n    # A simple scaling to avoid extreme sigmoid values too quickly.\n    # The range of (bins_remain_cap - item) can vary. Let's normalize it.\n    # For bins that fit, the \"slack\" is valid_bins_remain_cap - item.\n    # We want to prioritize smaller slack.\n    slack = valid_bins_remain_cap - item\n\n    # Normalize slack to be between 0 and 1 for sigmoid input.\n    # If all slack is the same, avoid division by zero.\n    if slack.size > 0:\n        min_slack = np.min(slack)\n        max_slack = np.max(slack)\n\n        if max_slack == min_slack:\n            normalized_slack = np.zeros_like(slack)\n        else:\n            # Map slack to a range where sigmoid can differentiate well.\n            # We want smaller slack to map to a higher priority.\n            # So, map min_slack (tightest fit) to a high sigmoid input,\n            # and max_slack (loosest fit) to a low sigmoid input.\n            # Consider the inverse of slack: 1 / (slack + epsilon) is similar to Best Fit.\n            # Let's use a transformation like: 1 - (slack / max_slack) or similar.\n            # A sigmoid on -(slack) might be good: larger negative means smaller slack.\n            # sigmoid_input = -slack\n            # To control steepness and range, we can use:\n            steepness = 5.0 # Tune this parameter\n            # We want smaller slack to give higher priority.\n            # So, we want a higher value when slack is small.\n            # Transform slack to a value that is higher for smaller slack.\n            # Example: max_slack - slack. Then normalize.\n            transformed_slack = max_slack - slack\n            if max_slack - min_slack > 0:\n                normalized_transformed_slack = transformed_slack / (max_slack - min_slack)\n            else:\n                normalized_transformed_slack = np.zeros_like(slack)\n\n            # Use sigmoid on the transformed slack. High transformed_slack (low original slack)\n            # should map to a high sigmoid output.\n            # We can use `steepness * (normalized_transformed_slack - 0.5)` to center around 0.5.\n            sigmoid_input = steepness * (normalized_transformed_slack - 0.5)\n            priorities[fit_mask] = 1 / (1 + np.exp(-sigmoid_input))\n        \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a sigmoid-like preference for tighter fits.\n    Prioritizes bins that leave minimal remaining capacity after packing,\n    but uses a scaled exponential to amplify this preference.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Find bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if not np.any(can_fit_mask):\n        return priorities # No bin can fit the item\n        \n    # Calculate remaining capacity after fitting the item\n    remaining_capacities = fitting_bins_remain_cap - item\n    \n    # Use a scaled exponential to prioritize tighter fits (similar to Heuristic 17 but simpler)\n    # This amplifies the preference for bins with smaller remaining_capacities\n    # Add a small epsilon to avoid division by zero if all remaining capacities are the same\n    epsilon = 1e-8\n    scaled_preference = np.exp(remaining_capacities)\n    \n    # Normalize the preference scores so they sum to 1 for the fitting bins\n    sum_scaled_preference = np.sum(scaled_preference)\n    if sum_scaled_preference > 0:\n        normalized_preference = scaled_preference / sum_scaled_preference\n    else:\n        # If all scaled preferences are zero (e.g., due to very large negative exponents if we used them)\n        # assign equal probability to all fitting bins.\n        normalized_preference = np.ones_like(fitting_bins_remain_cap) / len(fitting_bins_remain_cap)\n\n    priorities[can_fit_mask] = normalized_preference\n    \n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 vs Heuristic 2: Heuristic 1 uses a log transform and a `tightness_ratio` for its score, while Heuristic 2 uses an inverse remaining capacity and a normalized excess capacity penalty. Heuristic 1's score `best_fit_score * penalty_multiplier` (where `penalty_multiplier = 1.0 / (excess_ratio + 0.2)`) appears more direct in penalizing bins with large excess capacity relative to item size.\n\nComparing Heuristic 3 vs Heuristic 7 (which are identical): Both use `fill_ratio * tightness` as their score, which is `item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`. This score effectively balances the tightness of the fit with how much of the remaining space the item occupies, and it implicitly penalizes bins with large `suitable_bins_remain_cap`.\n\nComparing Heuristic 4 vs Heuristic 9 (which are identical): Both use a sigmoid function based on normalized slack. Heuristic 4 maps `max_slack - slack` to the sigmoid, aiming for higher scores for smaller slack. Heuristic 9 directly uses `differences * scale_factor` as the sigmoid input, achieving a similar goal.\n\nComparing Heuristic 5 vs Heuristic 6 (which are identical): Both prioritize exact fits with a high score, then use inverse remaining capacity for non-exact fits, normalizing these scores. This is a robust approach for prioritizing exact fits.\n\nComparing Heuristic 10 vs Heuristic 11/12: Heuristic 10 uses a product of Best Fit, Excess Capacity Penalty, and Distributional Balancing. Heuristics 11/12 combine Best Fit with a preference for \"almost full\" bins using a weighted sum. Heuristic 11/12's direct combination of `1.0 / (bins_remain_cap + epsilon_small)` and `0.5 * (-remaining_after_fit)` seems more straightforward than Heuristic 10's multiplicative approach with potentially complex interactions.\n\nComparing Heuristic 13/14/15 vs Heuristic 16: Heuristics 13/14/15 prioritize exact fits (score 1.0) and then use an exponential decay `exp(-relative_capacities / mean_relative_capacity)` for non-exact fits. Heuristic 16 prioritizes exact fits with a high score and uses `exp(-scaled_relative_capacities)` for non-exact fits, where `scaled_relative_capacities` is normalized between 0 and 1. Heuristic 16's normalization and conditional scaling might lead to more stable and comparable scores.\n\nComparing Heuristic 17/18 vs Heuristic 19/20: Heuristics 17/18 use a sigmoid `1 / (1 + exp(-scaled_differences))` based on the negative difference for tight fits. Heuristics 19/20 use `exp(remaining_capacities)` scaled and normalized, which seems to amplify the preference for tighter fits directly. The sigmoid in 17/18 might offer smoother control.\n\nOverall: Heuristics focusing on multiplicative combinations of \"tightness\" (like `1/(rem-item)`) and \"fill ratio\" (like `item/rem`), or those that directly penalize large initial remaining capacities, seem most promising for balancing fit quality and overall bin utilization. Exact fit prioritization is also a strong strategy. The complexity of penalties and normalizations can sometimes obscure the intended behavior.\n- \nHere's a redefined approach to self-reflection for heuristic design:\n\n*   **Keywords:** Iterative refinement, empirical validation, component interaction, adaptability.\n*   **Advice:** Focus on how heuristic components *interact* and *evolve* during search. Design for *adaptability* based on problem instance characteristics.\n*   **Avoid:** Over-reliance on fixed mathematical combinations or static penalties.\n*   **Explanation:** Instead of solely prioritizing fixed metrics like \"tightness\" or \"fill ratio,\" continuously evaluate their effectiveness *in conjunction* throughout the heuristic's execution. Tune component weights or logic dynamically, rather than with static formulas.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}