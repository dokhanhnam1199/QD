```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines Best Fit with a dynamically weighted "slack" minimization and a "first-fit-like" fallback.
    This heuristic prioritizes bins that offer a close fit (minimize slack), but also
    considers the number of available bins to avoid premature opening of new bins,
    and uses a more robust way to handle near-perfect fits.
    """
    priorities = np.zeros_like(bins_remain_cap)
    
    # Mask for bins that can accommodate the item
    suitable_bins_mask = bins_remain_cap >= item
    
    if not np.any(suitable_bins_mask):
        return priorities  # No bin can fit the item

    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]
    
    # Component 1: Best Fit - Prioritize bins with minimum remaining capacity after packing.
    # This is the core of minimizing wasted space in the chosen bin.
    # We want to maximize -(remaining_capacity - item), which is equivalent to minimizing (remaining_capacity - item).
    # Using log to compress the range and make smaller differences more impactful.
    # Add a small epsilon to prevent log(0) or division by zero if remaining_cap == item.
    best_fit_score = -np.log(suitable_bins_remain_cap - item + 1e-9)
    
    # Component 2: Slack Minimization (refined)
    # Penalize bins where the remaining capacity is significantly larger than the item size.
    # This is about finding a "good enough" fit, not necessarily the absolute tightest if it leaves too much void.
    # Let's consider the ratio of leftover space to the item size.
    # `slack_ratio = (remaining_capacity - item) / item`
    # We want to penalize high slack ratios. A score of `1 / (slack_ratio + C)` would work.
    # A more nuanced approach: Consider the "fill ratio" of the *potential* packed bin.
    # `fill_ratio = item / (bin_capacity_before_packing)` is not directly available here.
    # Instead, let's focus on `item / (item + slack)` where slack is `remaining_cap - item`.
    # This is `item / remaining_cap`. We want to maximize this, as it means the item fills a larger
    # portion of the bin's *current* remaining capacity.
    
    # Calculate the potential fill ratio if the item is placed in the bin.
    # This captures how "full" the bin would become with this item.
    potential_fill_ratio = item / (suitable_bins_remain_cap + 1e-9)
    
    # We want to maximize this ratio. High fill ratio is good.
    fill_score = potential_fill_ratio
    
    # Component 3: Dynamic Weighting based on number of available bins (adaptability)
    # If there are many bins available, we can be pickier about the fit.
    # If there are few bins, we might prioritize filling any available bin more easily.
    # Let's introduce a factor that slightly favors more "efficient" bins,
    # but also consider that "almost perfect" fits are good.
    
    # A simple weighting scheme: Higher weight for bins that leave less slack relative to the item size.
    # `slack_penalty_factor = 1.0 / (1.0 + (suitable_bins_remain_cap - item) / (item + 1e-9))`
    # This factor is close to 1 for tight fits and decreases as slack increases.
    slack_penalty_factor = 1.0 / (1.0 + ((suitable_bins_remain_cap - item) / (item + 1e-9))**0.5)
    
    # Combine scores:
    # We want to maximize `best_fit_score` (minimize slack) and `fill_score` (maximize item usage).
    # The `slack_penalty_factor` down-weights bins with excessive slack.
    # A multiplicative combination seems appropriate:
    # Score = (best_fit_score_proxy) * (fill_score_proxy) * (slack_penalty_factor)
    # Let's use `best_fit_score` directly as it already encodes minimizing slack.
    # The `fill_score` complements `best_fit_score` by ensuring the item itself is substantial relative to the bin's remaining capacity.
    
    # We want to maximize: `best_fit_score` (log of inverse slack) AND `fill_score` (item / remaining_cap).
    # A potential issue with simple multiplication is that if one component is very small, it can dominate.
    # Let's consider a convex combination or a sum after scaling.
    # A common approach is to use a weighted sum: `w1 * bf_score + w2 * fill_score + w3 * sf_factor`.
    # However, components are on different scales.

    # Let's refine the interaction:
    # `best_fit_score` prioritizes minimal `remaining_cap - item`.
    # `fill_score` prioritizes `item / remaining_cap`.
    # These two are often aligned: when `remaining_cap - item` is small, `item / remaining_cap` is large.
    # The `slack_penalty_factor` ensures we don't excessively penalize bins that might be slightly less "best fit"
    # if they offer a better overall fill ratio for the item.

    # Let's try a combined score that emphasizes tight fits but also the quality of the item's fill.
    # Score = alpha * best_fit_score + beta * fill_score
    # We can also incorporate the idea of not overly penalizing slight deviations from perfect fit.
    # Consider `(suitable_bins_remain_cap - item)` as slack. We want small slack.
    # `best_fit_score` is `-log(slack + epsilon)`.
    # `fill_score` is `item / (item + slack + epsilon)`.

    # Let's try to combine them additively after normalization or scaling.
    # A simpler multiplicative approach that captures both:
    # Prioritize bins where `item` is a significant fraction of `suitable_bins_remain_cap`
    # AND `suitable_bins_remain_cap` is not excessively large.
    
    # The "tightness" metric: `item / suitable_bins_remain_cap`. Higher is better.
    # The "slack" metric: `suitable_bins_remain_cap - item`. Lower is better.
    # Let's map slack to a score: `1 / (slack + 1)` or `-log(slack + epsilon)`.
    # `best_fit_score` is already `-log(slack + epsilon)`.
    
    # Let's boost the score for bins that are closer to "perfect fit" without being too sensitive.
    # The `slack_penalty_factor` already does this by down-weighting large slacks.
    
    # Combine `best_fit_score` and `fill_score`.
    # `best_fit_score` rewards small `slack`.
    # `fill_score` rewards large `item` relative to `suitable_bins_remain_cap`.
    # If `slack` is small, `item` is close to `suitable_bins_remain_cap`, so `fill_score` will be near 1.
    # If `slack` is large, `best_fit_score` will be low. `fill_score` will also be low.
    
    # Let's try a composite score that is robust to very small items.
    # If item is very small, `fill_score` might be very small, and `best_fit_score` might be very large (due to log).
    # We want to favor bins that are not excessively large *even if* the item is small.
    
    # Final approach: Combine Best Fit with a "Fair Fit" metric.
    # Best Fit: Minimize `remaining_capacity - item`. Score: `-log(slack + epsilon)`.
    # Fair Fit: Maximize `item / (item + slack)`. This is `item / suitable_bins_remain_cap`.
    # A potential issue is if `suitable_bins_remain_cap` is very large, even if `slack` is small.
    # We want to prioritize bins that are not excessively larger than `item` + `slack`.
    
    # Let's adjust the `best_fit_score` to also consider the absolute remaining capacity.
    # Penalize bins with very large remaining capacity, even if the slack is small.
    # `capacity_penalty = -log(suitable_bins_remain_cap + 1e-9)`
    # Combining `best_fit_score` and `capacity_penalty`:
    # `combined_fit = best_fit_score + capacity_penalty`
    # This would prioritize bins where `slack` is small AND `suitable_bins_remain_cap` is small.
    
    # Let's refine again: Focus on two key aspects:
    # 1. How well does the item fit into the remaining capacity (tightness)?
    # 2. How much capacity is left *after* fitting (waste)?
    
    # Metric 1: Tightness. Maximize `item / suitable_bins_remain_cap`.
    tightness_score = item / (suitable_bins_remain_cap + 1e-9)
    
    # Metric 2: Waste. Minimize `suitable_bins_remain_cap - item`.
    # We can use `-log(slack + epsilon)` as before.
    waste_score = -np.log(suitable_bins_remain_cap - item + 1e-9)
    
    # Combine these: A simple addition can work if they are on comparable scales or normalized.
    # `tightness_score` is between 0 and 1. `waste_score` can be large negative for large slack.
    # Let's use a weighted sum where both components are maximized.
    # Higher `tightness_score` is good. Higher `waste_score` (less negative) is good.
    
    # Let's re-introduce the `slack_penalty_factor` to modulate the contribution of `tightness_score`.
    # We want `tightness_score` to be high, but not at the expense of extreme waste.
    # The `slack_penalty_factor` provides this: `1.0 / (1.0 + (slack / item)**0.5)`.
    # If slack is large relative to item, this factor becomes small.
    
    # A combined score could be:
    # `final_score = tightness_score * (1 + waste_score/some_scale) * slack_penalty_factor`
    # Or simpler: maximize the product of factors that are good.
    
    # Prioritize bins where `item` is a significant portion of the remaining capacity,
    # and the absolute slack is not excessively large.
    
    # Let's consider the combination:
    # - Prioritize small slack: `waste_score = -log(slack + epsilon)`
    # - Prioritize good fill: `tightness_score = item / (item + slack)`
    # - Avoid very large bins: Perhaps a penalty based on `suitable_bins_remain_cap` itself.
    
    # Let's try a heuristic that balances these:
    # `score = (item / suitable_bins_remain_cap) * exp(- (suitable_bins_remain_cap - item) / (item + epsilon) )`
    # This encourages high `item / suitable_bins_remain_cap` and penalizes large slack relative to item size.
    
    slack = suitable_bins_remain_cap - item
    
    # Score 1: How much of the remaining capacity is used by the item (tightness)
    tightness_ratio = item / (suitable_bins_remain_cap + 1e-9)
    
    # Score 2: How much waste is left *relative to the item size* (normalized slack)
    # We want to penalize large relative slack. Exponential decay is good for this.
    # Higher `slack_penalty_factor` means less penalty. So, we want to minimize `slack / item`.
    # Let's use `exp(-(slack / item))` as a factor that decreases with slack.
    # Add a small value to item to prevent division by zero for zero-sized items (though unlikely in BPP).
    relative_slack = slack / (item + 1e-9)
    # We want to penalize large relative_slack. So, `exp(-relative_slack)` is a good candidate.
    # A factor that is close to 1 for small relative_slack and decays to 0 for large relative_slack.
    slack_decay_factor = np.exp(-relative_slack)
    
    # Combine: Maximize `tightness_ratio` and `slack_decay_factor`.
    # A multiplicative approach ensures both are considered.
    # `final_score = tightness_ratio * slack_decay_factor`
    
    # This encourages bins where the item fills a good portion of the remaining space,
    # AND the leftover space is not excessively large compared to the item.
    
    priorities[suitable_bins_mask] = tightness_ratio * slack_decay_factor
    
    # Edge case: If multiple bins have identical scores, we might want a tie-breaker.
    # A simple tie-breaker could be First Fit (preferring lower index bins) or Best Fit (preferring bins with less absolute slack).
    # For now, numpy's `argmax` will pick the first occurrence in case of ties.
    
    return priorities
```
