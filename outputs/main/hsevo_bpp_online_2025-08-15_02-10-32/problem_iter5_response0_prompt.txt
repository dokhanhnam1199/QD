{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a dynamic penalty based on the item size relative to bin capacity.\n    Prioritizes bins that offer a \"near-perfect\" fit without excessive leftover space,\n    dynamically adjusting the penalty based on how \"tight\" the fit is.\n    This aims for better space utilization by being more sensitive to the actual item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # \"Best Fit\" component: Prioritize bins with minimum remaining capacity after packing.\n    # We use the negative of the remaining capacity to transform minimization into maximization.\n    # Add a small epsilon to ensure no division by zero or log(0) if remaining_cap == item.\n    # Using log to compress the range and emphasize smaller differences.\n    best_fit_score = -np.log(suitable_bins_remain_cap - item + 1e-9)\n    \n    # Dynamic Penalty component: Penalize bins with significantly more capacity than needed.\n    # The penalty is higher when the \"excess capacity\" (remaining_cap - item) is large\n    # relative to the item's size. This makes the penalty scale with the item's magnitude.\n    \n    # Calculate the \"tightness ratio\": (item_size) / (remaining_capacity_after_packing)\n    # A higher ratio means a tighter fit.\n    tightness_ratio = item / (suitable_bins_remain_cap - item + 1e-9)\n    \n    # Calculate a penalty that is higher for bins with a lower tightness ratio (more excess capacity relative to item size)\n    # We want to penalize bins where (suitable_bins_remain_cap - item) is large compared to 'item'.\n    # Using a sigmoid-like function (inverse of a scaled ratio) to dampen extreme values and provide a smoother penalty.\n    # The scaling factor (e.g., 1.0) can be tuned.\n    \n    # Higher penalty for lower tightness_ratio. Invert and add 1 to avoid division by zero and ensure positive penalty.\n    # A larger suitable_bins_remain_cap relative to 'item' leads to a smaller tightness_ratio,\n    # which after inversion and addition, results in a larger penalty.\n    # We want to *subtract* this penalty from the best_fit_score, so a higher penalty means a lower final score.\n    penalty_component = 1.0 / (tightness_ratio + 0.5) # Add 0.5 to avoid issues with very tight fits.\n    \n    # Combine the scores. We want to maximize `best_fit_score` and minimize `penalty_component`.\n    # A simple subtraction works if interpreted as score = bf_score - penalty.\n    # Alternatively, we can multiply if penalties were designed as multipliers.\n    # Here, we aim for a higher combined score. Since `best_fit_score` is already a maximization proxy,\n    # and `penalty_component` is something we want to minimize (i.e., a higher penalty is bad),\n    # we subtract the penalty.\n    \n    # To make it a maximization problem directly, we can express it as:\n    # Score = best_fit_score - penalty_component\n    # or, if we want to penalize the penalty:\n    # Score = best_fit_score * (1 / (penalty_component + epsilon)) which is equivalent to\n    # Score = best_fit_score * tightness_ratio (approximately)\n    # Let's use a multiplicative approach where a higher `penalty_component` reduces the score.\n    # A simple way to combine: maximize `best_fit_score` and maximize `1 / (penalty_component + epsilon)`\n    # This means maximizing `best_fit_score * (tightness_ratio)`.\n    \n    # Let's refine the penalty: Penalize bins where `remaining_cap - item` is large relative to `item`.\n    # Consider `excess_ratio = (remaining_cap - item) / item`. We want to penalize high `excess_ratio`.\n    # Penalty_score = 1 / (excess_ratio + 1).\n    # This is similar to the tightness ratio logic but framed differently.\n    \n    excess_ratio = (suitable_bins_remain_cap - item) / (item + 1e-9)\n    # A bin with exact fit has excess_ratio = 0. A bin with large excess has large excess_ratio.\n    # We want to penalize large excess_ratio. So, a good penalty multiplier would be 1 / (excess_ratio + C).\n    # The smaller the `1 / (excess_ratio + C)`, the worse the bin.\n    # So, we want to maximize `best_fit_score` and maximize `1 / (excess_ratio + C)`.\n    # Thus, we can multiply them.\n    \n    penalty_multiplier = 1.0 / (excess_ratio + 0.2) # Add 0.2 to ensure it's not too aggressive.\n    \n    # Final priority is the product of the best-fit score proxy and the penalty multiplier.\n    # Higher best_fit_score is good. Higher penalty_multiplier is good (means low excess ratio).\n    priorities[suitable_bins_mask] = best_fit_score * penalty_multiplier\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines Best Fit with a Sigmoid for nuanced bin prioritization.\n\n    Prioritizes bins that offer a tighter fit using a sigmoid function,\n    favoring bins where the remaining capacity is closer to the item size.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[eligible_bins_mask]\n\n    if eligible_bins_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"tightness\" of the fit for eligible bins.\n    # A smaller difference means a tighter fit.\n    differences = eligible_bins_cap - item\n\n    # Use a sigmoid function to map the differences to a priority score.\n    # A smaller difference (tighter fit) should result in a higher priority.\n    # We want to invert the difference so smaller differences are \"better\".\n    # The sigmoid function will then map these inverted differences to [0, 1].\n    # Scaling factor to control the steepness of the sigmoid.\n    scale_factor = 5.0\n    # Add a small epsilon to avoid division by zero if difference is 0.\n    inverted_differences = 1.0 / (differences + 1e-9)\n    \n    # Shift the scores so that a perfect fit (difference of 0) gets a high score.\n    # Since inverted_differences will be large for small differences,\n    # we can directly apply sigmoid or shift if needed.\n    # Here, a larger inverted_differences (meaning smaller original difference)\n    # should lead to higher priority.\n    \n    # Sigmoid function: 1 / (1 + exp(-x))\n    # We want higher priority for smaller differences.\n    # Let's use exp(-difference) as a base for priority.\n    # A larger value for exp(-difference) means a smaller difference.\n    # Then apply sigmoid to these values.\n    \n    # Option 1: Directly use exp(-difference) and sigmoid\n    # shifted_inverted_differences = -differences * scale_factor\n    # priorities = 1 / (1 + np.exp(-shifted_inverted_differences))\n    \n    # Option 2: Use 1/(difference + epsilon) and sigmoid\n    # The 'fit_scores' from v1 can be interpreted as how much \"room\" is left relative to the item.\n    # A score of 1 means exact fit. We want scores close to 1 to have high priority.\n    # Let's revisit v1 logic for better interpretation:\n    # fit_scores = valid_bins_cap / (valid_bins_cap - item + 1e-9)\n    # A higher fit_score means the bin is less full relative to the item size.\n    # This is NOT what we want for \"tight fit\". We want small (valid_bins_cap - item).\n    \n    # Let's go back to prioritizing smaller differences.\n    # We can directly use the negative difference, scaled, within the sigmoid.\n    # A smaller difference means a more desirable fit.\n    # We want the sigmoid output to be higher for smaller `differences`.\n    # `1 / (1 + exp(-k * difference))` will achieve this: as `difference` decreases, `-k * difference` increases, and sigmoid output increases.\n    \n    scaled_differences = differences * scale_factor\n    priorities = 1 / (1 + np.exp(-scaled_differences))\n\n    # Map priorities back to the original bins_remain_cap array\n    original_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_priorities[eligible_bins_mask] = priorities\n    \n    return original_priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 vs Heuristic 2: Heuristic 1 uses a log transform and a `tightness_ratio` for its score, while Heuristic 2 uses an inverse remaining capacity and a normalized excess capacity penalty. Heuristic 1's score `best_fit_score * penalty_multiplier` (where `penalty_multiplier = 1.0 / (excess_ratio + 0.2)`) appears more direct in penalizing bins with large excess capacity relative to item size.\n\nComparing Heuristic 3 vs Heuristic 7 (which are identical): Both use `fill_ratio * tightness` as their score, which is `item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`. This score effectively balances the tightness of the fit with how much of the remaining space the item occupies, and it implicitly penalizes bins with large `suitable_bins_remain_cap`.\n\nComparing Heuristic 4 vs Heuristic 9 (which are identical): Both use a sigmoid function based on normalized slack. Heuristic 4 maps `max_slack - slack` to the sigmoid, aiming for higher scores for smaller slack. Heuristic 9 directly uses `differences * scale_factor` as the sigmoid input, achieving a similar goal.\n\nComparing Heuristic 5 vs Heuristic 6 (which are identical): Both prioritize exact fits with a high score, then use inverse remaining capacity for non-exact fits, normalizing these scores. This is a robust approach for prioritizing exact fits.\n\nComparing Heuristic 10 vs Heuristic 11/12: Heuristic 10 uses a product of Best Fit, Excess Capacity Penalty, and Distributional Balancing. Heuristics 11/12 combine Best Fit with a preference for \"almost full\" bins using a weighted sum. Heuristic 11/12's direct combination of `1.0 / (bins_remain_cap + epsilon_small)` and `0.5 * (-remaining_after_fit)` seems more straightforward than Heuristic 10's multiplicative approach with potentially complex interactions.\n\nComparing Heuristic 13/14/15 vs Heuristic 16: Heuristics 13/14/15 prioritize exact fits (score 1.0) and then use an exponential decay `exp(-relative_capacities / mean_relative_capacity)` for non-exact fits. Heuristic 16 prioritizes exact fits with a high score and uses `exp(-scaled_relative_capacities)` for non-exact fits, where `scaled_relative_capacities` is normalized between 0 and 1. Heuristic 16's normalization and conditional scaling might lead to more stable and comparable scores.\n\nComparing Heuristic 17/18 vs Heuristic 19/20: Heuristics 17/18 use a sigmoid `1 / (1 + exp(-scaled_differences))` based on the negative difference for tight fits. Heuristics 19/20 use `exp(remaining_capacities)` scaled and normalized, which seems to amplify the preference for tighter fits directly. The sigmoid in 17/18 might offer smoother control.\n\nOverall: Heuristics focusing on multiplicative combinations of \"tightness\" (like `1/(rem-item)`) and \"fill ratio\" (like `item/rem`), or those that directly penalize large initial remaining capacities, seem most promising for balancing fit quality and overall bin utilization. Exact fit prioritization is also a strong strategy. The complexity of penalties and normalizations can sometimes obscure the intended behavior.\n- \nHere's a redefined approach to self-reflection for heuristic design:\n\n*   **Keywords:** Iterative refinement, empirical validation, component interaction, adaptability.\n*   **Advice:** Focus on how heuristic components *interact* and *evolve* during search. Design for *adaptability* based on problem instance characteristics.\n*   **Avoid:** Over-reliance on fixed mathematical combinations or static penalties.\n*   **Explanation:** Instead of solely prioritizing fixed metrics like \"tightness\" or \"fill ratio,\" continuously evaluate their effectiveness *in conjunction* throughout the heuristic's execution. Tune component weights or logic dynamically, rather than with static formulas.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}