[
  {
    "stdout_filepath": "problem_iter5_response0.txt_stdout.txt",
    "code_path": "problem_iter5_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a dynamic penalty for large remaining capacity,\n    prioritizing tight fits and penalizing bins with excessive unused space.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    # If no bins can fit the item, return zero priorities\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Component 1: Best Fit (minimize remaining capacity after packing)\n    # We want to maximize -log(remaining_capacity_after_packing).\n    # Adding a small epsilon to prevent log(0) or division by zero issues.\n    remaining_after_fit = suitable_bins_remain_cap - item\n    best_fit_score = -np.log(remaining_after_fit + 1e-9)\n    \n    # Component 2: Dynamic Penalty for Excess Capacity\n    # Penalize bins where the *initial* remaining capacity is much larger than the item.\n    # We want to penalize bins with a high ratio of (bin_capacity / item_size)\n    # or equivalently, a low ratio of (item_size / bin_capacity).\n    # A simple penalty can be based on (item / bin_capacity).\n    # A high item/bin_capacity ratio is good.\n    # Let's use 1 - (item / bin_capacity) as a penalty: higher value means more excess.\n    # We want to *minimize* this penalty. So, use -(1 - item / bin_capacity) or (item / bin_capacity - 1).\n    # A different approach: penalize large initial remaining capacity relative to the item.\n    # Consider 'excess_ratio_initial' = (suitable_bins_remain_cap - item) / item\n    # A high excess_ratio_initial is bad. We want to penalize it.\n    # Penalty multiplier = 1 / (excess_ratio_initial + C)\n    # This is similar to the logic in priority_v0 for penalty_multiplier.\n    \n    # Let's use the concept of \"slack\" (initial remaining capacity) and \"tightness\" (remaining after fit)\n    # We want to prioritize bins with low slack AND low remaining_after_fit.\n    \n    # Using a multiplicative approach combining Best Fit and a penalty for initial large capacity.\n    # The penalty term should be higher for bins with larger initial capacity compared to the item.\n    # Consider the inverse of the remaining capacity as a score for \"fullness\".\n    # Add epsilon to avoid division by zero.\n    fullness_score = 1.0 / (suitable_bins_remain_cap + 1e-9)\n    \n    # Combine Best Fit score with the fullness score.\n    # A high best_fit_score (tight fit after packing) is good.\n    # A high fullness_score (bin is initially quite full) is also good.\n    # Multiplying them seems reasonable: a bin is good if it's a tight fit AND was already quite full.\n    combined_score = best_fit_score * fullness_score\n    \n    # Assign the calculated scores to the priority array\n    priorities[suitable_bins_mask] = combined_score\n    \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 67.92979656960512,
    "cyclomatic_complexity": 2.0,
    "halstead": 91.73835003173087,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response1.txt_stdout.txt",
    "code_path": "problem_iter5_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a penalty for large remaining capacity.\n    Prioritizes bins where the item fits snugly and penalizes bins with\n    significant excess capacity, promoting efficient bin usage.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    # Metric 1: Tightness of fit (similar to Best Fit)\n    # High score for small `remaining_capacity - item`.\n    # Uses `1.0 / (slack + epsilon)`\n    tightness_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Metric 2: Fill ratio of remaining space\n    # High score for `item / suitable_bins_remain_cap` when `suitable_bins_remain_cap` is small.\n    # This implicitly penalizes bins with large `suitable_bins_remain_cap`.\n    fill_ratio = item / (suitable_bins_remain_cap + 1e-9)\n    \n    # Combine metrics multiplicatively: Prioritize bins that are both a tight fit\n    # AND where the item occupies a significant portion of the remaining capacity.\n    # This combination naturally penalizes bins with large `suitable_bins_remain_cap`\n    # because `fill_ratio` will be small, and `tightness_score`'s denominator\n    # will also be larger (though less impactful).\n    combined_score = tightness_score * fill_ratio\n\n    priorities[suitable_bin_indices] = combined_score\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 94.01164534875782,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response2.txt_stdout.txt",
    "code_path": "problem_iter5_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a sigmoid for prioritizing tight fits and relative fill ratio.\n\n    Prioritizes bins that are a tight fit using a sigmoid on slack, and boosts\n    priority for bins with a higher fill ratio.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[fit_mask]\n    \n    # Component 1: Best Fit tightness using sigmoid\n    # Prioritize bins where the remaining capacity after placing the item is minimal.\n    slack = valid_bins_remain_cap - item\n    \n    # Normalize slack to a range suitable for sigmoid, focusing on smaller slack\n    # which indicates a tighter fit. We want smaller slack to yield a higher score.\n    min_slack = np.min(slack)\n    max_slack = np.max(slack)\n    \n    if max_slack == min_slack:\n        normalized_slack_for_sigmoid = np.zeros_like(slack)\n    else:\n        # Map slack to a range that gives higher sigmoid input for smaller slack.\n        # (max_slack - slack) makes smaller slack values larger.\n        transformed_slack = max_slack - slack\n        normalized_transformed_slack = transformed_slack / (max_slack - min_slack)\n        \n    # Apply sigmoid to the normalized transformed slack. Steepness controls sensitivity.\n    steepness = 5.0\n    sigmoid_input = steepness * (normalized_transformed_slack - 0.5)\n    best_fit_score = 1 / (1 + np.exp(-sigmoid_input))\n\n    # Component 2: Relative Fill Ratio (incorporating item size and bin capacity)\n    # Prioritize bins that are relatively full with respect to their current remaining capacity.\n    # This encourages using bins that already have a significant portion of their capacity used.\n    # We use `item / valid_bins_remain_cap` as a proxy for fill ratio.\n    # Add a small epsilon to avoid division by zero if remaining capacity is zero (though filtered by fit_mask).\n    fill_ratio_score = item / (valid_bins_remain_cap + 1e-9)\n    \n    # Combine scores: Multiply for synergy. Higher values from both components are preferred.\n    # This combines the tightness of the fit (sigmoid) with the relative fullness of the bin.\n    priorities[fit_mask] = best_fit_score * fill_ratio_score\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 36, in priority_v2\n    penalty = np.zeros_like(suitable_capacities, dtype=float)\nUnboundLocalError: local variable 'normalized_transformed_slack' referenced before assignment\n3\n208.89318279048564\n"
  },
  {
    "stdout_filepath": "problem_iter5_response3.txt_stdout.txt",
    "code_path": "problem_iter5_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines exact fit prioritization with a refined best-fit strategy that\n    penalizes overly large remaining capacities, encouraging fuller bins.\n    \"\"\"\n    epsilon_small = 1e-9\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Mask for bins that can fit the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    # Assign very high priority to exact fits\n    exact_fit_mask = np.isclose(bins_remain_cap, item)\n    priorities[exact_fit_mask] = 1e9\n\n    # For bins that are not an exact fit but can accommodate the item\n    non_exact_suitable_mask = suitable_bins_mask & ~exact_fit_mask\n\n    if np.any(non_exact_suitable_mask):\n        suitable_capacities = bins_remain_cap[non_exact_suitable_mask]\n        remaining_capacities_after_fit = suitable_capacities - item\n\n        # Base priority: inverse of remaining capacity after fit (Best Fit)\n        # Higher score for smaller remaining capacity\n        best_fit_scores = 1.0 / (remaining_capacities_after_fit + epsilon_small)\n\n        # Penalty for bins that are initially \"too empty\"\n        # We want to favor bins that are already somewhat filled.\n        # Penalize bins where initial `bins_remain_cap` is significantly larger than the item.\n        # A threshold of `item * 3.0` for initial capacity is used.\n        # The penalty is scaled inversely by the initial capacity itself to avoid\n        # overly aggressive penalties for moderately large capacities.\n        large_capacity_threshold = item * 3.0\n        penalty = np.zeros_like(suitable_capacities, dtype=float)\n        \n        large_capacity_mask = suitable_capacities > large_capacity_threshold\n        \n        # Calculate penalty: Lower score for larger initial capacities.\n        # Use a scaled inverse of the initial remaining capacity.\n        # Adding a small constant `epsilon_small` to the denominator to prevent division by zero.\n        # Subtracting this scaled inverse from the best_fit_scores.\n        penalty[large_capacity_mask] = 1.0 / (suitable_capacities[large_capacity_mask] + epsilon_small)\n        \n        # Combine scores: Prioritize tight fits, then penalize very empty bins.\n        # We add the penalty (which is negative in effect due to `1/cap` nature) to the best fit score.\n        # A smaller initial capacity (leading to a larger `1/cap`) is less penalized.\n        # A larger initial capacity (leading to a smaller `1/cap`) is more penalized.\n        \n        # Let's refine this. We want:\n        # 1. High score for small `remaining_capacities_after_fit`\n        # 2. High score for small `suitable_capacities` (initial)\n        \n        # Score = `(1.0 / (remaining_capacities_after_fit + epsilon_small))`  # Best Fit\n        #       `+ W * (1.0 / (suitable_capacities + epsilon_small))`      # Prefer \"almost full\" bins\n\n        # Let's use a weight `W=0.5` for the \"almost full\" preference.\n        weight_almost_full = 0.5\n        combined_priority = best_fit_scores + weight_almost_full * (1.0 / (suitable_capacities + epsilon_small))\n        \n        priorities[non_exact_suitable_mask] = combined_priority\n\n    # Normalize priorities for non-exact fits to ensure they don't overshadow exact fits\n    # and to make the best-fit scores relative among themselves.\n    if np.any(non_exact_suitable_mask):\n        non_exact_priorities = priorities[non_exact_suitable_mask]\n        sum_non_exact_priorities = np.sum(non_exact_priorities)\n        if sum_non_exact_priorities > 0:\n            priorities[non_exact_suitable_mask] = non_exact_priorities / sum_non_exact_priorities\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 230.62385799360038,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response4.txt_stdout.txt",
    "code_path": "problem_iter5_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines exact fit preference with a score inversely proportional to\n    remaining capacity for non-exact fits, prioritizing bins that leave\n    minimal waste after placement.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    # Assign very high priority to bins that provide an exact fit\n    exact_fit_mask = np.isclose(bins_remain_cap, item)\n    priorities[exact_fit_mask] = 1e9  # High score for exact fits\n    \n    # For bins that are not an exact fit but can still accommodate the item\n    non_exact_suitable_mask = suitable_bins_mask & ~exact_fit_mask\n    \n    if np.any(non_exact_suitable_mask):\n        # Calculate the remaining capacity *after* placing the item\n        remaining_after_fit = bins_remain_cap[non_exact_suitable_mask] - item\n        \n        # Prioritize bins with less remaining capacity (i.e., tighter fits)\n        # Add a small epsilon to avoid division by zero\n        priorities[non_exact_suitable_mask] = 1.0 / (remaining_after_fit + 1e-9)\n        \n        # Normalize the priorities of non-exact fits to ensure they don't\n        # unfairly dominate due to large inverse values. This makes the\n        # \"best fit\" scores relative among themselves.\n        non_exact_priorities = priorities[non_exact_suitable_mask]\n        if np.sum(non_exact_priorities) > 0:\n            priorities[non_exact_suitable_mask] = non_exact_priorities / np.sum(non_exact_priorities)\n            \n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 101.02330072391149,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response5.txt_stdout.txt",
    "code_path": "problem_iter5_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for excessive remaining capacity and\n    rewards for bins that are already substantially filled.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    # Best Fit Score: Prioritize bins that leave the smallest remaining capacity after fitting the item.\n    # Higher score for smaller (remaining_capacity - item).\n    best_fit_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Fill Ratio: Reward bins where the item occupies a larger portion of the *available* space.\n    # Higher score for larger item / suitable_bins_remain_cap.\n    fill_ratio = item / (suitable_bins_remain_cap + 1e-9)\n\n    # Combined Score: Multiply best_fit_score and fill_ratio. This favors bins that are\n    # both a tight fit and where the item significantly utilizes the remaining capacity.\n    # This implicitly penalizes bins with large absolute remaining capacity.\n    combined_score = best_fit_score * fill_ratio\n\n    # Additional Bonus for \"Almost Full\" bins:\n    # Explicitly reward bins that are already substantially filled (small remaining capacity).\n    # This uses a penalty inversely proportional to the remaining capacity.\n    # A small `suitable_bins_remain_cap` (but still >= item) gets a higher bonus.\n    # We add a small constant to the denominator to avoid division by zero for full bins.\n    # We use `item + epsilon` as a reference to avoid issues if a bin is exactly filled by the item.\n    almost_full_bonus = 1.0 / (suitable_bins_remain_cap + 1e-9)\n\n    # Final Priority: Combine the efficiency score with the almost-full bonus.\n    # The `combined_score` (efficiency) already captures tightness and fill ratio.\n    # The `almost_full_bonus` further boosts bins that are simply close to being full.\n    # A simple sum gives a weighted effect, where `combined_score` is the primary driver\n    # and `almost_full_bonus` acts as a secondary preference for already-full bins.\n    # We can tune the weight of the bonus if needed, but a simple sum is a good starting point.\n    final_priorities = combined_score + 0.5 * almost_full_bonus # Tunable weight for bonus\n\n    priorities[suitable_bin_indices] = final_priorities\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 156.0801066523054,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response6.txt_stdout.txt",
    "code_path": "problem_iter5_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines tightest fit with preference for less empty bins.\n\n    Prioritizes bins that leave minimal remaining capacity after packing (Best Fit),\n    while also favoring bins that are already closer to being full (smaller initial remaining capacity).\n    \"\"\"\n    epsilon_small = 1e-9\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can fit the item\n    fitting_indices = np.where(bins_remain_cap >= item)[0]\n\n    if fitting_indices.size > 0:\n        # Calculate remaining capacity after fitting the item\n        remaining_after_fit = bins_remain_cap[fitting_indices] - item\n\n        # Strategy:\n        # 1. Prioritize bins that are already \"almost full\" (low initial `bins_remain_cap`).\n        #    This is captured by `1.0 / (bins_remain_cap[fitting_indices] + epsilon_small)`.\n        #    Bins with smaller remaining capacity get a higher score here.\n        # 2. Among those, pick the one that results in the tightest fit (Best Fit).\n        #    This is captured by `(-remaining_after_fit)`. Smaller remaining space gets a higher score.\n\n        # Combine these two preferences. A weighted sum is a common approach.\n        # Let's use a weight of 1.0 for the \"almost full\" preference and 0.7 for the \"tightest fit\" preference.\n        # This means we slightly favor bins that are already less empty, and then refine with best fit.\n        weight_almost_full = 1.0\n        weight_tight_fit = 0.7\n\n        score_almost_full = weight_almost_full * (1.0 / (bins_remain_cap[fitting_indices] + epsilon_small))\n        score_tight_fit = weight_tight_fit * (-remaining_after_fit)\n\n        combined_priority = score_almost_full + score_tight_fit\n        priorities[fitting_indices] = combined_priority\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.0885520542481055,
    "cyclomatic_complexity": 2.0,
    "halstead": 128.3789500201924,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response7.txt_stdout.txt",
    "code_path": "problem_iter5_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines exact fit prioritization with a scaled preference for tighter fits\n    using an exponential decay based on normalized slack, similar to v0 but\n    with a more robust normalization for non-exact fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n    fitting_bins_caps = bins_remain_cap[can_fit_mask]\n    \n    exact_fit_mask = fitting_bins_caps == item\n    \n    # High priority for exact fits\n    priorities[fitting_bins_indices[exact_fit_mask]] = 1e10\n    \n    non_exact_fitting_indices = fitting_bins_indices[~exact_fit_mask]\n    non_exact_fitting_bins_caps = fitting_bins_caps[~exact_fit_mask]\n    \n    if non_exact_fitting_bins_caps.size > 0:\n        # Calculate slack for non-exact fits\n        slack = non_exact_fitting_bins_caps - item\n        \n        # Normalize slack to a [0, 1] range for exponential scaling\n        min_slack = np.min(slack)\n        max_slack = np.max(slack)\n\n        if max_slack == min_slack:\n            # If all slacks are the same, give them a uniform score (lower than exact fit)\n            normalized_slack = np.zeros_like(slack)\n        else:\n            normalized_slack = (slack - min_slack) / (max_slack - min_slack)\n        \n        # Exponential decay: smaller normalized slack (tighter fit) gets higher score\n        # Use exp(-x) so smaller x (tighter fit) gives larger score\n        scores = np.exp(-normalized_slack)\n        \n        # Scale scores to be less than the exact fit priority, but still prioritize tighter non-exact fits\n        # Find max score among non-exact fits and scale it slightly below exact fit priority\n        max_non_exact_score = np.max(scores)\n        if max_non_exact_score > 0:\n            scaled_scores = (scores / max_non_exact_score) * 1e9 # Scale to be just below 1e10\n        else:\n            scaled_scores = np.zeros_like(scores) # Should not happen if slack > 0\n\n        priorities[non_exact_fitting_indices] = scaled_scores\n        \n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 5.0,
    "halstead": 194.95038758870223,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response8.txt_stdout.txt",
    "code_path": "problem_iter5_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a preference for bins that utilize remaining capacity well.\n    Penalizes bins with excessive remaining capacity after fitting.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    # Metric 1: Tightness of fit (Best Fit heuristic component)\n    # Prioritizes bins where remaining capacity is closest to item size.\n    # Add epsilon to avoid division by zero for exact fits.\n    tightness_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Metric 2: Fill Ratio of remaining capacity\n    # Prioritizes bins where the item occupies a larger portion of the current remaining space.\n    # This helps in packing more densely.\n    fill_ratio_score = item / (suitable_bins_remain_cap + 1e-9)\n\n    # Combine metrics multiplicatively:\n    # We want both a tight fit AND a good fill ratio of the current remaining capacity.\n    # This multiplicative approach naturally penalizes bins with very large remaining capacities\n    # because the fill_ratio_score will be small for them, even if the tightness_score is high.\n    # It also favors bins where the item itself is large relative to the remaining space.\n    combined_score = tightness_score * fill_ratio_score\n\n    # Assign the calculated scores to the appropriate bins\n    priorities[suitable_bin_indices] = combined_score\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 94.01164534875782,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response9.txt_stdout.txt",
    "code_path": "problem_iter5_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes tight fits using a sigmoid, with a penalty for large initial capacities.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[fit_mask]\n\n    # Component 1: Sigmoid for tight fits (inspired by priority_v0)\n    # Prioritize bins where the remaining capacity after placing the item is small.\n    slack = valid_bins_remain_cap - item\n    \n    # Normalize slack to be in a range suitable for sigmoid, focusing on small slack.\n    # We map smaller slack to larger input for sigmoid, thus higher output.\n    min_slack = np.min(slack)\n    max_slack = np.max(slack)\n    \n    sigmoid_scores = np.zeros_like(slack, dtype=float)\n    if max_slack > min_slack:\n        # Transform slack: smaller slack -> larger transformed value\n        transformed_slack = max_slack - slack\n        normalized_transformed_slack = transformed_slack / (max_slack - min_slack)\n        # Apply sigmoid centered around 0.5 for smooth preference\n        steepness = 5.0 \n        sigmoid_input = steepness * (normalized_transformed_slack - 0.5)\n        sigmoid_scores = 1 / (1 + np.exp(-sigmoid_input))\n    elif slack.size > 0: # All slacks are the same\n        sigmoid_scores = np.full_like(slack, 0.5) # Neutral score if all fits are identical\n\n\n    # Component 2: Penalty for large initial capacities (inspired by the idea of penalizing large bins)\n    # Reduce priority for bins that have significantly more capacity than needed initially.\n    # This is a simple inverse capacity scaling. Add epsilon to avoid division by zero.\n    epsilon_large_cap = 1e-6\n    large_capacity_penalty = epsilon_large_cap / (valid_bins_remain_cap + epsilon_large_cap)\n\n\n    # Combine scores: Multiply sigmoid scores by penalty.\n    # A high sigmoid score (tight fit) AND a low penalty (not excessively large bin) is preferred.\n    combined_scores = sigmoid_scores * large_capacity_penalty\n\n    # Normalize combined scores for the fitting bins\n    sum_combined_scores = np.sum(combined_scores)\n    if sum_combined_scores > 0:\n        normalized_combined_scores = combined_scores / sum_combined_scores\n    else: # If all scores are zero (e.g., if penalty made them zero)\n        # Fallback: give equal priority to all fitting bins\n        normalized_combined_scores = np.ones_like(valid_bins_remain_cap) / len(valid_bins_remain_cap)\n\n    priorities[fit_mask] = normalized_combined_scores\n    \n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 5.0,
    "halstead": 282.11056593197316,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response0.txt_stdout.txt",
    "code_path": "problem_iter6_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive \"Best Fit with Dynamic Ratio\" heuristic.\n    This version dynamically adjusts the influence of the \"tightness\" or \"excess\" component\n    based on the item's size relative to the overall capacity distribution. It also\n    introduces a small preference for bins that are not \"too empty\".\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Base score: Primarily aims for Best Fit.\n    # Using log to compress the range and emphasize smaller differences.\n    # Add a small epsilon to avoid log(0).\n    best_fit_score = -np.log(suitable_bins_remain_cap - item + 1e-9)\n\n    # Dynamic weighting factor: Adjust the importance of the \"tightness\" component.\n    # If the item is small relative to typical bin capacities, we might be more\n    # forgiving of slightly larger remaining capacities. If the item is large,\n    # we want tighter fits.\n    # Consider the average remaining capacity of *all* bins (even unsuitable ones)\n    # as a proxy for the general state of the packing.\n    # Alternatively, use the average remaining capacity of *suitable* bins.\n    avg_suitable_remain_cap = np.mean(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 1.0\n    \n    # If item is large relative to the average suitable remaining capacity,\n    # we want to heavily favor tight fits. If item is small, the penalty for\n    # excess capacity is less critical.\n    # Ratio: item_size / avg_suitable_remain_cap\n    # We want to increase the weight of the tightness component when this ratio is high.\n    \n    # Let's define a \"tightness preference\" score.\n    # We want to reward bins where `remaining_cap - item` is small relative to `item`.\n    # `excess_ratio = (remaining_cap - item) / item`\n    # We prefer lower `excess_ratio`.\n    excess_ratio = (suitable_bins_remain_cap - item) / (item + 1e-9)\n    \n    # A score that is high when excess_ratio is low.\n    # Use a sigmoid-like function to bound and smooth the effect.\n    # `tightness_score = 1 / (1 + exp(k * excess_ratio))`\n    # A simpler approach: `tightness_score = 1 / (excess_ratio + C)`\n    # Let's use a scaled inverse of excess_ratio.\n    # The scaling factor `item / avg_suitable_remain_cap` will modify the impact.\n    \n    # We want the `tightness_score` component to be more influential for larger items.\n    # Let's consider `weight_for_tightness = min(1.0, item / avg_suitable_remain_cap)`\n    # This weight is between 0 and 1.\n    weight_for_tightness = np.clip(item / (avg_suitable_remain_cap + 1e-9), 0.1, 2.0) # Clip to avoid extreme values\n\n    # Refined tightness score: higher for bins with less excess capacity relative to item size,\n    # scaled by how important tightness is.\n    # `tightness_factor = 1.0 / (1 + excess_ratio)` gives higher values for tighter fits.\n    # Multiply by `weight_for_tightness` to make it adaptive.\n    # We want to ADD this adaptive tightness score to the best_fit_score (which is already a maximization score).\n    # The original `best_fit_score` rewards tighter fits. We want to ADD a component that\n    # also rewards tighter fits, but with an adaptive weight.\n    \n    # Combine: Score = best_fit_score + adaptive_tightness_bonus\n    # `adaptive_tightness_bonus = weight_for_tightness * (1.0 / (1.0 + excess_ratio))`\n    # This encourages tight fits, with the encouragement amplified for larger items.\n\n    adaptive_tightness_bonus = weight_for_tightness * (1.0 / (1.0 + excess_ratio))\n    \n    # Small preference for not being \"too empty\" for the item.\n    # If remaining capacity is much larger than the item, it's less preferred.\n    # `emptiness_penalty = (suitable_bins_remain_cap - item) / suitable_bins_remain_cap`\n    # We want to penalize high `emptiness_penalty`. So we want to ADD something that\n    # is small when `emptiness_penalty` is large.\n    # `emptiness_score = 1.0 / (1.0 + emptiness_penalty)`\n    # This is essentially `item / suitable_bins_remain_cap`.\n    \n    # Let's use the negative of remaining capacity as a simple penalty for being too empty.\n    # The original best_fit_score already prioritizes minimal remaining capacity.\n    # Adding a component that penalizes large remaining capacity *even more*:\n    # `excess_capacity_penalty = -(suitable_bins_remain_cap - item)`\n    # But we want this penalty to be LESS impactful if `item` is small.\n    # `scaled_excess_capacity_penalty = - (suitable_bins_remain_cap - item) / (item + 1e-9)`\n    # This is negative of the `excess_ratio`.\n    # So, we can add `weight_for_tightness * (excess_ratio)` to the score.\n    # This means we are rewarding smaller excess ratios more when `weight_for_tightness` is high.\n    \n    # Final score: Maximize the log-based best fit score, and add an adaptive bonus\n    # for tightness, where the bonus is stronger for larger items or items that\n    # create tighter fits.\n    \n    # Let's adjust the `best_fit_score` calculation slightly.\n    # Instead of log, let's use a linear inverse of remaining capacity, then scale it.\n    # `bf_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)`\n    # This already favors minimal remaining capacity.\n    \n    # Now, add the adaptive tightness bonus.\n    # We want to favor smaller `excess_ratio = (suitable_bins_remain_cap - item) / item`.\n    # Let's use `1.0 / (excess_ratio + 1.0)` as a tightness indicator.\n    # We scale this by `weight_for_tightness`.\n    \n    tightness_indicator = 1.0 / (excess_ratio + 1.0) # Higher is better, up to 1.0\n    \n    # The combined score will be:\n    # `score = (1.0 / (suitable_bins_remain_cap - item + 1e-9)) * (1.0 + weight_for_tightness * tightness_indicator)`\n    # This combines the best fit (inverse remaining capacity) with an adaptive bonus for tightness.\n    # The `+ 1.0` in the multiplier ensures that the best-fit part is always dominant,\n    # and the tightness bonus modifies it.\n    \n    # Let's simplify the base score to avoid potential issues with very small differences in remaining capacity.\n    # Instead of `log`, let's use a simple inverse.\n    # Consider the \"waste\" `suitable_bins_remain_cap - item`. We want to minimize this.\n    # Base score: `- (suitable_bins_remain_cap - item)` - Higher is better.\n    # This directly rewards the smallest remaining capacity.\n    \n    # Now, apply the adaptive tightness bonus.\n    # Bonus should be higher when `excess_ratio` is lower AND `weight_for_tightness` is higher.\n    # `tightness_bonus = weight_for_tightness * (1.0 / (excess_ratio + 1.0))`\n    \n    # Combine them additively.\n    # `final_score = -(suitable_bins_remain_cap - item) + weight_for_tightness * (1.0 / (excess_ratio + 1.0))`\n    # This is essentially maximizing `(item - suitable_bins_remain_cap) + adaptive_bonus`.\n    \n    # Let's try a multiplicative approach again but with refined components.\n    # Score = (BestFitComponent) * (TightnessComponent)\n    # BestFitComponent: `-np.log(suitable_bins_remain_cap - item + 1e-9)` (already good for BF)\n    # TightnessComponent: Needs to be high when `excess_ratio` is low.\n    # `tightness_factor = 1.0 / (excess_ratio + 1.0)` (0 to 1)\n    # Adaptive scaling for tightness: `adaptive_tightness = 1.0 + weight_for_tightness * (tightness_factor - 0.5)`\n    # The `-0.5` shifts the range of `tightness_factor` so that the bonus is centered around typical values.\n    # `weight_for_tightness` is `item / avg_suitable_remain_cap`.\n    \n    # Let's consider a direct combination:\n    # Maximize `f(item, remaining_cap)`\n    # We want `remaining_cap` to be close to `item`.\n    # And we want `item / remaining_cap` to be high.\n    \n    # Revised Strategy:\n    # 1. Start with a Best Fit metric: `1.0 / (remaining_cap - item + eps)`\n    # 2. Add an adaptive \"closeness\" metric: penalize bins where `remaining_cap` is much larger than `item`.\n    #    The penalty should increase with `(remaining_cap - item) / item`.\n    #    The sensitivity to this penalty should be modulated by `item / avg_suitable_remain_cap`.\n    \n    # Base Best Fit Score: Maximize `-(remaining_cap - item)`\n    base_bf_score = -(suitable_bins_remain_cap - item)\n    \n    # Adaptive Closeness Score:\n    # We want to ADD a score that is higher for smaller `excess_ratio`.\n    # The magnitude of this addition is scaled by `weight_for_tightness`.\n    # `closeness_score = weight_for_tightness * (1.0 / (excess_ratio + 1.0))`\n    # This is the same as `adaptive_tightness_bonus` from before.\n    \n    # Combine additively:\n    # `final_priorities = base_bf_score + closeness_score`\n    # This combines the direct \"minimize waste\" with an adaptive \"maximize tight fit\" bonus.\n    \n    final_priorities = base_bf_score + adaptive_tightness_bonus\n    \n    # Apply the calculated priorities back to the original array.\n    priorities[suitable_bins_mask] = final_priorities\n    \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.11846828879138,
    "cyclomatic_complexity": 3.0,
    "halstead": 276.90491672227165,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response1.txt_stdout.txt",
    "code_path": "problem_iter6_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit and Worst Fit principles with a dynamic \"gap\" penalty.\n    Prioritizes bins that minimize remaining capacity (Best Fit), but also considers\n    bins that have a larger capacity that could accommodate future larger items,\n    but penalizes a large \"gap\" between item size and bin capacity to avoid fragmentation.\n    The \"gap\" penalty is adaptive based on the item's size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Mask for bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # --- Component 1: Best Fit ---\n    # Prioritize bins with minimal remaining capacity after packing.\n    # This is a negative log of the remaining capacity after packing to maximize.\n    # Add epsilon for numerical stability when remaining_cap == item.\n    best_fit_score = -np.log(suitable_bins_remain_cap - item + 1e-9)\n    \n    # --- Component 2: Worst Fit (modified) ---\n    # Consider bins that have more excess capacity, potentially for larger future items.\n    # We want to give a *slight* preference to bins with more \"room\" but not too much.\n    # We can represent this as a score related to the excess capacity, but we want to\n    # avoid extremely large remaining capacities which might lead to fragmentation.\n    # Let's use a score that increases with excess capacity but saturates.\n    # A simple approach is a linear or logarithmic increase in the excess capacity itself.\n    # Let's use the excess capacity directly, scaled.\n    # `excess_capacity = suitable_bins_remain_cap - item`\n    # We want to favor bins where `excess_capacity` is moderate.\n    # A simple score could be `excess_capacity / (item + epsilon)`. High ratio means large excess relative to item.\n    # We want to favor lower excess *relative* to item size.\n    \n    # --- Component 3: Dynamic Gap Penalty ---\n    # Penalize bins where the difference (gap) between bin capacity and item size is large.\n    # This gap represents wasted space. The penalty should be stronger for larger gaps.\n    # The penalty should also be 'dynamic' in the sense that a gap of 5 units might be small\n    # for an item of size 100, but large for an item of size 5.\n    # So, the penalty should be related to `gap / item_size`.\n    \n    gap = suitable_bins_remain_cap - item\n    \n    # Penalty score: we want to penalize large `gap / item`.\n    # So, a good penalty multiplier would be `1 / (gap / item + C)` or `item / (gap + C)`.\n    # This multiplier should be applied to a score we want to maximize.\n    # A higher multiplier means a better bin (smaller relative gap).\n    \n    # Let's combine Best Fit and a modified Worst Fit idea.\n    # We want to maximize `best_fit_score`.\n    # For the \"room\" aspect, we can consider `gap`.\n    # If `gap` is very small, it's good for Best Fit.\n    # If `gap` is moderate, it might be good for accommodating future items.\n    # If `gap` is very large, it's bad (fragmentation).\n    \n    # Let's try to maximize:\n    # 1. `best_fit_score` (higher is better)\n    # 2. A term that favors moderate gaps over very large gaps.\n    #    Consider `gap_score = 1.0 / (gap / item + epsilon)` -- this favors smaller gaps.\n    #    This is similar to the penalty in v1.\n    \n    # Let's refine the interaction:\n    # We want to maximize the \"goodness\" of a fit.\n    # Goodness = (Score from Best Fit) * (Score from Gap Management)\n    \n    # Best Fit score: `-np.log(remaining_after_packing)` as before.\n    # Gap Management Score: We want to penalize large gaps relative to item size.\n    # So, a score that decreases as `gap/item` increases.\n    # Let's use `1 / (1 + (gap / (item + 1e-9)))`.\n    # This score is 1 for a perfect fit (gap=0) and decreases as the relative gap increases.\n    gap_management_score = 1.0 / (1.0 + (gap / (item + 1e-9)))\n    \n    # Final priority: Product of Best Fit and Gap Management Score.\n    # This encourages bins that are \"tight\" (high best_fit_score) while also\n    # not having excessively large relative gaps.\n    priorities[suitable_bins_mask] = best_fit_score * gap_management_score\n    \n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 3.948942959712818,
    "cyclomatic_complexity": 2.0,
    "halstead": 136.16184010614157,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response2.txt_stdout.txt",
    "code_path": "problem_iter6_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a dynamic penalty based on the item size relative to bin capacity,\n    but introduces adaptivity based on the distribution of remaining capacities.\n    It prioritizes bins that offer a good fit, while also considering the overall\n    \"tightness\" of the packing environment. Bins that leave a moderate amount of\n    space might be preferred if they are more numerous, promoting a more balanced packing.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Component 1: Best Fit (prioritize minimal remaining capacity after packing)\n    # We want to minimize `suitable_bins_remain_cap - item`.\n    # To convert to a maximization problem, we use the negative of this difference.\n    # Adding a small epsilon to avoid log(0) or division by zero if remaining_cap == item.\n    best_fit_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9) # Using inverse for maximization\n\n    # Component 2: Adaptable \"Slack Penalty\"\n    # This component penalizes bins that have significantly more capacity than needed.\n    # However, the penalty is modulated by the overall \"slack\" in the system.\n    # If most bins have a lot of slack, a moderate slack is less penalized.\n    # If most bins are nearly full, leaving any significant slack is penalized more.\n    \n    # Calculate the excess capacity relative to the item size for suitable bins.\n    excess_capacity = suitable_bins_remain_cap - item\n    \n    # Calculate a \"global slack factor\". This is the average excess capacity across all *suitable* bins.\n    # If this is high, it means many bins have plenty of room, so individual slack is less of a concern.\n    # If this is low, it means bins are generally tight, so individual slack is more penalized.\n    if len(excess_capacity) > 0:\n        avg_excess_capacity = np.mean(excess_capacity)\n    else:\n        avg_excess_capacity = 0.0\n\n    # Create a penalty factor that is inversely related to the excess capacity.\n    # A low excess_capacity is good (high penalty_factor), a high excess_capacity is bad (low penalty_factor).\n    # We normalize the excess capacity by the average excess capacity to make it adaptive.\n    # If excess_capacity is much smaller than avg_excess_capacity, the ratio is small, penalty_factor is high.\n    # If excess_capacity is much larger than avg_excess_capacity, the ratio is large, penalty_factor is low.\n    # Add 1 to the denominator to avoid division by zero and ensure a base penalty.\n    \n    # Avoid division by zero for avg_excess_capacity if no suitable bins or all have exactly item size.\n    adaptive_slack_penalty_factor = 1.0 / ( (excess_capacity / (avg_excess_capacity + 1e-9)) + 0.5 )\n\n    # Component 3: Bin Uniformity Preference\n    # Prefer bins that are \"closer\" to the average remaining capacity among suitable bins.\n    # This encourages a more uniform distribution of remaining capacities, potentially\n    # leaving larger contiguous spaces in other bins for future large items.\n    # We penalize bins that deviate significantly from the average.\n    \n    # Calculate deviation from the mean remaining capacity of suitable bins.\n    deviation_from_mean = np.abs(suitable_bins_remain_cap - avg_excess_capacity - item) # deviation from item + avg_slack\n    \n    # Higher deviation is worse. We want to reward lower deviation.\n    # Use an inverse relationship to convert minimization of deviation into maximization.\n    # Add a small constant to the denominator to avoid division by zero.\n    uniformity_score = 1.0 / (deviation_from_mean + 1.0)\n\n    # Combine the components.\n    # We want to maximize best_fit_score.\n    # We want to maximize adaptive_slack_penalty_factor (i.e., minimize excess capacity relative to average).\n    # We want to maximize uniformity_score (i.e., minimize deviation from average).\n    # A multiplicative combination can work if components are designed as maximization proxies.\n    # The weights for each component can be tuned. Here, let's use equal weighting initially.\n    \n    # Let's try a combined score that is a weighted sum of log-transformed components to dampen large values and a product for synergy.\n    # Using log-transforms to bring scales closer.\n    \n    # Log-transform for better distribution and to handle orders of magnitude. Add epsilon for log(0).\n    log_best_fit = np.log(best_fit_score + 1e-9)\n    log_adaptive_slack = np.log(adaptive_slack_penalty_factor + 1e-9)\n    log_uniformity = np.log(uniformity_score + 1e-9)\n\n    # A combined heuristic that blends the direct \"best fit\" with the \"adaptive slack\" and \"uniformity\" preferences.\n    # The idea is to favor good fits, but also adapt to the overall state of bins and encourage balance.\n    # For example, if a bin offers a slightly worse fit but has a much better adaptive slack score and uniformity, it might be preferred.\n    \n    # Simple weighted sum might not capture complex interactions.\n    # Let's try a multiplicative combination of the positive components and a penalty for negative aspects.\n    # We want high `best_fit_score`, high `adaptive_slack_penalty_factor`, and high `uniformity_score`.\n    # So, a product seems appropriate.\n    \n    # Adding a small epsilon to the final result to ensure positive scores if any component is zero (though unlikely with current formulation).\n    final_priorities = log_best_fit + log_adaptive_slack + log_uniformity\n    \n    # Ensure we only update priorities for suitable bins.\n    priorities[suitable_bins_mask] = final_priorities\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 17.401276426007183,
    "cyclomatic_complexity": 3.0,
    "halstead": 295.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response3.txt_stdout.txt",
    "code_path": "problem_iter6_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a dynamically weighted \"slack\" minimization and a \"first-fit-like\" fallback.\n    This heuristic prioritizes bins that offer a close fit (minimize slack), but also\n    considers the number of available bins to avoid premature opening of new bins,\n    and uses a more robust way to handle near-perfect fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Mask for bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities  # No bin can fit the item\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Component 1: Best Fit - Prioritize bins with minimum remaining capacity after packing.\n    # This is the core of minimizing wasted space in the chosen bin.\n    # We want to maximize -(remaining_capacity - item), which is equivalent to minimizing (remaining_capacity - item).\n    # Using log to compress the range and make smaller differences more impactful.\n    # Add a small epsilon to prevent log(0) or division by zero if remaining_cap == item.\n    best_fit_score = -np.log(suitable_bins_remain_cap - item + 1e-9)\n    \n    # Component 2: Slack Minimization (refined)\n    # Penalize bins where the remaining capacity is significantly larger than the item size.\n    # This is about finding a \"good enough\" fit, not necessarily the absolute tightest if it leaves too much void.\n    # Let's consider the ratio of leftover space to the item size.\n    # `slack_ratio = (remaining_capacity - item) / item`\n    # We want to penalize high slack ratios. A score of `1 / (slack_ratio + C)` would work.\n    # A more nuanced approach: Consider the \"fill ratio\" of the *potential* packed bin.\n    # `fill_ratio = item / (bin_capacity_before_packing)` is not directly available here.\n    # Instead, let's focus on `item / (item + slack)` where slack is `remaining_cap - item`.\n    # This is `item / remaining_cap`. We want to maximize this, as it means the item fills a larger\n    # portion of the bin's *current* remaining capacity.\n    \n    # Calculate the potential fill ratio if the item is placed in the bin.\n    # This captures how \"full\" the bin would become with this item.\n    potential_fill_ratio = item / (suitable_bins_remain_cap + 1e-9)\n    \n    # We want to maximize this ratio. High fill ratio is good.\n    fill_score = potential_fill_ratio\n    \n    # Component 3: Dynamic Weighting based on number of available bins (adaptability)\n    # If there are many bins available, we can be pickier about the fit.\n    # If there are few bins, we might prioritize filling any available bin more easily.\n    # Let's introduce a factor that slightly favors more \"efficient\" bins,\n    # but also consider that \"almost perfect\" fits are good.\n    \n    # A simple weighting scheme: Higher weight for bins that leave less slack relative to the item size.\n    # `slack_penalty_factor = 1.0 / (1.0 + (suitable_bins_remain_cap - item) / (item + 1e-9))`\n    # This factor is close to 1 for tight fits and decreases as slack increases.\n    slack_penalty_factor = 1.0 / (1.0 + ((suitable_bins_remain_cap - item) / (item + 1e-9))**0.5)\n    \n    # Combine scores:\n    # We want to maximize `best_fit_score` (minimize slack) and `fill_score` (maximize item usage).\n    # The `slack_penalty_factor` down-weights bins with excessive slack.\n    # A multiplicative combination seems appropriate:\n    # Score = (best_fit_score_proxy) * (fill_score_proxy) * (slack_penalty_factor)\n    # Let's use `best_fit_score` directly as it already encodes minimizing slack.\n    # The `fill_score` complements `best_fit_score` by ensuring the item itself is substantial relative to the bin's remaining capacity.\n    \n    # We want to maximize: `best_fit_score` (log of inverse slack) AND `fill_score` (item / remaining_cap).\n    # A potential issue with simple multiplication is that if one component is very small, it can dominate.\n    # Let's consider a convex combination or a sum after scaling.\n    # A common approach is to use a weighted sum: `w1 * bf_score + w2 * fill_score + w3 * sf_factor`.\n    # However, components are on different scales.\n\n    # Let's refine the interaction:\n    # `best_fit_score` prioritizes minimal `remaining_cap - item`.\n    # `fill_score` prioritizes `item / remaining_cap`.\n    # These two are often aligned: when `remaining_cap - item` is small, `item / remaining_cap` is large.\n    # The `slack_penalty_factor` ensures we don't excessively penalize bins that might be slightly less \"best fit\"\n    # if they offer a better overall fill ratio for the item.\n\n    # Let's try a combined score that emphasizes tight fits but also the quality of the item's fill.\n    # Score = alpha * best_fit_score + beta * fill_score\n    # We can also incorporate the idea of not overly penalizing slight deviations from perfect fit.\n    # Consider `(suitable_bins_remain_cap - item)` as slack. We want small slack.\n    # `best_fit_score` is `-log(slack + epsilon)`.\n    # `fill_score` is `item / (item + slack + epsilon)`.\n\n    # Let's try to combine them additively after normalization or scaling.\n    # A simpler multiplicative approach that captures both:\n    # Prioritize bins where `item` is a significant fraction of `suitable_bins_remain_cap`\n    # AND `suitable_bins_remain_cap` is not excessively large.\n    \n    # The \"tightness\" metric: `item / suitable_bins_remain_cap`. Higher is better.\n    # The \"slack\" metric: `suitable_bins_remain_cap - item`. Lower is better.\n    # Let's map slack to a score: `1 / (slack + 1)` or `-log(slack + epsilon)`.\n    # `best_fit_score` is already `-log(slack + epsilon)`.\n    \n    # Let's boost the score for bins that are closer to \"perfect fit\" without being too sensitive.\n    # The `slack_penalty_factor` already does this by down-weighting large slacks.\n    \n    # Combine `best_fit_score` and `fill_score`.\n    # `best_fit_score` rewards small `slack`.\n    # `fill_score` rewards large `item` relative to `suitable_bins_remain_cap`.\n    # If `slack` is small, `item` is close to `suitable_bins_remain_cap`, so `fill_score` will be near 1.\n    # If `slack` is large, `best_fit_score` will be low. `fill_score` will also be low.\n    \n    # Let's try a composite score that is robust to very small items.\n    # If item is very small, `fill_score` might be very small, and `best_fit_score` might be very large (due to log).\n    # We want to favor bins that are not excessively large *even if* the item is small.\n    \n    # Final approach: Combine Best Fit with a \"Fair Fit\" metric.\n    # Best Fit: Minimize `remaining_capacity - item`. Score: `-log(slack + epsilon)`.\n    # Fair Fit: Maximize `item / (item + slack)`. This is `item / suitable_bins_remain_cap`.\n    # A potential issue is if `suitable_bins_remain_cap` is very large, even if `slack` is small.\n    # We want to prioritize bins that are not excessively larger than `item` + `slack`.\n    \n    # Let's adjust the `best_fit_score` to also consider the absolute remaining capacity.\n    # Penalize bins with very large remaining capacity, even if the slack is small.\n    # `capacity_penalty = -log(suitable_bins_remain_cap + 1e-9)`\n    # Combining `best_fit_score` and `capacity_penalty`:\n    # `combined_fit = best_fit_score + capacity_penalty`\n    # This would prioritize bins where `slack` is small AND `suitable_bins_remain_cap` is small.\n    \n    # Let's refine again: Focus on two key aspects:\n    # 1. How well does the item fit into the remaining capacity (tightness)?\n    # 2. How much capacity is left *after* fitting (waste)?\n    \n    # Metric 1: Tightness. Maximize `item / suitable_bins_remain_cap`.\n    tightness_score = item / (suitable_bins_remain_cap + 1e-9)\n    \n    # Metric 2: Waste. Minimize `suitable_bins_remain_cap - item`.\n    # We can use `-log(slack + epsilon)` as before.\n    waste_score = -np.log(suitable_bins_remain_cap - item + 1e-9)\n    \n    # Combine these: A simple addition can work if they are on comparable scales or normalized.\n    # `tightness_score` is between 0 and 1. `waste_score` can be large negative for large slack.\n    # Let's use a weighted sum where both components are maximized.\n    # Higher `tightness_score` is good. Higher `waste_score` (less negative) is good.\n    \n    # Let's re-introduce the `slack_penalty_factor` to modulate the contribution of `tightness_score`.\n    # We want `tightness_score` to be high, but not at the expense of extreme waste.\n    # The `slack_penalty_factor` provides this: `1.0 / (1.0 + (slack / item)**0.5)`.\n    # If slack is large relative to item, this factor becomes small.\n    \n    # A combined score could be:\n    # `final_score = tightness_score * (1 + waste_score/some_scale) * slack_penalty_factor`\n    # Or simpler: maximize the product of factors that are good.\n    \n    # Prioritize bins where `item` is a significant portion of the remaining capacity,\n    # and the absolute slack is not excessively large.\n    \n    # Let's consider the combination:\n    # - Prioritize small slack: `waste_score = -log(slack + epsilon)`\n    # - Prioritize good fill: `tightness_score = item / (item + slack)`\n    # - Avoid very large bins: Perhaps a penalty based on `suitable_bins_remain_cap` itself.\n    \n    # Let's try a heuristic that balances these:\n    # `score = (item / suitable_bins_remain_cap) * exp(- (suitable_bins_remain_cap - item) / (item + epsilon) )`\n    # This encourages high `item / suitable_bins_remain_cap` and penalizes large slack relative to item size.\n    \n    slack = suitable_bins_remain_cap - item\n    \n    # Score 1: How much of the remaining capacity is used by the item (tightness)\n    tightness_ratio = item / (suitable_bins_remain_cap + 1e-9)\n    \n    # Score 2: How much waste is left *relative to the item size* (normalized slack)\n    # We want to penalize large relative slack. Exponential decay is good for this.\n    # Higher `slack_penalty_factor` means less penalty. So, we want to minimize `slack / item`.\n    # Let's use `exp(-(slack / item))` as a factor that decreases with slack.\n    # Add a small value to item to prevent division by zero for zero-sized items (though unlikely in BPP).\n    relative_slack = slack / (item + 1e-9)\n    # We want to penalize large relative_slack. So, `exp(-relative_slack)` is a good candidate.\n    # A factor that is close to 1 for small relative_slack and decays to 0 for large relative_slack.\n    slack_decay_factor = np.exp(-relative_slack)\n    \n    # Combine: Maximize `tightness_ratio` and `slack_decay_factor`.\n    # A multiplicative approach ensures both are considered.\n    # `final_score = tightness_ratio * slack_decay_factor`\n    \n    # This encourages bins where the item fills a good portion of the remaining space,\n    # AND the leftover space is not excessively large compared to the item.\n    \n    priorities[suitable_bins_mask] = tightness_ratio * slack_decay_factor\n    \n    # Edge case: If multiple bins have identical scores, we might want a tie-breaker.\n    # A simple tie-breaker could be First Fit (preferring lower index bins) or Best Fit (preferring bins with less absolute slack).\n    # For now, numpy's `argmax` will pick the first occurrence in case of ties.\n    \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.487435181491823,
    "cyclomatic_complexity": 2.0,
    "halstead": 355.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response4.txt_stdout.txt",
    "code_path": "problem_iter6_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a \"Most Full Bin\" preference with a \"Worst Fit\" penalty that adapts based on\n    the item's relative size to the bin's remaining capacity. This heuristic aims to\n    keep fuller bins available for potentially smaller items later, while penalizing\n    bins that would leave excessively large gaps relative to the item being packed.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    # Component 1: \"Most Full Bin\" preference (inverse of remaining capacity)\n    # We want to prioritize bins with *less* remaining capacity if they can fit the item,\n    # effectively favoring bins that are already quite full.\n    # A higher score here means a higher priority.\n    most_full_score = -suitable_bins_remain_cap\n\n    # Component 2: \"Worst Fit\" penalty based on relative excess capacity.\n    # We want to penalize bins where (remaining_capacity - item) is large relative to the item size.\n    # This penalty should be *subtracted* from the score.\n    # The penalty is higher for bins that leave a much larger gap *in proportion* to the item.\n    # Use a logarithmic scale to dampen the effect of very large excesses, making it more sensitive\n    # to moderate excesses.\n    # `excess_capacity_ratio`: (remaining_cap - item) / item\n    # We want to penalize bins with a high `excess_capacity_ratio`.\n    # So, we want to subtract a term that increases with this ratio.\n    # A simple form is `log(1 + excess_capacity_ratio)`.\n    \n    excess_capacity_ratio = (suitable_bins_remain_cap - item) / (item + 1e-9)\n    \n    # The penalty function should be monotonic increasing with excess_capacity_ratio.\n    # Using log1p for numerical stability and to moderate the penalty for very small excesses.\n    # We will subtract this penalty from the most_full_score.\n    # A larger penalty_component means a lower final score.\n    penalty_component = np.log1p(excess_capacity_ratio * 0.5) # Scale down the ratio to make penalty less aggressive.\n\n    # Combine scores: Maximize `most_full_score` and minimize `penalty_component`.\n    # This is achieved by `most_full_score - penalty_component`.\n    final_scores = most_full_score - penalty_component\n\n    # Assign the computed scores back to the original priority array\n    priorities[suitable_bin_indices] = final_scores\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 91.73835003173087,
    "exec_success": true
  }
]