```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Aims to improve upon v1 by introducing a more dynamic penalty that
    considers the item's size relative to the bin's capacity and a
    more nuanced "best fit" calculation. It also prioritizes filling bins
    more completely when they are already significantly filled, promoting
    the use of fewer bins.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    suitable_bins_mask = bins_remain_cap >= item

    if not np.any(suitable_bins_mask):
        return priorities

    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]
    suitable_bin_indices = np.where(suitable_bins_mask)[0]

    # More robust "best fit" score: prioritizing bins that leave the smallest remaining capacity.
    # We invert the difference `remaining_capacity - item`. Adding a small epsilon
    # to the denominator avoids division by zero and ensures bins that are an exact fit
    # get a high score.
    best_fit_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)

    # Dynamic penalty for "slack" or excess capacity.
    # The penalty is higher for bins that have a lot of remaining capacity
    # *relative to the item size*. This discourages placing small items
    # into bins that have a very large remaining capacity, saving those
    # large capacities for larger items.
    # We define "slack" as the ratio of remaining capacity to bin capacity,
    # but we only consider the capacity *beyond* what's needed for the item.
    # A smaller ratio of "excess capacity" (remaining capacity - item) to the
    # bin's original capacity is preferred.
    # Using a sigmoid-like function centered around 0.5 (meaning half-full is neutral)
    # penalizes bins that are very empty (slack close to 1) and also bins that are
    # almost full but still have a little room (slack close to 0, but we want to
    # prioritize those that are *almost* full to finish a bin).

    # Calculate the "slack" ratio: (remaining_capacity - item) / original_capacity
    # We use the original capacity before fitting the item as a reference.
    # A very small remaining capacity (after fitting) implies a good fit.
    # We want to penalize large *unused* capacity.
    # Let's consider `(bin_capacity - item)` as the "excess" after fitting.
    # The penalty should be higher if `excess_capacity` is large.

    # We can also introduce a factor that rewards filling bins that are already
    # substantially full. If a bin is already more than 50% full, we might
    # give it a slight boost to encourage completing it.

    # Let's try a penalty based on the *normalized remaining capacity* after fitting.
    # Higher remaining capacity after fitting should be penalized more.
    # Normalize the remaining capacity (`suitable_bins_remain_cap - item`)
    # by the maximum possible remaining capacity among suitable bins.
    max_possible_gap = np.max(suitable_bins_remain_cap - item)
    if max_possible_gap == 0:
        normalized_gap = np.zeros_like(suitable_bins_remain_cap)
    else:
        normalized_gap = (suitable_bins_remain_cap - item) / max_possible_gap

    # Penalty: Higher penalty for larger normalized gap.
    # Using 1 - normalized_gap means a small gap gets a high score, and a large gap gets a low score.
    # We want to penalize large gaps, so a lower score for large gaps is a penalty.
    # Let's invert this: higher penalty value for larger gaps.
    # So, penalty = normalized_gap (higher is worse).
    # We want to combine this with best_fit_score. A high best_fit_score is good.
    # A high penalty value is bad. So we should subtract the penalty or divide by it.

    # Let's re-evaluate the penalty. We want to penalize bins with *too much* excess capacity.
    # This means bins where `suitable_bins_remain_cap - item` is large.
    # We can use a function that grows with `suitable_bins_remain_cap - item`.
    # Let's consider the *proportion of capacity remaining* after placing the item.
    # `(suitable_bins_remain_cap - item) / suitable_bins_remain_cap`
    # However, if `suitable_bins_remain_cap` is just slightly larger than `item`,
    # this proportion can be very small.

    # New approach for penalty:
    # We want to penalize bins where `suitable_bins_remain_cap` is much larger than `item`.
    # Let's focus on `suitable_bins_remain_cap`.
    # Higher `suitable_bins_remain_cap` should have a penalty.
    # Let's use a transformation of `suitable_bins_remain_cap` that increases as it grows.
    # A simple inverse might penalize *all* bins with remaining capacity.

    # Instead of a penalty, let's directly reward filling bins.
    # If a bin is already quite full, placing an item that fits well is very good.
    # Let's consider the "fullness" of the bin *before* placing the item.
    # A bin that is already > 50% full should get a slight bonus for being filled further.

    # Calculate a "fill ratio" for each suitable bin relative to its *current* remaining capacity.
    # This isn't quite right. We need to consider the *original* capacity if available.
    # Without original capacity, we can only work with `bins_remain_cap`.

    # Let's refine the penalty based on the *slack*: `suitable_bins_remain_cap - item`.
    # We want to penalize large slack.
    # Let's use a logistic function to map the slack to a penalty factor.
    # We want a penalty that increases as slack increases.
    # Let `slack = suitable_bins_remain_cap - item`.
    # A reasonable maximum slack could be considered the largest remaining capacity among suitable bins.
    max_slack = np.max(suitable_bins_remain_cap - item)
    if max_slack < 1e-9: # All suitable bins are exact fits
        slack_penalty_factor = np.zeros_like(suitable_bins_remain_cap)
    else:
        normalized_slack = (suitable_bins_remain_cap - item) / max_slack
        # Penalty increases with normalized slack.
        # Use a function that is 0 for slack=0 and increases.
        # A simple linear relationship or a slightly more aggressive function.
        # Let's use a function that gives higher penalty for slack > average slack.
        # Option 1: `normalized_slack` itself (linear penalty)
        # Option 2: `np.exp(normalized_slack)` (exponential penalty)
        # Option 3: A piecewise function or a sigmoid that penalizes > threshold.

        # Let's try a combination:
        # Reward tight fits (high `best_fit_score`).
        # Penalize large residual capacity (`suitable_bins_remain_cap - item`).
        # A bin that is almost full (small `suitable_bins_remain_cap`) but can fit the item,
        # should be highly prioritized if the item fits tightly.

        # Consider the ratio `item / suitable_bins_remain_cap`.
        # Higher ratio is good (item takes up a larger portion of remaining space).
        fill_ratio = item / (suitable_bins_remain_cap + 1e-9)

        # Now, let's combine `best_fit_score` and `fill_ratio`.
        # Both should be maximized.
        # `best_fit_score` is `1 / (remaining - item)`.
        # `fill_ratio` is `item / remaining`.

        # If `remaining - item` is small, `best_fit_score` is large.
        # If `item / remaining` is large, `fill_ratio` is large.

        # Let's try a weighted sum, where `best_fit_score` has a higher weight,
        # but `fill_ratio` acts as a secondary criterion to break ties or to
        # favor bins that will be more "full" after packing.

        # Priority = `best_fit_score` + `w * fill_ratio`
        # However, the scales can be very different.

        # Alternative: Multiply.
        # Priority = `best_fit_score` * `fill_ratio`
        # This means we want both a tight fit AND a good fill ratio.
        # If `fill_ratio` is very small (item is tiny compared to remaining space),
        # the score will be low, even if it's a tight fit. This is good.

        # Let's consider the "efficiency" of the bin after packing.
        # Efficiency = `item / original_bin_capacity`.
        # We don't have original bin capacity.
        # Let's use `item / (item + slack)` which is `item / suitable_bins_remain_cap`.

        # Let's make the `best_fit_score` more robust to very small capacities.
        # Instead of `1 / (remaining - item)`, consider `(remaining - item) / remaining`.
        # This is the proportion of remaining capacity that is *not* used.
        # We want to minimize this proportion.
        # So, `1 - (remaining - item) / remaining` = `item / remaining`. This is `fill_ratio`.

        # So, `fill_ratio = item / suitable_bins_remain_cap` is a good metric for how much
        # of the available space the item occupies.

        # Let's reconsider `v1`'s goal: "tight fit" and "penalty for excess capacity".
        # `best_fit_score` addresses tight fit.
        # The penalty part in `v1` was `1.0 / (normalized_excess + 0.1)`.
        # `normalized_excess` was `(suitable_bins_remain_cap - item) / (max_suitable_cap - item)`.
        # This means `penalty` was high for small excess and low for large excess. This is inverted.
        # The combination was `inverse_remaining * penalty`. `inverse_remaining` was `1/(remaining-item)`.

        # Let's create a metric that is high for tight fits AND for bins that are "almost full".
        # Consider the value `suitable_bins_remain_cap`.
        # We want to select a bin where `suitable_bins_remain_cap` is small, but still `>= item`.
        # This means `suitable_bins_remain_cap` should be as close to `item` as possible.

        # Let's try a score that is `1.0 / suitable_bins_remain_cap`. This favors smaller remaining capacities.
        # And combine it with a measure of how "tight" the fit is.
        # Tight fit can be measured by `1.0 / (suitable_bins_remain_cap - item + epsilon)`.

        # Let's try a score that directly prioritizes bins with low remaining capacity *after* fitting the item.
        # `priority = 1.0 / (suitable_bins_remain_cap - item + epsilon)` (This is `best_fit_score`).
        # This already prioritizes tight fits.

        # How to penalize excess capacity more effectively?
        # We want to reduce the priority of bins where `suitable_bins_remain_cap` is large,
        # even if they are suitable.

        # Let's use a function that decreases as `suitable_bins_remain_cap` increases.
        # Example: `exp(-k * suitable_bins_remain_cap)` where `k` is a tuning parameter.
        # Or, normalize `suitable_bins_remain_cap` by the maximum *possible* remaining capacity
        # (which is bin_capacity - smallest_item). Without bin_capacity, we can normalize
        # by the maximum `suitable_bins_remain_cap` found.

        # Let's consider the "waste" `suitable_bins_remain_cap - item`.
        # We want to penalize large waste.
        # Let `waste = suitable_bins_remain_cap - item`.
        # Penalty function `P(waste)`. We want `P(waste)` to increase with `waste`.
        # `P(waste) = waste^2` or `exp(waste)`.

        # Let's combine `best_fit_score` with a penalty inversely related to `suitable_bins_remain_cap`.
        # Combined score: `best_fit_score * (1.0 / (suitable_bins_remain_cap + epsilon))`
        # This might over-penalize bins with moderate remaining capacity.

        # Let's use the `fill_ratio = item / suitable_bins_remain_cap` as a base metric.
        # Higher fill ratio is better.
        # Now, let's refine this with the tightness of the fit.
        # If the fit is very tight (`suitable_bins_remain_cap - item` is small), this is very good.
        # If the fit is loose (`suitable_bins_remain_cap - item` is large), this is less good.

        # Consider a score that is:
        # `fill_ratio`  -> How much of the available space is utilized.
        # `tightness`   -> How close `remaining` is to `item`.
        # Let `tightness = 1.0 / (suitable_bins_remain_cap - item + epsilon)`

        # Combined: `fill_ratio * tightness = (item / suitable_bins_remain_cap) * (1.0 / (suitable_bins_remain_cap - item + epsilon))`
        # This looks promising. It prioritizes bins where the item is a large fraction of the remaining space
        # AND the remaining space after fitting is small.

        # Let's analyze this product:
        # `item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + epsilon)`
        # If `suitable_bins_remain_cap` is slightly larger than `item` (tight fit, high `fill_ratio`),
        # the denominator term `suitable_bins_remain_cap - item` is small, leading to a large score.
        # If `suitable_bins_remain_cap` is much larger than `item`, the `fill_ratio` becomes small,
        # reducing the overall score, even if `suitable_bins_remain_cap - item` is small relative to `suitable_bins_remain_cap`.

        # This seems to capture both aspects well.
        # Let's call this the "efficiency score".

        efficiency_score = item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)

        # This score can become very large if `suitable_bins_remain_cap - item` is tiny.
        # Normalizing might be good, but it might also dampen the strong preference for tight fits.
        # Let's consider scaling or bounding the score if necessary, but start with this direct approach.

        # To make it more robust and less sensitive to extreme values, we can add a small constant
        # to the denominator to avoid infinities if `suitable_bins_remain_cap` is zero (though
        # `suitable_bins_remain_cap >= item` should prevent this if `item > 0`).
        # The `1e-9` is already there.

        # Consider the edge case where `suitable_bins_remain_cap == item`.
        # Then `suitable_bins_remain_cap - item` is 0, leading to infinity.
        # This means exact fits get infinitely high priority. This is generally good.

        # However, `v1` tried to penalize bins with *too much* excess capacity.
        # If we have two bins:
        # Bin A: remaining_cap = 10, item = 9. Score ~ 9 / (10 * 1) = 0.9
        # Bin B: remaining_cap = 2, item = 1. Score ~ 1 / (2 * 1) = 0.5

        # What if we have:
        # Bin C: remaining_cap = 100, item = 99. Score ~ 99 / (100 * 1) = 0.99
        # Bin D: remaining_cap = 5, item = 4. Score ~ 4 / (5 * 1) = 0.8

        # This seems to favor the "absolute" tightest fits, not necessarily the most efficient use of space when capacities differ widely.
        # If Bin C (100 capacity, 99 remaining) and Bin D (5 capacity, 4 remaining) are available for item=4:
        # Bin C: remaining_cap = 100, item = 4. Score ~ 4 / (100 * 96) = 4 / 9600 ~ 0.0004
        # Bin D: remaining_cap = 5, item = 4. Score ~ 4 / (5 * 1) = 0.8
        # This is good: Bin D is prioritized.

        # If item = 99:
        # Bin C: remaining_cap = 100, item = 99. Score ~ 99 / (100 * 1) = 0.99
        # Bin D: remaining_cap = 5, item = 99. (Not suitable)
        # Bin C is prioritized.

        # What if we have:
        # Bin E: remaining_cap = 10, item = 9. Score ~ 9 / (10 * 1) = 0.9
        # Bin F: remaining_cap = 20, item = 18. Score ~ 18 / (20 * 2) = 18 / 40 = 0.45

        # This suggests the current `efficiency_score` prioritizes the *absolute* tightness of fit.
        # `v1` tried to penalize "too much excess capacity".

        # Let's try to incorporate a penalty for large absolute remaining capacity.
        # `penalty_factor = 1.0 / (suitable_bins_remain_cap + epsilon)`
        # Combining `efficiency_score * penalty_factor`:
        # `[item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + epsilon)] * [1.0 / (suitable_bins_remain_cap + epsilon)]`
        # `= item / (suitable_bins_remain_cap^2 * (suitable_bins_remain_cap - item) + epsilon)`
        # This will strongly penalize larger remaining capacities.

        # Let's call this `refined_score`.
        refined_score = item / (suitable_bins_remain_cap**2 * (suitable_bins_remain_cap - item) + 1e-9)

        # Consider its behavior:
        # Bin E: remaining=10, item=9. Score ~ 9 / (100 * 1) = 0.09
        # Bin F: remaining=20, item=18. Score ~ 18 / (400 * 2) = 18 / 800 = 0.0225

        # This penalizes Bin E less than F, which seems to align with favoring tighter fits.
        # If we want to penalize bins with *large* remaining capacity, this `refined_score` does that.
        # It strongly emphasizes reducing `suitable_bins_remain_cap` in the denominator.

        # Let's rethink the "penalty for bins with too much excess capacity" from v1.
        # The goal was to avoid putting small items in large, nearly empty bins.
        # This means we want to *reduce* the priority of bins where `suitable_bins_remain_cap` is large.

        # Let's use a score that is a balance between best fit and minimizing remaining capacity.
        # Best fit: `1.0 / (suitable_bins_remain_cap - item + epsilon)`
        # Minimize remaining capacity: `1.0 / (suitable_bins_remain_cap + epsilon)`

        # Option 1: Average of scores
        # `(1.0 / (suitable_bins_remain_cap - item + epsilon) + 1.0 / (suitable_bins_remain_cap + epsilon)) / 2.0`
        # This combines the two ideas directly.

        # Option 2: Weighted average, or product.
        # Product: `(1.0 / (suitable_bins_remain_cap - item + epsilon)) * (1.0 / (suitable_bins_remain_cap + epsilon))`
        # `= 1.0 / ((suitable_bins_remain_cap - item + epsilon) * (suitable_bins_remain_cap + epsilon))`
        # This also heavily favors small `suitable_bins_remain_cap`.

        # Let's reconsider the `fill_ratio = item / suitable_bins_remain_cap`.
        # This is good because it directly relates to how "full" the bin will be.
        # And `tightness = 1.0 / (suitable_bins_remain_cap - item + epsilon)`.
        # Product: `(item / (suitable_bins_remain_cap + epsilon)) * (1.0 / (suitable_bins_remain_cap - item + epsilon))`
        # This is the `efficiency_score` we derived earlier.

        # How about a modification to `v1`'s penalty?
        # `v1` penalty was `1.0 / (normalized_excess + 0.1)`.
        # Normalized excess was `(suitable_bins_remain_cap - item) / (max_suitable_cap - item)`.
        # So `v1` penalty was high for small normalized excess, low for large.
        # It multiplied `inverse_remaining` by this penalty.
        # `priorities = (1/(remaining-item)) * (1/(norm_excess + 0.1))`
        # This means it favored small `remaining-item` and small `norm_excess`.

        # Let's try to make the penalty more direct: penalize large `suitable_bins_remain_cap`.
        # `score = best_fit_score / (suitable_bins_remain_cap + epsilon)`
        # `score = (1.0 / (suitable_bins_remain_cap - item + epsilon)) / (suitable_bins_remain_cap + epsilon)`
        # `= 1.0 / ((suitable_bins_remain_cap - item + epsilon) * (suitable_bins_remain_cap + epsilon))`
        # This is the product we saw before.

        # Let's try a different combination strategy.
        # Prioritize bins that are a tight fit (`best_fit_score`).
        # Then, among tight fits, prefer those that are "more full" relative to their capacity.
        # `fill_ratio = item / suitable_bins_remain_cap`

        # How to combine `best_fit_score` and `fill_ratio`?
        # `score = best_fit_score + weight * fill_ratio` ?
        # This can be problematic due to scale.

        # Let's go back to `efficiency_score = item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`
        # This score is high when `item` is large relative to `suitable_bins_remain_cap` AND `suitable_bins_remain_cap` is close to `item`.

        # Consider the objective: minimize number of bins.
        # This means we want to fill bins as much as possible, and prefer tighter fits to leave
        # more room in other bins for future items.

        # Let's add a term that explicitly favors filling bins that are already substantialy full.
        # A bin with `suitable_bins_remain_cap < BIN_CAPACITY / 2` (assuming BIN_CAPACITY is known, which it is not here).
        # Without BIN_CAPACITY, we can use `suitable_bins_remain_cap` relative to `item`.

        # Let's define a "completion bonus" for bins that are nearly full after placing the item.
        # If `suitable_bins_remain_cap - item` is very small, it's a good fill.
        # Let's add a score proportional to `1.0 / (suitable_bins_remain_cap + epsilon)`.
        # This rewards smaller remaining capacities.

        # Combined Score = `efficiency_score` + `bonus_weight * (1.0 / (suitable_bins_remain_cap + epsilon))`
        # `efficiency_score` already incorporates `1.0 / (suitable_bins_remain_cap - item + epsilon)`.

        # Let's try a different penalty approach for excess capacity.
        # We want to reduce the score if `suitable_bins_remain_cap` is large compared to `item`.
        # Let `excess_ratio = suitable_bins_remain_cap / item`.
        # We want to penalize large `excess_ratio`.
        # Penalty: `1.0 / (excess_ratio + epsilon)` or `exp(-k * excess_ratio)`.
        # Let's use `1.0 / (suitable_bins_remain_cap / item + epsilon)` which is `item / (suitable_bins_remain_cap + epsilon)`.
        # This is the `fill_ratio` we defined earlier.

        # So, we want to maximize `best_fit_score` and `fill_ratio`.
        # `best_fit_score = 1.0 / (suitable_bins_remain_cap - item + epsilon)`
        # `fill_ratio = item / (suitable_bins_remain_cap + epsilon)`

        # Let's consider the product: `best_fit_score * fill_ratio`.
        # This is the `efficiency_score`.

        # Let's consider another angle: penalize the *total* remaining capacity.
        # We have `best_fit_score` that favors small `remaining - item`.
        # We want to additionally favor smaller `remaining`.

        # Let's try using the `best_fit_score` and applying a penalty to it.
        # Penalty for large remaining capacity: `suitable_bins_remain_cap`.
        # Penalty factor should decrease as `suitable_bins_remain_cap` increases.
        # e.g., `exp(-k * suitable_bins_remain_cap)`.
        # `score = best_fit_score * exp(-k * suitable_bins_remain_cap)`
        # `score = (1.0 / (suitable_bins_remain_cap - item + epsilon)) * exp(-k * suitable_bins_remain_cap)`

        # Let's choose `k` such that it scales appropriately.
        # If `suitable_bins_remain_cap` is, say, 10 times `item`, we might want to penalize it.
        # Let's pick `k` so that if `suitable_bins_remain_cap` is twice the average `suitable_bins_remain_cap`,
        # the exponential term is significantly reduced.

        # Instead of exponential, let's use a simpler penalty that's easier to tune.
        # Penalty factor: `1.0 / (1.0 + alpha * (suitable_bins_remain_cap / item))` where alpha is a tuning parameter.
        # This penalizes bins where remaining capacity is much larger than item.
        # Penalty factor = `item / (item + alpha * suitable_bins_remain_cap)`

        # Let's combine: `best_fit_score * (item / (item + alpha * suitable_bins_remain_cap))`
        # `score = (1.0 / (suitable_bins_remain_cap - item + epsilon)) * (item / (item + alpha * suitable_bins_remain_cap))`

        # Example: item=10
        # Bin A: rem=12. best_fit=1/(12-10)=0.5. penalty=(10/(10+a*12)). Score=0.5 * 10/(10+12a) = 5/(10+12a)
        # Bin B: rem=20. best_fit=1/(20-10)=0.1. penalty=(10/(10+a*20)). Score=0.1 * 10/(10+20a) = 1/(10+20a)
        # Bin C: rem=15. best_fit=1/(15-10)=0.2. penalty=(10/(10+a*15)). Score=0.2 * 10/(10+15a) = 2/(10+15a)

        # If a=0 (no penalty):
        # A: 0.5
        # B: 0.1
        # C: 0.2
        # Bin A (tightest fit) is best.

        # If a=1:
        # A: 5 / (10+12) = 5/22 ~ 0.227
        # B: 1 / (10+20) = 1/30 ~ 0.033
        # C: 2 / (10+15) = 2/25 = 0.08
        # Bin A is still best. This penalty isn't strongly altering the order unless 'a' is large.

        # The core idea of `v1` was "Prioritizes bins that are a tight fit, penalizing those with large gaps."
        # The `v1` penalty was `1.0 / (normalized_excess + 0.1)`. This means higher penalty for *smaller* normalized excess.
        # This is confusing. Let's assume the intent was to penalize larger gaps.

        # Let's try a simple heuristic inspired by First Fit Decreasing or Best Fit Decreasing principles,
        # adapted for online. We want to pack items efficiently.

        # Strategy:
        # 1. Prioritize the "tightest" fit (like Best Fit).
        # 2. Among tight fits, prefer bins that are "more full" after packing.
        #    "More full" means smaller remaining capacity overall.

        # Metric 1: Tightness of fit.
        # `tightness = 1.0 / (suitable_bins_remain_cap - item + epsilon)`

        # Metric 2: Fill ratio of remaining space.
        # `fill_ratio = item / suitable_bins_remain_cap`

        # Let's combine these multiplicatively, as it inherently balances both.
        # `score = tightness * fill_ratio`
        # `score = (1.0 / (suitable_bins_remain_cap - item + epsilon)) * (item / (suitable_bins_remain_cap + epsilon))`
        # This is the `efficiency_score`.

        # Let's consider the self-reflection points:
        # - Precision: The `efficiency_score` is precise in valuing both tightness and fill.
        # - Adaptability: This score adapts to item sizes and available bin capacities.
        # - Explainability: The components (tightness, fill ratio) are understandable.
        # - Performance: It aims to reduce bin count.

        # Let's think about edge cases for `efficiency_score`:
        # If `suitable_bins_remain_cap` is very small and `item` is close to it:
        #   `item=4`, `rem=5`. Score = `1/(5-4) * 4/5 = 1 * 0.8 = 0.8`.
        # If `item=4`, `rem=10`. Score = `1/(10-4) * 4/10 = 1/6 * 0.4 = 0.066`.
        # This seems to correctly prioritize the tighter fit.

        # If `item=1`, `rem=100`. Score = `1/(100-1) * 1/100 = 1/99 * 0.01 ~ 0.0001`.
        # This correctly penalizes putting a small item into a vast bin.

        # What if we want to explicitly promote filling bins that are *already* mostly full?
        # We need information about the bin's initial state. Since we don't have it,
        # we can only infer from `suitable_bins_remain_cap`.
        # A small `suitable_bins_remain_cap` suggests the bin was likely quite full to begin with.
        # The `efficiency_score` already implicitly favors bins with smaller `suitable_bins_remain_cap`.

        # Consider the "best fit" criteria carefully.
        # Best Fit: Minimize `suitable_bins_remain_cap - item`.
        # This means `suitable_bins_remain_cap` should be just slightly larger than `item`.

        # Let's directly use `suitable_bins_remain_cap` as the primary sorting key,
        # and `suitable_bins_remain_cap - item` as a tie-breaker or secondary key.
        # But this is for sorting. We need a score.

        # Let's go with the `efficiency_score` and consider if it can be improved.
        # The formula `item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`
        # aims to maximize both how much of the current space is used (`item/remaining`)
        # and how little is wasted (`1/(remaining-item)`).

        # Alternative formulation:
        # We want to minimize `suitable_bins_remain_cap`.
        # And we want to minimize `suitable_bins_remain_cap - item`.
        # This means we want `suitable_bins_remain_cap` to be close to `item`.

        # Consider the "waste" `W = suitable_bins_remain_cap - item`. We want to minimize W.
        # Consider the "usage" `U = item / suitable_bins_remain_cap`. We want to maximize U.

        # Let's scale `U` and `1/W` (or similar for W=0) to combine them.
        # Max `U` is 1. Max `1/W` can be infinite.

        # Let's reconsider the v1 penalty: "penalty for bins with too much excess capacity".
        # The idea was to avoid large unused space *after* packing.
        # Let `slack = suitable_bins_remain_cap - item`.
        # We want to penalize large `slack`.
        # A penalty function `P(slack)`.
        # `score = best_fit_score * f(slack)` where `f(slack)` decreases as `slack` increases.
        # `best_fit_score = 1.0 / (slack + epsilon)`

        # If `f(slack) = 1.0 / (slack + epsilon)`
        # `score = (1.0 / (slack + epsilon)) * (1.0 / (slack + epsilon)) = 1.0 / (slack + epsilon)^2`
        # This strongly favors very tight fits.

        # If `f(slack) = 1.0 / (1 + slack)`
        # `score = (1.0 / (slack + epsilon)) * (1.0 / (1 + slack))`
        # `= 1.0 / ((slack + epsilon) * (1 + slack))`
        # This is similar to the `efficiency_score` if `suitable_bins_remain_cap` is roughly constant.

        # Let's try to modify `efficiency_score` to better reflect "avoiding large remaining space".
        # `efficiency_score = item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`

        # Consider the case: item = 10
        # Bin A: rem = 12. slack = 2. efficiency_score = 10 / (12 * 2) = 10/24 ~ 0.416
        # Bin B: rem = 50. slack = 40. efficiency_score = 10 / (50 * 40) = 10/2000 = 0.005

        # This score penalizes Bin B heavily due to its large `suitable_bins_remain_cap`.
        # This is precisely the behavior we want to achieve with "penalizing large gaps".

        # Let's evaluate `v1`'s score for comparison with this `efficiency_score`.
        # `v1` `inverse_remaining` = `1.0 / (suitable_bins_remain_cap - item + 1e-9)`
        # `v1` `normalized_excess` = `(suitable_bins_remain_cap - item) / (max_suitable_cap - item)`
        # `v1` `penalty` = `1.0 / (normalized_excess + 0.1)`
        # `v1` score = `inverse_remaining * penalty`

        # Example for `v1`: item = 10
        # Suitable bins: rem=[12, 50, 15]
        # suitable_bins_remain_cap = [12, 50, 15]
        # max_suitable_cap = 50

        # Bin 1 (rem=12):
        # inverse_remaining = 1/(12-10) = 0.5
        # normalized_excess = (12-10) / (50-10) = 2 / 40 = 0.05
        # penalty = 1 / (0.05 + 0.1) = 1 / 0.15 ~ 6.67
        # score1 = 0.5 * 6.67 = 3.335

        # Bin 2 (rem=50):
        # inverse_remaining = 1/(50-10) = 0.025
        # normalized_excess = (50-10) / (50-10) = 1.0
        # penalty = 1 / (1.0 + 0.1) = 1 / 1.1 ~ 0.909
        # score2 = 0.025 * 0.909 = 0.0227

        # Bin 3 (rem=15):
        # inverse_remaining = 1/(15-10) = 0.2
        # normalized_excess = (15-10) / (50-10) = 5 / 40 = 0.125
        # penalty = 1 / (0.125 + 0.1) = 1 / 0.225 ~ 4.44
        # score3 = 0.2 * 4.44 = 0.888

        # v1 priorities: [3.335, 0.0227, 0.888] -> Bin 1 (rem=12) is highest.

        # Let's check `efficiency_score` for the same: item=10, rem=[12, 50, 15]
        # Bin 1 (rem=12): efficiency_score = 10 / (12 * (12-10)) = 10 / (12 * 2) = 10/24 ~ 0.416
        # Bin 2 (rem=50): efficiency_score = 10 / (50 * (50-10)) = 10 / (50 * 40) = 10/2000 = 0.005
        # Bin 3 (rem=15): efficiency_score = 10 / (15 * (15-10)) = 10 / (15 * 5) = 10/75 ~ 0.133

        # efficiency_score priorities: [0.416, 0.005, 0.133] -> Bin 1 (rem=12) is highest.

        # Both prioritize Bin 1 (rem=12) which is the tightest fit.
        # `v1`'s penalty mechanism is trying to dampen scores for bins with large excess capacity (like rem=50).
        # `efficiency_score` achieves this by having `suitable_bins_remain_cap` in the denominator's factors.

        # The `efficiency_score` seems to be a good candidate. Let's refine it slightly.
        # The "penalty" in `v1` aimed to make scores more comparable and to avoid extremely high scores from extremely tight fits that might be rare.
        # The `efficiency_score` can lead to very high values if `suitable_bins_remain_cap - item` is tiny.

        # Let's add a cap or normalize the `efficiency_score` to avoid issues.
        # Or, introduce a smoother penalty for large remaining capacities.

        # Consider the `fill_ratio = item / suitable_bins_remain_cap`.
        # And `slack_ratio = (suitable_bins_remain_cap - item) / suitable_bins_remain_cap`. We want to minimize this.
        # So, we want to maximize `1.0 - slack_ratio = item / suitable_bins_remain_cap` (which is `fill_ratio`).

        # Let's try a combination that explicitly penalizes the absolute remaining capacity,
        # while also rewarding tightness.
        # Score = `(best_fit_score) * (fill_ratio)`  -> `efficiency_score`
        # Score = `(best_fit_score) / (suitable_bins_remain_cap)` -> `1.0 / ((suitable_bins_remain_cap - item + epsilon) * suitable_bins_remain_cap)`

        # Let's use the `efficiency_score` and add a term that boosts bins with smaller remaining capacity.
        # `score = efficiency_score + bonus_weight * (1.0 / (suitable_bins_remain_cap + epsilon))`
        # This adds a "fill bonus".

        # Let's check this new score:
        # Bin 1 (rem=12): efficiency = 0.416. bonus = 1/12 ~ 0.083. Total = 0.416 + w * 0.083
        # Bin 3 (rem=15): efficiency = 0.133. bonus = 1/15 ~ 0.066. Total = 0.133 + w * 0.066
        # Bin 2 (rem=50): efficiency = 0.005. bonus = 1/50 = 0.02. Total = 0.005 + w * 0.02

        # If w=1:
        # Bin 1: 0.416 + 0.083 = 0.499
        # Bin 3: 0.133 + 0.066 = 0.199
        # Bin 2: 0.005 + 0.02 = 0.025
        # Bin 1 is still highest.

        # If w=10:
        # Bin 1: 0.416 + 0.83 = 1.246
        # Bin 3: 0.133 + 0.66 = 0.793
        # Bin 2: 0.005 + 0.2 = 0.205
        # Bin 1 is still highest.

        # The `efficiency_score` itself already strongly favors smaller `suitable_bins_remain_cap` because it's in the denominator.
        # Adding `1.0 / suitable_bins_remain_cap` might be redundant or might over-emphasize small remaining capacities.

        # Let's stick with the `efficiency_score` as a solid baseline that balances tightness and fill.
        # `efficiency_score = item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`

        # A final consideration: What if the item is very small compared to the bin capacity?
        # item = 1, rem = 100.
        # efficiency_score = 1 / (100 * 99) = 1 / 9900 ~ 0.0001
        # This correctly gives a low score.

        # What if the item is large and the bin has just enough space?
        # item = 99, rem = 100.
        # efficiency_score = 99 / (100 * 1) = 0.99
        # This gives a high score, which is good.

        # Let's make it more "human-readable" and perhaps slightly more robust by considering the proportion of capacity used.
        # `priority = (item / suitable_bins_remain_cap) * (1.0 / (suitable_bins_remain_cap - item + epsilon))`
        # This is `fill_ratio * tightness`.

        # Let's consider a slightly different approach that explicitly penalizes slack.
        # Penalize `suitable_bins_remain_cap - item`.
        # Let `slack = suitable_bins_remain_cap - item`.
        # Score = `1.0 / (slack + epsilon)` (best fit)
        # Penalty factor = `1.0 / (1.0 + slack)` (penalizes slack)
        # `score = (1.0 / (slack + epsilon)) * (1.0 / (1.0 + slack))`
        # `score = 1.0 / ((slack + epsilon) * (1.0 + slack))`

        # Let's test this: item=10
        # Bin 1 (rem=12): slack=2. Score = 1 / (2 * 3) = 1/6 ~ 0.166
        # Bin 3 (rem=15): slack=5. Score = 1 / (5 * 6) = 1/30 ~ 0.033
        # Bin 2 (rem=50): slack=40. Score = 1 / (40 * 41) = 1/1640 ~ 0.0006

        # This new score prioritizes Bin 1 (rem=12) but much less strongly than the `efficiency_score`.
        # The `efficiency_score` `item / (suitable_bins_remain_cap * slack)`
        # Bin 1 (rem=12): 10 / (12 * 2) = 10/24 ~ 0.416
        # Bin 3 (rem=15): 10 / (15 * 5) = 10/75 ~ 0.133
        # Bin 2 (rem=50): 10 / (50 * 40) = 10/2000 = 0.005

        # The `efficiency_score` seems to better reflect the dual goal of tight fit and good space utilization by
        # considering how much of the *remaining capacity* is taken.

        # Let's refine `efficiency_score` calculation to ensure robustness and clarity.
        # `suitable_bins_remain_cap - item` is the slack.
        # `item / suitable_bins_remain_cap` is the fill ratio.

        # Final candidate score: `(item / (suitable_bins_remain_cap + 1e-9)) * (1.0 / (suitable_bins_remain_cap - item + 1e-9))`
        # This is `fill_ratio * tightness`. It's intuitive and captures both goals.
        # It penalizes large remaining capacities due to the `suitable_bins_remain_cap` in the fill ratio denominator.
        # It rewards tight fits due to the `suitable_bins_remain_cap - item` in the tightness denominator.

        # Let's consider the scenario where `suitable_bins_remain_cap` is very large, and `item` is small.
        # E.g., item=1, rem=1000.
        # Fill ratio = 1/1000 = 0.001
        # Tightness = 1/(1000-1) = 1/999 ~ 0.001
        # Score = 0.001 * 0.001 = 0.000001. Very low, as desired.

        # E.g., item=999, rem=1000.
        # Fill ratio = 999/1000 = 0.999
        # Tightness = 1/(1000-999) = 1/1 = 1.0
        # Score = 0.999 * 1.0 = 0.999. Very high, as desired.

        # This `fill_ratio * tightness` score seems robust and captures the desired behavior.
        # It's also more "explainable" than some complex penalty functions.

        # Let's ensure the data types are handled correctly.
        # `item` is float. `bins_remain_cap` is np.ndarray.
        # The calculations will result in floats.

        priorities[suitable_bin_indices] = (item / (suitable_bins_remain_cap + 1e-9)) * \
                                           (1.0 / (suitable_bins_remain_cap - item + 1e-9))

    return priorities
```
