{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a penalty for excessive remaining capacity and\n    rewards for bins that are already substantially filled.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    # Best Fit Score: Prioritize bins that leave the smallest remaining capacity after fitting the item.\n    # Higher score for smaller (remaining_capacity - item).\n    best_fit_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Fill Ratio: Reward bins where the item occupies a larger portion of the *available* space.\n    # Higher score for larger item / suitable_bins_remain_cap.\n    fill_ratio = item / (suitable_bins_remain_cap + 1e-9)\n\n    # Combined Score: Multiply best_fit_score and fill_ratio. This favors bins that are\n    # both a tight fit and where the item significantly utilizes the remaining capacity.\n    # This implicitly penalizes bins with large absolute remaining capacity.\n    combined_score = best_fit_score * fill_ratio\n\n    # Additional Bonus for \"Almost Full\" bins:\n    # Explicitly reward bins that are already substantially filled (small remaining capacity).\n    # This uses a penalty inversely proportional to the remaining capacity.\n    # A small `suitable_bins_remain_cap` (but still >= item) gets a higher bonus.\n    # We add a small constant to the denominator to avoid division by zero for full bins.\n    # We use `item + epsilon` as a reference to avoid issues if a bin is exactly filled by the item.\n    almost_full_bonus = 1.0 / (suitable_bins_remain_cap + 1e-9)\n\n    # Final Priority: Combine the efficiency score with the almost-full bonus.\n    # The `combined_score` (efficiency) already captures tightness and fill ratio.\n    # The `almost_full_bonus` further boosts bins that are simply close to being full.\n    # A simple sum gives a weighted effect, where `combined_score` is the primary driver\n    # and `almost_full_bonus` acts as a secondary preference for already-full bins.\n    # We can tune the weight of the bonus if needed, but a simple sum is a good starting point.\n    final_priorities = combined_score + 0.5 * almost_full_bonus # Tunable weight for bonus\n\n    priorities[suitable_bin_indices] = final_priorities\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a dynamic penalty based on the item size relative to bin capacity,\n    but introduces adaptivity based on the distribution of remaining capacities.\n    It prioritizes bins that offer a good fit, while also considering the overall\n    \"tightness\" of the packing environment. Bins that leave a moderate amount of\n    space might be preferred if they are more numerous, promoting a more balanced packing.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Component 1: Best Fit (prioritize minimal remaining capacity after packing)\n    # We want to minimize `suitable_bins_remain_cap - item`.\n    # To convert to a maximization problem, we use the negative of this difference.\n    # Adding a small epsilon to avoid log(0) or division by zero if remaining_cap == item.\n    best_fit_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9) # Using inverse for maximization\n\n    # Component 2: Adaptable \"Slack Penalty\"\n    # This component penalizes bins that have significantly more capacity than needed.\n    # However, the penalty is modulated by the overall \"slack\" in the system.\n    # If most bins have a lot of slack, a moderate slack is less penalized.\n    # If most bins are nearly full, leaving any significant slack is penalized more.\n    \n    # Calculate the excess capacity relative to the item size for suitable bins.\n    excess_capacity = suitable_bins_remain_cap - item\n    \n    # Calculate a \"global slack factor\". This is the average excess capacity across all *suitable* bins.\n    # If this is high, it means many bins have plenty of room, so individual slack is less of a concern.\n    # If this is low, it means bins are generally tight, so individual slack is more penalized.\n    if len(excess_capacity) > 0:\n        avg_excess_capacity = np.mean(excess_capacity)\n    else:\n        avg_excess_capacity = 0.0\n\n    # Create a penalty factor that is inversely related to the excess capacity.\n    # A low excess_capacity is good (high penalty_factor), a high excess_capacity is bad (low penalty_factor).\n    # We normalize the excess capacity by the average excess capacity to make it adaptive.\n    # If excess_capacity is much smaller than avg_excess_capacity, the ratio is small, penalty_factor is high.\n    # If excess_capacity is much larger than avg_excess_capacity, the ratio is large, penalty_factor is low.\n    # Add 1 to the denominator to avoid division by zero and ensure a base penalty.\n    \n    # Avoid division by zero for avg_excess_capacity if no suitable bins or all have exactly item size.\n    adaptive_slack_penalty_factor = 1.0 / ( (excess_capacity / (avg_excess_capacity + 1e-9)) + 0.5 )\n\n    # Component 3: Bin Uniformity Preference\n    # Prefer bins that are \"closer\" to the average remaining capacity among suitable bins.\n    # This encourages a more uniform distribution of remaining capacities, potentially\n    # leaving larger contiguous spaces in other bins for future large items.\n    # We penalize bins that deviate significantly from the average.\n    \n    # Calculate deviation from the mean remaining capacity of suitable bins.\n    deviation_from_mean = np.abs(suitable_bins_remain_cap - avg_excess_capacity - item) # deviation from item + avg_slack\n    \n    # Higher deviation is worse. We want to reward lower deviation.\n    # Use an inverse relationship to convert minimization of deviation into maximization.\n    # Add a small constant to the denominator to avoid division by zero.\n    uniformity_score = 1.0 / (deviation_from_mean + 1.0)\n\n    # Combine the components.\n    # We want to maximize best_fit_score.\n    # We want to maximize adaptive_slack_penalty_factor (i.e., minimize excess capacity relative to average).\n    # We want to maximize uniformity_score (i.e., minimize deviation from average).\n    # A multiplicative combination can work if components are designed as maximization proxies.\n    # The weights for each component can be tuned. Here, let's use equal weighting initially.\n    \n    # Let's try a combined score that is a weighted sum of log-transformed components to dampen large values and a product for synergy.\n    # Using log-transforms to bring scales closer.\n    \n    # Log-transform for better distribution and to handle orders of magnitude. Add epsilon for log(0).\n    log_best_fit = np.log(best_fit_score + 1e-9)\n    log_adaptive_slack = np.log(adaptive_slack_penalty_factor + 1e-9)\n    log_uniformity = np.log(uniformity_score + 1e-9)\n\n    # A combined heuristic that blends the direct \"best fit\" with the \"adaptive slack\" and \"uniformity\" preferences.\n    # The idea is to favor good fits, but also adapt to the overall state of bins and encourage balance.\n    # For example, if a bin offers a slightly worse fit but has a much better adaptive slack score and uniformity, it might be preferred.\n    \n    # Simple weighted sum might not capture complex interactions.\n    # Let's try a multiplicative combination of the positive components and a penalty for negative aspects.\n    # We want high `best_fit_score`, high `adaptive_slack_penalty_factor`, and high `uniformity_score`.\n    # So, a product seems appropriate.\n    \n    # Adding a small epsilon to the final result to ensure positive scores if any component is zero (though unlikely with current formulation).\n    final_priorities = log_best_fit + log_adaptive_slack + log_uniformity\n    \n    # Ensure we only update priorities for suitable bins.\n    priorities[suitable_bins_mask] = final_priorities\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), the code is identical. The ranking suggests a subtle difference in conceptualization, not implementation.\n\nComparing (2nd) vs (3rd), the code is identical. The ranking is arbitrary or based on external factors not evident from the code.\n\nComparing (3rd) vs (4th): Heuristic 3 attempts to combine \"Best Fit\" with a penalty for large excess capacity relative to item size using a `tightness_ratio` and `penalty_component`. Heuristic 4 introduces \"Worst Fit\" concepts and a \"dynamic gap penalty,\" but its implementation primarily focuses on `best_fit_score * gap_management_score`, where `gap_management_score` is `1.0 / (1.0 + (gap / item))`. Heuristic 3's penalty is more directly tied to the `remaining_cap - item` ratio, whereas Heuristic 4 uses `item / (remaining_cap)`. The core idea of penalizing large gaps is similar, but Heuristic 4's `gap_management_score` might be more stable for very small items.\n\nComparing (4th) vs (5th): Heuristic 4 uses a multiplicative combination of Best Fit and a gap management score. Heuristic 5 introduces a sigmoid for tight fits and a penalty for large initial capacities, combining them multiplicatively. Heuristic 5's sigmoid introduces non-linearity, potentially offering smoother preferences for tight fits. Heuristic 4's approach is more directly interpretable as Best Fit scaled by a tightness factor.\n\nComparing (5th) vs (6th): Heuristic 5 uses a sigmoid and a simple inverse capacity penalty. Heuristic 6 prioritizes exact fits with a very high score and then uses exponential decay based on normalized slack for non-exact fits. Heuristic 6's explicit handling of exact fits is a strong point, but its normalization of slack might be sensitive to extreme values.\n\nComparing (6th) vs (7th): Heuristic 6 has explicit exact fit handling and exponential decay. Heuristic 7 also prioritizes exact fits (with a slightly lower score) and then combines Best Fit (`1.0 / (remaining_capacities_after_fit)`) with a preference for \"almost full\" bins (using `1.0 / (suitable_capacities)`). Heuristic 7's additive combination of these two preferences, weighted, offers a different balance.\n\nComparing (7th) vs (8th): Heuristics 7 and 8 are identical.\n\nComparing (8th) vs (9th): Heuristic 8 uses a sigmoid for tight fits and an inverse capacity penalty. Heuristic 9 combines a tightness score (`1.0 / slack`) with a fill ratio (`item / remaining_cap`), using multiplication. Heuristic 9's multiplicative approach is more direct in rewarding both tight fits and good item utilization within the bin.\n\nComparing (9th) vs (10th): Heuristic 9 multiplies tightness and fill ratio. Heuristic 10 does the same but adds an \"almost full\" bonus, which is essentially another term favoring bins with low initial remaining capacity. The additive bonus provides an additional layer of preference.\n\nComparing (10th) vs (11th): Heuristic 10 combines efficiency (tightness * fill ratio) with an \"almost full\" bonus additively. Heuristic 11 uses a weighted additive combination of \"almost full\" preference and \"tightest fit\" preference. Heuristic 11's explicit weighting of preferences is more structured than Heuristic 10's additive bonus.\n\nComparing (11th) vs (12th): Heuristics 11 and 12 are identical.\n\nComparing (12th) vs (13th): Heuristics 12 and 13 are identical.\n\nComparing (13th) vs (14th): Heuristic 13 is identical to 11 and 12. Heuristic 14 introduces an \"adaptive Best Fit\" by dynamically weighting a tightness component based on the item's size relative to average remaining capacity. It also uses a log-based best-fit score. This adaptivity is a novel aspect.\n\nComparing (14th) vs (15th): Heuristics 14 and 15 are identical.\n\nComparing (15th) vs (16th): Heuristic 15 is identical to 14. Heuristic 16 combines Best Fit (log scale) with a refined slack minimization and a fill ratio component, using multiplication. It also introduces a `slack_decay_factor` (`exp(-relative_slack)`). This multiplicative approach with an exponential decay is a strong combination.\n\nComparing (16th) vs (17th): Heuristics 16 and 17 are identical.\n\nComparing (17th) vs (18th): Heuristics 17 and 18 are identical.\n\nComparing (18th) vs (19th): Heuristic 18 is identical to 16 and 17. Heuristic 19 attempts to combine Best Fit, an adaptive slack penalty (normalized by average excess capacity), and a uniformity score (penalizing deviation from average remaining capacity). This multi-faceted approach aims for more balanced packing.\n\nComparing (19th) vs (20th): Heuristic 19 combines Best Fit (log), adaptive slack penalty, and uniformity. Heuristic 20 combines Best Fit (log), fullness score (inverse capacity), multiplicatively. Heuristic 19's attempt at adaptivity and uniformity suggests a more sophisticated strategy than Heuristic 20's simpler multiplicative combination.\n\nOverall, heuristics that explicitly handle exact fits (6, 7), use adaptive weights or penalties based on item size or system state (14, 15, 19), or combine multiple strong metrics multiplicatively with decay functions (16, 17, 18) appear to be conceptually superior. The ranking seems to generally increase in complexity and adaptivity up to a point, then potentially plateau or slightly decline in perceived quality.\n- \nHere's a redefined approach to self-reflection for heuristic design:\n\n*   **Keywords:** Robustness, adaptability, clarity, parsimony, performance, objective function.\n*   **Advice:** Focus on designing heuristics with clear, interpretable objective functions that can adapt to varying problem states (e.g., item sizes, bin utilization). Prioritize vectorized implementations for efficiency.\n*   **Avoid:** Redundant or overly complex metric combinations, arbitrary thresholds, and heuristics that don't demonstrably improve performance across a range of scenarios.\n*   **Explanation:** True self-reflection means identifying core principles of good heuristic design (like clarity and adaptability) and ruthlessly eliminating elements that introduce complexity without clear benefit, ensuring robustness and efficient execution.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}