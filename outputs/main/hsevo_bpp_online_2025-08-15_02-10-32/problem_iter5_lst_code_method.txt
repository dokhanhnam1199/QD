{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a dynamic penalty based on the item size relative to bin capacity.\n    Prioritizes bins that offer a \"near-perfect\" fit without excessive leftover space,\n    dynamically adjusting the penalty based on how \"tight\" the fit is.\n    This aims for better space utilization by being more sensitive to the actual item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # \"Best Fit\" component: Prioritize bins with minimum remaining capacity after packing.\n    # We use the negative of the remaining capacity to transform minimization into maximization.\n    # Add a small epsilon to ensure no division by zero or log(0) if remaining_cap == item.\n    # Using log to compress the range and emphasize smaller differences.\n    best_fit_score = -np.log(suitable_bins_remain_cap - item + 1e-9)\n    \n    # Dynamic Penalty component: Penalize bins with significantly more capacity than needed.\n    # The penalty is higher when the \"excess capacity\" (remaining_cap - item) is large\n    # relative to the item's size. This makes the penalty scale with the item's magnitude.\n    \n    # Calculate the \"tightness ratio\": (item_size) / (remaining_capacity_after_packing)\n    # A higher ratio means a tighter fit.\n    tightness_ratio = item / (suitable_bins_remain_cap - item + 1e-9)\n    \n    # Calculate a penalty that is higher for bins with a lower tightness ratio (more excess capacity relative to item size)\n    # We want to penalize bins where (suitable_bins_remain_cap - item) is large compared to 'item'.\n    # Using a sigmoid-like function (inverse of a scaled ratio) to dampen extreme values and provide a smoother penalty.\n    # The scaling factor (e.g., 1.0) can be tuned.\n    \n    # Higher penalty for lower tightness_ratio. Invert and add 1 to avoid division by zero and ensure positive penalty.\n    # A larger suitable_bins_remain_cap relative to 'item' leads to a smaller tightness_ratio,\n    # which after inversion and addition, results in a larger penalty.\n    # We want to *subtract* this penalty from the best_fit_score, so a higher penalty means a lower final score.\n    penalty_component = 1.0 / (tightness_ratio + 0.5) # Add 0.5 to avoid issues with very tight fits.\n    \n    # Combine the scores. We want to maximize `best_fit_score` and minimize `penalty_component`.\n    # A simple subtraction works if interpreted as score = bf_score - penalty.\n    # Alternatively, we can multiply if penalties were designed as multipliers.\n    # Here, we aim for a higher combined score. Since `best_fit_score` is already a maximization proxy,\n    # and `penalty_component` is something we want to minimize (i.e., a higher penalty is bad),\n    # we subtract the penalty.\n    \n    # To make it a maximization problem directly, we can express it as:\n    # Score = best_fit_score - penalty_component\n    # or, if we want to penalize the penalty:\n    # Score = best_fit_score * (1 / (penalty_component + epsilon)) which is equivalent to\n    # Score = best_fit_score * tightness_ratio (approximately)\n    # Let's use a multiplicative approach where a higher `penalty_component` reduces the score.\n    # A simple way to combine: maximize `best_fit_score` and maximize `1 / (penalty_component + epsilon)`\n    # This means maximizing `best_fit_score * (tightness_ratio)`.\n    \n    # Let's refine the penalty: Penalize bins where `remaining_cap - item` is large relative to `item`.\n    # Consider `excess_ratio = (remaining_cap - item) / item`. We want to penalize high `excess_ratio`.\n    # Penalty_score = 1 / (excess_ratio + 1).\n    # This is similar to the tightness ratio logic but framed differently.\n    \n    excess_ratio = (suitable_bins_remain_cap - item) / (item + 1e-9)\n    # A bin with exact fit has excess_ratio = 0. A bin with large excess has large excess_ratio.\n    # We want to penalize large excess_ratio. So, a good penalty multiplier would be 1 / (excess_ratio + C).\n    # The smaller the `1 / (excess_ratio + C)`, the worse the bin.\n    # So, we want to maximize `best_fit_score` and maximize `1 / (excess_ratio + C)`.\n    # Thus, we can multiply them.\n    \n    penalty_multiplier = 1.0 / (excess_ratio + 0.2) # Add 0.2 to ensure it's not too aggressive.\n    \n    # Final priority is the product of the best-fit score proxy and the penalty multiplier.\n    # Higher best_fit_score is good. Higher penalty_multiplier is good (means low excess ratio).\n    priorities[suitable_bins_mask] = best_fit_score * penalty_multiplier\n    \n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for bins with too much excess capacity.\n    Prioritizes bins that are a tight fit, penalizing those with large gaps.\n    This aims for better space utilization by avoiding overly large remaining spaces.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Calculate inverse of remaining capacity (similar to Best Fit)\n    # Adding a small epsilon to avoid division by zero if remaining_cap == item\n    inverse_remaining = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n    \n    # Calculate a penalty for bins with a large gap (excess capacity)\n    # Using a sigmoid-like function to penalize larger gaps more significantly\n    # Normalize remaining capacity to a 0-1 scale for the penalty function\n    max_suitable_cap = np.max(suitable_bins_remain_cap)\n    min_suitable_cap = np.min(suitable_bins_remain_cap)\n    \n    # Avoid division by zero if all suitable bins have the same capacity\n    if max_suitable_cap == min_suitable_cap:\n        normalized_excess = np.zeros_like(suitable_bins_remain_cap)\n    else:\n        # Capacity of the bin relative to the range of suitable capacities\n        # We want to penalize bins with capacity much larger than the item\n        # Focus on the gap: suitable_bins_remain_cap - item\n        excess_capacity = suitable_bins_remain_cap - item\n        normalized_excess = excess_capacity / (max_suitable_cap - item + 1e-9) # Normalize by max possible excess\n        \n    # Apply a penalty: higher penalty for larger normalized excess\n    # A simple inverse of the normalized excess can work as a penalty,\n    # or a more aggressive function like exp(-k * normalized_excess)\n    # Let's use a simple inverse for now, penalizing bins with larger excess\n    # Add a small constant to avoid division by zero for bins that are exact fits after normalization\n    penalty = 1.0 / (normalized_excess + 0.1) \n    \n    # Combine the \"Best Fit\" score with the penalty\n    # We want to favor smaller remaining capacities (high inverse_remaining)\n    # and penalize larger excess capacities (low penalty value, as penalty is 1/(normalized_excess+c))\n    # So, we want to maximize inverse_remaining and minimize penalty\n    # A simple combination: inverse_remaining / penalty (effectively inverse_remaining * (normalized_excess + c))\n    # This gives higher scores to bins that are tight fits AND don't have excessive space after fitting.\n    priorities[suitable_bins_mask] = inverse_remaining * penalty\n    \n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Aims to improve upon v1 by introducing a more dynamic penalty that\n    considers the item's size relative to the bin's capacity and a\n    more nuanced \"best fit\" calculation. It also prioritizes filling bins\n    more completely when they are already significantly filled, promoting\n    the use of fewer bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    # More robust \"best fit\" score: prioritizing bins that leave the smallest remaining capacity.\n    # We invert the difference `remaining_capacity - item`. Adding a small epsilon\n    # to the denominator avoids division by zero and ensures bins that are an exact fit\n    # get a high score.\n    best_fit_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Dynamic penalty for \"slack\" or excess capacity.\n    # The penalty is higher for bins that have a lot of remaining capacity\n    # *relative to the item size*. This discourages placing small items\n    # into bins that have a very large remaining capacity, saving those\n    # large capacities for larger items.\n    # We define \"slack\" as the ratio of remaining capacity to bin capacity,\n    # but we only consider the capacity *beyond* what's needed for the item.\n    # A smaller ratio of \"excess capacity\" (remaining capacity - item) to the\n    # bin's original capacity is preferred.\n    # Using a sigmoid-like function centered around 0.5 (meaning half-full is neutral)\n    # penalizes bins that are very empty (slack close to 1) and also bins that are\n    # almost full but still have a little room (slack close to 0, but we want to\n    # prioritize those that are *almost* full to finish a bin).\n\n    # Calculate the \"slack\" ratio: (remaining_capacity - item) / original_capacity\n    # We use the original capacity before fitting the item as a reference.\n    # A very small remaining capacity (after fitting) implies a good fit.\n    # We want to penalize large *unused* capacity.\n    # Let's consider `(bin_capacity - item)` as the \"excess\" after fitting.\n    # The penalty should be higher if `excess_capacity` is large.\n\n    # We can also introduce a factor that rewards filling bins that are already\n    # substantially full. If a bin is already more than 50% full, we might\n    # give it a slight boost to encourage completing it.\n\n    # Let's try a penalty based on the *normalized remaining capacity* after fitting.\n    # Higher remaining capacity after fitting should be penalized more.\n    # Normalize the remaining capacity (`suitable_bins_remain_cap - item`)\n    # by the maximum possible remaining capacity among suitable bins.\n    max_possible_gap = np.max(suitable_bins_remain_cap - item)\n    if max_possible_gap == 0:\n        normalized_gap = np.zeros_like(suitable_bins_remain_cap)\n    else:\n        normalized_gap = (suitable_bins_remain_cap - item) / max_possible_gap\n\n    # Penalty: Higher penalty for larger normalized gap.\n    # Using 1 - normalized_gap means a small gap gets a high score, and a large gap gets a low score.\n    # We want to penalize large gaps, so a lower score for large gaps is a penalty.\n    # Let's invert this: higher penalty value for larger gaps.\n    # So, penalty = normalized_gap (higher is worse).\n    # We want to combine this with best_fit_score. A high best_fit_score is good.\n    # A high penalty value is bad. So we should subtract the penalty or divide by it.\n\n    # Let's re-evaluate the penalty. We want to penalize bins with *too much* excess capacity.\n    # This means bins where `suitable_bins_remain_cap - item` is large.\n    # We can use a function that grows with `suitable_bins_remain_cap - item`.\n    # Let's consider the *proportion of capacity remaining* after placing the item.\n    # `(suitable_bins_remain_cap - item) / suitable_bins_remain_cap`\n    # However, if `suitable_bins_remain_cap` is just slightly larger than `item`,\n    # this proportion can be very small.\n\n    # New approach for penalty:\n    # We want to penalize bins where `suitable_bins_remain_cap` is much larger than `item`.\n    # Let's focus on `suitable_bins_remain_cap`.\n    # Higher `suitable_bins_remain_cap` should have a penalty.\n    # Let's use a transformation of `suitable_bins_remain_cap` that increases as it grows.\n    # A simple inverse might penalize *all* bins with remaining capacity.\n\n    # Instead of a penalty, let's directly reward filling bins.\n    # If a bin is already quite full, placing an item that fits well is very good.\n    # Let's consider the \"fullness\" of the bin *before* placing the item.\n    # A bin that is already > 50% full should get a slight bonus for being filled further.\n\n    # Calculate a \"fill ratio\" for each suitable bin relative to its *current* remaining capacity.\n    # This isn't quite right. We need to consider the *original* capacity if available.\n    # Without original capacity, we can only work with `bins_remain_cap`.\n\n    # Let's refine the penalty based on the *slack*: `suitable_bins_remain_cap - item`.\n    # We want to penalize large slack.\n    # Let's use a logistic function to map the slack to a penalty factor.\n    # We want a penalty that increases as slack increases.\n    # Let `slack = suitable_bins_remain_cap - item`.\n    # A reasonable maximum slack could be considered the largest remaining capacity among suitable bins.\n    max_slack = np.max(suitable_bins_remain_cap - item)\n    if max_slack < 1e-9: # All suitable bins are exact fits\n        slack_penalty_factor = np.zeros_like(suitable_bins_remain_cap)\n    else:\n        normalized_slack = (suitable_bins_remain_cap - item) / max_slack\n        # Penalty increases with normalized slack.\n        # Use a function that is 0 for slack=0 and increases.\n        # A simple linear relationship or a slightly more aggressive function.\n        # Let's use a function that gives higher penalty for slack > average slack.\n        # Option 1: `normalized_slack` itself (linear penalty)\n        # Option 2: `np.exp(normalized_slack)` (exponential penalty)\n        # Option 3: A piecewise function or a sigmoid that penalizes > threshold.\n\n        # Let's try a combination:\n        # Reward tight fits (high `best_fit_score`).\n        # Penalize large residual capacity (`suitable_bins_remain_cap - item`).\n        # A bin that is almost full (small `suitable_bins_remain_cap`) but can fit the item,\n        # should be highly prioritized if the item fits tightly.\n\n        # Consider the ratio `item / suitable_bins_remain_cap`.\n        # Higher ratio is good (item takes up a larger portion of remaining space).\n        fill_ratio = item / (suitable_bins_remain_cap + 1e-9)\n\n        # Now, let's combine `best_fit_score` and `fill_ratio`.\n        # Both should be maximized.\n        # `best_fit_score` is `1 / (remaining - item)`.\n        # `fill_ratio` is `item / remaining`.\n\n        # If `remaining - item` is small, `best_fit_score` is large.\n        # If `item / remaining` is large, `fill_ratio` is large.\n\n        # Let's try a weighted sum, where `best_fit_score` has a higher weight,\n        # but `fill_ratio` acts as a secondary criterion to break ties or to\n        # favor bins that will be more \"full\" after packing.\n\n        # Priority = `best_fit_score` + `w * fill_ratio`\n        # However, the scales can be very different.\n\n        # Alternative: Multiply.\n        # Priority = `best_fit_score` * `fill_ratio`\n        # This means we want both a tight fit AND a good fill ratio.\n        # If `fill_ratio` is very small (item is tiny compared to remaining space),\n        # the score will be low, even if it's a tight fit. This is good.\n\n        # Let's consider the \"efficiency\" of the bin after packing.\n        # Efficiency = `item / original_bin_capacity`.\n        # We don't have original bin capacity.\n        # Let's use `item / (item + slack)` which is `item / suitable_bins_remain_cap`.\n\n        # Let's make the `best_fit_score` more robust to very small capacities.\n        # Instead of `1 / (remaining - item)`, consider `(remaining - item) / remaining`.\n        # This is the proportion of remaining capacity that is *not* used.\n        # We want to minimize this proportion.\n        # So, `1 - (remaining - item) / remaining` = `item / remaining`. This is `fill_ratio`.\n\n        # So, `fill_ratio = item / suitable_bins_remain_cap` is a good metric for how much\n        # of the available space the item occupies.\n\n        # Let's reconsider `v1`'s goal: \"tight fit\" and \"penalty for excess capacity\".\n        # `best_fit_score` addresses tight fit.\n        # The penalty part in `v1` was `1.0 / (normalized_excess + 0.1)`.\n        # `normalized_excess` was `(suitable_bins_remain_cap - item) / (max_suitable_cap - item)`.\n        # This means `penalty` was high for small excess and low for large excess. This is inverted.\n        # The combination was `inverse_remaining * penalty`. `inverse_remaining` was `1/(remaining-item)`.\n\n        # Let's create a metric that is high for tight fits AND for bins that are \"almost full\".\n        # Consider the value `suitable_bins_remain_cap`.\n        # We want to select a bin where `suitable_bins_remain_cap` is small, but still `>= item`.\n        # This means `suitable_bins_remain_cap` should be as close to `item` as possible.\n\n        # Let's try a score that is `1.0 / suitable_bins_remain_cap`. This favors smaller remaining capacities.\n        # And combine it with a measure of how \"tight\" the fit is.\n        # Tight fit can be measured by `1.0 / (suitable_bins_remain_cap - item + epsilon)`.\n\n        # Let's try a score that directly prioritizes bins with low remaining capacity *after* fitting the item.\n        # `priority = 1.0 / (suitable_bins_remain_cap - item + epsilon)` (This is `best_fit_score`).\n        # This already prioritizes tight fits.\n\n        # How to penalize excess capacity more effectively?\n        # We want to reduce the priority of bins where `suitable_bins_remain_cap` is large,\n        # even if they are suitable.\n\n        # Let's use a function that decreases as `suitable_bins_remain_cap` increases.\n        # Example: `exp(-k * suitable_bins_remain_cap)` where `k` is a tuning parameter.\n        # Or, normalize `suitable_bins_remain_cap` by the maximum *possible* remaining capacity\n        # (which is bin_capacity - smallest_item). Without bin_capacity, we can normalize\n        # by the maximum `suitable_bins_remain_cap` found.\n\n        # Let's consider the \"waste\" `suitable_bins_remain_cap - item`.\n        # We want to penalize large waste.\n        # Let `waste = suitable_bins_remain_cap - item`.\n        # Penalty function `P(waste)`. We want `P(waste)` to increase with `waste`.\n        # `P(waste) = waste^2` or `exp(waste)`.\n\n        # Let's combine `best_fit_score` with a penalty inversely related to `suitable_bins_remain_cap`.\n        # Combined score: `best_fit_score * (1.0 / (suitable_bins_remain_cap + epsilon))`\n        # This might over-penalize bins with moderate remaining capacity.\n\n        # Let's use the `fill_ratio = item / suitable_bins_remain_cap` as a base metric.\n        # Higher fill ratio is better.\n        # Now, let's refine this with the tightness of the fit.\n        # If the fit is very tight (`suitable_bins_remain_cap - item` is small), this is very good.\n        # If the fit is loose (`suitable_bins_remain_cap - item` is large), this is less good.\n\n        # Consider a score that is:\n        # `fill_ratio`  -> How much of the available space is utilized.\n        # `tightness`   -> How close `remaining` is to `item`.\n        # Let `tightness = 1.0 / (suitable_bins_remain_cap - item + epsilon)`\n\n        # Combined: `fill_ratio * tightness = (item / suitable_bins_remain_cap) * (1.0 / (suitable_bins_remain_cap - item + epsilon))`\n        # This looks promising. It prioritizes bins where the item is a large fraction of the remaining space\n        # AND the remaining space after fitting is small.\n\n        # Let's analyze this product:\n        # `item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + epsilon)`\n        # If `suitable_bins_remain_cap` is slightly larger than `item` (tight fit, high `fill_ratio`),\n        # the denominator term `suitable_bins_remain_cap - item` is small, leading to a large score.\n        # If `suitable_bins_remain_cap` is much larger than `item`, the `fill_ratio` becomes small,\n        # reducing the overall score, even if `suitable_bins_remain_cap - item` is small relative to `suitable_bins_remain_cap`.\n\n        # This seems to capture both aspects well.\n        # Let's call this the \"efficiency score\".\n\n        efficiency_score = item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)\n\n        # This score can become very large if `suitable_bins_remain_cap - item` is tiny.\n        # Normalizing might be good, but it might also dampen the strong preference for tight fits.\n        # Let's consider scaling or bounding the score if necessary, but start with this direct approach.\n\n        # To make it more robust and less sensitive to extreme values, we can add a small constant\n        # to the denominator to avoid infinities if `suitable_bins_remain_cap` is zero (though\n        # `suitable_bins_remain_cap >= item` should prevent this if `item > 0`).\n        # The `1e-9` is already there.\n\n        # Consider the edge case where `suitable_bins_remain_cap == item`.\n        # Then `suitable_bins_remain_cap - item` is 0, leading to infinity.\n        # This means exact fits get infinitely high priority. This is generally good.\n\n        # However, `v1` tried to penalize bins with *too much* excess capacity.\n        # If we have two bins:\n        # Bin A: remaining_cap = 10, item = 9. Score ~ 9 / (10 * 1) = 0.9\n        # Bin B: remaining_cap = 2, item = 1. Score ~ 1 / (2 * 1) = 0.5\n\n        # What if we have:\n        # Bin C: remaining_cap = 100, item = 99. Score ~ 99 / (100 * 1) = 0.99\n        # Bin D: remaining_cap = 5, item = 4. Score ~ 4 / (5 * 1) = 0.8\n\n        # This seems to favor the \"absolute\" tightest fits, not necessarily the most efficient use of space when capacities differ widely.\n        # If Bin C (100 capacity, 99 remaining) and Bin D (5 capacity, 4 remaining) are available for item=4:\n        # Bin C: remaining_cap = 100, item = 4. Score ~ 4 / (100 * 96) = 4 / 9600 ~ 0.0004\n        # Bin D: remaining_cap = 5, item = 4. Score ~ 4 / (5 * 1) = 0.8\n        # This is good: Bin D is prioritized.\n\n        # If item = 99:\n        # Bin C: remaining_cap = 100, item = 99. Score ~ 99 / (100 * 1) = 0.99\n        # Bin D: remaining_cap = 5, item = 99. (Not suitable)\n        # Bin C is prioritized.\n\n        # What if we have:\n        # Bin E: remaining_cap = 10, item = 9. Score ~ 9 / (10 * 1) = 0.9\n        # Bin F: remaining_cap = 20, item = 18. Score ~ 18 / (20 * 2) = 18 / 40 = 0.45\n\n        # This suggests the current `efficiency_score` prioritizes the *absolute* tightness of fit.\n        # `v1` tried to penalize \"too much excess capacity\".\n\n        # Let's try to incorporate a penalty for large absolute remaining capacity.\n        # `penalty_factor = 1.0 / (suitable_bins_remain_cap + epsilon)`\n        # Combining `efficiency_score * penalty_factor`:\n        # `[item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + epsilon)] * [1.0 / (suitable_bins_remain_cap + epsilon)]`\n        # `= item / (suitable_bins_remain_cap^2 * (suitable_bins_remain_cap - item) + epsilon)`\n        # This will strongly penalize larger remaining capacities.\n\n        # Let's call this `refined_score`.\n        refined_score = item / (suitable_bins_remain_cap**2 * (suitable_bins_remain_cap - item) + 1e-9)\n\n        # Consider its behavior:\n        # Bin E: remaining=10, item=9. Score ~ 9 / (100 * 1) = 0.09\n        # Bin F: remaining=20, item=18. Score ~ 18 / (400 * 2) = 18 / 800 = 0.0225\n\n        # This penalizes Bin E less than F, which seems to align with favoring tighter fits.\n        # If we want to penalize bins with *large* remaining capacity, this `refined_score` does that.\n        # It strongly emphasizes reducing `suitable_bins_remain_cap` in the denominator.\n\n        # Let's rethink the \"penalty for bins with too much excess capacity\" from v1.\n        # The goal was to avoid putting small items in large, nearly empty bins.\n        # This means we want to *reduce* the priority of bins where `suitable_bins_remain_cap` is large.\n\n        # Let's use a score that is a balance between best fit and minimizing remaining capacity.\n        # Best fit: `1.0 / (suitable_bins_remain_cap - item + epsilon)`\n        # Minimize remaining capacity: `1.0 / (suitable_bins_remain_cap + epsilon)`\n\n        # Option 1: Average of scores\n        # `(1.0 / (suitable_bins_remain_cap - item + epsilon) + 1.0 / (suitable_bins_remain_cap + epsilon)) / 2.0`\n        # This combines the two ideas directly.\n\n        # Option 2: Weighted average, or product.\n        # Product: `(1.0 / (suitable_bins_remain_cap - item + epsilon)) * (1.0 / (suitable_bins_remain_cap + epsilon))`\n        # `= 1.0 / ((suitable_bins_remain_cap - item + epsilon) * (suitable_bins_remain_cap + epsilon))`\n        # This also heavily favors small `suitable_bins_remain_cap`.\n\n        # Let's reconsider the `fill_ratio = item / suitable_bins_remain_cap`.\n        # This is good because it directly relates to how \"full\" the bin will be.\n        # And `tightness = 1.0 / (suitable_bins_remain_cap - item + epsilon)`.\n        # Product: `(item / (suitable_bins_remain_cap + epsilon)) * (1.0 / (suitable_bins_remain_cap - item + epsilon))`\n        # This is the `efficiency_score` we derived earlier.\n\n        # How about a modification to `v1`'s penalty?\n        # `v1` penalty was `1.0 / (normalized_excess + 0.1)`.\n        # Normalized excess was `(suitable_bins_remain_cap - item) / (max_suitable_cap - item)`.\n        # So `v1` penalty was high for small normalized excess, low for large.\n        # It multiplied `inverse_remaining` by this penalty.\n        # `priorities = (1/(remaining-item)) * (1/(norm_excess + 0.1))`\n        # This means it favored small `remaining-item` and small `norm_excess`.\n\n        # Let's try to make the penalty more direct: penalize large `suitable_bins_remain_cap`.\n        # `score = best_fit_score / (suitable_bins_remain_cap + epsilon)`\n        # `score = (1.0 / (suitable_bins_remain_cap - item + epsilon)) / (suitable_bins_remain_cap + epsilon)`\n        # `= 1.0 / ((suitable_bins_remain_cap - item + epsilon) * (suitable_bins_remain_cap + epsilon))`\n        # This is the product we saw before.\n\n        # Let's try a different combination strategy.\n        # Prioritize bins that are a tight fit (`best_fit_score`).\n        # Then, among tight fits, prefer those that are \"more full\" relative to their capacity.\n        # `fill_ratio = item / suitable_bins_remain_cap`\n\n        # How to combine `best_fit_score` and `fill_ratio`?\n        # `score = best_fit_score + weight * fill_ratio` ?\n        # This can be problematic due to scale.\n\n        # Let's go back to `efficiency_score = item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`\n        # This score is high when `item` is large relative to `suitable_bins_remain_cap` AND `suitable_bins_remain_cap` is close to `item`.\n\n        # Consider the objective: minimize number of bins.\n        # This means we want to fill bins as much as possible, and prefer tighter fits to leave\n        # more room in other bins for future items.\n\n        # Let's add a term that explicitly favors filling bins that are already substantialy full.\n        # A bin with `suitable_bins_remain_cap < BIN_CAPACITY / 2` (assuming BIN_CAPACITY is known, which it is not here).\n        # Without BIN_CAPACITY, we can use `suitable_bins_remain_cap` relative to `item`.\n\n        # Let's define a \"completion bonus\" for bins that are nearly full after placing the item.\n        # If `suitable_bins_remain_cap - item` is very small, it's a good fill.\n        # Let's add a score proportional to `1.0 / (suitable_bins_remain_cap + epsilon)`.\n        # This rewards smaller remaining capacities.\n\n        # Combined Score = `efficiency_score` + `bonus_weight * (1.0 / (suitable_bins_remain_cap + epsilon))`\n        # `efficiency_score` already incorporates `1.0 / (suitable_bins_remain_cap - item + epsilon)`.\n\n        # Let's try a different penalty approach for excess capacity.\n        # We want to reduce the score if `suitable_bins_remain_cap` is large compared to `item`.\n        # Let `excess_ratio = suitable_bins_remain_cap / item`.\n        # We want to penalize large `excess_ratio`.\n        # Penalty: `1.0 / (excess_ratio + epsilon)` or `exp(-k * excess_ratio)`.\n        # Let's use `1.0 / (suitable_bins_remain_cap / item + epsilon)` which is `item / (suitable_bins_remain_cap + epsilon)`.\n        # This is the `fill_ratio` we defined earlier.\n\n        # So, we want to maximize `best_fit_score` and `fill_ratio`.\n        # `best_fit_score = 1.0 / (suitable_bins_remain_cap - item + epsilon)`\n        # `fill_ratio = item / (suitable_bins_remain_cap + epsilon)`\n\n        # Let's consider the product: `best_fit_score * fill_ratio`.\n        # This is the `efficiency_score`.\n\n        # Let's consider another angle: penalize the *total* remaining capacity.\n        # We have `best_fit_score` that favors small `remaining - item`.\n        # We want to additionally favor smaller `remaining`.\n\n        # Let's try using the `best_fit_score` and applying a penalty to it.\n        # Penalty for large remaining capacity: `suitable_bins_remain_cap`.\n        # Penalty factor should decrease as `suitable_bins_remain_cap` increases.\n        # e.g., `exp(-k * suitable_bins_remain_cap)`.\n        # `score = best_fit_score * exp(-k * suitable_bins_remain_cap)`\n        # `score = (1.0 / (suitable_bins_remain_cap - item + epsilon)) * exp(-k * suitable_bins_remain_cap)`\n\n        # Let's choose `k` such that it scales appropriately.\n        # If `suitable_bins_remain_cap` is, say, 10 times `item`, we might want to penalize it.\n        # Let's pick `k` so that if `suitable_bins_remain_cap` is twice the average `suitable_bins_remain_cap`,\n        # the exponential term is significantly reduced.\n\n        # Instead of exponential, let's use a simpler penalty that's easier to tune.\n        # Penalty factor: `1.0 / (1.0 + alpha * (suitable_bins_remain_cap / item))` where alpha is a tuning parameter.\n        # This penalizes bins where remaining capacity is much larger than item.\n        # Penalty factor = `item / (item + alpha * suitable_bins_remain_cap)`\n\n        # Let's combine: `best_fit_score * (item / (item + alpha * suitable_bins_remain_cap))`\n        # `score = (1.0 / (suitable_bins_remain_cap - item + epsilon)) * (item / (item + alpha * suitable_bins_remain_cap))`\n\n        # Example: item=10\n        # Bin A: rem=12. best_fit=1/(12-10)=0.5. penalty=(10/(10+a*12)). Score=0.5 * 10/(10+12a) = 5/(10+12a)\n        # Bin B: rem=20. best_fit=1/(20-10)=0.1. penalty=(10/(10+a*20)). Score=0.1 * 10/(10+20a) = 1/(10+20a)\n        # Bin C: rem=15. best_fit=1/(15-10)=0.2. penalty=(10/(10+a*15)). Score=0.2 * 10/(10+15a) = 2/(10+15a)\n\n        # If a=0 (no penalty):\n        # A: 0.5\n        # B: 0.1\n        # C: 0.2\n        # Bin A (tightest fit) is best.\n\n        # If a=1:\n        # A: 5 / (10+12) = 5/22 ~ 0.227\n        # B: 1 / (10+20) = 1/30 ~ 0.033\n        # C: 2 / (10+15) = 2/25 = 0.08\n        # Bin A is still best. This penalty isn't strongly altering the order unless 'a' is large.\n\n        # The core idea of `v1` was \"Prioritizes bins that are a tight fit, penalizing those with large gaps.\"\n        # The `v1` penalty was `1.0 / (normalized_excess + 0.1)`. This means higher penalty for *smaller* normalized excess.\n        # This is confusing. Let's assume the intent was to penalize larger gaps.\n\n        # Let's try a simple heuristic inspired by First Fit Decreasing or Best Fit Decreasing principles,\n        # adapted for online. We want to pack items efficiently.\n\n        # Strategy:\n        # 1. Prioritize the \"tightest\" fit (like Best Fit).\n        # 2. Among tight fits, prefer bins that are \"more full\" after packing.\n        #    \"More full\" means smaller remaining capacity overall.\n\n        # Metric 1: Tightness of fit.\n        # `tightness = 1.0 / (suitable_bins_remain_cap - item + epsilon)`\n\n        # Metric 2: Fill ratio of remaining space.\n        # `fill_ratio = item / suitable_bins_remain_cap`\n\n        # Let's combine these multiplicatively, as it inherently balances both.\n        # `score = tightness * fill_ratio`\n        # `score = (1.0 / (suitable_bins_remain_cap - item + epsilon)) * (item / (suitable_bins_remain_cap + epsilon))`\n        # This is the `efficiency_score`.\n\n        # Let's consider the self-reflection points:\n        # - Precision: The `efficiency_score` is precise in valuing both tightness and fill.\n        # - Adaptability: This score adapts to item sizes and available bin capacities.\n        # - Explainability: The components (tightness, fill ratio) are understandable.\n        # - Performance: It aims to reduce bin count.\n\n        # Let's think about edge cases for `efficiency_score`:\n        # If `suitable_bins_remain_cap` is very small and `item` is close to it:\n        #   `item=4`, `rem=5`. Score = `1/(5-4) * 4/5 = 1 * 0.8 = 0.8`.\n        # If `item=4`, `rem=10`. Score = `1/(10-4) * 4/10 = 1/6 * 0.4 = 0.066`.\n        # This seems to correctly prioritize the tighter fit.\n\n        # If `item=1`, `rem=100`. Score = `1/(100-1) * 1/100 = 1/99 * 0.01 ~ 0.0001`.\n        # This correctly penalizes putting a small item into a vast bin.\n\n        # What if we want to explicitly promote filling bins that are *already* mostly full?\n        # We need information about the bin's initial state. Since we don't have it,\n        # we can only infer from `suitable_bins_remain_cap`.\n        # A small `suitable_bins_remain_cap` suggests the bin was likely quite full to begin with.\n        # The `efficiency_score` already implicitly favors bins with smaller `suitable_bins_remain_cap`.\n\n        # Consider the \"best fit\" criteria carefully.\n        # Best Fit: Minimize `suitable_bins_remain_cap - item`.\n        # This means `suitable_bins_remain_cap` should be just slightly larger than `item`.\n\n        # Let's directly use `suitable_bins_remain_cap` as the primary sorting key,\n        # and `suitable_bins_remain_cap - item` as a tie-breaker or secondary key.\n        # But this is for sorting. We need a score.\n\n        # Let's go with the `efficiency_score` and consider if it can be improved.\n        # The formula `item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`\n        # aims to maximize both how much of the current space is used (`item/remaining`)\n        # and how little is wasted (`1/(remaining-item)`).\n\n        # Alternative formulation:\n        # We want to minimize `suitable_bins_remain_cap`.\n        # And we want to minimize `suitable_bins_remain_cap - item`.\n        # This means we want `suitable_bins_remain_cap` to be close to `item`.\n\n        # Consider the \"waste\" `W = suitable_bins_remain_cap - item`. We want to minimize W.\n        # Consider the \"usage\" `U = item / suitable_bins_remain_cap`. We want to maximize U.\n\n        # Let's scale `U` and `1/W` (or similar for W=0) to combine them.\n        # Max `U` is 1. Max `1/W` can be infinite.\n\n        # Let's reconsider the v1 penalty: \"penalty for bins with too much excess capacity\".\n        # The idea was to avoid large unused space *after* packing.\n        # Let `slack = suitable_bins_remain_cap - item`.\n        # We want to penalize large `slack`.\n        # A penalty function `P(slack)`.\n        # `score = best_fit_score * f(slack)` where `f(slack)` decreases as `slack` increases.\n        # `best_fit_score = 1.0 / (slack + epsilon)`\n\n        # If `f(slack) = 1.0 / (slack + epsilon)`\n        # `score = (1.0 / (slack + epsilon)) * (1.0 / (slack + epsilon)) = 1.0 / (slack + epsilon)^2`\n        # This strongly favors very tight fits.\n\n        # If `f(slack) = 1.0 / (1 + slack)`\n        # `score = (1.0 / (slack + epsilon)) * (1.0 / (1 + slack))`\n        # `= 1.0 / ((slack + epsilon) * (1 + slack))`\n        # This is similar to the `efficiency_score` if `suitable_bins_remain_cap` is roughly constant.\n\n        # Let's try to modify `efficiency_score` to better reflect \"avoiding large remaining space\".\n        # `efficiency_score = item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`\n\n        # Consider the case: item = 10\n        # Bin A: rem = 12. slack = 2. efficiency_score = 10 / (12 * 2) = 10/24 ~ 0.416\n        # Bin B: rem = 50. slack = 40. efficiency_score = 10 / (50 * 40) = 10/2000 = 0.005\n\n        # This score penalizes Bin B heavily due to its large `suitable_bins_remain_cap`.\n        # This is precisely the behavior we want to achieve with \"penalizing large gaps\".\n\n        # Let's evaluate `v1`'s score for comparison with this `efficiency_score`.\n        # `v1` `inverse_remaining` = `1.0 / (suitable_bins_remain_cap - item + 1e-9)`\n        # `v1` `normalized_excess` = `(suitable_bins_remain_cap - item) / (max_suitable_cap - item)`\n        # `v1` `penalty` = `1.0 / (normalized_excess + 0.1)`\n        # `v1` score = `inverse_remaining * penalty`\n\n        # Example for `v1`: item = 10\n        # Suitable bins: rem=[12, 50, 15]\n        # suitable_bins_remain_cap = [12, 50, 15]\n        # max_suitable_cap = 50\n\n        # Bin 1 (rem=12):\n        # inverse_remaining = 1/(12-10) = 0.5\n        # normalized_excess = (12-10) / (50-10) = 2 / 40 = 0.05\n        # penalty = 1 / (0.05 + 0.1) = 1 / 0.15 ~ 6.67\n        # score1 = 0.5 * 6.67 = 3.335\n\n        # Bin 2 (rem=50):\n        # inverse_remaining = 1/(50-10) = 0.025\n        # normalized_excess = (50-10) / (50-10) = 1.0\n        # penalty = 1 / (1.0 + 0.1) = 1 / 1.1 ~ 0.909\n        # score2 = 0.025 * 0.909 = 0.0227\n\n        # Bin 3 (rem=15):\n        # inverse_remaining = 1/(15-10) = 0.2\n        # normalized_excess = (15-10) / (50-10) = 5 / 40 = 0.125\n        # penalty = 1 / (0.125 + 0.1) = 1 / 0.225 ~ 4.44\n        # score3 = 0.2 * 4.44 = 0.888\n\n        # v1 priorities: [3.335, 0.0227, 0.888] -> Bin 1 (rem=12) is highest.\n\n        # Let's check `efficiency_score` for the same: item=10, rem=[12, 50, 15]\n        # Bin 1 (rem=12): efficiency_score = 10 / (12 * (12-10)) = 10 / (12 * 2) = 10/24 ~ 0.416\n        # Bin 2 (rem=50): efficiency_score = 10 / (50 * (50-10)) = 10 / (50 * 40) = 10/2000 = 0.005\n        # Bin 3 (rem=15): efficiency_score = 10 / (15 * (15-10)) = 10 / (15 * 5) = 10/75 ~ 0.133\n\n        # efficiency_score priorities: [0.416, 0.005, 0.133] -> Bin 1 (rem=12) is highest.\n\n        # Both prioritize Bin 1 (rem=12) which is the tightest fit.\n        # `v1`'s penalty mechanism is trying to dampen scores for bins with large excess capacity (like rem=50).\n        # `efficiency_score` achieves this by having `suitable_bins_remain_cap` in the denominator's factors.\n\n        # The `efficiency_score` seems to be a good candidate. Let's refine it slightly.\n        # The \"penalty\" in `v1` aimed to make scores more comparable and to avoid extremely high scores from extremely tight fits that might be rare.\n        # The `efficiency_score` can lead to very high values if `suitable_bins_remain_cap - item` is tiny.\n\n        # Let's add a cap or normalize the `efficiency_score` to avoid issues.\n        # Or, introduce a smoother penalty for large remaining capacities.\n\n        # Consider the `fill_ratio = item / suitable_bins_remain_cap`.\n        # And `slack_ratio = (suitable_bins_remain_cap - item) / suitable_bins_remain_cap`. We want to minimize this.\n        # So, we want to maximize `1.0 - slack_ratio = item / suitable_bins_remain_cap` (which is `fill_ratio`).\n\n        # Let's try a combination that explicitly penalizes the absolute remaining capacity,\n        # while also rewarding tightness.\n        # Score = `(best_fit_score) * (fill_ratio)`  -> `efficiency_score`\n        # Score = `(best_fit_score) / (suitable_bins_remain_cap)` -> `1.0 / ((suitable_bins_remain_cap - item + epsilon) * suitable_bins_remain_cap)`\n\n        # Let's use the `efficiency_score` and add a term that boosts bins with smaller remaining capacity.\n        # `score = efficiency_score + bonus_weight * (1.0 / (suitable_bins_remain_cap + epsilon))`\n        # This adds a \"fill bonus\".\n\n        # Let's check this new score:\n        # Bin 1 (rem=12): efficiency = 0.416. bonus = 1/12 ~ 0.083. Total = 0.416 + w * 0.083\n        # Bin 3 (rem=15): efficiency = 0.133. bonus = 1/15 ~ 0.066. Total = 0.133 + w * 0.066\n        # Bin 2 (rem=50): efficiency = 0.005. bonus = 1/50 = 0.02. Total = 0.005 + w * 0.02\n\n        # If w=1:\n        # Bin 1: 0.416 + 0.083 = 0.499\n        # Bin 3: 0.133 + 0.066 = 0.199\n        # Bin 2: 0.005 + 0.02 = 0.025\n        # Bin 1 is still highest.\n\n        # If w=10:\n        # Bin 1: 0.416 + 0.83 = 1.246\n        # Bin 3: 0.133 + 0.66 = 0.793\n        # Bin 2: 0.005 + 0.2 = 0.205\n        # Bin 1 is still highest.\n\n        # The `efficiency_score` itself already strongly favors smaller `suitable_bins_remain_cap` because it's in the denominator.\n        # Adding `1.0 / suitable_bins_remain_cap` might be redundant or might over-emphasize small remaining capacities.\n\n        # Let's stick with the `efficiency_score` as a solid baseline that balances tightness and fill.\n        # `efficiency_score = item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`\n\n        # A final consideration: What if the item is very small compared to the bin capacity?\n        # item = 1, rem = 100.\n        # efficiency_score = 1 / (100 * 99) = 1 / 9900 ~ 0.0001\n        # This correctly gives a low score.\n\n        # What if the item is large and the bin has just enough space?\n        # item = 99, rem = 100.\n        # efficiency_score = 99 / (100 * 1) = 0.99\n        # This gives a high score, which is good.\n\n        # Let's make it more \"human-readable\" and perhaps slightly more robust by considering the proportion of capacity used.\n        # `priority = (item / suitable_bins_remain_cap) * (1.0 / (suitable_bins_remain_cap - item + epsilon))`\n        # This is `fill_ratio * tightness`.\n\n        # Let's consider a slightly different approach that explicitly penalizes slack.\n        # Penalize `suitable_bins_remain_cap - item`.\n        # Let `slack = suitable_bins_remain_cap - item`.\n        # Score = `1.0 / (slack + epsilon)` (best fit)\n        # Penalty factor = `1.0 / (1.0 + slack)` (penalizes slack)\n        # `score = (1.0 / (slack + epsilon)) * (1.0 / (1.0 + slack))`\n        # `score = 1.0 / ((slack + epsilon) * (1.0 + slack))`\n\n        # Let's test this: item=10\n        # Bin 1 (rem=12): slack=2. Score = 1 / (2 * 3) = 1/6 ~ 0.166\n        # Bin 3 (rem=15): slack=5. Score = 1 / (5 * 6) = 1/30 ~ 0.033\n        # Bin 2 (rem=50): slack=40. Score = 1 / (40 * 41) = 1/1640 ~ 0.0006\n\n        # This new score prioritizes Bin 1 (rem=12) but much less strongly than the `efficiency_score`.\n        # The `efficiency_score` `item / (suitable_bins_remain_cap * slack)`\n        # Bin 1 (rem=12): 10 / (12 * 2) = 10/24 ~ 0.416\n        # Bin 3 (rem=15): 10 / (15 * 5) = 10/75 ~ 0.133\n        # Bin 2 (rem=50): 10 / (50 * 40) = 10/2000 = 0.005\n\n        # The `efficiency_score` seems to better reflect the dual goal of tight fit and good space utilization by\n        # considering how much of the *remaining capacity* is taken.\n\n        # Let's refine `efficiency_score` calculation to ensure robustness and clarity.\n        # `suitable_bins_remain_cap - item` is the slack.\n        # `item / suitable_bins_remain_cap` is the fill ratio.\n\n        # Final candidate score: `(item / (suitable_bins_remain_cap + 1e-9)) * (1.0 / (suitable_bins_remain_cap - item + 1e-9))`\n        # This is `fill_ratio * tightness`. It's intuitive and captures both goals.\n        # It penalizes large remaining capacities due to the `suitable_bins_remain_cap` in the fill ratio denominator.\n        # It rewards tight fits due to the `suitable_bins_remain_cap - item` in the tightness denominator.\n\n        # Let's consider the scenario where `suitable_bins_remain_cap` is very large, and `item` is small.\n        # E.g., item=1, rem=1000.\n        # Fill ratio = 1/1000 = 0.001\n        # Tightness = 1/(1000-1) = 1/999 ~ 0.001\n        # Score = 0.001 * 0.001 = 0.000001. Very low, as desired.\n\n        # E.g., item=999, rem=1000.\n        # Fill ratio = 999/1000 = 0.999\n        # Tightness = 1/(1000-999) = 1/1 = 1.0\n        # Score = 0.999 * 1.0 = 0.999. Very high, as desired.\n\n        # This `fill_ratio * tightness` score seems robust and captures the desired behavior.\n        # It's also more \"explainable\" than some complex penalty functions.\n\n        # Let's ensure the data types are handled correctly.\n        # `item` is float. `bins_remain_cap` is np.ndarray.\n        # The calculations will result in floats.\n\n        priorities[suitable_bin_indices] = (item / (suitable_bins_remain_cap + 1e-9)) * \\\n                                           (1.0 / (suitable_bins_remain_cap - item + 1e-9))\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit (tightness) with a sigmoid for prioritizing near-exact fits.\n\n    Prioritizes bins that are almost full but can fit the item, using a sigmoid\n    to smooth the preference for tighter fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[fit_mask]\n\n    # Heuristic: Prioritize bins that are \"almost full\" but can still fit the item.\n    # This is inspired by Best Fit, but uses a sigmoid to give a smoother preference\n    # to bins where remaining_capacity - item is small.\n    # The input to the sigmoid is scaled such that tighter fits result in a higher score.\n    # We use -(valid_bins_remain_cap - item) to make smaller remaining space\n    # correspond to larger (less negative) sigmoid inputs.\n\n    # A simple scaling to avoid extreme sigmoid values too quickly.\n    # The range of (bins_remain_cap - item) can vary. Let's normalize it.\n    # For bins that fit, the \"slack\" is valid_bins_remain_cap - item.\n    # We want to prioritize smaller slack.\n    slack = valid_bins_remain_cap - item\n\n    # Normalize slack to be between 0 and 1 for sigmoid input.\n    # If all slack is the same, avoid division by zero.\n    if slack.size > 0:\n        min_slack = np.min(slack)\n        max_slack = np.max(slack)\n\n        if max_slack == min_slack:\n            normalized_slack = np.zeros_like(slack)\n        else:\n            # Map slack to a range where sigmoid can differentiate well.\n            # We want smaller slack to map to a higher priority.\n            # So, map min_slack (tightest fit) to a high sigmoid input,\n            # and max_slack (loosest fit) to a low sigmoid input.\n            # Consider the inverse of slack: 1 / (slack + epsilon) is similar to Best Fit.\n            # Let's use a transformation like: 1 - (slack / max_slack) or similar.\n            # A sigmoid on -(slack) might be good: larger negative means smaller slack.\n            # sigmoid_input = -slack\n            # To control steepness and range, we can use:\n            steepness = 5.0 # Tune this parameter\n            # We want smaller slack to give higher priority.\n            # So, we want a higher value when slack is small.\n            # Transform slack to a value that is higher for smaller slack.\n            # Example: max_slack - slack. Then normalize.\n            transformed_slack = max_slack - slack\n            if max_slack - min_slack > 0:\n                normalized_transformed_slack = transformed_slack / (max_slack - min_slack)\n            else:\n                normalized_transformed_slack = np.zeros_like(slack)\n\n            # Use sigmoid on the transformed slack. High transformed_slack (low original slack)\n            # should map to a high sigmoid output.\n            # We can use `steepness * (normalized_transformed_slack - 0.5)` to center around 0.5.\n            sigmoid_input = steepness * (normalized_transformed_slack - 0.5)\n            priorities[fit_mask] = 1 / (1 + np.exp(-sigmoid_input))\n        \n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins that are an exact fit, then uses an inverse remaining capacity\n    approach for other suitable bins, balancing precision and general effectiveness.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    # Exact fit priority (highest)\n    exact_fit_mask = np.isclose(bins_remain_cap, item)\n    priorities[exact_fit_mask] = 1e9 # Assign a very high priority for exact fits\n    \n    # Best fit for non-exact fits\n    non_exact_suitable_mask = suitable_bins_mask & ~exact_fit_mask\n    \n    if np.any(non_exact_suitable_mask):\n        remaining_capacities_for_non_exact = bins_remain_cap[non_exact_suitable_mask] - item\n        # Using inverse remaining capacity as a measure of \"best fit\"\n        priorities[non_exact_suitable_mask] = 1.0 / (remaining_capacities_for_non_exact + 1e-9)\n        \n    # Normalize priorities for non-exact fits to avoid overpowering exact fits\n    # and to make the best-fit values relative among themselves.\n    # We don't normalize the exact fit priorities as they are intended to be dominant.\n    if np.any(non_exact_suitable_mask):\n        non_exact_priorities = priorities[non_exact_suitable_mask]\n        if np.sum(non_exact_priorities) > 0:\n            priorities[non_exact_suitable_mask] = non_exact_priorities / np.sum(non_exact_priorities)\n\n    # If no bins are suitable, all priorities remain 0.\n    \n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins that are an exact fit, then uses an inverse remaining capacity\n    approach for other suitable bins, balancing precision and general effectiveness.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    # Exact fit priority (highest)\n    exact_fit_mask = np.isclose(bins_remain_cap, item)\n    priorities[exact_fit_mask] = 1e9 # Assign a very high priority for exact fits\n    \n    # Best fit for non-exact fits\n    non_exact_suitable_mask = suitable_bins_mask & ~exact_fit_mask\n    \n    if np.any(non_exact_suitable_mask):\n        remaining_capacities_for_non_exact = bins_remain_cap[non_exact_suitable_mask] - item\n        # Using inverse remaining capacity as a measure of \"best fit\"\n        priorities[non_exact_suitable_mask] = 1.0 / (remaining_capacities_for_non_exact + 1e-9)\n        \n    # Normalize priorities for non-exact fits to avoid overpowering exact fits\n    # and to make the best-fit values relative among themselves.\n    # We don't normalize the exact fit priorities as they are intended to be dominant.\n    if np.any(non_exact_suitable_mask):\n        non_exact_priorities = priorities[non_exact_suitable_mask]\n        if np.sum(non_exact_priorities) > 0:\n            priorities[non_exact_suitable_mask] = non_exact_priorities / np.sum(non_exact_priorities)\n\n    # If no bins are suitable, all priorities remain 0.\n    \n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Aims to improve upon v1 by introducing a more dynamic penalty that\n    considers the item's size relative to the bin's capacity and a\n    more nuanced \"best fit\" calculation. It also prioritizes filling bins\n    more completely when they are already significantly filled, promoting\n    the use of fewer bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    # More robust \"best fit\" score: prioritizing bins that leave the smallest remaining capacity.\n    # We invert the difference `remaining_capacity - item`. Adding a small epsilon\n    # to the denominator avoids division by zero and ensures bins that are an exact fit\n    # get a high score.\n    best_fit_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Dynamic penalty for \"slack\" or excess capacity.\n    # The penalty is higher for bins that have a lot of remaining capacity\n    # *relative to the item size*. This discourages placing small items\n    # into bins that have a very large remaining capacity, saving those\n    # large capacities for larger items.\n    # We define \"slack\" as the ratio of remaining capacity to bin capacity,\n    # but we only consider the capacity *beyond* what's needed for the item.\n    # A smaller ratio of \"excess capacity\" (remaining capacity - item) to the\n    # bin's original capacity is preferred.\n    # Using a sigmoid-like function centered around 0.5 (meaning half-full is neutral)\n    # penalizes bins that are very empty (slack close to 1) and also bins that are\n    # almost full but still have a little room (slack close to 0, but we want to\n    # prioritize those that are *almost* full to finish a bin).\n\n    # Calculate the \"slack\" ratio: (remaining_capacity - item) / original_capacity\n    # We use the original capacity before fitting the item as a reference.\n    # A very small remaining capacity (after fitting) implies a good fit.\n    # We want to penalize large *unused* capacity.\n    # Let's consider `(bin_capacity - item)` as the \"excess\" after fitting.\n    # The penalty should be higher if `excess_capacity` is large.\n\n    # We can also introduce a factor that rewards filling bins that are already\n    # substantially full. If a bin is already more than 50% full, we might\n    # give it a slight boost to encourage completing it.\n\n    # Let's try a penalty based on the *normalized remaining capacity* after fitting.\n    # Higher remaining capacity after fitting should be penalized more.\n    # Normalize the remaining capacity (`suitable_bins_remain_cap - item`)\n    # by the maximum possible remaining capacity among suitable bins.\n    max_possible_gap = np.max(suitable_bins_remain_cap - item)\n    if max_possible_gap == 0:\n        normalized_gap = np.zeros_like(suitable_bins_remain_cap)\n    else:\n        normalized_gap = (suitable_bins_remain_cap - item) / max_possible_gap\n\n    # Penalty: Higher penalty for larger normalized gap.\n    # Using 1 - normalized_gap means a small gap gets a high score, and a large gap gets a low score.\n    # We want to penalize large gaps, so a lower score for large gaps is a penalty.\n    # Let's invert this: higher penalty value for larger gaps.\n    # So, penalty = normalized_gap (higher is worse).\n    # We want to combine this with best_fit_score. A high best_fit_score is good.\n    # A high penalty value is bad. So we should subtract the penalty or divide by it.\n\n    # Let's re-evaluate the penalty. We want to penalize bins with *too much* excess capacity.\n    # This means bins where `suitable_bins_remain_cap - item` is large.\n    # We can use a function that grows with `suitable_bins_remain_cap - item`.\n    # Let's consider the *proportion of capacity remaining* after placing the item.\n    # `(suitable_bins_remain_cap - item) / suitable_bins_remain_cap`\n    # However, if `suitable_bins_remain_cap` is just slightly larger than `item`,\n    # this proportion can be very small.\n\n    # New approach for penalty:\n    # We want to penalize bins where `suitable_bins_remain_cap` is much larger than `item`.\n    # Let's focus on `suitable_bins_remain_cap`.\n    # Higher `suitable_bins_remain_cap` should have a penalty.\n    # Let's use a transformation of `suitable_bins_remain_cap` that increases as it grows.\n    # A simple inverse might penalize *all* bins with remaining capacity.\n\n    # Instead of a penalty, let's directly reward filling bins.\n    # If a bin is already quite full, placing an item that fits well is very good.\n    # Let's consider the \"fullness\" of the bin *before* placing the item.\n    # A bin that is already > 50% full should get a slight bonus for being filled further.\n\n    # Calculate a \"fill ratio\" for each suitable bin relative to its *current* remaining capacity.\n    # This isn't quite right. We need to consider the *original* capacity if available.\n    # Without original capacity, we can only work with `bins_remain_cap`.\n\n    # Let's refine the penalty based on the *slack*: `suitable_bins_remain_cap - item`.\n    # We want to penalize large slack.\n    # Let's use a logistic function to map the slack to a penalty factor.\n    # We want a penalty that increases as slack increases.\n    # Let `slack = suitable_bins_remain_cap - item`.\n    # A reasonable maximum slack could be considered the largest remaining capacity among suitable bins.\n    max_slack = np.max(suitable_bins_remain_cap - item)\n    if max_slack < 1e-9: # All suitable bins are exact fits\n        slack_penalty_factor = np.zeros_like(suitable_bins_remain_cap)\n    else:\n        normalized_slack = (suitable_bins_remain_cap - item) / max_slack\n        # Penalty increases with normalized slack.\n        # Use a function that is 0 for slack=0 and increases.\n        # A simple linear relationship or a slightly more aggressive function.\n        # Let's use a function that gives higher penalty for slack > average slack.\n        # Option 1: `normalized_slack` itself (linear penalty)\n        # Option 2: `np.exp(normalized_slack)` (exponential penalty)\n        # Option 3: A piecewise function or a sigmoid that penalizes > threshold.\n\n        # Let's try a combination:\n        # Reward tight fits (high `best_fit_score`).\n        # Penalize large residual capacity (`suitable_bins_remain_cap - item`).\n        # A bin that is almost full (small `suitable_bins_remain_cap`) but can fit the item,\n        # should be highly prioritized if the item fits tightly.\n\n        # Consider the ratio `item / suitable_bins_remain_cap`.\n        # Higher ratio is good (item takes up a larger portion of remaining space).\n        fill_ratio = item / (suitable_bins_remain_cap + 1e-9)\n\n        # Now, let's combine `best_fit_score` and `fill_ratio`.\n        # Both should be maximized.\n        # `best_fit_score` is `1 / (remaining - item)`.\n        # `fill_ratio` is `item / remaining`.\n\n        # If `remaining - item` is small, `best_fit_score` is large.\n        # If `item / remaining` is large, `fill_ratio` is large.\n\n        # Let's try a weighted sum, where `best_fit_score` has a higher weight,\n        # but `fill_ratio` acts as a secondary criterion to break ties or to\n        # favor bins that will be more \"full\" after packing.\n\n        # Priority = `best_fit_score` + `w * fill_ratio`\n        # However, the scales can be very different.\n\n        # Alternative: Multiply.\n        # Priority = `best_fit_score` * `fill_ratio`\n        # This means we want both a tight fit AND a good fill ratio.\n        # If `fill_ratio` is very small (item is tiny compared to remaining space),\n        # the score will be low, even if it's a tight fit. This is good.\n\n        # Let's consider the \"efficiency\" of the bin after packing.\n        # Efficiency = `item / original_bin_capacity`.\n        # We don't have original bin capacity.\n        # Let's use `item / (item + slack)` which is `item / suitable_bins_remain_cap`.\n\n        # Let's make the `best_fit_score` more robust to very small capacities.\n        # Instead of `1 / (remaining - item)`, consider `(remaining - item) / remaining`.\n        # This is the proportion of remaining capacity that is *not* used.\n        # We want to minimize this proportion.\n        # So, `1 - (remaining - item) / remaining` = `item / remaining`. This is `fill_ratio`.\n\n        # So, `fill_ratio = item / suitable_bins_remain_cap` is a good metric for how much\n        # of the available space the item occupies.\n\n        # Let's reconsider `v1`'s goal: \"tight fit\" and \"penalty for excess capacity\".\n        # `best_fit_score` addresses tight fit.\n        # The penalty part in `v1` was `1.0 / (normalized_excess + 0.1)`.\n        # `normalized_excess` was `(suitable_bins_remain_cap - item) / (max_suitable_cap - item)`.\n        # This means `penalty` was high for small excess and low for large excess. This is inverted.\n        # The combination was `inverse_remaining * penalty`. `inverse_remaining` was `1/(remaining-item)`.\n\n        # Let's create a metric that is high for tight fits AND for bins that are \"almost full\".\n        # Consider the value `suitable_bins_remain_cap`.\n        # We want to select a bin where `suitable_bins_remain_cap` is small, but still `>= item`.\n        # This means `suitable_bins_remain_cap` should be as close to `item` as possible.\n\n        # Let's try a score that is `1.0 / suitable_bins_remain_cap`. This favors smaller remaining capacities.\n        # And combine it with a measure of how \"tight\" the fit is.\n        # Tight fit can be measured by `1.0 / (suitable_bins_remain_cap - item + epsilon)`.\n\n        # Let's try a score that directly prioritizes bins with low remaining capacity *after* fitting the item.\n        # `priority = 1.0 / (suitable_bins_remain_cap - item + epsilon)` (This is `best_fit_score`).\n        # This already prioritizes tight fits.\n\n        # How to penalize excess capacity more effectively?\n        # We want to reduce the priority of bins where `suitable_bins_remain_cap` is large,\n        # even if they are suitable.\n\n        # Let's use a function that decreases as `suitable_bins_remain_cap` increases.\n        # Example: `exp(-k * suitable_bins_remain_cap)` where `k` is a tuning parameter.\n        # Or, normalize `suitable_bins_remain_cap` by the maximum *possible* remaining capacity\n        # (which is bin_capacity - smallest_item). Without bin_capacity, we can normalize\n        # by the maximum `suitable_bins_remain_cap` found.\n\n        # Let's consider the \"waste\" `suitable_bins_remain_cap - item`.\n        # We want to penalize large waste.\n        # Let `waste = suitable_bins_remain_cap - item`.\n        # Penalty function `P(waste)`. We want `P(waste)` to increase with `waste`.\n        # `P(waste) = waste^2` or `exp(waste)`.\n\n        # Let's combine `best_fit_score` with a penalty inversely related to `suitable_bins_remain_cap`.\n        # Combined score: `best_fit_score * (1.0 / (suitable_bins_remain_cap + epsilon))`\n        # This might over-penalize bins with moderate remaining capacity.\n\n        # Let's use the `fill_ratio = item / suitable_bins_remain_cap` as a base metric.\n        # Higher fill ratio is better.\n        # Now, let's refine this with the tightness of the fit.\n        # If the fit is very tight (`suitable_bins_remain_cap - item` is small), this is very good.\n        # If the fit is loose (`suitable_bins_remain_cap - item` is large), this is less good.\n\n        # Consider a score that is:\n        # `fill_ratio`  -> How much of the available space is utilized.\n        # `tightness`   -> How close `remaining` is to `item`.\n        # Let `tightness = 1.0 / (suitable_bins_remain_cap - item + epsilon)`\n\n        # Combined: `fill_ratio * tightness = (item / suitable_bins_remain_cap) * (1.0 / (suitable_bins_remain_cap - item + epsilon))`\n        # This looks promising. It prioritizes bins where the item is a large fraction of the remaining space\n        # AND the remaining space after fitting is small.\n\n        # Let's analyze this product:\n        # `item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + epsilon)`\n        # If `suitable_bins_remain_cap` is slightly larger than `item` (tight fit, high `fill_ratio`),\n        # the denominator term `suitable_bins_remain_cap - item` is small, leading to a large score.\n        # If `suitable_bins_remain_cap` is much larger than `item`, the `fill_ratio` becomes small,\n        # reducing the overall score, even if `suitable_bins_remain_cap - item` is small relative to `suitable_bins_remain_cap`.\n\n        # This seems to capture both aspects well.\n        # Let's call this the \"efficiency score\".\n\n        efficiency_score = item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)\n\n        # This score can become very large if `suitable_bins_remain_cap - item` is tiny.\n        # Normalizing might be good, but it might also dampen the strong preference for tight fits.\n        # Let's consider scaling or bounding the score if necessary, but start with this direct approach.\n\n        # To make it more robust and less sensitive to extreme values, we can add a small constant\n        # to the denominator to avoid infinities if `suitable_bins_remain_cap` is zero (though\n        # `suitable_bins_remain_cap >= item` should prevent this if `item > 0`).\n        # The `1e-9` is already there.\n\n        # Consider the edge case where `suitable_bins_remain_cap == item`.\n        # Then `suitable_bins_remain_cap - item` is 0, leading to infinity.\n        # This means exact fits get infinitely high priority. This is generally good.\n\n        # However, `v1` tried to penalize bins with *too much* excess capacity.\n        # If we have two bins:\n        # Bin A: remaining_cap = 10, item = 9. Score ~ 9 / (10 * 1) = 0.9\n        # Bin B: remaining_cap = 2, item = 1. Score ~ 1 / (2 * 1) = 0.5\n\n        # What if we have:\n        # Bin C: remaining_cap = 100, item = 99. Score ~ 99 / (100 * 1) = 0.99\n        # Bin D: remaining_cap = 5, item = 4. Score ~ 4 / (5 * 1) = 0.8\n\n        # This seems to favor the \"absolute\" tightest fits, not necessarily the most efficient use of space when capacities differ widely.\n        # If Bin C (100 capacity, 99 remaining) and Bin D (5 capacity, 4 remaining) are available for item=4:\n        # Bin C: remaining_cap = 100, item = 4. Score ~ 4 / (100 * 96) = 4 / 9600 ~ 0.0004\n        # Bin D: remaining_cap = 5, item = 4. Score ~ 4 / (5 * 1) = 0.8\n        # This is good: Bin D is prioritized.\n\n        # If item = 99:\n        # Bin C: remaining_cap = 100, item = 99. Score ~ 99 / (100 * 1) = 0.99\n        # Bin D: remaining_cap = 5, item = 99. (Not suitable)\n        # Bin C is prioritized.\n\n        # What if we have:\n        # Bin E: remaining_cap = 10, item = 9. Score ~ 9 / (10 * 1) = 0.9\n        # Bin F: remaining_cap = 20, item = 18. Score ~ 18 / (20 * 2) = 18 / 40 = 0.45\n\n        # This suggests the current `efficiency_score` prioritizes the *absolute* tightness of fit.\n        # `v1` tried to penalize \"too much excess capacity\".\n\n        # Let's try to incorporate a penalty for large absolute remaining capacity.\n        # `penalty_factor = 1.0 / (suitable_bins_remain_cap + epsilon)`\n        # Combining `efficiency_score * penalty_factor`:\n        # `[item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + epsilon)] * [1.0 / (suitable_bins_remain_cap + epsilon)]`\n        # `= item / (suitable_bins_remain_cap^2 * (suitable_bins_remain_cap - item) + epsilon)`\n        # This will strongly penalize larger remaining capacities.\n\n        # Let's call this `refined_score`.\n        refined_score = item / (suitable_bins_remain_cap**2 * (suitable_bins_remain_cap - item) + 1e-9)\n\n        # Consider its behavior:\n        # Bin E: remaining=10, item=9. Score ~ 9 / (100 * 1) = 0.09\n        # Bin F: remaining=20, item=18. Score ~ 18 / (400 * 2) = 18 / 800 = 0.0225\n\n        # This penalizes Bin E less than F, which seems to align with favoring tighter fits.\n        # If we want to penalize bins with *large* remaining capacity, this `refined_score` does that.\n        # It strongly emphasizes reducing `suitable_bins_remain_cap` in the denominator.\n\n        # Let's rethink the \"penalty for bins with too much excess capacity\" from v1.\n        # The goal was to avoid putting small items in large, nearly empty bins.\n        # This means we want to *reduce* the priority of bins where `suitable_bins_remain_cap` is large.\n\n        # Let's use a score that is a balance between best fit and minimizing remaining capacity.\n        # Best fit: `1.0 / (suitable_bins_remain_cap - item + epsilon)`\n        # Minimize remaining capacity: `1.0 / (suitable_bins_remain_cap + epsilon)`\n\n        # Option 1: Average of scores\n        # `(1.0 / (suitable_bins_remain_cap - item + epsilon) + 1.0 / (suitable_bins_remain_cap + epsilon)) / 2.0`\n        # This combines the two ideas directly.\n\n        # Option 2: Weighted average, or product.\n        # Product: `(1.0 / (suitable_bins_remain_cap - item + epsilon)) * (1.0 / (suitable_bins_remain_cap + epsilon))`\n        # `= 1.0 / ((suitable_bins_remain_cap - item + epsilon) * (suitable_bins_remain_cap + epsilon))`\n        # This also heavily favors small `suitable_bins_remain_cap`.\n\n        # Let's reconsider the `fill_ratio = item / suitable_bins_remain_cap`.\n        # This is good because it directly relates to how \"full\" the bin will be.\n        # And `tightness = 1.0 / (suitable_bins_remain_cap - item + epsilon)`.\n        # Product: `(item / (suitable_bins_remain_cap + epsilon)) * (1.0 / (suitable_bins_remain_cap - item + epsilon))`\n        # This is the `efficiency_score` we derived earlier.\n\n        # How about a modification to `v1`'s penalty?\n        # `v1` penalty was `1.0 / (normalized_excess + 0.1)`.\n        # Normalized excess was `(suitable_bins_remain_cap - item) / (max_suitable_cap - item)`.\n        # So `v1` penalty was high for small normalized excess, low for large.\n        # It multiplied `inverse_remaining` by this penalty.\n        # `priorities = (1/(remaining-item)) * (1/(norm_excess + 0.1))`\n        # This means it favored small `remaining-item` and small `norm_excess`.\n\n        # Let's try to make the penalty more direct: penalize large `suitable_bins_remain_cap`.\n        # `score = best_fit_score / (suitable_bins_remain_cap + epsilon)`\n        # `score = (1.0 / (suitable_bins_remain_cap - item + epsilon)) / (suitable_bins_remain_cap + epsilon)`\n        # `= 1.0 / ((suitable_bins_remain_cap - item + epsilon) * (suitable_bins_remain_cap + epsilon))`\n        # This is the product we saw before.\n\n        # Let's try a different combination strategy.\n        # Prioritize bins that are a tight fit (`best_fit_score`).\n        # Then, among tight fits, prefer those that are \"more full\" relative to their capacity.\n        # `fill_ratio = item / suitable_bins_remain_cap`\n\n        # How to combine `best_fit_score` and `fill_ratio`?\n        # `score = best_fit_score + weight * fill_ratio` ?\n        # This can be problematic due to scale.\n\n        # Let's go back to `efficiency_score = item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`\n        # This score is high when `item` is large relative to `suitable_bins_remain_cap` AND `suitable_bins_remain_cap` is close to `item`.\n\n        # Consider the objective: minimize number of bins.\n        # This means we want to fill bins as much as possible, and prefer tighter fits to leave\n        # more room in other bins for future items.\n\n        # Let's add a term that explicitly favors filling bins that are already substantialy full.\n        # A bin with `suitable_bins_remain_cap < BIN_CAPACITY / 2` (assuming BIN_CAPACITY is known, which it is not here).\n        # Without BIN_CAPACITY, we can use `suitable_bins_remain_cap` relative to `item`.\n\n        # Let's define a \"completion bonus\" for bins that are nearly full after placing the item.\n        # If `suitable_bins_remain_cap - item` is very small, it's a good fill.\n        # Let's add a score proportional to `1.0 / (suitable_bins_remain_cap + epsilon)`.\n        # This rewards smaller remaining capacities.\n\n        # Combined Score = `efficiency_score` + `bonus_weight * (1.0 / (suitable_bins_remain_cap + epsilon))`\n        # `efficiency_score` already incorporates `1.0 / (suitable_bins_remain_cap - item + epsilon)`.\n\n        # Let's try a different penalty approach for excess capacity.\n        # We want to reduce the score if `suitable_bins_remain_cap` is large compared to `item`.\n        # Let `excess_ratio = suitable_bins_remain_cap / item`.\n        # We want to penalize large `excess_ratio`.\n        # Penalty: `1.0 / (excess_ratio + epsilon)` or `exp(-k * excess_ratio)`.\n        # Let's use `1.0 / (suitable_bins_remain_cap / item + epsilon)` which is `item / (suitable_bins_remain_cap + epsilon)`.\n        # This is the `fill_ratio` we defined earlier.\n\n        # So, we want to maximize `best_fit_score` and `fill_ratio`.\n        # `best_fit_score = 1.0 / (suitable_bins_remain_cap - item + epsilon)`\n        # `fill_ratio = item / (suitable_bins_remain_cap + epsilon)`\n\n        # Let's consider the product: `best_fit_score * fill_ratio`.\n        # This is the `efficiency_score`.\n\n        # Let's consider another angle: penalize the *total* remaining capacity.\n        # We have `best_fit_score` that favors small `remaining - item`.\n        # We want to additionally favor smaller `remaining`.\n\n        # Let's try using the `best_fit_score` and applying a penalty to it.\n        # Penalty for large remaining capacity: `suitable_bins_remain_cap`.\n        # Penalty factor should decrease as `suitable_bins_remain_cap` increases.\n        # e.g., `exp(-k * suitable_bins_remain_cap)`.\n        # `score = best_fit_score * exp(-k * suitable_bins_remain_cap)`\n        # `score = (1.0 / (suitable_bins_remain_cap - item + epsilon)) * exp(-k * suitable_bins_remain_cap)`\n\n        # Let's choose `k` such that it scales appropriately.\n        # If `suitable_bins_remain_cap` is, say, 10 times `item`, we might want to penalize it.\n        # Let's pick `k` so that if `suitable_bins_remain_cap` is twice the average `suitable_bins_remain_cap`,\n        # the exponential term is significantly reduced.\n\n        # Instead of exponential, let's use a simpler penalty that's easier to tune.\n        # Penalty factor: `1.0 / (1.0 + alpha * (suitable_bins_remain_cap / item))` where alpha is a tuning parameter.\n        # This penalizes bins where remaining capacity is much larger than item.\n        # Penalty factor = `item / (item + alpha * suitable_bins_remain_cap)`\n\n        # Let's combine: `best_fit_score * (item / (item + alpha * suitable_bins_remain_cap))`\n        # `score = (1.0 / (suitable_bins_remain_cap - item + epsilon)) * (item / (item + alpha * suitable_bins_remain_cap))`\n\n        # Example: item=10\n        # Bin A: rem=12. best_fit=1/(12-10)=0.5. penalty=(10/(10+a*12)). Score=0.5 * 10/(10+12a) = 5/(10+12a)\n        # Bin B: rem=20. best_fit=1/(20-10)=0.1. penalty=(10/(10+a*20)). Score=0.1 * 10/(10+20a) = 1/(10+20a)\n        # Bin C: rem=15. best_fit=1/(15-10)=0.2. penalty=(10/(10+a*15)). Score=0.2 * 10/(10+15a) = 2/(10+15a)\n\n        # If a=0 (no penalty):\n        # A: 0.5\n        # B: 0.1\n        # C: 0.2\n        # Bin A (tightest fit) is best.\n\n        # If a=1:\n        # A: 5 / (10+12) = 5/22 ~ 0.227\n        # B: 1 / (10+20) = 1/30 ~ 0.033\n        # C: 2 / (10+15) = 2/25 = 0.08\n        # Bin A is still best. This penalty isn't strongly altering the order unless 'a' is large.\n\n        # The core idea of `v1` was \"Prioritizes bins that are a tight fit, penalizing those with large gaps.\"\n        # The `v1` penalty was `1.0 / (normalized_excess + 0.1)`. This means higher penalty for *smaller* normalized excess.\n        # This is confusing. Let's assume the intent was to penalize larger gaps.\n\n        # Let's try a simple heuristic inspired by First Fit Decreasing or Best Fit Decreasing principles,\n        # adapted for online. We want to pack items efficiently.\n\n        # Strategy:\n        # 1. Prioritize the \"tightest\" fit (like Best Fit).\n        # 2. Among tight fits, prefer bins that are \"more full\" after packing.\n        #    \"More full\" means smaller remaining capacity overall.\n\n        # Metric 1: Tightness of fit.\n        # `tightness = 1.0 / (suitable_bins_remain_cap - item + epsilon)`\n\n        # Metric 2: Fill ratio of remaining space.\n        # `fill_ratio = item / suitable_bins_remain_cap`\n\n        # Let's combine these multiplicatively, as it inherently balances both.\n        # `score = tightness * fill_ratio`\n        # `score = (1.0 / (suitable_bins_remain_cap - item + epsilon)) * (item / (suitable_bins_remain_cap + epsilon))`\n        # This is the `efficiency_score`.\n\n        # Let's consider the self-reflection points:\n        # - Precision: The `efficiency_score` is precise in valuing both tightness and fill.\n        # - Adaptability: This score adapts to item sizes and available bin capacities.\n        # - Explainability: The components (tightness, fill ratio) are understandable.\n        # - Performance: It aims to reduce bin count.\n\n        # Let's think about edge cases for `efficiency_score`:\n        # If `suitable_bins_remain_cap` is very small and `item` is close to it:\n        #   `item=4`, `rem=5`. Score = `1/(5-4) * 4/5 = 1 * 0.8 = 0.8`.\n        # If `item=4`, `rem=10`. Score = `1/(10-4) * 4/10 = 1/6 * 0.4 = 0.066`.\n        # This seems to correctly prioritize the tighter fit.\n\n        # If `item=1`, `rem=100`. Score = `1/(100-1) * 1/100 = 1/99 * 0.01 ~ 0.0001`.\n        # This correctly penalizes putting a small item into a vast bin.\n\n        # What if we want to explicitly promote filling bins that are *already* mostly full?\n        # We need information about the bin's initial state. Since we don't have it,\n        # we can only infer from `suitable_bins_remain_cap`.\n        # A small `suitable_bins_remain_cap` suggests the bin was likely quite full to begin with.\n        # The `efficiency_score` already implicitly favors bins with smaller `suitable_bins_remain_cap`.\n\n        # Consider the \"best fit\" criteria carefully.\n        # Best Fit: Minimize `suitable_bins_remain_cap - item`.\n        # This means `suitable_bins_remain_cap` should be just slightly larger than `item`.\n\n        # Let's directly use `suitable_bins_remain_cap` as the primary sorting key,\n        # and `suitable_bins_remain_cap - item` as a tie-breaker or secondary key.\n        # But this is for sorting. We need a score.\n\n        # Let's go with the `efficiency_score` and consider if it can be improved.\n        # The formula `item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`\n        # aims to maximize both how much of the current space is used (`item/remaining`)\n        # and how little is wasted (`1/(remaining-item)`).\n\n        # Alternative formulation:\n        # We want to minimize `suitable_bins_remain_cap`.\n        # And we want to minimize `suitable_bins_remain_cap - item`.\n        # This means we want `suitable_bins_remain_cap` to be close to `item`.\n\n        # Consider the \"waste\" `W = suitable_bins_remain_cap - item`. We want to minimize W.\n        # Consider the \"usage\" `U = item / suitable_bins_remain_cap`. We want to maximize U.\n\n        # Let's scale `U` and `1/W` (or similar for W=0) to combine them.\n        # Max `U` is 1. Max `1/W` can be infinite.\n\n        # Let's reconsider the v1 penalty: \"penalty for bins with too much excess capacity\".\n        # The idea was to avoid large unused space *after* packing.\n        # Let `slack = suitable_bins_remain_cap - item`.\n        # We want to penalize large `slack`.\n        # A penalty function `P(slack)`.\n        # `score = best_fit_score * f(slack)` where `f(slack)` decreases as `slack` increases.\n        # `best_fit_score = 1.0 / (slack + epsilon)`\n\n        # If `f(slack) = 1.0 / (slack + epsilon)`\n        # `score = (1.0 / (slack + epsilon)) * (1.0 / (slack + epsilon)) = 1.0 / (slack + epsilon)^2`\n        # This strongly favors very tight fits.\n\n        # If `f(slack) = 1.0 / (1 + slack)`\n        # `score = (1.0 / (slack + epsilon)) * (1.0 / (1 + slack))`\n        # `= 1.0 / ((slack + epsilon) * (1 + slack))`\n        # This is similar to the `efficiency_score` if `suitable_bins_remain_cap` is roughly constant.\n\n        # Let's try to modify `efficiency_score` to better reflect \"avoiding large remaining space\".\n        # `efficiency_score = item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`\n\n        # Consider the case: item = 10\n        # Bin A: rem = 12. slack = 2. efficiency_score = 10 / (12 * 2) = 10/24 ~ 0.416\n        # Bin B: rem = 50. slack = 40. efficiency_score = 10 / (50 * 40) = 10/2000 = 0.005\n\n        # This score penalizes Bin B heavily due to its large `suitable_bins_remain_cap`.\n        # This is precisely the behavior we want to achieve with \"penalizing large gaps\".\n\n        # Let's evaluate `v1`'s score for comparison with this `efficiency_score`.\n        # `v1` `inverse_remaining` = `1.0 / (suitable_bins_remain_cap - item + 1e-9)`\n        # `v1` `normalized_excess` = `(suitable_bins_remain_cap - item) / (max_suitable_cap - item)`\n        # `v1` `penalty` = `1.0 / (normalized_excess + 0.1)`\n        # `v1` score = `inverse_remaining * penalty`\n\n        # Example for `v1`: item = 10\n        # Suitable bins: rem=[12, 50, 15]\n        # suitable_bins_remain_cap = [12, 50, 15]\n        # max_suitable_cap = 50\n\n        # Bin 1 (rem=12):\n        # inverse_remaining = 1/(12-10) = 0.5\n        # normalized_excess = (12-10) / (50-10) = 2 / 40 = 0.05\n        # penalty = 1 / (0.05 + 0.1) = 1 / 0.15 ~ 6.67\n        # score1 = 0.5 * 6.67 = 3.335\n\n        # Bin 2 (rem=50):\n        # inverse_remaining = 1/(50-10) = 0.025\n        # normalized_excess = (50-10) / (50-10) = 1.0\n        # penalty = 1 / (1.0 + 0.1) = 1 / 1.1 ~ 0.909\n        # score2 = 0.025 * 0.909 = 0.0227\n\n        # Bin 3 (rem=15):\n        # inverse_remaining = 1/(15-10) = 0.2\n        # normalized_excess = (15-10) / (50-10) = 5 / 40 = 0.125\n        # penalty = 1 / (0.125 + 0.1) = 1 / 0.225 ~ 4.44\n        # score3 = 0.2 * 4.44 = 0.888\n\n        # v1 priorities: [3.335, 0.0227, 0.888] -> Bin 1 (rem=12) is highest.\n\n        # Let's check `efficiency_score` for the same: item=10, rem=[12, 50, 15]\n        # Bin 1 (rem=12): efficiency_score = 10 / (12 * (12-10)) = 10 / (12 * 2) = 10/24 ~ 0.416\n        # Bin 2 (rem=50): efficiency_score = 10 / (50 * (50-10)) = 10 / (50 * 40) = 10/2000 = 0.005\n        # Bin 3 (rem=15): efficiency_score = 10 / (15 * (15-10)) = 10 / (15 * 5) = 10/75 ~ 0.133\n\n        # efficiency_score priorities: [0.416, 0.005, 0.133] -> Bin 1 (rem=12) is highest.\n\n        # Both prioritize Bin 1 (rem=12) which is the tightest fit.\n        # `v1`'s penalty mechanism is trying to dampen scores for bins with large excess capacity (like rem=50).\n        # `efficiency_score` achieves this by having `suitable_bins_remain_cap` in the denominator's factors.\n\n        # The `efficiency_score` seems to be a good candidate. Let's refine it slightly.\n        # The \"penalty\" in `v1` aimed to make scores more comparable and to avoid extremely high scores from extremely tight fits that might be rare.\n        # The `efficiency_score` can lead to very high values if `suitable_bins_remain_cap - item` is tiny.\n\n        # Let's add a cap or normalize the `efficiency_score` to avoid issues.\n        # Or, introduce a smoother penalty for large remaining capacities.\n\n        # Consider the `fill_ratio = item / suitable_bins_remain_cap`.\n        # And `slack_ratio = (suitable_bins_remain_cap - item) / suitable_bins_remain_cap`. We want to minimize this.\n        # So, we want to maximize `1.0 - slack_ratio = item / suitable_bins_remain_cap` (which is `fill_ratio`).\n\n        # Let's try a combination that explicitly penalizes the absolute remaining capacity,\n        # while also rewarding tightness.\n        # Score = `(best_fit_score) * (fill_ratio)`  -> `efficiency_score`\n        # Score = `(best_fit_score) / (suitable_bins_remain_cap)` -> `1.0 / ((suitable_bins_remain_cap - item + epsilon) * suitable_bins_remain_cap)`\n\n        # Let's use the `efficiency_score` and add a term that boosts bins with smaller remaining capacity.\n        # `score = efficiency_score + bonus_weight * (1.0 / (suitable_bins_remain_cap + epsilon))`\n        # This adds a \"fill bonus\".\n\n        # Let's check this new score:\n        # Bin 1 (rem=12): efficiency = 0.416. bonus = 1/12 ~ 0.083. Total = 0.416 + w * 0.083\n        # Bin 3 (rem=15): efficiency = 0.133. bonus = 1/15 ~ 0.066. Total = 0.133 + w * 0.066\n        # Bin 2 (rem=50): efficiency = 0.005. bonus = 1/50 = 0.02. Total = 0.005 + w * 0.02\n\n        # If w=1:\n        # Bin 1: 0.416 + 0.083 = 0.499\n        # Bin 3: 0.133 + 0.066 = 0.199\n        # Bin 2: 0.005 + 0.02 = 0.025\n        # Bin 1 is still highest.\n\n        # If w=10:\n        # Bin 1: 0.416 + 0.83 = 1.246\n        # Bin 3: 0.133 + 0.66 = 0.793\n        # Bin 2: 0.005 + 0.2 = 0.205\n        # Bin 1 is still highest.\n\n        # The `efficiency_score` itself already strongly favors smaller `suitable_bins_remain_cap` because it's in the denominator.\n        # Adding `1.0 / suitable_bins_remain_cap` might be redundant or might over-emphasize small remaining capacities.\n\n        # Let's stick with the `efficiency_score` as a solid baseline that balances tightness and fill.\n        # `efficiency_score = item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`\n\n        # A final consideration: What if the item is very small compared to the bin capacity?\n        # item = 1, rem = 100.\n        # efficiency_score = 1 / (100 * 99) = 1 / 9900 ~ 0.0001\n        # This correctly gives a low score.\n\n        # What if the item is large and the bin has just enough space?\n        # item = 99, rem = 100.\n        # efficiency_score = 99 / (100 * 1) = 0.99\n        # This gives a high score, which is good.\n\n        # Let's make it more \"human-readable\" and perhaps slightly more robust by considering the proportion of capacity used.\n        # `priority = (item / suitable_bins_remain_cap) * (1.0 / (suitable_bins_remain_cap - item + epsilon))`\n        # This is `fill_ratio * tightness`.\n\n        # Let's consider a slightly different approach that explicitly penalizes slack.\n        # Penalize `suitable_bins_remain_cap - item`.\n        # Let `slack = suitable_bins_remain_cap - item`.\n        # Score = `1.0 / (slack + epsilon)` (best fit)\n        # Penalty factor = `1.0 / (1.0 + slack)` (penalizes slack)\n        # `score = (1.0 / (slack + epsilon)) * (1.0 / (1.0 + slack))`\n        # `score = 1.0 / ((slack + epsilon) * (1.0 + slack))`\n\n        # Let's test this: item=10\n        # Bin 1 (rem=12): slack=2. Score = 1 / (2 * 3) = 1/6 ~ 0.166\n        # Bin 3 (rem=15): slack=5. Score = 1 / (5 * 6) = 1/30 ~ 0.033\n        # Bin 2 (rem=50): slack=40. Score = 1 / (40 * 41) = 1/1640 ~ 0.0006\n\n        # This new score prioritizes Bin 1 (rem=12) but much less strongly than the `efficiency_score`.\n        # The `efficiency_score` `item / (suitable_bins_remain_cap * slack)`\n        # Bin 1 (rem=12): 10 / (12 * 2) = 10/24 ~ 0.416\n        # Bin 3 (rem=15): 10 / (15 * 5) = 10/75 ~ 0.133\n        # Bin 2 (rem=50): 10 / (50 * 40) = 10/2000 = 0.005\n\n        # The `efficiency_score` seems to better reflect the dual goal of tight fit and good space utilization by\n        # considering how much of the *remaining capacity* is taken.\n\n        # Let's refine `efficiency_score` calculation to ensure robustness and clarity.\n        # `suitable_bins_remain_cap - item` is the slack.\n        # `item / suitable_bins_remain_cap` is the fill ratio.\n\n        # Final candidate score: `(item / (suitable_bins_remain_cap + 1e-9)) * (1.0 / (suitable_bins_remain_cap - item + 1e-9))`\n        # This is `fill_ratio * tightness`. It's intuitive and captures both goals.\n        # It penalizes large remaining capacities due to the `suitable_bins_remain_cap` in the fill ratio denominator.\n        # It rewards tight fits due to the `suitable_bins_remain_cap - item` in the tightness denominator.\n\n        # Let's consider the scenario where `suitable_bins_remain_cap` is very large, and `item` is small.\n        # E.g., item=1, rem=1000.\n        # Fill ratio = 1/1000 = 0.001\n        # Tightness = 1/(1000-1) = 1/999 ~ 0.001\n        # Score = 0.001 * 0.001 = 0.000001. Very low, as desired.\n\n        # E.g., item=999, rem=1000.\n        # Fill ratio = 999/1000 = 0.999\n        # Tightness = 1/(1000-999) = 1/1 = 1.0\n        # Score = 0.999 * 1.0 = 0.999. Very high, as desired.\n\n        # This `fill_ratio * tightness` score seems robust and captures the desired behavior.\n        # It's also more \"explainable\" than some complex penalty functions.\n\n        # Let's ensure the data types are handled correctly.\n        # `item` is float. `bins_remain_cap` is np.ndarray.\n        # The calculations will result in floats.\n\n        priorities[suitable_bin_indices] = (item / (suitable_bins_remain_cap + 1e-9)) * \\\n                                           (1.0 / (suitable_bins_remain_cap - item + 1e-9))\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Aims to improve upon v1 by introducing a more dynamic penalty that\n    considers the item's size relative to the bin's capacity and a\n    more nuanced \"best fit\" calculation. It also prioritizes filling bins\n    more completely when they are already significantly filled, promoting\n    the use of fewer bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    # More robust \"best fit\" score: prioritizing bins that leave the smallest remaining capacity.\n    # We invert the difference `remaining_capacity - item`. Adding a small epsilon\n    # to the denominator avoids division by zero and ensures bins that are an exact fit\n    # get a high score.\n    best_fit_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Dynamic penalty for \"slack\" or excess capacity.\n    # The penalty is higher for bins that have a lot of remaining capacity\n    # *relative to the item size*. This discourages placing small items\n    # into bins that have a very large remaining capacity, saving those\n    # large capacities for larger items.\n    # We define \"slack\" as the ratio of remaining capacity to bin capacity,\n    # but we only consider the capacity *beyond* what's needed for the item.\n    # A smaller ratio of \"excess capacity\" (remaining capacity - item) to the\n    # bin's original capacity is preferred.\n    # Using a sigmoid-like function centered around 0.5 (meaning half-full is neutral)\n    # penalizes bins that are very empty (slack close to 1) and also bins that are\n    # almost full but still have a little room (slack close to 0, but we want to\n    # prioritize those that are *almost* full to finish a bin).\n\n    # Calculate the \"slack\" ratio: (remaining_capacity - item) / original_capacity\n    # We use the original capacity before fitting the item as a reference.\n    # A very small remaining capacity (after fitting) implies a good fit.\n    # We want to penalize large *unused* capacity.\n    # Let's consider `(bin_capacity - item)` as the \"excess\" after fitting.\n    # The penalty should be higher if `excess_capacity` is large.\n\n    # We can also introduce a factor that rewards filling bins that are already\n    # substantially full. If a bin is already more than 50% full, we might\n    # give it a slight boost to encourage completing it.\n\n    # Let's try a penalty based on the *normalized remaining capacity* after fitting.\n    # Higher remaining capacity after fitting should be penalized more.\n    # Normalize the remaining capacity (`suitable_bins_remain_cap - item`)\n    # by the maximum possible remaining capacity among suitable bins.\n    max_possible_gap = np.max(suitable_bins_remain_cap - item)\n    if max_possible_gap == 0:\n        normalized_gap = np.zeros_like(suitable_bins_remain_cap)\n    else:\n        normalized_gap = (suitable_bins_remain_cap - item) / max_possible_gap\n\n    # Penalty: Higher penalty for larger normalized gap.\n    # Using 1 - normalized_gap means a small gap gets a high score, and a large gap gets a low score.\n    # We want to penalize large gaps, so a lower score for large gaps is a penalty.\n    # Let's invert this: higher penalty value for larger gaps.\n    # So, penalty = normalized_gap (higher is worse).\n    # We want to combine this with best_fit_score. A high best_fit_score is good.\n    # A high penalty value is bad. So we should subtract the penalty or divide by it.\n\n    # Let's re-evaluate the penalty. We want to penalize bins with *too much* excess capacity.\n    # This means bins where `suitable_bins_remain_cap - item` is large.\n    # We can use a function that grows with `suitable_bins_remain_cap - item`.\n    # Let's consider the *proportion of capacity remaining* after placing the item.\n    # `(suitable_bins_remain_cap - item) / suitable_bins_remain_cap`\n    # However, if `suitable_bins_remain_cap` is just slightly larger than `item`,\n    # this proportion can be very small.\n\n    # New approach for penalty:\n    # We want to penalize bins where `suitable_bins_remain_cap` is much larger than `item`.\n    # Let's focus on `suitable_bins_remain_cap`.\n    # Higher `suitable_bins_remain_cap` should have a penalty.\n    # Let's use a transformation of `suitable_bins_remain_cap` that increases as it grows.\n    # A simple inverse might penalize *all* bins with remaining capacity.\n\n    # Instead of a penalty, let's directly reward filling bins.\n    # If a bin is already quite full, placing an item that fits well is very good.\n    # Let's consider the \"fullness\" of the bin *before* placing the item.\n    # A bin that is already > 50% full should get a slight bonus for being filled further.\n\n    # Calculate a \"fill ratio\" for each suitable bin relative to its *current* remaining capacity.\n    # This isn't quite right. We need to consider the *original* capacity if available.\n    # Without original capacity, we can only work with `bins_remain_cap`.\n\n    # Let's refine the penalty based on the *slack*: `suitable_bins_remain_cap - item`.\n    # We want to penalize large slack.\n    # Let's use a logistic function to map the slack to a penalty factor.\n    # We want a penalty that increases as slack increases.\n    # Let `slack = suitable_bins_remain_cap - item`.\n    # A reasonable maximum slack could be considered the largest remaining capacity among suitable bins.\n    max_slack = np.max(suitable_bins_remain_cap - item)\n    if max_slack < 1e-9: # All suitable bins are exact fits\n        slack_penalty_factor = np.zeros_like(suitable_bins_remain_cap)\n    else:\n        normalized_slack = (suitable_bins_remain_cap - item) / max_slack\n        # Penalty increases with normalized slack.\n        # Use a function that is 0 for slack=0 and increases.\n        # A simple linear relationship or a slightly more aggressive function.\n        # Let's use a function that gives higher penalty for slack > average slack.\n        # Option 1: `normalized_slack` itself (linear penalty)\n        # Option 2: `np.exp(normalized_slack)` (exponential penalty)\n        # Option 3: A piecewise function or a sigmoid that penalizes > threshold.\n\n        # Let's try a combination:\n        # Reward tight fits (high `best_fit_score`).\n        # Penalize large residual capacity (`suitable_bins_remain_cap - item`).\n        # A bin that is almost full (small `suitable_bins_remain_cap`) but can fit the item,\n        # should be highly prioritized if the item fits tightly.\n\n        # Consider the ratio `item / suitable_bins_remain_cap`.\n        # Higher ratio is good (item takes up a larger portion of remaining space).\n        fill_ratio = item / (suitable_bins_remain_cap + 1e-9)\n\n        # Now, let's combine `best_fit_score` and `fill_ratio`.\n        # Both should be maximized.\n        # `best_fit_score` is `1 / (remaining - item)`.\n        # `fill_ratio` is `item / remaining`.\n\n        # If `remaining - item` is small, `best_fit_score` is large.\n        # If `item / remaining` is large, `fill_ratio` is large.\n\n        # Let's try a weighted sum, where `best_fit_score` has a higher weight,\n        # but `fill_ratio` acts as a secondary criterion to break ties or to\n        # favor bins that will be more \"full\" after packing.\n\n        # Priority = `best_fit_score` + `w * fill_ratio`\n        # However, the scales can be very different.\n\n        # Alternative: Multiply.\n        # Priority = `best_fit_score` * `fill_ratio`\n        # This means we want both a tight fit AND a good fill ratio.\n        # If `fill_ratio` is very small (item is tiny compared to remaining space),\n        # the score will be low, even if it's a tight fit. This is good.\n\n        # Let's consider the \"efficiency\" of the bin after packing.\n        # Efficiency = `item / original_bin_capacity`.\n        # We don't have original bin capacity.\n        # Let's use `item / (item + slack)` which is `item / suitable_bins_remain_cap`.\n\n        # Let's make the `best_fit_score` more robust to very small capacities.\n        # Instead of `1 / (remaining - item)`, consider `(remaining - item) / remaining`.\n        # This is the proportion of remaining capacity that is *not* used.\n        # We want to minimize this proportion.\n        # So, `1 - (remaining - item) / remaining` = `item / remaining`. This is `fill_ratio`.\n\n        # So, `fill_ratio = item / suitable_bins_remain_cap` is a good metric for how much\n        # of the available space the item occupies.\n\n        # Let's reconsider `v1`'s goal: \"tight fit\" and \"penalty for excess capacity\".\n        # `best_fit_score` addresses tight fit.\n        # The penalty part in `v1` was `1.0 / (normalized_excess + 0.1)`.\n        # `normalized_excess` was `(suitable_bins_remain_cap - item) / (max_suitable_cap - item)`.\n        # This means `penalty` was high for small excess and low for large excess. This is inverted.\n        # The combination was `inverse_remaining * penalty`. `inverse_remaining` was `1/(remaining-item)`.\n\n        # Let's create a metric that is high for tight fits AND for bins that are \"almost full\".\n        # Consider the value `suitable_bins_remain_cap`.\n        # We want to select a bin where `suitable_bins_remain_cap` is small, but still `>= item`.\n        # This means `suitable_bins_remain_cap` should be as close to `item` as possible.\n\n        # Let's try a score that is `1.0 / suitable_bins_remain_cap`. This favors smaller remaining capacities.\n        # And combine it with a measure of how \"tight\" the fit is.\n        # Tight fit can be measured by `1.0 / (suitable_bins_remain_cap - item + epsilon)`.\n\n        # Let's try a score that directly prioritizes bins with low remaining capacity *after* fitting the item.\n        # `priority = 1.0 / (suitable_bins_remain_cap - item + epsilon)` (This is `best_fit_score`).\n        # This already prioritizes tight fits.\n\n        # How to penalize excess capacity more effectively?\n        # We want to reduce the priority of bins where `suitable_bins_remain_cap` is large,\n        # even if they are suitable.\n\n        # Let's use a function that decreases as `suitable_bins_remain_cap` increases.\n        # Example: `exp(-k * suitable_bins_remain_cap)` where `k` is a tuning parameter.\n        # Or, normalize `suitable_bins_remain_cap` by the maximum *possible* remaining capacity\n        # (which is bin_capacity - smallest_item). Without bin_capacity, we can normalize\n        # by the maximum `suitable_bins_remain_cap` found.\n\n        # Let's consider the \"waste\" `suitable_bins_remain_cap - item`.\n        # We want to penalize large waste.\n        # Let `waste = suitable_bins_remain_cap - item`.\n        # Penalty function `P(waste)`. We want `P(waste)` to increase with `waste`.\n        # `P(waste) = waste^2` or `exp(waste)`.\n\n        # Let's combine `best_fit_score` with a penalty inversely related to `suitable_bins_remain_cap`.\n        # Combined score: `best_fit_score * (1.0 / (suitable_bins_remain_cap + epsilon))`\n        # This might over-penalize bins with moderate remaining capacity.\n\n        # Let's use the `fill_ratio = item / suitable_bins_remain_cap` as a base metric.\n        # Higher fill ratio is better.\n        # Now, let's refine this with the tightness of the fit.\n        # If the fit is very tight (`suitable_bins_remain_cap - item` is small), this is very good.\n        # If the fit is loose (`suitable_bins_remain_cap - item` is large), this is less good.\n\n        # Consider a score that is:\n        # `fill_ratio`  -> How much of the available space is utilized.\n        # `tightness`   -> How close `remaining` is to `item`.\n        # Let `tightness = 1.0 / (suitable_bins_remain_cap - item + epsilon)`\n\n        # Combined: `fill_ratio * tightness = (item / suitable_bins_remain_cap) * (1.0 / (suitable_bins_remain_cap - item + epsilon))`\n        # This looks promising. It prioritizes bins where the item is a large fraction of the remaining space\n        # AND the remaining space after fitting is small.\n\n        # Let's analyze this product:\n        # `item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + epsilon)`\n        # If `suitable_bins_remain_cap` is slightly larger than `item` (tight fit, high `fill_ratio`),\n        # the denominator term `suitable_bins_remain_cap - item` is small, leading to a large score.\n        # If `suitable_bins_remain_cap` is much larger than `item`, the `fill_ratio` becomes small,\n        # reducing the overall score, even if `suitable_bins_remain_cap - item` is small relative to `suitable_bins_remain_cap`.\n\n        # This seems to capture both aspects well.\n        # Let's call this the \"efficiency score\".\n\n        efficiency_score = item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)\n\n        # This score can become very large if `suitable_bins_remain_cap - item` is tiny.\n        # Normalizing might be good, but it might also dampen the strong preference for tight fits.\n        # Let's consider scaling or bounding the score if necessary, but start with this direct approach.\n\n        # To make it more robust and less sensitive to extreme values, we can add a small constant\n        # to the denominator to avoid infinities if `suitable_bins_remain_cap` is zero (though\n        # `suitable_bins_remain_cap >= item` should prevent this if `item > 0`).\n        # The `1e-9` is already there.\n\n        # Consider the edge case where `suitable_bins_remain_cap == item`.\n        # Then `suitable_bins_remain_cap - item` is 0, leading to infinity.\n        # This means exact fits get infinitely high priority. This is generally good.\n\n        # However, `v1` tried to penalize bins with *too much* excess capacity.\n        # If we have two bins:\n        # Bin A: remaining_cap = 10, item = 9. Score ~ 9 / (10 * 1) = 0.9\n        # Bin B: remaining_cap = 2, item = 1. Score ~ 1 / (2 * 1) = 0.5\n\n        # What if we have:\n        # Bin C: remaining_cap = 100, item = 99. Score ~ 99 / (100 * 1) = 0.99\n        # Bin D: remaining_cap = 5, item = 4. Score ~ 4 / (5 * 1) = 0.8\n\n        # This seems to favor the \"absolute\" tightest fits, not necessarily the most efficient use of space when capacities differ widely.\n        # If Bin C (100 capacity, 99 remaining) and Bin D (5 capacity, 4 remaining) are available for item=4:\n        # Bin C: remaining_cap = 100, item = 4. Score ~ 4 / (100 * 96) = 4 / 9600 ~ 0.0004\n        # Bin D: remaining_cap = 5, item = 4. Score ~ 4 / (5 * 1) = 0.8\n        # This is good: Bin D is prioritized.\n\n        # If item = 99:\n        # Bin C: remaining_cap = 100, item = 99. Score ~ 99 / (100 * 1) = 0.99\n        # Bin D: remaining_cap = 5, item = 99. (Not suitable)\n        # Bin C is prioritized.\n\n        # What if we have:\n        # Bin E: remaining_cap = 10, item = 9. Score ~ 9 / (10 * 1) = 0.9\n        # Bin F: remaining_cap = 20, item = 18. Score ~ 18 / (20 * 2) = 18 / 40 = 0.45\n\n        # This suggests the current `efficiency_score` prioritizes the *absolute* tightness of fit.\n        # `v1` tried to penalize \"too much excess capacity\".\n\n        # Let's try to incorporate a penalty for large absolute remaining capacity.\n        # `penalty_factor = 1.0 / (suitable_bins_remain_cap + epsilon)`\n        # Combining `efficiency_score * penalty_factor`:\n        # `[item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + epsilon)] * [1.0 / (suitable_bins_remain_cap + epsilon)]`\n        # `= item / (suitable_bins_remain_cap^2 * (suitable_bins_remain_cap - item) + epsilon)`\n        # This will strongly penalize larger remaining capacities.\n\n        # Let's call this `refined_score`.\n        refined_score = item / (suitable_bins_remain_cap**2 * (suitable_bins_remain_cap - item) + 1e-9)\n\n        # Consider its behavior:\n        # Bin E: remaining=10, item=9. Score ~ 9 / (100 * 1) = 0.09\n        # Bin F: remaining=20, item=18. Score ~ 18 / (400 * 2) = 18 / 800 = 0.0225\n\n        # This penalizes Bin E less than F, which seems to align with favoring tighter fits.\n        # If we want to penalize bins with *large* remaining capacity, this `refined_score` does that.\n        # It strongly emphasizes reducing `suitable_bins_remain_cap` in the denominator.\n\n        # Let's rethink the \"penalty for bins with too much excess capacity\" from v1.\n        # The goal was to avoid putting small items in large, nearly empty bins.\n        # This means we want to *reduce* the priority of bins where `suitable_bins_remain_cap` is large.\n\n        # Let's use a score that is a balance between best fit and minimizing remaining capacity.\n        # Best fit: `1.0 / (suitable_bins_remain_cap - item + epsilon)`\n        # Minimize remaining capacity: `1.0 / (suitable_bins_remain_cap + epsilon)`\n\n        # Option 1: Average of scores\n        # `(1.0 / (suitable_bins_remain_cap - item + epsilon) + 1.0 / (suitable_bins_remain_cap + epsilon)) / 2.0`\n        # This combines the two ideas directly.\n\n        # Option 2: Weighted average, or product.\n        # Product: `(1.0 / (suitable_bins_remain_cap - item + epsilon)) * (1.0 / (suitable_bins_remain_cap + epsilon))`\n        # `= 1.0 / ((suitable_bins_remain_cap - item + epsilon) * (suitable_bins_remain_cap + epsilon))`\n        # This also heavily favors small `suitable_bins_remain_cap`.\n\n        # Let's reconsider the `fill_ratio = item / suitable_bins_remain_cap`.\n        # This is good because it directly relates to how \"full\" the bin will be.\n        # And `tightness = 1.0 / (suitable_bins_remain_cap - item + epsilon)`.\n        # Product: `(item / (suitable_bins_remain_cap + epsilon)) * (1.0 / (suitable_bins_remain_cap - item + epsilon))`\n        # This is the `efficiency_score` we derived earlier.\n\n        # How about a modification to `v1`'s penalty?\n        # `v1` penalty was `1.0 / (normalized_excess + 0.1)`.\n        # Normalized excess was `(suitable_bins_remain_cap - item) / (max_suitable_cap - item)`.\n        # So `v1` penalty was high for small normalized excess, low for large.\n        # It multiplied `inverse_remaining` by this penalty.\n        # `priorities = (1/(remaining-item)) * (1/(norm_excess + 0.1))`\n        # This means it favored small `remaining-item` and small `norm_excess`.\n\n        # Let's try to make the penalty more direct: penalize large `suitable_bins_remain_cap`.\n        # `score = best_fit_score / (suitable_bins_remain_cap + epsilon)`\n        # `score = (1.0 / (suitable_bins_remain_cap - item + epsilon)) / (suitable_bins_remain_cap + epsilon)`\n        # `= 1.0 / ((suitable_bins_remain_cap - item + epsilon) * (suitable_bins_remain_cap + epsilon))`\n        # This is the product we saw before.\n\n        # Let's try a different combination strategy.\n        # Prioritize bins that are a tight fit (`best_fit_score`).\n        # Then, among tight fits, prefer those that are \"more full\" relative to their capacity.\n        # `fill_ratio = item / suitable_bins_remain_cap`\n\n        # How to combine `best_fit_score` and `fill_ratio`?\n        # `score = best_fit_score + weight * fill_ratio` ?\n        # This can be problematic due to scale.\n\n        # Let's go back to `efficiency_score = item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`\n        # This score is high when `item` is large relative to `suitable_bins_remain_cap` AND `suitable_bins_remain_cap` is close to `item`.\n\n        # Consider the objective: minimize number of bins.\n        # This means we want to fill bins as much as possible, and prefer tighter fits to leave\n        # more room in other bins for future items.\n\n        # Let's add a term that explicitly favors filling bins that are already substantialy full.\n        # A bin with `suitable_bins_remain_cap < BIN_CAPACITY / 2` (assuming BIN_CAPACITY is known, which it is not here).\n        # Without BIN_CAPACITY, we can use `suitable_bins_remain_cap` relative to `item`.\n\n        # Let's define a \"completion bonus\" for bins that are nearly full after placing the item.\n        # If `suitable_bins_remain_cap - item` is very small, it's a good fill.\n        # Let's add a score proportional to `1.0 / (suitable_bins_remain_cap + epsilon)`.\n        # This rewards smaller remaining capacities.\n\n        # Combined Score = `efficiency_score` + `bonus_weight * (1.0 / (suitable_bins_remain_cap + epsilon))`\n        # `efficiency_score` already incorporates `1.0 / (suitable_bins_remain_cap - item + epsilon)`.\n\n        # Let's try a different penalty approach for excess capacity.\n        # We want to reduce the score if `suitable_bins_remain_cap` is large compared to `item`.\n        # Let `excess_ratio = suitable_bins_remain_cap / item`.\n        # We want to penalize large `excess_ratio`.\n        # Penalty: `1.0 / (excess_ratio + epsilon)` or `exp(-k * excess_ratio)`.\n        # Let's use `1.0 / (suitable_bins_remain_cap / item + epsilon)` which is `item / (suitable_bins_remain_cap + epsilon)`.\n        # This is the `fill_ratio` we defined earlier.\n\n        # So, we want to maximize `best_fit_score` and `fill_ratio`.\n        # `best_fit_score = 1.0 / (suitable_bins_remain_cap - item + epsilon)`\n        # `fill_ratio = item / (suitable_bins_remain_cap + epsilon)`\n\n        # Let's consider the product: `best_fit_score * fill_ratio`.\n        # This is the `efficiency_score`.\n\n        # Let's consider another angle: penalize the *total* remaining capacity.\n        # We have `best_fit_score` that favors small `remaining - item`.\n        # We want to additionally favor smaller `remaining`.\n\n        # Let's try using the `best_fit_score` and applying a penalty to it.\n        # Penalty for large remaining capacity: `suitable_bins_remain_cap`.\n        # Penalty factor should decrease as `suitable_bins_remain_cap` increases.\n        # e.g., `exp(-k * suitable_bins_remain_cap)`.\n        # `score = best_fit_score * exp(-k * suitable_bins_remain_cap)`\n        # `score = (1.0 / (suitable_bins_remain_cap - item + epsilon)) * exp(-k * suitable_bins_remain_cap)`\n\n        # Let's choose `k` such that it scales appropriately.\n        # If `suitable_bins_remain_cap` is, say, 10 times `item`, we might want to penalize it.\n        # Let's pick `k` so that if `suitable_bins_remain_cap` is twice the average `suitable_bins_remain_cap`,\n        # the exponential term is significantly reduced.\n\n        # Instead of exponential, let's use a simpler penalty that's easier to tune.\n        # Penalty factor: `1.0 / (1.0 + alpha * (suitable_bins_remain_cap / item))` where alpha is a tuning parameter.\n        # This penalizes bins where remaining capacity is much larger than item.\n        # Penalty factor = `item / (item + alpha * suitable_bins_remain_cap)`\n\n        # Let's combine: `best_fit_score * (item / (item + alpha * suitable_bins_remain_cap))`\n        # `score = (1.0 / (suitable_bins_remain_cap - item + epsilon)) * (item / (item + alpha * suitable_bins_remain_cap))`\n\n        # Example: item=10\n        # Bin A: rem=12. best_fit=1/(12-10)=0.5. penalty=(10/(10+a*12)). Score=0.5 * 10/(10+12a) = 5/(10+12a)\n        # Bin B: rem=20. best_fit=1/(20-10)=0.1. penalty=(10/(10+a*20)). Score=0.1 * 10/(10+20a) = 1/(10+20a)\n        # Bin C: rem=15. best_fit=1/(15-10)=0.2. penalty=(10/(10+a*15)). Score=0.2 * 10/(10+15a) = 2/(10+15a)\n\n        # If a=0 (no penalty):\n        # A: 0.5\n        # B: 0.1\n        # C: 0.2\n        # Bin A (tightest fit) is best.\n\n        # If a=1:\n        # A: 5 / (10+12) = 5/22 ~ 0.227\n        # B: 1 / (10+20) = 1/30 ~ 0.033\n        # C: 2 / (10+15) = 2/25 = 0.08\n        # Bin A is still best. This penalty isn't strongly altering the order unless 'a' is large.\n\n        # The core idea of `v1` was \"Prioritizes bins that are a tight fit, penalizing those with large gaps.\"\n        # The `v1` penalty was `1.0 / (normalized_excess + 0.1)`. This means higher penalty for *smaller* normalized excess.\n        # This is confusing. Let's assume the intent was to penalize larger gaps.\n\n        # Let's try a simple heuristic inspired by First Fit Decreasing or Best Fit Decreasing principles,\n        # adapted for online. We want to pack items efficiently.\n\n        # Strategy:\n        # 1. Prioritize the \"tightest\" fit (like Best Fit).\n        # 2. Among tight fits, prefer bins that are \"more full\" after packing.\n        #    \"More full\" means smaller remaining capacity overall.\n\n        # Metric 1: Tightness of fit.\n        # `tightness = 1.0 / (suitable_bins_remain_cap - item + epsilon)`\n\n        # Metric 2: Fill ratio of remaining space.\n        # `fill_ratio = item / suitable_bins_remain_cap`\n\n        # Let's combine these multiplicatively, as it inherently balances both.\n        # `score = tightness * fill_ratio`\n        # `score = (1.0 / (suitable_bins_remain_cap - item + epsilon)) * (item / (suitable_bins_remain_cap + epsilon))`\n        # This is the `efficiency_score`.\n\n        # Let's consider the self-reflection points:\n        # - Precision: The `efficiency_score` is precise in valuing both tightness and fill.\n        # - Adaptability: This score adapts to item sizes and available bin capacities.\n        # - Explainability: The components (tightness, fill ratio) are understandable.\n        # - Performance: It aims to reduce bin count.\n\n        # Let's think about edge cases for `efficiency_score`:\n        # If `suitable_bins_remain_cap` is very small and `item` is close to it:\n        #   `item=4`, `rem=5`. Score = `1/(5-4) * 4/5 = 1 * 0.8 = 0.8`.\n        # If `item=4`, `rem=10`. Score = `1/(10-4) * 4/10 = 1/6 * 0.4 = 0.066`.\n        # This seems to correctly prioritize the tighter fit.\n\n        # If `item=1`, `rem=100`. Score = `1/(100-1) * 1/100 = 1/99 * 0.01 ~ 0.0001`.\n        # This correctly penalizes putting a small item into a vast bin.\n\n        # What if we want to explicitly promote filling bins that are *already* mostly full?\n        # We need information about the bin's initial state. Since we don't have it,\n        # we can only infer from `suitable_bins_remain_cap`.\n        # A small `suitable_bins_remain_cap` suggests the bin was likely quite full to begin with.\n        # The `efficiency_score` already implicitly favors bins with smaller `suitable_bins_remain_cap`.\n\n        # Consider the \"best fit\" criteria carefully.\n        # Best Fit: Minimize `suitable_bins_remain_cap - item`.\n        # This means `suitable_bins_remain_cap` should be just slightly larger than `item`.\n\n        # Let's directly use `suitable_bins_remain_cap` as the primary sorting key,\n        # and `suitable_bins_remain_cap - item` as a tie-breaker or secondary key.\n        # But this is for sorting. We need a score.\n\n        # Let's go with the `efficiency_score` and consider if it can be improved.\n        # The formula `item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`\n        # aims to maximize both how much of the current space is used (`item/remaining`)\n        # and how little is wasted (`1/(remaining-item)`).\n\n        # Alternative formulation:\n        # We want to minimize `suitable_bins_remain_cap`.\n        # And we want to minimize `suitable_bins_remain_cap - item`.\n        # This means we want `suitable_bins_remain_cap` to be close to `item`.\n\n        # Consider the \"waste\" `W = suitable_bins_remain_cap - item`. We want to minimize W.\n        # Consider the \"usage\" `U = item / suitable_bins_remain_cap`. We want to maximize U.\n\n        # Let's scale `U` and `1/W` (or similar for W=0) to combine them.\n        # Max `U` is 1. Max `1/W` can be infinite.\n\n        # Let's reconsider the v1 penalty: \"penalty for bins with too much excess capacity\".\n        # The idea was to avoid large unused space *after* packing.\n        # Let `slack = suitable_bins_remain_cap - item`.\n        # We want to penalize large `slack`.\n        # A penalty function `P(slack)`.\n        # `score = best_fit_score * f(slack)` where `f(slack)` decreases as `slack` increases.\n        # `best_fit_score = 1.0 / (slack + epsilon)`\n\n        # If `f(slack) = 1.0 / (slack + epsilon)`\n        # `score = (1.0 / (slack + epsilon)) * (1.0 / (slack + epsilon)) = 1.0 / (slack + epsilon)^2`\n        # This strongly favors very tight fits.\n\n        # If `f(slack) = 1.0 / (1 + slack)`\n        # `score = (1.0 / (slack + epsilon)) * (1.0 / (1 + slack))`\n        # `= 1.0 / ((slack + epsilon) * (1 + slack))`\n        # This is similar to the `efficiency_score` if `suitable_bins_remain_cap` is roughly constant.\n\n        # Let's try to modify `efficiency_score` to better reflect \"avoiding large remaining space\".\n        # `efficiency_score = item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`\n\n        # Consider the case: item = 10\n        # Bin A: rem = 12. slack = 2. efficiency_score = 10 / (12 * 2) = 10/24 ~ 0.416\n        # Bin B: rem = 50. slack = 40. efficiency_score = 10 / (50 * 40) = 10/2000 = 0.005\n\n        # This score penalizes Bin B heavily due to its large `suitable_bins_remain_cap`.\n        # This is precisely the behavior we want to achieve with \"penalizing large gaps\".\n\n        # Let's evaluate `v1`'s score for comparison with this `efficiency_score`.\n        # `v1` `inverse_remaining` = `1.0 / (suitable_bins_remain_cap - item + 1e-9)`\n        # `v1` `normalized_excess` = `(suitable_bins_remain_cap - item) / (max_suitable_cap - item)`\n        # `v1` `penalty` = `1.0 / (normalized_excess + 0.1)`\n        # `v1` score = `inverse_remaining * penalty`\n\n        # Example for `v1`: item = 10\n        # Suitable bins: rem=[12, 50, 15]\n        # suitable_bins_remain_cap = [12, 50, 15]\n        # max_suitable_cap = 50\n\n        # Bin 1 (rem=12):\n        # inverse_remaining = 1/(12-10) = 0.5\n        # normalized_excess = (12-10) / (50-10) = 2 / 40 = 0.05\n        # penalty = 1 / (0.05 + 0.1) = 1 / 0.15 ~ 6.67\n        # score1 = 0.5 * 6.67 = 3.335\n\n        # Bin 2 (rem=50):\n        # inverse_remaining = 1/(50-10) = 0.025\n        # normalized_excess = (50-10) / (50-10) = 1.0\n        # penalty = 1 / (1.0 + 0.1) = 1 / 1.1 ~ 0.909\n        # score2 = 0.025 * 0.909 = 0.0227\n\n        # Bin 3 (rem=15):\n        # inverse_remaining = 1/(15-10) = 0.2\n        # normalized_excess = (15-10) / (50-10) = 5 / 40 = 0.125\n        # penalty = 1 / (0.125 + 0.1) = 1 / 0.225 ~ 4.44\n        # score3 = 0.2 * 4.44 = 0.888\n\n        # v1 priorities: [3.335, 0.0227, 0.888] -> Bin 1 (rem=12) is highest.\n\n        # Let's check `efficiency_score` for the same: item=10, rem=[12, 50, 15]\n        # Bin 1 (rem=12): efficiency_score = 10 / (12 * (12-10)) = 10 / (12 * 2) = 10/24 ~ 0.416\n        # Bin 2 (rem=50): efficiency_score = 10 / (50 * (50-10)) = 10 / (50 * 40) = 10/2000 = 0.005\n        # Bin 3 (rem=15): efficiency_score = 10 / (15 * (15-10)) = 10 / (15 * 5) = 10/75 ~ 0.133\n\n        # efficiency_score priorities: [0.416, 0.005, 0.133] -> Bin 1 (rem=12) is highest.\n\n        # Both prioritize Bin 1 (rem=12) which is the tightest fit.\n        # `v1`'s penalty mechanism is trying to dampen scores for bins with large excess capacity (like rem=50).\n        # `efficiency_score` achieves this by having `suitable_bins_remain_cap` in the denominator's factors.\n\n        # The `efficiency_score` seems to be a good candidate. Let's refine it slightly.\n        # The \"penalty\" in `v1` aimed to make scores more comparable and to avoid extremely high scores from extremely tight fits that might be rare.\n        # The `efficiency_score` can lead to very high values if `suitable_bins_remain_cap - item` is tiny.\n\n        # Let's add a cap or normalize the `efficiency_score` to avoid issues.\n        # Or, introduce a smoother penalty for large remaining capacities.\n\n        # Consider the `fill_ratio = item / suitable_bins_remain_cap`.\n        # And `slack_ratio = (suitable_bins_remain_cap - item) / suitable_bins_remain_cap`. We want to minimize this.\n        # So, we want to maximize `1.0 - slack_ratio = item / suitable_bins_remain_cap` (which is `fill_ratio`).\n\n        # Let's try a combination that explicitly penalizes the absolute remaining capacity,\n        # while also rewarding tightness.\n        # Score = `(best_fit_score) * (fill_ratio)`  -> `efficiency_score`\n        # Score = `(best_fit_score) / (suitable_bins_remain_cap)` -> `1.0 / ((suitable_bins_remain_cap - item + epsilon) * suitable_bins_remain_cap)`\n\n        # Let's use the `efficiency_score` and add a term that boosts bins with smaller remaining capacity.\n        # `score = efficiency_score + bonus_weight * (1.0 / (suitable_bins_remain_cap + epsilon))`\n        # This adds a \"fill bonus\".\n\n        # Let's check this new score:\n        # Bin 1 (rem=12): efficiency = 0.416. bonus = 1/12 ~ 0.083. Total = 0.416 + w * 0.083\n        # Bin 3 (rem=15): efficiency = 0.133. bonus = 1/15 ~ 0.066. Total = 0.133 + w * 0.066\n        # Bin 2 (rem=50): efficiency = 0.005. bonus = 1/50 = 0.02. Total = 0.005 + w * 0.02\n\n        # If w=1:\n        # Bin 1: 0.416 + 0.083 = 0.499\n        # Bin 3: 0.133 + 0.066 = 0.199\n        # Bin 2: 0.005 + 0.02 = 0.025\n        # Bin 1 is still highest.\n\n        # If w=10:\n        # Bin 1: 0.416 + 0.83 = 1.246\n        # Bin 3: 0.133 + 0.66 = 0.793\n        # Bin 2: 0.005 + 0.2 = 0.205\n        # Bin 1 is still highest.\n\n        # The `efficiency_score` itself already strongly favors smaller `suitable_bins_remain_cap` because it's in the denominator.\n        # Adding `1.0 / suitable_bins_remain_cap` might be redundant or might over-emphasize small remaining capacities.\n\n        # Let's stick with the `efficiency_score` as a solid baseline that balances tightness and fill.\n        # `efficiency_score = item / (suitable_bins_remain_cap * (suitable_bins_remain_cap - item) + 1e-9)`\n\n        # A final consideration: What if the item is very small compared to the bin capacity?\n        # item = 1, rem = 100.\n        # efficiency_score = 1 / (100 * 99) = 1 / 9900 ~ 0.0001\n        # This correctly gives a low score.\n\n        # What if the item is large and the bin has just enough space?\n        # item = 99, rem = 100.\n        # efficiency_score = 99 / (100 * 1) = 0.99\n        # This gives a high score, which is good.\n\n        # Let's make it more \"human-readable\" and perhaps slightly more robust by considering the proportion of capacity used.\n        # `priority = (item / suitable_bins_remain_cap) * (1.0 / (suitable_bins_remain_cap - item + epsilon))`\n        # This is `fill_ratio * tightness`.\n\n        # Let's consider a slightly different approach that explicitly penalizes slack.\n        # Penalize `suitable_bins_remain_cap - item`.\n        # Let `slack = suitable_bins_remain_cap - item`.\n        # Score = `1.0 / (slack + epsilon)` (best fit)\n        # Penalty factor = `1.0 / (1.0 + slack)` (penalizes slack)\n        # `score = (1.0 / (slack + epsilon)) * (1.0 / (1.0 + slack))`\n        # `score = 1.0 / ((slack + epsilon) * (1.0 + slack))`\n\n        # Let's test this: item=10\n        # Bin 1 (rem=12): slack=2. Score = 1 / (2 * 3) = 1/6 ~ 0.166\n        # Bin 3 (rem=15): slack=5. Score = 1 / (5 * 6) = 1/30 ~ 0.033\n        # Bin 2 (rem=50): slack=40. Score = 1 / (40 * 41) = 1/1640 ~ 0.0006\n\n        # This new score prioritizes Bin 1 (rem=12) but much less strongly than the `efficiency_score`.\n        # The `efficiency_score` `item / (suitable_bins_remain_cap * slack)`\n        # Bin 1 (rem=12): 10 / (12 * 2) = 10/24 ~ 0.416\n        # Bin 3 (rem=15): 10 / (15 * 5) = 10/75 ~ 0.133\n        # Bin 2 (rem=50): 10 / (50 * 40) = 10/2000 = 0.005\n\n        # The `efficiency_score` seems to better reflect the dual goal of tight fit and good space utilization by\n        # considering how much of the *remaining capacity* is taken.\n\n        # Let's refine `efficiency_score` calculation to ensure robustness and clarity.\n        # `suitable_bins_remain_cap - item` is the slack.\n        # `item / suitable_bins_remain_cap` is the fill ratio.\n\n        # Final candidate score: `(item / (suitable_bins_remain_cap + 1e-9)) * (1.0 / (suitable_bins_remain_cap - item + 1e-9))`\n        # This is `fill_ratio * tightness`. It's intuitive and captures both goals.\n        # It penalizes large remaining capacities due to the `suitable_bins_remain_cap` in the fill ratio denominator.\n        # It rewards tight fits due to the `suitable_bins_remain_cap - item` in the tightness denominator.\n\n        # Let's consider the scenario where `suitable_bins_remain_cap` is very large, and `item` is small.\n        # E.g., item=1, rem=1000.\n        # Fill ratio = 1/1000 = 0.001\n        # Tightness = 1/(1000-1) = 1/999 ~ 0.001\n        # Score = 0.001 * 0.001 = 0.000001. Very low, as desired.\n\n        # E.g., item=999, rem=1000.\n        # Fill ratio = 999/1000 = 0.999\n        # Tightness = 1/(1000-999) = 1/1 = 1.0\n        # Score = 0.999 * 1.0 = 0.999. Very high, as desired.\n\n        # This `fill_ratio * tightness` score seems robust and captures the desired behavior.\n        # It's also more \"explainable\" than some complex penalty functions.\n\n        # Let's ensure the data types are handled correctly.\n        # `item` is float. `bins_remain_cap` is np.ndarray.\n        # The calculations will result in floats.\n\n        priorities[suitable_bin_indices] = (item / (suitable_bins_remain_cap + 1e-9)) * \\\n                                           (1.0 / (suitable_bins_remain_cap - item + 1e-9))\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit (tightness) with a sigmoid for prioritizing near-exact fits.\n\n    Prioritizes bins that are almost full but can fit the item, using a sigmoid\n    to smooth the preference for tighter fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[fit_mask]\n\n    # Heuristic: Prioritize bins that are \"almost full\" but can still fit the item.\n    # This is inspired by Best Fit, but uses a sigmoid to give a smoother preference\n    # to bins where remaining_capacity - item is small.\n    # The input to the sigmoid is scaled such that tighter fits result in a higher score.\n    # We use -(valid_bins_remain_cap - item) to make smaller remaining space\n    # correspond to larger (less negative) sigmoid inputs.\n\n    # A simple scaling to avoid extreme sigmoid values too quickly.\n    # The range of (bins_remain_cap - item) can vary. Let's normalize it.\n    # For bins that fit, the \"slack\" is valid_bins_remain_cap - item.\n    # We want to prioritize smaller slack.\n    slack = valid_bins_remain_cap - item\n\n    # Normalize slack to be between 0 and 1 for sigmoid input.\n    # If all slack is the same, avoid division by zero.\n    if slack.size > 0:\n        min_slack = np.min(slack)\n        max_slack = np.max(slack)\n\n        if max_slack == min_slack:\n            normalized_slack = np.zeros_like(slack)\n        else:\n            # Map slack to a range where sigmoid can differentiate well.\n            # We want smaller slack to map to a higher priority.\n            # So, map min_slack (tightest fit) to a high sigmoid input,\n            # and max_slack (loosest fit) to a low sigmoid input.\n            # Consider the inverse of slack: 1 / (slack + epsilon) is similar to Best Fit.\n            # Let's use a transformation like: 1 - (slack / max_slack) or similar.\n            # A sigmoid on -(slack) might be good: larger negative means smaller slack.\n            # sigmoid_input = -slack\n            # To control steepness and range, we can use:\n            steepness = 5.0 # Tune this parameter\n            # We want smaller slack to give higher priority.\n            # So, we want a higher value when slack is small.\n            # Transform slack to a value that is higher for smaller slack.\n            # Example: max_slack - slack. Then normalize.\n            transformed_slack = max_slack - slack\n            if max_slack - min_slack > 0:\n                normalized_transformed_slack = transformed_slack / (max_slack - min_slack)\n            else:\n                normalized_transformed_slack = np.zeros_like(slack)\n\n            # Use sigmoid on the transformed slack. High transformed_slack (low original slack)\n            # should map to a high sigmoid output.\n            # We can use `steepness * (normalized_transformed_slack - 0.5)` to center around 0.5.\n            sigmoid_input = steepness * (normalized_transformed_slack - 0.5)\n            priorities[fit_mask] = 1 / (1 + np.exp(-sigmoid_input))\n        \n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a dynamic penalty based on the item's size relative\n    to the bin's remaining capacity, and considers the overall distribution of\n    remaining capacities.\n\n    Prioritizes bins that are a tight fit, but also dynamically penalizes bins\n    with excess capacity that is disproportionately large compared to the item size.\n    It also incorporates a factor that encourages using bins that are closer to\n    the average remaining capacity among suitable bins, promoting a more balanced\n    usage of bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # --- Component 1: Best Fit (Tightness) ---\n    # Prioritize bins with less remaining capacity after placing the item.\n    # Add a small epsilon to avoid division by zero if remaining_cap == item.\n    best_fit_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # --- Component 2: Dynamic Excess Capacity Penalty ---\n    # Penalize bins where the remaining capacity (after placing the item) is\n    # significantly larger than the item itself.\n    # We want to penalize `suitable_bins_remain_cap - item` when it's large relative to `item`.\n    # Using a log-based penalty can be more robust than a simple inverse\n    # and less sensitive to extreme outliers in excess capacity.\n    excess_capacity_ratio = (suitable_bins_remain_cap - item) / (item + 1e-9)\n    # Penalize more if the excess ratio is high. A higher penalty value means lower priority.\n    # We use 1 + log(1 + ratio) to ensure positive values and a gradual penalty.\n    # Adding 1 to log argument to handle cases where ratio is 0.\n    excess_penalty = 1.0 / (1.0 + np.log(1.0 + excess_capacity_ratio)) # Lower value for higher penalty\n\n    # --- Component 3: Distributional Balancing ---\n    # Consider how \"central\" a bin's remaining capacity is within the set of suitable bins.\n    # Bins that are closer to the mean remaining capacity of suitable bins might be\n    # preferred to avoid creating too many bins with very large remaining spaces,\n    # or conversely, using up all the slightly-larger-but-still-suitable bins too quickly.\n    if len(suitable_bins_remain_cap) > 1:\n        avg_suitable_cap = np.mean(suitable_bins_remain_cap)\n        # Score based on proximity to the average: higher score for being closer.\n        # Using inverse of absolute difference from average.\n        proximity_to_avg_score = 1.0 / (np.abs(suitable_bins_remain_cap - avg_suitable_cap) + 1e-9)\n    else:\n        # If only one suitable bin, this component has no effect.\n        proximity_to_avg_score = np.ones_like(suitable_bins_remain_cap)\n\n    # --- Combination Strategy ---\n    # We want to maximize best_fit_score and proximity_to_avg_score,\n    # and maximize excess_penalty (which means minimizing the penalty term).\n    # Multiply scores together.\n    # Using weights to balance the contributions of each component. These weights\n    # can be tuned based on empirical performance. For now, we give equal weight conceptually.\n    \n    # The effective priority for suitable bins is the product of their scores.\n    # Higher scores are better.\n    priorities[suitable_bins_mask] = (best_fit_score * excess_penalty * proximity_to_avg_score)\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a penalty for overly large remaining capacities.\n\n    Prioritizes bins that leave the least space after packing (Best Fit),\n    while slightly penalizing bins that have very large capacities initially\n    to encourage using bins that are already somewhat full.\n    \"\"\"\n    epsilon_small = 1e-9\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can fit the item\n    fitting_indices = np.where(bins_remain_cap >= item)[0]\n\n    if fitting_indices.size > 0:\n        # Calculate remaining capacity after fitting the item\n        remaining_after_fit = bins_remain_cap[fitting_indices] - item\n\n        # Calculate a base priority using Best Fit logic: higher for smaller remaining space\n        # This prioritizes bins that will have the least space left.\n        best_fit_priority = -remaining_after_fit\n\n        # Introduce a penalty for bins that are initially \"too empty\".\n        # If a bin's current remaining capacity is much larger than the item, it's less desirable.\n        # Let's define \"too empty\" as having remaining capacity significantly larger than the item.\n        # We can penalize bins where `bins_remain_cap` is, for example, more than 2*item.\n        # This encourages using bins that are already closer to being full.\n        penalty_threshold = item * 2.0\n        penalty = np.zeros_like(fitting_indices, dtype=float)\n\n        # Apply penalty to bins where current remaining capacity is large\n        large_capacity_mask = bins_remain_cap[fitting_indices] > penalty_threshold\n        # The penalty should be such that it reduces the priority.\n        # A simple linear penalty could work: -(bins_remain_cap - penalty_threshold)\n        # Or an inverse relationship: -1.0 / (bins_remain_cap + epsilon_small)\n        # Let's use a score that is low if bins_remain_cap is very large.\n        # We can use a scaled inverse: -(bins_remain_cap[fitting_indices] / np.max(bins_remain_cap[fitting_indices]))\n        # Or simply, a negative value proportional to the capacity itself.\n        # Let's try subtracting a scaled version of the original capacity.\n        # This makes bins with very large capacities have lower scores.\n        \n        # We want to prioritize bins that are already somewhat full.\n        # Using `1.0 / (bins_remain_cap[i] + epsilon_small)` for fitting bins\n        # prioritizes bins with small *initial* remaining capacity.\n        # Let's combine this with the \"best fit\" idea.\n        \n        # Strategy:\n        # 1. Prioritize bins that are \"almost full\" (low initial `bins_remain_cap`).\n        # 2. Among those, pick the one that results in the tightest fit (Best Fit).\n        \n        # Score = (some value based on initial capacity) + (value based on tightest fit)\n        \n        # Let's use the inverse of initial remaining capacity for \"almost full\" preference\n        # and the negative of the difference for \"best fit\" preference.\n        # We can add them, or use one as a primary and the other as a secondary refinement.\n        \n        # Let's prioritize bins that are ALREADY almost full. Small `bins_remain_cap` is good.\n        # Use `1.0 / (bins_remain_cap[i] + epsilon_small)` as the base priority.\n        # This captures the \"almost full\" aspect by favoring bins with low remaining capacity.\n        \n        initial_almost_full_priority = 1.0 / (bins_remain_cap[fitting_indices] + epsilon_small)\n        \n        # Now, within the \"almost full\" bins, let's refine using Best Fit (minimize remaining space).\n        # The \"best fit\" priority is `-remaining_after_fit`.\n        \n        # To combine: We want bins with small `bins_remain_cap` AND small `remaining_after_fit`.\n        # A simple sum or weighted sum could work.\n        # `combined_priority = w1 * initial_almost_full_priority + w2 * best_fit_priority`\n        \n        # Let's try a simpler approach: Prioritize bins that are \"almost full\" (low initial remaining capacity).\n        # If multiple bins are equally \"almost full\" (e.g., same small capacity), then pick the best fit.\n        # But the `1/r` score already heavily favors the smallest `r`.\n        \n        # Let's go with a direct interpretation: \"Almost Full Fit\" means filling bins.\n        # This is achieved by minimizing the space left after packing.\n        # This is precisely Best Fit: maximize `-(bins_remain_cap[i] - item)`.\n        \n        # If we want to emphasize \"almost full\" by penalizing bins that are *too* empty,\n        # we can modify the Best Fit score.\n        # Let's try a score that favors tight fits but penalizes very large initial capacities.\n        \n        # A score that is high when `remaining_after_fit` is small, AND `bins_remain_cap` is not excessively large.\n        \n        # Let's use `-(remaining_after_fit)` as the core Best Fit score.\n        # Then, subtract a penalty if `bins_remain_cap` is very large.\n        \n        # Penalty: if `bins_remain_cap[i] > item * K`, subtract a penalty.\n        # The penalty should be significant enough to push very large bins down.\n        \n        penalty_factor = 1.0 # Controls how much we penalize large capacities\n        penalty_value = 0.0\n        \n        # Penalize bins whose initial remaining capacity is significantly larger than the item size.\n        # A threshold relative to the item size is reasonable.\n        # Let's say we penalize if `bins_remain_cap > item * 3`.\n        threshold_large_capacity = item * 3.0\n        \n        # Calculate penalty for bins where initial capacity is large\n        large_capacity_indices_relative = np.where(bins_remain_cap[fitting_indices] > threshold_large_capacity)[0]\n        \n        if large_capacity_indices_relative.size > 0:\n            # The penalty should reduce the priority.\n            # Subtract a value proportional to how much larger the capacity is.\n            # The scale of this penalty should be comparable to the best_fit_priority range.\n            \n            # Best fit priorities are typically negative values like -0.1, -0.5, -2.0 etc.\n            # If bins_remain_cap is 100 and item is 1, remaining_after_fit is 99, best_fit_priority is -99.\n            # If we penalize for capacity 100, it should be a large negative number.\n            \n            # Let's subtract a value that scales with the extra capacity.\n            # The difference `bins_remain_cap[fitting_indices] - item` gives the remaining space.\n            # The penalty should be for large `bins_remain_cap`.\n            \n            # Let's try a score structure:\n            # Priority = `-(bins_remain_cap[i] - item)`   (Best Fit term)\n            #          - `f(bins_remain_cap[i])`        (Penalty for large initial capacity)\n            \n            # `f(x)` could be `(x / MaxCap) * Scale` or simply `x / Scale` for large x.\n            \n            # Let's use a simpler heuristic: prioritize bins that are \"almost full\" (low initial remaining capacity).\n            # If `bins_remain_cap` is small, priority is high.\n            # If `bins_remain_cap` is large, priority is low.\n            # THEN, among bins with similar \"almost fullness\", pick the best fit.\n            \n            # Score = `1.0 / (bins_remain_cap[i] + epsilon_small)`  (Prioritize small initial capacity)\n            # This is essentially \"First Fit Decreasing\" idea applied to capacities.\n            \n            # Let's try combining Best Fit with a preference for less \"empty\" bins.\n            # Priority = `-(bins_remain_cap[i] - item)` (Best Fit term)\n            # Add a term for how \"full\" the bin is initially. Small `bins_remain_cap` is good.\n            # Add `k * (1.0 / (bins_remain_cap[i] + epsilon_small))`\n            \n            # Let's simplify the objective:\n            # 1. MUST fit: `bins_remain_cap[i] >= item`\n            # 2. Prefer tightest fit: minimize `bins_remain_cap[i] - item`\n            # 3. Prefer bins that are already somewhat full: prefer small `bins_remain_cap[i]`\n            \n            # Consider the score: `priorities[fitting_indices] = -(bins_remain_cap[fitting_indices] - item)`\n            # This is Best Fit. It implicitly favors smaller initial capacities if the item size is fixed.\n            # E.g., if item=3, capacities [4, 5, 10].\n            # Scores: -(4-3)=-1, -(5-3)=-2, -(10-3)=-7.\n            # Picks capacity 4. It has the smallest initial capacity among those that fit and results in least space.\n            \n            # If we want to emphasize \"almost full\" as in \"prefer bins that are already small\",\n            # we can use `1.0 / (bins_remain_cap[i] + epsilon_small)`.\n            \n            # Let's try a composite score:\n            # Score = `(1.0 / (bins_remain_cap[fitting_indices] + epsilon_small))`  (Preference for already \"almost full\" bins)\n            #       `+ 0.5 * (-remaining_after_fit)`                           (Preference for tightest fit)\n            \n            # This prioritizes bins that are initially almost full, and among those, picks the best fit.\n            # The weight `0.5` can be tuned.\n            \n            combined_priority = (1.0 / (bins_remain_cap[fitting_indices] + epsilon_small)) + 0.5 * (-remaining_after_fit)\n            priorities[fitting_indices] = combined_priority\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a penalty for overly large remaining capacities.\n\n    Prioritizes bins that leave the least space after packing (Best Fit),\n    while slightly penalizing bins that have very large capacities initially\n    to encourage using bins that are already somewhat full.\n    \"\"\"\n    epsilon_small = 1e-9\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can fit the item\n    fitting_indices = np.where(bins_remain_cap >= item)[0]\n\n    if fitting_indices.size > 0:\n        # Calculate remaining capacity after fitting the item\n        remaining_after_fit = bins_remain_cap[fitting_indices] - item\n\n        # Calculate a base priority using Best Fit logic: higher for smaller remaining space\n        # This prioritizes bins that will have the least space left.\n        best_fit_priority = -remaining_after_fit\n\n        # Introduce a penalty for bins that are initially \"too empty\".\n        # If a bin's current remaining capacity is much larger than the item, it's less desirable.\n        # Let's define \"too empty\" as having remaining capacity significantly larger than the item.\n        # We can penalize bins where `bins_remain_cap` is, for example, more than 2*item.\n        # This encourages using bins that are already closer to being full.\n        penalty_threshold = item * 2.0\n        penalty = np.zeros_like(fitting_indices, dtype=float)\n\n        # Apply penalty to bins where current remaining capacity is large\n        large_capacity_mask = bins_remain_cap[fitting_indices] > penalty_threshold\n        # The penalty should be such that it reduces the priority.\n        # A simple linear penalty could work: -(bins_remain_cap - penalty_threshold)\n        # Or an inverse relationship: -1.0 / (bins_remain_cap + epsilon_small)\n        # Let's use a score that is low if bins_remain_cap is very large.\n        # We can use a scaled inverse: -(bins_remain_cap[fitting_indices] / np.max(bins_remain_cap[fitting_indices]))\n        # Or simply, a negative value proportional to the capacity itself.\n        # Let's try subtracting a scaled version of the original capacity.\n        # This makes bins with very large capacities have lower scores.\n        \n        # We want to prioritize bins that are already somewhat full.\n        # Using `1.0 / (bins_remain_cap[i] + epsilon_small)` for fitting bins\n        # prioritizes bins with small *initial* remaining capacity.\n        # Let's combine this with the \"best fit\" idea.\n        \n        # Strategy:\n        # 1. Prioritize bins that are \"almost full\" (low initial `bins_remain_cap`).\n        # 2. Among those, pick the one that results in the tightest fit (Best Fit).\n        \n        # Score = (some value based on initial capacity) + (value based on tightest fit)\n        \n        # Let's use the inverse of initial remaining capacity for \"almost full\" preference\n        # and the negative of the difference for \"best fit\" preference.\n        # We can add them, or use one as a primary and the other as a secondary refinement.\n        \n        # Let's prioritize bins that are ALREADY almost full. Small `bins_remain_cap` is good.\n        # Use `1.0 / (bins_remain_cap[i] + epsilon_small)` as the base priority.\n        # This captures the \"almost full\" aspect by favoring bins with low remaining capacity.\n        \n        initial_almost_full_priority = 1.0 / (bins_remain_cap[fitting_indices] + epsilon_small)\n        \n        # Now, within the \"almost full\" bins, let's refine using Best Fit (minimize remaining space).\n        # The \"best fit\" priority is `-remaining_after_fit`.\n        \n        # To combine: We want bins with small `bins_remain_cap` AND small `remaining_after_fit`.\n        # A simple sum or weighted sum could work.\n        # `combined_priority = w1 * initial_almost_full_priority + w2 * best_fit_priority`\n        \n        # Let's try a simpler approach: Prioritize bins that are \"almost full\" (low initial remaining capacity).\n        # If multiple bins are equally \"almost full\" (e.g., same small capacity), then pick the best fit.\n        # But the `1/r` score already heavily favors the smallest `r`.\n        \n        # Let's go with a direct interpretation: \"Almost Full Fit\" means filling bins.\n        # This is achieved by minimizing the space left after packing.\n        # This is precisely Best Fit: maximize `-(bins_remain_cap[i] - item)`.\n        \n        # If we want to emphasize \"almost full\" by penalizing bins that are *too* empty,\n        # we can modify the Best Fit score.\n        # Let's try a score that favors tight fits but penalizes very large initial capacities.\n        \n        # A score that is high when `remaining_after_fit` is small, AND `bins_remain_cap` is not excessively large.\n        \n        # Let's use `-(remaining_after_fit)` as the core Best Fit score.\n        # Then, subtract a penalty if `bins_remain_cap` is very large.\n        \n        # Penalty: if `bins_remain_cap[i] > item * K`, subtract a penalty.\n        # The penalty should be significant enough to push very large bins down.\n        \n        penalty_factor = 1.0 # Controls how much we penalize large capacities\n        penalty_value = 0.0\n        \n        # Penalize bins whose initial remaining capacity is significantly larger than the item size.\n        # A threshold relative to the item size is reasonable.\n        # Let's say we penalize if `bins_remain_cap > item * 3`.\n        threshold_large_capacity = item * 3.0\n        \n        # Calculate penalty for bins where initial capacity is large\n        large_capacity_indices_relative = np.where(bins_remain_cap[fitting_indices] > threshold_large_capacity)[0]\n        \n        if large_capacity_indices_relative.size > 0:\n            # The penalty should reduce the priority.\n            # Subtract a value proportional to how much larger the capacity is.\n            # The scale of this penalty should be comparable to the best_fit_priority range.\n            \n            # Best fit priorities are typically negative values like -0.1, -0.5, -2.0 etc.\n            # If bins_remain_cap is 100 and item is 1, remaining_after_fit is 99, best_fit_priority is -99.\n            # If we penalize for capacity 100, it should be a large negative number.\n            \n            # Let's subtract a value that scales with the extra capacity.\n            # The difference `bins_remain_cap[fitting_indices] - item` gives the remaining space.\n            # The penalty should be for large `bins_remain_cap`.\n            \n            # Let's try a score structure:\n            # Priority = `-(bins_remain_cap[i] - item)`   (Best Fit term)\n            #          - `f(bins_remain_cap[i])`        (Penalty for large initial capacity)\n            \n            # `f(x)` could be `(x / MaxCap) * Scale` or simply `x / Scale` for large x.\n            \n            # Let's use a simpler heuristic: prioritize bins that are \"almost full\" (low initial remaining capacity).\n            # If `bins_remain_cap` is small, priority is high.\n            # If `bins_remain_cap` is large, priority is low.\n            # THEN, among bins with similar \"almost fullness\", pick the best fit.\n            \n            # Score = `1.0 / (bins_remain_cap[i] + epsilon_small)`  (Prioritize small initial capacity)\n            # This is essentially \"First Fit Decreasing\" idea applied to capacities.\n            \n            # Let's try combining Best Fit with a preference for less \"empty\" bins.\n            # Priority = `-(bins_remain_cap[i] - item)` (Best Fit term)\n            # Add a term for how \"full\" the bin is initially. Small `bins_remain_cap` is good.\n            # Add `k * (1.0 / (bins_remain_cap[i] + epsilon_small))`\n            \n            # Let's simplify the objective:\n            # 1. MUST fit: `bins_remain_cap[i] >= item`\n            # 2. Prefer tightest fit: minimize `bins_remain_cap[i] - item`\n            # 3. Prefer bins that are already somewhat full: prefer small `bins_remain_cap[i]`\n            \n            # Consider the score: `priorities[fitting_indices] = -(bins_remain_cap[fitting_indices] - item)`\n            # This is Best Fit. It implicitly favors smaller initial capacities if the item size is fixed.\n            # E.g., if item=3, capacities [4, 5, 10].\n            # Scores: -(4-3)=-1, -(5-3)=-2, -(10-3)=-7.\n            # Picks capacity 4. It has the smallest initial capacity among those that fit and results in least space.\n            \n            # If we want to emphasize \"almost full\" as in \"prefer bins that are already small\",\n            # we can use `1.0 / (bins_remain_cap[i] + epsilon_small)`.\n            \n            # Let's try a composite score:\n            # Score = `(1.0 / (bins_remain_cap[fitting_indices] + epsilon_small))`  (Preference for already \"almost full\" bins)\n            #       `+ 0.5 * (-remaining_after_fit)`                           (Preference for tightest fit)\n            \n            # This prioritizes bins that are initially almost full, and among those, picks the best fit.\n            # The weight `0.5` can be tuned.\n            \n            combined_priority = (1.0 / (bins_remain_cap[fitting_indices] + epsilon_small)) + 0.5 * (-remaining_after_fit)\n            priorities[fitting_indices] = combined_priority\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by giving a higher score to bins that fit the item\n    tightly, using an exponential decay based on relative capacity,\n    but also strongly favoring exact fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n    \n    if fitting_bins_capacities.size > 0:\n        \n        exact_fit_mask = fitting_bins_capacities == item\n        \n        \n        relative_capacities = fitting_bins_capacities - item\n        \n        \n        non_exact_fit_mask = ~exact_fit_mask\n        \n        \n        non_exact_fitting_capacities = relative_capacities[non_exact_fit_mask]\n        \n        if non_exact_fitting_capacities.size > 0:\n            \n            mean_relative_capacity = np.mean(non_exact_fitting_capacities)\n            \n            \n            if mean_relative_capacity > 0:\n                \n                priorities[can_fit_mask][non_exact_fit_mask] = np.exp(-non_exact_fitting_capacities / mean_relative_capacity)\n            else:\n                \n                priorities[can_fit_mask][non_exact_fit_mask] = 0.0\n        \n        \n        priorities[can_fit_mask][exact_fit_mask] = 1.0 \n        \n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by giving a higher score to bins that fit the item\n    tightly, using an exponential decay based on relative capacity,\n    but also strongly favoring exact fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n    \n    if fitting_bins_capacities.size > 0:\n        \n        exact_fit_mask = fitting_bins_capacities == item\n        \n        \n        relative_capacities = fitting_bins_capacities - item\n        \n        \n        non_exact_fit_mask = ~exact_fit_mask\n        \n        \n        non_exact_fitting_capacities = relative_capacities[non_exact_fit_mask]\n        \n        if non_exact_fitting_capacities.size > 0:\n            \n            mean_relative_capacity = np.mean(non_exact_fitting_capacities)\n            \n            \n            if mean_relative_capacity > 0:\n                \n                priorities[can_fit_mask][non_exact_fit_mask] = np.exp(-non_exact_fitting_capacities / mean_relative_capacity)\n            else:\n                \n                priorities[can_fit_mask][non_exact_fit_mask] = 0.0\n        \n        \n        priorities[can_fit_mask][exact_fit_mask] = 1.0 \n        \n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by giving a higher score to bins that fit the item\n    tightly, using an exponential decay based on relative capacity,\n    but also strongly favoring exact fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n    \n    if fitting_bins_capacities.size > 0:\n        \n        exact_fit_mask = fitting_bins_capacities == item\n        \n        \n        relative_capacities = fitting_bins_capacities - item\n        \n        \n        non_exact_fit_mask = ~exact_fit_mask\n        \n        \n        non_exact_fitting_capacities = relative_capacities[non_exact_fit_mask]\n        \n        if non_exact_fitting_capacities.size > 0:\n            \n            mean_relative_capacity = np.mean(non_exact_fitting_capacities)\n            \n            \n            if mean_relative_capacity > 0:\n                \n                priorities[can_fit_mask][non_exact_fit_mask] = np.exp(-non_exact_fitting_capacities / mean_relative_capacity)\n            else:\n                \n                priorities[can_fit_mask][non_exact_fit_mask] = 0.0\n        \n        \n        priorities[can_fit_mask][exact_fit_mask] = 1.0 \n        \n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by favoring exact fits and then applying a scaled exponential\n    decay to the remaining capacity, balancing 'best fit' with a preference for\n    tight fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    fitting_bins_caps = bins_remain_cap[can_fit_mask]\n    \n    \n    exact_fit_mask = fitting_bins_caps == item\n    \n    if np.any(exact_fit_mask):\n        priorities[can_fit_mask][exact_fit_mask] = 1e10  \n    \n    \n    non_exact_fitting_bins_caps = fitting_bins_caps[~exact_fit_mask]\n    non_exact_fitting_indices = np.where(can_fit_mask)[0][~exact_fit_mask]\n    \n    if non_exact_fitting_bins_caps.size > 0:\n        \n        relative_capacities = non_exact_fitting_bins_caps - item\n        \n        \n        min_relative_capacity = np.min(relative_capacities)\n        max_relative_capacity = np.max(relative_capacities)\n\n        \n        if max_relative_capacity == min_relative_capacity:\n            scaled_relative_capacities = np.zeros_like(relative_capacities)\n        else:\n            \n            scaled_relative_capacities = (relative_capacities - min_relative_capacity) / (max_relative_capacity - min_relative_capacity)\n        \n        \n        priorities[non_exact_fitting_indices] = np.exp(-scaled_relative_capacities)\n        \n    \n    if np.any(exact_fit_mask):\n        priorities[can_fit_mask][exact_fit_mask] = np.max(priorities[can_fit_mask][~exact_fit_mask]) * 1.1 if np.any(~exact_fit_mask) else 1e10\n        \n    \n    priorities[~can_fit_mask] = 0.0\n    \n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a Sigmoid for nuanced bin prioritization.\n\n    Prioritizes bins that offer a tighter fit using a sigmoid function,\n    favoring bins where the remaining capacity is closer to the item size.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[eligible_bins_mask]\n\n    if eligible_bins_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"tightness\" of the fit for eligible bins.\n    # A smaller difference means a tighter fit.\n    differences = eligible_bins_cap - item\n\n    # Use a sigmoid function to map the differences to a priority score.\n    # A smaller difference (tighter fit) should result in a higher priority.\n    # We want to invert the difference so smaller differences are \"better\".\n    # The sigmoid function will then map these inverted differences to [0, 1].\n    # Scaling factor to control the steepness of the sigmoid.\n    scale_factor = 5.0\n    # Add a small epsilon to avoid division by zero if difference is 0.\n    inverted_differences = 1.0 / (differences + 1e-9)\n    \n    # Shift the scores so that a perfect fit (difference of 0) gets a high score.\n    # Since inverted_differences will be large for small differences,\n    # we can directly apply sigmoid or shift if needed.\n    # Here, a larger inverted_differences (meaning smaller original difference)\n    # should lead to higher priority.\n    \n    # Sigmoid function: 1 / (1 + exp(-x))\n    # We want higher priority for smaller differences.\n    # Let's use exp(-difference) as a base for priority.\n    # A larger value for exp(-difference) means a smaller difference.\n    # Then apply sigmoid to these values.\n    \n    # Option 1: Directly use exp(-difference) and sigmoid\n    # shifted_inverted_differences = -differences * scale_factor\n    # priorities = 1 / (1 + np.exp(-shifted_inverted_differences))\n    \n    # Option 2: Use 1/(difference + epsilon) and sigmoid\n    # The 'fit_scores' from v1 can be interpreted as how much \"room\" is left relative to the item.\n    # A score of 1 means exact fit. We want scores close to 1 to have high priority.\n    # Let's revisit v1 logic for better interpretation:\n    # fit_scores = valid_bins_cap / (valid_bins_cap - item + 1e-9)\n    # A higher fit_score means the bin is less full relative to the item size.\n    # This is NOT what we want for \"tight fit\". We want small (valid_bins_cap - item).\n    \n    # Let's go back to prioritizing smaller differences.\n    # We can directly use the negative difference, scaled, within the sigmoid.\n    # A smaller difference means a more desirable fit.\n    # We want the sigmoid output to be higher for smaller `differences`.\n    # `1 / (1 + exp(-k * difference))` will achieve this: as `difference` decreases, `-k * difference` increases, and sigmoid output increases.\n    \n    scaled_differences = differences * scale_factor\n    priorities = 1 / (1 + np.exp(-scaled_differences))\n\n    # Map priorities back to the original bins_remain_cap array\n    original_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_priorities[eligible_bins_mask] = priorities\n    \n    return original_priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a Sigmoid for nuanced bin prioritization.\n\n    Prioritizes bins that offer a tighter fit using a sigmoid function,\n    favoring bins where the remaining capacity is closer to the item size.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[eligible_bins_mask]\n\n    if eligible_bins_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"tightness\" of the fit for eligible bins.\n    # A smaller difference means a tighter fit.\n    differences = eligible_bins_cap - item\n\n    # Use a sigmoid function to map the differences to a priority score.\n    # A smaller difference (tighter fit) should result in a higher priority.\n    # We want to invert the difference so smaller differences are \"better\".\n    # The sigmoid function will then map these inverted differences to [0, 1].\n    # Scaling factor to control the steepness of the sigmoid.\n    scale_factor = 5.0\n    # Add a small epsilon to avoid division by zero if difference is 0.\n    inverted_differences = 1.0 / (differences + 1e-9)\n    \n    # Shift the scores so that a perfect fit (difference of 0) gets a high score.\n    # Since inverted_differences will be large for small differences,\n    # we can directly apply sigmoid or shift if needed.\n    # Here, a larger inverted_differences (meaning smaller original difference)\n    # should lead to higher priority.\n    \n    # Sigmoid function: 1 / (1 + exp(-x))\n    # We want higher priority for smaller differences.\n    # Let's use exp(-difference) as a base for priority.\n    # A larger value for exp(-difference) means a smaller difference.\n    # Then apply sigmoid to these values.\n    \n    # Option 1: Directly use exp(-difference) and sigmoid\n    # shifted_inverted_differences = -differences * scale_factor\n    # priorities = 1 / (1 + np.exp(-shifted_inverted_differences))\n    \n    # Option 2: Use 1/(difference + epsilon) and sigmoid\n    # The 'fit_scores' from v1 can be interpreted as how much \"room\" is left relative to the item.\n    # A score of 1 means exact fit. We want scores close to 1 to have high priority.\n    # Let's revisit v1 logic for better interpretation:\n    # fit_scores = valid_bins_cap / (valid_bins_cap - item + 1e-9)\n    # A higher fit_score means the bin is less full relative to the item size.\n    # This is NOT what we want for \"tight fit\". We want small (valid_bins_cap - item).\n    \n    # Let's go back to prioritizing smaller differences.\n    # We can directly use the negative difference, scaled, within the sigmoid.\n    # A smaller difference means a more desirable fit.\n    # We want the sigmoid output to be higher for smaller `differences`.\n    # `1 / (1 + exp(-k * difference))` will achieve this: as `difference` decreases, `-k * difference` increases, and sigmoid output increases.\n    \n    scaled_differences = differences * scale_factor\n    priorities = 1 / (1 + np.exp(-scaled_differences))\n\n    # Map priorities back to the original bins_remain_cap array\n    original_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_priorities[eligible_bins_mask] = priorities\n    \n    return original_priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a sigmoid-like preference for tighter fits.\n    Prioritizes bins that leave minimal remaining capacity after packing,\n    but uses a scaled exponential to amplify this preference.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Find bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if not np.any(can_fit_mask):\n        return priorities # No bin can fit the item\n        \n    # Calculate remaining capacity after fitting the item\n    remaining_capacities = fitting_bins_remain_cap - item\n    \n    # Use a scaled exponential to prioritize tighter fits (similar to Heuristic 17 but simpler)\n    # This amplifies the preference for bins with smaller remaining_capacities\n    # Add a small epsilon to avoid division by zero if all remaining capacities are the same\n    epsilon = 1e-8\n    scaled_preference = np.exp(remaining_capacities)\n    \n    # Normalize the preference scores so they sum to 1 for the fitting bins\n    sum_scaled_preference = np.sum(scaled_preference)\n    if sum_scaled_preference > 0:\n        normalized_preference = scaled_preference / sum_scaled_preference\n    else:\n        # If all scaled preferences are zero (e.g., due to very large negative exponents if we used them)\n        # assign equal probability to all fitting bins.\n        normalized_preference = np.ones_like(fitting_bins_remain_cap) / len(fitting_bins_remain_cap)\n\n    priorities[can_fit_mask] = normalized_preference\n    \n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a sigmoid-like preference for tighter fits.\n    Prioritizes bins that leave minimal remaining capacity after packing,\n    but uses a scaled exponential to amplify this preference.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Find bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if not np.any(can_fit_mask):\n        return priorities # No bin can fit the item\n        \n    # Calculate remaining capacity after fitting the item\n    remaining_capacities = fitting_bins_remain_cap - item\n    \n    # Use a scaled exponential to prioritize tighter fits (similar to Heuristic 17 but simpler)\n    # This amplifies the preference for bins with smaller remaining_capacities\n    # Add a small epsilon to avoid division by zero if all remaining capacities are the same\n    epsilon = 1e-8\n    scaled_preference = np.exp(remaining_capacities)\n    \n    # Normalize the preference scores so they sum to 1 for the fitting bins\n    sum_scaled_preference = np.sum(scaled_preference)\n    if sum_scaled_preference > 0:\n        normalized_preference = scaled_preference / sum_scaled_preference\n    else:\n        # If all scaled preferences are zero (e.g., due to very large negative exponents if we used them)\n        # assign equal probability to all fitting bins.\n        normalized_preference = np.ones_like(fitting_bins_remain_cap) / len(fitting_bins_remain_cap)\n\n    priorities[can_fit_mask] = normalized_preference\n    \n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}