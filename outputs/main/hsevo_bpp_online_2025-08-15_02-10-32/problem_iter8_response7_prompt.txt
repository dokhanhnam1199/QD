{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit and Worst Fit principles with a dynamic \"gap\" penalty.\n    Prioritizes bins that minimize remaining capacity (Best Fit), but also considers\n    bins that have a larger capacity that could accommodate future larger items,\n    but penalizes a large \"gap\" between item size and bin capacity to avoid fragmentation.\n    The \"gap\" penalty is adaptive based on the item's size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Mask for bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # --- Component 1: Best Fit ---\n    # Prioritize bins with minimal remaining capacity after packing.\n    # This is a negative log of the remaining capacity after packing to maximize.\n    # Add epsilon for numerical stability when remaining_cap == item.\n    best_fit_score = -np.log(suitable_bins_remain_cap - item + 1e-9)\n    \n    # --- Component 2: Worst Fit (modified) ---\n    # Consider bins that have more excess capacity, potentially for larger future items.\n    # We want to give a *slight* preference to bins with more \"room\" but not too much.\n    # We can represent this as a score related to the excess capacity, but we want to\n    # avoid extremely large remaining capacities which might lead to fragmentation.\n    # Let's use a score that increases with excess capacity but saturates.\n    # A simple approach is a linear or logarithmic increase in the excess capacity itself.\n    # Let's use the excess capacity directly, scaled.\n    # `excess_capacity = suitable_bins_remain_cap - item`\n    # We want to favor bins where `excess_capacity` is moderate.\n    # A simple score could be `excess_capacity / (item + epsilon)`. High ratio means large excess relative to item.\n    # We want to favor lower excess *relative* to item size.\n    \n    # --- Component 3: Dynamic Gap Penalty ---\n    # Penalize bins where the difference (gap) between bin capacity and item size is large.\n    # This gap represents wasted space. The penalty should be stronger for larger gaps.\n    # The penalty should also be 'dynamic' in the sense that a gap of 5 units might be small\n    # for an item of size 100, but large for an item of size 5.\n    # So, the penalty should be related to `gap / item_size`.\n    \n    gap = suitable_bins_remain_cap - item\n    \n    # Penalty score: we want to penalize large `gap / item`.\n    # So, a good penalty multiplier would be `1 / (gap / item + C)` or `item / (gap + C)`.\n    # This multiplier should be applied to a score we want to maximize.\n    # A higher multiplier means a better bin (smaller relative gap).\n    \n    # Let's combine Best Fit and a modified Worst Fit idea.\n    # We want to maximize `best_fit_score`.\n    # For the \"room\" aspect, we can consider `gap`.\n    # If `gap` is very small, it's good for Best Fit.\n    # If `gap` is moderate, it might be good for accommodating future items.\n    # If `gap` is very large, it's bad (fragmentation).\n    \n    # Let's try to maximize:\n    # 1. `best_fit_score` (higher is better)\n    # 2. A term that favors moderate gaps over very large gaps.\n    #    Consider `gap_score = 1.0 / (gap / item + epsilon)` -- this favors smaller gaps.\n    #    This is similar to the penalty in v1.\n    \n    # Let's refine the interaction:\n    # We want to maximize the \"goodness\" of a fit.\n    # Goodness = (Score from Best Fit) * (Score from Gap Management)\n    \n    # Best Fit score: `-np.log(remaining_after_packing)` as before.\n    # Gap Management Score: We want to penalize large gaps relative to item size.\n    # So, a score that decreases as `gap/item` increases.\n    # Let's use `1 / (1 + (gap / (item + 1e-9)))`.\n    # This score is 1 for a perfect fit (gap=0) and decreases as the relative gap increases.\n    gap_management_score = 1.0 / (1.0 + (gap / (item + 1e-9)))\n    \n    # Final priority: Product of Best Fit and Gap Management Score.\n    # This encourages bins that are \"tight\" (high best_fit_score) while also\n    # not having excessively large relative gaps.\n    priorities[suitable_bins_mask] = best_fit_score * gap_management_score\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes tight fits using a sigmoid, with a penalty for large initial capacities.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[fit_mask]\n\n    # Component 1: Sigmoid for tight fits (inspired by priority_v0)\n    # Prioritize bins where the remaining capacity after placing the item is small.\n    slack = valid_bins_remain_cap - item\n    \n    # Normalize slack to be in a range suitable for sigmoid, focusing on small slack.\n    # We map smaller slack to larger input for sigmoid, thus higher output.\n    min_slack = np.min(slack)\n    max_slack = np.max(slack)\n    \n    sigmoid_scores = np.zeros_like(slack, dtype=float)\n    if max_slack > min_slack:\n        # Transform slack: smaller slack -> larger transformed value\n        transformed_slack = max_slack - slack\n        normalized_transformed_slack = transformed_slack / (max_slack - min_slack)\n        # Apply sigmoid centered around 0.5 for smooth preference\n        steepness = 5.0 \n        sigmoid_input = steepness * (normalized_transformed_slack - 0.5)\n        sigmoid_scores = 1 / (1 + np.exp(-sigmoid_input))\n    elif slack.size > 0: # All slacks are the same\n        sigmoid_scores = np.full_like(slack, 0.5) # Neutral score if all fits are identical\n\n\n    # Component 2: Penalty for large initial capacities (inspired by the idea of penalizing large bins)\n    # Reduce priority for bins that have significantly more capacity than needed initially.\n    # This is a simple inverse capacity scaling. Add epsilon to avoid division by zero.\n    epsilon_large_cap = 1e-6\n    large_capacity_penalty = epsilon_large_cap / (valid_bins_remain_cap + epsilon_large_cap)\n\n\n    # Combine scores: Multiply sigmoid scores by penalty.\n    # A high sigmoid score (tight fit) AND a low penalty (not excessively large bin) is preferred.\n    combined_scores = sigmoid_scores * large_capacity_penalty\n\n    # Normalize combined scores for the fitting bins\n    sum_combined_scores = np.sum(combined_scores)\n    if sum_combined_scores > 0:\n        normalized_combined_scores = combined_scores / sum_combined_scores\n    else: # If all scores are zero (e.g., if penalty made them zero)\n        # Fallback: give equal priority to all fitting bins\n        normalized_combined_scores = np.ones_like(valid_bins_remain_cap) / len(valid_bins_remain_cap)\n\n    priorities[fit_mask] = normalized_combined_scores\n    \n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), the code is identical. The ranking suggests a subtle difference in conceptualization, not implementation.\n\nComparing (2nd) vs (3rd), the code is identical. The ranking is arbitrary or based on external factors not evident from the code.\n\nComparing (3rd) vs (4th): Heuristic 3 attempts to combine \"Best Fit\" with a penalty for large excess capacity relative to item size using a `tightness_ratio` and `penalty_component`. Heuristic 4 introduces \"Worst Fit\" concepts and a \"dynamic gap penalty,\" but its implementation primarily focuses on `best_fit_score * gap_management_score`, where `gap_management_score` is `1.0 / (1.0 + (gap / item))`. Heuristic 3's penalty is more directly tied to the `remaining_cap - item` ratio, whereas Heuristic 4 uses `item / (remaining_cap)`. The core idea of penalizing large gaps is similar, but Heuristic 4's `gap_management_score` might be more stable for very small items.\n\nComparing (4th) vs (5th): Heuristic 4 uses a multiplicative combination of Best Fit and a gap management score. Heuristic 5 introduces a sigmoid for tight fits and a penalty for large initial capacities, combining them multiplicatively. Heuristic 5's sigmoid introduces non-linearity, potentially offering smoother preferences for tight fits. Heuristic 4's approach is more directly interpretable as Best Fit scaled by a tightness factor.\n\nComparing (5th) vs (6th): Heuristic 5 uses a sigmoid and a simple inverse capacity penalty. Heuristic 6 prioritizes exact fits with a very high score and then uses exponential decay based on normalized slack for non-exact fits. Heuristic 6's explicit handling of exact fits is a strong point, but its normalization of slack might be sensitive to extreme values.\n\nComparing (6th) vs (7th): Heuristic 6 has explicit exact fit handling and exponential decay. Heuristic 7 also prioritizes exact fits (with a slightly lower score) and then combines Best Fit (`1.0 / (remaining_capacities_after_fit)`) with a preference for \"almost full\" bins (using `1.0 / (suitable_capacities)`). Heuristic 7's additive combination of these two preferences, weighted, offers a different balance.\n\nComparing (7th) vs (8th): Heuristics 7 and 8 are identical.\n\nComparing (8th) vs (9th): Heuristic 8 uses a sigmoid for tight fits and an inverse capacity penalty. Heuristic 9 combines a tightness score (`1.0 / slack`) with a fill ratio (`item / remaining_cap`), using multiplication. Heuristic 9's multiplicative approach is more direct in rewarding both tight fits and good item utilization within the bin.\n\nComparing (9th) vs (10th): Heuristic 9 multiplies tightness and fill ratio. Heuristic 10 does the same but adds an \"almost full\" bonus, which is essentially another term favoring bins with low initial remaining capacity. The additive bonus provides an additional layer of preference.\n\nComparing (10th) vs (11th): Heuristic 10 combines efficiency (tightness * fill ratio) with an \"almost full\" bonus additively. Heuristic 11 uses a weighted additive combination of \"almost full\" preference and \"tightest fit\" preference. Heuristic 11's explicit weighting of preferences is more structured than Heuristic 10's additive bonus.\n\nComparing (11th) vs (12th): Heuristics 11 and 12 are identical.\n\nComparing (12th) vs (13th): Heuristics 12 and 13 are identical.\n\nComparing (13th) vs (14th): Heuristic 13 is identical to 11 and 12. Heuristic 14 introduces an \"adaptive Best Fit\" by dynamically weighting a tightness component based on the item's size relative to average remaining capacity. It also uses a log-based best-fit score. This adaptivity is a novel aspect.\n\nComparing (14th) vs (15th): Heuristics 14 and 15 are identical.\n\nComparing (15th) vs (16th): Heuristic 15 is identical to 14. Heuristic 16 combines Best Fit (log scale) with a refined slack minimization and a fill ratio component, using multiplication. It also introduces a `slack_decay_factor` (`exp(-relative_slack)`). This multiplicative approach with an exponential decay is a strong combination.\n\nComparing (16th) vs (17th): Heuristics 16 and 17 are identical.\n\nComparing (17th) vs (18th): Heuristics 17 and 18 are identical.\n\nComparing (18th) vs (19th): Heuristic 18 is identical to 16 and 17. Heuristic 19 attempts to combine Best Fit, an adaptive slack penalty (normalized by average excess capacity), and a uniformity score (penalizing deviation from average remaining capacity). This multi-faceted approach aims for more balanced packing.\n\nComparing (19th) vs (20th): Heuristic 19 combines Best Fit (log), adaptive slack penalty, and uniformity. Heuristic 20 combines Best Fit (log), fullness score (inverse capacity), multiplicatively. Heuristic 19's attempt at adaptivity and uniformity suggests a more sophisticated strategy than Heuristic 20's simpler multiplicative combination.\n\nOverall, heuristics that explicitly handle exact fits (6, 7), use adaptive weights or penalties based on item size or system state (14, 15, 19), or combine multiple strong metrics multiplicatively with decay functions (16, 17, 18) appear to be conceptually superior. The ranking seems to generally increase in complexity and adaptivity up to a point, then potentially plateau or slightly decline in perceived quality.\n- \nHere's a redefined approach to self-reflection for heuristic design:\n\n*   **Keywords:** Robustness, adaptability, clarity, parsimony, performance, objective function.\n*   **Advice:** Focus on designing heuristics with clear, interpretable objective functions that can adapt to varying problem states (e.g., item sizes, bin utilization). Prioritize vectorized implementations for efficiency.\n*   **Avoid:** Redundant or overly complex metric combinations, arbitrary thresholds, and heuristics that don't demonstrably improve performance across a range of scenarios.\n*   **Explanation:** True self-reflection means identifying core principles of good heuristic design (like clarity and adaptability) and ruthlessly eliminating elements that introduce complexity without clear benefit, ensuring robustness and efficient execution.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}