{
     "algorithm": "This algorithm combines stochastic sampling with adaptive pheromone-inspired edge selection and reinforcement learning to estimate edge importance.}\"\"\"\n    n = distance_matrix.shape[0]\n    heuristics_matrix = np.zeros_like(distance_matrix)\n    pheromone_matrix = np.ones_like(distance_matrix)  # Initialize pheromone trails\n\n    num_samples = 500\n    learning_rate = 0.1\n    discount_factor = 0.9\n\n    for _ in range(num_samples):\n        current_node = 0\n        route = [0]\n        remaining_nodes = set(range(1, n))\n        current_capacity = capacity\n        total_distance = 0\n\n        while remaining_nodes:\n            probabilities = {}\n            for node in remaining_nodes:\n                if current_capacity >= demands[node]:\n                    distance_heuristic = 1 / (distance_matrix[current_node, node] + 1e-6)\n                    demand_heuristic = (current_capacity - demands[node]) / capacity\n                    probabilities[node] = distance_heuristic * (demand_heuristic + 0.1) * pheromone_matrix[current_node, node]\n\n            if not probabilities:\n                break\n\n            total_prob = sum(probabilities.values())\n            if total_prob == 0:\n                next_node = remaining_nodes.pop()\n            else:\n                normalized_probabilities = {node: prob / total_prob for node, prob in probabilities.items()",
     "code": "import numpy as np\nimport random\n\ndef heuristics_v2(distance_matrix, coordinates, demands, capacity):\n    \"\"\"{This algorithm combines stochastic sampling with adaptive pheromone-inspired edge selection and reinforcement learning to estimate edge importance.}\"\"\"\n    n = distance_matrix.shape[0]\n    heuristics_matrix = np.zeros_like(distance_matrix)\n    pheromone_matrix = np.ones_like(distance_matrix)  # Initialize pheromone trails\n\n    num_samples = 500\n    learning_rate = 0.1\n    discount_factor = 0.9\n\n    for _ in range(num_samples):\n        current_node = 0\n        route = [0]\n        remaining_nodes = set(range(1, n))\n        current_capacity = capacity\n        total_distance = 0\n\n        while remaining_nodes:\n            probabilities = {}\n            for node in remaining_nodes:\n                if current_capacity >= demands[node]:\n                    distance_heuristic = 1 / (distance_matrix[current_node, node] + 1e-6)\n                    demand_heuristic = (current_capacity - demands[node]) / capacity\n                    probabilities[node] = distance_heuristic * (demand_heuristic + 0.1) * pheromone_matrix[current_node, node]\n\n            if not probabilities:\n                break\n\n            total_prob = sum(probabilities.values())\n            if total_prob == 0:\n                next_node = remaining_nodes.pop()\n            else:\n                normalized_probabilities = {node: prob / total_prob for node, prob in probabilities.items()}\n                next_node = np.random.choice(list(normalized_probabilities.keys()), p=list(normalized_probabilities.values()))\n                \n            if current_capacity >= demands[next_node]:\n                route.append(next_node)\n                total_distance += distance_matrix[current_node, next_node]\n                current_capacity -= demands[next_node]\n                remaining_nodes.remove(next_node)\n                heuristics_matrix[current_node, next_node] += 1\n                heuristics_matrix[next_node, current_node] += 1\n                \n                # Update pheromone trail (reinforcement learning)\n                pheromone_matrix[current_node, next_node] = (1 - learning_rate) * pheromone_matrix[current_node, next_node] + learning_rate * (1 / (distance_matrix[current_node, next_node] + 1e-6))\n                pheromone_matrix[next_node, current_node] = (1 - learning_rate) * pheromone_matrix[next_node, current_node] + learning_rate * (1 / (distance_matrix[next_node, current_node] + 1e-6))\n                \n                current_node = next_node\n            else:\n                route.append(0)\n                total_distance += distance_matrix[current_node, 0]\n                heuristics_matrix[current_node, 0] += 1\n                heuristics_matrix[0, current_node] += 1\n\n                # Update pheromone trail (negative reward for returning to depot)\n                pheromone_matrix[current_node, 0] = (1 - learning_rate) * pheromone_matrix[current_node, 0]\n                pheromone_matrix[0, current_node] = (1 - learning_rate) * pheromone_matrix[0, current_node]\n                \n                current_node = 0\n                current_capacity = capacity\n        \n        if current_node != 0:\n            total_distance += distance_matrix[current_node, 0]\n            heuristics_matrix[current_node, 0] += 1\n            heuristics_matrix[0, current_node] += 1\n\n    for i in range(n):\n        for j in range(n):\n            if heuristics_matrix[i, j] > 0:\n               heuristics_matrix[i, j] /= (distance_matrix[i, j] + 1e-6)\n            \n    return heuristics_matrix",
     "objective": 14.36769,
     "other_inf": null
}