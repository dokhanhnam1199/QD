```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Prioritize bins using adaptive learning, dynamic balance factors, and sigmoid penalties for remaining capacity.
    
    Args:
        item: Size of item to be added to a bin.
        bins_remain_cap: Array of remaining capacities for each bin.
    
    Returns:
        Array of priority scores for each bin.
    """
    # Dynamic penalty for bins that cannot fit the item
    penalty_for_large_items = np.where(bins_remain_cap < item, -np.inf, 0)

    # Scaled Remaining Capacity with sigmoid penalty
    sigmoid_penalty_threshold = 7.870147266070587e-06
    scaled_remaining_capacity = np.where(
        bins_remain_cap >= item, 
        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), 
        -np.inf
    )

    # Dynamic Balance Factor: Encourage balanced distribution
    mean_cap = np.mean(bins_remain_cap)
    balance_factor_threshold = 8.54060876899628e-06
    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + balance_factor_threshold)
    balance_penalty = 1 - balance_factor

    # Last Fit Decrease (LFD) Heuristic
    last_fit_decrease = np.zeros_like(bins_remain_cap)
    if len(bins_remain_cap) > 1:
        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]

    # Adaptive weights based on item size and mean bin capacity
    delta = item - mean_cap
    adaptive_alpha = 0.8703526170915381 + 0.5 * np.tanh(delta)
    adaptive_beta = 0.26928992154797116 - 0.1 * np.tanh(delta)
    adaptive_gamma = 0.015623035472155156

    # Combine heuristics with adaptive learning
    priority_scores = (
        adaptive_alpha * scaled_remaining_capacity +
        adaptive_beta * balance_penalty +
        adaptive_gamma * last_fit_decrease
    )

    # Apply penalty for bins that cannot fit the item
    priority_scores += penalty_for_large_items

    return priority_scores
```
