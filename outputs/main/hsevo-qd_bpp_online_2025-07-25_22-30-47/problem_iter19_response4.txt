import numpy as np
import random
import math
import scipy
import torch
def priority_v2(
    item: float, 
    bins_remain_cap: np.ndarray, 
    alpha: float = 0.16864647112433873, 
    beta: float = 0.922107963928812, 
    gamma: float = 0.4763546064728851, 
    sigmoid_penalty_threshold: float = 8.790411360641736e-06, 
    balance_factor_threshold: float = 9.73984855329807e-06) -> np.ndarray:
    """
    Prioritize bins by combining adaptive learning, balance factor, and sigmoid penalty.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.
        alpha: Weight for Scaled Remaining Capacity.
        beta: Weight for Balance Factor.
        gamma: Weight for Last Fit Decrease.
        sigmoid_penalty_threshold: Threshold for sigmoid penalty calculation.
        balance_factor_threshold: Threshold for balance factor calculation.

    Returns:
        Array of priority scores for each bin.
    """
    # Scaled Remaining Capacity with sigmoid penalty
    scaled_remaining_capacity = np.where(
        bins_remain_cap >= item, 
        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), 
        -np.inf
    )

    # Balance Factor: Encourage a more balanced distribution
    mean_cap = np.mean(bins_remain_cap)
    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + balance_factor_threshold)

    # Last Fit Decrease (LFD) Heuristic
    last_fit_decrease = np.zeros_like(bins_remain_cap)
    if len(bins_remain_cap) > 1:
        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]

    # Combine heuristics with adaptive learning
    priority_scores = (
        alpha * scaled_remaining_capacity +
        beta * (1 - balance_factor) +
        gamma * last_fit_decrease
    )

    return priority_scores
