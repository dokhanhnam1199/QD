```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray, adaptation_factor: float = 0.05, iteration: int = 0, historical_data: np.ndarray = None) -> np.ndarray:
    """
    Prioritize bins by combining adaptive learning, dynamic weights, and balance factor with domain-specific adjustments.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.
        adaptation_factor: Rate of adaptation for weights based on historical data.
        iteration: Current iteration number to dynamically adjust weights.
        historical_data: Historical data of bin states and outcomes.

    Returns:
        Array of priority scores for each bin.
    """
    if historical_data is None:
        historical_data = np.zeros((100, len(bins_remain_cap)))  # Placeholder for historical data

    # Scaled Remaining Capacity with sigmoid penalty
    sigmoid_penalty_threshold = 1e-5
    scaled_remaining_capacity = np.where(
        bins_remain_cap >= item, 
        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), 
        -np.inf
    )

    # Balance Factor: Encourage a more balanced distribution, adaptive
    mean_cap = np.mean(bins_remain_cap)
    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + 1e-6)
    adaptive_balance_factor = balance_factor * (1 + adaptation_factor * np.sum(historical_data[:, bins_remain_cap.argmin()]))

    # Last Fit Decrease (LFD) Heuristic, adaptive
    last_fit_decrease = np.zeros_like(bins_remain_cap)
    if len(bins_remain_cap) > 1:
        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]
    adaptive_last_fit_decrease = last_fit_decrease * (1 + adaptation_factor * np.sum(historical_data[:, bins_remain_cap.argmax()]))

    # Dynamic weights based on iteration
    alpha = 0.9 / (1 + np.exp(-0.1 * (iteration - 20)))  # Adaptive weight for scaled remaining capacity
    beta = 0.8 / (1 + np.exp(-0.1 * (iteration - 40)))   # Adaptive weight for balance factor
    gamma = 0.3 / (1 + np.exp(-0.1 * (iteration - 60)))   # Adaptive weight for last fit decrease

    # Combine heuristics with adaptive learning
    priority_scores = (
        alpha * scaled_remaining_capacity +
        beta * (1 - adaptive_balance_factor) +
        gamma * adaptive_last_fit_decrease
    )

    # Update historical data
    historical_data[iteration % historical_data.shape[0]] = bins_remain_cap

    return priority_scores
```
