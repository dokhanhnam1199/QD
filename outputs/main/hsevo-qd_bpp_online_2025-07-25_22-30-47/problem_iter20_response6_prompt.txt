{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines adaptive learning with sigmoid penalties and balance factor for efficient bin placement.\n    \n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    alpha = 0.5  # Weight for Scaled Remaining Capacity\n    beta = 0.3   # Weight for Balance Factor\n    gamma = 0.2  # Weight for Last Fit Decrease\n    sigmoid_penalty_threshold = 1e-6\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    scaled_remaining_capacity = np.where(\n        bins_remain_cap >= item, \n        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n        -np.inf\n    )\n\n    # Balance Factor: Encourage a more balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + 1e-6)\n\n    # Last Fit Decrease (LFD) Heuristic\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        gamma * last_fit_decrease\n    )\n\n    return priority_scores\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritize bins using a hybrid heuristic that combines adaptive learning, dynamic adjustments,\n    and balanced penalties tailored to the domain of bin packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Calculate the remaining capacity after placing the item\n    remaining_capacity_after_item = bins_remain_cap - item\n\n    # Dynamic penalty for bins that cannot fit the item\n    penalty_for_large_items = np.where(remaining_capacity_after_item < 0, -np.inf, 0)\n\n    # Penalize bins that are close to being full\n    sigmoid_penalty_threshold = 0.0001  # Small threshold to avoid division by zero\n    sigmoid_penalty = 1.0 / (remaining_capacity_after_item + sigmoid_penalty_threshold)\n    \n    # Balance Factor: Encourage a more balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor_threshold = 0.001\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / (np.max(np.abs(mean_cap - bins_remain_cap)) + balance_factor_threshold)\n    balance_penalty = 1 - balance_factor  # Inverse balance factor to penalize imbalance\n    \n    # Reward bins that are less likely to be the last bin filled (First Fit Decreasing heuristic)\n    sorted_bins = np.sort(bins_remain_cap)\n    sorted_indices = np.argsort(bins_remain_cap)\n    reversed_sorted_indices = sorted_indices[::-1]\n    rank_based_reward = np.zeros_like(bins_remain_cap)\n    for rank, idx in enumerate(reversed_sorted_indices):\n        rank_based_reward[idx] = rank  # Lower rank (better traditional fit) gets higher score\n    \n    # Adaptive coefficients based on the difference between the item size and mean bin capacity\n    delta = item - mean_cap\n    adaptive_alpha = 1 / (1 + np.exp(-delta))  # Smooth step function for dynamic weighting\n    adaptive_beta = 1 - adaptive_alpha\n    adaptive_gamma = 0.1  # Small constant for ranking reward to keep it balanced\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        adaptive_alpha * sigmoid_penalty +\n        adaptive_beta * balance_penalty +\n        adaptive_gamma * rank_based_reward\n    )\n\n    # Apply penalty for bins that cannot fit the item\n    priority_scores += penalty_for_large_items\n\n    return priority_scores\n\n### Analyze & experience\n- Comparing (best) vs (worst), we see that the best heuristic incorporates historical data for adaptive learning, adaptive thresholds, and dynamic weights, resulting in a more nuanced and adaptive approach. The worst heuristic lacks these features, relying on static weights and thresholds. (2nd) vs (2nd worst) shows a similar trend where the better heuristic uses adaptive learning and dynamic adjustments. Comparing (1st) vs (2nd), the primary difference lies in the inclusion of historical data and adaptation factors in the better heuristic. (3rd) vs (4th) indicates that the better heuristic uses adaptive weights and dynamic thresholds, whereas the worse one relies on static parameters. Comparing (second worst) vs (worst), the worse heuristic does not adjust weights or incorporate adaptive learning, leading to less effective prioritization. Overall, the top-ranked heuristics benefit from adaptive learning, dynamic weights, and historical data, enabling them to better manage varying conditions and optimize bin placement.\n- \n- **Keywords:** Adaptive learning, dynamic weights, refined balance factor, domain-specific data\n- **Advice:** \n  - Integrate machine learning techniques for adaptive learning to fine-tune heuristics over time.\n  - Continuously adjust weights and thresholds based on performance data and feedback loops to enhance adaptability.\n  - Employ refined balance factors that effectively balance incentives for optimal bin placement with penalties for inefficiency.\n  - Leverage domain-specific data for parameter tuning to optimize heuristic efficiency without increasing complexity.\n- **Avoid:** \n  - Repeated use of static coefficients without dynamic adjustment.\n  - Incorporating overly complex or random transformations that do not significantly improve performance.\n  - Excessive focus on random elements or overly complicated methods.\n- **Explanation:** By focusing on adaptive learning and continuous refinement with concrete parameters informed by domain-specific data, the heuristic can evolve and improve without becoming overly complex. This approach ensures the heuristic remains efficient, maintainable, and tailored to specific optimization tasks.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}