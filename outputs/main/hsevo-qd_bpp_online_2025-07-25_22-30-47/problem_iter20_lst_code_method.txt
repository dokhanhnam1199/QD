{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins using dynamic adaptive learning with balanced penalties.\n    \n    Args:\n        item: Size of item to be added.\n        bins_remain_cap: Array of remaining bin capacities.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    alpha = 0.8703526170915381  # Weight for Scaled Remaining Capacity\n    beta = 0.26928992154797116   # Weight for Balance Factor\n    gamma = 0.015623035472155156  # Weight for Last Fit Decrease\n    sigmoid_penalty_threshold = 7.870147266070587e-06  # Sigmoid penalty threshold\n    balance_factor_threshold = 8.54060876899628e-06  # Balance factor threshold\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    scaled_remaining_capacity = np.where(bins_remain_cap >= item, \n                                       1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n                                       -np.inf)\n\n    # Balance Factor: Encourage a more balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + balance_factor_threshold)\n\n    # Last Fit Decrease (LFD) Heuristic\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        gamma * last_fit_decrease\n    )\n\n    return priority_scores\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins using dynamic adaptive learning with balanced penalties.\n    \n    Args:\n        item: Size of item to be added.\n        bins_remain_cap: Array of remaining bin capacities.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    alpha = 0.8703526170915381  # Weight for Scaled Remaining Capacity\n    beta = 0.26928992154797116   # Weight for Balance Factor\n    gamma = 0.015623035472155156  # Weight for Last Fit Decrease\n    sigmoid_penalty_threshold = 7.870147266070587e-06  # Sigmoid penalty threshold\n    balance_factor_threshold = 8.54060876899628e-06  # Balance factor threshold\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    scaled_remaining_capacity = np.where(bins_remain_cap >= item, \n                                       1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n                                       -np.inf)\n\n    # Balance Factor: Encourage a more balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + balance_factor_threshold)\n\n    # Last Fit Decrease (LFD) Heuristic\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        gamma * last_fit_decrease\n    )\n\n    return priority_scores\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins using a combination of adaptive learning and dynamic balance factors.\n    \n    Args:\n        item: Size of item to be added to a bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Adaptive weights for heuristics components\n    alpha = 0.8703526170915381  # Scaled Remaining Capacity\n    beta = 0.26928992154797116   # Balance Factor\n    gamma = 0.015623035472155156  # Last Fit Decrease\n\n    # Dynamic thresholds\n    sigmoid_penalty_threshold = 7.870147266070587e-06\n    balance_factor_threshold = 8.54060876899628e-06\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    scaled_remaining_capacity = np.where(\n        bins_remain_cap >= item, \n        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n        -np.inf\n    )\n\n    # Dynamic Balance Factor: Encourage balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + balance_factor_threshold)\n\n    # Last Fit Decrease (LFD) Heuristic\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        gamma * last_fit_decrease\n    )\n\n    return priority_scores\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins using dynamic adaptive learning with balanced penalties.\n    \n    Args:\n        item: Size of item to be added.\n        bins_remain_cap: Array of remaining bin capacities.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    alpha = 0.8703526170915381  # Weight for Scaled Remaining Capacity\n    beta = 0.26928992154797116   # Weight for Balance Factor\n    gamma = 0.015623035472155156  # Weight for Last Fit Decrease\n    sigmoid_penalty_threshold = 7.870147266070587e-06  # Sigmoid penalty threshold\n    balance_factor_threshold = 8.54060876899628e-06  # Balance factor threshold\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    scaled_remaining_capacity = np.where(bins_remain_cap >= item, \n                                       1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n                                       -np.inf)\n\n    # Balance Factor: Encourage a more balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + balance_factor_threshold)\n\n    # Last Fit Decrease (LFD) Heuristic\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        gamma * last_fit_decrease\n    )\n\n    return priority_scores\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins using a combination of adaptive learning and dynamic balance factors.\n    \n    Args:\n        item: Size of item to be added to a bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Adaptive weights for heuristics components\n    alpha = 0.8703526170915381  # Scaled Remaining Capacity\n    beta = 0.26928992154797116   # Balance Factor\n    gamma = 0.015623035472155156  # Last Fit Decrease\n\n    # Dynamic thresholds\n    sigmoid_penalty_threshold = 7.870147266070587e-06\n    balance_factor_threshold = 8.54060876899628e-06\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    scaled_remaining_capacity = np.where(\n        bins_remain_cap >= item, \n        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n        -np.inf\n    )\n\n    # Dynamic Balance Factor: Encourage balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + balance_factor_threshold)\n\n    # Last Fit Decrease (LFD) Heuristic\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        gamma * last_fit_decrease\n    )\n\n    return priority_scores\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, adaptation_factor: float = 0.05, iteration: int = 0, historical_data: np.ndarray = None) -> np.ndarray:\n    \"\"\"\n    Prioritize bins by combining adaptive learning, dynamic weights, and balance factor with domain-specific adjustments.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        adaptation_factor: Rate of adaptation for weights based on historical data.\n        iteration: Current iteration number to dynamically adjust weights.\n        historical_data: Historical data of bin states and outcomes.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    if historical_data is None:\n        historical_data = np.zeros((100, len(bins_remain_cap)))  # Placeholder for historical data\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    sigmoid_penalty_threshold = 1e-5\n    scaled_remaining_capacity = np.where(\n        bins_remain_cap >= item, \n        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n        -np.inf\n    )\n\n    # Balance Factor: Encourage a more balanced distribution, adaptive\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + 1e-6)\n    adaptive_balance_factor = balance_factor * (1 + adaptation_factor * np.sum(historical_data[:, bins_remain_cap.argmin()]))\n\n    # Last Fit Decrease (LFD) Heuristic, adaptive\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n    adaptive_last_fit_decrease = last_fit_decrease * (1 + adaptation_factor * np.sum(historical_data[:, bins_remain_cap.argmax()]))\n\n    # Dynamic weights based on iteration\n    alpha = 0.9 / (1 + np.exp(-0.1 * (iteration - 20)))  # Adaptive weight for scaled remaining capacity\n    beta = 0.8 / (1 + np.exp(-0.1 * (iteration - 40)))   # Adaptive weight for balance factor\n    gamma = 0.3 / (1 + np.exp(-0.1 * (iteration - 60)))   # Adaptive weight for last fit decrease\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - adaptive_balance_factor) +\n        gamma * adaptive_last_fit_decrease\n    )\n\n    # Update historical data\n    historical_data[iteration % historical_data.shape[0]] = bins_remain_cap\n\n    return priority_scores\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins using a combination of adaptive weights, balance factor, and dynamic thresholds.\n    \"\"\"\n    alpha = 0.870  # Weight for Scaled Remaining Capacity\n    beta = 0.270   # Weight for Balance Factor\n    gamma = 0.015  # Weight for Last Fit Decrease\n    sigmoid_penalty_threshold = 7.870e-06  # Threshold for sigmoid penalty\n    balance_factor_threshold = 8.540e-06  # Threshold for balance factor\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    scaled_remaining_capacity = np.where(\n        bins_remain_cap >= item, \n        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n        -np.inf\n    )\n\n    # Balance Factor: Encourage a more balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + balance_factor_threshold)\n\n    # Last Fit Decrease (LFD) Heuristic\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        gamma * last_fit_decrease\n    )\n\n    return priority_scores\n\n[Heuristics 8th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(\n    item: float, \n    bins_remain_cap: np.ndarray,\n    alpha: float = 0.6889072020207402,  # Weight for Scaled Remaining Capacity\n    beta: float = 0.27864877187020565,   # Weight for Balance Factor\n    gamma: float = 0.1859532598991715,  # Weight for Last Fit Decrease\n    sigmoid_penalty_threshold: float = 2.595177488362933e-06) -> np.ndarray:\n    \"\"\"\n    Combines adaptive learning with sigmoid penalties and balance factor for efficient bin placement.\n    \n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        alpha: Weight for Scaled Remaining Capacity.\n        beta: Weight for Balance Factor.\n        gamma: Weight for Last Fit Decrease.\n        sigmoid_penalty_threshold: Threshold for sigmoid penalty.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Scaled Remaining Capacity with sigmoid penalty\n    scaled_remaining_capacity = np.where(\n        bins_remain_cap >= item, \n        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n        -np.inf\n    )\n\n    # Balance Factor: Encourage a more balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + 1e-6)\n\n    # Last Fit Decrease (LFD) Heuristic\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        gamma * last_fit_decrease\n    )\n\n    return priority_scores\n\n[Heuristics 9th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(\n    item: float, \n    bins_remain_cap: np.ndarray,\n    alpha: float = 0.6889072020207402,  # Weight for Scaled Remaining Capacity\n    beta: float = 0.27864877187020565,   # Weight for Balance Factor\n    gamma: float = 0.1859532598991715,  # Weight for Last Fit Decrease\n    sigmoid_penalty_threshold: float = 2.595177488362933e-06) -> np.ndarray:\n    \"\"\"\n    Combines adaptive learning with sigmoid penalties and balance factor for efficient bin placement.\n    \n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        alpha: Weight for Scaled Remaining Capacity.\n        beta: Weight for Balance Factor.\n        gamma: Weight for Last Fit Decrease.\n        sigmoid_penalty_threshold: Threshold for sigmoid penalty.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Scaled Remaining Capacity with sigmoid penalty\n    scaled_remaining_capacity = np.where(\n        bins_remain_cap >= item, \n        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n        -np.inf\n    )\n\n    # Balance Factor: Encourage a more balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + 1e-6)\n\n    # Last Fit Decrease (LFD) Heuristic\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        gamma * last_fit_decrease\n    )\n\n    return priority_scores\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines adaptive learning with sigmoid penalties and balance factor for efficient bin placement.\n    \n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    alpha = 0.5  # Weight for Scaled Remaining Capacity\n    beta = 0.3   # Weight for Balance Factor\n    gamma = 0.2  # Weight for Last Fit Decrease\n    sigmoid_penalty_threshold = 1e-6\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    scaled_remaining_capacity = np.where(\n        bins_remain_cap >= item, \n        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n        -np.inf\n    )\n\n    # Balance Factor: Encourage a more balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + 1e-6)\n\n    # Last Fit Decrease (LFD) Heuristic\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        gamma * last_fit_decrease\n    )\n\n    return priority_scores\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines adaptive learning with sigmoid penalties and balance factor for efficient bin placement.\n    \n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    alpha = 0.5  # Weight for Scaled Remaining Capacity\n    beta = 0.3   # Weight for Balance Factor\n    gamma = 0.2  # Weight for Last Fit Decrease\n    sigmoid_penalty_threshold = 1e-6\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    scaled_remaining_capacity = np.where(\n        bins_remain_cap >= item, \n        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n        -np.inf\n    )\n\n    # Balance Factor: Encourage a more balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + 1e-6)\n\n    # Last Fit Decrease (LFD) Heuristic\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        gamma * last_fit_decrease\n    )\n\n    return priority_scores\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins by combining adaptive learning, balance factor, and sigmoid penalty.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Parameters for adaptive learning\n    alpha = 0.6  # Weight for Scaled Remaining Capacity\n    beta = 0.3   # Weight for Balance Factor\n    gamma = 0.1  # Weight for Last Fit Decrease\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    scaled_remaining_capacity = np.where(bins_remain_cap >= item, 1.0 / (bins_remain_cap - item + 1e-6), -np.inf)\n\n    # Balance Factor: Encourage a more balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + 1e-6)\n\n    # Last Fit Decrease (LFD) Heuristic\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        gamma * last_fit_decrease\n    )\n\n    return priority_scores\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins using a hybrid heuristic that combines adaptive learning, dynamic adjustments,\n    and balanced penalties tailored to the domain of bin packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Calculate the remaining capacity after placing the item\n    remaining_capacity_after_item = bins_remain_cap - item\n\n    # Dynamic penalty for bins that cannot fit the item\n    penalty_for_large_items = np.where(remaining_capacity_after_item < 0, -np.inf, 0)\n\n    # Penalize bins that are close to being full\n    sigmoid_penalty_threshold = 0.0001  # Small threshold to avoid division by zero\n    sigmoid_penalty = 1.0 / (remaining_capacity_after_item + sigmoid_penalty_threshold)\n    \n    # Balance Factor: Encourage a more balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor_threshold = 0.001\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / (np.max(np.abs(mean_cap - bins_remain_cap)) + balance_factor_threshold)\n    balance_penalty = 1 - balance_factor  # Inverse balance factor to penalize imbalance\n    \n    # Reward bins that are less likely to be the last bin filled (First Fit Decreasing heuristic)\n    sorted_bins = np.sort(bins_remain_cap)\n    sorted_indices = np.argsort(bins_remain_cap)\n    reversed_sorted_indices = sorted_indices[::-1]\n    rank_based_reward = np.zeros_like(bins_remain_cap)\n    for rank, idx in enumerate(reversed_sorted_indices):\n        rank_based_reward[idx] = rank  # Lower rank (better traditional fit) gets higher score\n    \n    # Adaptive coefficients based on the difference between the item size and mean bin capacity\n    delta = item - mean_cap\n    adaptive_alpha = 1 / (1 + np.exp(-delta))  # Smooth step function for dynamic weighting\n    adaptive_beta = 1 - adaptive_alpha\n    adaptive_gamma = 0.1  # Small constant for ranking reward to keep it balanced\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        adaptive_alpha * sigmoid_penalty +\n        adaptive_beta * balance_penalty +\n        adaptive_gamma * rank_based_reward\n    )\n\n    # Apply penalty for bins that cannot fit the item\n    priority_scores += penalty_for_large_items\n\n    return priority_scores\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins using a hybrid heuristic that combines adaptive learning, dynamic adjustments,\n    and balanced penalties tailored to the domain of bin packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Calculate the remaining capacity after placing the item\n    remaining_capacity_after_item = bins_remain_cap - item\n\n    # Dynamic penalty for bins that cannot fit the item\n    penalty_for_large_items = np.where(remaining_capacity_after_item < 0, -np.inf, 0)\n\n    # Penalize bins that are close to being full\n    sigmoid_penalty_threshold = 0.0001  # Small threshold to avoid division by zero\n    sigmoid_penalty = 1.0 / (remaining_capacity_after_item + sigmoid_penalty_threshold)\n    \n    # Balance Factor: Encourage a more balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor_threshold = 0.001\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / (np.max(np.abs(mean_cap - bins_remain_cap)) + balance_factor_threshold)\n    balance_penalty = 1 - balance_factor  # Inverse balance factor to penalize imbalance\n    \n    # Reward bins that are less likely to be the last bin filled (First Fit Decreasing heuristic)\n    sorted_bins = np.sort(bins_remain_cap)\n    sorted_indices = np.argsort(bins_remain_cap)\n    reversed_sorted_indices = sorted_indices[::-1]\n    rank_based_reward = np.zeros_like(bins_remain_cap)\n    for rank, idx in enumerate(reversed_sorted_indices):\n        rank_based_reward[idx] = rank  # Lower rank (better traditional fit) gets higher score\n    \n    # Adaptive coefficients based on the difference between the item size and mean bin capacity\n    delta = item - mean_cap\n    adaptive_alpha = 1 / (1 + np.exp(-delta))  # Smooth step function for dynamic weighting\n    adaptive_beta = 1 - adaptive_alpha\n    adaptive_gamma = 0.1  # Small constant for ranking reward to keep it balanced\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        adaptive_alpha * sigmoid_penalty +\n        adaptive_beta * balance_penalty +\n        adaptive_gamma * rank_based_reward\n    )\n\n    # Apply penalty for bins that cannot fit the item\n    priority_scores += penalty_for_large_items\n\n    return priority_scores\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins using a hybrid heuristic that combines adaptive learning, dynamic adjustments,\n    and balanced penalties tailored to the domain of bin packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Calculate the remaining capacity after placing the item\n    remaining_capacity_after_item = bins_remain_cap - item\n\n    # Dynamic penalty for bins that cannot fit the item\n    penalty_for_large_items = np.where(remaining_capacity_after_item < 0, -np.inf, 0)\n\n    # Penalize bins that are close to being full\n    sigmoid_penalty_threshold = 0.0001  # Small threshold to avoid division by zero\n    sigmoid_penalty = 1.0 / (remaining_capacity_after_item + sigmoid_penalty_threshold)\n    \n    # Balance Factor: Encourage a more balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor_threshold = 0.001\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / (np.max(np.abs(mean_cap - bins_remain_cap)) + balance_factor_threshold)\n    balance_penalty = 1 - balance_factor  # Inverse balance factor to penalize imbalance\n    \n    # Reward bins that are less likely to be the last bin filled (First Fit Decreasing heuristic)\n    sorted_bins = np.sort(bins_remain_cap)\n    sorted_indices = np.argsort(bins_remain_cap)\n    reversed_sorted_indices = sorted_indices[::-1]\n    rank_based_reward = np.zeros_like(bins_remain_cap)\n    for rank, idx in enumerate(reversed_sorted_indices):\n        rank_based_reward[idx] = rank  # Lower rank (better traditional fit) gets higher score\n    \n    # Adaptive coefficients based on the difference between the item size and mean bin capacity\n    delta = item - mean_cap\n    adaptive_alpha = 1 / (1 + np.exp(-delta))  # Smooth step function for dynamic weighting\n    adaptive_beta = 1 - adaptive_alpha\n    adaptive_gamma = 0.1  # Small constant for ranking reward to keep it balanced\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        adaptive_alpha * sigmoid_penalty +\n        adaptive_beta * balance_penalty +\n        adaptive_gamma * rank_based_reward\n    )\n\n    # Apply penalty for bins that cannot fit the item\n    priority_scores += penalty_for_large_items\n\n    return priority_scores\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins using a hybrid heuristic that combines adaptive learning, dynamic adjustments,\n    and balanced penalties tailored to the domain of bin packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Calculate the remaining capacity after placing the item\n    remaining_capacity_after_item = bins_remain_cap - item\n\n    # Dynamic penalty for bins that cannot fit the item\n    penalty_for_large_items = np.where(remaining_capacity_after_item < 0, -np.inf, 0)\n\n    # Penalize bins that are close to being full\n    sigmoid_penalty_threshold = 0.0001  # Small threshold to avoid division by zero\n    sigmoid_penalty = 1.0 / (remaining_capacity_after_item + sigmoid_penalty_threshold)\n    \n    # Balance Factor: Encourage a more balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor_threshold = 0.001\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / (np.max(np.abs(mean_cap - bins_remain_cap)) + balance_factor_threshold)\n    balance_penalty = 1 - balance_factor  # Inverse balance factor to penalize imbalance\n    \n    # Reward bins that are less likely to be the last bin filled (First Fit Decreasing heuristic)\n    sorted_bins = np.sort(bins_remain_cap)\n    sorted_indices = np.argsort(bins_remain_cap)\n    reversed_sorted_indices = sorted_indices[::-1]\n    rank_based_reward = np.zeros_like(bins_remain_cap)\n    for rank, idx in enumerate(reversed_sorted_indices):\n        rank_based_reward[idx] = rank  # Lower rank (better traditional fit) gets higher score\n    \n    # Adaptive coefficients based on the difference between the item size and mean bin capacity\n    delta = item - mean_cap\n    adaptive_alpha = 1 / (1 + np.exp(-delta))  # Smooth step function for dynamic weighting\n    adaptive_beta = 1 - adaptive_alpha\n    adaptive_gamma = 0.1  # Small constant for ranking reward to keep it balanced\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        adaptive_alpha * sigmoid_penalty +\n        adaptive_beta * balance_penalty +\n        adaptive_gamma * rank_based_reward\n    )\n\n    # Apply penalty for bins that cannot fit the item\n    priority_scores += penalty_for_large_items\n\n    return priority_scores\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins by combining adaptive learning, dynamic balance factor, and refined penalty mechanisms.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Adaptive Threshold Calculation\n    max_bin_capacity = np.max(bins_remain_cap)\n    threshold_factor = 0.05  # Dynamic adjustment factor\n    sigmoid_penalty_threshold = max_bin_capacity * threshold_factor\n    balance_factor_threshold = max_bin_capacity * threshold_factor\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    scaled_remaining_capacity = np.where(\n        bins_remain_cap >= item, \n        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n        -np.inf\n    )\n\n    # Refined Balance Factor: Encourage a more balanced distribution with adaptive threshold\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.where(\n        np.abs(mean_cap - bins_remain_cap) < balance_factor_threshold, \n        0, \n        np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + balance_factor_threshold)\n    )\n\n    # Last Fit Decrease (LFD) Heuristic with dynamic weighting\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n    last_fit_decrease_weight = 0.02  # Dynamic weight for LFD\n\n    # Dynamic Weights for heuristics based on the number of bins and average remaining capacity\n    alpha = 2 / (1 + np.exp(-0.5 * len(bins_remain_cap)))  # Scaled Remaining Capacity weight\n    beta = 1 - alpha  # Balance Factor weight\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        last_fit_decrease_weight * last_fit_decrease\n    )\n\n    return priority_scores\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins by combining adaptive learning, dynamic balance factor, and refined penalty mechanisms.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Adaptive Threshold Calculation\n    max_bin_capacity = np.max(bins_remain_cap)\n    threshold_factor = 0.05  # Dynamic adjustment factor\n    sigmoid_penalty_threshold = max_bin_capacity * threshold_factor\n    balance_factor_threshold = max_bin_capacity * threshold_factor\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    scaled_remaining_capacity = np.where(\n        bins_remain_cap >= item, \n        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n        -np.inf\n    )\n\n    # Refined Balance Factor: Encourage a more balanced distribution with adaptive threshold\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.where(\n        np.abs(mean_cap - bins_remain_cap) < balance_factor_threshold, \n        0, \n        np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + balance_factor_threshold)\n    )\n\n    # Last Fit Decrease (LFD) Heuristic with dynamic weighting\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n    last_fit_decrease_weight = 0.02  # Dynamic weight for LFD\n\n    # Dynamic Weights for heuristics based on the number of bins and average remaining capacity\n    alpha = 2 / (1 + np.exp(-0.5 * len(bins_remain_cap)))  # Scaled Remaining Capacity weight\n    beta = 1 - alpha  # Balance Factor weight\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        last_fit_decrease_weight * last_fit_decrease\n    )\n\n    return priority_scores\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins by combining adaptive learning, dynamic balance factor, and refined penalty mechanisms.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Adaptive Threshold Calculation\n    max_bin_capacity = np.max(bins_remain_cap)\n    threshold_factor = 0.05  # Dynamic adjustment factor\n    sigmoid_penalty_threshold = max_bin_capacity * threshold_factor\n    balance_factor_threshold = max_bin_capacity * threshold_factor\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    scaled_remaining_capacity = np.where(\n        bins_remain_cap >= item, \n        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n        -np.inf\n    )\n\n    # Refined Balance Factor: Encourage a more balanced distribution with adaptive threshold\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.where(\n        np.abs(mean_cap - bins_remain_cap) < balance_factor_threshold, \n        0, \n        np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + balance_factor_threshold)\n    )\n\n    # Last Fit Decrease (LFD) Heuristic with dynamic weighting\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n    last_fit_decrease_weight = 0.02  # Dynamic weight for LFD\n\n    # Dynamic Weights for heuristics based on the number of bins and average remaining capacity\n    alpha = 2 / (1 + np.exp(-0.5 * len(bins_remain_cap)))  # Scaled Remaining Capacity weight\n    beta = 1 - alpha  # Balance Factor weight\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        last_fit_decrease_weight * last_fit_decrease\n    )\n\n    return priority_scores\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins by combining adaptive learning, dynamic balance factor, and refined penalty mechanisms.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Adaptive Threshold Calculation\n    max_bin_capacity = np.max(bins_remain_cap)\n    threshold_factor = 0.05  # Dynamic adjustment factor\n    sigmoid_penalty_threshold = max_bin_capacity * threshold_factor\n    balance_factor_threshold = max_bin_capacity * threshold_factor\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    scaled_remaining_capacity = np.where(\n        bins_remain_cap >= item, \n        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n        -np.inf\n    )\n\n    # Refined Balance Factor: Encourage a more balanced distribution with adaptive threshold\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.where(\n        np.abs(mean_cap - bins_remain_cap) < balance_factor_threshold, \n        0, \n        np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + balance_factor_threshold)\n    )\n\n    # Last Fit Decrease (LFD) Heuristic with dynamic weighting\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n    last_fit_decrease_weight = 0.02  # Dynamic weight for LFD\n\n    # Dynamic Weights for heuristics based on the number of bins and average remaining capacity\n    alpha = 2 / (1 + np.exp(-0.5 * len(bins_remain_cap)))  # Scaled Remaining Capacity weight\n    beta = 1 - alpha  # Balance Factor weight\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        last_fit_decrease_weight * last_fit_decrease\n    )\n\n    return priority_scores\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}