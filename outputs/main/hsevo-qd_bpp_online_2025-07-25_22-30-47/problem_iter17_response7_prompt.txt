{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritize bins by dynamically adjusting weights based on real-time feedback and using\n    smooth sigmoid penalties to enhance bin packing efficiency.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Dynamic weights based on current system state\n    system_load = np.mean(bins_remain_cap)\n    total_capacity = np.sum(bins_remain_cap)\n    num_bins = len(bins_remain_cap)\n    load_factor = system_load / (total_capacity / num_bins) if total_capacity > 0 else 1.0\n    \n    # Adaptive learning: adjust weights based on load factor\n    alpha = np.clip(0.5 + 0.5 * np.tanh((load_factor - 0.5) * 2), 0.0, 1.0)  # Scaled Remaining Capacity\n    beta = np.clip(0.5 - 0.4 * np.tanh((load_factor - 0.5) * 2), 0.0, 1.0)  # Balance Factor\n    gamma = np.clip(0.2 + 0.3 * np.tanh((load_factor - 0.5) * 3), 0.0, 1.0)  # Last Fit Decrease\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    sigmoid_penalty_threshold = 1e-6\n    scaled_remaining_capacity = np.where(\n        bins_remain_cap >= item, \n        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n        -np.inf\n    )\n\n    # Balance Factor: Encourage a more balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + 1e-6)\n\n    # Last Fit Decrease (LFD) Heuristic\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        gamma * last_fit_decrease\n    )\n\n    return priority_scores\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritize bins using a hybrid heuristic that combines adaptive learning, dynamic adjustments,\n    and balanced penalties tailored to the domain of bin packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Calculate the remaining capacity after placing the item\n    remaining_capacity_after_item = bins_remain_cap - item\n\n    # Dynamic penalty for bins that cannot fit the item\n    penalty_for_large_items = np.where(remaining_capacity_after_item < 0, -np.inf, 0)\n\n    # Penalize bins that are close to being full\n    sigmoid_penalty_threshold = 0.0001  # Small threshold to avoid division by zero\n    sigmoid_penalty = 1.0 / (remaining_capacity_after_item + sigmoid_penalty_threshold)\n    \n    # Balance Factor: Encourage a more balanced distribution\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor_threshold = 0.001\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / (np.max(np.abs(mean_cap - bins_remain_cap)) + balance_factor_threshold)\n    balance_penalty = 1 - balance_factor  # Inverse balance factor to penalize imbalance\n    \n    # Reward bins that are less likely to be the last bin filled (First Fit Decreasing heuristic)\n    sorted_bins = np.sort(bins_remain_cap)\n    sorted_indices = np.argsort(bins_remain_cap)\n    reversed_sorted_indices = sorted_indices[::-1]\n    rank_based_reward = np.zeros_like(bins_remain_cap)\n    for rank, idx in enumerate(reversed_sorted_indices):\n        rank_based_reward[idx] = rank  # Lower rank (better traditional fit) gets higher score\n    \n    # Adaptive coefficients based on the difference between the item size and mean bin capacity\n    delta = item - mean_cap\n    adaptive_alpha = 1 / (1 + np.exp(-delta))  # Smooth step function for dynamic weighting\n    adaptive_beta = 1 - adaptive_alpha\n    adaptive_gamma = 0.1  # Small constant for ranking reward to keep it balanced\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        adaptive_alpha * sigmoid_penalty +\n        adaptive_beta * balance_penalty +\n        adaptive_gamma * rank_based_reward\n    )\n\n    # Apply penalty for bins that cannot fit the item\n    priority_scores += penalty_for_large_items\n\n    return priority_scores\n\n### Analyze & experience\n- Comparing (best) vs (worst), we see that the best heuristic dynamically adjusts weights using adaptive learning and feedback mechanisms, whereas the worst heuristic lacks these adaptive features and uses static weights. The best also includes a more refined balance factor and dynamic thresholds. Comparing the second best vs the second worst, we observe that the second best also utilizes adaptive weights and a refined balance factor, unlike the second worst, which uses static weights and less nuanced balance considerations. In comparing the first vs second, we see minor improvements with the first in terms of dynamic weighting and refined thresholds, but both are strong candidates. Comparing the third vs fourth, we see a simplification in the fourth where weights are dynamically adjusted similarly but with less refined penalties. Comparing the second worst vs the worst, we see minimal differences in terms of adaptiveness and threshold usage. Overall: The best heuristic demonstrates superior adaptability and refinement in balance factors, making it significantly more effective in dynamic environments.\n- \n- **Keywords**: Adaptive learning, dynamic weights, threshold optimization, domain-specific data\n- **Advice**: Continuously calibrate weights and thresholds based on domain-specific data to optimize performance. Implement adaptive learning to refine balance factors over iterations.\n- **Avoid**: Overly complex algorithms, static parameters, random adjustments, and generic approaches.\n- **Explanation**: By focusing on dynamic and adaptive mechanisms grounded in specific domain data, the heuristic can become more responsive and efficient. This avoids the pitfalls of one-size-fits-all solutions and the inefficiencies caused by static parameters or random transformations.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}