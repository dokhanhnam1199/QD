{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritize bins by combining adaptive learning, dynamic weights, and balance factor with domain-specific adjustments.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        adaptation_factor: Rate of adaptation for weights based on historical data.\n        iteration: Current iteration number to dynamically adjust weights.\n        historical_data: Historical data of bin states and outcomes.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    if historical_data is None:\n        historical_data = np.zeros((100, len(bins_remain_cap)))  # Placeholder for historical data\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    sigmoid_penalty_threshold = 1e-5\n    scaled_remaining_capacity = np.where(\n        bins_remain_cap >= item, \n        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n        -np.inf\n    )\n\n    # Balance Factor: Encourage a more balanced distribution, adaptive\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + 1e-6)\n    adaptive_balance_factor = balance_factor * (1 + adaptation_factor * np.sum(historical_data[:, bins_remain_cap.argmin()]))\n\n    # Last Fit Decrease (LFD) Heuristic, adaptive\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n    adaptive_last_fit_decrease = last_fit_decrease * (1 + adaptation_factor * np.sum(historical_data[:, bins_remain_cap.argmax()]))\n\n    # Dynamic weights based on iteration\n    alpha = 0.9 / (1 + np.exp(-0.1 * (iteration - 20)))  # Adaptive weight for scaled remaining capacity\n    beta = 0.8 / (1 + np.exp(-0.1 * (iteration - 40)))   # Adaptive weight for balance factor\n    gamma = 0.3 / (1 + np.exp(-0.1 * (iteration - 60)))   # Adaptive weight for last fit decrease\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - adaptive_balance_factor) +\n        gamma * adaptive_last_fit_decrease\n    )\n\n    # Update historical data\n    historical_data[iteration % historical_data.shape[0]] = bins_remain_cap\n\n    return priority_scores\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritize bins by combining adaptive learning, dynamic balance factor, and refined penalty mechanisms.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Adaptive Threshold Calculation\n    max_bin_capacity = np.max(bins_remain_cap)\n    threshold_factor = 0.05  # Dynamic adjustment factor\n    sigmoid_penalty_threshold = max_bin_capacity * threshold_factor\n    balance_factor_threshold = max_bin_capacity * threshold_factor\n\n    # Scaled Remaining Capacity with sigmoid penalty\n    scaled_remaining_capacity = np.where(\n        bins_remain_cap >= item, \n        1.0 / (bins_remain_cap - item + sigmoid_penalty_threshold), \n        -np.inf\n    )\n\n    # Refined Balance Factor: Encourage a more balanced distribution with adaptive threshold\n    mean_cap = np.mean(bins_remain_cap)\n    balance_factor = np.where(\n        np.abs(mean_cap - bins_remain_cap) < balance_factor_threshold, \n        0, \n        np.abs(mean_cap - bins_remain_cap) / np.max(np.abs(mean_cap - bins_remain_cap) + balance_factor_threshold)\n    )\n\n    # Last Fit Decrease (LFD) Heuristic with dynamic weighting\n    last_fit_decrease = np.zeros_like(bins_remain_cap)\n    if len(bins_remain_cap) > 1:\n        last_fit_decrease[1:] = bins_remain_cap[:-1] - bins_remain_cap[1:]\n    last_fit_decrease_weight = 0.02  # Dynamic weight for LFD\n\n    # Dynamic Weights for heuristics based on the number of bins and average remaining capacity\n    alpha = 2 / (1 + np.exp(-0.5 * len(bins_remain_cap)))  # Scaled Remaining Capacity weight\n    beta = 1 - alpha  # Balance Factor weight\n\n    # Combine heuristics with adaptive learning\n    priority_scores = (\n        alpha * scaled_remaining_capacity +\n        beta * (1 - balance_factor) +\n        last_fit_decrease_weight * last_fit_decrease\n    )\n\n    return priority_scores\n\n### Analyze & experience\n- Comparing (best) vs (worst), we see that the best heuristic incorporates historical data for adaptive learning, adaptive thresholds, and dynamic weights, resulting in a more nuanced and adaptive approach. The worst heuristic lacks these features, relying on static weights and thresholds. (2nd) vs (2nd worst) shows a similar trend where the better heuristic uses adaptive learning and dynamic adjustments. Comparing (1st) vs (2nd), the primary difference lies in the inclusion of historical data and adaptation factors in the better heuristic. (3rd) vs (4th) indicates that the better heuristic uses adaptive weights and dynamic thresholds, whereas the worse one relies on static parameters. Comparing (second worst) vs (worst), the worse heuristic does not adjust weights or incorporate adaptive learning, leading to less effective prioritization. Overall, the top-ranked heuristics benefit from adaptive learning, dynamic weights, and historical data, enabling them to better manage varying conditions and optimize bin placement.\n- \n- **Keywords:** Adaptive learning, dynamic weights, refined balance factor, domain-specific data\n- **Advice:** \n  - Integrate machine learning techniques for adaptive learning to fine-tune heuristics over time.\n  - Continuously adjust weights and thresholds based on performance data and feedback loops to enhance adaptability.\n  - Employ refined balance factors that effectively balance incentives for optimal bin placement with penalties for inefficiency.\n  - Leverage domain-specific data for parameter tuning to optimize heuristic efficiency without increasing complexity.\n- **Avoid:** \n  - Repeated use of static coefficients without dynamic adjustment.\n  - Incorporating overly complex or random transformations that do not significantly improve performance.\n  - Excessive focus on random elements or overly complicated methods.\n- **Explanation:** By focusing on adaptive learning and continuous refinement with concrete parameters informed by domain-specific data, the heuristic can evolve and improve without becoming overly complex. This approach ensures the heuristic remains efficient, maintainable, and tailored to specific optimization tasks.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}