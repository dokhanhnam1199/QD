[
  {
    "stdout_filepath": "problem_iter1_response21.txt_stdout.txt",
    "code_path": "problem_iter1_code21.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    priorities[suitable_bins_mask] = 1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)\n    return priorities",
    "response_id": 21,
    "tryHS": false,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 1.0,
    "halstead": 41.51317942364757,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response11.txt_stdout.txt",
    "code_path": "problem_iter1_code11.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \n    bin_capacities = 1.0  # Assuming a standard bin capacity of 1.0, can be generalized.\n    \n    potential_fits = bins_remain_cap - item\n    \n    valid_bins_mask = potential_fits >= 0\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    \n    if np.any(valid_bins_mask):\n        \n        priorities[valid_bins_mask] = 1.0 / (potential_fits[valid_bins_mask] + 1e-9) # Add epsilon to avoid division by zero\n\n    return priorities",
    "response_id": 11,
    "tryHS": false,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 43.01955000865388,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response29.txt_stdout.txt",
    "code_path": "problem_iter1_code29.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for each bin using a Softmax-Based Fit strategy\n    for the online Bin Packing Problem.\n\n    This strategy prioritizes bins that have remaining capacity, with a higher\n    priority given to bins that can accommodate the item without significant\n    wastage, and also considers the overall \"fullness\" of bins.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the\n                         remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    # Filter out bins that cannot fit the item\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        # If no bin can fit the item, return zero priorities for all bins\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate a score for each valid bin:\n    # We want to favor bins that are nearly full after placing the item.\n    # (capacity - item) represents the remaining capacity after placement.\n    # Smaller values of (capacity - item) are better.\n    # We can invert this by taking the negative or by calculating (item - capacity) if capacity < item.\n    # For simplicity and Softmax compatibility, let's focus on the 'goodness' of fit.\n    # A good fit means small remaining capacity. So, we can use 1 / (remaining_after_fit)\n    # or something similar.\n\n    # A common heuristic for BPP is \"Best Fit\": choosing the bin that leaves the least empty space.\n    # So, remaining_capacity - item should be minimized.\n    # We want to maximize the \"suitability\" score.\n    # Let's consider the negative of the remaining capacity after fitting as a base score.\n    # Larger negative values (closer to zero) are better.\n    base_scores = -(valid_bins_remain_cap - item)\n\n    # Add a penalty for bins that are already very full, encouraging spreading items if possible,\n    # unless an item perfectly fits. This can be tricky.\n    # For simplicity in v2, let's focus on the immediate fit.\n\n    # Apply Softmax to convert scores into probabilities (priorities)\n    # Softmax: exp(score) / sum(exp(all_scores))\n    # To avoid numerical instability with very large or small scores, we can shift scores.\n    # Subtracting the maximum score before exponentiation is a common technique.\n    shifted_scores = base_scores - np.max(base_scores)\n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Create the final priority array, placing calculated priorities in their original positions\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities",
    "response_id": 29,
    "tryHS": true,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 66.41714012534482,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response4.txt_stdout.txt",
    "code_path": "problem_iter2_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins by rewarding perfect fits and then favoring tighter fits using inverse proximity, normalized via Softmax.\n\n    Combines the 'perfect fit' bonus from priority_v0 with the normalized inverse proximity of priority_v1.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate effective remaining capacities for fitting bins\n    effective_capacities = bins_remain_cap[can_fit_mask] - item\n    \n    # Assign scores: perfect fits get a bonus, others get inverse proximity\n    scores = np.zeros_like(effective_capacities, dtype=float)\n    \n    perfect_fit_mask = effective_capacities == 0\n    scores[perfect_fit_mask] = 1.0  # High priority for perfect fits\n    \n    # For non-perfect fits, use inverse proximity to reward tighter fits\n    non_perfect_fit_mask = ~perfect_fit_mask\n    scores[non_perfect_fit_mask] = 1.0 / (effective_capacities[non_perfect_fit_mask] + 1e-9)\n    \n    # Apply scores to the corresponding bins\n    priorities[can_fit_mask] = scores\n    \n    # If no bins can fit the item, return uniform probabilities\n    if not np.any(can_fit_mask):\n        return np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n        \n    # Normalize priorities using Softmax to get a probability distribution\n    # Subtract max score for numerical stability before exponentiation\n    max_score = np.max(priorities[can_fit_mask])\n    exp_scores = np.exp(priorities[can_fit_mask] - max_score)\n    \n    normalized_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    normalized_priorities[can_fit_mask] = exp_scores / np.sum(exp_scores)\n    \n    return normalized_priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 128.3789500201924,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response14.txt_stdout.txt",
    "code_path": "problem_iter1_code14.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            priorities[i] = 1.0 / (bins_remain_cap[i] - item + 1e-9)\n    return priorities",
    "response_id": 14,
    "tryHS": false,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 3.0,
    "halstead": 41.51317942364757,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response15.txt_stdout.txt",
    "code_path": "problem_iter1_code15.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            \n            remaining_cap = bins_remain_cap[i]\n            \n            \n            if remaining_cap == item:\n                priorities[i] = 1.0  # Perfect fit, highest priority\n            else:\n                \n                priorities[i] = 1.0 / (remaining_cap - item + 1e-9) # Inverse proximity, higher score for bins that are closer to fitting the item without being perfect\n                \n    return priorities",
    "response_id": 15,
    "tryHS": false,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 4.0,
    "halstead": 53.77443751081735,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    valid_bins = bins_remain_cap >= item\n    \n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n    \n    effective_capacities = bins_remain_cap[valid_bins] - item\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bins] = np.exp(effective_capacities)\n    \n    if np.sum(priorities) == 0:\n        return np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n        \n    return priorities / np.sum(priorities)",
    "response_id": 19,
    "tryHS": false,
    "obj": 19.186278420422827,
    "cyclomatic_complexity": 3.0,
    "halstead": 66.41714012534482,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines best-fit inverse proximity with explicit perfect-fit handling\n    and Softmax normalization for robust bin selection.\n    Prioritizes bins that are a perfect fit, then bins with minimal remaining\n    space, normalized using Softmax for probabilistic selection.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(valid_bins_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[valid_bins_mask]\n    \n    perfect_fits_mask = valid_bins_capacities == item\n    \n    if np.any(perfect_fits_mask):\n        perfect_fit_indices = np.where(valid_bins_mask)[0][perfect_fits_mask]\n        priorities[perfect_fit_indices] = 1.0 \n\n    non_perfect_fit_indices = np.where(valid_bins_mask)[0][~perfect_fits_mask]\n    \n    if len(non_perfect_fit_indices) > 0:\n        non_perfect_capacities = bins_remain_cap[non_perfect_fit_indices]\n        effective_capacities = non_perfect_capacities - item\n        \n        # Use inverse proximity for non-perfect fits, preventing division by zero\n        priorities[non_perfect_fit_indices] = 1.0 / (effective_capacities + 1e-9)\n\n    # Softmax normalization to ensure probabilities sum to 1, or handle all-zero case\n    # Subtracting max before exp for numerical stability\n    max_priority = np.max(priorities)\n    exp_priorities = np.exp(priorities - max_priority)\n    \n    sum_exp_priorities = np.sum(exp_priorities)\n    \n    if sum_exp_priorities == 0:\n        # If all calculated priorities are zero (e.g., only invalid bins), return uniform distribution\n        return np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n    else:\n        return exp_priorities / sum_exp_priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 5.0,
    "halstead": 159.81495041679716,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response0.txt_stdout.txt",
    "code_path": "problem_iter3_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for each bin using a Hybrid Softmax-Based Fit strategy\n    for the online Bin Packing Problem.\n\n    This strategy aims to balance 'Best Fit' (minimizing waste) with a\n    'First Fit' tendency for larger items by introducing a penalty for\n    bins that would leave very little space after packing. It also incorporates\n    a diversification element by slightly favoring less full bins to avoid\n    prematurely filling a few bins.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the\n                         remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Base score: Invert remaining capacity after fitting. Lower remaining is better.\n    # This promotes 'Best Fit'.\n    best_fit_scores = -(valid_bins_remain_cap - item)\n\n    # Diversification/Exploration score:\n    # Favor bins that are not already too full. This encourages spreading items.\n    # A higher score for bins with more remaining capacity (but still fitting the item).\n    # We use a sigmoid-like function to bound this influence.\n    # The idea is to give a slight boost to bins that are not nearly full,\n    # especially if the item is large.\n    # We can use 1 / (1 + exp(-(capacity - item - threshold))) where threshold is some value\n    # or more simply, a transformation of the remaining capacity after packing.\n    # Let's consider the remaining capacity itself as a measure. Higher remaining capacity\n    # after packing is less preferred for 'Best Fit', but might be good for diversification.\n    # We can create a score that is inversely related to how \"full\" the bin becomes.\n    # A bin that becomes almost full (low remaining cap) is good for BF, bad for diversity.\n    # A bin that remains very open (high remaining cap) is bad for BF, good for diversity.\n\n    # Let's create a score that penalizes bins that will have very little remaining space.\n    # This is a form of \"near miss\" avoidance for the next items.\n    # A small remaining capacity after packing (e.g., < item/2) could be penalized.\n    # Let's map the remaining capacity after packing `rem_after_fit` to a score.\n    rem_after_fit = valid_bins_remain_cap - item\n    \n    # Soft penalty for small remaining capacities. If rem_after_fit is small, this score is high.\n    # We want to *reduce* the priority for bins that leave very little space.\n    # Use a sigmoid-like function that maps small positive values to a range close to 0,\n    # and larger values to a range close to 1.\n    # We want to penalize small `rem_after_fit`.\n    # So, a function that is high for small `rem_after_fit` and low for large `rem_after_fit`\n    # is needed, and this should be subtracted from the main score.\n    \n    # A simple penalty: penalize if remaining capacity is less than item/2.\n    # More nuanced: exponential decay of penalty as remaining capacity increases.\n    # Let's use a logistic function scaled to penalize small positive remaining capacities.\n    # f(x) = 1 / (1 + exp(-k * (x - x0)))\n    # We want a function that is high when rem_after_fit is low.\n    # So we can use: penalty = 1 / (1 + exp(k * rem_after_fit)) where k is positive.\n    # Or, more simply, we can use a score that increases with remaining capacity,\n    # but we want to *reduce* the priority of bins with low remaining capacity.\n    \n    # Let's refine the score:\n    # Primary goal: Minimize `rem_after_fit` (Best Fit). Score = -rem_after_fit.\n    # Secondary goal: Avoid making bins too full if possible (Diversification).\n    # This means, if multiple bins offer similar \"best fit\", prefer the one that\n    # was initially less full.\n    # This is tricky to encode directly in a simple priority score.\n\n    # Alternative approach: Combine Best Fit with a bonus for initial capacity.\n    # Score = -rem_after_fit + alpha * initial_capacity_of_bin\n    # However, we don't have initial capacity, only remaining.\n\n    # Let's try a score that is good for Best Fit, but has a \"decay\" for being too full.\n    # Consider the \"gap\" created: `valid_bins_remain_cap - item`.\n    # We want to minimize this gap.\n    # Let's add a term that slightly favors bins that are less full initially.\n    # We can use the *current* remaining capacity as a proxy for how full the bin is.\n    # Higher `valid_bins_remain_cap` means the bin is less full.\n    \n    # Let's try a composite score:\n    # Score = w1 * (- (valid_bins_remain_cap - item)) + w2 * (valid_bins_remain_cap)\n    # The first term is Best Fit. The second term favors less full bins.\n    # Let's normalize these to avoid one dominating the other.\n\n    # Normalization factor for Best Fit: The maximum possible \"goodness\" is 0 (perfect fit).\n    # The worst \"goodness\" is -(max_capacity - min_item_size).\n    # Let's normalize `-(valid_bins_remain_cap - item)` to be between 0 and 1 (roughly).\n    # Or, more simply, let's use the raw negative remaining capacity.\n\n    # Let's introduce a \"diversification bonus\" that is proportional to the remaining capacity\n    # of the bin *before* packing. This encourages using bins that have more space available,\n    # as long as they fit the item.\n    # `bonus = alpha * valid_bins_remain_cap` where alpha is a small positive weight.\n    # This bonus counters the Best Fit score if the remaining capacity is significantly large.\n\n    alpha = 0.1  # Weight for diversification bonus. Tune this parameter.\n    diversification_bonus = alpha * valid_bins_remain_cap\n    \n    # Combined score: Best Fit score + diversification bonus\n    # Higher score is better.\n    # We want to minimize `valid_bins_remain_cap - item`. So we want to maximize `-(valid_bins_remain_cap - item)`.\n    # Higher `valid_bins_remain_cap` is better for diversification.\n    \n    # So, we want to maximize `-(valid_bins_remain_cap - item) + alpha * valid_bins_remain_cap`\n    # This simplifies to `alpha * valid_bins_remain_cap - valid_bins_remain_cap + item`\n    # which is `(alpha - 1) * valid_bins_remain_cap + item`.\n    # This still favors larger `valid_bins_remain_cap` if `alpha < 1`, which is the case.\n    \n    # Let's reconsider the goal. We want bins that are *almost* full (good fit),\n    # but not *too* full such that the remaining space is almost unusable.\n    # This suggests a function that peaks for intermediate remaining capacities after packing.\n\n    # A refined approach:\n    # 1. Best Fit score: `-(valid_bins_remain_cap - item)`. Maximize this.\n    # 2. Penalty for \"too little\" remaining space: `exp(-beta * (valid_bins_remain_cap - item))`\n    #    where beta is a positive constant. This term is high for small remaining space,\n    #    and we want to penalize high values. So, we subtract this penalty.\n    \n    beta = 2.0  # Penalty factor for small remaining space.\n    # We want to penalize small positive remaining capacities.\n    # Let rem_cap_after_packing = valid_bins_remain_cap - item\n    # Penalty increases as rem_cap_after_packing approaches 0.\n    # A function like `exp(-beta * rem_cap_after_packing)` works.\n    # If rem_cap_after_packing = 0, penalty is 1. If rem_cap_after_packing is large, penalty approaches 0.\n    # So, we subtract this penalty.\n    \n    # Let's combine:\n    # Score = -(valid_bins_remain_cap - item) - penalty_factor * exp(-beta * (valid_bins_remain_cap - item))\n    # This aims to reward good fits, but slightly disincentivize fits that leave almost no space.\n\n    # Let's try a simpler form that is more directly interpretable with Softmax.\n    # We want to prioritize bins where `valid_bins_remain_cap - item` is small.\n    # Let's introduce a \"niceness\" score.\n    # `niceness = 1.0 / (1.0 + (valid_bins_remain_cap - item))` -- this is high for small remaining space.\n    # But Softmax needs scores that can be positive/negative.\n\n    # Let's go back to the composite score idea, but ensure Softmax handles it well.\n    # We want to prioritize bins with small `(remaining_capacity - item)`.\n    # We also want to slightly favor bins that have more overall capacity (less full).\n    # So, we want to maximize `-(remaining_capacity - item) + alpha * remaining_capacity`.\n    \n    # Let's re-evaluate the original v1's Softmax base: `-(valid_bins_remain_cap - item)`.\n    # This encourages Best Fit.\n    # To add diversification, we can add a term that is higher for bins that are less full.\n    # `current_remaining_capacity` is a proxy for \"less full\".\n    # So, `score = -(valid_bins_remain_cap - item) + gamma * valid_bins_remain_cap`.\n    # `gamma` is a small positive number.\n    \n    gamma = 0.2  # Weight for diversification (favoring less full bins).\n    \n    # The score is `-(valid_bins_remain_cap - item) + gamma * valid_bins_remain_cap`\n    # = `-valid_bins_remain_cap + item + gamma * valid_bins_remain_cap`\n    # = `(gamma - 1) * valid_bins_remain_cap + item`.\n    \n    # This score will be higher for larger `valid_bins_remain_cap` if `gamma < 1`,\n    # which is the intended effect of diversification.\n    # However, we also want to prioritize Best Fit.\n\n    # Let's normalize the contribution of each term to prevent one from dominating.\n    # Best Fit contribution: `-(valid_bins_remain_cap - item)`\n    # Diversification contribution: `valid_bins_remain_cap`\n\n    # Maximum possible best fit score: 0 (perfect fit). Minimum: -(max_cap - min_item).\n    # Maximum possible diversification contribution: max_capacity. Minimum: min_fitting_capacity.\n\n    # A common strategy is to use a weighted sum, and then apply softmax.\n    # Let's use the raw scores and rely on Softmax scaling.\n    \n    # Final proposed score for each valid bin:\n    # Score = w_bf * BestFitScore + w_div * DiversificationScore\n    # BestFitScore = -(remaining_capacity_after_packing) = -(valid_bins_remain_cap - item)\n    # DiversificationScore = current_remaining_capacity = valid_bins_remain_cap\n    \n    w_bf = 1.0\n    w_div = 0.3 # Tune this weight. Higher means more preference for less full bins.\n    \n    composite_scores = w_bf * (-(valid_bins_remain_cap - item)) + w_div * valid_bins_remain_cap\n\n    # Softmax transformation:\n    # Shift scores to avoid numerical instability (large positive/negative values)\n    # Subtracting the maximum score is standard.\n    if composite_scores.size > 0:\n        shifted_scores = composite_scores - np.max(composite_scores)\n        exp_scores = np.exp(shifted_scores)\n        probabilities = exp_scores / np.sum(exp_scores)\n    else:\n        probabilities = np.array([])\n\n    # Create the final priority array, placing calculated priorities in their original positions\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 185.75424759098897,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response8.txt_stdout.txt",
    "code_path": "problem_iter2_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (minimizing waste) with a Softmax approach for robust priority.\n\n    Prioritizes bins that leave minimal remaining capacity after item placement,\n    using Softmax for smooth probability distribution and better exploration.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(valid_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    # Base score: Negative of remaining capacity after fitting (closer to 0 is better)\n    # This embodies the \"Best Fit\" principle.\n    base_scores = -(suitable_bins_remain_cap - item)\n\n    # Softmax for normalization: Convert scores to a probability-like distribution\n    # Shift scores to prevent overflow/underflow before exponentiation\n    if np.max(base_scores) - np.min(base_scores) > 1e-9: # Avoid issues if all scores are identical\n        shifted_scores = base_scores - np.max(base_scores)\n        exp_scores = np.exp(shifted_scores)\n        # Ensure sum is not zero to avoid division by zero\n        sum_exp_scores = np.sum(exp_scores)\n        if sum_exp_scores > 1e-9:\n            probabilities = exp_scores / sum_exp_scores\n        else:\n            # Fallback if all exponentiated scores are effectively zero\n            probabilities = np.ones_like(base_scores) / len(base_scores)\n    else:\n        # If all base scores are the same, assign equal probability\n        probabilities = np.ones_like(base_scores) / len(base_scores)\n\n\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 140.2304206377674,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response4.txt_stdout.txt",
    "code_path": "problem_iter5_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for overly full bins and explicit perfect fit reward.\n    Prioritizes bins that fit the item well, penalizes excessive remaining space,\n    and gives a strong boost to perfect fits before Softmax normalization.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Base score: Prioritize bins that leave less remaining space (Best Fit).\n    # Maximize (item - remaining_capacity_after_fit), equivalent to minimize (remaining_capacity_after_fit).\n    # Use negative remaining capacity after fit as a base score for maximization.\n    base_scores = -(valid_bins_remain_cap - item)\n\n    # Add bonus for perfect fits\n    perfect_fit_bonus = 0.5  # A significant bonus\n    perfect_fit_mask = (valid_bins_remain_cap - item) < 1e-9\n    base_scores[perfect_fit_mask] += perfect_fit_bonus\n\n    # Add a slight penalty for bins that would become \"too empty\" after fitting.\n    # This encourages using bins that are already somewhat full.\n    # For example, if remaining_cap - item > threshold, subtract a penalty.\n    # Let's define \"too empty\" as having more than 75% of the original bin capacity remaining\n    # after the item is placed. This is a tunable parameter.\n    too_empty_threshold_ratio = 0.75\n    original_bin_capacities = bins_remain_cap[valid_bins_mask] # This assumes we know original bin sizes, which we don't have directly.\n    # A proxy: Penalize if remaining_cap - item is large relative to the item size itself,\n    # or relative to the original bin capacity if we had it.\n    # Let's use a penalty if the remaining space after fitting is large relative to the bin's *current* capacity.\n    # This might be problematic if current capacity is very small.\n    # A simpler approach is to penalize based on the resulting 'waste' if the fit is not tight.\n    # Let's penalize bins where `remaining_cap - item` is large.\n    # We want to *minimize* this value. So, we add a penalty to the score if it's large.\n    # This means subtracting a positive value from the score if `remaining_cap - item` is large.\n    large_remaining_penalty_factor = 0.1 # Tune this\n    penalty_threshold = 0.5 # Penalize if remaining space > 50% of original bin capacity (proxy: current remaining capacity)\n    \n    # Calculate penalty: if (valid_bins_remain_cap - item) > penalty_threshold * valid_bins_remain_cap\n    # This means remaining space is more than 50% of what was available.\n    # We want to penalize these. Subtract a value.\n    penalty = np.maximum(0, (valid_bins_remain_cap - item) - penalty_threshold * valid_bins_remain_cap) * large_remaining_penalty_factor\n    \n    scores = base_scores - penalty\n\n    # Softmax normalization\n    shifted_scores = scores - np.max(scores)\n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 199.1772208002305,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response3.txt_stdout.txt",
    "code_path": "problem_iter3_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for each bin using a combined strategy for\n    the online Bin Packing Problem, balancing \"best fit\" with a penalty for\n    bins that would become excessively full.\n\n    This strategy prioritizes bins that can accommodate the item, favoring those\n    that result in less remaining capacity (\"best fit\"). Additionally, it\n    introduces a penalty for bins that, after packing the item, would have\n    very little remaining capacity, encouraging a more diversified packing\n    strategy to potentially avoid early bin exhaustion.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the\n                         remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # \"Best Fit\" component: Minimize remaining capacity after placing the item.\n    # Higher score for smaller remaining capacity.\n    best_fit_scores = -(valid_bins_remain_cap - item)\n\n    # Diversification component: Penalize bins that become too full.\n    # A threshold can be used, e.g., if remaining_capacity - item < threshold.\n    # Here, we use a sigmoid-like function to penalize as remaining capacity decreases.\n    # This encourages not filling bins too tightly unless absolutely necessary.\n    # We want to *reduce* the score if the bin becomes too full.\n    # A smaller (remaining_capacity - item) means a fuller bin.\n    # Let's create a penalty that is high when (remaining_capacity - item) is low.\n    # Using exp(-x) where x is a scaled version of (remaining_capacity - item)\n    # Smaller x (tighter fit) leads to larger penalty.\n    # We need to be careful with scaling to avoid numerical issues.\n    # Let's use a threshold-based penalty that is smooth.\n    # Consider remaining capacity after packing: `rem_after_pack = valid_bins_remain_cap - item`\n    # We want to penalize small `rem_after_pack`.\n    # A simple penalty could be `penalty = 1 / (1 + exp(k * rem_after_pack))`\n    # where k controls the steepness. A small `rem_after_pack` results in large penalty.\n    # Let's use a small value for k to make it a soft penalty.\n    # Scale remaining capacity to avoid overflow in exp.\n    # For example, map to [0, 10] range.\n    min_rem_after_pack = 0.0\n    max_rem_after_pack = np.max(valid_bins_remain_cap) - item if np.max(valid_bins_remain_cap) >= item else 0.0\n    \n    if max_rem_after_pack == min_rem_after_pack: # Avoid division by zero if all valid bins have same remaining capacity\n        diversification_penalty = np.zeros_like(best_fit_scores)\n    else:\n        rem_after_pack = valid_bins_remain_cap - item\n        # Scale remaining capacity to a reasonable range for the penalty function\n        # e.g., map to [0, 10] to control the steepness of the penalty\n        scaled_rem_after_pack = 10 * (rem_after_pack - min_rem_after_pack) / (max_rem_after_pack - min_rem_after_pack)\n        # Penalty is high for small remaining capacity (tight fit)\n        diversification_penalty = 1.0 / (1.0 + np.exp(5.0 * (1.0 - scaled_rem_after_pack))) # penalty ~ 1 for tight fit, ~ 0 for loose fit\n\n    # Combine scores: Favor best fit, but penalize overly tight fits.\n    # We subtract the penalty from the best_fit_scores.\n    # A higher combined score is better.\n    combined_scores = best_fit_scores - diversification_penalty * 2.0 # Adjust multiplier for penalty strength\n\n    # Apply Softmax to convert scores into probabilities (priorities)\n    # Shift scores to avoid numerical overflow/underflow in exp\n    shifted_scores = combined_scores - np.max(combined_scores)\n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Create the final priority array\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 305.0255750850964,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response4.txt_stdout.txt",
    "code_path": "problem_iter3_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for each bin using an adaptive strategy for the\n    online Bin Packing Problem. This version aims to balance fitting tightly\n    (Best Fit) with spreading items (Worst Fit) and incorporating an element\n    of exploration.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the\n                         remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # --- Core Strategy: Balancing Best Fit and Worst Fit ---\n    # Best Fit component: Prioritize bins that leave minimal remaining space.\n    # We want to minimize (remaining_capacity - item).\n    # A good score for BF would be proportional to -(remaining_capacity - item).\n    # For Softmax, we want higher scores for better options. So, let's use\n    # a score that increases as remaining_capacity - item decreases.\n    # A simple inversion: 1 / (remaining_capacity - item + epsilon)\n    # Or, to keep it related to the previous approach: maximize -(remaining_capacity - item)\n    # To promote diversification, let's also consider the \"emptiness\" of the bin.\n    # Worst Fit component: Prioritize bins with *more* remaining capacity.\n    # This encourages spreading items.\n    # A score for WF could be proportional to remaining_capacity.\n\n    # Let's create a blended score.\n    # For Best Fit: prioritize small remaining capacity after placing the item.\n    # For Worst Fit: prioritize large initial remaining capacity.\n    # We want to maximize the utility.\n    # Let's consider the utility as a function of remaining capacity:\n    # utility = alpha * (1 / (valid_bins_remain_cap - item + 1e-9)) + beta * valid_bins_remain_cap\n\n    # For simplicity and to adapt the softmax approach, let's define scores\n    # where higher means more desirable.\n    # High score for small (remaining_capacity - item) => Best Fit tendency\n    # High score for large remaining_capacity => Worst Fit tendency (for exploration/diversification)\n\n    # Let's try a score that is a combination:\n    # Score = (large_capacity_bonus) * (remaining_capacity) - (misfit_penalty) * (remaining_capacity - item)\n    # A simpler approach:\n    # Prioritize bins where remaining_capacity - item is small (BF)\n    # BUT, also give a boost to bins that are \"more open\" (WF) to avoid early convergence.\n\n    # Let's combine the ideas:\n    # We want to favor small (remaining_capacity - item).\n    # Let's define a score for \"tightness\": TightnessScore = -(valid_bins_remain_cap - item)\n    # And a score for \"openness\": OpennessScore = valid_bins_remain_cap\n\n    # We can create a combined score, for example, by averaging or taking a weighted sum.\n    # A more robust approach is to introduce a \"temperature\" or \"exploration factor\"\n    # that modulates the influence of the Best Fit vs. Worst Fit tendencies.\n\n    # Let's try a score that is a compromise. We want to minimize (remaining_capacity - item).\n    # Let's use a function that is high when (remaining_capacity - item) is small.\n    # Consider a function like: `exp(-k * (remaining_capacity - item))`\n    # `k` can be an exploration parameter. A large `k` makes it more like Best Fit.\n    # A small `k` makes it flatter, more exploratory.\n\n    # To balance exploration and exploitation, let's make the \"tightness\" score\n    # have an exploratory element.\n    # Let's use a score that is high for bins that are \"good\" fits, but also\n    # has some preference for bins that aren't *too* full if an exact fit isn't available.\n\n    # --- Adaptive Exploration/Exploitation ---\n    # We can adapt the strength of the \"Best Fit\" tendency based on the distribution of remaining capacities.\n    # If capacities are very diverse, lean more towards Best Fit.\n    # If capacities are very similar, lean more towards diversification.\n\n    # A simple adaptation: Use a parameter `epsilon` that smooths the selection.\n    # Larger epsilon makes it more uniform (exploratory). Smaller epsilon makes it more greedy (exploitative).\n    # We can define epsilon based on the variance of remaining capacities.\n    variance_remain_cap = np.var(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 1.0\n    # Scale variance to a reasonable epsilon range. Higher variance -> higher epsilon for more exploration.\n    epsilon = 0.1 + 0.5 * (1 / (1 + np.exp(-0.1 * variance_remain_cap))) # Sigmoid to bound epsilon\n\n    # Calculate scores:\n    # Score for \"good fit\" (lower remaining_capacity - item is better)\n    # Using a negative exponential for a sharp decrease in score as misfit increases.\n    # Adding epsilon for numerical stability and exploration.\n    goodness_of_fit_scores = np.exp(-10.0 * (valid_bins_remain_cap - item) / (item + 1e-9)) # Scale by item size\n\n    # Score for \"openness\" (higher remaining_capacity is better for spreading)\n    # Using a scaled exponential to give a significant boost to very open bins.\n    openness_scores = np.exp(0.1 * valid_bins_remain_cap / (np.max(bins_remain_cap) + 1e-9)) # Scale by max capacity\n\n    # Combine scores using epsilon for adaptive weighting\n    # When epsilon is high (diverse capacities), openness_scores have more weight.\n    # When epsilon is low (similar capacities), goodness_of_fit_scores have more weight.\n    combined_scores = (1 - epsilon) * goodness_of_fit_scores + epsilon * openness_scores\n\n    # Softmax transformation to get priorities\n    # Shift scores to prevent overflow/underflow before exponentiation\n    shifted_scores = combined_scores - np.max(combined_scores)\n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Create the final priority array\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 55.47467092142002,
    "cyclomatic_complexity": 3.0,
    "halstead": 375.02864032326585,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response0.txt_stdout.txt",
    "code_path": "problem_iter5_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates bin priorities using a hybrid approach:\n    1. Prioritizes perfect fits (zero remaining capacity).\n    2. Favors \"best fit\" bins by penalizing remaining capacity.\n    3. Applies a penalty to avoid overly tight fits, promoting diversity.\n    4. Normalizes scores using Softmax for a probabilistic distribution.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Handle the case where no bins can fit the item\n    if not np.any(can_fit_mask):\n        return priorities\n    \n    # Extract capacities of bins that can fit the item\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Calculate remaining capacity after packing for valid bins\n    rem_after_pack = valid_bins_remain_cap - item\n    \n    # --- Component 1: Perfect Fit Bonus ---\n    # High score for bins that become exactly full\n    perfect_fit_bonus = np.where(rem_after_pack == 0, 10.0, 0.0)\n    \n    # --- Component 2: Best Fit (Inverse Proximity) ---\n    # Score inversely proportional to remaining capacity (smaller remaining capacity is better)\n    # Add a small epsilon to avoid division by zero if remaining capacity is exactly 0 (handled by bonus)\n    best_fit_score = -rem_after_pack\n    \n    # --- Component 3: Diversification Penalty ---\n    # Penalize bins that would have very little remaining capacity after packing,\n    # but only if they are not a perfect fit.\n    # We use a sigmoid-like function to penalize as remaining capacity decreases.\n    # Scale remaining capacity to avoid large exponential values.\n    # The penalty is higher for smaller `rem_after_pack`.\n    min_rem = np.min(rem_after_pack[rem_after_pack > 0]) if np.any(rem_after_pack > 0) else 0\n    max_rem = np.max(rem_after_pack) if np.any(rem_after_pack > 0) else 0\n    \n    diversification_penalty = np.zeros_like(valid_bins_remain_cap)\n    if max_rem > min_rem: # Avoid division by zero if all valid bins have the same remaining capacity > 0\n        # Scale non-zero remaining capacities to a range like [0, 10]\n        scaled_rem = 10 * (rem_after_pack[rem_after_pack > 0] - min_rem) / (max_rem - min_rem)\n        # Apply penalty: higher penalty for smaller scaled_rem (tighter fits)\n        # Using exp(k * (1 - x)) makes the penalty high when x is small.\n        penalty_strength = 5.0\n        diversification_penalty[rem_after_pack > 0] = 1.0 / (1.0 + np.exp(penalty_strength * (1.0 - scaled_rem)))\n    elif np.any(rem_after_pack == 0): # If there are perfect fits but no other valid fits, no diversification penalty needed\n        pass\n    elif np.any(rem_after_pack > 0): # If all valid fits have the same positive remaining capacity\n        pass # No penalty needed as they are all equally \"open\"\n\n    # Combine scores: perfect_fit_bonus + best_fit_score - diversification_penalty * weight\n    # The weight controls how strongly we penalize tight fits.\n    penalty_weight = 2.0\n    combined_scores = perfect_fit_bonus + best_fit_score - diversification_penalty * penalty_weight\n    \n    # --- Component 4: Softmax Normalization ---\n    # Convert scores to probabilities (priorities)\n    # Shift scores to avoid numerical instability with exp\n    shifted_scores = combined_scores - np.max(combined_scores)\n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Place probabilities back into the original priority array structure\n    priorities[can_fit_mask] = probabilities\n    \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 7.0,
    "halstead": 398.8424910217125,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response7.txt_stdout.txt",
    "code_path": "problem_iter5_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid heuristic combining Best Fit with a penalty for overly tight fits\n    and a bonus for bins with ample remaining capacity.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(valid_bins_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    # Best Fit Component: Penalize based on remaining capacity after packing.\n    # Smaller remaining capacity is better.\n    best_fit_scores = -(valid_bins_remain_cap - item)\n\n    # Diversification Component: Reward bins with more initial remaining capacity.\n    # This favors less full bins.\n    diversification_scores = valid_bins_remain_cap\n\n    # Combine components with weights. Tune these weights based on empirical performance.\n    # w_bf = 1.0 emphasizes Best Fit.\n    # w_div = 0.3 gives a moderate preference to less full bins.\n    w_bf = 1.0\n    w_div = 0.3\n    \n    combined_scores = w_bf * best_fit_scores + w_div * diversification_scores\n\n    # Apply Softmax to get probabilities.\n    # Shift scores to prevent overflow/underflow issues with large exponents.\n    if combined_scores.size > 0:\n        shifted_scores = combined_scores - np.max(combined_scores)\n        exp_scores = np.exp(shifted_scores)\n        probabilities = exp_scores / np.sum(exp_scores)\n    else:\n        probabilities = np.array([])\n\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 130.02797331369229,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response9.txt_stdout.txt",
    "code_path": "problem_iter5_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for overly full bins and an explicit bonus for perfect fits.\n    Prioritizes bins that closely fit the item, penalizes those that become too full,\n    and gives a high reward for exact matches.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate remaining capacity after packing\n    rem_after_pack = valid_bins_remain_cap - item\n\n    # Component 1: \"Best Fit\" - prioritize smaller remaining capacity\n    # Higher score for smaller remaining capacity\n    best_fit_scores = -rem_after_pack\n\n    # Component 2: Penalty for overly full bins (makes remaining capacity very small)\n    # We want to reduce the score if rem_after_pack is close to zero.\n    # Use an exponential decay that is strong for small remaining capacity.\n    # Scale rem_after_pack to avoid overflow/underflow in exp and control sensitivity.\n    min_rem = 0.0\n    max_rem = np.max(valid_bins_remain_cap) - item if np.max(valid_bins_remain_cap) >= item else 0.0\n    \n    if max_rem == min_rem: # Handle case where all valid bins have same remaining capacity\n        diversification_penalty = np.zeros_like(best_fit_scores)\n    else:\n        scaled_rem = 5 * rem_after_pack / max_rem # Scale to [0, 5] range\n        # Penalty is high when scaled_rem is low (i.e., tight fit)\n        # Use 1 - sigmoid to get a penalty that is high for low values\n        diversification_penalty = 1.0 - (1.0 / (1.0 + np.exp(10 * (1.0 - scaled_rem)))) # Penalty ~ 1 for tight fit, ~ 0 for loose fit\n\n    # Component 3: Bonus for perfect fits\n    # Assign a significantly higher score to bins where rem_after_pack is zero (or very close to zero)\n    perfect_fit_bonus = np.zeros_like(best_fit_scores)\n    perfect_fit_threshold = 1e-9 # Tolerance for floating point comparison\n    is_perfect_fit = rem_after_pack < perfect_fit_threshold\n    perfect_fit_bonus[is_perfect_fit] = 10.0 # Large bonus for perfect fits\n\n    # Combine scores: best_fit + bonus - penalty\n    # The penalty is subtracted from the score to reduce priority for overly full bins.\n    # The bonus is added to strongly favor perfect fits.\n    combined_scores = best_fit_scores + perfect_fit_bonus - diversification_penalty * 2.0 # Penalty multiplier to control its impact\n\n    # Apply Softmax to get probability distribution\n    # Shift scores to prevent overflow/underflow in exp\n    if combined_scores.size > 0:\n        shifted_scores = combined_scores - np.max(combined_scores)\n        exp_scores = np.exp(shifted_scores)\n        probabilities = exp_scores / np.sum(exp_scores)\n    else:\n        probabilities = np.array([])\n\n    # Map probabilities back to the original bins array\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 5.0,
    "halstead": 327.6949375694594,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response3.txt_stdout.txt",
    "code_path": "problem_iter6_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for each bin using an adaptive, multi-objective\n    approach for the online Bin Packing Problem.\n\n    This heuristic aims to balance immediate fit (minimizing waste) with\n    long-term bin utilization. It prioritizes bins that are a good fit for the\n    current item, while also considering bins that have a moderate amount of\n    remaining capacity, potentially for future larger items. It uses a\n    weighted sum of two components, with weights adaptively adjusted based on\n    the item's size relative to the average bin capacity and the proportion of\n    already occupied bins.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the\n                         remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    if num_bins == 0:\n        return np.array([], dtype=float)\n\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Component 1: Best Fit - Minimize remaining capacity after placing the item\n    # A smaller (remaining_capacity - item) is better. We want to maximize this score.\n    # We use a large negative value for bins that don't fit, then shift.\n    best_fit_scores = np.full_like(bins_remain_cap, -np.inf)\n    best_fit_scores[valid_bins_mask] = -(valid_bins_remain_cap - item)\n\n    # Component 2: Spread/Fill - Favor bins that are not too full, but also not too empty.\n    # This encourages distributing items to avoid prematurely filling bins.\n    # We can model this by favoring bins with remaining capacity around a 'target'\n    # which could be related to the average remaining capacity or a fraction of bin capacity.\n    # For simplicity, let's consider the inverse of remaining capacity (higher for less capacity)\n    # but only for bins that can fit the item.\n    # We want to penalize very empty bins more than moderately full bins, while still\n    # favoring bins that are not *completely* full.\n    # Let's try to capture the idea of 'not too full, not too empty'.\n    # Consider a Gaussian-like function centered around some 'ideal' remaining capacity.\n    # Or, a simpler approach: favor bins with moderate remaining capacity.\n    # Let's try: prioritize bins with remaining capacity such that it's not too small (for future items)\n    # but not too large either.\n    # A simple approach: penalize bins with very little remaining capacity more severely than\n    # bins with a lot of remaining capacity IF they don't fit the item perfectly.\n    # Let's create a score that is high when remaining capacity is moderate.\n    # For valid bins, a score proportional to remaining_capacity could be used, but\n    # this conflicts with Best Fit.\n\n    # Alternative for Component 2: Consider the overall \"emptiness\" of bins.\n    # A higher score for bins with less total remaining capacity.\n    # This encourages filling up existing bins before opening new ones.\n    # We will scale this by how much capacity is left *after* placing the item.\n    # So, if a bin has lots of capacity, but `item` fits, this component should still be decent.\n    # Let's try focusing on the inverse of total available capacity for valid bins.\n    # Higher score for less total capacity.\n    # `bins_remain_cap` represents remaining capacity. So, inverse of this could be a score.\n    # A higher score for bins that are *already* more full.\n    fill_scores = np.full_like(bins_remain_cap, -np.inf)\n    # For valid bins, a score that increases as bins_remain_cap decreases.\n    # Adding a small epsilon to avoid division by zero.\n    fill_scores[valid_bins_mask] = 1.0 / (bins_remain_cap[valid_bins_mask] + 1e-6)\n\n    # Adaptive weighting based on item size and bin fullness\n    # If item is small relative to typical bin capacity, we might want to spread items more (favor fill_scores)\n    # If item is large, we might want to prioritize best fit more.\n    # Average remaining capacity can indicate how \"full\" the system is.\n    avg_remain_cap = np.mean(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 1.0 # Avoid division by zero\n    # Assume a standard bin capacity, e.g., 1.0 or a parameter. Let's use 1.0 as a common baseline.\n    # If average remaining capacity is low, bins are generally full.\n    # If item is small compared to average remaining capacity, it's relatively easy to fit.\n    # If item is large compared to average remaining capacity, it's harder to fit.\n\n    # Heuristic for weights:\n    # Let 'target_fill_ratio' be a desired fill level for each bin (e.g., 0.8).\n    # If the average remaining capacity is low (bins are full), we might lean towards best fit.\n    # If average remaining capacity is high (bins are empty), we might lean towards spreading (fill_scores).\n\n    # Let's use a simple weight based on how \"full\" the system is.\n    # If bins are mostly empty (high avg_remain_cap), we might give more weight to `fill_scores`\n    # to encourage packing into fewer bins.\n    # If bins are mostly full (low avg_remain_cap), we might give more weight to `best_fit_scores`.\n\n    # Normalize average remaining capacity against a conceptual full bin (e.g., capacity 1.0)\n    # This gives a rough idea of how \"empty\" the system is.\n    # If avg_remain_cap = 0.8, system is 20% full on average.\n    # If avg_remain_cap = 0.2, system is 80% full on average.\n\n    # We want weight_fill to be high when avg_remain_cap is high (system is empty)\n    # and weight_best_fit to be high when avg_remain_cap is low (system is full).\n    # Let's use a sigmoid-like function or a linear interpolation.\n    # Let's assume bin capacity is 1 for normalization purposes.\n    # Normalized empty space: avg_remain_cap / 1.0\n    # If normalized empty space is high (e.g., 0.8), we want more weight on fill_scores.\n    # If normalized empty space is low (e.g., 0.2), we want more weight on best_fit_scores.\n\n    # Clamp avg_remain_cap to avoid extreme values and ensure meaningful weights\n    clamped_avg_remain_cap = np.clip(avg_remain_cap, 0.01, 0.99) # Assuming max bin capacity is around 1\n\n    # Weight for fill_scores: higher when system is emptier\n    # Weight for best_fit_scores: higher when system is fuller\n    # Let's try a simple linear interpolation.\n    # weight_fill = clamped_avg_remain_cap\n    # weight_best_fit = 1.0 - clamped_avg_remain_cap\n\n    # A slightly more nuanced approach:\n    # If the item is very small compared to remaining capacity, best-fit waste is proportionally large.\n    # If the item is large, best-fit waste is a smaller proportion of remaining capacity.\n    # Consider the ratio `item / bins_remain_cap[valid_bins_mask]`.\n    # When this ratio is close to 1, best-fit is good.\n    # When this ratio is small, best-fit might leave a lot of empty space.\n\n    # Let's simplify weighting:\n    # If there are many bins with large remaining capacities (system is sparse),\n    # we might prioritize filling existing bins (favor `fill_scores`).\n    # If bins are mostly utilized (sparse remaining capacities), we might prioritize\n    # minimizing immediate waste (`best_fit_scores`).\n\n    # Calculate a 'system fullness' metric:\n    # proportion of bins that are \"nearly full\" (e.g., remaining capacity < 0.2 of original capacity)\n    # For simplicity, let's use the average remaining capacity as the primary driver.\n\n    # Define a threshold for 'emptiness' of the system. If average remaining capacity > threshold,\n    # lean towards fill_scores. Otherwise, lean towards best_fit_scores.\n    emptiness_threshold = 0.5 # Example: if average remaining capacity is > 0.5, system is considered \"empty\"\n\n    if avg_remain_cap > emptiness_threshold:\n        # System is relatively empty, encourage filling existing bins\n        weight_fill = 0.7\n        weight_best_fit = 0.3\n    else:\n        # System is relatively full, prioritize minimizing waste\n        weight_fill = 0.3\n        weight_best_fit = 0.7\n\n    # Combine scores using adaptive weights\n    combined_scores = (weight_best_fit * best_fit_scores) + (weight_fill * fill_scores)\n\n    # Apply Softmax to convert scores into probabilities (priorities)\n    # Handle -inf scores by ensuring they don't get exponentiated directly, or by using a numerically stable softmax.\n    # A common practice is to shift scores so the minimum is 0.\n    # Max score will be highest after shifting.\n    max_score = np.max(combined_scores)\n    # Replace -inf with a very small number to avoid issues with exp.\n    # Or, shift by max_score directly, which will make all valid scores non-negative after exp.\n    # If combined_scores contains -inf, np.exp will result in 0.\n    shifted_scores = combined_scores - max_score\n\n    # Ensure no NaNs or Infs in shifted_scores before exponentiation\n    shifted_scores = np.nan_to_num(shifted_scores, nan=-np.inf, posinf=np.inf, neginf=-np.inf)\n\n    # Use exp where scores are not -inf\n    exp_values = np.zeros_like(shifted_scores)\n    valid_mask = np.isfinite(shifted_scores)\n    exp_values[valid_mask] = np.exp(shifted_scores[valid_mask])\n\n    # Sum of exponentiated values for normalization. Ignore -inf cases which result in 0.\n    sum_exp_values = np.sum(exp_values)\n\n    # Calculate probabilities\n    probabilities = np.zeros_like(combined_scores)\n    if sum_exp_values > 1e-9: # Avoid division by zero if all valid scores were -inf or very small\n        probabilities[valid_mask] = exp_values[valid_mask] / sum_exp_values\n\n    # Ensure probabilities sum to 1 for the valid bins\n    # The probabilities are currently for *all* bins, with 0 for invalid ones.\n    # The softmax was calculated over the combined scores of all bins.\n    # This means if a bin is invalid, its score remains -inf, exp(-inf) -> 0, so it correctly gets 0 probability.\n\n    return probabilities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 6.0,
    "halstead": 277.44167656471495,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response4.txt_stdout.txt",
    "code_path": "problem_iter6_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for each bin using an adaptive, sensitivity-aware\n    strategy for the online Bin Packing Problem.\n\n    This heuristic aims to balance \"Best Fit\" (minimizing waste) with a\n    \"First Fit Decreasing\"-like tendency by slightly favoring bins that are\n    less full initially, but can still accommodate the item. It also incorporates\n    an adaptive component that can subtly shift focus based on the diversity\n    of available bin capacities.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the\n                         remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # --- Core Heuristic Component: Best Fit / Least Waste ---\n    # Prioritize bins that leave minimal remaining capacity after packing.\n    # A lower (valid_bins_remain_cap - item) is better.\n    waste = valid_bins_remain_cap - item\n    best_fit_scores = -waste  # Maximize this score (minimize waste)\n\n    # --- Adaptive Component: Sensitivity to Bin Fullness ---\n    # Introduce a term that favors bins that are not excessively full *before*\n    # packing, but are still valid. This encourages spreading items initially,\n    # while still respecting the 'best fit' for the current item.\n    # We can achieve this by looking at the distribution of current bin capacities.\n    # If there's high variance in remaining capacities, we might slightly favor\n    # bins with more remaining capacity (but still fit the item).\n    # If capacities are very similar, focus more on best fit.\n\n    # Calculate a 'spread' factor based on the standard deviation of *all* bins' remaining capacities.\n    # A higher std dev means more diverse capacities.\n    if bins_remain_cap.size > 1:\n        std_dev_all_bins = np.std(bins_remain_cap)\n        # Normalize std_dev by the average capacity to make it somewhat scale-invariant.\n        # Add a small epsilon to avoid division by zero.\n        avg_cap_all_bins = np.mean(bins_remain_cap) + 1e-9\n        spread_factor = std_dev_all_bins / avg_cap_all_bins\n    else:\n        spread_factor = 0 # No spread if only one bin\n\n    # The adaptive score component:\n    # We want to increase the priority of bins with *more* remaining capacity\n    # when the spread_factor is high.\n    # Let's use the relative remaining capacity (valid_bins_remain_cap / bin_capacity_max_possible)\n    # for this term. Or simpler, just the raw remaining capacity.\n    # We want to positively correlate with remaining capacity, scaled by spread_factor.\n    # A simple approach: `spread_factor * (valid_bins_remain_cap / max_possible_capacity)`\n    # Let's use a simplified approach focusing on relative difference for now.\n    # We want to favor bins that are \"less full\" when spread is high.\n    # `valid_bins_remain_cap` is a proxy for \"less full\".\n    # So, a term like `spread_factor * valid_bins_remain_cap` could be added.\n\n    # A more nuanced approach: use the 'gap' (capacity - item)\n    # We want to penalize bins that result in a very small gap (best fit).\n    # Consider `spread_factor * (item / valid_bins_remain_cap)` - encourages using bins with more space\n    # when spread is high.\n\n    # Let's combine the two:\n    # Priority = w1 * (-waste) + w2 * (adaptive_term)\n    # We want to favor smaller waste.\n    # The adaptive term should make bins with *more* remaining capacity more attractive\n    # when `spread_factor` is high.\n\n    # Let's try a score that combines best-fit and a tendency to use bins\n    # that are not *critically* full.\n    # Score = BestFitScore - PenaltyForBeingTooFull\n    # PenaltyForBeingTooFull could be related to `1 / valid_bins_remain_cap` or `(max_cap - valid_bins_remain_cap)`.\n\n    # A new composite score:\n    # Base score: -(waste)  (favors minimal waste)\n    # Adaptive modifier: A term that slightly boosts bins with more remaining capacity,\n    # but this boost is stronger when `spread_factor` is higher.\n    # Consider `spread_factor * (valid_bins_remain_cap / average_remaining_capacity_of_valid_bins)`\n    # This gives a relative measure of how much space is left in a bin compared to average.\n\n    avg_valid_remain_cap = np.mean(valid_bins_remain_cap) + 1e-9\n    relative_remaining_cap = valid_bins_remain_cap / avg_valid_remain_cap\n\n    # The adaptive score: a multiplicative boost based on spread and relative remaining capacity.\n    # We want to amplify the priority of bins that are relatively spacious when diversity is high.\n    # This is a soft preference, not overriding best-fit entirely.\n    # A small additive term is safer for Softmax stability.\n    adaptive_scores = spread_factor * (relative_remaining_cap - 1.0) # Center around 0, positive for more remaining cap\n\n    # Combine scores: best_fit_scores are already designed to be maximized.\n    # adaptive_scores are also designed to be maximized (positive means good)\n    combined_scores = best_fit_scores + adaptive_scores\n\n    # Softmax to convert scores into probabilities (priorities)\n    # Shift scores to prevent overflow/underflow in exp.\n    if np.all(np.isfinite(combined_scores)):\n        shifted_scores = combined_scores - np.max(combined_scores)\n        exp_scores = np.exp(shifted_scores)\n        probabilities = exp_scores / np.sum(exp_scores)\n    else:\n        # Handle potential NaNs or Infs by reverting to a simple best-fit if calculation fails\n        # (though the logic above should prevent this with epsilon)\n        shifted_scores = best_fit_scores - np.max(best_fit_scores)\n        exp_scores = np.exp(shifted_scores)\n        probabilities = exp_scores / np.sum(exp_scores)\n\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 235.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response1.txt_stdout.txt",
    "code_path": "problem_iter8_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with an adaptive diversification strategy.\n    Prioritizes bins that fit the item well, while also considering\n    the variance of remaining capacities to balance exploration and exploitation.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(valid_bins_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    # Best Fit Component: Similarity to perfect fit. Smaller difference is better.\n    bf_scores = -(valid_bins_remain_cap - item)\n\n    # Adaptive Diversification Component: Use variance to adjust exploration.\n    # If variance is low (bins are similar), encourage exploration by rewarding larger capacities.\n    # If variance is high (bins are dissimilar), reduce exploration by penalizing larger capacities.\n    if len(valid_bins_remain_cap) > 1:\n        variance_capacity = np.var(valid_bins_remain_cap)\n        # Normalize variance to be a small value, can be a hyperparameter\n        normalized_variance = variance_capacity / np.mean(valid_bins_remain_cap)**2 if np.mean(valid_bins_remain_cap) > 0 else 0\n        # Heuristic: if variance is low, we want to diversify more by favoring emptier bins\n        # so we add a positive term proportional to capacity. If variance is high, we want to\n        # exploit good fits more, so we add a negative term proportional to capacity.\n        # This is a simplified approach to adaptive diversification.\n        adaptive_div_scores = -normalized_variance * valid_bins_remain_cap\n    else:\n        adaptive_div_scores = np.zeros_like(valid_bins_remain_cap)\n\n    # Combine components. Tune weights based on empirical performance.\n    w_bf = 1.0\n    w_adapt_div = 0.5 # Weight for adaptive diversification\n    \n    combined_scores = w_bf * bf_scores + w_adapt_div * adaptive_div_scores\n\n    # Apply Softmax for probabilistic selection. Shift for numerical stability.\n    if combined_scores.size > 0:\n        shifted_scores = combined_scores - np.max(combined_scores)\n        exp_scores = np.exp(shifted_scores)\n        probabilities = exp_scores / np.sum(exp_scores)\n    else:\n        probabilities = np.array([])\n\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 5.0,
    "halstead": 230.81773576252348,
    "exec_success": true
  }
]