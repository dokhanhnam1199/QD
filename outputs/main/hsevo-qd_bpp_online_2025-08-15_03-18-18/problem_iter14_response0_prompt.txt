{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Calculates priority scores for each bin using a Softmax-Based Fit strategy\n    for the online Bin Packing Problem.\n\n    This strategy prioritizes bins that have remaining capacity, with a higher\n    priority given to bins that can accommodate the item without significant\n    wastage, and also considers the overall \"fullness\" of bins.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the\n                         remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    # Filter out bins that cannot fit the item\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        # If no bin can fit the item, return zero priorities for all bins\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate a score for each valid bin:\n    # We want to favor bins that are nearly full after placing the item.\n    # (capacity - item) represents the remaining capacity after placement.\n    # Smaller values of (capacity - item) are better.\n    # We can invert this by taking the negative or by calculating (item - capacity) if capacity < item.\n    # For simplicity and Softmax compatibility, let's focus on the 'goodness' of fit.\n    # A good fit means small remaining capacity. So, we can use 1 / (remaining_after_fit)\n    # or something similar.\n\n    # A common heuristic for BPP is \"Best Fit\": choosing the bin that leaves the least empty space.\n    # So, remaining_capacity - item should be minimized.\n    # We want to maximize the \"suitability\" score.\n    # Let's consider the negative of the remaining capacity after fitting as a base score.\n    # Larger negative values (closer to zero) are better.\n    base_scores = -(valid_bins_remain_cap - item)\n\n    # Add a penalty for bins that are already very full, encouraging spreading items if possible,\n    # unless an item perfectly fits. This can be tricky.\n    # For simplicity in v2, let's focus on the immediate fit.\n\n    # Apply Softmax to convert scores into probabilities (priorities)\n    # Softmax: exp(score) / sum(exp(all_scores))\n    # To avoid numerical instability with very large or small scores, we can shift scores.\n    # Subtracting the maximum score before exponentiation is a common technique.\n    shifted_scores = base_scores - np.max(base_scores)\n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Create the final priority array, placing calculated priorities in their original positions\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \n    bin_capacities = 1.0  # Assuming a standard bin capacity of 1.0, can be generalized.\n    \n    potential_fits = bins_remain_cap - item\n    \n    valid_bins_mask = potential_fits >= 0\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    \n    if np.any(valid_bins_mask):\n        \n        priorities[valid_bins_mask] = 1.0 / (potential_fits[valid_bins_mask] + 1e-9) # Add epsilon to avoid division by zero\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1 and 5: Heuristic 1 uses `-(valid_bins_remain_cap - item)` as its base score, which directly implements the \"Best Fit\" principle by maximizing this value (minimizing waste). Heuristic 5 also uses this, but adds robustness checks for identical scores and zero sums, making it slightly more stable.\n\nComparing Heuristics 2 and 6: Both introduce an \"adaptive diversification\" component. Heuristic 2 uses variance to adjust the score, aiming to diversify when variance is low. Heuristic 6 refines this by adding a specific \"perfect fit bonus\" and a more complex adaptive diversification score tied to relative variance, making it more nuanced.\n\nComparing Heuristics 3 and 4: Heuristic 3 attempts to balance \"Best Fit\" with a tendency to favor less full bins based on overall bin capacity spread. It introduces a `spread_factor` and `relative_remaining_cap`. Heuristic 4 introduces an adaptive temperature parameter for Softmax, controlled by the number of valid bins and their standard deviation, along with a diversity bonus. Heuristic 4's adaptive temperature is a more sophisticated mechanism for controlling exploration/exploitation.\n\nComparing Heuristics 7 and 20: Heuristic 7 is identical to Heuristic 5. Heuristic 20 introduces a blend of \"Best Fit\" and \"Worst Fit\" tendencies, adapting the balance using an `epsilon` parameter derived from the variance of remaining capacities. This adaptive blending is a strong strategy for managing exploration vs. exploitation.\n\nComparing Heuristics 9-16 (repeated implementations of `1 / (potential_fits + epsilon)`) with Heuristics 1, 2, 3, 4, 6, 8, 20: The latter group uses more sophisticated scoring mechanisms, often involving combinations of factors (Best Fit, diversification, adaptive elements) and applying Softmax for probabilistic selection. Heuristics 9-16 are simple, potentially prone to extreme values and don't incorporate adaptive strategies or explicit diversification. Heuristic 17-19 use `np.exp(effective_capacities)` which would disproportionately favor bins with very large remaining capacities, not necessarily ideal for packing efficiency.\n\nOverall: Heuristics 1, 2, 3, 4, 6, 8, and 20 demonstrate a progression towards more complex, adaptive, and robust strategies. Heuristics 20, 4, and 6 appear to be the most advanced, incorporating adaptive parameters (temperature, epsilon) and multi-faceted scoring. Heuristics 1, 5, and 7 represent a solid \"Best Fit\" baseline with Softmax. Heuristics 9-16 are simplistic and less effective. Heuristics 17-19 have an unusual scoring mechanism.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Multi-objective, Adaptive, Probabilistic, Hybridization.\n*   **Advice:** Focus on heuristics that integrate multiple criteria (e.g., fit quality, diversity) and dynamically adjust their behavior based on problem instance characteristics. Leverage robust probabilistic selection methods like Softmax with careful parameterization.\n*   **Avoid:** Simplistic, static scoring functions, neglecting numerical stability, and overly complex, untuned adaptive mechanisms.\n*   **Explanation:** The goal is to create intelligent, flexible heuristics that balance exploration and exploitation by understanding and responding to the problem's evolving state, ensuring both efficiency and robustness.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}