```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Calculates priority scores for each bin using an advanced Softmax-Based Fit strategy
    for the online Bin Packing Problem.

    This strategy aims to improve upon v1 by incorporating:
    1.  A stronger preference for bins that result in a "perfect fit" (remaining capacity = 0).
    2.  A mechanism to temper the impact of very small remaining capacities when they are
        still significantly larger than the item, to avoid overly aggressive fitting
        that might lead to fragmentation later.
    3.  Adaptive scaling based on the overall variability of valid bin capacities to
        better control the Softmax distribution.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A numpy array where each element represents the
                         remaining capacity of a bin.

    Returns:
        A numpy array of the same size as bins_remain_cap, where each element
        is the priority score for placing the item in the corresponding bin.
    """
    valid_bins_mask = bins_remain_cap >= item
    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]

    if valid_bins_remain_cap.size == 0:
        return np.zeros_like(bins_remain_cap)

    # Calculate scores for valid bins
    # A perfect fit (remaining capacity = 0) should have a significantly higher intrinsic score.
    # For other bins, we want to minimize remaining capacity.
    # Let's use a score that is high for perfect fits and decreases as remaining capacity increases.

    # Score 1: Perfect Fit Bonus
    perfect_fit_bonus = np.zeros_like(valid_bins_remain_cap)
    perfect_fit_mask = (valid_bins_remain_cap - item) == 0
    perfect_fit_bonus[perfect_fit_mask] = 10.0  # Assign a substantial bonus for perfect fits

    # Score 2: Inverse of remaining capacity after fit, with tempering
    # We want to prioritize bins that leave little space.
    # (valid_bins_remain_cap - item) is the remaining space.
    # A common approach is 1 / (remaining_space + epsilon) or similar.
    # To avoid extreme values and provide a smoother distribution for non-perfect fits,
    # we can use a scaled inverse or a logarithmic transformation.
    # Let's use `1 / (remaining_space + 1)` to ensure division by zero is avoided and
    # to give diminishing returns for larger remaining spaces.
    # We also want to slightly penalize large remaining spaces.

    remaining_after_fit = valid_bins_remain_cap - item
    # Add a small constant to avoid division by zero and to give a baseline preference
    # to any bin that fits.
    base_suitability = 1.0 / (remaining_after_fit + 1.0)

    # Combine scores: give a strong preference to perfect fits.
    # Add the bonus to the base suitability.
    combined_scores = base_suitability + perfect_fit_bonus

    # Adaptive scaling for Softmax:
    # Softmax can be sensitive to the range of scores. If scores are very close,
    # probabilities will be very uniform. If scores are very spread out,
    # probabilities will be very skewed.
    # We can adjust the scaling factor (temperature) or shift scores.
    # Let's shift scores by subtracting the mean of the combined scores
    # to center the distribution around zero.
    # Alternatively, we could use the standard deviation to adjust sensitivity.
    # For v2, let's subtract the mean to center it, and perhaps add a small factor
    # to the difference from the max to control "sharpness".

    # Shift scores to improve numerical stability for Softmax
    # Subtracting the mean score helps in centering the exponentiation.
    mean_score = np.mean(combined_scores)
    shifted_scores = combined_scores - mean_score

    # A small adjustment factor (similar to temperature T in kT) could be added
    # or used to scale the differences from the max. For now, let's stick to centering.
    # If we want more "peaky" distributions (stronger preference for top bins),
    # we might use `shifted_scores = combined_scores - np.max(combined_scores)`.
    # Let's use the max subtraction as it generally leads to clearer preferences.
    if combined_scores.size > 0:
        max_score = np.max(combined_scores)
        shifted_scores = combined_scores - max_score
    else:
        shifted_scores = np.array([])


    # Apply Softmax
    if shifted_scores.size > 0:
        exp_scores = np.exp(shifted_scores)
        probabilities = exp_scores / np.sum(exp_scores)
    else:
        probabilities = np.array([])

    # Create the final priority array
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    if probabilities.size > 0:
        priorities[valid_bins_mask] = probabilities

    return priorities
```
