```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Sigmoid Fit Score.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    fit_scores = []
    for bin_cap in bins_remain_cap:
        if bin_cap >= item:
            fit = bin_cap - item
            # Using a sigmoid function to map fit to a priority score
            # Larger fit (more remaining capacity) should ideally lead to lower priority
            # to favor fuller bins first.
            # We invert the sigmoid by multiplying by -1 and adding a large constant,
            # or simply use 1 - sigmoid if we want a monotonic decreasing function.
            # A simple approach is to use the negative of the fit, or map it inversely.
            # For Sigmoid Fit Score, we want bins that *almost* fit the item to have higher priority.
            # So, bins with smaller `fit` are preferred.
            # Let's map the fit value to a score using a sigmoid-like behavior.
            # A common approach is to consider `item / bin_cap` or `item / (bin_cap + epsilon)`
            # or to prioritize bins that are "almost full" with respect to the item.
            # If bin_cap is much larger than item, priority should be low.
            # If bin_cap is just enough for item, priority should be high.

            # Let's define a metric related to how well the item fits.
            # A high score means the bin is a good fit.
            # A good fit means `bin_cap` is close to `item`.
            # So, `item / bin_cap` is a candidate. If `bin_cap` is small, this ratio is large.
            # However, `bin_cap` can be zero. Also, we want bins where `bin_cap >= item`.

            # Let's try a sigmoid on the difference (bin_cap - item).
            # We want smaller differences (closer to zero) to have higher priority.
            # sigmoid(x) is increasing. So we need to pass a decreasing function of the difference.
            # A simple approach could be to map the *remaining capacity after packing* to a score.
            # We want bins with minimal remaining capacity after packing.
            # So, the smaller `fit = bin_cap - item`, the higher the priority.

            # Let's use a sigmoid-like function where small `fit` values yield high scores.
            # We can define `score = 1 / (1 + exp(k * (fit - offset)))`
            # If we want higher priority for smaller 'fit', then we want `score` to be high when `fit` is low.
            # `exp(k * fit)` increases with `fit`.
            # So, `1 / (1 + exp(k * fit))` decreases with `fit`.
            # Let's set k > 0 to make it steep. And perhaps an offset for finer control.

            # A simpler interpretation of "Sigmoid Fit Score" might be prioritizing bins that
            # result in minimal waste, i.e., where `bin_cap - item` is small.
            # We can transform `bin_cap - item` using a sigmoid.
            # A direct mapping could be to use a function that is high when `bin_cap` is just above `item`.

            # Let's consider the ratio of remaining capacity to total capacity of the bin (if we knew it),
            # or ratio of item size to bin remaining capacity.
            # `item / bin_cap` if `bin_cap > 0`. High value means bin is getting full relative to its remaining space.
            # However, this doesn't directly model "fit score" as closeness.

            # Let's focus on `fit = bin_cap - item`. We want small `fit`.
            # Consider `sigmoid_score = 1 / (1 + exp(beta * fit))` where beta > 0.
            # This gives high scores for small `fit`.

            # Another common approach for bin packing heuristics (like Best Fit) is to choose the bin
            # that leaves the minimum remaining capacity after packing the item.
            # This is equivalent to maximizing `bin_cap - item`. This is not what we want.
            # We want to minimize `bin_cap - item`.

            # Let's go with the intuition that a "good fit" means the bin's remaining capacity
            # is *just enough* or slightly more than the item.
            # This implies `bin_cap` is close to `item`.
            # So, we want to prioritize bins where `abs(bin_cap - item)` is minimized,
            # with the constraint that `bin_cap >= item`.
            # If `bin_cap >= item`, then `abs(bin_cap - item) = bin_cap - item`.
            # Minimizing `bin_cap - item` means choosing the smallest `bin_cap` that can hold `item`.

            # Let's use a sigmoid to penalize bins with a large `fit = bin_cap - item`.
            # A high priority score should correspond to a low `fit`.
            # Let's define a score based on the reciprocal of `fit + epsilon` and apply a sigmoid.
            # Or, more directly, use a sigmoid where the input is related to how "tight" the fit is.

            # Let `x = bin_cap - item`. We want small `x`.
            # Sigmoid function: `1 / (1 + exp(-x))`. This increases with x.
            # We want high priority for small x. So, use `-x`.
            # `score = 1 / (1 + exp(-(bin_cap - item)))`
            # This means `score = 1 / (1 + exp(item - bin_cap))`.
            # As `bin_cap` increases for a fixed `item`, `item - bin_cap` becomes more negative,
            # and `exp(item - bin_cap)` approaches 0. The score approaches 1. This is wrong.

            # We want priority high when `bin_cap - item` is small (close to 0).
            # Let `waste = bin_cap - item`. We want to minimize waste.
            # A sigmoid on `waste`: `1 / (1 + exp(k * waste))` where `k > 0`.
            # This gives high scores for small `waste`.

            # Let's use `k = 5` as a starting point to make the sigmoid relatively steep around waste=0.
            # `priorities.append(1 / (1 + np.exp(5 * (bin_cap - item))))`
            # This would give priority close to 1 if `bin_cap - item` is very negative (which shouldn't happen because of the if condition)
            # or close to 0.5 if `bin_cap - item` is 0.
            # And priority close to 0 if `bin_cap - item` is large positive.

            # Let's adjust the sigmoid. We want scores close to 1 for bins that *just* fit, and closer to 0 for bins with lots of slack.
            # The term `item / bin_cap` can be useful, but it can be 0 if item is 0.
            # Or `item / (bin_cap + 1e-9)` to avoid division by zero.
            # If `bin_cap` is very large, `item / bin_cap` is close to 0. Low priority.
            # If `bin_cap` is just slightly larger than `item`, `item / bin_cap` is close to 1. High priority.

            # Let's try `sigmoid(k * (item / bin_cap))` with `k > 0`.
            # `sigmoid(x) = 1 / (1 + exp(-x))`
            # `score = 1 / (1 + np.exp(-k * (item / (bin_cap + 1e-9))))`
            # This gives high scores for bins where `item / bin_cap` is large.
            # This means `item` is a large fraction of `bin_cap`.
            # So, bins that are already quite full relative to their remaining capacity get higher priority.
            # This seems consistent with a "fit" strategy.

            # Let's choose `k=10` for a sharper response.
            # We are interested in bins where `bin_cap >= item`.

            priority = 1 / (1 + np.exp(-10 * (item / (bin_cap + 1e-9))))
            fit_scores.append(priority)
        else:
            # If the item doesn't fit, assign a priority of 0.
            fit_scores.append(0.0)

    return np.array(fit_scores)
```
