{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \n    bin_capacities = 1.0  # Assuming a standard bin capacity of 1.0, can be generalized.\n    \n    potential_fits = bins_remain_cap - item\n    \n    valid_bins_mask = potential_fits >= 0\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    \n    if np.any(valid_bins_mask):\n        \n        priorities[valid_bins_mask] = 1.0 / (potential_fits[valid_bins_mask] + 1e-9) # Add epsilon to avoid division by zero\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Calculates priority scores for each bin using an adaptive strategy for the\n    online Bin Packing Problem. This version aims to balance fitting tightly\n    (Best Fit) with spreading items (Worst Fit) and incorporating an element\n    of exploration.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the\n                         remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # --- Core Strategy: Balancing Best Fit and Worst Fit ---\n    # Best Fit component: Prioritize bins that leave minimal remaining space.\n    # We want to minimize (remaining_capacity - item).\n    # A good score for BF would be proportional to -(remaining_capacity - item).\n    # For Softmax, we want higher scores for better options. So, let's use\n    # a score that increases as remaining_capacity - item decreases.\n    # A simple inversion: 1 / (remaining_capacity - item + epsilon)\n    # Or, to keep it related to the previous approach: maximize -(remaining_capacity - item)\n    # To promote diversification, let's also consider the \"emptiness\" of the bin.\n    # Worst Fit component: Prioritize bins with *more* remaining capacity.\n    # This encourages spreading items.\n    # A score for WF could be proportional to remaining_capacity.\n\n    # Let's create a blended score.\n    # For Best Fit: prioritize small remaining capacity after placing the item.\n    # For Worst Fit: prioritize large initial remaining capacity.\n    # We want to maximize the utility.\n    # Let's consider the utility as a function of remaining capacity:\n    # utility = alpha * (1 / (valid_bins_remain_cap - item + 1e-9)) + beta * valid_bins_remain_cap\n\n    # For simplicity and to adapt the softmax approach, let's define scores\n    # where higher means more desirable.\n    # High score for small (remaining_capacity - item) => Best Fit tendency\n    # High score for large remaining_capacity => Worst Fit tendency (for exploration/diversification)\n\n    # Let's try a score that is a combination:\n    # Score = (large_capacity_bonus) * (remaining_capacity) - (misfit_penalty) * (remaining_capacity - item)\n    # A simpler approach:\n    # Prioritize bins where remaining_capacity - item is small (BF)\n    # BUT, also give a boost to bins that are \"more open\" (WF) to avoid early convergence.\n\n    # Let's combine the ideas:\n    # We want to favor small (remaining_capacity - item).\n    # Let's define a score for \"tightness\": TightnessScore = -(valid_bins_remain_cap - item)\n    # And a score for \"openness\": OpennessScore = valid_bins_remain_cap\n\n    # We can create a combined score, for example, by averaging or taking a weighted sum.\n    # A more robust approach is to introduce a \"temperature\" or \"exploration factor\"\n    # that modulates the influence of the Best Fit vs. Worst Fit tendencies.\n\n    # Let's try a score that is a compromise. We want to minimize (remaining_capacity - item).\n    # Let's use a function that is high when (remaining_capacity - item) is small.\n    # Consider a function like: `exp(-k * (remaining_capacity - item))`\n    # `k` can be an exploration parameter. A large `k` makes it more like Best Fit.\n    # A small `k` makes it flatter, more exploratory.\n\n    # To balance exploration and exploitation, let's make the \"tightness\" score\n    # have an exploratory element.\n    # Let's use a score that is high for bins that are \"good\" fits, but also\n    # has some preference for bins that aren't *too* full if an exact fit isn't available.\n\n    # --- Adaptive Exploration/Exploitation ---\n    # We can adapt the strength of the \"Best Fit\" tendency based on the distribution of remaining capacities.\n    # If capacities are very diverse, lean more towards Best Fit.\n    # If capacities are very similar, lean more towards diversification.\n\n    # A simple adaptation: Use a parameter `epsilon` that smooths the selection.\n    # Larger epsilon makes it more uniform (exploratory). Smaller epsilon makes it more greedy (exploitative).\n    # We can define epsilon based on the variance of remaining capacities.\n    variance_remain_cap = np.var(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 1.0\n    # Scale variance to a reasonable epsilon range. Higher variance -> higher epsilon for more exploration.\n    epsilon = 0.1 + 0.5 * (1 / (1 + np.exp(-0.1 * variance_remain_cap))) # Sigmoid to bound epsilon\n\n    # Calculate scores:\n    # Score for \"good fit\" (lower remaining_capacity - item is better)\n    # Using a negative exponential for a sharp decrease in score as misfit increases.\n    # Adding epsilon for numerical stability and exploration.\n    goodness_of_fit_scores = np.exp(-10.0 * (valid_bins_remain_cap - item) / (item + 1e-9)) # Scale by item size\n\n    # Score for \"openness\" (higher remaining_capacity is better for spreading)\n    # Using a scaled exponential to give a significant boost to very open bins.\n    openness_scores = np.exp(0.1 * valid_bins_remain_cap / (np.max(bins_remain_cap) + 1e-9)) # Scale by max capacity\n\n    # Combine scores using epsilon for adaptive weighting\n    # When epsilon is high (diverse capacities), openness_scores have more weight.\n    # When epsilon is low (similar capacities), goodness_of_fit_scores have more weight.\n    combined_scores = (1 - epsilon) * goodness_of_fit_scores + epsilon * openness_scores\n\n    # Softmax transformation to get priorities\n    # Shift scores to prevent overflow/underflow before exponentiation\n    shifted_scores = combined_scores - np.max(combined_scores)\n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Create the final priority array\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Best) vs. Heuristic 10 (Worst): Heuristic 1 uses a more sophisticated approach by combining \"Best Fit\" with a diversification penalty, and then applying Softmax for probability distribution. Heuristic 10 is a simple \"inverse proximity\" (Best Fit) without normalization or diversification.\n\nComparing Heuristic 1 vs. Heuristic 2: Heuristic 1 employs a hybrid strategy with a penalty for overly full bins, while Heuristic 2 focuses on perfect fits and inverse proximity. Both use Softmax, but Heuristic 1's diversification penalty is a more nuanced approach to avoid premature bin exhaustion.\n\nComparing Heuristic 2 vs. Heuristic 3: Heuristic 2 explicitly rewards perfect fits with a bonus, whereas Heuristic 3 only implicitly favors tighter fits through its base score calculation. Heuristic 2's direct reward for perfect fits is a stronger signal for that specific good outcome.\n\nComparing Heuristic 3 vs. Heuristic 11: Both focus on inverse proximity. Heuristic 11 adds a specific high priority for perfect fits, making it more explicit about rewarding exact matches, which can be beneficial.\n\nComparing Heuristic 11 vs. Heuristic 14: These are nearly identical, both prioritizing perfect fits and then using inverse proximity. The subtle difference in variable naming doesn't significantly alter behavior.\n\nComparing Heuristic 14 vs. Heuristic 12: Heuristic 14 explicitly handles perfect fits separately before applying inverse proximity, while Heuristic 12 directly applies inverse proximity (which handles perfect fits as a limit). Heuristic 14's explicit handling might be slightly more robust for edge cases or clarity.\n\nComparing Heuristic 12 vs. Heuristic 15: Heuristic 12 directly applies `1 / (remaining_capacity - item + 1e-9)` to valid bins. Heuristic 15 does the same but appears to have a static `bin_capacities = 1.0` which might be a placeholder and not used, making it functionally similar to Heuristic 12 if `bins_remain_cap` are already calculated differences. The use of `potential_fits` is cleaner.\n\nComparing Heuristic 15 vs. Heuristic 10: Both use the inverse proximity strategy. Heuristic 15 uses `potential_fits` and a mask, which is slightly more idiomatic NumPy than the explicit loop in Heuristic 10.\n\nComparing Heuristic 18/19 vs. Heuristic 20: Heuristics 18/19 use `exp(effective_capacities)` which rewards larger remaining capacities (more \"open\" bins). Heuristic 20 attempts a more complex adaptive strategy combining \"goodness of fit\" and \"openness,\" but its implementation might be overly complex or sensitive to parameter tuning. Heuristic 18/19's simplicity in rewarding openness is clear.\n\nComparing Heuristic 20 vs. Heuristic 1: Heuristic 1 is a more balanced and interpretable hybrid, directly penalizing overly tight fits. Heuristic 20's adaptive strategy with \"epsilon\" and scaled exponentials is more complex and potentially harder to tune effectively for general cases.\n\nOverall: More complex heuristics that combine multiple objectives (like Best Fit + Diversification) and use Softmax for normalization generally perform better (Heuristics 1, 2, 6, 7, 8, 20). Simple inverse proximity is decent but less robust (Heuristics 10, 11, 12, 13, 14, 15, 16, 17). Reward for perfect fits is a good addition (Heuristics 2, 11, 14).\n- \nHere's a redefined self-reflection for designing better heuristics:\n\n*   **Keywords:** Adaptive hybridization, Pareto fronts, meta-learning, sensitivity analysis.\n*   **Advice:** Focus on heuristics that adapt their strategies based on problem instance characteristics or performance feedback, rather than static combinations. Explore methods to identify and exploit \"sweet spots\" where certain heuristic components are most effective.\n*   **Avoid:** Blindly applying pre-defined hybrid strategies without understanding their impact on diverse problem instances. Over-reliance on single-objective optimization without considering trade-offs between conflicting goals.\n*   **Explanation:** True improvement comes from dynamic, data-informed adjustments, not just static mixing. Understanding how heuristic parameters or component choices affect outcomes across a spectrum of problems (like using sensitivity analysis or exploring Pareto fronts of heuristic properties) leads to more robust and performant designs.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}