{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a penalty for overly full bins and an explicit bonus for perfect fits.\n    Prioritizes bins that closely fit the item, penalizes those that become too full,\n    and gives a high reward for exact matches.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate remaining capacity after packing\n    rem_after_pack = valid_bins_remain_cap - item\n\n    # Component 1: \"Best Fit\" - prioritize smaller remaining capacity\n    # Higher score for smaller remaining capacity\n    best_fit_scores = -rem_after_pack\n\n    # Component 2: Penalty for overly full bins (makes remaining capacity very small)\n    # We want to reduce the score if rem_after_pack is close to zero.\n    # Use an exponential decay that is strong for small remaining capacity.\n    # Scale rem_after_pack to avoid overflow/underflow in exp and control sensitivity.\n    min_rem = 0.0\n    max_rem = np.max(valid_bins_remain_cap) - item if np.max(valid_bins_remain_cap) >= item else 0.0\n    \n    if max_rem == min_rem: # Handle case where all valid bins have same remaining capacity\n        diversification_penalty = np.zeros_like(best_fit_scores)\n    else:\n        scaled_rem = 5 * rem_after_pack / max_rem # Scale to [0, 5] range\n        # Penalty is high when scaled_rem is low (i.e., tight fit)\n        # Use 1 - sigmoid to get a penalty that is high for low values\n        diversification_penalty = 1.0 - (1.0 / (1.0 + np.exp(10 * (1.0 - scaled_rem)))) # Penalty ~ 1 for tight fit, ~ 0 for loose fit\n\n    # Component 3: Bonus for perfect fits\n    # Assign a significantly higher score to bins where rem_after_pack is zero (or very close to zero)\n    perfect_fit_bonus = np.zeros_like(best_fit_scores)\n    perfect_fit_threshold = 1e-9 # Tolerance for floating point comparison\n    is_perfect_fit = rem_after_pack < perfect_fit_threshold\n    perfect_fit_bonus[is_perfect_fit] = 10.0 # Large bonus for perfect fits\n\n    # Combine scores: best_fit + bonus - penalty\n    # The penalty is subtracted from the score to reduce priority for overly full bins.\n    # The bonus is added to strongly favor perfect fits.\n    combined_scores = best_fit_scores + perfect_fit_bonus - diversification_penalty * 2.0 # Penalty multiplier to control its impact\n\n    # Apply Softmax to get probability distribution\n    # Shift scores to prevent overflow/underflow in exp\n    if combined_scores.size > 0:\n        shifted_scores = combined_scores - np.max(combined_scores)\n        exp_scores = np.exp(shifted_scores)\n        probabilities = exp_scores / np.sum(exp_scores)\n    else:\n        probabilities = np.array([])\n\n    # Map probabilities back to the original bins array\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \n    bin_capacities = 1.0  # Assuming a standard bin capacity of 1.0, can be generalized.\n    \n    potential_fits = bins_remain_cap - item\n    \n    valid_bins_mask = potential_fits >= 0\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    \n    if np.any(valid_bins_mask):\n        \n        priorities[valid_bins_mask] = 1.0 / (potential_fits[valid_bins_mask] + 1e-9) # Add epsilon to avoid division by zero\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1 and 2: Heuristic 2 introduces an \"adaptive diversification\" component based on the variance of remaining capacities. This is a more sophisticated attempt at balancing exploration and exploitation than Heuristic 1's fixed diversification bonus.\n\nComparing Heuristics 2 and 3: Heuristic 3 refines the adaptive strategy by making the \"beta\" parameter (controlling Softmax sharpness) adaptive to the mean slack. It also explicitly handles near-perfect fits with a bonus, which is a valuable addition for reducing fragmentation.\n\nComparing Heuristics 3 and 4: Heuristic 4 introduces a \"penalty for overly full bins\" and a stronger \"bonus for perfect fits\" using a scaled sigmoid. It's more complex than Heuristic 3's adaptive beta but might over-penalize tight fits.\n\nComparing Heuristics 4 and 5: Heuristic 5 is a basic \"Softmax-Based Fit\" and lacks the adaptive or penalty mechanisms of Heuristics 3 and 4. It's a good baseline but less sophisticated.\n\nComparing Heuristics 5 and 6: Heuristic 6 is very similar to Heuristic 5, essentially a cleaner implementation of Softmax-Based Fit with slightly better handling of edge cases (e.g., all scores being identical).\n\nComparing Heuristics 6 and 7: Heuristic 7 introduces a \"perfect fit bonus\" and uses \"inverse proximity\" for non-perfect fits. This is simpler than Heuristic 3's adaptive beta but explicitly rewards perfect fits, which is beneficial. However, its scaling might be less nuanced than Heuristic 3's adaptive beta.\n\nComparing Heuristics 7 and 19: Heuristic 19 attempts a more complex balance between \"Best Fit\" and \"Worst Fit\" using an adaptive `epsilon` based on variance. This is a sophisticated attempt at balancing exploration and exploitation. Heuristic 7's \"perfect fit bonus\" is more direct.\n\nComparing Heuristics 19 and 20: Heuristics 19 and 20 are identical.\n\nComparing Heuristics 2 and 10: Heuristic 10 combines \"Best Fit\" with a \"diversification bonus\" proportional to remaining capacity, aiming to balance fitting tightly and spreading items. This is a more direct approach to diversification than Heuristic 2's variance-based method.\n\nComparing Heuristics 10 and 11-15: Heuristics 11-15 are all very similar, implementing a basic \"inverse remaining capacity\" heuristic, often without Softmax normalization or sophisticated adaptive strategies. They are simple but potentially less robust than Softmax-based approaches.\n\nComparing Heuristics 11-15 and 16-18: Heuristics 16-18 use `np.exp(effective_capacities)` which is essentially a form of Softmax but without explicit negative scaling for Best Fit. This might prioritize larger remaining capacities too much.\n\nOverall: Heuristics that dynamically adjust parameters (like beta or epsilon) based on the current state (variance, slack) and explicitly handle \"perfect fits\" tend to be better. The complexity in Heuristics 3, 4, and 19/20 suggests a trend towards more adaptive and nuanced scoring. Simple inversions (Heuristics 11-15) are generally less effective.\n- \nHere's a redefined \"Current Self-Reflection\" focused on designing better heuristics:\n\n*   **Keywords:** Adaptive Parameters, Hybridization, Reward Structures, Numerical Stability.\n*   **Advice:** Design adaptive strategies that dynamically tune parameters based on problem characteristics (e.g., variance, resource slack) to balance exploitation and exploration. Integrate hybrid selection mechanisms that combine greedy \"best fit\" with probabilistic diversification.\n*   **Avoid:** Overly simple scoring, ignoring perfect fits, fragile parameter tuning, and neglecting numerical precision.\n*   **Explanation:** This approach emphasizes sophisticated, state-aware adaptation and balanced selection, drawing from robust techniques like Softmax and explicitly valuing efficient solutions (perfect fits) while ensuring computational reliability.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}