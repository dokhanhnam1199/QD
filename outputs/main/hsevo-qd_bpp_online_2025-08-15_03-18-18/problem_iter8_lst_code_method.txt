{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for overly full bins and explicit perfect fit reward.\n    Prioritizes bins that fit the item well, penalizes excessive remaining space,\n    and gives a strong boost to perfect fits before Softmax normalization.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Base score: Prioritize bins that leave less remaining space (Best Fit).\n    # Maximize (item - remaining_capacity_after_fit), equivalent to minimize (remaining_capacity_after_fit).\n    # Use negative remaining capacity after fit as a base score for maximization.\n    base_scores = -(valid_bins_remain_cap - item)\n\n    # Add bonus for perfect fits\n    perfect_fit_bonus = 0.5  # A significant bonus\n    perfect_fit_mask = (valid_bins_remain_cap - item) < 1e-9\n    base_scores[perfect_fit_mask] += perfect_fit_bonus\n\n    # Add a slight penalty for bins that would become \"too empty\" after fitting.\n    # This encourages using bins that are already somewhat full.\n    # For example, if remaining_cap - item > threshold, subtract a penalty.\n    # Let's define \"too empty\" as having more than 75% of the original bin capacity remaining\n    # after the item is placed. This is a tunable parameter.\n    too_empty_threshold_ratio = 0.75\n    original_bin_capacities = bins_remain_cap[valid_bins_mask] # This assumes we know original bin sizes, which we don't have directly.\n    # A proxy: Penalize if remaining_cap - item is large relative to the item size itself,\n    # or relative to the original bin capacity if we had it.\n    # Let's use a penalty if the remaining space after fitting is large relative to the bin's *current* capacity.\n    # This might be problematic if current capacity is very small.\n    # A simpler approach is to penalize based on the resulting 'waste' if the fit is not tight.\n    # Let's penalize bins where `remaining_cap - item` is large.\n    # We want to *minimize* this value. So, we add a penalty to the score if it's large.\n    # This means subtracting a positive value from the score if `remaining_cap - item` is large.\n    large_remaining_penalty_factor = 0.1 # Tune this\n    penalty_threshold = 0.5 # Penalize if remaining space > 50% of original bin capacity (proxy: current remaining capacity)\n    \n    # Calculate penalty: if (valid_bins_remain_cap - item) > penalty_threshold * valid_bins_remain_cap\n    # This means remaining space is more than 50% of what was available.\n    # We want to penalize these. Subtract a value.\n    penalty = np.maximum(0, (valid_bins_remain_cap - item) - penalty_threshold * valid_bins_remain_cap) * large_remaining_penalty_factor\n    \n    scores = base_scores - penalty\n\n    # Softmax normalization\n    shifted_scores = scores - np.max(scores)\n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid heuristic combining Best Fit with a penalty for overly tight fits\n    and a bonus for bins with ample remaining capacity.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(valid_bins_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    # Best Fit Component: Penalize based on remaining capacity after packing.\n    # Smaller remaining capacity is better.\n    best_fit_scores = -(valid_bins_remain_cap - item)\n\n    # Diversification Component: Reward bins with more initial remaining capacity.\n    # This favors less full bins.\n    diversification_scores = valid_bins_remain_cap\n\n    # Combine components with weights. Tune these weights based on empirical performance.\n    # w_bf = 1.0 emphasizes Best Fit.\n    # w_div = 0.3 gives a moderate preference to less full bins.\n    w_bf = 1.0\n    w_div = 0.3\n    \n    combined_scores = w_bf * best_fit_scores + w_div * diversification_scores\n\n    # Apply Softmax to get probabilities.\n    # Shift scores to prevent overflow/underflow issues with large exponents.\n    if combined_scores.size > 0:\n        shifted_scores = combined_scores - np.max(combined_scores)\n        exp_scores = np.exp(shifted_scores)\n        probabilities = exp_scores / np.sum(exp_scores)\n    else:\n        probabilities = np.array([])\n\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (minimizing waste) with a Softmax approach for robust priority.\n\n    Prioritizes bins that leave minimal remaining capacity after item placement,\n    using Softmax for smooth probability distribution and better exploration.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(valid_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    # Base score: Negative of remaining capacity after fitting (closer to 0 is better)\n    # This embodies the \"Best Fit\" principle.\n    base_scores = -(suitable_bins_remain_cap - item)\n\n    # Softmax for normalization: Convert scores to a probability-like distribution\n    # Shift scores to prevent overflow/underflow before exponentiation\n    if np.max(base_scores) - np.min(base_scores) > 1e-9: # Avoid issues if all scores are identical\n        shifted_scores = base_scores - np.max(base_scores)\n        exp_scores = np.exp(shifted_scores)\n        # Ensure sum is not zero to avoid division by zero\n        sum_exp_scores = np.sum(exp_scores)\n        if sum_exp_scores > 1e-9:\n            probabilities = exp_scores / sum_exp_scores\n        else:\n            # Fallback if all exponentiated scores are effectively zero\n            probabilities = np.ones_like(base_scores) / len(base_scores)\n    else:\n        # If all base scores are the same, assign equal probability\n        probabilities = np.ones_like(base_scores) / len(base_scores)\n\n\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for each bin using a combined strategy for\n    the online Bin Packing Problem, balancing \"best fit\" with a penalty for\n    bins that would become excessively full.\n\n    This strategy prioritizes bins that can accommodate the item, favoring those\n    that result in less remaining capacity (\"best fit\"). Additionally, it\n    introduces a penalty for bins that, after packing the item, would have\n    very little remaining capacity, encouraging a more diversified packing\n    strategy to potentially avoid early bin exhaustion.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the\n                         remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # \"Best Fit\" component: Minimize remaining capacity after placing the item.\n    # Higher score for smaller remaining capacity.\n    best_fit_scores = -(valid_bins_remain_cap - item)\n\n    # Diversification component: Penalize bins that become too full.\n    # A threshold can be used, e.g., if remaining_capacity - item < threshold.\n    # Here, we use a sigmoid-like function to penalize as remaining capacity decreases.\n    # This encourages not filling bins too tightly unless absolutely necessary.\n    # We want to *reduce* the score if the bin becomes too full.\n    # A smaller (remaining_capacity - item) means a fuller bin.\n    # Let's create a penalty that is high when (remaining_capacity - item) is low.\n    # Using exp(-x) where x is a scaled version of (remaining_capacity - item)\n    # Smaller x (tighter fit) leads to larger penalty.\n    # We need to be careful with scaling to avoid numerical issues.\n    # Let's use a threshold-based penalty that is smooth.\n    # Consider remaining capacity after packing: `rem_after_pack = valid_bins_remain_cap - item`\n    # We want to penalize small `rem_after_pack`.\n    # A simple penalty could be `penalty = 1 / (1 + exp(k * rem_after_pack))`\n    # where k controls the steepness. A small `rem_after_pack` results in large penalty.\n    # Let's use a small value for k to make it a soft penalty.\n    # Scale remaining capacity to avoid overflow in exp.\n    # For example, map to [0, 10] range.\n    min_rem_after_pack = 0.0\n    max_rem_after_pack = np.max(valid_bins_remain_cap) - item if np.max(valid_bins_remain_cap) >= item else 0.0\n    \n    if max_rem_after_pack == min_rem_after_pack: # Avoid division by zero if all valid bins have same remaining capacity\n        diversification_penalty = np.zeros_like(best_fit_scores)\n    else:\n        rem_after_pack = valid_bins_remain_cap - item\n        # Scale remaining capacity to a reasonable range for the penalty function\n        # e.g., map to [0, 10] to control the steepness of the penalty\n        scaled_rem_after_pack = 10 * (rem_after_pack - min_rem_after_pack) / (max_rem_after_pack - min_rem_after_pack)\n        # Penalty is high for small remaining capacity (tight fit)\n        diversification_penalty = 1.0 / (1.0 + np.exp(5.0 * (1.0 - scaled_rem_after_pack))) # penalty ~ 1 for tight fit, ~ 0 for loose fit\n\n    # Combine scores: Favor best fit, but penalize overly tight fits.\n    # We subtract the penalty from the best_fit_scores.\n    # A higher combined score is better.\n    combined_scores = best_fit_scores - diversification_penalty * 2.0 # Adjust multiplier for penalty strength\n\n    # Apply Softmax to convert scores into probabilities (priorities)\n    # Shift scores to avoid numerical overflow/underflow in exp\n    shifted_scores = combined_scores - np.max(combined_scores)\n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Create the final priority array\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for overly full bins and explicit perfect fit reward.\n    Prioritizes bins that fit the item well, penalizes excessive remaining space,\n    and gives a strong boost to perfect fits before Softmax normalization.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Base score: Prioritize bins that leave less remaining space (Best Fit).\n    # Maximize (item - remaining_capacity_after_fit), equivalent to minimize (remaining_capacity_after_fit).\n    # Use negative remaining capacity after fit as a base score for maximization.\n    base_scores = -(valid_bins_remain_cap - item)\n\n    # Add bonus for perfect fits\n    perfect_fit_bonus = 0.5  # A significant bonus\n    perfect_fit_mask = (valid_bins_remain_cap - item) < 1e-9\n    base_scores[perfect_fit_mask] += perfect_fit_bonus\n\n    # Add a slight penalty for bins that would become \"too empty\" after fitting.\n    # This encourages using bins that are already somewhat full.\n    # For example, if remaining_cap - item > threshold, subtract a penalty.\n    # Let's define \"too empty\" as having more than 75% of the original bin capacity remaining\n    # after the item is placed. This is a tunable parameter.\n    too_empty_threshold_ratio = 0.75\n    original_bin_capacities = bins_remain_cap[valid_bins_mask] # This assumes we know original bin sizes, which we don't have directly.\n    # A proxy: Penalize if remaining_cap - item is large relative to the item size itself,\n    # or relative to the original bin capacity if we had it.\n    # Let's use a penalty if the remaining space after fitting is large relative to the bin's *current* capacity.\n    # This might be problematic if current capacity is very small.\n    # A simpler approach is to penalize based on the resulting 'waste' if the fit is not tight.\n    # Let's penalize bins where `remaining_cap - item` is large.\n    # We want to *minimize* this value. So, we add a penalty to the score if it's large.\n    # This means subtracting a positive value from the score if `remaining_cap - item` is large.\n    large_remaining_penalty_factor = 0.1 # Tune this\n    penalty_threshold = 0.5 # Penalize if remaining space > 50% of original bin capacity (proxy: current remaining capacity)\n    \n    # Calculate penalty: if (valid_bins_remain_cap - item) > penalty_threshold * valid_bins_remain_cap\n    # This means remaining space is more than 50% of what was available.\n    # We want to penalize these. Subtract a value.\n    penalty = np.maximum(0, (valid_bins_remain_cap - item) - penalty_threshold * valid_bins_remain_cap) * large_remaining_penalty_factor\n    \n    scores = base_scores - penalty\n\n    # Softmax normalization\n    shifted_scores = scores - np.max(scores)\n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates bin priorities using a hybrid approach:\n    1. Prioritizes perfect fits (zero remaining capacity).\n    2. Favors \"best fit\" bins by penalizing remaining capacity.\n    3. Applies a penalty to avoid overly tight fits, promoting diversity.\n    4. Normalizes scores using Softmax for a probabilistic distribution.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Handle the case where no bins can fit the item\n    if not np.any(can_fit_mask):\n        return priorities\n    \n    # Extract capacities of bins that can fit the item\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Calculate remaining capacity after packing for valid bins\n    rem_after_pack = valid_bins_remain_cap - item\n    \n    # --- Component 1: Perfect Fit Bonus ---\n    # High score for bins that become exactly full\n    perfect_fit_bonus = np.where(rem_after_pack == 0, 10.0, 0.0)\n    \n    # --- Component 2: Best Fit (Inverse Proximity) ---\n    # Score inversely proportional to remaining capacity (smaller remaining capacity is better)\n    # Add a small epsilon to avoid division by zero if remaining capacity is exactly 0 (handled by bonus)\n    best_fit_score = -rem_after_pack\n    \n    # --- Component 3: Diversification Penalty ---\n    # Penalize bins that would have very little remaining capacity after packing,\n    # but only if they are not a perfect fit.\n    # We use a sigmoid-like function to penalize as remaining capacity decreases.\n    # Scale remaining capacity to avoid large exponential values.\n    # The penalty is higher for smaller `rem_after_pack`.\n    min_rem = np.min(rem_after_pack[rem_after_pack > 0]) if np.any(rem_after_pack > 0) else 0\n    max_rem = np.max(rem_after_pack) if np.any(rem_after_pack > 0) else 0\n    \n    diversification_penalty = np.zeros_like(valid_bins_remain_cap)\n    if max_rem > min_rem: # Avoid division by zero if all valid bins have the same remaining capacity > 0\n        # Scale non-zero remaining capacities to a range like [0, 10]\n        scaled_rem = 10 * (rem_after_pack[rem_after_pack > 0] - min_rem) / (max_rem - min_rem)\n        # Apply penalty: higher penalty for smaller scaled_rem (tighter fits)\n        # Using exp(k * (1 - x)) makes the penalty high when x is small.\n        penalty_strength = 5.0\n        diversification_penalty[rem_after_pack > 0] = 1.0 / (1.0 + np.exp(penalty_strength * (1.0 - scaled_rem)))\n    elif np.any(rem_after_pack == 0): # If there are perfect fits but no other valid fits, no diversification penalty needed\n        pass\n    elif np.any(rem_after_pack > 0): # If all valid fits have the same positive remaining capacity\n        pass # No penalty needed as they are all equally \"open\"\n\n    # Combine scores: perfect_fit_bonus + best_fit_score - diversification_penalty * weight\n    # The weight controls how strongly we penalize tight fits.\n    penalty_weight = 2.0\n    combined_scores = perfect_fit_bonus + best_fit_score - diversification_penalty * penalty_weight\n    \n    # --- Component 4: Softmax Normalization ---\n    # Convert scores to probabilities (priorities)\n    # Shift scores to avoid numerical instability with exp\n    shifted_scores = combined_scores - np.max(combined_scores)\n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Place probabilities back into the original priority array structure\n    priorities[can_fit_mask] = probabilities\n    \n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for each bin using a Softmax-Based Fit strategy\n    for the online Bin Packing Problem.\n\n    This strategy prioritizes bins that have remaining capacity, with a higher\n    priority given to bins that can accommodate the item without significant\n    wastage, and also considers the overall \"fullness\" of bins.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the\n                         remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    # Filter out bins that cannot fit the item\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        # If no bin can fit the item, return zero priorities for all bins\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate a score for each valid bin:\n    # We want to favor bins that are nearly full after placing the item.\n    # (capacity - item) represents the remaining capacity after placement.\n    # Smaller values of (capacity - item) are better.\n    # We can invert this by taking the negative or by calculating (item - capacity) if capacity < item.\n    # For simplicity and Softmax compatibility, let's focus on the 'goodness' of fit.\n    # A good fit means small remaining capacity. So, we can use 1 / (remaining_after_fit)\n    # or something similar.\n\n    # A common heuristic for BPP is \"Best Fit\": choosing the bin that leaves the least empty space.\n    # So, remaining_capacity - item should be minimized.\n    # We want to maximize the \"suitability\" score.\n    # Let's consider the negative of the remaining capacity after fitting as a base score.\n    # Larger negative values (closer to zero) are better.\n    base_scores = -(valid_bins_remain_cap - item)\n\n    # Add a penalty for bins that are already very full, encouraging spreading items if possible,\n    # unless an item perfectly fits. This can be tricky.\n    # For simplicity in v2, let's focus on the immediate fit.\n\n    # Apply Softmax to convert scores into probabilities (priorities)\n    # Softmax: exp(score) / sum(exp(all_scores))\n    # To avoid numerical instability with very large or small scores, we can shift scores.\n    # Subtracting the maximum score before exponentiation is a common technique.\n    shifted_scores = base_scores - np.max(base_scores)\n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Create the final priority array, placing calculated priorities in their original positions\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    priorities[suitable_bins_mask] = 1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    priorities[suitable_bins_mask] = 1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    priorities[suitable_bins_mask] = 1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \n    bin_capacities = 1.0  # Assuming a standard bin capacity of 1.0, can be generalized.\n    \n    potential_fits = bins_remain_cap - item\n    \n    valid_bins_mask = potential_fits >= 0\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    \n    if np.any(valid_bins_mask):\n        \n        priorities[valid_bins_mask] = 1.0 / (potential_fits[valid_bins_mask] + 1e-9) # Add epsilon to avoid division by zero\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \n    bin_capacities = 1.0  # Assuming a standard bin capacity of 1.0, can be generalized.\n    \n    potential_fits = bins_remain_cap - item\n    \n    valid_bins_mask = potential_fits >= 0\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    \n    if np.any(valid_bins_mask):\n        \n        priorities[valid_bins_mask] = 1.0 / (potential_fits[valid_bins_mask] + 1e-9) # Add epsilon to avoid division by zero\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \n    bin_capacities = 1.0  # Assuming a standard bin capacity of 1.0, can be generalized.\n    \n    potential_fits = bins_remain_cap - item\n    \n    valid_bins_mask = potential_fits >= 0\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    \n    if np.any(valid_bins_mask):\n        \n        priorities[valid_bins_mask] = 1.0 / (potential_fits[valid_bins_mask] + 1e-9) # Add epsilon to avoid division by zero\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            \n            remaining_cap = bins_remain_cap[i]\n            \n            \n            if remaining_cap == item:\n                priorities[i] = 1.0  # Perfect fit, highest priority\n            else:\n                \n                priorities[i] = 1.0 / (remaining_cap - item + 1e-9) # Inverse proximity, higher score for bins that are closer to fitting the item without being perfect\n                \n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            \n            remaining_cap = bins_remain_cap[i]\n            \n            \n            if remaining_cap == item:\n                priorities[i] = 1.0  # Perfect fit, highest priority\n            else:\n                \n                priorities[i] = 1.0 / (remaining_cap - item + 1e-9) # Inverse proximity, higher score for bins that are closer to fitting the item without being perfect\n                \n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    priorities[suitable_bins_mask] = 1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    priorities[suitable_bins_mask] = 1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)\n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    valid_bins = bins_remain_cap >= item\n    \n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n    \n    effective_capacities = bins_remain_cap[valid_bins] - item\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bins] = np.exp(effective_capacities)\n    \n    if np.sum(priorities) == 0:\n        return np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n        \n    return priorities / np.sum(priorities)\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for each bin using an adaptive strategy for the\n    online Bin Packing Problem. This version aims to balance fitting tightly\n    (Best Fit) with spreading items (Worst Fit) and incorporating an element\n    of exploration.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the\n                         remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # --- Core Strategy: Balancing Best Fit and Worst Fit ---\n    # Best Fit component: Prioritize bins that leave minimal remaining space.\n    # We want to minimize (remaining_capacity - item).\n    # A good score for BF would be proportional to -(remaining_capacity - item).\n    # For Softmax, we want higher scores for better options. So, let's use\n    # a score that increases as remaining_capacity - item decreases.\n    # A simple inversion: 1 / (remaining_capacity - item + epsilon)\n    # Or, to keep it related to the previous approach: maximize -(remaining_capacity - item)\n    # To promote diversification, let's also consider the \"emptiness\" of the bin.\n    # Worst Fit component: Prioritize bins with *more* remaining capacity.\n    # This encourages spreading items.\n    # A score for WF could be proportional to remaining_capacity.\n\n    # Let's create a blended score.\n    # For Best Fit: prioritize small remaining capacity after placing the item.\n    # For Worst Fit: prioritize large initial remaining capacity.\n    # We want to maximize the utility.\n    # Let's consider the utility as a function of remaining capacity:\n    # utility = alpha * (1 / (valid_bins_remain_cap - item + 1e-9)) + beta * valid_bins_remain_cap\n\n    # For simplicity and to adapt the softmax approach, let's define scores\n    # where higher means more desirable.\n    # High score for small (remaining_capacity - item) => Best Fit tendency\n    # High score for large remaining_capacity => Worst Fit tendency (for exploration/diversification)\n\n    # Let's try a score that is a combination:\n    # Score = (large_capacity_bonus) * (remaining_capacity) - (misfit_penalty) * (remaining_capacity - item)\n    # A simpler approach:\n    # Prioritize bins where remaining_capacity - item is small (BF)\n    # BUT, also give a boost to bins that are \"more open\" (WF) to avoid early convergence.\n\n    # Let's combine the ideas:\n    # We want to favor small (remaining_capacity - item).\n    # Let's define a score for \"tightness\": TightnessScore = -(valid_bins_remain_cap - item)\n    # And a score for \"openness\": OpennessScore = valid_bins_remain_cap\n\n    # We can create a combined score, for example, by averaging or taking a weighted sum.\n    # A more robust approach is to introduce a \"temperature\" or \"exploration factor\"\n    # that modulates the influence of the Best Fit vs. Worst Fit tendencies.\n\n    # Let's try a score that is a compromise. We want to minimize (remaining_capacity - item).\n    # Let's use a function that is high when (remaining_capacity - item) is small.\n    # Consider a function like: `exp(-k * (remaining_capacity - item))`\n    # `k` can be an exploration parameter. A large `k` makes it more like Best Fit.\n    # A small `k` makes it flatter, more exploratory.\n\n    # To balance exploration and exploitation, let's make the \"tightness\" score\n    # have an exploratory element.\n    # Let's use a score that is high for bins that are \"good\" fits, but also\n    # has some preference for bins that aren't *too* full if an exact fit isn't available.\n\n    # --- Adaptive Exploration/Exploitation ---\n    # We can adapt the strength of the \"Best Fit\" tendency based on the distribution of remaining capacities.\n    # If capacities are very diverse, lean more towards Best Fit.\n    # If capacities are very similar, lean more towards diversification.\n\n    # A simple adaptation: Use a parameter `epsilon` that smooths the selection.\n    # Larger epsilon makes it more uniform (exploratory). Smaller epsilon makes it more greedy (exploitative).\n    # We can define epsilon based on the variance of remaining capacities.\n    variance_remain_cap = np.var(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 1.0\n    # Scale variance to a reasonable epsilon range. Higher variance -> higher epsilon for more exploration.\n    epsilon = 0.1 + 0.5 * (1 / (1 + np.exp(-0.1 * variance_remain_cap))) # Sigmoid to bound epsilon\n\n    # Calculate scores:\n    # Score for \"good fit\" (lower remaining_capacity - item is better)\n    # Using a negative exponential for a sharp decrease in score as misfit increases.\n    # Adding epsilon for numerical stability and exploration.\n    goodness_of_fit_scores = np.exp(-10.0 * (valid_bins_remain_cap - item) / (item + 1e-9)) # Scale by item size\n\n    # Score for \"openness\" (higher remaining_capacity is better for spreading)\n    # Using a scaled exponential to give a significant boost to very open bins.\n    openness_scores = np.exp(0.1 * valid_bins_remain_cap / (np.max(bins_remain_cap) + 1e-9)) # Scale by max capacity\n\n    # Combine scores using epsilon for adaptive weighting\n    # When epsilon is high (diverse capacities), openness_scores have more weight.\n    # When epsilon is low (similar capacities), goodness_of_fit_scores have more weight.\n    combined_scores = (1 - epsilon) * goodness_of_fit_scores + epsilon * openness_scores\n\n    # Softmax transformation to get priorities\n    # Shift scores to prevent overflow/underflow before exponentiation\n    shifted_scores = combined_scores - np.max(combined_scores)\n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Create the final priority array\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for each bin using an adaptive strategy for the\n    online Bin Packing Problem. This version aims to balance fitting tightly\n    (Best Fit) with spreading items (Worst Fit) and incorporating an element\n    of exploration.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the\n                         remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # --- Core Strategy: Balancing Best Fit and Worst Fit ---\n    # Best Fit component: Prioritize bins that leave minimal remaining space.\n    # We want to minimize (remaining_capacity - item).\n    # A good score for BF would be proportional to -(remaining_capacity - item).\n    # For Softmax, we want higher scores for better options. So, let's use\n    # a score that increases as remaining_capacity - item decreases.\n    # A simple inversion: 1 / (remaining_capacity - item + epsilon)\n    # Or, to keep it related to the previous approach: maximize -(remaining_capacity - item)\n    # To promote diversification, let's also consider the \"emptiness\" of the bin.\n    # Worst Fit component: Prioritize bins with *more* remaining capacity.\n    # This encourages spreading items.\n    # A score for WF could be proportional to remaining_capacity.\n\n    # Let's create a blended score.\n    # For Best Fit: prioritize small remaining capacity after placing the item.\n    # For Worst Fit: prioritize large initial remaining capacity.\n    # We want to maximize the utility.\n    # Let's consider the utility as a function of remaining capacity:\n    # utility = alpha * (1 / (valid_bins_remain_cap - item + 1e-9)) + beta * valid_bins_remain_cap\n\n    # For simplicity and to adapt the softmax approach, let's define scores\n    # where higher means more desirable.\n    # High score for small (remaining_capacity - item) => Best Fit tendency\n    # High score for large remaining_capacity => Worst Fit tendency (for exploration/diversification)\n\n    # Let's try a score that is a combination:\n    # Score = (large_capacity_bonus) * (remaining_capacity) - (misfit_penalty) * (remaining_capacity - item)\n    # A simpler approach:\n    # Prioritize bins where remaining_capacity - item is small (BF)\n    # BUT, also give a boost to bins that are \"more open\" (WF) to avoid early convergence.\n\n    # Let's combine the ideas:\n    # We want to favor small (remaining_capacity - item).\n    # Let's define a score for \"tightness\": TightnessScore = -(valid_bins_remain_cap - item)\n    # And a score for \"openness\": OpennessScore = valid_bins_remain_cap\n\n    # We can create a combined score, for example, by averaging or taking a weighted sum.\n    # A more robust approach is to introduce a \"temperature\" or \"exploration factor\"\n    # that modulates the influence of the Best Fit vs. Worst Fit tendencies.\n\n    # Let's try a score that is a compromise. We want to minimize (remaining_capacity - item).\n    # Let's use a function that is high when (remaining_capacity - item) is small.\n    # Consider a function like: `exp(-k * (remaining_capacity - item))`\n    # `k` can be an exploration parameter. A large `k` makes it more like Best Fit.\n    # A small `k` makes it flatter, more exploratory.\n\n    # To balance exploration and exploitation, let's make the \"tightness\" score\n    # have an exploratory element.\n    # Let's use a score that is high for bins that are \"good\" fits, but also\n    # has some preference for bins that aren't *too* full if an exact fit isn't available.\n\n    # --- Adaptive Exploration/Exploitation ---\n    # We can adapt the strength of the \"Best Fit\" tendency based on the distribution of remaining capacities.\n    # If capacities are very diverse, lean more towards Best Fit.\n    # If capacities are very similar, lean more towards diversification.\n\n    # A simple adaptation: Use a parameter `epsilon` that smooths the selection.\n    # Larger epsilon makes it more uniform (exploratory). Smaller epsilon makes it more greedy (exploitative).\n    # We can define epsilon based on the variance of remaining capacities.\n    variance_remain_cap = np.var(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 1.0\n    # Scale variance to a reasonable epsilon range. Higher variance -> higher epsilon for more exploration.\n    epsilon = 0.1 + 0.5 * (1 / (1 + np.exp(-0.1 * variance_remain_cap))) # Sigmoid to bound epsilon\n\n    # Calculate scores:\n    # Score for \"good fit\" (lower remaining_capacity - item is better)\n    # Using a negative exponential for a sharp decrease in score as misfit increases.\n    # Adding epsilon for numerical stability and exploration.\n    goodness_of_fit_scores = np.exp(-10.0 * (valid_bins_remain_cap - item) / (item + 1e-9)) # Scale by item size\n\n    # Score for \"openness\" (higher remaining_capacity is better for spreading)\n    # Using a scaled exponential to give a significant boost to very open bins.\n    openness_scores = np.exp(0.1 * valid_bins_remain_cap / (np.max(bins_remain_cap) + 1e-9)) # Scale by max capacity\n\n    # Combine scores using epsilon for adaptive weighting\n    # When epsilon is high (diverse capacities), openness_scores have more weight.\n    # When epsilon is low (similar capacities), goodness_of_fit_scores have more weight.\n    combined_scores = (1 - epsilon) * goodness_of_fit_scores + epsilon * openness_scores\n\n    # Softmax transformation to get priorities\n    # Shift scores to prevent overflow/underflow before exponentiation\n    shifted_scores = combined_scores - np.max(combined_scores)\n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Create the final priority array\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}