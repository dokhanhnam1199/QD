{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Calculates priority scores for each bin using a Softmax-Based Fit strategy\n    for the online Bin Packing Problem.\n\n    This strategy prioritizes bins that have remaining capacity, with a higher\n    priority given to bins that can accommodate the item without significant\n    wastage, and also considers the overall \"fullness\" of bins.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the\n                         remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    # Filter out bins that cannot fit the item\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        # If no bin can fit the item, return zero priorities for all bins\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate a score for each valid bin:\n    # We want to favor bins that are nearly full after placing the item.\n    # (capacity - item) represents the remaining capacity after placement.\n    # Smaller values of (capacity - item) are better.\n    # We can invert this by taking the negative or by calculating (item - capacity) if capacity < item.\n    # For simplicity and Softmax compatibility, let's focus on the 'goodness' of fit.\n    # A good fit means small remaining capacity. So, we can use 1 / (remaining_after_fit)\n    # or something similar.\n\n    # A common heuristic for BPP is \"Best Fit\": choosing the bin that leaves the least empty space.\n    # So, remaining_capacity - item should be minimized.\n    # We want to maximize the \"suitability\" score.\n    # Let's consider the negative of the remaining capacity after fitting as a base score.\n    # Larger negative values (closer to zero) are better.\n    base_scores = -(valid_bins_remain_cap - item)\n\n    # Add a penalty for bins that are already very full, encouraging spreading items if possible,\n    # unless an item perfectly fits. This can be tricky.\n    # For simplicity in v2, let's focus on the immediate fit.\n\n    # Apply Softmax to convert scores into probabilities (priorities)\n    # Softmax: exp(score) / sum(exp(all_scores))\n    # To avoid numerical instability with very large or small scores, we can shift scores.\n    # Subtracting the maximum score before exponentiation is a common technique.\n    shifted_scores = base_scores - np.max(base_scores)\n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Create the final priority array, placing calculated priorities in their original positions\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1\n    valid_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    if np.any(valid_bins_mask):\n        \n        suitable_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n        \n        greedy_scores = 1 / (suitable_bins_remain_cap - item + 1e-6) \n        \n        random_scores = np.random.rand(np.sum(valid_bins_mask))\n        \n        combined_scores = epsilon * random_scores + (1 - epsilon) * greedy_scores\n        \n        priorities[valid_bins_mask] = combined_scores\n        \n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1-3 (Softmax-based) with Heuristics 4-12 (Inverse proximity): The primary difference is the approach to score normalization. Heuristics 1-3 use Softmax, which converts scores into a probability distribution, ensuring that the sum of priorities is 1. This is generally a more robust approach for selection mechanisms that expect probabilities. Heuristics 4-12 use a simpler inverse proximity score, which can lead to very large or very small values and doesn't inherently normalize the probabilities.\n\nComparing Heuristics 1-3 with Heuristics 17-20 (Exponentiated effective capacities): Heuristics 17-20 use `np.exp(effective_capacities)` directly. While this also amplifies the preference for better fits, it doesn't normalize the output in the same way as Softmax. If all effective capacities are large and positive, the resulting priorities can become extremely large, potentially causing numerical issues. Softmax, by subtracting the max before exponentiation, mitigates this. The fallback to `np.ones_like(...) / len(...)` if the sum is zero is an interesting edge case handling in 17-20.\n\nComparing Heuristics 1-3 with Heuristics 13-16 (epsilon-weighted random + greedy): These heuristics introduce a random element, balancing exploration (random scores) with exploitation (greedy scores). The weighting factor `epsilon` controls this balance. This is a more sophisticated approach than purely greedy or purely Softmax-based methods, as it can help escape local optima and discover better packing configurations over time.\n\nComparing Heuristics 4-7 with Heuristics 8-12: Heuristics 8-12 introduce a special case for perfect fits (`remaining_cap == item`), assigning them a priority of 1.0. This is a sensible addition that directly rewards perfect utilization of bin space. Heuristics 4-7 and 9 also use `1.0 / (potential_fits + 1e-9)`, but they miss this explicit perfect-fit handling.\n\nComparing Heuristics 4-7, 9 with Heuristics 5-7, 9: These are identical. They represent a basic \"best fit\" heuristic where the priority is inversely proportional to the remaining space after placing the item. The addition of `1e-9` is a good practice to avoid division by zero.\n\nComparing Heuristics 1-3 and 17-20 with Heuristics 4-16: The Softmax-based (1-3) and exponential (17-20) approaches offer a more nuanced distribution of priorities compared to the simple inverse proximity (4-16). The Softmax approach in 1-3 is generally preferred over the raw exponential in 17-20 due to better numerical stability and explicit probability interpretation. The combination of greedy and random in 13-16 adds an element of exploration which can be beneficial.\n\nOverall: The Softmax-based approach (1-3) offers a good balance of exploitation and robustness. The hybrid greedy-random approach (13-16) is a strong contender for its exploration capability. The explicit perfect-fit handling (8-12) is a valuable refinement.\n- \nHere's a redefined self-reflection for designing better heuristics:\n\n*   **Keywords:** Exploration-Exploitation, Diversification, Intensification, Robustness, Efficiency.\n*   **Advice:** Focus on mechanisms that actively balance exploring novel solution spaces with exploiting promising areas. Design mechanisms to foster diversity within a search population.\n*   **Avoid:** Passive or purely greedy exploration, neglecting potential for premature convergence, and overlooking edge cases that break numerical stability.\n*   **Explanation:** Effective self-reflection should identify *why* a heuristic might fail (e.g., getting stuck in local optima) and proactively design mechanisms (like simulated annealing or adaptive random restarts) to mitigate these weaknesses, leading to more robust and efficient exploration.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}