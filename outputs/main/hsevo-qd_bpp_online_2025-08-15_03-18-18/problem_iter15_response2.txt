```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray, temperature: float = 1.0) -> np.ndarray:
    """
    Calculates priority scores for each bin using a Multi-Objective Softmax strategy
    for the online Bin Packing Problem.

    This strategy prioritizes bins based on a combination of:
    1. Fit Quality: Favoring bins that result in minimal wasted space after packing.
    2. Bin Uniformity: Encouraging a more even distribution of items across bins,
       preventing some bins from becoming excessively full while others remain empty.

    The temperature parameter allows for control over the exploration-exploitation trade-off.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A numpy array where each element represents the
                         remaining capacity of a bin.
        temperature: A float controlling the "softness" of the Softmax distribution.
                     Higher temperatures lead to more uniform probabilities,
                     while lower temperatures emphasize higher-scoring bins.

    Returns:
        A numpy array of the same size as bins_remain_cap, where each element
        is the priority score for placing the item in the corresponding bin.
    """
    valid_bins_mask = bins_remain_cap >= item

    if not np.any(valid_bins_mask):
        return np.zeros_like(bins_remain_cap, dtype=float)

    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]

    # Objective 1: Fit Quality (Best Fit Principle)
    # We want to minimize (remaining_capacity - item).
    # A score that is higher for smaller (remaining_capacity - item) is desirable.
    # Let's use -(remaining_capacity - item) as a base for this objective.
    fit_scores = -(valid_bins_remain_cap - item)

    # Objective 2: Bin Uniformity
    # We want to favor bins that are not too full, to encourage spreading.
    # The current remaining capacity is a good indicator. Higher remaining capacity
    # means the bin is less full.
    # Let's use the remaining capacity itself as a score for this objective.
    # Larger remaining capacity means higher score for uniformity.
    uniformity_scores = valid_bins_remain_cap

    # Combine the objectives. A simple weighted sum can be used.
    # For now, let's give equal weight, but this could be a hyperparameter.
    # Alternatively, we can normalize scores before combining.
    # Let's normalize both scores to be between 0 and 1 for better combination.

    # Normalize fit_scores: Scale to [0, 1] based on the range of valid fit_scores
    min_fit = np.min(fit_scores)
    max_fit = np.max(fit_scores)
    if max_fit > min_fit:
        normalized_fit_scores = (fit_scores - min_fit) / (max_fit - min_fit)
    else:
        normalized_fit_scores = np.ones_like(fit_scores) * 0.5 # Neutral if all fit scores are same

    # Normalize uniformity_scores: Scale to [0, 1] based on the range of valid uniformity_scores
    min_uni = np.min(uniformity_scores)
    max_uni = np.max(uniformity_scores)
    if max_uni > min_uni:
        normalized_uniformity_scores = (uniformity_scores - min_uni) / (max_uni - min_uni)
    else:
        normalized_uniformity_scores = np.ones_like(uniformity_scores) * 0.5 # Neutral if all uniformity scores are same

    # Combine normalized scores. Let's weight fit quality slightly higher.
    # These weights could be learned or tuned.
    weight_fit = 0.6
    weight_uniformity = 0.4
    combined_scores = weight_fit * normalized_fit_scores + weight_uniformity * normalized_uniformity_scores

    # Apply Softmax with temperature for probabilistic selection
    # Shift scores to prevent numerical overflow/underflow before exponentiation
    shifted_scores = combined_scores - np.max(combined_scores)
    
    # Ensure temperature is positive to avoid division by zero or negative exponents
    effective_temperature = max(temperature, 1e-6) 
    
    exp_scores = np.exp(shifted_scores / effective_temperature)
    
    # Handle cases where sum of exp_scores might be zero (e.g., all scores are -inf, unlikely here but good practice)
    sum_exp_scores = np.sum(exp_scores)
    if sum_exp_scores == 0:
        probabilities = np.ones_like(exp_scores) / len(exp_scores)
    else:
        probabilities = exp_scores / sum_exp_scores

    # Create the final priority array
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    priorities[valid_bins_mask] = probabilities

    return priorities
```
