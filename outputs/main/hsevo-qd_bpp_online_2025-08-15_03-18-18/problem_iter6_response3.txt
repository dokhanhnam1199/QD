```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Calculates priority scores for each bin using an adaptive, multi-objective
    approach for the online Bin Packing Problem.

    This heuristic aims to balance immediate fit (minimizing waste) with
    long-term bin utilization. It prioritizes bins that are a good fit for the
    current item, while also considering bins that have a moderate amount of
    remaining capacity, potentially for future larger items. It uses a
    weighted sum of two components, with weights adaptively adjusted based on
    the item's size relative to the average bin capacity and the proportion of
    already occupied bins.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A numpy array where each element represents the
                         remaining capacity of a bin.

    Returns:
        A numpy array of the same size as bins_remain_cap, where each element
        is the priority score for placing the item in the corresponding bin.
    """
    num_bins = len(bins_remain_cap)
    if num_bins == 0:
        return np.array([], dtype=float)

    valid_bins_mask = bins_remain_cap >= item
    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]

    if valid_bins_remain_cap.size == 0:
        return np.zeros_like(bins_remain_cap)

    # Component 1: Best Fit - Minimize remaining capacity after placing the item
    # A smaller (remaining_capacity - item) is better. We want to maximize this score.
    # We use a large negative value for bins that don't fit, then shift.
    best_fit_scores = np.full_like(bins_remain_cap, -np.inf)
    best_fit_scores[valid_bins_mask] = -(valid_bins_remain_cap - item)

    # Component 2: Spread/Fill - Favor bins that are not too full, but also not too empty.
    # This encourages distributing items to avoid prematurely filling bins.
    # We can model this by favoring bins with remaining capacity around a 'target'
    # which could be related to the average remaining capacity or a fraction of bin capacity.
    # For simplicity, let's consider the inverse of remaining capacity (higher for less capacity)
    # but only for bins that can fit the item.
    # We want to penalize very empty bins more than moderately full bins, while still
    # favoring bins that are not *completely* full.
    # Let's try to capture the idea of 'not too full, not too empty'.
    # Consider a Gaussian-like function centered around some 'ideal' remaining capacity.
    # Or, a simpler approach: favor bins with moderate remaining capacity.
    # Let's try: prioritize bins with remaining capacity such that it's not too small (for future items)
    # but not too large either.
    # A simple approach: penalize bins with very little remaining capacity more severely than
    # bins with a lot of remaining capacity IF they don't fit the item perfectly.
    # Let's create a score that is high when remaining capacity is moderate.
    # For valid bins, a score proportional to remaining_capacity could be used, but
    # this conflicts with Best Fit.

    # Alternative for Component 2: Consider the overall "emptiness" of bins.
    # A higher score for bins with less total remaining capacity.
    # This encourages filling up existing bins before opening new ones.
    # We will scale this by how much capacity is left *after* placing the item.
    # So, if a bin has lots of capacity, but `item` fits, this component should still be decent.
    # Let's try focusing on the inverse of total available capacity for valid bins.
    # Higher score for less total capacity.
    # `bins_remain_cap` represents remaining capacity. So, inverse of this could be a score.
    # A higher score for bins that are *already* more full.
    fill_scores = np.full_like(bins_remain_cap, -np.inf)
    # For valid bins, a score that increases as bins_remain_cap decreases.
    # Adding a small epsilon to avoid division by zero.
    fill_scores[valid_bins_mask] = 1.0 / (bins_remain_cap[valid_bins_mask] + 1e-6)

    # Adaptive weighting based on item size and bin fullness
    # If item is small relative to typical bin capacity, we might want to spread items more (favor fill_scores)
    # If item is large, we might want to prioritize best fit more.
    # Average remaining capacity can indicate how "full" the system is.
    avg_remain_cap = np.mean(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 1.0 # Avoid division by zero
    # Assume a standard bin capacity, e.g., 1.0 or a parameter. Let's use 1.0 as a common baseline.
    # If average remaining capacity is low, bins are generally full.
    # If item is small compared to average remaining capacity, it's relatively easy to fit.
    # If item is large compared to average remaining capacity, it's harder to fit.

    # Heuristic for weights:
    # Let 'target_fill_ratio' be a desired fill level for each bin (e.g., 0.8).
    # If the average remaining capacity is low (bins are full), we might lean towards best fit.
    # If average remaining capacity is high (bins are empty), we might lean towards spreading (fill_scores).

    # Let's use a simple weight based on how "full" the system is.
    # If bins are mostly empty (high avg_remain_cap), we might give more weight to `fill_scores`
    # to encourage packing into fewer bins.
    # If bins are mostly full (low avg_remain_cap), we might give more weight to `best_fit_scores`.

    # Normalize average remaining capacity against a conceptual full bin (e.g., capacity 1.0)
    # This gives a rough idea of how "empty" the system is.
    # If avg_remain_cap = 0.8, system is 20% full on average.
    # If avg_remain_cap = 0.2, system is 80% full on average.

    # We want weight_fill to be high when avg_remain_cap is high (system is empty)
    # and weight_best_fit to be high when avg_remain_cap is low (system is full).
    # Let's use a sigmoid-like function or a linear interpolation.
    # Let's assume bin capacity is 1 for normalization purposes.
    # Normalized empty space: avg_remain_cap / 1.0
    # If normalized empty space is high (e.g., 0.8), we want more weight on fill_scores.
    # If normalized empty space is low (e.g., 0.2), we want more weight on best_fit_scores.

    # Clamp avg_remain_cap to avoid extreme values and ensure meaningful weights
    clamped_avg_remain_cap = np.clip(avg_remain_cap, 0.01, 0.99) # Assuming max bin capacity is around 1

    # Weight for fill_scores: higher when system is emptier
    # Weight for best_fit_scores: higher when system is fuller
    # Let's try a simple linear interpolation.
    # weight_fill = clamped_avg_remain_cap
    # weight_best_fit = 1.0 - clamped_avg_remain_cap

    # A slightly more nuanced approach:
    # If the item is very small compared to remaining capacity, best-fit waste is proportionally large.
    # If the item is large, best-fit waste is a smaller proportion of remaining capacity.
    # Consider the ratio `item / bins_remain_cap[valid_bins_mask]`.
    # When this ratio is close to 1, best-fit is good.
    # When this ratio is small, best-fit might leave a lot of empty space.

    # Let's simplify weighting:
    # If there are many bins with large remaining capacities (system is sparse),
    # we might prioritize filling existing bins (favor `fill_scores`).
    # If bins are mostly utilized (sparse remaining capacities), we might prioritize
    # minimizing immediate waste (`best_fit_scores`).

    # Calculate a 'system fullness' metric:
    # proportion of bins that are "nearly full" (e.g., remaining capacity < 0.2 of original capacity)
    # For simplicity, let's use the average remaining capacity as the primary driver.

    # Define a threshold for 'emptiness' of the system. If average remaining capacity > threshold,
    # lean towards fill_scores. Otherwise, lean towards best_fit_scores.
    emptiness_threshold = 0.5 # Example: if average remaining capacity is > 0.5, system is considered "empty"

    if avg_remain_cap > emptiness_threshold:
        # System is relatively empty, encourage filling existing bins
        weight_fill = 0.7
        weight_best_fit = 0.3
    else:
        # System is relatively full, prioritize minimizing waste
        weight_fill = 0.3
        weight_best_fit = 0.7

    # Combine scores using adaptive weights
    combined_scores = (weight_best_fit * best_fit_scores) + (weight_fill * fill_scores)

    # Apply Softmax to convert scores into probabilities (priorities)
    # Handle -inf scores by ensuring they don't get exponentiated directly, or by using a numerically stable softmax.
    # A common practice is to shift scores so the minimum is 0.
    # Max score will be highest after shifting.
    max_score = np.max(combined_scores)
    # Replace -inf with a very small number to avoid issues with exp.
    # Or, shift by max_score directly, which will make all valid scores non-negative after exp.
    # If combined_scores contains -inf, np.exp will result in 0.
    shifted_scores = combined_scores - max_score

    # Ensure no NaNs or Infs in shifted_scores before exponentiation
    shifted_scores = np.nan_to_num(shifted_scores, nan=-np.inf, posinf=np.inf, neginf=-np.inf)

    # Use exp where scores are not -inf
    exp_values = np.zeros_like(shifted_scores)
    valid_mask = np.isfinite(shifted_scores)
    exp_values[valid_mask] = np.exp(shifted_scores[valid_mask])

    # Sum of exponentiated values for normalization. Ignore -inf cases which result in 0.
    sum_exp_values = np.sum(exp_values)

    # Calculate probabilities
    probabilities = np.zeros_like(combined_scores)
    if sum_exp_values > 1e-9: # Avoid division by zero if all valid scores were -inf or very small
        probabilities[valid_mask] = exp_values[valid_mask] / sum_exp_values

    # Ensure probabilities sum to 1 for the valid bins
    # The probabilities are currently for *all* bins, with 0 for invalid ones.
    # The softmax was calculated over the combined scores of all bins.
    # This means if a bin is invalid, its score remains -inf, exp(-inf) -> 0, so it correctly gets 0 probability.

    return probabilities
```
