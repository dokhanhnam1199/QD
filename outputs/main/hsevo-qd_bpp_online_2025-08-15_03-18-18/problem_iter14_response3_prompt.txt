{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Calculates priority scores for each bin using an adaptive Softmax-Based Fit\n    strategy for the online Bin Packing Problem, aiming for better adaptation\n    and robustness compared to v1.\n\n    This strategy prioritizes bins that can accommodate the item. It adapts\n    its scoring based on the \"tightness\" of the fit and the overall distribution\n    of available bin capacities. It uses a temperature parameter to control\n    exploration-exploitation.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the\n                         remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n\n    if not np.any(valid_bins_mask):\n        return np.zeros_like(bins_remain_cap, dtype=float)\n\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    # Calculate a \"fit score\". Lower remaining capacity after placement is better.\n    # We want to maximize a score related to this.\n    # Using the negative of the remaining capacity after placing the item.\n    # A smaller (more negative) score means a tighter fit.\n    fit_scores = -(valid_bins_remain_cap - item)\n\n    # Introduce a \"spread\" or \"diversity\" component.\n    # If all valid bins are very similar in remaining capacity, we might want\n    # to slightly favor bins that are not the absolute tightest, to avoid\n    # creating many bins that are *almost* full, which can be inefficient later.\n    # A simple way to capture this is to consider the variance or standard deviation\n    # of the remaining capacities among valid bins.\n    # If variance is low, we might want to slightly penalize the absolute best fit\n    # to encourage using other slightly less optimal but still valid bins.\n    \n    # Calculate the standard deviation of remaining capacities for valid bins.\n    std_dev_valid_bins = np.std(valid_bins_remain_cap)\n\n    # Create a diversity bonus. If std_dev is small, the bonus is larger.\n    # We want to add a small amount to the score to increase exploration.\n    # Normalize std_dev to be between 0 and 1 for better control.\n    # Max possible std_dev could be large, so we might want to cap it or use a\n    # robust measure. For simplicity, let's consider a relative measure.\n    # A simple approach: if std_dev is very small, add a small positive value.\n    # This encourages picking something other than the absolute best fit if\n    # many bins are almost identical.\n    diversity_bonus = np.exp(-std_dev_valid_bins * 5.0) * 0.1 # Tune multiplier\n\n    # Combine fit_scores with diversity_bonus.\n    # Add the diversity bonus to all valid bins. This slightly nudges\n    # away from the absolute greedy choice when options are similar.\n    adjusted_scores = fit_scores + diversity_bonus\n\n    # Adaptive temperature for Softmax.\n    # A higher temperature leads to a more uniform distribution (more exploration).\n    # A lower temperature leads to a more peaked distribution (more exploitation).\n    # We can set temperature based on how \"difficult\" the current situation is.\n    # For example, if many items are large relative to bin capacity, we might want\n    # more exploration. Or if there's a high variance in item sizes.\n    # A simple adaptive strategy: if there are many valid bins, or if valid bins\n    # have very diverse capacities (high std_dev), use a slightly higher temp.\n    \n    # A simple adaptive temperature: based on the number of valid bins and std_dev.\n    num_valid_bins = len(valid_bins_remain_cap)\n    # Base temperature, can be tuned.\n    base_temp = 1.0\n    \n    # Increase temperature if there are many options or if capacities are very spread out.\n    temp_multiplier = 1.0 + (num_valid_bins / 10.0) * (std_dev_valid_bins / np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 0)\n    temperature = base_temp * temp_multiplier\n    \n    # Ensure temperature is not zero to avoid division by zero or infinite softmax.\n    temperature = max(temperature, 0.1)\n\n    # Apply Softmax with adaptive temperature.\n    # Shift scores to prevent overflow/underflow before exponentiation.\n    max_score = np.max(adjusted_scores)\n    shifted_scores = (adjusted_scores - max_score) / temperature\n    \n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Create the final priority array, placing calculated priorities in their original positions.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Calculates priority scores for each bin using an adaptive strategy for the\n    online Bin Packing Problem. This version aims to balance fitting tightly\n    (Best Fit) with spreading items (Worst Fit) and incorporating an element\n    of exploration.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the\n                         remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # --- Core Strategy: Balancing Best Fit and Worst Fit ---\n    # Best Fit component: Prioritize bins that leave minimal remaining space.\n    # We want to minimize (remaining_capacity - item).\n    # A good score for BF would be proportional to -(remaining_capacity - item).\n    # For Softmax, we want higher scores for better options. So, let's use\n    # a score that increases as remaining_capacity - item decreases.\n    # A simple inversion: 1 / (remaining_capacity - item + epsilon)\n    # Or, to keep it related to the previous approach: maximize -(remaining_capacity - item)\n    # To promote diversification, let's also consider the \"emptiness\" of the bin.\n    # Worst Fit component: Prioritize bins with *more* remaining capacity.\n    # This encourages spreading items.\n    # A score for WF could be proportional to remaining_capacity.\n\n    # Let's create a blended score.\n    # For Best Fit: prioritize small remaining capacity after placing the item.\n    # For Worst Fit: prioritize large initial remaining capacity.\n    # We want to maximize the utility.\n    # Let's consider the utility as a function of remaining capacity:\n    # utility = alpha * (1 / (valid_bins_remain_cap - item + 1e-9)) + beta * valid_bins_remain_cap\n\n    # For simplicity and to adapt the softmax approach, let's define scores\n    # where higher means more desirable.\n    # High score for small (remaining_capacity - item) => Best Fit tendency\n    # High score for large remaining_capacity => Worst Fit tendency (for exploration/diversification)\n\n    # Let's try a score that is a combination:\n    # Score = (large_capacity_bonus) * (remaining_capacity) - (misfit_penalty) * (remaining_capacity - item)\n    # A simpler approach:\n    # Prioritize bins where remaining_capacity - item is small (BF)\n    # BUT, also give a boost to bins that are \"more open\" (WF) to avoid early convergence.\n\n    # Let's combine the ideas:\n    # We want to favor small (remaining_capacity - item).\n    # Let's define a score for \"tightness\": TightnessScore = -(valid_bins_remain_cap - item)\n    # And a score for \"openness\": OpennessScore = valid_bins_remain_cap\n\n    # We can create a combined score, for example, by averaging or taking a weighted sum.\n    # A more robust approach is to introduce a \"temperature\" or \"exploration factor\"\n    # that modulates the influence of the Best Fit vs. Worst Fit tendencies.\n\n    # Let's try a score that is a compromise. We want to minimize (remaining_capacity - item).\n    # Let's use a function that is high when (remaining_capacity - item) is small.\n    # Consider a function like: `exp(-k * (remaining_capacity - item))`\n    # `k` can be an exploration parameter. A large `k` makes it more like Best Fit.\n    # A small `k` makes it flatter, more exploratory.\n\n    # To balance exploration and exploitation, let's make the \"tightness\" score\n    # have an exploratory element.\n    # Let's use a score that is high for bins that are \"good\" fits, but also\n    # has some preference for bins that aren't *too* full if an exact fit isn't available.\n\n    # --- Adaptive Exploration/Exploitation ---\n    # We can adapt the strength of the \"Best Fit\" tendency based on the distribution of remaining capacities.\n    # If capacities are very diverse, lean more towards Best Fit.\n    # If capacities are very similar, lean more towards diversification.\n\n    # A simple adaptation: Use a parameter `epsilon` that smooths the selection.\n    # Larger epsilon makes it more uniform (exploratory). Smaller epsilon makes it more greedy (exploitative).\n    # We can define epsilon based on the variance of remaining capacities.\n    variance_remain_cap = np.var(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 1.0\n    # Scale variance to a reasonable epsilon range. Higher variance -> higher epsilon for more exploration.\n    epsilon = 0.1 + 0.5 * (1 / (1 + np.exp(-0.1 * variance_remain_cap))) # Sigmoid to bound epsilon\n\n    # Calculate scores:\n    # Score for \"good fit\" (lower remaining_capacity - item is better)\n    # Using a negative exponential for a sharp decrease in score as misfit increases.\n    # Adding epsilon for numerical stability and exploration.\n    goodness_of_fit_scores = np.exp(-10.0 * (valid_bins_remain_cap - item) / (item + 1e-9)) # Scale by item size\n\n    # Score for \"openness\" (higher remaining_capacity is better for spreading)\n    # Using a scaled exponential to give a significant boost to very open bins.\n    openness_scores = np.exp(0.1 * valid_bins_remain_cap / (np.max(bins_remain_cap) + 1e-9)) # Scale by max capacity\n\n    # Combine scores using epsilon for adaptive weighting\n    # When epsilon is high (diverse capacities), openness_scores have more weight.\n    # When epsilon is low (similar capacities), goodness_of_fit_scores have more weight.\n    combined_scores = (1 - epsilon) * goodness_of_fit_scores + epsilon * openness_scores\n\n    # Softmax transformation to get priorities\n    # Shift scores to prevent overflow/underflow before exponentiation\n    shifted_scores = combined_scores - np.max(combined_scores)\n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Create the final priority array\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1 and 5: Heuristic 1 uses `-(valid_bins_remain_cap - item)` as its base score, which directly implements the \"Best Fit\" principle by maximizing this value (minimizing waste). Heuristic 5 also uses this, but adds robustness checks for identical scores and zero sums, making it slightly more stable.\n\nComparing Heuristics 2 and 6: Both introduce an \"adaptive diversification\" component. Heuristic 2 uses variance to adjust the score, aiming to diversify when variance is low. Heuristic 6 refines this by adding a specific \"perfect fit bonus\" and a more complex adaptive diversification score tied to relative variance, making it more nuanced.\n\nComparing Heuristics 3 and 4: Heuristic 3 attempts to balance \"Best Fit\" with a tendency to favor less full bins based on overall bin capacity spread. It introduces a `spread_factor` and `relative_remaining_cap`. Heuristic 4 introduces an adaptive temperature parameter for Softmax, controlled by the number of valid bins and their standard deviation, along with a diversity bonus. Heuristic 4's adaptive temperature is a more sophisticated mechanism for controlling exploration/exploitation.\n\nComparing Heuristics 7 and 20: Heuristic 7 is identical to Heuristic 5. Heuristic 20 introduces a blend of \"Best Fit\" and \"Worst Fit\" tendencies, adapting the balance using an `epsilon` parameter derived from the variance of remaining capacities. This adaptive blending is a strong strategy for managing exploration vs. exploitation.\n\nComparing Heuristics 9-16 (repeated implementations of `1 / (potential_fits + epsilon)`) with Heuristics 1, 2, 3, 4, 6, 8, 20: The latter group uses more sophisticated scoring mechanisms, often involving combinations of factors (Best Fit, diversification, adaptive elements) and applying Softmax for probabilistic selection. Heuristics 9-16 are simple, potentially prone to extreme values and don't incorporate adaptive strategies or explicit diversification. Heuristic 17-19 use `np.exp(effective_capacities)` which would disproportionately favor bins with very large remaining capacities, not necessarily ideal for packing efficiency.\n\nOverall: Heuristics 1, 2, 3, 4, 6, 8, and 20 demonstrate a progression towards more complex, adaptive, and robust strategies. Heuristics 20, 4, and 6 appear to be the most advanced, incorporating adaptive parameters (temperature, epsilon) and multi-faceted scoring. Heuristics 1, 5, and 7 represent a solid \"Best Fit\" baseline with Softmax. Heuristics 9-16 are simplistic and less effective. Heuristics 17-19 have an unusual scoring mechanism.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Multi-objective, Adaptive, Probabilistic, Hybridization.\n*   **Advice:** Focus on heuristics that integrate multiple criteria (e.g., fit quality, diversity) and dynamically adjust their behavior based on problem instance characteristics. Leverage robust probabilistic selection methods like Softmax with careful parameterization.\n*   **Avoid:** Simplistic, static scoring functions, neglecting numerical stability, and overly complex, untuned adaptive mechanisms.\n*   **Explanation:** The goal is to create intelligent, flexible heuristics that balance exploration and exploitation by understanding and responding to the problem's evolving state, ensuring both efficiency and robustness.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}