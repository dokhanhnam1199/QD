```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Calculates priority scores for each bin using a hybrid strategy for the
    online Bin Packing Problem, balancing "best fit" with a "least wasted space"
    consideration and adaptive scaling.

    This strategy prioritizes bins that can fit the item. Among fitting bins,
    it favors those that result in minimal remaining capacity after packing
    (best fit). Additionally, it introduces a small bias towards bins that
    are already more full, encouraging better space utilization. The scaling
    is adaptive to avoid numerical issues.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A numpy array where each element represents the
                         remaining capacity of a bin.

    Returns:
        A numpy array of the same size as bins_remain_cap, where each element
        is the priority score for placing the item in the corresponding bin.
    """
    valid_bins_mask = bins_remain_cap >= item
    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]

    if valid_bins_remain_cap.size == 0:
        return np.zeros_like(bins_remain_cap, dtype=float)

    # Score 1: "Best Fit" - Minimize remaining capacity after placement
    # We want to maximize a score related to how full the bin becomes.
    # The remaining capacity after placing the item is (valid_bins_remain_cap - item).
    # Smaller values are better. Let's use the negative of this.
    best_fit_scores = -(valid_bins_remain_cap - item)

    # Score 2: "Least Wasted Space" / "Fullness Bias"
    # Favor bins that are already more full, but not so full that they can't fit.
    # This can be approximated by the inverse of the remaining capacity.
    # To avoid division by zero and to give a stronger preference to fuller bins,
    # we can use a term like 1 / (1 + remaining_capacity_before_packing).
    # However, we want to combine this with best_fit_scores.
    # A simpler approach is to consider the 'fullness ratio' before packing.
    # full_ratio = (bin_capacity - remaining_capacity) / bin_capacity
    # For simplicity here, let's use remaining capacity directly, favoring smaller values.
    # We can add this as a secondary score, scaled.
    # To make it compatible with best_fit_scores (which are negative), we also use negative.
    # Smaller remaining capacity is better.
    # Let's use a small epsilon to prevent division by zero if a bin had 0 remaining capacity
    # (though this should be filtered by valid_bins_mask if item > 0).
    epsilon = 1e-9
    fullness_scores = -valid_bins_remain_cap # Smaller remaining capacity is better

    # Combine scores. A simple linear combination can work.
    # The weight `w_fullness` can be tuned or made adaptive.
    # For now, let's give a slight preference to "Best Fit" and also reward fullness.
    # Let's try to combine them in a way that emphasizes the reduction in remaining capacity.
    # We want to maximize `-(remaining_after_fit)` and also `-(remaining_before_fit)`.
    # A weighted sum: `alpha * best_fit_scores + beta * fullness_scores`
    # A common approach is to use `(capacity - item)` as the penalty.
    # Let's consider `-(remaining_after_fit)` as primary and add a term
    # that encourages using bins that are not too empty.

    # Let's use a modified "Best Fit" score that also considers the original capacity.
    # A good candidate for a score to maximize is `(item - remaining_after_fit)`
    # which is `(item - (original_capacity - item))` which is `2*item - original_capacity`.
    # This is directly related to how much the bin is filled.
    # Alternatively, we can use `-(remaining_after_fit)` as before, but add a small boost
    # based on the *original* fullness of the bin.

    # Let's re-evaluate: we want to maximize fitness.
    # 1. Perfect Fit: If `valid_bins_remain_cap - item == 0`, this is ideal.
    # 2. Best Fit: Minimize `valid_bins_remain_cap - item`.
    # 3. Fullness Bias: Among bins with similar "best fit" scores, prefer those that were more full initially.

    # Let's try a score that directly relates to the *post-packing* state of the bin.
    # We want to maximize how "full" the bin becomes.
    # Score = (Bin Capacity - Remaining Capacity After Packing)
    # This is equivalent to `item` if `item` perfectly fills it.
    # It's `item - (remaining_capacity - item)` which is `2*item - remaining_capacity`.
    # This favors bins that were closer to `item` capacity initially.
    # This is the core of Best Fit.

    # To introduce the fullness bias, we can add a small bonus if the bin was already quite full.
    # Let `original_capacities` be the capacities of the valid bins.
    # We don't have original capacities here, only remaining.
    # So, "fullness" is inversely related to `valid_bins_remain_cap`.
    # A simple way to incorporate this is to add a small penalty for *large* remaining capacities.
    # So, we want to maximize `-(valid_bins_remain_cap - item) - lambda * valid_bins_remain_cap`.
    # This gives higher scores to bins that are almost full *after* packing,
    # and among those, prefer bins that were more full *before* packing (smaller `valid_bins_remain_cap`).

    # Let's try this composite score:
    # score = (item - (bin_rem_cap - item)) - lambda * bin_rem_cap
    # where `lambda` is a small positive constant (e.g., 0.01) to penalize remaining capacity.
    # This means we want to maximize: `2 * item - bin_rem_cap - lambda * bin_rem_cap`
    # or `2 * item - (1 + lambda) * bin_rem_cap`

    lambda_fullness = 0.05  # Tunable parameter to bias towards fuller bins
    scores = 2 * item - (1 + lambda_fullness) * valid_bins_remain_cap

    # Apply Softmax with adaptive scaling for numerical stability.
    # Subtracting the maximum score before exponentiation is standard.
    # We can also consider scaling by a factor related to the range of scores if needed,
    # but for now, standard shift is usually sufficient.

    # If all scores are the same (e.g., all bins are identical and fit the item),
    # then exp_scores will be uniform, and probabilities will be uniform.
    # This is desirable.
    if np.all(scores == scores[0]):
        probabilities = np.ones_like(scores) / scores.size
    else:
        max_score = np.max(scores)
        # Use a small multiplier to control the "sharpness" of the distribution.
        # A multiplier > 1 makes the distribution sharper (favors top scores more).
        # A multiplier < 1 makes it flatter. Let's keep it at 1 for now.
        sharpness_factor = 1.0
        shifted_scores = sharpness_factor * (scores - max_score)
        exp_scores = np.exp(shifted_scores)
        probabilities = exp_scores / np.sum(exp_scores)

    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    priorities[valid_bins_mask] = probabilities

    return priorities
```
