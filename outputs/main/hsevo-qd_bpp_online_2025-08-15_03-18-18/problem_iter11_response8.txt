```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines Best Fit with a "perfect fit" bonus and adaptive diversification.
    Prioritizes exact matches and uses adaptive scoring for other bins based on slack.
    """
    valid_bins_mask = bins_remain_cap >= item
    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]

    if valid_bins_remain_cap.size == 0:
        return np.zeros_like(bins_remain_cap)

    # Calculate remaining capacity after packing for valid bins
    rem_after_pack = valid_bins_remain_cap - item

    # Component 1: Perfect Fit Bonus
    # Assign a significantly higher score to bins where rem_after_pack is zero (or very close to zero)
    perfect_fit_bonus = np.zeros_like(valid_bins_remain_cap)
    perfect_fit_threshold = 1e-9
    is_perfect_fit = rem_after_pack < perfect_fit_threshold
    perfect_fit_bonus[is_perfect_fit] = 10.0  # High bonus for perfect fits

    # Component 2: Best Fit with Adaptive Slack Penalty
    # For non-perfect fits, score based on how tight the fit is.
    # A higher score for smaller remaining capacity after packing.
    # Use an exponential function for sensitivity, but scale it adaptively.
    # Calculate slack for non-perfect fits
    non_perfect_rem_after_pack = rem_after_pack[~is_perfect_fit]
    non_perfect_valid_bins_remain_cap = valid_bins_remain_cap[~is_perfect_fit]

    best_fit_scores = np.zeros_like(valid_bins_remain_cap)

    if non_perfect_rem_after_pack.size > 0:
        # Normalize remaining capacity to avoid large score variations and ensure stability
        # Use a small epsilon to prevent division by zero or near-zero slack
        slack_epsilon = 1e-6
        normalized_slack = non_perfect_rem_after_pack / (np.max(non_perfect_valid_bins_remain_cap) + slack_epsilon)

        # Adaptive penalty: The penalty for slack should be stronger when there's less slack.
        # We want to reward smaller `rem_after_pack`. Let's use `exp(-k * slack)`.
        # The scaling factor `k` can be adapted.
        # If slack is very diverse, we might want a less aggressive penalty to encourage exploration.
        # If slack is tight across many bins, we want to strongly favor the tightest.

        # Let's adapt `k` based on the variance of the slack of non-perfect fits.
        # Higher variance in slack suggests more diverse options, so we might slightly reduce `k`
        # to make the scores more uniform (more exploration).
        # Lower variance suggests options are similar, so we can increase `k` to pick the best.
        variance_slack = np.var(non_perfect_rem_after_pack)
        
        # Simple adaptive factor: higher variance -> smaller k (less penalty impact)
        # Clamp k to prevent extreme values
        adaptive_k = 5.0 * np.exp(-0.1 * variance_slack / (np.mean(non_perfect_rem_after_pack) + slack_epsilon))
        
        # Calculate scores for non-perfect fits: higher score for smaller slack
        bf_scores_non_perfect = np.exp(-adaptive_k * normalized_slack)
        best_fit_scores[~is_perfect_fit] = bf_scores_non_perfect

    # Combine perfect fit bonus with best fit scores
    combined_scores_valid = best_fit_scores + perfect_fit_bonus

    # Apply Softmax to get probability distribution
    if combined_scores_valid.size > 0:
        shifted_scores = combined_scores_valid - np.max(combined_scores_valid)
        exp_scores = np.exp(shifted_scores)
        probabilities = exp_scores / np.sum(exp_scores)
    else:
        probabilities = np.array([])

    # Map probabilities back to the original bins array
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    priorities[valid_bins_mask] = probabilities

    return priorities
```
