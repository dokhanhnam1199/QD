```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Calculates priority scores for each bin using a Multi-Objective Softmax strategy
    for the online Bin Packing Problem.

    This strategy prioritizes bins based on two objectives:
    1.  **Best Fit:** Minimizing wasted space after packing the item.
    2.  **Diversity/Spread:** Encouraging the use of less-filled bins to avoid
        creating too many nearly full bins early on.

    The scores from these objectives are combined, and then Softmax is applied
    to generate probabilistic priorities.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A numpy array where each element represents the
                         remaining capacity of a bin.

    Returns:
        A numpy array of the same size as bins_remain_cap, where each element
        is the priority score for placing the item in the corresponding bin.
    """
    valid_bins_mask = bins_remain_cap >= item
    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]
    original_indices = np.where(valid_bins_mask)[0]

    if valid_bins_remain_cap.size == 0:
        return np.zeros_like(bins_remain_cap)

    # Objective 1: Best Fit (minimize remaining capacity after placing the item)
    # We want to maximize a score that is inversely related to remaining capacity.
    # A common approach is to use the negative of the remaining capacity.
    # For numerical stability and Softmax, we can use -(remaining_capacity - item).
    best_fit_scores = -(valid_bins_remain_cap - item)

    # Objective 2: Diversity/Spread (prefer bins with more remaining capacity,
    # but not too much so that it's still a reasonable fit)
    # This encourages using bins that have some space left but aren't overly empty.
    # A simple approach is to use the remaining capacity itself.
    # Higher remaining capacity here is better for spread, but we'll combine it.
    diversity_scores = valid_bins_remain_cap

    # Combine objectives with weights. These weights can be learned or tuned.
    # For simplicity, let's use equal weighting here.
    # We need to ensure scores are in a comparable range for Softmax.
    # Normalizing or scaling could be done here. Let's normalize them.

    # Normalize best_fit_scores: Shift to be non-negative and then scale.
    if np.max(best_fit_scores) > np.min(best_fit_scores):
        normalized_best_fit = (best_fit_scores - np.min(best_fit_scores)) / (np.max(best_fit_scores) - np.min(best_fit_scores))
    else:
        normalized_best_fit = np.zeros_like(best_fit_scores) # All scores are the same

    # Normalize diversity_scores: Shift to be non-negative and then scale.
    if np.max(diversity_scores) > np.min(diversity_scores):
        normalized_diversity = (diversity_scores - np.min(diversity_scores)) / (np.max(diversity_scores) - np.min(diversity_scores))
    else:
        normalized_diversity = np.zeros_like(diversity_scores) # All scores are the same

    # Weighted combination of normalized scores.
    # Let's give a slight preference to best fit.
    weight_best_fit = 0.6
    weight_diversity = 0.4
    combined_scores = weight_best_fit * normalized_best_fit + weight_diversity * normalized_diversity

    # Apply Softmax to convert combined scores into probabilities (priorities)
    # Shift scores to prevent numerical overflow/underflow with exp.
    if combined_scores.size > 0:
        shifted_scores = combined_scores - np.max(combined_scores)
        exp_scores = np.exp(shifted_scores)
        probabilities = exp_scores / np.sum(exp_scores)
    else:
        probabilities = np.array([])

    # Create the final priority array, placing calculated priorities in their original positions
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    if original_indices.size > 0:
        priorities[original_indices] = probabilities

    return priorities
```
