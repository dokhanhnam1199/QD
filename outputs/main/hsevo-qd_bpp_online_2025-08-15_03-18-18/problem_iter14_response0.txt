```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines Best Fit with adaptive diversification for online BPP.
    Prioritizes bins with minimal remaining space after packing,
    and diversifies selection when bins are similarly sized.
    """
    valid_bins_mask = bins_remain_cap >= item
    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]

    if valid_bins_remain_cap.size == 0:
        return np.zeros_like(bins_remain_cap)

    # Best Fit component: Maximize -(remaining_capacity - item)
    best_fit_scores = -(valid_bins_remain_cap - item)

    # Adaptive Diversification component: Use standard deviation of remaining capacities
    # If std dev is low, items are similar; encourage diversity.
    # If std dev is high, items are diverse; lean towards Best Fit.
    std_dev_bins = np.std(valid_bins_remain_cap)
    mean_bins = np.mean(valid_bins_remain_cap)
    
    # A simple way to adapt: penalize best_fit_scores slightly based on std_dev.
    # A common approach is to use a 'temperature' or scaling factor.
    # Let's make the 'temperature' inversely proportional to std_dev.
    # If std_dev is very small, temperature is high (more uniform probabilities).
    # If std_dev is very large, temperature is low (more concentrated on best fit).
    # Add a small constant to avoid division by zero if std_dev is 0.
    temperature = 1.0 + (1.0 / (std_dev_bins + 1e-6))
    
    # Scale scores by temperature (higher temperature flattens probabilities)
    scaled_scores = best_fit_scores / temperature

    # Softmax to get probabilities
    shifted_scores = scaled_scores - np.max(scaled_scores)
    exp_scores = np.exp(shifted_scores)
    probabilities = exp_scores / np.sum(exp_scores)

    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    priorities[valid_bins_mask] = probabilities

    return priorities
```
