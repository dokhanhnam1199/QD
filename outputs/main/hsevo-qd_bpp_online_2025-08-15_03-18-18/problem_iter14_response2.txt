```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines Best Fit with adaptive diversification based on capacity spread.
    Prioritizes bins with minimal waste, but slightly favors more spacious bins
    when bin capacities are diverse.
    """
    valid_bins_mask = bins_remain_cap >= item
    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]

    if valid_bins_remain_cap.size == 0:
        return np.zeros_like(bins_remain_cap)

    # Core Best Fit: Minimize waste
    # Score is inversely related to remaining capacity after packing.
    # Maximizing -(waste) is equivalent to minimizing waste.
    waste = valid_bins_remain_cap - item
    best_fit_scores = -waste

    # Adaptive Diversification Component:
    # Encourage diversity when bin capacities are spread out.
    # Calculate the standard deviation of remaining capacities for all bins.
    # Normalize by the average capacity to make it scale-invariant.
    if bins_remain_cap.size > 1:
        std_dev_all_bins = np.std(bins_remain_cap)
        avg_cap_all_bins = np.mean(bins_remain_cap) + 1e-9 # Avoid division by zero
        spread_factor = std_dev_all_bins / avg_cap_all_bins
    else:
        spread_factor = 0.0 # No diversification benefit if only one bin

    # Adaptive score: Add a term that boosts bins with more remaining capacity,
    # scaled by the spread_factor. This makes spacious bins more attractive
    # when capacities are diverse.
    # Using (valid_bins_remain_cap / avg_valid_remain_cap) provides a relative measure.
    avg_valid_remain_cap = np.mean(valid_bins_remain_cap) + 1e-9
    relative_remaining_cap = valid_bins_remain_cap / avg_valid_remain_cap
    
    # The adaptive term is designed to be positive when relative_remaining_cap > 1
    # and scaled by spread_factor.
    adaptive_scores = spread_factor * (relative_remaining_cap - 1.0)

    # Combine scores: best_fit_scores aim for minimal waste, adaptive_scores for diversity.
    combined_scores = best_fit_scores + adaptive_scores

    # Use Softmax to convert scores into probabilities (priorities)
    # Shift scores to prevent numerical overflow/underflow in exp.
    # Handle potential non-finite values gracefully.
    if np.all(np.isfinite(combined_scores)):
        shifted_scores = combined_scores - np.max(combined_scores)
        exp_scores = np.exp(shifted_scores)
    else:
        # Fallback to best_fit_scores if combined_scores are not finite
        shifted_scores = best_fit_scores - np.max(best_fit_scores)
        exp_scores = np.exp(shifted_scores)

    # Ensure sum is not zero before normalization
    sum_exp_scores = np.sum(exp_scores)
    if sum_exp_scores == 0:
        probabilities = np.ones_like(exp_scores) / exp_scores.size if exp_scores.size > 0 else np.array([])
    else:
        probabilities = exp_scores / sum_exp_scores

    # Assign probabilities to the original bins array
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    priorities[valid_bins_mask] = probabilities

    return priorities
```
