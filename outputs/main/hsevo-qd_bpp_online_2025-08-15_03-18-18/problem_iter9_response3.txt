```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Calculates priority scores for each bin using an adaptive, probabilistic,
    and robust Softmax-Based Fit strategy for the online Bin Packing Problem.

    This strategy aims to balance fitting the item with minimal waste (Best Fit
    tendency) while also encouraging exploration of less-filled bins to maintain
    flexibility for future items. It incorporates adaptive scaling for
    Softmax to handle varying distributions of remaining capacities and to
    mitigate numerical instability.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A numpy array where each element represents the
                         remaining capacity of a bin.

    Returns:
        A numpy array of the same size as bins_remain_cap, where each element
        is the priority score for placing the item in the corresponding bin.
    """
    valid_bins_mask = bins_remain_cap >= item
    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]

    if valid_bins_remain_cap.size == 0:
        return np.zeros_like(bins_remain_cap, dtype=float)

    # Calculate a score that favors bins that result in less remaining capacity
    # after placing the item. Smaller remaining capacity is better for 'Best Fit'.
    # We use the negative of the remaining capacity to make 'good fits' have higher scores.
    fit_scores = -(valid_bins_remain_cap - item)

    # Introduce an exploration component: Slightly favor bins with more remaining capacity
    # beyond what's strictly needed for the current item. This encourages spreading items.
    # A small positive bonus can be added, scaled by the remaining capacity itself.
    # This bonus should be smaller than the fit_scores to prioritize good fits.
    exploration_bonus_factor = 0.1  # Hyperparameter to tune exploration
    exploration_scores = exploration_bonus_factor * valid_bins_remain_cap

    # Combine fit and exploration scores. Prioritize fit, but allow exploration to nudge.
    # The sum represents the overall "desirability" of a bin.
    combined_scores = fit_scores + exploration_scores

    # Adaptive scaling for Softmax: Normalize scores to a stable range
    # to prevent extreme values in the exponentiation.
    # Shift scores by subtracting the mean and dividing by the standard deviation
    # of the combined scores. This makes the distribution centered around 0
    # with a standard deviation of 1, improving numerical stability.
    mean_score = np.mean(combined_scores)
    std_score = np.std(combined_scores)

    # Avoid division by zero if all scores are identical
    if std_score > 1e-9:
        scaled_scores = (combined_scores - mean_score) / std_score
    else:
        scaled_scores = combined_scores - mean_score # Simple centering if std is near zero

    # Apply Softmax to convert scaled scores into probabilities (priorities)
    # Ensure numerical stability by subtracting the maximum scaled score before exponentiation.
    shifted_scaled_scores = scaled_scores - np.max(scaled_scores)
    exp_scores = np.exp(shifted_scaled_scores)
    probabilities = exp_scores / np.sum(exp_scores)

    # Create the final priority array, placing calculated probabilities in their original positions
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    priorities[valid_bins_mask] = probabilities

    return priorities
```
