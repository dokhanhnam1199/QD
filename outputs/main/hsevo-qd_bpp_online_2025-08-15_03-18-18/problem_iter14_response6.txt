```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines Best Fit with adaptive diversification and a perfect fit bonus,
    using a temperature parameter for Softmax to control exploration.
    """
    valid_bins_mask = bins_remain_cap >= item
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    if not np.any(valid_bins_mask):
        return priorities

    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]

    # Best Fit Component: Smaller difference (slack) is better.
    slack = valid_bins_remain_cap - item
    bf_scores = -slack

    # Perfect Fit Bonus: Strongly reward bins with zero slack.
    perfect_fit_bonus = np.where(slack == 0, 5.0, 0.0)

    # Adaptive Diversification Component: Encourage exploration of emptier bins.
    # Reward larger capacities when variance is high, penalize when low.
    if len(valid_bins_remain_cap) > 1:
        variance_capacity = np.var(valid_bins_remain_cap)
        mean_cap = np.mean(valid_bins_remain_cap)
        normalized_variance = (variance_capacity / (mean_cap**2)) if mean_cap > 0 else 0
        
        # Simple linear relationship: more variance, more reward for larger capacities.
        adaptive_div_scores = normalized_variance * valid_bins_remain_cap 
    else:
        adaptive_div_scores = np.zeros_like(valid_bins_remain_cap)

    # Combine components
    w_bf = 1.0
    w_perfect_fit = 1.0
    w_adapt_div = 0.2 # Reduced weight for diversification

    combined_scores = w_bf * bf_scores + w_perfect_fit * perfect_fit_bonus + w_adapt_div * adaptive_div_scores

    # Adaptive Temperature for Softmax
    # Lower temperature for more exploitation (when variance is low or few options)
    # Higher temperature for more exploration (when variance is high or many options)
    if len(valid_bins_remain_cap) > 1:
        std_dev_cap = np.std(valid_bins_remain_cap)
        # Heuristic: temperature inversely related to std dev, and directly to number of bins
        # Add a base temperature to avoid extreme probabilities.
        temperature = 1.0 + (0.5 * len(valid_bins_remain_cap)) / (std_dev_cap + 1e-6) 
    else:
        temperature = 1.0 # Default temperature

    # Apply Softmax with adaptive temperature
    if combined_scores.size > 0:
        shifted_scores = combined_scores - np.max(combined_scores)
        exp_scores = np.exp(shifted_scores / temperature)
        probabilities = exp_scores / np.sum(exp_scores)
    else:
        probabilities = np.array([])

    priorities[valid_bins_mask] = probabilities

    return priorities
```
