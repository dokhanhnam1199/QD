{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            priorities[i] = 1.0 / (bins_remain_cap[i] - item + 1e-9)\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    valid_bins = bins_remain_cap >= item\n    \n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n    \n    effective_capacities = bins_remain_cap[valid_bins] - item\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bins] = np.exp(effective_capacities)\n    \n    if np.sum(priorities) == 0:\n        return np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n        \n    return priorities / np.sum(priorities)\n\n### Analyze & experience\n- Comparing Heuristics 1-3 (Softmax-based) with Heuristics 4-12 (Inverse proximity): The primary difference is the approach to score normalization. Heuristics 1-3 use Softmax, which converts scores into a probability distribution, ensuring that the sum of priorities is 1. This is generally a more robust approach for selection mechanisms that expect probabilities. Heuristics 4-12 use a simpler inverse proximity score, which can lead to very large or very small values and doesn't inherently normalize the probabilities.\n\nComparing Heuristics 1-3 with Heuristics 17-20 (Exponentiated effective capacities): Heuristics 17-20 use `np.exp(effective_capacities)` directly. While this also amplifies the preference for better fits, it doesn't normalize the output in the same way as Softmax. If all effective capacities are large and positive, the resulting priorities can become extremely large, potentially causing numerical issues. Softmax, by subtracting the max before exponentiation, mitigates this. The fallback to `np.ones_like(...) / len(...)` if the sum is zero is an interesting edge case handling in 17-20.\n\nComparing Heuristics 1-3 with Heuristics 13-16 (epsilon-weighted random + greedy): These heuristics introduce a random element, balancing exploration (random scores) with exploitation (greedy scores). The weighting factor `epsilon` controls this balance. This is a more sophisticated approach than purely greedy or purely Softmax-based methods, as it can help escape local optima and discover better packing configurations over time.\n\nComparing Heuristics 4-7 with Heuristics 8-12: Heuristics 8-12 introduce a special case for perfect fits (`remaining_cap == item`), assigning them a priority of 1.0. This is a sensible addition that directly rewards perfect utilization of bin space. Heuristics 4-7 and 9 also use `1.0 / (potential_fits + 1e-9)`, but they miss this explicit perfect-fit handling.\n\nComparing Heuristics 4-7, 9 with Heuristics 5-7, 9: These are identical. They represent a basic \"best fit\" heuristic where the priority is inversely proportional to the remaining space after placing the item. The addition of `1e-9` is a good practice to avoid division by zero.\n\nComparing Heuristics 1-3 and 17-20 with Heuristics 4-16: The Softmax-based (1-3) and exponential (17-20) approaches offer a more nuanced distribution of priorities compared to the simple inverse proximity (4-16). The Softmax approach in 1-3 is generally preferred over the raw exponential in 17-20 due to better numerical stability and explicit probability interpretation. The combination of greedy and random in 13-16 adds an element of exploration which can be beneficial.\n\nOverall: The Softmax-based approach (1-3) offers a good balance of exploitation and robustness. The hybrid greedy-random approach (13-16) is a strong contender for its exploration capability. The explicit perfect-fit handling (8-12) is a valuable refinement.\n- \nHere's a redefined self-reflection for designing better heuristics:\n\n*   **Keywords:** Exploration-Exploitation, Diversification, Intensification, Robustness, Efficiency.\n*   **Advice:** Focus on mechanisms that actively balance exploring novel solution spaces with exploiting promising areas. Design mechanisms to foster diversity within a search population.\n*   **Avoid:** Passive or purely greedy exploration, neglecting potential for premature convergence, and overlooking edge cases that break numerical stability.\n*   **Explanation:** Effective self-reflection should identify *why* a heuristic might fail (e.g., getting stuck in local optima) and proactively design mechanisms (like simulated annealing or adaptive random restarts) to mitigate these weaknesses, leading to more robust and efficient exploration.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}