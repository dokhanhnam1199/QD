```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Calculates priority scores for each bin using an adaptive Softmax-Based Fit strategy
    for the online Bin Packing Problem.

    This strategy aims to improve upon v1 by:
    1. Incorporating an adaptive scaling factor to balance exploration (trying less
       optimal bins) and exploitation (fitting into the best available bins).
    2. Handling near-perfect fits more explicitly to avoid unnecessary fragmentation.
    3. Using a more robust score calculation that considers the inverse of remaining
       capacity to better differentiate between bins, especially when items are small.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A numpy array where each element represents the
                         remaining capacity of a bin.

    Returns:
        A numpy array of the same size as bins_remain_cap, where each element
        is the priority score for placing the item in the corresponding bin.
    """
    valid_bins_mask = bins_remain_cap >= item
    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]

    if valid_bins_remain_cap.size == 0:
        return np.zeros_like(bins_remain_cap)

    # --- Adaptive Scoring Mechanism ---
    # We want to assign higher priorities to bins that are good fits.
    # A "good fit" means the remaining capacity after placing the item is small.
    # The value (bins_remain_cap - item) represents the slack. We want to minimize this slack.
    # Using 1 / (slack + epsilon) can work, but can be sensitive to small slacks.
    # A better approach is to use a function that is high for small slacks and decreases.
    # Consider the "waste" or "difference" from a perfect fit:
    # A perfect fit would have remaining_capacity == item.
    # Let's define a score related to the 'badness' of the fit.
    # A very small positive slack is good. A large slack is bad.
    # Score: - (remaining_capacity - item) - we want to maximize this, so smaller remaining is better.

    # To make it adaptive, let's introduce a "preference" or "temperature" parameter
    # that controls how much we favor the best fit vs. exploring other options.
    # A higher 'beta' will make the distribution sharper (more greedy),
    # a lower 'beta' will make it flatter (more exploratory).
    # We can make beta dependent on the item size or the distribution of bin capacities.
    # For simplicity, let's start with an inverse relationship to the average slack.
    # This encourages exploration when slacks are generally large, and exploitation when slacks are small.

    # Calculate slack for valid bins
    slacks = valid_bins_remain_cap - item

    # Calculate an adaptive beta.
    # If average slack is small, we want to be more greedy (higher beta).
    # If average slack is large, we want to explore more (lower beta).
    # We can use 1 / (mean_slack + small_constant) or a similar inverse relationship.
    mean_slack = np.mean(slacks)
    # Add a small epsilon to avoid division by zero if all slacks are zero
    epsilon = 1e-9
    # Scale beta to avoid extremely sharp or flat distributions from raw inverse.
    # A scaling factor can be tuned. Let's use a modest scaling.
    beta = 1.0 / (mean_slack + epsilon)
    beta = np.clip(beta, 0.1, 10.0) # Clamp beta to a reasonable range

    # Softmax preparation:
    # We want to maximize the "goodness" of fit. Goodness is inversely related to slack.
    # A very small slack is highly desirable. A slightly larger slack is less desirable.
    # A large slack is undesirable.
    # Let's use a transformed score that is high for small slacks.
    # Example: -slack, or 1/(slack + epsilon), or a sigmoid-like function of slack.
    # Using -slack directly as in v1 is a good start.
    # To make it more nuanced, consider a score that penalizes larger slacks more heavily.
    # A transformation like exp(-k * slack) or a polynomial might be considered,
    # but let's refine the base score and apply beta.

    # Base scores: favor bins with smaller slacks.
    # Negative slack means perfect or near-perfect fit.
    # Positive slack means some remaining capacity.
    # We want to maximize `goodness`.
    # A good measure of goodness is minimizing `slack`.
    # So, let's use `-slack` as a base score. Higher values are better.
    base_scores = -slacks

    # Handle near-perfect fits more explicitly:
    # If slack is very close to zero (e.g., < 1% of item size or a small absolute threshold),
    # give these bins a significant boost. This encourages using bins that are almost full.
    near_perfect_fit_threshold = 0.01 * item + epsilon
    near_perfect_mask = slacks < near_perfect_fit_threshold
    # Add a bonus to near-perfect fits. The magnitude of the bonus should be substantial
    # to dominate the beta-scaled scores but not so large to always pick them if they are few.
    bonus = 10.0
    base_scores[near_perfect_mask] += bonus

    # Apply the adaptive beta to the base scores for Softmax
    # The beta controls the "sharpness" of the probability distribution.
    # Higher beta means probabilities concentrate on bins with higher base scores.
    # Lower beta means probabilities are more spread out.
    scaled_scores = beta * base_scores

    # Apply Softmax
    # Shift scores to prevent overflow/underflow issues with exp()
    max_scaled_score = np.max(scaled_scores)
    exp_scores = np.exp(scaled_scores - max_scaled_score)
    sum_exp_scores = np.sum(exp_scores)

    if sum_exp_scores == 0:
        # This can happen if all scaled_scores are extremely negative.
        # In such a case, distribute probability evenly among valid bins.
        probabilities = np.ones_like(valid_bins_remain_cap) / len(valid_bins_remain_cap)
    else:
        probabilities = exp_scores / sum_exp_scores

    # Construct the final priority array
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    priorities[valid_bins_mask] = probabilities

    return priorities
```
