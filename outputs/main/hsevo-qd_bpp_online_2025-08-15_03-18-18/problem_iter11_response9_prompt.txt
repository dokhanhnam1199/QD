{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Calculates priority scores for each bin using a Hybrid Softmax-Based Fit strategy\n    for the online Bin Packing Problem.\n\n    This strategy aims to balance 'Best Fit' (minimizing waste) with a\n    'First Fit' tendency for larger items by introducing a penalty for\n    bins that would leave very little space after packing. It also incorporates\n    a diversification element by slightly favoring less full bins to avoid\n    prematurely filling a few bins.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the\n                         remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Base score: Invert remaining capacity after fitting. Lower remaining is better.\n    # This promotes 'Best Fit'.\n    best_fit_scores = -(valid_bins_remain_cap - item)\n\n    # Diversification/Exploration score:\n    # Favor bins that are not already too full. This encourages spreading items.\n    # A higher score for bins with more remaining capacity (but still fitting the item).\n    # We use a sigmoid-like function to bound this influence.\n    # The idea is to give a slight boost to bins that are not nearly full,\n    # especially if the item is large.\n    # We can use 1 / (1 + exp(-(capacity - item - threshold))) where threshold is some value\n    # or more simply, a transformation of the remaining capacity after packing.\n    # Let's consider the remaining capacity itself as a measure. Higher remaining capacity\n    # after packing is less preferred for 'Best Fit', but might be good for diversification.\n    # We can create a score that is inversely related to how \"full\" the bin becomes.\n    # A bin that becomes almost full (low remaining cap) is good for BF, bad for diversity.\n    # A bin that remains very open (high remaining cap) is bad for BF, good for diversity.\n\n    # Let's create a score that penalizes bins that will have very little remaining space.\n    # This is a form of \"near miss\" avoidance for the next items.\n    # A small remaining capacity after packing (e.g., < item/2) could be penalized.\n    # Let's map the remaining capacity after packing `rem_after_fit` to a score.\n    rem_after_fit = valid_bins_remain_cap - item\n    \n    # Soft penalty for small remaining capacities. If rem_after_fit is small, this score is high.\n    # We want to *reduce* the priority for bins that leave very little space.\n    # Use a sigmoid-like function that maps small positive values to a range close to 0,\n    # and larger values to a range close to 1.\n    # We want to penalize small `rem_after_fit`.\n    # So, a function that is high for small `rem_after_fit` and low for large `rem_after_fit`\n    # is needed, and this should be subtracted from the main score.\n    \n    # A simple penalty: penalize if remaining capacity is less than item/2.\n    # More nuanced: exponential decay of penalty as remaining capacity increases.\n    # Let's use a logistic function scaled to penalize small positive remaining capacities.\n    # f(x) = 1 / (1 + exp(-k * (x - x0)))\n    # We want a function that is high when rem_after_fit is low.\n    # So we can use: penalty = 1 / (1 + exp(k * rem_after_fit)) where k is positive.\n    # Or, more simply, we can use a score that increases with remaining capacity,\n    # but we want to *reduce* the priority of bins with low remaining capacity.\n    \n    # Let's refine the score:\n    # Primary goal: Minimize `rem_after_fit` (Best Fit). Score = -rem_after_fit.\n    # Secondary goal: Avoid making bins too full if possible (Diversification).\n    # This means, if multiple bins offer similar \"best fit\", prefer the one that\n    # was initially less full.\n    # This is tricky to encode directly in a simple priority score.\n\n    # Alternative approach: Combine Best Fit with a bonus for initial capacity.\n    # Score = -rem_after_fit + alpha * initial_capacity_of_bin\n    # However, we don't have initial capacity, only remaining.\n\n    # Let's try a score that is good for Best Fit, but has a \"decay\" for being too full.\n    # Consider the \"gap\" created: `valid_bins_remain_cap - item`.\n    # We want to minimize this gap.\n    # Let's add a term that slightly favors bins that are less full initially.\n    # We can use the *current* remaining capacity as a proxy for how full the bin is.\n    # Higher `valid_bins_remain_cap` means the bin is less full.\n    \n    # Let's try a composite score:\n    # Score = w1 * (- (valid_bins_remain_cap - item)) + w2 * (valid_bins_remain_cap)\n    # The first term is Best Fit. The second term favors less full bins.\n    # Let's normalize these to avoid one dominating the other.\n\n    # Normalization factor for Best Fit: The maximum possible \"goodness\" is 0 (perfect fit).\n    # The worst \"goodness\" is -(max_capacity - min_item_size).\n    # Let's normalize `-(valid_bins_remain_cap - item)` to be between 0 and 1 (roughly).\n    # Or, more simply, let's use the raw negative remaining capacity.\n\n    # Let's introduce a \"diversification bonus\" that is proportional to the remaining capacity\n    # of the bin *before* packing. This encourages using bins that have more space available,\n    # as long as they fit the item.\n    # `bonus = alpha * valid_bins_remain_cap` where alpha is a small positive weight.\n    # This bonus counters the Best Fit score if the remaining capacity is significantly large.\n\n    alpha = 0.1  # Weight for diversification bonus. Tune this parameter.\n    diversification_bonus = alpha * valid_bins_remain_cap\n    \n    # Combined score: Best Fit score + diversification bonus\n    # Higher score is better.\n    # We want to minimize `valid_bins_remain_cap - item`. So we want to maximize `-(valid_bins_remain_cap - item)`.\n    # Higher `valid_bins_remain_cap` is better for diversification.\n    \n    # So, we want to maximize `-(valid_bins_remain_cap - item) + alpha * valid_bins_remain_cap`\n    # This simplifies to `alpha * valid_bins_remain_cap - valid_bins_remain_cap + item`\n    # which is `(alpha - 1) * valid_bins_remain_cap + item`.\n    # This still favors larger `valid_bins_remain_cap` if `alpha < 1`, which is the case.\n    \n    # Let's reconsider the goal. We want bins that are *almost* full (good fit),\n    # but not *too* full such that the remaining space is almost unusable.\n    # This suggests a function that peaks for intermediate remaining capacities after packing.\n\n    # A refined approach:\n    # 1. Best Fit score: `-(valid_bins_remain_cap - item)`. Maximize this.\n    # 2. Penalty for \"too little\" remaining space: `exp(-beta * (valid_bins_remain_cap - item))`\n    #    where beta is a positive constant. This term is high for small remaining space,\n    #    and we want to penalize high values. So, we subtract this penalty.\n    \n    beta = 2.0  # Penalty factor for small remaining space.\n    # We want to penalize small positive remaining capacities.\n    # Let rem_cap_after_packing = valid_bins_remain_cap - item\n    # Penalty increases as rem_cap_after_packing approaches 0.\n    # A function like `exp(-beta * rem_cap_after_packing)` works.\n    # If rem_cap_after_packing = 0, penalty is 1. If rem_cap_after_packing is large, penalty approaches 0.\n    # So, we subtract this penalty.\n    \n    # Let's combine:\n    # Score = -(valid_bins_remain_cap - item) - penalty_factor * exp(-beta * (valid_bins_remain_cap - item))\n    # This aims to reward good fits, but slightly disincentivize fits that leave almost no space.\n\n    # Let's try a simpler form that is more directly interpretable with Softmax.\n    # We want to prioritize bins where `valid_bins_remain_cap - item` is small.\n    # Let's introduce a \"niceness\" score.\n    # `niceness = 1.0 / (1.0 + (valid_bins_remain_cap - item))` -- this is high for small remaining space.\n    # But Softmax needs scores that can be positive/negative.\n\n    # Let's go back to the composite score idea, but ensure Softmax handles it well.\n    # We want to prioritize bins with small `(remaining_capacity - item)`.\n    # We also want to slightly favor bins that have more overall capacity (less full).\n    # So, we want to maximize `-(remaining_capacity - item) + alpha * remaining_capacity`.\n    \n    # Let's re-evaluate the original v1's Softmax base: `-(valid_bins_remain_cap - item)`.\n    # This encourages Best Fit.\n    # To add diversification, we can add a term that is higher for bins that are less full.\n    # `current_remaining_capacity` is a proxy for \"less full\".\n    # So, `score = -(valid_bins_remain_cap - item) + gamma * valid_bins_remain_cap`.\n    # `gamma` is a small positive number.\n    \n    gamma = 0.2  # Weight for diversification (favoring less full bins).\n    \n    # The score is `-(valid_bins_remain_cap - item) + gamma * valid_bins_remain_cap`\n    # = `-valid_bins_remain_cap + item + gamma * valid_bins_remain_cap`\n    # = `(gamma - 1) * valid_bins_remain_cap + item`.\n    \n    # This score will be higher for larger `valid_bins_remain_cap` if `gamma < 1`,\n    # which is the intended effect of diversification.\n    # However, we also want to prioritize Best Fit.\n\n    # Let's normalize the contribution of each term to prevent one from dominating.\n    # Best Fit contribution: `-(valid_bins_remain_cap - item)`\n    # Diversification contribution: `valid_bins_remain_cap`\n\n    # Maximum possible best fit score: 0 (perfect fit). Minimum: -(max_cap - min_item).\n    # Maximum possible diversification contribution: max_capacity. Minimum: min_fitting_capacity.\n\n    # A common strategy is to use a weighted sum, and then apply softmax.\n    # Let's use the raw scores and rely on Softmax scaling.\n    \n    # Final proposed score for each valid bin:\n    # Score = w_bf * BestFitScore + w_div * DiversificationScore\n    # BestFitScore = -(remaining_capacity_after_packing) = -(valid_bins_remain_cap - item)\n    # DiversificationScore = current_remaining_capacity = valid_bins_remain_cap\n    \n    w_bf = 1.0\n    w_div = 0.3 # Tune this weight. Higher means more preference for less full bins.\n    \n    composite_scores = w_bf * (-(valid_bins_remain_cap - item)) + w_div * valid_bins_remain_cap\n\n    # Softmax transformation:\n    # Shift scores to avoid numerical instability (large positive/negative values)\n    # Subtracting the maximum score is standard.\n    if composite_scores.size > 0:\n        shifted_scores = composite_scores - np.max(composite_scores)\n        exp_scores = np.exp(shifted_scores)\n        probabilities = exp_scores / np.sum(exp_scores)\n    else:\n        probabilities = np.array([])\n\n    # Create the final priority array, placing calculated priorities in their original positions\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Calculates priority scores for each bin using an adaptive strategy for the\n    online Bin Packing Problem. This version aims to balance fitting tightly\n    (Best Fit) with spreading items (Worst Fit) and incorporating an element\n    of exploration.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the\n                         remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # --- Core Strategy: Balancing Best Fit and Worst Fit ---\n    # Best Fit component: Prioritize bins that leave minimal remaining space.\n    # We want to minimize (remaining_capacity - item).\n    # A good score for BF would be proportional to -(remaining_capacity - item).\n    # For Softmax, we want higher scores for better options. So, let's use\n    # a score that increases as remaining_capacity - item decreases.\n    # A simple inversion: 1 / (remaining_capacity - item + epsilon)\n    # Or, to keep it related to the previous approach: maximize -(remaining_capacity - item)\n    # To promote diversification, let's also consider the \"emptiness\" of the bin.\n    # Worst Fit component: Prioritize bins with *more* remaining capacity.\n    # This encourages spreading items.\n    # A score for WF could be proportional to remaining_capacity.\n\n    # Let's create a blended score.\n    # For Best Fit: prioritize small remaining capacity after placing the item.\n    # For Worst Fit: prioritize large initial remaining capacity.\n    # We want to maximize the utility.\n    # Let's consider the utility as a function of remaining capacity:\n    # utility = alpha * (1 / (valid_bins_remain_cap - item + 1e-9)) + beta * valid_bins_remain_cap\n\n    # For simplicity and to adapt the softmax approach, let's define scores\n    # where higher means more desirable.\n    # High score for small (remaining_capacity - item) => Best Fit tendency\n    # High score for large remaining_capacity => Worst Fit tendency (for exploration/diversification)\n\n    # Let's try a score that is a combination:\n    # Score = (large_capacity_bonus) * (remaining_capacity) - (misfit_penalty) * (remaining_capacity - item)\n    # A simpler approach:\n    # Prioritize bins where remaining_capacity - item is small (BF)\n    # BUT, also give a boost to bins that are \"more open\" (WF) to avoid early convergence.\n\n    # Let's combine the ideas:\n    # We want to favor small (remaining_capacity - item).\n    # Let's define a score for \"tightness\": TightnessScore = -(valid_bins_remain_cap - item)\n    # And a score for \"openness\": OpennessScore = valid_bins_remain_cap\n\n    # We can create a combined score, for example, by averaging or taking a weighted sum.\n    # A more robust approach is to introduce a \"temperature\" or \"exploration factor\"\n    # that modulates the influence of the Best Fit vs. Worst Fit tendencies.\n\n    # Let's try a score that is a compromise. We want to minimize (remaining_capacity - item).\n    # Let's use a function that is high when (remaining_capacity - item) is small.\n    # Consider a function like: `exp(-k * (remaining_capacity - item))`\n    # `k` can be an exploration parameter. A large `k` makes it more like Best Fit.\n    # A small `k` makes it flatter, more exploratory.\n\n    # To balance exploration and exploitation, let's make the \"tightness\" score\n    # have an exploratory element.\n    # Let's use a score that is high for bins that are \"good\" fits, but also\n    # has some preference for bins that aren't *too* full if an exact fit isn't available.\n\n    # --- Adaptive Exploration/Exploitation ---\n    # We can adapt the strength of the \"Best Fit\" tendency based on the distribution of remaining capacities.\n    # If capacities are very diverse, lean more towards Best Fit.\n    # If capacities are very similar, lean more towards diversification.\n\n    # A simple adaptation: Use a parameter `epsilon` that smooths the selection.\n    # Larger epsilon makes it more uniform (exploratory). Smaller epsilon makes it more greedy (exploitative).\n    # We can define epsilon based on the variance of remaining capacities.\n    variance_remain_cap = np.var(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 1.0\n    # Scale variance to a reasonable epsilon range. Higher variance -> higher epsilon for more exploration.\n    epsilon = 0.1 + 0.5 * (1 / (1 + np.exp(-0.1 * variance_remain_cap))) # Sigmoid to bound epsilon\n\n    # Calculate scores:\n    # Score for \"good fit\" (lower remaining_capacity - item is better)\n    # Using a negative exponential for a sharp decrease in score as misfit increases.\n    # Adding epsilon for numerical stability and exploration.\n    goodness_of_fit_scores = np.exp(-10.0 * (valid_bins_remain_cap - item) / (item + 1e-9)) # Scale by item size\n\n    # Score for \"openness\" (higher remaining_capacity is better for spreading)\n    # Using a scaled exponential to give a significant boost to very open bins.\n    openness_scores = np.exp(0.1 * valid_bins_remain_cap / (np.max(bins_remain_cap) + 1e-9)) # Scale by max capacity\n\n    # Combine scores using epsilon for adaptive weighting\n    # When epsilon is high (diverse capacities), openness_scores have more weight.\n    # When epsilon is low (similar capacities), goodness_of_fit_scores have more weight.\n    combined_scores = (1 - epsilon) * goodness_of_fit_scores + epsilon * openness_scores\n\n    # Softmax transformation to get priorities\n    # Shift scores to prevent overflow/underflow before exponentiation\n    shifted_scores = combined_scores - np.max(combined_scores)\n    exp_scores = np.exp(shifted_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Create the final priority array\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[valid_bins_mask] = probabilities\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1 and 2: Heuristic 2 introduces an \"adaptive diversification\" component based on the variance of remaining capacities. This is a more sophisticated attempt at balancing exploration and exploitation than Heuristic 1's fixed diversification bonus.\n\nComparing Heuristics 2 and 3: Heuristic 3 refines the adaptive strategy by making the \"beta\" parameter (controlling Softmax sharpness) adaptive to the mean slack. It also explicitly handles near-perfect fits with a bonus, which is a valuable addition for reducing fragmentation.\n\nComparing Heuristics 3 and 4: Heuristic 4 introduces a \"penalty for overly full bins\" and a stronger \"bonus for perfect fits\" using a scaled sigmoid. It's more complex than Heuristic 3's adaptive beta but might over-penalize tight fits.\n\nComparing Heuristics 4 and 5: Heuristic 5 is a basic \"Softmax-Based Fit\" and lacks the adaptive or penalty mechanisms of Heuristics 3 and 4. It's a good baseline but less sophisticated.\n\nComparing Heuristics 5 and 6: Heuristic 6 is very similar to Heuristic 5, essentially a cleaner implementation of Softmax-Based Fit with slightly better handling of edge cases (e.g., all scores being identical).\n\nComparing Heuristics 6 and 7: Heuristic 7 introduces a \"perfect fit bonus\" and uses \"inverse proximity\" for non-perfect fits. This is simpler than Heuristic 3's adaptive beta but explicitly rewards perfect fits, which is beneficial. However, its scaling might be less nuanced than Heuristic 3's adaptive beta.\n\nComparing Heuristics 7 and 19: Heuristic 19 attempts a more complex balance between \"Best Fit\" and \"Worst Fit\" using an adaptive `epsilon` based on variance. This is a sophisticated attempt at balancing exploration and exploitation. Heuristic 7's \"perfect fit bonus\" is more direct.\n\nComparing Heuristics 19 and 20: Heuristics 19 and 20 are identical.\n\nComparing Heuristics 2 and 10: Heuristic 10 combines \"Best Fit\" with a \"diversification bonus\" proportional to remaining capacity, aiming to balance fitting tightly and spreading items. This is a more direct approach to diversification than Heuristic 2's variance-based method.\n\nComparing Heuristics 10 and 11-15: Heuristics 11-15 are all very similar, implementing a basic \"inverse remaining capacity\" heuristic, often without Softmax normalization or sophisticated adaptive strategies. They are simple but potentially less robust than Softmax-based approaches.\n\nComparing Heuristics 11-15 and 16-18: Heuristics 16-18 use `np.exp(effective_capacities)` which is essentially a form of Softmax but without explicit negative scaling for Best Fit. This might prioritize larger remaining capacities too much.\n\nOverall: Heuristics that dynamically adjust parameters (like beta or epsilon) based on the current state (variance, slack) and explicitly handle \"perfect fits\" tend to be better. The complexity in Heuristics 3, 4, and 19/20 suggests a trend towards more adaptive and nuanced scoring. Simple inversions (Heuristics 11-15) are generally less effective.\n- \nHere's a redefined \"Current Self-Reflection\" focused on designing better heuristics:\n\n*   **Keywords:** Adaptive Parameters, Hybridization, Reward Structures, Numerical Stability.\n*   **Advice:** Design adaptive strategies that dynamically tune parameters based on problem characteristics (e.g., variance, resource slack) to balance exploitation and exploration. Integrate hybrid selection mechanisms that combine greedy \"best fit\" with probabilistic diversification.\n*   **Avoid:** Overly simple scoring, ignoring perfect fits, fragile parameter tuning, and neglecting numerical precision.\n*   **Explanation:** This approach emphasizes sophisticated, state-aware adaptation and balanced selection, drawing from robust techniques like Softmax and explicitly valuing efficient solutions (perfect fits) while ensuring computational reliability.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}