```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Calculates priority scores for each bin using an Adaptive Softmax-Based Fit
    strategy for the online Bin Packing Problem.

    This strategy prioritizes bins that can accommodate the item, favoring
    bins that result in minimal remaining capacity after placement (best fit).
    It adaptively adjusts the "temperature" parameter of the softmax function
    to balance exploration (considering more bins) and exploitation (strongly
    favoring the best fit). A small penalty is applied to bins that become
    perfectly full to slightly encourage spreading if multiple perfect fits exist.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A numpy array where each element represents the
                         remaining capacity of a bin.

    Returns:
        A numpy array of the same size as bins_remain_cap, where each element
        is the priority score for placing the item in the corresponding bin.
    """
    valid_bins_mask = bins_remain_cap >= item

    if not np.any(valid_bins_mask):
        return np.zeros_like(bins_remain_cap)

    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]

    # Core metric: minimize remaining capacity after fitting the item (Best Fit)
    # Higher score for bins with smaller (remaining_capacity - item)
    fit_scores = -(valid_bins_remain_cap - item)

    # Introduce a small penalty for perfect fits to slightly encourage exploration
    # This can help in scenarios where multiple bins are perfect fits.
    # A very small epsilon is used to avoid division by zero if fit_scores is zero.
    epsilon = 1e-9
    perfect_fit_penalty = np.where(np.abs(fit_scores) < epsilon, -0.01, 0)
    adjusted_scores = fit_scores + perfect_fit_penalty

    # Adaptive Temperature for Softmax
    # A higher temperature leads to a softer (more uniform) distribution.
    # A lower temperature leads to a sharper (more greedy) distribution.
    # We can dynamically set the temperature. A simple approach is to
    # relate it to the spread of the adjusted scores.
    score_std = np.std(adjusted_scores)
    # If scores are very close, a higher temperature might be useful.
    # If scores are diverse, a lower temperature might be better for exploitation.
    # Clamp temperature to avoid extreme values.
    temperature = np.clip(1.0 + 0.5 * (score_std / (np.mean(np.abs(adjusted_scores)) + epsilon)), 0.1, 5.0)

    # Apply Softmax with adaptive temperature
    # Ensure scores are shifted to prevent overflow/underflow with exp
    shifted_scores = adjusted_scores - np.max(adjusted_scores)
    exp_scores = np.exp(shifted_scores / temperature)
    probabilities = exp_scores / np.sum(exp_scores)

    # Map probabilities back to the original bin array
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    priorities[valid_bins_mask] = probabilities

    return priorities
```
