{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                tiny_constant: float = 3.872792425245462e-05,\n                exploration_base: float = 0.0588708005223268,\n                max_exploration: float = 0.22627485634479824,\n                almost_full_threshold: float = 0.0731501402904877,\n                almost_full_penalty: float = 0.16293657163363146,\n                small_item_bin_multiple: float = 1.7118039205991789,\n                small_item_reward: float = 0.7609806445823415,\n                sweet_spot_lower_base: float = 0.6152888971175634,\n                sweet_spot_lower_item_scale: float = 0.20731655571678453,\n                sweet_spot_upper_base: float = 0.8729752618835456,\n                sweet_spot_upper_item_scale: float = 0.17849120545902175,\n                sweet_spot_reward: float = 0.5265016816500563,\n                usage_penalty_factor: float = 0.059912547982452435) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + tiny_constant)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(max_exploration, exploration_base * num_feasible * item)  # Increased base, scale by item size\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Stronger and more nuanced. Target almost-full bins.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold #Slightly less aggressive here\n        priorities[feasible_bins][almost_full] *= almost_full_penalty  # Significant penalty for using almost-full bins.\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > small_item_bin_multiple * item, small_item_reward, 0) #Increased reward and slightly larger bin requirement\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size.\n        sweet_spot_lower = sweet_spot_lower_base - (item * sweet_spot_lower_item_scale) #Dynamic Lower Bound\n        sweet_spot_upper = sweet_spot_upper_base - (item * sweet_spot_upper_item_scale) #Dynamic Upper Bound\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += sweet_spot_reward #Increased the reward.\n\n        # Bin History: Penalize bins that have been filled recently more aggressively.\n        # This requires an external mechanism (not part of this function) to track bin usage.\n        # This is a placeholder:  You'd need to maintain 'bin_usage_history' externally.\n        # Example: bin_usage_history = np.zeros_like(bins_remain_cap) #Initialize to all zeros\n        # Higher values = recently used more.\n\n        #In a separate step, you would update this value: bin_usage_history[chosen_bin] += 1\n\n        #The following assumes that bin_usage_history exist in the scope.\n        try:\n            bin_usage_history #Test if the variable exists, exception otherwise\n            usage_penalty = bin_usage_history[feasible_bins] * usage_penalty_factor #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        except NameError:\n            pass #If it doesn't exist, continue without this feature.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 0.00001)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.3, 0.05 * num_feasible * item)  # Increased base, scale by item size\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Stronger and more nuanced. Target almost-full bins.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.07 #Slightly less aggressive here\n        priorities[feasible_bins][almost_full] *= 0.3  # Significant penalty for using almost-full bins.\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > 1.6 * item, 0.5, 0) #Increased reward and slightly larger bin requirement\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size.\n        sweet_spot_lower = 0.65 - (item * 0.25) #Dynamic Lower Bound\n        sweet_spot_upper = 0.85 - (item * 0.15) #Dynamic Upper Bound\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5 #Increased the reward.\n\n        # Bin History: Penalize bins that have been filled recently more aggressively.\n        # This requires an external mechanism (not part of this function) to track bin usage.\n        # This is a placeholder:  You'd need to maintain 'bin_usage_history' externally.\n        # Example: bin_usage_history = np.zeros_like(bins_remain_cap) #Initialize to all zeros\n        # Higher values = recently used more.\n\n        #In a separate step, you would update this value: bin_usage_history[chosen_bin] += 1\n\n        #The following assumes that bin_usage_history exist in the scope.\n        try:\n            bin_usage_history #Test if the variable exists, exception otherwise\n            usage_penalty = bin_usage_history[feasible_bins] * 0.05 #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        except NameError:\n            pass #If it doesn't exist, continue without this feature.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and fragmentation control.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive Stochasticity: less aggressive as bins fill.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.1, 0.01 * num_feasible)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.7 + 0.2 * item\n        priorities[feasible_bins][almost_full] *= penalty\n\n        # Reward for large capacity\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 1.5, 0.2, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n        # Dynamic sweet spot incentive\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.2\n        \n        priorities[feasible_bins] *= (1 + 0.05 * item)\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability.\n    Version 2: Implements a more robust exploration strategy based on bin diversity and better sweet spot definition.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).  Slightly more aggressive than v1\n        priorities[feasible_bins] = 1 / (waste + 0.00001)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity:  More exploration when bins are plentiful or bin capacities are diverse.\n        num_feasible = np.sum(feasible_bins)\n        capacity_std = np.std(bins_remain_cap[feasible_bins])\n        exploration_factor = min(0.3, 0.05 * num_feasible + 0.1 * capacity_std)  # Caps at 0.3, considers capacity diversity\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty:  Stronger and more nuanced. Target almost-full bins.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.05 #More aggressive\n        priorities[feasible_bins][almost_full] *= 0.1  # Significant penalty for using almost-full bins. More aggressive penalty.\n\n        # Rewarding larger bins for smaller items, but with diminishing returns.\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > 1.5 * item, 0.3 * np.exp(-item), 0) # Diminishing return based on item size.\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size and a bit of bin capacity\n        sweet_spot_lower = 0.6 - (item * 0.2) - (capacity_std * 0.02) #Dynamic Lower Bound - lower bound decreases as item sizes increase. + capacity\n        sweet_spot_upper = 0.8 - (item * 0.1) + (capacity_std * 0.02) #Dynamic Upper Bound - upper bound decreases as item sizes increase, less aggressively. + capacity\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5 #Increased the reward.\n\n        #Penalize bins that after adding the item will have very little space left.\n        very_small_remaining = bins_remain_cap[feasible_bins] - item < 0.1\n        priorities[feasible_bins][very_small_remaining] *= 0.3 #Significant penalty\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins based on a combination of best-fit, fragmentation avoidance,\n    item-size awareness, and bin-utilization targets.  This version incorporates\n    a learning rate to adapt the penalty for fragmentation, and adjusts the\n    exploration factor based on bin fullness and the number of feasible bins.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-Fit Priority: Inverted waste, with a small constant to prevent division by zero\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive Exploration: Adjust exploration based on feasibility and bin fullness.\n        num_feasible = np.sum(feasible_bins)\n        avg_bin_utilization = np.mean((1 - bins_remain_cap[feasible_bins])) if num_feasible > 0 else 0\n        exploration_factor = min(0.3, 0.05 * num_feasible * (1 - avg_bin_utilization))  # Increased Base, reduced max. Adjust based on fullness.\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Dynamic penalty based on remaining capacity ratio.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.05\n        penalty = 0.3\n\n        # Introduce a Learning Rate (simple exponentially weighted average)\n        # This is a placeholder. In a real-world scenario, this would be persisted\n        # and updated over multiple calls to the function.  It simulates feedback\n        # on past bin choices and their impact on fragmentation.  We'll assume\n        # that excessive fragmentation leads to a higher penalty.\n        global fragmentation_penalty_adjustment\n        if 'fragmentation_penalty_adjustment' not in globals():\n            fragmentation_penalty_adjustment = 0.0  # Initialize penalty\n\n        # Simulate fragmentation feedback (replace with real-world feedback).\n        # Here, we reduce the adjustment if the bin is almost full; this incentivizes\n        # using the bin (reducing the penality) as long as it does not lead to much waste.\n        # The adjustment is for the NEXT iteraction!\n        if np.any(almost_full):\n            fragmentation_penalty_adjustment = 0.9 * fragmentation_penalty_adjustment  # Decay\n        else:\n            fragmentation_penalty_adjustment = 0.9 * fragmentation_penalty_adjustment + 0.01  # Increase\n\n        penalty -= fragmentation_penalty_adjustment  # Apply the adjustment. Make penalty smaller if fragmentation is low.\n        penalty = max(0, penalty) #Ensure Penalty non-negative.\n        \n        priorities[feasible_bins][almost_full] *= (1 - penalty)\n\n        # Reward Larger Bins for Smaller Items\n        small_item_threshold = 0.3  # Items smaller than this are considered \"small\"\n        large_bin_threshold = 1.5 * item #Define \"large\" bin based on item size.\n        if item < small_item_threshold:\n            large_bin = bins_remain_cap[feasible_bins] > large_bin_threshold\n            priorities[feasible_bins][large_bin] += 0.5  # Stronger reward for small items in large bins\n        \n        #Dynamic \"Sweet Spot\" Incentive - refined\n        sweet_spot_lower = 0.7 - (item * 0.25) #More aggressive adjustment\n        sweet_spot_upper = 0.9 - (item * 0.15)  #Less aggressive adjustment\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization >= sweet_spot_lower) & (utilization <= sweet_spot_upper) #Use >= and <=\n        priorities[feasible_bins][sweet_spot] += 0.6  # Boost \"sweet spot\"\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins based on a combination of best-fit, fragmentation avoidance,\n    item-size awareness, and bin-utilization targets.  This version incorporates\n    a learning rate to adapt the penalty for fragmentation, and adjusts the\n    exploration factor based on bin fullness and the number of feasible bins.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-Fit Priority: Inverted waste, with a small constant to prevent division by zero\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive Exploration: Adjust exploration based on feasibility and bin fullness.\n        num_feasible = np.sum(feasible_bins)\n        avg_bin_utilization = np.mean((1 - bins_remain_cap[feasible_bins])) if num_feasible > 0 else 0\n        exploration_factor = min(0.3, 0.05 * num_feasible * (1 - avg_bin_utilization))  # Increased Base, reduced max. Adjust based on fullness.\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Dynamic penalty based on remaining capacity ratio.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.05\n        penalty = 0.3\n\n        # Introduce a Learning Rate (simple exponentially weighted average)\n        # This is a placeholder. In a real-world scenario, this would be persisted\n        # and updated over multiple calls to the function.  It simulates feedback\n        # on past bin choices and their impact on fragmentation.  We'll assume\n        # that excessive fragmentation leads to a higher penalty.\n        global fragmentation_penalty_adjustment\n        if 'fragmentation_penalty_adjustment' not in globals():\n            fragmentation_penalty_adjustment = 0.0  # Initialize penalty\n\n        # Simulate fragmentation feedback (replace with real-world feedback).\n        # Here, we reduce the adjustment if the bin is almost full; this incentivizes\n        # using the bin (reducing the penality) as long as it does not lead to much waste.\n        # The adjustment is for the NEXT iteraction!\n        if np.any(almost_full):\n            fragmentation_penalty_adjustment = 0.9 * fragmentation_penalty_adjustment  # Decay\n        else:\n            fragmentation_penalty_adjustment = 0.9 * fragmentation_penalty_adjustment + 0.01  # Increase\n\n        penalty -= fragmentation_penalty_adjustment  # Apply the adjustment. Make penalty smaller if fragmentation is low.\n        penalty = max(0, penalty) #Ensure Penalty non-negative.\n        \n        priorities[feasible_bins][almost_full] *= (1 - penalty)\n\n        # Reward Larger Bins for Smaller Items\n        small_item_threshold = 0.3  # Items smaller than this are considered \"small\"\n        large_bin_threshold = 1.5 * item #Define \"large\" bin based on item size.\n        if item < small_item_threshold:\n            large_bin = bins_remain_cap[feasible_bins] > large_bin_threshold\n            priorities[feasible_bins][large_bin] += 0.5  # Stronger reward for small items in large bins\n        \n        #Dynamic \"Sweet Spot\" Incentive - refined\n        sweet_spot_lower = 0.7 - (item * 0.25) #More aggressive adjustment\n        sweet_spot_upper = 0.9 - (item * 0.15)  #Less aggressive adjustment\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization >= sweet_spot_lower) & (utilization <= sweet_spot_upper) #Use >= and <=\n        priorities[feasible_bins][sweet_spot] += 0.6  # Boost \"sweet spot\"\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins based on a combination of best-fit, fragmentation avoidance,\n    item-size awareness, and bin-utilization targets.  This version incorporates\n    a learning rate to adapt the penalty for fragmentation, and adjusts the\n    exploration factor based on bin fullness and the number of feasible bins.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-Fit Priority: Inverted waste, with a small constant to prevent division by zero\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive Exploration: Adjust exploration based on feasibility and bin fullness.\n        num_feasible = np.sum(feasible_bins)\n        avg_bin_utilization = np.mean((1 - bins_remain_cap[feasible_bins])) if num_feasible > 0 else 0\n        exploration_factor = min(0.3, 0.05 * num_feasible * (1 - avg_bin_utilization))  # Increased Base, reduced max. Adjust based on fullness.\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Dynamic penalty based on remaining capacity ratio.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.05\n        penalty = 0.3\n\n        # Introduce a Learning Rate (simple exponentially weighted average)\n        # This is a placeholder. In a real-world scenario, this would be persisted\n        # and updated over multiple calls to the function.  It simulates feedback\n        # on past bin choices and their impact on fragmentation.  We'll assume\n        # that excessive fragmentation leads to a higher penalty.\n        global fragmentation_penalty_adjustment\n        if 'fragmentation_penalty_adjustment' not in globals():\n            fragmentation_penalty_adjustment = 0.0  # Initialize penalty\n\n        # Simulate fragmentation feedback (replace with real-world feedback).\n        # Here, we reduce the adjustment if the bin is almost full; this incentivizes\n        # using the bin (reducing the penality) as long as it does not lead to much waste.\n        # The adjustment is for the NEXT iteraction!\n        if np.any(almost_full):\n            fragmentation_penalty_adjustment = 0.9 * fragmentation_penalty_adjustment  # Decay\n        else:\n            fragmentation_penalty_adjustment = 0.9 * fragmentation_penalty_adjustment + 0.01  # Increase\n\n        penalty -= fragmentation_penalty_adjustment  # Apply the adjustment. Make penalty smaller if fragmentation is low.\n        penalty = max(0, penalty) #Ensure Penalty non-negative.\n        \n        priorities[feasible_bins][almost_full] *= (1 - penalty)\n\n        # Reward Larger Bins for Smaller Items\n        small_item_threshold = 0.3  # Items smaller than this are considered \"small\"\n        large_bin_threshold = 1.5 * item #Define \"large\" bin based on item size.\n        if item < small_item_threshold:\n            large_bin = bins_remain_cap[feasible_bins] > large_bin_threshold\n            priorities[feasible_bins][large_bin] += 0.5  # Stronger reward for small items in large bins\n        \n        #Dynamic \"Sweet Spot\" Incentive - refined\n        sweet_spot_lower = 0.7 - (item * 0.25) #More aggressive adjustment\n        sweet_spot_upper = 0.9 - (item * 0.15)  #Less aggressive adjustment\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization >= sweet_spot_lower) & (utilization <= sweet_spot_upper) #Use >= and <=\n        priorities[feasible_bins][sweet_spot] += 0.6  # Boost \"sweet spot\"\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit, adaptive stochasticity, fragmentation control, and sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        priorities[feasible_bins] = np.minimum(priorities[feasible_bins], 50)\n\n        # Adaptive stochasticity, based on item size and num of feasible bins\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 * (1 - item) / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n\n        # Fragmentation penalty. Bins with > 90% utilisation.\n        wasted_space_ratio = waste / 1.0 # binsize fixed at 1\n        almost_full = wasted_space_ratio < 0.1\n        priorities[feasible_bins][almost_full] *= 0.4\n\n        # Dynamic sweet spot incentive\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and dynamic sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization.\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive stochasticity: smaller items, more exploration.\n        exploration_factor = max(0, 0.1 - (item * 0.05))\n        num_feasible = np.sum(feasible_bins)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Sweet Spot Incentive\n        sweet_spot_lower = 0.7 - (item * 0.1)\n        sweet_spot_upper = 0.9 - (item * 0.05)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n        \n        # Fragmentation penalty: Apply a moderate penalty for small waste, only if other bins exist\n        small_waste = (waste < 0.1)\n        if np.sum(feasible_bins) > 1:\n            priorities[feasible_bins][small_waste] *= 0.5 #reduce priority\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and dynamic sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization.\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive stochasticity: smaller items, more exploration.\n        exploration_factor = max(0, 0.1 - (item * 0.05))\n        num_feasible = np.sum(feasible_bins)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Sweet Spot Incentive\n        sweet_spot_lower = 0.7 - (item * 0.1)\n        sweet_spot_upper = 0.9 - (item * 0.05)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n        \n        # Fragmentation penalty: Apply a moderate penalty for small waste, only if other bins exist\n        small_waste = (waste < 0.1)\n        if np.sum(feasible_bins) > 1:\n            priorities[feasible_bins][small_waste] *= 0.5 #reduce priority\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and dynamic sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization.\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive stochasticity: smaller items, more exploration.\n        exploration_factor = max(0, 0.1 - (item * 0.05))\n        num_feasible = np.sum(feasible_bins)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Sweet Spot Incentive\n        sweet_spot_lower = 0.7 - (item * 0.1)\n        sweet_spot_upper = 0.9 - (item * 0.05)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n        \n        # Fragmentation penalty: Apply a moderate penalty for small waste, only if other bins exist\n        small_waste = (waste < 0.1)\n        if np.sum(feasible_bins) > 1:\n            priorities[feasible_bins][small_waste] *= 0.5 #reduce priority\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit, adaptive stochasticity, fragmentation control, and sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        priorities[feasible_bins] = np.minimum(priorities[feasible_bins], 50)\n\n        # Adaptive stochasticity, based on item size and num of feasible bins\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 * (1 - item) / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n\n        # Fragmentation penalty. Bins with > 90% utilisation.\n        wasted_space_ratio = waste / 1.0 # binsize fixed at 1\n        almost_full = wasted_space_ratio < 0.1\n        priorities[feasible_bins][almost_full] *= 0.4\n\n        # Dynamic sweet spot incentive\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and dynamic sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization.\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive stochasticity: smaller items, more exploration.\n        exploration_factor = max(0, 0.1 - (item * 0.05))\n        num_feasible = np.sum(feasible_bins)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Sweet Spot Incentive\n        sweet_spot_lower = 0.7 - (item * 0.1)\n        sweet_spot_upper = 0.9 - (item * 0.05)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n        \n        # Fragmentation penalty: Apply a moderate penalty for small waste, only if other bins exist\n        small_waste = (waste < 0.1)\n        if np.sum(feasible_bins) > 1:\n            priorities[feasible_bins][small_waste] *= 0.5 #reduce priority\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\nbest_fit_epsilon = 1e-9\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive stochasticity, and dynamic fragmentation penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization with regularization.\n        priorities[feasible_bins] = 1 / (waste + best_fit_epsilon)\n\n        # Adaptive stochasticity based on number of feasible bins\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 / (num_feasible + 1e-6)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n\n        # Dynamic fragmentation penalty: Item-aware.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.05\n        penalty_factor = 0.3 + 0.2 * item  # Larger items, stronger penalty\n        priorities[feasible_bins][almost_full] *= (1 - min(penalty_factor, 0.5))\n\n        # \"Sweet spot\" reward: target bins near full capacity.\n        fill_ratio = (bins_remain_cap[feasible_bins] - waste) / bins_remain_cap[feasible_bins]\n        significantly_filled = fill_ratio > 0.5\n        priorities[feasible_bins][significantly_filled] += 0.2\n\n        # Reward for placing into larger bins, threshold adapts to item size.\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * (1.5 + 0.5 * item), 0.25, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\nbest_fit_epsilon = 1e-9\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive stochasticity, and dynamic fragmentation penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization with regularization.\n        priorities[feasible_bins] = 1 / (waste + best_fit_epsilon)\n\n        # Adaptive stochasticity based on number of feasible bins\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 / (num_feasible + 1e-6)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n\n        # Dynamic fragmentation penalty: Item-aware.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.05\n        penalty_factor = 0.3 + 0.2 * item  # Larger items, stronger penalty\n        priorities[feasible_bins][almost_full] *= (1 - min(penalty_factor, 0.5))\n\n        # \"Sweet spot\" reward: target bins near full capacity.\n        fill_ratio = (bins_remain_cap[feasible_bins] - waste) / bins_remain_cap[feasible_bins]\n        significantly_filled = fill_ratio > 0.5\n        priorities[feasible_bins][significantly_filled] += 0.2\n\n        # Reward for placing into larger bins, threshold adapts to item size.\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * (1.5 + 0.5 * item), 0.25, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\nbest_fit_epsilon = 1e-9\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive stochasticity, and dynamic fragmentation penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization with regularization.\n        priorities[feasible_bins] = 1 / (waste + best_fit_epsilon)\n\n        # Adaptive stochasticity based on number of feasible bins\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 / (num_feasible + 1e-6)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n\n        # Dynamic fragmentation penalty: Item-aware.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.05\n        penalty_factor = 0.3 + 0.2 * item  # Larger items, stronger penalty\n        priorities[feasible_bins][almost_full] *= (1 - min(penalty_factor, 0.5))\n\n        # \"Sweet spot\" reward: target bins near full capacity.\n        fill_ratio = (bins_remain_cap[feasible_bins] - waste) / bins_remain_cap[feasible_bins]\n        significantly_filled = fill_ratio > 0.5\n        priorities[feasible_bins][significantly_filled] += 0.2\n\n        # Reward for placing into larger bins, threshold adapts to item size.\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * (1.5 + 0.5 * item), 0.25, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit, adaptive stochasticity, dynamic fragmentation penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 0.0001)\n\n        # Adaptive stochasticity: less exploration when bins are fuller.\n        stochasticity_scale = 0.1 * (1 - np.mean(bins_remain_cap[feasible_bins]) if len(bins_remain_cap[feasible_bins]) > 0 else 0)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_scale\n\n        # Penalize almost full bins, scaled by item size.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.5 + 0.2 * item  # Larger items, higher penalty\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty) # Ensure not negative\n\n        # Large item reward: only if there's significantly more space.\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 2, 0.5, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit, adaptive stochasticity, dynamic fragmentation penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 0.0001)\n\n        # Adaptive stochasticity: less exploration when bins are fuller.\n        stochasticity_scale = 0.1 * (1 - np.mean(bins_remain_cap[feasible_bins]) if len(bins_remain_cap[feasible_bins]) > 0 else 0)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_scale\n\n        # Penalize almost full bins, scaled by item size.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.5 + 0.2 * item  # Larger items, higher penalty\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty) # Ensure not negative\n\n        # Large item reward: only if there's significantly more space.\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 2, 0.5, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration,\n    with improvements on fragmentation handling and bin selection based on item size.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste). Adding a small constant to avoid division by zero\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive Stochasticity: Exploration based on number of feasible bins and item size.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.3, 0.05 * num_feasible * (1 - item))  # Scale exploration by item size\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: More aggressive penalty for bins nearing full capacity.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1 #Slightly relaxed to encourage use.\n        priorities[feasible_bins][almost_full] *= 0.3  # Increased penalty to discourage further fragmentation.\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > 1.2 * item + 0.2, 0.5, 0) #Adjusted condition to favor larger bins even more.\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.3)  #Adjusted parameters for larger impact of item size\n        sweet_spot_upper = 0.9 - (item * 0.2)  #Also adjusted to accommodate possible waste.\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.6 #Increased incentive to maximize utilization of bins.\n\n        # Bonus for filling a bin completely (or near-completely)\n        almost_full_bin = waste < 0.05\n        priorities[feasible_bins][almost_full_bin] += 0.8  #Major reward for filling a bin.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration,\n    with improvements on fragmentation handling and bin selection based on item size.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste). Adding a small constant to avoid division by zero\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive Stochasticity: Exploration based on number of feasible bins and item size.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.3, 0.05 * num_feasible * (1 - item))  # Scale exploration by item size\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: More aggressive penalty for bins nearing full capacity.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1 #Slightly relaxed to encourage use.\n        priorities[feasible_bins][almost_full] *= 0.3  # Increased penalty to discourage further fragmentation.\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > 1.2 * item + 0.2, 0.5, 0) #Adjusted condition to favor larger bins even more.\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.3)  #Adjusted parameters for larger impact of item size\n        sweet_spot_upper = 0.9 - (item * 0.2)  #Also adjusted to accommodate possible waste.\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.6 #Increased incentive to maximize utilization of bins.\n\n        # Bonus for filling a bin completely (or near-completely)\n        almost_full_bin = waste < 0.05\n        priorities[feasible_bins][almost_full_bin] += 0.8  #Major reward for filling a bin.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}