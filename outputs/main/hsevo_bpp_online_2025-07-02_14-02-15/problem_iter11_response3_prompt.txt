{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes best-fit, stochasticity, fragmentation penalty, and bin fill.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 0.0001)\n\n        # Stochasticity (reduced with more feasible bins)\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 / (num_feasible + 1e-6)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_factor\n\n        # Fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.05\n        priorities[feasible_bins][almost_full] *= 0.3\n\n        # Reward significantly filled bins\n        fill_ratio = (bins_remain_cap[feasible_bins] - waste) / bins_remain_cap[feasible_bins]\n        significantly_filled = fill_ratio > 0.5\n        priorities[feasible_bins][significantly_filled] += 0.2\n\n        # Large item reward if sufficient capacity exists\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 2, 0.25, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes best-fit, adds moderate stochasticity, actively manages fragmentation,\n    dynamically adjusts based on item size, and encourages balanced bin utilization.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Prioritize based on inverse waste (best fit), with a slight smoothing factor\n        priorities[feasible_bins] = 1 / (waste + 0.001)\n        \n        # Moderate stochasticity for exploration, scaled by item size.  Smaller items get more stochasticity.\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * (0.05 / (item + 0.1))\n\n        # Aggressively penalize almost full bins to combat fragmentation, relative to item size.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < (0.1 + 0.05*item) #Dynamic threshold based on item size\n        priorities[feasible_bins][almost_full] *= 0.2  # Further reduce priority of almost full bins\n\n        # Reward filling up larger bins, but only if the item is reasonably large. This mitigates issues of tiny items over-filling large bins\n        if item > 0.2:\n            large_cap_reward = np.where(bins_remain_cap[feasible_bins] > (1 - item)*0.8, 0.4, 0)\n            priorities[feasible_bins] += large_cap_reward\n        \n        # Incentivize bins towards a target utilization range, adjusted dynamically\n        target_utilization_low = 0.6 - 0.1 * item  # Target lower for larger items\n        target_utilization_high = 0.8 - 0.05*item #Target higher for smaller items.\n        \n        utilization = (bins_remain_cap[feasible_bins] - waste) \n        utilization /= 1.0 #Assuming bin size is 1\n        sweet_spot = (utilization >= target_utilization_low) & (utilization <= target_utilization_high)\n        priorities[feasible_bins][sweet_spot] += 0.3  # Give a boost to bins in the sweet spot.\n\n        # Small Items to empty bins:\n        empty_ish = bins_remain_cap[feasible_bins] > 0.9\n        if item < 0.1:\n             priorities[feasible_bins][empty_ish] += 0.2\n\n    else:\n        priorities[:] = -np.inf # Assign negative infinity priority to infeasible bins\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), we see the code is exactly the same, indicating that performance differences are subtle and may depend on the specific problem instance or random seed. Comparing (1st) vs (20th), we see the first one has \"Adaptive Stochasticity\", \"Fragmentation Penalty\", \"Rewarding larger bins for smaller items\", and \"Dynamic \"Sweet Spot\" Incentive\" whereas the last one only has \"stochasticity\", \"Penalize almost full bins to prevent fragmentation\", and \"Incentivize bins in a \"sweet spot\" of utilization\".\n\nComparing (4th) vs (5th), the 4th heuristic uses a lot of constants while 5th heuristic uses hard code. Comparing (8th) vs (9th), the 8th has \"Adaptive stochasticity (reduce exploration as bins fill)\" and  \"Large capacity reward (adaptive threshold)\" whereas the 9th has \"Stochasticity (reduced with more feasible bins)\".\n\nComparing (15th) vs (16th), heuristic 15th has  \"Penalize almost full bins dynamically based on item size - stronger penalty for larger items\" and \"Dynamically incentivize larger bins, especially if item is large\" whereas 16th has \"Fragmentation penalty: Moderate, based on waste ratio.\" and \"Large item reward if bin has sufficient capacity.\". Comparing (19th) vs (20th), the codes are exactly the same.\n\nOverall: better heuristics tend to have more adaptive components, especially those that dynamically adjust parameters based on item size, remaining bin capacity, or the number of feasible bins. They also incorporate mechanisms to balance exploration and exploitation, with more sophisticated approaches to stochasticity. Fragmentation penalties and sweet spot incentives are common, but the way they are applied and tuned is crucial.\n- \nOkay, let's refine \"Current self-reflection\" to design better heuristics, while explicitly avoiding the pitfalls of \"Ineffective self-reflection.\"\n\nHere's a revised perspective on self-reflection to guide heuristic design:\n\n*   **Keywords:** Adaptive, Dynamic, Problem-Aware, Empirical Tuning, Balanced, Iterative Refinement.\n*   **Advice:** Focus on a core, functional heuristic. Adapt parameters based on *observable* problem characteristics. Prioritize gradual, empirically validated refinements over complex, speculative features.\n*   **Avoid:** Blindly combining factors without understanding their interactions, adding stochasticity without a clear purpose, and over-engineering with non-linear functions before establishing a solid foundation.\n*   **Explanation:** Start simple, understand your base heuristic's performance, and then introduce targeted adaptations. The goal is controlled, explainable improvements based on data, not a black box of complex interactions.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}