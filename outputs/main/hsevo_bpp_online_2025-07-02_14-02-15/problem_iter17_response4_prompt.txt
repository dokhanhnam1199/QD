{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\nbest_fit_epsilon = 1e-9\n\n    \"\"\"Combines best-fit, adaptive stochasticity, and dynamic fragmentation penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization with regularization.\n        priorities[feasible_bins] = 1 / (waste + best_fit_epsilon)\n\n        # Adaptive stochasticity based on number of feasible bins\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 / (num_feasible + 1e-6)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n\n        # Dynamic fragmentation penalty: Item-aware.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.05\n        penalty_factor = 0.3 + 0.2 * item  # Larger items, stronger penalty\n        priorities[feasible_bins][almost_full] *= (1 - min(penalty_factor, 0.5))\n\n        # \"Sweet spot\" reward: target bins near full capacity.\n        fill_ratio = (bins_remain_cap[feasible_bins] - waste) / bins_remain_cap[feasible_bins]\n        significantly_filled = fill_ratio > 0.5\n        priorities[feasible_bins][significantly_filled] += 0.2\n\n        # Reward for placing into larger bins, threshold adapts to item size.\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * (1.5 + 0.5 * item), 0.25, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes best-fit, adaptive stochasticity, dynamic fragmentation penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 0.0001)\n\n        # Adaptive stochasticity: less exploration when bins are fuller.\n        stochasticity_scale = 0.1 * (1 - np.mean(bins_remain_cap[feasible_bins]) if len(bins_remain_cap[feasible_bins]) > 0 else 0)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_scale\n\n        # Penalize almost full bins, scaled by item size.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.5 + 0.2 * item  # Larger items, higher penalty\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty) # Ensure not negative\n\n        # Large item reward: only if there's significantly more space.\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 2, 0.5, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n### Analyze & experience\n- *   **Comparing (1st) vs (20th):** The first heuristic employs a more sophisticated and configurable approach, utilizing numerous tunable parameters to fine-tune the bin selection process. It incorporates a bin usage history penalty (if available) and defines sweet spots with parameters. The 20th heuristic lacks the bin usage history and has simpler sweet spot definitions.\n\n*   **Comparing (2nd) vs (19th):** The 2nd heuristic is similar to the 19th, but has less aggressive bonuses and penalties.\n\n*   **Comparing (1st) vs (2nd):** The first heuristic introduces configurable parameters, bin usage history, refined exploration with `exploration_base`, `max_exploration`, `sweet_spot_lower_base`, `sweet_spot_lower_item_scale`, `sweet_spot_upper_base`, `sweet_spot_upper_item_scale`, `sweet_spot_reward`, `usage_penalty_factor` and `tiny_constant`. The second heuristic has simpler parameters.\n\n*   **Comparing (3rd) vs (4th):** The fourth heuristic incorporates bin diversity (capacity standard deviation) into the exploration factor and sweet spot definition, as well as penalizing bins that will have very little space left after adding the item. The third heuristic lacks these features.\n\n*   **Comparing (19th) vs (20th):** Both are virtually identical.\n\n*   **Comparing (17th) vs (18th):** Both are virtually identical.\n\n*   **Comparing (15th) vs (16th):** All are virtually identical.\n\n*   **Comparing (second worst) vs (worst):** They have very similar logic but the best-fit epsilon variable.\n\n*   **Overall:** Better heuristics incorporate more adaptive and nuanced mechanisms such as:\n\n    *   **Configurable Parameters:** Allow for fine-tuning of various aspects of the heuristic, like exploration rate, fragmentation penalties, and sweet spot definitions.\n    *   **Bin Usage History:** Penalizing recently used bins can promote a more balanced utilization of available bins.\n    *   **Dynamic Sweet Spots:** Adjusting the \"sweet spot\" range based on item size helps to optimize bin utilization.\n    *   **Bin Diversity:** Consider the standard deviation of bin capacities to influence exploration.\n    *   **Item-Aware Penalties/Rewards:** Scaling penalties and rewards based on item size can improve overall performance.\n- \nOkay, let's refine \"Current self-reflection\" for better heuristic design, steering clear of the pitfalls outlined in \"Ineffective self-reflection.\"\n\nHere's a breakdown to guide improvements:\n\n*   **Keywords:** Adaptive, iterative, empirical, balance, problem-aware.\n\n*   **Advice:** Begin with a simple, demonstrably effective heuristic core. Incrementally add complexity only when justified by empirical results. Prioritize problem-aware adaptations over excessive parameters.\n\n*   **Avoid:** Over-engineering, non-linear complexity without justification, fixed stochasticity, and premature optimization. Avoid adding features without understanding their isolated effects.\n\n*   **Explanation:** Focus on *why* an adaptation helps, not just *that* it does. Small, problem-aware adjustments are better than large, untuned ones. Build incrementally, validating each addition.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}