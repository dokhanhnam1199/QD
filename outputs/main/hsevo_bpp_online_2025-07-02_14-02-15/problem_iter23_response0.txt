```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Prioritizes best-fit with adaptive exploration.
    Balances bin utilization and prevents fragmentation."""

    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    feasible_bins = bins_remain_cap >= item

    if np.any(feasible_bins):
        waste = bins_remain_cap[feasible_bins] - item

        # Best-fit prioritization
        tiny_constant = 1e-06
        priorities[feasible_bins] = 1 / (waste + tiny_constant)

        # Adaptive Exploration: Item size and bin fill level
        num_feasible = np.sum(feasible_bins)
        exploration_base = 0.05
        max_exploration = 0.2
        exploration_factor = min(max_exploration, exploration_base * (1- bins_remain_cap[feasible_bins].mean()) * item )
        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor

        # Dynamic Sweet Spot Incentive
        sweet_spot_lower = 0.6 - (item * 0.2)
        sweet_spot_upper = 0.9 - (item * 0.1)
        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0
        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)
        priorities[feasible_bins][sweet_spot] += 0.5


        # Fragmentation Penalty: Target almost-full bins, tuned threshold and penalty
        almost_full_threshold = 0.1  #Slightly higher threshold.
        almost_full_penalty = 0.2 #Increase the penalty
        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]
        almost_full = wasted_space_ratio < almost_full_threshold
        priorities[feasible_bins][almost_full] *= (1-almost_full_penalty)  # Apply Penalty
    else:
        priorities[:] = -np.inf

    return priorities
```
