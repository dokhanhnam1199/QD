{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                tiny_constant: float = 3.872792425245462e-05,\n                exploration_base: float = 0.0588708005223268,\n                max_exploration: float = 0.22627485634479824,\n                almost_full_threshold: float = 0.0731501402904877,\n                almost_full_penalty: float = 0.16293657163363146,\n                small_item_bin_multiple: float = 1.7118039205991789,\n                small_item_reward: float = 0.7609806445823415,\n                sweet_spot_lower_base: float = 0.6152888971175634,\n                sweet_spot_lower_item_scale: float = 0.20731655571678453,\n                sweet_spot_upper_base: float = 0.8729752618835456,\n                sweet_spot_upper_item_scale: float = 0.17849120545902175,\n                sweet_spot_reward: float = 0.5265016816500563,\n                usage_penalty_factor: float = 0.059912547982452435) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + tiny_constant)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(max_exploration, exploration_base * num_feasible * item)  # Increased base, scale by item size\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Stronger and more nuanced. Target almost-full bins.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold #Slightly less aggressive here\n        priorities[feasible_bins][almost_full] *= almost_full_penalty  # Significant penalty for using almost-full bins.\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > small_item_bin_multiple * item, small_item_reward, 0) #Increased reward and slightly larger bin requirement\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size.\n        sweet_spot_lower = sweet_spot_lower_base - (item * sweet_spot_lower_item_scale) #Dynamic Lower Bound\n        sweet_spot_upper = sweet_spot_upper_base - (item * sweet_spot_upper_item_scale) #Dynamic Upper Bound\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += sweet_spot_reward #Increased the reward.\n\n        # Bin History: Penalize bins that have been filled recently more aggressively.\n        # This requires an external mechanism (not part of this function) to track bin usage.\n        # This is a placeholder:  You'd need to maintain 'bin_usage_history' externally.\n        # Example: bin_usage_history = np.zeros_like(bins_remain_cap) #Initialize to all zeros\n        # Higher values = recently used more.\n\n        #In a separate step, you would update this value: bin_usage_history[chosen_bin] += 1\n\n        #The following assumes that bin_usage_history exist in the scope.\n        try:\n            bin_usage_history #Test if the variable exists, exception otherwise\n            usage_penalty = bin_usage_history[feasible_bins] * usage_penalty_factor #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        except NameError:\n            pass #If it doesn't exist, continue without this feature.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                tiny_constant: float = 3.872792425245462e-05,\n                exploration_base: float = 0.0588708005223268,\n                max_exploration: float = 0.22627485634479824,\n                almost_full_threshold: float = 0.0731501402904877,\n                almost_full_penalty: float = 0.16293657163363146,\n                small_item_bin_multiple: float = 1.7118039205991789,\n                small_item_reward: float = 0.7609806445823415,\n                sweet_spot_lower_base: float = 0.6152888971175634,\n                sweet_spot_lower_item_scale: float = 0.20731655571678453,\n                sweet_spot_upper_base: float = 0.8729752618835456,\n                sweet_spot_upper_item_scale: float = 0.17849120545902175,\n                sweet_spot_reward: float = 0.5265016816500563,\n                usage_penalty_factor: float = 0.059912547982452435) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + tiny_constant)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(max_exploration, exploration_base * num_feasible * item)  # Increased base, scale by item size\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Stronger and more nuanced. Target almost-full bins.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold #Slightly less aggressive here\n        priorities[feasible_bins][almost_full] *= almost_full_penalty  # Significant penalty for using almost-full bins.\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > small_item_bin_multiple * item, small_item_reward, 0) #Increased reward and slightly larger bin requirement\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size.\n        sweet_spot_lower = sweet_spot_lower_base - (item * sweet_spot_lower_item_scale) #Dynamic Lower Bound\n        sweet_spot_upper = sweet_spot_upper_base - (item * sweet_spot_upper_item_scale) #Dynamic Upper Bound\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += sweet_spot_reward #Increased the reward.\n\n        # Bin History: Penalize bins that have been filled recently more aggressively.\n        # This requires an external mechanism (not part of this function) to track bin usage.\n        # This is a placeholder:  You'd need to maintain 'bin_usage_history' externally.\n        # Example: bin_usage_history = np.zeros_like(bins_remain_cap) #Initialize to all zeros\n        # Higher values = recently used more.\n\n        #In a separate step, you would update this value: bin_usage_history[chosen_bin] += 1\n\n        #The following assumes that bin_usage_history exist in the scope.\n        try:\n            bin_usage_history #Test if the variable exists, exception otherwise\n            usage_penalty = bin_usage_history[feasible_bins] * usage_penalty_factor #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        except NameError:\n            pass #If it doesn't exist, continue without this feature.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Hybrid heuristic: Best-fit core, adaptive exploration, sweet spot, and fragmentation control.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-Fit Core\n        priorities[feasible_bins] = 1 / (waste + 1e-5)\n\n        # Adaptive Exploration: Scaled by item size and feasible bins\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.2, 0.05 * num_feasible * item)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Sweet Spot Incentive: Dynamic range based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.2)\n        sweet_spot_upper = 0.9 - (item * 0.1)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n\n        # Fragmentation Penalty: Target almost-full bins\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.08\n        priorities[feasible_bins][almost_full] *= 0.7\n\n        # Reward larger bins based on item size.\n        large_cap_factor = 1.5\n        large_cap_reward = 0.3\n        large_cap_reward_values = np.where(bins_remain_cap[feasible_bins] > item * large_cap_factor, large_cap_reward, 0)\n        priorities[feasible_bins] += large_cap_reward_values\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 1e-9)  # Tiny constant to avoid division by zero\n\n        # Exploration: Add some randomness to explore different options.  Reduced exploration.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = 0.01 * item  # Further reduced and scale by item size\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Target almost-full bins.  Simplified logic.\n        almost_full_threshold = 0.1\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold\n        priorities[feasible_bins][almost_full] *= 0.5  # Significant penalty for using almost-full bins.\n\n        # Reward for placing smaller items into bins with larger remaining capacity. Simplified scaling.\n        if item < 0.2:  # Only apply for smaller items\n            large_bin_threshold = 0.7\n            large_bin = bins_remain_cap[feasible_bins] > large_bin_threshold\n            priorities[feasible_bins][large_bin] += 0.3  # Increased reward for large bins\n\n        # Sweet spot incentive, simplified.\n        sweet_spot_lower = 0.6 - (0.1 * item)\n        sweet_spot_upper = 0.9 - (0.05 * item)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.2\n\n        # Bin History: Penalize bins that have been filled recently more aggressively.\n        # This requires an external mechanism (not part of this function) to track bin usage.\n        # This is a placeholder:  You'd need to maintain 'bin_usage_history' externally.\n        # Example: bin_usage_history = np.zeros_like(bins_remain_cap) #Initialize to all zeros\n        # Higher values = recently used more.\n\n        #In a separate step, you would update this value: bin_usage_history[chosen_bin] += 1\n\n        #The following assumes that bin_usage_history exist in the scope.\n        try:\n            bin_usage_history #Test if the variable exists, exception otherwise\n            usage_penalty = bin_usage_history[feasible_bins] * 0.05 #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        except NameError:\n            pass #If it doesn't exist, continue without this feature.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Best-fit core, adaptive exploration, sweet spot, and fragmentation control.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-Fit Core\n        priorities[feasible_bins] = 1 / (waste + 1e-5)\n\n        # Adaptive Exploration: Scaled by item size and feasible bins\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.2, 0.05 * num_feasible * item)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Sweet Spot Incentive: Dynamic range based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.2)\n        sweet_spot_upper = 0.9 - (item * 0.1)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n\n        # Fragmentation Penalty: Target almost-full bins\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.08\n        priorities[feasible_bins][almost_full] *= 0.7\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Best-fit core, adaptive exploration, sweet spot, and fragmentation control.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-Fit Core\n        priorities[feasible_bins] = 1 / (waste + 1e-5)\n\n        # Adaptive Exploration: Scaled by item size and feasible bins\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.2, 0.05 * num_feasible * item)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Sweet Spot Incentive: Dynamic range based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.2)\n        sweet_spot_upper = 0.9 - (item * 0.1)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n\n        # Fragmentation Penalty: Target almost-full bins\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.08\n        priorities[feasible_bins][almost_full] *= 0.7\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_usage_history: np.ndarray = None) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 1e-6)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size.  Reduced scale to avoid over-exploration.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = 0.02 * min(1, num_feasible * item) # Clamped exploration and scaled by both item and num_feasible\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Targetting bins which are close to full AND small\n        almost_full_threshold = 0.1\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = (wasted_space_ratio < almost_full_threshold) & (bins_remain_cap[feasible_bins] < 0.5)  # More targeted. small and almost full\n        priorities[feasible_bins][almost_full] *= 0.2 # Significant penalty for using almost-full bins.\n\n        # Sweet Spot Incentive: Encourage utilization around 70-90%. Simplified and more robust.\n        utilization_lower = 0.7\n        utilization_upper = 0.9\n        utilization = (bins_remain_cap[feasible_bins] - waste)  # No need to divide by bin size if bin size == 1\n        sweet_spot = (utilization > utilization_lower) & (utilization < utilization_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4  # Flat reward\n\n        # Bin History: Penalize bins that have been filled recently.\n        if bin_usage_history is not None:\n            usage_penalty = bin_usage_history[feasible_bins] * 0.05 #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        \n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_usage_history: np.ndarray = None) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 1e-6)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size.  Reduced scale to avoid over-exploration.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = 0.02 * min(1, num_feasible * item) # Clamped exploration and scaled by both item and num_feasible\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Targetting bins which are close to full AND small\n        almost_full_threshold = 0.1\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = (wasted_space_ratio < almost_full_threshold) & (bins_remain_cap[feasible_bins] < 0.5)  # More targeted. small and almost full\n        priorities[feasible_bins][almost_full] *= 0.2 # Significant penalty for using almost-full bins.\n\n        # Sweet Spot Incentive: Encourage utilization around 70-90%. Simplified and more robust.\n        utilization_lower = 0.7\n        utilization_upper = 0.9\n        utilization = (bins_remain_cap[feasible_bins] - waste)  # No need to divide by bin size if bin size == 1\n        sweet_spot = (utilization > utilization_lower) & (utilization < utilization_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4  # Flat reward\n\n        # Bin History: Penalize bins that have been filled recently.\n        if bin_usage_history is not None:\n            usage_penalty = bin_usage_history[feasible_bins] * 0.05 #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        \n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_usage_history: np.ndarray = None) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 1e-6)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size.  Reduced scale to avoid over-exploration.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = 0.02 * min(1, num_feasible * item) # Clamped exploration and scaled by both item and num_feasible\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Targetting bins which are close to full AND small\n        almost_full_threshold = 0.1\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = (wasted_space_ratio < almost_full_threshold) & (bins_remain_cap[feasible_bins] < 0.5)  # More targeted. small and almost full\n        priorities[feasible_bins][almost_full] *= 0.2 # Significant penalty for using almost-full bins.\n\n        # Sweet Spot Incentive: Encourage utilization around 70-90%. Simplified and more robust.\n        utilization_lower = 0.7\n        utilization_upper = 0.9\n        utilization = (bins_remain_cap[feasible_bins] - waste)  # No need to divide by bin size if bin size == 1\n        sweet_spot = (utilization > utilization_lower) & (utilization < utilization_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4  # Flat reward\n\n        # Bin History: Penalize bins that have been filled recently.\n        if bin_usage_history is not None:\n            usage_penalty = bin_usage_history[feasible_bins] * 0.05 #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        \n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_usage_history: np.ndarray = None) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 1e-6)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size.  Reduced scale to avoid over-exploration.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = 0.02 * min(1, num_feasible * item) # Clamped exploration and scaled by both item and num_feasible\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Targetting bins which are close to full AND small\n        almost_full_threshold = 0.1\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = (wasted_space_ratio < almost_full_threshold) & (bins_remain_cap[feasible_bins] < 0.5)  # More targeted. small and almost full\n        priorities[feasible_bins][almost_full] *= 0.2 # Significant penalty for using almost-full bins.\n\n        # Sweet Spot Incentive: Encourage utilization around 70-90%. Simplified and more robust.\n        utilization_lower = 0.7\n        utilization_upper = 0.9\n        utilization = (bins_remain_cap[feasible_bins] - waste)  # No need to divide by bin size if bin size == 1\n        sweet_spot = (utilization > utilization_lower) & (utilization < utilization_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4  # Flat reward\n\n        # Bin History: Penalize bins that have been filled recently.\n        if bin_usage_history is not None:\n            usage_penalty = bin_usage_history[feasible_bins] * 0.05 #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        \n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_usage_history: np.ndarray = None) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 1e-6)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size.  Reduced scale to avoid over-exploration.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = 0.02 * min(1, num_feasible * item) # Clamped exploration and scaled by both item and num_feasible\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Targetting bins which are close to full AND small\n        almost_full_threshold = 0.1\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = (wasted_space_ratio < almost_full_threshold) & (bins_remain_cap[feasible_bins] < 0.5)  # More targeted. small and almost full\n        priorities[feasible_bins][almost_full] *= 0.2 # Significant penalty for using almost-full bins.\n\n        # Sweet Spot Incentive: Encourage utilization around 70-90%. Simplified and more robust.\n        utilization_lower = 0.7\n        utilization_upper = 0.9\n        utilization = (bins_remain_cap[feasible_bins] - waste)  # No need to divide by bin size if bin size == 1\n        sweet_spot = (utilization > utilization_lower) & (utilization < utilization_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4  # Flat reward\n\n        # Bin History: Penalize bins that have been filled recently.\n        if bin_usage_history is not None:\n            usage_penalty = bin_usage_history[feasible_bins] * 0.05 #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        \n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_usage_history: np.ndarray = None) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).  Slightly more aggressive\n        priorities[feasible_bins] = 1 / (waste + 1e-6)**0.7 # Reduced exponent\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size. More targeted\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = 0.02 * min(0.5, item * num_feasible**0.5) # Reduced base exploration\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Stronger and more nuanced. Target almost-full bins.\n        almost_full_threshold = 0.05  # Slightly tighter\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold\n        priorities[feasible_bins][almost_full] *= 0.2  # Significant penalty\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.9 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n\n        # Bin History: Penalize bins that have been filled recently more aggressively.\n        if bin_usage_history is not None: # Require the argument to be passed in\n            usage_penalty = bin_usage_history[feasible_bins] * 0.1  # Adjust scaling\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive exploration and sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        priorities[feasible_bins] = np.minimum(priorities[feasible_bins], 50)\n\n        # Adaptive exploration based on item size and num feasible bins\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 * (1 - item) / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n        \n        # Dynamic sweet spot incentive based on item size\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive exploration and fragmentation control.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 1e-9)\n\n        # Adaptive Exploration\n        relative_item_size = item / bins_remain_cap[feasible_bins]\n        exploration_factor = 0.1 * (1 - relative_item_size)\n        exploration_factor = np.clip(exploration_factor, 0.01, 0.2)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n\n        # Fragmentation Penalty (Capacity-Aware)\n        common_item_sizes = np.array([0.2, 0.3, 0.4])\n        remaining_capacity_after_packing = waste\n        fragmentation_penalty = np.zeros_like(remaining_capacity_after_packing)\n        for size in common_item_sizes:\n            fragmentation_penalty += np.exp(-np.abs(remaining_capacity_after_packing - size) / 0.05)\n        priorities[feasible_bins] -= 0.05 * fragmentation_penalty\n\n        # Sweet Spot Incentive\n        sweet_spot_lower = 0.6 - (item * 0.3)\n        sweet_spot_upper = 0.9 - (item * 0.2)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.6\n\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive exploration and fragmentation control.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 1e-9)\n\n        # Adaptive Exploration\n        relative_item_size = item / bins_remain_cap[feasible_bins]\n        exploration_factor = 0.1 * (1 - relative_item_size)\n        exploration_factor = np.clip(exploration_factor, 0.01, 0.2)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n\n        # Fragmentation Penalty (Capacity-Aware)\n        common_item_sizes = np.array([0.2, 0.3, 0.4])\n        remaining_capacity_after_packing = waste\n        fragmentation_penalty = np.zeros_like(remaining_capacity_after_packing)\n        for size in common_item_sizes:\n            fragmentation_penalty += np.exp(-np.abs(remaining_capacity_after_packing - size) / 0.05)\n        priorities[feasible_bins] -= 0.05 * fragmentation_penalty\n\n        # Sweet Spot Incentive\n        sweet_spot_lower = 0.6 - (item * 0.3)\n        sweet_spot_upper = 0.9 - (item * 0.2)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.6\n\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity, item-aware fragmentation, and large bin rewards.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 1e-6)\n\n        # Adaptive stochasticity\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_scale = 0.1 * (1 - np.mean(bins_remain_cap[feasible_bins]) if num_feasible > 0 else 0)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_scale\n\n        # Item-aware fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.3 + 0.2 * item\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty)\n\n        # Reward for large bins relative to item size\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 2, 0.25, 0)\n        priorities[feasible_bins] += large_cap_reward\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity, item-aware fragmentation, and large bin rewards.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 1e-6)\n\n        # Adaptive stochasticity\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_scale = 0.1 * (1 - np.mean(bins_remain_cap[feasible_bins]) if num_feasible > 0 else 0)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_scale\n\n        # Item-aware fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.3 + 0.2 * item\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty)\n\n        # Reward for large bins relative to item size\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 2, 0.25, 0)\n        priorities[feasible_bins] += large_cap_reward\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    V2: Simplifies and focuses on core components with refined parameters.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).  Slightly more aggressive.\n        tiny_constant = 1e-06  #Keep it very small\n        priorities[feasible_bins] = 1 / (waste + tiny_constant)\n\n        # Adaptive Stochasticity: Exploration based on remaining capacity and item size.\n        num_feasible = np.sum(feasible_bins)\n        exploration_base = 0.05\n        max_exploration = 0.2\n        exploration_factor = min(max_exploration, exploration_base * (1- bins_remain_cap[feasible_bins].mean()) * item ) #Exploration scales with item size and how full the bins are.\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Target almost-full bins, tuned threshold and penalty\n        almost_full_threshold = 0.1  #Slightly higher threshold.\n        almost_full_penalty = 0.2 #Increase the penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold\n        priorities[feasible_bins][almost_full] *= (1-almost_full_penalty)  # Apply Penalty\n\n        # Rewarding larger bins for smaller items - adjusted parameters\n        small_item_bin_multiple = 1.5 #Slightly less restrictive\n        small_item_reward = 0.5 #Reduce reward\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > small_item_bin_multiple * item, small_item_reward, 0)\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n        # Dynamic \"Sweet Spot\" Incentive: Simplified and more focused.\n        sweet_spot_lower_base = 0.6\n        sweet_spot_upper_base = 0.9\n        sweet_spot_reward = 0.3\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower_base) & (utilization < sweet_spot_upper_base)\n        priorities[feasible_bins][sweet_spot] += sweet_spot_reward\n\n        # Bin History:  Simplified penalty.\n        try:\n            bin_usage_history\n            usage_penalty_factor = 0.1 # Moderate penalty.\n            usage_penalty = bin_usage_history[feasible_bins] * usage_penalty_factor\n            priorities[feasible_bins] -= usage_penalty\n        except NameError:\n            pass\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit, adaptive exploration, and dynamic sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive Exploration\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.2, 0.05 * num_feasible * (1 - item))\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Dynamic Sweet Spot Incentive\n        sweet_spot_lower = 0.6 - (item * 0.2)\n        sweet_spot_upper = 0.9 - (item * 0.1)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive exploration and sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        priorities[feasible_bins] = 1 / (waste + 1e-9)\n\n        exploration_factor = 0.05 * item * np.sum(feasible_bins)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n        utilization = (bins_remain_cap[feasible_bins] - waste)\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.3\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}