{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability.  Version 2 focuses on simplified adaptivity\n    and more targeted fragmentation control.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Core: Prioritize best fit (minimize waste).  Simplified inverse waste calculation.\n        priorities[feasible_bins] = 1 / (waste + 0.0001)\n\n        # Adaptive Exploration: Scale exploration based on item size.  Larger items get less exploration.\n        exploration_factor = 0.1 * (1 - item)  # Reduced overall exploration and scaled by item size.\n        num_feasible = np.sum(feasible_bins)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Targeted Fragmentation Penalty: Only penalize bins that would become *very* full.\n        remaining_capacity_ratio = waste / 1.0 #Ratio to bin capacity, binsize fixed at 1.\n        almost_full = remaining_capacity_ratio < 0.1 #Bins with > 90% utilisation.\n        priorities[feasible_bins][almost_full] *= 0.5  # Increased penalty for almost-full bins. More direct penalty.\n\n        # Sweet Spot Incentive: A fixed sweet spot, but only applied if it improves utilization.\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > 0.6) & (utilization < 0.8)  # Fixed sweet spot.\n        improvement = utilization[sweet_spot] > item # Only add sweetspot, if it improves.\n        priorities[feasible_bins][sweet_spot] += 0.3 # Increased the reward.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes best-fit with adaptive penalties, dynamic exploration, and bin-aware adjustments.\n    Emphasizes bin utilization, prevents fragmentation, and balances exploration based on item size\n    and remaining bin capacities. Focuses on simplicity and targeted adaptation.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste). More pronounced best-fit.\n        priorities[feasible_bins] = 10 / (waste + 0.00001)  # Increased the impact of best-fit\n\n        # Adaptive Stochasticity: Exploration decreases with item size and low remaining capacities\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.1, 0.01 * num_feasible * (1 - item)) #Reduced overall exploration & item aware\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Target almost-full bins dynamically scaled.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < (0.05 + 0.02 * item)  #Dynamically increase threshold for larger items.\n        priorities[feasible_bins][almost_full] *= 0.1  # Increased penalty.\n\n        # Rewarding larger bins for smaller items. More focused reward.\n        large_bin_threshold = 1.2 + 0.1*item\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > large_bin_threshold, 0.3, 0) # Slightly reduced reward\n        priorities[feasible_bins] += small_item_large_bin_reward\n        \n        # Dynamic \"Sweet Spot\" Incentive: Adapted Sweet Spot.\n        sweet_spot_lower = 0.6 - (item * 0.15)  # Adjusted\n        sweet_spot_upper = 0.85 - (item * 0.05) # Adjusted\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.3 #Increased reward slightly\n\n        #Bin Awareness - Encourage filling bins with lower remaining capacity\n        bin_capacity_rank = np.argsort(bins_remain_cap[feasible_bins])\n        capacity_incentive = np.linspace(0,0.15, num = len(bin_capacity_rank)) #Slight linear incentive.\n\n        priorities[feasible_bins][bin_capacity_rank] += capacity_incentive\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), we see both are identical. This suggests the ranking process might not be perfectly accurate or there are other factors influencing the ranking beyond the code itself.\nComparing (3rd) vs (4th), the primary difference is that the 3rd version parameterizes most of the magic numbers, making the heuristic more configurable but also more complex. The 4th version has hardcoded values but introduces a dynamic \"sweet spot\" incentive, potentially improving performance.\nComparing (5th) vs (6th), these are identical, similar to the first two.\n\nComparing (1st) vs (5th), the best-fit prioritization in (5th) uses `10 / (waste + 0.0001)` while (1st) uses `1 / (waste + 0.00001)`.  The higher constant in the numerator in (5th) could lead to a stronger best-fit bias initially. Stochasticity in (5th) is scaled by `0.1 * np.mean(bins_remain_cap)`, which is adaptive to the bin fill levels, while (1st) scales with the number of feasible bins.  Fragmentation penalty in (5th) is a direct multiplication by 0.2, while (1st) has a more nuanced approach with `penalty_factor = 0.2 + item * 0.1`. The large item reward in (5th) uses a fixed threshold `item * 1.25`, while (1st) uses `1.5 * item` as the threshold to rewards.\n\nComparing (20th) vs (19th), they are identical.\nComparing (18th) vs (17th), these are identical.\n\nComparing (second worst) vs (worst), (20th) vs (19th) there is no difference\nOverall: The better heuristics tend to incorporate item-size awareness into various components like fragmentation penalty, exploration factor and sweet spot incentives. They use adaptive stochasticity, often based on the number of feasible bins or the mean remaining capacity. Introducing a \"sweet spot\" to prioritize bins with utilization in a specific range seems to be a beneficial strategy. Penalizing fragmentation dynamically based on the size of the item being placed is also frequently observed in the better heuristics. Simpler is better, avoid unnescessary paramaterization.\n- \nOkay, let's redefine \"Current Self-Reflection\" for designing better heuristics, focusing on actionable advice and avoiding the pitfalls of ineffective reflection.\n\nHere's a breakdown:\n\n*   **Keywords:** Adaptive components, empirical tuning, iterative refinement, problem state.\n*   **Advice:** Build heuristics incrementally, starting with a simple, working base. Prioritize adaptive mechanisms over static parameters. Use empirical testing to tune weights and thresholds.\n*   **Avoid:** Premature complexity, non-linear functions without justification, and excessive customization.\n*   **Explanation:** Focus on gradual refinement and data-driven decisions. Begin with basic adaptive strategies and build upon them, avoiding overly complex models initially.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}