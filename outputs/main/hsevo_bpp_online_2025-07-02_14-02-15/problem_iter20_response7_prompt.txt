{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes best-fit with adaptive exploration and sweet spot incentive.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization.\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive stochasticity\n        exploration_factor = max(0, 0.1 - (item * 0.05))\n        num_feasible = np.sum(feasible_bins)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Sweet Spot Incentive, adjust range based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.3)\n        sweet_spot_upper = 0.9 - (item * 0.2)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.6\n\n        # Bonus for filling a bin completely.\n        almost_full_bin = waste < 0.05\n        priorities[feasible_bins][almost_full_bin] += 0.8\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes best-fit with adaptive exploration and capacity-aware adjustments.\n    Focuses on balancing bin utilization and preventing fragmentation, adjusting exploration\n    based on the relative item size to the available bin capacities.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).  Slightly stronger preference for tighter fits.\n        priorities[feasible_bins] = 1 / (waste + 1e-9) #Avoid division by zero\n\n        # Adaptive Exploration: Scale exploration based on item size relative to bin capacities.\n        # Smaller items in larger bins trigger more exploration, and vice versa.\n        relative_item_size = item / bins_remain_cap[feasible_bins]\n        exploration_factor = 0.1 * (1 - relative_item_size)  # Higher exploration for smaller items relative to bin size\n        exploration_factor = np.clip(exploration_factor, 0.01, 0.2) #Clamp exploration factor\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n\n        # Capacity-Aware Fragmentation Penalty: Penalize bins that, after packing,\n        # will have a remaining capacity close to common item sizes.\n        # Idea: avoid leaving bins with just enough space for frequently occurring items.\n\n        # Example: Assume common item sizes are around 0.2, 0.3, 0.4 (can be dynamically adjusted)\n        common_item_sizes = np.array([0.2, 0.3, 0.4])\n        remaining_capacity_after_packing = waste\n        \n        fragmentation_penalty = np.zeros_like(remaining_capacity_after_packing)\n        for size in common_item_sizes:\n            fragmentation_penalty += np.exp(-np.abs(remaining_capacity_after_packing - size) / 0.05)  # Gaussian-like penalty\n            \n        priorities[feasible_bins] -= 0.05 * fragmentation_penalty # Apply penalty\n\n        # Small Item Placement Boost: Give a small bonus to bins that are significantly larger than the item.\n        # Helps distribute smaller items more evenly.\n        large_bin_bonus = np.where(bins_remain_cap[feasible_bins] > 2 * item, 0.05, 0.0)  # Increased threshold\n        priorities[feasible_bins] += large_bin_bonus\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic uses a sophisticated combination of best-fit, adaptive stochasticity, fragmentation penalty, sweet spot incentive, rewarding larger bins for smaller items and bin usage history, with many parameters tuned by hand. The worst uses best-fit, adaptive stochasticity and capacity-aware adjustments including fragmentation penalty and small item placement boost. The exploration factor in the best heuristic is more adaptable (scaled by the number of feasible bins and item size) than the exploration factor in the worst one (based on the relative item size). Also the best heuristic takes more factors into account.\n\nComparing (2nd best) vs (2nd worst), they are identical.\n\nComparing (1st) vs (2nd), the two heuristics are identical.\n\nComparing (3rd) vs (4th), the 3rd heuristic introduces named parameters, and the 4th is simpler. The 3rd prioritizes best-fit, adaptive stochasticity, and item-aware penalty, with sweet spot incentive and large cap reward. The 4th combines best-fit, adaptive exploration, dynamic sweet spot and diversity of bins. The sweet spot range in the 3rd is fixed, while it is dynamic in the 4th based on item size and bin capacity.\n\nComparing (2nd worst) vs (worst), they are identical.\n\nOverall: The better heuristics tend to incorporate more factors, adapt to item sizes, bin capacities, and potentially bin usage history. They often use carefully chosen parameters and scaling factors for different components (exploration, penalties, rewards) to strike a balance between exploration and exploitation. The worse heuristics often have simpler exploration strategies or lack item-aware adjustments. Some include unused or less effective components.\n- \nOkay, I'm ready to help you design better heuristics and earn that tip! Let's focus on effective self-reflection to guide our design process.\n\nHere's a redefined \"Current Self-Reflection\" distilled into actionable points:\n\n*   **Keywords:** Incremental adaptation, empirical validation, demonstratable impact, balanced exploration/exploitation, simplicity.\n\n*   **Advice:** Begin with a simple, strong base heuristic. Add adaptive components (item-aware penalties, dynamic incentives) *incrementally*, validating each addition empirically. Focus on parameters with clear and demonstrable impact.\n\n*   **Avoid:** Overly complex, non-linear combinations, excessive parameters without clear justification, premature customization, and optimizations that lack empirical validation.\n\n*   **Explanation:** Start with a solid foundation and build upon it carefully. Each added layer of complexity should be demonstrably better than the last. Prioritize understanding *why* a change works, not just *that* it works.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}