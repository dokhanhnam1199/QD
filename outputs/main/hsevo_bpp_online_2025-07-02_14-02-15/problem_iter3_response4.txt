```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """

    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Fit Score: Prioritize bins that can fit the item. Non-linear scaling.
    fit_mask = bins_remain_cap >= item
    waste = bins_remain_cap - item
    relative_waste = waste / item
    priorities[fit_mask] += np.exp(-relative_waste[fit_mask])  # Exponential decay of priority as waste increases

    # Capacity Utilization: Encourage filling bins close to full.
    utilization = (bins_remain_cap - item) / bins_remain_cap
    utilization = np.clip(utilization, 0, 1)  # Ensure utilization is between 0 and 1
    priorities[fit_mask] += (1-utilization[fit_mask])**2

    # Fragmentation Penalty: Discourage leaving small gaps.  Larger gaps penalized more.
    fragmentation_threshold = item * 0.2 #Define threshold relative to item size
    fragmentation_penalty = np.where((waste > 0) & (waste < fragmentation_threshold), -0.5, 0)
    priorities += fragmentation_penalty

    # Stochastic Element: Introduce randomness for exploration.
    exploration_factor = 0.01  # Low value to limit randomness
    priorities += np.random.rand(len(bins_remain_cap)) * exploration_factor


    # Large item Reward
    large_cap_reward = np.where(bins_remain_cap > item * 1.5, np.tanh(bins_remain_cap / item), 0)  # tanh to bound reward
    priorities += large_cap_reward

    return priorities
```
