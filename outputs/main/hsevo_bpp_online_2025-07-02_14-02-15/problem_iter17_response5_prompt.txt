{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritizes bins based on a combination of best-fit, fragmentation avoidance,\n    item-size awareness, and bin-utilization targets.  This version incorporates\n    a learning rate to adapt the penalty for fragmentation, and adjusts the\n    exploration factor based on bin fullness and the number of feasible bins.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-Fit Priority: Inverted waste, with a small constant to prevent division by zero\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive Exploration: Adjust exploration based on feasibility and bin fullness.\n        num_feasible = np.sum(feasible_bins)\n        avg_bin_utilization = np.mean((1 - bins_remain_cap[feasible_bins])) if num_feasible > 0 else 0\n        exploration_factor = min(0.3, 0.05 * num_feasible * (1 - avg_bin_utilization))  # Increased Base, reduced max. Adjust based on fullness.\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Dynamic penalty based on remaining capacity ratio.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.05\n        penalty = 0.3\n\n        # Introduce a Learning Rate (simple exponentially weighted average)\n        # This is a placeholder. In a real-world scenario, this would be persisted\n        # and updated over multiple calls to the function.  It simulates feedback\n        # on past bin choices and their impact on fragmentation.  We'll assume\n        # that excessive fragmentation leads to a higher penalty.\n        global fragmentation_penalty_adjustment\n        if 'fragmentation_penalty_adjustment' not in globals():\n            fragmentation_penalty_adjustment = 0.0  # Initialize penalty\n\n        # Simulate fragmentation feedback (replace with real-world feedback).\n        # Here, we reduce the adjustment if the bin is almost full; this incentivizes\n        # using the bin (reducing the penality) as long as it does not lead to much waste.\n        # The adjustment is for the NEXT iteraction!\n        if np.any(almost_full):\n            fragmentation_penalty_adjustment = 0.9 * fragmentation_penalty_adjustment  # Decay\n        else:\n            fragmentation_penalty_adjustment = 0.9 * fragmentation_penalty_adjustment + 0.01  # Increase\n\n        penalty -= fragmentation_penalty_adjustment  # Apply the adjustment. Make penalty smaller if fragmentation is low.\n        penalty = max(0, penalty) #Ensure Penalty non-negative.\n        \n        priorities[feasible_bins][almost_full] *= (1 - penalty)\n\n        # Reward Larger Bins for Smaller Items\n        small_item_threshold = 0.3  # Items smaller than this are considered \"small\"\n        large_bin_threshold = 1.5 * item #Define \"large\" bin based on item size.\n        if item < small_item_threshold:\n            large_bin = bins_remain_cap[feasible_bins] > large_bin_threshold\n            priorities[feasible_bins][large_bin] += 0.5  # Stronger reward for small items in large bins\n        \n        #Dynamic \"Sweet Spot\" Incentive - refined\n        sweet_spot_lower = 0.7 - (item * 0.25) #More aggressive adjustment\n        sweet_spot_upper = 0.9 - (item * 0.15)  #Less aggressive adjustment\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization >= sweet_spot_lower) & (utilization <= sweet_spot_upper) #Use >= and <=\n        priorities[feasible_bins][sweet_spot] += 0.6  # Boost \"sweet spot\"\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and dynamic sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization.\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive stochasticity: smaller items, more exploration.\n        exploration_factor = max(0, 0.1 - (item * 0.05))\n        num_feasible = np.sum(feasible_bins)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Sweet Spot Incentive\n        sweet_spot_lower = 0.7 - (item * 0.1)\n        sweet_spot_upper = 0.9 - (item * 0.05)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n        \n        # Fragmentation penalty: Apply a moderate penalty for small waste, only if other bins exist\n        small_waste = (waste < 0.1)\n        if np.sum(feasible_bins) > 1:\n            priorities[feasible_bins][small_waste] *= 0.5 #reduce priority\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n### Analyze & experience\n- *   **Comparing (1st) vs (20th):** The first heuristic employs a more sophisticated and configurable approach, utilizing numerous tunable parameters to fine-tune the bin selection process. It incorporates a bin usage history penalty (if available) and defines sweet spots with parameters. The 20th heuristic lacks the bin usage history and has simpler sweet spot definitions.\n\n*   **Comparing (2nd) vs (19th):** The 2nd heuristic is similar to the 19th, but has less aggressive bonuses and penalties.\n\n*   **Comparing (1st) vs (2nd):** The first heuristic introduces configurable parameters, bin usage history, refined exploration with `exploration_base`, `max_exploration`, `sweet_spot_lower_base`, `sweet_spot_lower_item_scale`, `sweet_spot_upper_base`, `sweet_spot_upper_item_scale`, `sweet_spot_reward`, `usage_penalty_factor` and `tiny_constant`. The second heuristic has simpler parameters.\n\n*   **Comparing (3rd) vs (4th):** The fourth heuristic incorporates bin diversity (capacity standard deviation) into the exploration factor and sweet spot definition, as well as penalizing bins that will have very little space left after adding the item. The third heuristic lacks these features.\n\n*   **Comparing (19th) vs (20th):** Both are virtually identical.\n\n*   **Comparing (17th) vs (18th):** Both are virtually identical.\n\n*   **Comparing (15th) vs (16th):** All are virtually identical.\n\n*   **Comparing (second worst) vs (worst):** They have very similar logic but the best-fit epsilon variable.\n\n*   **Overall:** Better heuristics incorporate more adaptive and nuanced mechanisms such as:\n\n    *   **Configurable Parameters:** Allow for fine-tuning of various aspects of the heuristic, like exploration rate, fragmentation penalties, and sweet spot definitions.\n    *   **Bin Usage History:** Penalizing recently used bins can promote a more balanced utilization of available bins.\n    *   **Dynamic Sweet Spots:** Adjusting the \"sweet spot\" range based on item size helps to optimize bin utilization.\n    *   **Bin Diversity:** Consider the standard deviation of bin capacities to influence exploration.\n    *   **Item-Aware Penalties/Rewards:** Scaling penalties and rewards based on item size can improve overall performance.\n- \nOkay, let's refine \"Current self-reflection\" for better heuristic design, steering clear of the pitfalls outlined in \"Ineffective self-reflection.\"\n\nHere's a breakdown to guide improvements:\n\n*   **Keywords:** Adaptive, iterative, empirical, balance, problem-aware.\n\n*   **Advice:** Begin with a simple, demonstrably effective heuristic core. Incrementally add complexity only when justified by empirical results. Prioritize problem-aware adaptations over excessive parameters.\n\n*   **Avoid:** Over-engineering, non-linear complexity without justification, fixed stochasticity, and premature optimization. Avoid adding features without understanding their isolated effects.\n\n*   **Explanation:** Focus on *why* an adaptation helps, not just *that* it does. Small, problem-aware adjustments are better than large, untuned ones. Build incrementally, validating each addition.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}