{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                tiny_constant: float = 3.872792425245462e-05,\n                exploration_base: float = 0.0588708005223268,\n                max_exploration: float = 0.22627485634479824,\n                almost_full_threshold: float = 0.0731501402904877,\n                almost_full_penalty: float = 0.16293657163363146,\n                small_item_bin_multiple: float = 1.7118039205991789,\n                small_item_reward: float = 0.7609806445823415,\n                sweet_spot_lower_base: float = 0.6152888971175634,\n                sweet_spot_lower_item_scale: float = 0.20731655571678453,\n                sweet_spot_upper_base: float = 0.8729752618835456,\n                sweet_spot_upper_item_scale: float = 0.17849120545902175,\n                sweet_spot_reward: float = 0.5265016816500563,\n                usage_penalty_factor: float = 0.059912547982452435) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + tiny_constant)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(max_exploration, exploration_base * num_feasible * item)  # Increased base, scale by item size\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Stronger and more nuanced. Target almost-full bins.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold #Slightly less aggressive here\n        priorities[feasible_bins][almost_full] *= almost_full_penalty  # Significant penalty for using almost-full bins.\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > small_item_bin_multiple * item, small_item_reward, 0) #Increased reward and slightly larger bin requirement\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size.\n        sweet_spot_lower = sweet_spot_lower_base - (item * sweet_spot_lower_item_scale) #Dynamic Lower Bound\n        sweet_spot_upper = sweet_spot_upper_base - (item * sweet_spot_upper_item_scale) #Dynamic Upper Bound\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += sweet_spot_reward #Increased the reward.\n\n        # Bin History: Penalize bins that have been filled recently more aggressively.\n        # This requires an external mechanism (not part of this function) to track bin usage.\n        # This is a placeholder:  You'd need to maintain 'bin_usage_history' externally.\n        # Example: bin_usage_history = np.zeros_like(bins_remain_cap) #Initialize to all zeros\n        # Higher values = recently used more.\n\n        #In a separate step, you would update this value: bin_usage_history[chosen_bin] += 1\n\n        #The following assumes that bin_usage_history exist in the scope.\n        try:\n            bin_usage_history #Test if the variable exists, exception otherwise\n            usage_penalty = bin_usage_history[feasible_bins] * usage_penalty_factor #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        except NameError:\n            pass #If it doesn't exist, continue without this feature.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                tiny_constant: float = 3.872792425245462e-05,\n                exploration_base: float = 0.0588708005223268,\n                max_exploration: float = 0.22627485634479824,\n                almost_full_threshold: float = 0.0731501402904877,\n                almost_full_penalty: float = 0.16293657163363146,\n                small_item_bin_multiple: float = 1.7118039205991789,\n                small_item_reward: float = 0.7609806445823415,\n                sweet_spot_lower_base: float = 0.6152888971175634,\n                sweet_spot_lower_item_scale: float = 0.20731655571678453,\n                sweet_spot_upper_base: float = 0.8729752618835456,\n                sweet_spot_upper_item_scale: float = 0.17849120545902175,\n                sweet_spot_reward: float = 0.5265016816500563,\n                usage_penalty_factor: float = 0.059912547982452435) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + tiny_constant)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(max_exploration, exploration_base * num_feasible * item)  # Increased base, scale by item size\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Stronger and more nuanced. Target almost-full bins.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold #Slightly less aggressive here\n        priorities[feasible_bins][almost_full] *= almost_full_penalty  # Significant penalty for using almost-full bins.\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > small_item_bin_multiple * item, small_item_reward, 0) #Increased reward and slightly larger bin requirement\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size.\n        sweet_spot_lower = sweet_spot_lower_base - (item * sweet_spot_lower_item_scale) #Dynamic Lower Bound\n        sweet_spot_upper = sweet_spot_upper_base - (item * sweet_spot_upper_item_scale) #Dynamic Upper Bound\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += sweet_spot_reward #Increased the reward.\n\n        # Bin History: Penalize bins that have been filled recently more aggressively.\n        # This requires an external mechanism (not part of this function) to track bin usage.\n        # This is a placeholder:  You'd need to maintain 'bin_usage_history' externally.\n        # Example: bin_usage_history = np.zeros_like(bins_remain_cap) #Initialize to all zeros\n        # Higher values = recently used more.\n\n        #In a separate step, you would update this value: bin_usage_history[chosen_bin] += 1\n\n        #The following assumes that bin_usage_history exist in the scope.\n        try:\n            bin_usage_history #Test if the variable exists, exception otherwise\n            usage_penalty = bin_usage_history[feasible_bins] * usage_penalty_factor #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        except NameError:\n            pass #If it doesn't exist, continue without this feature.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                waste_epsilon: float = 6.882654483108006e-09,\n                exploration_scale: float = 0.11998258000090561,\n                almost_full_threshold: float = 0.04208930415522055,\n                penalty_base: float = 0.2991730804270332,\n                penalty_scale: float = 0.17169098775098807,\n                max_penalty_reduction: float = 0.41810633397145414,\n                significantly_filled_threshold: float = 0.5404821775259523,\n                sweet_spot_incentive: float = 0.09483027188739752,\n                large_cap_factor: float = 1.530037025269068,\n                large_cap_reward: float = 0.31029768301945104) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive stochasticity, and item-aware penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        priorities[feasible_bins] = 1 / (waste + waste_epsilon)\n\n        # Adaptive stochasticity: scaled by feasible bin count.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = exploration_scale / (num_feasible + waste_epsilon)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Item-aware fragmentation penalty.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold\n        penalty_factor = penalty_base + penalty_scale * item  # Adjust penalty with item size.\n        priorities[feasible_bins][almost_full] *= (1 - min(penalty_factor, max_penalty_reduction))\n\n        # Sweet spot incentive near full capacity.\n        fill_ratio = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        significantly_filled = fill_ratio > significantly_filled_threshold\n        priorities[feasible_bins][significantly_filled] += sweet_spot_incentive\n\n        # Reward larger bins based on item size.\n        large_cap_reward_values = np.where(bins_remain_cap[feasible_bins] > item * large_cap_factor, large_cap_reward, 0)\n        priorities[feasible_bins] += large_cap_reward_values\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive exploration, and dynamic sweet spot. \n    Prioritizes based on item size and bin diversity.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        num_feasible = np.sum(feasible_bins)\n        capacity_std = np.std(bins_remain_cap[feasible_bins])\n        exploration_factor = min(0.3, 0.05 * num_feasible + 0.1 * capacity_std)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.05\n        priorities[feasible_bins][almost_full] *= 0.1\n\n        sweet_spot_lower = 0.6 - (item * 0.2) - (capacity_std * 0.02)\n        sweet_spot_upper = 0.8 - (item * 0.1) + (capacity_std * 0.02)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n        very_small_remaining = bins_remain_cap[feasible_bins] - item < 0.1\n        priorities[feasible_bins][very_small_remaining] *= 0.3\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Hybrid heuristic: Best-fit core, adaptive exploration, sweet spot, and fragmentation control.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-Fit Core\n        priorities[feasible_bins] = 1 / (waste + 1e-5)\n\n        # Adaptive Exploration: Scaled by item size and feasible bins\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.2, 0.05 * num_feasible * item)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Sweet Spot Incentive: Dynamic range based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.2)\n        sweet_spot_upper = 0.9 - (item * 0.1)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n\n        # Fragmentation Penalty: Target almost-full bins\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.08\n        priorities[feasible_bins][almost_full] *= 0.7\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Hybrid heuristic: Best-fit core, adaptive exploration, sweet spot, and fragmentation control.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-Fit Core\n        priorities[feasible_bins] = 1 / (waste + 1e-5)\n\n        # Adaptive Exploration: Scaled by item size and feasible bins\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.2, 0.05 * num_feasible * item)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Sweet Spot Incentive: Dynamic range based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.2)\n        sweet_spot_upper = 0.9 - (item * 0.1)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n\n        # Fragmentation Penalty: Target almost-full bins\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.08\n        priorities[feasible_bins][almost_full] *= 0.7\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and dynamic sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        priorities[feasible_bins] = np.minimum(priorities[feasible_bins], 50)\n\n        # Adaptive stochasticity\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 * (1 - item) / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n\n        # Dynamic sweet spot incentive\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0 # binsize fixed at 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and dynamic sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        priorities[feasible_bins] = np.minimum(priorities[feasible_bins], 50)\n\n        # Adaptive stochasticity\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 * (1 - item) / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n\n        # Dynamic sweet spot incentive\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0 # binsize fixed at 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive exploration and bin utilization awareness.\n\n    This version focuses on simplicity and problem-aware adjustments.\n    It prioritizes bins with sufficient space, introduces adaptive exploration,\n    and penalizes bins with low utilization *after* placement.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 1e-9)  # Adding a small constant to avoid division by zero\n\n        # Adaptive Exploration: Reduce exploration as the bin gets fuller\n        exploration_factor = np.clip(1.0 - bins_remain_cap[feasible_bins], 0.0, 0.2) #Explore less as bins are empty\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n\n        # Encourage filling bins to a reasonable level, but only *after* placing the item\n        # Target utilization between 70% and 95% after placing item.\n        future_utilization = (bins_remain_cap[feasible_bins] - item)\n        sweet_spot = (future_utilization > 0.05) & (future_utilization < 0.30)\n\n        priorities[feasible_bins][sweet_spot] *= 2.0 #give a significant boost to bins that will be within the utilization range\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and dynamic sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        priorities[feasible_bins] = np.minimum(priorities[feasible_bins], 50)\n\n        # Adaptive stochasticity\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 * (1 - item) / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n\n        # Dynamic sweet spot incentive\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0 # binsize fixed at 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive exploration and sweet spot incentive.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization.\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive stochasticity\n        exploration_factor = max(0, 0.1 - (item * 0.05))\n        num_feasible = np.sum(feasible_bins)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Sweet Spot Incentive, adjust range based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.3)\n        sweet_spot_upper = 0.9 - (item * 0.2)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.6\n\n        # Bonus for filling a bin completely.\n        almost_full_bin = waste < 0.05\n        priorities[feasible_bins][almost_full_bin] += 0.8\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive exploration and fragmentation control.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        priorities[feasible_bins] = np.minimum(priorities[feasible_bins], 50)\n\n        # Adaptive stochasticity\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 * (1 - item) / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n\n        # Fragmentation penalty\n        wasted_space_ratio = waste / 1.0\n        almost_full = wasted_space_ratio < 0.1\n        priorities[feasible_bins][almost_full] *= 0.4\n\n        # Dynamic sweet spot incentive\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > 1.6 * item, 0.5, 0) #Increased reward and slightly larger bin requirement\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and dynamic penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization.\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive stochasticity: scale with remaining capacity.\n        stochasticity_scale = 0.1 * (1 - np.mean(bins_remain_cap[feasible_bins]) if len(bins_remain_cap[feasible_bins]) > 0 else 0)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_scale\n\n\n        # Penalize almost full bins, scaled by item size.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.5 + 0.2 * item  # Larger items, higher penalty\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty)\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive stochasticity, item-aware fragmentation penalty, and large bin reward.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization with small constant to avoid division by zero\n        priorities[feasible_bins] = 1 / (waste + 1e-6)\n\n        # Adaptive stochasticity: scaled by remaining capacity\n        stochasticity_scale = 0.1 * (1 - np.mean(bins_remain_cap[feasible_bins]) if len(bins_remain_cap[feasible_bins]) > 0 else 0)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_scale\n\n        # Item-aware fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.3 + 0.2 * item  # Larger items, higher penalty\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty)  # Ensure not negative\n\n        # Reward for large bins relative to the item size\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 2, 0.25, 0)\n        priorities[feasible_bins] += large_cap_reward\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive stochasticity, item-aware fragmentation penalty, and large bin reward.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization with small constant to avoid division by zero\n        priorities[feasible_bins] = 1 / (waste + 1e-6)\n\n        # Adaptive stochasticity: scaled by remaining capacity\n        stochasticity_scale = 0.1 * (1 - np.mean(bins_remain_cap[feasible_bins]) if len(bins_remain_cap[feasible_bins]) > 0 else 0)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_scale\n\n        # Item-aware fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.3 + 0.2 * item  # Larger items, higher penalty\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty)  # Ensure not negative\n\n        # Reward for large bins relative to the item size\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 2, 0.25, 0)\n        priorities[feasible_bins] += large_cap_reward\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive stochasticity, item-aware fragmentation penalty, and large bin reward.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization with small constant to avoid division by zero\n        priorities[feasible_bins] = 1 / (waste + 1e-6)\n\n        # Adaptive stochasticity: scaled by remaining capacity\n        stochasticity_scale = 0.1 * (1 - np.mean(bins_remain_cap[feasible_bins]) if len(bins_remain_cap[feasible_bins]) > 0 else 0)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_scale\n\n        # Item-aware fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.3 + 0.2 * item  # Larger items, higher penalty\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty)  # Ensure not negative\n\n        # Reward for large bins relative to the item size\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 2, 0.25, 0)\n        priorities[feasible_bins] += large_cap_reward\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive exploration and sweet spot incentives.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive Exploration\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.2, 0.05 * num_feasible * (1 - item))\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Sweet Spot Incentive\n        sweet_spot_lower = 0.6 - (item * 0.2)\n        sweet_spot_upper = 0.9 - (item * 0.1)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n        # Fragmentation Penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        if num_feasible > 1:\n             priorities[feasible_bins][almost_full] *= 0.4 #reduced penalty\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    This version prioritizes simplicity and demonstrable impact, validated empirically.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 1e-9)  # Tiny constant to avoid division by zero\n\n        # Adaptive Exploration:  Simple exploration scaled by item size and feasibility.\n        exploration_factor = 0.05 * item * np.sum(feasible_bins) # Keep exploration simple initially.\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n\n        # Fragmentation Penalty: Focus on almost-full bins with a simple penalty.\n        almost_full_threshold = 0.1  # Define \"almost full\" as remaining capacity <= 10% of bin size\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold\n        priorities[feasible_bins][almost_full] *= 0.5  # Simple penalty: reduce priority by half.\n\n        # Sweet Spot Incentive: Prioritize bins within a desirable utilization range.\n        sweet_spot_lower = 0.6\n        sweet_spot_upper = 0.9\n        utilization = (bins_remain_cap[feasible_bins] - waste)\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.3  # Add a reward for bins in the sweet spot.\n\n        # Bin Usage History: Penalize recently used bins (if available).\n        try:\n            bin_usage_history  # Check if bin_usage_history exists\n            usage_penalty = bin_usage_history[feasible_bins] * 0.1  # Simple penalty.\n            priorities[feasible_bins] -= usage_penalty\n        except NameError:\n            pass  # If bin_usage_history doesn't exist, skip this step.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive exploration and capacity-aware adjustments.\n    Focuses on balancing bin utilization and preventing fragmentation, adjusting exploration\n    based on the relative item size to the available bin capacities.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).  Slightly stronger preference for tighter fits.\n        priorities[feasible_bins] = 1 / (waste + 1e-9) #Avoid division by zero\n\n        # Adaptive Exploration: Scale exploration based on item size relative to bin capacities.\n        # Smaller items in larger bins trigger more exploration, and vice versa.\n        relative_item_size = item / bins_remain_cap[feasible_bins]\n        exploration_factor = 0.1 * (1 - relative_item_size)  # Higher exploration for smaller items relative to bin size\n        exploration_factor = np.clip(exploration_factor, 0.01, 0.2) #Clamp exploration factor\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n\n        # Capacity-Aware Fragmentation Penalty: Penalize bins that, after packing,\n        # will have a remaining capacity close to common item sizes.\n        # Idea: avoid leaving bins with just enough space for frequently occurring items.\n\n        # Example: Assume common item sizes are around 0.2, 0.3, 0.4 (can be dynamically adjusted)\n        common_item_sizes = np.array([0.2, 0.3, 0.4])\n        remaining_capacity_after_packing = waste\n        \n        fragmentation_penalty = np.zeros_like(remaining_capacity_after_packing)\n        for size in common_item_sizes:\n            fragmentation_penalty += np.exp(-np.abs(remaining_capacity_after_packing - size) / 0.05)  # Gaussian-like penalty\n            \n        priorities[feasible_bins] -= 0.05 * fragmentation_penalty # Apply penalty\n\n        # Small Item Placement Boost: Give a small bonus to bins that are significantly larger than the item.\n        # Helps distribute smaller items more evenly.\n        large_bin_bonus = np.where(bins_remain_cap[feasible_bins] > 2 * item, 0.05, 0.0)  # Increased threshold\n        priorities[feasible_bins] += large_bin_bonus\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive exploration and capacity-aware adjustments.\n    Focuses on balancing bin utilization and preventing fragmentation, adjusting exploration\n    based on the relative item size to the available bin capacities.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).  Slightly stronger preference for tighter fits.\n        priorities[feasible_bins] = 1 / (waste + 1e-9) #Avoid division by zero\n\n        # Adaptive Exploration: Scale exploration based on item size relative to bin capacities.\n        # Smaller items in larger bins trigger more exploration, and vice versa.\n        relative_item_size = item / bins_remain_cap[feasible_bins]\n        exploration_factor = 0.1 * (1 - relative_item_size)  # Higher exploration for smaller items relative to bin size\n        exploration_factor = np.clip(exploration_factor, 0.01, 0.2) #Clamp exploration factor\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n\n        # Capacity-Aware Fragmentation Penalty: Penalize bins that, after packing,\n        # will have a remaining capacity close to common item sizes.\n        # Idea: avoid leaving bins with just enough space for frequently occurring items.\n\n        # Example: Assume common item sizes are around 0.2, 0.3, 0.4 (can be dynamically adjusted)\n        common_item_sizes = np.array([0.2, 0.3, 0.4])\n        remaining_capacity_after_packing = waste\n        \n        fragmentation_penalty = np.zeros_like(remaining_capacity_after_packing)\n        for size in common_item_sizes:\n            fragmentation_penalty += np.exp(-np.abs(remaining_capacity_after_packing - size) / 0.05)  # Gaussian-like penalty\n            \n        priorities[feasible_bins] -= 0.05 * fragmentation_penalty # Apply penalty\n\n        # Small Item Placement Boost: Give a small bonus to bins that are significantly larger than the item.\n        # Helps distribute smaller items more evenly.\n        large_bin_bonus = np.where(bins_remain_cap[feasible_bins] > 2 * item, 0.05, 0.0)  # Increased threshold\n        priorities[feasible_bins] += large_bin_bonus\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}