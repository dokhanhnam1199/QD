{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and fragmentation control.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive Stochasticity\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.2, 0.02 * num_feasible)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation penalty (stronger for larger items)\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.05\n        penalty_factor = 0.2 + item * 0.1 #item size adjusts the penalty dynamically.\n        priorities[feasible_bins][almost_full] *= penalty_factor\n        priorities[feasible_bins][almost_full] = np.clip(priorities[feasible_bins][almost_full], 0, 1) #prevent overflow\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > 1.5 * item, 0.4, 0)\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and fragmentation control.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive Stochasticity\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.2, 0.02 * num_feasible)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation penalty (stronger for larger items)\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.05\n        penalty_factor = 0.2 + item * 0.1 #item size adjusts the penalty dynamically.\n        priorities[feasible_bins][almost_full] *= penalty_factor\n        priorities[feasible_bins][almost_full] = np.clip(priorities[feasible_bins][almost_full], 0, 1) #prevent overflow\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > 1.5 * item, 0.4, 0)\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, best_fit_epsilon: float = 9.258210465617616e-05,\n                max_exploration_factor: float = 0.3946096761709214, exploration_scaling: float = 0.022615188286624184,\n                almost_full_threshold: float = 0.04667996859110015, base_penalty: float = 0.37352075749777613,\n                item_penalty_scaling: float = 0.07283690765445279, large_bin_threshold_multiplier: float = 1.5061089220110162,\n                large_bin_reward: float = 0.4480255263596984) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and fragmentation control.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + best_fit_epsilon)\n\n        # Adaptive Stochasticity\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(max_exploration_factor, exploration_scaling * num_feasible)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation penalty (stronger for larger items)\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold\n        penalty_factor = base_penalty + item * item_penalty_scaling #item size adjusts the penalty dynamically.\n        priorities[feasible_bins][almost_full] *= penalty_factor\n        priorities[feasible_bins][almost_full] = np.clip(priorities[feasible_bins][almost_full], 0, 1) #prevent overflow\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > large_bin_threshold_multiplier * item, large_bin_reward, 0)\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with dynamic sweet spot and adaptive penalty.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive Stochasticity: less aggressive as bins fill.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.1, 0.01 * num_feasible)\n        if np.random.rand() < 0.2:  # Add stochasticity only sometimes\n            priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Stronger when item is large.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.7 + 0.2 * item #up to 20% stronger penalty depending on item size\n        priorities[feasible_bins][almost_full] *= penalty\n\n        # Reward for large capacity\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 1.5, 0.2, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.2\n\n        priorities[feasible_bins] *= (1 + 0.05 * item) #Up to 5% best fit increase based on item size\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Best-fit, adaptive stochasticity, dynamic fragmentation penalty, adaptive large item reward.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        \n        # Adaptive stochasticity\n        exploration_factor = max(0.01, 0.1 * np.mean(bins_remain_cap))\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n        \n        # Dynamic fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        priorities[feasible_bins][almost_full] *= 0.2 # Stronger penalty\n        \n        # Adaptive large item reward\n        large_cap_threshold = item * 1.25\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > large_cap_threshold, 0.5, 0)\n        priorities[feasible_bins] += large_cap_reward\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Best-fit, adaptive stochasticity, dynamic fragmentation penalty, adaptive large item reward.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        \n        # Adaptive stochasticity\n        exploration_factor = max(0.01, 0.1 * np.mean(bins_remain_cap))\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n        \n        # Dynamic fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        priorities[feasible_bins][almost_full] *= 0.2 # Stronger penalty\n        \n        # Adaptive large item reward\n        large_cap_threshold = item * 1.25\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > large_cap_threshold, 0.5, 0)\n        priorities[feasible_bins] += large_cap_reward\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability.  Version 2 focuses on simplified adaptivity\n    and more targeted fragmentation control.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Core: Prioritize best fit (minimize waste).  Simplified inverse waste calculation.\n        priorities[feasible_bins] = 1 / (waste + 0.0001)\n\n        # Adaptive Exploration: Scale exploration based on item size.  Larger items get less exploration.\n        exploration_factor = 0.1 * (1 - item)  # Reduced overall exploration and scaled by item size.\n        num_feasible = np.sum(feasible_bins)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Targeted Fragmentation Penalty: Only penalize bins that would become *very* full.\n        remaining_capacity_ratio = waste / 1.0 #Ratio to bin capacity, binsize fixed at 1.\n        almost_full = remaining_capacity_ratio < 0.1 #Bins with > 90% utilisation.\n        priorities[feasible_bins][almost_full] *= 0.5  # Increased penalty for almost-full bins. More direct penalty.\n\n        # Sweet Spot Incentive: A fixed sweet spot, but only applied if it improves utilization.\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > 0.6) & (utilization < 0.8)  # Fixed sweet spot.\n        improvement = utilization[sweet_spot] > item # Only add sweetspot, if it improves.\n        priorities[feasible_bins][sweet_spot] += 0.3 # Increased the reward.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability.  Version 2 focuses on simplified adaptivity\n    and more targeted fragmentation control.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Core: Prioritize best fit (minimize waste).  Simplified inverse waste calculation.\n        priorities[feasible_bins] = 1 / (waste + 0.0001)\n\n        # Adaptive Exploration: Scale exploration based on item size.  Larger items get less exploration.\n        exploration_factor = 0.1 * (1 - item)  # Reduced overall exploration and scaled by item size.\n        num_feasible = np.sum(feasible_bins)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Targeted Fragmentation Penalty: Only penalize bins that would become *very* full.\n        remaining_capacity_ratio = waste / 1.0 #Ratio to bin capacity, binsize fixed at 1.\n        almost_full = remaining_capacity_ratio < 0.1 #Bins with > 90% utilisation.\n        priorities[feasible_bins][almost_full] *= 0.5  # Increased penalty for almost-full bins. More direct penalty.\n\n        # Sweet Spot Incentive: A fixed sweet spot, but only applied if it improves utilization.\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > 0.6) & (utilization < 0.8)  # Fixed sweet spot.\n        improvement = utilization[sweet_spot] > item # Only add sweetspot, if it improves.\n        priorities[feasible_bins][sweet_spot] += 0.3 # Increased the reward.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_size_assumption: float = 1.0) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with dynamic sweet spot and fragmentation control.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        priorities[feasible_bins] = np.minimum(priorities[feasible_bins], 50)\n\n        # Adaptive stochasticity\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n\n        # Fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        priorities[feasible_bins][almost_full] *= 0.4\n\n        # Dynamic sweet spot incentive\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / bin_size_assumption\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration,\n    incorporating bin fill level awareness and targeted fragmentation control.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive Stochasticity: Item size dictates exploration. Larger items, less exploration.\n        exploration_factor = max(0, 0.1 - (item * 0.05))  #exploration decreases as item increases\n        num_feasible = np.sum(feasible_bins)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Bin Fill Level Awareness: Reward bins close to full but still feasible.\n        fill_ratio = (1.0 - bins_remain_cap[feasible_bins]) #Assuming bin size is 1\n        almost_full = (fill_ratio > 0.7)\n        priorities[feasible_bins][almost_full] += 0.3 #Boost priority\n\n        # Targeted Fragmentation Control: Penalize creating small remainders, ONLY if alternative bins exist\n        small_waste = (waste < 0.1) #waste is smaller than 0.1\n        \n        if np.sum(feasible_bins) > 1: #check for alternative\n             priorities[feasible_bins][small_waste] *= 0.5 #reduce priority if waste is too small and alternates exist\n\n        # Dynamic \"Sweet Spot\" Incentive\n        sweet_spot_lower = 0.7 - (item * 0.1) #Slightly adjust sweet spot\n        sweet_spot_upper = 0.9 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_size_assumption: float = 1.0) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with dynamic sweet spot and fragmentation control.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        priorities[feasible_bins] = np.minimum(priorities[feasible_bins], 50)\n\n        # Adaptive stochasticity\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n\n        # Fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        priorities[feasible_bins][almost_full] *= 0.4\n\n        # Dynamic sweet spot incentive\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / bin_size_assumption\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit, dynamically adjusts fragmentation penalty, \n    and provides adaptive large bin incentives.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Prioritize based on inverse waste (best fit)\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n\n        # Adaptive stochasticity (reduced exploration as bins fill)\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = max(0, 0.1 - 0.01 * num_feasible)  # Reduce exploration with more options\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Penalize almost full bins dynamically based on item size\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty_factor = 0.2 + item * 0.5 # Adjust sensitivity of almost full depending on size.\n        priorities[feasible_bins][almost_full] *= (1 - penalty_factor)\n        \n        #Dynamically incentivize larger bins if remaining capacity is high enough\n        large_cap_threshold = item * (1.2 + item * 0.4)\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > large_cap_threshold, 0.4 + item * 0.2, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and fragmentation handling.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization.\n        priorities[feasible_bins] = np.minimum(10 / (waste + 0.0001), 50)\n\n        # Adaptive stochasticity: Less exploration with more feasible bins.\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_factor\n\n        # Fragmentation penalty: Apply a moderate penalty for almost-full bins.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        priorities[feasible_bins][almost_full] *= 0.4\n\n        # Reward filling bins well.\n        fill_ratio = item / bins_remain_cap[feasible_bins]\n        good_fill = (fill_ratio > 0.7) & (fill_ratio <= 1.0)\n        priorities[feasible_bins][good_fill] += 0.5\n\n        # Large item reward.\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 1.25, 0.25, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n        # Overfill penalty.\n        overfill_penalty = np.where(fill_ratio > 1, -1, 0)\n        priorities[feasible_bins] += overfill_penalty\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive stochasticity, and dynamic fragmentation penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization with small constant\n        priorities[feasible_bins] = 1 / (waste + 0.0001)\n\n        # Adaptive stochasticity based on the number of feasible bins\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 / (num_feasible + 1e-6)  # Reduce exploration as bins fill\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_factor\n\n        # Dynamic fragmentation penalty based on item size\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.05\n        penalty_factor = 0.3 + 0.2 * item  # Larger items incur a stronger penalty\n        priorities[feasible_bins][almost_full] *= (1- min(penalty_factor, 0.5))  # cap to 0.5\n\n        # Reward significantly filled bins\n        fill_ratio = (bins_remain_cap[feasible_bins] - waste) / bins_remain_cap[feasible_bins]\n        significantly_filled = fill_ratio > 0.5\n        priorities[feasible_bins][significantly_filled] += 0.2\n\n        # Large item reward if sufficient capacity exists, dynamic threshold\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * (1.5 + 0.5 * item), 0.25, 0) #threshold adapts to item size\n        priorities[feasible_bins] += large_cap_reward\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and fragmentation handling.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization.\n        priorities[feasible_bins] = np.minimum(10 / (waste + 0.0001), 50)\n\n        # Adaptive stochasticity: Less exploration with more feasible bins.\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_factor\n\n        # Fragmentation penalty: Apply a moderate penalty for almost-full bins.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        priorities[feasible_bins][almost_full] *= 0.4\n\n        # Reward filling bins well.\n        fill_ratio = item / bins_remain_cap[feasible_bins]\n        good_fill = (fill_ratio > 0.7) & (fill_ratio <= 1.0)\n        priorities[feasible_bins][good_fill] += 0.5\n\n        # Large item reward.\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 1.25, 0.25, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n        # Overfill penalty.\n        overfill_penalty = np.where(fill_ratio > 1, -1, 0)\n        priorities[feasible_bins] += overfill_penalty\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. This version\n    focuses on simplified adaptive components and a more robust handling of edge cases.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 0.00001)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity:  Exploration proportional to available capacity, capped.\n        total_capacity = np.sum(bins_remain_cap)\n        exploration_factor = min(0.1, total_capacity * 0.001) # Proportional to total remaining capacity\n        num_feasible = np.sum(feasible_bins)        \n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty:  Focus on *relative* wasted space. Less aggressive, more stable.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1 # Less aggressive threshold\n        priorities[feasible_bins][almost_full] *= 0.5  # Reduced penalty\n\n        # Reward larger bins for small items, but only if it's a *significant* size difference.\n        if item < 0.3: #Only triggers when dealing with small items\n          large_bin_reward = bins_remain_cap[feasible_bins] > (1.0 - item) #is it approaching full bin?\n          priorities[feasible_bins][large_bin_reward] += 0.3\n        \n\n        # Dynamic \"Sweet Spot\" Incentive:  Simplified, based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.15)  #Dynamic Lower Bound - adjusted scale\n        sweet_spot_upper = 0.9 - (item * 0.1) #Dynamic Upper Bound - decreased upper bound\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.3\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and fragmentation penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Prioritize based on inverse waste (best fit)\n        priorities[feasible_bins] = 1 / (waste + 0.0001)\n\n        # Adaptive stochasticity: less exploration when bins are fuller.\n        stochasticity_scale = 0.1 * (1 - np.mean(bins_remain_cap[feasible_bins]) if len(bins_remain_cap[feasible_bins]) > 0 else 0)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_scale\n\n        # Penalize almost full bins, scaled by item size.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.5 + 0.2 * item  # Larger items, higher penalty\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty) # Ensure not negative\n\n        # Large item reward: only if there's significantly more space.\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 2, 0.5, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n        # Sweet spot incentive (adaptive range)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0 #Assuming bin size is 1\n        sweet_spot_lower = 0.5\n        sweet_spot_upper = 0.75\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.3\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and fragmentation penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Prioritize based on inverse waste (best fit)\n        priorities[feasible_bins] = 1 / (waste + 0.0001)\n\n        # Adaptive stochasticity: less exploration when bins are fuller.\n        stochasticity_scale = 0.1 * (1 - np.mean(bins_remain_cap[feasible_bins]) if len(bins_remain_cap[feasible_bins]) > 0 else 0)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_scale\n\n        # Penalize almost full bins, scaled by item size.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.5 + 0.2 * item  # Larger items, higher penalty\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty) # Ensure not negative\n\n        # Large item reward: only if there's significantly more space.\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 2, 0.5, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n        # Sweet spot incentive (adaptive range)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0 #Assuming bin size is 1\n        sweet_spot_lower = 0.5\n        sweet_spot_upper = 0.75\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.3\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties, dynamic exploration, and bin-aware adjustments.\n    Emphasizes bin utilization, prevents fragmentation, and balances exploration based on item size\n    and remaining bin capacities. Focuses on simplicity and targeted adaptation.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste). More pronounced best-fit.\n        priorities[feasible_bins] = 10 / (waste + 0.00001)  # Increased the impact of best-fit\n\n        # Adaptive Stochasticity: Exploration decreases with item size and low remaining capacities\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.1, 0.01 * num_feasible * (1 - item)) #Reduced overall exploration & item aware\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Target almost-full bins dynamically scaled.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < (0.05 + 0.02 * item)  #Dynamically increase threshold for larger items.\n        priorities[feasible_bins][almost_full] *= 0.1  # Increased penalty.\n\n        # Rewarding larger bins for smaller items. More focused reward.\n        large_bin_threshold = 1.2 + 0.1*item\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > large_bin_threshold, 0.3, 0) # Slightly reduced reward\n        priorities[feasible_bins] += small_item_large_bin_reward\n        \n        # Dynamic \"Sweet Spot\" Incentive: Adapted Sweet Spot.\n        sweet_spot_lower = 0.6 - (item * 0.15)  # Adjusted\n        sweet_spot_upper = 0.85 - (item * 0.05) # Adjusted\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.3 #Increased reward slightly\n\n        #Bin Awareness - Encourage filling bins with lower remaining capacity\n        bin_capacity_rank = np.argsort(bins_remain_cap[feasible_bins])\n        capacity_incentive = np.linspace(0,0.15, num = len(bin_capacity_rank)) #Slight linear incentive.\n\n        priorities[feasible_bins][bin_capacity_rank] += capacity_incentive\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties, dynamic exploration, and bin-aware adjustments.\n    Emphasizes bin utilization, prevents fragmentation, and balances exploration based on item size\n    and remaining bin capacities. Focuses on simplicity and targeted adaptation.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste). More pronounced best-fit.\n        priorities[feasible_bins] = 10 / (waste + 0.00001)  # Increased the impact of best-fit\n\n        # Adaptive Stochasticity: Exploration decreases with item size and low remaining capacities\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.1, 0.01 * num_feasible * (1 - item)) #Reduced overall exploration & item aware\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Target almost-full bins dynamically scaled.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < (0.05 + 0.02 * item)  #Dynamically increase threshold for larger items.\n        priorities[feasible_bins][almost_full] *= 0.1  # Increased penalty.\n\n        # Rewarding larger bins for smaller items. More focused reward.\n        large_bin_threshold = 1.2 + 0.1*item\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > large_bin_threshold, 0.3, 0) # Slightly reduced reward\n        priorities[feasible_bins] += small_item_large_bin_reward\n        \n        # Dynamic \"Sweet Spot\" Incentive: Adapted Sweet Spot.\n        sweet_spot_lower = 0.6 - (item * 0.15)  # Adjusted\n        sweet_spot_upper = 0.85 - (item * 0.05) # Adjusted\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.3 #Increased reward slightly\n\n        #Bin Awareness - Encourage filling bins with lower remaining capacity\n        bin_capacity_rank = np.argsort(bins_remain_cap[feasible_bins])\n        capacity_incentive = np.linspace(0,0.15, num = len(bin_capacity_rank)) #Slight linear incentive.\n\n        priorities[feasible_bins][bin_capacity_rank] += capacity_incentive\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}