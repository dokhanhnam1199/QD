[
  {
    "stdout_filepath": "problem_iter23_response0.txt_stdout.txt",
    "code_path": "problem_iter23_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive exploration.\n    Balances bin utilization and prevents fragmentation.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        tiny_constant = 1e-06\n        priorities[feasible_bins] = 1 / (waste + tiny_constant)\n\n        # Adaptive Exploration: Item size and bin fill level\n        num_feasible = np.sum(feasible_bins)\n        exploration_base = 0.05\n        max_exploration = 0.2\n        exploration_factor = min(max_exploration, exploration_base * (1- bins_remain_cap[feasible_bins].mean()) * item )\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Dynamic Sweet Spot Incentive\n        sweet_spot_lower = 0.6 - (item * 0.2)\n        sweet_spot_upper = 0.9 - (item * 0.1)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n\n        # Fragmentation Penalty: Target almost-full bins, tuned threshold and penalty\n        almost_full_threshold = 0.1  #Slightly higher threshold.\n        almost_full_penalty = 0.2 #Increase the penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold\n        priorities[feasible_bins][almost_full] *= (1-almost_full_penalty)  # Apply Penalty\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 30.09573195053849,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response1.txt_stdout.txt",
    "code_path": "problem_iter23_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_usage_history: np.ndarray = None) -> np.ndarray:\n    \"\"\"Prioritizes best-fit, adaptive exploration, and sweet spot with bin history.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization (modified scaling)\n        priorities[feasible_bins] = 5 / (waste + 0.0001)  # Reduced scale\n        priorities[feasible_bins] = np.minimum(priorities[feasible_bins], 25)  # Cap the priority\n\n        # Adaptive exploration (item-dependent, num_feasible bins)\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.05 * (1 - item) / (num_feasible + 0.1)  # Further scaled down\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n\n        # Dynamic sweet spot incentive (item-dependent)\n        sweet_spot_lower = 0.6 - (item * 0.05) # Reduced sensitivity to item size\n        sweet_spot_upper = 0.8 - (item * 0.025) # Reduced sensitivity to item size\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.3  # Reduced reward\n\n        # Bin History: Penalize bins that have been filled recently.\n        if bin_usage_history is not None:\n            usage_penalty = bin_usage_history[feasible_bins] * 0.03 #Scaling factor can be tuned. Reduce scale\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response2.txt_stdout.txt",
    "code_path": "problem_iter23_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit, adaptive exploration, frag. penalty, sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit\n        priorities[feasible_bins] = 1 / (waste + 1e-6)\n\n        # Adaptive exploration\n        relative_item_size = item / bins_remain_cap[feasible_bins]\n        exploration_factor = 0.1 * (1 - relative_item_size)\n        exploration_factor = np.clip(exploration_factor, 0.01, 0.2)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n\n        # Fragmentation Penalty (Capacity-Aware)\n        common_item_sizes = np.array([0.2, 0.3, 0.4])\n        remaining_capacity_after_packing = waste\n        fragmentation_penalty = np.zeros_like(remaining_capacity_after_packing)\n        for size in common_item_sizes:\n            fragmentation_penalty += np.exp(-np.abs(remaining_capacity_after_packing - size) / 0.05)\n        priorities[feasible_bins] -= 0.05 * fragmentation_penalty\n        \n        # Sweet Spot Incentive: Simplified and robust.\n        utilization_lower = 0.7\n        utilization_upper = 0.9\n        utilization = (bins_remain_cap[feasible_bins] - waste)  # No need to divide by bin size if bin size == 1\n        sweet_spot = (utilization > utilization_lower) & (utilization < utilization_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 7.54886318308736,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response3.txt_stdout.txt",
    "code_path": "problem_iter23_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive exploration, dynamic sweet spot, and fragmentation control.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        priorities[feasible_bins] = 1 / (waste + 1e-9)\n\n        exploration_factor = 0.01 * item\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n\n        sweet_spot_lower = 0.6 - (0.1 * item)\n        sweet_spot_upper = 0.9 - (0.05 * item)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.2\n\n        almost_full_threshold = 0.1\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold\n        priorities[feasible_bins][almost_full] *= 0.5\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 3.1013163143199183,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response4.txt_stdout.txt",
    "code_path": "problem_iter23_code4.py",
    "code": "import numpy as np\n\nbin_usage_history = None # Initialize bin_usage_history outside the function\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit, adaptive exploration, sweet spot, & bin history.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 1e-6)\n\n        # Adaptive exploration\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = 0.02 * min(1, num_feasible * item)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Sweet spot incentive\n        utilization_lower = 0.7 - (item * 0.05)  # Adaptive sweet spot\n        utilization_upper = 0.9 - (item * 0.025)\n        utilization = bins_remain_cap[feasible_bins] - waste\n        sweet_spot = (utilization > utilization_lower) & (utilization < utilization_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n\n        # Bin history penalty\n        global bin_usage_history\n        if bin_usage_history is not None:\n            usage_penalty = bin_usage_history[feasible_bins] * 0.05\n            priorities[feasible_bins] -= usage_penalty\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 3.9888312724371757,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response5.txt_stdout.txt",
    "code_path": "problem_iter23_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive exploration, and dynamic sweet spot.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        priorities[feasible_bins] = 1 / (waste + 1e-6)\n\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = 0.02 * min(1, num_feasible * item)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        utilization_lower = 0.7 - (0.1 * item)\n        utilization_upper = 0.9 - (0.05 * item)\n        utilization = bins_remain_cap[feasible_bins] - waste\n        sweet_spot = (utilization > utilization_lower) & (utilization < utilization_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n\n        try:\n            bin_usage_history\n            usage_penalty = bin_usage_history[feasible_bins] * 0.05\n            priorities[feasible_bins] -= usage_penalty\n        except NameError:\n            pass\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.01874750698045,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response6.txt_stdout.txt",
    "code_path": "problem_iter23_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive exploration and sweet spot incentive.\n    Balances bin utilization and prevents fragmentation, adjusting strategies based on item size.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-Fit Core\n        priorities[feasible_bins] = 1 / (waste + 1e-5)\n\n        # Adaptive Exploration: Scaled by item size and feasible bins\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.2, 0.05 * num_feasible * item)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Sweet Spot Incentive: Dynamic range based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.2)\n        sweet_spot_upper = 0.9 - (item * 0.1)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n\n        # Fragmentation Penalty: Target almost-full bins, more targeted for small bins\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = (wasted_space_ratio < 0.08) & (bins_remain_cap[feasible_bins] < 0.5)\n        priorities[feasible_bins][almost_full] *= 0.7\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 3.6597526924611135,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response7.txt_stdout.txt",
    "code_path": "problem_iter23_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Best-fit, adaptive exploration, sweet spot, fragmentation control.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-Fit\n        priorities[feasible_bins] = 1 / (waste + 1e-5)\n\n        # Adaptive Exploration: item size relative to bin capacity\n        relative_item_size = item / bins_remain_cap[feasible_bins]\n        exploration_factor = 0.1 * (1 - relative_item_size)\n        exploration_factor = np.clip(exploration_factor, 0.01, 0.2)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n\n\n        # Sweet Spot Incentive: Dynamic sweet spot based on item size\n        sweet_spot_lower = 0.6 - (item * 0.2)\n        sweet_spot_upper = 0.9 - (item * 0.1)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n\n        # Fragmentation Penalty: Target almost-full bins\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.08\n        priorities[feasible_bins][almost_full] *= 0.7\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 7.110091743119263,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response8.txt_stdout.txt",
    "code_path": "problem_iter23_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Best-fit, adaptive exploration, item-aware fragmentation,\n    and sweet spot incentive. Balance exploration and exploitation.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 1e-5)\n\n        # Adaptive Exploration: Item-dependent scale.\n        num_feasible = np.sum(feasible_bins)\n        exploration_scale = min(0.2, 0.05 * item * num_feasible)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_scale\n\n        # Sweet Spot Incentive: Dynamic, item-related range.\n        sweet_spot_lower = 0.6 - (item * 0.2)\n        sweet_spot_upper = 0.9 - (item * 0.1)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n\n        # Item-aware Fragmentation Penalty: Adaptive scaling.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.08\n        penalty_factor = 0.3 + 0.2 * item # Item aware\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty_factor)\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 3.689668927004388,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response9.txt_stdout.txt",
    "code_path": "problem_iter23_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with dynamic exploration and item-aware adjustments.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 1e-6)\n\n        # Adaptive exploration\n        num_feasible = np.sum(feasible_bins)\n        exploration_scale = 0.1 * item * num_feasible\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_scale\n\n        # Item-aware fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.3 + 0.2 * item\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty)\n\n        # Dynamic sweet spot\n        sweet_spot_lower = 0.6 - (item * 0.2)\n        sweet_spot_upper = 0.9 - (item * 0.1)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.25\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 35.82967690466693,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter24_response0.txt_stdout.txt",
    "code_path": "problem_iter24_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_quantiles: np.ndarray = None) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration,\n    incorporating bin capacity quantiles for normalization and relative comparisons.\n\n    Args:\n        item (float): The size of the item to be packed.\n        bins_remain_cap (np.ndarray): Array of remaining capacities for each bin.\n        bin_quantiles (np.ndarray, optional): Quantiles representing capacity distribution of bins. Defaults to None.\n\n    Returns:\n        np.ndarray: Array of priority scores for each bin.\n    \"\"\"\n\n    tiny_constant: float = 1e-05\n    exploration_base: float = 0.06\n    max_exploration: float = 0.25\n    almost_full_threshold: float = 0.07\n    almost_full_penalty: float = 0.2\n    small_item_bin_multiple: float = 1.7\n    small_item_reward: float = 0.8\n    sweet_spot_lower_base: float = 0.6\n    sweet_spot_lower_item_scale: float = 0.2\n    sweet_spot_upper_base: float = 0.85\n    sweet_spot_upper_item_scale: float = 0.18\n    sweet_spot_reward: float = 0.6\n    usage_penalty_factor: float = 0.06\n    quantile_penalty_factor: float = 0.1\n\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).  Scale by bin capacity for normalization.\n        priorities[feasible_bins] = bins_remain_cap[feasible_bins] / (waste + tiny_constant)\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(max_exploration, exploration_base * num_feasible * item)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Stronger, targets almost-full bins, scaled by item size.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold\n        priorities[feasible_bins][almost_full] *= (1 - almost_full_penalty * item) # Scale penalty by item size.\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > small_item_bin_multiple * item, small_item_reward, 0)\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size.\n        sweet_spot_lower = sweet_spot_lower_base - (item * sweet_spot_lower_item_scale)\n        sweet_spot_upper = sweet_spot_upper_base - (item * sweet_spot_upper_item_scale)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += sweet_spot_reward\n\n        # Bin History: Penalize bins that have been filled recently more aggressively.\n        try:\n            bin_usage_history\n            usage_penalty = bin_usage_history[feasible_bins] * usage_penalty_factor\n            priorities[feasible_bins] -= usage_penalty\n        except NameError:\n            pass\n\n        # Quantile Penalty: Penalize using bins in lower quantiles more heavily.\n        if bin_quantiles is not None:\n            quantile_index = np.searchsorted(np.sort(bins_remain_cap), bins_remain_cap[feasible_bins])\n            quantile_penalty = quantile_index / len(bins_remain_cap) * quantile_penalty_factor\n            priorities[feasible_bins] -= quantile_penalty\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 3.9988033506182825,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter24_response1.txt_stdout.txt",
    "code_path": "problem_iter24_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive elements, focusing on simplicity and efficiency.\n    Emphasizes a balance between bin utilization and preventing fragmentation.\n    Includes bin history and adaptive exploration.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 1e-6)  # Tiny constant to avoid division by zero\n\n        # Adaptive Exploration: Reduced complexity, focusing on item size and number of feasible bins.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = 0.1 * item * np.sqrt(num_feasible) #Simplified exploration\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Simplified, focusing on bins that are very close to full.\n        almost_full_threshold = 0.1 #Tuneable parameter for what is considered almost full.\n        wasted_space_ratio = waste / item #Compare waste to item size\n        almost_full = wasted_space_ratio < almost_full_threshold\n        priorities[feasible_bins][almost_full] *= 0.5 #Reduce priority of almost full bins\n\n        #Sweet Spot Reward: Incentive for bins filled between 70-95% after insertion\n        sweet_spot_lower = 0.7\n        sweet_spot_upper = 0.95\n        new_fill = (bins_remain_cap[feasible_bins] - waste)\n        sweet_spot_bin = (new_fill >= sweet_spot_lower) & (new_fill <= sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot_bin] += 0.3\n\n        #Bin usage history penalty. Requires external bin_usage_history.\n        try:\n            bin_usage_history\n            usage_penalty = bin_usage_history[feasible_bins] * 0.02 #Simple penalty\n            priorities[feasible_bins] -= usage_penalty\n        except NameError:\n            pass\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 30.893498205025928,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter24_response2.txt_stdout.txt",
    "code_path": "problem_iter24_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).  Slightly adjusted to favor smaller waste more strongly.\n        priorities[feasible_bins] = 1 / (waste**0.75 + 1e-6)  # Tiny constant to avoid division by zero.  Waste to a fractional power\n\n        # Exploration: Adaptive exploration based on the relative number of feasible bins.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = 0.1 / (1 + num_feasible) # Exploration decreases as more bins are available.\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Target almost-full bins more intelligently.\n        remaining_capacity_ratio = bins_remain_cap[feasible_bins] / 1.0  # Assuming bin size is 1\n        almost_full = remaining_capacity_ratio < item * 1.25  # Dynamically adjust threshold based on item size\n        priorities[feasible_bins][almost_full] *= 0.5 #Significant penalty for using almost-full bins.\n\n        # Reward for filling bins significantly\n        fill_ratio = item / bins_remain_cap[feasible_bins]\n        significant_fill = fill_ratio > 0.75\n        priorities[feasible_bins][significant_fill] += 0.25\n\n        # Bin History: Penalize bins that have been filled recently.\n        try:\n            bin_usage_history #Test if the variable exists, exception otherwise\n            usage_penalty = bin_usage_history[feasible_bins] * 0.03\n            priorities[feasible_bins] -= usage_penalty\n        except NameError:\n            pass\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter24_response3.txt_stdout.txt",
    "code_path": "problem_iter24_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).  Weight it slightly less\n        priorities[feasible_bins] = 0.8 / (waste + 1e-5)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size. Reduce exploration\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.1, 0.02 * num_feasible * item)  # Capped exploration\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Target almost-full bins. Aggressive penalty for small waste.\n        waste_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = waste_ratio < 0.05 # Smaller waste threshold\n        priorities[feasible_bins][almost_full] -= 0.2  # Stronger penalty\n\n        # Reward for filling bins to a good level, based on item size\n        fill_ratio = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot_lower = 0.7 - (item * 0.15) #Adaptive sweet spot\n        sweet_spot_upper = 0.9 - (item * 0.05)\n\n        sweet_spot = (fill_ratio > sweet_spot_lower) & (fill_ratio < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4  #Reward sweet spot more\n\n        # Bin History: Penalize recently used bins.  Only consider if it exists\n        try:\n            bin_usage_history  # Check if it exists\n            normalized_usage = bin_usage_history[feasible_bins] / (np.max(bin_usage_history) + 1e-9) #Normalize\n            priorities[feasible_bins] -= 0.1 * normalized_usage #Moderate penalty\n\n        except NameError:\n            pass # ignore if it does not exist\n\n        # Bonus for larger bins that can fit the item comfortably\n        large_bin_bonus = np.where(bins_remain_cap[feasible_bins] > (1.5 * item), 0.3, 0) #Add bonus for significantly larger bins.\n        priorities[feasible_bins] += large_bin_bonus\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities",
    "response_id": 3,
    "tryHS": true,
    "obj": 1.1767052253689738,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter24_response4.txt_stdout.txt",
    "code_path": "problem_iter24_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).  Higher score for better fit\n        priorities[feasible_bins] = 1.0 / (waste + 1e-9)  # Tiny constant to avoid division by zero\n\n        # Exploration:  Reduce exploration as bin fullness increases.\n        bin_fullness = 1.0 - bins_remain_cap[feasible_bins]  # assuming bin capacity is 1.0\n        exploration_factor = 0.1 * (1.0 - bin_fullness) * item # Reduced overall exploration\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n\n        # Fragmentation Penalty: Focus on bins nearing full capacity.\n        almost_full_threshold = 0.9 # Increased threshold for almost full\n        almost_full = bins_remain_cap[feasible_bins] < (1 - almost_full_threshold)\n        priorities[feasible_bins][almost_full] *= 0.5  # Significant penalty, avoid filling nearly full bins\n\n        # Reward filling \"sweet spot\" to encourage efficient bin use.\n        sweet_spot_lower = 0.2 + 0.1*item # Item-dependent sweet spot\n        sweet_spot_upper = 0.7 - 0.1*item\n        bin_utilization = 1.0 - bins_remain_cap[feasible_bins]\n        sweet_spot = (bin_utilization > sweet_spot_lower) & (bin_utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.3  # Add reward for sweet spot.\n\n        # Small item bonus for bins with relatively large remaining capacity\n        small_item_threshold = 0.2\n        if item < small_item_threshold:\n            large_bin_bonus = bins_remain_cap[feasible_bins] > 0.5 #Give bonus only when remain cap is large\n            priorities[feasible_bins][large_bin_bonus] += 0.2\n\n        # Usage history penalty (if available).\n        try:\n            bin_usage_history  # Check if bin_usage_history exists\n            usage_penalty = bin_usage_history[feasible_bins] * 0.05  # Further reduced penalty\n            priorities[feasible_bins] -= usage_penalty\n        except NameError:\n            pass  # If bin_usage_history is not available, continue without it\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 86.58755484643,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter25_hs2.txt_stdout.txt",
    "code_path": "problem_iter25_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, best_fit_weight: float = 0.29688274177019125, exploration_base: float = 0.009667567451355424, exploration_cap: float = 0.12459150494248482,\n                almost_full_threshold: float = 0.09633898076132821, almost_full_penalty: float = 0.44205044697130974, sweet_spot_reward: float = 0.6680081513137437,\n                sweet_spot_lower_base: float = 0.6801426280346945, sweet_spot_lower_item_factor: float = 0.0924069226112605,\n                sweet_spot_upper_base: float = 0.8612557229616438, sweet_spot_upper_item_factor: float = 0.06780419588485531,\n                bin_history_penalty: float = 0.0331002589764816, large_bin_factor: float = 1.552469976403848, large_bin_bonus: float = 0.13498581267972265) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).  Weight it slightly less\n        priorities[feasible_bins] = best_fit_weight / (waste + 1e-5)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size. Reduce exploration\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(exploration_cap, exploration_base * num_feasible * item)  # Capped exploration\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Target almost-full bins. Aggressive penalty for small waste.\n        waste_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = waste_ratio < almost_full_threshold # Smaller waste threshold\n        priorities[feasible_bins][almost_full] -= almost_full_penalty  # Stronger penalty\n\n        # Reward for filling bins to a good level, based on item size\n        fill_ratio = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot_lower = sweet_spot_lower_base - (item * sweet_spot_lower_item_factor) #Adaptive sweet spot\n        sweet_spot_upper = sweet_spot_upper_base - (item * sweet_spot_upper_item_factor)\n\n        sweet_spot = (fill_ratio > sweet_spot_lower) & (fill_ratio < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += sweet_spot_reward  #Reward sweet spot more\n\n        # Bin History: Penalize recently used bins.  Only consider if it exists\n        try:\n            bin_usage_history  # Check if it exists\n            normalized_usage = bin_usage_history[feasible_bins] / (np.max(bin_usage_history) + 1e-9) #Normalize\n            priorities[feasible_bins] -= bin_history_penalty * normalized_usage #Moderate penalty\n\n        except NameError:\n            pass # ignore if it does not exist\n\n        # Bonus for larger bins that can fit the item comfortably\n        large_bin_bonus_condition = bins_remain_cap[feasible_bins] > (large_bin_factor * item)\n        large_bin_bonus_values = np.where(large_bin_bonus_condition, large_bin_bonus, 0)\n        priorities[feasible_bins] += large_bin_bonus_values\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 1.206621459912248,
    "exec_success": true
  }
]