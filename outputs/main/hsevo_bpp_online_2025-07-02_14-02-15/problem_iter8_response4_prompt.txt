{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes best-fit, adds stochasticity, penalizes fragmentation,\n    and considers bin fill ratio with dynamic scaling.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Prioritize based on inverse waste (best fit) - stronger preference\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        \n        # Add stochasticity (exploration) - reduced amplitude\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * 0.05\n\n        # Penalize almost full bins to prevent fragmentation - adaptive penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        priorities[feasible_bins][almost_full] *= 0.3 # Reduce priority more aggressively\n\n        # Reward larger bins if enough capacity exists - tweaked reward size\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item*1.5, 0.2, 0) #Reduced threshold for large cap, decreased magnitude\n        priorities[feasible_bins] += large_cap_reward\n\n        # Dynamic scaling based on bin fill ratio\n        fill_ratio = (bins_remain_cap[feasible_bins] - waste) / bins_remain_cap[feasible_bins]\n        priorities[feasible_bins] *= (1 + fill_ratio * 0.5)  # Boost priority for bins that will be well-filled\n\n        # Further penalty for bins that, after placing the item, would have capacity less than a threshold.\n        small_remaining = bins_remain_cap[feasible_bins] - item < 0.1\n        priorities[feasible_bins][small_remaining] = -np.inf #Make unfeasible\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins based on fit, waste, and stochasticity.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    eligible_bins = bins_remain_cap >= item\n\n    if not np.any(eligible_bins):\n        return priorities\n\n    # Fit Score: Prioritize bins that can fit the item. Exponential decay.\n    fit_mask = bins_remain_cap >= item\n    waste = bins_remain_cap - item\n    relative_waste = waste / (item + 1e-6)\n    priorities[fit_mask] += np.exp(-relative_waste[fit_mask])\n\n    # Stochastic Element: Introduce randomness for exploration.\n    exploration_factor = 0.01\n    priorities += np.random.rand(len(bins_remain_cap)) * exploration_factor\n\n    # Fragmentation Penalty: Penalize small gaps.\n    fragmentation_threshold = item * 0.2\n    fragmentation_penalty = np.where((waste > 0) & (waste < fragmentation_threshold), -0.5, 0)\n    priorities += fragmentation_penalty\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic `priority_v2` incorporates \"best-fit\" prioritization, stochasticity for exploration, fragmentation penalty, utilization sweet spot, and large item reward. The worst only considers fit score (exponential decay on relative waste), stochasticity, and fragmentation penalty.  (2nd best) vs (2nd worst) the code is similar. Comparing (1st) vs (2nd), we see the function are similar, therefore small adjustments to parameters can significantly impact performance. (3rd) vs (4th), we see that the 4th heuristic dynamically adjusts stochasticity and rewards significantly filled bins, while the 3rd one is simpler. Comparing (second worst) vs (worst), we see the codes are similar. Overall: the best heuristics incorporate more factors and adaptive parameters, while simpler heuristics focusing only on basic factors like waste tend to perform worse. The weighting of different factors, such as the strength of the fragmentation penalty or the magnitude of stochasticity, significantly impacts heuristic performance. Sweet spot and dynamic scaling of parameters are useful.\n- \nOkay, let's refine \"Current self-reflection\" to make it more actionable for designing better heuristics, avoiding the pitfalls of \"Ineffective self-reflection.\"\n\nHere's a revised approach:\n\n*   **Keywords:** Iterative refinement, empirical tuning, dynamic adaptation, balanced factors.\n*   **Advice:** Start simple, measure impact of added complexity, adjust based on problem state, and focus on performance metrics.\n*   **Avoid:** Unjustified complexity, pre-mature stochasticity, and complex non-linear functions without clear justification.\n*   **Explanation:** Build heuristics incrementally, focusing on demonstrable performance gains at each step. Prioritize clear, tunable parameters, and adaptive responses to problem characteristics.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}