[
  {
    "stdout_filepath": "problem_iter20_response0.txt_stdout.txt",
    "code_path": "problem_iter20_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive exploration and sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        priorities[feasible_bins] = 1 / (waste + 1e-9)\n\n        exploration_factor = 0.05 * item * np.sum(feasible_bins)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n        utilization = (bins_remain_cap[feasible_bins] - waste)\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.3\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 36.25847626645394,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response1.txt_stdout.txt",
    "code_path": "problem_iter20_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive exploration, dynamic sweet spot, and item-aware penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        num_feasible = np.sum(feasible_bins)\n        capacity_std = np.std(bins_remain_cap[feasible_bins])\n        exploration_factor = min(0.3, 0.05 * num_feasible + 0.1 * capacity_std)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.5 + 0.2 * item  # Larger items, higher penalty\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty)\n\n        sweet_spot_lower = 0.6 - (item * 0.2) - (capacity_std * 0.02)\n        sweet_spot_upper = 0.8 - (item * 0.1) + (capacity_std * 0.02)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 3.3506182688472412,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response2.txt_stdout.txt",
    "code_path": "problem_iter20_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive exploration and sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        priorities[feasible_bins] = np.minimum(priorities[feasible_bins], 50)\n\n        # Adaptive exploration based on item size and num feasible bins\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 * (1 - item) / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n        \n        # Dynamic sweet spot incentive based on item size\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response3.txt_stdout.txt",
    "code_path": "problem_iter20_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit, adaptive exploration, and item-aware fragmentation penalty.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 1e-6)\n\n        # Adaptive stochasticity, scaled by remaining capacity\n        stochasticity_scale = 0.1 * (1 - np.mean(bins_remain_cap[feasible_bins]) if len(bins_remain_cap[feasible_bins]) > 0 else 0)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_scale\n\n        # Item-aware fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.3 + 0.2 * item  # Larger items, higher penalty\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty)\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 8.37654567211807,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response4.txt_stdout.txt",
    "code_path": "problem_iter20_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Hybrid heuristic: Best-fit core, adaptive exploration, sweet spot, and fragmentation control.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-Fit Core\n        priorities[feasible_bins] = 1 / (waste + 1e-5)\n\n        # Adaptive Exploration: Scaled by item size and feasible bins\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.2, 0.05 * num_feasible * item)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Sweet Spot Incentive: Dynamic range based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.2)\n        sweet_spot_upper = 0.9 - (item * 0.1)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n\n        # Fragmentation Penalty: Target almost-full bins\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.08\n        priorities[feasible_bins][almost_full] *= 0.7\n\n        # Reward larger bins based on item size.\n        large_cap_factor = 1.5\n        large_cap_reward = 0.3\n        large_cap_reward_values = np.where(bins_remain_cap[feasible_bins] > item * large_cap_factor, large_cap_reward, 0)\n        priorities[feasible_bins] += large_cap_reward_values\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 1.2863980853609984,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response5.txt_stdout.txt",
    "code_path": "problem_iter20_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Best-fit core, adaptive exploration, sweet spot, and fragmentation control.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-Fit Core\n        priorities[feasible_bins] = 1 / (waste + 1e-5)\n\n        # Adaptive Exploration: Scaled by item size and feasible bins\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.2, 0.05 * num_feasible * item)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Sweet Spot Incentive: Dynamic range based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.2)\n        sweet_spot_upper = 0.9 - (item * 0.1)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n\n        # Fragmentation Penalty: Target almost-full bins\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.08\n        priorities[feasible_bins][almost_full] *= 0.7\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 3.7495013960909587,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response6.txt_stdout.txt",
    "code_path": "problem_iter20_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit, adaptive exploration, and dynamic sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive Exploration\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.2, 0.05 * num_feasible * (1 - item))\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Dynamic Sweet Spot Incentive\n        sweet_spot_lower = 0.6 - (item * 0.2)\n        sweet_spot_upper = 0.9 - (item * 0.1)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 35.999202233745514,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response7.txt_stdout.txt",
    "code_path": "problem_iter20_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive exploration and fragmentation control.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 1e-9)\n\n        # Adaptive Exploration\n        relative_item_size = item / bins_remain_cap[feasible_bins]\n        exploration_factor = 0.1 * (1 - relative_item_size)\n        exploration_factor = np.clip(exploration_factor, 0.01, 0.2)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n\n        # Fragmentation Penalty (Capacity-Aware)\n        common_item_sizes = np.array([0.2, 0.3, 0.4])\n        remaining_capacity_after_packing = waste\n        fragmentation_penalty = np.zeros_like(remaining_capacity_after_packing)\n        for size in common_item_sizes:\n            fragmentation_penalty += np.exp(-np.abs(remaining_capacity_after_packing - size) / 0.05)\n        priorities[feasible_bins] -= 0.05 * fragmentation_penalty\n\n        # Sweet Spot Incentive\n        sweet_spot_lower = 0.6 - (item * 0.3)\n        sweet_spot_upper = 0.9 - (item * 0.2)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.6\n\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 7.070203430394904,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response8.txt_stdout.txt",
    "code_path": "problem_iter20_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive stochasticity, and item-aware fragmentation penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        priorities[feasible_bins] = np.minimum(priorities[feasible_bins], 50)\n\n        # Adaptive stochasticity\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 * (1 - item) / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n\n        # Fragmentation penalty\n        wasted_space_ratio = waste / 1.0\n        almost_full = wasted_space_ratio < 0.1\n        priorities[feasible_bins][almost_full] *= 0.4\n\n        # Dynamic sweet spot incentive\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > 1.6 * item, 0.5, 0) #Increased reward and slightly larger bin requirement\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.497407259672929,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response9.txt_stdout.txt",
    "code_path": "problem_iter20_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity, item-aware fragmentation, and large bin rewards.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 1e-6)\n\n        # Adaptive stochasticity\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_scale = 0.1 * (1 - np.mean(bins_remain_cap[feasible_bins]) if num_feasible > 0 else 0)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_scale\n\n        # Item-aware fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.3 + 0.2 * item\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty)\n\n        # Reward for large bins relative to item size\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 2, 0.25, 0)\n        priorities[feasible_bins] += large_cap_reward\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 20.941364180295174,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter21_response0.txt_stdout.txt",
    "code_path": "problem_iter21_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_usage_history: np.ndarray = None) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).  Slightly more aggressive\n        priorities[feasible_bins] = 1 / (waste + 1e-6)**0.7 # Reduced exponent\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size. More targeted\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = 0.02 * min(0.5, item * num_feasible**0.5) # Reduced base exploration\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Stronger and more nuanced. Target almost-full bins.\n        almost_full_threshold = 0.05  # Slightly tighter\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold\n        priorities[feasible_bins][almost_full] *= 0.2  # Significant penalty\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.9 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n\n        # Bin History: Penalize bins that have been filled recently more aggressively.\n        if bin_usage_history is not None: # Require the argument to be passed in\n            usage_penalty = bin_usage_history[feasible_bins] * 0.1  # Adjust scaling\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.028719585161557,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter21_response1.txt_stdout.txt",
    "code_path": "problem_iter21_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_usage_history: np.ndarray = None) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 1e-6)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size.  Reduced scale to avoid over-exploration.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = 0.02 * min(1, num_feasible * item) # Clamped exploration and scaled by both item and num_feasible\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Targetting bins which are close to full AND small\n        almost_full_threshold = 0.1\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = (wasted_space_ratio < almost_full_threshold) & (bins_remain_cap[feasible_bins] < 0.5)  # More targeted. small and almost full\n        priorities[feasible_bins][almost_full] *= 0.2 # Significant penalty for using almost-full bins.\n\n        # Sweet Spot Incentive: Encourage utilization around 70-90%. Simplified and more robust.\n        utilization_lower = 0.7\n        utilization_upper = 0.9\n        utilization = (bins_remain_cap[feasible_bins] - waste)  # No need to divide by bin size if bin size == 1\n        sweet_spot = (utilization > utilization_lower) & (utilization < utilization_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4  # Flat reward\n\n        # Bin History: Penalize bins that have been filled recently.\n        if bin_usage_history is not None:\n            usage_penalty = bin_usage_history[feasible_bins] * 0.05 #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        \n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 3.9888312724371757,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter21_response2.txt_stdout.txt",
    "code_path": "problem_iter21_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability.\n\n    Simplified and refined for clarity and effectiveness.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).  Simplified tiny constant.\n        priorities[feasible_bins] = 1 / (waste + 1e-6)\n\n        # Exploration:  Reduced and simplified.  Only apply to feasible bins.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = 0.05 * item  # Smaller base exploration, scaled by item.\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n\n        # Fragmentation Penalty:  Focus on almost-full bins, simplified threshold.\n        almost_full_threshold = 0.1\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold\n        priorities[feasible_bins][almost_full] *= 0.2 # Reduced penalty value\n\n        # Sweet Spot Incentive:  Simplified range, focusing on higher utilization.\n        sweet_spot_lower = 0.7\n        sweet_spot_upper = 0.95\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.3\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 2.961707219784608,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter21_response3.txt_stdout.txt",
    "code_path": "problem_iter21_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 1e-9)  # Tiny constant to avoid division by zero\n\n        # Exploration: Add some randomness to explore different options.  Reduced exploration.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = 0.01 * item  # Further reduced and scale by item size\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Target almost-full bins.  Simplified logic.\n        almost_full_threshold = 0.1\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold\n        priorities[feasible_bins][almost_full] *= 0.5  # Significant penalty for using almost-full bins.\n\n        # Reward for placing smaller items into bins with larger remaining capacity. Simplified scaling.\n        if item < 0.2:  # Only apply for smaller items\n            large_bin_threshold = 0.7\n            large_bin = bins_remain_cap[feasible_bins] > large_bin_threshold\n            priorities[feasible_bins][large_bin] += 0.3  # Increased reward for large bins\n\n        # Sweet spot incentive, simplified.\n        sweet_spot_lower = 0.6 - (0.1 * item)\n        sweet_spot_upper = 0.9 - (0.05 * item)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.2\n\n        # Bin History: Penalize bins that have been filled recently more aggressively.\n        # This requires an external mechanism (not part of this function) to track bin usage.\n        # This is a placeholder:  You'd need to maintain 'bin_usage_history' externally.\n        # Example: bin_usage_history = np.zeros_like(bins_remain_cap) #Initialize to all zeros\n        # Higher values = recently used more.\n\n        #In a separate step, you would update this value: bin_usage_history[chosen_bin] += 1\n\n        #The following assumes that bin_usage_history exist in the scope.\n        try:\n            bin_usage_history #Test if the variable exists, exception otherwise\n            usage_penalty = bin_usage_history[feasible_bins] * 0.05 #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        except NameError:\n            pass #If it doesn't exist, continue without this feature.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 3.131232548863192,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter21_response4.txt_stdout.txt",
    "code_path": "problem_iter21_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    V2: Simplifies and focuses on core components with refined parameters.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).  Slightly more aggressive.\n        tiny_constant = 1e-06  #Keep it very small\n        priorities[feasible_bins] = 1 / (waste + tiny_constant)\n\n        # Adaptive Stochasticity: Exploration based on remaining capacity and item size.\n        num_feasible = np.sum(feasible_bins)\n        exploration_base = 0.05\n        max_exploration = 0.2\n        exploration_factor = min(max_exploration, exploration_base * (1- bins_remain_cap[feasible_bins].mean()) * item ) #Exploration scales with item size and how full the bins are.\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Target almost-full bins, tuned threshold and penalty\n        almost_full_threshold = 0.1  #Slightly higher threshold.\n        almost_full_penalty = 0.2 #Increase the penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold\n        priorities[feasible_bins][almost_full] *= (1-almost_full_penalty)  # Apply Penalty\n\n        # Rewarding larger bins for smaller items - adjusted parameters\n        small_item_bin_multiple = 1.5 #Slightly less restrictive\n        small_item_reward = 0.5 #Reduce reward\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > small_item_bin_multiple * item, small_item_reward, 0)\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n        # Dynamic \"Sweet Spot\" Incentive: Simplified and more focused.\n        sweet_spot_lower_base = 0.6\n        sweet_spot_upper_base = 0.9\n        sweet_spot_reward = 0.3\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower_base) & (utilization < sweet_spot_upper_base)\n        priorities[feasible_bins][sweet_spot] += sweet_spot_reward\n\n        # Bin History:  Simplified penalty.\n        try:\n            bin_usage_history\n            usage_penalty_factor = 0.1 # Moderate penalty.\n            usage_penalty = bin_usage_history[feasible_bins] * usage_penalty_factor\n            priorities[feasible_bins] -= usage_penalty\n        except NameError:\n            pass\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 33.96489828480257,
    "exec_success": true
  }
]