```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray, bin_usage_history: np.ndarray = None) -> np.ndarray:
    """Prioritizes best-fit, adaptive exploration, and sweet spot with bin history."""

    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    feasible_bins = bins_remain_cap >= item

    if np.any(feasible_bins):
        waste = bins_remain_cap[feasible_bins] - item

        # Best-fit prioritization (modified scaling)
        priorities[feasible_bins] = 5 / (waste + 0.0001)  # Reduced scale
        priorities[feasible_bins] = np.minimum(priorities[feasible_bins], 25)  # Cap the priority

        # Adaptive exploration (item-dependent, num_feasible bins)
        num_feasible = np.sum(feasible_bins)
        stochasticity_factor = 0.05 * (1 - item) / (num_feasible + 0.1)  # Further scaled down
        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor

        # Dynamic sweet spot incentive (item-dependent)
        sweet_spot_lower = 0.6 - (item * 0.05) # Reduced sensitivity to item size
        sweet_spot_upper = 0.8 - (item * 0.025) # Reduced sensitivity to item size

        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0
        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)
        priorities[feasible_bins][sweet_spot] += 0.3  # Reduced reward

        # Bin History: Penalize bins that have been filled recently.
        if bin_usage_history is not None:
            usage_penalty = bin_usage_history[feasible_bins] * 0.03 #Scaling factor can be tuned. Reduce scale
            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.

    else:
        priorities[:] = -np.inf

    return priorities
```
