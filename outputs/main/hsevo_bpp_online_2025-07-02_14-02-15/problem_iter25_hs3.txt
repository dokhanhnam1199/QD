import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray, best_fit_weight: float = 0.29688274177019125, exploration_base: float = 0.008884373177713715, exploration_cap: float = 0.21409272999240947,
                almost_full_threshold: float = 0.04856311946672973, almost_full_penalty: float = 0.34145980387561503, sweet_spot_reward: float = 0.739739851835232,
                sweet_spot_lower_base: float = 0.8399660102568508, sweet_spot_lower_item_factor: float = 0.08655948572147563,
                sweet_spot_upper_base: float = 0.8612557229616438, sweet_spot_upper_item_factor: float = 0.14636524763788616,
                bin_history_penalty: float = 0.13647118908569544, large_bin_factor: float = 1.721640119708729, large_bin_bonus: float = 0.25673854656599104) -> np.ndarray:
    """Prioritizes best-fit with adaptive penalties and dynamic exploration.
    Emphasizes a balance between bin utilization and preventing extreme fragmentation,
    adjusting strategies based on item size and bin availability. Includes bin history.
    """

    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    feasible_bins = bins_remain_cap >= item

    if np.any(feasible_bins):
        waste = bins_remain_cap[feasible_bins] - item
        
        # Core: Prioritize best fit (minimize waste).  Weight it slightly less
        priorities[feasible_bins] = best_fit_weight / (waste + 1e-5)  # Tiny constant to avoid division by zero

        # Adaptive Stochasticity: Exploration based on feasibility and item size. Reduce exploration
        num_feasible = np.sum(feasible_bins)
        exploration_factor = min(exploration_cap, exploration_base * num_feasible * item)  # Capped exploration
        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor

        # Fragmentation Penalty: Target almost-full bins. Aggressive penalty for small waste.
        waste_ratio = waste / bins_remain_cap[feasible_bins]
        almost_full = waste_ratio < almost_full_threshold # Smaller waste threshold
        priorities[feasible_bins][almost_full] -= almost_full_penalty  # Stronger penalty

        # Reward for filling bins to a good level, based on item size
        fill_ratio = (bins_remain_cap[feasible_bins] - waste) / 1.0
        sweet_spot_lower = sweet_spot_lower_base - (item * sweet_spot_lower_item_factor) #Adaptive sweet spot
        sweet_spot_upper = sweet_spot_upper_base - (item * sweet_spot_upper_item_factor)

        sweet_spot = (fill_ratio > sweet_spot_lower) & (fill_ratio < sweet_spot_upper)
        priorities[feasible_bins][sweet_spot] += sweet_spot_reward  #Reward sweet spot more

        # Bin History: Penalize recently used bins.  Only consider if it exists
        try:
            bin_usage_history  # Check if it exists
            normalized_usage = bin_usage_history[feasible_bins] / (np.max(bin_usage_history) + 1e-9) #Normalize
            priorities[feasible_bins] -= bin_history_penalty * normalized_usage #Moderate penalty

        except NameError:
            pass # ignore if it does not exist

        # Bonus for larger bins that can fit the item comfortably
        large_bin_bonus_condition = bins_remain_cap[feasible_bins] > (large_bin_factor * item)
        large_bin_bonus_values = np.where(large_bin_bonus_condition, large_bin_bonus, 0)
        priorities[feasible_bins] += large_bin_bonus_values

    else:
        priorities[:] = -np.inf  # No feasible bins

    return priorities
