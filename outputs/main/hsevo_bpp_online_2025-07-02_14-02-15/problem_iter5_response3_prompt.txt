{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Hybrid heuristic: potential well + relative waste + stochasticity.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    eligible_bins = bins_remain_cap >= item\n\n    if not np.any(eligible_bins):\n        return priorities\n\n    # Potential well around near-perfect fits.\n    fit_difference = bins_remain_cap - item\n    near_fit_mask = fit_difference >= 0\n    priorities[near_fit_mask] = np.exp(-np.abs(fit_difference[near_fit_mask]) / (item + 1e-6))\n\n    # Relative wasted space penalty.\n    wasted_space = bins_remain_cap - item\n    priorities[eligible_bins] -= 0.5 * (wasted_space[eligible_bins] / (bins_remain_cap[eligible_bins] + 1e-6))\n\n    # Stochastic exploration: favor fuller bins with small probability.\n    fill_level = 1 - (bins_remain_cap / np.max(bins_remain_cap))\n    priorities += 0.01 * np.random.rand(len(bins_remain_cap)) * fill_level**2\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Fit Score: Prioritize bins that can fit the item. Non-linear scaling.\n    fit_mask = bins_remain_cap >= item\n    waste = bins_remain_cap - item\n    relative_waste = waste / item\n    priorities[fit_mask] += np.exp(-relative_waste[fit_mask])  # Exponential decay of priority as waste increases\n\n    # Capacity Utilization: Encourage filling bins close to full.\n    utilization = (bins_remain_cap - item) / bins_remain_cap\n    utilization = np.clip(utilization, 0, 1)  # Ensure utilization is between 0 and 1\n    priorities[fit_mask] += (1-utilization[fit_mask])**2\n\n    # Fragmentation Penalty: Discourage leaving small gaps.  Larger gaps penalized more.\n    fragmentation_threshold = item * 0.2 #Define threshold relative to item size\n    fragmentation_penalty = np.where((waste > 0) & (waste < fragmentation_threshold), -0.5, 0)\n    priorities += fragmentation_penalty\n\n    # Stochastic Element: Introduce randomness for exploration.\n    exploration_factor = 0.01  # Low value to limit randomness\n    priorities += np.random.rand(len(bins_remain_cap)) * exploration_factor\n\n\n    # Large item Reward\n    large_cap_reward = np.where(bins_remain_cap > item * 1.5, np.tanh(bins_remain_cap / item), 0)  # tanh to bound reward\n    priorities += large_cap_reward\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the top heuristics aren't penalized for number of bins but do implement fragmentation and waste penalties. (2nd) vs (19th) are identical. (3rd) vs (18th) are identical. Comparing (1st) vs (2nd), we see no difference. Comparing (3rd) vs (4th), we see no difference. Comparing (second worst) vs (worst), we see no difference. Overall: The initial heuristics (1st-5th) are identical, and the later heuristics introduce a range of penalty and reward mechanisms, including fragmentation penalties, stochastic elements, and incentives for filling larger spaces, number of bins penalty. The functions incorporate diverse strategies, such as non-linear scaling and exploration factors to optimize bin usage.\n- \nOkay, let's redefine \"Current self-reflection\" to be more effective in guiding heuristic design.\n\nHere's a breakdown:\n\n*   **Keywords:** Incremental improvement, base heuristic, parameter tuning, empirical validation.\n*   **Advice:** Begin with a solid, easily understood core heuristic. Introduce enhancements (e.g., fragmentation handling, specialized rules) one at a time. Meticulously adjust parameters using thorough testing.\n*   **Avoid:** Unnecessary complexity from the outset, premature optimization, reliance on intuition without data.\n*   **Explanation:** Focus on controlled experimentation and data-driven adjustments. Add complexity gradually, verifying each addition's benefit through rigorous empirical validation. This prevents over-engineering and ensures each component contributes meaningfully.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}