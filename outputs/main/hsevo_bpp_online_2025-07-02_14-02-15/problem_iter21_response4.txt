```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Prioritizes best-fit with adaptive penalties and dynamic exploration.
    Emphasizes a balance between bin utilization and preventing extreme fragmentation,
    adjusting strategies based on item size and bin availability. Includes bin history.
    V2: Simplifies and focuses on core components with refined parameters.
    """

    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    feasible_bins = bins_remain_cap >= item

    if np.any(feasible_bins):
        waste = bins_remain_cap[feasible_bins] - item
        
        # Core: Prioritize best fit (minimize waste).  Slightly more aggressive.
        tiny_constant = 1e-06  #Keep it very small
        priorities[feasible_bins] = 1 / (waste + tiny_constant)

        # Adaptive Stochasticity: Exploration based on remaining capacity and item size.
        num_feasible = np.sum(feasible_bins)
        exploration_base = 0.05
        max_exploration = 0.2
        exploration_factor = min(max_exploration, exploration_base * (1- bins_remain_cap[feasible_bins].mean()) * item ) #Exploration scales with item size and how full the bins are.
        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor

        # Fragmentation Penalty: Target almost-full bins, tuned threshold and penalty
        almost_full_threshold = 0.1  #Slightly higher threshold.
        almost_full_penalty = 0.2 #Increase the penalty
        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]
        almost_full = wasted_space_ratio < almost_full_threshold
        priorities[feasible_bins][almost_full] *= (1-almost_full_penalty)  # Apply Penalty

        # Rewarding larger bins for smaller items - adjusted parameters
        small_item_bin_multiple = 1.5 #Slightly less restrictive
        small_item_reward = 0.5 #Reduce reward
        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > small_item_bin_multiple * item, small_item_reward, 0)
        priorities[feasible_bins] += small_item_large_bin_reward

        # Dynamic "Sweet Spot" Incentive: Simplified and more focused.
        sweet_spot_lower_base = 0.6
        sweet_spot_upper_base = 0.9
        sweet_spot_reward = 0.3
        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1
        sweet_spot = (utilization > sweet_spot_lower_base) & (utilization < sweet_spot_upper_base)
        priorities[feasible_bins][sweet_spot] += sweet_spot_reward

        # Bin History:  Simplified penalty.
        try:
            bin_usage_history
            usage_penalty_factor = 0.1 # Moderate penalty.
            usage_penalty = bin_usage_history[feasible_bins] * usage_penalty_factor
            priorities[feasible_bins] -= usage_penalty
        except NameError:
            pass

    else:
        priorities[:] = -np.inf  # No feasible bins

    return priorities
```
