[
  {
    "stdout_filepath": "problem_iter11_response0.txt_stdout.txt",
    "code_path": "problem_iter11_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_size_assumption: float = 1.0) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with dynamic sweet spot and fragmentation control.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        priorities[feasible_bins] = np.minimum(priorities[feasible_bins], 50)\n\n        # Adaptive stochasticity\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n\n        # Fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        priorities[feasible_bins][almost_full] *= 0.4\n\n        # Dynamic sweet spot incentive\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / bin_size_assumption\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response1.txt_stdout.txt",
    "code_path": "problem_iter11_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive fragmentation penalty, and dynamic sweet spot incentive.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    waste = bins_remain_cap[feasible_bins] - item\n\n    # Best-fit prioritization\n    priorities[feasible_bins] = 1.0 / (waste + 0.0001)\n\n    # Adaptive fragmentation penalty (stronger for larger items)\n    wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n    penalty_threshold = 0.1 + (item * 0.05) #Adaptive threshold\n    nearly_full = wasted_space_ratio < penalty_threshold\n    priorities[feasible_bins][nearly_full] -= 0.5 * (1 + item)  # Scale penalty by item size\n\n    # Dynamic \"sweet spot\" incentive\n    sweet_spot_lower = 0.6 - (item * 0.2)\n    sweet_spot_upper = 0.8 - (item * 0.1)\n    utilization = item / bins_remain_cap[feasible_bins]\n    sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n    priorities[feasible_bins][sweet_spot] += 0.5\n\n    # Adaptive stochasticity (less exploration when fewer bins are available)\n    num_feasible = np.sum(feasible_bins)\n    exploration_factor = min(0.2, 0.5 / (num_feasible + 1)) # Inverse exploration\n    priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response2.txt_stdout.txt",
    "code_path": "problem_iter11_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Best-fit, adaptive stochasticity, dynamic fragmentation penalty, adaptive large item reward.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        \n        # Adaptive stochasticity\n        exploration_factor = max(0.01, 0.1 * np.mean(bins_remain_cap))\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n        \n        # Dynamic fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        priorities[feasible_bins][almost_full] *= 0.2 # Stronger penalty\n        \n        # Adaptive large item reward\n        large_cap_threshold = item * 1.25\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > large_cap_threshold, 0.5, 0)\n        priorities[feasible_bins] += large_cap_reward\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 2.4730753889110444,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response3.txt_stdout.txt",
    "code_path": "problem_iter11_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit, adaptive stochasticity, dynamic fragmentation penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        priorities[feasible_bins] = 1 / (waste + 0.001)\n\n        # Adaptive stochasticity: smaller items, more exploration.\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * (0.05 / (item + 0.1))\n\n        # Dynamic fragmentation penalty, relative to item size.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < (0.1 + 0.05*item)\n        priorities[feasible_bins][almost_full] *= 0.2\n\n        # Incentivize bins towards a target utilization range\n        target_utilization_low = 0.6 - 0.1 * item\n        target_utilization_high = 0.8 - 0.05*item\n        \n        utilization = (bins_remain_cap[feasible_bins] - waste) \n        utilization /= 1.0 \n        sweet_spot = (utilization >= target_utilization_low) & (utilization <= target_utilization_high)\n        priorities[feasible_bins][sweet_spot] += 0.3\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response4.txt_stdout.txt",
    "code_path": "problem_iter11_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and fragmentation control.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive Stochasticity\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.2, 0.02 * num_feasible)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation penalty (stronger for larger items)\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.05\n        penalty_factor = 0.2 + item * 0.1 #item size adjusts the penalty dynamically.\n        priorities[feasible_bins][almost_full] *= penalty_factor\n        priorities[feasible_bins][almost_full] = np.clip(priorities[feasible_bins][almost_full], 0, 1) #prevent overflow\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > 1.5 * item, 0.4, 0)\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 1.1069006781013186,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response5.txt_stdout.txt",
    "code_path": "problem_iter11_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive stochasticity, and dynamic fragmentation penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization with small constant\n        priorities[feasible_bins] = 1 / (waste + 0.0001)\n\n        # Adaptive stochasticity based on the number of feasible bins\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 / (num_feasible + 1e-6)  # Reduce exploration as bins fill\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_factor\n\n        # Dynamic fragmentation penalty based on item size\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.05\n        penalty_factor = 0.3 + 0.2 * item  # Larger items incur a stronger penalty\n        priorities[feasible_bins][almost_full] *= (1- min(penalty_factor, 0.5))  # cap to 0.5\n\n        # Reward significantly filled bins\n        fill_ratio = (bins_remain_cap[feasible_bins] - waste) / bins_remain_cap[feasible_bins]\n        significantly_filled = fill_ratio > 0.5\n        priorities[feasible_bins][significantly_filled] += 0.2\n\n        # Large item reward if sufficient capacity exists, dynamic threshold\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * (1.5 + 0.5 * item), 0.25, 0) #threshold adapts to item size\n        priorities[feasible_bins] += large_cap_reward\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.058635819704831,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response6.txt_stdout.txt",
    "code_path": "problem_iter11_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and fragmentation penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Prioritize based on inverse waste (best fit)\n        priorities[feasible_bins] = 1 / (waste + 0.0001)\n\n        # Adaptive stochasticity: less exploration when bins are fuller.\n        stochasticity_scale = 0.1 * (1 - np.mean(bins_remain_cap[feasible_bins]) if len(bins_remain_cap[feasible_bins]) > 0 else 0)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_scale\n\n        # Penalize almost full bins, scaled by item size.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.5 + 0.2 * item  # Larger items, higher penalty\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty) # Ensure not negative\n\n        # Large item reward: only if there's significantly more space.\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 2, 0.5, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n        # Sweet spot incentive (adaptive range)\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0 #Assuming bin size is 1\n        sweet_spot_lower = 0.5\n        sweet_spot_upper = 0.75\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.3\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 21.26047068209015,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response7.txt_stdout.txt",
    "code_path": "problem_iter11_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with dynamic sweet spot and adaptive penalty.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive Stochasticity: less aggressive as bins fill.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.1, 0.01 * num_feasible)\n        if np.random.rand() < 0.2:  # Add stochasticity only sometimes\n            priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Stronger when item is large.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.7 + 0.2 * item #up to 20% stronger penalty depending on item size\n        priorities[feasible_bins][almost_full] *= penalty\n\n        # Reward for large capacity\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 1.5, 0.2, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.2\n\n        priorities[feasible_bins] *= (1 + 0.05 * item) #Up to 5% best fit increase based on item size\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 1.6553649780614303,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response8.txt_stdout.txt",
    "code_path": "problem_iter11_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit, dynamically adjusts fragmentation penalty, \n    and provides adaptive large bin incentives.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Prioritize based on inverse waste (best fit)\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n\n        # Adaptive stochasticity (reduced exploration as bins fill)\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = max(0, 0.1 - 0.01 * num_feasible)  # Reduce exploration with more options\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Penalize almost full bins dynamically based on item size\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty_factor = 0.2 + item * 0.5 # Adjust sensitivity of almost full depending on size.\n        priorities[feasible_bins][almost_full] *= (1 - penalty_factor)\n        \n        #Dynamically incentivize larger bins if remaining capacity is high enough\n        large_cap_threshold = item * (1.2 + item * 0.4)\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > large_cap_threshold, 0.4 + item * 0.2, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.058635819704831,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response9.txt_stdout.txt",
    "code_path": "problem_iter11_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and fragmentation handling.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization.\n        priorities[feasible_bins] = np.minimum(10 / (waste + 0.0001), 50)\n\n        # Adaptive stochasticity: Less exploration with more feasible bins.\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_factor\n\n        # Fragmentation penalty: Apply a moderate penalty for almost-full bins.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        priorities[feasible_bins][almost_full] *= 0.4\n\n        # Reward filling bins well.\n        fill_ratio = item / bins_remain_cap[feasible_bins]\n        good_fill = (fill_ratio > 0.7) & (fill_ratio <= 1.0)\n        priorities[feasible_bins][good_fill] += 0.5\n\n        # Large item reward.\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 1.25, 0.25, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n        # Overfill penalty.\n        overfill_penalty = np.where(fill_ratio > 1, -1, 0)\n        priorities[feasible_bins] += overfill_penalty\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.058635819704831,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response0.txt_stdout.txt",
    "code_path": "problem_iter12_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability.  Version 2 focuses on simplified adaptivity\n    and more targeted fragmentation control.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Core: Prioritize best fit (minimize waste).  Simplified inverse waste calculation.\n        priorities[feasible_bins] = 1 / (waste + 0.0001)\n\n        # Adaptive Exploration: Scale exploration based on item size.  Larger items get less exploration.\n        exploration_factor = 0.1 * (1 - item)  # Reduced overall exploration and scaled by item size.\n        num_feasible = np.sum(feasible_bins)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Targeted Fragmentation Penalty: Only penalize bins that would become *very* full.\n        remaining_capacity_ratio = waste / 1.0 #Ratio to bin capacity, binsize fixed at 1.\n        almost_full = remaining_capacity_ratio < 0.1 #Bins with > 90% utilisation.\n        priorities[feasible_bins][almost_full] *= 0.5  # Increased penalty for almost-full bins. More direct penalty.\n\n        # Sweet Spot Incentive: A fixed sweet spot, but only applied if it improves utilization.\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > 0.6) & (utilization < 0.8)  # Fixed sweet spot.\n        improvement = utilization[sweet_spot] > item # Only add sweetspot, if it improves.\n        priorities[feasible_bins][sweet_spot] += 0.3 # Increased the reward.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 3.8791384124451627,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response1.txt_stdout.txt",
    "code_path": "problem_iter12_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration,\n    enhanced with bin-aware and item-aware adjustments. Aims for a more robust\n    and efficient bin packing strategy.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).  More aggressive\n        priorities[feasible_bins] = 1 / (waste + 0.00001)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Less stochasticity if bins are scarce.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.1, 0.01 * num_feasible)  # Reduced exploration, caps at 0.1\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty:  Stronger and more targeted.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.05 #More aggressive\n        priorities[feasible_bins][almost_full] *= 0.1  # Significant penalty for using almost-full bins. Stronger penalty\n\n        # Rewarding larger bins for smaller items - Less aggressive\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > 1.5 * item, 0.2, 0) #Reduced the reward.\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size.\n        # Aim is to encourage utilization in a range that avoids both extreme fragmentation\n        # and under-utilization.  Smaller items should aim for higher utilization.\n        sweet_spot_lower = 0.6 - (item * 0.15) #Dynamic Lower Bound - decreased the impact of item size\n        sweet_spot_upper = 0.8 - (item * 0.05) #Dynamic Upper Bound - decreased the impact of item size, less aggressively.\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.3 #Reduced the reward\n\n        # Bin-Aware Capacity Reward: Give a slight preference to bins with larger remaining capacity if best-fit difference isn't large\n        capacity_advantage = bins_remain_cap[feasible_bins] / np.max(bins_remain_cap[feasible_bins])\n        priorities[feasible_bins] += 0.05 * capacity_advantage # Reduced to 0.05.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 2.911846828879143,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response2.txt_stdout.txt",
    "code_path": "problem_iter12_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. This version\n    focuses on simplified adaptive components and a more robust handling of edge cases.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 0.00001)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity:  Exploration proportional to available capacity, capped.\n        total_capacity = np.sum(bins_remain_cap)\n        exploration_factor = min(0.1, total_capacity * 0.001) # Proportional to total remaining capacity\n        num_feasible = np.sum(feasible_bins)        \n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty:  Focus on *relative* wasted space. Less aggressive, more stable.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1 # Less aggressive threshold\n        priorities[feasible_bins][almost_full] *= 0.5  # Reduced penalty\n\n        # Reward larger bins for small items, but only if it's a *significant* size difference.\n        if item < 0.3: #Only triggers when dealing with small items\n          large_bin_reward = bins_remain_cap[feasible_bins] > (1.0 - item) #is it approaching full bin?\n          priorities[feasible_bins][large_bin_reward] += 0.3\n        \n\n        # Dynamic \"Sweet Spot\" Incentive:  Simplified, based on item size.\n        sweet_spot_lower = 0.6 - (item * 0.15)  #Dynamic Lower Bound - adjusted scale\n        sweet_spot_upper = 0.9 - (item * 0.1) #Dynamic Upper Bound - decreased upper bound\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.3\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.307937774232155,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response3.txt_stdout.txt",
    "code_path": "problem_iter12_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration,\n    incorporating bin fill level awareness and targeted fragmentation control.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 0.00001)\n\n        # Adaptive Stochasticity: Item size dictates exploration. Larger items, less exploration.\n        exploration_factor = max(0, 0.1 - (item * 0.05))  #exploration decreases as item increases\n        num_feasible = np.sum(feasible_bins)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Bin Fill Level Awareness: Reward bins close to full but still feasible.\n        fill_ratio = (1.0 - bins_remain_cap[feasible_bins]) #Assuming bin size is 1\n        almost_full = (fill_ratio > 0.7)\n        priorities[feasible_bins][almost_full] += 0.3 #Boost priority\n\n        # Targeted Fragmentation Control: Penalize creating small remainders, ONLY if alternative bins exist\n        small_waste = (waste < 0.1) #waste is smaller than 0.1\n        \n        if np.sum(feasible_bins) > 1: #check for alternative\n             priorities[feasible_bins][small_waste] *= 0.5 #reduce priority if waste is too small and alternates exist\n\n        # Dynamic \"Sweet Spot\" Incentive\n        sweet_spot_lower = 0.7 - (item * 0.1) #Slightly adjust sweet spot\n        sweet_spot_upper = 0.9 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response4.txt_stdout.txt",
    "code_path": "problem_iter12_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties, dynamic exploration, and bin-aware adjustments.\n    Emphasizes bin utilization, prevents fragmentation, and balances exploration based on item size\n    and remaining bin capacities. Focuses on simplicity and targeted adaptation.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste). More pronounced best-fit.\n        priorities[feasible_bins] = 10 / (waste + 0.00001)  # Increased the impact of best-fit\n\n        # Adaptive Stochasticity: Exploration decreases with item size and low remaining capacities\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.1, 0.01 * num_feasible * (1 - item)) #Reduced overall exploration & item aware\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Target almost-full bins dynamically scaled.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < (0.05 + 0.02 * item)  #Dynamically increase threshold for larger items.\n        priorities[feasible_bins][almost_full] *= 0.1  # Increased penalty.\n\n        # Rewarding larger bins for smaller items. More focused reward.\n        large_bin_threshold = 1.2 + 0.1*item\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > large_bin_threshold, 0.3, 0) # Slightly reduced reward\n        priorities[feasible_bins] += small_item_large_bin_reward\n        \n        # Dynamic \"Sweet Spot\" Incentive: Adapted Sweet Spot.\n        sweet_spot_lower = 0.6 - (item * 0.15)  # Adjusted\n        sweet_spot_upper = 0.85 - (item * 0.05) # Adjusted\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.3 #Increased reward slightly\n\n        #Bin Awareness - Encourage filling bins with lower remaining capacity\n        bin_capacity_rank = np.argsort(bins_remain_cap[feasible_bins])\n        capacity_incentive = np.linspace(0,0.15, num = len(bin_capacity_rank)) #Slight linear incentive.\n\n        priorities[feasible_bins][bin_capacity_rank] += capacity_incentive\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 28.57997606701238,
    "exec_success": true
  }
]