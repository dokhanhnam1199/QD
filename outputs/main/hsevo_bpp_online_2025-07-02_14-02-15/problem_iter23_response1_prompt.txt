{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 1e-6)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size.  Reduced scale to avoid over-exploration.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = 0.02 * min(1, num_feasible * item) # Clamped exploration and scaled by both item and num_feasible\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Targetting bins which are close to full AND small\n        almost_full_threshold = 0.1\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = (wasted_space_ratio < almost_full_threshold) & (bins_remain_cap[feasible_bins] < 0.5)  # More targeted. small and almost full\n        priorities[feasible_bins][almost_full] *= 0.2 # Significant penalty for using almost-full bins.\n\n        # Sweet Spot Incentive: Encourage utilization around 70-90%. Simplified and more robust.\n        utilization_lower = 0.7\n        utilization_upper = 0.9\n        utilization = (bins_remain_cap[feasible_bins] - waste)  # No need to divide by bin size if bin size == 1\n        sweet_spot = (utilization > utilization_lower) & (utilization < utilization_upper)\n        priorities[feasible_bins][sweet_spot] += 0.4  # Flat reward\n\n        # Bin History: Penalize bins that have been filled recently.\n        if bin_usage_history is not None:\n            usage_penalty = bin_usage_history[feasible_bins] * 0.05 #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        \n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes best-fit with adaptive exploration and sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        priorities[feasible_bins] = np.minimum(priorities[feasible_bins], 50)\n\n        # Adaptive exploration based on item size and num feasible bins\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 * (1 - item) / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n        \n        # Dynamic sweet spot incentive based on item size\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the 1st has a much more complex and adaptive system for exploration, fragmentation penalty, and sweet spot incentive, including parameter tuning and bin history, while the 20th uses simpler, static calculations.\nComparing (2nd best) vs (second worst), we see 2nd has the bin history, while 19th focuses on the core components, best-fit, adaptive exploration, and a dynamic sweet spot.\nComparing (1st) vs (2nd), we see the difference is the presence of explicit default argument values and imports of unnecessary libraries like `math`, `scipy`, and `torch` in the 2nd, which do not affect the logic.\nComparing (3rd) vs (4th), we see that the 3rd uses dynamic sweet spot incentive ranges and includes a reward for larger bins based on item size while the 4th uses a fixed sweet spot range and rewards small items in large bins. The 4th also attempts bin history but may fail because bin_usage_history may not exist.\nComparing (second worst) vs (worst), we see that the 19th incorporates a dynamic sweet spot and item-dependent exploration, while the 20th has a simpler exploration factor and sweet spot calculation. Overall: The better heuristics are characterized by more sophisticated adaptive strategies, parameter tuning, and consideration of bin history (when available), whereas the poorer ones rely on simplified static calculations and lack item-specific adjustments.\n- \nHere's a refined perspective on self-reflection for heuristic design, aiming for effective strategies:\n\n*   **Keywords:** Adaptive, Empirical, Incremental, Balance.\n\n*   **Advice:** Focus on simple, adaptive components first. Validate each addition empirically before further complexity.\n\n*   **Avoid:** Over-parameterization, premature optimization, and complex, non-linear combinations of factors without clear justification.\n\n*   **Explanation:** Start with a baseline heuristic, then add adaptive elements iteratively, ensuring each addition demonstrably improves performance through rigorous testing. Prioritize understandability and maintainability.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}