```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Prioritizes best-fit with adaptive penalties and dynamic exploration.
    Emphasizes a balance between bin utilization and preventing extreme fragmentation,
    adjusting strategies based on item size and bin availability.  Version 2 focuses on simplified adaptivity
    and more targeted fragmentation control."""

    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    feasible_bins = bins_remain_cap >= item

    if np.any(feasible_bins):
        waste = bins_remain_cap[feasible_bins] - item

        # Core: Prioritize best fit (minimize waste).  Simplified inverse waste calculation.
        priorities[feasible_bins] = 1 / (waste + 0.0001)

        # Adaptive Exploration: Scale exploration based on item size.  Larger items get less exploration.
        exploration_factor = 0.1 * (1 - item)  # Reduced overall exploration and scaled by item size.
        num_feasible = np.sum(feasible_bins)
        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor

        # Targeted Fragmentation Penalty: Only penalize bins that would become *very* full.
        remaining_capacity_ratio = waste / 1.0 #Ratio to bin capacity, binsize fixed at 1.
        almost_full = remaining_capacity_ratio < 0.1 #Bins with > 90% utilisation.
        priorities[feasible_bins][almost_full] *= 0.5  # Increased penalty for almost-full bins. More direct penalty.

        # Sweet Spot Incentive: A fixed sweet spot, but only applied if it improves utilization.
        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0
        sweet_spot = (utilization > 0.6) & (utilization < 0.8)  # Fixed sweet spot.
        improvement = utilization[sweet_spot] > item # Only add sweetspot, if it improves.
        priorities[feasible_bins][sweet_spot] += 0.3 # Increased the reward.

    else:
        priorities[:] = -np.inf  # No feasible bins

    return priorities
```
