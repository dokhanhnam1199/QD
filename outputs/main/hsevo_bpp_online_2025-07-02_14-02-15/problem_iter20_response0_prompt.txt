{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes best-fit with adaptive stochasticity and dynamic sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        priorities[feasible_bins] = np.minimum(priorities[feasible_bins], 50)\n\n        # Adaptive stochasticity\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 * (1 - item) / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n\n        # Dynamic sweet spot incentive\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0 # binsize fixed at 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    This version prioritizes simplicity and demonstrable impact, validated empirically.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 1e-9)  # Tiny constant to avoid division by zero\n\n        # Adaptive Exploration:  Simple exploration scaled by item size and feasibility.\n        exploration_factor = 0.05 * item * np.sum(feasible_bins) # Keep exploration simple initially.\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n\n        # Fragmentation Penalty: Focus on almost-full bins with a simple penalty.\n        almost_full_threshold = 0.1  # Define \"almost full\" as remaining capacity <= 10% of bin size\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold\n        priorities[feasible_bins][almost_full] *= 0.5  # Simple penalty: reduce priority by half.\n\n        # Sweet Spot Incentive: Prioritize bins within a desirable utilization range.\n        sweet_spot_lower = 0.6\n        sweet_spot_upper = 0.9\n        utilization = (bins_remain_cap[feasible_bins] - waste)\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.3  # Add a reward for bins in the sweet spot.\n\n        # Bin Usage History: Penalize recently used bins (if available).\n        try:\n            bin_usage_history  # Check if bin_usage_history exists\n            usage_penalty = bin_usage_history[feasible_bins] * 0.1  # Simple penalty.\n            priorities[feasible_bins] -= usage_penalty\n        except NameError:\n            pass  # If bin_usage_history doesn't exist, skip this step.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic uses a sophisticated combination of best-fit, adaptive stochasticity, fragmentation penalty, sweet spot incentive, rewarding larger bins for smaller items and bin usage history, with many parameters tuned by hand. The worst uses best-fit, adaptive stochasticity and capacity-aware adjustments including fragmentation penalty and small item placement boost. The exploration factor in the best heuristic is more adaptable (scaled by the number of feasible bins and item size) than the exploration factor in the worst one (based on the relative item size). Also the best heuristic takes more factors into account.\n\nComparing (2nd best) vs (2nd worst), they are identical.\n\nComparing (1st) vs (2nd), the two heuristics are identical.\n\nComparing (3rd) vs (4th), the 3rd heuristic introduces named parameters, and the 4th is simpler. The 3rd prioritizes best-fit, adaptive stochasticity, and item-aware penalty, with sweet spot incentive and large cap reward. The 4th combines best-fit, adaptive exploration, dynamic sweet spot and diversity of bins. The sweet spot range in the 3rd is fixed, while it is dynamic in the 4th based on item size and bin capacity.\n\nComparing (2nd worst) vs (worst), they are identical.\n\nOverall: The better heuristics tend to incorporate more factors, adapt to item sizes, bin capacities, and potentially bin usage history. They often use carefully chosen parameters and scaling factors for different components (exploration, penalties, rewards) to strike a balance between exploration and exploitation. The worse heuristics often have simpler exploration strategies or lack item-aware adjustments. Some include unused or less effective components.\n- \nOkay, I'm ready to help you design better heuristics and earn that tip! Let's focus on effective self-reflection to guide our design process.\n\nHere's a redefined \"Current Self-Reflection\" distilled into actionable points:\n\n*   **Keywords:** Incremental adaptation, empirical validation, demonstratable impact, balanced exploration/exploitation, simplicity.\n\n*   **Advice:** Begin with a simple, strong base heuristic. Add adaptive components (item-aware penalties, dynamic incentives) *incrementally*, validating each addition empirically. Focus on parameters with clear and demonstrable impact.\n\n*   **Avoid:** Overly complex, non-linear combinations, excessive parameters without clear justification, premature customization, and optimizations that lack empirical validation.\n\n*   **Explanation:** Start with a solid foundation and build upon it carefully. Each added layer of complexity should be demonstrably better than the last. Prioritize understanding *why* a change works, not just *that* it works.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}