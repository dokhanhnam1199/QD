{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Best-fit, adaptive stochasticity, dynamic fragmentation penalty, adaptive large item reward.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        \n        # Adaptive stochasticity\n        exploration_factor = max(0.01, 0.1 * np.mean(bins_remain_cap))\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_factor\n        \n        # Dynamic fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        priorities[feasible_bins][almost_full] *= 0.2 # Stronger penalty\n        \n        # Adaptive large item reward\n        large_cap_threshold = item * 1.25\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > large_cap_threshold, 0.5, 0)\n        priorities[feasible_bins] += large_cap_reward\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes best-fit, dynamically adjusts fragmentation penalty, \n    and provides adaptive large bin incentives.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Prioritize based on inverse waste (best fit)\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n\n        # Adaptive stochasticity (reduced exploration as bins fill)\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = max(0, 0.1 - 0.01 * num_feasible)  # Reduce exploration with more options\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Penalize almost full bins dynamically based on item size\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty_factor = 0.2 + item * 0.5 # Adjust sensitivity of almost full depending on size.\n        priorities[feasible_bins][almost_full] *= (1 - penalty_factor)\n        \n        #Dynamically incentivize larger bins if remaining capacity is high enough\n        large_cap_threshold = item * (1.2 + item * 0.4)\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > large_cap_threshold, 0.4 + item * 0.2, 0)\n        priorities[feasible_bins] += large_cap_reward\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), we see both are identical. This suggests the ranking process might not be perfectly accurate or there are other factors influencing the ranking beyond the code itself.\nComparing (3rd) vs (4th), the primary difference is that the 3rd version parameterizes most of the magic numbers, making the heuristic more configurable but also more complex. The 4th version has hardcoded values but introduces a dynamic \"sweet spot\" incentive, potentially improving performance.\nComparing (5th) vs (6th), these are identical, similar to the first two.\n\nComparing (1st) vs (5th), the best-fit prioritization in (5th) uses `10 / (waste + 0.0001)` while (1st) uses `1 / (waste + 0.00001)`.  The higher constant in the numerator in (5th) could lead to a stronger best-fit bias initially. Stochasticity in (5th) is scaled by `0.1 * np.mean(bins_remain_cap)`, which is adaptive to the bin fill levels, while (1st) scales with the number of feasible bins.  Fragmentation penalty in (5th) is a direct multiplication by 0.2, while (1st) has a more nuanced approach with `penalty_factor = 0.2 + item * 0.1`. The large item reward in (5th) uses a fixed threshold `item * 1.25`, while (1st) uses `1.5 * item` as the threshold to rewards.\n\nComparing (20th) vs (19th), they are identical.\nComparing (18th) vs (17th), these are identical.\n\nComparing (second worst) vs (worst), (20th) vs (19th) there is no difference\nOverall: The better heuristics tend to incorporate item-size awareness into various components like fragmentation penalty, exploration factor and sweet spot incentives. They use adaptive stochasticity, often based on the number of feasible bins or the mean remaining capacity. Introducing a \"sweet spot\" to prioritize bins with utilization in a specific range seems to be a beneficial strategy. Penalizing fragmentation dynamically based on the size of the item being placed is also frequently observed in the better heuristics. Simpler is better, avoid unnescessary paramaterization.\n- \nOkay, let's redefine \"Current Self-Reflection\" for designing better heuristics, focusing on actionable advice and avoiding the pitfalls of ineffective reflection.\n\nHere's a breakdown:\n\n*   **Keywords:** Adaptive components, empirical tuning, iterative refinement, problem state.\n*   **Advice:** Build heuristics incrementally, starting with a simple, working base. Prioritize adaptive mechanisms over static parameters. Use empirical testing to tune weights and thresholds.\n*   **Avoid:** Premature complexity, non-linear functions without justification, and excessive customization.\n*   **Explanation:** Focus on gradual refinement and data-driven decisions. Begin with basic adaptive strategies and build upon them, avoiding overly complex models initially.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}