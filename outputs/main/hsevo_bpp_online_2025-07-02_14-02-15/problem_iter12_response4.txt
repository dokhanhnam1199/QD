```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Prioritizes best-fit with adaptive penalties, dynamic exploration, and bin-aware adjustments.
    Emphasizes bin utilization, prevents fragmentation, and balances exploration based on item size
    and remaining bin capacities. Focuses on simplicity and targeted adaptation."""

    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    feasible_bins = bins_remain_cap >= item

    if np.any(feasible_bins):
        waste = bins_remain_cap[feasible_bins] - item
        
        # Core: Prioritize best fit (minimize waste). More pronounced best-fit.
        priorities[feasible_bins] = 10 / (waste + 0.00001)  # Increased the impact of best-fit

        # Adaptive Stochasticity: Exploration decreases with item size and low remaining capacities
        num_feasible = np.sum(feasible_bins)
        exploration_factor = min(0.1, 0.01 * num_feasible * (1 - item)) #Reduced overall exploration & item aware
        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor

        # Fragmentation Penalty: Target almost-full bins dynamically scaled.
        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]
        almost_full = wasted_space_ratio < (0.05 + 0.02 * item)  #Dynamically increase threshold for larger items.
        priorities[feasible_bins][almost_full] *= 0.1  # Increased penalty.

        # Rewarding larger bins for smaller items. More focused reward.
        large_bin_threshold = 1.2 + 0.1*item
        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > large_bin_threshold, 0.3, 0) # Slightly reduced reward
        priorities[feasible_bins] += small_item_large_bin_reward
        
        # Dynamic "Sweet Spot" Incentive: Adapted Sweet Spot.
        sweet_spot_lower = 0.6 - (item * 0.15)  # Adjusted
        sweet_spot_upper = 0.85 - (item * 0.05) # Adjusted

        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0
        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)
        priorities[feasible_bins][sweet_spot] += 0.3 #Increased reward slightly

        #Bin Awareness - Encourage filling bins with lower remaining capacity
        bin_capacity_rank = np.argsort(bins_remain_cap[feasible_bins])
        capacity_incentive = np.linspace(0,0.15, num = len(bin_capacity_rank)) #Slight linear incentive.

        priorities[feasible_bins][bin_capacity_rank] += capacity_incentive

    else:
        priorities[:] = -np.inf  # No feasible bins

    return priorities
```
