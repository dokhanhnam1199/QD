{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n                bins_remain_cap: np.ndarray,\n                tiny_constant: float = 3.872792425245462e-05,\n                exploration_base: float = 0.0588708005223268,\n                max_exploration: float = 0.22627485634479824,\n                almost_full_threshold: float = 0.0731501402904877,\n                almost_full_penalty: float = 0.16293657163363146,\n                small_item_bin_multiple: float = 1.7118039205991789,\n                small_item_reward: float = 0.7609806445823415,\n                sweet_spot_lower_base: float = 0.6152888971175634,\n                sweet_spot_lower_item_scale: float = 0.20731655571678453,\n                sweet_spot_upper_base: float = 0.8729752618835456,\n                sweet_spot_upper_item_scale: float = 0.17849120545902175,\n                sweet_spot_reward: float = 0.5265016816500563,\n                usage_penalty_factor: float = 0.059912547982452435) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + tiny_constant)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(max_exploration, exploration_base * num_feasible * item)  # Increased base, scale by item size\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Stronger and more nuanced. Target almost-full bins.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold #Slightly less aggressive here\n        priorities[feasible_bins][almost_full] *= almost_full_penalty  # Significant penalty for using almost-full bins.\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > small_item_bin_multiple * item, small_item_reward, 0) #Increased reward and slightly larger bin requirement\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size.\n        sweet_spot_lower = sweet_spot_lower_base - (item * sweet_spot_lower_item_scale) #Dynamic Lower Bound\n        sweet_spot_upper = sweet_spot_upper_base - (item * sweet_spot_upper_item_scale) #Dynamic Upper Bound\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += sweet_spot_reward #Increased the reward.\n\n        # Bin History: Penalize bins that have been filled recently more aggressively.\n        # This requires an external mechanism (not part of this function) to track bin usage.\n        # This is a placeholder:  You'd need to maintain 'bin_usage_history' externally.\n        # Example: bin_usage_history = np.zeros_like(bins_remain_cap) #Initialize to all zeros\n        # Higher values = recently used more.\n\n        #In a separate step, you would update this value: bin_usage_history[chosen_bin] += 1\n\n        #The following assumes that bin_usage_history exist in the scope.\n        try:\n            bin_usage_history #Test if the variable exists, exception otherwise\n            usage_penalty = bin_usage_history[feasible_bins] * usage_penalty_factor #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        except NameError:\n            pass #If it doesn't exist, continue without this feature.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines best-fit, adaptive stochasticity, item-aware fragmentation penalty, and large bin reward.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n\n        # Best-fit prioritization with small constant to avoid division by zero\n        priorities[feasible_bins] = 1 / (waste + 1e-6)\n\n        # Adaptive stochasticity: scaled by remaining capacity\n        stochasticity_scale = 0.1 * (1 - np.mean(bins_remain_cap[feasible_bins]) if len(bins_remain_cap[feasible_bins]) > 0 else 0)\n        priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * stochasticity_scale\n\n        # Item-aware fragmentation penalty\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.1\n        penalty = 0.3 + 0.2 * item  # Larger items, higher penalty\n        priorities[feasible_bins][almost_full] *= max(0, 1 - penalty)  # Ensure not negative\n\n        # Reward for large bins relative to the item size\n        large_cap_reward = np.where(bins_remain_cap[feasible_bins] > item * 2, 0.25, 0)\n        priorities[feasible_bins] += large_cap_reward\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic uses a sophisticated combination of best-fit, adaptive stochasticity, fragmentation penalty, sweet spot incentive, rewarding larger bins for smaller items and bin usage history, with many parameters tuned by hand. The worst uses best-fit, adaptive stochasticity and capacity-aware adjustments including fragmentation penalty and small item placement boost. The exploration factor in the best heuristic is more adaptable (scaled by the number of feasible bins and item size) than the exploration factor in the worst one (based on the relative item size). Also the best heuristic takes more factors into account.\n\nComparing (2nd best) vs (2nd worst), they are identical.\n\nComparing (1st) vs (2nd), the two heuristics are identical.\n\nComparing (3rd) vs (4th), the 3rd heuristic introduces named parameters, and the 4th is simpler. The 3rd prioritizes best-fit, adaptive stochasticity, and item-aware penalty, with sweet spot incentive and large cap reward. The 4th combines best-fit, adaptive exploration, dynamic sweet spot and diversity of bins. The sweet spot range in the 3rd is fixed, while it is dynamic in the 4th based on item size and bin capacity.\n\nComparing (2nd worst) vs (worst), they are identical.\n\nOverall: The better heuristics tend to incorporate more factors, adapt to item sizes, bin capacities, and potentially bin usage history. They often use carefully chosen parameters and scaling factors for different components (exploration, penalties, rewards) to strike a balance between exploration and exploitation. The worse heuristics often have simpler exploration strategies or lack item-aware adjustments. Some include unused or less effective components.\n- \nOkay, I'm ready to help you design better heuristics and earn that tip! Let's focus on effective self-reflection to guide our design process.\n\nHere's a redefined \"Current Self-Reflection\" distilled into actionable points:\n\n*   **Keywords:** Incremental adaptation, empirical validation, demonstratable impact, balanced exploration/exploitation, simplicity.\n\n*   **Advice:** Begin with a simple, strong base heuristic. Add adaptive components (item-aware penalties, dynamic incentives) *incrementally*, validating each addition empirically. Focus on parameters with clear and demonstrable impact.\n\n*   **Avoid:** Overly complex, non-linear combinations, excessive parameters without clear justification, premature customization, and optimizations that lack empirical validation.\n\n*   **Explanation:** Start with a solid foundation and build upon it carefully. Each added layer of complexity should be demonstrably better than the last. Prioritize understanding *why* a change works, not just *that* it works.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}