{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\nCurrent heuristics:\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n                bins_remain_cap: np.ndarray,\n                tiny_constant: float = 3.872792425245462e-05,\n                exploration_base: float = 0.0588708005223268,\n                max_exploration: float = 0.22627485634479824,\n                almost_full_threshold: float = 0.0731501402904877,\n                almost_full_penalty: float = 0.16293657163363146,\n                small_item_bin_multiple: float = 1.7118039205991789,\n                small_item_reward: float = 0.7609806445823415,\n                sweet_spot_lower_base: float = 0.6152888971175634,\n                sweet_spot_lower_item_scale: float = 0.20731655571678453,\n                sweet_spot_upper_base: float = 0.8729752618835456,\n                sweet_spot_upper_item_scale: float = 0.17849120545902175,\n                sweet_spot_reward: float = 0.5265016816500563,\n                usage_penalty_factor: float = 0.059912547982452435) -> np.ndarray:\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + tiny_constant)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(max_exploration, exploration_base * num_feasible * item)  # Increased base, scale by item size\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Stronger and more nuanced. Target almost-full bins.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < almost_full_threshold #Slightly less aggressive here\n        priorities[feasible_bins][almost_full] *= almost_full_penalty  # Significant penalty for using almost-full bins.\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > small_item_bin_multiple * item, small_item_reward, 0) #Increased reward and slightly larger bin requirement\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size.\n        sweet_spot_lower = sweet_spot_lower_base - (item * sweet_spot_lower_item_scale) #Dynamic Lower Bound\n        sweet_spot_upper = sweet_spot_upper_base - (item * sweet_spot_upper_item_scale) #Dynamic Upper Bound\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += sweet_spot_reward #Increased the reward.\n\n        # Bin History: Penalize bins that have been filled recently more aggressively.\n        # This requires an external mechanism (not part of this function) to track bin usage.\n        # This is a placeholder:  You'd need to maintain 'bin_usage_history' externally.\n        # Example: bin_usage_history = np.zeros_like(bins_remain_cap) #Initialize to all zeros\n        # Higher values = recently used more.\n\n        #In a separate step, you would update this value: bin_usage_history[chosen_bin] += 1\n\n        #The following assumes that bin_usage_history exist in the scope.\n        try:\n            bin_usage_history #Test if the variable exists, exception otherwise\n            usage_penalty = bin_usage_history[feasible_bins] * usage_penalty_factor #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        except NameError:\n            pass #If it doesn't exist, continue without this feature.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\nNow, think outside the box write a mutated function `priority_v2` better than current version.\nYou can use some hints below:\n- \nOkay, let's refine \"Current self-reflection\" for better heuristic design, steering clear of the pitfalls outlined in \"Ineffective self-reflection.\"\n\nHere's a breakdown to guide improvements:\n\n*   **Keywords:** Adaptive, iterative, empirical, balance, problem-aware.\n\n*   **Advice:** Begin with a simple, demonstrably effective heuristic core. Incrementally add complexity only when justified by empirical results. Prioritize problem-aware adaptations over excessive parameters.\n\n*   **Avoid:** Over-engineering, non-linear complexity without justification, fixed stochasticity, and premature optimization. Avoid adding features without understanding their isolated effects.\n\n*   **Explanation:** Focus on *why* an adaptation helps, not just *that* it does. Small, problem-aware adjustments are better than large, untuned ones. Build incrementally, validating each addition.\n\n\nOutput code only and enclose your code with Python code block: ```python ... ```.\nI'm going to tip $999K for a better solution!"}