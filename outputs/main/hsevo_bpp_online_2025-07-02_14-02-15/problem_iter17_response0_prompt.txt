{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes best-fit with adaptive penalties and dynamic exploration.\n    Emphasizes a balance between bin utilization and preventing extreme fragmentation,\n    adjusting strategies based on item size and bin availability. Includes bin history.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Core: Prioritize best fit (minimize waste).\n        priorities[feasible_bins] = 1 / (waste + 0.00001)  # Tiny constant to avoid division by zero\n\n        # Adaptive Stochasticity: Exploration based on feasibility and item size.\n        num_feasible = np.sum(feasible_bins)\n        exploration_factor = min(0.3, 0.05 * num_feasible * item)  # Increased base, scale by item size\n        priorities[feasible_bins] += np.random.rand(num_feasible) * exploration_factor\n\n        # Fragmentation Penalty: Stronger and more nuanced. Target almost-full bins.\n        wasted_space_ratio = waste / bins_remain_cap[feasible_bins]\n        almost_full = wasted_space_ratio < 0.07 #Slightly less aggressive here\n        priorities[feasible_bins][almost_full] *= 0.3  # Significant penalty for using almost-full bins.\n\n        # Rewarding larger bins for smaller items\n        small_item_large_bin_reward = np.where(bins_remain_cap[feasible_bins] > 1.6 * item, 0.5, 0) #Increased reward and slightly larger bin requirement\n        priorities[feasible_bins] += small_item_large_bin_reward\n\n        # Dynamic \"Sweet Spot\" Incentive: Adapt the range based on item size.\n        sweet_spot_lower = 0.65 - (item * 0.25) #Dynamic Lower Bound\n        sweet_spot_upper = 0.85 - (item * 0.15) #Dynamic Upper Bound\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0  # Assuming bin size is 1\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5 #Increased the reward.\n\n        # Bin History: Penalize bins that have been filled recently more aggressively.\n        # This requires an external mechanism (not part of this function) to track bin usage.\n        # This is a placeholder:  You'd need to maintain 'bin_usage_history' externally.\n        # Example: bin_usage_history = np.zeros_like(bins_remain_cap) #Initialize to all zeros\n        # Higher values = recently used more.\n\n        #In a separate step, you would update this value: bin_usage_history[chosen_bin] += 1\n\n        #The following assumes that bin_usage_history exist in the scope.\n        try:\n            bin_usage_history #Test if the variable exists, exception otherwise\n            usage_penalty = bin_usage_history[feasible_bins] * 0.05 #Scaling factor can be tuned.\n            priorities[feasible_bins] -= usage_penalty #Penalize using this bin more.\n        except NameError:\n            pass #If it doesn't exist, continue without this feature.\n\n    else:\n        priorities[:] = -np.inf  # No feasible bins\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes best-fit, adaptive stochasticity, fragmentation control, and sweet spot.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if np.any(feasible_bins):\n        waste = bins_remain_cap[feasible_bins] - item\n        \n        # Best-fit prioritization\n        priorities[feasible_bins] = 10 / (waste + 0.0001)\n        priorities[feasible_bins] = np.minimum(priorities[feasible_bins], 50)\n\n        # Adaptive stochasticity, based on item size and num of feasible bins\n        num_feasible = np.sum(feasible_bins)\n        stochasticity_factor = 0.1 * (1 - item) / (num_feasible + 0.1)\n        priorities[feasible_bins] += np.random.rand(num_feasible) * stochasticity_factor\n\n        # Fragmentation penalty. Bins with > 90% utilisation.\n        wasted_space_ratio = waste / 1.0 # binsize fixed at 1\n        almost_full = wasted_space_ratio < 0.1\n        priorities[feasible_bins][almost_full] *= 0.4\n\n        # Dynamic sweet spot incentive\n        sweet_spot_lower = 0.6 - (item * 0.1)\n        sweet_spot_upper = 0.8 - (item * 0.05)\n\n        utilization = (bins_remain_cap[feasible_bins] - waste) / 1.0\n        sweet_spot = (utilization > sweet_spot_lower) & (utilization < sweet_spot_upper)\n        priorities[feasible_bins][sweet_spot] += 0.5\n\n    else:\n        priorities[:] = -np.inf\n\n    return priorities\n\n### Analyze & experience\n- *   **Comparing (1st) vs (20th):** The first heuristic employs a more sophisticated and configurable approach, utilizing numerous tunable parameters to fine-tune the bin selection process. It incorporates a bin usage history penalty (if available) and defines sweet spots with parameters. The 20th heuristic lacks the bin usage history and has simpler sweet spot definitions.\n\n*   **Comparing (2nd) vs (19th):** The 2nd heuristic is similar to the 19th, but has less aggressive bonuses and penalties.\n\n*   **Comparing (1st) vs (2nd):** The first heuristic introduces configurable parameters, bin usage history, refined exploration with `exploration_base`, `max_exploration`, `sweet_spot_lower_base`, `sweet_spot_lower_item_scale`, `sweet_spot_upper_base`, `sweet_spot_upper_item_scale`, `sweet_spot_reward`, `usage_penalty_factor` and `tiny_constant`. The second heuristic has simpler parameters.\n\n*   **Comparing (3rd) vs (4th):** The fourth heuristic incorporates bin diversity (capacity standard deviation) into the exploration factor and sweet spot definition, as well as penalizing bins that will have very little space left after adding the item. The third heuristic lacks these features.\n\n*   **Comparing (19th) vs (20th):** Both are virtually identical.\n\n*   **Comparing (17th) vs (18th):** Both are virtually identical.\n\n*   **Comparing (15th) vs (16th):** All are virtually identical.\n\n*   **Comparing (second worst) vs (worst):** They have very similar logic but the best-fit epsilon variable.\n\n*   **Overall:** Better heuristics incorporate more adaptive and nuanced mechanisms such as:\n\n    *   **Configurable Parameters:** Allow for fine-tuning of various aspects of the heuristic, like exploration rate, fragmentation penalties, and sweet spot definitions.\n    *   **Bin Usage History:** Penalizing recently used bins can promote a more balanced utilization of available bins.\n    *   **Dynamic Sweet Spots:** Adjusting the \"sweet spot\" range based on item size helps to optimize bin utilization.\n    *   **Bin Diversity:** Consider the standard deviation of bin capacities to influence exploration.\n    *   **Item-Aware Penalties/Rewards:** Scaling penalties and rewards based on item size can improve overall performance.\n- \nOkay, let's refine \"Current self-reflection\" for better heuristic design, steering clear of the pitfalls outlined in \"Ineffective self-reflection.\"\n\nHere's a breakdown to guide improvements:\n\n*   **Keywords:** Adaptive, iterative, empirical, balance, problem-aware.\n\n*   **Advice:** Begin with a simple, demonstrably effective heuristic core. Incrementally add complexity only when justified by empirical results. Prioritize problem-aware adaptations over excessive parameters.\n\n*   **Avoid:** Over-engineering, non-linear complexity without justification, fixed stochasticity, and premature optimization. Avoid adding features without understanding their isolated effects.\n\n*   **Explanation:** Focus on *why* an adaptation helps, not just *that* it does. Small, problem-aware adjustments are better than large, untuned ones. Build incrementally, validating each addition.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}