import numpy as np

def heuristics_v2(distance_matrix):
    """
    {This algorithm initializes edge probabilities based on distance, refines them using reinforcement learning with a reward proportional to solution quality, and adds a pheromone-inspired component to encourage exploration.}
    """
    n = len(distance_matrix)
    heuristics_matrix = np.zeros((n, n))

    # Initial heuristic based on inverse distance
    for i in range(n):
        for j in range(i + 1, n):
            distance = distance_matrix[i, j]
            heuristics_matrix[i, j] = heuristics_matrix[j, i] = 1 / (distance + 1e-9)

    # Normalize to probabilities
    for i in range(n):
        row_sum = np.sum(heuristics_matrix[i, :])
        if row_sum > 0:
            heuristics_matrix[i, :] /= row_sum

    # Reinforcement Learning parameters
    learning_rate = 0.1
    discount_factor = 0.9
    pheromone_decay = 0.1
    
    # Initialize Q-table (edge values)
    q_table = np.zeros((n, n))
    pheromone = np.ones((n, n)) * 0.1 #Pheromone matrix

    num_episodes = 50
    for episode in range(num_episodes):
        # Generate a solution (tour) based on current heuristics
        current_node = np.random.randint(n)
        tour = [current_node]
        unvisited = set(range(n))
        unvisited.remove(current_node)

        while unvisited:
            probabilities = heuristics_matrix[current_node, :] * pheromone[current_node, :]
            probabilities = np.array([probabilities[i] if i in unvisited else 0 for i in range(n)])

            if np.sum(probabilities) == 0:
                next_node = min(unvisited, key=lambda x: distance_matrix[current_node, x])
            else:
                probabilities /= np.sum(probabilities)
                next_node = np.random.choice(n, p=probabilities)
                while next_node not in unvisited:
                    probabilities[next_node] = 0
                    if np.sum(probabilities) == 0:
                        next_node = min(unvisited, key=lambda x: distance_matrix[current_node, x])
                        break
                    probabilities /= np.sum(probabilities)
                    next_node = np.random.choice(n, p=probabilities)

            tour.append(next_node)
            unvisited.remove(next_node)
            current_node = next_node

        tour.append(tour[0])  # Return to start

        # Calculate the cost (negative reward)
        cost = 0
        for i in range(n):
            cost += distance_matrix[tour[i], tour[i+1]]

        # Update Q-values and pheromones using the reward
        for i in range(n):
            node1 = tour[i]
            node2 = tour[i+1]
            reward = -cost #Negative reward
            old_q_value = q_table[node1, node2]
            new_q_value = old_q_value + learning_rate * (reward + discount_factor * np.max(q_table[node2, :]) - old_q_value)
            q_table[node1, node2] = new_q_value
            q_table[node2, node1] = new_q_value

            pheromone[node1, node2] = (1 - pheromone_decay) * pheromone[node1, node2] + pheromone_decay * (1/ (cost + 1e-9))
            pheromone[node2, node1] = (1 - pheromone_decay) * pheromone[node2, node1] + pheromone_decay * (1/ (cost + 1e-9))

        # Update heuristics matrix based on Q-table and pheromones
        for i in range(n):
            for j in range(i + 1, n):
                heuristics_matrix[i, j] = (1 / (distance_matrix[i, j] + 1e-9)) + q_table[i, j]  # Combine distance and Q-value
                heuristics_matrix[j, i] = heuristics_matrix[i, j]

        # Normalize heuristics to probabilities
        for i in range(n):
            row_sum = np.sum(heuristics_matrix[i, :])
            if row_sum > 0:
                heuristics_matrix[i, :] /= row_sum

    return heuristics_matrix
