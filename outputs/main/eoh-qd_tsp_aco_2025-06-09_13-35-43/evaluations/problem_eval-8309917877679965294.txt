import numpy as np

def heuristics_v2(distance_matrix):
    """{This algorithm combines edge frequency sampling with reinforcement learning by iteratively constructing tours, rewarding edges present in shorter tours, and using the reward signal to bias future tour construction.}"""
    n = distance_matrix.shape[0]
    num_episodes = 100
    learning_rate = 0.1
    discount_factor = 0.9
    initial_reward = 1.0 / n  # Initialize with a small positive reward

    # Initialize heuristics matrix with initial rewards
    heuristics_matrix = np.full_like(distance_matrix, initial_reward)

    for episode in range(num_episodes):
        start_node = np.random.randint(n)
        unvisited = set(range(n))
        unvisited.remove(start_node)
        current_node = start_node
        path = [start_node]
        tour_length = 0

        while unvisited:
            # Probabilistically select the next node based on heuristic values
            probabilities = heuristics_matrix[current_node, list(unvisited)]
            probabilities = probabilities / np.sum(probabilities)  # Normalize

            next_node = np.random.choice(list(unvisited), p=probabilities)
            
            path.append(next_node)
            tour_length += distance_matrix[current_node, next_node]
            unvisited.remove(next_node)

            current_node = next_node

        # Complete the tour by returning to the start node
        tour_length += distance_matrix[current_node, start_node]
        path.append(start_node)

        # Update heuristic values based on tour length (reward)
        reward = -tour_length  # Shorter tour = higher reward

        for i in range(n):
            u, v = path[i], path[(i + 1) % n]
            heuristics_matrix[u, v] += learning_rate * reward
            heuristics_matrix[v, u] += learning_rate * reward


    # Normalize the heuristic values (optional)
    max_val = np.max(heuristics_matrix)
    if max_val > 0:
        heuristics_matrix = heuristics_matrix / max_val
    
    return heuristics_matrix
