import numpy as np

def heuristics_v2(distance_matrix):
    """{This algorithm combines edge sampling with reinforcement learning, iteratively refining edge probabilities based on tour quality and a Q-learning update rule.}"""
    n = len(distance_matrix)
    heuristics_matrix = np.zeros((n, n))
    q_matrix = np.zeros((n, n))
    learning_rate = 0.1
    discount_factor = 0.9
    num_samples = 1000

    for _ in range(num_samples):
        current_node = np.random.randint(n)
        unvisited_nodes = set(range(n))
        unvisited_nodes.remove(current_node)
        path = [current_node]
        tour_length = 0

        while unvisited_nodes:
            probabilities = np.zeros(n)
            for neighbor in unvisited_nodes:
                probabilities[neighbor] = np.exp(q_matrix[current_node, neighbor]) # Use Q-values to guide selection

            probabilities = probabilities / np.sum(probabilities)
            
            next_node = np.random.choice(n, p=probabilities)
            
            if next_node not in unvisited_nodes:
                available_nodes = list(unvisited_nodes)
                if len(available_nodes) > 0:
                    next_node = available_nodes[np.argmin([distance_matrix[current_node, node] for node in available_nodes])]
                else:
                    next_node = path[0]

            path.append(next_node)
            tour_length += distance_matrix[current_node, next_node]
            unvisited_nodes.remove(next_node)
            current_node = next_node

        # Return to the starting node
        path.append(path[0])
        tour_length += distance_matrix[path[-2], path[-1]]
        
        # Q-learning update
        reward = -tour_length  # Negative tour length as reward
        for i in range(len(path) - 2):
            state = path[i]
            action = path[i+1]
            next_state = path[i+2]
            
            best_next_q = np.max(q_matrix[action, :]) # Estimate of optimal future value
            q_matrix[state, action] = (1 - learning_rate) * q_matrix[state, action] + \
                                    learning_rate * (reward + discount_factor * best_next_q)

    # Populate heuristics_matrix based on Q-values
    for i in range(n):
        for j in range(n):
            heuristics_matrix[i, j] = q_matrix[i, j]

    return heuristics_matrix
