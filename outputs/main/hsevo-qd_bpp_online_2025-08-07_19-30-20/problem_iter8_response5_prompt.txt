{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin.\n    Random Fit: choose a bin at random among those that can fit the item.\n    We add a weighted random component to prioritize bins with less free space.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    fit = bins_remain_cap >= item\n    if np.any(fit):\n        remaining = bins_remain_cap[fit] - item\n        eps = 1e-6\n        weights = 1.0 / (remaining + eps)\n        random_vals = np.random.rand(np.count_nonzero(fit))\n        priorities[fit] = weights * random_vals\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    eps = 1e-9\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    fit = bins_remain_cap >= item\n    if not np.any(fit):\n        return priorities\n    remaining = bins_remain_cap[fit] - item\n    weight1 = 1.0 / (remaining + eps)\n    max_rem = bins_remain_cap.max() + eps\n    norm_left = remaining / max_rem\n    thresh = 0.2\n    scale = 0.05\n    weight2 = 1.0 / (1.0 + np.exp((norm_left - thresh) / scale))\n    random_vals = np.random.rand(np.count_nonzero(fit))\n    combined = weight1 * weight2 * (1 + 0.1 * random_vals)\n    priorities[fit] = combined\n    return priorities\n\n### Analyze & experience\n- - **Comparing (best) vs (worst):** The best heuristic (1st) combines an inverse\u2011residual weight, a tiny capacity\u2011bias, and a small random jitter, then normalises the result into a probability distribution. It explicitly zeroes out infeasible bins and never returns `\u2011inf`. The worst heuristic (20th) simply returns the raw remaining capacity for feasible bins (worst\u2011fit) and `\u2011inf` otherwise \u2013 it ignores how tightly the item fits and provides no stochastic tie\u2011breaking.  \n\n- **(Second best) vs (second worst):** The 2nd heuristic uses pure inverse\u2011remaining weighting multiplied by a uniform random factor, with `\u2011inf` for infeasible bins. The 19th heuristic (second worst) adds a sigmoid term that attenuates the weight for bins with large residuals and also multiplies by a random factor. The extra sigmoid makes the scoring less aggressive, which explains its lower rank.  \n\n- **Comparing (1st) vs (2nd):** Both favour tight fits via `1/(remaining)`. The 1st heuristic further adds a small bias proportional to absolute bin capacity, a jitter proportional to the weight, and finally normalises to probabilities. The 2nd heuristic lacks bias, jitter and normalisation, so it is more myopic and can produce extreme values.  \n\n- **(3rd) vs (4th):** These two functions are identical copies of the 2nd heuristic. Their presence illustrates that identical implementations can occupy adjacent ranks without affecting performance; any difference in ranking would stem from external factors (e.g., execution context) rather than code.  \n\n- **Comparing (second worst) vs (worst):** The 19th heuristic still respects the item\u2011size constraint (uses `bins_remain_cap >= item`) and blends inverse\u2011remaining, sigmoid smoothing, and randomness. The 20th worst\u2011fit ignores the residual entirely and only maximises remaining capacity, which often leads to poor packing efficiency.  \n\n- **Overall:** The top\u2011ranked heuristics reward tight fits, add controlled randomness to break ties, and optionally bias toward larger bins while keeping scores numerically stable and normalised. Lower\u2011ranked heuristics either drop randomness, over\u2011simplify (worst\u2011fit), or apply excessive smoothing that dilutes the tight\u2011fit signal.\n- \n- **Keywords:** inverse capacity weighting, modest size bias, tie\u2011break jitter, lean implementation.  \n- **Advice:** Prioritise bins by inverse remaining capacity with a small bonus for larger bins; use tiny random jitter to resolve ties; normalise scores to probabilities; keep the algorithm straightforward and fast.  \n- **Avoid:** hard masks, numeric stability hacks, smooth transforms like softmax, deterministic or worst\u2011fit strategies, excessive documentation; explicit \u2013inf masking, epsilon tricks, max subtraction, sigmoid/softmax, and stochastic bias for tight fits.  \n- **Explanation:** This balances fit efficiency with computational simplicity, preventing over\u2011complexity while still offering a robust, lightweight heuristic that can be benchmarked easily.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}