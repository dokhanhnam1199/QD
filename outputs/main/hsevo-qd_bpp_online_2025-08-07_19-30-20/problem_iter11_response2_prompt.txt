{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    available = bins_remain_cap >= item\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    if available.any():\n        priorities[available] = -(bins_remain_cap[available] - item)\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n# Hybrid priority: tight-fit weight, sigmoid scaling, and random perturbation.\n    \"\"\"Combine 1/(remaining+\u03b5) weight, sigmoid of normalized residual, and small random factor.\"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    fit = bins_remain_cap >= item\n    if not np.any(fit):\n        return priorities\n    remaining = bins_remain_cap[fit] - item\n    eps = 1e-12\n    weight = 1.0 / (remaining + eps)\n    max_rem = bins_remain_cap.max() if bins_remain_cap.size else 1.0\n    normalized = remaining / (max_rem + eps)\n    k = 12.0\n    offset = 0.2\n    sigmoid = 1.0 / (1.0 and 1.0 + np.exp(k * (normalized - offset)))  # sigmoid bias toward tight fit\n    beta = 0.2\n    random_factor = 1.0 + beta * (np.random.rand(remaining.shape[0]) - 0.5)\n    priorities[fit] = weight * sigmoid * random_factor\n    return priorities\n\n### Analyze & experience\n- - **(1st) vs (20th)** \u2013 The top heuristic uses a clean *inverse\u2011remaining\u2011capacity* weight multiplied by a uniform random factor, and explicitly marks infeasible bins with `-inf`. The worst heuristic adds a tiny capacity\u2011bonus and jitter to the same inverse weight, but returns **0** for infeasible bins and mixes additive terms (`weight + bias + jitter`). This can blur the ordering of feasible bins and fails to exploit the sentinel `-inf` that many bin\u2011packing selectors rely on.  \n\n- **(2nd) vs (19th)** \u2013 Identical to the above comparison: the second\u2011best mirrors the first, while the second\u2011worst repeats the additive\u2011bias design of the 20th, confirming that the extra bias/jitter does not improve performance.  \n\n- **(3rd) vs (18th)** \u2013 The third heuristic introduces configurable parameters (`weight`, `eps`, `min_priority`) and retains the multiplicative random weighting, preserving a monotonic relationship between residual size and priority. The eighteenth heuristic (same as 17th\u201320th) again mixes additive bias and jitter, lacking the configurable control and yielding noisier rankings.  \n\n- **(1st) vs (2nd)** \u2013 Both are identical implementations; their joint top rank suggests that the baseline weighted\u2011random approach is already optimal among the candidates, and any duplication does not affect the ordering.  \n\n- **(3rd) vs (4th)** \u2013 The fourth heuristic enriches the priority with a *bias toward larger bins*, a variance\u2011scaled jitter, and an extra random factor. While conceptually richer, the extra terms dilute the primary inverse\u2011capacity signal and introduce unnecessary stochasticity, causing it to fall below the simpler, parameter\u2011tuned third heuristic.  \n\n- **(19th) vs (20th)** \u2013 These two are duplicates; both suffer from the same additive\u2011bias issue and share the same low rank, confirming that the additive formulation is consistently inferior.  \n\n- **(Overall)** \u2013 The best heuristics favor **multiplicative** combinations of a clear monotonic weight (inverse residual) with modest randomness and proper handling of infeasibility (`-inf`). Heuristics that **add** extra terms (bias, jitter) or replace `-inf` with zero degrade performance, especially when the added components are not carefully scaled. Simplicity, monotonicity, and explicit infeasibility markers emerge as the dominant design principles.\n- \n- **Keywords**: best\u2011fit, deterministic, integer arithmetic, lightweight, tie\u2011breaking  \n- **Advice**: Use best\u2011fit selection by computing the leftover space after placement and picking the bin that leaves the least; break ties randomly. Do not add \u03b5 or \u2013\u221e masking; use raw integer values.  \n- **Avoid**: Explicit \u2013\u221e masking, \u03b5 tricks, sigmoid/softmax smoothing, jitter, probability normalization, bias toward larger bins, deterministic worst\u2011fit.  \n- **Explanation**: This keeps the heuristic simple, fully deterministic except for tie\u2011breaking, avoids the pitfalls highlighted in the ineffective reflection, and uses only integer arithmetic for robustness.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}