{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\nCurrent heuristics:\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Inverse\u2011residual weighting with slight capacity bias and tiny random jitter.\"\"\"\n    mask = bins_remain_cap >= item\n    if not np.any(mask):\n        return np.zeros_like(bins_remain_cap, dtype=float)\n    residual = bins_remain_cap - item\n    eps = 1e-12\n    inv_res = 1.0 / (residual + eps)\n    bias_factor = 0.05\n    max_cap = bins_remain_cap.max()\n    bias = bias_factor * bins_remain_cap / (max_cap + eps)\n    weight = inv_res + bias\n    weight[~mask] = 0.0\n    jitter = 1e-3 * weight * np.random.rand(weight.size)\n    weight += jitter\n    total = weight.sum()\n    if total == 0.0:\n        return np.zeros_like(bins_remain_cap, dtype=float)\n    return weight / total\n\nNow, think outside the box write a mutated function `priority_v2` better than current version.\nYou can use some hints below:\n- \n- **Keywords:** inverse capacity weighting, modest size bias, tie\u2011break jitter, lean implementation.  \n- **Advice:** Prioritise bins by inverse remaining capacity with a small bonus for larger bins; use tiny random jitter to resolve ties; normalise scores to probabilities; keep the algorithm straightforward and fast.  \n- **Avoid:** hard masks, numeric stability hacks, smooth transforms like softmax, deterministic or worst\u2011fit strategies, excessive documentation; explicit \u2013inf masking, epsilon tricks, max subtraction, sigmoid/softmax, and stochastic bias for tight fits.  \n- **Explanation:** This balances fit efficiency with computational simplicity, preventing over\u2011complexity while still offering a robust, lightweight heuristic that can be benchmarked easily.\n\nOutput code only and enclose your code with Python code block: ```python ... ```.\nI'm going to tip $999K for a better solution!"}