{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    eps = 1e-12\n    priorities = np.full(bins_remain_cap.shape, -np.inf, dtype=np.float64)\n    fit = bins_remain_cap >= item\n    if not np.any(fit):\n        return priorities\n    remaining = bins_remain_cap[fit] - item\n    tight_weight = 1.0 / (remaining + eps)\n    max_cap = np.max(bins_remain_cap) if bins_remain_cap.size > 0 else 1.0\n    fill_level = 1.0 - np.mean(bins_remain_cap) / (max_cap + eps)\n    temp = np.clip(1.0 - fill_level, eps, 1.0)\n    scaled = np.log(tight_weight + eps) / temp\n    max_scaled = np.max(scaled)\n    exp_scaled = np.exp(scaled - max_scaled)\n    soft_weights = exp_scaled\n    rnd = np.random.rand(remaining.shape[0])\n    var_factor = np.std(remaining) / (np.mean(remaining) + eps)\n    alpha = np.clip(var_factor, 0.0, 1.0)\n    scores = (1 - alpha) * soft_weights + alpha * rnd\n    priorities[fit] = scores\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n# Hybrid priority: tight-fit weight, sigmoid scaling, and random perturbation.\n    \"\"\"Combine 1/(remaining+\u03b5) weight, sigmoid of normalized residual, and small random factor.\"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    fit = bins_remain_cap >= item\n    if not np.any(fit):\n        return priorities\n    remaining = bins_remain_cap[fit] - item\n    eps = 1e-12\n    weight = 1.0 / (remaining + eps)\n    max_rem = bins_remain_cap.max() if bins_remain_cap.size else 1.0\n    normalized = remaining / (max_rem + eps)\n    k = 12.0\n    offset = 0.2\n    sigmoid = 1.0 / (1.0 and 1.0 + np.exp(k * (normalized - offset)))  # sigmoid bias toward tight fit\n    beta = 0.2\n    random_factor = 1.0 + beta * (np.random.rand(remaining.shape[0]) - 0.5)\n    priorities[fit] = weight * sigmoid * random_factor\n    return priorities\n\n### Analyze & experience\n- Comparing (best) vs (worst), we see that the best heuristic uses weighted random choice to favor tight fits, adding a random factor for tie\u2011breaking, whereas the worst uses a deterministic worst\u2011fit strategy that often leads to poor bin utilisation. (second best) vs (second worst), we see the second best matches the best in simplicity and performance, while the second worst introduces a hybrid sigmoid\u2011based weighting and random perturbation that unnecessarily increases complexity without clear benefit. Comparing (1st) vs (2nd), we see the implementations are identical, highlighting code duplication. (3rd) vs (4th), we see another identical weighted\u2011random implementation, further indicating redundancy. Comparing (second worst) vs (worst), we see the second worst improves upon the worst\u2011fit by incorporating a weighted sigmoid and stochastic component, leading to better bin utilisation. Overall, the ranking reflects a trade\u2011off between simplicity, randomness, and computational overhead, with the top heuristics delivering robust packing through lightweight randomised weighting, while the bottom ones either over\u2011engineer or oversimplify the problem.\n- \n- **Keywords:** inverse\u2011remaining weighting, random tie\u2011breaker, lean code, benchmark.  \n- **Advice:** Apply 1/(remaining) weights, calibrate weighting to prioritize tight fits, add minimal random jitter to break ties, keep the implementation simple, run performance tests.  \n- **Avoid:** Heavy softmax/sigmoid, explicit -inf masks, complex numeric tricks, duplicate logic.  \n- **Explanation:** The inverse\u2011remaining method gives tighter fits with low cost; randomness removes deterministic bias; simplicity aids maintenance; benchmarks confirm gains.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}