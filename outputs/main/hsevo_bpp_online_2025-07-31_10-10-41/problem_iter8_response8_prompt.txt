{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Adaptive Best Fit (ABF) with remnant preference. Prioritizes perfect fits, then standard\n    best fit, while penalizing very small non-zero remnants to encourage more usable leftover space.\n    \"\"\"\n    if item <= 0:\n        return -np.inf * np.ones_like(bins_remain_cap)\n\n    remaining_after_placement = bins_remain_cap - item\n    priorities = -np.inf * np.ones_like(bins_remain_cap)\n\n    # Core Best Fit: Prioritize bins by minimizing remaining space\n    fits_mask = remaining_after_placement >= 0\n    priorities[fits_mask] = -remaining_after_placement[fits_mask]\n\n    # Adaptive Component: Penalize very small, non-zero remnants\n    # This constant (e.g., 5% of a typical bin capacity) needs tuning based on problem scale.\n    SMALL_REMNANT_THRESHOLD = 0.05 \n    # Penalty value to make small remnants less attractive than better fits.\n    PENALTY_VALUE = 0.1 \n\n    small_remnant_mask = (remaining_after_placement > 0) & \\\n                         (remaining_after_placement < SMALL_REMNANT_THRESHOLD) & \\\n                         fits_mask\n    priorities[small_remnant_mask] -= PENALTY_VALUE\n\n    # Absolute Highest Priority: Ensure perfect fits are always chosen first\n    perfect_fit_mask = (remaining_after_placement == 0)\n    priorities[perfect_fit_mask] = np.inf\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority for each bin using an adaptive Best Fit strategy with a dynamic\n    preference for consolidating smaller items into already fuller bins.\n\n    This heuristic attempts to balance the Best Fit principle (minimizing wasted space per item)\n    with a strategy to reduce fragmentation, especially for smaller items. For small items,\n    it slightly prioritizes bins that are already more full (have less remaining capacity),\n    aiming to 'finish' those bins or prevent creation of many small gaps across new bins.\n    For larger items, it reverts closer to a pure Best Fit strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Assumption for Bin Capacity:\n    # In many online Bin Packing problems, bin capacities are normalized, often to 1.0.\n    # If the problem context implies a different fixed bin capacity, this value should be adjusted.\n    BIN_CAPACITY = 1.0\n\n    # Hyperparameter K:\n    # This constant controls the strength of the adaptive preference for fuller bins.\n    # A value of K=0 makes this function equivalent to the original Best Fit (priority_v1).\n    # Increasing K (e.g., from 0.1 to 1.0) intensifies the preference for fuller bins\n    # for smaller items. This value might need tuning for specific problem instances.\n    K = 0.5 \n\n    # Calculate remaining capacity after placing the item in each bin.\n    # This is crucial for identifying bins where the item fits.\n    remaining_after_placement = bins_remain_cap - item\n\n    # --- Core Priority Calculation ---\n    # The base of the priority score is (item - bins_remain_cap), which is equivalent to\n    # the negative of the remaining space after placement. This component drives the\n    # Best Fit behavior: higher score for less remaining space (closer to perfect fit).\n\n    # Adaptive Factor:\n    # This factor dynamically adjusts the influence of the bin's current fullness based on the item's size.\n    # - For smaller items (item << BIN_CAPACITY), (1.0 - item / BIN_CAPACITY) is close to 1.0.\n    #   This means the adaptive_factor is larger, giving more weight to the 'bins_remain_cap' term.\n    #   The effect: a stronger preference for putting small items into bins that are already more full.\n    # - For larger items (item approaches BIN_CAPACITY), (1.0 - item / BIN_CAPACITY) is close to 0.0.\n    #   This means the adaptive_factor is smaller, reducing the weight of the 'bins_remain_cap' term.\n    #   The effect: the priority calculation becomes very similar to pure Best Fit.\n    \n    # Ensure BIN_CAPACITY is not zero to prevent division by zero, though in BPP it's always positive.\n    adaptive_factor = K * (1.0 - item / BIN_CAPACITY) if BIN_CAPACITY > 0 else 0.0\n    \n    # The combined priority score:\n    # The first part (item - bins_remain_cap) is the standard Best Fit component.\n    # The second part (- adaptive_factor * bins_remain_cap) applies a penalty that is:\n    #   1. Larger (more negative) for bins with more remaining capacity (i.e., less full bins).\n    #   2. Scaled by 'adaptive_factor', meaning this penalty is more significant for smaller items.\n    # This synergistically pushes smaller items into already fuller bins while maintaining Best Fit\n    # for larger items.\n    priorities = (item - bins_remain_cap) - adaptive_factor * bins_remain_cap\n\n    # --- Handling Invalid Placements ---\n    # Crucially, any bin where the item cannot fit (resulting in negative remaining_after_placement)\n    # must be assigned the lowest possible priority (-np.inf) to ensure it is never chosen.\n    priorities[remaining_after_placement < 0] = -np.inf\n\n    return priorities\n\n### Analyze & experience\n- Comparing (Heuristics 1st) vs (Heuristics 20th), we see that the simplest Best Fit (H1), which minimizes absolute remaining capacity, significantly outperforms a more complex \"Adaptive Best Fit\" (H20). H20 attempts to dynamically bias item placement for smaller items into fuller bins, but its elaborate formula with parameters like `K` and reliance on `BIN_CAPACITY` appears to lead to worse performance. This suggests that over-engineering or adding complex adaptive factors without precise tuning can be detrimental.\n\nComparing (Heuristics 2nd) vs (Heuristics 17th), both aim for bin consolidation but with different mechanisms. Heuristic 2 adds an explicit \"Bin Completion Bonus\" to already full bins, while Heuristic 17 (identical to H20) applies a dynamic penalty based on current bin capacity and item size. Heuristic 2's bonus-based approach is ranked higher, indicating that encouraging desired states through positive reinforcement (a bonus for being full) may be more effective than complex, dynamically scaled penalties.\n\nComparing (Heuristics 1st) vs (Heuristics 2nd), pure Best Fit (H1) is slightly superior to Best Fit with an added Bin Completion Bonus (H2). This suggests that for this problem, the direct greedy objective of minimizing the immediate remaining space (Best Fit) might be more robust and effective than trying to enforce a global \"bin completion\" strategy with a fixed `beta` parameter. Simplicity often provides a robust baseline.\n\nComparing (Heuristics 3rd) vs (Heuristics 4th), Heuristic 3 attempts to refine Best Fit by penalizing small remnants and strongly prioritizing perfect fits (assigning `np.inf`). Heuristic 4 is functionally almost identical to Heuristic 1 (pure Best Fit). The ranking indicates that Heuristic 3's aggressive perfect fit prioritization and small remnant penalty, while intuitive, is slightly less effective than pure Best Fit, perhaps due to unintended side effects of diverting items from locally optimal (but not perfectly fitting) bins.\n\nComparing (Heuristics 6th) vs (Heuristics 17th), \"Proportional Best Fit\" (H6, 7, 9, 10, 11) is ranked notably higher than the \"Adaptive Best Fit\" (H17-20). This implies that maximizing the proportion of remaining capacity filled is a better strategy than the dynamic adaptive approach of H17-20, even if it's still inferior to standard Best Fit.\n\nComparing (Heuristics 16th) vs (Heuristics 20th), the absolute lowest-ranked heuristics (H12-16) are incomplete, lacking the core logic for placing items beyond a basic invalid item check. This fundamental flaw explains their worst-in-class performance. Heuristics 17-20 are fully implemented but still perform poorly, highlighting that a complete implementation of a suboptimal heuristic is still worse than an incomplete one (if no items can be placed).\n\nOverall: Simple Best Fit (minimizing absolute remaining capacity) appears to be the most robust and effective. Attempts to add complexity, such as dynamic penalties or less direct proportional fits, often lead to worse performance unless extremely well-tuned. Aggressive prioritization of perfect fits is somewhat beneficial but not consistently better than pure Best Fit. Finally, correctly implementing the core logic is paramount.\n- \n*   **Keywords:** Heuristic Baseline, Additive Incentives, Perfect Fits, Logic Validation, Parameter Tuning.\n*   **Advice:** Use Best Fit as an analytical starting concept. Implement additive bonuses for desirable outcomes like bin completion. Strongly prioritize perfect item-bin fits. Verify all core placement logic for completeness. Rigorously tune heuristic parameters.\n*   **Avoid:** Stating Best Fit's general performance, prescribing specific linear/non-linear penalties, or vague calls for calculation simplicity or robustness for invalid inputs.\n*   **Explanation:** This approach emphasizes building from a conceptual baseline with positive, targeted mechanisms, ensuring foundational correctness, and optimizing through calibration, rather than focusing on specific penalty forms or broad performance comparisons.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}