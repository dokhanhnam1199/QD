[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    ratios = item / bins_remain_cap\n    log_ratios = np.log(ratios)\n    priorities = -log_ratios\n    return priorities",
    "response_id": 0,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 11.60964047443681,
    "mi": 94.04446327225541,
    "token_count": 47.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate waste if item is placed in each bin\n    waste = bins_remain_cap - item\n\n    # Give high priority to bins where item fits and waste is minimized\n    fit_mask = waste >= 0\n    priorities[fit_mask] = 1 / (waste[fit_mask] + 0.000001)  # Add a small constant to avoid division by zero\n\n    # Give slightly lower priority to bins where item doesn't fit, but the overflow is minimized\n    # This encourages splitting items across bins less often but still allows it when needed\n    overflow_mask = ~fit_mask\n    priorities[overflow_mask] = - (item - bins_remain_cap[overflow_mask]) / (np.max(bins_remain_cap) + 0.000001) #Prioritize bins closer to fitting the item\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 92.0,
    "mi": 83.94080546850132,
    "token_count": 115.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on waste and overflow, improved version.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    waste = bins_remain_cap - item\n\n    fit_mask = waste >= 0\n    priorities[fit_mask] = 1 / (waste[fit_mask] + 0.000001)\n\n    overflow_mask = ~fit_mask\n    overflow = item - bins_remain_cap[overflow_mask]\n    max_cap = np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0 # avoid empty array\n    priorities[overflow_mask] = - overflow / (max_cap + 0.000001)\n\n    # Introduce a small bonus for bins that are already relatively full\n    fullness = 1 - bins_remain_cap / (np.max(bins_remain_cap)+0.000001) # Avoid zero division and normalized to max bin size\n    priorities += 0.1 * fullness  # scale fullness\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 159.81495041679716,
    "mi": 88.77760257484631,
    "token_count": 118.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response0.txt_stdout.txt",
    "code_path": "problem_iter3_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate waste if item is placed in each bin\n    waste = bins_remain_cap - item\n    \n    # Give high priority to bins where item fits and waste is minimized\n    fit_mask = waste >= 0\n    \n    # Prioritize bins with smaller waste, but also consider the fill ratio\n    # Encourage filling bins as much as possible\n    fill_ratio = (bins_remain_cap[fit_mask] - waste[fit_mask]) / bins_remain_cap[fit_mask]\n    priorities[fit_mask] = (1 / (waste[fit_mask] + 0.000001)) * (1 + fill_ratio) # Combine waste and fill ratio\n\n    # Adjust priority based on the item size relative to the bin capacity\n    # If the item is relatively large, prioritize bins that are closer to being full.\n    large_item_threshold = 0.5 * np.max(bins_remain_cap)  # Define a threshold for \"large\" items\n    if item > large_item_threshold:\n        priorities[fit_mask] *= (bins_remain_cap[fit_mask] / np.max(bins_remain_cap)) #Prioritize almost full bins\n\n    # Give negative priority to bins where item doesn't fit, proportional to overflow\n    overflow_mask = ~fit_mask\n    overflow = item - bins_remain_cap[overflow_mask]\n    priorities[overflow_mask] = -overflow / (np.max(bins_remain_cap) + 0.000001)  # Prioritize bins with smaller overflow\n\n    # Add a small bonus to bins with capacity close to the item size\n    # This encourages perfect fits\n    close_fit_mask = (bins_remain_cap >= (item - 0.1 * item)) & (bins_remain_cap <= (item + 0.1 * item)) & fit_mask\n    priorities[close_fit_mask] += 0.5\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 177.87213211613133,
    "mi": 79.3725642842327,
    "token_count": 165.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response0.txt_stdout.txt",
    "code_path": "problem_iter7_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, fit_priority_scale: float = 3.2155309410593764, waste_epsilon: float = 3.241139675159933e-05, overflow_priority_scale: float = 9.976943356947597, max_cap_epsilon: float = 0.0001001235545987124) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        fit_priority_scale: Scale of the priority when the item fits.\n        waste_epsilon: Small constant to avoid division by zero when calculating waste priority.\n        overflow_priority_scale: Scale of the priority when the item doesn't fit.\n        max_cap_epsilon: Small constant to avoid division by zero when prioritizing bins closer to fitting.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate waste if item is placed in each bin\n    waste = bins_remain_cap - item\n\n    # Give high priority to bins where item fits and waste is minimized\n    fit_mask = waste >= 0\n    priorities[fit_mask] = fit_priority_scale / (waste[fit_mask] + waste_epsilon)  # Add a small constant to avoid division by zero\n\n    # Give slightly lower priority to bins where item doesn't fit, but the overflow is minimized\n    # This encourages splitting items across bins less often but still allows it when needed\n    overflow_mask = ~fit_mask\n    priorities[overflow_mask] = -overflow_priority_scale * (item - bins_remain_cap[overflow_mask]) / (np.max(bins_remain_cap) + max_cap_epsilon) #Prioritize bins closer to fitting the item\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 128.3789500201924,
    "mi": 67.81667210426089,
    "token_count": 139.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response0.txt_stdout.txt",
    "code_path": "problem_iter5_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on waste, overflow, and fill level.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    waste = bins_remain_cap - item\n    fit_mask = waste >= 0\n\n    #Prioritize bins where item fits, minimizing waste.\n    priorities[fit_mask] = 1 / (waste[fit_mask] + 0.000001)\n\n    #Penalize overflow, but prioritize bins closer to fitting.\n    overflow_mask = ~fit_mask\n    priorities[overflow_mask] = - (item - bins_remain_cap[overflow_mask]) / (np.max(bins_remain_cap) + 0.000001)\n\n    #Incentivize filling bins that are already relatively full.\n    fullness = (1 - bins_remain_cap / np.max(bins_remain_cap))\n    priorities += fullness * 0.1 #Scale down to avoid dominating other factors\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 10.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 175.93083758004835,
    "mi": 90.27968087310084,
    "token_count": 129.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response2.txt_stdout.txt",
    "code_path": "problem_iter6_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Enhanced priority function considering multiple factors for better bin packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    bin_count = len(bins_remain_cap)\n\n    # Calculate waste if item is placed in each bin\n    waste = bins_remain_cap - item\n\n    # Fit Heuristic: Prioritize bins where the item fits.\n    fit_mask = waste >= 0\n    if np.any(fit_mask):  # Only apply if at least one bin can fit the item\n        priorities[fit_mask] = (1 / (waste[fit_mask] + 0.000001)) + 0.1  # Increate priority compared to overflow. Avoid zero division.\n\n        # Best Fit Improvement:  Slightly boost the priority of bins with minimal waste.\n        min_waste = np.min(waste[fit_mask])\n        best_fit_mask = (waste == min_waste) & fit_mask\n        priorities[best_fit_mask] += 0.2  # A small bonus for the best fit\n\n        # Reward near-full bins:\n        near_full_threshold = 0.1  # Define a threshold for \"near full\" (e.g., 10% of bin capacity)\n        near_full_mask = (bins_remain_cap <= (item + near_full_threshold)) & fit_mask\n        priorities[near_full_mask] += 0.3 #Big bonus for filling near-full bins.\n    # Overflow Heuristic: Only used when NO bin fits.\n    else:\n        overflow_mask = ~fit_mask\n        priorities[overflow_mask] = - (item - bins_remain_cap[overflow_mask]) / (np.max(bins_remain_cap) + 0.000001)\n        # Try to balance load (least overflow)\n        min_overflow = np.min(item - bins_remain_cap[overflow_mask])\n        least_overflow_mask = (item - bins_remain_cap == min_overflow) & overflow_mask\n        priorities[least_overflow_mask] += 0.2 #Bonus for minimizing overflow when no fit\n\n    # Bin balancing. Incentivize bins with higher remaining capacity (avoid using bins too unevenly if possible)\n    priorities += bins_remain_cap / (np.sum(bins_remain_cap) + 0.000001) #Added term for load balancing across bins\n\n    return priorities",
    "response_id": 2,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 317.77665530336594,
    "mi": 78.8165571098739,
    "token_count": 209.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response2.txt_stdout.txt",
    "code_path": "problem_iter11_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste, overflow, fullness, and adaptive bin handling for priority.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    waste = bins_remain_cap - item\n    max_cap = np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    avg_cap = np.mean(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n\n    # Hyperparameters (Tunable)\n    fit_reward = 1.0\n    overflow_penalty = 0.5\n    fullness_bonus = 0.2\n    close_fit_boost = 0.7\n    close_fit_threshold = 0.2\n    empty_bin_penalty = 0.3\n    diversity_bonus_weight = 0.01\n\n    # Fit Reward\n    fit_mask = waste >= 0\n    priorities[fit_mask] += fit_reward / (waste[fit_mask] + 0.000001)\n\n    # Overflow Penalty\n    overflow_mask = ~fit_mask\n    overflow = item - bins_remain_cap[overflow_mask]\n    priorities[overflow_mask] -= overflow_penalty * overflow / (max_cap + 0.000001)\n\n    # Fullness Bonus\n    fullness = 1 - bins_remain_cap / (max_cap+0.000001)\n    priorities += fullness_bonus * fullness\n\n    # Close Fit Boost\n    close_fit_mask = fit_mask & (waste <= (close_fit_threshold * max_cap))\n    if np.any(close_fit_mask):\n        ratios = item / bins_remain_cap[close_fit_mask]\n        priorities[close_fit_mask] += close_fit_boost * np.log(ratios)\n\n    # Adaptive Empty Bin Handling\n    near_empty_mask = bins_remain_cap > (0.9 * max_cap)\n    if item > 0.5 * max_cap:\n        priorities[near_empty_mask] -= 0.05 * empty_bin_penalty\n    else:\n        priorities[near_empty_mask] -= empty_bin_penalty\n\n    # Bin Diversity Consideration\n    cap_diff = np.abs(bins_remain_cap - avg_cap)\n    diversity_bonus = diversity_bonus_weight * (max_cap - cap_diff)\n    priorities += diversity_bonus\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 1.6753091344236206,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 322.4095353505972,
    "mi": 83.84711046495362,
    "token_count": 197.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response1.txt_stdout.txt",
    "code_path": "problem_iter9_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins considering waste, overflow, fullness, and adaptive strategies.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    waste = bins_remain_cap - item\n    max_cap = np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    avg_cap = np.mean(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    \n    #Hyperparameters (Tuned using some manual exploration and intuition)\n    fit_reward = 1.0\n    overflow_penalty = 0.5\n    fullness_bonus = 0.2\n    close_fit_boost = 0.7\n    close_fit_threshold = 0.2\n    empty_bin_penalty = 0.3\n    \n    # Reward bins where the item fits\n    fit_mask = waste >= 0\n    priorities[fit_mask] += fit_reward / (waste[fit_mask] + 0.000001)\n\n    # Penalize overflow, relative to the maximum bin capacity\n    overflow_mask = ~fit_mask\n    overflow = item - bins_remain_cap[overflow_mask]\n    priorities[overflow_mask] -= overflow_penalty * overflow / (max_cap + 0.000001)\n\n    # Bonus for bins that are already relatively full\n    fullness = 1 - bins_remain_cap / (max_cap+0.000001)\n    priorities += fullness_bonus * fullness\n\n    # Further boost bins with small waste, using a ratio-based approach\n    close_fit_mask = fit_mask & (waste <= (close_fit_threshold * max_cap))\n    if np.any(close_fit_mask):\n        ratios = item / bins_remain_cap[close_fit_mask]\n        priorities[close_fit_mask] += close_fit_boost * np.log(ratios)\n\n    # Adaptive Empty Bin Handling: Penalize near-empty bins less if item is large\n    empty_bin_threshold = 0.1 * max_cap\n    near_empty_mask = bins_remain_cap > (0.9 * max_cap) # or > (max_cap - empty_bin_threshold)\n    if item > 0.5 * max_cap:  # If item is relatively large\n          priorities[near_empty_mask] -= 0.05 * empty_bin_penalty #Reduced penalty\n    else:\n          priorities[near_empty_mask] -= empty_bin_penalty  #Standard penalty\n          \n    #Bin Diversity Consideration\n    cap_diff = np.abs(bins_remain_cap - avg_cap)\n    diversity_bonus = 0.01 * (max_cap - cap_diff) # Bias toward bins that have capacities closer to the average\n    priorities += diversity_bonus\n\n    return priorities",
    "response_id": 1,
    "tryHS": true,
    "obj": 1.6753091344236206,
    "SLOC": 30.0,
    "cyclomatic_complexity": 5.0,
    "halstead": 504.05700000156156,
    "mi": 76.9490152829919,
    "token_count": 310.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter13_response0.txt_stdout.txt",
    "code_path": "problem_iter13_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, fit_priority_increase: float = 0.6502353301165812, best_fit_bonus: float = 0.9622107935804203, near_full_threshold: float = 0.9861530516189907, near_full_bonus: float = 0.6602366997289232, least_overflow_bonus: float = 0.723329145439484) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Enhanced priority function considering multiple factors for better bin packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n        fit_priority_increase: Increase in priority for bins where the item fits.\n        best_fit_bonus: Bonus for bins that are the best fit (minimal waste).\n        near_full_threshold: Threshold for considering a bin \"near full\" (as a fraction of item size).\n        near_full_bonus: Bonus for filling bins that are near full.\n        least_overflow_bonus: Bonus for minimizing overflow when no fit is possible.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    bin_count = len(bins_remain_cap)\n\n    # Calculate waste if item is placed in each bin\n    waste = bins_remain_cap - item\n\n    # Fit Heuristic: Prioritize bins where the item fits.\n    fit_mask = waste >= 0\n    if np.any(fit_mask):  # Only apply if at least one bin can fit the item\n        priorities[fit_mask] = (1 / (waste[fit_mask] + 0.000001)) + fit_priority_increase  # Increate priority compared to overflow. Avoid zero division.\n\n        # Best Fit Improvement:  Slightly boost the priority of bins with minimal waste.\n        min_waste = np.min(waste[fit_mask])\n        best_fit_mask = (waste == min_waste) & fit_mask\n        priorities[best_fit_mask] += best_fit_bonus  # A small bonus for the best fit\n\n        # Reward near-full bins:\n        near_full_mask = (bins_remain_cap <= (item + near_full_threshold)) & fit_mask\n        priorities[near_full_mask] += near_full_bonus #Big bonus for filling near-full bins.\n    # Overflow Heuristic: Only used when NO bin fits.\n    else:\n        overflow_mask = ~fit_mask\n        priorities[overflow_mask] = - (item - bins_remain_cap[overflow_mask]) / (np.max(bins_remain_cap) + 0.000001)\n        # Try to balance load (least overflow)\n        min_overflow = np.min(item - bins_remain_cap[overflow_mask])\n        least_overflow_mask = (item - bins_remain_cap == min_overflow) & overflow_mask\n        priorities[least_overflow_mask] += least_overflow_bonus #Bonus for minimizing overflow when no fit\n\n    # Bin balancing. Incentivize bins with higher remaining capacity (avoid using bins too unevenly if possible)\n    priorities += bins_remain_cap / (np.sum(bins_remain_cap) + 0.000001) #Added term for load balancing across bins\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 20.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 400.90527603206624,
    "mi": 73.20578358461792,
    "token_count": 280.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response2.txt_stdout.txt",
    "code_path": "problem_iter12_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins considering waste, overflow, fullness, item size relative to bin sizes, and adaptive strategies.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    waste = bins_remain_cap - item\n    max_cap = np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    avg_cap = np.mean(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    min_cap = np.min(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    \n    # Hyperparameters (Tuned using some manual exploration and intuition)\n    fit_reward = 1.2  # Slightly increased reward for fitting\n    overflow_penalty = 0.6 # Slightly increased penalty for overflow\n    fullness_bonus = 0.25 # Slightly increased bonus for fullness\n    close_fit_boost = 0.8  # Increased boost for close fits\n    close_fit_threshold = 0.2\n    empty_bin_penalty = 0.3\n    item_size_penalty_factor = 0.5 # Penalty scaling for item size vs bin size.\n    capacity_std = np.std(bins_remain_cap) if len(bins_remain_cap) > 1 else 0.0  # standard deviation of capacities\n    std_dev_penalty = 0.05 #Penalty associated with high standard deviation\n    \n    # Reward bins where the item fits\n    fit_mask = waste >= 0\n    priorities[fit_mask] += fit_reward / (waste[fit_mask] + 0.000001)\n\n    # Penalize overflow, relative to the maximum bin capacity\n    overflow_mask = ~fit_mask\n    overflow = item - bins_remain_cap[overflow_mask]\n    priorities[overflow_mask] -= overflow_penalty * overflow / (max_cap + 0.000001)\n\n    # Bonus for bins that are already relatively full\n    fullness = 1 - bins_remain_cap / (max_cap+0.000001)\n    priorities += fullness_bonus * fullness\n\n    # Further boost bins with small waste, using a ratio-based approach\n    close_fit_mask = fit_mask & (waste <= (close_fit_threshold * max_cap))\n    if np.any(close_fit_mask):\n        ratios = item / bins_remain_cap[close_fit_mask]\n        priorities[close_fit_mask] += close_fit_boost * np.log(ratios)\n\n    # Adaptive Empty Bin Handling: Penalize near-empty bins less if item is large\n    empty_bin_threshold = 0.1 * max_cap\n    near_empty_mask = bins_remain_cap > (0.9 * max_cap)\n    if item > 0.5 * max_cap:  # If item is relatively large\n          priorities[near_empty_mask] -= 0.05 * empty_bin_penalty  # Reduced penalty\n    else:\n          priorities[near_empty_mask] -= empty_bin_penalty  # Standard penalty\n\n    # Item Size Relative to Bin Size Penalty:\n    # Penalize bins that are only slightly larger than the item. This encourages using larger bins\n    # for larger items and smaller bins for smaller items\n    slightly_larger_mask = fit_mask & (waste < (0.5 * item))\n    priorities[slightly_larger_mask] -= item_size_penalty_factor * (item / (max_cap + 0.000001))\n\n    #Bin Diversity Consideration\n    cap_diff = np.abs(bins_remain_cap - avg_cap)\n    diversity_bonus = 0.01 * (max_cap - cap_diff) # Bias toward bins that have capacities closer to the average\n    priorities += diversity_bonus\n    \n    # Capacity standard deviation penalty.\n    priorities -= std_dev_penalty * capacity_std / (max_cap + 0.000001)\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 1.5157558835261316,
    "SLOC": 40.0,
    "cyclomatic_complexity": 9.0,
    "halstead": 829.4364908899223,
    "mi": 72.34442958723001,
    "token_count": 408.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response6.txt_stdout.txt",
    "code_path": "problem_iter14_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on adaptive waste, overflow, fullness and diversity.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    waste = bins_remain_cap - item\n    max_cap = np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    avg_cap = np.mean(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n\n    fit_reward = 1.0\n    overflow_penalty = 0.5\n    fullness_bonus = 0.2\n    close_fit_boost = 0.7\n    close_fit_threshold = 0.2\n    empty_bin_penalty = 0.3\n\n    # Fit priority\n    fit_mask = waste >= 0\n    priorities[fit_mask] += fit_reward / (waste[fit_mask] + 0.000001)\n\n    # Overflow penalty\n    overflow_mask = ~fit_mask\n    overflow = item - bins_remain_cap[overflow_mask]\n    priorities[overflow_mask] -= overflow_penalty * overflow / (max_cap + 0.000001)\n\n    # Fullness bonus\n    fullness = 1 - bins_remain_cap / (max_cap + 0.000001)\n    priorities += fullness_bonus * fullness\n\n    # Close fit bonus\n    close_fit_mask = fit_mask & (waste <= (close_fit_threshold * max_cap))\n    if np.any(close_fit_mask):\n        ratios = item / bins_remain_cap[close_fit_mask]\n        priorities[close_fit_mask] += close_fit_boost * np.log(ratios)\n    \n    #Adaptive Empty Bin Handling\n    empty_bin_threshold = 0.1 * max_cap\n    near_empty_mask = bins_remain_cap > (0.9 * max_cap)\n    if item > 0.5 * max_cap:\n        priorities[near_empty_mask] -= 0.05 * empty_bin_penalty\n    else:\n        priorities[near_empty_mask] -= empty_bin_penalty\n\n    # Bin diversity consideration\n    cap_diff = np.abs(bins_remain_cap - avg_cap)\n    diversity_bonus = 0.01 * (max_cap - cap_diff)\n    priorities += diversity_bonus\n    \n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 1.6753091344236206,
    "SLOC": 33.0,
    "cyclomatic_complexity": 5.0,
    "halstead": 611.7948771336315,
    "mi": 74.93553140837257,
    "token_count": 327.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response2.txt_stdout.txt",
    "code_path": "problem_iter15_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins based on a combination of factors, with adaptive weighting to improve performance.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    waste = bins_remain_cap - item\n    max_cap = np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    avg_cap = np.mean(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    min_cap = np.min(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    capacity_std = np.std(bins_remain_cap) if len(bins_remain_cap) > 1 else 0.0\n    \n    # Adaptive Weighting Factors (Initialized, to be potentially modified based on conditions)\n    fit_reward_weight = 1.2\n    overflow_penalty_weight = 0.7\n    fullness_bonus_weight = 0.3\n    close_fit_boost_weight = 0.9\n    empty_bin_penalty_weight = 0.35\n    item_size_penalty_weight = 0.6\n    std_dev_penalty_weight = 0.06\n    diversity_bonus_weight = 0.01\n\n    close_fit_threshold = 0.2\n\n    # Scenario-Specific Adjustments (Example: Adapt weights based on item size relative to bin sizes)\n    if item > 0.7 * max_cap:\n        # Increase importance of fitting and reduce penalty for some waste\n        fit_reward_weight *= 1.1\n        overflow_penalty_weight *= 0.9\n        close_fit_boost_weight *= 0.9  # Closer fit is less critical with larger items\n    elif item < 0.3 * max_cap:\n        # Emphasize fullness and penalize using almost empty bins\n        fullness_bonus_weight *= 1.2\n        empty_bin_penalty_weight *= 1.3\n        item_size_penalty_weight *= 0.8 # Reduce size penalty for small item\n\n    # Reward bins where the item fits\n    fit_mask = waste >= 0\n    priorities[fit_mask] += fit_reward_weight / (waste[fit_mask] + 0.000001)\n\n    # Penalize overflow, relative to the maximum bin capacity\n    overflow_mask = ~fit_mask\n    overflow = item - bins_remain_cap[overflow_mask]\n    priorities[overflow_mask] -= overflow_penalty_weight * overflow / (max_cap + 0.000001)\n\n    # Bonus for bins that are already relatively full\n    fullness = 1 - bins_remain_cap / (max_cap+0.000001)\n    priorities += fullness_bonus_weight * fullness\n\n    # Further boost bins with small waste, using a ratio-based approach\n    close_fit_mask = fit_mask & (waste <= (close_fit_threshold * max_cap))\n    if np.any(close_fit_mask):\n        ratios = item / bins_remain_cap[close_fit_mask]\n        priorities[close_fit_mask] += close_fit_boost_weight * np.log(ratios)\n\n    # Adaptive Empty Bin Handling: Penalize near-empty bins less if item is large\n    empty_bin_threshold = 0.1 * max_cap\n    near_empty_mask = bins_remain_cap > (0.9 * max_cap)\n    if item > 0.5 * max_cap:  # If item is relatively large\n          priorities[near_empty_mask] -= 0.05 * empty_bin_penalty_weight  # Reduced penalty\n    else:\n          priorities[near_empty_mask] -= empty_bin_penalty_weight  # Standard penalty\n\n    # Item Size Relative to Bin Size Penalty:\n    # Penalize bins that are only slightly larger than the item. This encourages using larger bins\n    # for larger items and smaller bins for smaller items\n    slightly_larger_mask = fit_mask & (waste < (0.5 * item))\n    priorities[slightly_larger_mask] -= item_size_penalty_weight * (item / (max_cap + 0.000001))\n\n    #Bin Diversity Consideration\n    cap_diff = np.abs(bins_remain_cap - avg_cap)\n    diversity_bonus = diversity_bonus_weight * (max_cap - cap_diff) # Bias toward bins that have capacities closer to the average\n    priorities += diversity_bonus\n    \n    # Capacity standard deviation penalty.\n    priorities -= std_dev_penalty_weight * capacity_std / (max_cap + 0.000001)\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 1.1767052253689738,
    "SLOC": 44.0,
    "cyclomatic_complexity": 7.0,
    "halstead": 1291.2540037804947,
    "mi": 70.35317250815551,
    "token_count": 500.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter16_response4.txt_stdout.txt",
    "code_path": "problem_iter16_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray,\n                  fit_reward: float = 0.9038723604333703,\n                  overflow_penalty: float = 0.5514080157850613,\n                  fullness_bonus: float = 0.3083172703843148,\n                  close_fit_boost: float = 1.1912791615214977,\n                  close_fit_threshold: float = 0.3292863189318351,\n                  empty_bin_penalty: float = 0.4200769727918078,\n                  large_item_threshold: float = 0.3258499455548476,\n                  reduced_empty_bin_penalty_factor: float = 0.027783936542707534,\n                  diversity_bonus_factor: float = 0.010456913489370105,\n                  near_empty_threshold: float = 0.918347896026916) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins considering waste, overflow, fullness, and adaptive strategies.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    waste = bins_remain_cap - item\n    max_cap = np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    avg_cap = np.mean(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    \n    # Reward bins where the item fits\n    fit_mask = waste >= 0\n    priorities[fit_mask] += fit_reward / (waste[fit_mask] + 0.000001)\n\n    # Penalize overflow, relative to the maximum bin capacity\n    overflow_mask = ~fit_mask\n    overflow = item - bins_remain_cap[overflow_mask]\n    priorities[overflow_mask] -= overflow_penalty * overflow / (max_cap + 0.000001)\n\n    # Bonus for bins that are already relatively full\n    fullness = 1 - bins_remain_cap / (max_cap+0.000001)\n    priorities += fullness_bonus * fullness\n\n    # Further boost bins with small waste, using a ratio-based approach\n    close_fit_mask = fit_mask & (waste <= (close_fit_threshold * max_cap))\n    if np.any(close_fit_mask):\n        ratios = item / bins_remain_cap[close_fit_mask]\n        priorities[close_fit_mask] += close_fit_boost * np.log(ratios)\n\n    # Adaptive Empty Bin Handling: Penalize near-empty bins less if item is large\n    empty_bin_threshold = 0.1 * max_cap\n    near_empty_mask = bins_remain_cap > (near_empty_threshold * max_cap) # or > (max_cap - empty_bin_threshold)\n    if item > large_item_threshold * max_cap:  # If item is relatively large\n          priorities[near_empty_mask] -= reduced_empty_bin_penalty_factor * empty_bin_penalty #Reduced penalty\n    else:\n          priorities[near_empty_mask] -= empty_bin_penalty  #Standard penalty\n          \n    #Bin Diversity Consideration\n    cap_diff = np.abs(bins_remain_cap - avg_cap)\n    diversity_bonus = diversity_bonus_factor * (max_cap - cap_diff) # Bias toward bins that have capacities closer to the average\n    priorities += diversity_bonus\n\n    return priorities",
    "response_id": 4,
    "tryHS": true,
    "obj": 1.126844834463509,
    "SLOC": 36.0,
    "cyclomatic_complexity": 5.0,
    "halstead": 632.0372937301115,
    "mi": 76.60502468646449,
    "token_count": 365.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response1.txt_stdout.txt",
    "code_path": "problem_iter17_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins using adaptive weighting and multiple factors.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    waste = bins_remain_cap - item\n    max_cap = np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    avg_cap = np.mean(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n\n    # Hyperparameters with adaptive adjustments\n    fit_reward = 1.0\n    overflow_penalty = 0.5\n    fullness_bonus = 0.2\n    close_fit_boost = 0.7\n    close_fit_threshold = 0.2\n    empty_bin_penalty = 0.3\n    diversity_bonus_weight = 0.01\n    item_size_penalty = 0.4 #New factor\n\n    # Adaptive adjustments based on item size\n    if item > 0.7 * max_cap:\n        fit_reward *= 1.1\n        overflow_penalty *= 0.9\n        close_fit_boost *= 0.9\n    elif item < 0.3 * max_cap:\n        fullness_bonus *= 1.2\n        empty_bin_penalty *= 1.3\n        item_size_penalty *= 0.6\n\n    # Fit Reward\n    fit_mask = waste >= 0\n    priorities[fit_mask] += fit_reward / (waste[fit_mask] + 0.000001)\n\n    # Overflow Penalty\n    overflow_mask = ~fit_mask\n    overflow = item - bins_remain_cap[overflow_mask]\n    priorities[overflow_mask] -= overflow_penalty * overflow / (max_cap + 0.000001)\n\n    # Fullness Bonus\n    fullness = 1 - bins_remain_cap / (max_cap + 0.000001)\n    priorities += fullness_bonus * fullness\n\n    # Close Fit Boost\n    close_fit_mask = fit_mask & (waste <= (close_fit_threshold * max_cap))\n    if np.any(close_fit_mask):\n        ratios = item / bins_remain_cap[close_fit_mask]\n        priorities[close_fit_mask] += close_fit_boost * np.log(ratios)\n\n    # Adaptive Empty Bin Handling\n    near_empty_mask = bins_remain_cap > (0.9 * max_cap)\n    if item > 0.5 * max_cap:\n        priorities[near_empty_mask] -= 0.05 * empty_bin_penalty\n    else:\n        priorities[near_empty_mask] -= empty_bin_penalty\n\n    # Bin Diversity Consideration\n    cap_diff = np.abs(bins_remain_cap - avg_cap)\n    diversity_bonus = diversity_bonus_weight * (max_cap - cap_diff)\n    priorities += diversity_bonus\n    \n    # Item Size penalty\n    slightly_larger_mask = fit_mask & (waste < (0.5 * item))\n    priorities[slightly_larger_mask] -= item_size_penalty * (item / (max_cap + 0.000001))\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 1.0370961308336748,
    "SLOC": 38.0,
    "cyclomatic_complexity": 6.0,
    "halstead": 887.95183128494,
    "mi": 66.54997485447755,
    "token_count": 388.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter18_response3.txt_stdout.txt",
    "code_path": "problem_iter18_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins using adaptive weighting, multiple factors, and item-aware adjustments.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    waste = bins_remain_cap - item\n    max_cap = np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    avg_cap = np.mean(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    min_cap = np.min(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0 # New\n\n    # Hyperparameters with adaptive adjustments\n    fit_reward = 1.0\n    overflow_penalty = 0.5\n    fullness_bonus = 0.2\n    close_fit_boost = 0.7\n    close_fit_threshold = 0.2\n    empty_bin_penalty = 0.3\n    diversity_bonus_weight = 0.01\n    item_size_penalty = 0.4\n    fragmentation_penalty = 0.1  # New\n\n    # Adaptive adjustments based on item size relative to bin capacities\n    item_ratio_max = item / max_cap if max_cap > 0 else 0.0\n    item_ratio_avg = item / avg_cap if avg_cap > 0 else 0.0\n    item_ratio_min = item / min_cap if min_cap > 0 else 0.0 #New\n\n    if item_ratio_max > 0.7:  # Large item relative to largest bin\n        fit_reward *= 1.1\n        overflow_penalty *= 0.9\n        close_fit_boost *= 0.9\n        empty_bin_penalty *= 0.5 # Reduced for large items\n    elif item_ratio_max < 0.3:  # Small item relative to largest bin\n        fullness_bonus *= 1.2\n        empty_bin_penalty *= 1.3\n        item_size_penalty *= 0.6\n        fragmentation_penalty *= 1.2 # Increased for small items, to avoid many small gaps\n    \n    if item_ratio_avg > 1.0: # item is larger than the average capacity\n        fit_reward *= 0.8\n        overflow_penalty *= 1.2\n\n    # Fit Reward\n    fit_mask = waste >= 0\n    priorities[fit_mask] += fit_reward / (waste[fit_mask] + 0.000001)\n\n    # Overflow Penalty\n    overflow_mask = ~fit_mask\n    overflow = item - bins_remain_cap[overflow_mask]\n    priorities[overflow_mask] -= overflow_penalty * overflow / (max_cap + 0.000001)\n\n    # Fullness Bonus\n    fullness = 1 - bins_remain_cap / (max_cap + 0.000001)\n    priorities += fullness_bonus * fullness\n\n    # Close Fit Boost\n    close_fit_mask = fit_mask & (waste <= (close_fit_threshold * max_cap))\n    if np.any(close_fit_mask):\n        ratios = item / bins_remain_cap[close_fit_mask]\n        priorities[close_fit_mask] += close_fit_boost * np.log(ratios)\n\n    # Adaptive Empty Bin Handling\n    near_empty_mask = bins_remain_cap > (0.9 * max_cap)\n    if item > 0.5 * max_cap:\n        priorities[near_empty_mask] -= 0.05 * empty_bin_penalty\n    else:\n        priorities[near_empty_mask] -= empty_bin_penalty\n\n    # Bin Diversity Consideration\n    cap_diff = np.abs(bins_remain_cap - avg_cap)\n    diversity_bonus = diversity_bonus_weight * (max_cap - cap_diff)\n    priorities += diversity_bonus\n    \n    # Item Size penalty\n    slightly_larger_mask = fit_mask & (waste < (0.5 * item))\n    priorities[slightly_larger_mask] -= item_size_penalty * (item / (max_cap + 0.000001))\n    \n    # Fragmentation Penalty (encourage filling up bins)\n    priorities[fit_mask] -= fragmentation_penalty * (waste[fit_mask] / (max_cap + 0.000001))\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 1.1069006781013186,
    "SLOC": 52.0,
    "cyclomatic_complexity": 9.0,
    "halstead": 1230.6022077071073,
    "mi": 65.76655526532262,
    "token_count": 486.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter19_hs0.txt_stdout.txt",
    "code_path": "problem_iter19_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray,\n                fit_reward: float = 1.8606401153475742,\n                overflow_penalty: float = 0.23311623299493256,\n                fullness_bonus: float = 0.24692029962156997,\n                close_fit_boost: float = 0.755929203444544,\n                close_fit_threshold: float = 0.31756747877776315,\n                empty_bin_penalty: float = 0.18111007569931253,\n                large_item_threshold: float = 0.6365035670064028,\n                reduced_empty_bin_penalty_factor: float = 0.09119484752969076,\n                diversity_bonus_factor: float = 0.004873566384835245,\n                near_empty_threshold: float = 0.8819279417449031) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins considering waste, overflow, fullness, and adaptive strategies.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    waste = bins_remain_cap - item\n    max_cap = np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    avg_cap = np.mean(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    \n    # Reward bins where the item fits\n    fit_mask = waste >= 0\n    priorities[fit_mask] += fit_reward / (waste[fit_mask] + 0.000001)\n\n    # Penalize overflow, relative to the maximum bin capacity\n    overflow_mask = ~fit_mask\n    overflow = item - bins_remain_cap[overflow_mask]\n    priorities[overflow_mask] -= overflow_penalty * overflow / (max_cap + 0.000001)\n\n    # Bonus for bins that are already relatively full\n    fullness = 1 - bins_remain_cap / (max_cap+0.000001)\n    priorities += fullness_bonus * fullness\n\n    # Further boost bins with small waste, using a ratio-based approach\n    close_fit_mask = fit_mask & (waste <= (close_fit_threshold * max_cap))\n    if np.any(close_fit_mask):\n        ratios = item / bins_remain_cap[close_fit_mask]\n        priorities[close_fit_mask] += close_fit_boost * np.log(ratios)\n\n    # Adaptive Empty Bin Handling: Penalize near-empty bins less if item is large\n    empty_bin_threshold = 0.1 * max_cap\n    near_empty_mask = bins_remain_cap > (near_empty_threshold * max_cap) # or > (max_cap - empty_bin_threshold)\n    if item > large_item_threshold * max_cap:  # If item is relatively large\n          priorities[near_empty_mask] -= reduced_empty_bin_penalty_factor * empty_bin_penalty #Reduced penalty\n    else:\n          priorities[near_empty_mask] -= empty_bin_penalty  #Standard penalty\n          \n    #Bin Diversity Consideration\n    cap_diff = np.abs(bins_remain_cap - avg_cap)\n    diversity_bonus = diversity_bonus_factor * (max_cap - cap_diff) # Bias toward bins that have capacities closer to the average\n    priorities += diversity_bonus\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 3.7495013960909587,
    "SLOC": 36.0,
    "cyclomatic_complexity": 5.0,
    "halstead": 632.0372937301115,
    "mi": 76.60502468646449,
    "token_count": 365.0,
    "exec_success": true
  }
]