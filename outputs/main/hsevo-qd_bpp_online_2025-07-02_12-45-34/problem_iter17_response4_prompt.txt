{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritizes bins considering waste, overflow, fullness, item size relative to bin sizes, and adaptive strategies.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    waste = bins_remain_cap - item\n    max_cap = np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    avg_cap = np.mean(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    min_cap = np.min(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    \n    # Hyperparameters (Tuned using some manual exploration and intuition)\n    fit_reward = 1.2  # Slightly increased reward for fitting\n    overflow_penalty = 0.6 # Slightly increased penalty for overflow\n    fullness_bonus = 0.25 # Slightly increased bonus for fullness\n    close_fit_boost = 0.8  # Increased boost for close fits\n    close_fit_threshold = 0.2\n    empty_bin_penalty = 0.3\n    item_size_penalty_factor = 0.5 # Penalty scaling for item size vs bin size.\n    capacity_std = np.std(bins_remain_cap) if len(bins_remain_cap) > 1 else 0.0  # standard deviation of capacities\n    std_dev_penalty = 0.05 #Penalty associated with high standard deviation\n    \n    # Reward bins where the item fits\n    fit_mask = waste >= 0\n    priorities[fit_mask] += fit_reward / (waste[fit_mask] + 0.000001)\n\n    # Penalize overflow, relative to the maximum bin capacity\n    overflow_mask = ~fit_mask\n    overflow = item - bins_remain_cap[overflow_mask]\n    priorities[overflow_mask] -= overflow_penalty * overflow / (max_cap + 0.000001)\n\n    # Bonus for bins that are already relatively full\n    fullness = 1 - bins_remain_cap / (max_cap+0.000001)\n    priorities += fullness_bonus * fullness\n\n    # Further boost bins with small waste, using a ratio-based approach\n    close_fit_mask = fit_mask & (waste <= (close_fit_threshold * max_cap))\n    if np.any(close_fit_mask):\n        ratios = item / bins_remain_cap[close_fit_mask]\n        priorities[close_fit_mask] += close_fit_boost * np.log(ratios)\n\n    # Adaptive Empty Bin Handling: Penalize near-empty bins less if item is large\n    empty_bin_threshold = 0.1 * max_cap\n    near_empty_mask = bins_remain_cap > (0.9 * max_cap)\n    if item > 0.5 * max_cap:  # If item is relatively large\n          priorities[near_empty_mask] -= 0.05 * empty_bin_penalty  # Reduced penalty\n    else:\n          priorities[near_empty_mask] -= empty_bin_penalty  # Standard penalty\n\n    # Item Size Relative to Bin Size Penalty:\n    # Penalize bins that are only slightly larger than the item. This encourages using larger bins\n    # for larger items and smaller bins for smaller items\n    slightly_larger_mask = fit_mask & (waste < (0.5 * item))\n    priorities[slightly_larger_mask] -= item_size_penalty_factor * (item / (max_cap + 0.000001))\n\n    #Bin Diversity Consideration\n    cap_diff = np.abs(bins_remain_cap - avg_cap)\n    diversity_bonus = 0.01 * (max_cap - cap_diff) # Bias toward bins that have capacities closer to the average\n    priorities += diversity_bonus\n    \n    # Capacity standard deviation penalty.\n    priorities -= std_dev_penalty * capacity_std / (max_cap + 0.000001)\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate waste if item is placed in each bin\n    waste = bins_remain_cap - item\n\n    # Give high priority to bins where item fits and waste is minimized\n    fit_mask = waste >= 0\n    priorities[fit_mask] = 1 / (waste[fit_mask] + 0.000001)  # Add a small constant to avoid division by zero\n\n    # Give slightly lower priority to bins where item doesn't fit, but the overflow is minimized\n    # This encourages splitting items across bins less often but still allows it when needed\n    overflow_mask = ~fit_mask\n    priorities[overflow_mask] = - (item - bins_remain_cap[overflow_mask]) / (np.max(bins_remain_cap) + 0.000001) #Prioritize bins closer to fitting the item\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the 1st heuristic incorporates significantly more factors into its priority calculation, including adaptive weighting, fullness bonus, close-fit boost, empty bin penalty, item size penalty, diversity consideration, and capacity standard deviation penalty, whereas the 20th only considers ratios of item size to bin capacity.\n(2nd best) vs (second worst): The 2nd heuristic is similar to the first, with most factors except bin size penalty. The second worst considers item size to bin capacity ratio, which may lead to faster calculation but ignores many bin properties.\nComparing (1st) vs (2nd), we see that the first heuristic utilizes scenario-specific adjustments to the weights based on item size, while the second does not change anything compare to the first.\n(3rd) vs (4th): These two heuristics are almost identical.\nComparing (second worst) vs (worst), we see that the second worst uses a log function, which will lead to negative values. It only considers the ratio between the item and bin sizes, while the worst-performing uses some hardcoded constants which is more likely to be unoptimized.\nOverall: The better heuristics incorporate a wider range of factors and adaptive strategies, while the worse heuristics focus on fewer factors or use less sophisticated methods. The use of adaptive weighting based on item size is a key differentiator for the best-performing heuristics. Also, penalties for bad fits and bonuses for good fits improved the models.\n- \nOkay, here's a redefined \"Current Self-Reflection\" aimed at improving heuristic design, specifically addressing the pitfalls of \"Ineffective Self-Reflection\":\n\n*   **Keywords:** Multi-faceted, adaptive, incremental refinement, hyperparameter tuning, problem-specific context, synergy, bias awareness.\n*   **Advice:** Design heuristics that combine multiple relevant factors (not just waste/overflow), dynamically adjusted based on the *specific* problem context and stage of the solution. Favor synergy between factors.\n*   **Avoid:** Over-reliance on simple ratios or single factors. Prematurely complex models. Blind hyperparameter tuning without understanding their impact.\n*   **Explanation:** Heuristics should evolve incrementally, adding complexity thoughtfully. Recognize inherent biases in chosen factors and address them proactively. Focus on designing systems that learn and adapt *within* the constraints of the problem.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}