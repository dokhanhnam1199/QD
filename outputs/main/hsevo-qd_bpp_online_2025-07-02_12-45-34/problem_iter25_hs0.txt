import numpy as np
import random
import math
import scipy
import torch
def priority_v2(item: float, bins_remain_cap: np.ndarray, 
                fit_reward: float = 2.5104701182492977, 
                overflow_penalty: float = 0.6320037557298047, 
                fullness_bonus: float = 0.14501166868832482, 
                close_fit_boost: float = 2.1705500011700534, 
                close_fit_threshold: float = 0.22941274160351566, 
                empty_bin_penalty: float = 0.26462954811389827,
                empty_bin_threshold_factor: float = 0.05891646238016135,
                large_item_threshold_factor: float = 0.6171376840036267,
                reduced_empty_bin_penalty_factor: float = 0.1024616828180952,
                diversity_bonus_factor: float = 0.004704940331520757,
                small_number: float = 8.034898163944783e-06,
                near_empty_threshold: float = 0.26939475940221647) -> np.ndarray:
    """
    Prioritizes bins considering waste, overflow, fullness, and adaptive strategies.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    waste = bins_remain_cap - item
    max_cap = np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0
    avg_cap = np.mean(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0
    

    # Reward bins where the item fits
    fit_mask = waste >= 0
    priorities[fit_mask] += fit_reward / (waste[fit_mask] + small_number)

    # Penalize overflow, relative to the maximum bin capacity
    overflow_mask = ~fit_mask
    overflow = item - bins_remain_cap[overflow_mask]
    priorities[overflow_mask] -= overflow_penalty * overflow / (max_cap + small_number)

    # Bonus for bins that are already relatively full
    fullness = 1 - bins_remain_cap / (max_cap+small_number)
    priorities += fullness_bonus * fullness

    # Further boost bins with small waste, using a ratio-based approach
    close_fit_mask = fit_mask & (waste <= (close_fit_threshold * max_cap))
    if np.any(close_fit_mask):
        ratios = item / bins_remain_cap[close_fit_mask]
        priorities[close_fit_mask] += close_fit_boost * np.log(ratios)

    # Adaptive Empty Bin Handling: Penalize near-empty bins less if item is large
    empty_bin_threshold = empty_bin_threshold_factor * max_cap
    near_empty_mask = bins_remain_cap > (near_empty_threshold * max_cap) # or > (max_cap - empty_bin_threshold)
    if item > large_item_threshold_factor * max_cap:  # If item is relatively large
          priorities[near_empty_mask] -= reduced_empty_bin_penalty_factor * empty_bin_penalty #Reduced penalty
    else:
          priorities[near_empty_mask] -= empty_bin_penalty  #Standard penalty
          
    #Bin Diversity Consideration
    cap_diff = np.abs(bins_remain_cap - avg_cap)
    diversity_bonus = diversity_bonus_factor * (max_cap - cap_diff) # Bias toward bins that have capacities closer to the average
    priorities += diversity_bonus

    return priorities
