```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Adaptive bin priority: fit, balance, item size. Weights adapt to item size."""
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    waste = bins_remain_cap - item
    max_cap = np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0
    avg_cap = np.mean(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0
    total_capacity = np.sum(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0
    num_bins = len(bins_remain_cap)

    # Hyperparameters (adaptive)
    fit_reward = 1.0
    overflow_penalty = 0.5
    fullness_bonus = 0.2
    close_fit_boost = 0.7
    close_fit_threshold = 0.2
    empty_bin_penalty = 0.3
    diversity_bonus_weight = 0.01
    item_size_penalty = 0.4
    capacity_utilization_bonus_weight = 0.1

    # Adaptive adjustments based on item size
    if item > 0.7 * max_cap:
        fit_reward *= 1.1
        overflow_penalty *= 0.9
        close_fit_boost *= 0.9
    elif item < 0.3 * max_cap:
        fullness_bonus *= 1.2
        empty_bin_penalty *= 1.3
        item_size_penalty *= 0.6

    # Fit Reward
    fit_mask = waste >= 0
    if np.any(fit_mask):
        priorities[fit_mask] += fit_reward / (waste[fit_mask] + 0.000001)

        # Best Fit Improvement:  Slightly boost the priority of bins with minimal waste.
        min_waste = np.min(waste[fit_mask])
        best_fit_mask = (waste == min_waste) & fit_mask
        priorities[best_fit_mask] += 0.2

        # Reward near-full bins:
        near_full_threshold = 0.1
        near_full_mask = (bins_remain_cap <= (item + near_full_threshold * max_cap)) & fit_mask
        priorities[near_full_mask] += 0.3
    # Overflow Penalty
    else:
        overflow_mask = ~fit_mask
        overflow = item - bins_remain_cap[overflow_mask]
        priorities[overflow_mask] -= overflow_penalty * overflow / (max_cap + 0.000001)

        min_overflow = np.min(item - bins_remain_cap[overflow_mask])
        least_overflow_mask = (item - bins_remain_cap == min_overflow) & overflow_mask
        priorities[least_overflow_mask] += 0.2


    # Fullness Bonus
    fullness = 1 - bins_remain_cap / (max_cap + 0.000001)
    priorities += fullness_bonus * fullness

    # Close Fit Boost
    close_fit_mask = fit_mask & (waste <= (close_fit_threshold * max_cap))
    if np.any(close_fit_mask):
        ratios = item / bins_remain_cap[close_fit_mask]
        priorities[close_fit_mask] += close_fit_boost * np.log(ratios)

    # Adaptive Empty Bin Handling
    near_empty_mask = bins_remain_cap > (0.9 * max_cap)
    if item > 0.5 * max_cap:
        priorities[near_empty_mask] -= 0.05 * empty_bin_penalty
    else:
        priorities[near_empty_mask] -= empty_bin_penalty

    # Bin Diversity Consideration (Bin Balancing)
    cap_diff = np.abs(bins_remain_cap - avg_cap)
    diversity_bonus = diversity_bonus_weight * (max_cap - cap_diff)
    priorities += diversity_bonus

    # Item Size penalty
    slightly_larger_mask = fit_mask & (waste < (0.5 * item))
    priorities[slightly_larger_mask] -= item_size_penalty * (item / (max_cap + 0.000001))

    # Capacity Utilization Bonus (Global Balancing)
    priorities += capacity_utilization_bonus_weight * (bins_remain_cap / (total_capacity + 0.000001))


    return priorities
```
