```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Prioritizes bins considering waste, overflow, fullness, item size relative to bin sizes,
    adaptive strategies based on item size and bin capacity distribution, and bin diversity,
    with adaptive weight tuning.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    waste = bins_remain_cap - item
    max_cap = np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0
    avg_cap = np.mean(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0
    min_cap = np.min(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0
    capacity_std = np.std(bins_remain_cap) if len(bins_remain_cap) > 1 else 0.0
    
    # Adaptive Weight Tuning based on item size and capacity distribution
    item_size_ratio = item / max_cap
    capacity_std_ratio = capacity_std / max_cap

    # Initial Hyperparameters (Base values)
    fit_reward_base = 1.2
    overflow_penalty_base = 0.6
    fullness_bonus_base = 0.25
    close_fit_boost_base = 0.8
    empty_bin_penalty_base = 0.3
    item_size_penalty_factor_base = 0.5
    std_dev_penalty_base = 0.05
    diversity_bonus_factor_base = 0.01

    # Adaptive adjustments based on item size and capacity distribution
    fit_reward = fit_reward_base * (1 + 0.2 * item_size_ratio - 0.1 * capacity_std_ratio)
    overflow_penalty = overflow_penalty_base * (1 + 0.1 * item_size_ratio + 0.1 * capacity_std_ratio)
    fullness_bonus = fullness_bonus_base * (1 - 0.1 * item_size_ratio + 0.05 * capacity_std_ratio)
    close_fit_boost = close_fit_boost_base * (1 + 0.15 * item_size_ratio - 0.05 * capacity_std_ratio)
    empty_bin_penalty = empty_bin_penalty_base * (1 - 0.2 * item_size_ratio + 0.1 * capacity_std_ratio)
    item_size_penalty_factor = item_size_penalty_factor_base * (1 + 0.1 * item_size_ratio)
    std_dev_penalty = std_dev_penalty_base * (1 + 0.1 * capacity_std_ratio)
    diversity_bonus_factor = diversity_bonus_factor_base * (1 - 0.05 * capacity_std_ratio)
    
    close_fit_threshold = 0.2

    # Reward bins where the item fits
    fit_mask = waste >= 0
    priorities[fit_mask] += fit_reward / (waste[fit_mask] + 0.000001)

    # Penalize overflow, relative to the maximum bin capacity
    overflow_mask = ~fit_mask
    overflow = item - bins_remain_cap[overflow_mask]
    priorities[overflow_mask] -= overflow_penalty * overflow / (max_cap + 0.000001)

    # Bonus for bins that are already relatively full
    fullness = 1 - bins_remain_cap / (max_cap+0.000001)
    priorities += fullness_bonus * fullness

    # Further boost bins with small waste, using a ratio-based approach
    close_fit_mask = fit_mask & (waste <= (close_fit_threshold * max_cap))
    if np.any(close_fit_mask):
        ratios = item / bins_remain_cap[close_fit_mask]
        priorities[close_fit_mask] += close_fit_boost * np.log(ratios)

    # Adaptive Empty Bin Handling: Penalize near-empty bins less if item is large
    near_empty_mask = bins_remain_cap > (0.9 * max_cap)
    if item > 0.5 * max_cap:  # If item is relatively large
          priorities[near_empty_mask] -= 0.05 * empty_bin_penalty  # Reduced penalty
    else:
          priorities[near_empty_mask] -= empty_bin_penalty  # Standard penalty

    # Item Size Relative to Bin Size Penalty:
    slightly_larger_mask = fit_mask & (waste < (0.5 * item))
    priorities[slightly_larger_mask] -= item_size_penalty_factor * (item / (max_cap + 0.000001))

    # Bin Diversity Consideration
    cap_diff = np.abs(bins_remain_cap - avg_cap)
    diversity_bonus = diversity_bonus_factor * (max_cap - cap_diff)  # Bias toward bins closer to average
    priorities += diversity_bonus
    
    # Capacity standard deviation penalty.
    priorities -= std_dev_penalty * capacity_std / (max_cap + 0.000001)

    # Prioritize bins with capacity close to the item size
    capacity_similarity = np.exp(-np.abs(bins_remain_cap - item) / (0.2 * max_cap + 0.000001))
    priorities += 0.1 * capacity_similarity

    return priorities
```
