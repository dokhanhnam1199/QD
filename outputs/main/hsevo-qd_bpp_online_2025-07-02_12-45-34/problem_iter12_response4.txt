```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Prioritizes bins using a refined approach considering waste, overflow, fullness,
    adaptive thresholds, bin diversity, and item-size awareness.  Uses a weighted
    combination of factors and incorporates more nuanced penalties/rewards.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    waste = bins_remain_cap - item
    max_cap = np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0
    avg_cap = np.mean(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0
    min_cap = np.min(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0


    # Hyperparameters (more extensively tuned based on rationale)
    fit_reward = 1.2          # Increased reward for fitting
    overflow_penalty = 0.6    # Slightly higher penalty for overflow
    fullness_bonus = 0.3      # Increased importance of fullness
    close_fit_boost = 0.8     # Further boost for close fits
    close_fit_threshold = 0.15  # Slightly tighter close-fit definition
    empty_bin_penalty = 0.4     # Increased penalty for near-empty bins
    diversity_bonus_scale = 0.015 # Increased scaling for bin diversity
    large_item_multiplier = 0.7   # Reduces importance of diversity for large items
    min_cap_penalty = 0.2

    # Reward bins where the item fits, scaled by remaining capacity
    fit_mask = waste >= 0
    priorities[fit_mask] += fit_reward / (0.000001 + waste[fit_mask])

    # Penalize overflow, relative to the maximum bin capacity
    overflow_mask = ~fit_mask
    overflow = item - bins_remain_cap[overflow_mask]
    priorities[overflow_mask] -= overflow_penalty * overflow / (max_cap + 0.000001)

    # Bonus for bins that are already relatively full
    fullness = 1 - bins_remain_cap / (max_cap + 0.000001)
    priorities += fullness_bonus * fullness

    # Further boost bins with small waste, using a ratio-based approach
    close_fit_mask = fit_mask & (waste <= (close_fit_threshold * max_cap))
    if np.any(close_fit_mask):
        ratios = item / bins_remain_cap[close_fit_mask]
        priorities[close_fit_mask] += close_fit_boost * np.log(ratios)

    # Adaptive Empty Bin Handling: Penalize near-empty bins less if item is large
    near_empty_mask = bins_remain_cap > (0.9 * max_cap)
    if item > 0.5 * max_cap:  # If item is relatively large
        priorities[near_empty_mask] -= 0.05 * empty_bin_penalty  # Reduced penalty
    else:
        priorities[near_empty_mask] -= empty_bin_penalty  # Standard penalty

    # Bin Diversity Consideration, scaled by item size
    cap_diff = np.abs(bins_remain_cap - avg_cap)
    diversity_bonus = diversity_bonus_scale * (max_cap - cap_diff)
    if item > 0.75 * max_cap: # Reduce diversity importance for very large items
        diversity_bonus *= large_item_multiplier
    priorities += diversity_bonus

    #Penalize bins that have very small remaining capacity in general.
    min_cap_mask = bins_remain_cap == min_cap
    if np.any(min_cap_mask) and item <= min_cap:
        priorities[min_cap_mask] -= min_cap_penalty/ (bins_remain_cap[min_cap_mask] + 0.000001)

    return priorities
```
