```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Prioritizes bins using a more adaptive and refined strategy, 
    dynamically adjusting weights based on item size and bin capacity distribution.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    
    # Basic statistics of remaining capacities
    max_cap = np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0
    avg_cap = np.mean(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0
    min_cap = np.min(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0
    std_cap = np.std(bins_remain_cap) if len(bins_remain_cap) > 1 else 0.0
    
    # --- Adaptive Weighting Factors (Dynamically Adjusted) ---
    
    # Fit Reward: Smaller items get a higher reward for perfect fits
    fit_reward_base = 1.5 
    fit_reward = fit_reward_base + 0.5 * (1 - item / max_cap)  # Larger items, less reward for fitting
    
    # Overflow Penalty: Scale penalty based on how much the item overflows
    overflow_penalty_base = 0.7
    overflow_penalty = overflow_penalty_base + 0.3 * (item / max_cap)  # Larger items, higher overflow penalty

    # Fullness Bonus: Bins closer to average fullness get a bigger bonus, modulated by item size
    fullness_bonus_base = 0.3
    fullness_bonus = fullness_bonus_base * (1 - 0.5 * (item / max_cap)) #Larger items, less importance of average fullness

    # Close Fit Boost: Promote bins where waste is minimal, especially for smaller items
    close_fit_boost_base = 1.0
    close_fit_boost = close_fit_boost_base + 0.4 * (1 - item / max_cap) #Smaller items, larger close fit boost

    # Empty Bin Handling: Adaptive penalty; less penalty for larger items to encourage opening new bins
    empty_bin_penalty_base = 0.4
    empty_bin_penalty = empty_bin_penalty_base * (1 - 0.2 * (item / max_cap))

    # Item Size Relative Penalty: Discourage placing items in bins only slightly larger, adjusted by std dev
    item_size_penalty_factor_base = 0.6
    item_size_penalty_factor = item_size_penalty_factor_base + 0.2 * (std_cap / (max_cap + 0.000001))

    # Capacity Standard Deviation Penalty: Penalize high standard deviation among bin capacities
    std_dev_penalty_base = 0.1
    std_dev_penalty = std_dev_penalty_base * (1 + 0.3 * (item / max_cap)) #Larger Items, more penalty of high capacity standard deviation

    # --- Core Logic ---
    
    waste = bins_remain_cap - item
    fit_mask = waste >= 0
    overflow_mask = ~fit_mask

    # Reward bins where the item fits
    if np.any(fit_mask):
        priorities[fit_mask] += fit_reward / (waste[fit_mask] + 0.000001)

    # Penalize overflow
    if np.any(overflow_mask):
        overflow = item - bins_remain_cap[overflow_mask]
        priorities[overflow_mask] -= overflow_penalty * overflow / (max_cap + 0.000001)

    # Bonus for bins that are relatively full, especially around the average
    fullness = 1 - bins_remain_cap / (max_cap + 0.000001)
    cap_diff_from_avg = np.abs(bins_remain_cap - avg_cap)
    fullness_weights = np.exp(-cap_diff_from_avg / (0.2 * max_cap + 0.000001))  # Gaussian-like weighting
    priorities += fullness_bonus * fullness * fullness_weights

    # Boost bins with a close fit
    close_fit_threshold = 0.2 * max_cap
    close_fit_mask = fit_mask & (waste <= close_fit_threshold)
    if np.any(close_fit_mask):
        ratios = item / bins_remain_cap[close_fit_mask]
        priorities[close_fit_mask] += close_fit_boost * np.log(ratios)

    # Adaptive Empty Bin Handling
    near_empty_mask = bins_remain_cap > (0.9 * max_cap)
    priorities[near_empty_mask] -= empty_bin_penalty

    # Item Size Relative to Bin Size Penalty
    slightly_larger_mask = fit_mask & (waste < (0.5 * item))
    priorities[slightly_larger_mask] -= item_size_penalty_factor * (item / (max_cap + 0.000001))

    # Capacity standard deviation penalty.
    priorities -= std_dev_penalty * std_cap / (max_cap + 0.000001)

    return priorities
```
