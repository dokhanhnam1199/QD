```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Softmax-Based Fit strategy.

    The strategy assigns a higher priority to bins that have a remaining capacity
    just enough to fit the item, with a decay for bins that are too large or too small.
    A temperature parameter controls the softness of the softmax.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Parameter to control the "softness" of the softmax distribution.
    # A higher temperature makes the distribution more uniform, a lower temperature
    # makes it more peaked towards the best bin.
    temperature = 0.5

    # Calculate a "fitness" score for each bin.
    # We prefer bins where the remaining capacity is close to the item size.
    # A large negative value for capacity less than item size, and a value that
    # decreases as capacity increases beyond the item size.
    fitness_scores = np.zeros_like(bins_remain_cap, dtype=float)

    # Calculate differences: how much space is left after placing the item
    differences = bins_remain_cap - item

    # Assign scores based on the differences
    # If the bin can fit the item (difference >= 0)
    # We want to penalize bins that have a lot of leftover space.
    # A simple way is to use the negative of the difference for positive differences,
    # or exp(-difference/T) where difference is how much *more* capacity there is.
    # A smaller difference means higher priority.
    # For bins that *cannot* fit the item (difference < 0), assign a very low score (large negative).
    # The exponential function will map these to values close to zero.

    # Let's create scores that are higher for bins that fit snugly.
    # For bins that can fit the item (differences >= 0):
    # We want to prioritize bins with smaller 'differences'.
    # A good approach is to use exp(-(difference)/T). This ensures that
    # smaller differences (tighter fit) result in higher values.
    # For bins that cannot fit the item (differences < 0):
    # Assign a very low base score, e.g., -infinity effectively. When exponentiated,
    # this will be close to zero. A large negative number like -1e9 will work.

    # Filter for bins that can fit the item
    can_fit_mask = differences >= 0
    cannot_fit_mask = ~can_fit_mask

    # Scores for bins that can fit the item
    # We want to reward smaller remaining capacity (tight fit)
    # Let's map 'differences' to scores. Smaller difference -> higher score.
    # exp(-(difference / T)) -> larger if difference is small.
    if np.any(can_fit_mask):
        fitness_scores[can_fit_mask] = np.exp(-differences[can_fit_mask] / temperature)

    # Scores for bins that cannot fit the item
    # These should have very low priority. Assigning a very small number before softmax.
    # np.exp(-large_number) will be close to 0.
    if np.any(cannot_fit_mask):
        # Use a large negative number to ensure their softmax output is close to zero.
        # The magnitude of this negative number should be large relative to the
        # values in fitness_scores[can_fit_mask] / temperature.
        # Alternatively, we can set their scores to -np.inf directly for clarity,
        # but this might cause issues with the softmax calculation if not handled.
        # A safe approach is a very small positive number after exp.
        # Let's ensure they are significantly smaller than the fitting bins.
        # A simple approach: set them to 0 if they cannot fit. This implies they get
        # a probability of 0 after softmax if other bins can fit.
        fitness_scores[cannot_fit_mask] = 0.0 # Or a very small negative value if you want to differentiate

    # Apply Softmax to get probabilities (priorities)
    # Avoid division by zero in softmax if all scores are the same.
    # A common numerical stability trick is to subtract the maximum value.
    max_score = np.max(fitness_scores)
    if max_score == -np.inf or np.isinf(max_score): # Handles cases where all bins are invalid/too small
        priorities = np.ones_like(fitness_scores) / len(fitness_scores) # Uniform probability if no good option
    else:
        exp_scores = np.exp((fitness_scores - max_score) / temperature)
        priorities = exp_scores / np.sum(exp_scores)

    return priorities

```
