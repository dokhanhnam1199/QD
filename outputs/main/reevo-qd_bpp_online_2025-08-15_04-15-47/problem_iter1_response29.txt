```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Softmax-Based Fit.

    The priority is higher for bins that can fit the item and have a remaining
    capacity closer to the item's size (minimizing wasted space). A small
    constant is added to avoid division by zero or very small values, and
    a temperature parameter (implicitly set by the scaling factor) controls
    the "sharpness" of the softmax.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Calculate the "goodness" of fit for each bin.
    # A good fit means the remaining capacity is just enough to fit the item.
    # We want to maximize (bins_remain_cap - item).
    # However, bins that *cannot* fit the item should have a very low priority.
    # We can achieve this by setting a very small negative value or zero
    # for bins where bins_remain_cap < item.

    # Identify bins that can fit the item
    can_fit_mask = bins_remain_cap >= item

    # Calculate potential priority scores. We want to favor bins where the
    # remaining capacity is *close* to the item size (but still fits).
    # A simple approach is to use the negative difference: -(bins_remain_cap - item).
    # This makes smaller differences (better fits) have higher (less negative) scores.
    # To use softmax, we want positive values. Let's use a value proportional to how well it fits.
    # A good heuristic is to consider how much capacity is left *after* fitting the item.
    # Smaller remaining capacity after fitting is generally better.
    # So, (bins_remain_cap - item) should be small and non-negative.

    # Let's transform the "fit" such that better fits get higher scores.
    # A simple score could be (bins_remain_cap - item). Higher values here mean
    # more space left, which isn't ideal.
    # Alternatively, we can aim to *minimize* wasted space, which is (bins_remain_cap - item).
    # So, we want to maximize -(bins_remain_cap - item).

    # For bins that can fit, calculate a "fit score". A score that is higher
    # for bins where the remaining capacity is closer to the item size.
    # A simple transformation could be 1 / (bins_remain_cap - item + epsilon)
    # Or, to use softmax, let's define an "attractiveness" for each bin.
    # A bin is attractive if it can fit the item and leaves little space.
    # So, if remaining_cap = item, the attraction is maximal.
    # If remaining_cap = item + k (k > 0), attraction decreases.

    # Let's try a score that is high when bins_remain_cap is *close* to item.
    # Consider the difference `diff = bins_remain_cap - item`. We want `diff` to be small and non-negative.
    # We can use a function that is high for `diff` near 0.
    # A candidate function could be `exp(-alpha * diff)`.
    # alpha controls the sensitivity. Larger alpha means more sensitivity to small differences.

    alpha = 10.0  # Sensitivity parameter (tuneable hyperparameter)
    epsilon = 1e-6 # Small value to avoid division by zero or log(0)

    # Calculate scores: for bins that can fit, we want higher scores if remaining capacity is close to item size.
    # A good heuristic is `exp(-alpha * (bins_remain_cap - item))`.
    # For bins that cannot fit, their score should be very low.

    # Initialize scores to a very small negative number (or zero) to ensure they are not picked by softmax
    scores = np.full_like(bins_remain_cap, -np.inf)

    # Calculate scores only for bins that can fit the item
    fit_diff = bins_remain_cap[can_fit_mask] - item
    # We want to maximize the "goodness" of the fit. Goodness is high when diff is small and non-negative.
    # A score like `1 / (fit_diff + epsilon)` works, but using `exp` is more common in softmax.
    # Let's use `exp(-alpha * fit_diff)` to penalize larger gaps.
    scores[can_fit_mask] = np.exp(-alpha * fit_diff)

    # Apply softmax. Softmax will normalize these scores into probabilities.
    # The bin with the highest score will have the highest probability.
    # If all scores are -inf (no bin can fit), then softmax will produce NaNs or zeros depending on implementation.
    # We assume there will be at least one bin that can fit.
    # A standard way to compute softmax is: exp(score) / sum(exp(scores))
    # To avoid numerical stability issues with very large/small exp values, we can subtract the max score.
    exp_scores = np.exp(scores - np.max(scores))
    priorities = exp_scores / np.sum(exp_scores)

    # Handle the case where no bin can fit the item. In such a case, all scores might be 0 or NaN.
    # If np.sum(exp_scores) is 0, it means all original scores were -inf.
    if np.sum(exp_scores) == 0:
        return np.zeros_like(bins_remain_cap) # Or a specific indicator, e.g., all zeros

    return priorities
```
