[
  {
    "stdout_filepath": "problem_iter1_response3.txt_stdout.txt",
    "code_path": "problem_iter1_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    The Almost Full Fit strategy prioritizes bins that have just enough capacity\n    to fit the item, leaving the most empty space in the bins that are already\n    quite full. This aims to keep fuller bins fuller and sparser bins sparser,\n    potentially leading to better packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We only consider bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities to a very low value for bins that cannot fit the item\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # For bins that can fit the item, calculate a priority score.\n    # The score is the remaining capacity AFTER placing the item.\n    # We want to maximize this value for the \"Almost Full Fit\" heuristic.\n    # A larger remaining capacity means the bin was less full to begin with,\n    # but for the 'almost full' idea, we actually want to pick bins where\n    # the remaining capacity after fitting is minimized, i.e., the bin is closer\n    # to being full.\n    # So, let's rephrase: we want to pick bins where the *difference*\n    # between the remaining capacity and the item size is minimized.\n    # Or, equivalently, where `bins_remain_cap - item` is minimized.\n    # Since we want the *highest* priority for the chosen bin, we can invert this\n    # and assign a priority of `-(bins_remain_cap - item)`.\n    # A simpler way to think about \"Almost Full Fit\" is that we want to place the\n    # item into a bin such that the *resulting* remaining capacity is as *small* as\n    # possible (while still being non-negative). This means the bin was already\n    # quite full.\n\n    # Calculate the remaining capacity if the item were placed in each bin\n    potential_remaining_caps = bins_remain_cap - item\n\n    # The priority will be the negative of the potential remaining capacity.\n    # This way, bins that will have the smallest remaining capacity (i.e., are closest to full)\n    # will have the highest priority score.\n    priorities[can_fit_mask] = -potential_remaining_caps[can_fit_mask]\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 28.07354922057604,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit.\n\n    The First Fit strategy prioritizes bins that can accommodate the item\n    and among those, it gives higher priority to bins that have less remaining\n    capacity after the item is placed, to encourage tighter packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Find bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity after placing the item in suitable bins\n    remaining_caps_after_placement = bins_remain_cap[suitable_bins_mask] - item\n\n    # Assign priorities: Higher priority to bins with less remaining capacity after placement\n    # This encourages filling bins more completely.\n    # We use a large number for bins that cannot fit the item to ensure they are not chosen.\n    # For suitable bins, the priority is the inverse of (remaining capacity + 1)\n    # to encourage smaller remaining capacities. Adding 1 to avoid division by zero.\n    # A simple large negative number for unsuitable bins would also work.\n    \n    if np.any(suitable_bins_mask):\n        # Prioritize bins that can fit the item. Lower remaining capacity after packing = higher priority.\n        # We use negative of remaining capacity to make smaller remaining capacities have higher priority.\n        priorities[suitable_bins_mask] = -remaining_caps_after_placement\n    else:\n        # If no bin can fit the item, return an array of zeros or a specific indicator\n        # (though in a real online scenario, a new bin would likely be created).\n        # For this function's purpose, returning zeros implies no preferred bin among current ones.\n        pass\n\n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 22.458839376460833,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response6.txt_stdout.txt",
    "code_path": "problem_iter1_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score strategy.\n\n    The Sigmoid Fit Score strategy prioritizes bins based on how well they fit the item,\n    using a sigmoid function to emphasize bins that are neither too full nor too empty.\n    A higher score indicates a better fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We only consider bins that can accommodate the item\n    available_bins_mask = bins_remain_cap >= item\n    \n    # Calculate a 'fit score' for each available bin.\n    # A good fit is where the remaining capacity is just enough for the item.\n    # We want to penalize bins that are too full (low remaining capacity)\n    # and bins that are too empty (high remaining capacity, potentially wasted space).\n    # A common approach is to consider the difference between remaining capacity and item size.\n    # To map this to a Sigmoid function that favors middle ground (good fit),\n    # we can use a transformation.\n    \n    # Let's define a 'tightness' factor: how much is left after packing the item.\n    # The ideal scenario is tightness close to 0.\n    tightness = bins_remain_cap[available_bins_mask] - item\n\n    # We want to map tightness values to a sigmoid curve.\n    # The sigmoid function typically goes from 0 to 1.\n    # A sigmoid function with a steep slope around 0 would give higher scores\n    # to bins with tightness close to 0.\n    # Let's center the sigmoid around a tightness of 0.\n    # The formula for sigmoid is 1 / (1 + exp(-x)).\n    # To control the steepness, we can multiply x by a steepness factor (k).\n    # A larger k means a steeper slope.\n\n    steepness_factor = 5.0  # Adjust this for desired sensitivity to fit\n    \n    # Apply sigmoid to the negative of tightness to prioritize smaller (closer to 0) tightness values\n    # The `np.exp` function can handle large negative inputs gracefully, resulting in values close to 0.\n    # For positive inputs to exp (i.e., negative tightness), we might get large values.\n    # Let's ensure tightness is not excessively negative (though our mask already handles this)\n    # and consider how to scale `tightness` for the sigmoid.\n\n    # A simple approach is to map `tightness` to a range that the sigmoid will effectively cover.\n    # If `tightness` can be large positive (wasteful bins), we might want to compress it.\n    # Let's normalize `tightness` relative to the bin's original capacity if that was available,\n    # but here we only have remaining capacity.\n\n    # For simplicity and direct application of sigmoid to closeness to zero tightness:\n    # We can scale `tightness` by a factor and apply sigmoid.\n    # `sigmoid(-k * tightness)` means as `tightness` approaches 0, `-k * tightness` approaches 0,\n    # and sigmoid(0) = 0.5. As `tightness` increases (more space left), `-k * tightness` becomes more negative,\n    # sigmoid approaches 0. As `tightness` decreases (less space left, approaching -item, but masked),\n    # `-k * tightness` becomes more positive, sigmoid approaches 1.\n    # This is the opposite of what we want - we want bins that are *just enough*.\n\n    # Let's redefine our priority based on \"how much capacity is used\".\n    # Used capacity = item.\n    # Remaining capacity = bins_remain_cap.\n    # A perfect fit leaves `bins_remain_cap - item = 0`.\n    # We want to maximize the priority when `bins_remain_cap - item` is close to 0.\n\n    # Consider `(item / bins_remain_cap)` as a fill ratio.\n    # We want bins where `item / bins_remain_cap` is close to 1 (if the bin capacity were fixed at item size)\n    # or where the remaining capacity is minimized *after* placing the item.\n\n    # Let's stick to the 'tightness' (remaining capacity after packing) and sigmoid.\n    # We want `tightness` to be small.\n    # Consider the function `f(x) = 1 / (1 + exp(-k * (target - x)))`\n    # Here, `x` is `tightness`. Our `target` for tightness is 0.\n    # So, `f(tightness) = 1 / (1 + exp(-k * (0 - tightness))) = 1 / (1 + exp(k * tightness))`.\n    # This sigmoid gives high values when `tightness` is negative (which isn't possible here due to the mask)\n    # and low values when `tightness` is positive. This is still not ideal.\n\n    # Let's try to make the sigmoid sensitive to the *gap* between item and remaining capacity.\n    # We want to prioritize bins where `bins_remain_cap - item` is minimized (but non-negative).\n    # Let `gap = bins_remain_cap - item`.\n    # We want to maximize `priority` when `gap` is small.\n    # A sigmoid function that increases as its input decreases would work.\n    # `sigmoid(a - b*x)` where `x` is `gap`.\n    # So, `priority = sigmoid(constant - steepness * gap)`.\n    # `priority = 1 / (1 + exp(-(constant - steepness * gap)))`\n    # If `gap` is small, `steepness * gap` is small. `-constant` will be large negative,\n    # so exp() will be near 0, and priority will be close to 1.\n    # If `gap` is large, `steepness * gap` is large. `-constant - large` will be large negative,\n    # so exp() will be near 0, and priority will be close to 1. This is also not right.\n\n    # Let's use a sigmoid to map the \"slack\" (remaining capacity) to a preference.\n    # A perfect fit means `bins_remain_cap` is exactly `item`.\n    # Slack = `bins_remain_cap - item`. We want slack to be 0.\n    # Sigmoid function `S(x) = 1 / (1 + exp(-x))` is S-shaped, increasing from 0 to 1.\n    # To have a peak or high value when slack is 0, we can use a bell-shaped curve\n    # transformed from a sigmoid, or simply use a sigmoid creatively.\n\n    # Let's define a score where:\n    # - Bins that are too small (`bins_remain_cap < item`) get a score of 0.\n    # - Bins that have `bins_remain_cap == item` (perfect fit) get a high score.\n    # - Bins that have `bins_remain_cap > item` get a score that decreases as `bins_remain_cap` increases.\n\n    # Consider a transform of the remaining capacity:\n    # `transform = item / bins_remain_cap[available_bins_mask]`\n    # This ratio is 1 for a perfect fit (if `bins_remain_cap == item`).\n    # If `bins_remain_cap > item`, the ratio is less than 1.\n    # If `bins_remain_cap` is very large, the ratio is close to 0.\n    # We want to prioritize ratios close to 1.\n\n    # Let `fill_ratio = item / bins_remain_cap[available_bins_mask]`\n    # We can use sigmoid to map `fill_ratio` to a preference.\n    # We want preference to be high when `fill_ratio` is close to 1.\n    # Sigmoid: `1 / (1 + exp(-(x)))`. If x is `fill_ratio - target`, and target is 1.\n    # `priority_component = 1 / (1 + exp(-(fill_ratio - 1)))`\n    # This means priority is high when `fill_ratio` is close to 1.\n    # As `fill_ratio` approaches 1, `fill_ratio - 1` approaches 0, exp(0) = 1, priority = 0.5.\n    # If `fill_ratio` is slightly less than 1, `fill_ratio - 1` is negative, exp is small, priority approaches 1.\n    # If `fill_ratio` is slightly more than 1 (not possible due to mask), exp is large, priority approaches 0.\n    # This is good! It prioritizes bins where the item fits with minimal leftover space.\n\n    # Let's refine this. We want to penalize both too little and too much remaining space.\n    # This implies a peak score around the \"best fit\".\n\n    # Let's use the `tightness` (`bins_remain_cap - item`) and apply a sigmoid in a way that\n    # a tightness of 0 gives the highest score.\n    # A function like `sigmoid(steepness * (item - bins_remain_cap))` can work.\n    # `term = steepness * (item - bins_remain_cap[available_bins_mask])`\n    # If `bins_remain_cap` is very close to `item`, `item - bins_remain_cap` is close to 0, `term` is close to 0, `exp(0)=1`, sigmoid is 0.5.\n    # If `bins_remain_cap` is slightly larger than `item`, `item - bins_remain_cap` is negative, `term` is negative, exp is small, sigmoid is close to 1. (Not good, we want tightness ~ 0).\n    # If `bins_remain_cap` is slightly smaller than `item`, `item - bins_remain_cap` is positive, `term` is positive, exp is large, sigmoid is close to 0. (Good).\n\n    # The problem is that sigmoid naturally increases. We need a function that peaks.\n    # A common way to use sigmoid for a preference for a specific value `T` is to use\n    # `sigmoid(k * (T - x)) * sigmoid(k * (x - T))`. This creates a bell shape.\n    # Let `x = bins_remain_cap[available_bins_mask]`. Target `T` is `item`.\n    # We want `bins_remain_cap` to be close to `item`.\n    # The `gap = bins_remain_cap - item` should be close to 0.\n    # Let's use `sigmoid_score(gap, steepness)`\n    # We want `sigmoid_score(0)` to be max.\n    # A sigmoid function: `1 / (1 + exp(-x))` maps `x` to `[0, 1]`.\n    # If we want a peak at `x=0`, we can use `exp(-x^2)`. This isn't sigmoid based.\n\n    # Let's reconsider the \"Sigmoid Fit Score\" strategy description.\n    # It implies using sigmoid. The most direct interpretation for \"fit\" often relates to\n    # how much space is left after packing.\n    # Let `slack = bins_remain_cap[available_bins_mask] - item`. We want slack to be minimal.\n    # We want to map slack to a preference score.\n\n    # Let's use a slightly different sigmoid formulation to achieve the desired peak.\n    # Consider a score related to `1 / (1 + exp(-k * (some_measure)))`.\n    # If we want high scores when `bins_remain_cap` is close to `item`:\n    # Let `diff = bins_remain_cap[available_bins_mask] - item`.\n    # We want high scores when `diff` is close to 0.\n\n    # Sigmoid score can be designed as `score = sigmoid(k * (target_value - actual_value))`.\n    # If we want to prioritize bins that are \"just enough\", the target `bins_remain_cap` for the item `item` is `item` itself.\n    # So, we want `bins_remain_cap` to be close to `item`.\n    # Let `target_capacity = item`.\n    # The deviation from the target is `abs(bins_remain_cap[available_bins_mask] - item)`.\n    # We want to penalize this deviation.\n    # Sigmoid usually increases. To penalize a positive value, we use `sigmoid(constant - slope * value)`.\n    # Let `deviation = bins_remain_cap[available_bins_mask] - item`.\n    # We want high score when `deviation` is close to 0.\n    # `priority_component = sigmoid(steepness * (item - bins_remain_cap[available_bins_mask]))`\n    # `priority_component = 1 / (1 + exp(-steepness * (item - bins_remain_cap[available_bins_mask])))`\n    # Let `x = bins_remain_cap[available_bins_mask]`.\n    # `score(x) = 1 / (1 + exp(-k * (item - x)))`\n    # If `x = item`, `score = 1 / (1 + exp(0)) = 1 / (1 + 1) = 0.5`.\n    # If `x > item` (more space), `item - x` is negative, `-k * (item - x)` is positive, `exp` is large, score approaches 0. (Penalizes over-capacity)\n    # If `x < item` (not possible due to mask, but if it were), `item - x` is positive, `-k * (item - x)` is negative, `exp` is small, score approaches 1. (This is why the mask is crucial)\n\n    # This setup biases towards *under* filling the bin as much as possible.\n    # We want to bias towards *optimal* filling, which means minimal waste.\n    # Minimal waste occurs when `bins_remain_cap` is just above `item`.\n    # So, we want a peak when `bins_remain_cap - item` is small and positive.\n\n    # Let's consider the inverse of the sigmoid's steepness parameter `k`.\n    # The midpoint of the sigmoid `1 / (1 + exp(-k * x))` is at `x=0`.\n    # To create a peak at a specific deviation, we might need a different function.\n\n    # However, a common interpretation of \"Sigmoid Fit Score\" in this context is\n    # to model the preference based on the degree of \"fullness\" or \"slakness\".\n    # A bin that is \"almost full\" (i.e., `bins_remain_cap` is slightly larger than `item`) is preferred.\n    # Let's model the preference using a sigmoid that goes from low to high as the remaining capacity\n    # decreases towards the item size.\n\n    # Let `fill_level = item / bins_remain_cap[available_bins_mask]`.\n    # This ratio indicates how much of the *current remaining capacity* is used by the item.\n    # A value close to 1 means the item takes up most of the remaining space.\n    # We want to prioritize bins where this ratio is high (close to 1), indicating a good fit.\n    # `priority_score = sigmoid(k * (fill_level - target_fill_level))`\n    # Target fill level is 1.\n    # `priority_score = 1 / (1 + exp(-k * (fill_level - 1)))`\n    # Where `k` is a steepness factor.\n\n    # Let's implement this.\n    \n    # Initialize priorities to a very low value for bins that cannot fit the item.\n    priorities = np.full_like(bins_remain_cap, -float('inf')) # Use negative infinity for non-fitting bins\n    \n    # Calculate priorities only for bins that can accommodate the item.\n    if np.any(available_bins_mask):\n        # Calculate the fill level for available bins.\n        # This is the ratio of the item size to the bin's remaining capacity.\n        # We want this ratio to be as close to 1 as possible, meaning the item\n        # fills up the remaining space nicely.\n        fill_level = item / bins_remain_cap[available_bins_mask]\n        \n        # Apply the sigmoid function. The goal is to give higher scores when\n        # fill_level is close to 1.\n        # The sigmoid function is `1 / (1 + exp(-x))`.\n        # To have the highest value when `fill_level` is 1, we can use `x = k * (fill_level - 1)`.\n        # If `fill_level` is 1, `x = 0`, `exp(0) = 1`, `score = 1 / (1+1) = 0.5`.\n        # If `fill_level` is slightly less than 1 (meaning more leftover space than ideal),\n        # `fill_level - 1` is negative, `x` is negative, `exp(x)` is small, score approaches 1.\n        # If `fill_level` is slightly more than 1 (meaning item is larger than capacity, which is masked),\n        # `fill_level - 1` is positive, `x` is positive, `exp(x)` is large, score approaches 0.\n\n        # This strategy prioritizes bins where `fill_level < 1` and `fill_level` is close to 1.\n        # This means it favors bins with `bins_remain_cap` slightly larger than `item`.\n\n        steepness = 10.0  # Controls how sharply the priority changes around fill_level = 1\n        \n        # Calculate the argument for the sigmoid function.\n        # We want to center the \"peak\" preference around a fill_level of 1.\n        # By using `fill_level - 1`, we ensure that when `fill_level` is 1, the sigmoid argument is 0.\n        # The multiplication by `steepness` controls the slope.\n        sigmoid_arg = steepness * (fill_level - 1.0)\n        \n        # Apply the sigmoid function.\n        # A higher sigmoid value means a better fit.\n        fit_scores = 1 / (1 + np.exp(-sigmoid_arg))\n        \n        # Assign the calculated scores to the corresponding available bins.\n        priorities[available_bins_mask] = fit_scores\n\n    # Ensure no negative infinities are returned if no bins were available or fitted\n    if np.all(priorities == -float('inf')):\n        return np.zeros_like(bins_remain_cap)\n\n    return priorities",
    "response_id": 6,
    "obj": 4.487435181491823,
    "cyclomatic_complexity": 3.0,
    "halstead": 133.78294855911892,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response8.txt_stdout.txt",
    "code_path": "problem_iter1_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    The strategy aims to balance exploration (trying less-filled bins) and exploitation\n    (placing items in bins where they fit best).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.2  # Probability of exploration\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        # No bin can fit the item, return all zeros (or handle as an error/special case)\n        return priorities\n\n    # Exploitation: Prioritize bins that leave minimal remaining space after packing\n    # This is a greedy choice aiming to fill bins efficiently.\n    # We want to minimize the remaining capacity after placing the item,\n    # so a smaller (bins_remain_cap - item) is better.\n    # We can represent this as a negative value to use max on priorities.\n    # However, for simplicity and to avoid potential issues with zero remaining capacity,\n    # let's consider the *percentage* of remaining capacity or simply the\n    # *remaining capacity itself* after placing the item.\n    # The goal is to get 'closer' to zero remaining capacity without overshooting.\n    # A simple heuristic is to maximize the (bins_remain_cap - item).\n    # However, a more effective heuristic for minimizing bins is often to pick the\n    # bin that results in the *least* remaining capacity, which is equivalent to\n    # maximizing (bins_remain_cap - item). This is the \"Best Fit\" strategy.\n\n    # Let's adapt this to be more \"epsilon-greedy\"\n    # We'll have a \"greedy\" part and an \"exploratory\" part.\n\n    # Greedy part: Calculate the benefit of placing the item in each suitable bin.\n    # A good greedy metric could be the remaining capacity after placing the item.\n    # We want to minimize this, so we can score it as the negative of this value,\n    # or more simply, consider the \"tightness\" of the fit.\n    # The tighter the fit (less remaining capacity), the higher the priority.\n    # Let's use (bins_remain_cap - item) and normalize/invert it.\n    # A common approach is to use a value inversely proportional to remaining capacity\n    # or directly proportional to the fill percentage.\n    # For Best Fit, we want to maximize (bins_remain_cap - item).\n    # Let's use a metric that is higher for bins that are nearly full but still fit.\n    # A simple greedy score could be related to (bin_capacity - item_size) / bin_capacity,\n    # or just prioritize bins that are \"closest\" to fitting the item without being too full.\n    # The \"best fit\" strategy aims to leave the *least* remaining space.\n    # So, a higher priority should go to bins where `bins_remain_cap - item` is smallest.\n    # We can achieve this by maximizing `-(bins_remain_cap - item)` which is `item - bins_remain_cap`.\n    # Or, for simplicity, we can just directly maximize `bins_remain_cap - item` and\n    # handle the selection from there.\n\n    # Let's define the greedy scores for suitable bins.\n    # A good \"best fit\" score would be how much space is left after placing the item.\n    # We want to MINIMIZE this remaining space.\n    # So, a HIGHER priority should correspond to SMALLER `bins_remain_cap - item`.\n    # Let's create scores that are higher for better fits.\n    # We can do `-(bins_remain_cap[suitable_bins_mask] - item)`\n    # Or, perhaps more intuitively, a higher priority for bins with capacity *just enough* for the item.\n    # Let's consider `bins_remain_cap[suitable_bins_mask] - item`. We want to MINIMIZE this.\n    # So, we can invert it or use it as a negative score.\n    # A robust way is to use `1 / (1 + (bins_remain_cap[suitable_bins_mask] - item))` for minimization.\n    # Or simply, we want to pick the smallest positive value of `bins_remain_cap - item`.\n    # Let's consider the \"fitness\" as `bins_remain_cap[suitable_bins_mask] - item`.\n    # We want to pick the smallest positive value.\n    # So, let's define scores as: `scores = -(bins_remain_cap[suitable_bins_mask] - item)` which means higher score for smaller remaining space.\n\n    greedy_scores = -(bins_remain_cap[suitable_bins_mask] - item)\n    priorities[suitable_bins_mask] = greedy_scores\n\n    # Exploration part: With probability epsilon, choose a random suitable bin.\n    if np.random.rand() < epsilon:\n        # Pick a random suitable bin and assign it a very high priority,\n        # or simply override the greedy scores.\n        # Let's add a random exploration bonus to a random suitable bin.\n        random_suitable_bin_index = np.random.choice(np.where(suitable_bins_mask)[0])\n        # Add a positive boost to this randomly selected bin.\n        # The magnitude of the boost can be arbitrary, as long as it's positive\n        # and potentially enough to make it stand out.\n        # Or, we can simply assign a high constant priority.\n        # Let's make the priority for the randomly chosen bin higher than any possible greedy score.\n        # Max possible greedy score is `item - min_positive_remaining_capacity`.\n        # A simple way: give a random boost.\n        priorities[suitable_bins_mask] += np.random.uniform(0, 1) # Add a small random value to all, then boost one further?\n                                                                 # Or, more directly, assign a random priority.\n\n        # Let's assign random priorities to all suitable bins and then pick one randomly.\n        # Or, simpler: choose one random suitable bin and give it a high priority,\n        # then the rest of the suitable bins have their greedy priority.\n        # This means the random choice will likely win.\n\n        # A more common epsilon-greedy implementation for selection:\n        # 1. Calculate greedy scores.\n        # 2. With probability epsilon, pick a random *suitable* bin.\n        # 3. With probability 1-epsilon, pick the *best* suitable bin based on greedy scores.\n\n        # Let's implement that logic directly for *selection*, not for generating priorities array.\n        # The function is supposed to RETURN priorities, from which a selection will be made.\n        # So, we need to modify the `priorities` array itself.\n\n        # Option 1: Add random noise to all suitable bins, ensuring some randomness in selection.\n        # priorities[suitable_bins_mask] += np.random.normal(0, 0.1, size=np.sum(suitable_bins_mask))\n\n        # Option 2: Assign random priorities to a fraction of suitable bins.\n        num_exploratory_bins = int(epsilon * np.sum(suitable_bins_mask))\n        if num_exploratory_bins > 0:\n            exploratory_indices = np.random.choice(np.where(suitable_bins_mask)[0], size=num_exploratory_bins, replace=False)\n            # Assign random priorities to these bins. These could be higher or lower than greedy.\n            # To ensure exploration, let's assign them *lower* priorities so that a random choice might pick them\n            # if they also have a decent greedy score, or higher priorities if we want to force exploration.\n            # For epsilon-greedy for *selection*, we mean pick a random bin *instead* of the best.\n            # So, to simulate that in the priority list: make the chosen random bins have artificially high priority.\n            priorities[exploratory_indices] = np.random.uniform(np.max(priorities[suitable_bins_mask]) + 1,\n                                                              np.max(priorities[suitable_bins_mask]) + 10,\n                                                              size=num_exploratory_bins)\n\n\n    # Normalize priorities so that higher value means higher priority\n    # This isn't strictly necessary if the selection logic just picks the max,\n    # but can be useful for other selection methods.\n    # Let's ensure all suitable bins have positive priorities, and unsuitable ones are zero.\n    priorities[~suitable_bins_mask] = -np.inf # Make sure unsuitable bins are never chosen\n\n    # The greedy scores were set up such that higher means better (less remaining space).\n    # Exploration part is tricky when modifying the whole priority array.\n    # If the goal is to modify the priority array to reflect an epsilon-greedy choice strategy:\n    # With prob epsilon, select a random suitable bin, give it highest priority.\n    # With prob 1-epsilon, select best greedy bin, give it highest priority.\n\n    # Let's re-approach: The output IS the priority array. The selection mechanism will pick from this.\n    # If we want to influence the selection via priorities:\n    # For suitable bins:\n    #   With prob epsilon: assign a \"random\" high priority (to encourage selection)\n    #   With prob 1-epsilon: assign the \"greedy\" priority\n\n    # This approach is more direct in modifying the priority list itself to reflect the strategy.\n    final_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    final_priorities[~suitable_bins_mask] = -np.inf # Unsuitable bins have no chance\n\n    if np.any(suitable_bins_mask):\n        suitable_indices = np.where(suitable_bins_mask)[0]\n        num_suitable = len(suitable_indices)\n\n        # Calculate greedy priorities for all suitable bins\n        greedy_values = -(bins_remain_cap[suitable_bins_mask] - item)\n        # Convert to probabilities or direct priorities. Let's assume higher is better.\n\n        # Determine how many bins will get an \"exploratory\" boost\n        num_exploratory_bins = int(np.ceil(epsilon * num_suitable)) # Using ceil to ensure at least one if epsilon is small but num_suitable > 0\n\n        # Identify which bins will be \"randomly\" chosen for exploration\n        exploratory_indices_in_suitable = np.random.choice(num_suitable, size=min(num_exploratory_bins, num_suitable), replace=False)\n        exploratory_absolute_indices = suitable_indices[exploratory_indices_in_suitable]\n\n        # Assign \"high random\" priorities to exploratory bins\n        max_greedy_value = np.max(greedy_values) if num_suitable > 0 else 0\n        exploration_priority_boost = np.random.uniform(max_greedy_value + 1, max_greedy_value + 10, size=len(exploratory_absolute_indices))\n        final_priorities[exploratory_absolute_indices] = exploration_priority_boost\n\n        # Assign greedy priorities to the rest of the suitable bins\n        non_exploratory_suitable_indices = np.setdiff1d(suitable_indices, exploratory_absolute_indices)\n\n        if len(non_exploratory_suitable_indices) > 0:\n            greedy_values_for_non_exploratory = -(bins_remain_cap[non_exploratory_suitable_indices] - item)\n            final_priorities[non_exploratory_suitable_indices] = greedy_values_for_non_exploratory\n\n    # Ensure that if epsilon is 0 or 1, or if num_suitable is 0, it behaves correctly.\n    # If epsilon is 0, all suitable bins get greedy priorities.\n    # If epsilon is 1, all suitable bins get random high priorities.\n    # If num_suitable is 0, all are -inf.\n\n    return final_priorities",
    "response_id": 8,
    "obj": 73.97287594734743,
    "cyclomatic_complexity": 7.0,
    "halstead": 310.3352333162707,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response9.txt_stdout.txt",
    "code_path": "problem_iter1_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit strategy.\n\n    The strategy assigns a higher priority to bins that have a remaining capacity\n    just enough to fit the item, with a decay for bins that are too large or too small.\n    A temperature parameter controls the softness of the softmax.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Parameter to control the \"softness\" of the softmax distribution.\n    # A higher temperature makes the distribution more uniform, a lower temperature\n    # makes it more peaked towards the best bin.\n    temperature = 0.5\n\n    # Calculate a \"fitness\" score for each bin.\n    # We prefer bins where the remaining capacity is close to the item size.\n    # A large negative value for capacity less than item size, and a value that\n    # decreases as capacity increases beyond the item size.\n    fitness_scores = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate differences: how much space is left after placing the item\n    differences = bins_remain_cap - item\n\n    # Assign scores based on the differences\n    # If the bin can fit the item (difference >= 0)\n    # We want to penalize bins that have a lot of leftover space.\n    # A simple way is to use the negative of the difference for positive differences,\n    # or exp(-difference/T) where difference is how much *more* capacity there is.\n    # A smaller difference means higher priority.\n    # For bins that *cannot* fit the item (difference < 0), assign a very low score (large negative).\n    # The exponential function will map these to values close to zero.\n\n    # Let's create scores that are higher for bins that fit snugly.\n    # For bins that can fit the item (differences >= 0):\n    # We want to prioritize bins with smaller 'differences'.\n    # A good approach is to use exp(-(difference)/T). This ensures that\n    # smaller differences (tighter fit) result in higher values.\n    # For bins that cannot fit the item (differences < 0):\n    # Assign a very low base score, e.g., -infinity effectively. When exponentiated,\n    # this will be close to zero. A large negative number like -1e9 will work.\n\n    # Filter for bins that can fit the item\n    can_fit_mask = differences >= 0\n    cannot_fit_mask = ~can_fit_mask\n\n    # Scores for bins that can fit the item\n    # We want to reward smaller remaining capacity (tight fit)\n    # Let's map 'differences' to scores. Smaller difference -> higher score.\n    # exp(-(difference / T)) -> larger if difference is small.\n    if np.any(can_fit_mask):\n        fitness_scores[can_fit_mask] = np.exp(-differences[can_fit_mask] / temperature)\n\n    # Scores for bins that cannot fit the item\n    # These should have very low priority. Assigning a very small number before softmax.\n    # np.exp(-large_number) will be close to 0.\n    if np.any(cannot_fit_mask):\n        # Use a large negative number to ensure their softmax output is close to zero.\n        # The magnitude of this negative number should be large relative to the\n        # values in fitness_scores[can_fit_mask] / temperature.\n        # Alternatively, we can set their scores to -np.inf directly for clarity,\n        # but this might cause issues with the softmax calculation if not handled.\n        # A safe approach is a very small positive number after exp.\n        # Let's ensure they are significantly smaller than the fitting bins.\n        # A simple approach: set them to 0 if they cannot fit. This implies they get\n        # a probability of 0 after softmax if other bins can fit.\n        fitness_scores[cannot_fit_mask] = 0.0 # Or a very small negative value if you want to differentiate\n\n    # Apply Softmax to get probabilities (priorities)\n    # Avoid division by zero in softmax if all scores are the same.\n    # A common numerical stability trick is to subtract the maximum value.\n    max_score = np.max(fitness_scores)\n    if max_score == -np.inf or np.isinf(max_score): # Handles cases where all bins are invalid/too small\n        priorities = np.ones_like(fitness_scores) / len(fitness_scores) # Uniform probability if no good option\n    else:\n        exp_scores = np.exp((fitness_scores - max_score) / temperature)\n        priorities = exp_scores / np.sum(exp_scores)\n\n    return priorities",
    "response_id": 9,
    "obj": 4.078579976067022,
    "cyclomatic_complexity": 5.0,
    "halstead": 155.11451069865606,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response14.txt_stdout.txt",
    "code_path": "problem_iter1_code14.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First strategy.\n\n    The Exact Fit First strategy prioritizes bins where the item fits exactly,\n    meaning the remaining capacity of the bin will be zero after placing the item.\n    Among bins that allow an exact fit, no further prioritization is applied in this version.\n    If no exact fit is possible, bins that can accommodate the item are given a lower priority\n    based on their remaining capacity (smaller remaining capacity is better, meaning less wasted space).\n    Bins that cannot accommodate the item are given a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity if the item is placed in a fitting bin\n    potential_remaining_caps = bins_remain_cap[can_fit_mask] - item\n\n    # Strategy: Prioritize exact fits (remaining capacity of 0)\n    exact_fit_mask = can_fit_mask.copy()\n    exact_fit_mask[can_fit_mask] = (potential_remaining_caps == 0)\n\n    # Assign highest priority to exact fits\n    priorities[exact_fit_mask] = 1.0\n\n    # For bins that can fit but not exactly, prioritize those with less wasted space\n    # Assign a priority inversely proportional to the remaining capacity after fit\n    # Use a small epsilon to avoid division by zero if potential_remaining_caps is 0 (already handled by exact fit)\n    non_exact_fit_mask = can_fit_mask.copy()\n    non_exact_fit_mask[can_fit_mask] = ~exact_fit_mask[can_fit_mask]\n\n    if np.any(non_exact_fit_mask):\n        # Calculate priorities for non-exact fits: higher score for smaller leftover capacity\n        # We want smaller leftover capacity to have a higher priority.\n        # So, we can use 1 / (1 + leftover_capacity) to get values between (0, 1]\n        # Adding 1 to leftover_capacity to ensure division by non-zero and scale appropriately.\n        leftover_capacities = bins_remain_cap[non_exact_fit_mask] - item\n        priorities[non_exact_fit_mask] = 1.0 / (1.0 + leftover_capacities)\n\n\n    # Normalize priorities to be between 0 and 1, and ensure exact fits are clearly higher\n    # Exact fits have priority 1.0. Non-exact fits will be between (0, 1) depending on leftover space.\n    # Bins that don't fit remain 0.\n\n    return priorities",
    "response_id": 14,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 80.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response16.txt_stdout.txt",
    "code_path": "problem_iter1_code16.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score prioritizes bins that have a remaining capacity\n    close to the item size, using a sigmoid function to create a bell-shaped\n    curve. Bins with capacity exactly matching the item size get the highest\n    priority. Bins with significantly larger or smaller capacities get lower\n    priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Avoid division by zero or very small numbers which can cause instability.\n    # A small epsilon is added to the denominator.\n    epsilon = 1e-9\n\n    # Calculate the difference between the bin's remaining capacity and the item size.\n    diff = bins_remain_cap - item\n\n    # We want bins where diff is close to 0 to have high priority.\n    # The sigmoid function naturally maps values to the range (0, 1).\n    # A common sigmoid function is 1 / (1 + exp(-x)).\n    # To center the peak at diff=0, we can use exp(-abs(diff)) or a scaled version.\n    # A Gaussian-like shape centered at 0 can be achieved using exp(-(diff/scale)^2).\n    # Let's scale the difference so that a \"good fit\" difference (e.g., diff close to 0)\n    # maps to a significant change in the sigmoid's output.\n\n    # A simple approach is to use exp(-abs(diff)). This will peak at 1 when diff is 0.\n    # However, the sigmoid is generally defined as 1 / (1 + exp(-x)).\n    # To make it peak at 0, we can use exp(-k * diff**2) or related transformations.\n\n    # Let's define a score based on how close the remaining capacity is to the item size.\n    # We want to maximize the priority when bins_remain_cap is close to item.\n    # Consider a sigmoid centered at the item size.\n    # score = sigmoid(k * (item - bins_remain_cap)) or sigmoid(k * (bins_remain_cap - item))\n    # If we use sigmoid(k * (item - bins_remain_cap)), the peak is when item - bins_remain_cap = 0\n    # i.e., bins_remain_cap = item.\n\n    # We need to control the steepness of the sigmoid. A larger k means a steeper curve.\n    # We can set k based on the expected range of capacities or item sizes.\n    # For simplicity, let's use a fixed k. A value like 0.5 or 1 might be reasonable.\n    # Let's also handle cases where item > bins_remain_cap, as these bins are invalid for the item.\n    # We can assign a very low priority (e.g., 0) to such bins.\n\n    # Let's map diff to a sigmoid such that diff = 0 gets a high score.\n    # A possible transformation: use exp(-diff^2 / (2 * sigma^2)). This is a Gaussian.\n    # To make it a sigmoid-like curve, let's reconsider the sigmoid form.\n    # The logistic function f(x) = 1 / (1 + exp(-x)).\n    # We want f(0) to be high. Let x = -k * diff.\n    # Then f(-k*diff) = 1 / (1 + exp(k*diff)).\n    # When diff = 0, f(0) = 1/2. This is not ideal.\n\n    # A better approach for \"fit score\" would be to reward bins that are \"almost full\"\n    # but can still fit the item.\n    # This means bins with remaining capacity `c` such that `c >= item`.\n    # Among these, we want `c` to be as close to `item` as possible.\n\n    # Let's define the score based on the 'tightness' of the fit.\n    # For bins where bins_remain_cap >= item:\n    # score = sigmoid(k * (item - bins_remain_cap))\n    # This will make the score decrease as bins_remain_cap increases beyond item.\n    # The maximum score is 0.5 when bins_remain_cap == item.\n    # If we want the peak to be 1, we can scale or shift.\n\n    # Let's try a different interpretation of \"Sigmoid Fit Score\" that focuses on\n    # the \"fitness\" as the value being transformed by a sigmoid.\n    # A bin is \"fit\" if its remaining capacity is greater than or equal to the item size.\n    # For bins that are fit, we want to score based on how close the remaining capacity is to the item size.\n\n    # Option 1: Gaussian-like approach for closeness.\n    # sigma controls the width of the \"good fit\" zone.\n    sigma = 0.5  # Hyperparameter controlling the \"tightness\" of the fit.\n    scores_gaussian = np.exp(-(diff**2) / (2 * sigma**2))\n\n    # Ensure bins that cannot fit the item have zero priority.\n    invalid_mask = bins_remain_cap < item\n    scores_gaussian[invalid_mask] = 0\n\n    # This gives a Gaussian shape, peaking at 1 when diff=0.\n    # If we want a sigmoid shape that smoothly transitions to 0, we could use\n    # 1 / (1 + exp(k * diff)) for diff > 0 and something else for diff < 0.\n\n    # Let's refine this to be more sigmoid-like, but still centered on 0 diff.\n    # Consider a sigmoid where the input is proportional to `item - bins_remain_cap`.\n    # Let k be the steepness.\n    k = 2.0 # Steepness parameter\n    # The standard logistic sigmoid: 1 / (1 + exp(-x))\n    # If we use x = k * (item - bins_remain_cap), the peak is at item == bins_remain_cap.\n    # When item > bins_remain_cap, x is positive, exp(x) is large, sigmoid -> 0.\n    # When item < bins_remain_cap, x is negative, exp(x) is small, sigmoid -> 1.\n    # This is the opposite of what we want if we interpret sigmoid(0) as the best.\n\n    # Let's use the logistic function to penalize larger differences.\n    # We want higher scores when bins_remain_cap is close to 'item'.\n    # For bins where bins_remain_cap >= item:\n    # Let's transform `bins_remain_cap - item`. We want smaller values of this to be good.\n    # The function `1 / (1 + exp(x))` decreases as x increases.\n    # If we set x = k * (bins_remain_cap - item), then when diff=0, x=0, score=0.5.\n    # When diff > 0, x > 0, score < 0.5. When diff < 0, x < 0, score > 0.5.\n    # This is also not quite right if we want the peak at diff=0.\n\n    # Let's consider what \"Sigmoid Fit Score\" might imply more directly:\n    # The \"fitness\" is the closeness of `bins_remain_cap` to `item`.\n    # A function that maps `|bins_remain_cap - item|` to a score.\n    # We want `score` to be high when `|bins_remain_cap - item|` is low.\n\n    # Let's use a sigmoid to transform a measure of \"badness\" into a priority.\n    # \"Badness\" could be `|bins_remain_cap - item|`.\n    # Higher badness -> lower priority.\n    # Let's use `1 - sigmoid(k * |diff|)` where sigmoid is logistic.\n    # sigmoid(0) = 0.5. So 1 - 0.5 = 0.5.\n    # As |diff| increases, sigmoid(k*|diff|) increases, so 1 - sigmoid(...) decreases.\n    # This gives a priority that peaks at 0.5 and decreases.\n\n    # Another interpretation: \"Sigmoid Fit Score\" means the *probability* of fitting,\n    # where this probability is modeled by a sigmoid function of some quality metric.\n    # Quality metric = how close `bins_remain_cap` is to `item`.\n\n    # Let's use the idea that bins with remaining capacity *just* large enough to fit\n    # the item are good candidates. We want to prioritize bins that are \"tight fits\".\n    # We can model this using a sigmoid that peaks when `bins_remain_cap` is\n    # slightly larger than `item`.\n\n    # Let's define a measure of \"how well the item fits\":\n    # If `bins_remain_cap < item`, fit is impossible (very low score).\n    # If `bins_remain_cap >= item`, the fit is possible. The \"tightness\" is\n    # `bins_remain_cap - item`. We want this difference to be small.\n    # Let `tightness = bins_remain_cap - item`.\n    # We want to maximize priority when `tightness` is small (close to 0).\n\n    # Let's use a sigmoid of the form `1 / (1 + exp(-x))` where `x` is designed\n    # to be high for small `tightness`.\n    # If we use `x = -k * tightness`, then when `tightness = 0`, `x=0`, sigmoid=0.5.\n    # When `tightness > 0` (but small), `x < 0`, sigmoid > 0.5.\n    # When `tightness` is large, `x` is large negative, sigmoid -> 1. This is NOT what we want.\n\n    # Let's flip the sigmoid: `1 / (1 + exp(x))`. This decreases as x increases.\n    # If `x = k * tightness`, when `tightness=0`, `x=0`, sigmoid=0.5.\n    # When `tightness > 0` (small), `x > 0`, sigmoid < 0.5.\n    # When `tightness` is large, `x` is large positive, sigmoid -> 0.\n    # This penalizes large `tightness`.\n    # The peak is at 0.5, not 1.\n\n    # To achieve a peak of 1, we can consider:\n    # `1 - 1 / (1 + exp(-x))` => `exp(-x) / (1 + exp(-x))`\n    # Let `x = k * (item - bins_remain_cap)`.\n    # If `item == bins_remain_cap`, `x=0`, score = 0.5 / (1+0.5) = 1/3. Still not 1.\n\n    # Let's use a sigmoid to map the *normalized* \"closeness\" to a priority.\n    # Normalize `bins_remain_cap - item` relative to `item` itself, or some max capacity.\n    # For a fixed bin capacity `C` and item size `s`:\n    # If `c < s`, invalid.\n    # If `c >= s`:\n    # Consider the \"waste\" `w = c - s`. We want to minimize `w`.\n    # `w` ranges from 0 up to `C - s`.\n\n    # Let's consider a simplified sigmoid score: the proportion of remaining capacity that is *unused* by the item.\n    # If `bins_remain_cap < item`, unused proportion is effectively infinite or invalid.\n    # If `bins_remain_cap >= item`:\n    #   Unused capacity = `bins_remain_cap - item`\n    #   Total capacity available for this item (that isn't strictly needed) = `bins_remain_cap`\n    #   Unused proportion = `(bins_remain_cap - item) / bins_remain_cap`\n    # This value is between 0 (perfect fit) and `(C-item)/C` (if `bins_remain_cap` is `C`).\n    # We want to penalize larger unused proportions.\n    # Let `u = (bins_remain_cap - item) / bins_remain_cap` for `bins_remain_cap > 0`.\n    # We want a sigmoid that maps `u` to a priority.\n    # If `u=0` (perfect fit), priority should be high (e.g., 1).\n    # If `u` is large, priority should be low.\n    # The function `1 / (1 + exp(k * u))` works well for this.\n    #   When `u=0`, score=0.5.\n    #   When `u > 0`, `exp(k*u)` increases, score decreases.\n    #   To get a peak of 1 at `u=0`, we could use `1 - (1 / (1 + exp(-k*u)))` (logistic),\n    #   or similar, but we need to manage the invalid cases.\n\n    # Let's define a \"fit quality\" function first, then apply a sigmoid.\n    # A simple fit quality: `item / bins_remain_cap` if `bins_remain_cap >= item`.\n    # This value is between `item/item = 1` (perfect fit) and `item/C` (if `bins_remain_cap` is `C`).\n    # We want to maximize priority when `fit_quality` is close to 1.\n    # Let `q = item / bins_remain_cap`.\n    # We want a sigmoid that maps `q` to priority.\n    # `1 / (1 + exp(-k * (q - 1)))`\n    # When `q = 1` (perfect fit), `x=0`, sigmoid = 0.5.\n    # When `q < 1` (bins_remain_cap > item), `q-1 < 0`, `x < 0`, sigmoid > 0.5.\n    # When `q > 1` (bins_remain_cap < item), this case is invalid.\n\n    # Let's simplify and use a common heuristic for BPP which is often called \"Best Fit\".\n    # Best Fit: choose the bin that leaves the least remaining capacity.\n    # This means prioritizing bins where `bins_remain_cap - item` is minimized.\n    # Our sigmoid approach should reflect this.\n\n    # Let's focus on the structure: sigmoid(f(item, bins_remain_cap))\n    # where f is a metric of \"goodness\" of fit.\n    # metric `m = bins_remain_cap - item`. We want to minimize `m`.\n    # A function `g(m)` that is high for small `m` and low for large `m`.\n    # Then apply sigmoid: `Sigmoid(k * g(m))`.\n\n    # Let `g(m) = -m` if `m >= 0`, and some very small value otherwise.\n    # If `m >= 0`: Sigmoid(k * (-m)) = 1 / (1 + exp(k*m)).\n    # This gives peak of 0.5 at m=0, and decreases.\n\n    # If we want the peak to be 1:\n    # Consider a shifted and scaled sigmoid.\n    # A standard sigmoid is S(x) = 1 / (1 + exp(-x)).\n    # We want S(0) = 1. This is not possible with this form.\n\n    # Let's use a Sigmoid-like function that peaks at 1.\n    # A bell curve scaled to be like a sigmoid that goes from 0 to 1.\n    # e.g. `0.5 + 0.5 * tanh(k * (item - bins_remain_cap))`\n    # `tanh(x)` goes from -1 to 1. `tanh(0)=0`.\n    # So, `0.5 + 0.5 * tanh(0)` = 0.5. Still not peaking at 1.\n\n    # Revisit the Gaussian-like approach:\n    # `np.exp(-(diff**2) / (2 * sigma**2))` is a good candidate for a *fitness* score,\n    # peaking at 1 when diff is 0. It's not strictly sigmoid, but it's often used\n    # as a smooth priority function.\n\n    # Let's try to interpret \"Sigmoid Fit Score\" as using the sigmoid function itself\n    # to define the priority for bins that can accommodate the item.\n    # For bins where `bins_remain_cap >= item`:\n    # The \"degree of fit\" can be measured by how much capacity is \"left over\".\n    # Left over = `bins_remain_cap - item`.\n    # We want to assign higher priority to smaller \"left over\" values.\n\n    # Consider the transformation: `1 / (1 + exp(k * (bins_remain_cap - item)))`\n    # If `bins_remain_cap == item`, score = 0.5.\n    # If `bins_remain_cap > item` (and `k > 0`), score < 0.5.\n    # If `bins_remain_cap < item` (invalid), `k*(negative value)` would lead to a large score.\n\n    # Let's set `k` and then handle invalid bins.\n    # A moderate `k` could be 2 or 3.\n    k_sigmoid = 3.0\n\n    # Calculate potential scores using sigmoid(k * (item - bins_remain_cap))\n    # This maps item == bin_remain_cap to 0.5.\n    # item < bin_remain_cap maps to > 0.5.\n    # item > bin_remain_cap maps to < 0.5.\n\n    # We want:\n    # - High priority when `bins_remain_cap` is *just enough* (i.e., `bins_remain_cap - item` is small and non-negative).\n    # - Low priority when `bins_remain_cap` is much larger than `item`.\n    # - Zero priority when `bins_remain_cap < item`.\n\n    # Let's define a function `f(x) = 1 / (1 + exp(-x))` as our sigmoid.\n    # We want to input a value `v` such that `v=0` gives `f(0)=0.5`, and we want to\n    # transform `v` to center the desired priority.\n    # If `bins_remain_cap == item`, we want high priority.\n    # Let `residual = bins_remain_cap - item`.\n    # We want high priority when `residual` is close to 0.\n\n    # A robust sigmoid approach for prioritizing closeness to a target `T`:\n    # Priority = sigmoid(slope * (target - current_value))\n    # Here, `target = item`, `current_value = bins_remain_cap`.\n    # Priority = sigmoid(k * (item - bins_remain_cap))\n\n    # If item == bins_remain_cap: `k * 0` -> `sigmoid(0) = 0.5`\n    # If item < bins_remain_cap: `k * negative` -> `sigmoid(negative) > 0.5`\n    # If item > bins_remain_cap: `k * positive` -> `sigmoid(positive) < 0.5`\n\n    # This is prioritizing bins where `bins_remain_cap` is *larger* than `item`, which is incorrect.\n\n    # Let's try the inverse:\n    # Priority = sigmoid(k * (bins_remain_cap - item))\n    # If bins_remain_cap == item: `k * 0` -> `sigmoid(0) = 0.5`\n    # If bins_remain_cap > item: `k * positive` -> `sigmoid(positive) > 0.5`\n    # If bins_remain_cap < item: `k * negative` -> `sigmoid(negative) < 0.5`\n\n    # This prioritizes bins with *more* remaining capacity, not less.\n\n    # The \"Sigmoid Fit Score\" strategy should really focus on the \"fit\" itself.\n    # Consider the \"slack\" or \"waste\": `waste = bins_remain_cap - item`.\n    # We want to maximize priority for `waste = 0`.\n    # Let's map `waste` to a sigmoid.\n    # A function that maps `waste=0` to 1, and increases as `waste` increases (decaying priority).\n    # Try `1 / (1 + exp(k * waste))`.\n    # If `waste=0`, score = 0.5.\n    # If `waste > 0`, score < 0.5.\n    # If `waste < 0` (invalid), `k*waste` is negative, score > 0.5.\n\n    # We need to ensure invalid bins get 0 priority.\n    # Let's define `scaled_waste = (bins_remain_cap - item) / max_possible_waste`.\n    # `max_possible_waste` could be `bin_capacity - min_item_size` or similar.\n\n    # A simpler approach that is often described as sigmoid-like for BPP is to use\n    # the value `item / bins_remain_cap` if `bins_remain_cap >= item`, and 0 otherwise,\n    # and then apply a sigmoid transformation.\n\n    # Let's use the sigmoid `1 / (1 + exp(-x))` as the core.\n    # We need to find an input `x` that, when fed into this sigmoid, gives us the desired priority.\n    # We want priority = 1 when `bins_remain_cap` is just slightly larger than `item`.\n    # And priority decreases as `bins_remain_cap` grows larger.\n\n    # Let `penalty = bins_remain_cap - item`.\n    # If `penalty < 0`, priority is 0.\n    # If `penalty >= 0`:\n    # We want a function that is high for small `penalty` and low for large `penalty`.\n    # Consider `sigmoid_score = 1 / (1 + exp(k * penalty))`\n    # If `penalty = 0`, score = 0.5.\n    # If `penalty = 1`, score = `1 / (1 + exp(k))`.\n    # If `penalty = large`, score approaches 0.\n\n    # To get a peak of 1 at `penalty = 0`, we can try:\n    # `1 - 0.5 * sigmoid_score`  => `1 - 0.5 * (1 / (1 + exp(k * penalty)))`\n    # If `penalty = 0`, `1 - 0.5 * 0.5` = `1 - 0.25` = 0.75. Still not 1.\n\n    # Let's use the \"Best Fit\" heuristic concept and warp it with a sigmoid.\n    # Best Fit Score: `bins_remain_cap - item`. Minimize this.\n    # We want to convert `bins_remain_cap - item` (which is >= 0 for valid bins)\n    # into a priority, where smaller values yield higher priorities.\n\n    # Define `x = bins_remain_cap - item`.\n    # If `x < 0`, priority = 0.\n    # If `x >= 0`, priority = `1 / (1 + exp(k * x))` is a decreasing function.\n    # To make it peak at `x=0` and go to 0 for large `x`, we want `f(0) = 1`.\n\n    # Let's use a transformation of `x = bins_remain_cap - item`.\n    # A function that is 0 when `x` is large, and 1 when `x` is close to 0.\n    # Sigmoid properties: mapping a continuous input to (0, 1) or (-1, 1).\n\n    # Let's consider the probability that a bin is \"ideal\" for the item.\n    # Ideal = remaining capacity is very close to item size.\n    # Probability = `sigmoid(k * (target - current))`\n    # We want target = item, current = bins_remain_cap.\n    # But we need to handle the constraint `bins_remain_cap >= item`.\n\n    # Let's re-implement the logic for a tight fit:\n    # Prioritize bins where `bins_remain_cap` is *just* large enough.\n    # This means `bins_remain_cap - item` should be small and non-negative.\n\n    # Metric `m = bins_remain_cap - item`.\n    # If `m < 0`, priority = 0.\n    # If `m >= 0`:\n    #   We want to map `m` to `[0, 1]` such that `m=0` maps to `1`.\n    #   And `m` increasing maps to `priority` decreasing.\n    #   A sigmoid like `1 / (1 + exp(k * m))` achieves decreasing, but peaks at 0.5.\n\n    # Let's shift and scale the input to the sigmoid to achieve the desired range.\n    # Target function: f(m) where f(0)=1 and f(large)=0.\n    # We can use `1 - sigmoid(a*m + b)`.\n    # sigmoid(x) = 1/(1+exp(-x))\n    # 1 - 1/(1+exp(-a*m - b)) = exp(-a*m - b) / (1+exp(-a*m - b))\n    # This function decreases. If we want it to start at 1 for m=0:\n    # At m=0: exp(-b) / (1+exp(-b)) = 1. This is only possible if exp(-b) -> infinity, which is not standard.\n\n    # A common way to model priorities for a \"best fit\" approach with a sigmoid shape:\n    # Normalize the \"slack\": `slack = bins_remain_cap - item`.\n    # We only consider bins where `slack >= 0`.\n    # Scale the slack to be in a range that makes sense for sigmoid.\n    # E.g., if bin capacities are up to 100, item sizes up to 50, max slack could be 100.\n    # Let `scaled_slack = (bins_remain_cap - item) / MAX_CAPACITY`.\n\n    # A simpler sigmoid definition:\n    # Let's assign a high priority to bins where `bins_remain_cap` is slightly above `item`.\n    # Use `sigmoid(slope * (item - (bins_remain_cap - threshold)))`\n    # where threshold is small positive value to ensure `bins_remain_cap > item`.\n\n    # Final attempt with a direct sigmoid mapping:\n    # Let `quality = item / bins_remain_cap` for valid bins.\n    # This is >= 1 for good fits. We want priority to be high when this is close to 1.\n    # Let `x = k * (item / bins_remain_cap)`.\n    # `1 / (1 + exp(-x))`\n    # If `item/bins_remain_cap = 1`, `x=k`, sigmoid = 0.5.\n    # If `item/bins_remain_cap < 1` (bins_remain_cap > item), `x < k`, sigmoid > 0.5.\n    # If `item/bins_remain_cap > 1` (bins_remain_cap < item), this is invalid.\n\n    # Let's combine the \"invalid bin\" condition with the sigmoid.\n    # For valid bins (`bins_remain_cap >= item`):\n    # Calculate a \"fit score\" that is high when `bins_remain_cap` is close to `item`.\n    # Metric: `1 - (bins_remain_cap - item) / MAX_SINGLE_ITEM_SIZE` (or similar scaling)\n    # This metric is 1 for perfect fit, and decreases.\n\n    # A common way to implement \"best fit\" with a sigmoid-like characteristic is to prioritize\n    # the bin that leaves the *least* remaining capacity.\n    # We can represent this by transforming `bins_remain_cap - item`.\n    # Let `x = bins_remain_cap - item`.\n    # For invalid bins (`x < 0`), priority is 0.\n    # For valid bins (`x >= 0`):\n    # We want a function `f(x)` where `f(0)` is max, and `f(x)` decreases as `x` increases.\n    # A sigmoid function of `x` could work if we invert it or scale it correctly.\n    # `1 - sigmoid(k * x)` is a candidate if `k > 0`.\n    # `1 - 1/(1+exp(-k*x))` = `exp(-k*x) / (1+exp(-k*x))`.\n    # For `x=0`, this is `1 / (1+1) = 0.5`. Still not 1.\n\n    # Let's use `1 - sigmoid_inv(x)` where `sigmoid_inv` is increasing.\n    # Consider `sigmoid(x) = 1/(1+exp(-x))`.\n    # Let's define `priority = 1 - sigmoid(k * (bins_remain_cap - item))`\n    # If `bins_remain_cap == item`: `1 - sigmoid(0)` = `1 - 0.5` = 0.5.\n    # If `bins_remain_cap > item`: `1 - sigmoid(positive)` = `1 - (>0.5)` = `(<0.5)`. This is decreasing priority.\n    # If `bins_remain_cap < item`: `1 - sigmoid(negative)` = `1 - (<0.5)` = `(>0.5)`. This is incorrectly prioritizing invalid bins.\n\n    # Let's ensure invalid bins get 0 priority, and then apply a sigmoid to the rest.\n    # Parameter `k`: Controls how sharply the priority drops as remaining capacity increases.\n    # Higher `k` means tighter fit preference.\n    k_fit = 2.0 # Steepness parameter for the sigmoid\n\n    # Calculate `residual = bins_remain_cap - item` for all bins.\n    residual = bins_remain_cap - item\n\n    # Initialize priorities to zero (for bins that cannot fit the item).\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item.\n    fit_mask = residual >= 0\n\n    # For bins that can fit, calculate a score using a sigmoid function.\n    # We want to give high priority to bins with small `residual` (i.e., tight fits).\n    # The sigmoid `1 / (1 + exp(-x))` increases with x.\n    # So we want to transform `residual` to be a decreasing function of `x`.\n    # Let's use `1 / (1 + exp(k_fit * residual))`.\n    # This function decreases from 1 (as residual -> -inf) to 0 (as residual -> inf).\n    # If `residual=0`, score is 0.5.\n    # If `residual>0`, score < 0.5.\n    # If `residual<0`, score > 0.5.\n\n    # To make it peak at 1 when residual=0:\n    # Consider `1 - 0.5 * sigmoid(k_fit * residual)` => `1 - 0.5 / (1 + exp(-k_fit * residual))`\n    # This seems complicated.\n\n    # A common strategy in literature is to use `1 / (1 + exp(k * (b_rem - item)))`\n    # and apply it only to bins that fit.\n    # If `b_rem - item = 0`, score = 0.5.\n    # If `b_rem - item = large`, score -> 0.\n\n    # Let's use this:\n    # `priority_for_fit_bins = 1 / (1 + np.exp(k_fit * residual[fit_mask]))`\n    # This yields scores between (0, 0.5] for `residual >= 0`.\n    # To get scores in [0, 1] and peak at 1, we need to transform.\n\n    # Let's scale the residual relative to something like the item size or a typical bin size.\n    # For simplicity, let's use the residual directly but control the sigmoid's steepness.\n\n    # A more principled approach for \"Sigmoid Fit Score\":\n    # Imagine a probability of *selection* based on fit quality.\n    # Fit quality = how close `bins_remain_cap` is to `item`.\n    # Let `value_to_sigmoid = -k_fit * residual`. This maps small residuals to large positive values.\n    # `sigmoid(value_to_sigmoid)` would be close to 1 for small residuals.\n    # So, for valid bins:\n    # `sigmoid_scores = 1 / (1 + np.exp(-k_fit * residual))`\n    # If `residual=0`, score = 0.5.\n    # If `residual > 0`, score < 0.5.\n    # If `residual < 0`, score > 0.5.\n\n    # The typical \"Best Fit\" heuristic aims to MINIMIZE `bins_remain_cap - item`.\n    # So, we want the priority to be high when `bins_remain_cap - item` is small.\n    # Let `x = bins_remain_cap - item`.\n    # Consider the sigmoid transformation `1 / (1 + exp(k * x))`.\n    # This maps small `x` to high values and large `x` to low values.\n    # For `x=0`, score=0.5. For `x>0`, score<0.5. For `x<0`, score>0.5.\n\n    # Let's use a robust sigmoid that maps distance to 0.\n    # `priority = sigmoid(k * (target_value - current_value))`\n    # Target value for `bins_remain_cap` is `item`.\n    # We want higher priority as `bins_remain_cap` approaches `item`.\n\n    # Let's try: `priority = 1 - sigmoid(k * (bins_remain_cap - item))` on valid bins.\n    # For valid bins: `bins_remain_cap >= item`.\n    # `k = 2.0`\n    # `metric = bins_remain_cap - item`\n    # `priority_valid = 1 - (1 / (1 + np.exp(-k * metric)))`\n    # For `metric = 0`, `priority = 1 - 0.5 = 0.5`.\n    # For `metric > 0`, `1 - sigmoid(positive)` = `1 - (>0.5)` = `(<0.5)`.\n    # This gives a decreasing priority for increasing residuals.\n\n    # Let's make it peak at 1.\n    # `priority = C * sigmoid(k * (ideal - current))`\n    # We want `ideal = item` for `bins_remain_cap`.\n    # The sigmoid `1 / (1 + exp(-x))` has range (0, 1).\n    # We want to center it and scale it.\n\n    # Consider the function: `f(r) = exp(-k*r)` where `r = bins_remain_cap - item`.\n    # For valid bins `r >= 0`.\n    # `f(0) = 1`. `f(r)` decreases as `r` increases.\n    # This is close to what we want. It's Gaussian-like but decays exponentially.\n    # Let's add epsilon to avoid log(0) if we ever use logs.\n\n    # Let's try: `priority = exp(-(bins_remain_cap - item) / scale)`\n    # This is exponential decay.\n\n    # For Sigmoid Fit Score, let's use the standard sigmoid `1 / (1 + exp(-x))`\n    # and find a suitable `x`.\n    # We want priority to be high when `bins_remain_cap` is close to `item`.\n    # Let's consider the metric `item - bins_remain_cap`.\n    # If `bins_remain_cap >= item`, then `item - bins_remain_cap <= 0`.\n    # If `bins_remain_cap < item`, then `item - bins_remain_cap > 0`.\n\n    # Let `x = k * (item - bins_remain_cap)`\n    # `priority = 1 / (1 + exp(-x))`\n    # If `bins_remain_cap == item`: `x = 0`, `priority = 0.5`.\n    # If `bins_remain_cap > item`: `x < 0`, `priority > 0.5`.\n    # If `bins_remain_cap < item`: `x > 0`, `priority < 0.5`.\n\n    # This prioritizes bins with MORE capacity. Not what we want.\n\n    # Let's reverse the argument: `x = k * (bins_remain_cap - item)`\n    # `priority = 1 / (1 + exp(-x))`\n    # If `bins_remain_cap == item`: `x = 0`, `priority = 0.5`.\n    # If `bins_remain_cap > item`: `x > 0`, `priority > 0.5`.\n    # If `bins_remain_cap < item`: `x < 0`, `priority < 0.5`.\n\n    # This still prioritizes bins with more capacity.\n\n    # To prioritize bins with *less* remaining capacity, we want a function that\n    # DECREASES with `bins_remain_cap - item`.\n    # Use `1 / (1 + exp(k * (bins_remain_cap - item)))`.\n    # Let `x = bins_remain_cap - item`.\n    # If `x = 0` (perfect fit), `priority = 1 / (1 + exp(0))` = `1 / 2` = 0.5.\n    # If `x > 0` (more capacity), `exp(k*x)` is larger, `priority` is smaller.\n    # If `x < 0` (invalid bin), `exp(k*x)` is smaller, `priority` is larger.\n\n    # We must handle invalid bins explicitly.\n    # Define `k` as a parameter affecting the \"tightness\" of the fit.\n    k_tightness = 3.0  # Higher k means tighter fit preference\n\n    # Calculate the \"slack\" or \"waste\" in each bin: remaining capacity minus item size.\n    slack = bins_remain_cap - item\n\n    # Initialize priorities to 0 (for bins that are too small).\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find the indices of bins that have enough remaining capacity to fit the item.\n    can_fit_mask = slack >= 0\n\n    # For bins that can fit, calculate their priority.\n    # We want higher priority for smaller slack values.\n    # Use the sigmoid function `1 / (1 + exp(-z))`.\n    # We need to map `slack` to `z` such that:\n    # - `slack = 0` (perfect fit) maps to a high priority.\n    # - `slack` increasing maps to priority decreasing.\n\n    # Let's use `z = -k_tightness * slack`.\n    # For `slack = 0`, `z = 0`, `sigmoid(0) = 0.5`.\n    # For `slack > 0`, `z < 0`, `sigmoid(z) < 0.5`.\n    # For `slack < 0`, `z > 0`, `sigmoid(z) > 0.5`.\n\n    # The function `1 / (1 + exp(k * slack))` is better for decreasing priority.\n    # It ranges from (0, 1) as slack goes from inf to -inf.\n    # At `slack = 0`, score = 0.5.\n    # At `slack > 0`, score < 0.5.\n    # At `slack < 0`, score > 0.5.\n\n    # So, apply this to `can_fit_mask` bins:\n    # Use `priorities[can_fit_mask] = 1 / (1 + np.exp(k_tightness * slack[can_fit_mask]))`\n    # This yields priorities in (0, 0.5] for valid bins.\n    # To get it into [0, 1] with a peak of 1 at slack=0, we can transform.\n\n    # A common mapping for \"best fit\" that resembles a sigmoid:\n    # Maximize `-(bins_remain_cap - item)` for valid bins.\n    # Map this to a sigmoid-like score.\n\n    # Let's use the logistic function `1 / (1 + exp(-x))`.\n    # We want to set `x` such that for valid bins (`slack >= 0`),\n    # the priority is high when `slack` is small.\n    # Let `x = k_tightness * (-(bins_remain_cap - item))`\n    # For valid bins: `x = k_tightness * (item - bins_remain_cap)`\n    # If `slack = 0` (`bins_remain_cap = item`): `x = 0`, `priority = 0.5`.\n    # If `slack > 0` (`bins_remain_cap > item`): `x < 0`, `priority < 0.5`.\n\n    # This prioritizes bins with MORE remaining capacity when scaled by sigmoid.\n    # The reverse mapping for descending priority:\n    # `priority = 1 - 1 / (1 + exp(-x))` where `x` is an \"error\" or \"badness\"\n    # Error `e = bins_remain_cap - item`.\n    # `priority = 1 - 1 / (1 + exp(-k_tightness * e))`\n    # If `e = 0`, `priority = 1 - 0.5 = 0.5`.\n    # If `e > 0`, `priority = 1 - (<0.5)` = `(>0.5)`. Wrong.\n\n    # Let's reconsider the exponential decay form for \"closeness\"\n    # `exp(-k * abs(slack))` is like a Gaussian.\n    # `exp(-k * slack)` for slack >= 0.\n\n    # Let's adopt a pragmatic approach based on common heuristic implementations:\n    # Prioritize bins that leave the least waste.\n    # The score is inversely related to `bins_remain_cap - item`.\n    # Use `1 / (1 + exp(k * (bins_remain_cap - item)))` for valid bins.\n    # This gives (0, 0.5] for valid bins.\n    # To shift this to [0, 1] and peak at 1:\n    # Add 0.5: `0.5 + 1 / (1 + exp(k * (bins_remain_cap - item)))`\n    # At slack=0: `0.5 + 0.5 = 1`.\n    # At slack>0: `0.5 + (<0.5)` = `(<1)`.\n    # This looks promising.\n\n    # Ensure k_tightness is positive.\n    if k_tightness <= 0:\n        k_tightness = 1.0 # Default to a reasonable value if invalid\n\n    # Calculate slack for all bins.\n    slack = bins_remain_cap - item\n\n    # Initialize priorities array.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item.\n    can_fit_mask = slack >= 0\n\n    # Calculate the core sigmoid-like score for fitting bins.\n    # Using `1 / (1 + exp(-x))` where `x` should be larger for better fits.\n    # Let `x = k_tightness * (-(slack))` which is `k_tightness * (item - bins_remain_cap)`.\n    # For `slack = 0`, `x=0`, score=0.5.\n    # For `slack > 0`, `x < 0`, score < 0.5.\n\n    # This is also not quite right if we want a peak at 1.\n\n    # Let's consider the input `bins_remain_cap` and `item`.\n    # The desired outcome is a high score when `bins_remain_cap` is close to `item`.\n    # Use `f(c) = 1 / (1 + exp(k * (c - item)))` where `c = bins_remain_cap`.\n    # If `c = item`, `f = 0.5`.\n    # If `c > item`, `f < 0.5`.\n    # If `c < item`, `f > 0.5`.\n\n    # Let's transform this output. We only care about `c >= item`.\n    # Let `v = 1 / (1 + exp(k * (c - item)))`. For valid bins, `v` is in (0, 0.5].\n    # We want to map `v=0.5` to `1` and `v->0` to `0`.\n    # This is like inverting `0.5 + v` and then mapping to `[0,1]`.\n    # `priority = 2 * v` is problematic (range (0, 1]).\n\n    # How about `priority = 1 - (item / bins_remain_cap)` for bins that fit?\n    # If `item=5`, `bins_remain_cap=5`, priority = `1-1=0`. WRONG.\n    # If `item=5`, `bins_remain_cap=10`, priority = `1-0.5=0.5`.\n\n    # Final strategy:\n    # We want to prioritize bins that have just enough capacity.\n    # Metric: `bins_remain_cap - item`. We want to minimize this non-negative value.\n    # Use `priority = sigmoid(gain * (ideal - current))`\n    # `ideal = item`\n    # `current = bins_remain_cap`\n    # `gain` relates to steepness.\n    # A function `1 / (1 + exp(-x))`\n    # Input `x = k * (item - bins_remain_cap)`.\n    # If `item == bins_remain_cap`, `x=0`, score=0.5.\n    # If `item < bins_remain_cap`, `x<0`, score<0.5.\n    # If `item > bins_remain_cap`, `x>0`, score>0.5.\n\n    # This prioritizes bins with *more* capacity, not less.\n\n    # Let's try to model the *probability of being the best fit*.\n    # Best fit means minimizing `bins_remain_cap - item`.\n    # Use `sigmoid(k * (item - bins_remain_cap))` where the sigmoid maps input `z` to `(0,1)`.\n    # We want high priority when `item - bins_remain_cap` is `0` or negative.\n\n    # Let's use the inverse of a decreasing function of `slack = bins_remain_cap - item`.\n    # A good function is `exp(-k * slack)`. It's 1 at slack=0, and decays.\n    # Let's map this to the (0, 1) range of a sigmoid if needed.\n    # `sigmoid(log(exp(-k*slack) / (1-exp(-k*slack))))`\n\n    # A robust choice that uses sigmoid properties:\n    # `score = 1 / (1 + exp(k * (bins_remain_cap - item)))` for `bins_remain_cap >= item`.\n    # This gives scores in `(0, 0.5]`.\n    # To map to `[0, 1]` and peak at `1` when `bins_remain_cap == item`:\n    # Add `0.5`: `0.5 + 1 / (1 + exp(k * (bins_remain_cap - item)))`\n    # This maps `0.5` to `1` and `0` to `0.5`.\n    # Let's ensure it goes up to 1.\n    # For bins with `bins_remain_cap` far greater than `item`, the second term goes to 0.\n    # So the score goes to `0.5`. We want it to approach 0 for very large slacks.\n\n    # A simple transformation:\n    # For valid bins (`bins_remain_cap >= item`):\n    # Score = `exp(-k * (bins_remain_cap - item))`\n    # Scale this to `[0, 1]` if necessary.\n    # This gives a value that decays exponentially.\n\n    # Let's use the common sigmoid form `1 / (1 + exp(-z))`.\n    # We want high priority for low `slack = bins_remain_cap - item`.\n    # So we want `z` to be high when `slack` is low.\n    # Let `z = k * (-slack)`.\n    # For valid bins (`slack >= 0`):\n    # `z` will be 0 or negative.\n    # If `slack=0`, `z=0`, score = 0.5.\n    # If `slack > 0`, `z < 0`, score < 0.5.\n\n    # To get a peak of 1 at `slack=0`, we can use `1 - sigmoid(k * slack)`.\n    # This gives `1 - sigmoid(0) = 0.5` for `slack=0`.\n    # And `1 - sigmoid(positive)` = `1 - (>0.5)` = `(<0.5)` for `slack > 0`.\n\n    # Let's use a robust approach to get a peak of 1 for `slack=0`.\n    # For valid bins, `priority = exp(-k * slack)`. This is already a decaying function.\n    # Let's scale it.\n    # The maximum value is 1 when slack=0.\n    # This is already in the `[0, 1]` range.\n\n    # Let's use `exp(-k * slack)` but transform to sigmoid form.\n    # Let `y = exp(-k * slack)`.\n    # We want to map `y` (which is in `(0, 1]`) to a priority.\n    # The priority should be `y` itself if we accept the exponential decay.\n\n    # If we MUST use sigmoid form `1/(1+exp(-z))`.\n    # Let `priority = 1/(1+exp(-z))`.\n    # For `slack=0`, we want priority=1. `1/(1+exp(-z)) = 1` implies `exp(-z)=0`, which is not possible.\n    # If we want priority approaching 1. Let `z` be large positive.\n    # This means `-k * slack` must be large positive. So `k` negative and `slack` positive.\n\n    # A common interpretation of \"sigmoid fit score\" for bin packing\n    # is to prioritize bins that leave the *least* amount of residual space.\n    # This is the \"Best Fit\" heuristic.\n    # We want to maximize `P(bin | item)` where `P` is high for bins with small `bins_remain_cap - item`.\n\n    # A simple implementation of this using sigmoid:\n    # `score = 1 / (1 + exp(k * (bins_remain_cap - item)))`\n    # This score for valid bins is in `(0, 0.5]`.\n    # We can map this to `[0, 1]` by `2 * score`.\n    # So, `priorities[can_fit_mask] = 2.0 * (1 / (1 + np.exp(k_tightness * slack[can_fit_mask])))`\n    # At slack=0: `2.0 * 0.5 = 1.0`.\n    # At slack > 0: `2.0 * (<0.5)` = `(<1.0)`.\n    # This is a valid interpretation and implementation.\n\n    # Let's use this form.\n    k_sigmoid_fit = 4.0 # Higher k means preference for tighter fits\n\n    # Calculate slack: how much capacity is left after putting the item in.\n    slack = bins_remain_cap - item\n\n    # Initialize priorities to zero.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find bins that have enough capacity.\n    can_fit_mask = slack >= 0\n\n    # For bins that can fit, calculate their priority using the sigmoid form.\n    # The expression `1 / (1 + exp(k * slack))` ranges from (0, 0.5] for slack >= 0.\n    # We multiply by 2 to scale the range to (0, 1] and peak at 1.\n    priorities[can_fit_mask] = 2.0 * (1.0 / (1.0 + np.exp(k_sigmoid_fit * slack[can_fit_mask])))\n\n    # Ensure priorities are not exactly zero unless the bin cannot fit.\n    # Small positive values are fine for bins that fit.\n    # The sigmoid output `1 / (1 + exp(k*slack))` is strictly positive for finite inputs.\n    # The `2.0 * ...` multiplication ensures it's also strictly positive.\n\n    return priorities",
    "response_id": 16,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 283.6339404013986,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response18.txt_stdout.txt",
    "code_path": "problem_iter1_code18.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements an epsilon-greedy strategy for online bin packing.\n\n    This heuristic balances exploration (trying different bins) with exploitation\n    (preferring bins that are a good fit).\n\n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: A numpy array where each element is the remaining\n                         capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, containing the\n        priority score for placing the item in each bin.\n    \"\"\"\n    epsilon = 0.2  # Probability of exploration\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify valid bins where the item can fit\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_indices = np.where(valid_bins_mask)[0]\n\n    if len(valid_bins_indices) == 0:\n        # No bin can fit the item, return all zeros (or handle as error)\n        return priorities\n\n    # Exploitation: Calculate a 'goodness' score for each valid bin\n    # A good fit is a bin where remaining capacity is close to the item size,\n    # but not too much larger (to avoid wasted space). We want bins that are\n    # \"tight\" fits.\n    # We use a kernel-like approach where the score is higher for bins with\n    # remaining capacity closer to the item size.\n    # A simple approach: -(remaining_capacity - item_size)^2, then clip to 0\n    # if the item doesn't fit, and normalize.\n    \n    # Calculate 'tightness' score for valid bins\n    tightness_scores = (bins_remain_cap[valid_bins_mask] - item)\n\n    # We want smaller remaining capacity for a tighter fit.\n    # Let's assign higher priority to bins with smaller positive remaining capacity.\n    # We can use the inverse of the remaining capacity as a proxy for priority.\n    # Avoid division by zero if remaining capacity is exactly the item size.\n    # A common technique is to add a small epsilon or use a different scoring.\n    # For this example, let's score based on how \"full\" the bin becomes with this item.\n    # Priority = 1 / (1 + (bins_remain_cap - item)) for valid bins\n    # or simpler: priority = - (bins_remain_cap - item) for tighter fits\n    \n    # A more robust approach: similarity score.\n    # Higher priority if remaining capacity is closer to 'item'\n    # using a Gaussian-like function centered at 'item' or slightly above.\n    # Let's use a score that penalizes large gaps.\n    # score = max(0, 1 - (bins_remain_cap[valid_bins_mask] - item) / max(1, np.max(bins_remain_cap)))\n    # Or, a simpler heuristic: prioritize bins that have *just enough* space.\n    # Let's score based on how much space is left after packing:\n    # score = - (bins_remain_cap[valid_bins_mask] - item) \n    # We want the smallest non-negative residual capacity.\n\n    # Let's define the \"goodness\" as the negative of the residual capacity.\n    # This means bins with residual capacity closer to 0 get higher scores.\n    exploitation_scores = -tightness_scores\n\n    # Normalize exploitation scores to be between 0 and 1\n    if exploitation_scores.size > 0:\n        min_score = np.min(exploitation_scores)\n        max_score = np.max(exploitation_scores)\n        if max_score > min_score:\n            priorities[valid_bins_mask] = (exploitation_scores - min_score) / (max_score - min_score)\n        else:\n            # All valid bins have the same score\n            priorities[valid_bins_mask] = 1.0\n    else:\n        # Should not happen if valid_bins_indices is not empty, but for safety\n        return priorities\n\n    # Exploration: With probability epsilon, pick a random valid bin\n    if len(valid_bins_indices) > 0:\n        exploration_prob = epsilon / len(valid_bins_indices)\n        # Add a small random perturbation to encourage exploration\n        random_perturbation = np.random.rand(num_bins) * (epsilon / 2) # Small noise\n        priorities += random_perturbation\n        \n        # Ensure exploration component is properly handled.\n        # A cleaner way to implement epsilon-greedy:\n        # With probability epsilon, choose a random VALID bin.\n        # With probability (1-epsilon), choose the best bin according to some heuristic.\n        \n        # Let's redefine the strategy more directly:\n        \n        # 1. Identify all bins that can fit the item\n        can_fit_indices = np.where(bins_remain_cap >= item)[0]\n        \n        if len(can_fit_indices) == 0:\n            return np.zeros_like(bins_remain_cap) # Cannot fit anywhere\n            \n        # 2. Decide whether to explore or exploit\n        if np.random.rand() < epsilon:\n            # Explore: Pick a random bin from those that can fit\n            chosen_bin_index = np.random.choice(can_fit_indices)\n            # Assign high priority to the chosen bin, and lower to others\n            exploration_priorities = np.zeros_like(bins_remain_cap)\n            exploration_priorities[chosen_bin_index] = 1.0 # Highest priority for exploration choice\n            return exploration_priorities\n        else:\n            # Exploit: Choose the bin that results in the smallest remaining capacity\n            # or a tightest fit.\n            \n            # For 'tightest fit', we prefer bins where bins_remain_cap - item is minimized\n            # but still non-negative.\n            residual_capacities = bins_remain_cap[can_fit_indices] - item\n            min_residual_index_in_subset = np.argmin(residual_capacities)\n            \n            # The actual index in the original bins_remain_cap array\n            best_exploit_bin_index = can_fit_indices[min_residual_index_in_subset]\n            \n            # Assign high priority to the best bin found via exploitation\n            exploitation_priorities = np.zeros_like(bins_remain_cap)\n            exploitation_priorities[best_exploit_bin_index] = 1.0 # Highest priority for exploitation choice\n            \n            return exploitation_priorities\n\n    else:\n        # No bin can fit the item\n        return np.zeros_like(bins_remain_cap)",
    "response_id": 18,
    "obj": 4.2979656960510715,
    "cyclomatic_complexity": 7.0,
    "halstead": 265.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    This heuristic prioritizes bins that have enough capacity for the item,\n    with a preference for bins that will leave a smaller remaining capacity\n    after packing the item. This aims to \"fill up\" bins more efficiently.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Only consider bins that have enough capacity for the item\n    feasible_bins_mask = bins_remain_cap >= item\n    \n    # Calculate the remaining capacity if the item is placed in a feasible bin\n    # This is the difference between the bin's remaining capacity and the item's size\n    remaining_capacities_after_packing = bins_remain_cap[feasible_bins_mask] - item\n\n    # We want to prioritize bins that leave a *smaller* remaining capacity after packing.\n    # A smaller remaining capacity implies a tighter fit.\n    # To use Softmax effectively for prioritization, we typically want higher values\n    # to represent higher priority. Thus, we invert the remaining capacities\n    # to make smaller remaining capacities result in larger scores.\n    # Adding a small epsilon to avoid division by zero or log(0) if a bin\n    # becomes exactly full.\n    scores = 1.0 / (remaining_capacities_after_packing + 1e-9)\n    \n    # Apply the Softmax function to convert scores into probabilities (priorities)\n    # Softmax ensures that the sum of priorities is 1, but more importantly,\n    # it emphasizes differences between scores.\n    exp_scores = np.exp(scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Initialize priorities for all bins to 0\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Assign the calculated probabilities to the feasible bins\n    priorities[feasible_bins_mask] = probabilities\n\n    return priorities",
    "response_id": 19,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 55.506595772116384,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response24.txt_stdout.txt",
    "code_path": "problem_iter1_code24.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    The Exact Fit First strategy prioritizes bins that can fit the item exactly,\n    meaning their remaining capacity is equal to the item's size. Among the\n    bins that fit exactly, it prioritizes those with the smallest remaining\n    capacity (which would be the ones with remaining capacity equal to the item size).\n    If no bin fits the item exactly, it falls back to prioritizing bins that\n    can simply accommodate the item, favoring those with the least remaining\n    capacity (tightest fit among those that can hold the item).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can exactly fit the item\n    exact_fit_mask = (bins_remain_cap == item)\n    if np.any(exact_fit_mask):\n        # For exact fits, assign a very high priority (e.g., infinity)\n        # and then sort them by remaining capacity (which is the same for all here)\n        # This ensures exact fits are preferred.\n        priorities[exact_fit_mask] = np.inf\n    else:\n        # If no exact fit, find bins that can fit the item\n        can_fit_mask = (bins_remain_cap >= item)\n        if np.any(can_fit_mask):\n            # For bins that can fit, assign priority based on the inverse of\n            # the remaining capacity plus a small epsilon to avoid division by zero\n            # and to ensure smaller remaining capacity gets higher priority.\n            # Adding a large constant to ensure these priorities are lower than exact fits.\n            priorities[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9) + 1000.0\n        # Bins that cannot fit the item will have a priority of 0 (from initialization)\n\n    return priorities",
    "response_id": 24,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 68.53238859703687,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response26.txt_stdout.txt",
    "code_path": "problem_iter1_code26.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score prioritizes bins that are a \"good fit\" for the item.\n    A \"good fit\" is defined by a sigmoid function applied to the ratio of\n    item size to remaining bin capacity. Bins that are almost full or\n    almost empty relative to the item size receive lower scores. Bins\n    where the item size is a significant fraction of the remaining capacity\n    receive higher scores.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Avoid division by zero or very small numbers for capacities\n    # Consider bins with capacity >= item size only\n    valid_bins_mask = bins_remain_cap >= item\n    \n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate the ratio of item size to remaining capacity for valid bins\n    # This ratio indicates how much of the remaining space the item would occupy.\n    # A ratio closer to 1 (but not exceeding it) indicates a better fit.\n    ratios = np.where(valid_bins_mask, item / bins_remain_cap, 0)\n\n    # Apply a sigmoid-like transformation. We want to favor ratios that are\n    # close to, but not exceeding, 1. A standard sigmoid (1 / (1 + exp(-x)))\n    # squashes values to (0, 1). We can shift and scale it to our needs.\n    # Let's use a scaled and shifted sigmoid: 1 / (1 + exp(-k * (x - c)))\n    # where k controls the steepness and c controls the center.\n    # We want the peak around x=1. Let's use k=10 and c=0.8 to emphasize\n    # fits where the item takes up at least 80% of the remaining capacity,\n    # with diminishing returns as it gets closer to 100%.\n    # The factor 2 is to stretch the output range to [-1, 1] if we used c=0\n    # and then shift and scale. A simpler approach for our purpose is to focus\n    # on the \"gap\" left. If the gap (bins_remain_cap - item) is small, it's a good fit.\n    # Let's directly penalize large remaining gaps.\n\n    # A more intuitive approach for \"good fit\" in BPP: prioritize bins\n    # where the remaining capacity is just enough or slightly more than the item.\n    # The 'waste' when placing the item should be minimized.\n    # waste = bins_remain_cap - item\n    # We want to minimize waste, but also consider that very small remaining capacity\n    # might be restrictive for future items.\n\n    # Let's try a score that is high when (bins_remain_cap - item) is small,\n    # but not negative, and also not excessively large.\n    # This can be achieved by a function that peaks at waste = 0.\n    # A negative exponential or a reversed parabola shape might work.\n\n    # Sigmoid Fit Score: Prioritize bins where the remaining capacity is\n    # just sufficient for the item. The \"fit score\" could be thought of\n    # as how well the item \"completes\" the bin.\n\n    # Consider the remaining capacity relative to the *item size*.\n    # A bin with capacity 10 and item size 8 has a ratio of 0.8.\n    # A bin with capacity 5 and item size 4 has a ratio of 0.8.\n    # We want to give a higher score to ratios closer to 1.\n\n    # Let's define a scoring function `f(r)` where `r = item / bins_remain_cap`.\n    # We want `f(r)` to be high for `r` close to 1 (but `r <= 1`).\n    # A sigmoid can do this.\n    # `sigmoid(x) = 1 / (1 + exp(-x))`\n    # Let's map `r` to `x`. If `r=1`, `x` should be large positive. If `r=0`, `x` should be negative.\n    # A simple mapping could be `x = a * (r - b)`.\n    # If `r=1`, `x = a * (1 - b)`. We want this to be large.\n    # If `r=0`, `x = a * (-b)`. We want this to be small (negative).\n\n    # Let's consider the inverse: ratio of remaining capacity to item size.\n    # `ratio_inv = bins_remain_cap / item`\n    # For valid bins, this ratio is >= 1. We want this ratio to be close to 1.\n    # Let's call this `fit_ratio`.\n    fit_ratios = np.where(valid_bins_mask, bins_remain_cap / item, 0)\n\n    # Now, we want a function `g(fit_ratio)` that is high when `fit_ratio` is close to 1.\n    # A Gaussian-like function centered at 1 could work.\n    # Or a transformed sigmoid: if `x = 1 - fit_ratio`, we want high scores for small `x`.\n    # `sigmoid(-k * x)` where `k > 0` gives high scores for small `x`.\n    # `sigmoid(-k * (1 - fit_ratio))` = `sigmoid(k * (fit_ratio - 1))`\n    # Let's use k=10 (steepness).\n    # For fit_ratio = 1, sigmoid(0) = 0.5.\n    # For fit_ratio = 1.1 (remaining cap is 10% more than item), sigmoid(1) = 0.73.\n    # For fit_ratio = 0.9 (this case is excluded by valid_bins_mask), it would be sigmoid(-1) = 0.27.\n    # For fit_ratio = 2.0, sigmoid(10) is close to 1. This is not ideal. We don't want bins with massive excess capacity to be top priority.\n\n    # Let's refine the sigmoid. We want to penalize bins that are too large.\n    # So, a high score when `bins_remain_cap - item` is small and positive.\n    # Let `gap = bins_remain_cap - item`. We want `gap` to be close to 0.\n    # Let's transform `gap`.\n    # For valid bins: `gaps = bins_remain_cap - item`\n    gaps = np.where(valid_bins_mask, bins_remain_cap - item, np.inf) # Penalize invalid bins\n\n    # Now we want a function that is high for `gaps` close to 0.\n    # Let's use a sigmoid on the negative gap, shifted and scaled.\n    # `sigmoid(k * (c - gap))`\n    # Choose `k` for steepness, `c` for center.\n    # We want peak at `gap = 0`. So, `c=0`.\n    # `sigmoid(k * (-gap))` = `1 / (1 + exp(k * gap))`\n    # For `gap = 0`, score = 0.5.\n    # For `gap = small_positive` (e.g., 0.1), score = `1 / (1 + exp(0.1k))`. If k=10, exp(1) = 2.7, score = 1/3.7 = 0.27. This is lower than 0.5. We want higher.\n\n    # Let's use `sigmoid(-k * gap)` for `gap > 0` and assign 0 otherwise.\n    # If `gap` is small and positive, `-k * gap` is small and negative. Sigmoid will be < 0.5.\n    # We need the sigmoid to peak at gap=0.\n\n    # Let's rethink the input to sigmoid.\n    # Consider `x = item / bins_remain_cap`. We want `x` close to 1.\n    # `sigmoid(k * (x - c))`.\n    # Let `k = 5`, `c = 0.8`.\n    # If `r = 0.8`, `x = 5 * (0.8 - 0.8) = 0`. Sigmoid(0) = 0.5.\n    # If `r = 0.9`, `x = 5 * (0.9 - 0.8) = 0.5`. Sigmoid(0.5) = 0.62.\n    # If `r = 0.95`, `x = 5 * (0.95 - 0.8) = 0.75`. Sigmoid(0.75) = 0.68.\n    # If `r = 1.0`, `x = 5 * (1.0 - 0.8) = 1.0`. Sigmoid(1.0) = 0.73.\n    # If `r = 0.7`, `x = 5 * (0.7 - 0.8) = -0.5`. Sigmoid(-0.5) = 0.38.\n    # If `r = 0.5`, `x = 5 * (0.5 - 0.8) = -1.5`. Sigmoid(-1.5) = 0.18.\n\n    # This function prioritizes bins where the item takes up a good chunk of remaining capacity,\n    # favoring ratios closer to 1. It penalizes bins where the item is too small for the remaining capacity.\n\n    # Parameters for the sigmoid function:\n    # `k`: Controls the steepness of the sigmoid curve. Higher k means a sharper transition.\n    # `c`: The center of the sigmoid curve. We want the peak around `item / remaining_cap = 1`.\n    # Let's set `c` slightly below 1 to prioritize bins that are not *exactly* full,\n    # but have some residual space. For example, if `c=0.9`, bins where `item/cap >= 0.9` get higher scores.\n    # If we want bins that are *almost* full, `c` should be close to 1. Let's use `c=0.95`.\n    # If we want bins where the item fits reasonably well, `c` could be lower.\n    # Let's make `c` a parameter that depends on the item size relative to bin size,\n    # or simply set a fixed heuristic value.\n\n    # Let's use `k=10` and `c=0.9` as a starting point.\n    k = 10.0\n    c = 0.90\n\n    # Calculate `x = k * (item / bins_remain_cap - c)` for valid bins\n    # Where bins_remain_cap is sufficient for the item.\n    # For bins where item > bins_remain_cap, the ratio is effectively infinity,\n    # leading to very small sigmoid values (close to 0).\n    \n    # Handle the case where bins_remain_cap is 0 or very small leading to division by zero.\n    # We've already filtered for bins_remain_cap >= item, so this is less of an issue\n    # unless item itself is 0 or very small.\n    \n    # Calculate ratios where valid:\n    ratios = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_mask = bins_remain_cap >= item\n    ratios[valid_mask] = item / bins_remain_cap[valid_mask]\n\n    # Calculate the sigmoid input:\n    sigmoid_input = k * (ratios - c)\n\n    # Apply the sigmoid function:\n    # `priorities = 1 / (1 + np.exp(-sigmoid_input))`\n    # This function gives values between 0 and 1.\n    # High priority for ratios closer to `c` and then increasing towards 1.\n    # Ratios significantly below `c` get low scores.\n    # Ratios above `c` (but <= 1) get higher scores.\n    # For ratios > 1, sigmoid_input is large positive, sigmoid value is close to 1.\n    # We need to cap this or handle it.\n\n    # Let's redefine. Prioritize bins with minimal remaining capacity *after* placing the item,\n    # provided that capacity is non-negative.\n    # remaining_after_item = bins_remain_cap - item\n    # We want to maximize `1 / (1 + remaining_after_item)` if `remaining_after_item >= 0`.\n    # This penalizes large remaining capacities.\n    \n    # Let's use a modified sigmoid that focuses on the 'waste' or 'gap'.\n    # `score = sigmoid(k * (c - gap))` where `gap = bins_remain_cap - item`.\n    # `k > 0`, `c > 0`. Peak at `gap = c`.\n    # We want peak at `gap = 0`. So, `sigmoid(k * (0 - gap)) = sigmoid(-k * gap)`.\n    # This gives high scores for negative gaps (invalid). We want high for `gap = 0`.\n    \n    # Let's consider the \"tightness\" of the fit.\n    # A bin is \"tightly fitting\" if `bins_remain_cap - item` is small.\n    # Let `tightness = 1 / (1 + bins_remain_cap - item)`.\n    # This is high when `bins_remain_cap - item` is small.\n    # But this doesn't penalize bins that are too large for the item.\n\n    # Final approach: Sigmoid on the ratio `item / bins_remain_cap`.\n    # We want scores to be high when this ratio is close to 1.\n    # Use a sigmoid shifted and scaled.\n    # `score = sigmoid(a * (ratio - b))`\n    # `ratio = item / bins_remain_cap`\n    # If `ratio = 1`, score should be high.\n    # If `ratio < 1`, score should still be relatively high if `ratio` is close to 1.\n    # If `ratio` is much less than 1, score should be low.\n    # If `ratio > 1`, this bin is invalid, score should be 0.\n\n    scores = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Consider only bins where item can fit\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins that can fit the item:\n    # Calculate the \"fit quality\". We want the remaining capacity to be as close to zero as possible\n    # *after* placing the item.\n    # Let `remaining_space_after = bins_remain_cap[can_fit_mask] - item`\n    # We want to prioritize bins where `remaining_space_after` is small.\n    # Use a sigmoid function that maps small positive `remaining_space_after` to high values.\n    \n    # Let's try a score based on `1 - (remaining_space_after / bin_capacity)`\n    # This is related to `1 - waste / capacity`\n    \n    # A good heuristic is to prioritize bins that are nearly full but can still accommodate the item.\n    # This is often called the \"Best Fit\" strategy.\n    # The Sigmoid Fit Score aims to formalize this \"best fit\" intuition.\n    \n    # Let's define the score as: `sigmoid(k * ( (bin_capacity - item) - (bin_capacity_avg - item_avg) ))`\n    # This is getting complicated. Let's stick to a simpler, more direct sigmoid application.\n\n    # The core idea: Score should be high when `item` is close to `bins_remain_cap`.\n    # Let `x = bins_remain_cap / item`. We want `x` close to 1.\n    # `score = sigmoid(k * (1 - x))` for `x >= 1`.\n    # `x` here is `bins_remain_cap / item`.\n    \n    x_vals = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_mask_for_x = bins_remain_cap >= item\n    \n    # Avoid division by zero if item is 0, though unlikely for BPP\n    if item > 0:\n        x_vals[valid_mask_for_x] = bins_remain_cap[valid_mask_for_x] / item\n    else: # If item size is 0, any bin works, priority could be uniform or based on capacity\n        x_vals[valid_mask_for_x] = 1.0 # Treat as a perfect fit for 0-size items\n\n    # Now, `x_vals` contains `bins_remain_cap / item` for valid bins.\n    # We want scores to be high when `x_vals` is close to 1.\n    # `sigmoid(k * (1 - x_vals))`\n    # If `x_vals = 1`, score = sigmoid(0) = 0.5.\n    # If `x_vals = 1.1` (remaining cap is 10% more than item), score = sigmoid(-0.1) = 0.47.\n    # If `x_vals = 2.0` (remaining cap is twice the item), score = sigmoid(-1) = 0.27.\n    # This prioritizes bins that are closer to `item` size, and penalizes larger bins.\n    # This aligns with a \"Best Fit\" like strategy.\n    \n    # Let's use `k=5` to make it a bit smoother.\n    k_sigmoid = 5.0\n    \n    scores = 1 / (1 + np.exp(k_sigmoid * (x_vals - 1)))\n\n    # The scores range from 0 to 1.\n    # Values where `bins_remain_cap < item` will have `x_vals = 0`,\n    # leading to `1 / (1 + exp(k * (-1)))` which is ~0.27 for k=5.\n    # We want these invalid bins to have a score of 0.\n\n    final_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    final_priorities[valid_mask_for_x] = scores[valid_mask_for_x]\n\n    return final_priorities",
    "response_id": 26,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 210.83123629338053,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response28.txt_stdout.txt",
    "code_path": "problem_iter1_code28.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    The Epsilon-Greedy strategy balances exploration (trying less optimal bins)\n    and exploitation (choosing the best available bin).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploration\n\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Find bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if len(suitable_bins_indices) == 0:\n        return priorities  # No bin can fit the item\n\n    # Exploitation: Calculate the \"best fit\" priority for suitable bins\n    # A common heuristic for bin packing is \"best fit\": choosing the bin\n    # where the remaining capacity after placing the item is minimized.\n    # We want higher priority for bins that leave less remaining capacity\n    # (i.e., are a tighter fit).\n    remaining_capacities_after_fit = bins_remain_cap[suitable_bins_mask] - item\n    # Higher priority for smaller remaining capacities. We can use inverse of\n    # remaining capacity, but need to handle zero or very small remaining capacities.\n    # Adding a small constant to avoid division by zero and to normalize.\n    # A simple approach is to use 1 / (remaining_capacity + epsilon)\n    # or to transform it so that smaller remaining capacity means higher priority.\n    # Let's use the negative of the remaining capacity, which naturally\n    # gives higher values to smaller remaining capacities.\n    exploitation_priorities = -remaining_capacities_after_fit\n\n    # Assign these exploitation priorities to the suitable bins\n    priorities[suitable_bins_mask] = exploitation_priorities\n\n    # Exploration: With probability epsilon, pick a random suitable bin\n    # and assign it a very high priority to encourage exploration.\n    if np.random.rand() < epsilon and len(suitable_bins_indices) > 0:\n        random_bin_index = np.random.choice(suitable_bins_indices)\n        # Assign a high priority (e.g., a value larger than any possible\n        # exploitation priority). Since exploitation priorities are negative,\n        # any positive value will do. Let's pick a value that's clearly\n        # better than the best exploitation score.\n        exploration_priority_value = np.max(exploitation_priorities) + 1.0\n        priorities[random_bin_index] = exploration_priority_value\n\n    return priorities",
    "response_id": 28,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 4.0,
    "halstead": 101.02330072391149,
    "exec_success": true
  }
]