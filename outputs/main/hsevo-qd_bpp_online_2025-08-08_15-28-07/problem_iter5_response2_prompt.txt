{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines Best Fit and a Sigmoid-based preference for near-optimal residual capacity.\"\"\"\n    \n    # Calculate priorities based on Best Fit: prioritize bins that leave minimal remaining capacity.\n    # Use negative remaining capacity to ensure higher values for smaller remaining capacities.\n    best_fit_scores = -bins_remain_cap\n\n    # Identify bins that can fit the item.\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins that can fit, calculate a secondary score using a sigmoid function.\n    # This sigmoid favors bins where remaining capacity is close to the item size (but slightly larger).\n    # We want a peak around remaining_capacity = item * ideal_factor.\n    ideal_factor = 1.1  # Prefer bins leaving ~10% of item size as residual capacity\n    k_steepness = 6.0   # Controls the sharpness of the preference peak\n\n    # Calculate the ratio of remaining capacity to item size.\n    # Use a small epsilon to avoid division by zero if item is negligible.\n    ratios = np.where(item > 1e-9, bins_remain_cap / item, 1.0)\n\n    # Sigmoid 1: Increases as ratio increases (favors less full bins).\n    # Peaks when ratio > ideal_factor.\n    sigmoid1 = 1 / (1 + np.exp(-k_steepness * (ratios - ideal_factor)))\n\n    # Sigmoid 2: Decreases as ratio increases (favors more full bins).\n    # Peaks when ratio < ideal_factor.\n    sigmoid2 = 1 / (1 + np.exp(k_steepness * (ratios - ideal_factor)))\n\n    # Combine sigmoids to create a peak preference around ideal_factor.\n    # This score is high for bins that are neither too empty nor too full.\n    sigmoid_scores = sigmoid1 * sigmoid2\n\n    # Initialize priorities to zero.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Apply the combined scores only to bins that can fit the item.\n    # We want to combine the \"best fit\" tendency with the \"ideal residual capacity\" preference.\n    # A simple multiplicative approach can work:\n    # A bin is good if it's a good fit (high best_fit_scores) AND it has an ideal residual capacity (high sigmoid_scores).\n    # However, best_fit_scores are negative, so a simple multiplication might not be intuitive.\n    \n    # Let's prioritize based on the sigmoid score for bins that can fit,\n    # and use best-fit as a tie-breaker or as the primary driver.\n    # A common approach is to add a scaled sigmoid score to the best-fit score.\n    # We scale the sigmoid scores to influence the best-fit score without overwhelming it.\n    # The sigmoid scores are in [0, 0.25] (peak value of sigmoid1*sigmoid2 is at ratio=ideal_factor, which is 0.5*0.5=0.25).\n    # Scaling it by a factor, e.g., 10, would make it comparable to best-fit scores.\n\n    # Scale sigmoid scores to have a significant impact\n    scaled_sigmoid_scores = sigmoid_scores * 10.0 \n\n    # Combine the two scores. Bins that can fit get a combined score.\n    # Best fit is still the primary driver (negative values means smaller remaining space is better).\n    # The sigmoid score adds a bonus for being \"just right\".\n    priorities[can_fit_mask] = best_fit_scores[can_fit_mask] + scaled_sigmoid_scores[can_fit_mask]\n\n    # Ensure bins that cannot fit have zero priority.\n    priorities[~can_fit_mask] = -np.inf # Assign a very low priority to bins that cannot fit\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    available_bins_mask = bins_remain_cap >= item\n    available_bins_cap = bins_remain_cap[available_bins_mask]\n    \n    if available_bins_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing Heuristic 1 (Best) vs. Heuristic 2 (Worse): Heuristic 1 uses a sigmoid function on `-(bins_remain_cap - item)` scaled by `sensitivity` to prioritize tight fits, effectively implementing a \"Best Fit\" strategy by mapping small positive remaining spaces to high priorities. It correctly handles non-fitting bins by assigning near-zero priority. Heuristic 2 uses a simple additive combination of two poorly scaled scores (`-(bins_remain_cap - item)` and `1.0 / (remaining_capacity_after_item + 1e-6)`) without proper normalization or a clear strategy for combining \"Best Fit\" and \"Almost Full Fit\" objectives, leading to potentially unstable or suboptimal prioritization.\n\nComparing Heuristic 1 (Best) vs. Heuristic 3 (Worse): Heuristic 1 focuses on tight fits using a sigmoid. Heuristic 3 attempts a complex combination using two sigmoids to create a peak preference around an \"ideal ratio\" of remaining capacity to item size, but the logic of combining the \"best fit\" scores negatively with scaled sigmoid scores (which peak in the middle) is unclear and potentially counterproductive. The negative best-fit scores combined with positive sigmoid influence can lead to unpredictable results.\n\nComparing Heuristic 1 (Best) vs. Heuristic 4 (Worse): Heuristic 1 effectively implements \"Best Fit\" with a sigmoid. Heuristic 4 attempts to combine \"tightness\" and \"capacity\" scores before applying a sigmoid. However, the logic of `tightness_score = bins_remain_cap - item` and `capacity_score = bins_remain_cap / 100.0` with `weight_tightness * (-tightness_score) + weight_capacity * capacity_score` before sigmoid is convoluted. It doesn't clearly prioritize tight fits as effectively as Heuristic 1 and introduces a potentially arbitrary scaling for capacity.\n\nComparing Heuristic 1 (Best) vs. Heuristic 5 (Worse): Heuristic 1 is a clean \"Best Fit\" using sigmoid. Heuristic 5 combines \"Almost Full Fit\" (inverse of remaining capacity after fit) with the original remaining capacity multiplicatively. This prioritizes bins that are both \"almost full\" after packing and \"somewhat full\" before packing, which might not always align with minimizing bins, and the multiplicative interaction can be less intuitive than a focused \"Best Fit.\"\n\nComparing Heuristic 1 (Best) vs. Heuristic 6 & 7 (Worse): Heuristics 6 and 7 are identical and combine \"tightest fit\" (inverse of remaining capacity after fit) with \"initially fuller bins\" (inverse of initial remaining capacity) multiplicatively. While a valid combination strategy, Heuristic 1's focused \"Best Fit\" approach using sigmoid is often a more robust starting point for bin packing heuristics, as it directly optimizes for minimal waste. The multiplicative combination in 6/7 can amplify issues if either component is poorly scaled.\n\nComparing Heuristic 1 (Best) vs. Heuristic 8 (Worse): Heuristic 1 is a clear \"Best Fit\". Heuristic 8 tries to smooth the \"tight fit\" preference by slightly penalizing extremely tight fits using `available_bins_cap / (available_bins_cap - item + smoothing_factor + 1e-9)`. While trying to avoid \"too tight\" fits is sometimes useful, Heuristic 1's direct \"Best Fit\" is generally more robust for the primary goal of minimizing bins. The smoothing factor (`0.15 * item`) might also lead to suboptimal choices for different item sizes.\n\nComparing Heuristic 1 (Best) vs. Heuristic 9 (Worse): Heuristic 1 is \"Best Fit\". Heuristic 9 attempts to apply a similar smoothing as Heuristic 8 but with a different smoothing factor and calculation (`available_bins_cap / (available_bins_cap - item + smoothing_factor + 1e-9)`). The core issue remains: aggressively penalizing tight fits might not be optimal for general bin packing, and the specific smoothing logic is less direct than Heuristic 1's focus on minimizing waste.\n\nComparing Heuristic 1 (Best) vs. Heuristic 10 (Worse): Heuristic 1 is a deterministic \"Best Fit\". Heuristic 10 introduces an Epsilon-Greedy approach, randomly selecting a bin with probability `epsilon` (0.2). While exploration can be useful in learning-based methods, for a pure heuristic, deterministic \"Best Fit\" is usually preferred for consistency and direct optimization. The random selection can lead to suboptimal choices, undermining the goal of minimizing bins.\n\nComparing Heuristic 1 (Best) vs. Heuristics 11, 13, 15, 16 (Worse): These heuristics simply return zeros or an empty array without any logic for prioritization. They are non-functional and thus the worst possible heuristics.\n\nComparing Heuristic 1 (Best) vs. Heuristics 12, 14 (Worse): These heuristics check for available bins but then fail to implement any actual prioritization logic, returning zeros. They are also non-functional.\n\nComparing Heuristic 1 (Best) vs. Heuristic 17 (Worse): Heuristic 1 is a clean \"Best Fit\". Heuristic 17 attempts a \"Sigmoid Fit Score\" by combining two sigmoids to create a peak preference around an \"ideal ratio\" (`item * 1.2`). While this aims to balance tightness and available space, the complexity of the combined sigmoid and the arbitrary 'ideal ratio' might be less robust than a direct \"Best Fit\" for minimizing bin count. Heuristic 1's approach is more straightforward and directly targets the primary optimization goal.\n\nComparing Heuristic 1 (Best) vs. Heuristic 18 (Worse): Heuristic 1 is a clean \"Best Fit\". Heuristic 18 uses a \"Sigmoid Fit Score\" by combining two sigmoids to create a peak preference around `ideal_ratio` (1.1), controlled by steepness `k` (5.0). The logic of combining `sigmoid(k*(ratio-ideal))` and `sigmoid(-k*(ratio-ideal))` to create a peak, while a valid mathematical construct, is more complex and potentially less direct for the bin packing goal of minimizing bins compared to Heuristic 1's focused \"Best Fit\" strategy. The arbitrary `ideal_ratio` and `k` also make it less universally applicable.\n\nComparing Heuristic 1 (Best) vs. Heuristic 19 (Worse): Heuristic 1 is a clean \"Best Fit\". Heuristic 19 also uses a two-sigmoid approach to create a peak preference around an `ideal_ratio` (1.1) with steepness `k` (5.0), similar to Heuristic 18. The complexity and reliance on arbitrary parameters (`ideal_ratio`, `k`) make it less direct and potentially less robust than the straightforward \"Best Fit\" of Heuristic 1 for the primary objective of minimizing bins.\n\nComparing Heuristic 1 (Best) vs. Heuristic 20 (Worse): Heuristic 1 is a clean \"Best Fit\". Heuristic 20 also employs a two-sigmoid approach similar to 17, 18, and 19, aiming for a peak preference around `ideal_ratio` (1.1) with steepness `k` (5.0). The combination of sigmoids to create a preference peak, while mathematically sound, is more complex and less directly aligned with the primary bin packing goal of minimizing bins compared to Heuristic 1's direct \"Best Fit\" strategy. The arbitrary nature of `ideal_ratio` and `k` is also a drawback.\n\nOverall: Heuristic 1 implements a robust \"Best Fit\" strategy by prioritizing bins that minimize leftover space after placing an item, effectively using a sigmoid to map this fit quality to a priority. Heuristics 2-16 are either flawed, incomplete, or non-functional. Heuristics 17-20 attempt more complex \"Sigmoid Fit Score\" strategies, often involving combinations of sigmoids to create preference peaks, which are more intricate and rely on tunable parameters, making them potentially less robust or generalizable than the straightforward \"Best Fit\" of Heuristic 1.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Adaptive, contextual, performance-driven, goal-alignment.\n*   **Advice:** Shift focus from *a priori* simplicity to *a posteriori* performance. Design heuristics that adapt to the problem's specific characteristics and performance feedback, rather than strictly adhering to pre-defined simple rules.\n*   **Avoid:** Over-reliance on \"tight fit\" or \"almost full\" metaphors. These are often proxies for underlying goals, not the goals themselves. Avoid assuming monotonic preferences without empirical validation.\n*   **Explanation:** True effectiveness comes from aligning heuristic behavior with measurable optimization outcomes. This means being willing to explore complexity if it demonstrably improves performance on specific objectives, while rigorously testing for robustness and edge-case handling.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}