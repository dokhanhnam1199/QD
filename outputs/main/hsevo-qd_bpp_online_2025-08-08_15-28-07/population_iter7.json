[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    return priorities",
    "response_id": 0,
    "obj": 4.487435181491823,
    "SLOC": 3.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response21.txt_stdout.txt",
    "code_path": "problem_iter1_code21.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements the Best Fit heuristic for the online Bin Packing Problem.\n    The priority is the remaining capacity of the bin after placing the item,\n    with higher priority given to bins that leave less remaining capacity\n    (i.e., fit the item best).\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            remaining_capacity = bins_remain_cap[i] - item\n            \n            \n            priorities[i] = -remaining_capacity\n            \n    return priorities",
    "response_id": 21,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response3.txt_stdout.txt",
    "code_path": "problem_iter1_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the remaining capacity after adding the item for fitting bins\n    remaining_caps_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # The priority is higher for bins that will be almost full after adding the item\n    # A small positive value is added to ensure that fitting bins have higher priority than non-fitting bins.\n    # The degree of \"almost full\" is inversely proportional to the remaining capacity.\n    # A smaller remaining capacity leads to a higher priority.\n    priorities[can_fit_mask] = 1.0 / (remaining_caps_after_fit + 1e-6) # Add epsilon to avoid division by zero\n    \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response6.txt_stdout.txt",
    "code_path": "problem_iter1_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We want bins that are a good fit but not too tight.\n    # A sigmoid function can model this preference: high score for medium-tightness.\n    # The \"center\" of the sigmoid should ideally be related to the item size itself,\n    # aiming for bins that are slightly larger than the item.\n    # Let's use a scaling factor that inversely relates to the item size,\n    # pushing the sigmoid's steep part towards bins with remaining capacity close to item size.\n\n    # Avoid division by zero or extremely small capacities that would lead to huge k values.\n    # Also, prevent k from becoming excessively large for very small items.\n    safe_bins_remain_cap = np.maximum(bins_remain_cap, 1e-9)\n\n    # We want to prioritize bins where remaining capacity is \"just right\" for the item.\n    # This means remaining capacity slightly larger than the item.\n    # The sigmoid function is suitable here:\n    # - For bins much larger than item, the sigmoid should approach 1.\n    # - For bins much smaller than item, the sigmoid should approach 0.\n    # - For bins with remaining capacity close to item size, we want a high score.\n\n    # Let's consider a \"sweet spot\" for remaining capacity as `item * (1 + margin)`\n    # where `margin` is a small positive value (e.g., 0.1 for 10% overhead).\n    # This is where we want the sigmoid to peak.\n\n    # A common form of sigmoid is 1 / (1 + exp(-k * (x - x0))).\n    # Here, x is the \"fit quality\", and we want the peak at a specific fit.\n    # Let's define \"fit quality\" as remaining_capacity / item_size.\n    # We want the peak when remaining_capacity / item_size is slightly > 1.\n    # So, let's say our peak is at remaining_capacity / item_size = 1.2.\n    # This means x0 = 1.2.\n\n    # The steepness 'k' can be influenced by how selective we are.\n    # A larger 'k' means a sharper peak.\n    # We can make 'k' dependent on the item size itself, such that smaller items\n    # are more flexible in their bin choices (lower k), while larger items\n    # require more precise fits (higher k). Or, the other way around: larger items\n    # have fewer fitting bins, so we want to be more aggressive in picking a good one.\n\n    # Let's try to make the \"good fit\" region centered around the item size.\n    # Bins that are too full (remaining_cap < item) should have a very low priority.\n    # Bins that are very empty (remaining_cap >> item) should have a medium-high priority,\n    # as they offer flexibility for future items.\n    # Bins where remaining_cap is slightly larger than item should have the highest priority.\n\n    # Let's use the sigmoid to model the \"how well does this bin accept the item\n    # without being too empty or too full\".\n\n    # Consider the ratio `bins_remain_cap / item`.\n    # If item is 0, this is problematic. Handle this case.\n    if item < 1e-9:\n        # If the item is negligible, any bin is a good fit.\n        # Prioritize bins with less remaining capacity to consolidate.\n        # But to keep it within the sigmoid idea, let's just give them a high score.\n        # Or maybe prioritize less filled bins to encourage spreading out.\n        # For this problem, if item is zero, we don't need to place it. Let's return zeros or a very small value.\n        return np.zeros_like(bins_remain_cap)\n\n    ratios = bins_remain_cap / item\n\n    # We want a peak when ratio is slightly greater than 1.\n    # Let's aim for a peak around ratio = 1.2 (meaning bin is 20% larger than item).\n    x0 = 1.2\n\n    # Steepness factor k. We want a higher score for ratios closer to x0.\n    # Let's try to make k inversely proportional to item size: smaller items are more flexible.\n    # Or, more directly, let's make k related to how tightly we want to pack.\n    # A simple sigmoid: 1 / (1 + exp(-k * (ratios - x0)))\n    # This function will be near 0 for ratios < x0 and near 1 for ratios > x0.\n    # We want the opposite: high score for ratios close to x0.\n    # So let's use: exp(-k * abs(ratios - x0))\n    # Or a sigmoid where the center is x0 and it decays away from it.\n    # A Gaussian-like shape could work: exp(- (ratios - x0)^2 / (2 * sigma^2))\n    # But we are asked to use Sigmoid Fit Score strategy.\n\n    # Let's use a sigmoid that peaks at x0 and decays on both sides.\n    # This can be achieved by combining two sigmoids or using a logistic function with a central shift.\n    # A simpler approach is to use the sigmoid's property of mapping to [0, 1] and\n    # transform it.\n\n    # Let's use a standard sigmoid: sigmoid(z) = 1 / (1 + exp(-z)).\n    # We want higher values when `ratios` is close to `x0`.\n    # Let's map `ratios` to `z` such that `z=0` when `ratios=x0`.\n    # `z = -k * (ratios - x0)`.\n\n    # The steepness 'k' can be tuned. Let's try to make it moderate.\n    # A fixed k might be too sensitive. Let's try a k that adapts a bit.\n    # For example, k could be related to the average bin fill ratio across all bins.\n    # For now, let's pick a reasonable fixed k.\n    k = 5.0 # Controls the steepness of the sigmoid. Higher k means a sharper peak.\n\n    # Calculate the sigmoid value. This will be close to 0 for ratios < x0 and close to 1 for ratios > x0.\n    # We want the opposite: low priority for ratios too small, high for ratios around x0,\n    # and medium-high for ratios much larger than x0 (to keep them as fallback).\n    # This suggests we are looking for bins that are 'just right'.\n\n    # Let's consider the \"distance\" from the ideal ratio `x0`.\n    # We want bins where `ratios` is close to `x0`.\n    # Let's re-center the sigmoid: `1 / (1 + exp(-k * (ratios - x0)))`\n    # This gives higher values for larger ratios.\n\n    # We want bins that are NOT too full (ratio >= 1).\n    # Bins with ratio < 1 should have zero priority.\n    # Bins with ratio >= 1, we want to prioritize those closest to `x0`.\n\n    # Let's define a function that is high around `x0` and decreases away.\n    # Option 1: Logistic \"bump\" function: `1 / (1 + exp(k * (ratios - x0))) * 1 / (1 + exp(-k * (ratios - x0)))`\n    # This is effectively `sech^2(k * (ratios - x0) / 2)`. This is Gaussian-like.\n\n    # Option 2: Two-sided sigmoid approach.\n    # If `ratios < x0`, we want priority to increase as `ratios` increases.\n    # If `ratios > x0`, we want priority to decrease as `ratios` increases.\n\n    # Let's try mapping the ratio to a value that we can then apply a sigmoid to.\n    # We want the \"best\" fit to be around `item` capacity.\n    # Consider the \"slack\" in the bin: `bins_remain_cap - item`.\n    # We want slack to be small but positive.\n\n    # Let's try a sigmoid that models \"how suitable\" a bin is.\n    # For bins with `bins_remain_cap < item`, they are unsuitable. Priority = 0.\n    # For bins with `bins_remain_cap >= item`, they are suitable to some degree.\n    # Let's focus on `bins_remain_cap >= item`.\n    # We want to give higher scores to bins with `bins_remain_cap` closer to `item`.\n    # But also, bins with slightly more capacity than `item` might be better than exact fits.\n\n    # Let's use the original item size `item` as the reference.\n    # And `bins_remain_cap` as the actual remaining capacity.\n    # We are looking for bins where `bins_remain_cap` is \"just enough\" but not too much.\n\n    # Let `ideal_cap = item * 1.1` (a bit more than the item itself).\n    # And `slack = bins_remain_cap - ideal_cap`.\n    # We want to maximize the sigmoid of `-slack`. This means small positive slack is best.\n\n    # To use the sigmoid fitting strategy, we need to map our 'quality' metric\n    # to an argument for the sigmoid function.\n    # Let's use the ratio `bins_remain_cap / item`.\n    # We want to prioritize bins where this ratio is close to 1.0 to 1.2.\n    # Let `x = bins_remain_cap / item`.\n    # Let's use a sigmoid function of the form `1 / (1 + exp(-k * (x - x0)))`.\n    # If we want higher values for `x` around `x0`, this standard sigmoid is problematic.\n\n    # Let's reframe: A good bin is one that accepts the item AND leaves a \"good\" amount of space.\n    # \"Good\" space is relative to the item.\n\n    # Consider a scoring function that rewards bins that are not too full and not too empty.\n    # Bins that cannot fit the item get a score of 0.\n    can_fit = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # For bins that can fit the item, let's compute a score based on remaining capacity.\n    # We want remaining capacity slightly larger than `item`.\n    # Let's define a \"target remaining capacity\" `target_rc`.\n    # A reasonable target is `item * 1.2`.\n    target_rc = item * 1.2\n\n    # The difference from the target: `bins_remain_cap[can_fit] - target_rc`\n    # We want this difference to be close to 0.\n    # Let's map this difference using a sigmoid that peaks at 0.\n    # `1 / (1 + exp(-k * (-difference)))` which is `1 / (1 + exp(k * difference))`.\n    # This function is high when `difference` is negative (i.e., `bins_remain_cap` is less than `target_rc`).\n    # And low when `difference` is positive. This is the opposite of what we want.\n\n    # We want a function that is high when `difference` is near 0.\n    # So, let's consider `exp(-k * (difference)^2)` which is Gaussian.\n    # But we need a sigmoid.\n\n    # Let's go back to the ratio `ratios = bins_remain_cap / item`.\n    # Target ratio `x0 = 1.2`.\n    # We want values close to `x0` to have high priority.\n    # Let's try a sigmoid which increases towards `x0` and then decreases after `x0`.\n    # This can be seen as `sigmoid(k * (ratios - x0))` where `k` is negative for the decreasing part.\n    # This implies `sigmoid_part1 = 1 / (1 + exp(-k1 * (ratios - x0)))` and\n    # `sigmoid_part2 = 1 / (1 + exp(k2 * (ratios - x0)))`.\n    # Then combine them: `score = sigmoid_part1 * sigmoid_part2`.\n    # For a symmetric peak, k1=k2. Let k1 = k2 = k.\n    # `score = (1 / (1 + exp(-k * (ratios - x0)))) * (1 / (1 + exp(k * (ratios - x0))))`\n    # `score = 1 / ((1 + exp(-k * (ratios - x0))) * (1 + exp(k * (ratios - x0))))`\n    # `score = 1 / (1 + exp(-k*(ratios-x0)) + exp(k*(ratios-x0)) + 1)`\n    # `score = 1 / (2 + 2 * cosh(k * (ratios - x0)))`\n    # This is proportional to `sech^2`.\n\n    # Let's simplify. We want bins that are \"just right\".\n    # bins_remain_cap is the remaining capacity. `item` is the item size.\n    # The \"fit\" can be measured by `bins_remain_cap - item`.\n    # We want this difference to be small and positive.\n\n    # Let's define `fit_metric = bins_remain_cap / item`.\n    # Target `x0 = 1.2`.\n    # Sigmoid: `S(z) = 1 / (1 + exp(-z))`\n    # We want high score for `fit_metric` near `x0`.\n\n    # Let's try to model \"fit goodness\" with a sigmoid.\n    # A bin that's too full has a negative value. A bin that's perfectly full has a zero value.\n    # A bin that's just right has a small positive value.\n    # A bin that's too empty has a large positive value.\n\n    # Let's consider the \"filling ratio\" if the item is placed: `(bins_remain_cap - item) / bins_remain_cap_initial`.\n    # This is tricky as initial capacity isn't given directly in this function.\n\n    # Let's use the remaining capacity and item size directly.\n    # A bin is \"good\" if `bins_remain_cap >= item`.\n    # Among these, we want those where `bins_remain_cap` is closest to `item`.\n\n    # Sigmoid strategy implies using `1 / (1 + exp(-x))` or its variants.\n    # We want to map our \"goodness\" measure to `x`.\n    # Let's define a \"desirability\" score:\n    # Bins that cannot fit the item get score 0.\n    # For bins that can fit (`bins_remain_cap >= item`):\n    # We want higher scores for `bins_remain_cap` closer to `item`.\n    # Let `distance_from_ideal = bins_remain_cap - item`.\n    # We want to minimize this distance.\n    # Let's map `distance_from_ideal` to `z` for a sigmoid.\n\n    # Let's try to get a score that is high for `bins_remain_cap` slightly greater than `item`.\n    # Consider `score = 1 / (1 + exp(-k * (bins_remain_cap - item)))`.\n    # This score increases with `bins_remain_cap`. This favors larger bins.\n    # We need to penalize bins that are too large.\n\n    # Let's use the ratio again: `ratios = bins_remain_cap / item`.\n    # Target `x0 = 1.2`.\n    # Let's consider the \"excess capacity ratio\": `excess_ratio = (bins_remain_cap - item) / item`.\n    # We want `excess_ratio` to be small and positive.\n    # Let `y = excess_ratio`. We want peak at `y` near 0.2 (which corresponds to ratio 1.2).\n    # A sigmoid that peaks around a value and decreases symmetrically is difficult with a single sigmoid.\n\n    # Let's try a simple sigmoid interpretation that captures \"goodness of fit\".\n    # The sigmoid function is monotonic. So we can use it to express \"how likely is this to be a good fit\".\n    # A common approach is to map a \"quality\" to the argument of the sigmoid.\n\n    # Let's consider bins where `bins_remain_cap >= item`.\n    # For these bins, let's define a \"fit quality\" `f`.\n    # We want `f` to be higher when `bins_remain_cap` is closer to `item`.\n    # Let's use the ratio `ratios = bins_remain_cap / item`.\n    # We want higher scores for `ratios` around `1.0` to `1.2`.\n\n    # Let's use `score = 1 / (1 + exp(-k * (ratios - x0)))`.\n    # This score increases with `ratios`. So it favors larger remaining capacities.\n    # To favor bins that are \"just right\", we need a score that drops for larger capacities.\n\n    # Consider a \"penalty\" for being too empty: `penalty_empty = max(0, item - bins_remain_cap)`.\n    # Consider a \"penalty\" for being too full (but still fits): `penalty_full = max(0, bins_remain_cap - item * 1.2)`.\n    # We want to minimize penalties.\n\n    # Let's use the Sigmoid Fit Score as a measure of \"how well does this bin 'fit' the item\n    # in a way that's optimal for future packing.\"\n    # The ideal scenario is a bin that is *almost* full, but not quite.\n    # Let's consider bins that have remaining capacity `c`.\n    # We are placing an item of size `i`.\n    # A bin is a potential candidate if `c >= i`.\n    # Among these, we want bins where `c` is \"just enough\" but not excessively large.\n\n    # Let's use the ratio `c / i`.\n    # Target ratio `x0 = 1.2` (meaning bin has 20% spare capacity).\n    # A standard sigmoid `S(z) = 1 / (1 + exp(-z))` is increasing.\n    # If we use `z = k * (c/i - x0)`, then score increases as `c/i` increases.\n    # This isn't quite right, as it rewards large remaining capacities.\n\n    # Let's consider the complement of the standard sigmoid for the \"decreasing\" part.\n    # `1 - S(z) = exp(-z) / (1 + exp(-z))`. This is decreasing.\n    # If we want a peak, we can combine two sigmoids:\n    # `Score = S(k * (c/i - x0)) * (1 - S(k * (c/i - x0)))` -- this is Gaussian-like.\n\n    # Let's consider a simpler interpretation of \"Sigmoid Fit Score\".\n    # We want to prioritize bins that are not too full and not too empty.\n    # Let's define a \"tightness score\":\n    # `tightness = bins_remain_cap / item`\n    # We want tightness to be around `1.0` to `1.2`.\n    # Let `ideal_tightness = 1.1`.\n    # `z = k * (tightness - ideal_tightness)`\n    # A score like `1 / (1 + exp(-z))` favors higher tightness.\n\n    # Let's consider the \"waste\": `waste = bins_remain_cap - item`.\n    # We want waste to be small and positive.\n    # Let `waste_score = exp(-k * waste)`. This favors smaller waste.\n    # This isn't a sigmoid-fit score though.\n\n    # Let's use a Sigmoid to represent \"how well does this bin perform IF it fits\".\n    # A bin that is too full has poor performance.\n    # A bin that is very empty also has poor performance (wasted space).\n    # The best performance is for bins that are \"just right\".\n\n    # Let `r = bins_remain_cap / item`.\n    # We want a high score for `r` around `1.0` to `1.2`.\n    # Let's use `x0 = 1.1` as the center of our preference.\n    # Consider `sigmoid_up = 1 / (1 + np.exp(-k * (r - x0)))`. This increases with `r`.\n    # Consider `sigmoid_down = 1 / (1 + np.exp(k * (r - x0)))`. This decreases with `r`.\n    # A product could give a peak: `score = sigmoid_up * sigmoid_down`.\n    # This would be `1 / (2 + exp(k*(r-x0)) + exp(-k*(r-x0)))`.\n\n    # Let's consider bins that are too small first.\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # For bins that can fit, let's calculate their suitability score.\n    # Suitability should be higher for bins with remaining capacity closer to `item`,\n    # but slightly larger.\n    # Let the target remaining capacity be `target = item * 1.2`.\n    target_capacity = item * 1.2\n\n    # We want to maximize a score related to `target_capacity - bins_remain_cap`.\n    # Specifically, we want a high score when `target_capacity - bins_remain_cap` is near 0.\n    # This is essentially `bins_remain_cap - target_capacity`. We want this to be near 0.\n    # Let `diff = bins_remain_cap[can_fit_mask] - target_capacity`\n    # We want a function that is high for `diff` near 0.\n    # Let's use `f(diff) = 1 / (1 + exp(-k * diff))`. This peaks at 0 and is decreasing.\n    # No, this peaks at 0 and is INCREASING. `1 / (1 + exp(-z))`.\n    # `f(z) = 1 / (1 + exp(-z))` means `f` increases with `z`. So if `z = k * diff`, `f` increases with `diff`.\n    # This means it favors bins that are LARGER than the target.\n\n    # Let's use `z = k * (target_capacity - bins_remain_cap[can_fit_mask])`.\n    # Then `f(z) = 1 / (1 + exp(-z))` will be high when `target_capacity - bins_remain_cap` is positive and large.\n    # This favors bins that are SMALLER than the target.\n\n    # We need a function that is high when `bins_remain_cap` is NEAR `target_capacity`.\n    # A sigmoid centered at the target capacity is needed for the \"goodness\".\n\n    # Let's try using `sigmoid(k * (bin_capacity - item))` to favor bins that are not too small.\n    # And `sigmoid(k * (item * 1.5 - bin_capacity))` to favor bins that are not too large.\n    # Let `target_peak = item * 1.1`.\n    # We can use the ratio `r = bins_remain_cap / item`.\n    # We want to score bins where `r` is near `1.0` to `1.2`.\n\n    # Consider a metric: how \"close\" is `bins_remain_cap` to `item`?\n    # `distance = abs(bins_remain_cap - item)`\n    # We want to minimize this distance for bins that can fit.\n\n    # Let's use the sigmoid function directly to represent the desirability of remaining capacity.\n    # For a bin to be desirable, it should have capacity `c` such that `item <= c <= item * some_factor`.\n    # Let the factor be `F = 1.5`. So we prefer `item <= c <= 1.5 * item`.\n    # `ratios = bins_remain_cap / item`. We prefer `1.0 <= ratios <= 1.5`.\n\n    # Sigmoid form: `1 / (1 + exp(-k * x))`\n    # If we want a score that peaks, it means it increases and then decreases.\n    # Let `ideal_ratio = 1.1`.\n    # Let `k = 5.0` (controls steepness).\n\n    # Consider the \"goodness\" metric: `goodness = bins_remain_cap / item`.\n    # We want to give a higher score to values close to `1.1`.\n    # Let `z = k * (goodness - 1.1)`.\n    # We want the sigmoid function to evaluate to something high when `z` is close to `0`.\n    # The function `1 / (1 + exp(-z))` increases as `z` increases.\n    # To get a peak, we need a function that increases and then decreases.\n    # This can be achieved by multiplying two sigmoids, or using a bell-shaped curve.\n    # A common approach to get a peak using sigmoids is:\n    # `score = Sigmoid(k1 * (value - center1)) * Sigmoid(-k2 * (value - center2))`\n    # For a symmetric peak, center1 = center2 = `ideal_value`, and `k1 = k2 = k`.\n    # `score = Sigmoid(k * (value - ideal_value)) * Sigmoid(-k * (value - ideal_value))`\n    # `score = [1 / (1 + exp(-k*(value - ideal_value)))] * [1 / (1 + exp(k*(value - ideal_value)))]`\n\n    # Let's apply this:\n    # `value = bins_remain_cap / item`\n    # `ideal_value = 1.1` (meaning we prefer bins with 10% more capacity than the item)\n    # `k = 5.0` (controls how sharp the peak is)\n\n    # First, handle cases where `item` is zero or negative (although problem implies positive sizes).\n    if item <= 1e-9:\n        # If item size is negligible, all bins are equally suitable.\n        # Prioritize bins with less remaining capacity to encourage packing tighter.\n        # A simple approach is to return the inverse of remaining capacity, normalized.\n        # Or just return a flat score, as any bin will do.\n        # For consistency with the sigmoid idea, let's give a high score to less filled bins.\n        # Let's return a score based on how much capacity is LEFT, inversely.\n        # The flatter the capacity, the better it is for future items.\n        # So, a higher score for bins with LESS remaining capacity.\n        # We want a high score for smaller `bins_remain_cap`.\n        # Let's use sigmoid `1 / (1 + exp(-k * (-bins_remain_cap)))` which is `1 / (1 + exp(k * bins_remain_cap))`.\n        # This decreases with `bins_remain_cap`. So we need the inverse: `1 - 1 / (1 + exp(k * bins_remain_cap))`\n        # which is `exp(k * bins_remain_cap) / (1 + exp(k * bins_remain_cap))`. This increases with capacity.\n        # We want to prioritize LESS capacity.\n        # Let's use sigmoid on inverse capacity: `1 / (1 + exp(-k * (-bins_remain_cap)))` where `k` is positive.\n        # So, `1 / (1 + exp(k * bins_remain_cap))`.\n        # To prioritize LESS remaining capacity, we want this score to be HIGH for small bins_remain_cap.\n        # The function `1 / (1 + exp(k * x))` decreases with x. So this works.\n        # However, we are trying to PACK an item. If item is 0, it doesn't need packing.\n        # Let's return zeros, or perhaps a preference for the most empty bins to leave room for larger items.\n        # For the problem statement (packing an item), a zero item doesn't need placing.\n        # However, if we interpret it as \"if there's a zero item, which bin is most 'optimal' to put it in\",\n        # it might be the least filled bin.\n        # Let's return a score that is higher for bins with less remaining capacity.\n        # `priorities = 1.0 - bins_remain_cap / np.max(bins_remain_cap)` is simple, but not sigmoid.\n        # `priorities = 1 / (1 + np.exp(5 * bins_remain_cap))`. This decreases as capacity increases.\n        # Let's just return a constant high value, indicating all are equally good for a null item.\n        return np.ones_like(bins_remain_cap) * 0.5 # Mid-range preference, not too full, not too empty.\n\n    ratios = bins_remain_cap / item\n    ideal_ratio = 1.1  # Target: bin capacity should be 1.1 times the item size.\n    k = 5.0          # Steepness parameter for the sigmoid.\n\n    # Calculate the first sigmoid component: favors ratios less than or equal to ideal_ratio.\n    # `sigmoid_up = 1 / (1 + exp(-k * (ratios - ideal_ratio)))`\n    # This score is high when `ratios - ideal_ratio` is positive, i.e., ratios > ideal_ratio.\n    # We want high score when `ratios` is NOT too large.\n    # So, let's use `1 - sigmoid(k * (ratios - ideal_ratio))`\n    # which is `exp(-k * (ratios - ideal_ratio)) / (1 + exp(-k * (ratios - ideal_ratio)))`.\n    # This is `sigmoid(-k * (ratios - ideal_ratio))`.\n    sigmoid_part1 = 1 / (1 + np.exp(-k * (ratios - ideal_ratio))) # High for ratios > ideal_ratio\n\n    # Calculate the second sigmoid component: favors ratios greater than or equal to ideal_ratio.\n    # We want high score when `ratios` is NOT too small (i.e., >= item).\n    # `sigmoid_down = 1 / (1 + exp(k * (ratios - ideal_ratio)))`\n    # This score is high when `ratios - ideal_ratio` is negative, i.e., ratios < ideal_ratio.\n    # We want to prioritize bins that can fit the item (`ratios >= 1`).\n    # So, let's use this to favor ratios closer to ideal_ratio from below.\n    sigmoid_part2 = 1 / (1 + np.exp(k * (ratios - ideal_ratio))) # High for ratios < ideal_ratio\n\n    # Combine the two: the product will be high only when both sigmoids are moderately high.\n    # This creates a peak around `ideal_ratio`.\n    priorities = sigmoid_part1 * sigmoid_part2\n\n    # Ensure that bins that cannot fit the item (bins_remain_cap < item) get zero priority.\n    # This means `ratios < 1`.\n    # For `ratios < 1`, `ratios - ideal_ratio` is negative and large.\n    # `sigmoid_part1` will be close to 0. `sigmoid_part2` will be close to 1.\n    # The product `priorities` will be close to 0. This is good.\n\n    # Let's refine the \"ideal_ratio\". It should ideally be related to `item` size.\n    # A larger `k` makes the peak narrower and higher.\n    # The product `sigmoid_part1 * sigmoid_part2` creates a bell-like shape.\n    # `sigmoid_part1` increases with `ratios`. `sigmoid_part2` decreases with `ratios`.\n    # The peak is at `ratios = ideal_ratio`.\n\n    # Let's reconsider the core idea: Sigmoid Fit Score.\n    # This should capture how well the item *fits*.\n    # A tight fit is good, but not if it leaves no room.\n    # An excess of room is also not ideal (wasted space).\n    # The \"perfect\" fit leaves minimal, but positive, remaining space.\n\n    # Let's use a single sigmoid to represent the \"quality of space left\".\n    # `remaining_space = bins_remain_cap - item`.\n    # We want this to be positive and small.\n    # Let `ideal_remaining_space = item * 0.1` (10% of item size).\n    # `z = k * (remaining_space - ideal_remaining_space)`\n    # `score = 1 / (1 + exp(-z))`\n    # This score increases as `remaining_space` increases. So it favors larger remaining spaces.\n    # We need to cap this and also ensure it's 0 for `remaining_space < 0`.\n\n    # Let's go back to the `ratios = bins_remain_cap / item`.\n    # The goal is to place the item into a bin such that the resulting `bins_remain_cap` is optimal.\n    # The optimal state is often considered to be bins that are \"nearly full\".\n    # Let's assign a priority score that represents how close we are to this \"nearly full\" state.\n\n    # Let's use a Sigmoid to represent the \"closeness\" to being full.\n    # Remaining capacity `c`. Item size `i`.\n    # Bin is \"fuller\" when `c` is smaller (for a fixed item `i`).\n    # We want smaller `c` values (but `c >= i`).\n    # Let's focus on bins where `bins_remain_cap >= item`.\n    # Let `x = bins_remain_cap`.\n    # We want to prioritize `x` values that are small, but not smaller than `item`.\n    # Let `ideal_max_remain_cap = item * 1.2`.\n    # We want high scores for `bins_remain_cap` in the range `[item, ideal_max_remain_cap]`.\n\n    # Let's use `score = 1 / (1 + exp(-k * (ideal_max_remain_cap - bins_remain_cap)))` for bins where `bins_remain_cap >= item`.\n    # This function increases as `bins_remain_cap` increases. It peaks at `bins_remain_cap = ideal_max_remain_cap`.\n    # This is still favoring larger remaining capacities up to a point.\n\n    # The Sigmoid Fit Score implies that the *fit itself* should be evaluated using a sigmoid.\n    # Let's try again with `ratios = bins_remain_cap / item`.\n    # `ideal_ratio = 1.1`.\n    # We want a function that is high for ratios near `1.1`.\n    # `sigmoid_peak = lambda val: 1 / (1 + np.exp(-k * (val - x0)))`\n    # A symmetric peak can be achieved with `sigmoid_peak(val) * sigmoid_peak(-val)`\n    # Or `sigmoid_peak(val) * (1 - sigmoid_peak(val))` (Gaussian-like).\n\n    # Let's simplify the interpretation: We want bins that are not overly full, and not overly empty.\n    # `score = sigmoid(k * (bins_remain_cap - item))` penalizes bins where `bins_remain_cap < item`.\n    # `score = 1 / (1 + exp(-k * (bins_remain_cap - item)))`.\n    # This score is low if `bins_remain_cap` is much less than `item`. It increases as `bins_remain_cap` increases.\n    # This already gives a preference to bins that can fit.\n    # To refine it, we want to favor bins that aren't excessively large.\n\n    # Let's use `k1` for the initial increase and `k2` for the subsequent decrease.\n    # `part1 = 1 / (1 + np.exp(-k1 * (bins_remain_cap - item)))` (favors fitting)\n    # `part2 = 1 / (1 + np.exp(-k2 * (item * target_factor - bins_remain_cap)))` (favors not too large)\n    # `target_factor = 1.2`\n    # `priorities = part1 * part2`\n\n    # Ensure `item` is positive for division.\n    if item < 1e-9:\n        # If item is zero, all bins are equally \"good\" or irrelevant.\n        # Returning a uniform score (e.g., 0.5) is reasonable.\n        return np.ones_like(bins_remain_cap) * 0.5\n\n    # Calculate suitability for bins that can fit the item.\n    # `ratios = bins_remain_cap / item`\n    # We want to prioritize ratios that are close to a 'sweet spot'.\n    # The sweet spot is slightly larger than 1, allowing some remaining capacity.\n    # Let the ideal remaining capacity be `item * 1.2`.\n    # So the ideal ratio is `1.2`.\n\n    # Using two sigmoids to create a peak.\n    # `k_steepness` controls how sharp the peak is.\n    k_steepness = 6.0\n    ideal_ratio = 1.1  # Prefer bins that leave around 10% of item size as remaining capacity.\n\n    # Sigmoid 1: Score increases as `bins_remain_cap / item` increases.\n    # This favors bins that are not too full.\n    # `score1 = 1 / (1 + exp(-k * (ratio - x0)))`\n    # Let `x0 = ideal_ratio`. So `score1` is high for `ratio > ideal_ratio`.\n    # We want to favor bins that are not excessively full, and not excessively empty.\n    # Let's use `sigmoid(k * (ratio - ideal_ratio))`. This is high for `ratio > ideal_ratio`.\n    score_part1 = 1 / (1 + np.exp(-k_steepness * (ratios - ideal_ratio)))\n\n    # Sigmoid 2: Score decreases as `bins_remain_cap / item` increases.\n    # This favors bins that are not too empty.\n    # We want score to be high for `ratio < ideal_ratio`.\n    # Use `sigmoid(-k * (ratio - ideal_ratio))`. This is high for `ratio < ideal_ratio`.\n    score_part2 = 1 / (1 + np.exp(k_steepness * (ratios - ideal_ratio)))\n\n    # The product gives a peak around `ideal_ratio`.\n    priorities = score_part1 * score_part2\n\n    # Ensure bins too small to fit the item have zero priority.\n    # For `ratios < 1`, `ratios - ideal_ratio` is negative.\n    # `score_part1` will be close to 0, `score_part2` will be close to 1. Product is close to 0. Correct.\n\n    # What if `bins_remain_cap` is very large?\n    # `ratios` is very large. `score_part1` is near 1. `score_part2` is near 0. Product is near 0. Correct.\n\n    # This approach effectively creates a Gaussian-like peak, using sigmoids.\n    # This is a common way to interpret \"Sigmoid Fit Score\" for optimization where\n    # a middle ground is preferred.\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.916234543278815,
    "SLOC": 29.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response25.txt_stdout.txt",
    "code_path": "problem_iter1_code25.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    available_bins_mask = bins_remain_cap >= item\n    available_bins_cap = bins_remain_cap[available_bins_mask]\n    \n    if available_bins_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    inverse_distances = available_bins_cap / (available_bins_cap - item + 1e-9)\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[available_bins_mask] = inverse_distances\n    \n    return priorities",
    "response_id": 25,
    "tryHS": true,
    "obj": 3.9589150378939015,
    "SLOC": 9.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response4.txt_stdout.txt",
    "code_path": "problem_iter2_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins by favoring tight fits and initially fuller bins.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    \n    fits = bins_remain_cap[can_fit_mask] - item\n    initial_rem_caps = bins_remain_cap[can_fit_mask]\n\n    # Primary score: Inverse of remaining capacity after item placement (tightest fit)\n    # Maximizes the \"fullness\" achieved by placing the item.\n    tight_fit_score = 1.0 / (fits + 1e-9)\n    \n    # Secondary score: Inverse of initial remaining capacity (prefers fuller bins)\n    # Adds a bonus for bins that were already less empty.\n    initial_fill_score = 1.0 / (initial_rem_caps + 1e-9)\n    \n    # Combine scores multiplicatively: rewards bins that are both tight-fitting and initially fuller.\n    # This aims to find a balance for \"Almost Full Fit\".\n    priorities[can_fit_mask] = tight_fit_score * initial_fill_score\n    \n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response18.txt_stdout.txt",
    "code_path": "problem_iter1_code18.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\"\"\"\n    epsilon = 0.2  # Exploration rate\n    n_bins = len(bins_remain_cap)\n    priorities = np.zeros(n_bins)\n\n    # Greedy choice: Find bins that can fit the item\n    suitable_bins_indices = np.where(bins_remain_cap >= item)[0]\n\n    if len(suitable_bins_indices) > 0:\n        # Calculate preference for suitable bins\n        # Prioritize bins that leave less remaining space after packing (Best Fit heuristic)\n        remaining_capacities_after_packing = bins_remain_cap[suitable_bins_indices] - item\n        # Higher priority for smaller remaining capacity\n        preferences = 1 / (1 + remaining_capacities_after_packing)\n        \n        # Normalize preferences to sum to 1 for probability distribution\n        if np.sum(preferences) > 0:\n            probabilities = preferences / np.sum(preferences)\n        else:\n            probabilities = np.ones(len(suitable_bins_indices)) / len(suitable_bins_indices)\n\n        # Epsilon-Greedy: With probability epsilon, choose a random suitable bin\n        if np.random.rand() < epsilon:\n            random_index = np.random.choice(len(suitable_bins_indices))\n            priorities[suitable_bins_indices[random_index]] = 1.0\n        else:\n            # With probability 1-epsilon, choose the bin with the highest preference\n            best_fit_index_in_suitable = np.argmax(preferences)\n            priorities[suitable_bins_indices[best_fit_index_in_suitable]] = 1.0\n    else:\n        # If no bin can fit the item, we can't assign a priority in this context\n        # (or we might consider creating a new bin, but that's outside this function's scope)\n        pass\n\n    return priorities",
    "response_id": 18,
    "tryHS": false,
    "obj": 4.238133226964499,
    "SLOC": 21.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response26.txt_stdout.txt",
    "code_path": "problem_iter1_code26.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Heuristic: Prioritize bins where the item fits snugly, but also consider\n    # bins with ample remaining capacity for future large items.\n    # The sigmoid function will compress scores between 0 and 1.\n    \n    # Calculate how \"tight\" the fit would be for each bin\n    # A smaller positive difference means a tighter fit.\n    tightness_score = bins_remain_cap - item\n    \n    # Ensure we don't have negative tightness scores (item doesn't fit)\n    tightness_score = np.maximum(tightness_score, -float('inf'))\n\n    # Calculate a score based on remaining capacity\n    # Larger remaining capacity gets a higher score, scaled for sigmoid\n    capacity_score = bins_remain_cap / 100.0 # Scale to prevent overflow with sigmoid\n    \n    # Combine scores using a sigmoid function to map to a [0, 1] range.\n    # We want to favor bins that are a good fit (tightness_score closer to 0)\n    # and also bins that have more remaining capacity.\n    # Let's use a weighted sum before the sigmoid.\n    \n    # Higher negative tightness_score means better fit, so we use -tightness_score\n    # A positive capacity_score means more space.\n    \n    # Example weights: Give more importance to a tighter fit\n    weight_tightness = 2.0\n    weight_capacity = 1.0\n    \n    combined_score_raw = weight_tightness * (-tightness_score) + weight_capacity * capacity_score\n    \n    # Apply sigmoid function\n    # We want to give a higher priority to bins where the item fits well,\n    # meaning bins_remain_cap - item is close to 0.\n    # For bins where it doesn't fit, the priority should be very low.\n    # The sigmoid function maps any real number to (0, 1).\n    # A larger input to sigmoid results in a value closer to 1.\n    # A smaller input results in a value closer to 0.\n\n    # Let's adjust the input to sigmoid to reflect our priorities.\n    # We want high priority for bins where (bins_remain_cap - item) is small and positive.\n    # And we want lower priority where (bins_remain_cap - item) is negative or very large positive.\n    \n    # For bins where item fits (bins_remain_cap >= item):\n    # The \"gap\" (bins_remain_cap - item) determines the \"snugness\".\n    # A smaller gap is better. We can use something like 1 / (1 + gap) or sigmoid of negative gap.\n    \n    # Let's try a simpler approach:\n    # Priority = Sigmoid( (bins_remain_cap - item) * k_fit + bins_remain_cap * k_capacity )\n    # k_fit: Controls sensitivity to how well the item fits. Higher k_fit means more penalty for poor fits.\n    # k_capacity: Controls sensitivity to remaining capacity. Higher k_capacity means prioritizing fuller bins more.\n\n    k_fit = 0.5  # Sensitivity to the fit. Larger values penalize poor fits more.\n    k_capacity = 0.1 # Sensitivity to remaining capacity. Larger values prefer more open bins.\n    \n    # Calculate scores. Only consider bins where the item fits.\n    fits = bins_remain_cap >= item\n    \n    # For bins where it fits, calculate a combined score\n    # High priority for small remaining capacity after fitting (tight fit)\n    # High priority for large remaining capacity overall (future flexibility)\n    \n    # A simple approach could be:\n    # Priority for fitting bins: A high score if remaining_cap - item is small and positive\n    # A low score if remaining_cap - item is large and positive.\n    \n    # Let's use a logistic function where the input represents a combination\n    # of how much space is left after placing the item and the total remaining space.\n    \n    # Option 1: Favor tight fits, but don't entirely ignore bins with more space.\n    # Map (bins_remain_cap - item) to a \"goodness of fit\" score.\n    # Small positive difference = good.\n    # Large positive difference = okay.\n    # Negative difference = bad.\n    \n    # We can use a sigmoid on the inverse of the remaining capacity after placement.\n    # If remaining_cap - item is small, then 1 / (remaining_cap - item) is large.\n    # If remaining_cap - item is large, then 1 / (remaining_cap - item) is small.\n    \n    # To handle the case where item does not fit, we set priority to 0.\n    # For bins that fit, we want to prioritize those with `bins_remain_cap - item` closer to 0.\n    # Also, having `bins_remain_cap` itself not too large might be good to avoid creating\n    # bins with too much empty space.\n    \n    # Let's define a preference for bins where the remaining capacity after placing the item is minimized.\n    # We can use the sigmoid function on a term that decreases as (bins_remain_cap - item) increases.\n    \n    # Consider the \"waste\" if we place the item: bins_remain_cap - item.\n    # We want to minimize this waste for a perfect fit, but we also want\n    # bins with larger `bins_remain_cap` to be considered if the waste isn't too large.\n    \n    # A score that prioritizes bins where (bins_remain_cap - item) is small and positive.\n    # Let's try sigmoid(- (bins_remain_cap - item) * 0.5 + bins_remain_cap * 0.05)\n    \n    # This formula will give higher values for:\n    # 1. Bins where `bins_remain_cap - item` is small and positive (due to the negative sign on this term)\n    # 2. Bins where `bins_remain_cap` is large (due to the positive sign on this term)\n    \n    # Let's refine: We want a higher score if `bins_remain_cap` is large enough to fit the item,\n    # and within those, we prefer those that result in less remaining capacity after fitting.\n    # This means `bins_remain_cap - item` should be as small as possible, but non-negative.\n    \n    # Let's use the negative of the remaining capacity after placing the item as input to sigmoid.\n    # This favors bins where `bins_remain_cap - item` is small (i.e., a tight fit).\n    # We also want to consider the overall remaining capacity to some extent.\n    \n    # Option: Sigmoid of a function that decreases with (bins_remain_cap - item)\n    # and increases with bins_remain_cap.\n    \n    # Let's combine two aspects:\n    # 1. How well the item fits (smaller remainder is better).\n    # 2. How much total capacity is left (larger might be good for future items).\n    \n    # Consider the term `bins_remain_cap - item`. We want this to be close to zero, but positive.\n    # Let's use `np.exp(-(bins_remain_cap - item))` which gives higher values for smaller `bins_remain_cap - item`.\n    # Then, apply sigmoid to scale these values and the overall remaining capacity.\n    \n    # Final idea: Prioritize bins where the item fits, and among those,\n    # prefer bins that have less remaining space *after* placing the item.\n    # This encourages filling bins efficiently.\n    \n    # We want `bins_remain_cap - item` to be small and non-negative.\n    # `sigmoid( -(bins_remain_cap - item) )` would do this.\n    # However, we also want to prefer bins that generally have more capacity\n    # for future items, but not excessively so that it leads to too many half-empty bins.\n    \n    # Let's try this: Sigmoid on the term that captures \"snugness\".\n    # A bin is a good candidate if `bins_remain_cap >= item`.\n    # Among fitting bins, a higher score for smaller `bins_remain_cap - item`.\n    \n    # Use a scaling factor to control the steepness of the sigmoid curve.\n    # `scale = 1.0` makes the transition around 0.\n    # We want to map `bins_remain_cap - item` such that values near 0\n    # result in high priority.\n    \n    # Let's use `np.exp(-bins_remain_cap / C)`. Higher `bins_remain_cap` -> lower score.\n    # This is for the \"fill them up\" strategy.\n    \n    # For the \"first fit decreasing\" or \"best fit\" idea, we want a tight fit.\n    # `sigmoid(-(bins_remain_cap - item))`\n    # If `bins_remain_cap - item` is small (tight fit), the argument is close to 0, sigmoid is ~0.5.\n    # If `bins_remain_cap - item` is large negative (item too big), argument is large positive, sigmoid is ~1.\n    # If `bins_remain_cap - item` is large positive (loose fit), argument is large negative, sigmoid is ~0.\n    \n    # This is the inverse of what we want. We want higher priority for tight fits.\n    \n    # Try: `sigmoid(k * (bins_remain_cap - item))`\n    # If `bins_remain_cap - item` is small positive (tight fit), sigmoid argument is small positive, ~0.5.\n    # If `bins_remain_cap - item` is large positive (loose fit), sigmoid argument is large positive, ~1.\n    # If `bins_remain_cap - item` is negative (won't fit), sigmoid argument is negative, ~0.\n    \n    # This seems to align better with favoring bins with less remaining capacity after placement.\n    # The `k` parameter controls how sensitive we are to the \"tightness\".\n    # `k=1.0` gives sigmoid(0) = 0.5 for a perfect fit.\n    \n    # Let's add a slight preference for bins that have *some* space left,\n    # to avoid immediately creating many bins with no room left.\n    # Maybe `sigmoid(k * (bins_remain_cap - item) + c * bins_remain_cap)`\n    \n    # Let's consider the goal: fill bins optimally.\n    # A good bin is one that can accommodate the item and leaves minimal remaining space.\n    # `remaining_after_fit = bins_remain_cap - item`\n    # We want `remaining_after_fit` to be small and non-negative.\n    \n    # Consider a function `f(x)` where `x = bins_remain_cap - item`.\n    # We want `f(x)` to be high when `x` is small and non-negative.\n    # `f(x) = exp(-x / scale)` for `x >= 0`, and `0` otherwise.\n    # Then scale this with sigmoid.\n    \n    # Calculate how much space would be left if we put the item in.\n    space_left = bins_remain_cap - item\n    \n    # Create a \"fit score\" that is high for small, non-negative `space_left`.\n    # We'll use the negative of `space_left` to map \"small positive\" to \"large positive\"\n    # for the sigmoid input.\n    # If `space_left` is negative (item doesn't fit), we want a very low priority.\n    # So, we can use a very large negative number for sigmoid input.\n    \n    fit_input = np.where(space_left >= 0, -space_left, -1e9) # Penalize items that don't fit\n    \n    # We can also incorporate a term related to the absolute remaining capacity.\n    # Perhaps bins that are already quite full (but can still fit the item) are prioritized.\n    # Let's consider the *normalized* remaining capacity as a secondary factor.\n    # However, this can be tricky without knowing the overall bin capacity limit.\n    # Assuming a standard bin capacity (e.g., 100):\n    \n    # Let's stick to the primary goal: tight fits.\n    # The input to sigmoid: `k * (-space_left)`\n    # `k` controls the sensitivity to tightness.\n    # `k = 1.0` -> `sigmoid(-space_left)`\n    # If `space_left` is 0 (perfect fit), sigmoid(0) = 0.5\n    # If `space_left` is 1 (loose fit), sigmoid(-1) = ~0.27\n    # If `space_left` is 5 (very loose), sigmoid(-5) = ~0.0067\n    # If `space_left` is -1 (item too big), fit_input is -1e9, sigmoid(-1e9) = ~0.\n    \n    # This seems to prioritize bins with smaller positive remaining space.\n    # Let's call this `best_fit_score`.\n    \n    # What if we also want to slightly favor bins that have a lot of capacity,\n    # but only if they *also* provide a relatively good fit?\n    # This is where it gets tricky to combine with sigmoid elegantly.\n    \n    # For a pure \"Best Fit\" heuristic, `sigmoid(- (bins_remain_cap - item))` is good.\n    # We can adjust the steepness with a multiplier.\n    \n    # Let's go with a strong bias towards best fit, modulated by the possibility of filling a bin.\n    # The score should be higher if `bins_remain_cap - item` is small and positive.\n    \n    # Consider a function `f(x)` where `x = bins_remain_cap`.\n    # We want to give a higher score if `x` is moderately large, but also\n    # if `x - item` is small.\n    \n    # Let's simplify: Prioritize bins where `bins_remain_cap` is just enough to fit the item.\n    # The value `bins_remain_cap - item` should be small and positive.\n    # `np.exp(-(bins_remain_cap - item))` for items that fit.\n    \n    # Transform `bins_remain_cap - item` into a score:\n    # Items that fit: prioritize small, positive `bins_remain_cap - item`.\n    # Items that don't fit: zero priority.\n    \n    # Let's create a term that peaks when `bins_remain_cap - item` is small and positive.\n    # Gaussian-like function centered around 0?\n    # `np.exp(-(bins_remain_cap - item)**2 / sigma**2)`\n    # This would favor fits near 0, but also loose fits equally to tight fits if `bins_remain_cap` is the same.\n    \n    # Best fit strategy is essentially minimizing `bins_remain_cap - item` for `bins_remain_cap >= item`.\n    # We can use sigmoid for this.\n    # `sigmoid( -(bins_remain_cap - item) * sensitivity)`\n    # Sensitivity controls how sharply we drop off for looser fits.\n    \n    sensitivity = 2.0 # Higher sensitivity for tighter fits\n    \n    # Calculate the argument for the sigmoid function.\n    # For bins where the item fits (bins_remain_cap >= item), the argument is\n    # `-(bins_remain_cap - item) * sensitivity`.\n    # This means a tight fit (small positive `bins_remain_cap - item`) gives an argument close to 0,\n    # resulting in a sigmoid score close to 0.5.\n    # A loose fit (large positive `bins_remain_cap - item`) gives a large negative argument,\n    # resulting in a score close to 0.\n    # An item that doesn't fit (`bins_remain_cap < item`) means `bins_remain_cap - item` is negative.\n    # So, `-(bins_remain_cap - item)` is positive. This gives a score close to 1.\n    # This is the opposite of what we want: items that don't fit should have zero priority.\n    \n    # Let's correct the logic: we want higher priority for bins where the item FITS and leaves less space.\n    # Input to sigmoid should be *higher* for better bins.\n    \n    # Let `y = bins_remain_cap`. We want to maximize a function that is high when `y >= item`\n    # and `y - item` is small.\n    \n    # Consider `score = sigmoid(k * (bins_remain_cap - item))`\n    # if `bins_remain_cap >= item`:\n    #   If `bins_remain_cap - item = 0` (perfect fit), score = sigmoid(0) = 0.5\n    #   If `bins_remain_cap - item = 10` (loose fit), score = sigmoid(10k)\n    # if `bins_remain_cap < item`:\n    #   score = sigmoid(<negative value>) -> close to 0.\n    \n    # This means loose fits get higher scores than perfect fits if `k` is negative.\n    # If `k` is positive, perfect fits get higher scores.\n    \n    # Let's try `k = -1.0` (Best Fit - minimizes remaining capacity).\n    # `priorities = 1 / (1 + np.exp(-sensitivity * (bins_remain_cap - item)))`\n    # For `bins_remain_cap - item = 0`: `sigmoid(0)` = 0.5\n    # For `bins_remain_cap - item = 10`: `sigmoid(-10)` ~ 0.000045\n    # For `bins_remain_cap - item = -1`: `sigmoid(1)` ~ 0.73\n    # This still prioritizes items that don't fit.\n    \n    # The simplest way to handle \"does not fit\" is to zero out their score.\n    \n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fits = bins_remain_cap >= item\n    \n    # For bins where the item fits, we want to prioritize those with minimal `bins_remain_cap - item`.\n    # The function `sigmoid(C * (bins_remain_cap - item))` does the following:\n    # - If `bins_remain_cap - item` is negative (item too big), input is negative, sigmoid ~0.\n    # - If `bins_remain_cap - item` is small positive (tight fit), input is small positive, sigmoid ~0.5.\n    # - If `bins_remain_cap - item` is large positive (loose fit), input is large positive, sigmoid ~1.\n    \n    # This seems to be prioritizing loose fits if C > 0.\n    # If C < 0, it prioritizes tight fits.\n    \n    # Let's use C < 0 for best fit.\n    # C = -1.0\n    # The argument will be `- (bins_remain_cap - item)`.\n    # This is equivalent to `(item - bins_remain_cap)`.\n    # We want to prioritize small values of `item - bins_remain_cap` (i.e., `bins_remain_cap - item` close to 0).\n    \n    # Consider the score `sigmoid(k * (item - bins_remain_cap))`.\n    # `k` is sensitivity. Higher `k` means more pronounced difference.\n    # If `item - bins_remain_cap` is small positive (tight fit), `sigmoid` is ~0.5.\n    # If `item - bins_remain_cap` is large positive (loose fit), `sigmoid` is ~1.\n    # If `item - bins_remain_cap` is negative (item too big), `sigmoid` is ~0.\n    \n    # This is again prioritizing loose fits.\n    \n    # Okay, let's try a different approach to map `bins_remain_cap - item` to a priority score.\n    # We want to map [0, large_positive] to [high_priority, low_priority].\n    # A simple mapping is `1 / (1 + (bins_remain_cap - item) / scale)`.\n    # This is similar to sigmoid's shape.\n    \n    # Let's use sigmoid on `-(bins_remain_cap - item)` which is `item - bins_remain_cap`.\n    # `sigmoid(k * (item - bins_remain_cap))`\n    # If `item - bins_remain_cap = 0` (perfect fit): `sigmoid(0)` = 0.5\n    # If `item - bins_remain_cap = 10` (loose fit): `sigmoid(10k)`\n    # If `item - bins_remain_cap = -10` (item too big): `sigmoid(-10k)`\n    \n    # Let `k = 1.0`.\n    # If `item - bins_remain_cap` is small positive (tight fit), sigmoid(small_pos) ~ 0.5\n    # If `item - bins_remain_cap` is large positive (loose fit), sigmoid(large_pos) ~ 1.\n    # If `item - bins_remain_cap` is negative (item too big), sigmoid(negative) ~ 0.\n    \n    # This appears to prioritize loose fits over tight fits.\n    \n    # Let's redefine our objective:\n    # We are designing a priority function for the *selection* of a bin.\n    # Higher priority means it's *more likely* to be chosen.\n    \n    # We want to favor bins that are \"good\".\n    # A good bin is one that can fit the item and has minimal space left over.\n    # The value `bins_remain_cap - item` should be minimized, subject to `bins_remain_cap >= item`.\n    \n    # We can use `sigmoid` to map the \"badness\" (`bins_remain_cap - item`) to a score.\n    # If `bins_remain_cap - item` is 0, we want a high score.\n    # If `bins_remain_cap - item` is large positive, we want a low score.\n    # If `bins_remain_cap - item` is negative, we want a score of 0.\n    \n    # Consider `sigmoid(-k * (bins_remain_cap - item))` where `k > 0`.\n    # `k=1`:\n    # `bins_remain_cap - item = 0` (perfect fit): `sigmoid(0)` = 0.5\n    # `bins_remain_cap - item = 10` (loose fit): `sigmoid(-10)` ~ 0.000045\n    # `bins_remain_cap - item = -1` (too big): `sigmoid(1)` ~ 0.73\n    \n    # Still a problem with items that don't fit.\n    # Let's enforce the \"fits\" condition first by zeroing out scores.\n    \n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fits_mask = bins_remain_cap >= item\n    \n    # For bins that fit, calculate a \"fit quality\" score.\n    # Higher score for smaller `bins_remain_cap - item`.\n    \n    # Let's use `sigmoid(C * (item - bins_remain_cap))`\n    # This maps `item - bins_remain_cap` to a [0, 1] range.\n    # `item - bins_remain_cap` = `-(bins_remain_cap - item)`\n    \n    # We want `bins_remain_cap - item` to be small and positive.\n    # This means `item - bins_remain_cap` should be small and negative.\n    \n    # Let `x = bins_remain_cap - item`. We want to map `[0, large_pos]` to `[high_score, low_score]`.\n    # The function `1 / (1 + x / scale)` or `sigmoid(log(x / scale))` could work.\n    \n    # Let's use the direct property of sigmoid: `sigmoid(z)` increases from 0 to 1 as `z` increases.\n    # We want a higher score for smaller `bins_remain_cap - item`.\n    # This means we want the argument to sigmoid to be smaller as `bins_remain_cap - item` increases.\n    # So, the argument should be proportional to `-(bins_remain_cap - item)`.\n    \n    # Let `argument = -sensitivity * (bins_remain_cap - item)`.\n    # If `bins_remain_cap - item` is 0 (perfect fit), argument is 0, sigmoid(0) = 0.5.\n    # If `bins_remain_cap - item` is 10 (loose fit), argument is -10*sensitivity.\n    # If `sensitivity = 1`, sigmoid(-10) is very small.\n    # If `bins_remain_cap - item` is -1 (too big), argument is 1*sensitivity.\n    # If `sensitivity = 1`, sigmoid(1) is ~0.73.\n    \n    # So, with `sigmoid(-sensitivity * (bins_remain_cap - item))`:\n    # - For items that fit, scores decrease as the fit gets looser. Good.\n    # - For items that don't fit, scores are high. Bad.\n    \n    # To fix the \"don't fit\" problem, we can set the argument to a very small number\n    # if the item doesn't fit.\n    \n    sensitivity = 3.0 # Controls how quickly priority drops for looser fits.\n    \n    # Calculate the argument for the sigmoid.\n    # If item fits, argument is `-sensitivity * (bins_remain_cap - item)`\n    # If item doesn't fit, argument is a very small number (to ensure sigmoid is close to 0).\n    argument = np.where(\n        fits,\n        -sensitivity * (bins_remain_cap - item),\n        -1e9  # A very small number for sigmoid to produce a near-zero output.\n    )\n    \n    # Calculate the priority scores using the sigmoid function.\n    priorities = 1 / (1 + np.exp(-argument))\n    \n    # This heuristic prioritizes bins that provide the tightest fit for the item.\n    # It's a form of the \"Best Fit\" strategy.\n    # The `sensitivity` parameter controls how strongly we penalize loose fits.\n    \n    return priorities",
    "response_id": 26,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 25.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritize bins that offer a tight fit, with a secondary preference for bins that are not excessively empty after packing.\"\"\"\n    \n    # Calculate remaining capacity after placing the item.\n    remaining_after_fit = bins_remain_cap - item\n    \n    # We want to prioritize bins where 'remaining_after_fit' is small and non-negative.\n    # A perfect fit (remaining_after_fit = 0) should get a high score.\n    # Loose fits (large positive remaining_after_fit) should get lower scores.\n    # Bins where the item doesn't fit (negative remaining_after_fit) should get zero priority.\n    \n    # Use a sigmoid function: 1 / (1 + exp(-x)).\n    # To favor small positive 'remaining_after_fit', we want 'x' in sigmoid to be\n    # inversely related to 'remaining_after_fit'.\n    # So, use -k * (remaining_after_fit).\n    # Let 'k' be a sensitivity parameter.\n    \n    sensitivity = 2.0  # Controls how quickly priority drops for looser fits.\n    \n    # Argument for the sigmoid. If the item fits, we use -sensitivity * remaining_after_fit.\n    # If it doesn't fit, we use a very small number to ensure the sigmoid output is close to 0.\n    argument = np.where(\n        remaining_after_fit >= 0,\n        -sensitivity * remaining_after_fit,\n        -1e9  # Effectively zero priority for bins where the item doesn't fit.\n    )\n    \n    # Calculate the primary \"best fit\" priority score.\n    # This maps remaining_after_fit=0 to 0.5, and larger positive values to scores closer to 0.\n    best_fit_priority = 1 / (1 + np.exp(-argument))\n    \n    # Secondary preference: Slightly favor bins that are not excessively empty AFTER packing.\n    # This encourages filling bins more completely, without sacrificing the best fit.\n    # We can add a small bonus proportional to the inverse of the remaining capacity,\n    # but only if the bin already offers a decent fit (to avoid favoring very full bins\n    # that lead to poor fits).\n    \n    # Let's scale the remaining capacity after fit. Small positive remaining means a good fit.\n    # We want to give a slight boost to bins where this value is not *too* large,\n    # but also not so small that it's an immediate \"waste\".\n    \n    # A simple approach: add a term that slightly rewards bins with less slack.\n    # `slack_penalty = remaining_after_fit`\n    # We want to reward bins with *smaller* slack.\n    # Let's add `1.0 / (remaining_after_fit + 1e-6)` but this can be volatile.\n    \n    # A more stable approach: add a term proportional to the *inverse* of the\n    # remaining capacity, scaled. This boosts bins that leave less space.\n    # This term should be applied only to bins that already have a good fit.\n    \n    # Let's combine by scaling the \"remaining_after_fit\" term before the sigmoid.\n    # We want to slightly increase the priority for bins with less remaining capacity.\n    # `sigmoid(-sensitivity * (remaining_after_fit - capacity_bonus_factor * bins_remain_cap))`\n    # This is complex.\n    \n    # Let's use a simpler additive combination:\n    # `priority = best_fit_priority + alpha * secondary_score`\n    # `secondary_score` should be higher for smaller `remaining_after_fit`.\n    # `secondary_score = exp(-remaining_after_fit / scale_secondary)`\n    \n    # Let's refine the `argument` itself to implicitly include this.\n    # We want to increase the priority for smaller `remaining_after_fit`.\n    # `sigmoid(-sensitivity * remaining_after_fit + bonus_for_fullness)`\n    # `bonus_for_fullness` should be higher for bins that are already fuller (and can still fit).\n    # So, `bonus_for_fullness` could be proportional to `item / bins_remain_cap` if we consider\n    # how much of the bin the item occupies, or proportional to `1/bins_remain_cap`.\n    \n    # Let's combine by slightly reducing the effective remaining capacity for fuller bins.\n    # Consider a bin that is 80% full and an item takes 20% of it.\n    # Consider a bin that is 20% full and an item takes 10% of it.\n    \n    # Let's add a term that is higher for bins that are already relatively full.\n    # `fullness_score = item / (bins_remain_cap + 1e-6)` (ratio of item size to bin's current capacity)\n    # This score is high if the item takes up a large portion of the bin.\n    \n    # The argument for sigmoid could be: `-sensitivity * remaining_after_fit + weight_fullness * fullness_score`\n    \n    weight_fullness = 0.5 # Controls how much we favor fuller bins\n    \n    # Calculate fullness score. Normalize by bin capacity to avoid extreme values.\n    # A simpler fullness metric: `bins_remain_cap / MAX_CAPACITY`.\n    # Or, how much space is left *relative* to total capacity.\n    # `relative_remaining = bins_remain_cap / MAX_CAPACITY`\n    # We want to prioritize bins with *low* `relative_remaining` (i.e., fuller bins).\n    # So, `bonus = -weight_fullness * (bins_remain_cap / MAX_CAPACITY)`\n    \n    # Let's simplify: a slight preference for bins that are \"just right\" - not too empty, not too full.\n    # This is hard to capture with a single sigmoid input directly without peaks.\n    \n    # Let's stick to the Best Fit principle and add a small bonus for bins that are already\n    # somewhat filled, as long as they provide a good fit.\n    \n    # The current `best_fit_priority` is good.\n    # We can add a small, positive bonus for bins that are not excessively empty *after* fitting.\n    # The \"excessive emptiness\" is captured by `remaining_after_fit`.\n    # A smaller `remaining_after_fit` is good.\n    \n    # Let's add a term that is proportional to `1 / (remaining_after_fit + epsilon)`.\n    # This rewards smaller remaining capacities.\n    \n    bonus_for_tightness = 1.0 / (remaining_after_fit + 1e-6) # Higher for smaller positive remaining\n    \n    # Combine the best fit priority with a scaled bonus.\n    # Scale the bonus so it doesn't overwhelm the best fit score.\n    scale_bonus = 0.1 # Controls the influence of the secondary bonus.\n    \n    # Apply bonus only where the item fits.\n    combined_priority = np.where(\n        remaining_after_fit >= 0,\n        best_fit_priority + scale_bonus * (bonus_for_tightness / np.max(bonus_for_tightness[remaining_after_fit >= 0] + 1e-6)), # Normalize bonus\n        0.0 # Zero priority if item doesn't fit\n    )\n    \n    # Ensure priorities are within a reasonable range, though sigmoid already ensures [0,1] for best_fit_priority.\n    # The bonus might push it slightly above 1 if not careful.\n    # Let's reconsider the sigmoid argument to directly incorporate this.\n    \n    # Final attempt: Modify the sigmoid argument directly.\n    # We want `sigmoid(arg)` to be high when `remaining_after_fit` is small and positive.\n    # `arg = -sensitivity * remaining_after_fit` gives this.\n    # To add preference for smaller `remaining_after_fit`, we can add a term proportional to\n    # `1 / remaining_after_fit`.\n    # `arg = -sensitivity * remaining_after_fit - weight_fill * (1.0 / (remaining_after_fit + epsilon))`\n    # This will shift the peak preference to smaller positive values.\n    \n    sensitivity = 3.0\n    weight_fill = 0.5 # How much to favor bins that are already quite full (leaving less space)\n    epsilon = 1e-6\n    \n    # Calculate argument for sigmoid\n    # If item fits: use `-sensitivity * remaining_after_fit - weight_fill * (1.0 / (remaining_after_fit + epsilon))`\n    # This term `1.0 / (remaining_after_fit + epsilon)` is large for small positive `remaining_after_fit`,\n    # and small for large positive `remaining_after_fit`.\n    # By subtracting it, we are *reducing* the sigmoid argument for small `remaining_after_fit`.\n    # This means `sigmoid` will be *lower* for tight fits. This is the opposite of what we want.\n    \n    # Let's try adding a term that increases as remaining_after_fit decreases.\n    # `arg = -sensitivity * remaining_after_fit + weight_fill * (1.0 / (remaining_after_fit + epsilon))`\n    # This is still problematic as it peaks.\n    \n    # Revert to Best Fit using sigmoid, and then add a simple bonus.\n    \n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fits_mask = bins_remain_cap >= item\n    \n    # Best Fit score (higher for tighter fits)\n    # sigmoid(-sensitivity * (bins_remain_cap - item))\n    sensitivity = 3.0\n    best_fit_arg = -sensitivity * (bins_remain_cap - item)\n    \n    # Apply the sigmoid, but only to bins that fit.\n    best_fit_score = np.zeros_like(bins_remain_cap, dtype=float)\n    best_fit_score[fits_mask] = 1 / (1 + np.exp(-best_fit_arg[fits_mask]))\n    \n    # Secondary score: Bonus for being \"almost full\" after placing the item.\n    # Higher score for smaller (but positive) remaining_after_fit.\n    # Use 1 / (remaining_after_fit + epsilon) for this.\n    \n    remaining_after_fit = bins_remain_cap - item\n    \n    # Calculate bonus only for bins that fit\n    bonus_score = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate the bonus term for fitting bins. Normalize it to be between 0 and 1.\n    fitting_remaining = remaining_after_fit[fits_mask]\n    if fitting_remaining.size > 0:\n        normalized_bonus_term = 1.0 / (fitting_remaining + epsilon)\n        # Normalize this term to prevent it from overwhelming the best_fit_score\n        max_bonus_term = np.max(normalized_bonus_term)\n        if max_bonus_term > 0:\n            normalized_bonus = normalized_bonus_term / max_bonus_term\n            bonus_score[fits_mask] = normalized_bonus\n    \n    # Combine scores: weighted sum of best fit and the bonus.\n    # Give more weight to the best fit aspect.\n    weight_best_fit = 0.8\n    weight_bonus = 0.2\n    \n    final_priorities = weight_best_fit * best_fit_score + weight_bonus * bonus_score\n    \n    # Ensure no negative priorities and cap at 1.0 (though combination should be okay)\n    final_priorities = np.clip(final_priorities, 0.0, 1.0)\n    \n    return final_priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 40.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response1.txt_stdout.txt",
    "code_path": "problem_iter2_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Calculates bin priorities using a hybrid of Almost Full Fit and Best Fit.\n\n    This heuristic prioritizes bins that can fit the item, favoring those\n    that leave minimal remaining capacity (tightest fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities # No bins can fit the item\n\n    # Calculate remaining capacity for bins that can fit the item\n    remaining_caps_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Assign higher priority to bins with smaller remaining capacity after fitting\n    # This is the \"Almost Full Fit\" component, aiming for a tight pack.\n    # Add a small epsilon to prevent division by zero and ensure fitting bins have non-zero priority.\n    priorities[can_fit_mask] = 1.0 / (remaining_caps_after_fit + 1e-9)\n    \n    # Consider the \"Best Fit\" aspect by implicitly prioritizing smaller remaining capacities\n    # which naturally leads to higher values in the inverse calculation.\n    # For instance, if two bins can fit, and remaining capacities are 0.1 and 0.5,\n    # their priorities will be ~10 and ~2, respectively, with the former being higher.\n    \n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response2.txt_stdout.txt",
    "code_path": "problem_iter2_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a penalty for bins that are too full or too empty.\n\n    Prioritizes bins that fit the item and are close to being full after packing,\n    but penalizes bins that would become excessively full.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the remaining capacity after adding the item for fitting bins\n    remaining_caps_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # High priority for bins that leave minimal remaining space (tight fit)\n    # Add a small epsilon to avoid division by zero and to ensure fitting bins have priority\n    tight_fit_score = 1.0 / (remaining_caps_after_fit + 1e-6)\n    \n    # Penalize bins that become \"too full\" after packing.\n    # We define \"too full\" as having very little remaining capacity (e.g., < 0.1 of bin capacity).\n    # This penalty is a small negative value that reduces the priority.\n    # We'll use a fraction of the bin's original capacity for the penalty calculation.\n    # Assuming a standard bin capacity (e.g., 1.0 for normalization, or we can infer it if needed)\n    # Let's assume a default bin capacity of 1.0 for penalty calculation if not provided.\n    # A more robust approach might involve knowing the original bin capacity or average.\n    # For simplicity, we'll use a fixed threshold or relative threshold.\n    # Penalty increases as remaining_caps_after_fit gets smaller.\n    # Here, we use a soft penalty that becomes more significant as remaining_caps_after_fit approaches zero.\n    # Using a gaussian-like penalty centered at 0 remaining capacity could work,\n    # but a simpler inverse function might suffice.\n    # Let's adapt the tight fit score to also penalize very small remaining capacities.\n    # A simple approach is to cap the inverse to avoid extremely high priorities.\n    # Or, introduce a secondary term that penalizes extreme tightness.\n    \n    # Let's refine the priority:\n    # Primary goal: prioritize bins that are \"almost full\" after packing (tight fit).\n    # Secondary goal: avoid bins that become *too* full, which might be problematic.\n    # This can be achieved by giving a slightly lower priority to bins with near-zero remaining capacity.\n    \n    # Method 1: Cap the priority or use a saturating function.\n    # Method 2: Introduce a penalty term for very small remaining capacities.\n    # Let's try Method 2: Add a term that reduces priority for very small remaining_caps_after_fit.\n    # A simple way is to subtract a function that is large for small remaining_caps_after_fit.\n    # For example, subtract a scaled inverse of the remaining capacity itself, but only when it's small.\n    \n    # We want to favor remaining_caps_after_fit that are small but not necessarily zero.\n    # The `tight_fit_score` already favors smaller `remaining_caps_after_fit`.\n    # To penalize *excessively* full bins, we can subtract a penalty that grows as `remaining_caps_after_fit` approaches 0.\n    # Let's consider the original capacity of the bin to define \"too full\".\n    # If we don't know the original capacity, we can use a relative threshold.\n    # A heuristic for \"too full\" could be `remaining_caps_after_fit < item * 0.1` (leaves less than 10% of item size as remainder)\n    # or `remaining_caps_after_fit < some_small_constant`.\n    \n    # Let's combine the \"almost full fit\" idea with a slight penalty for extremely tight fits.\n    # We can use a function that peaks at a small positive remaining capacity, rather than at zero.\n    # A function like `x * exp(-x)` or `x / (x^2 + c)` could achieve this.\n    # However, to keep it simpler and build upon existing ideas:\n    # The \"Almost Full Fit\" score `1.0 / (remaining_caps_after_fit + 1e-6)` already prioritizes smaller remainders.\n    # To penalize the *absolute smallest* remainders, we can subtract a term that increases as `remaining_caps_after_fit` decreases.\n    # Let's try subtracting a small fraction of the original \"tightness\" if the remaining capacity is very small.\n    \n    # Let's reconsider the problem: we want to prioritize bins that are ALMOST FULL.\n    # This means remaining_caps_after_fit should be small.\n    # The `priority_v0` already does this well.\n    # The \"Analyze & experience\" suggests combining tight fit with something else.\n    # What if we penalize bins that have ALREADY very little capacity remaining, even before placing the item?\n    # This would be like a \"First Fit Decreasing\" or \"Worst Fit Decreasing\" aspect if we sort items.\n    # But this is online, so we can't sort items.\n    \n    # Let's try to refine the \"Almost Full Fit\" by adding a factor that prefers bins that aren't already nearly empty.\n    # If a bin is almost empty, placing an item there might not be optimal if other bins have more space.\n    # However, the goal of BPP is to minimize the NUMBER of bins. So filling bins efficiently is key.\n    \n    # The core idea of \"Almost Full Fit\" is to minimize wasted space.\n    # The `priority_v0` captures this. Let's try to combine it with a Best Fit aspect.\n    # Best Fit: minimize `bins_remain_cap - item`. This is equivalent to minimizing `remaining_caps_after_fit`.\n    # So, `priority_v0` is essentially a variant of Best Fit focused on the *after* state.\n    \n    # What if we want to prioritize bins that have a moderate amount of remaining capacity *before* packing,\n    # but then aim for a tight fit? This seems counter-intuitive for minimizing bins.\n    \n    # Let's go back to the \"Analyze & experience\" hint: combine \"tight fit\" with something.\n    # The sigmoid heuristics (19/20) tried to balance fullness.\n    # The prompt asks to COMBINE elements.\n    # `priority_v0` is a strong \"Almost Full Fit\".\n    # Let's combine it with a slight \"Best Fit\" preference.\n    # Best Fit usually means picking the bin where `bins_remain_cap - item` is minimized and non-negative.\n    # This is exactly what `remaining_caps_after_fit` represents.\n    # So, `priority_v0` IS already a Best Fit heuristic aimed at minimizing the *resulting* empty space.\n    \n    # Let's consider a heuristic that aims to fill bins that are already quite full, but not so full that the item doesn't fit.\n    # This is what `priority_v0` does.\n    \n    # Perhaps the combination should be:\n    # 1. Maximize the tightness of the fit (minimize `remaining_caps_after_fit`).\n    # 2. Add a factor that prefers bins that are generally larger (more capacity initially), IF they can achieve a tight fit.\n    # This might seem counter-intuitive, as we want to fill smaller bins first.\n    \n    # Let's try a simpler combination:\n    # Use the `priority_v0` score (inverse of remaining capacity after fit) as a base.\n    # Then, add a small bonus if the bin was already relatively full *before* placing the item.\n    # \"Relatively full\" can be defined as `bins_remain_cap / total_capacity`.\n    # Let's assume a total capacity of 1.0 for simplicity if not given.\n    \n    # Let's try a slight modification of priority_v0:\n    # priority_v0 = 1.0 / (remaining_caps_after_fit + 1e-6)\n    # This strongly favors bins where `remaining_caps_after_fit` is smallest.\n    \n    # What if we want to avoid bins that are *extremely* full (i.e., remaining capacity very close to 0)?\n    # We can apply a penalty.\n    # Penalty = `max(0, K - remaining_caps_after_fit)` where K is a threshold for \"too full\".\n    # Let's say K = 0.1 (10% of a unit capacity bin).\n    # Modified Priority = `priority_v0` - `penalty`\n    \n    # Let's try another angle: combine the \"Almost Full Fit\" score with the original remaining capacity in a multiplicative way.\n    # Prioritize bins that have a high \"almost full fit\" score AND were already somewhat full.\n    # Score = `(1.0 / (remaining_caps_after_fit + 1e-6)) * (bins_remain_cap[can_fit_mask])`\n    # This would favor bins where the resulting empty space is small, AND the original remaining space was also not too large.\n    \n    # Let's stick to a clear combination principle.\n    # Principle: Favor tight fits, but break ties (or provide a secondary preference) using another metric.\n    # Metric: \"Almost Full Fit\" (as in v0) is good.\n    # What other heuristic is relevant? Maybe avoiding bins that are already too full?\n    \n    # Let's combine the \"Almost Full Fit\" score with a slight bias towards bins that are not excessively empty.\n    # A bin that is almost empty might be better used for a larger item later (if we knew future items).\n    # In online, we don't. So filling up bins seems paramount.\n    \n    # Let's try to combine the core idea of `priority_v0` (tightest fit after placement) with a secondary factor.\n    # The secondary factor could be the original remaining capacity.\n    # We want smaller `remaining_caps_after_fit`.\n    # Let's say we also want smaller `bins_remain_cap` initially, IF they provide a tight fit.\n    # This would be like a \"Best Fit\" on the original capacities, but weighted by the tightness.\n    \n    # Let's reconsider `priority_v0`. It's already a strong heuristic for BPP.\n    # The goal is to combine elements.\n    # Maybe combine \"Almost Full Fit\" with a form of \"Worst Fit\"?\n    # Worst Fit would try to put the item in the bin with the MOST remaining capacity. This is generally bad for BPP.\n    \n    # Let's try to enhance `priority_v0` by adding a small penalty for bins that are *already* very full.\n    # This might prevent a situation where a bin becomes impossibly tight for future items.\n    # Penalty: `max(0, K - bins_remain_cap[can_fit_mask])` where K is a threshold for \"too full\" original capacity.\n    # If `bins_remain_cap` is already small, we might want to avoid making it even smaller.\n    # Example: If bin capacity is 1.0, and `bins_remain_cap` is 0.05. If item is 0.02.\n    # `remaining_caps_after_fit` = 0.03. `priority_v0` score = 1 / (0.03 + 1e-6) = ~33.3.\n    # If we penalize bins that are already very full (say, < 0.1 remaining), this bin would get a penalty.\n    # Penalty = `max(0, 0.1 - 0.05)` = 0.05.\n    # New Priority = 33.3 - 0.05 = 33.25.\n    # This is a very minor change. The dominant factor is still the tight fit.\n    \n    # Let's try a multiplicative approach:\n    # Priority = `(1.0 / (remaining_caps_after_fit + 1e-6)) * (bins_remain_cap[can_fit_mask] / MAX_CAPACITY)`\n    # This favors tighter fits AND bins that were initially larger. This is likely not good.\n    \n    # How about: Prioritize tight fits, but use the original remaining capacity as a tie-breaker,\n    # preferring bins that were initially smaller (to fill up smaller bins first).\n    # This sounds like a variant of Best Fit (minimize `remaining_caps_after_fit`) combined with First Fit (prefer earlier bins or bins with less initial capacity).\n    \n    # Let's implement a combination of \"Almost Full Fit\" (from v0) and a penalty for bins that are already too full.\n    # \"Too full\" can be relative to the item size. If a bin has very little space left compared to the item size, it's problematic.\n    # Let's define \"problematic\" as having remaining capacity less than a fraction of the item size.\n    # e.g., `bins_remain_cap[can_fit_mask] < item * 0.2` (remaining capacity is less than 20% of item size)\n    \n    # Let's try combining the inverse of remaining capacity after fit (from v0)\n    # with the negative of the original remaining capacity (from Best Fit).\n    # We want to minimize `remaining_caps_after_fit` and also minimize `bins_remain_cap`.\n    # But we only care about bins that fit.\n    \n    # Let's create a score that prioritizes bins that result in small `remaining_caps_after_fit`,\n    # but also adds a bonus if the bin originally had substantial capacity remaining. This is counter-intuitive.\n    \n    # The core idea of \"Almost Full Fit\" is sound. Let's augment it.\n    # What if we give a slight boost to bins that have *just enough* space?\n    # Consider a bin with remaining capacity R. We put item I. New remaining R-I.\n    # Priority is high if R-I is small.\n    # What if we also give a slight preference to bins where R is not *too* small initially?\n    # This sounds like we want R to be moderately large, but R-I to be small.\n    # This suggests R should be slightly larger than I.\n    \n    # Let's combine the \"Almost Full Fit\" with a \"Slightly Empty\" preference.\n    # Score = `(1.0 / (remaining_caps_after_fit + 1e-6)) * (bins_remain_cap[can_fit_mask])`\n    # This rewards bins that have a tight fit AND originally had more space. This might be okay.\n    # Let's try this:\n    \n    # Using the \"Almost Full Fit\" score from priority_v0 and multiplying it by the original remaining capacity.\n    # This favors bins that will be nearly full AFTER packing, AND were already somewhat full BEFORE packing.\n    # This might help ensure that items are placed into bins that are already progressing towards being filled.\n    \n    # Calculate the base priority (tight fit score)\n    base_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the remaining capacity after adding the item for fitting bins\n    remaining_caps_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # The \"Almost Full Fit\" score: higher for smaller remaining capacity after fit\n    almost_full_score = 1.0 / (remaining_caps_after_fit + 1e-6)\n    \n    # Combine with original remaining capacity:\n    # Multiply the \"almost full fit\" score by the original remaining capacity.\n    # This prioritizes bins that are both \"almost full\" after packing AND were already reasonably full before packing.\n    # This can be interpreted as: \"Fill bins that have room, but are already somewhat utilized, to achieve a tight fit.\"\n    # We normalize `bins_remain_cap` by a typical bin size (e.g., 1.0) to avoid large scale differences.\n    # If bin capacities vary widely, a dynamic normalization might be better, but for simplicity, assume a unit capacity or use the max observed.\n    # Let's assume a maximum capacity of 1.0 for normalization purposes if not specified.\n    # Or, simply use the raw `bins_remain_cap` values.\n    \n    # Let's use the raw values for `bins_remain_cap`.\n    # The product `almost_full_score * bins_remain_cap[can_fit_mask]`\n    # favors cases where `remaining_caps_after_fit` is small AND `bins_remain_cap` is large.\n    # This might lead to filling larger bins first if they can achieve a tight fit.\n    \n    # Alternative combination:\n    # Favor tight fits AND bins that were initially smaller.\n    # This would mean: `(1.0 / (remaining_caps_after_fit + 1e-6)) * (1.0 / (bins_remain_cap[can_fit_mask] + 1e-6))`\n    # This prioritizes bins with small remaining capacity after fit, AND small original remaining capacity.\n    # This seems more aligned with filling up available space efficiently.\n    \n    # Let's try this last one:\n    # Score = (Tightness Score) * (Initial Space Score)\n    # Tightness Score = 1.0 / (remaining_caps_after_fit + 1e-6) -> favors small remaining_caps_after_fit\n    # Initial Space Score = 1.0 / (bins_remain_cap[can_fit_mask] + 1e-6) -> favors small bins_remain_cap\n    \n    priorities[can_fit_mask] = almost_full_score * (1.0 / (bins_remain_cap[can_fit_mask] + 1e-6))\n    \n    # This heuristic prioritizes bins that result in a tight fit AND were initially less utilized (had more remaining capacity).\n    # This might be good for spreading items initially but could lead to more bins if the \"less utilized\" bins are large.\n    \n    # Let's revert to a simpler combination based on the \"tight fit\" being primary.\n    # The \"Almost Full Fit\" `1.0 / (remaining_caps_after_fit + 1e-6)` is excellent.\n    # What if we want to break ties using Best Fit principle on the remaining capacity?\n    # Best Fit minimizes `remaining_caps_after_fit`.\n    # The current score already strongly rewards minimum `remaining_caps_after_fit`.\n    \n    # Let's try combining \"Almost Full Fit\" with a slight penalty for bins that are already \"too full\".\n    # A bin is \"too full\" if its remaining capacity is very small relative to a typical bin size.\n    # Let's use a constant threshold for \"too full\", e.g., remaining capacity < 0.1.\n    # Penalty for bins with `bins_remain_cap[can_fit_mask] < 0.1`\n    \n    # Let's refine the `priority_v0` logic slightly.\n    # `priority_v0` prioritizes bins that leave the *least* amount of space.\n    # What if we want bins that leave *some* space, but not too much?\n    # This is where sigmoid functions came in.\n    \n    # Let's try combining the \"tight fit\" idea with a preference for bins that are not already nearly empty.\n    # If a bin has very little capacity remaining, placing an item there might be suboptimal if it uses up that small capacity entirely.\n    # Consider the bin's original remaining capacity `R`. We place item `I`. New remaining `R-I`.\n    # We want `R-I` to be small. This is `priority_v0`.\n    # What if we also want `R` to be not extremely small?\n    # Let's say `R` should be greater than some `min_R`.\n    # This is like a conditional Best Fit: find the best fit among bins where `R > min_R`.\n    \n    # Let's try combining the tight fit score with a bonus proportional to the *original* remaining capacity.\n    # This favors bins that are already somewhat utilized and can be filled tightly.\n    \n    # Final Attempt: Combine the \"Almost Full Fit\" score with a slight penalty for bins that are already extremely full.\n    # This adds robustness by discouraging placing items into bins that are already nearly unusable.\n    \n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    fitting_bins_caps = bins_remain_cap[can_fit_mask]\n    remaining_caps_after_fit = fitting_bins_caps - item\n    \n    # Base priority: \"Almost Full Fit\" - higher for smaller remaining capacity after packing\n    base_priority = 1.0 / (remaining_caps_after_fit + 1e-6)\n    \n    # Penalty for bins that are already \"too full\".\n    # Let's define \"too full\" as having remaining capacity less than 10% of a typical bin capacity (assuming 1.0).\n    # Or, relative to the item size: remaining capacity < item * 0.2.\n    # Let's use a fixed threshold for simplicity, assuming a normalized capacity context.\n    # If remaining capacity is less than `penalty_threshold`, apply a penalty.\n    penalty_threshold = 0.05 # Bins with less than 5% capacity remaining are penalized.\n    \n    # The penalty should be larger for smaller remaining capacities.\n    # Let's make the penalty proportional to how much smaller the remaining capacity is than the threshold.\n    # Penalty = max(0, penalty_threshold - fitting_bins_caps) * penalty_factor\n    # We want to penalize bins with small `fitting_bins_caps`.\n    # If `fitting_bins_caps` is small, say 0.02, and threshold is 0.05.\n    # The base priority `1.0 / (fitting_bins_caps - item + 1e-6)` will be high if `fitting_bins_caps - item` is small.\n    \n    # Let's reconsider: Combine \"Almost Full Fit\" with \"Best Fit\" where Best Fit means minimizing remaining capacity.\n    # `priority_v0` ALREADY does this by maximizing `1 / remaining_capacity`.\n    # The request is to COMBINE elements.\n    \n    # Let's combine the \"tightest fit\" with a secondary preference for bins that were initially less utilized.\n    # Score = (Tightness Score) * (Initial Underutilization Score)\n    # Tightness Score = 1.0 / (remaining_caps_after_fit + 1e-6)\n    # Initial Underutilization Score = 1.0 / (bins_remain_cap[can_fit_mask] + 1e-6) # Favor bins that had more initial capacity\n    \n    # This prioritizes bins that have small `remaining_caps_after_fit` AND large `bins_remain_cap`.\n    # This might fill up larger bins first if they allow a tight fit.\n    \n    # Let's try the other way:\n    # Score = (Tightness Score) * (Initial Utilization Score)\n    # Tightness Score = 1.0 / (remaining_caps_after_fit + 1e-6)\n    # Initial Utilization Score = bins_remain_cap[can_fit_mask] # Favor bins that were already more utilized\n    \n    # This combination favors bins that are ALREADY somewhat full AND can achieve a tight fit.\n    # This feels like a sensible combination: use bins that are already progressing and fill them tightly.\n    \n    # Let's apply this combined score.\n    priorities[can_fit_mask] = (1.0 / (remaining_caps_after_fit + 1e-6)) * fitting_bins_caps\n    \n    # This heuristic prioritizes bins that achieve a tight fit (small remaining capacity after placement)\n    # and also prioritizes bins that were already more utilized (larger remaining capacity before placement).\n    # This aims to fill up bins that are already making progress towards being full, by achieving a tight fit.\n    \n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response6.txt_stdout.txt",
    "code_path": "problem_iter2_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit and Almost Full Fit for balanced packing.\n\n    Prioritizes bins that are a tight fit (Best Fit) but also considers\n    bins that leave minimal space after packing (Almost Full Fit),\n    aiming for a good balance between immediate fit quality and future space.\n    \"\"\"\n    if item <= 1e-9:\n        # If item is negligible, any bin is fine. Prioritize less filled bins.\n        return 1.0 / (bins_remain_cap + 1e-9)\n\n    # Best Fit component: Prioritize bins with minimum remaining capacity after fitting.\n    # We want to maximize -(bins_remain_cap - item), which is item - bins_remain_cap.\n    # Using a small epsilon to avoid division by zero and prioritize exact fits.\n    best_fit_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    best_fit_priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item) + 1e-6\n\n    # Almost Full Fit component: Prioritize bins that leave minimal space after fitting.\n    # This means maximizing 1 / (remaining_capacity_after_item + epsilon)\n    almost_full_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_caps_after_fit = bins_remain_cap[can_fit_mask] - item\n    # Add a small value to avoid division by zero and ensure positive scores.\n    almost_full_priorities[can_fit_mask] = 1.0 / (remaining_caps_after_fit + 1e-6)\n\n    # Combine the heuristics. A simple additive combination can work.\n    # We normalize them to prevent one heuristic from dominating due to scale.\n    # Normalization is tricky without knowing the typical ranges.\n    # For simplicity, let's use a weighted sum. Weights can be tuned.\n    # Here, we give equal weight initially.\n\n    combined_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Apply the combined scores only to bins that can fit the item.\n    combined_priorities[can_fit_mask] = (best_fit_priorities[can_fit_mask] + almost_full_priorities[can_fit_mask]) / 2.0\n\n    # Ensure bins that cannot fit the item have zero priority.\n    # This is already handled by initializing with zeros and only updating fitting bins.\n\n    return combined_priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit and a Sigmoid-based preference for near-optimal residual capacity.\"\"\"\n    \n    # Calculate priorities based on Best Fit: prioritize bins that leave minimal remaining capacity.\n    # Use negative remaining capacity to ensure higher values for smaller remaining capacities.\n    best_fit_scores = -bins_remain_cap\n\n    # Identify bins that can fit the item.\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins that can fit, calculate a secondary score using a sigmoid function.\n    # This sigmoid favors bins where remaining capacity is close to the item size (but slightly larger).\n    # We want a peak around remaining_capacity = item * ideal_factor.\n    ideal_factor = 1.1  # Prefer bins leaving ~10% of item size as residual capacity\n    k_steepness = 6.0   # Controls the sharpness of the preference peak\n\n    # Calculate the ratio of remaining capacity to item size.\n    # Use a small epsilon to avoid division by zero if item is negligible.\n    ratios = np.where(item > 1e-9, bins_remain_cap / item, 1.0)\n\n    # Sigmoid 1: Increases as ratio increases (favors less full bins).\n    # Peaks when ratio > ideal_factor.\n    sigmoid1 = 1 / (1 + np.exp(-k_steepness * (ratios - ideal_factor)))\n\n    # Sigmoid 2: Decreases as ratio increases (favors more full bins).\n    # Peaks when ratio < ideal_factor.\n    sigmoid2 = 1 / (1 + np.exp(k_steepness * (ratios - ideal_factor)))\n\n    # Combine sigmoids to create a peak preference around ideal_factor.\n    # This score is high for bins that are neither too empty nor too full.\n    sigmoid_scores = sigmoid1 * sigmoid2\n\n    # Initialize priorities to zero.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Apply the combined scores only to bins that can fit the item.\n    # We want to combine the \"best fit\" tendency with the \"ideal residual capacity\" preference.\n    # A simple multiplicative approach can work:\n    # A bin is good if it's a good fit (high best_fit_scores) AND it has an ideal residual capacity (high sigmoid_scores).\n    # However, best_fit_scores are negative, so a simple multiplication might not be intuitive.\n    \n    # Let's prioritize based on the sigmoid score for bins that can fit,\n    # and use best-fit as a tie-breaker or as the primary driver.\n    # A common approach is to add a scaled sigmoid score to the best-fit score.\n    # We scale the sigmoid scores to influence the best-fit score without overwhelming it.\n    # The sigmoid scores are in [0, 0.25] (peak value of sigmoid1*sigmoid2 is at ratio=ideal_factor, which is 0.5*0.5=0.25).\n    # Scaling it by a factor, e.g., 10, would make it comparable to best-fit scores.\n\n    # Scale sigmoid scores to have a significant impact\n    scaled_sigmoid_scores = sigmoid_scores * 10.0 \n\n    # Combine the two scores. Bins that can fit get a combined score.\n    # Best fit is still the primary driver (negative values means smaller remaining space is better).\n    # The sigmoid score adds a bonus for being \"just right\".\n    priorities[can_fit_mask] = best_fit_scores[can_fit_mask] + scaled_sigmoid_scores[can_fit_mask]\n\n    # Ensure bins that cannot fit have zero priority.\n    priorities[~can_fit_mask] = -np.inf # Assign a very low priority to bins that cannot fit\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response1.txt_stdout.txt",
    "code_path": "problem_iter5_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a preference for moderately filled bins using sigmoid.\n\n    Prioritizes bins that offer a tight fit (Best Fit) but also favors bins\n    that aren't excessively empty after packing, aiming for a balanced state.\n    \"\"\"\n    if item < 1e-9:\n        # If item is negligible, any bin is fine. Prioritize less filled bins.\n        # Return higher scores for bins with less remaining capacity.\n        return 1.0 / (bins_remain_cap + 1e-9)\n\n    # --- Best Fit Component ---\n    # Prioritize bins that leave minimal remaining capacity after fitting the item.\n    # This is equivalent to maximizing -(bins_remain_cap - item), or item - bins_remain_cap.\n    # We use a large negative value for bins that can fit, favoring smaller differences.\n    # Add a small epsilon to avoid division by zero issues and to prioritize exact fits.\n    best_fit_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the remaining capacity for bins that can fit the item.\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Apply a sigmoid to map the \"tightness\" (small remaining capacity) to a high score.\n    # We want a high score when remaining_after_fit is small (close to 0).\n    # A sigmoid of the form 1 / (1 + exp(-k * x)) increases with x.\n    # So, we use x = -(remaining_after_fit) to make it decrease with remaining_after_fit.\n    # Or, we use a reversed sigmoid: 1 - (1 / (1 + exp(-k * x))) which is exp(-k*x)/(1+exp(-k*x))\n    # A simpler way for tight fit: use a large negative value for bins_remain_cap - item.\n    # The smaller (bins_remain_cap - item) is, the higher the score.\n    # Let's assign a score based on -(remaining_after_fit).\n    # To make it more sigmoid-like and smooth, let's use a sigmoid centered at 0 for this \"slack\".\n    # We want a peak at slack=0, so we can use a Gaussian-like function formed by two sigmoids.\n    # For this iteration, let's simplify the best-fit component:\n    # Prioritize bins that are closest to fitting the item.\n    # A simple \"best fit\" score can be the negative difference: item - bins_remain_cap\n    # which becomes positive for bins that can fit.\n    # Let's use a sigmoid on this difference to get a score.\n    # Target slack = 0. Use sigmoid(k * (-slack))\n    slack = remaining_after_fit\n    k_fit = 8.0 # Steepness for \"best fit\" preference\n    \n    # Score component that favors small, non-negative slack\n    best_fit_scores[can_fit_mask] = 1.0 / (1.0 + np.exp(k_fit * slack)) # High when slack is small\n\n    # --- Almost Full Fit Component ---\n    # Prioritize bins that are not excessively empty after fitting.\n    # This means we don't want too much capacity left.\n    # Let's consider the ratio of remaining capacity to item size.\n    # We prefer ratios closer to 1 (tight fit) but also moderate ratios, not excessively large ones.\n    # Let's use a sigmoid that penalizes bins that are much larger than needed.\n    # Define an \"ideal\" remaining capacity as slightly larger than the item, e.g., item * 1.3\n    # This component will penalize bins where (bins_remain_cap - item) is large.\n    \n    ideal_excess_capacity = item * 0.3 # Prefer bins that leave up to 30% of item size as excess\n    excess_capacity = bins_remain_cap[can_fit_mask] - item\n    \n    # Use a sigmoid to penalize large excess capacity.\n    # A sigmoid that decreases with excess capacity.\n    # 1 / (1 + exp(k * x)) decreases with x. Let x = excess_capacity - ideal_excess_capacity.\n    k_penalty = 4.0 # Steepness for penalizing excess capacity\n    \n    almost_full_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    almost_full_scores[can_fit_mask] = 1.0 / (1.0 + np.exp(k_penalty * (excess_capacity - ideal_excess_capacity))) # High when excess_capacity is small/ideal\n\n\n    # --- Combination ---\n    # Combine the two scores. The best fit score is high for tight fits.\n    # The almost full fit score is high for bins that aren't overly empty.\n    # A multiplicative combination can work well: prioritize bins that score high on *both*.\n    combined_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    combined_priorities[can_fit_mask] = best_fit_scores[can_fit_mask] * almost_full_scores[can_fit_mask]\n    \n    # Ensure bins that cannot fit have zero priority.\n    # This is handled by initializing with zeros and only updating can_fit_mask bins.\n\n    return combined_priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 17.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response0.txt_stdout.txt",
    "code_path": "problem_iter4_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 0.000845463644702381) -> np.ndarray:\n    available_bins_mask = bins_remain_cap >= item\n    available_bins_cap = bins_remain_cap[available_bins_mask]\n    \n    if available_bins_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.487435181491823,
    "SLOC": 5.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response7.txt_stdout.txt",
    "code_path": "problem_iter5_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines 'Best Fit' with a 'Not Too Empty' preference using a sigmoid.\n\n    Prioritizes bins that minimize remaining space (Best Fit), while also\n    favoring bins that aren't excessively empty using a sigmoid.\n    \"\"\"\n    # Ensure item size is positive for calculations.\n    if item <= 1e-9:\n        # If item is negligible, all bins are equally \"good\".\n        # Prioritize less filled bins to leave room for larger items later.\n        # A score inversely proportional to remaining capacity.\n        # Use sigmoid on negative capacity to get decreasing score for increasing capacity.\n        # Add a small epsilon to avoid division by zero or log(0).\n        return 1 / (1 + np.exp(5 * (bins_remain_cap + 1e-9)))\n\n    # Mask for bins that can fit the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities to zero.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # --- Component 1: Best Fit (Minimize Remaining Space) ---\n    # Calculate remaining space after placing the item.\n    remaining_space = bins_remain_cap - item\n\n    # Use a sigmoid to give higher priority to smaller remaining spaces.\n    # The sigmoid `1 / (1 + exp(-k * x))` increases with x.\n    # We want to increase priority as `remaining_space` decreases.\n    # So, use `x = -remaining_space`.\n    # `k_fit` controls the sensitivity to tightness. Higher k means tighter fit preference.\n    k_fit = 8.0\n    # Apply to bins that can fit.\n    best_fit_scores = 1 / (1 + np.exp(-k_fit * (-remaining_space[can_fit_mask])))\n    \n    # --- Component 2: Not Too Empty (Preference for bins that are somewhat full) ---\n    # This component aims to give a slight boost to bins that are not excessively empty,\n    # even if they are not the absolute tightest fit.\n    # Use the ratio of remaining capacity to item size.\n    ratios = bins_remain_cap[can_fit_mask] / item\n    \n    # We want to favor bins where `ratios` is around 1.0 to 1.5.\n    # Use a sigmoid centered around an \"ideal ratio\" to give a moderate boost.\n    ideal_ratio = 1.1  # Prefer bins with ~10% extra capacity.\n    k_slack = 4.0      # Controls the width of the boost.\n\n    # Sigmoid that peaks around ideal_ratio:\n    # sigmoid_part1 favors ratios > ideal_ratio\n    # sigmoid_part2 favors ratios < ideal_ratio\n    sigmoid_part1 = 1 / (1 + np.exp(-k_slack * (ratios - ideal_ratio)))\n    sigmoid_part2 = 1 / (1 + np.exp(k_slack * (ratios - ideal_ratio)))\n    not_too_empty_scores = sigmoid_part1 * sigmoid_part2\n\n    # --- Combine Scores ---\n    # Combine Best Fit scores with the \"Not Too Empty\" boost.\n    # A simple multiplication or addition can work. Multiplication emphasizes bins that are good on both criteria.\n    # Let's use multiplication, as a very poor fit (low best_fit_score) should dominate.\n    # Ensure scores are within a reasonable range (e.g., [0, 1]) if needed, though multiplication naturally handles this if inputs are in [0,1].\n    \n    # The `best_fit_scores` already capture the \"fit\" quality, including the preference for minimal remaining space.\n    # The `not_too_empty_scores` add a secondary preference for bins that aren't overly large.\n    # A simple additive approach might work better here to ensure that the \"best fit\" (low remaining space) isn't overly penalized if it's a bit more empty than another bin that's less of a \"best fit\".\n    \n    # Let's try a weighted sum. Best Fit is primary. \"Not Too Empty\" is secondary.\n    # Normalize the \"Not Too Empty\" scores to ensure they contribute as a secondary factor.\n    # The product `sigmoid_part1 * sigmoid_part2` naturally ranges from 0 to ~0.25 (at peak).\n    # Let's rescale `not_too_empty_scores` to be more comparable to `best_fit_scores`.\n    # The peak of `sigmoid_part1 * sigmoid_part2` is at `ideal_ratio`, where both are 0.5, so product is 0.25.\n    # Let's scale it up to have a similar magnitude to `best_fit_scores`.\n    rescaled_not_too_empty_scores = not_too_empty_scores * 2.0 # Scale up to roughly match magnitude of best_fit_scores\n\n    # Combine by adding the secondary preference to the primary \"Best Fit\" score.\n    # This ensures that primary criterion (tight fit) is maintained, and secondary criterion (not too empty) refines selection.\n    combined_scores = best_fit_scores + rescaled_not_too_empty_scores\n    \n    # Assign the combined scores back to the original array.\n    priorities[can_fit_mask] = combined_scores\n\n    # Normalize priorities to be in the range [0, 1] if needed, although this is often implicitly handled.\n    # The maximum possible value of `best_fit_scores` is 1. The max of `rescaled_not_too_empty_scores` is ~2.0.\n    # So combined scores can reach ~3.0. For typical heuristics, raw scores are fine.\n    # If normalization is desired:\n    # max_priority = np.max(priorities)\n    # if max_priority > 1e-9:\n    #     priorities /= max_priority\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.168328679696844,
    "SLOC": 18.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response0.txt_stdout.txt",
    "code_path": "problem_iter6_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    available_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n\n    if not np.any(available_bins_mask):\n        return priorities\n\n    available_bins_remain_cap = bins_remain_cap[available_bins_mask]\n\n    # Calculate the \"waste\" if the item is placed in each available bin\n    waste = available_bins_remain_cap - item\n\n    # Heuristic: Prioritize bins that result in less waste.\n    # To encourage exploration and avoid getting stuck in local optima,\n    # we can introduce a term that favors bins with slightly more remaining capacity\n    # if the waste is similar. This is a form of diversification.\n    # We use a logarithmic scale for waste to dampen the effect of very small wastes.\n    # A small epsilon is added to avoid log(0).\n    log_waste_penalty = np.log(waste + 1e-9)\n\n    # Introduce a diversification factor: slightly prefer bins with more remaining capacity\n    # among those with similar waste. This can be achieved by adding a small bonus\n    # based on the remaining capacity itself.\n    # The weight of this diversification term can be tuned.\n    diversification_bonus = 0.1 * available_bins_remain_cap\n\n    # Combine the waste consideration and diversification\n    # We want to minimize waste, so we use the negative of log_waste_penalty.\n    # Higher scores are better, so we add the diversification bonus.\n    scores = -log_waste_penalty + diversification_bonus\n\n    # Normalize scores to prevent extremely large or small values influencing selection too much\n    # and to ensure they are comparable across different item/bin states.\n    if scores.max() > scores.min():\n        normalized_scores = (scores - scores.min()) / (scores.max() - scores.min())\n    else:\n        normalized_scores = np.ones_like(scores) * 0.5 # If all scores are the same\n\n    # Assign the calculated priorities to the original bins array\n    priorities[available_bins_mask] = normalized_scores\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 86.56761069006784,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response1.txt_stdout.txt",
    "code_path": "problem_iter6_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    available_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap)\n\n    if not np.any(available_bins_mask):\n        return priorities\n\n    available_bins_cap = bins_remain_cap[available_bins_mask]\n\n    # Prioritize bins that leave the least remaining capacity after packing the item\n    # This is a variation of the \"Best Fit\" strategy\n    # We want to maximize the \"waste\" in the bin if it's not a perfect fit,\n    # or minimize waste if it's a perfect fit.\n    # A good proxy is to maximize (bin_capacity - item) / bin_capacity for non-perfect fits\n    # and give a very high score to perfect fits.\n\n    perfect_fit_mask = np.abs(available_bins_cap - item) < 1e-9\n    non_perfect_fit_mask = ~perfect_fit_mask\n\n    scores = np.zeros_like(available_bins_cap)\n\n    # High score for perfect fits\n    if np.any(perfect_fit_mask):\n        scores[perfect_fit_mask] = 1.0\n\n    # For non-perfect fits, prioritize bins that are \"almost full\" after packing\n    # This encourages filling bins as much as possible.\n    # We can use a metric like (bin_capacity - item) to represent the remaining space.\n    # We want to minimize this remaining space, so we can invert it or use its inverse.\n    # A common approach is to use (bin_capacity - item) as a penalty, and we want to minimize it.\n    # To turn this into a priority (higher is better), we can use 1 / (remaining_space + epsilon).\n    # Or, more robustly, we can use a function that gives higher scores to smaller remaining spaces.\n    # Let's try a score that is inversely proportional to the remaining capacity after packing,\n    # but with a preference for leaving *some* space rather than being completely full if not a perfect fit.\n    # A simple approach is to maximize the ratio of packed item to remaining capacity,\n    # effectively prioritizing bins where the item takes up a larger proportion of the remaining space.\n    if np.any(non_perfect_fit_mask):\n        non_perfect_caps = available_bins_cap[non_perfect_fit_mask]\n        remaining_after_packing = non_perfect_caps - item\n        # Use a score that emphasizes bins that are nearly full but not perfectly full.\n        # A higher score means less remaining capacity after packing.\n        # A simple heuristic: 1 / (remaining_capacity_after_packing + epsilon)\n        # To avoid issues with very small remaining capacities, we can add a small constant.\n        # Or, consider a score that penalizes leaving too much space:\n        # For example, (bin_capacity - item) / bin_capacity, which ranges from 0 to 1.\n        # Smaller remaining space (closer to 0) gives a score closer to 1.\n        # This means we prefer bins where the item fits snugly, but not perfectly.\n        scores[non_perfect_fit_mask] = (non_perfect_caps - item) / (non_perfect_caps + 1e-9)\n\n    priorities[available_bins_mask] = scores\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.198244914240141,
    "SLOC": 17.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response4.txt_stdout.txt",
    "code_path": "problem_iter6_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    available_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(available_bins_mask):\n        return np.zeros_like(bins_remain_cap)\n\n    available_bins_cap = bins_remain_cap[available_bins_mask]\n    \n    # Calculate a score based on how \"tight\" the fit is, but also consider the remaining capacity\n    # A bin that is almost full but can still fit the item might be good.\n    # Also, a bin with a lot of remaining capacity might be good if it's much larger than the item\n    # to potentially accommodate future larger items.\n    \n    # Option 1: Prioritize bins that leave the least remaining space after packing\n    tight_fit_score = 1.0 / (available_bins_cap - item + 1e-9)\n\n    # Option 2: Prioritize bins that have a good amount of remaining capacity for future items,\n    # but not excessively large to avoid fragmentation.\n    # We can use a sigmoid-like function or a scaled version of remaining capacity.\n    # Let's consider a penalty for very large remaining capacities.\n    # Normalize remaining capacity relative to bin capacity (assuming max bin capacity is known or estimated)\n    # For simplicity, let's assume a nominal bin capacity if not provided, or infer from max available.\n    # If we don't have a global bin capacity, we can normalize by the maximum possible capacity in the current set.\n    max_potential_capacity = np.max(bins_remain_cap) if bins_remain_cap.size > 0 else 1.0\n    normalized_remaining_cap = (available_bins_cap - item) / (max_potential_capacity + 1e-9)\n    \n    # A simple approach: score is high if remaining capacity is small, and also if it's reasonably large.\n    # We can combine the two ideas.\n    # Let's try a score that is high for tight fits and also for bins that are not too full but offer significant space.\n\n    # Metric 1: Tightness of fit (inverse of remaining space)\n    tightness = 1.0 / (available_bins_cap - item + 1e-9)\n\n    # Metric 2: Benefit of future packing (remaining capacity, but penalized if excessively large)\n    # Let's use a \"quality\" score for remaining capacity. A bin that has a lot of space but not too much is good.\n    # We can model this as a concave function of remaining space.\n    # For example, we can use sqrt or log of remaining space, scaled.\n    # Let's use a scaled remaining capacity, but penalize very large capacities.\n    # A simple way to penalize very large capacities is to divide by a power of the remaining capacity.\n    # Or, consider the ratio of remaining capacity to item size.\n    remaining_space_ratio = (available_bins_cap - item) / (item + 1e-9)\n    # A high ratio means a lot of extra space relative to the item. We might want to favour moderate ratios.\n    # Let's try a score that increases with remaining space up to a point, then decreases.\n    # A simple function: remaining_space / (remaining_space + C)\n    future_packing_benefit = (available_bins_cap - item) / (available_bins_cap - item + 0.5 * item + 1e-9) # Hybrid approach\n\n    # Combine metrics: Prioritize tightness, but also consider future packing benefit.\n    # A weighted sum might work, or a product if we want to ensure both are somewhat met.\n    # Let's try a combination that favors tight fits but gives a boost to bins with substantial remaining space.\n    \n    # A simple combination: (tightness score) * (1 + future_packing_benefit_boost)\n    # The boost should be smaller for very large remaining capacities.\n    \n    # Let's define a priority as a balance between minimizing wasted space AND maximizing future capacity.\n    # Consider the ratio of item size to remaining capacity.\n    # High score if item size is close to remaining capacity (tight fit) AND if remaining capacity is not excessively large.\n    \n    # Let's re-evaluate based on the hint: \"shift focus from a priori simplicity to a posteriori performance\" and \"avoid over-reliance on 'tight fit' or 'almost full' metaphors.\"\n    # This suggests we should consider a more holistic view. What if we score bins based on how well they absorb the *current* item and *potentially* future items?\n    # We want to find a bin that is \"good enough\" for the current item and leaves a \"good enough\" state for future items.\n    \n    # Let's try a score that favors bins that are not too full, not too empty, and provide a good balance.\n    # A score that is maximized when remaining_cap is `item` and also when `remaining_cap` is some moderate value.\n    # This can be achieved by looking at the relative remaining capacity.\n\n    # Consider the inverse of the *wasted space* if the item is placed.\n    # Wasted space = remaining_cap - item\n    # Score for wasted space: 1 / (wasted_space + epsilon)\n    \n    # Consider the *opportunity cost* of placing the item in this bin.\n    # If we place the item, the remaining capacity becomes `remaining_cap - item`.\n    # A good bin is one that, after placing the item, still has a useful amount of capacity for future items.\n    \n    # Let's try a heuristic that tries to keep bins \"balanced\".\n    # Score based on how close the remaining capacity is to the item size,\n    # and also how close it is to some \"ideal\" intermediate capacity.\n\n    # For each available bin, calculate a score.\n    # The score should reflect the \"goodness\" of placing the item in this bin.\n\n    # Consider the remaining capacity *after* placing the item: `next_remaining_cap = available_bins_cap - item`\n    # A good `next_remaining_cap` is one that is small (tight fit) OR is a \"comfortable\" size for future items.\n    # Let's try to penalize bins that become very empty after placement.\n    \n    # Score function idea: prioritize bins where `available_bins_cap` is close to `item`.\n    # AND also, among those that are close, favor those that don't leave *too little* capacity.\n    \n    # Let's try to directly model the \"goal alignment\" with performance.\n    # A bin is good if placing the item in it leads to a state that is \"better\" for the overall packing.\n    # This is hard to do directly in an online setting without lookahead.\n\n    # Let's try a metric that favors bins that are \"almost full\" but also bins that are \"moderately full\".\n    # We can use a Gaussian-like shape centered around some target remaining capacity.\n    # However, the \"target\" capacity depends on the items seen so far and their distribution.\n\n    # Let's try a simpler approach: a score that is high when the bin is \"just right\".\n    # \"Just right\" could mean:\n    # 1. The item fits well (small gap).\n    # 2. After fitting, the remaining capacity is still useful, not too small, not too large.\n\n    # Let's define a score based on the *relative* remaining capacity.\n    # Relative remaining capacity = (bin_remain_cap - item) / item\n    # If this ratio is small (close to 0), it's a tight fit.\n    # If this ratio is moderate, it's a good space for future items.\n    \n    # Consider the score as a function of `gap = available_bins_cap - item`\n    # We want to minimize `gap`. But we also don't want `gap` to be extremely small if it means the bin is very nearly full.\n    \n    # Let's try a score that is higher for bins where `available_bins_cap` is closer to `item`,\n    # but with a penalty if the `available_bins_cap` is much larger than `item`.\n    \n    # New idea: Score based on the \"potential future utility\" of the bin.\n    # If we place `item`, the remaining capacity is `available_bins_cap - item`.\n    # A high score for this remaining capacity is desired.\n    # What is a good remaining capacity? Not too small, not too large.\n    # Let's consider the ratio `(available_bins_cap - item) / some_reference_capacity`.\n    # If `some_reference_capacity` is the bin's total capacity, we don't have it.\n    # If we use `item` as reference: `(available_bins_cap - item) / item`.\n    # This ratio is good if it's small (tight fit) or if it's moderate.\n    \n    # Let's try a composite score.\n    # Score 1: Inverse of residual capacity (favors tight fit)\n    tight_fit_score = 1.0 / (available_bins_cap - item + 1e-9)\n    \n    # Score 2: Favor bins that leave a \"comfortable\" residual capacity.\n    # Let's consider a target residual capacity, perhaps related to the average item size seen so far.\n    # Without history, let's consider a fixed \"good\" residual capacity, say 0.5 * bin_capacity.\n    # Since we don't know bin capacity, let's use `item` as a proxy or normalize by max capacity.\n    \n    # Let's simplify: Focus on the state *after* packing.\n    # We want the remaining capacity `R = available_bins_cap - item` to be \"good\".\n    # A good `R` is one that is not too small (so it's not wasted) and not too large (so it's not inefficiently used).\n    # This suggests a peak at some intermediate `R`.\n    \n    # Consider the ratio `available_bins_cap / item`.\n    # If `available_bins_cap / item` is close to 1, it's a tight fit.\n    # If `available_bins_cap / item` is large, it means the bin is much larger than the item.\n\n    # Let's try to create a score that peaks at a certain relative fullness.\n    # Example: A bin is \"best\" when its remaining capacity is `item`, but it's also good if the remaining capacity is `2*item`.\n    # We can use a function like `f(x) = x / (x + C)` for some constant C, where `x` is the remaining capacity *after* packing.\n    # `f(R) = R / (R + C)`\n    # This function increases with R, but plateaus. It doesn't have a peak.\n    \n    # How about a score that rewards being close to `item` but penalizes being too far from `item` in either direction?\n    # This is getting complex. Let's go back to a simpler, but improved version.\n\n    # Priority v1: `available_bins_cap / (available_bins_cap - item + 1e-9)`\n    # This is equivalent to `(available_bins_cap - item + item) / (available_bins_cap - item + 1e-9)`\n    # Which is `1 + item / (available_bins_cap - item + 1e-9)`\n    # This clearly favors smaller `available_bins_cap - item`. So it's a tight-fit heuristic.\n\n    # Let's try to incorporate a secondary objective: leaving substantial capacity.\n    # We want `available_bins_cap - item` to be small, but not too small.\n    # Consider the metric: `item / available_bins_cap`.\n    # This is high if the item is a large fraction of the bin's current capacity.\n    \n    # Let's try a score that is a harmonic mean of how well the item fits and how much capacity is left.\n    # Metric 1: How well the item fits (inverse of gap)\n    gap = available_bins_cap - item\n    fit_score = 1.0 / (gap + 1e-9)\n\n    # Metric 2: How much capacity is left relative to the item size (encourages leaving space)\n    # We want this to be moderate. Let's use `residual_capacity / item`.\n    # A high value here is good, but maybe too high is bad.\n    # Let's try `item / (residual_capacity + item)` - this is high when residual capacity is small.\n    # Let's try `residual_capacity / (residual_capacity + item)` - this is high when residual capacity is large.\n    # Let's call this `future_utility_score`.\n    \n    # A balanced approach might be to favor bins where the item takes up a significant portion,\n    # but not so much that no useful space is left.\n    # Let's try prioritizing bins where `available_bins_cap` is close to `item` AND also close to some \"optimal\" remaining capacity.\n    \n    # Let's define an \"optimal\" remaining capacity as `item`.\n    # We want `available_bins_cap - item` to be close to 0.\n    # But we also want `available_bins_cap - item` to be not too small, e.g., at least `0.1 * item`.\n    \n    # Let's try a score that is a combination of two factors:\n    # 1. Tightness: `1 / (available_bins_cap - item + epsilon)`\n    # 2. Residual capacity utility: `(available_bins_cap - item) / (available_bins_cap - item + C)` for some C.\n    # Let C be related to the item size, e.g., `C = 0.5 * item`.\n    \n    residual_capacity = available_bins_cap - item\n    \n    # Factor 1: Favor tight fits (smaller residual capacity)\n    tightness_factor = 1.0 / (residual_capacity + 1e-9)\n\n    # Factor 2: Favor useful residual capacity (not too small, not too large).\n    # Let's use a logistic-like function that rewards moderate residual capacity.\n    # A common approach in evolutionary algorithms is to use fitness functions that are shaped.\n    # For example, if we want to maximize `residual_capacity`, a simple `residual_capacity` works.\n    # If we want to minimize it, `1/residual_capacity` works.\n    # If we want it to be around a certain value, we can use a Gaussian.\n    # Let's try to make it peak when `residual_capacity` is proportional to `item`.\n    # For example, peak when `residual_capacity = 0.5 * item`.\n    # A function like `exp(- (residual_capacity - target_residual)^2 / sigma^2)`\n    # `target_residual = 0.5 * item`\n    # `sigma = 0.5 * item` (makes the peak relatively broad)\n    \n    target_residual = 0.5 * item\n    # Using a sigmoid-like function to reward a balance:\n    # Reward bins that have a residual capacity that's not too small and not too large.\n    # Let's consider the ratio `residual_capacity / item`.\n    # We want this ratio to be not close to 0 and not excessively large.\n    # A function that is high for ratios around 0.5 to 1.5 might be good.\n    \n    # Let's re-think \"contextual\" and \"performance-driven\".\n    # Contextual means it should adapt to the current state.\n    # Performance-driven means it should try to optimize the outcome.\n    \n    # Let's consider a metric that looks at the efficiency of the bin.\n    # If we place the item, the \"fullness\" of the bin becomes `item / available_bins_cap`.\n    # We want this to be high (close to 1) for a tight fit.\n    # However, this doesn't consider future items.\n    \n    # Let's try a heuristic that tries to \"fill\" bins to a certain level, but not overfill them.\n    # This implies we want `available_bins_cap` to be close to `item`.\n    \n    # Let's try a score that combines the inverse gap with the inverse of the capacity if it's too large.\n    # `score = (1 / (gap + eps)) * f(available_bins_cap)` where f penalizes large capacities.\n    # Or, a score that is high when `available_bins_cap` is `item` and also when `available_bins_cap` is something like `2 * item`.\n    \n    # Consider the \"density\" of the fit.\n    # For a bin with remaining capacity `C`, and item size `I`:\n    # Score could be `I / C` (tightness)\n    # Or `I / (C - I + eps)` (inverse residual space)\n    \n    # Let's consider the \"quality\" of the remaining space.\n    # After packing, remaining space is `R = C - I`.\n    # We want `R` to be \"useful\".\n    # A heuristic: `score = (1 + R) / (1 + R + I)` ? This is `(1+R) / (1+C)`.\n    # This function increases with R, so it favors larger residual capacities.\n    \n    # Let's try to penalize bins that become \"too empty\".\n    # If `available_bins_cap - item` is very small, it's good for tightness.\n    # If `available_bins_cap - item` is very large, it might be inefficient.\n    \n    # Let's try a score that is proportional to `item / available_bins_cap` (how much of the current capacity is used)\n    # and inversely proportional to `(available_bins_cap - item)` (residual space).\n    \n    # How about we reward bins that have a remaining capacity that is a multiple of the item size?\n    # This is too specific.\n    \n    # Let's consider a score that is a geometric mean of tightness and residual utility.\n    # Tightness score: `item / available_bins_cap`\n    # Residual utility score: `(available_bins_cap - item) / available_bins_cap`\n    # Geometric mean: `sqrt( (item / available_bins_cap) * ((available_bins_cap - item) / available_bins_cap) )`\n    # This is `sqrt( item * (available_bins_cap - item) ) / available_bins_cap`\n    # This metric is maximized when `item = available_bins_cap / 2`.\n    # So it favors bins that are exactly half-full. This is a strong preference.\n\n    # Let's try a score that combines the tight fit aspect with a more forgiving residual capacity.\n    # Consider `residual_capacity = available_bins_cap - item`.\n    # Score = `(1 + item) / (residual_capacity + item + 1e-9)`\n    # This is `(1 + item) / available_bins_cap`\n    # This metric prioritizes bins where the item is a large fraction of the current capacity.\n    # It's similar to \"First Fit Decreasing\" idea of packing larger items first.\n    \n    # Let's try to combine tightness with a penalty for *very* little remaining space.\n    # Score = `1 / (residual_capacity + epsilon)` BUT penalize if `residual_capacity < some_threshold`.\n    \n    # How about a score that favors bins that leave a `residual_capacity` that is \"just enough\" to pack another item of average size?\n    # Without average size, let's consider `residual_capacity` relative to `item`.\n    \n    # Let's try to define the \"best\" bin as one that, after packing, leaves a remaining capacity `R` such that `R` is \"close\" to `item`.\n    # This means `available_bins_cap - item` is close to `item`, so `available_bins_cap` is close to `2 * item`.\n    # This heuristic is called \"Best Fit\".\n    \n    # Let's try to create a \"contextual\" score.\n    # What if we reward bins that have a remaining capacity `C` such that `C` is slightly larger than `item`?\n    # The \"gap\" is `C - item`. We want this to be small.\n    # What if we add a bonus for bins that are not too empty *after* packing?\n    \n    # Let's consider the ratio `available_bins_cap / item`.\n    # If this ratio is 1, it's a perfect fit (but leaves 0 space).\n    # If this ratio is 1.1, it leaves 0.1 * item space.\n    # If this ratio is 2, it leaves `item` space.\n    \n    # Let's try to reward bins that are not too close to `item` and not too far from `item`.\n    # A score that peaks when `available_bins_cap` is around `1.5 * item` or `2 * item`.\n    \n    # Let's try a score that is a combination of fitting the current item well and leaving enough space.\n    # Score for bin `i` with remaining capacity `C_i`:\n    # `priority_i = f(C_i, item)`\n    \n    # Consider `priority_i = C_i / item`. This favors larger bins.\n    # Consider `priority_i = item / C_i`. This favors bins that are almost full.\n    \n    # Let's try to balance these.\n    # `priority_i = (item / C_i) * (1 - item / C_i)` ? This peaks when `item/C_i = 0.5`, so `C_i = 2 * item`.\n    # This is a \"Best Fit\" type heuristic but focused on the bin's *current* capacity relative to item.\n\n    # Let's refine this. We want to maximize `item / available_bins_cap` (tightness)\n    # AND we want to maximize `(available_bins_cap - item) / available_bins_cap` (residual space)\n    \n    # Let's try a weighted sum of these two factors.\n    # `w1 * (item / available_bins_cap) + w2 * ((available_bins_cap - item) / available_bins_cap)`\n    # `w1 * (item / C) + w2 * (C - item) / C`\n    # `(w1 * item + w2 * C - w2 * item) / C`\n    # `((w1 - w2) * item + w2 * C) / C`\n    \n    # If `w1 > w2`, this favors bins with higher `item / C`. So it favors tighter fits.\n    # If `w2 > w1`, this favors bins with higher `C`.\n    # If `w1 = w2`, this becomes `item / C`.\n    \n    # Let's try a different approach:\n    # A score that is high if `available_bins_cap` is \"just enough\" for the item,\n    # and also high if `available_bins_cap` is \"plenty\" for the item.\n    \n    # Let's define a score as the inverse of the \"wasted potential\".\n    # Wasted potential could be the gap `(available_bins_cap - item)`.\n    # But we also don't want bins to be too empty.\n    \n    # Let's try a heuristic that aims to leave a residual capacity that is \"useful\".\n    # A useful residual capacity could be one that can fit at least half of the current item, or the smallest item seen so far.\n    # Without history, let's try to ensure the residual capacity is at least `0.1 * item` and not excessively large.\n\n    # Let's try the \"Most Fitting\" heuristic, where we pick the bin with the smallest `available_bins_cap - item`.\n    # This is what v1 is trying to do, but with a specific form.\n    \n    # Let's try to combine \"Best Fit\" (minimize gap) with a penalty for leaving too little space.\n    # Score = `1 / (gap + epsilon)` (favors small gap)\n    # Penalty for small gap: `1 / (max(0, residual_capacity - threshold) + epsilon)`\n    # Let `threshold = 0.2 * item`.\n    \n    residual_capacity = available_bins_cap - item\n    \n    # Score is high if `residual_capacity` is small (tight fit).\n    # Score is also somewhat high if `residual_capacity` is moderate.\n    # Score is low if `residual_capacity` is extremely small or extremely large.\n    \n    # Let's use a function that rewards `residual_capacity` being in a certain range.\n    # A Gaussian-like shape centered on a \"good\" residual capacity.\n    # What is a good residual capacity? Maybe `item` or `0.5 * item`.\n    # Let's try to make it peak when `residual_capacity` is proportional to `item`.\n    \n    # Score = `item / (residual_capacity + item + 1e-9)`\n    # This is `item / available_bins_cap`. This favors bins where the item is a large fraction of the current capacity.\n    # This is like \"tightest fit\" relative to current capacity.\n    \n    # Let's try to be \"contextual\".\n    # Consider the ratio `available_bins_cap / item`.\n    # If this ratio is very close to 1, it's a tight fit.\n    # If this ratio is 2, it means after packing, the remaining capacity is `item`. This is a good state.\n    # If this ratio is very large, it means a lot of wasted space.\n    \n    # Let's define a score that tries to match `available_bins_cap` to `2 * item`.\n    # Score = `1 / (abs(available_bins_cap - 2 * item) + epsilon)`\n    # This favors bins where the *current* capacity is twice the item size.\n    # This is a bit of a \"lookahead\" idea, assuming an item of size `item` will be packed next.\n    \n    # Let's try to achieve a balance between tight fit and leaving some residual space.\n    # Score = `(item / available_bins_cap) * ( (available_bins_cap - item) / available_bins_cap )` (geometric mean idea)\n    # This peaks when `available_bins_cap = 2 * item`.\n    # Let's scale it to avoid issues with very small capacities.\n    \n    # Let's try: `item / (available_bins_cap - item + item/2 + 1e-9)`\n    # This is `item / (available_bins_cap - item/2 + 1e-9)`\n    # If `available_bins_cap` is close to `item`, this score is high.\n    # If `available_bins_cap` is close to `2*item`, this score is `item / (1.5*item) = 2/3`.\n    # If `available_bins_cap` is very large, this score approaches 0.\n    \n    # Let's try a score that directly targets a \"good\" remaining capacity.\n    # Suppose we want to leave `item` capacity remaining. Then `available_bins_cap` should be `2 * item`.\n    # Suppose we want to leave `0.5 * item` capacity remaining. Then `available_bins_cap` should be `1.5 * item`.\n    \n    # Let's try a function that rewards being close to `2 * item`.\n    # `priority = 1.0 / (abs(available_bins_cap - 2 * item) + 1e-9)`\n    # This is like \"Best Fit for a bin that's twice the item size\".\n    \n    # Let's try to combine tight fit with this \"ideal residual\" idea.\n    # Tight fit score: `1.0 / (residual_capacity + 1e-9)`\n    # Ideal residual score: `1.0 / (abs(residual_capacity - item) + 1e-9)`\n    \n    # Weighted sum: `w1 * (1.0 / (residual_capacity + 1e-9)) + w2 * (1.0 / (abs(residual_capacity - item) + 1e-9))`\n    # Let `w1 = 0.5`, `w2 = 0.5`. This is an average of \"tightest fit\" and \"best fit for residual = item\".\n    \n    # Let's test this idea: `priority = 0.5 * (1.0 / (residual_capacity + 1e-9)) + 0.5 * (1.0 / (abs(residual_capacity - item) + 1e-9))`\n    \n    # Another approach: Focus on the efficiency of the bin.\n    # Efficiency can be measured by how much of the *current* capacity is used by the item.\n    # `efficiency = item / available_bins_cap`\n    # We want this to be high (close to 1).\n    # However, this doesn't account for leaving space.\n    \n    # Let's try a score that is high when `available_bins_cap` is slightly larger than `item`.\n    # Let's use a function that is high for `available_bins_cap - item` in a range, say `[0.1*item, 0.5*item]`.\n    \n    # Final approach idea: Reward bins that provide a good trade-off between filling current item and leaving space.\n    # Consider the ratio `available_bins_cap / item`.\n    # If this ratio is close to 1, it's a tight fit.\n    # If this ratio is close to 2, it means the remaining space is equal to the item size.\n    # Let's try a function that is high for ratios near 1 and near 2.\n    \n    # Consider the function `f(x) = x / (x + C)`. This increases and plateaus.\n    # Consider `f(x) = 1 / (x + C)`. This decreases.\n    \n    # Let's try a score that is high for bins that are \"almost full\" but not so empty afterwards.\n    # Consider `score = available_bins_cap / (item + available_bins_cap)`.\n    # This is `1 / (1 + item / available_bins_cap)`.\n    # This score is high when `item / available_bins_cap` is high (tight fit).\n    # It's also somewhat high when `item / available_bins_cap` is moderate.\n    # E.g., if `available_bins_cap = 2 * item`, score = `2*item / (item + 2*item) = 2/3`.\n    # If `available_bins_cap = 1.1 * item`, score = `1.1*item / (item + 1.1*item) = 1.1/2.1 ~ 0.52`.\n    # If `available_bins_cap = 1.5 * item`, score = `1.5*item / (item + 1.5*item) = 1.5/2.5 = 0.6`.\n    # This score peaks when `available_bins_cap` is large relative to `item`. It favors leaving more space.\n    \n    # Let's try to invert the focus. What if we prioritize bins that leave a \"good\" residual capacity?\n    # Let `R = available_bins_cap - item`.\n    # We want `R` to be not too small, not too large.\n    # Let's try a score that is high for `R` when `R` is proportional to `item`.\n    # Example: `R = 0.5 * item`.\n    # So, `available_bins_cap - item = 0.5 * item` => `available_bins_cap = 1.5 * item`.\n    \n    # Let's try a score that rewards bins where `available_bins_cap` is approximately `1.5 * item`.\n    # `score = 1.0 / (abs(available_bins_cap - 1.5 * item) + 1e-9)`\n    \n    # What if we try to balance the \"tight fit\" with this \"ideal residual\" idea?\n    # Tightness: `1.0 / (available_bins_cap - item + 1e-9)`\n    # Ideal Residual Match: `1.0 / (abs(available_bins_cap - 1.5 * item) + 1e-9)`\n    \n    # Let's average them:\n    # `priority = 0.5 * (1.0 / (available_bins_cap - item + 1e-9)) + 0.5 * (1.0 / (abs(available_bins_cap - 1.5 * item) + 1e-9))`\n    # This heuristic attempts to balance filling the current item snugly with leaving a good amount of space for future items.\n\n    residual_capacity = available_bins_cap - item\n    \n    # Heuristic 1: Tight fit (smaller residual capacity is better)\n    tightness_score = 1.0 / (residual_capacity + 1e-9)\n    \n    # Heuristic 2: \"Ideal\" residual capacity. We hypothesize that leaving about 50% of the item's size as residual capacity is good.\n    # This means `residual_capacity = 0.5 * item`.\n    # So, `available_bins_cap - item = 0.5 * item`, which means `available_bins_cap = 1.5 * item`.\n    # We want to be close to this value.\n    ideal_residual = 0.5 * item\n    ideal_residual_score = 1.0 / (np.abs(residual_capacity - ideal_residual) + 1e-9)\n    \n    # Combine the two scores. A simple average for a balanced approach.\n    # This aims to prioritize bins that are both tight fits and leave a reasonable amount of space.\n    priorities = 0.5 * tightness_score + 0.5 * ideal_residual_score\n    \n    # Ensure the priorities are applied to the original bins_remain_cap array structure.\n    final_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    final_priorities[available_bins_mask] = priorities\n    \n    return final_priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.597128041483859,
    "SLOC": 26.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  }
]