{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Heuristic: Prioritize bins where the item fits snugly, but also consider\n    # bins with ample remaining capacity for future large items.\n    # The sigmoid function will compress scores between 0 and 1.\n    \n    # Calculate how \"tight\" the fit would be for each bin\n    # A smaller positive difference means a tighter fit.\n    tightness_score = bins_remain_cap - item\n    \n    # Ensure we don't have negative tightness scores (item doesn't fit)\n    tightness_score = np.maximum(tightness_score, -float('inf'))\n\n    # Calculate a score based on remaining capacity\n    # Larger remaining capacity gets a higher score, scaled for sigmoid\n    capacity_score = bins_remain_cap / 100.0 # Scale to prevent overflow with sigmoid\n    \n    # Combine scores using a sigmoid function to map to a [0, 1] range.\n    # We want to favor bins that are a good fit (tightness_score closer to 0)\n    # and also bins that have more remaining capacity.\n    # Let's use a weighted sum before the sigmoid.\n    \n    # Higher negative tightness_score means better fit, so we use -tightness_score\n    # A positive capacity_score means more space.\n    \n    # Example weights: Give more importance to a tighter fit\n    weight_tightness = 2.0\n    weight_capacity = 1.0\n    \n    combined_score_raw = weight_tightness * (-tightness_score) + weight_capacity * capacity_score\n    \n    # Apply sigmoid function\n    # We want to give a higher priority to bins where the item fits well,\n    # meaning bins_remain_cap - item is close to 0.\n    # For bins where it doesn't fit, the priority should be very low.\n    # The sigmoid function maps any real number to (0, 1).\n    # A larger input to sigmoid results in a value closer to 1.\n    # A smaller input results in a value closer to 0.\n\n    # Let's adjust the input to sigmoid to reflect our priorities.\n    # We want high priority for bins where (bins_remain_cap - item) is small and positive.\n    # And we want lower priority where (bins_remain_cap - item) is negative or very large positive.\n    \n    # For bins where item fits (bins_remain_cap >= item):\n    # The \"gap\" (bins_remain_cap - item) determines the \"snugness\".\n    # A smaller gap is better. We can use something like 1 / (1 + gap) or sigmoid of negative gap.\n    \n    # Let's try a simpler approach:\n    # Priority = Sigmoid( (bins_remain_cap - item) * k_fit + bins_remain_cap * k_capacity )\n    # k_fit: Controls sensitivity to how well the item fits. Higher k_fit means more penalty for poor fits.\n    # k_capacity: Controls sensitivity to remaining capacity. Higher k_capacity means prioritizing fuller bins more.\n\n    k_fit = 0.5  # Sensitivity to the fit. Larger values penalize poor fits more.\n    k_capacity = 0.1 # Sensitivity to remaining capacity. Larger values prefer more open bins.\n    \n    # Calculate scores. Only consider bins where the item fits.\n    fits = bins_remain_cap >= item\n    \n    # For bins where it fits, calculate a combined score\n    # High priority for small remaining capacity after fitting (tight fit)\n    # High priority for large remaining capacity overall (future flexibility)\n    \n    # A simple approach could be:\n    # Priority for fitting bins: A high score if remaining_cap - item is small and positive\n    # A low score if remaining_cap - item is large and positive.\n    \n    # Let's use a logistic function where the input represents a combination\n    # of how much space is left after placing the item and the total remaining space.\n    \n    # Option 1: Favor tight fits, but don't entirely ignore bins with more space.\n    # Map (bins_remain_cap - item) to a \"goodness of fit\" score.\n    # Small positive difference = good.\n    # Large positive difference = okay.\n    # Negative difference = bad.\n    \n    # We can use a sigmoid on the inverse of the remaining capacity after placement.\n    # If remaining_cap - item is small, then 1 / (remaining_cap - item) is large.\n    # If remaining_cap - item is large, then 1 / (remaining_cap - item) is small.\n    \n    # To handle the case where item does not fit, we set priority to 0.\n    # For bins that fit, we want to prioritize those with `bins_remain_cap - item` closer to 0.\n    # Also, having `bins_remain_cap` itself not too large might be good to avoid creating\n    # bins with too much empty space.\n    \n    # Let's define a preference for bins where the remaining capacity after placing the item is minimized.\n    # We can use the sigmoid function on a term that decreases as (bins_remain_cap - item) increases.\n    \n    # Consider the \"waste\" if we place the item: bins_remain_cap - item.\n    # We want to minimize this waste for a perfect fit, but we also want\n    # bins with larger `bins_remain_cap` to be considered if the waste isn't too large.\n    \n    # A score that prioritizes bins where (bins_remain_cap - item) is small and positive.\n    # Let's try sigmoid(- (bins_remain_cap - item) * 0.5 + bins_remain_cap * 0.05)\n    \n    # This formula will give higher values for:\n    # 1. Bins where `bins_remain_cap - item` is small and positive (due to the negative sign on this term)\n    # 2. Bins where `bins_remain_cap` is large (due to the positive sign on this term)\n    \n    # Let's refine: We want a higher score if `bins_remain_cap` is large enough to fit the item,\n    # and within those, we prefer those that result in less remaining capacity after fitting.\n    # This means `bins_remain_cap - item` should be as small as possible, but non-negative.\n    \n    # Let's use the negative of the remaining capacity after placing the item as input to sigmoid.\n    # This favors bins where `bins_remain_cap - item` is small (i.e., a tight fit).\n    # We also want to consider the overall remaining capacity to some extent.\n    \n    # Option: Sigmoid of a function that decreases with (bins_remain_cap - item)\n    # and increases with bins_remain_cap.\n    \n    # Let's combine two aspects:\n    # 1. How well the item fits (smaller remainder is better).\n    # 2. How much total capacity is left (larger might be good for future items).\n    \n    # Consider the term `bins_remain_cap - item`. We want this to be close to zero, but positive.\n    # Let's use `np.exp(-(bins_remain_cap - item))` which gives higher values for smaller `bins_remain_cap - item`.\n    # Then, apply sigmoid to scale these values and the overall remaining capacity.\n    \n    # Final idea: Prioritize bins where the item fits, and among those,\n    # prefer bins that have less remaining space *after* placing the item.\n    # This encourages filling bins efficiently.\n    \n    # We want `bins_remain_cap - item` to be small and non-negative.\n    # `sigmoid( -(bins_remain_cap - item) )` would do this.\n    # However, we also want to prefer bins that generally have more capacity\n    # for future items, but not excessively so that it leads to too many half-empty bins.\n    \n    # Let's try this: Sigmoid on the term that captures \"snugness\".\n    # A bin is a good candidate if `bins_remain_cap >= item`.\n    # Among fitting bins, a higher score for smaller `bins_remain_cap - item`.\n    \n    # Use a scaling factor to control the steepness of the sigmoid curve.\n    # `scale = 1.0` makes the transition around 0.\n    # We want to map `bins_remain_cap - item` such that values near 0\n    # result in high priority.\n    \n    # Let's use `np.exp(-bins_remain_cap / C)`. Higher `bins_remain_cap` -> lower score.\n    # This is for the \"fill them up\" strategy.\n    \n    # For the \"first fit decreasing\" or \"best fit\" idea, we want a tight fit.\n    # `sigmoid(-(bins_remain_cap - item))`\n    # If `bins_remain_cap - item` is small (tight fit), the argument is close to 0, sigmoid is ~0.5.\n    # If `bins_remain_cap - item` is large negative (item too big), argument is large positive, sigmoid is ~1.\n    # If `bins_remain_cap - item` is large positive (loose fit), argument is large negative, sigmoid is ~0.\n    \n    # This is the inverse of what we want. We want higher priority for tight fits.\n    \n    # Try: `sigmoid(k * (bins_remain_cap - item))`\n    # If `bins_remain_cap - item` is small positive (tight fit), sigmoid argument is small positive, ~0.5.\n    # If `bins_remain_cap - item` is large positive (loose fit), sigmoid argument is large positive, ~1.\n    # If `bins_remain_cap - item` is negative (won't fit), sigmoid argument is negative, ~0.\n    \n    # This seems to align better with favoring bins with less remaining capacity after placement.\n    # The `k` parameter controls how sensitive we are to the \"tightness\".\n    # `k=1.0` gives sigmoid(0) = 0.5 for a perfect fit.\n    \n    # Let's add a slight preference for bins that have *some* space left,\n    # to avoid immediately creating many bins with no room left.\n    # Maybe `sigmoid(k * (bins_remain_cap - item) + c * bins_remain_cap)`\n    \n    # Let's consider the goal: fill bins optimally.\n    # A good bin is one that can accommodate the item and leaves minimal remaining space.\n    # `remaining_after_fit = bins_remain_cap - item`\n    # We want `remaining_after_fit` to be small and non-negative.\n    \n    # Consider a function `f(x)` where `x = bins_remain_cap - item`.\n    # We want `f(x)` to be high when `x` is small and non-negative.\n    # `f(x) = exp(-x / scale)` for `x >= 0`, and `0` otherwise.\n    # Then scale this with sigmoid.\n    \n    # Calculate how much space would be left if we put the item in.\n    space_left = bins_remain_cap - item\n    \n    # Create a \"fit score\" that is high for small, non-negative `space_left`.\n    # We'll use the negative of `space_left` to map \"small positive\" to \"large positive\"\n    # for the sigmoid input.\n    # If `space_left` is negative (item doesn't fit), we want a very low priority.\n    # So, we can use a very large negative number for sigmoid input.\n    \n    fit_input = np.where(space_left >= 0, -space_left, -1e9) # Penalize items that don't fit\n    \n    # We can also incorporate a term related to the absolute remaining capacity.\n    # Perhaps bins that are already quite full (but can still fit the item) are prioritized.\n    # Let's consider the *normalized* remaining capacity as a secondary factor.\n    # However, this can be tricky without knowing the overall bin capacity limit.\n    # Assuming a standard bin capacity (e.g., 100):\n    \n    # Let's stick to the primary goal: tight fits.\n    # The input to sigmoid: `k * (-space_left)`\n    # `k` controls the sensitivity to tightness.\n    # `k = 1.0` -> `sigmoid(-space_left)`\n    # If `space_left` is 0 (perfect fit), sigmoid(0) = 0.5\n    # If `space_left` is 1 (loose fit), sigmoid(-1) = ~0.27\n    # If `space_left` is 5 (very loose), sigmoid(-5) = ~0.0067\n    # If `space_left` is -1 (item too big), fit_input is -1e9, sigmoid(-1e9) = ~0.\n    \n    # This seems to prioritize bins with smaller positive remaining space.\n    # Let's call this `best_fit_score`.\n    \n    # What if we also want to slightly favor bins that have a lot of capacity,\n    # but only if they *also* provide a relatively good fit?\n    # This is where it gets tricky to combine with sigmoid elegantly.\n    \n    # For a pure \"Best Fit\" heuristic, `sigmoid(- (bins_remain_cap - item))` is good.\n    # We can adjust the steepness with a multiplier.\n    \n    # Let's go with a strong bias towards best fit, modulated by the possibility of filling a bin.\n    # The score should be higher if `bins_remain_cap - item` is small and positive.\n    \n    # Consider a function `f(x)` where `x = bins_remain_cap`.\n    # We want to give a higher score if `x` is moderately large, but also\n    # if `x - item` is small.\n    \n    # Let's simplify: Prioritize bins where `bins_remain_cap` is just enough to fit the item.\n    # The value `bins_remain_cap - item` should be small and positive.\n    # `np.exp(-(bins_remain_cap - item))` for items that fit.\n    \n    # Transform `bins_remain_cap - item` into a score:\n    # Items that fit: prioritize small, positive `bins_remain_cap - item`.\n    # Items that don't fit: zero priority.\n    \n    # Let's create a term that peaks when `bins_remain_cap - item` is small and positive.\n    # Gaussian-like function centered around 0?\n    # `np.exp(-(bins_remain_cap - item)**2 / sigma**2)`\n    # This would favor fits near 0, but also loose fits equally to tight fits if `bins_remain_cap` is the same.\n    \n    # Best fit strategy is essentially minimizing `bins_remain_cap - item` for `bins_remain_cap >= item`.\n    # We can use sigmoid for this.\n    # `sigmoid( -(bins_remain_cap - item) * sensitivity)`\n    # Sensitivity controls how sharply we drop off for looser fits.\n    \n    sensitivity = 2.0 # Higher sensitivity for tighter fits\n    \n    # Calculate the argument for the sigmoid function.\n    # For bins where the item fits (bins_remain_cap >= item), the argument is\n    # `-(bins_remain_cap - item) * sensitivity`.\n    # This means a tight fit (small positive `bins_remain_cap - item`) gives an argument close to 0,\n    # resulting in a sigmoid score close to 0.5.\n    # A loose fit (large positive `bins_remain_cap - item`) gives a large negative argument,\n    # resulting in a score close to 0.\n    # An item that doesn't fit (`bins_remain_cap < item`) means `bins_remain_cap - item` is negative.\n    # So, `-(bins_remain_cap - item)` is positive. This gives a score close to 1.\n    # This is the opposite of what we want: items that don't fit should have zero priority.\n    \n    # Let's correct the logic: we want higher priority for bins where the item FITS and leaves less space.\n    # Input to sigmoid should be *higher* for better bins.\n    \n    # Let `y = bins_remain_cap`. We want to maximize a function that is high when `y >= item`\n    # and `y - item` is small.\n    \n    # Consider `score = sigmoid(k * (bins_remain_cap - item))`\n    # if `bins_remain_cap >= item`:\n    #   If `bins_remain_cap - item = 0` (perfect fit), score = sigmoid(0) = 0.5\n    #   If `bins_remain_cap - item = 10` (loose fit), score = sigmoid(10k)\n    # if `bins_remain_cap < item`:\n    #   score = sigmoid(<negative value>) -> close to 0.\n    \n    # This means loose fits get higher scores than perfect fits if `k` is negative.\n    # If `k` is positive, perfect fits get higher scores.\n    \n    # Let's try `k = -1.0` (Best Fit - minimizes remaining capacity).\n    # `priorities = 1 / (1 + np.exp(-sensitivity * (bins_remain_cap - item)))`\n    # For `bins_remain_cap - item = 0`: `sigmoid(0)` = 0.5\n    # For `bins_remain_cap - item = 10`: `sigmoid(-10)` ~ 0.000045\n    # For `bins_remain_cap - item = -1`: `sigmoid(1)` ~ 0.73\n    # This still prioritizes items that don't fit.\n    \n    # The simplest way to handle \"does not fit\" is to zero out their score.\n    \n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fits = bins_remain_cap >= item\n    \n    # For bins where the item fits, we want to prioritize those with minimal `bins_remain_cap - item`.\n    # The function `sigmoid(C * (bins_remain_cap - item))` does the following:\n    # - If `bins_remain_cap - item` is negative (item too big), input is negative, sigmoid ~0.\n    # - If `bins_remain_cap - item` is small positive (tight fit), input is small positive, sigmoid ~0.5.\n    # - If `bins_remain_cap - item` is large positive (loose fit), input is large positive, sigmoid ~1.\n    \n    # This seems to be prioritizing loose fits if C > 0.\n    # If C < 0, it prioritizes tight fits.\n    \n    # Let's use C < 0 for best fit.\n    # C = -1.0\n    # The argument will be `- (bins_remain_cap - item)`.\n    # This is equivalent to `(item - bins_remain_cap)`.\n    # We want to prioritize small values of `item - bins_remain_cap` (i.e., `bins_remain_cap - item` close to 0).\n    \n    # Consider the score `sigmoid(k * (item - bins_remain_cap))`.\n    # `k` is sensitivity. Higher `k` means more pronounced difference.\n    # If `item - bins_remain_cap` is small positive (tight fit), `sigmoid` is ~0.5.\n    # If `item - bins_remain_cap` is large positive (loose fit), `sigmoid` is ~1.\n    # If `item - bins_remain_cap` is negative (item too big), `sigmoid` is ~0.\n    \n    # This is again prioritizing loose fits.\n    \n    # Okay, let's try a different approach to map `bins_remain_cap - item` to a priority score.\n    # We want to map [0, large_positive] to [high_priority, low_priority].\n    # A simple mapping is `1 / (1 + (bins_remain_cap - item) / scale)`.\n    # This is similar to sigmoid's shape.\n    \n    # Let's use sigmoid on `-(bins_remain_cap - item)` which is `item - bins_remain_cap`.\n    # `sigmoid(k * (item - bins_remain_cap))`\n    # If `item - bins_remain_cap = 0` (perfect fit): `sigmoid(0)` = 0.5\n    # If `item - bins_remain_cap = 10` (loose fit): `sigmoid(10k)`\n    # If `item - bins_remain_cap = -10` (item too big): `sigmoid(-10k)`\n    \n    # Let `k = 1.0`.\n    # If `item - bins_remain_cap` is small positive (tight fit), sigmoid(small_pos) ~ 0.5\n    # If `item - bins_remain_cap` is large positive (loose fit), sigmoid(large_pos) ~ 1.\n    # If `item - bins_remain_cap` is negative (item too big), sigmoid(negative) ~ 0.\n    \n    # This appears to prioritize loose fits over tight fits.\n    \n    # Let's redefine our objective:\n    # We are designing a priority function for the *selection* of a bin.\n    # Higher priority means it's *more likely* to be chosen.\n    \n    # We want to favor bins that are \"good\".\n    # A good bin is one that can fit the item and has minimal space left over.\n    # The value `bins_remain_cap - item` should be minimized, subject to `bins_remain_cap >= item`.\n    \n    # We can use `sigmoid` to map the \"badness\" (`bins_remain_cap - item`) to a score.\n    # If `bins_remain_cap - item` is 0, we want a high score.\n    # If `bins_remain_cap - item` is large positive, we want a low score.\n    # If `bins_remain_cap - item` is negative, we want a score of 0.\n    \n    # Consider `sigmoid(-k * (bins_remain_cap - item))` where `k > 0`.\n    # `k=1`:\n    # `bins_remain_cap - item = 0` (perfect fit): `sigmoid(0)` = 0.5\n    # `bins_remain_cap - item = 10` (loose fit): `sigmoid(-10)` ~ 0.000045\n    # `bins_remain_cap - item = -1` (too big): `sigmoid(1)` ~ 0.73\n    \n    # Still a problem with items that don't fit.\n    # Let's enforce the \"fits\" condition first by zeroing out scores.\n    \n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fits_mask = bins_remain_cap >= item\n    \n    # For bins that fit, calculate a \"fit quality\" score.\n    # Higher score for smaller `bins_remain_cap - item`.\n    \n    # Let's use `sigmoid(C * (item - bins_remain_cap))`\n    # This maps `item - bins_remain_cap` to a [0, 1] range.\n    # `item - bins_remain_cap` = `-(bins_remain_cap - item)`\n    \n    # We want `bins_remain_cap - item` to be small and positive.\n    # This means `item - bins_remain_cap` should be small and negative.\n    \n    # Let `x = bins_remain_cap - item`. We want to map `[0, large_pos]` to `[high_score, low_score]`.\n    # The function `1 / (1 + x / scale)` or `sigmoid(log(x / scale))` could work.\n    \n    # Let's use the direct property of sigmoid: `sigmoid(z)` increases from 0 to 1 as `z` increases.\n    # We want a higher score for smaller `bins_remain_cap - item`.\n    # This means we want the argument to sigmoid to be smaller as `bins_remain_cap - item` increases.\n    # So, the argument should be proportional to `-(bins_remain_cap - item)`.\n    \n    # Let `argument = -sensitivity * (bins_remain_cap - item)`.\n    # If `bins_remain_cap - item` is 0 (perfect fit), argument is 0, sigmoid(0) = 0.5.\n    # If `bins_remain_cap - item` is 10 (loose fit), argument is -10*sensitivity.\n    # If `sensitivity = 1`, sigmoid(-10) is very small.\n    # If `bins_remain_cap - item` is -1 (too big), argument is 1*sensitivity.\n    # If `sensitivity = 1`, sigmoid(1) is ~0.73.\n    \n    # So, with `sigmoid(-sensitivity * (bins_remain_cap - item))`:\n    # - For items that fit, scores decrease as the fit gets looser. Good.\n    # - For items that don't fit, scores are high. Bad.\n    \n    # To fix the \"don't fit\" problem, we can set the argument to a very small number\n    # if the item doesn't fit.\n    \n    sensitivity = 3.0 # Controls how quickly priority drops for looser fits.\n    \n    # Calculate the argument for the sigmoid.\n    # If item fits, argument is `-sensitivity * (bins_remain_cap - item)`\n    # If item doesn't fit, argument is a very small number (to ensure sigmoid is close to 0).\n    argument = np.where(\n        fits,\n        -sensitivity * (bins_remain_cap - item),\n        -1e9  # A very small number for sigmoid to produce a near-zero output.\n    )\n    \n    # Calculate the priority scores using the sigmoid function.\n    priorities = 1 / (1 + np.exp(-argument))\n    \n    # This heuristic prioritizes bins that provide the tightest fit for the item.\n    # It's a form of the \"Best Fit\" strategy.\n    # The `sensitivity` parameter controls how strongly we penalize loose fits.\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (inverse distances) with Heuristic 2 (sigmoid with weighted sum), Heuristic 1 is simpler and directly targets bins that provide a tight fit by using the inverse ratio of capacities. Heuristic 2 attempts to balance tightness and overall capacity using a sigmoid, but its complexity might not translate to better performance without careful tuning of weights and scaling factors.\n\nComparing Heuristic 3-8 (\"Almost Full Fit\" using `1.0 / (remaining_caps_after_fit + epsilon)`) with Heuristic 9 (Best Fit using `-remaining_capacity`), Heuristics 3-8 directly implement the \"Almost Full Fit\" by prioritizing bins that leave minimal space after placement. Heuristic 9 is a cleaner Best Fit implementation by directly minimizing the negative remaining capacity. The repeated \"Almost Full Fit\" heuristics (3-8) suggest a strong focus on minimizing residual space, which is a core aspect of efficient bin packing.\n\nComparing Heuristic 10 (multiplicative combination for \"Almost Full Fit\") with Heuristic 3-8, Heuristic 10 attempts to combine tight fit (`1/fits`) with initial bin fullness (`item/bins_remain_cap`), which might offer a more nuanced \"Almost Full\" strategy. However, the simpler inverse of `fits` (as in 3-8) is a more direct interpretation of minimizing residual space.\n\nComparing Heuristic 19 and 20 (Sigmoid Fit Score with peak around ideal ratio) with others, these heuristics try to find a balance between not too full and not too empty bins using a sigmoid function to model preference. They are more complex than direct \"tightest fit\" heuristics but aim to optimize for a specific packing characteristic. Heuristic 19/20's approach of creating a peak around an ideal ratio is a sophisticated way to model \"good fit\" using sigmoids.\n\nHeuristics 14-18 (all zeros) are clearly the worst as they provide no discriminatory priority. Heuristics 11-13 (Epsilon-Greedy) add an exploration component to a Best Fit strategy, which is more applicable in learning scenarios than for deterministic heuristic design.\n\nOverall: Heuristics that directly target minimizing residual space (like \"Almost Full Fit\" or \"Best Fit\") appear to be generally strong. Complexity increases with sigmoid-based approaches, which might offer benefits if tuned correctly but can also introduce fragility. Simple, direct mappings often perform well.\n- \nHere's a redefined approach to self-reflection for heuristic design:\n\n*   **Keywords:** Goal-oriented, iterative, context-aware, measurable impact.\n*   **Advice:** Focus on achieving the heuristic's core objective, not just superficial characteristics. Measure performance against defined goals, not just against \"tight fit.\"\n*   **Avoid:** Over-emphasis on simplicity at the expense of effectiveness. Assuming \"tight fit\" automatically equates to good performance.\n*   **Explanation:** True effectiveness comes from solving the problem. Continuously test and refine based on actual outcomes, adapting the heuristic to the specific problem context.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}