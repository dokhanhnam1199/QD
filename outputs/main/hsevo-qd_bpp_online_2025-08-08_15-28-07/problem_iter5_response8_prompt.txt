{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins by favoring tight fits and initially fuller bins.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    \n    fits = bins_remain_cap[can_fit_mask] - item\n    initial_rem_caps = bins_remain_cap[can_fit_mask]\n\n    # Primary score: Inverse of remaining capacity after item placement (tightest fit)\n    # Maximizes the \"fullness\" achieved by placing the item.\n    tight_fit_score = 1.0 / (fits + 1e-9)\n    \n    # Secondary score: Inverse of initial remaining capacity (prefers fuller bins)\n    # Adds a bonus for bins that were already less empty.\n    initial_fill_score = 1.0 / (initial_rem_caps + 1e-9)\n    \n    # Combine scores multiplicatively: rewards bins that are both tight-fitting and initially fuller.\n    # This aims to find a balance for \"Almost Full Fit\".\n    priorities[can_fit_mask] = tight_fit_score * initial_fill_score\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Best) vs. Heuristic 2 (Worse): Heuristic 1 uses a sigmoid function on `-(bins_remain_cap - item)` scaled by `sensitivity` to prioritize tight fits, effectively implementing a \"Best Fit\" strategy by mapping small positive remaining spaces to high priorities. It correctly handles non-fitting bins by assigning near-zero priority. Heuristic 2 uses a simple additive combination of two poorly scaled scores (`-(bins_remain_cap - item)` and `1.0 / (remaining_capacity_after_item + 1e-6)`) without proper normalization or a clear strategy for combining \"Best Fit\" and \"Almost Full Fit\" objectives, leading to potentially unstable or suboptimal prioritization.\n\nComparing Heuristic 1 (Best) vs. Heuristic 3 (Worse): Heuristic 1 focuses on tight fits using a sigmoid. Heuristic 3 attempts a complex combination using two sigmoids to create a peak preference around an \"ideal ratio\" of remaining capacity to item size, but the logic of combining the \"best fit\" scores negatively with scaled sigmoid scores (which peak in the middle) is unclear and potentially counterproductive. The negative best-fit scores combined with positive sigmoid influence can lead to unpredictable results.\n\nComparing Heuristic 1 (Best) vs. Heuristic 4 (Worse): Heuristic 1 effectively implements \"Best Fit\" with a sigmoid. Heuristic 4 attempts to combine \"tightness\" and \"capacity\" scores before applying a sigmoid. However, the logic of `tightness_score = bins_remain_cap - item` and `capacity_score = bins_remain_cap / 100.0` with `weight_tightness * (-tightness_score) + weight_capacity * capacity_score` before sigmoid is convoluted. It doesn't clearly prioritize tight fits as effectively as Heuristic 1 and introduces a potentially arbitrary scaling for capacity.\n\nComparing Heuristic 1 (Best) vs. Heuristic 5 (Worse): Heuristic 1 is a clean \"Best Fit\" using sigmoid. Heuristic 5 combines \"Almost Full Fit\" (inverse of remaining capacity after fit) with the original remaining capacity multiplicatively. This prioritizes bins that are both \"almost full\" after packing and \"somewhat full\" before packing, which might not always align with minimizing bins, and the multiplicative interaction can be less intuitive than a focused \"Best Fit.\"\n\nComparing Heuristic 1 (Best) vs. Heuristic 6 & 7 (Worse): Heuristics 6 and 7 are identical and combine \"tightest fit\" (inverse of remaining capacity after fit) with \"initially fuller bins\" (inverse of initial remaining capacity) multiplicatively. While a valid combination strategy, Heuristic 1's focused \"Best Fit\" approach using sigmoid is often a more robust starting point for bin packing heuristics, as it directly optimizes for minimal waste. The multiplicative combination in 6/7 can amplify issues if either component is poorly scaled.\n\nComparing Heuristic 1 (Best) vs. Heuristic 8 (Worse): Heuristic 1 is a clear \"Best Fit\". Heuristic 8 tries to smooth the \"tight fit\" preference by slightly penalizing extremely tight fits using `available_bins_cap / (available_bins_cap - item + smoothing_factor + 1e-9)`. While trying to avoid \"too tight\" fits is sometimes useful, Heuristic 1's direct \"Best Fit\" is generally more robust for the primary goal of minimizing bins. The smoothing factor (`0.15 * item`) might also lead to suboptimal choices for different item sizes.\n\nComparing Heuristic 1 (Best) vs. Heuristic 9 (Worse): Heuristic 1 is \"Best Fit\". Heuristic 9 attempts to apply a similar smoothing as Heuristic 8 but with a different smoothing factor and calculation (`available_bins_cap / (available_bins_cap - item + smoothing_factor + 1e-9)`). The core issue remains: aggressively penalizing tight fits might not be optimal for general bin packing, and the specific smoothing logic is less direct than Heuristic 1's focus on minimizing waste.\n\nComparing Heuristic 1 (Best) vs. Heuristic 10 (Worse): Heuristic 1 is a deterministic \"Best Fit\". Heuristic 10 introduces an Epsilon-Greedy approach, randomly selecting a bin with probability `epsilon` (0.2). While exploration can be useful in learning-based methods, for a pure heuristic, deterministic \"Best Fit\" is usually preferred for consistency and direct optimization. The random selection can lead to suboptimal choices, undermining the goal of minimizing bins.\n\nComparing Heuristic 1 (Best) vs. Heuristics 11, 13, 15, 16 (Worse): These heuristics simply return zeros or an empty array without any logic for prioritization. They are non-functional and thus the worst possible heuristics.\n\nComparing Heuristic 1 (Best) vs. Heuristics 12, 14 (Worse): These heuristics check for available bins but then fail to implement any actual prioritization logic, returning zeros. They are also non-functional.\n\nComparing Heuristic 1 (Best) vs. Heuristic 17 (Worse): Heuristic 1 is a clean \"Best Fit\". Heuristic 17 attempts a \"Sigmoid Fit Score\" by combining two sigmoids to create a peak preference around an \"ideal ratio\" (`item * 1.2`). While this aims to balance tightness and available space, the complexity of the combined sigmoid and the arbitrary 'ideal ratio' might be less robust than a direct \"Best Fit\" for minimizing bin count. Heuristic 1's approach is more straightforward and directly targets the primary optimization goal.\n\nComparing Heuristic 1 (Best) vs. Heuristic 18 (Worse): Heuristic 1 is a clean \"Best Fit\". Heuristic 18 uses a \"Sigmoid Fit Score\" by combining two sigmoids to create a peak preference around `ideal_ratio` (1.1), controlled by steepness `k` (5.0). The logic of combining `sigmoid(k*(ratio-ideal))` and `sigmoid(-k*(ratio-ideal))` to create a peak, while a valid mathematical construct, is more complex and potentially less direct for the bin packing goal of minimizing bins compared to Heuristic 1's focused \"Best Fit\" strategy. The arbitrary `ideal_ratio` and `k` also make it less universally applicable.\n\nComparing Heuristic 1 (Best) vs. Heuristic 19 (Worse): Heuristic 1 is a clean \"Best Fit\". Heuristic 19 also uses a two-sigmoid approach to create a peak preference around an `ideal_ratio` (1.1) with steepness `k` (5.0), similar to Heuristic 18. The complexity and reliance on arbitrary parameters (`ideal_ratio`, `k`) make it less direct and potentially less robust than the straightforward \"Best Fit\" of Heuristic 1 for the primary objective of minimizing bins.\n\nComparing Heuristic 1 (Best) vs. Heuristic 20 (Worse): Heuristic 1 is a clean \"Best Fit\". Heuristic 20 also employs a two-sigmoid approach similar to 17, 18, and 19, aiming for a peak preference around `ideal_ratio` (1.1) with steepness `k` (5.0). The combination of sigmoids to create a preference peak, while mathematically sound, is more complex and less directly aligned with the primary bin packing goal of minimizing bins compared to Heuristic 1's direct \"Best Fit\" strategy. The arbitrary nature of `ideal_ratio` and `k` is also a drawback.\n\nOverall: Heuristic 1 implements a robust \"Best Fit\" strategy by prioritizing bins that minimize leftover space after placing an item, effectively using a sigmoid to map this fit quality to a priority. Heuristics 2-16 are either flawed, incomplete, or non-functional. Heuristics 17-20 attempt more complex \"Sigmoid Fit Score\" strategies, often involving combinations of sigmoids to create preference peaks, which are more intricate and rely on tunable parameters, making them potentially less robust or generalizable than the straightforward \"Best Fit\" of Heuristic 1.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Adaptive, contextual, performance-driven, goal-alignment.\n*   **Advice:** Shift focus from *a priori* simplicity to *a posteriori* performance. Design heuristics that adapt to the problem's specific characteristics and performance feedback, rather than strictly adhering to pre-defined simple rules.\n*   **Avoid:** Over-reliance on \"tight fit\" or \"almost full\" metaphors. These are often proxies for underlying goals, not the goals themselves. Avoid assuming monotonic preferences without empirical validation.\n*   **Explanation:** True effectiveness comes from aligning heuristic behavior with measurable optimization outcomes. This means being willing to explore complexity if it demonstrably improves performance on specific objectives, while rigorously testing for robustness and edge-case handling.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}