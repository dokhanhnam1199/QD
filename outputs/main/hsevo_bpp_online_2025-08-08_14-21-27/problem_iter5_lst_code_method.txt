{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for bins using an adaptive prioritization strategy.\n    This heuristic combines proximity fit with a look-ahead to favor bins that,\n    after packing the current item, leave a remaining capacity that is \"useful\"\n    for future items (i.e., not too large, not too small).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_cap.size > 0:\n        # Proximity Fit component: Higher priority for bins closer to item size\n        differences = fitting_bins_cap - item\n        proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Adaptive component: Penalize bins that leave very large or very small remainders\n        # The goal is to keep remaining capacities in a range that is more likely to be filled by future items.\n        # A common heuristic is to target remaining capacities around half the bin size or specific item sizes.\n        # For simplicity here, we'll penalize very large remainders, aiming for a tighter fit.\n        # The penalty is inversely proportional to how much \"wasted\" space is left.\n        # We'll define \"wasted\" space as the remainder itself.\n        # A larger remainder means a higher penalty.\n        \n        # Example: If bin capacity is 100, and item is 30, remainder is 70.\n        # If item is 60, remainder is 40.\n        # We want to penalize the remainder of 70 more than 40.\n        # We can use a score that is high for small remainders and low for large remainders.\n        # Let's use 1 / (remainder + epsilon)\n        \n        # To make it adaptive, we can consider the *distribution* of remaining capacities.\n        # If most bins have large remainders, a large remainder might not be that bad.\n        # However, for a general heuristic, aiming for smaller remainders is often good.\n        \n        # Let's consider the \"goodness\" of the remaining capacity.\n        # A remaining capacity of 0 is good if it means the bin is full.\n        # A remaining capacity that is exactly another item's size is good.\n        # A remaining capacity that is too large is bad (wasted space).\n        # A remaining capacity that is too small is also bad (cannot fit many items).\n        \n        # A simple adaptive approach: penalize large remainders.\n        # Consider the range of remaining capacities for bins that can fit the item.\n        # A simple inverse relation to the remainder: 1 / (remainder + epsilon)\n        # This is similar to proximity, but we are looking at the *resulting* remainder.\n        \n        # Let's refine this:\n        # We want to favor bins where (bin_cap - item) is small, but not too small.\n        # And we want to avoid leaving very large residual capacities.\n        \n        # Let's use a score that is high for intermediate remainders and low for extreme remainders.\n        # For example, a Gaussian-like function centered around a \"target\" remainder.\n        # A simple target could be the average remainder of fitting bins.\n        \n        target_remainder = np.mean(differences) if differences.size > 0 else 0\n        \n        # If target_remainder is very small or very large, this might not be ideal.\n        # Let's use a more robust target, e.g., half the item size or a fixed value like 10.\n        # Or, more simply, let's just penalize large remainders.\n        \n        # Consider a score that is 1 / (1 + difference) which is similar to proximity.\n        # Now let's add a penalty for the *resulting* remainder.\n        # The resulting remainder is `fitting_bins_cap - item`.\n        # We want to favor smaller resulting remainders.\n        \n        # Let's combine proximity score with a score for the resulting remainder.\n        # Score = proximity_score * (1 / (resulting_remainder + epsilon))\n        # This means bins that are a close fit AND leave a small remainder are prioritized.\n        \n        resulting_remainders = fitting_bins_cap - item\n        \n        # To avoid issues with very small resulting remainders (which are good),\n        # let's ensure the score doesn't become infinite.\n        # The proximity score already handles \"close fit\".\n        \n        # Let's try a different approach for adaptiveness:\n        # The ideal scenario is to leave a remaining capacity that can fit another item perfectly.\n        # We don't know future items, but we can try to leave a \"versatile\" capacity.\n        # A remaining capacity around half of the typical item size, or a fixed small value,\n        # might be more versatile than very large or very small remainders.\n        \n        # Let's create a score that favors remainders within a certain range.\n        # For simplicity, let's favor remainders that are not too large.\n        # If we penalize large remainders, this encourages packing items tightly.\n        \n        # Let's create a \"utility\" score for the remaining capacity.\n        # A remaining capacity `r` is good if it's small (tight fit) but not zero (unless bin is full).\n        # It's also good if it can accommodate some small future items.\n        \n        # Let's combine proximity and a \"utility\" of the remaining capacity.\n        # Utility function: Higher for smaller remainders, but not zero.\n        # e.g., 1 / (resulting_remainder + epsilon)\n        \n        # Combined score: proximity_score * utility_score\n        # This means we prefer bins that are a close fit AND leave a small remainder.\n        \n        utility_scores = 1.0 / (resulting_remainders + 1e-9)\n        \n        # Combining the two:\n        # We want to give higher priority to bins that are a good fit AND leave a good remainder.\n        # Let's scale both and multiply.\n        \n        # Proximity score is 1 / (fitting_bins_cap - item + epsilon)\n        # Utility score is 1 / (fitting_bins_cap - item + epsilon)\n        # This is redundant.\n        \n        # Let's rethink the adaptive part.\n        # The key idea of adaptiveness is to use information from the current state to guide the heuristic.\n        # The state is the `bins_remain_cap`.\n        \n        # Consider the variance of remaining capacities. If variance is low, most bins are similar.\n        # If variance is high, there's a wide range.\n        \n        # Let's go back to the original goal: minimize the number of bins.\n        # This implies trying to pack items as tightly as possible, but also avoiding creating \"unusable\" small spaces.\n        \n        # A simple adaptive heuristic:\n        # Prioritize bins that are a good fit (proximity) but also consider the resulting capacity.\n        # Let's favor bins that leave a remainder that is \"balanced\" - not too small, not too large.\n        # A common strategy in online algorithms is to use a \"look-ahead\" or to consider properties of the remaining capacity distribution.\n        \n        # Let's define \"good\" remaining capacity as being between a small threshold and a large threshold.\n        # For example, if bin capacity is 100, a remaining capacity of 10-40 might be considered good.\n        # This is hard to generalize without knowing item size distribution.\n        \n        # A simpler approach: Prioritize bins that are a tight fit (small remainder) but\n        # avoid bins that would leave an *extremely* small remainder, as that might be less flexible.\n        \n        # Let's use a score that rewards proximity but then penalizes if the resulting remainder is too small.\n        # For example, if item is 30, and bin has 31 remaining. Proximity is high. Resulting remainder is 1.\n        # We might prefer a bin with 40 remaining (item 30, remainder 10).\n        \n        # Score = proximity_score * penalty_for_small_remainder\n        # Penalty for small remainder: 1 / (1 + small_remainder_value)\n        \n        # Let's try:\n        # Score = (1 / (difference + epsilon)) * (1 / (1 + resulting_remainder))\n        # This means we prioritize bins that are a good fit AND leave a small remainder.\n        # This is also quite similar to just proximity.\n        \n        # Let's focus on the *distribution* of remaining capacities of fitting bins.\n        # If the item is large, it might only fit in a few bins, and those bins might have large remainders.\n        # If the item is small, it might fit in many bins, some with small remainders, some with large.\n        \n        # Adaptive strategy: Consider the \"value\" of the remaining capacity.\n        # A more sophisticated approach could involve learning or predicting future item sizes.\n        # For a pure heuristic, we need to encode this \"value\" based on current state.\n        \n        # Let's use the proximity score and then modify it based on the resulting remainder relative to other fitting bins.\n        \n        # For each fitting bin:\n        #   Calculate proximity: prox = 1 / (remaining_cap - item + epsilon)\n        #   Calculate resulting remainder: res_rem = remaining_cap - item\n        #   Calculate a \"remainder score\": rem_score.\n        #     This score should be higher for remainders that are \"useful\".\n        #     Useful could mean: not too small, not too large.\n        #     Let's try a score that peaks at a certain remainder size, e.g., around 50% of the bin capacity, or a fixed small value.\n        #     For simplicity, let's just reward smaller remainders, but with a cap to avoid extreme scores.\n        #     rem_score = 1 / (res_rem + epsilon) # This is still just proximity.\n        \n        # Let's try to make the priority dependent on how *unique* a bin is in terms of its remaining capacity.\n        # If an item fits perfectly in many bins, it doesn't matter which we pick.\n        # If an item fits in only one bin, we should pick that one, regardless of its remainder.\n        \n        # How about: Priority = proximity_score * (1 + C * normalized_residual_capacity)\n        # Where C is a parameter and normalized_residual_capacity is the residual capacity\n        # scaled by something, e.g., the average residual capacity of fitting bins.\n        \n        # Let's try a simpler adaptive idea:\n        # Prioritize bins that are a good fit (proximity), BUT if there are multiple bins\n        # with similar proximity scores, favor the one that leaves a smaller remainder.\n        # This is implicitly done by 1/(diff + epsilon) if diff is small.\n        \n        # The \"adaptive\" part should perhaps respond to the overall state of the bins.\n        # If all bins have very large remaining capacities, maybe we should prioritize the closest fit.\n        # If bins have varying capacities, we can be more selective.\n        \n        # Let's introduce a penalty for leaving \"too much\" excess capacity.\n        # The excess capacity is `remaining_cap - item`.\n        # We want to minimize this excess capacity.\n        # So, our score should be inversely related to `excess_capacity`.\n        \n        # Let's try:\n        # Score = proximity_score * (1 / (resulting_remainder + epsilon))\n        # This means we favor bins that are a good fit AND leave a small remainder.\n        # This is essentially rewarding tight packing.\n        \n        # The adaptiveness can come from how we combine proximity and remainder score.\n        # Instead of multiplying, maybe we add or use a more complex function.\n        \n        # Let's consider the \"waste\" ratio: `(remaining_cap - item) / remaining_cap`\n        # We want to minimize this waste ratio.\n        # Score would be inversely proportional to this ratio.\n        \n        # Let's try a combined score:\n        # Score = proximity_score - lambda * waste_ratio\n        # Where lambda is a weighting factor.\n        \n        # Let's stick to a score that prioritizes bins that are a close fit and leave a small remainder.\n        # The problem with `1.0 / (differences + 1e-9)` is that it highly favors the absolute closest fit.\n        # This might leave very small remainders which are not useful.\n        \n        # Adaptive approach:\n        # Favor bins that are a close fit, BUT also favor bins that leave a \"reasonably sized\" remainder.\n        # A remainder that's too small is bad. A remainder that's too large is also bad.\n        # Let's define a \"good\" remainder as being within a certain range.\n        # For example, a remainder `r` is good if `epsilon <= r <= target_remainder_max`.\n        # We can penalize remainders outside this range.\n        \n        # Let's define a score for the remainder:\n        # If `res_rem < epsilon`: penalty (e.g., 0.1)\n        # If `res_rem > target_remainder_max`: penalty (e.g., 0.1 * res_rem / target_remainder_max)\n        # If `epsilon <= res_rem <= target_remainder_max`: reward (e.g., 1.0)\n        \n        # This requires defining `target_remainder_max`. Let's set it to be, say, 20% of bin capacity or a fixed small value.\n        # This is getting complicated for a simple heuristic.\n        \n        # Let's simplify the adaptive idea:\n        # Prioritize bins that are a close fit (proximity_score).\n        # If there are multiple bins with high proximity scores, break ties by choosing the one with the smallest resulting remainder.\n        # This is still implicitly handled by the `1/(diff)` if `diff` is small.\n        \n        # Let's try a score that is sensitive to the *distribution* of remaining capacities.\n        # If a bin's remaining capacity is very close to the item size, that's good (proximity).\n        # If a bin's remaining capacity is such that `remaining_cap - item` is small, that's also good (tight packing).\n        \n        # Let's combine proximity and \"tightness of packing\" directly.\n        # Proximity Score: `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)`\n        # Tightness Score: `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` (This is the same!)\n        \n        # Let's try to reward bins that, after packing, leave a remaining capacity that is *not too small*.\n        # This encourages leaving space for potentially future items, rather than filling up bins too precisely.\n        \n        # Score = proximity_score * (1 + resulting_remainder / average_fitting_remainder)\n        # This rewards bins that are a close fit AND leave a larger-than-average remainder. This seems counter-intuitive for minimizing bins.\n        \n        # Let's try to reward bins that leave a remainder that is \"medium\" sized.\n        # For example, a remainder `r` is good if `r` is around `item_size / 2` or some other target.\n        \n        # Let's use a simpler adaptive idea:\n        # Prioritize bins that are a close fit.\n        # THEN, among those, prioritize bins that leave a remainder that is itself a \"good\" candidate for future items.\n        # A \"good\" remainder could be one that is not too small and not too large.\n        # Let's represent this by penalizing very small and very large remainders.\n        \n        resulting_remainders = fitting_bins_cap - item\n        \n        # Let's use a score that penalizes large remainders more heavily.\n        # This promotes tighter packing.\n        # Score = proximity_score * (1 / (resulting_remainder + epsilon))\n        # This gives more weight to bins that are a close fit AND leave a small remainder.\n        \n        # Let's try a different combination:\n        # Proximity score favors `remaining_cap` closest to `item`.\n        # Let's also consider the \"waste ratio\": `(remaining_cap - item) / bin_capacity`.\n        # We want to minimize this waste ratio.\n        \n        # Let's combine the two using a weighted sum, or by modifying the proximity score.\n        \n        # Adaptive component: If an item fits into multiple bins with similar proximity scores,\n        # then we want to make a choice that is \"better\" for the overall packing.\n        # This often means leaving a remainder that is \"useful\".\n        \n        # Let's try this:\n        # The base priority is proximity.\n        # We will add a bonus if the resulting remainder is \"good\".\n        # A \"good\" remainder is one that is not too small.\n        # Let's define \"good\" as being greater than some small threshold, e.g., 10% of bin capacity or a fixed small value.\n        \n        # This is hard to implement generically without knowing bin capacity.\n        # Let's assume a fixed bin capacity or use an average.\n        \n        # Simple adaptive idea:\n        # Prioritize bins that are a close fit.\n        # Among bins with similar \"closeness\", prefer the one that leaves a smaller remaining capacity.\n        # This is already captured by the `1/(diff + epsilon)` if we prioritize the smallest `diff`.\n        \n        # Let's introduce a \"balancing\" factor.\n        # The ideal scenario is to leave a remainder that can fit another item.\n        # We can't know future items, but we can aim for \"versatile\" remainders.\n        # A versatile remainder might be one that is not extremely small or extremely large.\n        \n        # Let's combine proximity with a penalty for large remainders.\n        # Score = proximity_score - lambda * (resulting_remainder / max_possible_remainder)\n        \n        # Let's try a different adaptive approach based on the distribution of fitting bins:\n        # If there's only one bin that fits, its priority is 1.\n        # If multiple bins fit, we use proximity.\n        # THEN, we can adjust the scores based on the resulting remainders.\n        \n        # Let's try to penalize bins that leave very large remainders.\n        # Score = proximity_score * (1 / (1 + resulting_remainder))\n        # This emphasizes bins that are a close fit AND leave a small remainder.\n        \n        # This is still essentially proximity, but with a stronger preference for smaller remainders.\n        \n        # Let's try a more direct \"adaptive\" approach:\n        # Consider the *average* remaining capacity of the bins that can fit the item.\n        # If the current item is large, it might only fit in bins with large remaining capacities.\n        # If the current item is small, it might fit in bins with various remaining capacities.\n        \n        # Let's try to encourage leaving remainders that are not too small.\n        # If `resulting_remainder` is very small, reduce the priority.\n        \n        # Score = proximity_score * (1 + alpha * min(0, resulting_remainder - threshold))\n        # Where threshold is a small value. This penalizes remainders below the threshold.\n        \n        # Let's refine the proximity score to be less sensitive to tiny differences.\n        # Instead of `1 / (diff + epsilon)`, maybe `1 / (1 + diff^p)` for `p > 1`.\n        # Or, use a step function, or capped inverse.\n        \n        # Let's go with a simple adaptive idea:\n        # Prioritize bins that are a close fit (proximity_score).\n        # THEN, among bins with similar proximity scores, prioritize the one that leaves the smallest remainder.\n        # This is often achieved by `1 / (diff + epsilon)`.\n        \n        # What if we modify the proximity score based on the \"usefulness\" of the resulting capacity?\n        # A useful capacity is one that can fit other items.\n        # Let's consider the \"ideal\" remainder to be around half of the bin capacity (as a guess).\n        # Score = proximity_score * exp(-(resulting_remainder - target_remainder)^2 / (2 * variance))\n        # This favors remainders close to the target.\n        \n        # For a simpler heuristic, let's just penalize large remainders more.\n        # Score = proximity_score * (1 / (1 + resulting_remainder))\n        # This aims for a tight fit.\n        \n        # Let's try to make it more \"adaptive\" by considering the *range* of fitting bins.\n        # If all fitting bins have very similar remaining capacities, the proximity score is enough.\n        # If fitting bins have a wide range of capacities, maybe we should prefer those that are not \"extreme\" in their leftover space.\n        \n        # Let's use a score that rewards proximity and penalizes large resulting remainders.\n        # Score = (1 / (difference + 1e-9)) * (1 / (1 + resulting_remainders))\n        # This effectively gives higher scores to bins that are a good fit AND leave a small remainder.\n        \n        # Let's try to introduce a slight bias away from perfectly filling bins,\n        # by penalizing very small remainders, but still favoring tight fits overall.\n        \n        # Score = (1 / (difference + 1e-9)) * (resulting_remainders + 1) / (resulting_remainders + 1 + small_penalty_term)\n        \n        # Let's try this adaptive approach:\n        # Combine proximity with a score that reflects how \"full\" the bin becomes.\n        # We want bins to be full, but not *so* full that the remainder is unusable.\n        # Let's use a score that is high for small differences, but decreases if the resulting remainder is too small.\n        \n        # Score = 1.0 / (differences + 1e-9)  # Proximity\n        # Let's modify this by penalizing small resulting remainders.\n        # If resulting_remainder < some_threshold (e.g., 10% of bin capacity): penalize.\n        \n        # Let's make the priority score a function of both the difference AND the resulting remainder,\n        # aiming for a balance between a close fit and a useful leftover space.\n        \n        # Score = (1 / (difference + 1e-9)) * (1 / (1 + resulting_remainders))\n        # This prioritizes bins that are a close fit AND leave a small remainder. This encourages tight packing.\n        \n        # Let's try to achieve \"balanced packing\" by rewarding remainders that are not too small.\n        # Score = proximity_score * (resulting_remainders + 1) / (resulting_remainders + 1 + alpha * small_remainder_penalty)\n        \n        # A simple adaptive heuristic:\n        # Prioritize bins that are a close fit (proximity score).\n        # Then, among those, prioritize the ones that leave a remainder that is \"balanced\".\n        # Balanced remainder: not too small, not too large.\n        # Let's use a score that favors remainders in a medium range.\n        \n        # Let's try to make the priority score proportional to the proximity, but also inversely proportional to the resulting remainder.\n        # This encourages close fits AND small remainders.\n        \n        resulting_remainders = fitting_bins_cap - item\n        \n        # Let's use a combined score that emphasizes both proximity and small resulting remainders.\n        # Score = (1 / (difference + 1e-9)) * (1 / (resulting_remainders + 1e-9))\n        # This effectively prioritizes bins that are a very close fit and leave a minimal remainder.\n        \n        # To make it more \"adaptive\", let's consider the ratio of difference to resulting remainder.\n        # Or, let's modify the proximity score based on the *quality* of the resulting remainder.\n        \n        # Let's define a \"remainder quality\" score.\n        # If `r` is the remainder, and `max_r` is the maximum remainder among fitting bins.\n        # Quality could be `1 - (r / max_r)` (favors smaller remainders).\n        \n        # Let's combine proximity and a penalty for large remainders.\n        # Score = proximity_score * (1 / (1 + resulting_remainders))\n        \n        # This still heavily favors the absolute closest fit.\n        \n        # Let's try to add a term that rewards remainders that are not too small.\n        # Score = proximity_score + alpha * log(resulting_remainders + 1)\n        # This adds a bonus for larger remainders, but log scales it down.\n        \n        # Let's try the simplest form of adaptiveness:\n        # If multiple bins have the same best proximity score, choose the one with the smallest remainder.\n        # The `1 / (diff + epsilon)` already does this.\n        \n        # Let's try to make the priority score less sensitive to minuscule differences by capping the inverse.\n        # Or by using a different function like `1 / (1 + diff^2)`.\n        \n        # Let's introduce an adaptive component by considering the *distribution* of remaining capacities.\n        # If the item is small and fits in many bins, we can be more selective.\n        # If the item is large and fits in few bins, we might need to accept larger remainders.\n        \n        # Let's try to achieve a \"balanced\" packing:\n        # Prioritize bins that are a close fit.\n        # Then, among those, prioritize bins that leave a remainder that is not too small.\n        # Score = proximity_score * (1 + alpha * (resulting_remainders / avg_fitting_remainder))\n        # This would favor bins that are a close fit AND leave a *larger* remainder. This is counterproductive.\n        \n        # Let's try rewarding bins that leave a remainder that is \"useful\".\n        # A useful remainder might be one that's not too small.\n        \n        # Score = proximity_score * f(resulting_remainders)\n        # Where f is a function that is high for medium remainders and low for small/large remainders.\n        # For simplicity, let's just penalize large remainders.\n        \n        # Score = proximity_score * (1 / (1 + resulting_remainders))\n        # This favors bins that are a close fit and leave a small remainder.\n        \n        # Let's try to make it more adaptive by considering the *scale* of the differences.\n        # If all differences are large, `1/diff` might not differentiate well.\n        # If all differences are small, `1/diff` can lead to extreme values.\n        \n        # Let's use the inverse of the *normalized* difference.\n        # normalized_diff = (fitting_bins_cap - item) / fitting_bins_cap\n        # Score = 1 / (normalized_diff + epsilon)\n        \n        # This prioritizes bins where the item takes up a larger proportion of the bin.\n        \n        # Let's combine proximity and \"fullness\" of the bin.\n        # Fullness Score = item / fitting_bins_cap\n        # Proximity Score = 1 / (fitting_bins_cap - item + epsilon)\n        \n        # Let's try to create a score that is sensitive to the *relative* closeness.\n        # Consider the \"gap\" = `fitting_bins_cap - item`.\n        # We want small gaps.\n        # Let's also consider the \"fill ratio\" = `item / fitting_bins_cap`.\n        # We want high fill ratio, but small gaps.\n        \n        # Let's try a score that rewards both:\n        # Score = (item / fitting_bins_cap) * (1 / (fitting_bins_cap - item + epsilon))\n        # This multiplies the fill ratio by the proximity score.\n        # A bin that is almost full and has a small remaining capacity will get a high score.\n        \n        fill_ratios = item / fitting_bins_cap\n        differences = fitting_bins_cap - item\n        \n        # Combined score: higher fill ratio AND smaller difference.\n        # Let's try multiplying them.\n        # Score = fill_ratios * (1 / (differences + 1e-9))\n        \n        # Example: Bin Cap = 100\n        # Item = 30: fits in bins with caps >= 30.\n        # Bin 1: rem_cap = 40. diff = 10. fill_ratio = 30/40 = 0.75. score = 0.75 * (1/10) = 0.075\n        # Bin 2: rem_cap = 35. diff = 5.  fill_ratio = 30/35 = 0.857. score = 0.857 * (1/5) = 0.1714\n        # Bin 3: rem_cap = 100. diff = 70. fill_ratio = 30/100 = 0.3. score = 0.3 * (1/70) = 0.004\n        \n        # This seems to work well: it favors bins that are close fits AND have a high fill ratio.\n        # The fill ratio component implicitly penalizes bins that would leave very large remainders.\n        \n        # Now, we need to normalize these scores.\n        \n        priorities[can_fit_mask] = fill_ratios * (1.0 / (differences + 1e-9))\n        \n        # Normalize priorities so that the best fit bin has a score of 1\n        max_priority = np.max(priorities[can_fit_mask])\n        if max_priority > 0:\n            priorities[can_fit_mask] /= max_priority\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive Priority: Combines 'Best Fit' with an 'Almost Full' bias.\n    Prioritizes bins that leave minimal space, favoring near-perfect fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate remaining capacity for bins that can fit the item\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Heuristic: Prioritize bins that result in less remaining capacity.\n    # This is a \"Best Fit\" like strategy.\n    # Using 1 / (1 + residual_capacity) to give higher scores to smaller residuals.\n    # A residual capacity of 0 gets a score of 1. A large residual gets a score close to 0.\n    priorities[can_fit_mask] = 1.0 / (1.0 + remaining_capacity_after_fit)\n    \n    # Additional bias: Slightly boost priority for bins that become nearly full (e.g., residual < 0.1 * bin_capacity)\n    # This \"almost full\" bias encourages tighter packing and potentially better overall utilization.\n    # We'll apply a small multiplier to these bins.\n    original_bin_capacities = bins_remain_cap[can_fit_mask] # Assuming we know original capacities or can infer\n    # For this example, let's assume a fixed bin capacity, say 1.0, for demonstration\n    # In a real scenario, bin capacity would be a parameter or known context.\n    # If bin_capacity is not fixed, this bias needs adjustment or a different approach.\n    # For simplicity here, let's assume a standard bin capacity is known or implied.\n    # Let's use a placeholder if bin capacity is not explicitly available.\n    # If bin_capacity is available, it would be:\n    # almost_full_mask = remaining_capacity_after_fit < (bin_capacity * 0.1) \n    \n    # Without explicit bin capacity, we'll use a small absolute residual as a proxy for 'almost full'\n    # For example, if the remaining capacity is very small (e.g., less than 0.05)\n    small_residual_bias_mask = remaining_capacity_after_fit < 0.05\n    priorities[can_fit_mask][small_residual_bias_mask] *= 1.1 # Apply a small boost\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive Priority: Combines 'Best Fit' with an 'Almost Full' bias.\n    Prioritizes bins that leave minimal space, favoring near-perfect fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate remaining capacity for bins that can fit the item\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Heuristic: Prioritize bins that result in less remaining capacity.\n    # This is a \"Best Fit\" like strategy.\n    # Using 1 / (1 + residual_capacity) to give higher scores to smaller residuals.\n    # A residual capacity of 0 gets a score of 1. A large residual gets a score close to 0.\n    priorities[can_fit_mask] = 1.0 / (1.0 + remaining_capacity_after_fit)\n    \n    # Additional bias: Slightly boost priority for bins that become nearly full (e.g., residual < 0.1 * bin_capacity)\n    # This \"almost full\" bias encourages tighter packing and potentially better overall utilization.\n    # We'll apply a small multiplier to these bins.\n    original_bin_capacities = bins_remain_cap[can_fit_mask] # Assuming we know original capacities or can infer\n    # For this example, let's assume a fixed bin capacity, say 1.0, for demonstration\n    # In a real scenario, bin capacity would be a parameter or known context.\n    # If bin_capacity is not fixed, this bias needs adjustment or a different approach.\n    # For simplicity here, let's assume a standard bin capacity is known or implied.\n    # Let's use a placeholder if bin capacity is not explicitly available.\n    # If bin_capacity is available, it would be:\n    # almost_full_mask = remaining_capacity_after_fit < (bin_capacity * 0.1) \n    \n    # Without explicit bin capacity, we'll use a small absolute residual as a proxy for 'almost full'\n    # For example, if the remaining capacity is very small (e.g., less than 0.05)\n    small_residual_bias_mask = remaining_capacity_after_fit < 0.05\n    priorities[can_fit_mask][small_residual_bias_mask] *= 1.1 # Apply a small boost\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines the \"Best Fit\" greedy strategy with an exploration component.\n    Prioritizes bins that best fit the item, with a small chance of exploring other options.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    # Calculate greedy scores: inverse of remaining capacity after placement (Best Fit)\n    # Higher score for bins with less remaining capacity after placing the item\n    greedy_scores = 1.0 / (bins_remain_cap[fittable_bins_mask] - item + 1e-9)\n\n    # Introduce a small exploration factor: slightly boost all fittable bins\n    # This encourages trying bins that might not be the absolute best fit but could lead to better overall packing later.\n    exploration_boost = 0.05\n    priorities[fittable_bins_mask] = greedy_scores + exploration_boost\n\n    # Normalize priorities to ensure they are comparable and avoid extreme values\n    if np.max(priorities) > 1e-9: # Avoid division by zero if all priorities are zero\n        priorities /= np.max(priorities)\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for bins using an adaptive approach that\n    prioritizes bins that are a \"good fit\" (not too much remaining capacity)\n    while also considering bins that are \"close fits\" (just enough capacity).\n    It also incorporates a slight penalty for bins that are *exactly* full\n    to encourage more flexibility in future packing, and a bonus for bins\n    that can accommodate the item with minimal waste.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    \n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    \n    if fitting_bins_cap.size > 0:\n        differences = fitting_bins_cap - item\n        \n        # Primary score: Inverse of the difference (proximity fit)\n        # Add a small epsilon to avoid division by zero if an item perfectly fits a bin\n        proximity_scores = 1.0 / (differences + 1e-9)\n        \n        # Secondary score: Bonus for minimal waste (closer to zero difference)\n        # This emphasizes bins that are almost perfect fits\n        minimal_waste_bonus = np.exp(-differences * 5.0)  # Exponentially rewards smaller differences\n        \n        # Tertiary consideration: Slight penalty for exact fits to encourage future flexibility\n        # We do this by slightly reducing the score for bins where difference is near zero\n        exact_fit_penalty = np.ones_like(differences)\n        exact_fit_mask = np.abs(differences) < 1e-6\n        exact_fit_penalty[exact_fit_mask] = 0.95 # Small penalty for perfect fits\n\n        # Combine scores: Proximity as base, minimal waste as enhancement, with penalty for exact fits\n        combined_scores = (proximity_scores * (1 + minimal_waste_bonus)) * exact_fit_penalty\n        \n        priorities[can_fit_mask] = combined_scores\n        \n        # Normalize priorities so that the best bin has a score of 1\n        max_priority = np.max(priorities[can_fit_mask])\n        if max_priority > 0:\n            priorities[can_fit_mask] /= max_priority\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for bins using an adaptive prioritization strategy.\n    This heuristic combines proximity fit with a look-ahead to favor bins that,\n    after packing the current item, leave a remaining capacity that is \"useful\"\n    for future items (i.e., not too large, not too small).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_cap.size > 0:\n        # Proximity Fit component: Higher priority for bins closer to item size\n        differences = fitting_bins_cap - item\n        proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Adaptive component: Penalize bins that leave very large or very small remainders\n        # The goal is to keep remaining capacities in a range that is more likely to be filled by future items.\n        # A common heuristic is to target remaining capacities around half the bin size or specific item sizes.\n        # For simplicity here, we'll penalize very large remainders, aiming for a tighter fit.\n        # The penalty is inversely proportional to how much \"wasted\" space is left.\n        # We'll define \"wasted\" space as the remainder itself.\n        # A larger remainder means a higher penalty.\n        \n        # Example: If bin capacity is 100, and item is 30, remainder is 70.\n        # If item is 60, remainder is 40.\n        # We want to penalize the remainder of 70 more than 40.\n        # We can use a score that is high for small remainders and low for large remainders.\n        # Let's use 1 / (remainder + epsilon)\n        \n        # To make it adaptive, we can consider the *distribution* of remaining capacities.\n        # If most bins have large remainders, a large remainder might not be that bad.\n        # However, for a general heuristic, aiming for smaller remainders is often good.\n        \n        # Let's consider the \"goodness\" of the remaining capacity.\n        # A remaining capacity of 0 is good if it means the bin is full.\n        # A remaining capacity that is exactly another item's size is good.\n        # A remaining capacity that is too large is bad (wasted space).\n        # A remaining capacity that is too small is also bad (cannot fit many items).\n        \n        # A simple adaptive approach: penalize large remainders.\n        # Consider the range of remaining capacities for bins that can fit the item.\n        # A simple inverse relation to the remainder: 1 / (remainder + epsilon)\n        # This is similar to proximity, but we are looking at the *resulting* remainder.\n        \n        # Let's refine this:\n        # We want to favor bins where (bin_cap - item) is small, but not too small.\n        # And we want to avoid leaving very large residual capacities.\n        \n        # Let's use a score that is high for intermediate remainders and low for extreme remainders.\n        # For example, a Gaussian-like function centered around a \"target\" remainder.\n        # A simple target could be the average remainder of fitting bins.\n        \n        target_remainder = np.mean(differences) if differences.size > 0 else 0\n        \n        # If target_remainder is very small or very large, this might not be ideal.\n        # Let's use a more robust target, e.g., half the item size or a fixed value like 10.\n        # Or, more simply, let's just penalize large remainders.\n        \n        # Consider a score that is 1 / (1 + difference) which is similar to proximity.\n        # Now let's add a penalty for the *resulting* remainder.\n        # The resulting remainder is `fitting_bins_cap - item`.\n        # We want to favor smaller resulting remainders.\n        \n        # Let's combine proximity score with a score for the resulting remainder.\n        # Score = proximity_score * (1 / (resulting_remainder + epsilon))\n        # This means bins that are a close fit AND leave a small remainder are prioritized.\n        \n        resulting_remainders = fitting_bins_cap - item\n        \n        # To avoid issues with very small resulting remainders (which are good),\n        # let's ensure the score doesn't become infinite.\n        # The proximity score already handles \"close fit\".\n        \n        # Let's try a different approach for adaptiveness:\n        # The ideal scenario is to leave a remaining capacity that can fit another item perfectly.\n        # We don't know future items, but we can try to leave a \"versatile\" capacity.\n        # A remaining capacity around half of the typical item size, or a fixed small value,\n        # might be more versatile than very large or very small remainders.\n        \n        # Let's create a score that favors remainders within a certain range.\n        # For simplicity, let's favor remainders that are not too large.\n        # If we penalize large remainders, this encourages packing items tightly.\n        \n        # Let's create a \"utility\" score for the remaining capacity.\n        # A remaining capacity `r` is good if it's small (tight fit) but not zero (unless bin is full).\n        # It's also good if it can accommodate some small future items.\n        \n        # Let's combine proximity and a \"utility\" of the remaining capacity.\n        # Utility function: Higher for smaller remainders, but not zero.\n        # e.g., 1 / (resulting_remainder + epsilon)\n        \n        # Combined score: proximity_score * utility_score\n        # This means we prefer bins that are a close fit AND leave a small remainder.\n        \n        utility_scores = 1.0 / (resulting_remainders + 1e-9)\n        \n        # Combining the two:\n        # We want to give higher priority to bins that are a good fit AND leave a good remainder.\n        # Let's scale both and multiply.\n        \n        # Proximity score is 1 / (fitting_bins_cap - item + epsilon)\n        # Utility score is 1 / (fitting_bins_cap - item + epsilon)\n        # This is redundant.\n        \n        # Let's rethink the adaptive part.\n        # The key idea of adaptiveness is to use information from the current state to guide the heuristic.\n        # The state is the `bins_remain_cap`.\n        \n        # Consider the variance of remaining capacities. If variance is low, most bins are similar.\n        # If variance is high, there's a wide range.\n        \n        # Let's go back to the original goal: minimize the number of bins.\n        # This implies trying to pack items as tightly as possible, but also avoiding creating \"unusable\" small spaces.\n        \n        # A simple adaptive heuristic:\n        # Prioritize bins that are a good fit (proximity) but also consider the resulting capacity.\n        # Let's favor bins that leave a remainder that is \"balanced\" - not too small, not too large.\n        # A common strategy in online algorithms is to use a \"look-ahead\" or to consider properties of the remaining capacity distribution.\n        \n        # Let's define \"good\" remaining capacity as being between a small threshold and a large threshold.\n        # For example, if bin capacity is 100, a remaining capacity of 10-40 might be considered good.\n        # This is hard to generalize without knowing item size distribution.\n        \n        # A simpler approach: Prioritize bins that are a tight fit (small remainder) but\n        # avoid bins that would leave an *extremely* small remainder, as that might be less flexible.\n        \n        # Let's use a score that rewards proximity but then penalizes if the resulting remainder is too small.\n        # For example, if item is 30, and bin has 31 remaining. Proximity is high. Resulting remainder is 1.\n        # We might prefer a bin with 40 remaining (item 30, remainder 10).\n        \n        # Score = proximity_score * penalty_for_small_remainder\n        # Penalty for small remainder: 1 / (1 + small_remainder_value)\n        \n        # Let's try:\n        # Score = (1 / (difference + epsilon)) * (1 / (1 + resulting_remainder))\n        # This means we prioritize bins that are a good fit AND leave a small remainder.\n        # This is also quite similar to just proximity.\n        \n        # Let's focus on the *distribution* of remaining capacities of fitting bins.\n        # If the item is large, it might only fit in a few bins, and those bins might have large remainders.\n        # If the item is small, it might fit in many bins, some with small remainders, some with large.\n        \n        # Adaptive strategy: Consider the \"value\" of the remaining capacity.\n        # A more sophisticated approach could involve learning or predicting future item sizes.\n        # For a pure heuristic, we need to encode this \"value\" based on current state.\n        \n        # Let's use the proximity score and then modify it based on the resulting remainder relative to other fitting bins.\n        \n        # For each fitting bin:\n        #   Calculate proximity: prox = 1 / (remaining_cap - item + epsilon)\n        #   Calculate resulting remainder: res_rem = remaining_cap - item\n        #   Calculate a \"remainder score\": rem_score.\n        #     This score should be higher for remainders that are \"useful\".\n        #     Useful could mean: not too small, not too large.\n        #     Let's try a score that peaks at a certain remainder size, e.g., around 50% of the bin capacity, or a fixed small value.\n        #     For simplicity, let's just reward smaller remainders, but with a cap to avoid extreme scores.\n        #     rem_score = 1 / (res_rem + epsilon) # This is still just proximity.\n        \n        # Let's try to make the priority dependent on how *unique* a bin is in terms of its remaining capacity.\n        # If an item fits perfectly in many bins, it doesn't matter which we pick.\n        # If an item fits in only one bin, we should pick that one, regardless of its remainder.\n        \n        # How about: Priority = proximity_score * (1 + C * normalized_residual_capacity)\n        # Where C is a parameter and normalized_residual_capacity is the residual capacity\n        # scaled by something, e.g., the average residual capacity of fitting bins.\n        \n        # Let's try a simpler adaptive idea:\n        # Prioritize bins that are a good fit (proximity), BUT if there are multiple bins\n        # with similar proximity scores, favor the one that leaves a smaller remainder.\n        # This is implicitly done by 1/(diff + epsilon) if diff is small.\n        \n        # The \"adaptive\" part should perhaps respond to the overall state of the bins.\n        # If all bins have very large remaining capacities, maybe we should prioritize the closest fit.\n        # If bins have varying capacities, we can be more selective.\n        \n        # Let's introduce a penalty for leaving \"too much\" excess capacity.\n        # The excess capacity is `remaining_cap - item`.\n        # We want to minimize this excess capacity.\n        # So, our score should be inversely related to `excess_capacity`.\n        \n        # Let's try:\n        # Score = proximity_score * (1 / (resulting_remainder + epsilon))\n        # This means we favor bins that are a good fit AND leave a small remainder.\n        # This is essentially rewarding tight packing.\n        \n        # The adaptiveness can come from how we combine proximity and remainder score.\n        # Instead of multiplying, maybe we add or use a more complex function.\n        \n        # Let's consider the \"waste\" ratio: `(remaining_cap - item) / remaining_cap`\n        # We want to minimize this waste ratio.\n        # Score would be inversely proportional to this ratio.\n        \n        # Let's try a combined score:\n        # Score = proximity_score - lambda * waste_ratio\n        # Where lambda is a weighting factor.\n        \n        # Let's stick to a score that prioritizes bins that are a close fit and leave a small remainder.\n        # The problem with `1.0 / (differences + 1e-9)` is that it highly favors the absolute closest fit.\n        # This might leave very small remainders which are not useful.\n        \n        # Adaptive approach:\n        # Favor bins that are a close fit, BUT also favor bins that leave a \"reasonably sized\" remainder.\n        # A remainder that's too small is bad. A remainder that's too large is also bad.\n        # Let's define a \"good\" remainder as being within a certain range.\n        # For example, a remainder `r` is good if `epsilon <= r <= target_remainder_max`.\n        # We can penalize remainders outside this range.\n        \n        # Let's define a score for the remainder:\n        # If `res_rem < epsilon`: penalty (e.g., 0.1)\n        # If `res_rem > target_remainder_max`: penalty (e.g., 0.1 * res_rem / target_remainder_max)\n        # If `epsilon <= res_rem <= target_remainder_max`: reward (e.g., 1.0)\n        \n        # This requires defining `target_remainder_max`. Let's set it to be, say, 20% of bin capacity or a fixed small value.\n        # This is getting complicated for a simple heuristic.\n        \n        # Let's simplify the adaptive idea:\n        # Prioritize bins that are a close fit (proximity_score).\n        # If there are multiple bins with high proximity scores, break ties by choosing the one with the smallest resulting remainder.\n        # This is still implicitly handled by the `1/(diff)` if `diff` is small.\n        \n        # Let's try a score that is sensitive to the *distribution* of remaining capacities.\n        # If a bin's remaining capacity is very close to the item size, that's good (proximity).\n        # If a bin's remaining capacity is such that `remaining_cap - item` is small, that's also good (tight packing).\n        \n        # Let's combine proximity and \"tightness of packing\" directly.\n        # Proximity Score: `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)`\n        # Tightness Score: `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` (This is the same!)\n        \n        # Let's try to reward bins that, after packing, leave a remaining capacity that is *not too small*.\n        # This encourages leaving space for potentially future items, rather than filling up bins too precisely.\n        \n        # Score = proximity_score * (1 + resulting_remainder / average_fitting_remainder)\n        # This rewards bins that are a close fit AND leave a larger-than-average remainder. This seems counter-intuitive for minimizing bins.\n        \n        # Let's try to reward bins that leave a remainder that is \"medium\" sized.\n        # For example, a remainder `r` is good if `r` is around `item_size / 2` or some other target.\n        \n        # Let's use a simpler adaptive idea:\n        # Prioritize bins that are a close fit.\n        # THEN, among those, prioritize bins that leave a remainder that is itself a \"good\" candidate for future items.\n        # A \"good\" remainder could be one that is not too small and not too large.\n        # Let's represent this by penalizing very small and very large remainders.\n        \n        resulting_remainders = fitting_bins_cap - item\n        \n        # Let's use a score that penalizes large remainders more heavily.\n        # This promotes tighter packing.\n        # Score = proximity_score * (1 / (resulting_remainder + epsilon))\n        # This gives more weight to bins that are a close fit AND leave a small remainder.\n        \n        # Let's try a different combination:\n        # Proximity score favors `remaining_cap` closest to `item`.\n        # Let's also consider the \"waste ratio\": `(remaining_cap - item) / bin_capacity`.\n        # We want to minimize this waste ratio.\n        \n        # Let's combine the two using a weighted sum, or by modifying the proximity score.\n        \n        # Adaptive component: If an item fits into multiple bins with similar proximity scores,\n        # then we want to make a choice that is \"better\" for the overall packing.\n        # This often means leaving a remainder that is \"useful\".\n        \n        # Let's try this:\n        # The base priority is proximity.\n        # We will add a bonus if the resulting remainder is \"good\".\n        # A \"good\" remainder is one that is not too small.\n        # Let's define \"good\" as being greater than some small threshold, e.g., 10% of bin capacity or a fixed small value.\n        \n        # This is hard to implement generically without knowing bin capacity.\n        # Let's assume a fixed bin capacity or use an average.\n        \n        # Simple adaptive idea:\n        # Prioritize bins that are a close fit.\n        # Among bins with similar \"closeness\", prefer the one that leaves a smaller remaining capacity.\n        # This is already captured by the `1/(diff + epsilon)` if we prioritize the smallest `diff`.\n        \n        # Let's introduce a \"balancing\" factor.\n        # The ideal scenario is to leave a remainder that can fit another item.\n        # We can't know future items, but we can aim for \"versatile\" remainders.\n        # A versatile remainder might be one that is not extremely small or extremely large.\n        \n        # Let's combine proximity with a penalty for large remainders.\n        # Score = proximity_score - lambda * (resulting_remainder / max_possible_remainder)\n        \n        # Let's try a different adaptive approach based on the distribution of fitting bins:\n        # If there's only one bin that fits, its priority is 1.\n        # If multiple bins fit, we use proximity.\n        # THEN, we can adjust the scores based on the resulting remainders.\n        \n        # Let's try to penalize bins that leave very large remainders.\n        # Score = proximity_score * (1 / (1 + resulting_remainder))\n        # This emphasizes bins that are a close fit AND leave a small remainder.\n        \n        # This is still essentially proximity, but with a stronger preference for smaller remainders.\n        \n        # Let's try a more direct \"adaptive\" approach:\n        # Consider the *average* remaining capacity of the bins that can fit the item.\n        # If the current item is large, it might only fit in bins with large remaining capacities.\n        # If the current item is small, it might fit in bins with various remaining capacities.\n        \n        # Let's try to encourage leaving remainders that are not too small.\n        # If `resulting_remainder` is very small, reduce the priority.\n        \n        # Score = proximity_score * (1 + alpha * min(0, resulting_remainder - threshold))\n        # Where threshold is a small value. This penalizes remainders below the threshold.\n        \n        # Let's refine the proximity score to be less sensitive to tiny differences.\n        # Instead of `1 / (diff + epsilon)`, maybe `1 / (1 + diff^p)` for `p > 1`.\n        # Or, use a step function, or capped inverse.\n        \n        # Let's go with a simple adaptive idea:\n        # Prioritize bins that are a close fit (proximity_score).\n        # THEN, among bins with similar proximity scores, prioritize the one that leaves the smallest remainder.\n        # This is often achieved by `1 / (diff + epsilon)`.\n        \n        # What if we modify the proximity score based on the \"usefulness\" of the resulting capacity?\n        # A useful capacity is one that can fit other items.\n        # Let's consider the \"ideal\" remainder to be around half of the bin capacity (as a guess).\n        # Score = proximity_score * exp(-(resulting_remainder - target_remainder)^2 / (2 * variance))\n        # This favors remainders close to the target.\n        \n        # For a simpler heuristic, let's just penalize large remainders more.\n        # Score = proximity_score * (1 / (1 + resulting_remainder))\n        # This aims for a tight fit.\n        \n        # Let's try to make it more \"adaptive\" by considering the *range* of fitting bins.\n        # If all fitting bins have very similar remaining capacities, the proximity score is enough.\n        # If fitting bins have a wide range of capacities, maybe we should prefer those that are not \"extreme\" in their leftover space.\n        \n        # Let's use a score that rewards proximity and penalizes large resulting remainders.\n        # Score = (1 / (difference + 1e-9)) * (1 / (1 + resulting_remainders))\n        # This effectively gives higher scores to bins that are a good fit AND leave a small remainder.\n        \n        # Let's try to introduce a slight bias away from perfectly filling bins,\n        # by penalizing very small remainders, but still favoring tight fits overall.\n        \n        # Score = (1 / (difference + 1e-9)) * (resulting_remainders + 1) / (resulting_remainders + 1 + small_penalty_term)\n        \n        # Let's try this adaptive approach:\n        # Combine proximity with a score that reflects how \"full\" the bin becomes.\n        # We want bins to be full, but not *so* full that the remainder is unusable.\n        # Let's use a score that is high for small differences, but decreases if the resulting remainder is too small.\n        \n        # Score = 1.0 / (differences + 1e-9)  # Proximity\n        # Let's modify this by penalizing small resulting remainders.\n        # If resulting_remainder < some_threshold (e.g., 10% of bin capacity): penalize.\n        \n        # Let's make the priority score a function of both the difference AND the resulting remainder,\n        # aiming for a balance between a close fit and a useful leftover space.\n        \n        # Score = (1 / (difference + 1e-9)) * (1 / (1 + resulting_remainders))\n        # This prioritizes bins that are a close fit AND leave a small remainder. This encourages tight packing.\n        \n        # Let's try to achieve \"balanced packing\" by rewarding remainders that are not too small.\n        # Score = proximity_score * (resulting_remainders + 1) / (resulting_remainders + 1 + alpha * small_remainder_penalty)\n        \n        # A simple adaptive heuristic:\n        # Prioritize bins that are a close fit (proximity score).\n        # Then, among those, prioritize the ones that leave a remainder that is \"balanced\".\n        # Balanced remainder: not too small, not too large.\n        # Let's use a score that favors remainders in a medium range.\n        \n        # Let's try to make the priority score proportional to the proximity, but also inversely proportional to the resulting remainder.\n        # This encourages close fits AND small remainders.\n        \n        resulting_remainders = fitting_bins_cap - item\n        \n        # Let's use a combined score that emphasizes both proximity and small resulting remainders.\n        # Score = (1 / (difference + 1e-9)) * (1 / (resulting_remainders + 1e-9))\n        # This effectively prioritizes bins that are a very close fit and leave a minimal remainder.\n        \n        # To make it more \"adaptive\", let's consider the ratio of difference to resulting remainder.\n        # Or, let's modify the proximity score based on the *quality* of the resulting remainder.\n        \n        # Let's define a \"remainder quality\" score.\n        # If `r` is the remainder, and `max_r` is the maximum remainder among fitting bins.\n        # Quality could be `1 - (r / max_r)` (favors smaller remainders).\n        \n        # Let's combine proximity and a penalty for large remainders.\n        # Score = proximity_score * (1 / (1 + resulting_remainders))\n        \n        # This still heavily favors the absolute closest fit.\n        \n        # Let's try to add a term that rewards remainders that are not too small.\n        # Score = proximity_score + alpha * log(resulting_remainders + 1)\n        # This adds a bonus for larger remainders, but log scales it down.\n        \n        # Let's try the simplest form of adaptiveness:\n        # If multiple bins have the same best proximity score, choose the one with the smallest remainder.\n        # The `1 / (diff + epsilon)` already does this.\n        \n        # Let's try to make the priority score less sensitive to minuscule differences by capping the inverse.\n        # Or by using a different function like `1 / (1 + diff^2)`.\n        \n        # Let's introduce an adaptive component by considering the *distribution* of remaining capacities.\n        # If the item is small and fits in many bins, we can be more selective.\n        # If the item is large and fits in few bins, we might need to accept larger remainders.\n        \n        # Let's try to achieve a \"balanced\" packing:\n        # Prioritize bins that are a close fit.\n        # Then, among those, prioritize bins that leave a remainder that is not too small.\n        # Score = proximity_score * (1 + alpha * (resulting_remainders / avg_fitting_remainder))\n        # This would favor bins that are a close fit AND leave a *larger* remainder. This is counterproductive.\n        \n        # Let's try rewarding bins that leave a remainder that is \"useful\".\n        # A useful remainder might be one that's not too small.\n        \n        # Score = proximity_score * f(resulting_remainders)\n        # Where f is a function that is high for medium remainders and low for small/large remainders.\n        # For simplicity, let's just penalize large remainders.\n        \n        # Score = proximity_score * (1 / (1 + resulting_remainders))\n        # This favors bins that are a close fit and leave a small remainder.\n        \n        # Let's try to make it more adaptive by considering the *scale* of the differences.\n        # If all differences are large, `1/diff` might not differentiate well.\n        # If all differences are small, `1/diff` can lead to extreme values.\n        \n        # Let's use the inverse of the *normalized* difference.\n        # normalized_diff = (fitting_bins_cap - item) / fitting_bins_cap\n        # Score = 1 / (normalized_diff + epsilon)\n        \n        # This prioritizes bins where the item takes up a larger proportion of the bin.\n        \n        # Let's combine proximity and \"fullness\" of the bin.\n        # Fullness Score = item / fitting_bins_cap\n        # Proximity Score = 1 / (fitting_bins_cap - item + epsilon)\n        \n        # Let's try to create a score that is sensitive to the *relative* closeness.\n        # Consider the \"gap\" = `fitting_bins_cap - item`.\n        # We want small gaps.\n        # Let's also consider the \"fill ratio\" = `item / fitting_bins_cap`.\n        # We want high fill ratio, but small gaps.\n        \n        # Let's try a score that rewards both:\n        # Score = (item / fitting_bins_cap) * (1 / (fitting_bins_cap - item + epsilon))\n        # This multiplies the fill ratio by the proximity score.\n        # A bin that is almost full and has a small remaining capacity will get a high score.\n        \n        fill_ratios = item / fitting_bins_cap\n        differences = fitting_bins_cap - item\n        \n        # Combined score: higher fill ratio AND smaller difference.\n        # Let's try multiplying them.\n        # Score = fill_ratios * (1 / (differences + 1e-9))\n        \n        # Example: Bin Cap = 100\n        # Item = 30: fits in bins with caps >= 30.\n        # Bin 1: rem_cap = 40. diff = 10. fill_ratio = 30/40 = 0.75. score = 0.75 * (1/10) = 0.075\n        # Bin 2: rem_cap = 35. diff = 5.  fill_ratio = 30/35 = 0.857. score = 0.857 * (1/5) = 0.1714\n        # Bin 3: rem_cap = 100. diff = 70. fill_ratio = 30/100 = 0.3. score = 0.3 * (1/70) = 0.004\n        \n        # This seems to work well: it favors bins that are close fits AND have a high fill ratio.\n        # The fill ratio component implicitly penalizes bins that would leave very large remainders.\n        \n        # Now, we need to normalize these scores.\n        \n        priorities[can_fit_mask] = fill_ratios * (1.0 / (differences + 1e-9))\n        \n        # Normalize priorities so that the best fit bin has a score of 1\n        max_priority = np.max(priorities[can_fit_mask])\n        if max_priority > 0:\n            priorities[can_fit_mask] /= max_priority\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines the \"Best Fit\" greedy strategy with an exploration component.\n    Prioritizes bins that best fit the item, with a small chance of exploring other options.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    # Calculate greedy scores: inverse of remaining capacity after placement (Best Fit)\n    # Higher score for bins with less remaining capacity after placing the item\n    greedy_scores = 1.0 / (bins_remain_cap[fittable_bins_mask] - item + 1e-9)\n\n    # Introduce a small exploration factor: slightly boost all fittable bins\n    # This encourages trying bins that might not be the absolute best fit but could lead to better overall packing later.\n    exploration_boost = 0.05\n    priorities[fittable_bins_mask] = greedy_scores + exploration_boost\n\n    # Normalize priorities to ensure they are comparable and avoid extreme values\n    if np.max(priorities) > 1e-9: # Avoid division by zero if all priorities are zero\n        priorities /= np.max(priorities)\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines the \"Best Fit\" greedy strategy with an exploration component.\n    Prioritizes bins that best fit the item, with a small chance of exploring other options.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    # Calculate greedy scores: inverse of remaining capacity after placement (Best Fit)\n    # Higher score for bins with less remaining capacity after placing the item\n    greedy_scores = 1.0 / (bins_remain_cap[fittable_bins_mask] - item + 1e-9)\n\n    # Introduce a small exploration factor: slightly boost all fittable bins\n    # This encourages trying bins that might not be the absolute best fit but could lead to better overall packing later.\n    exploration_boost = 0.05\n    priorities[fittable_bins_mask] = greedy_scores + exploration_boost\n\n    # Normalize priorities to ensure they are comparable and avoid extreme values\n    if np.max(priorities) > 1e-9: # Avoid division by zero if all priorities are zero\n        priorities /= np.max(priorities)\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins by favoring those with minimal remaining capacity after packing, while also considering the overall bin count.\n\n    This heuristic aims to fill bins as much as possible (Best Fit like) while\n    implicitly encouraging the use of fewer bins by giving a slight boost to bins\n    that are already well-utilized.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins_mask = bins_remain_cap >= item\n\n    if not np.any(valid_bins_mask):\n        return priorities\n\n[Heuristics 10th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins by favoring those with minimal remaining capacity after packing, while also considering the overall bin count.\n\n    This heuristic aims to fill bins as much as possible (Best Fit like) while\n    implicitly encouraging the use of fewer bins by giving a slight boost to bins\n    that are already well-utilized.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins_mask = bins_remain_cap >= item\n\n    if not np.any(valid_bins_mask):\n        return priorities\n\n[Heuristics 11th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on a balance of minimal wasted space and avoiding fragmentation.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    valid_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(valid_bins_mask):\n        return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a greedy approach with a small probability of random selection\n    to balance exploration and exploitation for bin packing.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    eligible_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(eligible_bins_mask):\n        return priorities\n        \n    eligible_bins_remain_cap = bins_remain_cap[eligible_bins_mask]\n    \n    \n    greedy_scores = 1.0 / (eligible_bins_remain_cap - item + 1e-9)\n    \n    \n    epsilon = 0.05 # Small exploration rate\n    num_eligible_bins = eligible_bins_mask.sum()\n    \n    \n    exploration_indices = np.random.choice(\n        np.arange(num_eligible_bins), \n        size=int(epsilon * num_eligible_bins), \n        replace=False\n    )\n    \n    \n    priorities[eligible_bins_mask][exploration_indices] = 1.0 \n    \n    \n    exploitation_indices = np.setdiff1d(np.arange(num_eligible_bins), exploration_indices)\n    priorities[eligible_bins_mask][exploitation_indices] = greedy_scores[exploitation_indices]\n    \n    \n    if np.max(priorities) > 0:\n        priorities = priorities / np.max(priorities)\n        \n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for bins using an adaptive strategy that\n    considers both proximity and potential for future fits.\n    Bins that can fit the item will have a non-zero priority.\n    The priority is higher for bins that are a tighter fit, but also\n    gives a slight bonus to bins that leave more remaining capacity,\n    aiming to balance immediate packing efficiency with future flexibility.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_cap.size > 0:\n        differences = fitting_bins_cap - item\n        # Base proximity score: higher for tighter fits\n        proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Adaptive bonus: encourage leaving more capacity if the fit is not perfect\n        # This bonus is higher for bins that are not a perfect fit,\n        # to encourage using them and saving tighter fits for potentially larger items.\n        # The weight (0.1) can be tuned.\n        adaptive_bonus = 0.1 * (fitting_bins_cap - item) / (np.max(fitting_bins_cap) + 1e-9)\n\n        combined_scores = proximity_scores + adaptive_bonus\n\n        priorities[can_fit_mask] = combined_scores\n\n        # Normalize priorities so that the best bin has a score of 1 (or close to it)\n        max_priority = np.max(priorities[can_fit_mask])\n        if max_priority > 0:\n            priorities[can_fit_mask] /= max_priority\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for bins using an adaptive strategy that\n    considers both proximity and potential for future fits.\n    Bins that can fit the item will have a non-zero priority.\n    The priority is higher for bins that are a tighter fit, but also\n    gives a slight bonus to bins that leave more remaining capacity,\n    aiming to balance immediate packing efficiency with future flexibility.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_cap.size > 0:\n        differences = fitting_bins_cap - item\n        # Base proximity score: higher for tighter fits\n        proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Adaptive bonus: encourage leaving more capacity if the fit is not perfect\n        # This bonus is higher for bins that are not a perfect fit,\n        # to encourage using them and saving tighter fits for potentially larger items.\n        # The weight (0.1) can be tuned.\n        adaptive_bonus = 0.1 * (fitting_bins_cap - item) / (np.max(fitting_bins_cap) + 1e-9)\n\n        combined_scores = proximity_scores + adaptive_bonus\n\n        priorities[can_fit_mask] = combined_scores\n\n        # Normalize priorities so that the best bin has a score of 1 (or close to it)\n        max_priority = np.max(priorities[can_fit_mask])\n        if max_priority > 0:\n            priorities[can_fit_mask] /= max_priority\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by favoring those that leave minimal remaining capacity after packing,\n    with an adaptive sigmoid scaling to emphasize better fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    valid_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(valid_bins_mask):\n        return priorities\n\n    remaining_caps_for_valid_bins = bins_remain_cap[valid_bins_mask]\n    diffs = remaining_caps_for_valid_bins - item\n    \n    # Using a scaled inverse difference for a preference towards tighter fits\n    # Add a small epsilon to avoid division by zero if diff is exactly 0\n    inverse_diff_scores = 1.0 / (diffs + 1e-9)\n    \n    # Adaptive sigmoid scaling to emphasize bins with very small differences (better fits)\n    # This part is inspired by v1's idea of using sigmoid for better differentiation\n    # We center the sigmoid around the median difference to adapt to the current state\n    median_diff = np.median(diffs)\n    k = 10.0 # Sensitivity parameter for sigmoid\n    sigmoid_scores = 1 / (1 + np.exp(-k * (diffs - median_diff)))\n    \n    # Combine inverse difference for general preference and sigmoid for fine-tuning\n    # The sigmoid amplifies the preference for bins closer to the median difference\n    priorities[valid_bins_mask] = inverse_diff_scores * sigmoid_scores\n    \n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for bins using an adaptive strategy that considers both proximity and bin fullness.\n    Bins that can fit the item are prioritized. Among fitting bins, those with remaining capacity closer to the item size\n    (but not too small) are preferred. Additionally, a penalty is introduced for using bins that are already very full\n    to encourage spreading items, and a bonus for using bins that are less full to make use of existing capacity.\n\n    The priority is calculated as:\n    priority = (proximity_score * proximity_weight) + (fullness_bonus * fullness_weight)\n\n    proximity_score: Higher for bins where remaining capacity is close to item size.\n                     A small offset is added to avoid division by zero.\n    fullness_bonus: A bonus for bins that have more remaining capacity.\n\n    Args:\n        item (float): The size of the item to be packed.\n        bins_remain_cap (np.ndarray): An array of the remaining capacities of all bins.\n\n    Returns:\n        np.ndarray: An array of priority scores for each bin. Bins that cannot fit the item have a score of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_cap.size > 0:\n        # Proximity Score: Inverse of (difference + epsilon) to favor closer fits.\n        # Add a small buffer (e.g., 0.1 * item) to avoid extremely small differences\n        # that might lead to disproportionately high priorities.\n        differences = fitting_bins_cap - item\n        proximity_score = 1.0 / (differences + 0.1 * item + 1e-9)\n\n        # Fullness Bonus: Reward bins with more remaining capacity.\n        # Normalize remaining capacity to a [0, 1] range based on the maximum possible remaining capacity\n        # which can be considered the bin capacity (assuming bins start empty).\n        # For simplicity, let's consider the maximum remaining capacity among fitting bins as a reference.\n        max_fitting_cap = np.max(fitting_bins_cap)\n        fullness_bonus = fitting_bins_cap / (max_fitting_cap + 1e-9)\n\n        # Weights for balancing proximity and fullness\n        proximity_weight = 0.7\n        fullness_weight = 0.3\n\n        # Combine scores\n        combined_priorities = (proximity_score * proximity_weight) + (fullness_bonus * fullness_weight)\n\n        # Normalize to [0, 1] based on the maximum combined priority\n        max_combined_priority = np.max(combined_priorities)\n        if max_combined_priority > 0:\n            priorities[can_fit_mask] = combined_priorities / max_combined_priority\n\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for bins using an adaptive strategy that considers both proximity and bin fullness.\n    Bins that can fit the item are prioritized. Among fitting bins, those with remaining capacity closer to the item size\n    (but not too small) are preferred. Additionally, a penalty is introduced for using bins that are already very full\n    to encourage spreading items, and a bonus for using bins that are less full to make use of existing capacity.\n\n    The priority is calculated as:\n    priority = (proximity_score * proximity_weight) + (fullness_bonus * fullness_weight)\n\n    proximity_score: Higher for bins where remaining capacity is close to item size.\n                     A small offset is added to avoid division by zero.\n    fullness_bonus: A bonus for bins that have more remaining capacity.\n\n    Args:\n        item (float): The size of the item to be packed.\n        bins_remain_cap (np.ndarray): An array of the remaining capacities of all bins.\n\n    Returns:\n        np.ndarray: An array of priority scores for each bin. Bins that cannot fit the item have a score of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_cap.size > 0:\n        # Proximity Score: Inverse of (difference + epsilon) to favor closer fits.\n        # Add a small buffer (e.g., 0.1 * item) to avoid extremely small differences\n        # that might lead to disproportionately high priorities.\n        differences = fitting_bins_cap - item\n        proximity_score = 1.0 / (differences + 0.1 * item + 1e-9)\n\n        # Fullness Bonus: Reward bins with more remaining capacity.\n        # Normalize remaining capacity to a [0, 1] range based on the maximum possible remaining capacity\n        # which can be considered the bin capacity (assuming bins start empty).\n        # For simplicity, let's consider the maximum remaining capacity among fitting bins as a reference.\n        max_fitting_cap = np.max(fitting_bins_cap)\n        fullness_bonus = fitting_bins_cap / (max_fitting_cap + 1e-9)\n\n        # Weights for balancing proximity and fullness\n        proximity_weight = 0.7\n        fullness_weight = 0.3\n\n        # Combine scores\n        combined_priorities = (proximity_score * proximity_weight) + (fullness_bonus * fullness_weight)\n\n        # Normalize to [0, 1] based on the maximum combined priority\n        max_combined_priority = np.max(combined_priorities)\n        if max_combined_priority > 0:\n            priorities[can_fit_mask] = combined_priorities / max_combined_priority\n\n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by favoring those with least remaining capacity after packing,\n    while also considering the likelihood of future fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate the remaining capacity if the item is placed in each bin\n    potential_remaining_cap = bins_remain_cap - item\n    \n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins that can fit the item, assign priority based on remaining capacity\n    # Higher priority for bins with less remaining capacity (tightest fit)\n    # Add a small epsilon to avoid division by zero\n    priorities[can_fit_mask] = 1.0 / (potential_remaining_cap[can_fit_mask] + 1e-9)\n    \n    # Further adjust priorities: bins that leave more remaining capacity\n    # after packing might be preferred if they are large enough to fit\n    # future, potentially larger items. This is a simple form of adaptive\n    # prioritization, favoring slightly less tight fits that retain more 'room'.\n    # We multiply by the potential remaining capacity itself.\n    # This is a heuristic that balances tight fits with future flexibility.\n    priorities[can_fit_mask] *= potential_remaining_cap[can_fit_mask]\n\n    # Normalize priorities to avoid extremely large values and ensure a better distribution\n    # This makes the heuristic less sensitive to extreme differences in remaining capacity.\n    if np.any(priorities):\n        priorities /= np.max(priorities)\n        \n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by favoring those with least remaining capacity after packing,\n    while also considering the likelihood of future fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate the remaining capacity if the item is placed in each bin\n    potential_remaining_cap = bins_remain_cap - item\n    \n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins that can fit the item, assign priority based on remaining capacity\n    # Higher priority for bins with less remaining capacity (tightest fit)\n    # Add a small epsilon to avoid division by zero\n    priorities[can_fit_mask] = 1.0 / (potential_remaining_cap[can_fit_mask] + 1e-9)\n    \n    # Further adjust priorities: bins that leave more remaining capacity\n    # after packing might be preferred if they are large enough to fit\n    # future, potentially larger items. This is a simple form of adaptive\n    # prioritization, favoring slightly less tight fits that retain more 'room'.\n    # We multiply by the potential remaining capacity itself.\n    # This is a heuristic that balances tight fits with future flexibility.\n    priorities[can_fit_mask] *= potential_remaining_cap[can_fit_mask]\n\n    # Normalize priorities to avoid extremely large values and ensure a better distribution\n    # This makes the heuristic less sensitive to extreme differences in remaining capacity.\n    if np.any(priorities):\n        priorities /= np.max(priorities)\n        \n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by favoring those with least remaining capacity after packing,\n    while also considering the likelihood of future fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate the remaining capacity if the item is placed in each bin\n    potential_remaining_cap = bins_remain_cap - item\n    \n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins that can fit the item, assign priority based on remaining capacity\n    # Higher priority for bins with less remaining capacity (tightest fit)\n    # Add a small epsilon to avoid division by zero\n    priorities[can_fit_mask] = 1.0 / (potential_remaining_cap[can_fit_mask] + 1e-9)\n    \n    # Further adjust priorities: bins that leave more remaining capacity\n    # after packing might be preferred if they are large enough to fit\n    # future, potentially larger items. This is a simple form of adaptive\n    # prioritization, favoring slightly less tight fits that retain more 'room'.\n    # We multiply by the potential remaining capacity itself.\n    # This is a heuristic that balances tight fits with future flexibility.\n    priorities[can_fit_mask] *= potential_remaining_cap[can_fit_mask]\n\n    # Normalize priorities to avoid extremely large values and ensure a better distribution\n    # This makes the heuristic less sensitive to extreme differences in remaining capacity.\n    if np.any(priorities):\n        priorities /= np.max(priorities)\n        \n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}