```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    fittable_bins_mask = bins_remain_cap >= item

    if not np.any(fittable_bins_mask):
        return priorities

    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]

    # Score 1: Best Fit - Prioritize bins that leave minimal remaining capacity after packing.
    # This aims for tighter fits. Adding epsilon for numerical stability.
    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-6)

    # Score 2: Worst Fit - Prioritize bins with the most remaining capacity among fittable ones.
    # This attempts to keep smaller gaps for future smaller items, a form of "spreading" the load.
    # Normalized by the maximum remaining capacity among fittable bins to give a relative measure.
    max_rem_cap_fittable = np.max(fittable_bins_remain_cap)
    worst_fit_score = (fittable_bins_remain_cap - item) / (max_rem_cap_fittable + 1e-6)

    # Score 3: Bin Fullness (Inverse Remaining Capacity) - Reward bins that are already more full.
    # This encourages consolidating items into fewer bins.
    # Using the inverse of remaining capacity among fittable bins.
    fullness_score = (max_rem_cap_fittable - fittable_bins_remain_cap) / (max_rem_cap_fittable + 1e-6)

    # Score 4: Fit Tightness Penalty - Penalize bins that leave a "medium" amount of space.
    # This is a form of "avoiding the middle ground" by discouraging fits that are neither very tight nor very loose.
    # We use a Gaussian-like function centered around a "target" leftover capacity.
    # Let's define a target leftover capacity as a small fraction of the bin capacity (e.g., 10%).
    # For simplicity and generality across different bin sizes, we can use a relative leftover capacity.
    # If item is large, we might want to fit it tightly. If item is small, we might want to fill bins more.
    # For now, let's consider a residual capacity that is still significant but not excessive.
    # We'll penalize capacities that leave, say, 20-50% of remaining capacity.
    # A simple approach is to use a sigmoid-like penalty.
    # For simplicity, let's focus on penalizing larger residual capacities for now in a non-linear way.
    residual_capacity = fittable_bins_remain_cap - item
    # A simple penalty that increases as residual capacity increases, but less steeply for very small residuals.
    # We can use a power function or log, but let's try a simple approach with a sigmoid-like effect.
    # Penalize larger residuals more heavily.
    fit_tightness_penalty = np.exp(-residual_capacity / np.mean(fittable_bins_remain_cap) * 2) # Exponential decay, higher value means less penalty (better)

    # Combine scores with adaptive weights based on the item size relative to the bin capacity.
    # If the item is large (e.g., > 50% of bin capacity), prioritize tight fits (Best Fit).
    # If the item is small, prioritize spreading (Worst Fit) and fullness (Fullness Score).
    bin_capacity_estimate = np.mean(fittable_bins_remain_cap) + item # Estimated capacity of bins that can fit the item
    relative_item_size = item / bin_capacity_estimate

    weight_best_fit = 0.5 + 0.5 * relative_item_size  # Higher weight for larger items
    weight_worst_fit = 0.5 - 0.5 * relative_item_size # Higher weight for smaller items
    weight_fullness = 0.3 # A moderate weight to encourage fuller bins
    weight_tightness_penalty = 0.2 # A small weight to slightly penalize large gaps, but we are already using fit_tightness_score

    # Re-evaluating the fit_tightness_penalty concept to be a preference rather than penalty.
    # Let's call it "Space Utilization Preference".
    # We want to prefer bins where the residual capacity after fitting the item is "reasonable".
    # A residual capacity that is too large is bad. A residual capacity that is too small is also potentially bad if it leads to fragmentation.
    # Let's create a score that is high for moderate residuals and low for extreme residuals.
    # Using a quadratic or Gaussian-like function peaking at a certain residual.
    # Target residual might be related to the item size itself. e.g., fitting item 'i' and leaving 'i' space.
    # For now, let's simplify and focus on discouraging very large remaining capacities.
    # A simple inversion of remaining capacity but capped at some point could work.
    # Let's use a score that favors smaller residuals.
    space_utilization_score = 1.0 / (residual_capacity + 1e-6)

    # Combine scores:
    # Best Fit: Higher is better (tighter fit)
    # Worst Fit: Higher is better (more remaining capacity)
    # Fullness Score: Higher is better (already fuller bins)
    # Space Utilization Score: Higher is better (smaller residual capacity)

    # Let's try to combine these in a way that reflects different strategies.
    # The current version has `best_fit_scores` and `fullness_bonus` and `adaptive_bonus`.
    # My v2 tries to balance Best Fit and Worst Fit with Fullness.

    # A combined score that balances the desire for tight fits (best_fit_score)
    # with the desire to leave more space for potentially smaller items (worst_fit_score),
    # and also rewards bins that are already somewhat full (fullness_score).

    # Let's try a score that is a weighted sum of these.
    # We want to prioritize bins that are tight fits AND are already relatively full.
    # A synergistic approach might be:
    # Prefer tight fits (best_fit_score)
    # BUT, if multiple bins offer tight fits, prefer the one that is already more full (fullness_score).
    # And, consider the worst-fit aspect to avoid leaving very little space in some bins.

    # Let's try a primary score from Best Fit, and a secondary bonus from Fullness.
    # The worst fit idea is important, so maybe it should be a separate consideration.

    # Redefining approach:
    # Score 1: Best Fit - essential for efficiency.
    # Score 2: Fullness - important for reducing bin count.
    # Score 3: A "Balance" score - tries to keep remaining capacities relatively balanced.
    #   This could be achieved by penalizing bins that are too full or too empty relative to the average.
    #   Let's try a score that rewards bins whose remaining capacity after fit is close to the average remaining capacity of fittable bins.

    # Re-evaluating: let's focus on a single cohesive heuristic.
    # The core trade-off is between immediate (tight) fit and future flexibility (leaving space).
    # A good heuristic should balance these.

    # Let's try a modified Best Fit approach with a bonus for fullness.
    # The key is how to combine them.

    # For `priority_v2`, let's use a score that:
    # 1. Strongly prefers tight fits (Best Fit).
    # 2. Provides a bonus for bins that are already quite full (Fullness).
    # 3.  Introduces a "cohesion" bonus: incentivizes packing into bins that are *already* moderately filled.
    #    This means we don't just want the *most* full bin, but a bin that is "comfortably" full.

    # Score 1: Best Fit (primary driver)
    # Higher value for smaller (fittable_bins_remain_cap - item)
    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-6)

    # Score 2: Fullness Bonus (secondary driver)
    # Reward bins that are already more full.
    # Using inverse of remaining capacity.
    fullness_score = (max_rem_cap_fittable - fittable_bins_remain_cap) / (max_rem_cap_fittable + 1e-6)

    # Score 3: Cohesion Bonus (tertiary driver)
    # This score aims to encourage packing into bins that have a "good" amount of remaining capacity.
    # A bin that is almost empty might not be ideal, nor is a bin that is almost full (where best fit already covers it).
    # We want to reward bins where `fittable_bins_remain_cap - item` is not too small and not too large.
    # Let's define a "target residual capacity". A simple heuristic is that the residual capacity
    # should be proportional to the item size itself, perhaps 0.5 to 1.5 times the item size.
    # Or, we can penalize extreme remaining capacities.
    # Let's try a score that is high for medium residual capacities and low for very small or very large.
    # A Gaussian-like function centered on a "desirable" residual.
    # For simplicity, let's try to penalize very large residual capacities.
    # And, perhaps, slightly penalize very small residual capacities (if not covered by best_fit).
    # Let's try a score that is high for moderate residual capacity (e.g., 20-50% of bin capacity).
    # Instead of a complex function, let's try a simpler idea:
    # Reward bins where the *ratio* of remaining capacity to item size is in a sweet spot.
    # e.g., (fittable_bins_remain_cap - item) / item.
    # A ratio around 0.5 to 1.5 might be good.
    # Let's define a score that peaks when (fittable_bins_remain_cap - item) is moderate.
    # The ideal residual is perhaps `item` itself, meaning the bin is filled by 50%.
    # Let `target_residual = item`.
    # Let `actual_residual = fittable_bins_remain_cap - item`.
    # We want to maximize `exp(- (actual_residual - target_residual)^2 / sigma^2)`.
    # For simplicity, let's use a score that is inversely proportional to residual capacity squared.
    # This will strongly penalize large residuals.
    # We need to make sure this doesn't conflict too much with Best Fit.

    # Let's try a simpler "cohesion" idea: encourage packing into bins that are not *too* empty.
    # This is somewhat captured by fullness, but let's make it more direct:
    # Score 3: Minimum Residual Capacity Bonus.
    # Prefer bins that, after packing, still have a reasonable minimum remaining capacity.
    # This encourages not leaving bins "almost full" but with tiny unusable gaps.
    # This is somewhat counter-intuitive for BPP, usually we want tight fits.
    # Let's reconsider the "avoiding the middle ground" idea.
    # We want to avoid bins that are too full AND bins that are too empty relative to potential future items.

    # New approach for v2: Prioritize bins that are a good fit AND are already relatively full,
    # with a penalty for leaving excessive remaining space.

    # Score 1: Best Fit Ratio (tighter fits are better)
    # Higher score for smaller residual capacity
    best_fit_ratio = 1.0 / (fittable_bins_remain_cap - item + 1e-6)

    # Score 2: Fullness Ratio (bins that are already more full are better)
    # Higher score for bins with less remaining capacity.
    fullness_ratio = 1.0 / (fittable_bins_remain_cap + 1e-6)

    # Score 3: Fit Penalty (discourage leaving very large gaps)
    # Exponential penalty for residual capacity. This penalizes leaving large amounts of space.
    # Normalize residual capacity by a typical bin size or max remaining capacity.
    # Let's use the average remaining capacity among fittable bins as a reference.
    avg_rem_cap_fittable = np.mean(fittable_bins_remain_cap)
    fit_penalty = np.exp(- (fittable_bins_remain_cap - item) / (avg_rem_cap_fittable + 1e-6) * 1.0) # Exponential decay


    # Combine scores with weights that can be tuned.
    # The goal is to balance tight fits with fuller bins, while penalizing over-filling.
    # Best fit is usually a strong indicator. Fullness is good for reducing bin count.
    # The penalty term helps prevent leaving too much wasted space, which can be detrimental.

    # Let's try a weighted sum:
    # bf_weight: how much we prioritize tight fits.
    # f_weight: how much we prioritize already fuller bins.
    # p_weight: how much we penalize leaving large gaps.

    # Adaptive weighting based on item size:
    # If item is large, tight fit (bf) is crucial. Fullness (f) is still good. Penalty (p) is less critical than for small items.
    # If item is small, fullness (f) becomes more important to consolidate. Tight fit (bf) is less critical, but still good. Penalty (p) is more important to avoid fragmentation.

    # Let's simplify weights:
    bf_weight = 1.0
    f_weight = 0.7
    p_weight = 0.4

    combined_scores = (bf_weight * best_fit_ratio) + (f_weight * fullness_ratio) + (p_weight * fit_penalty)

    # Normalize scores to [0, 1] for better interpretability and to prevent large values from dominating.
    max_score = np.max(combined_scores)
    if max_score > 1e-9:
        priorities[fittable_bins_mask] = combined_scores / max_score
    else:
        priorities[fittable_bins_mask] = 0.1 # Fallback to a small uniform priority if all scores are near zero

    return priorities
```
