[
  {
    "stdout_filepath": "problem_iter8_response0.txt_stdout.txt",
    "code_path": "problem_iter8_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a \"fill ratio\" bonus. Prioritizes bins that fit the item snugly\n    and are already relatively full, aiming for denser packing.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    eligible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(eligible_bins_mask):\n        return priorities\n\n    eligible_bins_caps = bins_remain_cap[eligible_bins_mask]\n\n    # Score 1: Best Fit - favors bins with minimal remaining capacity after packing.\n    best_fit_scores = 1.0 / (eligible_bins_caps - item + 1e-9)\n\n    # Score 2: Fill Ratio - favors bins that are already more full.\n    # Using the ratio of current capacity to maximum possible capacity for fitting bins.\n    max_eligible_cap = np.max(eligible_bins_caps)\n    fill_ratio_scores = eligible_bins_caps / (max_eligible_cap + 1e-9)\n\n    # Combine scores multiplicatively: prioritize bins that are both a good fit and already full.\n    combined_scores = best_fit_scores * fill_ratio_scores\n\n    # Assign combined scores to the priorities array\n    priorities[eligible_bins_mask] = combined_scores\n\n    # Normalize priorities to the range [0, 1] for consistent comparison.\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response1.txt_stdout.txt",
    "code_path": "problem_iter8_code1.py",
    "code": null,
    "response_id": 1,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Invalid response!"
  },
  {
    "stdout_filepath": "problem_iter8_response2.txt_stdout.txt",
    "code_path": "problem_iter8_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for very small remaining capacity,\n    favoring tight fits while slightly discouraging bins that become overly full.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fitting_bins_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Best Fit Score: Inverse of remaining capacity after placement.\n    # Higher score for bins that leave less capacity (tighter fit).\n    best_fit_scores = 1.0 / (fitting_bins_cap - item + 1e-9)\n\n    # Adaptive Penalty for Small Residuals: Penalize bins that leave a very small remainder.\n    # This aims to avoid situations where a bin is *too* full, potentially leaving no room\n    # for even slightly larger items later. We use an inverse relationship with the remainder.\n    # A small remainder (e.g., 0.1) gets a lower penalty score (e.g., 0.1 / (0.1 + 0.5) ~ 0.16),\n    # while a larger remainder (e.g., 10) gets a higher penalty score (e.g., 10 / (10 + 0.5) ~ 0.95).\n    # We want to *discourage* very small remainders, so we'll use this score to adjust the best_fit_scores.\n    # Specifically, we'll multiply the best_fit_scores by a factor that decreases as the remainder gets smaller.\n    # Let's invert this penalty concept: a *good* residual quality score should be *high* for moderate remainders.\n    # Instead of penalty, let's frame it as a \"residual quality bonus\" where small residuals are penalized.\n    # A simple penalty function for small residuals: exp(-residual / sensitivity)\n    # Where sensitivity is a parameter controlling how quickly the penalty kicks in.\n    sensitivity = 2.0  # Controls how strongly small remainders are penalized.\n    residual_quality_factor = np.exp(-(fitting_bins_cap - item) / sensitivity)\n\n\n    # Combine scores: Multiply Best Fit by the residual quality factor.\n    # This prioritizes tight fits (high best_fit_scores) but reduces their priority\n    # if they leave an extremely small remainder (low residual_quality_factor).\n    combined_scores = best_fit_scores * residual_quality_factor\n\n    # Normalize combined scores to [0, 1] to make them comparable across different calls.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response3.txt_stdout.txt",
    "code_path": "problem_iter8_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best-Fit proximity with a fill ratio bonus, prioritizing tight fits\n    and the utilization of already fuller bins for better overall packing.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    differences = fitting_bins_cap - item\n\n    # Best-Fit component: Higher score for smaller remaining capacity after fitting\n    # Add a small epsilon to prevent division by zero for perfect fits.\n    best_fit_score = 1.0 / (differences + 1e-9)\n\n    # Fill Ratio component: Bonus for bins that are already more utilized.\n    # Use the inverse of remaining capacity as a proxy for fullness relative to bin size.\n    # Normalize by the maximum remaining capacity to get a relative fullness score.\n    max_total_capacity = np.max(bins_remain_cap) # Assuming all bins have same max capacity\n    fill_ratio_bonus = fitting_bins_cap / (max_total_capacity + 1e-9)\n\n    # Combine scores multiplicatively, giving weight to both tight fit and fullness.\n    # The addition of 1 to fill_ratio_bonus ensures that even less full bins\n    # contribute positively to the score, preventing zeroing out the best_fit_score.\n    combined_scores = best_fit_score * (1 + fill_ratio_bonus * 0.7) # Tunable weight for bonus\n\n    priorities[can_fit_mask] = combined_scores\n\n    # Normalize priorities to ensure the highest score is 1.0 for consistent selection.\n    max_priority = np.max(priorities[can_fit_mask])\n    if max_priority > 0:\n        priorities[can_fit_mask] /= max_priority\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response4.txt_stdout.txt",
    "code_path": "problem_iter8_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (proximity) with an adaptive bonus for bins that\n    leave a smaller residual capacity after packing, encouraging tighter fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    \n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    \n    if fitting_bins_cap.size > 0:\n        differences = fitting_bins_cap - item\n        # Base score: Inverse of residual capacity (Best Fit)\n        proximity_score = 1.0 / (differences + 1e-9)\n        \n        # Adaptive bonus: Penalize larger residuals more heavily (e.g., squared difference)\n        # This encourages bins where the difference is small.\n        adaptive_bonus = 1.0 / (differences**2 + 1e-9)\n        \n        # Combine scores: Multiplicative combination. Prioritize bins that are\n        # both good fits and have small residuals.\n        combined_score = proximity_score * adaptive_bonus\n        \n        # Normalize to ensure the best bin gets a score of 1.0\n        max_score = np.max(combined_score)\n        if max_score > 0:\n            priorities[can_fit_mask] = combined_score / max_score\n            \n        # Ensure that even if the adaptive bonus causes issues with very small differences,\n        # the proximity score still plays a role. This is a safeguard.\n        # If the combined score is still very low or zero for a fitting bin,\n        # boost it with its proximity score if it's higher.\n        priorities[can_fit_mask] = np.maximum(priorities[can_fit_mask], proximity_score / np.max(proximity_score + 1e-9))\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response5.txt_stdout.txt",
    "code_path": "problem_iter8_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a fill-ratio bonus, favoring bins that are both a good fit and already utilized.\n    This heuristic prioritizes bins that are nearly full and leave minimal residual capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Filter bins that can fit the item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Calculate the remaining capacity after fitting the item\n    remaining_capacity_after_fit = fitting_bins_remain_cap - item\n    \n    # --- Best Fit Component ---\n    # Score is inversely proportional to the residual capacity after fitting.\n    # Adding a small epsilon to avoid division by zero.\n    # A score of 1.0 means the bin is perfectly filled after adding the item.\n    proximity_score = 1.0 / (remaining_capacity_after_fit + 1e-9)\n    \n    # --- Fill Ratio Component ---\n    # Assumes a default bin capacity of 1.0.\n    # Fill ratio is the proportion of capacity already used. Higher is better.\n    bin_capacity = 1.0 \n    current_fill_ratio = (bin_capacity - fitting_bins_remain_cap) / bin_capacity\n    fill_ratio_score = current_fill_ratio\n    \n    # --- Combined Score ---\n    # Multiply proximity score by fill ratio score.\n    # This prioritizes bins that are both a good fit (high proximity) AND already well-utilized (high fill ratio).\n    # The idea is to prefer bins that are already \"almost full\" and can still accommodate the item snugly.\n    combined_score = proximity_score * fill_ratio_score\n    \n    # --- Refinement: Ensure proximity is considered for empty bins ---\n    # If a bin was empty (fill_ratio=0), combined_score would be 0.\n    # We want to ensure that even in this case, the proximity score is still considered,\n    # as an empty bin might be the only option or a good first fit.\n    # We take the maximum of the combined score and the proximity score itself,\n    # effectively giving proximity score at least its due when fill ratio is zero.\n    priorities[can_fit_mask] = np.maximum(combined_score, proximity_score * (fill_ratio_score > 1e-9))\n    \n    # Add a small epsilon to all valid priorities to ensure that even if\n    # all scores are very low, they are distinct and positive, aiding tie-breaking.\n    priorities[can_fit_mask] += 1e-6\n    \n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.487435181491823,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response6.txt_stdout.txt",
    "code_path": "problem_iter8_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit and a utility bonus for moderate remaining capacity.\n    Favors bins that are a tight fit but also leave some usable space.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    potential_remaining_cap = bins_remain_cap[can_fit_mask] - item\n\n    # Best Fit component: favors bins that leave minimal remaining capacity.\n    # Higher score for smaller remaining capacity.\n    best_fit_score = 1.0 / (potential_remaining_cap + 1e-9)\n\n    # Utility component: favors bins that leave a moderate amount of capacity.\n    # This is a simplified Gaussian-like bonus, centered around item/2.\n    # It rewards bins that aren't too tight or too loose.\n    moderate_capacity_center = item / 2.0\n    utility_bonus = np.exp(-((potential_remaining_cap - moderate_capacity_center)**2) / (2 * (moderate_capacity_center**2 + 1e-9)))\n\n    # Combine scores multiplicatively: A bin must be good for both components.\n    # This balances the 'tightest fit' with the 'most useful residual space'.\n    combined_score = best_fit_score * utility_bonus\n\n    # Assign priorities to the valid bins.\n    priorities[can_fit_mask] = combined_score\n\n    # Normalize priorities to ensure relative ranking and prevent extreme values.\n    if np.any(priorities):\n        max_priority = np.max(priorities)\n        if max_priority > 1e-9:\n            priorities[can_fit_mask] /= max_priority\n        else:\n            # If all resulting priorities are near zero, assign a small uniform priority.\n            priorities[can_fit_mask] = 1e-6\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response7.txt_stdout.txt",
    "code_path": "problem_iter8_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a bonus for already full bins, using a logarithmic bonus\n    to balance tight fits with encouraging fuller bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Best Fit Score: Higher for bins that leave less remaining capacity.\n    # Adding a small epsilon to avoid division by zero.\n    best_fit_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Fullness Bonus: A bonus for bins that are already more full.\n    # Using the inverse of remaining capacity on fittable bins.\n    # Normalize by the maximum remaining capacity among fittable bins to get a relative measure.\n    max_remaining_cap_fittable = np.max(fittable_bins_remain_cap)\n    fullness_bonus = (max_remaining_cap_fittable - fittable_bins_remain_cap) / (max_remaining_cap_fittable + 1e-6)\n\n    # Adaptive Bonus using logarithm of remaining capacity after fit.\n    # This penalizes leaving excessively large gaps but gives smaller penalties for smaller gaps.\n    # Adding 1 to prevent log(0) and ensure positive values.\n    adaptive_bonus = np.log1p(fittable_bins_remain_cap - item)\n\n    # Combine: Weighted sum of Best Fit and Fullness Bonus, with Adaptive Bonus as a modifier.\n    # Weights are heuristic and can be tuned. Here, Best Fit is primary, Fullness adds context,\n    # and the Adaptive Bonus influences the penalty for leftover space.\n    # We invert the adaptive bonus as smaller leftover space (lower log) should be better.\n    combined_scores = (best_fit_scores * 1.0) + (fullness_bonus * 0.5) - (adaptive_bonus * 0.2)\n\n    # Normalize priorities to a [0, 1] range for better comparability and to avoid extreme values.\n    if np.max(combined_scores) > 1e-9:\n        priorities[fittable_bins_mask] = np.clip(combined_scores / np.max(combined_scores), 0, 1)\n    else:\n        # If all scores are near zero, assign a small uniform priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 3.9788591942560925,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response8.txt_stdout.txt",
    "code_path": "problem_iter8_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit preference with a bonus for bins that are already significantly filled,\n    aiming for efficient space utilization and minimizing the number of bins used.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    \n    if fitting_bins_cap.size > 0:\n        differences = fitting_bins_cap - item\n        \n        # Best Fit component: Higher score for smaller remaining capacity\n        best_fit_score = 1.0 / (differences + 1e-9)\n        \n        # Fullness bonus: Reward bins that are already more utilized (closer to full)\n        # This is a simplified version of the fullness bonus from v0,\n        # focusing on the absolute remaining capacity as a proxy for fullness.\n        # Bins with less remaining capacity are considered \"more full\".\n        fullness_bonus = (np.max(bins_remain_cap) - fitting_bins_cap) / (np.max(bins_remain_cap) + 1e-9)\n        \n        # Combine scores: Additive combination. Prioritize tight fits (Best Fit)\n        # and give a boost to bins that are already quite full.\n        combined_scores = best_fit_score + fullness_bonus * 0.5 # Tunable weight for bonus\n        \n        priorities[can_fit_mask] = combined_scores\n        \n        # Normalize priorities for bins that can fit the item\n        max_priority = np.max(priorities[can_fit_mask])\n        if max_priority > 0:\n            priorities[can_fit_mask] /= max_priority\n            \n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response9.txt_stdout.txt",
    "code_path": "problem_iter8_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit proximity with an adaptive bonus for moderate remaining capacity,\n    favoring bins that are a close fit but also leave some useful space.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_cap.size > 0:\n        # Best Fit proximity: Score higher for bins with smaller remaining capacity after fitting.\n        differences = fitting_bins_cap - item\n        proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Adaptive \"Usefulness\" Bonus: Reward bins that leave a moderate amount of space.\n        # A logarithmic function provides diminishing returns, preventing excessively large remainders\n        # from dominating the score, while still giving a bonus for leaving more space.\n        # This encourages leaving space for future items without being overly wasteful.\n        resulting_remainders = fitting_bins_cap - item\n        \n        # We use log(1 + remainder) so that even empty bins (remainder 0) get a small base bonus,\n        # and the bonus increases with remaining space, but at a decreasing rate.\n        # A small constant is added inside log to prevent log(0) if resulting_remainders is 0.\n        adaptive_bonus = np.log(1 + resulting_remainders + 1e-9)\n        \n        # Combine proximity and adaptive bonus.\n        # The proximity score drives the heuristic towards the tightest fit.\n        # The adaptive bonus slightly counteracts pure greediness by rewarding bins\n        # that leave more space, up to a point.\n        # A multiplicative combination ensures that a good fit is still paramount,\n        # but the bonus can differentiate between equally good fits.\n        combined_scores = proximity_scores * (1 + 0.2 * adaptive_bonus) # 0.2 is a tunable parameter\n\n        # Normalize scores for the bins that can fit the item.\n        # Ensures the highest priority bin gets a score of 1.0, making priorities relative.\n        max_score = np.max(combined_scores)\n        if max_score > 0:\n            priorities[can_fit_mask] = combined_scores / max_score\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response0.txt_stdout.txt",
    "code_path": "problem_iter9_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Score 1: Best Fit - Prioritize bins that leave minimal remaining capacity after packing.\n    # This aims for tighter fits. Adding epsilon for numerical stability.\n    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Score 2: Worst Fit - Prioritize bins with the most remaining capacity among fittable ones.\n    # This attempts to keep smaller gaps for future smaller items, a form of \"spreading\" the load.\n    # Normalized by the maximum remaining capacity among fittable bins to give a relative measure.\n    max_rem_cap_fittable = np.max(fittable_bins_remain_cap)\n    worst_fit_score = (fittable_bins_remain_cap - item) / (max_rem_cap_fittable + 1e-6)\n\n    # Score 3: Bin Fullness (Inverse Remaining Capacity) - Reward bins that are already more full.\n    # This encourages consolidating items into fewer bins.\n    # Using the inverse of remaining capacity among fittable bins.\n    fullness_score = (max_rem_cap_fittable - fittable_bins_remain_cap) / (max_rem_cap_fittable + 1e-6)\n\n    # Score 4: Fit Tightness Penalty - Penalize bins that leave a \"medium\" amount of space.\n    # This is a form of \"avoiding the middle ground\" by discouraging fits that are neither very tight nor very loose.\n    # We use a Gaussian-like function centered around a \"target\" leftover capacity.\n    # Let's define a target leftover capacity as a small fraction of the bin capacity (e.g., 10%).\n    # For simplicity and generality across different bin sizes, we can use a relative leftover capacity.\n    # If item is large, we might want to fit it tightly. If item is small, we might want to fill bins more.\n    # For now, let's consider a residual capacity that is still significant but not excessive.\n    # We'll penalize capacities that leave, say, 20-50% of remaining capacity.\n    # A simple approach is to use a sigmoid-like penalty.\n    # For simplicity, let's focus on penalizing larger residual capacities for now in a non-linear way.\n    residual_capacity = fittable_bins_remain_cap - item\n    # A simple penalty that increases as residual capacity increases, but less steeply for very small residuals.\n    # We can use a power function or log, but let's try a simple approach with a sigmoid-like effect.\n    # Penalize larger residuals more heavily.\n    fit_tightness_penalty = np.exp(-residual_capacity / np.mean(fittable_bins_remain_cap) * 2) # Exponential decay, higher value means less penalty (better)\n\n    # Combine scores with adaptive weights based on the item size relative to the bin capacity.\n    # If the item is large (e.g., > 50% of bin capacity), prioritize tight fits (Best Fit).\n    # If the item is small, prioritize spreading (Worst Fit) and fullness (Fullness Score).\n    bin_capacity_estimate = np.mean(fittable_bins_remain_cap) + item # Estimated capacity of bins that can fit the item\n    relative_item_size = item / bin_capacity_estimate\n\n    weight_best_fit = 0.5 + 0.5 * relative_item_size  # Higher weight for larger items\n    weight_worst_fit = 0.5 - 0.5 * relative_item_size # Higher weight for smaller items\n    weight_fullness = 0.3 # A moderate weight to encourage fuller bins\n    weight_tightness_penalty = 0.2 # A small weight to slightly penalize large gaps, but we are already using fit_tightness_score\n\n    # Re-evaluating the fit_tightness_penalty concept to be a preference rather than penalty.\n    # Let's call it \"Space Utilization Preference\".\n    # We want to prefer bins where the residual capacity after fitting the item is \"reasonable\".\n    # A residual capacity that is too large is bad. A residual capacity that is too small is also potentially bad if it leads to fragmentation.\n    # Let's create a score that is high for moderate residuals and low for extreme residuals.\n    # Using a quadratic or Gaussian-like function peaking at a certain residual.\n    # Target residual might be related to the item size itself. e.g., fitting item 'i' and leaving 'i' space.\n    # For now, let's simplify and focus on discouraging very large remaining capacities.\n    # A simple inversion of remaining capacity but capped at some point could work.\n    # Let's use a score that favors smaller residuals.\n    space_utilization_score = 1.0 / (residual_capacity + 1e-6)\n\n    # Combine scores:\n    # Best Fit: Higher is better (tighter fit)\n    # Worst Fit: Higher is better (more remaining capacity)\n    # Fullness Score: Higher is better (already fuller bins)\n    # Space Utilization Score: Higher is better (smaller residual capacity)\n\n    # Let's try to combine these in a way that reflects different strategies.\n    # The current version has `best_fit_scores` and `fullness_bonus` and `adaptive_bonus`.\n    # My v2 tries to balance Best Fit and Worst Fit with Fullness.\n\n    # A combined score that balances the desire for tight fits (best_fit_score)\n    # with the desire to leave more space for potentially smaller items (worst_fit_score),\n    # and also rewards bins that are already somewhat full (fullness_score).\n\n    # Let's try a score that is a weighted sum of these.\n    # We want to prioritize bins that are tight fits AND are already relatively full.\n    # A synergistic approach might be:\n    # Prefer tight fits (best_fit_score)\n    # BUT, if multiple bins offer tight fits, prefer the one that is already more full (fullness_score).\n    # And, consider the worst-fit aspect to avoid leaving very little space in some bins.\n\n    # Let's try a primary score from Best Fit, and a secondary bonus from Fullness.\n    # The worst fit idea is important, so maybe it should be a separate consideration.\n\n    # Redefining approach:\n    # Score 1: Best Fit - essential for efficiency.\n    # Score 2: Fullness - important for reducing bin count.\n    # Score 3: A \"Balance\" score - tries to keep remaining capacities relatively balanced.\n    #   This could be achieved by penalizing bins that are too full or too empty relative to the average.\n    #   Let's try a score that rewards bins whose remaining capacity after fit is close to the average remaining capacity of fittable bins.\n\n    # Re-evaluating: let's focus on a single cohesive heuristic.\n    # The core trade-off is between immediate (tight) fit and future flexibility (leaving space).\n    # A good heuristic should balance these.\n\n    # Let's try a modified Best Fit approach with a bonus for fullness.\n    # The key is how to combine them.\n\n    # For `priority_v2`, let's use a score that:\n    # 1. Strongly prefers tight fits (Best Fit).\n    # 2. Provides a bonus for bins that are already quite full (Fullness).\n    # 3.  Introduces a \"cohesion\" bonus: incentivizes packing into bins that are *already* moderately filled.\n    #    This means we don't just want the *most* full bin, but a bin that is \"comfortably\" full.\n\n    # Score 1: Best Fit (primary driver)\n    # Higher value for smaller (fittable_bins_remain_cap - item)\n    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Score 2: Fullness Bonus (secondary driver)\n    # Reward bins that are already more full.\n    # Using inverse of remaining capacity.\n    fullness_score = (max_rem_cap_fittable - fittable_bins_remain_cap) / (max_rem_cap_fittable + 1e-6)\n\n    # Score 3: Cohesion Bonus (tertiary driver)\n    # This score aims to encourage packing into bins that have a \"good\" amount of remaining capacity.\n    # A bin that is almost empty might not be ideal, nor is a bin that is almost full (where best fit already covers it).\n    # We want to reward bins where `fittable_bins_remain_cap - item` is not too small and not too large.\n    # Let's define a \"target residual capacity\". A simple heuristic is that the residual capacity\n    # should be proportional to the item size itself, perhaps 0.5 to 1.5 times the item size.\n    # Or, we can penalize extreme remaining capacities.\n    # Let's try a score that is high for medium residual capacities and low for very small or very large.\n    # A Gaussian-like function centered on a \"desirable\" residual.\n    # For simplicity, let's try to penalize very large residual capacities.\n    # And, perhaps, slightly penalize very small residual capacities (if not covered by best_fit).\n    # Let's try a score that is high for moderate residual capacity (e.g., 20-50% of bin capacity).\n    # Instead of a complex function, let's try a simpler idea:\n    # Reward bins where the *ratio* of remaining capacity to item size is in a sweet spot.\n    # e.g., (fittable_bins_remain_cap - item) / item.\n    # A ratio around 0.5 to 1.5 might be good.\n    # Let's define a score that peaks when (fittable_bins_remain_cap - item) is moderate.\n    # The ideal residual is perhaps `item` itself, meaning the bin is filled by 50%.\n    # Let `target_residual = item`.\n    # Let `actual_residual = fittable_bins_remain_cap - item`.\n    # We want to maximize `exp(- (actual_residual - target_residual)^2 / sigma^2)`.\n    # For simplicity, let's use a score that is inversely proportional to residual capacity squared.\n    # This will strongly penalize large residuals.\n    # We need to make sure this doesn't conflict too much with Best Fit.\n\n    # Let's try a simpler \"cohesion\" idea: encourage packing into bins that are not *too* empty.\n    # This is somewhat captured by fullness, but let's make it more direct:\n    # Score 3: Minimum Residual Capacity Bonus.\n    # Prefer bins that, after packing, still have a reasonable minimum remaining capacity.\n    # This encourages not leaving bins \"almost full\" but with tiny unusable gaps.\n    # This is somewhat counter-intuitive for BPP, usually we want tight fits.\n    # Let's reconsider the \"avoiding the middle ground\" idea.\n    # We want to avoid bins that are too full AND bins that are too empty relative to potential future items.\n\n    # New approach for v2: Prioritize bins that are a good fit AND are already relatively full,\n    # with a penalty for leaving excessive remaining space.\n\n    # Score 1: Best Fit Ratio (tighter fits are better)\n    # Higher score for smaller residual capacity\n    best_fit_ratio = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Score 2: Fullness Ratio (bins that are already more full are better)\n    # Higher score for bins with less remaining capacity.\n    fullness_ratio = 1.0 / (fittable_bins_remain_cap + 1e-6)\n\n    # Score 3: Fit Penalty (discourage leaving very large gaps)\n    # Exponential penalty for residual capacity. This penalizes leaving large amounts of space.\n    # Normalize residual capacity by a typical bin size or max remaining capacity.\n    # Let's use the average remaining capacity among fittable bins as a reference.\n    avg_rem_cap_fittable = np.mean(fittable_bins_remain_cap)\n    fit_penalty = np.exp(- (fittable_bins_remain_cap - item) / (avg_rem_cap_fittable + 1e-6) * 1.0) # Exponential decay\n\n\n    # Combine scores with weights that can be tuned.\n    # The goal is to balance tight fits with fuller bins, while penalizing over-filling.\n    # Best fit is usually a strong indicator. Fullness is good for reducing bin count.\n    # The penalty term helps prevent leaving too much wasted space, which can be detrimental.\n\n    # Let's try a weighted sum:\n    # bf_weight: how much we prioritize tight fits.\n    # f_weight: how much we prioritize already fuller bins.\n    # p_weight: how much we penalize leaving large gaps.\n\n    # Adaptive weighting based on item size:\n    # If item is large, tight fit (bf) is crucial. Fullness (f) is still good. Penalty (p) is less critical than for small items.\n    # If item is small, fullness (f) becomes more important to consolidate. Tight fit (bf) is less critical, but still good. Penalty (p) is more important to avoid fragmentation.\n\n    # Let's simplify weights:\n    bf_weight = 1.0\n    f_weight = 0.7\n    p_weight = 0.4\n\n    combined_scores = (bf_weight * best_fit_ratio) + (f_weight * fullness_ratio) + (p_weight * fit_penalty)\n\n    # Normalize scores to [0, 1] for better interpretability and to prevent large values from dominating.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n    else:\n        priorities[fittable_bins_mask] = 0.1 # Fallback to a small uniform priority if all scores are near zero\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 35.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response1.txt_stdout.txt",
    "code_path": "problem_iter9_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n    fittable_bin_indices = np.where(fittable_bins_mask)[0]\n\n    # 1. Best Fit (BF): Prioritize bins that leave the smallest remaining capacity.\n    # This is crucial for minimizing wasted space immediately.\n    best_fit_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-9)\n\n    # 2. First Fit Decreasing (FFD) like idea: Prioritize bins that have been used for larger items.\n    # This is implicitly captured by looking at the remaining capacity, but we can add a bonus\n    # for bins that are *already* significantly filled, as they likely contain larger items\n    # and are good candidates to continue filling.\n    # We use the inverse of remaining capacity as a proxy for fullness.\n    fullness_scores = 1.0 / (fittable_bins_remain_cap + 1e-9)\n\n    # 3. Next Fit Decreasing (NFD) related: Consider the 'tightness' of the fit relative\n    # to the bin's original capacity. While we don't know the original capacity directly\n    # here, we can approximate by assuming bins with larger *remaining* capacity\n    # might have had more space to begin with, and thus a tighter fit might be more valuable.\n    # This is a weaker heuristic but adds another dimension.\n    # Let's consider a penalty for leaving *too much* space.\n    # We want to favor bins where (remaining_cap - item) is small.\n    # log1p is good for diminishing returns on small remaining spaces.\n    # Let's use log of the *original* capacity if we knew it. Since we don't,\n    # we can use the *current* remaining capacity as a proxy for \"how much space is left to play with\".\n    # A bin with a lot of remaining capacity might be better to keep for a large item later.\n    # So, we want to penalize using bins that have lots of remaining capacity if a tighter fit exists.\n    # Let's use the log of the remaining capacity after the fit. A smaller log value is better.\n    # We will invert it so higher values are better.\n    log_remaining_after_fit = np.log1p(fittable_bins_remain_cap - item)\n    space_utilization_scores = 1.0 / (log_remaining_after_fit + 1e-9)\n\n\n    # Combination Strategy:\n    # - BF is the primary driver: we want tight fits.\n    # - Fullness provides a secondary boost: encouraging to fill up bins that are already somewhat full.\n    # - Space Utilization adds a nuanced preference: preferring bins where the remaining space\n    #   after fitting is not excessively large (using the log to moderate this effect).\n\n    # Weights are adjusted to give primacy to Best Fit, with secondary consideration to fullness\n    # and the impact on future packing potential (space_utilization).\n    # We use a slightly more aggressive weighting for BF and a balanced approach for others.\n    combined_scores = (best_fit_scores * 1.5) + (fullness_scores * 0.7) + (space_utilization_scores * 0.5)\n\n    # Normalize scores for the fittable bins to a [0, 1] range.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        normalized_priorities = np.clip(combined_scores / max_score, 0, 1)\n        priorities[fittable_bins_mask] = normalized_priorities\n    else:\n        # If all scores are near zero (e.g., all remaining capacities are very close),\n        # assign a uniform moderate priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.5\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 19.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response2.txt_stdout.txt",
    "code_path": "problem_iter9_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Core Strategy: Prioritize bins that leave minimal remaining capacity (Best Fit).\n    # Use the inverse of remaining capacity after fitting. Add a small epsilon for stability.\n    # This directly targets the objective of minimizing wasted space.\n    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-9)\n\n    # Predictive Component: Consider the \"potential\" of a bin for future items.\n    # A bin with slightly more remaining capacity after fitting a current item might be\n    # more versatile for subsequent items. This is a trade-off: a tighter fit is good now,\n    # but a slightly looser fit might be better overall.\n    # We can model this by giving a bonus to bins that are \"almost full\" but can still fit the item,\n    # prioritizing those that would have a \"just right\" remaining capacity for common item sizes.\n    # A simple approach is to penalize bins that leave a very large gap.\n    # We can use the negative logarithm of the remaining capacity after fit. Smaller remaining capacity is better,\n    # so a smaller value for log1p(remaining_cap) is better. We invert this by multiplying by -1.\n    # Using log1p to handle cases where remaining capacity is 0 gracefully.\n    future_potential_score = -np.log1p(fittable_bins_remain_cap - item)\n\n    # Adaptability: The relative importance of \"tight fit\" vs. \"future potential\" can change.\n    # For instance, if items are generally small, prioritizing slightly larger remaining capacity might be good.\n    # If items are large, tight fits are paramount.\n    # We can introduce a dynamic weighting factor based on the item size relative to the bin capacity (implicitly, the median or average remaining capacity).\n    # For simplicity, let's consider the item size relative to the bin capacity if we knew it, but we only have remaining capacities.\n    # A proxy could be the item size relative to the *average* remaining capacity of fittable bins.\n    # If the item is small relative to available space, future potential might be more important.\n    # If the item is large, fitting it tightly is more important.\n\n    avg_remaining_fittable = np.mean(fittable_bins_remain_cap)\n    item_size_ratio = item / (avg_remaining_fittable + 1e-9)\n\n    # Weighting:\n    # If item_size_ratio is high (item is large relative to average remaining capacity),\n    # prioritize Best Fit more. If low (item is small), give more weight to future potential.\n    # A sigmoid-like function could map item_size_ratio to a weight for Best Fit.\n    # Let's use a simple linear scaling for now.\n\n    # Normalize item_size_ratio to a [0, 1] range (approximate).\n    # A more robust normalization might involve historical data or a fixed large value.\n    # For now, assume item sizes are within a reasonable range compared to bin capacities.\n    # A heuristic upper bound for item_size_ratio could be the bin capacity itself (if items <= bin_cap).\n    # Let's consider the max possible ratio for a single item being 1 if item == bin_cap.\n    # So, max_ratio_considered = 1.0 (for item fitting exactly).\n    # The weight for best_fit_score will increase as item_size_ratio increases.\n    # The weight for future_potential_score will decrease as item_size_ratio increases.\n\n    # Let's use a sigmoid-like weighting for Best Fit: w_bf = 1 / (1 + exp(-k * (item_size_ratio - threshold)))\n    # A simpler approach: linear interpolation.\n    # If item_size_ratio is small (e.g., 0.1), bf_weight=0.2, fp_weight=0.8\n    # If item_size_ratio is large (e.g., 1.0), bf_weight=1.0, fp_weight=0.0\n    # We can use an exponential decay for future potential weight.\n    bf_weight = 1.0 - np.exp(-item_size_ratio * 2.0)  # As item_size_ratio grows, bf_weight approaches 1\n    fp_weight = np.exp(-item_size_ratio * 2.0)       # As item_size_ratio grows, fp_weight approaches 0\n\n    # Combine scores with adaptive weights\n    combined_scores = (best_fit_score * bf_weight) + (future_potential_score * fp_weight)\n\n    # Normalize priorities to a [0, 1] range.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n    else:\n        # If all scores are effectively zero, assign a small uniform priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.148384523334677,
    "SLOC": 19.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response3.txt_stdout.txt",
    "code_path": "problem_iter9_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Best Fit Score: Prioritize bins with minimal remaining capacity after fitting.\n    # Added epsilon for numerical stability.\n    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-9)\n\n    # Remaining Capacity Score: Prioritize bins with larger remaining capacity if they are not too large.\n    # This aims to keep bins with significant space available for larger future items.\n    # We use the inverse of the squared remaining capacity to penalize very large gaps more.\n    # Capped to prevent extreme values and to allow for a smoother transition.\n    remaining_capacity_score = 1.0 / np.square(fittable_bins_remain_cap + 1e-9)\n    \n    # Novelty Score: Give a slight preference to bins that have not been used recently or are less utilized.\n    # This helps in spreading items across bins rather than concentrating them.\n    # For an online setting without explicit bin usage history, we can proxy this with\n    # the *inverse* of the bin's current remaining capacity, effectively favoring less full bins\n    # that can still fit the item. This is a subtle shift from the \"fullness bonus\" to\n    # a \"spread preference\".\n    spread_preference_score = fittable_bins_remain_cap\n\n    # Combine scores with adaptive weights.\n    # Weights are adjusted based on the item size relative to bin capacity (assuming a standard bin size).\n    # This is a simplification; a more robust approach would involve the actual bin capacity.\n    # For demonstration, let's assume a notional bin capacity of 1.0.\n    notional_bin_capacity = 1.0\n    item_size_ratio = item / notional_bin_capacity\n\n    # Weights can be tuned. Here we give a strong preference to Best Fit,\n    # a moderate preference to spreading items, and a lesser preference to keeping capacity.\n    # The 'spread_preference_score' is inverted here because a higher value means more remaining capacity,\n    # which is what we want to prioritize for spreading, BUT we are applying it as a score,\n    # so we want to emphasize bins that are less full.\n    # A higher remaining capacity for a fittable bin is generally good for future items.\n    # The inverse of remaining capacity for spread preference is tricky. Let's re-evaluate.\n    # Instead of inverse remaining capacity for spread, let's consider a bonus for bins\n    # that have a moderate amount of remaining space *after* fitting the item.\n    # This encourages using bins that are not too full but also not too empty.\n\n    # Let's rethink the \"spread preference\". Instead of just preferring less full bins,\n    # let's create a score that prefers bins that, after fitting the item,\n    # leave a \"reasonable\" amount of space, not too little (handled by Best Fit) and not too much.\n    # This is like a \"just right\" fit.\n    # A Gaussian-like function centered around a sweet spot of remaining capacity.\n    # Let's assume a sweet spot is roughly half the bin capacity.\n    sweet_spot_ratio = 0.5\n    sweet_spot_capacity = notional_bin_capacity * sweet_spot_ratio\n    \n    # Penalize bins that leave too little space (handled by best_fit) and too much space.\n    # The distance from the sweet spot.\n    distance_from_sweet_spot = np.abs((fittable_bins_remain_cap - item) - sweet_spot_capacity)\n    # Inverse of distance, so closer to sweet spot is higher score. Add epsilon.\n    sweet_spot_score = 1.0 / (distance_from_sweet_spot + 1e-9)\n    \n    # Let's also add a component that favors bins that are already quite full,\n    # but only if they can fit the item. This is similar to original but with a different logic.\n    # Instead of inverse remaining capacity, let's use the negative of remaining capacity.\n    # This means smaller remaining capacity (more full) gets a higher score.\n    # We will normalize this so it doesn't dominate.\n    fullness_score = -fittable_bins_remain_cap\n\n    # Combining them:\n    # Best Fit is primary.\n    # Sweet Spot preference helps balance immediate tight fit with future capacity.\n    # Fullness score encourages using bins that are already substantially occupied.\n    \n    # Normalizing component scores before combining to prevent dominance.\n    # Max-min normalization for scores that should be positive.\n    if np.max(best_fit_score) > 1e-9:\n        norm_best_fit = best_fit_score / np.max(best_fit_score)\n    else:\n        norm_best_fit = np.zeros_like(best_fit_score)\n\n    if np.max(sweet_spot_score) > 1e-9:\n        norm_sweet_spot = sweet_spot_score / np.max(sweet_spot_score)\n    else:\n        norm_sweet_spot = np.zeros_like(sweet_spot_score)\n\n    # For fullness_score, it's negative. We want higher (less negative) to be better.\n    # so, we can normalize (max - x) / (max - min) or just shift and normalize.\n    # Let's normalize it to a positive range [0,1] where more full bins get higher scores.\n    # min_fullness = np.min(fittable_bins_remain_cap)\n    # max_fullness = np.max(fittable_bins_remain_cap)\n    # if max_fullness > min_fullness:\n    #     norm_fullness = (max_fullness - fittable_bins_remain_cap) / (max_fullness - min_fullness)\n    # else:\n    #     norm_fullness = np.ones_like(fittable_bins_remain_cap) * 0.5 # Neutral if all same\n\n    # Simpler normalization for fullness: treat -remaining_capacity. Higher is better.\n    # Normalize by shifting so min is 0, then divide by max.\n    min_score = np.min(fullness_score)\n    max_score = np.max(fullness_score)\n    if max_score > min_score:\n        norm_fullness = (fullness_score - min_score) / (max_score - min_score)\n    else:\n        norm_fullness = np.ones_like(fullness_score) * 0.5 # Neutral if all same\n\n\n    # Weighted combination\n    # Best Fit is weighted heavily.\n    # Sweet spot preference provides a balance.\n    # Fullness score encourages consolidation.\n    \n    # Weights are tunable parameters.\n    w_best_fit = 0.6\n    w_sweet_spot = 0.3\n    w_fullness = 0.1\n\n    combined_scores = (w_best_fit * norm_best_fit) + \\\n                      (w_sweet_spot * norm_sweet_spot) + \\\n                      (w_fullness * norm_fullness)\n\n    # Assign the calculated priorities to the fittable bins\n    priorities[fittable_bins_mask] = combined_scores\n\n    # Normalize the final priorities to [0, 1] for consistent selection.\n    if np.max(priorities) > 1e-9:\n        priorities = priorities / np.max(priorities)\n    else:\n        # If all scores are near zero, assign a small uniform priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 42.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response4.txt_stdout.txt",
    "code_path": "problem_iter9_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A refined heuristic that balances immediate fit quality with long-term bin utilization,\n    prioritizing bins that are nearly full but can still accommodate the item,\n    while also considering the potential for future fits of smaller items.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Score 1: Tightness of Fit (Best Fit Component)\n    # Prioritizes bins where the remaining capacity after packing is minimal.\n    # Add a small epsilon to prevent division by zero.\n    tightness_score = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Score 2: Bin Fullness (Encouraging fuller bins)\n    # This score is high for bins that are already quite full (low remaining capacity).\n    # We use the inverse of the remaining capacity.\n    fullness_score = 1.0 / (fittable_bins_remain_cap + 1e-6)\n\n    # Score 3: Future Fit Potential (Adaptive Component)\n    # This score is higher for bins that, after fitting the current item,\n    # will still have a significant amount of remaining capacity. This aims to\n    # leave \"room\" for smaller items later, preventing premature bin exhaustion.\n    # We use a logarithmic function to give diminishing returns for very large remaining capacities.\n    # Adding 1 to prevent log(0) or log(negative).\n    future_fit_score = np.log1p(bins_remain_cap[fittable_bins_mask] - item + 1)\n\n    # Combine scores with adaptive weighting.\n    # The weights are designed to:\n    # - Heavily favor bins that offer a tight fit (tightness_score).\n    # - Give a moderate boost to bins that are already fuller (fullness_score).\n    # - Introduce a penalty for leaving excessively large gaps by inverting the future_fit_score.\n    #   A smaller (less leftover space) future_fit_score is better, so we subtract it.\n    # The coefficients (1.0, 0.7, 0.3) are hyperparameters that can be tuned.\n    # We are emphasizing the tightness and fullness more, while still penalizing large leftovers.\n    combined_scores = (tightness_score * 1.0) + (fullness_score * 0.7) - (future_fit_score * 0.3)\n\n    # Normalize priorities to a [0, 1] range.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = np.clip(combined_scores / max_score, 0, 1)\n    else:\n        # If all scores are effectively zero, assign a minimal uniform priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.01\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.108496210610296,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  }
]