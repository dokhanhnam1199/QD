{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Aims to improve upon v1 by using a multiplicative scoring approach that\n    inherently balances competing objectives and is less sensitive to arbitrary\n    weight adjustments. It prioritizes bins that offer a tight fit AND have\n    substantial remaining capacity (to avoid prematurely fragmenting larger spaces),\n    while also penalizing bins that are already nearly full to encourage\n    utilization of less occupied bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Score 1: Tight Fit Preference (similar to Best Fit).\n    # Higher score for smaller leftover space. Add epsilon for numerical stability.\n    tight_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-9)\n\n    # Score 2: Space Utilization Preference.\n    # Prioritize bins that are already more utilized (i.e., have less remaining capacity).\n    # This encourages filling up existing bins before opening new ones.\n    # Normalize by bin capacity (assuming capacity is uniform, which it is in BPP)\n    # Or more generally, by the maximum capacity if it varied. Here, remaining capacity is sufficient.\n    # Higher score for smaller remaining capacity among fittable bins.\n    space_utilization_score = 1.0 / (fittable_bins_remain_cap + 1e-9)\n\n    # Score 3: Robustness/Avoidance of Over-fragmentation.\n    # Penalize bins that, after fitting the item, leave a very large amount of capacity.\n    # This aims to avoid packing an item into a very large bin if a medium-sized bin is available.\n    # We want to prefer bins that leave a \"reasonable\" amount of space, not too little and not too much.\n    # Using a Gaussian-like penalty centered around a \"good\" leftover space, or more simply,\n    # penalizing very large leftovers. A simple inverse square of remaining capacity after fit.\n    # Or even simpler, penalize bins that leave capacity significantly larger than the item size.\n    # Let's use a penalty proportional to the log of the remaining capacity AFTER the fit.\n    # Smaller log means better.\n    leftover_space_penalty = np.log1p(fittable_bins_remain_cap - item) # Smaller is better\n\n    # Combine scores multiplicatively. This creates a synergy where bins must perform well\n    # on multiple criteria. A bin with a good tight fit but terrible space utilization (e.g., very empty)\n    # will be penalized. Similarly, a bin with high utilization but a very poor fit will be penalized.\n\n    # We want to:\n    # 1. Maximize tight fit (high tight_fit_score)\n    # 2. Maximize space utilization (high space_utilization_score)\n    # 3. Minimize leftover space penalty (low leftover_space_penalty)\n\n    # Multiplicative combination: Score = (TightFit) * (SpaceUtil) / (1 + LeftoverPenalty)\n    # Adding 1 to penalty to avoid division by zero and to ensure it acts as a penalty.\n    # A lower leftover_space_penalty should result in a higher combined score.\n    # So, divide by (1 + penalty) or multiply by 1/(1+penalty)\n    combined_scores = tight_fit_score * space_utilization_score * (1.0 / (1.0 + leftover_space_penalty))\n\n    # Normalize priorities to a [0, 1] range.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n    else:\n        # If all scores are effectively zero, assign a small uniform priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a penalty for bins that are too large,\n    using a multiplicative scoring to encourage tighter fits and penalize waste.\n    Includes an adaptive term that favors bins with remaining capacity closer to item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Best Fit Score: Higher for bins that leave less remaining capacity after fitting.\n    # Add a small epsilon to prevent division by zero.\n    best_fit_component = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Penalty for \"too large\" bins: Bins with significantly more remaining capacity than needed\n    # are less desirable as they might be better utilized by larger items later.\n    # We use an inverse relationship with the excess capacity.\n    excess_capacity = fittable_bins_remain_cap - item\n    # Penalty is higher for larger excess capacity. Cap to avoid extremely small values.\n    large_bin_penalty_component = 1.0 / (excess_capacity + 1.0)\n\n    # Adaptive \"Tightness\" Score: Aims to pick bins where the remaining capacity\n    # is *just enough* or slightly more than the item.\n    # This is similar to Best Fit but can be weighted differently.\n    # We want to maximize the remaining capacity if it's very close to the item size.\n    # We use a Gaussian-like function centered slightly above the item size.\n    # The goal is to favor bins where (remaining_cap - item) is small.\n    # For a given item, we want to find bins where remaining_cap is close to item.\n    # Let's use a score that is high when (remaining_cap - item) is small.\n    # A negative exponential function on the squared difference can work.\n    # Shifted to be centered around 0 remaining capacity after fit.\n    # Adding 1 to the exponent to avoid exp(0) = 1 and ensure lower values for larger gaps.\n    tightness_component = np.exp(-0.1 * (excess_capacity**2))\n\n\n    # Combine components using a multiplicative approach.\n    # This encourages all components to be good simultaneously.\n    # We use the inverse of the large bin penalty as a multiplier: higher penalty (smaller value)\n    # reduces the overall score.\n    # The tightness component directly contributes positively.\n    combined_scores = best_fit_component * (1.0 - large_bin_penalty_component) * tightness_component\n\n    # Normalize priorities to a [0, 1] range.\n    if np.max(combined_scores) > 1e-9:\n        priorities[fittable_bins_mask] = np.clip(combined_scores / np.max(combined_scores), 0, 1)\n    else:\n        # If all scores are near zero, assign a small uniform priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1st (weighted additive with log bonus) vs. Heuristic 2nd (multiplicative best-fit and fill ratio): Heuristic 1st attempts a more nuanced combination with a logarithmic term to manage leftover space, which can be more stable. Heuristic 2nd uses a simple multiplicative approach which can be too aggressive.\n\nComparing Heuristic 1st (weighted additive with log bonus) vs. Heuristic 3rd (complex additive with multiple components): Heuristic 1st is simpler and more direct. Heuristic 3rd's attempt at \"Proximity to Half-Capacity\" and \"Worst Fit Incentive\" without clear definitions or robust proxies makes it less reliable.\n\nComparing Heuristic 2nd (multiplicative best-fit and fill ratio) vs. Heuristic 5th (identical to 2nd, possibly a copy-paste error): They are identical, making comparison moot for ranking. However, the core idea of multiplicative best-fit and fill ratio is present in 2nd, 5th, 6th, 7th, 8th, 10th, 15th, 16th, 17th, 18th, 19th, 20th. Heuristic 10th uses a logarithmic fill bonus, which is a slight variation. Heuristics 5th, 6th, 7th, 8th are indeed identical.\n\nComparing Heuristic 4th (multiplicative with explicit penalty/bonus) vs. Heuristic 9th (multiplicative best-fit and normalized fullness): Heuristic 4th's explicit `1.0 / (1.0 + leftover_space_penalty)` is a reasonable way to incorporate a penalty, but the interaction with other terms might be sensitive. Heuristic 9th's `fullness_scores = 1.0 - (fittable_bins_remain_cap / max_initial_remain_cap)` attempts to normalize fullness, which can be more robust than absolute remaining capacity. However, it adds a constant `+ 0.1` to `fullness_scores` before multiplication, which is a heuristic dampener.\n\nComparing Heuristic 1st (weighted additive) vs. Heuristic 12th (multiplicative with adaptive tightness): Heuristic 1st's additive approach is generally more controllable than Heuristic 12th's multiplicative approach, especially with an exponential tightness component which can be sensitive. Heuristic 12th's `(1.0 - large_bin_penalty_component)` might lead to negative values if `large_bin_penalty_component` is greater than 1, which is unlikely here but a potential issue.\n\nComparing Heuristic 2nd (multiplicative best-fit and fill ratio) vs. Heuristic 10th (multiplicative best-fit and log fullness bonus): Heuristic 10th's use of `log1p(1.0 / (fittable_bins_remain_cap + 1e-9))` for fullness is a more sophisticated way to handle varying levels of fullness compared to Heuristic 2nd's simple inverse.\n\nOverall: Heuristic 1st seems to strike a good balance between simplicity, interpretability, and robustness with its weighted additive approach. Multiplicative combinations (like Heuristics 2nd, 4th, 9th, 10th) can be powerful but risk being too sensitive to specific term values or interactions. Heuristics involving complex adaptive terms or unclear proxies (like parts of Heuristic 3rd, 12th, 13th, 14th) are harder to justify without empirical validation. The numerous identical heuristics (5th, 6th, 7th, 8th) are a clear weakness in the list.\n- \nHere's a redefined \"Current self-reflection\" focused on designing better heuristics, avoiding pitfalls of ineffective self-reflection:\n\n*   **Keywords:** Multi-objective, Adaptive Weighting, Robustness, Interpretability, Granular Control.\n*   **Advice:** Design heuristics with granular control via additive components, carefully normalized. Prioritize explicit balancing of immediate packing efficiency and future bin utility. Implement adaptive weighting based on item characteristics for improved robustness.\n*   **Avoid:** Redundant heuristics, overly aggressive multiplicative combinations without clear justification, and implicit assumptions about scale or edge cases. Do not rely on near-identical implementations.\n*   **Explanation:** This approach emphasizes clarity, adaptability, and the avoidance of duplication. It suggests building complex behaviors from well-understood, controlled components rather than relying on opaque, potentially brittle multiplicative interactions or redundant strategies.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}