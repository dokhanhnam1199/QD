[
  {
    "stdout_filepath": "problem_iter17_response0.txt_stdout.txt",
    "code_path": "problem_iter17_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit's tightness with a normalized fill ratio, prioritizing bins\n    that snugly fit the item and are already substantially full, adaptively.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    eligible_bins_caps = bins_remain_cap[fittable_bins_mask]\n\n    # Score 1: Best Fit (inverse of remaining capacity after packing).\n    # Smaller remaining capacity after packing yields a higher score.\n    best_fit_scores = 1.0 / (eligible_bins_caps - item + 1e-9)\n\n    # Score 2: Normalized Fill Ratio.\n    # Aims to prioritize bins that are more full. We use a formula that is\n    # sensitive to how much space is left relative to the total capacity,\n    # but we normalize it against the maximum initial remaining capacity\n    # to provide a more consistent fill measure across different scenarios.\n    # Adding a small epsilon for stability and to avoid zero if all bins are empty.\n    max_initial_remain_cap = np.max(bins_remain_cap) if np.any(bins_remain_cap > 0) else 1.0\n    fill_ratio_scores = 1.0 - (eligible_bins_caps / (max_initial_remain_cap + 1e-9))\n    # Add a small constant to ensure even less full bins have a positive score,\n    # preventing the multiplicative term from becoming zero too easily.\n    fill_ratio_scores = fill_ratio_scores + 0.1\n\n    # Combine scores multiplicatively.\n    # This approach emphasizes bins that are good in both aspects: tight fit (best_fit_scores)\n    # and already substantially full (fill_ratio_scores).\n    combined_scores = best_fit_scores * fill_ratio_scores\n\n    # Assign combined scores to the priorities array for eligible bins.\n    priorities[fittable_bins_mask] = combined_scores\n\n    # Normalize priorities to the range [0, 1] for consistent comparison.\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response1.txt_stdout.txt",
    "code_path": "problem_iter17_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit's tightness with a balanced preference for bins that are\n    not excessively empty after packing. This aims for efficient packing while\n    keeping options open for future items.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n    residual_capacity = fittable_bins_remain_cap - item\n\n    # Component 1: Best Fit (tightness)\n    # Higher score for smaller residual capacity.\n    # Using inverse with epsilon for numerical stability.\n    best_fit_score = 1.0 / (residual_capacity + 1e-9)\n\n    # Component 2: \"Bin Fullness Preference\"\n    # Favor bins that are already somewhat full, meaning they have less remaining capacity.\n    # This discourages leaving many bins nearly empty.\n    # Higher score for smaller fittable_bins_remain_cap.\n    bin_fullness_score = 1.0 / (fittable_bins_remain_cap + 1e-9)\n\n    # Combine scores using a weighted additive approach.\n    # Weight for Best Fit (tightness) is primary.\n    # Weight for Bin Fullness is secondary, to encourage using less empty bins.\n    w_best_fit = 1.0\n    w_bin_fullness = 0.5\n\n    combined_scores = (w_best_fit * best_fit_score) + (w_bin_fullness * bin_fullness_score)\n\n    # Normalize the combined scores for the fittable bins to be in a comparable range (e.g., 0 to 1).\n    # This prevents one component from dominating solely due to its scale.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        normalized_scores = combined_scores / max_score\n    else:\n        # If all scores are very small (e.g., all residuals are huge), assign a minimal uniform priority.\n        normalized_scores = np.full_like(combined_scores, 0.1)\n\n    # Assign the calculated priorities to the fittable bins.\n    priorities[fittable_bins_mask] = normalized_scores\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 19.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response2.txt_stdout.txt",
    "code_path": "problem_iter17_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a refined Best Fit approach with a penalty for leaving excessive\n    remaining capacity, using a multiplicative scoring for synergy.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Component 1: Best Fit - Prioritize bins that leave minimal remaining space.\n    # This is crucial for immediate packing efficiency.\n    # Using 1/(difference + epsilon) to favor smaller differences.\n    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-9)\n\n    # Component 2: Penalty for Large Leftover Space - Discourage packing into\n    # bins that leave significantly more capacity than needed. This aims to\n    # preserve larger spaces for potentially larger future items.\n    # We use a logarithmic penalty: smaller leftovers are better, but the\n    # penalty grows slower for larger leftovers, avoiding over-penalization.\n    # The +1 in log1p handles cases where remaining capacity after fit is 0.\n    leftover_space_penalty = np.log1p(fittable_bins_remain_cap - item)\n\n    # Combine scores multiplicatively to ensure bins perform well on both criteria.\n    # We want to maximize the best_fit_score and minimize the leftover_space_penalty.\n    # Therefore, we divide by (1 + penalty) to get a score that is higher when\n    # the penalty is lower.\n    combined_scores = best_fit_score / (1.0 + leftover_space_penalty)\n\n    # Normalize priorities to a [0, 1] range for consistent selection.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n    else:\n        # If all calculated scores are negligible, assign a small uniform priority\n        # to all fittable bins to ensure at least one is chosen if available.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response3.txt_stdout.txt",
    "code_path": "problem_iter17_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit's tightness with a normalized measure of bin fullness,\n    using an additive approach for better control and interpretability.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Best Fit Score: Prioritize bins that leave minimal remaining capacity.\n    # Using inverse of remaining space after fit, with epsilon for stability.\n    best_fit_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-9)\n\n    # Fullness Score: Prioritize bins that are already more full.\n    # Normalized by the *initial* maximum remaining capacity to provide a stable baseline.\n    # This measure is relative to the overall capacity landscape, not just fittable bins.\n    # A higher score means the bin has less *initial* remaining capacity (i.e., is fuller).\n    if bins_remain_cap.size > 0 and np.max(bins_remain_cap) > 1e-9:\n        max_initial_remain_cap = np.max(bins_remain_cap)\n        fullness_scores = (max_initial_remain_cap - fittable_bins_remain_cap) / (max_initial_remain_cap + 1e-9)\n    else:\n        # If no bins or all bins are empty, fullness doesn't differentiate much.\n        fullness_scores = np.zeros_like(fittable_bins_remain_cap)\n\n    # Combine Scores Additively:\n    # Weight Best Fit more heavily, but give a significant boost for fuller bins.\n    # This aims for a balance: prioritize tight fits but also leverage existing bin fullness.\n    # Weights can be tuned for performance.\n    combined_scores = (best_fit_scores * 1.0) + (fullness_scores * 0.75)\n\n    # Normalize combined scores to [0, 1] for consistent priority assignment.\n    if np.max(combined_scores) > 1e-9:\n        priorities[fittable_bins_mask] = np.clip(combined_scores / np.max(combined_scores), 0, 1)\n    else:\n        # If all combined scores are effectively zero, assign a small uniform priority.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response4.txt_stdout.txt",
    "code_path": "problem_iter17_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit's tightness with a normalized fill ratio, prioritizing bins\n    that fit snugly and are already substantially full, using additive weighting for control.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    eligible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(eligible_bins_mask):\n        return priorities\n\n    eligible_bins_caps = bins_remain_cap[eligible_bins_mask]\n\n    # Best Fit component: Prioritize bins with least remaining capacity after packing.\n    # A smaller difference (eligible_bins_caps - item) yields a higher score.\n    best_fit_scores = 1.0 / (eligible_bins_caps - item + 1e-9)\n\n    # Fill Ratio component: Prioritize bins that are already more utilized relative to their total capacity.\n    # Normalize the current remaining capacity by the maximum initial remaining capacity across all bins.\n    # This score is higher for bins that are more full. Using log1p for smoother distribution.\n    max_initial_remain_cap = np.max(bins_remain_cap) # Consider initial state for normalization\n    if max_initial_remain_cap == 0: # Handle case where all bins might be full initially\n        fill_ratio_scores = np.ones_like(eligible_bins_caps) # All eligible bins are equally \"full\"\n    else:\n        fill_ratio_scores = np.log1p(1.0 / (eligible_bins_caps / max_initial_remain_cap + 1e-9))\n\n    # Combine scores additively with weights. This provides more granular control.\n    # Weight for Best Fit: 0.7 (emphasizes immediate packing efficiency)\n    # Weight for Fill Ratio: 0.3 (emphasizes future bin utilization)\n    # The weights are chosen to balance immediate fit with long-term packing density.\n    combined_scores = 0.7 * best_fit_scores + 0.3 * fill_ratio_scores\n\n    priorities[eligible_bins_mask] = combined_scores\n\n    # Normalize priorities to the range [0, 1].\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response5.txt_stdout.txt",
    "code_path": "problem_iter17_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with an adaptive fullness bonus. Prioritizes bins that are\n    nearly full and can accommodate the item with minimal remaining space,\n    using a normalized fullness score for better adaptability.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Best Fit component: Prioritize bins with smallest remaining capacity after fitting.\n    # Adding a small epsilon to avoid division by zero.\n    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-9)\n\n    # Adaptive Fullness component: Prioritize bins that are already more full,\n    # using a normalized measure relative to the maximum initial remaining capacity\n    # among fittable bins to ensure better cross-bin comparison.\n    # This score is higher for bins that are already fuller.\n    # Add a small epsilon to the denominator to avoid division by zero if all fittable bins are empty.\n    max_initial_remain_cap = np.max(fittable_bins_remain_cap)\n    fullness_score = 1.0 - (fittable_bins_remain_cap / (max_initial_remain_cap + 1e-9))\n\n    # Combine scores multiplicatively: Boosts bins that are both tight fits and more full.\n    # Add a small constant to fullness_score to ensure it always contributes positively\n    # and doesn't zero out the overall score if a bin is very empty.\n    combined_scores = best_fit_score * (fullness_score + 0.5)\n\n    priorities[fittable_bins_mask] = combined_scores\n\n    # Normalize priorities for fittable bins to a [0, 1] range.\n    max_priority = np.max(priorities[fittable_bins_mask])\n    if max_priority > 0:\n        priorities[fittable_bins_mask] /= max_priority\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response6.txt_stdout.txt",
    "code_path": "problem_iter17_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit's tightness and a normalized fill ratio using additive weights.\n    Balances immediate packing efficiency with future bin utility.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Component 1: Best Fit (inverse of remaining capacity after packing)\n    # Prioritizes bins where the item fits most snugly.\n    best_fit_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-9)\n\n    # Component 2: Normalized Fill Ratio\n    # Measures how full a bin is relative to its capacity, encouraging use of already utilized bins.\n    # We use (capacity - remaining_capacity) / capacity for the overall bin fill,\n    # but for priority, we consider item/remaining_capacity as a measure of how much\n    # the item \"fills\" the *available* space. A higher ratio is better.\n    fill_ratio_scores = item / (fittable_bins_remain_cap + 1e-9)\n\n    # Weights for combining components. These can be tuned.\n    # Beta emphasizes fitting snugly, Alpha emphasizes already fuller bins.\n    beta = 0.7\n    alpha = 0.3\n\n    # Combined score using weighted additive approach for better control.\n    # We normalize each component before weighting to ensure fair contribution.\n    # Normalize best_fit_scores (higher is better, higher score means smaller leftover)\n    max_bf = np.max(best_fit_scores)\n    normalized_best_fit = best_fit_scores / max_bf if max_bf > 1e-9 else np.ones_like(best_fit_scores)\n\n    # Normalize fill_ratio_scores (higher is better, higher score means item is a larger fraction of available space)\n    max_fr = np.max(fill_ratio_scores)\n    normalized_fill_ratio = fill_ratio_scores / max_fr if max_fr > 1e-9 else np.ones_like(fill_ratio_scores)\n\n    combined_scores = alpha * normalized_fill_ratio + beta * normalized_best_fit\n\n    # Assign calculated scores back and normalize the final priorities.\n    if np.max(combined_scores) > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / np.max(combined_scores)\n    else:\n        # If all scores are zero or near-zero, assign uniform small priority\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 20.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response7.txt_stdout.txt",
    "code_path": "problem_iter17_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit's preference for tight fits with a normalized Fill Ratio bonus.\n    Prioritizes bins that fit the item snugly and are already substantially full,\n    with adaptive weighting for robustness.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    eligible_bins_caps = bins_remain_cap[fittable_bins_mask]\n\n    # Component 1: Best Fit (tightness) - Inverse of remaining capacity after packing\n    # Higher score for smaller leftover space.\n    best_fit_score = 1.0 / (eligible_bins_caps - item + 1e-9)\n\n    # Component 2: Fill Ratio (bin fullness) - Normalized inverse of remaining capacity\n    # This component is derived from Heuristic 9's approach to normalize based on\n    # maximum initial capacity if it were known. Here, we use a general\n    # penalty for large remaining capacities by inverting the capacity.\n    # Adding a small constant to avoid zero and ensure all bins have some base priority.\n    fill_ratio_score = 1.0 / (eligible_bins_caps + 1.0) # Penalize large remaining space more\n\n    # Adaptive Weighting: Use item size to adjust the importance of fill ratio.\n    # Larger items might benefit more from prioritizing bins that are already fuller,\n    # while smaller items might be more sensitive to the tightness of the fit.\n    # This is a simple heuristic: a higher weight for fill ratio for larger items.\n    # Let's assume a normalized item size for this scaling (e.g., item / max_possible_item_size).\n    # For simplicity, we'll use a direct relationship with item size, capped.\n    fill_ratio_weight = min(item * 2.0, 1.5) # Weight for fill ratio, capped at 1.5\n\n    # Combine scores: Weighted sum for better control and interpretability.\n    # This offers more granular control than pure multiplicative combinations.\n    combined_scores = best_fit_score + fill_ratio_weight * fill_ratio_score\n\n    # Assign combined scores to the priorities array for eligible bins.\n    priorities[fittable_bins_mask] = combined_scores\n\n    # Normalize priorities to the range [0, 1] for consistent comparison.\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response8.txt_stdout.txt",
    "code_path": "problem_iter17_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit and a normalized fullness score multiplicatively.\n    Prioritizes bins that are a tight fit and also already quite full,\n    with careful normalization to ensure score stability.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Best Fit Score: Inverse of remaining capacity after fitting. Smaller remaining capacity is better.\n    # Add a small epsilon to prevent division by zero if remaining capacity is exactly item size.\n    best_fit_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-9)\n\n    # Fullness Score: Ratio of capacity already used relative to the maximum remaining capacity among fittable bins.\n    # This provides a relative measure of how \"full\" a bin is compared to other potential candidates.\n    max_initial_remain_cap = np.max(fittable_bins_remain_cap)\n    if max_initial_remain_cap < 1e-9:\n        # If all fittable bins have negligible remaining capacity, they are all equally \"full\" in a relative sense.\n        fullness_scores = np.ones_like(fittable_bins_remain_cap)\n    else:\n        # Score is higher for bins with less initial remaining capacity (i.e., they are fuller).\n        fullness_scores = 1.0 - (fittable_bins_remain_cap / max_initial_remain_cap)\n\n    # Combine scores multiplicatively. This rewards bins that are both a good fit and already full.\n    # Adding a small constant to fullness scores ensures that even bins that are not maximally full\n    # still contribute a positive multiplicative factor, preventing the overall score from collapsing to zero.\n    combined_scores = best_fit_scores * (fullness_scores + 0.1)\n\n    # Normalize the combined scores for fittable bins to a [0, 1] range.\n    # This ensures that the priority values are on a consistent scale, regardless of the absolute magnitudes of the scores.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n    else:\n        # If all combined scores are near zero (e.g., all fittable bins are very large and empty),\n        # assign a small uniform priority to fittable bins to allow selection.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response9.txt_stdout.txt",
    "code_path": "problem_iter17_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit preference with a logarithmic fullness bonus,\n    additively weighted, prioritizing bins that are a tight fit and already full.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Best Fit component: Inverse of remaining capacity after fitting. Prioritizes tight fits.\n    # Add epsilon for numerical stability.\n    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-9)\n\n    # Fullness Bonus: Logarithmic scale of how full the bin is before fitting.\n    # log1p(1/remaining_cap) is used to give higher bonus to fuller bins (smaller remaining_cap).\n    fullness_bonus = np.log1p(1.0 / (fittable_bins_remain_cap + 1e-9))\n\n    # Combine scores additively with weights for balance.\n    # Weighting allows for tunable influence of each component.\n    # Adding 1.0 to fullness_bonus to ensure it contributes positively.\n    # Adjusted weights to balance Best Fit and Fullness.\n    combined_scores = 0.7 * best_fit_score + 0.3 * (1.0 + fullness_bonus * 0.5)\n\n    # Normalize priorities to a [0, 1] range.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n    else:\n        # Fallback: uniform small priority if all scores are negligible.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter18_response0.txt_stdout.txt",
    "code_path": "problem_iter18_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Component 1: Best Fit (BF) - Prioritize bins that leave minimal remaining capacity.\n    # Higher score for smaller remaining capacity after fitting.\n    # Add epsilon to avoid division by zero.\n    best_fit_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-9)\n\n    # Component 2: First Fit Decreasing (FFD) influence - Prioritize bins that are already more full.\n    # This encourages using existing partially filled bins.\n    # Calculated as (bin_capacity - remaining_capacity) / bin_capacity.\n    # We use the inverse of remaining capacity as a proxy for fullness, scaled.\n    # Normalize by the maximum possible remaining capacity (which is the bin capacity, assuming a standard bin size, or maximum initial capacity if variable).\n    # For simplicity, let's assume a fixed bin capacity of 1.0, so fullness is 1 - remaining_capacity.\n    # For fittable bins, fullness is (1.0 - fittable_bins_remain_cap)\n    # A simple measure of \"how full\" a bin is, relative to other fittable bins.\n    # High value for bins that are already quite full (low remaining capacity).\n    fullness_scores = (1.0 - fittable_bins_remain_cap)\n    if np.max(fullness_scores) > 1e-9:\n        fullness_scores = fullness_scores / np.max(fullness_scores)\n    else:\n        fullness_scores = np.zeros_like(fittable_bins_remain_cap)\n\n\n    # Component 3: Adaptive Bonus (AB) - Penalize leaving large gaps, but adaptively.\n    # Using log1p to penalize smaller gaps less severely than larger gaps.\n    # Aims to encourage packing items without creating too many small, unusable spaces.\n    # We want smaller leftover space to be better, so we use the negative of the log.\n    # Add 1 to prevent log(0) and ensure positive values.\n    leftover_space = fittable_bins_remain_cap - item\n    adaptive_scores = -np.log1p(leftover_space)\n    if np.max(adaptive_scores) > 1e-9:\n        adaptive_scores = adaptive_scores / np.max(adaptive_scores)\n    else:\n        adaptive_scores = np.zeros_like(fittable_bins_remain_cap)\n\n    # Component 4: Item Size Consideration (ISC) - Larger items might need more consideration.\n    # Give a slight bonus to bins that can accommodate larger items with more remaining space,\n    # to reserve tighter fits for smaller items later.\n    # This is a counter-balance to pure Best Fit.\n    # This bonus is higher for bins with more remaining capacity *after* fitting the item.\n    # Normalize remaining capacity after fit.\n    remaining_after_fit = fittable_bins_remain_cap - item\n    item_size_scores = remaining_after_fit\n    if np.max(item_size_scores) > 1e-9:\n        item_size_scores = item_size_scores / np.max(item_size_scores)\n    else:\n        item_size_scores = np.zeros_like(fittable_bins_remain_cap)\n\n\n    # Combine components with adaptive weights.\n    # Weights can be tuned. For now, we'll use fixed weights, but this could be made adaptive\n    # based on item size or current bin states.\n    # BF is generally strong. Fullness helps utilize bins. Adaptive balances gaps. ISC reserves space.\n    # Let's give BF the highest weight. Fullness and Adaptive are secondary. ISC is a smaller factor.\n    w_bf = 1.0\n    w_fullness = 0.6\n    w_adaptive = 0.3\n    w_item_size = 0.2\n\n    combined_scores = (w_bf * best_fit_scores +\n                       w_fullness * fullness_scores +\n                       w_adaptive * adaptive_scores +\n                       w_item_size * item_size_scores)\n\n    # Normalize the combined scores to a [0, 1] range.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n    else:\n        # If all scores are zero (e.g., only one fittable bin and it's a perfect fit),\n        # assign a small uniform priority.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 80.21539688871162,
    "SLOC": 38.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter18_response1.txt_stdout.txt",
    "code_path": "problem_iter18_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Component 1: Best Fit - Prioritize bins with minimal remaining capacity after packing.\n    # This is a direct measure of immediate packing efficiency.\n    # Adding epsilon to avoid division by zero.\n    best_fit_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Component 2: Future Utility - Prioritize bins that have more remaining capacity\n    # after packing, as they might be more useful for future items.\n    # This is a measure of long-term bin utility.\n    # We use the inverse of the capacity after packing, so larger capacity is better.\n    future_utility_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n    # Invert this score so that *larger* remaining capacity is *better*.\n    future_utility_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-6) if np.all(fittable_bins_remain_cap - item > 0) else np.zeros_like(fittable_bins_remain_cap)\n\n\n    # Component 3: Balance of Fit - Penalize bins that leave a very large gap.\n    # This encourages more compact packing.\n    # We use a log scale to penalize large gaps less severely than small gaps.\n    # A value of 1 is added to avoid log(0).\n    balance_scores = -np.log1p(fittable_bins_remain_cap - item)\n\n    # Adaptive Weighting based on item size relative to bin capacity\n    # If the item is large (close to bin capacity), prioritize Best Fit.\n    # If the item is small, prioritize Future Utility and Balance.\n    # Let's assume a nominal bin capacity, or use the maximum initial capacity if known.\n    # For simplicity, let's assume an average initial capacity or use the max of bins_remain_cap if it represents initial state.\n    # Without knowing the initial state, we'll use a proxy: the item size itself to gauge \"largeness\".\n    # A simple approach: if item is > 50% of some reference capacity, lean towards BF.\n    # Let's use a hypothetical 1.0 as a reference capacity for simplicity in this example.\n    reference_capacity = 1.0\n    item_size_ratio = item / reference_capacity\n\n    weight_best_fit = 1.0\n    weight_future_utility = 1.0\n    weight_balance = 1.0\n\n    if item_size_ratio > 0.7: # For large items\n        weight_best_fit = 1.5\n        weight_future_utility = 0.5\n        weight_balance = 1.0\n    elif item_size_ratio < 0.3: # For small items\n        weight_best_fit = 0.5\n        weight_future_utility = 1.5\n        weight_balance = 1.2\n    else: # For medium items\n        weight_best_fit = 1.0\n        weight_future_utility = 1.0\n        weight_balance = 1.0\n\n    combined_scores = (\n        (best_fit_scores * weight_best_fit) +\n        (future_utility_scores * weight_future_utility) +\n        (balance_scores * weight_balance)\n    )\n\n    # Normalize scores for fittable bins to a [0, 1] range\n    if np.max(combined_scores) > 1e-9:\n        normalized_scores = np.clip(combined_scores / np.max(combined_scores), 0, 1)\n    else:\n        # If all scores are zero or very small, assign a small uniform priority\n        normalized_scores = np.full_like(combined_scores, 0.1)\n\n    priorities[fittable_bins_mask] = normalized_scores\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.198244914240141,
    "SLOC": 38.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter18_response2.txt_stdout.txt",
    "code_path": "problem_iter18_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit, Worst Fit (for potentially larger items later), and\n    a penalty for bins that would become too empty after packing.\n    Weights are adaptive based on item size relative to bin capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Component 1: Best Fit (BF) - Prioritize bins that minimize remaining capacity\n    # Add epsilon to prevent division by zero. Smaller remaining capacity after fit is better.\n    bf_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Component 2: Worst Fit (WF) - Prioritize bins that leave more remaining capacity (less tight fit)\n    # This can be beneficial for potentially packing larger items later if a loose fit is available.\n    # Larger remaining capacity after fit is better.\n    wf_scores = fittable_bins_remain_cap - item\n\n    # Component 3: Emptying Penalty (EP) - Penalize bins that become very empty after packing.\n    # A bin that becomes too empty might be considered \"wasted\" space.\n    # We want to penalize (fittable_bins_remain_cap - item) when it's large.\n    # Using log1p to dampen the effect of very large remaining capacities.\n    ep_scores = -np.log1p(fittable_bins_remain_cap - item) # Negative because we want to penalize large values\n\n    # Adaptive Weighting based on item size relative to the *average* remaining capacity of fittable bins\n    avg_fittable_remain_cap = np.mean(fittable_bins_remain_cap) if fittable_bins_remain_cap.size > 0 else 1.0\n    # If item is relatively large compared to average remaining capacity, BF is more important.\n    # If item is relatively small, WF might be more useful.\n    bf_weight = np.clip(item / (avg_fittable_remain_cap + 1e-6), 0.5, 2.0)\n    wf_weight = 1.0 - bf_weight / 2.0 # Reduce WF weight if BF is highly favored\n    wf_weight = np.clip(wf_weight, 0.1, 0.8)\n    ep_weight = 0.3 # A moderate penalty for creating very empty bins\n\n    # Combine scores with adaptive weights\n    # Normalize components before combining to prevent one component from dominating due to scale.\n    # Normalize BF scores: Higher is better.\n    norm_bf = (bf_scores - np.min(bf_scores)) / (np.max(bf_scores) - np.min(bf_scores) + 1e-6)\n    # Normalize WF scores: Higher is better.\n    norm_wf = (wf_scores - np.min(wf_scores)) / (np.max(wf_scores) - np.min(wf_scores) + 1e-6)\n    # Normalize EP scores: Higher (less negative) is better, meaning less penalty.\n    norm_ep = (ep_scores - np.min(ep_scores)) / (np.max(ep_scores) - np.min(ep_scores) + 1e-6)\n\n    # The combined score aims to balance BF (tight fit), WF (loose fit), and EP (avoiding too empty bins)\n    combined_scores = (bf_weight * norm_bf) + (wf_weight * norm_wf) + (ep_weight * norm_ep)\n\n    # Assign priorities to the fittable bins\n    if np.max(combined_scores) > 1e-9:\n        priorities[fittable_bins_mask] = np.clip(combined_scores / np.max(combined_scores), 0, 1)\n    else:\n        # If all scores are effectively zero, assign a small uniform priority.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 23.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter18_response3.txt_stdout.txt",
    "code_path": "problem_iter18_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A multi-objective priority function that balances immediate fit (Best Fit)\n    with the potential for future packing (Worst Fit and a penalty for too-small gaps).\n    Weights are adaptively adjusted based on the item's size relative to bin capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Component 1: Best Fit Score (prioritizes bins that leave minimal space)\n    # Smaller remaining space after fit is better. Add epsilon to avoid division by zero.\n    best_fit_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-9)\n\n    # Component 2: Worst Fit Score (prioritizes bins with more remaining capacity,\n    # aiming to keep larger gaps available for potentially larger future items)\n    # Larger remaining space is better. Invert for direct comparison with Best Fit.\n    # Add epsilon to avoid division by zero.\n    worst_fit_scores = fittable_bins_remain_cap / (bins_remain_cap.max() + 1e-9) # Normalize by overall max capacity\n\n\n    # Component 3: Gap Penalty (penalizes leaving very small gaps after fitting)\n    # Small gaps are undesirable as they are less flexible for future items.\n    # Use a sigmoid-like function to penalize small gaps more severely.\n    # Values close to 0 for (fittable_bins_remain_cap - item) will result in a score close to 1.\n    # Values far from 0 will result in a score close to 0.\n    # We want to penalize small gaps, so a higher score here is worse.\n    gap_penalty = 1.0 / (1.0 + np.exp(10 * (fittable_bins_remain_cap - item + 1e-9)))\n\n\n    # Adaptive Weighting based on item size relative to average remaining capacity\n    # If item is large, prioritize fitting it anywhere (Best Fit is more important).\n    # If item is small, consider future packing (Worst Fit and gap penalty become more relevant).\n    avg_remain_cap = np.mean(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 1.0\n    size_ratio = item / (avg_remain_cap + 1e-9)\n\n    # Heuristic weights that adapt. These can be tuned.\n    # w_bf: importance of immediate tight fit\n    # w_wf: importance of leaving larger gaps for future items\n    # w_gp: importance of penalizing creation of small, unusable gaps\n    w_bf = 1.0 + 0.5 * size_ratio  # Larger items give more weight to Best Fit\n    w_wf = 1.0 + 0.5 * (1.0 - size_ratio) # Smaller items give more weight to Worst Fit\n    w_gp = 1.0 # Consistent penalty for small gaps\n\n    # Normalize components before combining to ensure they are on a similar scale\n    # Normalize Best Fit scores: larger is better\n    if np.max(best_fit_scores) > 1e-9:\n        norm_bf = np.clip(best_fit_scores / np.max(best_fit_scores), 0, 1)\n    else:\n        norm_bf = np.zeros_like(best_fit_scores)\n\n    # Normalize Worst Fit scores: larger is better\n    if np.max(worst_fit_scores) > 1e-9:\n        norm_wf = np.clip(worst_fit_scores / np.max(worst_fit_scores), 0, 1)\n    else:\n        norm_wf = np.zeros_like(worst_fit_scores)\n\n    # Normalize Gap Penalty: lower is better (so we invert it for the score)\n    # We want a higher score for bins that result in a SMALLER penalty.\n    if np.max(gap_penalty) > 1e-9:\n        norm_gp = np.clip((1.0 - gap_penalty) / np.max(1.0 - gap_penalty), 0, 1)\n    else:\n        norm_gp = np.zeros_like(gap_penalty)\n\n\n    # Combine scores with adaptive weights\n    # Higher score means higher priority.\n    combined_scores = (w_bf * norm_bf) + (w_wf * norm_wf) + (w_gp * norm_gp)\n\n    # Normalize final priorities to [0, 1]\n    if np.max(combined_scores) > 1e-9:\n        priorities[fittable_bins_mask] = np.clip(combined_scores / np.max(combined_scores), 0, 1)\n    else:\n        # If all scores are near zero, assign a small uniform priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 31.75109692859993,
    "SLOC": 32.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter18_response4.txt_stdout.txt",
    "code_path": "problem_iter18_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_capacity: float) -> np.ndarray:\n    \"\"\"\n    A multi-objective priority function that balances:\n    1. Best Fit: Minimizing leftover space after packing the current item.\n    2. Fill Level Awareness: Prioritizing bins that are already relatively full,\n       to encourage consolidating items.\n    3. Item Size Adaptability: Adjusting the emphasis on fill level based on\n       the current item's size relative to the bin capacity. Larger items\n       might benefit more from prioritizing existing fuller bins to avoid\n       creating many partially filled bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n    fittable_bin_indices = np.where(fittable_bins_mask)[0]\n\n    # Component 1: Best Fit Score\n    # Higher score for bins that leave less remaining capacity.\n    # Add epsilon to prevent division by zero.\n    best_fit_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Component 2: Fill Level Awareness\n    # Higher score for bins that are more full (less remaining capacity).\n    # Normalize by bin_capacity to get a relative fill level.\n    # Use inverse of remaining capacity for higher score when capacity is low.\n    fill_level_scores = (bin_capacity - fittable_bins_remain_cap) / bin_capacity\n\n    # Component 3: Item Size Adaptability\n    # This component modifies the weight given to the fill level based on item size.\n    # If the item is large relative to bin capacity, we might want to be more\n    # aggressive in filling existing bins.\n    # We use a sigmoid-like function to smoothly transition from low to high emphasis.\n    # Small items (item/bin_capacity close to 0) get low adaptability weight.\n    # Large items (item/bin_capacity close to 1) get high adaptability weight.\n    # Additive scaling factor for smooth transition, tunable.\n    adaptability_factor = 0.5 * (1.0 / (1.0 + np.exp(-5 * (item / bin_capacity - 0.5))))\n\n    # Combine the scores with adaptive weights\n    # Best Fit is the primary driver.\n    # Fill Level score is modulated by the adaptability factor.\n    # Weight for fill_level_scores is adaptability_factor.\n    # Weight for best_fit_scores is 1.0.\n    # We want higher scores to be better.\n    combined_scores = best_fit_scores + (fill_level_scores * adaptability_factor)\n\n    # Normalize the combined scores for the fittable bins to a [0, 1] range.\n    if np.max(combined_scores) > 1e-9:\n        normalized_scores = np.clip(combined_scores / np.max(combined_scores), 0, 1)\n        priorities[fittable_bin_indices] = normalized_scores\n    else:\n        # If all scores are near zero (e.g., very small item and many bins with similar large capacity),\n        # assign a small uniform priority to fittable bins to ensure a selection.\n        priorities[fittable_bin_indices] = 0.1\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\nTypeError: priority_v2() missing 1 required positional argument: 'bin_capacity'\n17\n3\n"
  }
]