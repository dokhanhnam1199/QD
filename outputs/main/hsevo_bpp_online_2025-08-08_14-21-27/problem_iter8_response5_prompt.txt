{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit (proximity) with a \"Fill Ratio\" bonus for bins that are already well-utilized.\n    This encourages using bins that are both a good fit and already contain a significant amount of items.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the remaining capacity for bins that can fit the item\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Calculate a \"proximity score\" (Best Fit): higher score for smaller remaining capacity\n    # Using 1 / (1 + residual_capacity) gives a score between (0, 1]. 0 residual is 1.\n    proximity_score = 1.0 / (1.0 + remaining_capacity_after_fit)\n    \n    # Calculate a \"fill ratio score\": higher score for bins that are already more full.\n    # This is (bin_capacity - remaining_capacity) / bin_capacity.\n    # Assuming a default bin capacity of 1.0 for calculating fill ratio if not provided.\n    # In a real scenario, bin_capacity would be a known constant.\n    bin_capacity = 1.0 \n    current_fill_ratio = (bin_capacity - bins_remain_cap[can_fit_mask]) / bin_capacity\n    fill_ratio_score = current_fill_ratio\n    \n    # Combine scores: Multiply proximity score by fill ratio score.\n    # This prioritizes bins that are both a good fit AND already well-utilized.\n    # A bin that has high fill ratio and leaves minimal residual will get the highest score.\n    combined_score = proximity_score * fill_ratio_score\n\n    # Handle cases where combined_score might be 0 if fill_ratio is 0 and proximity is also low.\n    # For example, if an item fits in a nearly empty bin, fill_ratio is near 0.\n    # We want to ensure that even in such cases, the proximity score is still considered.\n    # If fill_ratio is 0, the combined_score becomes 0. We can add a small epsilon or\n    # simply use max(proximity_score, combined_score) if we want proximity to always dominate empty bins.\n    # A simpler approach: if fill_ratio is 0, the combined score is 0. This might disincentivize\n    # using an empty bin, even if it's a perfect fit.\n    # Let's ensure that if fill_ratio is 0, we still assign the proximity score.\n    # A small additive factor can help prevent zero scores if both are low but not zero.\n    # Or, more robustly, ensure proximity is at least considered.\n    \n    # If fill_ratio is 0 (meaning the bin was empty), the combined_score will be 0.\n    # In such cases, we should still consider the proximity score.\n    # We can ensure the score is at least the proximity score when the fill ratio is zero.\n    priorities[can_fit_mask] = np.maximum(combined_score, proximity_score * (current_fill_ratio > 1e-9))\n    \n    # Add a small epsilon to all valid priorities to ensure no zero scores if all bins are valid fits.\n    # This also helps break ties in a consistent manner.\n    priorities[can_fit_mask] += 1e-6\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines proximity fit with a bonus for leaving 'useful' remaining capacity,\n    favoring bins that are a close fit but not so tight they leave minimal space.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_cap.size > 0:\n        # Proximity Score: Higher for bins closer to the item size.\n        differences = fitting_bins_cap - item\n        proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Adaptive \"Usefulness\" Score: Penalize bins that leave very small or very large remainders.\n        # A simple approach is to reward remainders that are not too small.\n        # Here, we reward remainders that are greater than a small fraction of the item size.\n        # This encourages leaving some space for potential future items.\n        # We use a linear scaling for remainders up to a certain point, and then a decaying score.\n        # For simplicity, let's reward larger remainders, but capped to avoid dominance.\n        # A simpler adaptive heuristic: reward bins that leave a moderate remainder.\n        # We can use 1 + (remainder / average_remainder) but that can be unstable.\n        # Let's try a score that favors remainders that are a fraction of the bin's original capacity.\n        # Consider a score that peaks when the remainder is around 20-30% of the bin capacity.\n        \n        # A more direct adaptive approach: Favor bins that are a good fit (proximity)\n        # AND leave a remainder that is not excessively small.\n        # Let's use a score that is `proximity_score * (1 + residual_bonus)`\n        # Where residual_bonus is higher for moderate residuals.\n        # A simple way: `residual_bonus = log(1 + residual)` scaled.\n\n        # Let's combine: Proximity score * (1 + log(1 + resulting_remainder))\n        # This rewards close fits and adds a bonus for leaving more space.\n        resulting_remainders = fitting_bins_cap - item\n        \n        # We want to avoid penalizing very small remainders too harshly, as tight fits are good.\n        # Let's try to make the bonus more significant for larger remainders.\n        # The log function provides diminishing returns.\n        \n        # A simpler combination: proximity * (1 + scaled_remainder).\n        # Let's use a score that reflects how \"full\" the bin becomes, but with a penalty for being *too* full.\n        # Fill ratio: item / fitting_bins_cap\n        # This implicitly penalizes very large remainders.\n        \n        # Let's combine proximity with fill ratio as in heuristic_v0, but add a slight penalty for very small remainders.\n        fill_ratios = item / fitting_bins_cap\n        \n        # We want to prioritize bins that are a good fit AND have a high fill ratio.\n        # Combined score: `fill_ratios * (1 / (differences + epsilon))`\n        # This inherently favors bins that are nearly full and have a small gap.\n        \n        # To make it more adaptive, let's add a term that slightly penalizes\n        # bins that would leave an extremely small remainder.\n        # If `resulting_remainders` is very small, reduce the score.\n        # For example, `1 / (1 + resulting_remainder)` rewards small remainders.\n        # We want the opposite: penalize small remainders.\n        # Let's use `(resulting_remainders + 1) / (resulting_remainders + 1 + penalty_factor)`\n        # where penalty_factor is small.\n        \n        penalty_factor = 5.0 # Adjust this parameter to tune the penalty\n        remainder_quality_score = (resulting_remainders + 1) / (resulting_remainders + 1 + penalty_factor)\n        \n        # Combine proximity, fill ratio, and remainder quality.\n        # The fill_ratio * proximity already favors tight fits.\n        # The remainder_quality_score will slightly boost bins with moderate remainders\n        # and slightly reduce bins with extremely small remainders.\n        \n        # Let's try a simpler combination:\n        # Prioritize bins that are a close fit (proximity_score).\n        # Among those, favor bins that leave a remainder that's not too small.\n        # Score = proximity_score * (1 + alpha * log(resulting_remainders + 1))\n        \n        alpha = 0.1 # Tuning parameter for the bonus\n        adaptive_bonus = np.log(1 + resulting_remainders + 1e-9)\n        \n        # Combine proximity and the adaptive bonus.\n        # The proximity term drives towards the tightest fit.\n        # The adaptive bonus slightly favors bins that leave more space,\n        # making it less greedy on the absolute tightest fit.\n        priorities[can_fit_mask] = proximity_scores + alpha * adaptive_bonus\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1st and 2nd, 1st uses `eligible_bins / (max_eligible_cap + 1e-9)` for fill ratio, while 2nd uses `fittable_bins_remain_cap / (max_eligible_cap + 1e-9)`. 1st's approach seems more directly related to how full the bin is relative to other fitting bins.\n\nComparing Heuristics 2nd and 7th (which are identical), no change is observed.\n\nComparing Heuristics 4th and 8th (which are identical), no change is observed.\n\nComparing Heuristics 5th and Heuristics 14th/15th/16th: Heuristics 14-16 attempt to incorporate an \"adaptive bonus\" based on the logarithm of the resulting remainder, aiming to balance tight fits with leaving some space. Heuristic 5th solely relies on inverse difference (Best Fit). The inclusion of an adaptive bonus in 14-16 suggests a more nuanced approach than simple Best Fit.\n\nComparing Heuristics 10th/11th/12th/13th with others: These heuristics combine proximity with fill ratio. Heuristic 10th's `proximity_score * fill_ratio_score` seems like a straightforward combination. The `np.maximum(combined_score, proximity_score * (current_fill_ratio > 1e-9))` attempts to ensure proximity is considered even for empty bins, which is a good robustness measure.\n\nComparing Heuristics 17th/18th/19th/20th: These heuristics use `base_priority + adaptive_bonus`, where `base_priority` is Best Fit and `adaptive_bonus` is `remaining_capacity_after_fit * 0.5`. This additive approach for the bonus is a simpler alternative to multiplicative combinations seen in other heuristics.\n\nOverall: Heuristics that combine multiple factors (Best Fit, Fill Ratio, adaptive bonuses) tend to be ranked higher. The specific combination method (multiplicative vs. additive) and the nature of the adaptive bonus (logarithmic, linear scaling) seem to differentiate performance. Normalization is consistently applied to keep scores in a comparable range.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Objective Alignment, Trade-offs, Robustness, Predictive Power.\n*   **Advice:** Focus on aligning heuristic scoring with underlying optimization goals, explicitly modeling trade-offs between immediate gains and future packing potential. Use adaptive mechanisms that learn or respond to problem state for enhanced predictive power.\n*   **Avoid:** Redundant comparisons of near-identical strategies. Overly simplistic or static weighting schemes that don't account for dynamic problem states or interactions between objectives.\n*   **Explanation:** Effective self-reflection identifies *why* a heuristic works or fails, focusing on the underlying principles and interactions. This allows for the creation of more sophisticated, robust, and predictive heuristics by understanding the nuanced trade-offs and potential future implications of each decision.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}