```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Calculates priority scores for bins using an adaptive prioritization strategy.
    This heuristic combines proximity fit with a look-ahead to favor bins that,
    after packing the current item, leave a remaining capacity that is "useful"
    for future items (i.e., not too large, not too small).
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    can_fit_mask = bins_remain_cap >= item

    fitting_bins_cap = bins_remain_cap[can_fit_mask]

    if fitting_bins_cap.size > 0:
        # Proximity Fit component: Higher priority for bins closer to item size
        differences = fitting_bins_cap - item
        proximity_scores = 1.0 / (differences + 1e-9)

        # Adaptive component: Penalize bins that leave very large or very small remainders
        # The goal is to keep remaining capacities in a range that is more likely to be filled by future items.
        # A common heuristic is to target remaining capacities around half the bin size or specific item sizes.
        # For simplicity here, we'll penalize very large remainders, aiming for a tighter fit.
        # The penalty is inversely proportional to how much "wasted" space is left.
        # We'll define "wasted" space as the remainder itself.
        # A larger remainder means a higher penalty.
        
        # Example: If bin capacity is 100, and item is 30, remainder is 70.
        # If item is 60, remainder is 40.
        # We want to penalize the remainder of 70 more than 40.
        # We can use a score that is high for small remainders and low for large remainders.
        # Let's use 1 / (remainder + epsilon)
        
        # To make it adaptive, we can consider the *distribution* of remaining capacities.
        # If most bins have large remainders, a large remainder might not be that bad.
        # However, for a general heuristic, aiming for smaller remainders is often good.
        
        # Let's consider the "goodness" of the remaining capacity.
        # A remaining capacity of 0 is good if it means the bin is full.
        # A remaining capacity that is exactly another item's size is good.
        # A remaining capacity that is too large is bad (wasted space).
        # A remaining capacity that is too small is also bad (cannot fit many items).
        
        # A simple adaptive approach: penalize large remainders.
        # Consider the range of remaining capacities for bins that can fit the item.
        # A simple inverse relation to the remainder: 1 / (remainder + epsilon)
        # This is similar to proximity, but we are looking at the *resulting* remainder.
        
        # Let's refine this:
        # We want to favor bins where (bin_cap - item) is small, but not too small.
        # And we want to avoid leaving very large residual capacities.
        
        # Let's use a score that is high for intermediate remainders and low for extreme remainders.
        # For example, a Gaussian-like function centered around a "target" remainder.
        # A simple target could be the average remainder of fitting bins.
        
        target_remainder = np.mean(differences) if differences.size > 0 else 0
        
        # If target_remainder is very small or very large, this might not be ideal.
        # Let's use a more robust target, e.g., half the item size or a fixed value like 10.
        # Or, more simply, let's just penalize large remainders.
        
        # Consider a score that is 1 / (1 + difference) which is similar to proximity.
        # Now let's add a penalty for the *resulting* remainder.
        # The resulting remainder is `fitting_bins_cap - item`.
        # We want to favor smaller resulting remainders.
        
        # Let's combine proximity score with a score for the resulting remainder.
        # Score = proximity_score * (1 / (resulting_remainder + epsilon))
        # This means bins that are a close fit AND leave a small remainder are prioritized.
        
        resulting_remainders = fitting_bins_cap - item
        
        # To avoid issues with very small resulting remainders (which are good),
        # let's ensure the score doesn't become infinite.
        # The proximity score already handles "close fit".
        
        # Let's try a different approach for adaptiveness:
        # The ideal scenario is to leave a remaining capacity that can fit another item perfectly.
        # We don't know future items, but we can try to leave a "versatile" capacity.
        # A remaining capacity around half of the typical item size, or a fixed small value,
        # might be more versatile than very large or very small remainders.
        
        # Let's create a score that favors remainders within a certain range.
        # For simplicity, let's favor remainders that are not too large.
        # If we penalize large remainders, this encourages packing items tightly.
        
        # Let's create a "utility" score for the remaining capacity.
        # A remaining capacity `r` is good if it's small (tight fit) but not zero (unless bin is full).
        # It's also good if it can accommodate some small future items.
        
        # Let's combine proximity and a "utility" of the remaining capacity.
        # Utility function: Higher for smaller remainders, but not zero.
        # e.g., 1 / (resulting_remainder + epsilon)
        
        # Combined score: proximity_score * utility_score
        # This means we prefer bins that are a close fit AND leave a small remainder.
        
        utility_scores = 1.0 / (resulting_remainders + 1e-9)
        
        # Combining the two:
        # We want to give higher priority to bins that are a good fit AND leave a good remainder.
        # Let's scale both and multiply.
        
        # Proximity score is 1 / (fitting_bins_cap - item + epsilon)
        # Utility score is 1 / (fitting_bins_cap - item + epsilon)
        # This is redundant.
        
        # Let's rethink the adaptive part.
        # The key idea of adaptiveness is to use information from the current state to guide the heuristic.
        # The state is the `bins_remain_cap`.
        
        # Consider the variance of remaining capacities. If variance is low, most bins are similar.
        # If variance is high, there's a wide range.
        
        # Let's go back to the original goal: minimize the number of bins.
        # This implies trying to pack items as tightly as possible, but also avoiding creating "unusable" small spaces.
        
        # A simple adaptive heuristic:
        # Prioritize bins that are a good fit (proximity) but also consider the resulting capacity.
        # Let's favor bins that leave a remainder that is "balanced" - not too small, not too large.
        # A common strategy in online algorithms is to use a "look-ahead" or to consider properties of the remaining capacity distribution.
        
        # Let's define "good" remaining capacity as being between a small threshold and a large threshold.
        # For example, if bin capacity is 100, a remaining capacity of 10-40 might be considered good.
        # This is hard to generalize without knowing item size distribution.
        
        # A simpler approach: Prioritize bins that are a tight fit (small remainder) but
        # avoid bins that would leave an *extremely* small remainder, as that might be less flexible.
        
        # Let's use a score that rewards proximity but then penalizes if the resulting remainder is too small.
        # For example, if item is 30, and bin has 31 remaining. Proximity is high. Resulting remainder is 1.
        # We might prefer a bin with 40 remaining (item 30, remainder 10).
        
        # Score = proximity_score * penalty_for_small_remainder
        # Penalty for small remainder: 1 / (1 + small_remainder_value)
        
        # Let's try:
        # Score = (1 / (difference + epsilon)) * (1 / (1 + resulting_remainder))
        # This means we prioritize bins that are a good fit AND leave a small remainder.
        # This is also quite similar to just proximity.
        
        # Let's focus on the *distribution* of remaining capacities of fitting bins.
        # If the item is large, it might only fit in a few bins, and those bins might have large remainders.
        # If the item is small, it might fit in many bins, some with small remainders, some with large.
        
        # Adaptive strategy: Consider the "value" of the remaining capacity.
        # A more sophisticated approach could involve learning or predicting future item sizes.
        # For a pure heuristic, we need to encode this "value" based on current state.
        
        # Let's use the proximity score and then modify it based on the resulting remainder relative to other fitting bins.
        
        # For each fitting bin:
        #   Calculate proximity: prox = 1 / (remaining_cap - item + epsilon)
        #   Calculate resulting remainder: res_rem = remaining_cap - item
        #   Calculate a "remainder score": rem_score.
        #     This score should be higher for remainders that are "useful".
        #     Useful could mean: not too small, not too large.
        #     Let's try a score that peaks at a certain remainder size, e.g., around 50% of the bin capacity, or a fixed small value.
        #     For simplicity, let's just reward smaller remainders, but with a cap to avoid extreme scores.
        #     rem_score = 1 / (res_rem + epsilon) # This is still just proximity.
        
        # Let's try to make the priority dependent on how *unique* a bin is in terms of its remaining capacity.
        # If an item fits perfectly in many bins, it doesn't matter which we pick.
        # If an item fits in only one bin, we should pick that one, regardless of its remainder.
        
        # How about: Priority = proximity_score * (1 + C * normalized_residual_capacity)
        # Where C is a parameter and normalized_residual_capacity is the residual capacity
        # scaled by something, e.g., the average residual capacity of fitting bins.
        
        # Let's try a simpler adaptive idea:
        # Prioritize bins that are a good fit (proximity), BUT if there are multiple bins
        # with similar proximity scores, favor the one that leaves a smaller remainder.
        # This is implicitly done by 1/(diff + epsilon) if diff is small.
        
        # The "adaptive" part should perhaps respond to the overall state of the bins.
        # If all bins have very large remaining capacities, maybe we should prioritize the closest fit.
        # If bins have varying capacities, we can be more selective.
        
        # Let's introduce a penalty for leaving "too much" excess capacity.
        # The excess capacity is `remaining_cap - item`.
        # We want to minimize this excess capacity.
        # So, our score should be inversely related to `excess_capacity`.
        
        # Let's try:
        # Score = proximity_score * (1 / (resulting_remainder + epsilon))
        # This means we favor bins that are a good fit AND leave a small remainder.
        # This is essentially rewarding tight packing.
        
        # The adaptiveness can come from how we combine proximity and remainder score.
        # Instead of multiplying, maybe we add or use a more complex function.
        
        # Let's consider the "waste" ratio: `(remaining_cap - item) / remaining_cap`
        # We want to minimize this waste ratio.
        # Score would be inversely proportional to this ratio.
        
        # Let's try a combined score:
        # Score = proximity_score - lambda * waste_ratio
        # Where lambda is a weighting factor.
        
        # Let's stick to a score that prioritizes bins that are a close fit and leave a small remainder.
        # The problem with `1.0 / (differences + 1e-9)` is that it highly favors the absolute closest fit.
        # This might leave very small remainders which are not useful.
        
        # Adaptive approach:
        # Favor bins that are a close fit, BUT also favor bins that leave a "reasonably sized" remainder.
        # A remainder that's too small is bad. A remainder that's too large is also bad.
        # Let's define a "good" remainder as being within a certain range.
        # For example, a remainder `r` is good if `epsilon <= r <= target_remainder_max`.
        # We can penalize remainders outside this range.
        
        # Let's define a score for the remainder:
        # If `res_rem < epsilon`: penalty (e.g., 0.1)
        # If `res_rem > target_remainder_max`: penalty (e.g., 0.1 * res_rem / target_remainder_max)
        # If `epsilon <= res_rem <= target_remainder_max`: reward (e.g., 1.0)
        
        # This requires defining `target_remainder_max`. Let's set it to be, say, 20% of bin capacity or a fixed small value.
        # This is getting complicated for a simple heuristic.
        
        # Let's simplify the adaptive idea:
        # Prioritize bins that are a close fit (proximity_score).
        # If there are multiple bins with high proximity scores, break ties by choosing the one with the smallest resulting remainder.
        # This is still implicitly handled by the `1/(diff)` if `diff` is small.
        
        # Let's try a score that is sensitive to the *distribution* of remaining capacities.
        # If a bin's remaining capacity is very close to the item size, that's good (proximity).
        # If a bin's remaining capacity is such that `remaining_cap - item` is small, that's also good (tight packing).
        
        # Let's combine proximity and "tightness of packing" directly.
        # Proximity Score: `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)`
        # Tightness Score: `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` (This is the same!)
        
        # Let's try to reward bins that, after packing, leave a remaining capacity that is *not too small*.
        # This encourages leaving space for potentially future items, rather than filling up bins too precisely.
        
        # Score = proximity_score * (1 + resulting_remainder / average_fitting_remainder)
        # This rewards bins that are a close fit AND leave a larger-than-average remainder. This seems counter-intuitive for minimizing bins.
        
        # Let's try to reward bins that leave a remainder that is "medium" sized.
        # For example, a remainder `r` is good if `r` is around `item_size / 2` or some other target.
        
        # Let's use a simpler adaptive idea:
        # Prioritize bins that are a close fit.
        # THEN, among those, prioritize bins that leave a remainder that is itself a "good" candidate for future items.
        # A "good" remainder could be one that is not too small and not too large.
        # Let's represent this by penalizing very small and very large remainders.
        
        resulting_remainders = fitting_bins_cap - item
        
        # Let's use a score that penalizes large remainders more heavily.
        # This promotes tighter packing.
        # Score = proximity_score * (1 / (resulting_remainder + epsilon))
        # This gives more weight to bins that are a close fit AND leave a small remainder.
        
        # Let's try a different combination:
        # Proximity score favors `remaining_cap` closest to `item`.
        # Let's also consider the "waste ratio": `(remaining_cap - item) / bin_capacity`.
        # We want to minimize this waste ratio.
        
        # Let's combine the two using a weighted sum, or by modifying the proximity score.
        
        # Adaptive component: If an item fits into multiple bins with similar proximity scores,
        # then we want to make a choice that is "better" for the overall packing.
        # This often means leaving a remainder that is "useful".
        
        # Let's try this:
        # The base priority is proximity.
        # We will add a bonus if the resulting remainder is "good".
        # A "good" remainder is one that is not too small.
        # Let's define "good" as being greater than some small threshold, e.g., 10% of bin capacity or a fixed small value.
        
        # This is hard to implement generically without knowing bin capacity.
        # Let's assume a fixed bin capacity or use an average.
        
        # Simple adaptive idea:
        # Prioritize bins that are a close fit.
        # Among bins with similar "closeness", prefer the one that leaves a smaller remaining capacity.
        # This is already captured by the `1/(diff + epsilon)` if we prioritize the smallest `diff`.
        
        # Let's introduce a "balancing" factor.
        # The ideal scenario is to leave a remainder that can fit another item.
        # We can't know future items, but we can aim for "versatile" remainders.
        # A versatile remainder might be one that is not extremely small or extremely large.
        
        # Let's combine proximity with a penalty for large remainders.
        # Score = proximity_score - lambda * (resulting_remainder / max_possible_remainder)
        
        # Let's try a different adaptive approach based on the distribution of fitting bins:
        # If there's only one bin that fits, its priority is 1.
        # If multiple bins fit, we use proximity.
        # THEN, we can adjust the scores based on the resulting remainders.
        
        # Let's try to penalize bins that leave very large remainders.
        # Score = proximity_score * (1 / (1 + resulting_remainder))
        # This emphasizes bins that are a close fit AND leave a small remainder.
        
        # This is still essentially proximity, but with a stronger preference for smaller remainders.
        
        # Let's try a more direct "adaptive" approach:
        # Consider the *average* remaining capacity of the bins that can fit the item.
        # If the current item is large, it might only fit in bins with large remaining capacities.
        # If the current item is small, it might fit in bins with various remaining capacities.
        
        # Let's try to encourage leaving remainders that are not too small.
        # If `resulting_remainder` is very small, reduce the priority.
        
        # Score = proximity_score * (1 + alpha * min(0, resulting_remainder - threshold))
        # Where threshold is a small value. This penalizes remainders below the threshold.
        
        # Let's refine the proximity score to be less sensitive to tiny differences.
        # Instead of `1 / (diff + epsilon)`, maybe `1 / (1 + diff^p)` for `p > 1`.
        # Or, use a step function, or capped inverse.
        
        # Let's go with a simple adaptive idea:
        # Prioritize bins that are a close fit (proximity_score).
        # THEN, among bins with similar proximity scores, prioritize the one that leaves the smallest remainder.
        # This is often achieved by `1 / (diff + epsilon)`.
        
        # What if we modify the proximity score based on the "usefulness" of the resulting capacity?
        # A useful capacity is one that can fit other items.
        # Let's consider the "ideal" remainder to be around half of the bin capacity (as a guess).
        # Score = proximity_score * exp(-(resulting_remainder - target_remainder)^2 / (2 * variance))
        # This favors remainders close to the target.
        
        # For a simpler heuristic, let's just penalize large remainders more.
        # Score = proximity_score * (1 / (1 + resulting_remainder))
        # This aims for a tight fit.
        
        # Let's try to make it more "adaptive" by considering the *range* of fitting bins.
        # If all fitting bins have very similar remaining capacities, the proximity score is enough.
        # If fitting bins have a wide range of capacities, maybe we should prefer those that are not "extreme" in their leftover space.
        
        # Let's use a score that rewards proximity and penalizes large resulting remainders.
        # Score = (1 / (difference + 1e-9)) * (1 / (1 + resulting_remainders))
        # This effectively gives higher scores to bins that are a good fit AND leave a small remainder.
        
        # Let's try to introduce a slight bias away from perfectly filling bins,
        # by penalizing very small remainders, but still favoring tight fits overall.
        
        # Score = (1 / (difference + 1e-9)) * (resulting_remainders + 1) / (resulting_remainders + 1 + small_penalty_term)
        
        # Let's try this adaptive approach:
        # Combine proximity with a score that reflects how "full" the bin becomes.
        # We want bins to be full, but not *so* full that the remainder is unusable.
        # Let's use a score that is high for small differences, but decreases if the resulting remainder is too small.
        
        # Score = 1.0 / (differences + 1e-9)  # Proximity
        # Let's modify this by penalizing small resulting remainders.
        # If resulting_remainder < some_threshold (e.g., 10% of bin capacity): penalize.
        
        # Let's make the priority score a function of both the difference AND the resulting remainder,
        # aiming for a balance between a close fit and a useful leftover space.
        
        # Score = (1 / (difference + 1e-9)) * (1 / (1 + resulting_remainders))
        # This prioritizes bins that are a close fit AND leave a small remainder. This encourages tight packing.
        
        # Let's try to achieve "balanced packing" by rewarding remainders that are not too small.
        # Score = proximity_score * (resulting_remainders + 1) / (resulting_remainders + 1 + alpha * small_remainder_penalty)
        
        # A simple adaptive heuristic:
        # Prioritize bins that are a close fit (proximity score).
        # Then, among those, prioritize the ones that leave a remainder that is "balanced".
        # Balanced remainder: not too small, not too large.
        # Let's use a score that favors remainders in a medium range.
        
        # Let's try to make the priority score proportional to the proximity, but also inversely proportional to the resulting remainder.
        # This encourages close fits AND small remainders.
        
        resulting_remainders = fitting_bins_cap - item
        
        # Let's use a combined score that emphasizes both proximity and small resulting remainders.
        # Score = (1 / (difference + 1e-9)) * (1 / (resulting_remainders + 1e-9))
        # This effectively prioritizes bins that are a very close fit and leave a minimal remainder.
        
        # To make it more "adaptive", let's consider the ratio of difference to resulting remainder.
        # Or, let's modify the proximity score based on the *quality* of the resulting remainder.
        
        # Let's define a "remainder quality" score.
        # If `r` is the remainder, and `max_r` is the maximum remainder among fitting bins.
        # Quality could be `1 - (r / max_r)` (favors smaller remainders).
        
        # Let's combine proximity and a penalty for large remainders.
        # Score = proximity_score * (1 / (1 + resulting_remainders))
        
        # This still heavily favors the absolute closest fit.
        
        # Let's try to add a term that rewards remainders that are not too small.
        # Score = proximity_score + alpha * log(resulting_remainders + 1)
        # This adds a bonus for larger remainders, but log scales it down.
        
        # Let's try the simplest form of adaptiveness:
        # If multiple bins have the same best proximity score, choose the one with the smallest remainder.
        # The `1 / (diff + epsilon)` already does this.
        
        # Let's try to make the priority score less sensitive to minuscule differences by capping the inverse.
        # Or by using a different function like `1 / (1 + diff^2)`.
        
        # Let's introduce an adaptive component by considering the *distribution* of remaining capacities.
        # If the item is small and fits in many bins, we can be more selective.
        # If the item is large and fits in few bins, we might need to accept larger remainders.
        
        # Let's try to achieve a "balanced" packing:
        # Prioritize bins that are a close fit.
        # Then, among those, prioritize bins that leave a remainder that is not too small.
        # Score = proximity_score * (1 + alpha * (resulting_remainders / avg_fitting_remainder))
        # This would favor bins that are a close fit AND leave a *larger* remainder. This is counterproductive.
        
        # Let's try rewarding bins that leave a remainder that is "useful".
        # A useful remainder might be one that's not too small.
        
        # Score = proximity_score * f(resulting_remainders)
        # Where f is a function that is high for medium remainders and low for small/large remainders.
        # For simplicity, let's just penalize large remainders.
        
        # Score = proximity_score * (1 / (1 + resulting_remainders))
        # This favors bins that are a close fit and leave a small remainder.
        
        # Let's try to make it more adaptive by considering the *scale* of the differences.
        # If all differences are large, `1/diff` might not differentiate well.
        # If all differences are small, `1/diff` can lead to extreme values.
        
        # Let's use the inverse of the *normalized* difference.
        # normalized_diff = (fitting_bins_cap - item) / fitting_bins_cap
        # Score = 1 / (normalized_diff + epsilon)
        
        # This prioritizes bins where the item takes up a larger proportion of the bin.
        
        # Let's combine proximity and "fullness" of the bin.
        # Fullness Score = item / fitting_bins_cap
        # Proximity Score = 1 / (fitting_bins_cap - item + epsilon)
        
        # Let's try to create a score that is sensitive to the *relative* closeness.
        # Consider the "gap" = `fitting_bins_cap - item`.
        # We want small gaps.
        # Let's also consider the "fill ratio" = `item / fitting_bins_cap`.
        # We want high fill ratio, but small gaps.
        
        # Let's try a score that rewards both:
        # Score = (item / fitting_bins_cap) * (1 / (fitting_bins_cap - item + epsilon))
        # This multiplies the fill ratio by the proximity score.
        # A bin that is almost full and has a small remaining capacity will get a high score.
        
        fill_ratios = item / fitting_bins_cap
        differences = fitting_bins_cap - item
        
        # Combined score: higher fill ratio AND smaller difference.
        # Let's try multiplying them.
        # Score = fill_ratios * (1 / (differences + 1e-9))
        
        # Example: Bin Cap = 100
        # Item = 30: fits in bins with caps >= 30.
        # Bin 1: rem_cap = 40. diff = 10. fill_ratio = 30/40 = 0.75. score = 0.75 * (1/10) = 0.075
        # Bin 2: rem_cap = 35. diff = 5.  fill_ratio = 30/35 = 0.857. score = 0.857 * (1/5) = 0.1714
        # Bin 3: rem_cap = 100. diff = 70. fill_ratio = 30/100 = 0.3. score = 0.3 * (1/70) = 0.004
        
        # This seems to work well: it favors bins that are close fits AND have a high fill ratio.
        # The fill ratio component implicitly penalizes bins that would leave very large remainders.
        
        # Now, we need to normalize these scores.
        
        priorities[can_fit_mask] = fill_ratios * (1.0 / (differences + 1e-9))
        
        # Normalize priorities so that the best fit bin has a score of 1
        max_priority = np.max(priorities[can_fit_mask])
        if max_priority > 0:
            priorities[can_fit_mask] /= max_priority

    return priorities
```
