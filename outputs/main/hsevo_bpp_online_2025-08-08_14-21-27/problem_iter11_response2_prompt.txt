{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a penalty for very small remaining capacity,\n    favoring tight fits while slightly discouraging bins that become overly full.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fitting_bins_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Best Fit Score: Inverse of remaining capacity after placement.\n    # Higher score for bins that leave less capacity (tighter fit).\n    best_fit_scores = 1.0 / (fitting_bins_cap - item + 1e-9)\n\n    # Adaptive Penalty for Small Residuals: Penalize bins that leave a very small remainder.\n    # This aims to avoid situations where a bin is *too* full, potentially leaving no room\n    # for even slightly larger items later. We use an inverse relationship with the remainder.\n    # A small remainder (e.g., 0.1) gets a lower penalty score (e.g., 0.1 / (0.1 + 0.5) ~ 0.16),\n    # while a larger remainder (e.g., 10) gets a higher penalty score (e.g., 10 / (10 + 0.5) ~ 0.95).\n    # We want to *discourage* very small remainders, so we'll use this score to adjust the best_fit_scores.\n    # Specifically, we'll multiply the best_fit_scores by a factor that decreases as the remainder gets smaller.\n    # Let's invert this penalty concept: a *good* residual quality score should be *high* for moderate remainders.\n    # Instead of penalty, let's frame it as a \"residual quality bonus\" where small residuals are penalized.\n    # A simple penalty function for small residuals: exp(-residual / sensitivity)\n    # Where sensitivity is a parameter controlling how quickly the penalty kicks in.\n    sensitivity = 2.0  # Controls how strongly small remainders are penalized.\n    residual_quality_factor = np.exp(-(fitting_bins_cap - item) / sensitivity)\n\n\n    # Combine scores: Multiply Best Fit by the residual quality factor.\n    # This prioritizes tight fits (high best_fit_scores) but reduces their priority\n    # if they leave an extremely small remainder (low residual_quality_factor).\n    combined_scores = best_fit_scores * residual_quality_factor\n\n    # Normalize combined scores to [0, 1] to make them comparable across different calls.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a fill-ratio bonus, favoring bins that are both a good fit and already utilized.\n    This heuristic prioritizes bins that are nearly full and leave minimal residual capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Filter bins that can fit the item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Calculate the remaining capacity after fitting the item\n    remaining_capacity_after_fit = fitting_bins_remain_cap - item\n    \n    # --- Best Fit Component ---\n    # Score is inversely proportional to the residual capacity after fitting.\n    # Adding a small epsilon to avoid division by zero.\n    # A score of 1.0 means the bin is perfectly filled after adding the item.\n    proximity_score = 1.0 / (remaining_capacity_after_fit + 1e-9)\n    \n    # --- Fill Ratio Component ---\n    # Assumes a default bin capacity of 1.0.\n    # Fill ratio is the proportion of capacity already used. Higher is better.\n    bin_capacity = 1.0 \n    current_fill_ratio = (bin_capacity - fitting_bins_remain_cap) / bin_capacity\n    fill_ratio_score = current_fill_ratio\n    \n    # --- Combined Score ---\n    # Multiply proximity score by fill ratio score.\n    # This prioritizes bins that are both a good fit (high proximity) AND already well-utilized (high fill ratio).\n    # The idea is to prefer bins that are already \"almost full\" and can still accommodate the item snugly.\n    combined_score = proximity_score * fill_ratio_score\n    \n    # --- Refinement: Ensure proximity is considered for empty bins ---\n    # If a bin was empty (fill_ratio=0), combined_score would be 0.\n    # We want to ensure that even in this case, the proximity score is still considered,\n    # as an empty bin might be the only option or a good first fit.\n    # We take the maximum of the combined score and the proximity score itself,\n    # effectively giving proximity score at least its due when fill ratio is zero.\n    priorities[can_fit_mask] = np.maximum(combined_score, proximity_score * (fill_ratio_score > 1e-9))\n    \n    # Add a small epsilon to all valid priorities to ensure that even if\n    # all scores are very low, they are distinct and positive, aiding tie-breaking.\n    priorities[can_fit_mask] += 1e-6\n    \n    return priorities\n\n### Analyze & experience\n- *   **Heuristics 1st vs. Heuristics 2nd:** These are identical. The ranking suggests a tie or a misunderstanding in the prompt.\n*   **Heuristics 3rd vs. Heuristics 4th:** Heuristic 3rd uses a multiplicative combination of Best Fit and Fill Ratio. Heuristic 4th uses Best Fit multiplied by an exponential `residual_quality_factor`. Heuristic 3rd's multiplicative approach with a simple fill ratio is conceptually clearer for dense packing than the exponential penalty on small residuals, which might be too aggressive or not well-calibrated without tuning.\n*   **Heuristics 5th vs. Heuristics 6th:** Heuristic 5th is identical to Heuristic 4th. Heuristic 6th introduces multiple scores (Best Fit, Worst Fit, Fullness, Fit Tightness Penalty) and adaptive weights, making it more complex and potentially harder to tune than simpler combinations. The \"adaptive weights\" in Heuristic 6th seem intended to react to item size but are not clearly defined or implemented in a way that demonstrably improves upon simpler strategies without extensive parameter tuning.\n*   **Heuristics 7th vs. Heuristics 8th:** Heuristic 7th is identical to Heuristic 6th. Heuristic 8th combines Best Fit with a simple \"fullness bonus\" (inverse of remaining capacity). This is less sophisticated than Heuristic 1st or 3rd, as it doesn't directly penalize large residual gaps or explicitly consider the \"tightness\" of the fit in its bonus calculation.\n*   **Heuristics 9th vs. Heuristics 10th:** Heuristic 9th is identical to Heuristics 4th and 5th. Heuristic 10th is identical to Heuristic 3rd.\n*   **Heuristics 11th vs. Heuristics 12th:** Heuristic 11th combines Best Fit proximity with a logarithmic \"Usefulness Bonus\" on remaining capacity. Heuristic 12th combines tightness, fullness, and a negative logarithmic \"Future Fit Score.\" Heuristic 12th's approach is more comprehensive, attempting to balance multiple aspects (tightness, fullness, and not leaving *too* much space) but with a subtraction of the future fit score, which might be counter-intuitive. Heuristic 11th's multiplicative approach with `(1 + 0.2 * adaptive_bonus)` is a more direct way to boost good fits that also leave reasonable space.\n*   **Heuristics 13th vs. Heuristics 14th:** These are identical. They use adaptive weighting based on item size ratio to balance Best Fit and Future Potential. The exponential weighting is a reasonable approach for adaptability.\n*   **Heuristics 15th vs. Heuristics 16th:** Heuristic 15th is identical to Heuristics 13th and 14th. Heuristic 16th is incomplete, only containing imports and docstrings, and includes unused library imports (random, math, scipy, torch).\n*   **Heuristics 17th vs. Heuristics 18th:** Heuristics 17th and 20th are identical. They combine Best Fit (proximity) and Fill Ratio, using a multiplicative approach, with a refinement to handle empty bins. Heuristic 18th is identical to Heuristic 16th, incomplete.\n*   **Heuristics 19th vs. Heuristics 20th:** Heuristic 19th is identical to Heuristics 17th and 20th.\n\n**Overall:** The higher-ranked heuristics (1st, 3rd, 11th, 17th/19th/20th) tend to combine Best Fit with another factor (fullness, fill ratio, or a bonus for moderate remaining space) using either additive or multiplicative logic. Lower-ranked heuristics either introduce too many complex, potentially conflicting scores (6th, 7th), use simpler additive combinations without nuanced logic (8th), or are incomplete (16th, 18th). Heuristics that attempt adaptive weighting based on item size (13th, 14th, 15th) are conceptually good but rely on careful tuning of those weights. The multiplicative approach of combining Best Fit with a Fill Ratio (3rd, 10th, 17th, 19th, 20th) seems robust.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Multi-objective optimization, multiplicative scoring, adaptive weighting, robustness, simplification.\n*   **Advice:** Design heuristics that explicitly balance immediate packing gains (e.g., Best Fit) with long-term bin utilization (e.g., fill ratio). Experiment with multiplicative combinations of these metrics.\n*   **Avoid:** Redundant implementations, overly complex scoring functions without clear justification, and heuristics that don't directly address the trade-off between immediate and future packing efficiency.\n*   **Explanation:** Focusing on combining well-performing, conceptually distinct metrics multiplicatively creates synergistic effects. Avoiding complexity ensures interpretability and easier tuning, leading to more robust and generalizable heuristics.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}