{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines the \"Fill Ratio * Proximity\" score with a sigmoid scaling\n    to adaptively prioritize bins that are both a close fit and well-utilized,\n    while amplifying differences for bins near the median fit.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_cap.size > 0:\n        # Calculate Proximity: Higher score for bins closer to the item size.\n        # Using 1 / (difference + epsilon) to prioritize minimal remaining capacity.\n        differences = fitting_bins_cap - item\n        proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Calculate Fill Ratio: Higher score for bins that are already well-utilized.\n        # This penalizes bins with excessively large remaining capacities implicitly.\n        fill_ratios = item / fitting_bins_cap\n        \n        # Combine Proximity and Fill Ratio multiplicatively.\n        # This favors bins that are both a good fit (small difference) and already have\n        # a high fill ratio (meaning they are not wasting much space).\n        combined_scores = fill_ratios * proximity_scores\n        \n        # Adaptive Sigmoid Scaling: Applied to the combined scores.\n        # This part aims to adapt to the distribution of fits.\n        # It amplifies the differences for bins whose combined score is close to the median,\n        # making the heuristic more sensitive to \"average\" good fits.\n        median_score = np.median(combined_scores)\n        k = 5.0  # Sensitivity parameter for the sigmoid. Adjust as needed.\n        # Sigmoid function: 1 / (1 + exp(-k * (x - median)))\n        # This function is close to 0 for x << median, 0.5 at x = median, and close to 1 for x >> median.\n        # Multiplying the combined_scores by this sigmoid scales them up if they are better than the median,\n        # and down if they are worse, accentuating the differences.\n        adaptive_scaling = 1 / (1 + np.exp(-k * (combined_scores - median_score)))\n        \n        final_priorities = combined_scores * adaptive_scaling\n\n        # Normalize the final priorities for the bins that can fit the item.\n        # This ensures that the highest priority bin has a score of 1.0,\n        # providing a consistent scale for comparison.\n        max_priority = np.max(final_priorities)\n        if max_priority > 0:\n            priorities[can_fit_mask] = final_priorities / max_priority\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines proximity fit with a bonus for leaving 'useful' remaining capacity,\n    favoring bins that are a close fit but not so tight they leave minimal space.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_cap.size > 0:\n        # Proximity Score: Higher for bins closer to the item size.\n        differences = fitting_bins_cap - item\n        proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Adaptive \"Usefulness\" Score: Penalize bins that leave very small or very large remainders.\n        # A simple approach is to reward remainders that are not too small.\n        # Here, we reward remainders that are greater than a small fraction of the item size.\n        # This encourages leaving some space for potential future items.\n        # We use a linear scaling for remainders up to a certain point, and then a decaying score.\n        # For simplicity, let's reward larger remainders, but capped to avoid dominance.\n        # A simpler adaptive heuristic: reward bins that leave a moderate remainder.\n        # We can use 1 + (remainder / average_remainder) but that can be unstable.\n        # Let's try a score that favors remainders that are a fraction of the bin's original capacity.\n        # Consider a score that peaks when the remainder is around 20-30% of the bin capacity.\n        \n        # A more direct adaptive approach: Favor bins that are a good fit (proximity)\n        # AND leave a remainder that is not excessively small.\n        # Let's use a score that is `proximity_score * (1 + residual_bonus)`\n        # Where residual_bonus is higher for moderate residuals.\n        # A simple way: `residual_bonus = log(1 + residual)` scaled.\n\n        # Let's combine: Proximity score * (1 + log(1 + resulting_remainder))\n        # This rewards close fits and adds a bonus for leaving more space.\n        resulting_remainders = fitting_bins_cap - item\n        \n        # We want to avoid penalizing very small remainders too harshly, as tight fits are good.\n        # Let's try to make the bonus more significant for larger remainders.\n        # The log function provides diminishing returns.\n        \n        # A simpler combination: proximity * (1 + scaled_remainder).\n        # Let's use a score that reflects how \"full\" the bin becomes, but with a penalty for being *too* full.\n        # Fill ratio: item / fitting_bins_cap\n        # This implicitly penalizes very large remainders.\n        \n        # Let's combine proximity with fill ratio as in heuristic_v0, but add a slight penalty for very small remainders.\n        fill_ratios = item / fitting_bins_cap\n        \n        # We want to prioritize bins that are a good fit AND have a high fill ratio.\n        # Combined score: `fill_ratios * (1 / (differences + epsilon))`\n        # This inherently favors bins that are nearly full and have a small gap.\n        \n        # To make it more adaptive, let's add a term that slightly penalizes\n        # bins that would leave an extremely small remainder.\n        # If `resulting_remainders` is very small, reduce the score.\n        # For example, `1 / (1 + resulting_remainder)` rewards small remainders.\n        # We want the opposite: penalize small remainders.\n        # Let's use `(resulting_remainders + 1) / (resulting_remainders + 1 + penalty_factor)`\n        # where penalty_factor is small.\n        \n        penalty_factor = 5.0 # Adjust this parameter to tune the penalty\n        remainder_quality_score = (resulting_remainders + 1) / (resulting_remainders + 1 + penalty_factor)\n        \n        # Combine proximity, fill ratio, and remainder quality.\n        # The fill_ratio * proximity already favors tight fits.\n        # The remainder_quality_score will slightly boost bins with moderate remainders\n        # and slightly reduce bins with extremely small remainders.\n        \n        # Let's try a simpler combination:\n        # Prioritize bins that are a close fit (proximity_score).\n        # Among those, favor bins that leave a remainder that's not too small.\n        # Score = proximity_score * (1 + alpha * log(resulting_remainders + 1))\n        \n        alpha = 0.1 # Tuning parameter for the bonus\n        adaptive_bonus = np.log(1 + resulting_remainders + 1e-9)\n        \n        # Combine proximity and the adaptive bonus.\n        # The proximity term drives towards the tightest fit.\n        # The adaptive bonus slightly favors bins that leave more space,\n        # making it less greedy on the absolute tightest fit.\n        priorities[can_fit_mask] = proximity_scores + alpha * adaptive_bonus\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1st and 2nd, 1st uses `eligible_bins / (max_eligible_cap + 1e-9)` for fill ratio, while 2nd uses `fittable_bins_remain_cap / (max_eligible_cap + 1e-9)`. 1st's approach seems more directly related to how full the bin is relative to other fitting bins.\n\nComparing Heuristics 2nd and 7th (which are identical), no change is observed.\n\nComparing Heuristics 4th and 8th (which are identical), no change is observed.\n\nComparing Heuristics 5th and Heuristics 14th/15th/16th: Heuristics 14-16 attempt to incorporate an \"adaptive bonus\" based on the logarithm of the resulting remainder, aiming to balance tight fits with leaving some space. Heuristic 5th solely relies on inverse difference (Best Fit). The inclusion of an adaptive bonus in 14-16 suggests a more nuanced approach than simple Best Fit.\n\nComparing Heuristics 10th/11th/12th/13th with others: These heuristics combine proximity with fill ratio. Heuristic 10th's `proximity_score * fill_ratio_score` seems like a straightforward combination. The `np.maximum(combined_score, proximity_score * (current_fill_ratio > 1e-9))` attempts to ensure proximity is considered even for empty bins, which is a good robustness measure.\n\nComparing Heuristics 17th/18th/19th/20th: These heuristics use `base_priority + adaptive_bonus`, where `base_priority` is Best Fit and `adaptive_bonus` is `remaining_capacity_after_fit * 0.5`. This additive approach for the bonus is a simpler alternative to multiplicative combinations seen in other heuristics.\n\nOverall: Heuristics that combine multiple factors (Best Fit, Fill Ratio, adaptive bonuses) tend to be ranked higher. The specific combination method (multiplicative vs. additive) and the nature of the adaptive bonus (logarithmic, linear scaling) seem to differentiate performance. Normalization is consistently applied to keep scores in a comparable range.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Objective Alignment, Trade-offs, Robustness, Predictive Power.\n*   **Advice:** Focus on aligning heuristic scoring with underlying optimization goals, explicitly modeling trade-offs between immediate gains and future packing potential. Use adaptive mechanisms that learn or respond to problem state for enhanced predictive power.\n*   **Avoid:** Redundant comparisons of near-identical strategies. Overly simplistic or static weighting schemes that don't account for dynamic problem states or interactions between objectives.\n*   **Explanation:** Effective self-reflection identifies *why* a heuristic works or fails, focusing on the underlying principles and interactions. This allows for the creation of more sophisticated, robust, and predictive heuristics by understanding the nuanced trade-offs and potential future implications of each decision.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}