{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (proximity) with a bonus for bins that are already well-utilized,\n    aiming for denser packing by prioritizing items that fit snugly into nearly full bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    eligible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(eligible_bins_mask):\n        return priorities\n\n    eligible_bins = bins_remain_cap[eligible_bins_mask]\n\n    # Score 1: Best Fit - favors bins with minimal remaining capacity after packing.\n    # Higher score for smaller remaining capacity (closer fit). Add epsilon to avoid division by zero.\n    best_fit_scores = 1.0 / (eligible_bins - item + 1e-9)\n\n    # Score 2: Fill Ratio - favors bins that are already more full.\n    # This is the inverse of remaining capacity, normalized.\n    # Max possible remaining capacity could be the bin capacity, but we use max eligible bin's remaining\n    # capacity as a relative measure of \"fullness\" among fitting bins.\n    max_eligible_cap = np.max(eligible_bins)\n    fill_ratio_scores = eligible_bins / (max_eligible_cap + 1e-9)\n\n    # Combine scores multiplicatively: prioritize bins that are both a good fit and already full.\n    combined_scores = best_fit_scores * fill_ratio_scores\n\n    # Assign combined scores to the priorities array\n    priorities[eligible_bins_mask] = combined_scores\n\n    # Normalize priorities to the range [0, 1]\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a bonus for bins that are already quite full,\n    prioritizing tight fits while encouraging fuller bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Score based on Best Fit: lower remaining capacity after placement is better.\n    # Adding a small epsilon to avoid division by zero and make it slightly less sensitive to exact fits.\n    best_fit_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Bonus for already full bins: higher bonus for bins with less remaining capacity.\n    # This encourages packing into bins that are already well-utilized.\n    # We use the original remaining capacity here.\n    fullness_bonus = 1.0 / (fittable_bins_remain_cap + 1e-6)\n\n    # Combine Best Fit and Fullness Bonus: multiplicative approach to balance both.\n    # This prioritizes bins that are both a good fit and already have high utilization.\n    combined_scores = best_fit_scores * fullness_bonus\n\n    # Normalize priorities to prevent extremely large values and ensure comparability.\n    if np.max(combined_scores) > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / np.max(combined_scores)\n    else:\n        # If all combined scores are near zero (e.g., all bins have huge remaining capacity\n        # or item is very small), just assign a uniform small priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines 'Best Fit' with a bonus for bins that leave minimal residual capacity.\n    Favors bins that are a tight fit and result in less wasted space.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fitting_bins_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Calculate \"Best Fit\" score: inverse of remaining capacity after placement.\n    # Higher score for bins that leave less capacity after fitting the item.\n    best_fit_scores = 1.0 / (fitting_bins_cap - item + 1e-9)\n\n    # Calculate \"Minimal Residual\" bonus: This is related to the 'fullness' of the bin *after* packing.\n    # We want bins that are almost full after packing.\n    # A simple way is to take the fill ratio of the *remaining* capacity,\n    # but we want to encourage *low* remaining capacity.\n    # So, let's use the inverse of the remaining capacity, similar to best fit.\n    # However, to make it distinct and penalize *very* large residuals more,\n    # we can use a slightly different formulation.\n    # Let's use the inverse of (1 + residual capacity). This gives a bonus to smaller residuals.\n    # A residual of 0 gets a bonus of 1. A residual of 1 gets a bonus of 0.5.\n    # A residual of 10 gets a bonus of ~0.09.\n    minimal_residual_bonus = 1.0 / (1.0 + (fitting_bins_cap - item) + 1e-9)\n\n    # Combine scores: Multiply best fit by the minimal residual bonus.\n    # This prioritizes bins that are both a good fit AND leave little remainder.\n    combined_scores = best_fit_scores * minimal_residual_bonus\n\n    # Normalize combined scores to [0, 1]\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight fit preference with a bonus for bins that are nearly full,\n    prioritizing efficient use of space and minimizing waste.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    \n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    \n    if fitting_bins_cap.size > 0:\n        differences = fitting_bins_cap - item\n        \n        # Proximity score: Inverse of the difference (higher for tighter fits)\n        proximity_scores = 1.0 / (differences + 1e-9)\n        \n        # Fullness bonus: Higher for bins that are already more utilized\n        # This rewards bins that are \"nearly full\" and can still accommodate the item.\n        # We use the inverse of the remaining capacity as a proxy for fullness.\n        fullness_bonus = fitting_bins_cap / (np.max(bins_remain_cap) + 1e-9) \n        \n        # Combine scores: Multiply proximity by fullness bonus.\n        # This favors bins that are both a tight fit AND are already substantially filled.\n        combined_scores = proximity_scores * (1 + fullness_bonus * 0.5) # Slightly weight the fullness bonus\n        \n        priorities[can_fit_mask] = combined_scores\n        \n        # Normalize priorities to ensure the highest score is 1\n        max_priority = np.max(priorities[can_fit_mask])\n        if max_priority > 0:\n            priorities[can_fit_mask] /= max_priority\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority scores for bins using the Inverse Distance (Proximity Fit) strategy.\n    The closer a bin's remaining capacity is to the item size, the higher its priority.\n    Bins that can fit the item will have a non-zero priority, while those that cannot will have zero.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    \n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    \n    if fitting_bins_cap.size > 0:\n        differences = fitting_bins_cap - item\n        # Add a small epsilon to avoid division by zero if an item perfectly fits a bin\n        priorities[can_fit_mask] = 1.0 / (differences + 1e-9)\n        \n        # Normalize priorities so that the best fit bin has a score of 1\n        max_priority = np.max(priorities[can_fit_mask])\n        if max_priority > 0:\n            priorities[can_fit_mask] /= max_priority\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins by a combination of Best Fit and encouraging bins with moderate remaining capacity.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    potential_remaining_cap = bins_remain_cap[can_fit_mask] - item\n    \n    # Best Fit component: favors bins that leave minimal remaining capacity.\n    best_fit_score = 1.0 / (potential_remaining_cap + 1e-9)\n    \n    # Utility component: favors bins that leave a moderate amount of capacity,\n    # balancing tight fits with potential for future items. We use a Gaussian-like\n    # approach centered on a moderate residual (e.g., half the item size)\n    # to provide a bonus to bins that aren't extremely tight or extremely loose.\n    # The 'center' is heuristic and could be tuned. Here, we use item/2 as a simple center.\n    moderate_capacity_center = item / 2.0\n    # Ensure we don't get negative exponents for negative remainders (already filtered by can_fit_mask)\n    utility_score = np.exp(-((potential_remaining_cap - moderate_capacity_center)**2) / (2 * (moderate_capacity_center**2)))\n\n    # Combine scores: Multiplies Best Fit with a utility bonus.\n    # This favors bins that are both a good fit and leave useful remaining capacity.\n    combined_score = best_fit_score * utility_score\n    \n    # Assign priorities to the valid bins\n    priorities[can_fit_mask] = combined_score\n\n    # Normalize priorities to prevent extreme values and ensure a relative ranking\n    if np.any(priorities):\n        max_priority = np.max(priorities)\n        if max_priority > 1e-9: # Avoid division by zero if all priorities are near zero\n            priorities[can_fit_mask] /= max_priority\n        else: # If all are near zero, assign a small uniform priority to all valid bins\n            priorities[can_fit_mask] = 1e-6\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a bonus for bins that are already quite full,\n    prioritizing tight fits while encouraging fuller bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Score based on Best Fit: lower remaining capacity after placement is better.\n    # Adding a small epsilon to avoid division by zero and make it slightly less sensitive to exact fits.\n    best_fit_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Bonus for already full bins: higher bonus for bins with less remaining capacity.\n    # This encourages packing into bins that are already well-utilized.\n    # We use the original remaining capacity here.\n    fullness_bonus = 1.0 / (fittable_bins_remain_cap + 1e-6)\n\n    # Combine Best Fit and Fullness Bonus: multiplicative approach to balance both.\n    # This prioritizes bins that are both a good fit and already have high utilization.\n    combined_scores = best_fit_scores * fullness_bonus\n\n    # Normalize priorities to prevent extremely large values and ensure comparability.\n    if np.max(combined_scores) > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / np.max(combined_scores)\n    else:\n        # If all combined scores are near zero (e.g., all bins have huge remaining capacity\n        # or item is very small), just assign a uniform small priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight fit preference with a bonus for bins that are nearly full,\n    prioritizing efficient use of space and minimizing waste.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    \n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    \n    if fitting_bins_cap.size > 0:\n        differences = fitting_bins_cap - item\n        \n        # Proximity score: Inverse of the difference (higher for tighter fits)\n        proximity_scores = 1.0 / (differences + 1e-9)\n        \n        # Fullness bonus: Higher for bins that are already more utilized\n        # This rewards bins that are \"nearly full\" and can still accommodate the item.\n        # We use the inverse of the remaining capacity as a proxy for fullness.\n        fullness_bonus = fitting_bins_cap / (np.max(bins_remain_cap) + 1e-9) \n        \n        # Combine scores: Multiply proximity by fullness bonus.\n        # This favors bins that are both a tight fit AND are already substantially filled.\n        combined_scores = proximity_scores * (1 + fullness_bonus * 0.5) # Slightly weight the fullness bonus\n        \n        priorities[can_fit_mask] = combined_scores\n        \n        # Normalize priorities to ensure the highest score is 1\n        max_priority = np.max(priorities[can_fit_mask])\n        if max_priority > 0:\n            priorities[can_fit_mask] /= max_priority\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines the \"Fill Ratio * Proximity\" score with a sigmoid scaling\n    to adaptively prioritize bins that are both a close fit and well-utilized,\n    while amplifying differences for bins near the median fit.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_cap.size > 0:\n        # Calculate Proximity: Higher score for bins closer to the item size.\n        # Using 1 / (difference + epsilon) to prioritize minimal remaining capacity.\n        differences = fitting_bins_cap - item\n        proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Calculate Fill Ratio: Higher score for bins that are already well-utilized.\n        # This penalizes bins with excessively large remaining capacities implicitly.\n        fill_ratios = item / fitting_bins_cap\n        \n        # Combine Proximity and Fill Ratio multiplicatively.\n        # This favors bins that are both a good fit (small difference) and already have\n        # a high fill ratio (meaning they are not wasting much space).\n        combined_scores = fill_ratios * proximity_scores\n        \n        # Adaptive Sigmoid Scaling: Applied to the combined scores.\n        # This part aims to adapt to the distribution of fits.\n        # It amplifies the differences for bins whose combined score is close to the median,\n        # making the heuristic more sensitive to \"average\" good fits.\n        median_score = np.median(combined_scores)\n        k = 5.0  # Sensitivity parameter for the sigmoid. Adjust as needed.\n        # Sigmoid function: 1 / (1 + exp(-k * (x - median)))\n        # This function is close to 0 for x << median, 0.5 at x = median, and close to 1 for x >> median.\n        # Multiplying the combined_scores by this sigmoid scales them up if they are better than the median,\n        # and down if they are worse, accentuating the differences.\n        adaptive_scaling = 1 / (1 + np.exp(-k * (combined_scores - median_score)))\n        \n        final_priorities = combined_scores * adaptive_scaling\n\n        # Normalize the final priorities for the bins that can fit the item.\n        # This ensures that the highest priority bin has a score of 1.0,\n        # providing a consistent scale for comparison.\n        max_priority = np.max(final_priorities)\n        if max_priority > 0:\n            priorities[can_fit_mask] = final_priorities / max_priority\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (proximity) with a \"Fill Ratio\" bonus for bins that are already well-utilized.\n    This encourages using bins that are both a good fit and already contain a significant amount of items.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the remaining capacity for bins that can fit the item\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Calculate a \"proximity score\" (Best Fit): higher score for smaller remaining capacity\n    # Using 1 / (1 + residual_capacity) gives a score between (0, 1]. 0 residual is 1.\n    proximity_score = 1.0 / (1.0 + remaining_capacity_after_fit)\n    \n    # Calculate a \"fill ratio score\": higher score for bins that are already more full.\n    # This is (bin_capacity - remaining_capacity) / bin_capacity.\n    # Assuming a default bin capacity of 1.0 for calculating fill ratio if not provided.\n    # In a real scenario, bin_capacity would be a known constant.\n    bin_capacity = 1.0 \n    current_fill_ratio = (bin_capacity - bins_remain_cap[can_fit_mask]) / bin_capacity\n    fill_ratio_score = current_fill_ratio\n    \n    # Combine scores: Multiply proximity score by fill ratio score.\n    # This prioritizes bins that are both a good fit AND already well-utilized.\n    # A bin that has high fill ratio and leaves minimal residual will get the highest score.\n    combined_score = proximity_score * fill_ratio_score\n\n    # Handle cases where combined_score might be 0 if fill_ratio is 0 and proximity is also low.\n    # For example, if an item fits in a nearly empty bin, fill_ratio is near 0.\n    # We want to ensure that even in such cases, the proximity score is still considered.\n    # If fill_ratio is 0, the combined_score becomes 0. We can add a small epsilon or\n    # simply use max(proximity_score, combined_score) if we want proximity to always dominate empty bins.\n    # A simpler approach: if fill_ratio is 0, the combined score is 0. This might disincentivize\n    # using an empty bin, even if it's a perfect fit.\n    # Let's ensure that if fill_ratio is 0, we still assign the proximity score.\n    # A small additive factor can help prevent zero scores if both are low but not zero.\n    # Or, more robustly, ensure proximity is at least considered.\n    \n    # If fill_ratio is 0 (meaning the bin was empty), the combined_score will be 0.\n    # In such cases, we should still consider the proximity score.\n    # We can ensure the score is at least the proximity score when the fill ratio is zero.\n    priorities[can_fit_mask] = np.maximum(combined_score, proximity_score * (current_fill_ratio > 1e-9))\n    \n    # Add a small epsilon to all valid priorities to ensure no zero scores if all bins are valid fits.\n    # This also helps break ties in a consistent manner.\n    priorities[can_fit_mask] += 1e-6\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (proximity) with a \"Fill Ratio\" bonus for bins that are already well-utilized.\n    This encourages using bins that are both a good fit and already contain a significant amount of items.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the remaining capacity for bins that can fit the item\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Calculate a \"proximity score\" (Best Fit): higher score for smaller remaining capacity\n    # Using 1 / (1 + residual_capacity) gives a score between (0, 1]. 0 residual is 1.\n    proximity_score = 1.0 / (1.0 + remaining_capacity_after_fit)\n    \n    # Calculate a \"fill ratio score\": higher score for bins that are already more full.\n    # This is (bin_capacity - remaining_capacity) / bin_capacity.\n    # Assuming a default bin capacity of 1.0 for calculating fill ratio if not provided.\n    # In a real scenario, bin_capacity would be a known constant.\n    bin_capacity = 1.0 \n    current_fill_ratio = (bin_capacity - bins_remain_cap[can_fit_mask]) / bin_capacity\n    fill_ratio_score = current_fill_ratio\n    \n    # Combine scores: Multiply proximity score by fill ratio score.\n    # This prioritizes bins that are both a good fit AND already well-utilized.\n    # A bin that has high fill ratio and leaves minimal residual will get the highest score.\n    combined_score = proximity_score * fill_ratio_score\n\n    # Handle cases where combined_score might be 0 if fill_ratio is 0 and proximity is also low.\n    # For example, if an item fits in a nearly empty bin, fill_ratio is near 0.\n    # We want to ensure that even in such cases, the proximity score is still considered.\n    # If fill_ratio is 0, the combined_score becomes 0. We can add a small epsilon or\n    # simply use max(proximity_score, combined_score) if we want proximity to always dominate empty bins.\n    # A simpler approach: if fill_ratio is 0, the combined score is 0. This might disincentivize\n    # using an empty bin, even if it's a perfect fit.\n    # Let's ensure that if fill_ratio is 0, we still assign the proximity score.\n    # A small additive factor can help prevent zero scores if both are low but not zero.\n    # Or, more robustly, ensure proximity is at least considered.\n    \n    # If fill_ratio is 0 (meaning the bin was empty), the combined_score will be 0.\n    # In such cases, we should still consider the proximity score.\n    # We can ensure the score is at least the proximity score when the fill ratio is zero.\n    priorities[can_fit_mask] = np.maximum(combined_score, proximity_score * (current_fill_ratio > 1e-9))\n    \n    # Add a small epsilon to all valid priorities to ensure no zero scores if all bins are valid fits.\n    # This also helps break ties in a consistent manner.\n    priorities[can_fit_mask] += 1e-6\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (proximity) with a \"Fill Ratio\" bonus for bins that are already well-utilized.\n    This encourages using bins that are both a good fit and already contain a significant amount of items.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the remaining capacity for bins that can fit the item\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Calculate a \"proximity score\" (Best Fit): higher score for smaller remaining capacity\n    # Using 1 / (1 + residual_capacity) gives a score between (0, 1]. 0 residual is 1.\n    proximity_score = 1.0 / (1.0 + remaining_capacity_after_fit)\n    \n    # Calculate a \"fill ratio score\": higher score for bins that are already more full.\n    # This is (bin_capacity - remaining_capacity) / bin_capacity.\n    # Assuming a default bin capacity of 1.0 for calculating fill ratio if not provided.\n    # In a real scenario, bin_capacity would be a known constant.\n    bin_capacity = 1.0 \n    current_fill_ratio = (bin_capacity - bins_remain_cap[can_fit_mask]) / bin_capacity\n    fill_ratio_score = current_fill_ratio\n    \n    # Combine scores: Multiply proximity score by fill ratio score.\n    # This prioritizes bins that are both a good fit AND already well-utilized.\n    # A bin that has high fill ratio and leaves minimal residual will get the highest score.\n    combined_score = proximity_score * fill_ratio_score\n\n    # Handle cases where combined_score might be 0 if fill_ratio is 0 and proximity is also low.\n    # For example, if an item fits in a nearly empty bin, fill_ratio is near 0.\n    # We want to ensure that even in such cases, the proximity score is still considered.\n    # If fill_ratio is 0, the combined_score becomes 0. We can add a small epsilon or\n    # simply use max(proximity_score, combined_score) if we want proximity to always dominate empty bins.\n    # A simpler approach: if fill_ratio is 0, the combined score is 0. This might disincentivize\n    # using an empty bin, even if it's a perfect fit.\n    # Let's ensure that if fill_ratio is 0, we still assign the proximity score.\n    # A small additive factor can help prevent zero scores if both are low but not zero.\n    # Or, more robustly, ensure proximity is at least considered.\n    \n    # If fill_ratio is 0 (meaning the bin was empty), the combined_score will be 0.\n    # In such cases, we should still consider the proximity score.\n    # We can ensure the score is at least the proximity score when the fill ratio is zero.\n    priorities[can_fit_mask] = np.maximum(combined_score, proximity_score * (current_fill_ratio > 1e-9))\n    \n    # Add a small epsilon to all valid priorities to ensure no zero scores if all bins are valid fits.\n    # This also helps break ties in a consistent manner.\n    priorities[can_fit_mask] += 1e-6\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (proximity) with a \"Fill Ratio\" bonus for bins that are already well-utilized.\n    This encourages using bins that are both a good fit and already contain a significant amount of items.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the remaining capacity for bins that can fit the item\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Calculate a \"proximity score\" (Best Fit): higher score for smaller remaining capacity\n    # Using 1 / (1 + residual_capacity) gives a score between (0, 1]. 0 residual is 1.\n    proximity_score = 1.0 / (1.0 + remaining_capacity_after_fit)\n    \n    # Calculate a \"fill ratio score\": higher score for bins that are already more full.\n    # This is (bin_capacity - remaining_capacity) / bin_capacity.\n    # Assuming a default bin capacity of 1.0 for calculating fill ratio if not provided.\n    # In a real scenario, bin_capacity would be a known constant.\n    bin_capacity = 1.0 \n    current_fill_ratio = (bin_capacity - bins_remain_cap[can_fit_mask]) / bin_capacity\n    fill_ratio_score = current_fill_ratio\n    \n    # Combine scores: Multiply proximity score by fill ratio score.\n    # This prioritizes bins that are both a good fit AND already well-utilized.\n    # A bin that has high fill ratio and leaves minimal residual will get the highest score.\n    combined_score = proximity_score * fill_ratio_score\n\n    # Handle cases where combined_score might be 0 if fill_ratio is 0 and proximity is also low.\n    # For example, if an item fits in a nearly empty bin, fill_ratio is near 0.\n    # We want to ensure that even in such cases, the proximity score is still considered.\n    # If fill_ratio is 0, the combined_score becomes 0. We can add a small epsilon or\n    # simply use max(proximity_score, combined_score) if we want proximity to always dominate empty bins.\n    # A simpler approach: if fill_ratio is 0, the combined score is 0. This might disincentivize\n    # using an empty bin, even if it's a perfect fit.\n    # Let's ensure that if fill_ratio is 0, we still assign the proximity score.\n    # A small additive factor can help prevent zero scores if both are low but not zero.\n    # Or, more robustly, ensure proximity is at least considered.\n    \n    # If fill_ratio is 0 (meaning the bin was empty), the combined_score will be 0.\n    # In such cases, we should still consider the proximity score.\n    # We can ensure the score is at least the proximity score when the fill ratio is zero.\n    priorities[can_fit_mask] = np.maximum(combined_score, proximity_score * (current_fill_ratio > 1e-9))\n    \n    # Add a small epsilon to all valid priorities to ensure no zero scores if all bins are valid fits.\n    # This also helps break ties in a consistent manner.\n    priorities[can_fit_mask] += 1e-6\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines proximity fit with a bonus for leaving 'useful' remaining capacity,\n    favoring bins that are a close fit but not so tight they leave minimal space.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_cap.size > 0:\n        # Proximity Score: Higher for bins closer to the item size.\n        differences = fitting_bins_cap - item\n        proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Adaptive \"Usefulness\" Score: Penalize bins that leave very small or very large remainders.\n        # A simple approach is to reward remainders that are not too small.\n        # Here, we reward remainders that are greater than a small fraction of the item size.\n        # This encourages leaving some space for potential future items.\n        # We use a linear scaling for remainders up to a certain point, and then a decaying score.\n        # For simplicity, let's reward larger remainders, but capped to avoid dominance.\n        # A simpler adaptive heuristic: reward bins that leave a moderate remainder.\n        # We can use 1 + (remainder / average_remainder) but that can be unstable.\n        # Let's try a score that favors remainders that are a fraction of the bin's original capacity.\n        # Consider a score that peaks when the remainder is around 20-30% of the bin capacity.\n        \n        # A more direct adaptive approach: Favor bins that are a good fit (proximity)\n        # AND leave a remainder that is not excessively small.\n        # Let's use a score that is `proximity_score * (1 + residual_bonus)`\n        # Where residual_bonus is higher for moderate residuals.\n        # A simple way: `residual_bonus = log(1 + residual)` scaled.\n\n        # Let's combine: Proximity score * (1 + log(1 + resulting_remainder))\n        # This rewards close fits and adds a bonus for leaving more space.\n        resulting_remainders = fitting_bins_cap - item\n        \n        # We want to avoid penalizing very small remainders too harshly, as tight fits are good.\n        # Let's try to make the bonus more significant for larger remainders.\n        # The log function provides diminishing returns.\n        \n        # A simpler combination: proximity * (1 + scaled_remainder).\n        # Let's use a score that reflects how \"full\" the bin becomes, but with a penalty for being *too* full.\n        # Fill ratio: item / fitting_bins_cap\n        # This implicitly penalizes very large remainders.\n        \n        # Let's combine proximity with fill ratio as in heuristic_v0, but add a slight penalty for very small remainders.\n        fill_ratios = item / fitting_bins_cap\n        \n        # We want to prioritize bins that are a good fit AND have a high fill ratio.\n        # Combined score: `fill_ratios * (1 / (differences + epsilon))`\n        # This inherently favors bins that are nearly full and have a small gap.\n        \n        # To make it more adaptive, let's add a term that slightly penalizes\n        # bins that would leave an extremely small remainder.\n        # If `resulting_remainders` is very small, reduce the score.\n        # For example, `1 / (1 + resulting_remainder)` rewards small remainders.\n        # We want the opposite: penalize small remainders.\n        # Let's use `(resulting_remainders + 1) / (resulting_remainders + 1 + penalty_factor)`\n        # where penalty_factor is small.\n        \n        penalty_factor = 5.0 # Adjust this parameter to tune the penalty\n        remainder_quality_score = (resulting_remainders + 1) / (resulting_remainders + 1 + penalty_factor)\n        \n        # Combine proximity, fill ratio, and remainder quality.\n        # The fill_ratio * proximity already favors tight fits.\n        # The remainder_quality_score will slightly boost bins with moderate remainders\n        # and slightly reduce bins with extremely small remainders.\n        \n        # Let's try a simpler combination:\n        # Prioritize bins that are a close fit (proximity_score).\n        # Among those, favor bins that leave a remainder that's not too small.\n        # Score = proximity_score * (1 + alpha * log(resulting_remainders + 1))\n        \n        alpha = 0.1 # Tuning parameter for the bonus\n        adaptive_bonus = np.log(1 + resulting_remainders + 1e-9)\n        \n        # Combine proximity and the adaptive bonus.\n        # The proximity term drives towards the tightest fit.\n        # The adaptive bonus slightly favors bins that leave more space,\n        # making it less greedy on the absolute tightest fit.\n        priorities[can_fit_mask] = proximity_scores + alpha * adaptive_bonus\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines proximity fit with a bonus for leaving 'useful' remaining capacity,\n    favoring bins that are a close fit but not so tight they leave minimal space.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_cap.size > 0:\n        # Proximity Score: Higher for bins closer to the item size.\n        differences = fitting_bins_cap - item\n        proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Adaptive \"Usefulness\" Score: Penalize bins that leave very small or very large remainders.\n        # A simple approach is to reward remainders that are not too small.\n        # Here, we reward remainders that are greater than a small fraction of the item size.\n        # This encourages leaving some space for potential future items.\n        # We use a linear scaling for remainders up to a certain point, and then a decaying score.\n        # For simplicity, let's reward larger remainders, but capped to avoid dominance.\n        # A simpler adaptive heuristic: reward bins that leave a moderate remainder.\n        # We can use 1 + (remainder / average_remainder) but that can be unstable.\n        # Let's try a score that favors remainders that are a fraction of the bin's original capacity.\n        # Consider a score that peaks when the remainder is around 20-30% of the bin capacity.\n        \n        # A more direct adaptive approach: Favor bins that are a good fit (proximity)\n        # AND leave a remainder that is not excessively small.\n        # Let's use a score that is `proximity_score * (1 + residual_bonus)`\n        # Where residual_bonus is higher for moderate residuals.\n        # A simple way: `residual_bonus = log(1 + residual)` scaled.\n\n        # Let's combine: Proximity score * (1 + log(1 + resulting_remainder))\n        # This rewards close fits and adds a bonus for leaving more space.\n        resulting_remainders = fitting_bins_cap - item\n        \n        # We want to avoid penalizing very small remainders too harshly, as tight fits are good.\n        # Let's try to make the bonus more significant for larger remainders.\n        # The log function provides diminishing returns.\n        \n        # A simpler combination: proximity * (1 + scaled_remainder).\n        # Let's use a score that reflects how \"full\" the bin becomes, but with a penalty for being *too* full.\n        # Fill ratio: item / fitting_bins_cap\n        # This implicitly penalizes very large remainders.\n        \n        # Let's combine proximity with fill ratio as in heuristic_v0, but add a slight penalty for very small remainders.\n        fill_ratios = item / fitting_bins_cap\n        \n        # We want to prioritize bins that are a good fit AND have a high fill ratio.\n        # Combined score: `fill_ratios * (1 / (differences + epsilon))`\n        # This inherently favors bins that are nearly full and have a small gap.\n        \n        # To make it more adaptive, let's add a term that slightly penalizes\n        # bins that would leave an extremely small remainder.\n        # If `resulting_remainders` is very small, reduce the score.\n        # For example, `1 / (1 + resulting_remainder)` rewards small remainders.\n        # We want the opposite: penalize small remainders.\n        # Let's use `(resulting_remainders + 1) / (resulting_remainders + 1 + penalty_factor)`\n        # where penalty_factor is small.\n        \n        penalty_factor = 5.0 # Adjust this parameter to tune the penalty\n        remainder_quality_score = (resulting_remainders + 1) / (resulting_remainders + 1 + penalty_factor)\n        \n        # Combine proximity, fill ratio, and remainder quality.\n        # The fill_ratio * proximity already favors tight fits.\n        # The remainder_quality_score will slightly boost bins with moderate remainders\n        # and slightly reduce bins with extremely small remainders.\n        \n        # Let's try a simpler combination:\n        # Prioritize bins that are a close fit (proximity_score).\n        # Among those, favor bins that leave a remainder that's not too small.\n        # Score = proximity_score * (1 + alpha * log(resulting_remainders + 1))\n        \n        alpha = 0.1 # Tuning parameter for the bonus\n        adaptive_bonus = np.log(1 + resulting_remainders + 1e-9)\n        \n        # Combine proximity and the adaptive bonus.\n        # The proximity term drives towards the tightest fit.\n        # The adaptive bonus slightly favors bins that leave more space,\n        # making it less greedy on the absolute tightest fit.\n        priorities[can_fit_mask] = proximity_scores + alpha * adaptive_bonus\n\n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines proximity fit with a bonus for leaving 'useful' remaining capacity,\n    favoring bins that are a close fit but not so tight they leave minimal space.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_cap.size > 0:\n        # Proximity Score: Higher for bins closer to the item size.\n        differences = fitting_bins_cap - item\n        proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Adaptive \"Usefulness\" Score: Penalize bins that leave very small or very large remainders.\n        # A simple approach is to reward remainders that are not too small.\n        # Here, we reward remainders that are greater than a small fraction of the item size.\n        # This encourages leaving some space for potential future items.\n        # We use a linear scaling for remainders up to a certain point, and then a decaying score.\n        # For simplicity, let's reward larger remainders, but capped to avoid dominance.\n        # A simpler adaptive heuristic: reward bins that leave a moderate remainder.\n        # We can use 1 + (remainder / average_remainder) but that can be unstable.\n        # Let's try a score that favors remainders that are a fraction of the bin's original capacity.\n        # Consider a score that peaks when the remainder is around 20-30% of the bin capacity.\n        \n        # A more direct adaptive approach: Favor bins that are a good fit (proximity)\n        # AND leave a remainder that is not excessively small.\n        # Let's use a score that is `proximity_score * (1 + residual_bonus)`\n        # Where residual_bonus is higher for moderate residuals.\n        # A simple way: `residual_bonus = log(1 + residual)` scaled.\n\n        # Let's combine: Proximity score * (1 + log(1 + resulting_remainder))\n        # This rewards close fits and adds a bonus for leaving more space.\n        resulting_remainders = fitting_bins_cap - item\n        \n        # We want to avoid penalizing very small remainders too harshly, as tight fits are good.\n        # Let's try to make the bonus more significant for larger remainders.\n        # The log function provides diminishing returns.\n        \n        # A simpler combination: proximity * (1 + scaled_remainder).\n        # Let's use a score that reflects how \"full\" the bin becomes, but with a penalty for being *too* full.\n        # Fill ratio: item / fitting_bins_cap\n        # This implicitly penalizes very large remainders.\n        \n        # Let's combine proximity with fill ratio as in heuristic_v0, but add a slight penalty for very small remainders.\n        fill_ratios = item / fitting_bins_cap\n        \n        # We want to prioritize bins that are a good fit AND have a high fill ratio.\n        # Combined score: `fill_ratios * (1 / (differences + epsilon))`\n        # This inherently favors bins that are nearly full and have a small gap.\n        \n        # To make it more adaptive, let's add a term that slightly penalizes\n        # bins that would leave an extremely small remainder.\n        # If `resulting_remainders` is very small, reduce the score.\n        # For example, `1 / (1 + resulting_remainder)` rewards small remainders.\n        # We want the opposite: penalize small remainders.\n        # Let's use `(resulting_remainders + 1) / (resulting_remainders + 1 + penalty_factor)`\n        # where penalty_factor is small.\n        \n        penalty_factor = 5.0 # Adjust this parameter to tune the penalty\n        remainder_quality_score = (resulting_remainders + 1) / (resulting_remainders + 1 + penalty_factor)\n        \n        # Combine proximity, fill ratio, and remainder quality.\n        # The fill_ratio * proximity already favors tight fits.\n        # The remainder_quality_score will slightly boost bins with moderate remainders\n        # and slightly reduce bins with extremely small remainders.\n        \n        # Let's try a simpler combination:\n        # Prioritize bins that are a close fit (proximity_score).\n        # Among those, favor bins that leave a remainder that's not too small.\n        # Score = proximity_score * (1 + alpha * log(resulting_remainders + 1))\n        \n        alpha = 0.1 # Tuning parameter for the bonus\n        adaptive_bonus = np.log(1 + resulting_remainders + 1e-9)\n        \n        # Combine proximity and the adaptive bonus.\n        # The proximity term drives towards the tightest fit.\n        # The adaptive bonus slightly favors bins that leave more space,\n        # making it less greedy on the absolute tightest fit.\n        priorities[can_fit_mask] = proximity_scores + alpha * adaptive_bonus\n\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive Best-Fit: Combines 'Best Fit' proximity with a bonus for bins that\n    leave a small but not zero remainder, promoting tighter packing and future flexibility.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate remaining capacity after fitting the item\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Base priority: Inverse of remaining capacity (Best Fit)\n    # Higher score for smaller remaining capacity\n    # Add a small epsilon to prevent division by zero for perfect fits\n    base_priority = 1.0 / (remaining_capacity_after_fit + 1e-9)\n    \n    # Adaptive bonus: Reward bins that leave a small, positive remainder.\n    # This encourages tighter packing while leaving a tiny bit of space,\n    # which can be beneficial for subsequent items.\n    # We use a scaled inverse of the remaining capacity for this bonus.\n    # A small positive residual gets a higher bonus.\n    # This is inspired by heuristics that balance tight fit with future utility.\n    # A simple approach is to multiply by the remaining capacity itself,\n    # effectively favoring residuals that are small but non-zero.\n    # This is similar to some elements in 'priority_v1' but without normalization.\n    adaptive_bonus = remaining_capacity_after_fit * 0.5 # A tunable factor for the bonus\n\n    # Combine base priority with adaptive bonus\n    # Higher values indicate better suitability\n    priorities[can_fit_mask] = base_priority + adaptive_bonus\n    \n    # Ensure priorities are non-negative, though logic should prevent this\n    priorities = np.maximum(0, priorities)\n    \n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive Best-Fit: Combines 'Best Fit' proximity with a bonus for bins that\n    leave a small but not zero remainder, promoting tighter packing and future flexibility.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate remaining capacity after fitting the item\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Base priority: Inverse of remaining capacity (Best Fit)\n    # Higher score for smaller remaining capacity\n    # Add a small epsilon to prevent division by zero for perfect fits\n    base_priority = 1.0 / (remaining_capacity_after_fit + 1e-9)\n    \n    # Adaptive bonus: Reward bins that leave a small, positive remainder.\n    # This encourages tighter packing while leaving a tiny bit of space,\n    # which can be beneficial for subsequent items.\n    # We use a scaled inverse of the remaining capacity for this bonus.\n    # A small positive residual gets a higher bonus.\n    # This is inspired by heuristics that balance tight fit with future utility.\n    # A simple approach is to multiply by the remaining capacity itself,\n    # effectively favoring residuals that are small but non-zero.\n    # This is similar to some elements in 'priority_v1' but without normalization.\n    adaptive_bonus = remaining_capacity_after_fit * 0.5 # A tunable factor for the bonus\n\n    # Combine base priority with adaptive bonus\n    # Higher values indicate better suitability\n    priorities[can_fit_mask] = base_priority + adaptive_bonus\n    \n    # Ensure priorities are non-negative, though logic should prevent this\n    priorities = np.maximum(0, priorities)\n    \n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive Best-Fit: Combines 'Best Fit' proximity with a bonus for bins that\n    leave a small but not zero remainder, promoting tighter packing and future flexibility.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate remaining capacity after fitting the item\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Base priority: Inverse of remaining capacity (Best Fit)\n    # Higher score for smaller remaining capacity\n    # Add a small epsilon to prevent division by zero for perfect fits\n    base_priority = 1.0 / (remaining_capacity_after_fit + 1e-9)\n    \n    # Adaptive bonus: Reward bins that leave a small, positive remainder.\n    # This encourages tighter packing while leaving a tiny bit of space,\n    # which can be beneficial for subsequent items.\n    # We use a scaled inverse of the remaining capacity for this bonus.\n    # A small positive residual gets a higher bonus.\n    # This is inspired by heuristics that balance tight fit with future utility.\n    # A simple approach is to multiply by the remaining capacity itself,\n    # effectively favoring residuals that are small but non-zero.\n    # This is similar to some elements in 'priority_v1' but without normalization.\n    adaptive_bonus = remaining_capacity_after_fit * 0.5 # A tunable factor for the bonus\n\n    # Combine base priority with adaptive bonus\n    # Higher values indicate better suitability\n    priorities[can_fit_mask] = base_priority + adaptive_bonus\n    \n    # Ensure priorities are non-negative, though logic should prevent this\n    priorities = np.maximum(0, priorities)\n    \n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive Best-Fit: Combines 'Best Fit' proximity with a bonus for bins that\n    leave a small but not zero remainder, promoting tighter packing and future flexibility.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate remaining capacity after fitting the item\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Base priority: Inverse of remaining capacity (Best Fit)\n    # Higher score for smaller remaining capacity\n    # Add a small epsilon to prevent division by zero for perfect fits\n    base_priority = 1.0 / (remaining_capacity_after_fit + 1e-9)\n    \n    # Adaptive bonus: Reward bins that leave a small, positive remainder.\n    # This encourages tighter packing while leaving a tiny bit of space,\n    # which can be beneficial for subsequent items.\n    # We use a scaled inverse of the remaining capacity for this bonus.\n    # A small positive residual gets a higher bonus.\n    # This is inspired by heuristics that balance tight fit with future utility.\n    # A simple approach is to multiply by the remaining capacity itself,\n    # effectively favoring residuals that are small but non-zero.\n    # This is similar to some elements in 'priority_v1' but without normalization.\n    adaptive_bonus = remaining_capacity_after_fit * 0.5 # A tunable factor for the bonus\n\n    # Combine base priority with adaptive bonus\n    # Higher values indicate better suitability\n    priorities[can_fit_mask] = base_priority + adaptive_bonus\n    \n    # Ensure priorities are non-negative, though logic should prevent this\n    priorities = np.maximum(0, priorities)\n    \n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}