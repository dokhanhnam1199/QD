{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Score 1: Best Fit - Prioritize bins that leave minimal remaining capacity after packing.\n    # This aims for tighter fits. Adding epsilon for numerical stability.\n    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Score 2: Worst Fit - Prioritize bins with the most remaining capacity among fittable ones.\n    # This attempts to keep smaller gaps for future smaller items, a form of \"spreading\" the load.\n    # Normalized by the maximum remaining capacity among fittable bins to give a relative measure.\n    max_rem_cap_fittable = np.max(fittable_bins_remain_cap)\n    worst_fit_score = (fittable_bins_remain_cap - item) / (max_rem_cap_fittable + 1e-6)\n\n    # Score 3: Bin Fullness (Inverse Remaining Capacity) - Reward bins that are already more full.\n    # This encourages consolidating items into fewer bins.\n    # Using the inverse of remaining capacity among fittable bins.\n    fullness_score = (max_rem_cap_fittable - fittable_bins_remain_cap) / (max_rem_cap_fittable + 1e-6)\n\n    # Score 4: Fit Tightness Penalty - Penalize bins that leave a \"medium\" amount of space.\n    # This is a form of \"avoiding the middle ground\" by discouraging fits that are neither very tight nor very loose.\n    # We use a Gaussian-like function centered around a \"target\" leftover capacity.\n    # Let's define a target leftover capacity as a small fraction of the bin capacity (e.g., 10%).\n    # For simplicity and generality across different bin sizes, we can use a relative leftover capacity.\n    # If item is large, we might want to fit it tightly. If item is small, we might want to fill bins more.\n    # For now, let's consider a residual capacity that is still significant but not excessive.\n    # We'll penalize capacities that leave, say, 20-50% of remaining capacity.\n    # A simple approach is to use a sigmoid-like penalty.\n    # For simplicity, let's focus on penalizing larger residual capacities for now in a non-linear way.\n    residual_capacity = fittable_bins_remain_cap - item\n    # A simple penalty that increases as residual capacity increases, but less steeply for very small residuals.\n    # We can use a power function or log, but let's try a simple approach with a sigmoid-like effect.\n    # Penalize larger residuals more heavily.\n    fit_tightness_penalty = np.exp(-residual_capacity / np.mean(fittable_bins_remain_cap) * 2) # Exponential decay, higher value means less penalty (better)\n\n    # Combine scores with adaptive weights based on the item size relative to the bin capacity.\n    # If the item is large (e.g., > 50% of bin capacity), prioritize tight fits (Best Fit).\n    # If the item is small, prioritize spreading (Worst Fit) and fullness (Fullness Score).\n    bin_capacity_estimate = np.mean(fittable_bins_remain_cap) + item # Estimated capacity of bins that can fit the item\n    relative_item_size = item / bin_capacity_estimate\n\n    weight_best_fit = 0.5 + 0.5 * relative_item_size  # Higher weight for larger items\n    weight_worst_fit = 0.5 - 0.5 * relative_item_size # Higher weight for smaller items\n    weight_fullness = 0.3 # A moderate weight to encourage fuller bins\n    weight_tightness_penalty = 0.2 # A small weight to slightly penalize large gaps, but we are already using fit_tightness_score\n\n    # Re-evaluating the fit_tightness_penalty concept to be a preference rather than penalty.\n    # Let's call it \"Space Utilization Preference\".\n    # We want to prefer bins where the residual capacity after fitting the item is \"reasonable\".\n    # A residual capacity that is too large is bad. A residual capacity that is too small is also potentially bad if it leads to fragmentation.\n    # Let's create a score that is high for moderate residuals and low for extreme residuals.\n    # Using a quadratic or Gaussian-like function peaking at a certain residual.\n    # Target residual might be related to the item size itself. e.g., fitting item 'i' and leaving 'i' space.\n    # For now, let's simplify and focus on discouraging very large remaining capacities.\n    # A simple inversion of remaining capacity but capped at some point could work.\n    # Let's use a score that favors smaller residuals.\n    space_utilization_score = 1.0 / (residual_capacity + 1e-6)\n\n    # Combine scores:\n    # Best Fit: Higher is better (tighter fit)\n    # Worst Fit: Higher is better (more remaining capacity)\n    # Fullness Score: Higher is better (already fuller bins)\n    # Space Utilization Score: Higher is better (smaller residual capacity)\n\n    # Let's try to combine these in a way that reflects different strategies.\n    # The current version has `best_fit_scores` and `fullness_bonus` and `adaptive_bonus`.\n    # My v2 tries to balance Best Fit and Worst Fit with Fullness.\n\n    # A combined score that balances the desire for tight fits (best_fit_score)\n    # with the desire to leave more space for potentially smaller items (worst_fit_score),\n    # and also rewards bins that are already somewhat full (fullness_score).\n\n    # Let's try a score that is a weighted sum of these.\n    # We want to prioritize bins that are tight fits AND are already relatively full.\n    # A synergistic approach might be:\n    # Prefer tight fits (best_fit_score)\n    # BUT, if multiple bins offer tight fits, prefer the one that is already more full (fullness_score).\n    # And, consider the worst-fit aspect to avoid leaving very little space in some bins.\n\n    # Let's try a primary score from Best Fit, and a secondary bonus from Fullness.\n    # The worst fit idea is important, so maybe it should be a separate consideration.\n\n    # Redefining approach:\n    # Score 1: Best Fit - essential for efficiency.\n    # Score 2: Fullness - important for reducing bin count.\n    # Score 3: A \"Balance\" score - tries to keep remaining capacities relatively balanced.\n    #   This could be achieved by penalizing bins that are too full or too empty relative to the average.\n    #   Let's try a score that rewards bins whose remaining capacity after fit is close to the average remaining capacity of fittable bins.\n\n    # Re-evaluating: let's focus on a single cohesive heuristic.\n    # The core trade-off is between immediate (tight) fit and future flexibility (leaving space).\n    # A good heuristic should balance these.\n\n    # Let's try a modified Best Fit approach with a bonus for fullness.\n    # The key is how to combine them.\n\n    # For `priority_v2`, let's use a score that:\n    # 1. Strongly prefers tight fits (Best Fit).\n    # 2. Provides a bonus for bins that are already quite full (Fullness).\n    # 3.  Introduces a \"cohesion\" bonus: incentivizes packing into bins that are *already* moderately filled.\n    #    This means we don't just want the *most* full bin, but a bin that is \"comfortably\" full.\n\n    # Score 1: Best Fit (primary driver)\n    # Higher value for smaller (fittable_bins_remain_cap - item)\n    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Score 2: Fullness Bonus (secondary driver)\n    # Reward bins that are already more full.\n    # Using inverse of remaining capacity.\n    fullness_score = (max_rem_cap_fittable - fittable_bins_remain_cap) / (max_rem_cap_fittable + 1e-6)\n\n    # Score 3: Cohesion Bonus (tertiary driver)\n    # This score aims to encourage packing into bins that have a \"good\" amount of remaining capacity.\n    # A bin that is almost empty might not be ideal, nor is a bin that is almost full (where best fit already covers it).\n    # We want to reward bins where `fittable_bins_remain_cap - item` is not too small and not too large.\n    # Let's define a \"target residual capacity\". A simple heuristic is that the residual capacity\n    # should be proportional to the item size itself, perhaps 0.5 to 1.5 times the item size.\n    # Or, we can penalize extreme remaining capacities.\n    # Let's try a score that is high for medium residual capacities and low for very small or very large.\n    # A Gaussian-like function centered on a \"desirable\" residual.\n    # For simplicity, let's try to penalize very large residual capacities.\n    # And, perhaps, slightly penalize very small residual capacities (if not covered by best_fit).\n    # Let's try a score that is high for moderate residual capacity (e.g., 20-50% of bin capacity).\n    # Instead of a complex function, let's try a simpler idea:\n    # Reward bins where the *ratio* of remaining capacity to item size is in a sweet spot.\n    # e.g., (fittable_bins_remain_cap - item) / item.\n    # A ratio around 0.5 to 1.5 might be good.\n    # Let's define a score that peaks when (fittable_bins_remain_cap - item) is moderate.\n    # The ideal residual is perhaps `item` itself, meaning the bin is filled by 50%.\n    # Let `target_residual = item`.\n    # Let `actual_residual = fittable_bins_remain_cap - item`.\n    # We want to maximize `exp(- (actual_residual - target_residual)^2 / sigma^2)`.\n    # For simplicity, let's use a score that is inversely proportional to residual capacity squared.\n    # This will strongly penalize large residuals.\n    # We need to make sure this doesn't conflict too much with Best Fit.\n\n    # Let's try a simpler \"cohesion\" idea: encourage packing into bins that are not *too* empty.\n    # This is somewhat captured by fullness, but let's make it more direct:\n    # Score 3: Minimum Residual Capacity Bonus.\n    # Prefer bins that, after packing, still have a reasonable minimum remaining capacity.\n    # This encourages not leaving bins \"almost full\" but with tiny unusable gaps.\n    # This is somewhat counter-intuitive for BPP, usually we want tight fits.\n    # Let's reconsider the \"avoiding the middle ground\" idea.\n    # We want to avoid bins that are too full AND bins that are too empty relative to potential future items.\n\n    # New approach for v2: Prioritize bins that are a good fit AND are already relatively full,\n    # with a penalty for leaving excessive remaining space.\n\n    # Score 1: Best Fit Ratio (tighter fits are better)\n    # Higher score for smaller residual capacity\n    best_fit_ratio = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Score 2: Fullness Ratio (bins that are already more full are better)\n    # Higher score for bins with less remaining capacity.\n    fullness_ratio = 1.0 / (fittable_bins_remain_cap + 1e-6)\n\n    # Score 3: Fit Penalty (discourage leaving very large gaps)\n    # Exponential penalty for residual capacity. This penalizes leaving large amounts of space.\n    # Normalize residual capacity by a typical bin size or max remaining capacity.\n    # Let's use the average remaining capacity among fittable bins as a reference.\n    avg_rem_cap_fittable = np.mean(fittable_bins_remain_cap)\n    fit_penalty = np.exp(- (fittable_bins_remain_cap - item) / (avg_rem_cap_fittable + 1e-6) * 1.0) # Exponential decay\n\n\n    # Combine scores with weights that can be tuned.\n    # The goal is to balance tight fits with fuller bins, while penalizing over-filling.\n    # Best fit is usually a strong indicator. Fullness is good for reducing bin count.\n    # The penalty term helps prevent leaving too much wasted space, which can be detrimental.\n\n    # Let's try a weighted sum:\n    # bf_weight: how much we prioritize tight fits.\n    # f_weight: how much we prioritize already fuller bins.\n    # p_weight: how much we penalize leaving large gaps.\n\n    # Adaptive weighting based on item size:\n    # If item is large, tight fit (bf) is crucial. Fullness (f) is still good. Penalty (p) is less critical than for small items.\n    # If item is small, fullness (f) becomes more important to consolidate. Tight fit (bf) is less critical, but still good. Penalty (p) is more important to avoid fragmentation.\n\n    # Let's simplify weights:\n    bf_weight = 1.0\n    f_weight = 0.7\n    p_weight = 0.4\n\n    combined_scores = (bf_weight * best_fit_ratio) + (f_weight * fullness_ratio) + (p_weight * fit_penalty)\n\n    # Normalize scores to [0, 1] for better interpretability and to prevent large values from dominating.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n    else:\n        priorities[fittable_bins_mask] = 0.1 # Fallback to a small uniform priority if all scores are near zero\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Core Strategy: Prioritize bins that leave minimal remaining capacity (Best Fit).\n    # Use the inverse of remaining capacity after fitting. Add a small epsilon for stability.\n    # This directly targets the objective of minimizing wasted space.\n    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-9)\n\n    # Predictive Component: Consider the \"potential\" of a bin for future items.\n    # A bin with slightly more remaining capacity after fitting a current item might be\n    # more versatile for subsequent items. This is a trade-off: a tighter fit is good now,\n    # but a slightly looser fit might be better overall.\n    # We can model this by giving a bonus to bins that are \"almost full\" but can still fit the item,\n    # prioritizing those that would have a \"just right\" remaining capacity for common item sizes.\n    # A simple approach is to penalize bins that leave a very large gap.\n    # We can use the negative logarithm of the remaining capacity after fit. Smaller remaining capacity is better,\n    # so a smaller value for log1p(remaining_cap) is better. We invert this by multiplying by -1.\n    # Using log1p to handle cases where remaining capacity is 0 gracefully.\n    future_potential_score = -np.log1p(fittable_bins_remain_cap - item)\n\n    # Adaptability: The relative importance of \"tight fit\" vs. \"future potential\" can change.\n    # For instance, if items are generally small, prioritizing slightly larger remaining capacity might be good.\n    # If items are large, tight fits are paramount.\n    # We can introduce a dynamic weighting factor based on the item size relative to the bin capacity (implicitly, the median or average remaining capacity).\n    # For simplicity, let's consider the item size relative to the bin capacity if we knew it, but we only have remaining capacities.\n    # A proxy could be the item size relative to the *average* remaining capacity of fittable bins.\n    # If the item is small relative to available space, future potential might be more important.\n    # If the item is large, fitting it tightly is more important.\n\n    avg_remaining_fittable = np.mean(fittable_bins_remain_cap)\n    item_size_ratio = item / (avg_remaining_fittable + 1e-9)\n\n    # Weighting:\n    # If item_size_ratio is high (item is large relative to average remaining capacity),\n    # prioritize Best Fit more. If low (item is small), give more weight to future potential.\n    # A sigmoid-like function could map item_size_ratio to a weight for Best Fit.\n    # Let's use a simple linear scaling for now.\n\n    # Normalize item_size_ratio to a [0, 1] range (approximate).\n    # A more robust normalization might involve historical data or a fixed large value.\n    # For now, assume item sizes are within a reasonable range compared to bin capacities.\n    # A heuristic upper bound for item_size_ratio could be the bin capacity itself (if items <= bin_cap).\n    # Let's consider the max possible ratio for a single item being 1 if item == bin_cap.\n    # So, max_ratio_considered = 1.0 (for item fitting exactly).\n    # The weight for best_fit_score will increase as item_size_ratio increases.\n    # The weight for future_potential_score will decrease as item_size_ratio increases.\n\n    # Let's use a sigmoid-like weighting for Best Fit: w_bf = 1 / (1 + exp(-k * (item_size_ratio - threshold)))\n    # A simpler approach: linear interpolation.\n    # If item_size_ratio is small (e.g., 0.1), bf_weight=0.2, fp_weight=0.8\n    # If item_size_ratio is large (e.g., 1.0), bf_weight=1.0, fp_weight=0.0\n    # We can use an exponential decay for future potential weight.\n    bf_weight = 1.0 - np.exp(-item_size_ratio * 2.0)  # As item_size_ratio grows, bf_weight approaches 1\n    fp_weight = np.exp(-item_size_ratio * 2.0)       # As item_size_ratio grows, fp_weight approaches 0\n\n    # Combine scores with adaptive weights\n    combined_scores = (best_fit_score * bf_weight) + (future_potential_score * fp_weight)\n\n    # Normalize priorities to a [0, 1] range.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n    else:\n        # If all scores are effectively zero, assign a small uniform priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities\n\n### Analyze & experience\n- *   **Heuristics 1st vs. Heuristics 2nd:** These are identical. The ranking suggests a tie or a misunderstanding in the prompt.\n*   **Heuristics 3rd vs. Heuristics 4th:** Heuristic 3rd uses a multiplicative combination of Best Fit and Fill Ratio. Heuristic 4th uses Best Fit multiplied by an exponential `residual_quality_factor`. Heuristic 3rd's multiplicative approach with a simple fill ratio is conceptually clearer for dense packing than the exponential penalty on small residuals, which might be too aggressive or not well-calibrated without tuning.\n*   **Heuristics 5th vs. Heuristics 6th:** Heuristic 5th is identical to Heuristic 4th. Heuristic 6th introduces multiple scores (Best Fit, Worst Fit, Fullness, Fit Tightness Penalty) and adaptive weights, making it more complex and potentially harder to tune than simpler combinations. The \"adaptive weights\" in Heuristic 6th seem intended to react to item size but are not clearly defined or implemented in a way that demonstrably improves upon simpler strategies without extensive parameter tuning.\n*   **Heuristics 7th vs. Heuristics 8th:** Heuristic 7th is identical to Heuristic 6th. Heuristic 8th combines Best Fit with a simple \"fullness bonus\" (inverse of remaining capacity). This is less sophisticated than Heuristic 1st or 3rd, as it doesn't directly penalize large residual gaps or explicitly consider the \"tightness\" of the fit in its bonus calculation.\n*   **Heuristics 9th vs. Heuristics 10th:** Heuristic 9th is identical to Heuristics 4th and 5th. Heuristic 10th is identical to Heuristic 3rd.\n*   **Heuristics 11th vs. Heuristics 12th:** Heuristic 11th combines Best Fit proximity with a logarithmic \"Usefulness Bonus\" on remaining capacity. Heuristic 12th combines tightness, fullness, and a negative logarithmic \"Future Fit Score.\" Heuristic 12th's approach is more comprehensive, attempting to balance multiple aspects (tightness, fullness, and not leaving *too* much space) but with a subtraction of the future fit score, which might be counter-intuitive. Heuristic 11th's multiplicative approach with `(1 + 0.2 * adaptive_bonus)` is a more direct way to boost good fits that also leave reasonable space.\n*   **Heuristics 13th vs. Heuristics 14th:** These are identical. They use adaptive weighting based on item size ratio to balance Best Fit and Future Potential. The exponential weighting is a reasonable approach for adaptability.\n*   **Heuristics 15th vs. Heuristics 16th:** Heuristic 15th is identical to Heuristics 13th and 14th. Heuristic 16th is incomplete, only containing imports and docstrings, and includes unused library imports (random, math, scipy, torch).\n*   **Heuristics 17th vs. Heuristics 18th:** Heuristics 17th and 20th are identical. They combine Best Fit (proximity) and Fill Ratio, using a multiplicative approach, with a refinement to handle empty bins. Heuristic 18th is identical to Heuristic 16th, incomplete.\n*   **Heuristics 19th vs. Heuristics 20th:** Heuristic 19th is identical to Heuristics 17th and 20th.\n\n**Overall:** The higher-ranked heuristics (1st, 3rd, 11th, 17th/19th/20th) tend to combine Best Fit with another factor (fullness, fill ratio, or a bonus for moderate remaining space) using either additive or multiplicative logic. Lower-ranked heuristics either introduce too many complex, potentially conflicting scores (6th, 7th), use simpler additive combinations without nuanced logic (8th), or are incomplete (16th, 18th). Heuristics that attempt adaptive weighting based on item size (13th, 14th, 15th) are conceptually good but rely on careful tuning of those weights. The multiplicative approach of combining Best Fit with a Fill Ratio (3rd, 10th, 17th, 19th, 20th) seems robust.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Multi-objective optimization, multiplicative scoring, adaptive weighting, robustness, simplification.\n*   **Advice:** Design heuristics that explicitly balance immediate packing gains (e.g., Best Fit) with long-term bin utilization (e.g., fill ratio). Experiment with multiplicative combinations of these metrics.\n*   **Avoid:** Redundant implementations, overly complex scoring functions without clear justification, and heuristics that don't directly address the trade-off between immediate and future packing efficiency.\n*   **Explanation:** Focusing on combining well-performing, conceptually distinct metrics multiplicatively creates synergistic effects. Avoiding complexity ensures interpretability and easier tuning, leading to more robust and generalizable heuristics.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}