```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    A multi-objective priority function that balances immediate fit (Best Fit)
    with the potential for future packing (Worst Fit and a penalty for too-small gaps).
    Weights are adaptively adjusted based on the item's size relative to bin capacity.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    fittable_bins_mask = bins_remain_cap >= item

    if not np.any(fittable_bins_mask):
        return priorities

    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]

    # Component 1: Best Fit Score (prioritizes bins that leave minimal space)
    # Smaller remaining space after fit is better. Add epsilon to avoid division by zero.
    best_fit_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-9)

    # Component 2: Worst Fit Score (prioritizes bins with more remaining capacity,
    # aiming to keep larger gaps available for potentially larger future items)
    # Larger remaining space is better. Invert for direct comparison with Best Fit.
    # Add epsilon to avoid division by zero.
    worst_fit_scores = fittable_bins_remain_cap / (bins_remain_cap.max() + 1e-9) # Normalize by overall max capacity


    # Component 3: Gap Penalty (penalizes leaving very small gaps after fitting)
    # Small gaps are undesirable as they are less flexible for future items.
    # Use a sigmoid-like function to penalize small gaps more severely.
    # Values close to 0 for (fittable_bins_remain_cap - item) will result in a score close to 1.
    # Values far from 0 will result in a score close to 0.
    # We want to penalize small gaps, so a higher score here is worse.
    gap_penalty = 1.0 / (1.0 + np.exp(10 * (fittable_bins_remain_cap - item + 1e-9)))


    # Adaptive Weighting based on item size relative to average remaining capacity
    # If item is large, prioritize fitting it anywhere (Best Fit is more important).
    # If item is small, consider future packing (Worst Fit and gap penalty become more relevant).
    avg_remain_cap = np.mean(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 1.0
    size_ratio = item / (avg_remain_cap + 1e-9)

    # Heuristic weights that adapt. These can be tuned.
    # w_bf: importance of immediate tight fit
    # w_wf: importance of leaving larger gaps for future items
    # w_gp: importance of penalizing creation of small, unusable gaps
    w_bf = 1.0 + 0.5 * size_ratio  # Larger items give more weight to Best Fit
    w_wf = 1.0 + 0.5 * (1.0 - size_ratio) # Smaller items give more weight to Worst Fit
    w_gp = 1.0 # Consistent penalty for small gaps

    # Normalize components before combining to ensure they are on a similar scale
    # Normalize Best Fit scores: larger is better
    if np.max(best_fit_scores) > 1e-9:
        norm_bf = np.clip(best_fit_scores / np.max(best_fit_scores), 0, 1)
    else:
        norm_bf = np.zeros_like(best_fit_scores)

    # Normalize Worst Fit scores: larger is better
    if np.max(worst_fit_scores) > 1e-9:
        norm_wf = np.clip(worst_fit_scores / np.max(worst_fit_scores), 0, 1)
    else:
        norm_wf = np.zeros_like(worst_fit_scores)

    # Normalize Gap Penalty: lower is better (so we invert it for the score)
    # We want a higher score for bins that result in a SMALLER penalty.
    if np.max(gap_penalty) > 1e-9:
        norm_gp = np.clip((1.0 - gap_penalty) / np.max(1.0 - gap_penalty), 0, 1)
    else:
        norm_gp = np.zeros_like(gap_penalty)


    # Combine scores with adaptive weights
    # Higher score means higher priority.
    combined_scores = (w_bf * norm_bf) + (w_wf * norm_wf) + (w_gp * norm_gp)

    # Normalize final priorities to [0, 1]
    if np.max(combined_scores) > 1e-9:
        priorities[fittable_bins_mask] = np.clip(combined_scores / np.max(combined_scores), 0, 1)
    else:
        # If all scores are near zero, assign a small uniform priority to fittable bins.
        priorities[fittable_bins_mask] = 0.1

    return priorities
```
