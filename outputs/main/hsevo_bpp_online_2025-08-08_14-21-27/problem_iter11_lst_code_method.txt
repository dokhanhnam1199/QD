{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a bonus for already full bins, using a logarithmic bonus\n    to balance tight fits with encouraging fuller bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Best Fit Score: Higher for bins that leave less remaining capacity.\n    # Adding a small epsilon to avoid division by zero.\n    best_fit_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Fullness Bonus: A bonus for bins that are already more full.\n    # Using the inverse of remaining capacity on fittable bins.\n    # Normalize by the maximum remaining capacity among fittable bins to get a relative measure.\n    max_remaining_cap_fittable = np.max(fittable_bins_remain_cap)\n    fullness_bonus = (max_remaining_cap_fittable - fittable_bins_remain_cap) / (max_remaining_cap_fittable + 1e-6)\n\n    # Adaptive Bonus using logarithm of remaining capacity after fit.\n    # This penalizes leaving excessively large gaps but gives smaller penalties for smaller gaps.\n    # Adding 1 to prevent log(0) and ensure positive values.\n    adaptive_bonus = np.log1p(fittable_bins_remain_cap - item)\n\n    # Combine: Weighted sum of Best Fit and Fullness Bonus, with Adaptive Bonus as a modifier.\n    # Weights are heuristic and can be tuned. Here, Best Fit is primary, Fullness adds context,\n    # and the Adaptive Bonus influences the penalty for leftover space.\n    # We invert the adaptive bonus as smaller leftover space (lower log) should be better.\n    combined_scores = (best_fit_scores * 1.0) + (fullness_bonus * 0.5) - (adaptive_bonus * 0.2)\n\n    # Normalize priorities to a [0, 1] range for better comparability and to avoid extreme values.\n    if np.max(combined_scores) > 1e-9:\n        priorities[fittable_bins_mask] = np.clip(combined_scores / np.max(combined_scores), 0, 1)\n    else:\n        # If all scores are near zero, assign a small uniform priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a bonus for already full bins, using a logarithmic bonus\n    to balance tight fits with encouraging fuller bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Best Fit Score: Higher for bins that leave less remaining capacity.\n    # Adding a small epsilon to avoid division by zero.\n    best_fit_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Fullness Bonus: A bonus for bins that are already more full.\n    # Using the inverse of remaining capacity on fittable bins.\n    # Normalize by the maximum remaining capacity among fittable bins to get a relative measure.\n    max_remaining_cap_fittable = np.max(fittable_bins_remain_cap)\n    fullness_bonus = (max_remaining_cap_fittable - fittable_bins_remain_cap) / (max_remaining_cap_fittable + 1e-6)\n\n    # Adaptive Bonus using logarithm of remaining capacity after fit.\n    # This penalizes leaving excessively large gaps but gives smaller penalties for smaller gaps.\n    # Adding 1 to prevent log(0) and ensure positive values.\n    adaptive_bonus = np.log1p(fittable_bins_remain_cap - item)\n\n    # Combine: Weighted sum of Best Fit and Fullness Bonus, with Adaptive Bonus as a modifier.\n    # Weights are heuristic and can be tuned. Here, Best Fit is primary, Fullness adds context,\n    # and the Adaptive Bonus influences the penalty for leftover space.\n    # We invert the adaptive bonus as smaller leftover space (lower log) should be better.\n    combined_scores = (best_fit_scores * 1.0) + (fullness_bonus * 0.5) - (adaptive_bonus * 0.2)\n\n    # Normalize priorities to a [0, 1] range for better comparability and to avoid extreme values.\n    if np.max(combined_scores) > 1e-9:\n        priorities[fittable_bins_mask] = np.clip(combined_scores / np.max(combined_scores), 0, 1)\n    else:\n        # If all scores are near zero, assign a small uniform priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a \"fill ratio\" bonus. Prioritizes bins that fit the item snugly\n    and are already relatively full, aiming for denser packing.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    eligible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(eligible_bins_mask):\n        return priorities\n\n    eligible_bins_caps = bins_remain_cap[eligible_bins_mask]\n\n    # Score 1: Best Fit - favors bins with minimal remaining capacity after packing.\n    best_fit_scores = 1.0 / (eligible_bins_caps - item + 1e-9)\n\n    # Score 2: Fill Ratio - favors bins that are already more full.\n    # Using the ratio of current capacity to maximum possible capacity for fitting bins.\n    max_eligible_cap = np.max(eligible_bins_caps)\n    fill_ratio_scores = eligible_bins_caps / (max_eligible_cap + 1e-9)\n\n    # Combine scores multiplicatively: prioritize bins that are both a good fit and already full.\n    combined_scores = best_fit_scores * fill_ratio_scores\n\n    # Assign combined scores to the priorities array\n    priorities[eligible_bins_mask] = combined_scores\n\n    # Normalize priorities to the range [0, 1] for consistent comparison.\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for very small remaining capacity,\n    favoring tight fits while slightly discouraging bins that become overly full.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fitting_bins_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Best Fit Score: Inverse of remaining capacity after placement.\n    # Higher score for bins that leave less capacity (tighter fit).\n    best_fit_scores = 1.0 / (fitting_bins_cap - item + 1e-9)\n\n    # Adaptive Penalty for Small Residuals: Penalize bins that leave a very small remainder.\n    # This aims to avoid situations where a bin is *too* full, potentially leaving no room\n    # for even slightly larger items later. We use an inverse relationship with the remainder.\n    # A small remainder (e.g., 0.1) gets a lower penalty score (e.g., 0.1 / (0.1 + 0.5) ~ 0.16),\n    # while a larger remainder (e.g., 10) gets a higher penalty score (e.g., 10 / (10 + 0.5) ~ 0.95).\n    # We want to *discourage* very small remainders, so we'll use this score to adjust the best_fit_scores.\n    # Specifically, we'll multiply the best_fit_scores by a factor that decreases as the remainder gets smaller.\n    # Let's invert this penalty concept: a *good* residual quality score should be *high* for moderate remainders.\n    # Instead of penalty, let's frame it as a \"residual quality bonus\" where small residuals are penalized.\n    # A simple penalty function for small residuals: exp(-residual / sensitivity)\n    # Where sensitivity is a parameter controlling how quickly the penalty kicks in.\n    sensitivity = 2.0  # Controls how strongly small remainders are penalized.\n    residual_quality_factor = np.exp(-(fitting_bins_cap - item) / sensitivity)\n\n\n    # Combine scores: Multiply Best Fit by the residual quality factor.\n    # This prioritizes tight fits (high best_fit_scores) but reduces their priority\n    # if they leave an extremely small remainder (low residual_quality_factor).\n    combined_scores = best_fit_scores * residual_quality_factor\n\n    # Normalize combined scores to [0, 1] to make them comparable across different calls.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for very small remaining capacity,\n    favoring tight fits while slightly discouraging bins that become overly full.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fitting_bins_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Best Fit Score: Inverse of remaining capacity after placement.\n    # Higher score for bins that leave less capacity (tighter fit).\n    best_fit_scores = 1.0 / (fitting_bins_cap - item + 1e-9)\n\n    # Adaptive Penalty for Small Residuals: Penalize bins that leave a very small remainder.\n    # This aims to avoid situations where a bin is *too* full, potentially leaving no room\n    # for even slightly larger items later. We use an inverse relationship with the remainder.\n    # A small remainder (e.g., 0.1) gets a lower penalty score (e.g., 0.1 / (0.1 + 0.5) ~ 0.16),\n    # while a larger remainder (e.g., 10) gets a higher penalty score (e.g., 10 / (10 + 0.5) ~ 0.95).\n    # We want to *discourage* very small remainders, so we'll use this score to adjust the best_fit_scores.\n    # Specifically, we'll multiply the best_fit_scores by a factor that decreases as the remainder gets smaller.\n    # Let's invert this penalty concept: a *good* residual quality score should be *high* for moderate remainders.\n    # Instead of penalty, let's frame it as a \"residual quality bonus\" where small residuals are penalized.\n    # A simple penalty function for small residuals: exp(-residual / sensitivity)\n    # Where sensitivity is a parameter controlling how quickly the penalty kicks in.\n    sensitivity = 2.0  # Controls how strongly small remainders are penalized.\n    residual_quality_factor = np.exp(-(fitting_bins_cap - item) / sensitivity)\n\n\n    # Combine scores: Multiply Best Fit by the residual quality factor.\n    # This prioritizes tight fits (high best_fit_scores) but reduces their priority\n    # if they leave an extremely small remainder (low residual_quality_factor).\n    combined_scores = best_fit_scores * residual_quality_factor\n\n    # Normalize combined scores to [0, 1] to make them comparable across different calls.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Score 1: Best Fit - Prioritize bins that leave minimal remaining capacity after packing.\n    # This aims for tighter fits. Adding epsilon for numerical stability.\n    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Score 2: Worst Fit - Prioritize bins with the most remaining capacity among fittable ones.\n    # This attempts to keep smaller gaps for future smaller items, a form of \"spreading\" the load.\n    # Normalized by the maximum remaining capacity among fittable bins to give a relative measure.\n    max_rem_cap_fittable = np.max(fittable_bins_remain_cap)\n    worst_fit_score = (fittable_bins_remain_cap - item) / (max_rem_cap_fittable + 1e-6)\n\n    # Score 3: Bin Fullness (Inverse Remaining Capacity) - Reward bins that are already more full.\n    # This encourages consolidating items into fewer bins.\n    # Using the inverse of remaining capacity among fittable bins.\n    fullness_score = (max_rem_cap_fittable - fittable_bins_remain_cap) / (max_rem_cap_fittable + 1e-6)\n\n    # Score 4: Fit Tightness Penalty - Penalize bins that leave a \"medium\" amount of space.\n    # This is a form of \"avoiding the middle ground\" by discouraging fits that are neither very tight nor very loose.\n    # We use a Gaussian-like function centered around a \"target\" leftover capacity.\n    # Let's define a target leftover capacity as a small fraction of the bin capacity (e.g., 10%).\n    # For simplicity and generality across different bin sizes, we can use a relative leftover capacity.\n    # If item is large, we might want to fit it tightly. If item is small, we might want to fill bins more.\n    # For now, let's consider a residual capacity that is still significant but not excessive.\n    # We'll penalize capacities that leave, say, 20-50% of remaining capacity.\n    # A simple approach is to use a sigmoid-like penalty.\n    # For simplicity, let's focus on penalizing larger residual capacities for now in a non-linear way.\n    residual_capacity = fittable_bins_remain_cap - item\n    # A simple penalty that increases as residual capacity increases, but less steeply for very small residuals.\n    # We can use a power function or log, but let's try a simple approach with a sigmoid-like effect.\n    # Penalize larger residuals more heavily.\n    fit_tightness_penalty = np.exp(-residual_capacity / np.mean(fittable_bins_remain_cap) * 2) # Exponential decay, higher value means less penalty (better)\n\n    # Combine scores with adaptive weights based on the item size relative to the bin capacity.\n    # If the item is large (e.g., > 50% of bin capacity), prioritize tight fits (Best Fit).\n    # If the item is small, prioritize spreading (Worst Fit) and fullness (Fullness Score).\n    bin_capacity_estimate = np.mean(fittable_bins_remain_cap) + item # Estimated capacity of bins that can fit the item\n    relative_item_size = item / bin_capacity_estimate\n\n    weight_best_fit = 0.5 + 0.5 * relative_item_size  # Higher weight for larger items\n    weight_worst_fit = 0.5 - 0.5 * relative_item_size # Higher weight for smaller items\n    weight_fullness = 0.3 # A moderate weight to encourage fuller bins\n    weight_tightness_penalty = 0.2 # A small weight to slightly penalize large gaps, but we are already using fit_tightness_score\n\n    # Re-evaluating the fit_tightness_penalty concept to be a preference rather than penalty.\n    # Let's call it \"Space Utilization Preference\".\n    # We want to prefer bins where the residual capacity after fitting the item is \"reasonable\".\n    # A residual capacity that is too large is bad. A residual capacity that is too small is also potentially bad if it leads to fragmentation.\n    # Let's create a score that is high for moderate residuals and low for extreme residuals.\n    # Using a quadratic or Gaussian-like function peaking at a certain residual.\n    # Target residual might be related to the item size itself. e.g., fitting item 'i' and leaving 'i' space.\n    # For now, let's simplify and focus on discouraging very large remaining capacities.\n    # A simple inversion of remaining capacity but capped at some point could work.\n    # Let's use a score that favors smaller residuals.\n    space_utilization_score = 1.0 / (residual_capacity + 1e-6)\n\n    # Combine scores:\n    # Best Fit: Higher is better (tighter fit)\n    # Worst Fit: Higher is better (more remaining capacity)\n    # Fullness Score: Higher is better (already fuller bins)\n    # Space Utilization Score: Higher is better (smaller residual capacity)\n\n    # Let's try to combine these in a way that reflects different strategies.\n    # The current version has `best_fit_scores` and `fullness_bonus` and `adaptive_bonus`.\n    # My v2 tries to balance Best Fit and Worst Fit with Fullness.\n\n    # A combined score that balances the desire for tight fits (best_fit_score)\n    # with the desire to leave more space for potentially smaller items (worst_fit_score),\n    # and also rewards bins that are already somewhat full (fullness_score).\n\n    # Let's try a score that is a weighted sum of these.\n    # We want to prioritize bins that are tight fits AND are already relatively full.\n    # A synergistic approach might be:\n    # Prefer tight fits (best_fit_score)\n    # BUT, if multiple bins offer tight fits, prefer the one that is already more full (fullness_score).\n    # And, consider the worst-fit aspect to avoid leaving very little space in some bins.\n\n    # Let's try a primary score from Best Fit, and a secondary bonus from Fullness.\n    # The worst fit idea is important, so maybe it should be a separate consideration.\n\n    # Redefining approach:\n    # Score 1: Best Fit - essential for efficiency.\n    # Score 2: Fullness - important for reducing bin count.\n    # Score 3: A \"Balance\" score - tries to keep remaining capacities relatively balanced.\n    #   This could be achieved by penalizing bins that are too full or too empty relative to the average.\n    #   Let's try a score that rewards bins whose remaining capacity after fit is close to the average remaining capacity of fittable bins.\n\n    # Re-evaluating: let's focus on a single cohesive heuristic.\n    # The core trade-off is between immediate (tight) fit and future flexibility (leaving space).\n    # A good heuristic should balance these.\n\n    # Let's try a modified Best Fit approach with a bonus for fullness.\n    # The key is how to combine them.\n\n    # For `priority_v2`, let's use a score that:\n    # 1. Strongly prefers tight fits (Best Fit).\n    # 2. Provides a bonus for bins that are already quite full (Fullness).\n    # 3.  Introduces a \"cohesion\" bonus: incentivizes packing into bins that are *already* moderately filled.\n    #    This means we don't just want the *most* full bin, but a bin that is \"comfortably\" full.\n\n    # Score 1: Best Fit (primary driver)\n    # Higher value for smaller (fittable_bins_remain_cap - item)\n    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Score 2: Fullness Bonus (secondary driver)\n    # Reward bins that are already more full.\n    # Using inverse of remaining capacity.\n    fullness_score = (max_rem_cap_fittable - fittable_bins_remain_cap) / (max_rem_cap_fittable + 1e-6)\n\n    # Score 3: Cohesion Bonus (tertiary driver)\n    # This score aims to encourage packing into bins that have a \"good\" amount of remaining capacity.\n    # A bin that is almost empty might not be ideal, nor is a bin that is almost full (where best fit already covers it).\n    # We want to reward bins where `fittable_bins_remain_cap - item` is not too small and not too large.\n    # Let's define a \"target residual capacity\". A simple heuristic is that the residual capacity\n    # should be proportional to the item size itself, perhaps 0.5 to 1.5 times the item size.\n    # Or, we can penalize extreme remaining capacities.\n    # Let's try a score that is high for medium residual capacities and low for very small or very large.\n    # A Gaussian-like function centered on a \"desirable\" residual.\n    # For simplicity, let's try to penalize very large residual capacities.\n    # And, perhaps, slightly penalize very small residual capacities (if not covered by best_fit).\n    # Let's try a score that is high for moderate residual capacity (e.g., 20-50% of bin capacity).\n    # Instead of a complex function, let's try a simpler idea:\n    # Reward bins where the *ratio* of remaining capacity to item size is in a sweet spot.\n    # e.g., (fittable_bins_remain_cap - item) / item.\n    # A ratio around 0.5 to 1.5 might be good.\n    # Let's define a score that peaks when (fittable_bins_remain_cap - item) is moderate.\n    # The ideal residual is perhaps `item` itself, meaning the bin is filled by 50%.\n    # Let `target_residual = item`.\n    # Let `actual_residual = fittable_bins_remain_cap - item`.\n    # We want to maximize `exp(- (actual_residual - target_residual)^2 / sigma^2)`.\n    # For simplicity, let's use a score that is inversely proportional to residual capacity squared.\n    # This will strongly penalize large residuals.\n    # We need to make sure this doesn't conflict too much with Best Fit.\n\n    # Let's try a simpler \"cohesion\" idea: encourage packing into bins that are not *too* empty.\n    # This is somewhat captured by fullness, but let's make it more direct:\n    # Score 3: Minimum Residual Capacity Bonus.\n    # Prefer bins that, after packing, still have a reasonable minimum remaining capacity.\n    # This encourages not leaving bins \"almost full\" but with tiny unusable gaps.\n    # This is somewhat counter-intuitive for BPP, usually we want tight fits.\n    # Let's reconsider the \"avoiding the middle ground\" idea.\n    # We want to avoid bins that are too full AND bins that are too empty relative to potential future items.\n\n    # New approach for v2: Prioritize bins that are a good fit AND are already relatively full,\n    # with a penalty for leaving excessive remaining space.\n\n    # Score 1: Best Fit Ratio (tighter fits are better)\n    # Higher score for smaller residual capacity\n    best_fit_ratio = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Score 2: Fullness Ratio (bins that are already more full are better)\n    # Higher score for bins with less remaining capacity.\n    fullness_ratio = 1.0 / (fittable_bins_remain_cap + 1e-6)\n\n    # Score 3: Fit Penalty (discourage leaving very large gaps)\n    # Exponential penalty for residual capacity. This penalizes leaving large amounts of space.\n    # Normalize residual capacity by a typical bin size or max remaining capacity.\n    # Let's use the average remaining capacity among fittable bins as a reference.\n    avg_rem_cap_fittable = np.mean(fittable_bins_remain_cap)\n    fit_penalty = np.exp(- (fittable_bins_remain_cap - item) / (avg_rem_cap_fittable + 1e-6) * 1.0) # Exponential decay\n\n\n    # Combine scores with weights that can be tuned.\n    # The goal is to balance tight fits with fuller bins, while penalizing over-filling.\n    # Best fit is usually a strong indicator. Fullness is good for reducing bin count.\n    # The penalty term helps prevent leaving too much wasted space, which can be detrimental.\n\n    # Let's try a weighted sum:\n    # bf_weight: how much we prioritize tight fits.\n    # f_weight: how much we prioritize already fuller bins.\n    # p_weight: how much we penalize leaving large gaps.\n\n    # Adaptive weighting based on item size:\n    # If item is large, tight fit (bf) is crucial. Fullness (f) is still good. Penalty (p) is less critical than for small items.\n    # If item is small, fullness (f) becomes more important to consolidate. Tight fit (bf) is less critical, but still good. Penalty (p) is more important to avoid fragmentation.\n\n    # Let's simplify weights:\n    bf_weight = 1.0\n    f_weight = 0.7\n    p_weight = 0.4\n\n    combined_scores = (bf_weight * best_fit_ratio) + (f_weight * fullness_ratio) + (p_weight * fit_penalty)\n\n    # Normalize scores to [0, 1] for better interpretability and to prevent large values from dominating.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n    else:\n        priorities[fittable_bins_mask] = 0.1 # Fallback to a small uniform priority if all scores are near zero\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Score 1: Best Fit - Prioritize bins that leave minimal remaining capacity after packing.\n    # This aims for tighter fits. Adding epsilon for numerical stability.\n    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Score 2: Worst Fit - Prioritize bins with the most remaining capacity among fittable ones.\n    # This attempts to keep smaller gaps for future smaller items, a form of \"spreading\" the load.\n    # Normalized by the maximum remaining capacity among fittable bins to give a relative measure.\n    max_rem_cap_fittable = np.max(fittable_bins_remain_cap)\n    worst_fit_score = (fittable_bins_remain_cap - item) / (max_rem_cap_fittable + 1e-6)\n\n    # Score 3: Bin Fullness (Inverse Remaining Capacity) - Reward bins that are already more full.\n    # This encourages consolidating items into fewer bins.\n    # Using the inverse of remaining capacity among fittable bins.\n    fullness_score = (max_rem_cap_fittable - fittable_bins_remain_cap) / (max_rem_cap_fittable + 1e-6)\n\n    # Score 4: Fit Tightness Penalty - Penalize bins that leave a \"medium\" amount of space.\n    # This is a form of \"avoiding the middle ground\" by discouraging fits that are neither very tight nor very loose.\n    # We use a Gaussian-like function centered around a \"target\" leftover capacity.\n    # Let's define a target leftover capacity as a small fraction of the bin capacity (e.g., 10%).\n    # For simplicity and generality across different bin sizes, we can use a relative leftover capacity.\n    # If item is large, we might want to fit it tightly. If item is small, we might want to fill bins more.\n    # For now, let's consider a residual capacity that is still significant but not excessive.\n    # We'll penalize capacities that leave, say, 20-50% of remaining capacity.\n    # A simple approach is to use a sigmoid-like penalty.\n    # For simplicity, let's focus on penalizing larger residual capacities for now in a non-linear way.\n    residual_capacity = fittable_bins_remain_cap - item\n    # A simple penalty that increases as residual capacity increases, but less steeply for very small residuals.\n    # We can use a power function or log, but let's try a simple approach with a sigmoid-like effect.\n    # Penalize larger residuals more heavily.\n    fit_tightness_penalty = np.exp(-residual_capacity / np.mean(fittable_bins_remain_cap) * 2) # Exponential decay, higher value means less penalty (better)\n\n    # Combine scores with adaptive weights based on the item size relative to the bin capacity.\n    # If the item is large (e.g., > 50% of bin capacity), prioritize tight fits (Best Fit).\n    # If the item is small, prioritize spreading (Worst Fit) and fullness (Fullness Score).\n    bin_capacity_estimate = np.mean(fittable_bins_remain_cap) + item # Estimated capacity of bins that can fit the item\n    relative_item_size = item / bin_capacity_estimate\n\n    weight_best_fit = 0.5 + 0.5 * relative_item_size  # Higher weight for larger items\n    weight_worst_fit = 0.5 - 0.5 * relative_item_size # Higher weight for smaller items\n    weight_fullness = 0.3 # A moderate weight to encourage fuller bins\n    weight_tightness_penalty = 0.2 # A small weight to slightly penalize large gaps, but we are already using fit_tightness_score\n\n    # Re-evaluating the fit_tightness_penalty concept to be a preference rather than penalty.\n    # Let's call it \"Space Utilization Preference\".\n    # We want to prefer bins where the residual capacity after fitting the item is \"reasonable\".\n    # A residual capacity that is too large is bad. A residual capacity that is too small is also potentially bad if it leads to fragmentation.\n    # Let's create a score that is high for moderate residuals and low for extreme residuals.\n    # Using a quadratic or Gaussian-like function peaking at a certain residual.\n    # Target residual might be related to the item size itself. e.g., fitting item 'i' and leaving 'i' space.\n    # For now, let's simplify and focus on discouraging very large remaining capacities.\n    # A simple inversion of remaining capacity but capped at some point could work.\n    # Let's use a score that favors smaller residuals.\n    space_utilization_score = 1.0 / (residual_capacity + 1e-6)\n\n    # Combine scores:\n    # Best Fit: Higher is better (tighter fit)\n    # Worst Fit: Higher is better (more remaining capacity)\n    # Fullness Score: Higher is better (already fuller bins)\n    # Space Utilization Score: Higher is better (smaller residual capacity)\n\n    # Let's try to combine these in a way that reflects different strategies.\n    # The current version has `best_fit_scores` and `fullness_bonus` and `adaptive_bonus`.\n    # My v2 tries to balance Best Fit and Worst Fit with Fullness.\n\n    # A combined score that balances the desire for tight fits (best_fit_score)\n    # with the desire to leave more space for potentially smaller items (worst_fit_score),\n    # and also rewards bins that are already somewhat full (fullness_score).\n\n    # Let's try a score that is a weighted sum of these.\n    # We want to prioritize bins that are tight fits AND are already relatively full.\n    # A synergistic approach might be:\n    # Prefer tight fits (best_fit_score)\n    # BUT, if multiple bins offer tight fits, prefer the one that is already more full (fullness_score).\n    # And, consider the worst-fit aspect to avoid leaving very little space in some bins.\n\n    # Let's try a primary score from Best Fit, and a secondary bonus from Fullness.\n    # The worst fit idea is important, so maybe it should be a separate consideration.\n\n    # Redefining approach:\n    # Score 1: Best Fit - essential for efficiency.\n    # Score 2: Fullness - important for reducing bin count.\n    # Score 3: A \"Balance\" score - tries to keep remaining capacities relatively balanced.\n    #   This could be achieved by penalizing bins that are too full or too empty relative to the average.\n    #   Let's try a score that rewards bins whose remaining capacity after fit is close to the average remaining capacity of fittable bins.\n\n    # Re-evaluating: let's focus on a single cohesive heuristic.\n    # The core trade-off is between immediate (tight) fit and future flexibility (leaving space).\n    # A good heuristic should balance these.\n\n    # Let's try a modified Best Fit approach with a bonus for fullness.\n    # The key is how to combine them.\n\n    # For `priority_v2`, let's use a score that:\n    # 1. Strongly prefers tight fits (Best Fit).\n    # 2. Provides a bonus for bins that are already quite full (Fullness).\n    # 3.  Introduces a \"cohesion\" bonus: incentivizes packing into bins that are *already* moderately filled.\n    #    This means we don't just want the *most* full bin, but a bin that is \"comfortably\" full.\n\n    # Score 1: Best Fit (primary driver)\n    # Higher value for smaller (fittable_bins_remain_cap - item)\n    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Score 2: Fullness Bonus (secondary driver)\n    # Reward bins that are already more full.\n    # Using inverse of remaining capacity.\n    fullness_score = (max_rem_cap_fittable - fittable_bins_remain_cap) / (max_rem_cap_fittable + 1e-6)\n\n    # Score 3: Cohesion Bonus (tertiary driver)\n    # This score aims to encourage packing into bins that have a \"good\" amount of remaining capacity.\n    # A bin that is almost empty might not be ideal, nor is a bin that is almost full (where best fit already covers it).\n    # We want to reward bins where `fittable_bins_remain_cap - item` is not too small and not too large.\n    # Let's define a \"target residual capacity\". A simple heuristic is that the residual capacity\n    # should be proportional to the item size itself, perhaps 0.5 to 1.5 times the item size.\n    # Or, we can penalize extreme remaining capacities.\n    # Let's try a score that is high for medium residual capacities and low for very small or very large.\n    # A Gaussian-like function centered on a \"desirable\" residual.\n    # For simplicity, let's try to penalize very large residual capacities.\n    # And, perhaps, slightly penalize very small residual capacities (if not covered by best_fit).\n    # Let's try a score that is high for moderate residual capacity (e.g., 20-50% of bin capacity).\n    # Instead of a complex function, let's try a simpler idea:\n    # Reward bins where the *ratio* of remaining capacity to item size is in a sweet spot.\n    # e.g., (fittable_bins_remain_cap - item) / item.\n    # A ratio around 0.5 to 1.5 might be good.\n    # Let's define a score that peaks when (fittable_bins_remain_cap - item) is moderate.\n    # The ideal residual is perhaps `item` itself, meaning the bin is filled by 50%.\n    # Let `target_residual = item`.\n    # Let `actual_residual = fittable_bins_remain_cap - item`.\n    # We want to maximize `exp(- (actual_residual - target_residual)^2 / sigma^2)`.\n    # For simplicity, let's use a score that is inversely proportional to residual capacity squared.\n    # This will strongly penalize large residuals.\n    # We need to make sure this doesn't conflict too much with Best Fit.\n\n    # Let's try a simpler \"cohesion\" idea: encourage packing into bins that are not *too* empty.\n    # This is somewhat captured by fullness, but let's make it more direct:\n    # Score 3: Minimum Residual Capacity Bonus.\n    # Prefer bins that, after packing, still have a reasonable minimum remaining capacity.\n    # This encourages not leaving bins \"almost full\" but with tiny unusable gaps.\n    # This is somewhat counter-intuitive for BPP, usually we want tight fits.\n    # Let's reconsider the \"avoiding the middle ground\" idea.\n    # We want to avoid bins that are too full AND bins that are too empty relative to potential future items.\n\n    # New approach for v2: Prioritize bins that are a good fit AND are already relatively full,\n    # with a penalty for leaving excessive remaining space.\n\n    # Score 1: Best Fit Ratio (tighter fits are better)\n    # Higher score for smaller residual capacity\n    best_fit_ratio = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Score 2: Fullness Ratio (bins that are already more full are better)\n    # Higher score for bins with less remaining capacity.\n    fullness_ratio = 1.0 / (fittable_bins_remain_cap + 1e-6)\n\n    # Score 3: Fit Penalty (discourage leaving very large gaps)\n    # Exponential penalty for residual capacity. This penalizes leaving large amounts of space.\n    # Normalize residual capacity by a typical bin size or max remaining capacity.\n    # Let's use the average remaining capacity among fittable bins as a reference.\n    avg_rem_cap_fittable = np.mean(fittable_bins_remain_cap)\n    fit_penalty = np.exp(- (fittable_bins_remain_cap - item) / (avg_rem_cap_fittable + 1e-6) * 1.0) # Exponential decay\n\n\n    # Combine scores with weights that can be tuned.\n    # The goal is to balance tight fits with fuller bins, while penalizing over-filling.\n    # Best fit is usually a strong indicator. Fullness is good for reducing bin count.\n    # The penalty term helps prevent leaving too much wasted space, which can be detrimental.\n\n    # Let's try a weighted sum:\n    # bf_weight: how much we prioritize tight fits.\n    # f_weight: how much we prioritize already fuller bins.\n    # p_weight: how much we penalize leaving large gaps.\n\n    # Adaptive weighting based on item size:\n    # If item is large, tight fit (bf) is crucial. Fullness (f) is still good. Penalty (p) is less critical than for small items.\n    # If item is small, fullness (f) becomes more important to consolidate. Tight fit (bf) is less critical, but still good. Penalty (p) is more important to avoid fragmentation.\n\n    # Let's simplify weights:\n    bf_weight = 1.0\n    f_weight = 0.7\n    p_weight = 0.4\n\n    combined_scores = (bf_weight * best_fit_ratio) + (f_weight * fullness_ratio) + (p_weight * fit_penalty)\n\n    # Normalize scores to [0, 1] for better interpretability and to prevent large values from dominating.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n    else:\n        priorities[fittable_bins_mask] = 0.1 # Fallback to a small uniform priority if all scores are near zero\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit preference with a bonus for bins that are already significantly filled,\n    aiming for efficient space utilization and minimizing the number of bins used.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    \n    if fitting_bins_cap.size > 0:\n        differences = fitting_bins_cap - item\n        \n        # Best Fit component: Higher score for smaller remaining capacity\n        best_fit_score = 1.0 / (differences + 1e-9)\n        \n        # Fullness bonus: Reward bins that are already more utilized (closer to full)\n        # This is a simplified version of the fullness bonus from v0,\n        # focusing on the absolute remaining capacity as a proxy for fullness.\n        # Bins with less remaining capacity are considered \"more full\".\n        fullness_bonus = (np.max(bins_remain_cap) - fitting_bins_cap) / (np.max(bins_remain_cap) + 1e-9)\n        \n        # Combine scores: Additive combination. Prioritize tight fits (Best Fit)\n        # and give a boost to bins that are already quite full.\n        combined_scores = best_fit_score + fullness_bonus * 0.5 # Tunable weight for bonus\n        \n        priorities[can_fit_mask] = combined_scores\n        \n        # Normalize priorities for bins that can fit the item\n        max_priority = np.max(priorities[can_fit_mask])\n        if max_priority > 0:\n            priorities[can_fit_mask] /= max_priority\n            \n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for very small remaining capacity,\n    favoring tight fits while slightly discouraging bins that become overly full.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fitting_bins_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Best Fit Score: Inverse of remaining capacity after placement.\n    # Higher score for bins that leave less capacity (tighter fit).\n    best_fit_scores = 1.0 / (fitting_bins_cap - item + 1e-9)\n\n    # Adaptive Penalty for Small Residuals: Penalize bins that leave a very small remainder.\n    # This aims to avoid situations where a bin is *too* full, potentially leaving no room\n    # for even slightly larger items later. We use an inverse relationship with the remainder.\n    # A small remainder (e.g., 0.1) gets a lower penalty score (e.g., 0.1 / (0.1 + 0.5) ~ 0.16),\n    # while a larger remainder (e.g., 10) gets a higher penalty score (e.g., 10 / (10 + 0.5) ~ 0.95).\n    # We want to *discourage* very small remainders, so we'll use this score to adjust the best_fit_scores.\n    # Specifically, we'll multiply the best_fit_scores by a factor that decreases as the remainder gets smaller.\n    # Let's invert this penalty concept: a *good* residual quality score should be *high* for moderate remainders.\n    # Instead of penalty, let's frame it as a \"residual quality bonus\" where small residuals are penalized.\n    # A simple penalty function for small residuals: exp(-residual / sensitivity)\n    # Where sensitivity is a parameter controlling how quickly the penalty kicks in.\n    sensitivity = 2.0  # Controls how strongly small remainders are penalized.\n    residual_quality_factor = np.exp(-(fitting_bins_cap - item) / sensitivity)\n\n\n    # Combine scores: Multiply Best Fit by the residual quality factor.\n    # This prioritizes tight fits (high best_fit_scores) but reduces their priority\n    # if they leave an extremely small remainder (low residual_quality_factor).\n    combined_scores = best_fit_scores * residual_quality_factor\n\n    # Normalize combined scores to [0, 1] to make them comparable across different calls.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a \"fill ratio\" bonus. Prioritizes bins that fit the item snugly\n    and are already relatively full, aiming for denser packing.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    eligible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(eligible_bins_mask):\n        return priorities\n\n    eligible_bins_caps = bins_remain_cap[eligible_bins_mask]\n\n    # Score 1: Best Fit - favors bins with minimal remaining capacity after packing.\n    best_fit_scores = 1.0 / (eligible_bins_caps - item + 1e-9)\n\n    # Score 2: Fill Ratio - favors bins that are already more full.\n    # Using the ratio of current capacity to maximum possible capacity for fitting bins.\n    max_eligible_cap = np.max(eligible_bins_caps)\n    fill_ratio_scores = eligible_bins_caps / (max_eligible_cap + 1e-9)\n\n    # Combine scores multiplicatively: prioritize bins that are both a good fit and already full.\n    combined_scores = best_fit_scores * fill_ratio_scores\n\n    # Assign combined scores to the priorities array\n    priorities[eligible_bins_mask] = combined_scores\n\n    # Normalize priorities to the range [0, 1] for consistent comparison.\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit proximity with an adaptive bonus for moderate remaining capacity,\n    favoring bins that are a close fit but also leave some useful space.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_cap.size > 0:\n        # Best Fit proximity: Score higher for bins with smaller remaining capacity after fitting.\n        differences = fitting_bins_cap - item\n        proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Adaptive \"Usefulness\" Bonus: Reward bins that leave a moderate amount of space.\n        # A logarithmic function provides diminishing returns, preventing excessively large remainders\n        # from dominating the score, while still giving a bonus for leaving more space.\n        # This encourages leaving space for future items without being overly wasteful.\n        resulting_remainders = fitting_bins_cap - item\n        \n        # We use log(1 + remainder) so that even empty bins (remainder 0) get a small base bonus,\n        # and the bonus increases with remaining space, but at a decreasing rate.\n        # A small constant is added inside log to prevent log(0) if resulting_remainders is 0.\n        adaptive_bonus = np.log(1 + resulting_remainders + 1e-9)\n        \n        # Combine proximity and adaptive bonus.\n        # The proximity score drives the heuristic towards the tightest fit.\n        # The adaptive bonus slightly counteracts pure greediness by rewarding bins\n        # that leave more space, up to a point.\n        # A multiplicative combination ensures that a good fit is still paramount,\n        # but the bonus can differentiate between equally good fits.\n        combined_scores = proximity_scores * (1 + 0.2 * adaptive_bonus) # 0.2 is a tunable parameter\n\n        # Normalize scores for the bins that can fit the item.\n        # Ensures the highest priority bin gets a score of 1.0, making priorities relative.\n        max_score = np.max(combined_scores)\n        if max_score > 0:\n            priorities[can_fit_mask] = combined_scores / max_score\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A refined heuristic that balances immediate fit quality with long-term bin utilization,\n    prioritizing bins that are nearly full but can still accommodate the item,\n    while also considering the potential for future fits of smaller items.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Score 1: Tightness of Fit (Best Fit Component)\n    # Prioritizes bins where the remaining capacity after packing is minimal.\n    # Add a small epsilon to prevent division by zero.\n    tightness_score = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Score 2: Bin Fullness (Encouraging fuller bins)\n    # This score is high for bins that are already quite full (low remaining capacity).\n    # We use the inverse of the remaining capacity.\n    fullness_score = 1.0 / (fittable_bins_remain_cap + 1e-6)\n\n    # Score 3: Future Fit Potential (Adaptive Component)\n    # This score is higher for bins that, after fitting the current item,\n    # will still have a significant amount of remaining capacity. This aims to\n    # leave \"room\" for smaller items later, preventing premature bin exhaustion.\n    # We use a logarithmic function to give diminishing returns for very large remaining capacities.\n    # Adding 1 to prevent log(0) or log(negative).\n    future_fit_score = np.log1p(bins_remain_cap[fittable_bins_mask] - item + 1)\n\n    # Combine scores with adaptive weighting.\n    # The weights are designed to:\n    # - Heavily favor bins that offer a tight fit (tightness_score).\n    # - Give a moderate boost to bins that are already fuller (fullness_score).\n    # - Introduce a penalty for leaving excessively large gaps by inverting the future_fit_score.\n    #   A smaller (less leftover space) future_fit_score is better, so we subtract it.\n    # The coefficients (1.0, 0.7, 0.3) are hyperparameters that can be tuned.\n    # We are emphasizing the tightness and fullness more, while still penalizing large leftovers.\n    combined_scores = (tightness_score * 1.0) + (fullness_score * 0.7) - (future_fit_score * 0.3)\n\n    # Normalize priorities to a [0, 1] range.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = np.clip(combined_scores / max_score, 0, 1)\n    else:\n        # If all scores are effectively zero, assign a minimal uniform priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.01\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Core Strategy: Prioritize bins that leave minimal remaining capacity (Best Fit).\n    # Use the inverse of remaining capacity after fitting. Add a small epsilon for stability.\n    # This directly targets the objective of minimizing wasted space.\n    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-9)\n\n    # Predictive Component: Consider the \"potential\" of a bin for future items.\n    # A bin with slightly more remaining capacity after fitting a current item might be\n    # more versatile for subsequent items. This is a trade-off: a tighter fit is good now,\n    # but a slightly looser fit might be better overall.\n    # We can model this by giving a bonus to bins that are \"almost full\" but can still fit the item,\n    # prioritizing those that would have a \"just right\" remaining capacity for common item sizes.\n    # A simple approach is to penalize bins that leave a very large gap.\n    # We can use the negative logarithm of the remaining capacity after fit. Smaller remaining capacity is better,\n    # so a smaller value for log1p(remaining_cap) is better. We invert this by multiplying by -1.\n    # Using log1p to handle cases where remaining capacity is 0 gracefully.\n    future_potential_score = -np.log1p(fittable_bins_remain_cap - item)\n\n    # Adaptability: The relative importance of \"tight fit\" vs. \"future potential\" can change.\n    # For instance, if items are generally small, prioritizing slightly larger remaining capacity might be good.\n    # If items are large, tight fits are paramount.\n    # We can introduce a dynamic weighting factor based on the item size relative to the bin capacity (implicitly, the median or average remaining capacity).\n    # For simplicity, let's consider the item size relative to the bin capacity if we knew it, but we only have remaining capacities.\n    # A proxy could be the item size relative to the *average* remaining capacity of fittable bins.\n    # If the item is small relative to available space, future potential might be more important.\n    # If the item is large, fitting it tightly is more important.\n\n    avg_remaining_fittable = np.mean(fittable_bins_remain_cap)\n    item_size_ratio = item / (avg_remaining_fittable + 1e-9)\n\n    # Weighting:\n    # If item_size_ratio is high (item is large relative to average remaining capacity),\n    # prioritize Best Fit more. If low (item is small), give more weight to future potential.\n    # A sigmoid-like function could map item_size_ratio to a weight for Best Fit.\n    # Let's use a simple linear scaling for now.\n\n    # Normalize item_size_ratio to a [0, 1] range (approximate).\n    # A more robust normalization might involve historical data or a fixed large value.\n    # For now, assume item sizes are within a reasonable range compared to bin capacities.\n    # A heuristic upper bound for item_size_ratio could be the bin capacity itself (if items <= bin_cap).\n    # Let's consider the max possible ratio for a single item being 1 if item == bin_cap.\n    # So, max_ratio_considered = 1.0 (for item fitting exactly).\n    # The weight for best_fit_score will increase as item_size_ratio increases.\n    # The weight for future_potential_score will decrease as item_size_ratio increases.\n\n    # Let's use a sigmoid-like weighting for Best Fit: w_bf = 1 / (1 + exp(-k * (item_size_ratio - threshold)))\n    # A simpler approach: linear interpolation.\n    # If item_size_ratio is small (e.g., 0.1), bf_weight=0.2, fp_weight=0.8\n    # If item_size_ratio is large (e.g., 1.0), bf_weight=1.0, fp_weight=0.0\n    # We can use an exponential decay for future potential weight.\n    bf_weight = 1.0 - np.exp(-item_size_ratio * 2.0)  # As item_size_ratio grows, bf_weight approaches 1\n    fp_weight = np.exp(-item_size_ratio * 2.0)       # As item_size_ratio grows, fp_weight approaches 0\n\n    # Combine scores with adaptive weights\n    combined_scores = (best_fit_score * bf_weight) + (future_potential_score * fp_weight)\n\n    # Normalize priorities to a [0, 1] range.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n    else:\n        # If all scores are effectively zero, assign a small uniform priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Core Strategy: Prioritize bins that leave minimal remaining capacity (Best Fit).\n    # Use the inverse of remaining capacity after fitting. Add a small epsilon for stability.\n    # This directly targets the objective of minimizing wasted space.\n    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-9)\n\n    # Predictive Component: Consider the \"potential\" of a bin for future items.\n    # A bin with slightly more remaining capacity after fitting a current item might be\n    # more versatile for subsequent items. This is a trade-off: a tighter fit is good now,\n    # but a slightly looser fit might be better overall.\n    # We can model this by giving a bonus to bins that are \"almost full\" but can still fit the item,\n    # prioritizing those that would have a \"just right\" remaining capacity for common item sizes.\n    # A simple approach is to penalize bins that leave a very large gap.\n    # We can use the negative logarithm of the remaining capacity after fit. Smaller remaining capacity is better,\n    # so a smaller value for log1p(remaining_cap) is better. We invert this by multiplying by -1.\n    # Using log1p to handle cases where remaining capacity is 0 gracefully.\n    future_potential_score = -np.log1p(fittable_bins_remain_cap - item)\n\n    # Adaptability: The relative importance of \"tight fit\" vs. \"future potential\" can change.\n    # For instance, if items are generally small, prioritizing slightly larger remaining capacity might be good.\n    # If items are large, tight fits are paramount.\n    # We can introduce a dynamic weighting factor based on the item size relative to the bin capacity (implicitly, the median or average remaining capacity).\n    # For simplicity, let's consider the item size relative to the bin capacity if we knew it, but we only have remaining capacities.\n    # A proxy could be the item size relative to the *average* remaining capacity of fittable bins.\n    # If the item is small relative to available space, future potential might be more important.\n    # If the item is large, fitting it tightly is more important.\n\n    avg_remaining_fittable = np.mean(fittable_bins_remain_cap)\n    item_size_ratio = item / (avg_remaining_fittable + 1e-9)\n\n    # Weighting:\n    # If item_size_ratio is high (item is large relative to average remaining capacity),\n    # prioritize Best Fit more. If low (item is small), give more weight to future potential.\n    # A sigmoid-like function could map item_size_ratio to a weight for Best Fit.\n    # Let's use a simple linear scaling for now.\n\n    # Normalize item_size_ratio to a [0, 1] range (approximate).\n    # A more robust normalization might involve historical data or a fixed large value.\n    # For now, assume item sizes are within a reasonable range compared to bin capacities.\n    # A heuristic upper bound for item_size_ratio could be the bin capacity itself (if items <= bin_cap).\n    # Let's consider the max possible ratio for a single item being 1 if item == bin_cap.\n    # So, max_ratio_considered = 1.0 (for item fitting exactly).\n    # The weight for best_fit_score will increase as item_size_ratio increases.\n    # The weight for future_potential_score will decrease as item_size_ratio increases.\n\n    # Let's use a sigmoid-like weighting for Best Fit: w_bf = 1 / (1 + exp(-k * (item_size_ratio - threshold)))\n    # A simpler approach: linear interpolation.\n    # If item_size_ratio is small (e.g., 0.1), bf_weight=0.2, fp_weight=0.8\n    # If item_size_ratio is large (e.g., 1.0), bf_weight=1.0, fp_weight=0.0\n    # We can use an exponential decay for future potential weight.\n    bf_weight = 1.0 - np.exp(-item_size_ratio * 2.0)  # As item_size_ratio grows, bf_weight approaches 1\n    fp_weight = np.exp(-item_size_ratio * 2.0)       # As item_size_ratio grows, fp_weight approaches 0\n\n    # Combine scores with adaptive weights\n    combined_scores = (best_fit_score * bf_weight) + (future_potential_score * fp_weight)\n\n    # Normalize priorities to a [0, 1] range.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n    else:\n        # If all scores are effectively zero, assign a small uniform priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Core Strategy: Prioritize bins that leave minimal remaining capacity (Best Fit).\n    # Use the inverse of remaining capacity after fitting. Add a small epsilon for stability.\n    # This directly targets the objective of minimizing wasted space.\n    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-9)\n\n    # Predictive Component: Consider the \"potential\" of a bin for future items.\n    # A bin with slightly more remaining capacity after fitting a current item might be\n    # more versatile for subsequent items. This is a trade-off: a tighter fit is good now,\n    # but a slightly looser fit might be better overall.\n    # We can model this by giving a bonus to bins that are \"almost full\" but can still fit the item,\n    # prioritizing those that would have a \"just right\" remaining capacity for common item sizes.\n    # A simple approach is to penalize bins that leave a very large gap.\n    # We can use the negative logarithm of the remaining capacity after fit. Smaller remaining capacity is better,\n    # so a smaller value for log1p(remaining_cap) is better. We invert this by multiplying by -1.\n    # Using log1p to handle cases where remaining capacity is 0 gracefully.\n    future_potential_score = -np.log1p(fittable_bins_remain_cap - item)\n\n    # Adaptability: The relative importance of \"tight fit\" vs. \"future potential\" can change.\n    # For instance, if items are generally small, prioritizing slightly larger remaining capacity might be good.\n    # If items are large, tight fits are paramount.\n    # We can introduce a dynamic weighting factor based on the item size relative to the bin capacity (implicitly, the median or average remaining capacity).\n    # For simplicity, let's consider the item size relative to the bin capacity if we knew it, but we only have remaining capacities.\n    # A proxy could be the item size relative to the *average* remaining capacity of fittable bins.\n    # If the item is small relative to available space, future potential might be more important.\n    # If the item is large, fitting it tightly is more important.\n\n    avg_remaining_fittable = np.mean(fittable_bins_remain_cap)\n    item_size_ratio = item / (avg_remaining_fittable + 1e-9)\n\n    # Weighting:\n    # If item_size_ratio is high (item is large relative to average remaining capacity),\n    # prioritize Best Fit more. If low (item is small), give more weight to future potential.\n    # A sigmoid-like function could map item_size_ratio to a weight for Best Fit.\n    # Let's use a simple linear scaling for now.\n\n    # Normalize item_size_ratio to a [0, 1] range (approximate).\n    # A more robust normalization might involve historical data or a fixed large value.\n    # For now, assume item sizes are within a reasonable range compared to bin capacities.\n    # A heuristic upper bound for item_size_ratio could be the bin capacity itself (if items <= bin_cap).\n    # Let's consider the max possible ratio for a single item being 1 if item == bin_cap.\n    # So, max_ratio_considered = 1.0 (for item fitting exactly).\n    # The weight for best_fit_score will increase as item_size_ratio increases.\n    # The weight for future_potential_score will decrease as item_size_ratio increases.\n\n    # Let's use a sigmoid-like weighting for Best Fit: w_bf = 1 / (1 + exp(-k * (item_size_ratio - threshold)))\n    # A simpler approach: linear interpolation.\n    # If item_size_ratio is small (e.g., 0.1), bf_weight=0.2, fp_weight=0.8\n    # If item_size_ratio is large (e.g., 1.0), bf_weight=1.0, fp_weight=0.0\n    # We can use an exponential decay for future potential weight.\n    bf_weight = 1.0 - np.exp(-item_size_ratio * 2.0)  # As item_size_ratio grows, bf_weight approaches 1\n    fp_weight = np.exp(-item_size_ratio * 2.0)       # As item_size_ratio grows, fp_weight approaches 0\n\n    # Combine scores with adaptive weights\n    combined_scores = (best_fit_score * bf_weight) + (future_potential_score * fp_weight)\n\n    # Normalize priorities to a [0, 1] range.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n    else:\n        # If all scores are effectively zero, assign a small uniform priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.1\n\n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 0.00032513075268971587, fill_ratio_weight: float = 0.3828821161554049) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a \"fill ratio\" bonus. Prioritizes bins that fit the item snugly\n    and are already relatively full, aiming for denser packing.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array representing the remaining capacity of each bin.\n        epsilon: A small value to prevent division by zero.\n        fill_ratio_weight: A weight to adjust the influence of the fill ratio score.\n\n    Returns:\n        A numpy array representing the priority of each bin for packing the item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    eligible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(eligible_bins_mask):\n        return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a fill-ratio bonus, favoring bins that are both a good fit and already utilized.\n    This heuristic prioritizes bins that are nearly full and leave minimal residual capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Filter bins that can fit the item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Calculate the remaining capacity after fitting the item\n    remaining_capacity_after_fit = fitting_bins_remain_cap - item\n    \n    # --- Best Fit Component ---\n    # Score is inversely proportional to the residual capacity after fitting.\n    # Adding a small epsilon to avoid division by zero.\n    # A score of 1.0 means the bin is perfectly filled after adding the item.\n    proximity_score = 1.0 / (remaining_capacity_after_fit + 1e-9)\n    \n    # --- Fill Ratio Component ---\n    # Assumes a default bin capacity of 1.0.\n    # Fill ratio is the proportion of capacity already used. Higher is better.\n    bin_capacity = 1.0 \n    current_fill_ratio = (bin_capacity - fitting_bins_remain_cap) / bin_capacity\n    fill_ratio_score = current_fill_ratio\n    \n    # --- Combined Score ---\n    # Multiply proximity score by fill ratio score.\n    # This prioritizes bins that are both a good fit (high proximity) AND already well-utilized (high fill ratio).\n    # The idea is to prefer bins that are already \"almost full\" and can still accommodate the item snugly.\n    combined_score = proximity_score * fill_ratio_score\n    \n    # --- Refinement: Ensure proximity is considered for empty bins ---\n    # If a bin was empty (fill_ratio=0), combined_score would be 0.\n    # We want to ensure that even in this case, the proximity score is still considered,\n    # as an empty bin might be the only option or a good first fit.\n    # We take the maximum of the combined score and the proximity score itself,\n    # effectively giving proximity score at least its due when fill ratio is zero.\n    priorities[can_fit_mask] = np.maximum(combined_score, proximity_score * (fill_ratio_score > 1e-9))\n    \n    # Add a small epsilon to all valid priorities to ensure that even if\n    # all scores are very low, they are distinct and positive, aiding tie-breaking.\n    priorities[can_fit_mask] += 1e-6\n    \n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 0.00032513075268971587, fill_ratio_weight: float = 0.3828821161554049) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a \"fill ratio\" bonus. Prioritizes bins that fit the item snugly\n    and are already relatively full, aiming for denser packing.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array representing the remaining capacity of each bin.\n        epsilon: A small value to prevent division by zero.\n        fill_ratio_weight: A weight to adjust the influence of the fill ratio score.\n\n    Returns:\n        A numpy array representing the priority of each bin for packing the item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    eligible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(eligible_bins_mask):\n        return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a fill-ratio bonus, favoring bins that are both a good fit and already utilized.\n    This heuristic prioritizes bins that are nearly full and leave minimal residual capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Filter bins that can fit the item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Calculate the remaining capacity after fitting the item\n    remaining_capacity_after_fit = fitting_bins_remain_cap - item\n    \n    # --- Best Fit Component ---\n    # Score is inversely proportional to the residual capacity after fitting.\n    # Adding a small epsilon to avoid division by zero.\n    # A score of 1.0 means the bin is perfectly filled after adding the item.\n    proximity_score = 1.0 / (remaining_capacity_after_fit + 1e-9)\n    \n    # --- Fill Ratio Component ---\n    # Assumes a default bin capacity of 1.0.\n    # Fill ratio is the proportion of capacity already used. Higher is better.\n    bin_capacity = 1.0 \n    current_fill_ratio = (bin_capacity - fitting_bins_remain_cap) / bin_capacity\n    fill_ratio_score = current_fill_ratio\n    \n    # --- Combined Score ---\n    # Multiply proximity score by fill ratio score.\n    # This prioritizes bins that are both a good fit (high proximity) AND already well-utilized (high fill ratio).\n    # The idea is to prefer bins that are already \"almost full\" and can still accommodate the item snugly.\n    combined_score = proximity_score * fill_ratio_score\n    \n    # --- Refinement: Ensure proximity is considered for empty bins ---\n    # If a bin was empty (fill_ratio=0), combined_score would be 0.\n    # We want to ensure that even in this case, the proximity score is still considered,\n    # as an empty bin might be the only option or a good first fit.\n    # We take the maximum of the combined score and the proximity score itself,\n    # effectively giving proximity score at least its due when fill ratio is zero.\n    priorities[can_fit_mask] = np.maximum(combined_score, proximity_score * (fill_ratio_score > 1e-9))\n    \n    # Add a small epsilon to all valid priorities to ensure that even if\n    # all scores are very low, they are distinct and positive, aiding tie-breaking.\n    priorities[can_fit_mask] += 1e-6\n    \n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a fill-ratio bonus, favoring bins that are both a good fit and already utilized.\n    This heuristic prioritizes bins that are nearly full and leave minimal residual capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Filter bins that can fit the item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Calculate the remaining capacity after fitting the item\n    remaining_capacity_after_fit = fitting_bins_remain_cap - item\n    \n    # --- Best Fit Component ---\n    # Score is inversely proportional to the residual capacity after fitting.\n    # Adding a small epsilon to avoid division by zero.\n    # A score of 1.0 means the bin is perfectly filled after adding the item.\n    proximity_score = 1.0 / (remaining_capacity_after_fit + 1e-9)\n    \n    # --- Fill Ratio Component ---\n    # Assumes a default bin capacity of 1.0.\n    # Fill ratio is the proportion of capacity already used. Higher is better.\n    bin_capacity = 1.0 \n    current_fill_ratio = (bin_capacity - fitting_bins_remain_cap) / bin_capacity\n    fill_ratio_score = current_fill_ratio\n    \n    # --- Combined Score ---\n    # Multiply proximity score by fill ratio score.\n    # This prioritizes bins that are both a good fit (high proximity) AND already well-utilized (high fill ratio).\n    # The idea is to prefer bins that are already \"almost full\" and can still accommodate the item snugly.\n    combined_score = proximity_score * fill_ratio_score\n    \n    # --- Refinement: Ensure proximity is considered for empty bins ---\n    # If a bin was empty (fill_ratio=0), combined_score would be 0.\n    # We want to ensure that even in this case, the proximity score is still considered,\n    # as an empty bin might be the only option or a good first fit.\n    # We take the maximum of the combined score and the proximity score itself,\n    # effectively giving proximity score at least its due when fill ratio is zero.\n    priorities[can_fit_mask] = np.maximum(combined_score, proximity_score * (fill_ratio_score > 1e-9))\n    \n    # Add a small epsilon to all valid priorities to ensure that even if\n    # all scores are very low, they are distinct and positive, aiding tie-breaking.\n    priorities[can_fit_mask] += 1e-6\n    \n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}