{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Calculates priority scores for bins using an adaptive prioritization strategy.\n    This heuristic combines proximity fit with a look-ahead to favor bins that,\n    after packing the current item, leave a remaining capacity that is \"useful\"\n    for future items (i.e., not too large, not too small).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_cap.size > 0:\n        # Proximity Fit component: Higher priority for bins closer to item size\n        differences = fitting_bins_cap - item\n        proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Adaptive component: Penalize bins that leave very large or very small remainders\n        # The goal is to keep remaining capacities in a range that is more likely to be filled by future items.\n        # A common heuristic is to target remaining capacities around half the bin size or specific item sizes.\n        # For simplicity here, we'll penalize very large remainders, aiming for a tighter fit.\n        # The penalty is inversely proportional to how much \"wasted\" space is left.\n        # We'll define \"wasted\" space as the remainder itself.\n        # A larger remainder means a higher penalty.\n        \n        # Example: If bin capacity is 100, and item is 30, remainder is 70.\n        # If item is 60, remainder is 40.\n        # We want to penalize the remainder of 70 more than 40.\n        # We can use a score that is high for small remainders and low for large remainders.\n        # Let's use 1 / (remainder + epsilon)\n        \n        # To make it adaptive, we can consider the *distribution* of remaining capacities.\n        # If most bins have large remainders, a large remainder might not be that bad.\n        # However, for a general heuristic, aiming for smaller remainders is often good.\n        \n        # Let's consider the \"goodness\" of the remaining capacity.\n        # A remaining capacity of 0 is good if it means the bin is full.\n        # A remaining capacity that is exactly another item's size is good.\n        # A remaining capacity that is too large is bad (wasted space).\n        # A remaining capacity that is too small is also bad (cannot fit many items).\n        \n        # A simple adaptive approach: penalize large remainders.\n        # Consider the range of remaining capacities for bins that can fit the item.\n        # A simple inverse relation to the remainder: 1 / (remainder + epsilon)\n        # This is similar to proximity, but we are looking at the *resulting* remainder.\n        \n        # Let's refine this:\n        # We want to favor bins where (bin_cap - item) is small, but not too small.\n        # And we want to avoid leaving very large residual capacities.\n        \n        # Let's use a score that is high for intermediate remainders and low for extreme remainders.\n        # For example, a Gaussian-like function centered around a \"target\" remainder.\n        # A simple target could be the average remainder of fitting bins.\n        \n        target_remainder = np.mean(differences) if differences.size > 0 else 0\n        \n        # If target_remainder is very small or very large, this might not be ideal.\n        # Let's use a more robust target, e.g., half the item size or a fixed value like 10.\n        # Or, more simply, let's just penalize large remainders.\n        \n        # Consider a score that is 1 / (1 + difference) which is similar to proximity.\n        # Now let's add a penalty for the *resulting* remainder.\n        # The resulting remainder is `fitting_bins_cap - item`.\n        # We want to favor smaller resulting remainders.\n        \n        # Let's combine proximity score with a score for the resulting remainder.\n        # Score = proximity_score * (1 / (resulting_remainder + epsilon))\n        # This means bins that are a close fit AND leave a small remainder are prioritized.\n        \n        resulting_remainders = fitting_bins_cap - item\n        \n        # To avoid issues with very small resulting remainders (which are good),\n        # let's ensure the score doesn't become infinite.\n        # The proximity score already handles \"close fit\".\n        \n        # Let's try a different approach for adaptiveness:\n        # The ideal scenario is to leave a remaining capacity that can fit another item perfectly.\n        # We don't know future items, but we can try to leave a \"versatile\" capacity.\n        # A remaining capacity around half of the typical item size, or a fixed small value,\n        # might be more versatile than very large or very small remainders.\n        \n        # Let's create a score that favors remainders within a certain range.\n        # For simplicity, let's favor remainders that are not too large.\n        # If we penalize large remainders, this encourages packing items tightly.\n        \n        # Let's create a \"utility\" score for the remaining capacity.\n        # A remaining capacity `r` is good if it's small (tight fit) but not zero (unless bin is full).\n        # It's also good if it can accommodate some small future items.\n        \n        # Let's combine proximity and a \"utility\" of the remaining capacity.\n        # Utility function: Higher for smaller remainders, but not zero.\n        # e.g., 1 / (resulting_remainder + epsilon)\n        \n        # Combined score: proximity_score * utility_score\n        # This means we prefer bins that are a close fit AND leave a small remainder.\n        \n        utility_scores = 1.0 / (resulting_remainders + 1e-9)\n        \n        # Combining the two:\n        # We want to give higher priority to bins that are a good fit AND leave a good remainder.\n        # Let's scale both and multiply.\n        \n        # Proximity score is 1 / (fitting_bins_cap - item + epsilon)\n        # Utility score is 1 / (fitting_bins_cap - item + epsilon)\n        # This is redundant.\n        \n        # Let's rethink the adaptive part.\n        # The key idea of adaptiveness is to use information from the current state to guide the heuristic.\n        # The state is the `bins_remain_cap`.\n        \n        # Consider the variance of remaining capacities. If variance is low, most bins are similar.\n        # If variance is high, there's a wide range.\n        \n        # Let's go back to the original goal: minimize the number of bins.\n        # This implies trying to pack items as tightly as possible, but also avoiding creating \"unusable\" small spaces.\n        \n        # A simple adaptive heuristic:\n        # Prioritize bins that are a good fit (proximity) but also consider the resulting capacity.\n        # Let's favor bins that leave a remainder that is \"balanced\" - not too small, not too large.\n        # A common strategy in online algorithms is to use a \"look-ahead\" or to consider properties of the remaining capacity distribution.\n        \n        # Let's define \"good\" remaining capacity as being between a small threshold and a large threshold.\n        # For example, if bin capacity is 100, a remaining capacity of 10-40 might be considered good.\n        # This is hard to generalize without knowing item size distribution.\n        \n        # A simpler approach: Prioritize bins that are a tight fit (small remainder) but\n        # avoid bins that would leave an *extremely* small remainder, as that might be less flexible.\n        \n        # Let's use a score that rewards proximity but then penalizes if the resulting remainder is too small.\n        # For example, if item is 30, and bin has 31 remaining. Proximity is high. Resulting remainder is 1.\n        # We might prefer a bin with 40 remaining (item 30, remainder 10).\n        \n        # Score = proximity_score * penalty_for_small_remainder\n        # Penalty for small remainder: 1 / (1 + small_remainder_value)\n        \n        # Let's try:\n        # Score = (1 / (difference + epsilon)) * (1 / (1 + resulting_remainder))\n        # This means we prioritize bins that are a good fit AND leave a small remainder.\n        # This is also quite similar to just proximity.\n        \n        # Let's focus on the *distribution* of remaining capacities of fitting bins.\n        # If the item is large, it might only fit in a few bins, and those bins might have large remainders.\n        # If the item is small, it might fit in many bins, some with small remainders, some with large.\n        \n        # Adaptive strategy: Consider the \"value\" of the remaining capacity.\n        # A more sophisticated approach could involve learning or predicting future item sizes.\n        # For a pure heuristic, we need to encode this \"value\" based on current state.\n        \n        # Let's use the proximity score and then modify it based on the resulting remainder relative to other fitting bins.\n        \n        # For each fitting bin:\n        #   Calculate proximity: prox = 1 / (remaining_cap - item + epsilon)\n        #   Calculate resulting remainder: res_rem = remaining_cap - item\n        #   Calculate a \"remainder score\": rem_score.\n        #     This score should be higher for remainders that are \"useful\".\n        #     Useful could mean: not too small, not too large.\n        #     Let's try a score that peaks at a certain remainder size, e.g., around 50% of the bin capacity, or a fixed small value.\n        #     For simplicity, let's just reward smaller remainders, but with a cap to avoid extreme scores.\n        #     rem_score = 1 / (res_rem + epsilon) # This is still just proximity.\n        \n        # Let's try to make the priority dependent on how *unique* a bin is in terms of its remaining capacity.\n        # If an item fits perfectly in many bins, it doesn't matter which we pick.\n        # If an item fits in only one bin, we should pick that one, regardless of its remainder.\n        \n        # How about: Priority = proximity_score * (1 + C * normalized_residual_capacity)\n        # Where C is a parameter and normalized_residual_capacity is the residual capacity\n        # scaled by something, e.g., the average residual capacity of fitting bins.\n        \n        # Let's try a simpler adaptive idea:\n        # Prioritize bins that are a good fit (proximity), BUT if there are multiple bins\n        # with similar proximity scores, favor the one that leaves a smaller remainder.\n        # This is implicitly done by 1/(diff + epsilon) if diff is small.\n        \n        # The \"adaptive\" part should perhaps respond to the overall state of the bins.\n        # If all bins have very large remaining capacities, maybe we should prioritize the closest fit.\n        # If bins have varying capacities, we can be more selective.\n        \n        # Let's introduce a penalty for leaving \"too much\" excess capacity.\n        # The excess capacity is `remaining_cap - item`.\n        # We want to minimize this excess capacity.\n        # So, our score should be inversely related to `excess_capacity`.\n        \n        # Let's try:\n        # Score = proximity_score * (1 / (resulting_remainder + epsilon))\n        # This means we favor bins that are a good fit AND leave a small remainder.\n        # This is essentially rewarding tight packing.\n        \n        # The adaptiveness can come from how we combine proximity and remainder score.\n        # Instead of multiplying, maybe we add or use a more complex function.\n        \n        # Let's consider the \"waste\" ratio: `(remaining_cap - item) / remaining_cap`\n        # We want to minimize this waste ratio.\n        # Score would be inversely proportional to this ratio.\n        \n        # Let's try a combined score:\n        # Score = proximity_score - lambda * waste_ratio\n        # Where lambda is a weighting factor.\n        \n        # Let's stick to a score that prioritizes bins that are a close fit and leave a small remainder.\n        # The problem with `1.0 / (differences + 1e-9)` is that it highly favors the absolute closest fit.\n        # This might leave very small remainders which are not useful.\n        \n        # Adaptive approach:\n        # Favor bins that are a close fit, BUT also favor bins that leave a \"reasonably sized\" remainder.\n        # A remainder that's too small is bad. A remainder that's too large is also bad.\n        # Let's define a \"good\" remainder as being within a certain range.\n        # For example, a remainder `r` is good if `epsilon <= r <= target_remainder_max`.\n        # We can penalize remainders outside this range.\n        \n        # Let's define a score for the remainder:\n        # If `res_rem < epsilon`: penalty (e.g., 0.1)\n        # If `res_rem > target_remainder_max`: penalty (e.g., 0.1 * res_rem / target_remainder_max)\n        # If `epsilon <= res_rem <= target_remainder_max`: reward (e.g., 1.0)\n        \n        # This requires defining `target_remainder_max`. Let's set it to be, say, 20% of bin capacity or a fixed small value.\n        # This is getting complicated for a simple heuristic.\n        \n        # Let's simplify the adaptive idea:\n        # Prioritize bins that are a close fit (proximity_score).\n        # If there are multiple bins with high proximity scores, break ties by choosing the one with the smallest resulting remainder.\n        # This is still implicitly handled by the `1/(diff)` if `diff` is small.\n        \n        # Let's try a score that is sensitive to the *distribution* of remaining capacities.\n        # If a bin's remaining capacity is very close to the item size, that's good (proximity).\n        # If a bin's remaining capacity is such that `remaining_cap - item` is small, that's also good (tight packing).\n        \n        # Let's combine proximity and \"tightness of packing\" directly.\n        # Proximity Score: `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)`\n        # Tightness Score: `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` (This is the same!)\n        \n        # Let's try to reward bins that, after packing, leave a remaining capacity that is *not too small*.\n        # This encourages leaving space for potentially future items, rather than filling up bins too precisely.\n        \n        # Score = proximity_score * (1 + resulting_remainder / average_fitting_remainder)\n        # This rewards bins that are a close fit AND leave a larger-than-average remainder. This seems counter-intuitive for minimizing bins.\n        \n        # Let's try to reward bins that leave a remainder that is \"medium\" sized.\n        # For example, a remainder `r` is good if `r` is around `item_size / 2` or some other target.\n        \n        # Let's use a simpler adaptive idea:\n        # Prioritize bins that are a close fit.\n        # THEN, among those, prioritize bins that leave a remainder that is itself a \"good\" candidate for future items.\n        # A \"good\" remainder could be one that is not too small and not too large.\n        # Let's represent this by penalizing very small and very large remainders.\n        \n        resulting_remainders = fitting_bins_cap - item\n        \n        # Let's use a score that penalizes large remainders more heavily.\n        # This promotes tighter packing.\n        # Score = proximity_score * (1 / (resulting_remainder + epsilon))\n        # This gives more weight to bins that are a close fit AND leave a small remainder.\n        \n        # Let's try a different combination:\n        # Proximity score favors `remaining_cap` closest to `item`.\n        # Let's also consider the \"waste ratio\": `(remaining_cap - item) / bin_capacity`.\n        # We want to minimize this waste ratio.\n        \n        # Let's combine the two using a weighted sum, or by modifying the proximity score.\n        \n        # Adaptive component: If an item fits into multiple bins with similar proximity scores,\n        # then we want to make a choice that is \"better\" for the overall packing.\n        # This often means leaving a remainder that is \"useful\".\n        \n        # Let's try this:\n        # The base priority is proximity.\n        # We will add a bonus if the resulting remainder is \"good\".\n        # A \"good\" remainder is one that is not too small.\n        # Let's define \"good\" as being greater than some small threshold, e.g., 10% of bin capacity or a fixed small value.\n        \n        # This is hard to implement generically without knowing bin capacity.\n        # Let's assume a fixed bin capacity or use an average.\n        \n        # Simple adaptive idea:\n        # Prioritize bins that are a close fit.\n        # Among bins with similar \"closeness\", prefer the one that leaves a smaller remaining capacity.\n        # This is already captured by the `1/(diff + epsilon)` if we prioritize the smallest `diff`.\n        \n        # Let's introduce a \"balancing\" factor.\n        # The ideal scenario is to leave a remainder that can fit another item.\n        # We can't know future items, but we can aim for \"versatile\" remainders.\n        # A versatile remainder might be one that is not extremely small or extremely large.\n        \n        # Let's combine proximity with a penalty for large remainders.\n        # Score = proximity_score - lambda * (resulting_remainder / max_possible_remainder)\n        \n        # Let's try a different adaptive approach based on the distribution of fitting bins:\n        # If there's only one bin that fits, its priority is 1.\n        # If multiple bins fit, we use proximity.\n        # THEN, we can adjust the scores based on the resulting remainders.\n        \n        # Let's try to penalize bins that leave very large remainders.\n        # Score = proximity_score * (1 / (1 + resulting_remainder))\n        # This emphasizes bins that are a close fit AND leave a small remainder.\n        \n        # This is still essentially proximity, but with a stronger preference for smaller remainders.\n        \n        # Let's try a more direct \"adaptive\" approach:\n        # Consider the *average* remaining capacity of the bins that can fit the item.\n        # If the current item is large, it might only fit in bins with large remaining capacities.\n        # If the current item is small, it might fit in bins with various remaining capacities.\n        \n        # Let's try to encourage leaving remainders that are not too small.\n        # If `resulting_remainder` is very small, reduce the priority.\n        \n        # Score = proximity_score * (1 + alpha * min(0, resulting_remainder - threshold))\n        # Where threshold is a small value. This penalizes remainders below the threshold.\n        \n        # Let's refine the proximity score to be less sensitive to tiny differences.\n        # Instead of `1 / (diff + epsilon)`, maybe `1 / (1 + diff^p)` for `p > 1`.\n        # Or, use a step function, or capped inverse.\n        \n        # Let's go with a simple adaptive idea:\n        # Prioritize bins that are a close fit (proximity_score).\n        # THEN, among bins with similar proximity scores, prioritize the one that leaves the smallest remainder.\n        # This is often achieved by `1 / (diff + epsilon)`.\n        \n        # What if we modify the proximity score based on the \"usefulness\" of the resulting capacity?\n        # A useful capacity is one that can fit other items.\n        # Let's consider the \"ideal\" remainder to be around half of the bin capacity (as a guess).\n        # Score = proximity_score * exp(-(resulting_remainder - target_remainder)^2 / (2 * variance))\n        # This favors remainders close to the target.\n        \n        # For a simpler heuristic, let's just penalize large remainders more.\n        # Score = proximity_score * (1 / (1 + resulting_remainder))\n        # This aims for a tight fit.\n        \n        # Let's try to make it more \"adaptive\" by considering the *range* of fitting bins.\n        # If all fitting bins have very similar remaining capacities, the proximity score is enough.\n        # If fitting bins have a wide range of capacities, maybe we should prefer those that are not \"extreme\" in their leftover space.\n        \n        # Let's use a score that rewards proximity and penalizes large resulting remainders.\n        # Score = (1 / (difference + 1e-9)) * (1 / (1 + resulting_remainders))\n        # This effectively gives higher scores to bins that are a good fit AND leave a small remainder.\n        \n        # Let's try to introduce a slight bias away from perfectly filling bins,\n        # by penalizing very small remainders, but still favoring tight fits overall.\n        \n        # Score = (1 / (difference + 1e-9)) * (resulting_remainders + 1) / (resulting_remainders + 1 + small_penalty_term)\n        \n        # Let's try this adaptive approach:\n        # Combine proximity with a score that reflects how \"full\" the bin becomes.\n        # We want bins to be full, but not *so* full that the remainder is unusable.\n        # Let's use a score that is high for small differences, but decreases if the resulting remainder is too small.\n        \n        # Score = 1.0 / (differences + 1e-9)  # Proximity\n        # Let's modify this by penalizing small resulting remainders.\n        # If resulting_remainder < some_threshold (e.g., 10% of bin capacity): penalize.\n        \n        # Let's make the priority score a function of both the difference AND the resulting remainder,\n        # aiming for a balance between a close fit and a useful leftover space.\n        \n        # Score = (1 / (difference + 1e-9)) * (1 / (1 + resulting_remainders))\n        # This prioritizes bins that are a close fit AND leave a small remainder. This encourages tight packing.\n        \n        # Let's try to achieve \"balanced packing\" by rewarding remainders that are not too small.\n        # Score = proximity_score * (resulting_remainders + 1) / (resulting_remainders + 1 + alpha * small_remainder_penalty)\n        \n        # A simple adaptive heuristic:\n        # Prioritize bins that are a close fit (proximity score).\n        # Then, among those, prioritize the ones that leave a remainder that is \"balanced\".\n        # Balanced remainder: not too small, not too large.\n        # Let's use a score that favors remainders in a medium range.\n        \n        # Let's try to make the priority score proportional to the proximity, but also inversely proportional to the resulting remainder.\n        # This encourages close fits AND small remainders.\n        \n        resulting_remainders = fitting_bins_cap - item\n        \n        # Let's use a combined score that emphasizes both proximity and small resulting remainders.\n        # Score = (1 / (difference + 1e-9)) * (1 / (resulting_remainders + 1e-9))\n        # This effectively prioritizes bins that are a very close fit and leave a minimal remainder.\n        \n        # To make it more \"adaptive\", let's consider the ratio of difference to resulting remainder.\n        # Or, let's modify the proximity score based on the *quality* of the resulting remainder.\n        \n        # Let's define a \"remainder quality\" score.\n        # If `r` is the remainder, and `max_r` is the maximum remainder among fitting bins.\n        # Quality could be `1 - (r / max_r)` (favors smaller remainders).\n        \n        # Let's combine proximity and a penalty for large remainders.\n        # Score = proximity_score * (1 / (1 + resulting_remainders))\n        \n        # This still heavily favors the absolute closest fit.\n        \n        # Let's try to add a term that rewards remainders that are not too small.\n        # Score = proximity_score + alpha * log(resulting_remainders + 1)\n        # This adds a bonus for larger remainders, but log scales it down.\n        \n        # Let's try the simplest form of adaptiveness:\n        # If multiple bins have the same best proximity score, choose the one with the smallest remainder.\n        # The `1 / (diff + epsilon)` already does this.\n        \n        # Let's try to make the priority score less sensitive to minuscule differences by capping the inverse.\n        # Or by using a different function like `1 / (1 + diff^2)`.\n        \n        # Let's introduce an adaptive component by considering the *distribution* of remaining capacities.\n        # If the item is small and fits in many bins, we can be more selective.\n        # If the item is large and fits in few bins, we might need to accept larger remainders.\n        \n        # Let's try to achieve a \"balanced\" packing:\n        # Prioritize bins that are a close fit.\n        # Then, among those, prioritize bins that leave a remainder that is not too small.\n        # Score = proximity_score * (1 + alpha * (resulting_remainders / avg_fitting_remainder))\n        # This would favor bins that are a close fit AND leave a *larger* remainder. This is counterproductive.\n        \n        # Let's try rewarding bins that leave a remainder that is \"useful\".\n        # A useful remainder might be one that's not too small.\n        \n        # Score = proximity_score * f(resulting_remainders)\n        # Where f is a function that is high for medium remainders and low for small/large remainders.\n        # For simplicity, let's just penalize large remainders.\n        \n        # Score = proximity_score * (1 / (1 + resulting_remainders))\n        # This favors bins that are a close fit and leave a small remainder.\n        \n        # Let's try to make it more adaptive by considering the *scale* of the differences.\n        # If all differences are large, `1/diff` might not differentiate well.\n        # If all differences are small, `1/diff` can lead to extreme values.\n        \n        # Let's use the inverse of the *normalized* difference.\n        # normalized_diff = (fitting_bins_cap - item) / fitting_bins_cap\n        # Score = 1 / (normalized_diff + epsilon)\n        \n        # This prioritizes bins where the item takes up a larger proportion of the bin.\n        \n        # Let's combine proximity and \"fullness\" of the bin.\n        # Fullness Score = item / fitting_bins_cap\n        # Proximity Score = 1 / (fitting_bins_cap - item + epsilon)\n        \n        # Let's try to create a score that is sensitive to the *relative* closeness.\n        # Consider the \"gap\" = `fitting_bins_cap - item`.\n        # We want small gaps.\n        # Let's also consider the \"fill ratio\" = `item / fitting_bins_cap`.\n        # We want high fill ratio, but small gaps.\n        \n        # Let's try a score that rewards both:\n        # Score = (item / fitting_bins_cap) * (1 / (fitting_bins_cap - item + epsilon))\n        # This multiplies the fill ratio by the proximity score.\n        # A bin that is almost full and has a small remaining capacity will get a high score.\n        \n        fill_ratios = item / fitting_bins_cap\n        differences = fitting_bins_cap - item\n        \n        # Combined score: higher fill ratio AND smaller difference.\n        # Let's try multiplying them.\n        # Score = fill_ratios * (1 / (differences + 1e-9))\n        \n        # Example: Bin Cap = 100\n        # Item = 30: fits in bins with caps >= 30.\n        # Bin 1: rem_cap = 40. diff = 10. fill_ratio = 30/40 = 0.75. score = 0.75 * (1/10) = 0.075\n        # Bin 2: rem_cap = 35. diff = 5.  fill_ratio = 30/35 = 0.857. score = 0.857 * (1/5) = 0.1714\n        # Bin 3: rem_cap = 100. diff = 70. fill_ratio = 30/100 = 0.3. score = 0.3 * (1/70) = 0.004\n        \n        # This seems to work well: it favors bins that are close fits AND have a high fill ratio.\n        # The fill ratio component implicitly penalizes bins that would leave very large remainders.\n        \n        # Now, we need to normalize these scores.\n        \n        priorities[can_fit_mask] = fill_ratios * (1.0 / (differences + 1e-9))\n        \n        # Normalize priorities so that the best fit bin has a score of 1\n        max_priority = np.max(priorities[can_fit_mask])\n        if max_priority > 0:\n            priorities[can_fit_mask] /= max_priority\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritizes bins by favoring those with least remaining capacity after packing,\n    while also considering the likelihood of future fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate the remaining capacity if the item is placed in each bin\n    potential_remaining_cap = bins_remain_cap - item\n    \n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins that can fit the item, assign priority based on remaining capacity\n    # Higher priority for bins with less remaining capacity (tightest fit)\n    # Add a small epsilon to avoid division by zero\n    priorities[can_fit_mask] = 1.0 / (potential_remaining_cap[can_fit_mask] + 1e-9)\n    \n    # Further adjust priorities: bins that leave more remaining capacity\n    # after packing might be preferred if they are large enough to fit\n    # future, potentially larger items. This is a simple form of adaptive\n    # prioritization, favoring slightly less tight fits that retain more 'room'.\n    # We multiply by the potential remaining capacity itself.\n    # This is a heuristic that balances tight fits with future flexibility.\n    priorities[can_fit_mask] *= potential_remaining_cap[can_fit_mask]\n\n    # Normalize priorities to avoid extremely large values and ensure a better distribution\n    # This makes the heuristic less sensitive to extreme differences in remaining capacity.\n    if np.any(priorities):\n        priorities /= np.max(priorities)\n        \n    return priorities\n\n### Analyze & experience\n- *   **Heuristics 1 & 6 vs. Heuristics 2 & 3:** Heuristics 1 and 6 attempt to combine proximity fit with a consideration for the *resulting* remainder, aiming for a balance. They use a multiplicative approach (fill ratio * proximity) or explicitly try to penalize large remainders. Heuristics 2 and 3 focus on a \"Best Fit\" with an \"Almost Full\" bias, but their adaptive component is less sophisticated, relying on an arbitrary small residual or absolute threshold. The core idea of balancing fit with future utility is stronger in 1 & 6.\n\n*   **Heuristics 1 & 6 vs. Heuristics 4 & 7 & 8 & 12:** Heuristics 4, 7, 8, and 12 introduce an \"exploration\" component (adding a small boost or random selection) to the basic \"Best Fit\" (inverse of remaining capacity). While exploration can be beneficial in some search contexts, for a greedy priority heuristic, it often dilutes the core objective of finding the *best* immediate fit. Heuristics 1 and 6's adaptive components are more directly related to packing efficiency.\n\n*   **Heuristics 1 & 6 vs. Heuristics 5:** Heuristic 5 tries to combine proximity, minimal waste bonus, and an exact fit penalty. While it attempts multiple factors, the combination (multiplication with enhancement and penalty) can be complex and harder to tune than the more straightforward multiplicative approach of Heuristic 1 (fill ratio * proximity).\n\n*   **Heuristics 1 & 6 vs. Heuristics 9, 10, 11:** Heuristics 9, 10, and 11 are incomplete and only define the initialization of priorities and a mask. They don't implement any scoring logic, making them inherently the worst.\n\n*   **Heuristics 1 & 6 vs. Heuristics 13 & 14:** Heuristics 13 and 14 combine proximity with an \"adaptive bonus\" that rewards leaving more capacity for non-perfect fits. This is a different strategy than 1 & 6, which aim to reward *tight* fits but consider the resulting remainder. The bonus for leaving *more* capacity seems counter-intuitive for a heuristic focused on minimizing bins unless specifically designed for a scenario where future items are guaranteed to be much larger. Heuristic 1's approach (high fill ratio + proximity) is generally more aligned with dense packing.\n\n*   **Heuristics 1 & 6 vs. Heuristics 15:** Heuristic 15 uses an inverse difference score combined with a sigmoid centered on the median difference. This is a sophisticated adaptive approach aiming to differentiate between bins near the median. However, it might be overly complex compared to Heuristic 1's more direct multiplicative strategy, which achieves a similar goal of favoring tighter fits and penalizing large remainders via the fill ratio.\n\n*   **Heuristics 1 & 6 vs. Heuristics 16 & 17:** Heuristics 16 and 17 combine proximity with a \"fullness bonus,\" weighted differently. They try to balance a preference for tighter fits (proximity) with a preference for bins with more remaining capacity (fullness). This is a reasonable strategy but might be less direct than Heuristic 1's approach of maximizing the \"fill ratio * proximity\" which inherently favors bins that are both close fits and have high initial fill ratios.\n\n*   **Heuristics 1 & 6 vs. Heuristics 18, 19, 20:** Heuristics 18, 19, and 20 implement a \"Best Fit\" (1/residual) multiplied by the residual itself. This heuristic attempts to balance tight fits with future flexibility by favoring bins that leave *some* remaining capacity, but not too much. Heuristic 1's multiplicative approach (fill ratio * proximity) is arguably more direct in achieving dense packing by rewarding bins that are already well-utilized and also represent a close fit. The multiplication by `potential_remaining_cap` in 18-20 might overly favor bins with moderate residuals, potentially at the cost of a truly tight fit.\n\nOverall, Heuristics 1 and 6 stand out for their balanced approach using a multiplicative score that combines high fill ratio with proximity, aiming for dense packing.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Balance, Utility, Efficiency, Simplicity, Exploration Bias.\n*   **Advice:** Focus on simple, quantifiable metrics that capture both immediate fit and long-term potential. Introduce complexity *only* when data clearly indicates current strategies are insufficient.\n*   **Avoid:** Redundant implementations that offer marginal improvements. Over-reliance on arbitrary thresholds or overly complex weighting schemes without empirical justification.\n*   **Explanation:** Ineffective reflection often gets bogged down in minor implementation differences. Effective reflection questions *why* a strategy is chosen, assessing its core principle and its impact on the overall objective (better packing). Strive for elegant solutions, not just variations on a theme.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}