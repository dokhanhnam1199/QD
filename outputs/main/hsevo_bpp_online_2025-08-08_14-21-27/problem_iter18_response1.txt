```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    fittable_bins_mask = bins_remain_cap >= item

    if not np.any(fittable_bins_mask):
        return priorities

    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]

    # Component 1: Best Fit - Prioritize bins with minimal remaining capacity after packing.
    # This is a direct measure of immediate packing efficiency.
    # Adding epsilon to avoid division by zero.
    best_fit_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-6)

    # Component 2: Future Utility - Prioritize bins that have more remaining capacity
    # after packing, as they might be more useful for future items.
    # This is a measure of long-term bin utility.
    # We use the inverse of the capacity after packing, so larger capacity is better.
    future_utility_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-6)
    # Invert this score so that *larger* remaining capacity is *better*.
    future_utility_scores = 1.0 / (fittable_bins_remain_cap - item + 1e-6) if np.all(fittable_bins_remain_cap - item > 0) else np.zeros_like(fittable_bins_remain_cap)


    # Component 3: Balance of Fit - Penalize bins that leave a very large gap.
    # This encourages more compact packing.
    # We use a log scale to penalize large gaps less severely than small gaps.
    # A value of 1 is added to avoid log(0).
    balance_scores = -np.log1p(fittable_bins_remain_cap - item)

    # Adaptive Weighting based on item size relative to bin capacity
    # If the item is large (close to bin capacity), prioritize Best Fit.
    # If the item is small, prioritize Future Utility and Balance.
    # Let's assume a nominal bin capacity, or use the maximum initial capacity if known.
    # For simplicity, let's assume an average initial capacity or use the max of bins_remain_cap if it represents initial state.
    # Without knowing the initial state, we'll use a proxy: the item size itself to gauge "largeness".
    # A simple approach: if item is > 50% of some reference capacity, lean towards BF.
    # Let's use a hypothetical 1.0 as a reference capacity for simplicity in this example.
    reference_capacity = 1.0
    item_size_ratio = item / reference_capacity

    weight_best_fit = 1.0
    weight_future_utility = 1.0
    weight_balance = 1.0

    if item_size_ratio > 0.7: # For large items
        weight_best_fit = 1.5
        weight_future_utility = 0.5
        weight_balance = 1.0
    elif item_size_ratio < 0.3: # For small items
        weight_best_fit = 0.5
        weight_future_utility = 1.5
        weight_balance = 1.2
    else: # For medium items
        weight_best_fit = 1.0
        weight_future_utility = 1.0
        weight_balance = 1.0

    combined_scores = (
        (best_fit_scores * weight_best_fit) +
        (future_utility_scores * weight_future_utility) +
        (balance_scores * weight_balance)
    )

    # Normalize scores for fittable bins to a [0, 1] range
    if np.max(combined_scores) > 1e-9:
        normalized_scores = np.clip(combined_scores / np.max(combined_scores), 0, 1)
    else:
        # If all scores are zero or very small, assign a small uniform priority
        normalized_scores = np.full_like(combined_scores, 0.1)

    priorities[fittable_bins_mask] = normalized_scores

    return priorities
```
