{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines tight fit preference with a bonus for bins that are nearly full,\n    prioritizing efficient use of space and minimizing waste.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    \n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    \n    if fitting_bins_cap.size > 0:\n        differences = fitting_bins_cap - item\n        \n        # Proximity score: Inverse of the difference (higher for tighter fits)\n        proximity_scores = 1.0 / (differences + 1e-9)\n        \n        # Fullness bonus: Higher for bins that are already more utilized\n        # This rewards bins that are \"nearly full\" and can still accommodate the item.\n        # We use the inverse of the remaining capacity as a proxy for fullness.\n        fullness_bonus = fitting_bins_cap / (np.max(bins_remain_cap) + 1e-9) \n        \n        # Combine scores: Multiply proximity by fullness bonus.\n        # This favors bins that are both a tight fit AND are already substantially filled.\n        combined_scores = proximity_scores * (1 + fullness_bonus * 0.5) # Slightly weight the fullness bonus\n        \n        priorities[can_fit_mask] = combined_scores\n        \n        # Normalize priorities to ensure the highest score is 1\n        max_priority = np.max(priorities[can_fit_mask])\n        if max_priority > 0:\n            priorities[can_fit_mask] /= max_priority\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Adaptive Best-Fit: Combines 'Best Fit' proximity with a bonus for bins that\n    leave a small but not zero remainder, promoting tighter packing and future flexibility.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate remaining capacity after fitting the item\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Base priority: Inverse of remaining capacity (Best Fit)\n    # Higher score for smaller remaining capacity\n    # Add a small epsilon to prevent division by zero for perfect fits\n    base_priority = 1.0 / (remaining_capacity_after_fit + 1e-9)\n    \n    # Adaptive bonus: Reward bins that leave a small, positive remainder.\n    # This encourages tighter packing while leaving a tiny bit of space,\n    # which can be beneficial for subsequent items.\n    # We use a scaled inverse of the remaining capacity for this bonus.\n    # A small positive residual gets a higher bonus.\n    # This is inspired by heuristics that balance tight fit with future utility.\n    # A simple approach is to multiply by the remaining capacity itself,\n    # effectively favoring residuals that are small but non-zero.\n    # This is similar to some elements in 'priority_v1' but without normalization.\n    adaptive_bonus = remaining_capacity_after_fit * 0.5 # A tunable factor for the bonus\n\n    # Combine base priority with adaptive bonus\n    # Higher values indicate better suitability\n    priorities[can_fit_mask] = base_priority + adaptive_bonus\n    \n    # Ensure priorities are non-negative, though logic should prevent this\n    priorities = np.maximum(0, priorities)\n    \n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1st and 2nd, 1st uses `eligible_bins / (max_eligible_cap + 1e-9)` for fill ratio, while 2nd uses `fittable_bins_remain_cap / (max_eligible_cap + 1e-9)`. 1st's approach seems more directly related to how full the bin is relative to other fitting bins.\n\nComparing Heuristics 2nd and 7th (which are identical), no change is observed.\n\nComparing Heuristics 4th and 8th (which are identical), no change is observed.\n\nComparing Heuristics 5th and Heuristics 14th/15th/16th: Heuristics 14-16 attempt to incorporate an \"adaptive bonus\" based on the logarithm of the resulting remainder, aiming to balance tight fits with leaving some space. Heuristic 5th solely relies on inverse difference (Best Fit). The inclusion of an adaptive bonus in 14-16 suggests a more nuanced approach than simple Best Fit.\n\nComparing Heuristics 10th/11th/12th/13th with others: These heuristics combine proximity with fill ratio. Heuristic 10th's `proximity_score * fill_ratio_score` seems like a straightforward combination. The `np.maximum(combined_score, proximity_score * (current_fill_ratio > 1e-9))` attempts to ensure proximity is considered even for empty bins, which is a good robustness measure.\n\nComparing Heuristics 17th/18th/19th/20th: These heuristics use `base_priority + adaptive_bonus`, where `base_priority` is Best Fit and `adaptive_bonus` is `remaining_capacity_after_fit * 0.5`. This additive approach for the bonus is a simpler alternative to multiplicative combinations seen in other heuristics.\n\nOverall: Heuristics that combine multiple factors (Best Fit, Fill Ratio, adaptive bonuses) tend to be ranked higher. The specific combination method (multiplicative vs. additive) and the nature of the adaptive bonus (logarithmic, linear scaling) seem to differentiate performance. Normalization is consistently applied to keep scores in a comparable range.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Objective Alignment, Trade-offs, Robustness, Predictive Power.\n*   **Advice:** Focus on aligning heuristic scoring with underlying optimization goals, explicitly modeling trade-offs between immediate gains and future packing potential. Use adaptive mechanisms that learn or respond to problem state for enhanced predictive power.\n*   **Avoid:** Redundant comparisons of near-identical strategies. Overly simplistic or static weighting schemes that don't account for dynamic problem states or interactions between objectives.\n*   **Explanation:** Effective self-reflection identifies *why* a heuristic works or fails, focusing on the underlying principles and interactions. This allows for the creation of more sophisticated, robust, and predictive heuristics by understanding the nuanced trade-offs and potential future implications of each decision.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}