{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Adaptive Priority: Combines 'Best Fit' with an 'Almost Full' bias.\n    Prioritizes bins that leave minimal space, favoring near-perfect fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate remaining capacity for bins that can fit the item\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Heuristic: Prioritize bins that result in less remaining capacity.\n    # This is a \"Best Fit\" like strategy.\n    # Using 1 / (1 + residual_capacity) to give higher scores to smaller residuals.\n    # A residual capacity of 0 gets a score of 1. A large residual gets a score close to 0.\n    priorities[can_fit_mask] = 1.0 / (1.0 + remaining_capacity_after_fit)\n    \n    # Additional bias: Slightly boost priority for bins that become nearly full (e.g., residual < 0.1 * bin_capacity)\n    # This \"almost full\" bias encourages tighter packing and potentially better overall utilization.\n    # We'll apply a small multiplier to these bins.\n    original_bin_capacities = bins_remain_cap[can_fit_mask] # Assuming we know original capacities or can infer\n    # For this example, let's assume a fixed bin capacity, say 1.0, for demonstration\n    # In a real scenario, bin capacity would be a parameter or known context.\n    # If bin_capacity is not fixed, this bias needs adjustment or a different approach.\n    # For simplicity here, let's assume a standard bin capacity is known or implied.\n    # Let's use a placeholder if bin capacity is not explicitly available.\n    # If bin_capacity is available, it would be:\n    # almost_full_mask = remaining_capacity_after_fit < (bin_capacity * 0.1) \n    \n    # Without explicit bin capacity, we'll use a small absolute residual as a proxy for 'almost full'\n    # For example, if the remaining capacity is very small (e.g., less than 0.05)\n    small_residual_bias_mask = remaining_capacity_after_fit < 0.05\n    priorities[can_fit_mask][small_residual_bias_mask] *= 1.1 # Apply a small boost\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins by favoring those with minimal remaining capacity after packing, while also considering the overall bin count.\n\n    This heuristic aims to fill bins as much as possible (Best Fit like) while\n    implicitly encouraging the use of fewer bins by giving a slight boost to bins\n    that are already well-utilized.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins_mask = bins_remain_cap >= item\n\n    if not np.any(valid_bins_mask):\n        return priorities\n\n### Analyze & experience\n- *   **Heuristics 1 & 6 vs. Heuristics 2 & 3:** Heuristics 1 and 6 attempt to combine proximity fit with a consideration for the *resulting* remainder, aiming for a balance. They use a multiplicative approach (fill ratio * proximity) or explicitly try to penalize large remainders. Heuristics 2 and 3 focus on a \"Best Fit\" with an \"Almost Full\" bias, but their adaptive component is less sophisticated, relying on an arbitrary small residual or absolute threshold. The core idea of balancing fit with future utility is stronger in 1 & 6.\n\n*   **Heuristics 1 & 6 vs. Heuristics 4 & 7 & 8 & 12:** Heuristics 4, 7, 8, and 12 introduce an \"exploration\" component (adding a small boost or random selection) to the basic \"Best Fit\" (inverse of remaining capacity). While exploration can be beneficial in some search contexts, for a greedy priority heuristic, it often dilutes the core objective of finding the *best* immediate fit. Heuristics 1 and 6's adaptive components are more directly related to packing efficiency.\n\n*   **Heuristics 1 & 6 vs. Heuristics 5:** Heuristic 5 tries to combine proximity, minimal waste bonus, and an exact fit penalty. While it attempts multiple factors, the combination (multiplication with enhancement and penalty) can be complex and harder to tune than the more straightforward multiplicative approach of Heuristic 1 (fill ratio * proximity).\n\n*   **Heuristics 1 & 6 vs. Heuristics 9, 10, 11:** Heuristics 9, 10, and 11 are incomplete and only define the initialization of priorities and a mask. They don't implement any scoring logic, making them inherently the worst.\n\n*   **Heuristics 1 & 6 vs. Heuristics 13 & 14:** Heuristics 13 and 14 combine proximity with an \"adaptive bonus\" that rewards leaving more capacity for non-perfect fits. This is a different strategy than 1 & 6, which aim to reward *tight* fits but consider the resulting remainder. The bonus for leaving *more* capacity seems counter-intuitive for a heuristic focused on minimizing bins unless specifically designed for a scenario where future items are guaranteed to be much larger. Heuristic 1's approach (high fill ratio + proximity) is generally more aligned with dense packing.\n\n*   **Heuristics 1 & 6 vs. Heuristics 15:** Heuristic 15 uses an inverse difference score combined with a sigmoid centered on the median difference. This is a sophisticated adaptive approach aiming to differentiate between bins near the median. However, it might be overly complex compared to Heuristic 1's more direct multiplicative strategy, which achieves a similar goal of favoring tighter fits and penalizing large remainders via the fill ratio.\n\n*   **Heuristics 1 & 6 vs. Heuristics 16 & 17:** Heuristics 16 and 17 combine proximity with a \"fullness bonus,\" weighted differently. They try to balance a preference for tighter fits (proximity) with a preference for bins with more remaining capacity (fullness). This is a reasonable strategy but might be less direct than Heuristic 1's approach of maximizing the \"fill ratio * proximity\" which inherently favors bins that are both close fits and have high initial fill ratios.\n\n*   **Heuristics 1 & 6 vs. Heuristics 18, 19, 20:** Heuristics 18, 19, and 20 implement a \"Best Fit\" (1/residual) multiplied by the residual itself. This heuristic attempts to balance tight fits with future flexibility by favoring bins that leave *some* remaining capacity, but not too much. Heuristic 1's multiplicative approach (fill ratio * proximity) is arguably more direct in achieving dense packing by rewarding bins that are already well-utilized and also represent a close fit. The multiplication by `potential_remaining_cap` in 18-20 might overly favor bins with moderate residuals, potentially at the cost of a truly tight fit.\n\nOverall, Heuristics 1 and 6 stand out for their balanced approach using a multiplicative score that combines high fill ratio with proximity, aiming for dense packing.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Balance, Utility, Efficiency, Simplicity, Exploration Bias.\n*   **Advice:** Focus on simple, quantifiable metrics that capture both immediate fit and long-term potential. Introduce complexity *only* when data clearly indicates current strategies are insufficient.\n*   **Avoid:** Redundant implementations that offer marginal improvements. Over-reliance on arbitrary thresholds or overly complex weighting schemes without empirical justification.\n*   **Explanation:** Ineffective reflection often gets bogged down in minor implementation differences. Effective reflection questions *why* a strategy is chosen, assessing its core principle and its impact on the overall objective (better packing). Strive for elegant solutions, not just variations on a theme.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}