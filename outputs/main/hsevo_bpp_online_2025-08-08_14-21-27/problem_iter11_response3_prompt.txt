{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a penalty for very small remaining capacity,\n    favoring tight fits while slightly discouraging bins that become overly full.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fitting_bins_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Best Fit Score: Inverse of remaining capacity after placement.\n    # Higher score for bins that leave less capacity (tighter fit).\n    best_fit_scores = 1.0 / (fitting_bins_cap - item + 1e-9)\n\n    # Adaptive Penalty for Small Residuals: Penalize bins that leave a very small remainder.\n    # This aims to avoid situations where a bin is *too* full, potentially leaving no room\n    # for even slightly larger items later. We use an inverse relationship with the remainder.\n    # A small remainder (e.g., 0.1) gets a lower penalty score (e.g., 0.1 / (0.1 + 0.5) ~ 0.16),\n    # while a larger remainder (e.g., 10) gets a higher penalty score (e.g., 10 / (10 + 0.5) ~ 0.95).\n    # We want to *discourage* very small remainders, so we'll use this score to adjust the best_fit_scores.\n    # Specifically, we'll multiply the best_fit_scores by a factor that decreases as the remainder gets smaller.\n    # Let's invert this penalty concept: a *good* residual quality score should be *high* for moderate remainders.\n    # Instead of penalty, let's frame it as a \"residual quality bonus\" where small residuals are penalized.\n    # A simple penalty function for small residuals: exp(-residual / sensitivity)\n    # Where sensitivity is a parameter controlling how quickly the penalty kicks in.\n    sensitivity = 2.0  # Controls how strongly small remainders are penalized.\n    residual_quality_factor = np.exp(-(fitting_bins_cap - item) / sensitivity)\n\n\n    # Combine scores: Multiply Best Fit by the residual quality factor.\n    # This prioritizes tight fits (high best_fit_scores) but reduces their priority\n    # if they leave an extremely small remainder (low residual_quality_factor).\n    combined_scores = best_fit_scores * residual_quality_factor\n\n    # Normalize combined scores to [0, 1] to make them comparable across different calls.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = combined_scores / max_score\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    A refined heuristic that balances immediate fit quality with long-term bin utilization,\n    prioritizing bins that are nearly full but can still accommodate the item,\n    while also considering the potential for future fits of smaller items.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n\n    # Score 1: Tightness of Fit (Best Fit Component)\n    # Prioritizes bins where the remaining capacity after packing is minimal.\n    # Add a small epsilon to prevent division by zero.\n    tightness_score = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n\n    # Score 2: Bin Fullness (Encouraging fuller bins)\n    # This score is high for bins that are already quite full (low remaining capacity).\n    # We use the inverse of the remaining capacity.\n    fullness_score = 1.0 / (fittable_bins_remain_cap + 1e-6)\n\n    # Score 3: Future Fit Potential (Adaptive Component)\n    # This score is higher for bins that, after fitting the current item,\n    # will still have a significant amount of remaining capacity. This aims to\n    # leave \"room\" for smaller items later, preventing premature bin exhaustion.\n    # We use a logarithmic function to give diminishing returns for very large remaining capacities.\n    # Adding 1 to prevent log(0) or log(negative).\n    future_fit_score = np.log1p(bins_remain_cap[fittable_bins_mask] - item + 1)\n\n    # Combine scores with adaptive weighting.\n    # The weights are designed to:\n    # - Heavily favor bins that offer a tight fit (tightness_score).\n    # - Give a moderate boost to bins that are already fuller (fullness_score).\n    # - Introduce a penalty for leaving excessively large gaps by inverting the future_fit_score.\n    #   A smaller (less leftover space) future_fit_score is better, so we subtract it.\n    # The coefficients (1.0, 0.7, 0.3) are hyperparameters that can be tuned.\n    # We are emphasizing the tightness and fullness more, while still penalizing large leftovers.\n    combined_scores = (tightness_score * 1.0) + (fullness_score * 0.7) - (future_fit_score * 0.3)\n\n    # Normalize priorities to a [0, 1] range.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9:\n        priorities[fittable_bins_mask] = np.clip(combined_scores / max_score, 0, 1)\n    else:\n        # If all scores are effectively zero, assign a minimal uniform priority to fittable bins.\n        priorities[fittable_bins_mask] = 0.01\n\n    return priorities\n\n### Analyze & experience\n- *   **Heuristics 1st vs. Heuristics 2nd:** These are identical. The ranking suggests a tie or a misunderstanding in the prompt.\n*   **Heuristics 3rd vs. Heuristics 4th:** Heuristic 3rd uses a multiplicative combination of Best Fit and Fill Ratio. Heuristic 4th uses Best Fit multiplied by an exponential `residual_quality_factor`. Heuristic 3rd's multiplicative approach with a simple fill ratio is conceptually clearer for dense packing than the exponential penalty on small residuals, which might be too aggressive or not well-calibrated without tuning.\n*   **Heuristics 5th vs. Heuristics 6th:** Heuristic 5th is identical to Heuristic 4th. Heuristic 6th introduces multiple scores (Best Fit, Worst Fit, Fullness, Fit Tightness Penalty) and adaptive weights, making it more complex and potentially harder to tune than simpler combinations. The \"adaptive weights\" in Heuristic 6th seem intended to react to item size but are not clearly defined or implemented in a way that demonstrably improves upon simpler strategies without extensive parameter tuning.\n*   **Heuristics 7th vs. Heuristics 8th:** Heuristic 7th is identical to Heuristic 6th. Heuristic 8th combines Best Fit with a simple \"fullness bonus\" (inverse of remaining capacity). This is less sophisticated than Heuristic 1st or 3rd, as it doesn't directly penalize large residual gaps or explicitly consider the \"tightness\" of the fit in its bonus calculation.\n*   **Heuristics 9th vs. Heuristics 10th:** Heuristic 9th is identical to Heuristics 4th and 5th. Heuristic 10th is identical to Heuristic 3rd.\n*   **Heuristics 11th vs. Heuristics 12th:** Heuristic 11th combines Best Fit proximity with a logarithmic \"Usefulness Bonus\" on remaining capacity. Heuristic 12th combines tightness, fullness, and a negative logarithmic \"Future Fit Score.\" Heuristic 12th's approach is more comprehensive, attempting to balance multiple aspects (tightness, fullness, and not leaving *too* much space) but with a subtraction of the future fit score, which might be counter-intuitive. Heuristic 11th's multiplicative approach with `(1 + 0.2 * adaptive_bonus)` is a more direct way to boost good fits that also leave reasonable space.\n*   **Heuristics 13th vs. Heuristics 14th:** These are identical. They use adaptive weighting based on item size ratio to balance Best Fit and Future Potential. The exponential weighting is a reasonable approach for adaptability.\n*   **Heuristics 15th vs. Heuristics 16th:** Heuristic 15th is identical to Heuristics 13th and 14th. Heuristic 16th is incomplete, only containing imports and docstrings, and includes unused library imports (random, math, scipy, torch).\n*   **Heuristics 17th vs. Heuristics 18th:** Heuristics 17th and 20th are identical. They combine Best Fit (proximity) and Fill Ratio, using a multiplicative approach, with a refinement to handle empty bins. Heuristic 18th is identical to Heuristic 16th, incomplete.\n*   **Heuristics 19th vs. Heuristics 20th:** Heuristic 19th is identical to Heuristics 17th and 20th.\n\n**Overall:** The higher-ranked heuristics (1st, 3rd, 11th, 17th/19th/20th) tend to combine Best Fit with another factor (fullness, fill ratio, or a bonus for moderate remaining space) using either additive or multiplicative logic. Lower-ranked heuristics either introduce too many complex, potentially conflicting scores (6th, 7th), use simpler additive combinations without nuanced logic (8th), or are incomplete (16th, 18th). Heuristics that attempt adaptive weighting based on item size (13th, 14th, 15th) are conceptually good but rely on careful tuning of those weights. The multiplicative approach of combining Best Fit with a Fill Ratio (3rd, 10th, 17th, 19th, 20th) seems robust.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Multi-objective optimization, multiplicative scoring, adaptive weighting, robustness, simplification.\n*   **Advice:** Design heuristics that explicitly balance immediate packing gains (e.g., Best Fit) with long-term bin utilization (e.g., fill ratio). Experiment with multiplicative combinations of these metrics.\n*   **Avoid:** Redundant implementations, overly complex scoring functions without clear justification, and heuristics that don't directly address the trade-off between immediate and future packing efficiency.\n*   **Explanation:** Focusing on combining well-performing, conceptually distinct metrics multiplicatively creates synergistic effects. Avoiding complexity ensures interpretability and easier tuning, leading to more robust and generalizable heuristics.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}