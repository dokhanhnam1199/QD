```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    fittable_bins_mask = bins_remain_cap >= item

    if not np.any(fittable_bins_mask):
        return priorities

    fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]

    # Core Strategy: Prioritize bins that leave minimal remaining capacity (Best Fit).
    # Use the inverse of remaining capacity after fitting. Add a small epsilon for stability.
    # This directly targets the objective of minimizing wasted space.
    best_fit_score = 1.0 / (fittable_bins_remain_cap - item + 1e-9)

    # Predictive Component: Consider the "potential" of a bin for future items.
    # A bin with slightly more remaining capacity after fitting a current item might be
    # more versatile for subsequent items. This is a trade-off: a tighter fit is good now,
    # but a slightly looser fit might be better overall.
    # We can model this by giving a bonus to bins that are "almost full" but can still fit the item,
    # prioritizing those that would have a "just right" remaining capacity for common item sizes.
    # A simple approach is to penalize bins that leave a very large gap.
    # We can use the negative logarithm of the remaining capacity after fit. Smaller remaining capacity is better,
    # so a smaller value for log1p(remaining_cap) is better. We invert this by multiplying by -1.
    # Using log1p to handle cases where remaining capacity is 0 gracefully.
    future_potential_score = -np.log1p(fittable_bins_remain_cap - item)

    # Adaptability: The relative importance of "tight fit" vs. "future potential" can change.
    # For instance, if items are generally small, prioritizing slightly larger remaining capacity might be good.
    # If items are large, tight fits are paramount.
    # We can introduce a dynamic weighting factor based on the item size relative to the bin capacity (implicitly, the median or average remaining capacity).
    # For simplicity, let's consider the item size relative to the bin capacity if we knew it, but we only have remaining capacities.
    # A proxy could be the item size relative to the *average* remaining capacity of fittable bins.
    # If the item is small relative to available space, future potential might be more important.
    # If the item is large, fitting it tightly is more important.

    avg_remaining_fittable = np.mean(fittable_bins_remain_cap)
    item_size_ratio = item / (avg_remaining_fittable + 1e-9)

    # Weighting:
    # If item_size_ratio is high (item is large relative to average remaining capacity),
    # prioritize Best Fit more. If low (item is small), give more weight to future potential.
    # A sigmoid-like function could map item_size_ratio to a weight for Best Fit.
    # Let's use a simple linear scaling for now.

    # Normalize item_size_ratio to a [0, 1] range (approximate).
    # A more robust normalization might involve historical data or a fixed large value.
    # For now, assume item sizes are within a reasonable range compared to bin capacities.
    # A heuristic upper bound for item_size_ratio could be the bin capacity itself (if items <= bin_cap).
    # Let's consider the max possible ratio for a single item being 1 if item == bin_cap.
    # So, max_ratio_considered = 1.0 (for item fitting exactly).
    # The weight for best_fit_score will increase as item_size_ratio increases.
    # The weight for future_potential_score will decrease as item_size_ratio increases.

    # Let's use a sigmoid-like weighting for Best Fit: w_bf = 1 / (1 + exp(-k * (item_size_ratio - threshold)))
    # A simpler approach: linear interpolation.
    # If item_size_ratio is small (e.g., 0.1), bf_weight=0.2, fp_weight=0.8
    # If item_size_ratio is large (e.g., 1.0), bf_weight=1.0, fp_weight=0.0
    # We can use an exponential decay for future potential weight.
    bf_weight = 1.0 - np.exp(-item_size_ratio * 2.0)  # As item_size_ratio grows, bf_weight approaches 1
    fp_weight = np.exp(-item_size_ratio * 2.0)       # As item_size_ratio grows, fp_weight approaches 0

    # Combine scores with adaptive weights
    combined_scores = (best_fit_score * bf_weight) + (future_potential_score * fp_weight)

    # Normalize priorities to a [0, 1] range.
    max_score = np.max(combined_scores)
    if max_score > 1e-9:
        priorities[fittable_bins_mask] = combined_scores / max_score
    else:
        # If all scores are effectively zero, assign a small uniform priority to fittable bins.
        priorities[fittable_bins_mask] = 0.1

    return priorities
```
