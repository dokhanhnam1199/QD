{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n                bins_remain_cap: np.ndarray,\n                waste_penalty_factor: float = 0.8575834075161552,\n                bin_fraction_penalty: float = 0.4693634879473551,\n                item_size_weight: float = 2.0884148461764993,\n                random_perturbation_scale: float = 0.06717097613120268) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics:\n        1. Waste Minimization: Prioritizes bins where the remaining space after\n           packing the item is minimal.  A near-perfect fit is highly valued.\n        2. Capacity Threshold: Avoids bins with extremely small remaining\n           capacity to reduce fragmentation. Bins that cannot fit are penalized.\n        3. Bin Level Awareness : Considers the initial capacity of the bins for a more balanced distribution\n        4. Balancing Bin Utilization:  Slight preference for bins that are not\n           completely empty or completely full to maintain flexibility.\n        5. Random Perturbation: Introduces small randomness to avoid getting stuck\n           in local optima and explore slightly different packing arrangements.\n        6. Item size awareness : Larger Items should fill bins as much as possible\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        waste_penalty_factor: Factor to adjust the impact of waste minimization.\n        bin_fraction_penalty: Factor to adjust the balancing of bin utilization.\n        item_size_weight: Weight of item size awareness.\n        random_perturbation_scale: Scale of random noise.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n\n    # Near-perfect fit bonus (minimize waste)\n    priorities += waste_penalty_factor * np.exp(-waste)  # Exponential decay for increasing waste.\n\n    # Bin utilization balancing - slightly prefers bins that aren't empty or full.  avoids extremities.  Parabolic preference\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += bin_fraction_penalty * -(bin_fraction - 0.5)**2  # Adds a parabolic preference curve.\n    \n    # Item Size Awareness.  Larger items fill bins up\n    priorities += item_size_weight * item/original_capacity\n\n    # Random Perturbation (introduces some \"quantum\" fluctuation). Very small value for numerical stability\n    priorities += np.random.normal(0, random_perturbation_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines waste minimization with a target utilization, penalizing infeasible bins.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    wasted_space = bins_remain_cap - item\n    fit_mask = wasted_space >= 0\n    if np.any(fit_mask):\n        scaling_factor = 2.0\n        priorities[fit_mask] = np.exp(-scaling_factor * wasted_space[fit_mask]**2 / bins_remain_cap[fit_mask].mean())\n        desirable_remaining_space = 0.2 * np.max(bins_remain_cap)\n        priority_values = np.exp(-((wasted_space[fit_mask] ) ** 2) / (2 * (desirable_remaining_space/2)** 2))\n        priorities[fit_mask] = 0.5*priorities[fit_mask]+ 0.5*priority_values\n    priorities[~fit_mask] = -1e9\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see 1st uses waste minimization, capacity threshold, bin utilization balancing, item size awareness, and random perturbation with parameter tuning while the 20th use gravity/velocity with capacity and a buffer, lacking parameter tuning and random perturbation.\nComparing (2nd best) vs (second worst), we see 2nd uses waste minimization, capacity threshold, bin utilization balancing, item size awareness, and random perturbation with parameter tuning while the 19th use gravity/velocity with capacity and a buffer, lacking parameter tuning and random perturbation. Comparing (1st) vs (2nd), we see the two are identical, likely a copy-paste error. (3rd) vs (4th) also shows identical code, likely a copy-paste error. Comparing (second worst) vs (worst), we see the two are identical, likely a copy-paste error. Overall: The best heuristics include multiple factors (waste, capacity, utilization, item size, randomness), use parameter tuning and more sophisticated equations (exponential, sigmoid). Poorer heuristics focus on fewer factors and simpler equations, without parameter tuning. Specifically, the better heuristics prioritize a combination of waste minimization, bin utilization balancing, item size considerations, and random perturbation, often employing techniques like sigmoid scaling and adaptive adjustments. They also include mechanisms to avoid fragmentation. The worse heuristics tend to focus on a smaller subset of these factors, use simpler mathematical formulations, and lack adaptive or dynamic adjustments based on problem characteristics. The penalty terms for infeasible bins are also better handled in the top-ranked functions.\n- \nOkay, let's refine \"Current Self-Reflection\" for designing better heuristics.\n\n*   **Keywords:** Multifactorial Analysis, Adaptive Weights, Exploration, Granularity.\n*   **Advice:** Go beyond simple combinations. Explore interaction effects between factors (e.g., waste & item size). Employ adaptive weight adjustments based on real-time performance feedback.\n*   **Avoid:** Static, linear factor combinations; premature convergence.\n*   **Explanation:** Adaptive learning, fine-grained factor integration, and diversification help us to design better heuristics.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}