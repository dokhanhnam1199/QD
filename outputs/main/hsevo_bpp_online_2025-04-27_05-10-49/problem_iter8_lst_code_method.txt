{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                waste_penalty_factor: float = 0.8575834075161552,\n                bin_fraction_penalty: float = 0.4693634879473551,\n                item_size_weight: float = 2.0884148461764993,\n                random_perturbation_scale: float = 0.06717097613120268) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics:\n        1. Waste Minimization: Prioritizes bins where the remaining space after\n           packing the item is minimal.  A near-perfect fit is highly valued.\n        2. Capacity Threshold: Avoids bins with extremely small remaining\n           capacity to reduce fragmentation. Bins that cannot fit are penalized.\n        3. Bin Level Awareness : Considers the initial capacity of the bins for a more balanced distribution\n        4. Balancing Bin Utilization:  Slight preference for bins that are not\n           completely empty or completely full to maintain flexibility.\n        5. Random Perturbation: Introduces small randomness to avoid getting stuck\n           in local optima and explore slightly different packing arrangements.\n        6. Item size awareness : Larger Items should fill bins as much as possible\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        waste_penalty_factor: Factor to adjust the impact of waste minimization.\n        bin_fraction_penalty: Factor to adjust the balancing of bin utilization.\n        item_size_weight: Weight of item size awareness.\n        random_perturbation_scale: Scale of random noise.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n\n    # Near-perfect fit bonus (minimize waste)\n    priorities += waste_penalty_factor * np.exp(-waste)  # Exponential decay for increasing waste.\n\n    # Bin utilization balancing - slightly prefers bins that aren't empty or full.  avoids extremities.  Parabolic preference\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += bin_fraction_penalty * -(bin_fraction - 0.5)**2  # Adds a parabolic preference curve.\n    \n    # Item Size Awareness.  Larger items fill bins up\n    priorities += item_size_weight * item/original_capacity\n\n    # Random Perturbation (introduces some \"quantum\" fluctuation). Very small value for numerical stability\n    priorities += np.random.normal(0, random_perturbation_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                waste_penalty_factor: float = 0.8575834075161552,\n                bin_fraction_penalty: float = 0.4693634879473551,\n                item_size_weight: float = 2.0884148461764993,\n                random_perturbation_scale: float = 0.06717097613120268) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics:\n        1. Waste Minimization: Prioritizes bins where the remaining space after\n           packing the item is minimal.  A near-perfect fit is highly valued.\n        2. Capacity Threshold: Avoids bins with extremely small remaining\n           capacity to reduce fragmentation. Bins that cannot fit are penalized.\n        3. Bin Level Awareness : Considers the initial capacity of the bins for a more balanced distribution\n        4. Balancing Bin Utilization:  Slight preference for bins that are not\n           completely empty or completely full to maintain flexibility.\n        5. Random Perturbation: Introduces small randomness to avoid getting stuck\n           in local optima and explore slightly different packing arrangements.\n        6. Item size awareness : Larger Items should fill bins as much as possible\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        waste_penalty_factor: Factor to adjust the impact of waste minimization.\n        bin_fraction_penalty: Factor to adjust the balancing of bin utilization.\n        item_size_weight: Weight of item size awareness.\n        random_perturbation_scale: Scale of random noise.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n\n    # Near-perfect fit bonus (minimize waste)\n    priorities += waste_penalty_factor * np.exp(-waste)  # Exponential decay for increasing waste.\n\n    # Bin utilization balancing - slightly prefers bins that aren't empty or full.  avoids extremities.  Parabolic preference\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += bin_fraction_penalty * -(bin_fraction - 0.5)**2  # Adds a parabolic preference curve.\n    \n    # Item Size Awareness.  Larger items fill bins up\n    priorities += item_size_weight * item/original_capacity\n\n    # Random Perturbation (introduces some \"quantum\" fluctuation). Very small value for numerical stability\n    priorities += np.random.normal(0, random_perturbation_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                waste_weight: float = 0.9157986374887577,\n                utilization_weight: float = 0.3499427264725965,\n                item_weight: float = 0.6481512628315893,\n                gravity_weight: float = 0.7039409664757693,\n                random_perturbation_scale: float = 0.08554357502333183,\n                bin_fraction_midpoint: float = 0.9354784706519869) -> np.ndarray:\n    \"\"\"Combines waste minimization, bin utilization, item size, and randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization & Infeasibility\n    waste = bins_remain_cap - item\n    infeasible_mask = waste < 0\n    priorities[infeasible_mask] = -np.inf\n    waste[infeasible_mask] = np.inf\n\n    # Waste Minimization Score\n    waste_score = np.exp(-waste)\n\n    # Bin Utilization Score\n    bin_fraction = bins_remain_cap / original_capacity\n    utilization_score = -(bin_fraction - bin_fraction_midpoint)**2\n\n    # Item Size Awareness Score\n    item_size_score = item / original_capacity\n\n    # Gravitational component from the second heuristic, but adapted\n    fit_mask = bins_remain_cap >= item\n    fill_ratios = np.where(fit_mask, item / bins_remain_cap, 0)  # Avoid division by zero\n    gravity_score = np.where(fit_mask, (1 - np.abs(1 - fill_ratios)), 0)  # Only add for feasible bins\n\n    # Combine scores with potentially adaptive weights (using placeholder values for now, consider later adaptations).\n\n    priorities += waste_weight * waste_score + utilization_weight * utilization_score + item_weight * item_size_score + gravity_weight * gravity_score\n\n    # Small random perturbation\n    priorities += np.random.normal(0, random_perturbation_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines waste minimization, capacity threshold, utilization,\n    item size, and adaptive randomness for bin priority.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Infeasible bin handling\n    infeasible = bins_remain_cap < item\n    priorities[infeasible] = -np.inf\n\n    # Waste minimization & near-perfect fit\n    waste = bins_remain_cap - item\n    waste[infeasible] = np.inf\n    priorities += np.exp(-waste)\n\n    # Bin utilization balancing\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += -(bin_fraction - 0.5)**2\n\n    # Item Size Awareness\n    priorities += item/original_capacity\n\n    # Adaptive Random Perturbation\n    # Introduce more randomness when bins are similarly full/empty.\n    capacity_std = np.std(bins_remain_cap)\n    randomness_scale = 0.01 * (1 + np.tanh(capacity_std))  # adaptive scale\n    priorities += np.random.normal(0, randomness_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines waste minimization, capacity threshold, utilization,\n    item size, and adaptive randomness for bin priority.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Infeasible bin handling\n    infeasible = bins_remain_cap < item\n    priorities[infeasible] = -np.inf\n\n    # Waste minimization & near-perfect fit\n    waste = bins_remain_cap - item\n    waste[infeasible] = np.inf\n    priorities += np.exp(-waste)\n\n    # Bin utilization balancing\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += -(bin_fraction - 0.5)**2\n\n    # Item Size Awareness\n    priorities += item/original_capacity\n\n    # Adaptive Random Perturbation\n    # Introduce more randomness when bins are similarly full/empty.\n    capacity_std = np.std(bins_remain_cap)\n    randomness_scale = 0.01 * (1 + np.tanh(capacity_std))  # adaptive scale\n    priorities += np.random.normal(0, randomness_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, bin utilization, item size, randomness, & capacity-based velocity.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Infeasible bin handling\n    infeasible = bins_remain_cap < item\n    priorities[infeasible] = -np.inf\n\n    # Waste Minimization with exponential scaling\n    waste = bins_remain_cap - item\n    waste[infeasible] = np.inf  # Ensure waste is infinite for infeasible bins\n    priorities += np.exp(-waste)\n\n    # Bin utilization balancing\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += -(bin_fraction - 0.5)**2\n\n    # Item size awareness\n    priorities += item / original_capacity\n\n    # Capacity-based Velocity (similar to v1 but scaled)\n    velocity = np.exp(-np.abs(bins_remain_cap - item) / original_capacity)\n    priorities += velocity * (1 - bin_fraction)\n\n    # Random Perturbation\n    priorities += np.random.normal(0, 0.01, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, target utilization, and item snugness with adaptive scaling.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    wasted_space = bins_remain_cap - item\n    fit_mask = wasted_space >= 0\n\n    if np.any(fit_mask):\n        # Waste Minimization with adaptive scaling\n        scaling_factor = 1.5  # Adjust for different item size distributions\n        priorities[fit_mask] = np.exp(-scaling_factor * wasted_space[fit_mask]**2 / bins_remain_cap[fit_mask].mean())\n\n        # Target Utilization (balance remaining space)\n        desirable_remaining_space = 0.2 * np.max(bins_remain_cap)  # Can be tuned adaptively\n        utilization_priority = np.exp(-((wasted_space[fit_mask] - desirable_remaining_space) ** 2) / (2 * (desirable_remaining_space/2)** 2))\n        priorities[fit_mask] = 0.6*priorities[fit_mask]+ 0.4*utilization_priority # Adjust weights dynamically if needed\n\n        # Snugness (item size relative to bin capacity), only if there is available space.\n        snugness = item / bins_remain_cap[fit_mask]\n        priorities[fit_mask] = 0.7*priorities[fit_mask] + 0.3*snugness\n\n    priorities[~fit_mask] = -1e9 # Penalize infeasible bins\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines utilization, near-fit, and adaptive capacity considerations.\"\"\"\n    epsilon = 1e-9\n    available_space = bins_remain_cap + epsilon\n\n    utilization = item / available_space\n    waste = available_space - item\n    near_perfect_fit = np.exp(-np.abs(waste) / item) # Scale by item size\n    relative_capacity = bins_remain_cap / np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else np.zeros_like(bins_remain_cap)\n\n\n    feasible_bins = bins_remain_cap >= item\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n\n    priority[feasible_bins] = (\n        1.0 * utilization[feasible_bins]\n        + 1.0 * near_perfect_fit[feasible_bins]\n        + 0.5 * relative_capacity[feasible_bins] # Adaptive capacity\n    )\n    return priority\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                waste_penalty_factor: float = 0.8575834075161552,\n                bin_fraction_penalty: float = 0.4693634879473551,\n                item_size_weight: float = 2.0884148461764993,\n                random_perturbation_scale: float = 0.06717097613120268,\n                large_item_threshold: float = 0.7,\n                waste_threshold: float = 0.1) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics with adaptive considerations:\n        1. Waste Minimization: Prioritizes bins where the remaining space after\n           packing the item is minimal.  A near-perfect fit is highly valued.\n        2. Capacity Threshold: Avoids bins with extremely small remaining\n           capacity to reduce fragmentation. Bins that cannot fit are penalized.\n        3. Bin Level Awareness : Considers the initial capacity of the bins for a more balanced distribution\n        4. Balancing Bin Utilization:  Slight preference for bins that are not\n           completely empty or completely full to maintain flexibility.\n        5. Random Perturbation: Introduces small randomness to avoid getting stuck\n           in local optima and explore slightly different packing arrangements.\n        6. Item size awareness : Larger Items should fill bins as much as possible\n        7. Adaptive Waste Penalty: Increase waste penalty for large items to prefer tighter fits.\n        8. Waste Thresholding: Strongly penalize bins that will have very little remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        waste_penalty_factor: Factor to adjust the impact of waste minimization.\n        bin_fraction_penalty: Factor to adjust the balancing of bin utilization.\n        item_size_weight: Weight of item size awareness.\n        random_perturbation_scale: Scale of random noise.\n        large_item_threshold: Threshold to consider an item as large, relative to bin capacity.\n        waste_threshold: The minimum waste tolerance.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n    \n    # Adaptive Waste Penalty.  If item is large, prefer very tight fits.\n    if item / original_capacity > large_item_threshold:\n        adaptive_waste_penalty = waste_penalty_factor * 2  # Increase penalty for large items\n    else:\n        adaptive_waste_penalty = waste_penalty_factor\n    \n    # Near-perfect fit bonus (minimize waste)\n    priorities += adaptive_waste_penalty * np.exp(-waste)  # Exponential decay for increasing waste.\n    \n    # Waste Threshold penalty\n    too_much_waste = waste > original_capacity * waste_threshold\n    priorities[too_much_waste] -= 0.5 * waste[too_much_waste]\n    \n\n    # Bin utilization balancing - slightly prefers bins that aren't empty or full.  avoids extremities.  Parabolic preference\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += bin_fraction_penalty * -(bin_fraction - 0.5)**2  # Adds a parabolic preference curve.\n    \n    # Item Size Awareness.  Larger items fill bins up\n    priorities += item_size_weight * item/original_capacity\n\n    # Random Perturbation (introduces some \"quantum\" fluctuation). Very small value for numerical stability\n    priorities += np.random.normal(0, random_perturbation_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                waste_penalty_factor: float = 0.8575834075161552,\n                bin_fraction_penalty: float = 0.4693634879473551,\n                item_size_weight: float = 2.0884148461764993,\n                random_perturbation_scale: float = 0.06717097613120268,\n                large_item_threshold: float = 0.7,\n                waste_threshold: float = 0.1) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics with adaptive considerations:\n        1. Waste Minimization: Prioritizes bins where the remaining space after\n           packing the item is minimal.  A near-perfect fit is highly valued.\n        2. Capacity Threshold: Avoids bins with extremely small remaining\n           capacity to reduce fragmentation. Bins that cannot fit are penalized.\n        3. Bin Level Awareness : Considers the initial capacity of the bins for a more balanced distribution\n        4. Balancing Bin Utilization:  Slight preference for bins that are not\n           completely empty or completely full to maintain flexibility.\n        5. Random Perturbation: Introduces small randomness to avoid getting stuck\n           in local optima and explore slightly different packing arrangements.\n        6. Item size awareness : Larger Items should fill bins as much as possible\n        7. Adaptive Waste Penalty: Increase waste penalty for large items to prefer tighter fits.\n        8. Waste Thresholding: Strongly penalize bins that will have very little remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        waste_penalty_factor: Factor to adjust the impact of waste minimization.\n        bin_fraction_penalty: Factor to adjust the balancing of bin utilization.\n        item_size_weight: Weight of item size awareness.\n        random_perturbation_scale: Scale of random noise.\n        large_item_threshold: Threshold to consider an item as large, relative to bin capacity.\n        waste_threshold: The minimum waste tolerance.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n    \n    # Adaptive Waste Penalty.  If item is large, prefer very tight fits.\n    if item / original_capacity > large_item_threshold:\n        adaptive_waste_penalty = waste_penalty_factor * 2  # Increase penalty for large items\n    else:\n        adaptive_waste_penalty = waste_penalty_factor\n    \n    # Near-perfect fit bonus (minimize waste)\n    priorities += adaptive_waste_penalty * np.exp(-waste)  # Exponential decay for increasing waste.\n    \n    # Waste Threshold penalty\n    too_much_waste = waste > original_capacity * waste_threshold\n    priorities[too_much_waste] -= 0.5 * waste[too_much_waste]\n    \n\n    # Bin utilization balancing - slightly prefers bins that aren't empty or full.  avoids extremities.  Parabolic preference\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += bin_fraction_penalty * -(bin_fraction - 0.5)**2  # Adds a parabolic preference curve.\n    \n    # Item Size Awareness.  Larger items fill bins up\n    priorities += item_size_weight * item/original_capacity\n\n    # Random Perturbation (introduces some \"quantum\" fluctuation). Very small value for numerical stability\n    priorities += np.random.normal(0, random_perturbation_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                waste_penalty_factor: float = 0.8575834075161552,\n                bin_fraction_penalty: float = 0.4693634879473551,\n                item_size_weight: float = 2.0884148461764993,\n                random_perturbation_scale: float = 0.06717097613120268,\n                large_item_threshold: float = 0.7,\n                waste_threshold: float = 0.1) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics with adaptive considerations:\n        1. Waste Minimization: Prioritizes bins where the remaining space after\n           packing the item is minimal.  A near-perfect fit is highly valued.\n        2. Capacity Threshold: Avoids bins with extremely small remaining\n           capacity to reduce fragmentation. Bins that cannot fit are penalized.\n        3. Bin Level Awareness : Considers the initial capacity of the bins for a more balanced distribution\n        4. Balancing Bin Utilization:  Slight preference for bins that are not\n           completely empty or completely full to maintain flexibility.\n        5. Random Perturbation: Introduces small randomness to avoid getting stuck\n           in local optima and explore slightly different packing arrangements.\n        6. Item size awareness : Larger Items should fill bins as much as possible\n        7. Adaptive Waste Penalty: Increase waste penalty for large items to prefer tighter fits.\n        8. Waste Thresholding: Strongly penalize bins that will have very little remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        waste_penalty_factor: Factor to adjust the impact of waste minimization.\n        bin_fraction_penalty: Factor to adjust the balancing of bin utilization.\n        item_size_weight: Weight of item size awareness.\n        random_perturbation_scale: Scale of random noise.\n        large_item_threshold: Threshold to consider an item as large, relative to bin capacity.\n        waste_threshold: The minimum waste tolerance.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n    \n    # Adaptive Waste Penalty.  If item is large, prefer very tight fits.\n    if item / original_capacity > large_item_threshold:\n        adaptive_waste_penalty = waste_penalty_factor * 2  # Increase penalty for large items\n    else:\n        adaptive_waste_penalty = waste_penalty_factor\n    \n    # Near-perfect fit bonus (minimize waste)\n    priorities += adaptive_waste_penalty * np.exp(-waste)  # Exponential decay for increasing waste.\n    \n    # Waste Threshold penalty\n    too_much_waste = waste > original_capacity * waste_threshold\n    priorities[too_much_waste] -= 0.5 * waste[too_much_waste]\n    \n\n    # Bin utilization balancing - slightly prefers bins that aren't empty or full.  avoids extremities.  Parabolic preference\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += bin_fraction_penalty * -(bin_fraction - 0.5)**2  # Adds a parabolic preference curve.\n    \n    # Item Size Awareness.  Larger items fill bins up\n    priorities += item_size_weight * item/original_capacity\n\n    # Random Perturbation (introduces some \"quantum\" fluctuation). Very small value for numerical stability\n    priorities += np.random.normal(0, random_perturbation_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, utilization balancing, item size, and adaptive randomness.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    avg_cap = np.mean(bins_remain_cap)\n\n    # Waste Minimization with Sigmoid\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf\n    waste[too_small] = np.inf\n    sigmoid_scale = 5\n    priorities += 1 / (1 + np.exp(sigmoid_scale * waste))\n\n    # Bin Utilization Balancing with Adaptive Center\n    bin_fraction = bins_remain_cap / np.max(bins_remain_cap)\n    utilization_center = 0.5 + 0.2 * np.clip(item / avg_cap, 0, 1)\n    priorities += -((bin_fraction - utilization_center) ** 2)\n\n    # Item Size Consideration\n    priorities += item / avg_cap\n\n    # Adaptive Random Perturbation\n    noise_scale = 0.01 + 0.05 * np.clip(item / avg_cap, 0, 1)\n    priorities += np.random.normal(0, noise_scale, size=bins_remain_cap.shape)\n    \n    # Fragmentation Avoidance\n    remaining_fraction = waste / np.max(bins_remain_cap)\n    too_fragmented = (waste > 0) & (remaining_fraction < 0.1)\n    priorities[too_fragmented] -= 0.5\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, utilization balancing, item size, and adaptive randomness.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    avg_cap = np.mean(bins_remain_cap)\n\n    # Waste Minimization with Sigmoid\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf\n    waste[too_small] = np.inf\n    sigmoid_scale = 5\n    priorities += 1 / (1 + np.exp(sigmoid_scale * waste))\n\n    # Bin Utilization Balancing with Adaptive Center\n    bin_fraction = bins_remain_cap / np.max(bins_remain_cap)\n    utilization_center = 0.5 + 0.2 * np.clip(item / avg_cap, 0, 1)\n    priorities += -((bin_fraction - utilization_center) ** 2)\n\n    # Item Size Consideration\n    priorities += item / avg_cap\n\n    # Adaptive Random Perturbation\n    noise_scale = 0.01 + 0.05 * np.clip(item / avg_cap, 0, 1)\n    priorities += np.random.normal(0, noise_scale, size=bins_remain_cap.shape)\n    \n    # Fragmentation Avoidance\n    remaining_fraction = waste / np.max(bins_remain_cap)\n    too_fragmented = (waste > 0) & (remaining_fraction < 0.1)\n    priorities[too_fragmented] -= 0.5\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                waste_penalty_factor: float = 0.8575834075161552,\n                bin_fraction_penalty: float = 0.4693634879473551,\n                item_size_weight: float = 2.0884148461764993,\n                random_perturbation_scale: float = 0.06717097613120268,\n                large_item_threshold: float = 0.7,\n                almost_full_threshold: float = 0.1) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics with adaptive elements:\n        1. Waste Minimization: Prioritizes bins where the remaining space after\n           packing the item is minimal.  A near-perfect fit is highly valued.\n        2. Capacity Threshold: Avoids bins with extremely small remaining\n           capacity to reduce fragmentation. Bins that cannot fit are penalized.\n        3. Bin Level Awareness : Considers the initial capacity of the bins for a more balanced distribution\n        4. Balancing Bin Utilization:  Slight preference for bins that are not\n           completely empty or completely full to maintain flexibility.\n        5. Random Perturbation: Introduces small randomness to avoid getting stuck\n           in local optima and explore slightly different packing arrangements.\n        6. Item size awareness : Larger Items should fill bins as much as possible\n        7. Adaptive Waste Penalty: Adjusts waste penalty based on item size.\n           Larger items penalize waste more aggressively.\n        8. Almost Full Bin Preference: If an item can almost fill a bin, prioritize it to reduce fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        waste_penalty_factor: Factor to adjust the impact of waste minimization.\n        bin_fraction_penalty: Factor to adjust the balancing of bin utilization.\n        item_size_weight: Weight of item size awareness.\n        random_perturbation_scale: Scale of random noise.\n        large_item_threshold: Threshold for considering an item as large (relative to bin capacity).\n        almost_full_threshold: Threshold for considering a bin as almost full after adding the item.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n\n    # Adaptive Waste Penalty (Larger Items)\n    adaptive_waste_penalty = waste_penalty_factor\n    if item > large_item_threshold * original_capacity:\n        adaptive_waste_penalty *= 2  # Increase penalty for waste with larger items\n\n    # Near-perfect fit bonus (minimize waste)\n    priorities += adaptive_waste_penalty * np.exp(-waste)  # Exponential decay for increasing waste.\n\n    # Bin utilization balancing - slightly prefers bins that aren't empty or full.  avoids extremities.  Parabolic preference\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += bin_fraction_penalty * -(bin_fraction - 0.5)**2  # Adds a parabolic preference curve.\n    \n    # Item Size Awareness.  Larger items fill bins up\n    priorities += item_size_weight * item/original_capacity\n    \n    # Almost Full Bin Preference\n    almost_full = (waste > 0) & (waste/original_capacity < almost_full_threshold)\n    priorities[almost_full] += 1.0  # Slightly boost priority for bins that will be almost full\n\n    # Random Perturbation (introduces some \"quantum\" fluctuation). Very small value for numerical stability\n    priorities += np.random.normal(0, random_perturbation_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                waste_penalty_factor: float = 0.8575834075161552,\n                bin_fraction_penalty: float = 0.4693634879473551,\n                item_size_weight: float = 2.0884148461764993,\n                random_perturbation_scale: float = 0.06717097613120268,\n                large_item_threshold: float = 0.7,\n                almost_full_threshold: float = 0.1) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics with adaptive elements:\n        1. Waste Minimization: Prioritizes bins where the remaining space after\n           packing the item is minimal.  A near-perfect fit is highly valued.\n        2. Capacity Threshold: Avoids bins with extremely small remaining\n           capacity to reduce fragmentation. Bins that cannot fit are penalized.\n        3. Bin Level Awareness : Considers the initial capacity of the bins for a more balanced distribution\n        4. Balancing Bin Utilization:  Slight preference for bins that are not\n           completely empty or completely full to maintain flexibility.\n        5. Random Perturbation: Introduces small randomness to avoid getting stuck\n           in local optima and explore slightly different packing arrangements.\n        6. Item size awareness : Larger Items should fill bins as much as possible\n        7. Adaptive Waste Penalty: Adjusts waste penalty based on item size.\n           Larger items penalize waste more aggressively.\n        8. Almost Full Bin Preference: If an item can almost fill a bin, prioritize it to reduce fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        waste_penalty_factor: Factor to adjust the impact of waste minimization.\n        bin_fraction_penalty: Factor to adjust the balancing of bin utilization.\n        item_size_weight: Weight of item size awareness.\n        random_perturbation_scale: Scale of random noise.\n        large_item_threshold: Threshold for considering an item as large (relative to bin capacity).\n        almost_full_threshold: Threshold for considering a bin as almost full after adding the item.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n\n    # Adaptive Waste Penalty (Larger Items)\n    adaptive_waste_penalty = waste_penalty_factor\n    if item > large_item_threshold * original_capacity:\n        adaptive_waste_penalty *= 2  # Increase penalty for waste with larger items\n\n    # Near-perfect fit bonus (minimize waste)\n    priorities += adaptive_waste_penalty * np.exp(-waste)  # Exponential decay for increasing waste.\n\n    # Bin utilization balancing - slightly prefers bins that aren't empty or full.  avoids extremities.  Parabolic preference\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += bin_fraction_penalty * -(bin_fraction - 0.5)**2  # Adds a parabolic preference curve.\n    \n    # Item Size Awareness.  Larger items fill bins up\n    priorities += item_size_weight * item/original_capacity\n    \n    # Almost Full Bin Preference\n    almost_full = (waste > 0) & (waste/original_capacity < almost_full_threshold)\n    priorities[almost_full] += 1.0  # Slightly boost priority for bins that will be almost full\n\n    # Random Perturbation (introduces some \"quantum\" fluctuation). Very small value for numerical stability\n    priorities += np.random.normal(0, random_perturbation_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                waste_penalty_factor: float = 0.8575834075161552,\n                bin_fraction_penalty: float = 0.4693634879473551,\n                item_size_weight: float = 2.0884148461764993,\n                random_perturbation_scale: float = 0.06717097613120268,\n                fit_threshold: float = 0.9,\n                large_item_threshold: float = 0.7) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics with adaptive components and interaction effects:\n        1. Waste Minimization: Prioritizes bins where the remaining space after\n           packing the item is minimal.  A near-perfect fit is highly valued.\n        2. Capacity Threshold: Avoids bins with extremely small remaining\n           capacity to reduce fragmentation. Bins that cannot fit are penalized.\n        3. Bin Level Awareness : Considers the initial capacity of the bins for a more balanced distribution\n        4. Balancing Bin Utilization:  Slight preference for bins that are not\n           completely empty or completely full to maintain flexibility.\n        5. Random Perturbation: Introduces small randomness to avoid getting stuck\n           in local optima and explore slightly different packing arrangements.\n        6. Item size awareness : Larger Items should fill bins as much as possible\n        7. Adaptive Waste Penalty: Adjusts waste penalty based on item size.\n        8. Fit Threshold Bonus:  If an item fills a bin above a certain threshold, give an extra bonus.\n        9. Large Item Strategy: For large items, prioritize bins that will be filled above large_item_threshold to encourage complete fills.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        waste_penalty_factor: Factor to adjust the impact of waste minimization.\n        bin_fraction_penalty: Factor to adjust the balancing of bin utilization.\n        item_size_weight: Weight of item size awareness.\n        random_perturbation_scale: Scale of random noise.\n        fit_threshold: Threshold (as fraction of bin capacity) above which a \"near fit\" bonus is applied\n        large_item_threshold: Threshold to consider an item \"large\" (as a fraction of bin capacity).\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n\n    # Near-perfect fit bonus (minimize waste)\n    adaptive_waste_penalty = waste_penalty_factor * (1 - min(item / original_capacity, 0.5)) # Reduce waste penalty for larger items\n    priorities += adaptive_waste_penalty * np.exp(-waste)  # Exponential decay for increasing waste.\n\n    # Bin utilization balancing - slightly prefers bins that aren't empty or full.  avoids extremities.  Parabolic preference\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += bin_fraction_penalty * -(bin_fraction - 0.5)**2  # Adds a parabolic preference curve.\n    \n    # Item Size Awareness.  Larger items fill bins up\n    priorities += item_size_weight * item/original_capacity\n\n    # Fit Threshold Bonus\n    remaining_fraction = waste / original_capacity\n    near_fit = (1 - remaining_fraction) >= fit_threshold\n    priorities[near_fit] += 2 * waste_penalty_factor # Substantial bonus for near-perfect fit\n\n    # Large Item Strategy\n    if item / original_capacity > large_item_threshold:\n      fill_percentage = (bins_remain_cap - waste) / original_capacity\n      large_item_bins = fill_percentage >= large_item_threshold\n      priorities[large_item_bins] += 3 * item_size_weight # Heavily prioritize near-full bins for large items\n\n    # Random Perturbation (introduces some \"quantum\" fluctuation). Very small value for numerical stability\n    priorities += np.random.normal(0, random_perturbation_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, gravity, and adaptive perturbation.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    waste = bins_remain_cap - item\n    fits = waste >= 0\n\n    if np.any(fits):\n        # Prioritize fitting bins, smaller waste is better.\n        priorities[fits] = np.exp(-waste[fits])\n\n        #Encourage filling bins closer to full by gravity\n        gravity = (item / bins_remain_cap[fits]) ** 2\n        priorities[fits]+= gravity\n        \n        #Adaptive perturbation based on remaining capacity.\n        perturbation_level = 0.01 * np.mean(bins_remain_cap)\n        priorities[fits] += np.random.normal(0, perturbation_level, size=bins_remain_cap[fits].shape)\n\n    else:\n        # If no bin fits, prioritize based on remaining capacity and item size.\n        priorities = (bins_remain_cap / np.sum(bins_remain_cap + 1e-9)) * (item / np.max(bins_remain_cap + 1e-9))\n\n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, target utilization, and adaptive scaling.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    wasted_space = bins_remain_cap - item\n    fit_mask = wasted_space >= 0\n\n    if np.any(fit_mask):\n        # Adaptive scaling based on item size and remaining capacity distribution.\n        item_fraction = item / np.max(bins_remain_cap)\n        scaling_factor = 1.0 + 2.0 * item_fraction # Increase scaling if item is a significant fraction of bin size\n        priorities[fit_mask] = np.exp(-scaling_factor * wasted_space[fit_mask]**2 / (bins_remain_cap[fit_mask].mean() + 1e-9))\n\n        desirable_remaining_space = 0.2 * np.max(bins_remain_cap)\n        priority_values = np.exp(-((wasted_space[fit_mask] ) ** 2) / (2 * (desirable_remaining_space/2)** 2)) # Target utilization\n        priorities[fit_mask] = 0.6*priorities[fit_mask]+ 0.4*priority_values #Combine with weight\n        \n        # Introduce a small bias to prefer bins with larger remaining capacity if waste is similar. Mitigates fragmentation.\n        capacity_bias = 0.1 * bins_remain_cap[fit_mask] / np.max(bins_remain_cap)\n        priorities[fit_mask] += capacity_bias\n\n        # Small random perturbation for exploration\n        priorities[fit_mask] += np.random.normal(0, 0.01, size=np.sum(fit_mask))\n\n    priorities[~fit_mask] = -1e9 # Penalize infeasible bins severely\n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, target utilization, and adaptive scaling.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    wasted_space = bins_remain_cap - item\n    fit_mask = wasted_space >= 0\n\n    if np.any(fit_mask):\n        # Adaptive scaling based on item size and remaining capacity distribution.\n        item_fraction = item / np.max(bins_remain_cap)\n        scaling_factor = 1.0 + 2.0 * item_fraction # Increase scaling if item is a significant fraction of bin size\n        priorities[fit_mask] = np.exp(-scaling_factor * wasted_space[fit_mask]**2 / (bins_remain_cap[fit_mask].mean() + 1e-9))\n\n        desirable_remaining_space = 0.2 * np.max(bins_remain_cap)\n        priority_values = np.exp(-((wasted_space[fit_mask] ) ** 2) / (2 * (desirable_remaining_space/2)** 2)) # Target utilization\n        priorities[fit_mask] = 0.6*priorities[fit_mask]+ 0.4*priority_values #Combine with weight\n        \n        # Introduce a small bias to prefer bins with larger remaining capacity if waste is similar. Mitigates fragmentation.\n        capacity_bias = 0.1 * bins_remain_cap[fit_mask] / np.max(bins_remain_cap)\n        priorities[fit_mask] += capacity_bias\n\n        # Small random perturbation for exploration\n        priorities[fit_mask] += np.random.normal(0, 0.01, size=np.sum(fit_mask))\n\n    priorities[~fit_mask] = -1e9 # Penalize infeasible bins severely\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                waste_penalty_factor: float = 0.8575834075161552,\n                bin_fraction_penalty: float = 0.4693634879473551,\n                item_size_weight: float = 2.0884148461764993,\n                random_perturbation_scale: float = 0.06717097613120268,\n                capacity_usage_factor: float = 1.5,\n                large_item_threshold: float = 0.7,\n                waste_threshold: float = 0.1) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics with adaptive and interaction considerations:\n\n        1.  Waste Minimization: Prioritizes bins where the remaining space after\n            packing the item is minimal.  A near-perfect fit is highly valued.\n        2.  Capacity Threshold: Avoids bins with extremely small remaining\n            capacity to reduce fragmentation. Bins that cannot fit are penalized.\n        3.  Bin Level Awareness : Considers the initial capacity of the bins for a\n            more balanced distribution\n        4.  Balancing Bin Utilization:  Slight preference for bins that are not\n            completely empty or completely full to maintain flexibility.\n        5.  Random Perturbation: Introduces small randomness to avoid getting stuck\n            in local optima and explore slightly different packing arrangements.\n        6.  Item size awareness : Larger Items should fill bins as much as possible\n        7. Capacity Usage: More emphasis on using bins with higher capacity\n        8. Interaction effects between waste and item size.\n        9. Adaptive Scaling based on Item Size: Dynamically scales the influence of\n            different factors based on whether the item is considered \"large\" or not.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        waste_penalty_factor: Factor to adjust the impact of waste minimization.\n        bin_fraction_penalty: Factor to adjust the balancing of bin utilization.\n        item_size_weight: Weight of item size awareness.\n        random_perturbation_scale: Scale of random noise.\n        capacity_usage_factor: weight to original capacity.\n        large_item_threshold: threshold to determine if item is considered large.\n        waste_threshold: threshold to determine if waste is significant.\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n\n    # Near-perfect fit bonus (minimize waste)\n    priorities += waste_penalty_factor * np.exp(-waste)  # Exponential decay for increasing waste.\n\n    # Bin utilization balancing - slightly prefers bins that aren't empty or full.  avoids extremities.  Parabolic preference\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += bin_fraction_penalty * -(bin_fraction - 0.5)**2  # Adds a parabolic preference curve.\n\n    # Item Size Awareness.  Larger items fill bins up\n    priorities += item_size_weight * item / original_capacity\n    \n    # Capacity Usage: Preferentially use bins with higher remaining capacity\n    priorities += capacity_usage_factor * bins_remain_cap/original_capacity\n\n    # Interaction effects: If waste is small and item is large, prioritize more\n    is_large_item = item > large_item_threshold * original_capacity\n    is_small_waste = waste < waste_threshold * original_capacity\n    \n    if is_large_item:\n      priorities += 2*waste_penalty_factor * np.exp(-waste)\n    \n    # Adaptive Scaling: Adjust weights based on item size\n    if is_large_item:\n        # For large items, emphasize packing efficiency and capacity usage\n        waste_penalty_factor *= 1.2  # Increase importance of minimizing waste\n        capacity_usage_factor *= 1.1  # Slight increase in using available space\n        bin_fraction_penalty *= 0.8  # Reduce importance of bin balancing\n    else:\n        # For smaller items, emphasize bin balancing and waste\n        bin_fraction_penalty *= 1.2\n        waste_penalty_factor *= 0.9\n        capacity_usage_factor *= 0.9\n\n    # Random Perturbation (introduces some \"quantum\" fluctuation). Very small value for numerical stability\n    priorities += np.random.normal(0, random_perturbation_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}