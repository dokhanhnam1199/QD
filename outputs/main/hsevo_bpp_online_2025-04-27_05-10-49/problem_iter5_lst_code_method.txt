{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                waste_penalty_factor: float = 0.8575834075161552,\n                bin_fraction_penalty: float = 0.4693634879473551,\n                item_size_weight: float = 2.0884148461764993,\n                random_perturbation_scale: float = 0.06717097613120268) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics:\n        1. Waste Minimization: Prioritizes bins where the remaining space after\n           packing the item is minimal.  A near-perfect fit is highly valued.\n        2. Capacity Threshold: Avoids bins with extremely small remaining\n           capacity to reduce fragmentation. Bins that cannot fit are penalized.\n        3. Bin Level Awareness : Considers the initial capacity of the bins for a more balanced distribution\n        4. Balancing Bin Utilization:  Slight preference for bins that are not\n           completely empty or completely full to maintain flexibility.\n        5. Random Perturbation: Introduces small randomness to avoid getting stuck\n           in local optima and explore slightly different packing arrangements.\n        6. Item size awareness : Larger Items should fill bins as much as possible\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        waste_penalty_factor: Factor to adjust the impact of waste minimization.\n        bin_fraction_penalty: Factor to adjust the balancing of bin utilization.\n        item_size_weight: Weight of item size awareness.\n        random_perturbation_scale: Scale of random noise.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n\n    # Near-perfect fit bonus (minimize waste)\n    priorities += waste_penalty_factor * np.exp(-waste)  # Exponential decay for increasing waste.\n\n    # Bin utilization balancing - slightly prefers bins that aren't empty or full.  avoids extremities.  Parabolic preference\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += bin_fraction_penalty * -(bin_fraction - 0.5)**2  # Adds a parabolic preference curve.\n    \n    # Item Size Awareness.  Larger items fill bins up\n    priorities += item_size_weight * item/original_capacity\n\n    # Random Perturbation (introduces some \"quantum\" fluctuation). Very small value for numerical stability\n    priorities += np.random.normal(0, random_perturbation_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                waste_penalty_factor: float = 0.8575834075161552,\n                bin_fraction_penalty: float = 0.4693634879473551,\n                item_size_weight: float = 2.0884148461764993,\n                random_perturbation_scale: float = 0.06717097613120268) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics:\n        1. Waste Minimization: Prioritizes bins where the remaining space after\n           packing the item is minimal.  A near-perfect fit is highly valued.\n        2. Capacity Threshold: Avoids bins with extremely small remaining\n           capacity to reduce fragmentation. Bins that cannot fit are penalized.\n        3. Bin Level Awareness : Considers the initial capacity of the bins for a more balanced distribution\n        4. Balancing Bin Utilization:  Slight preference for bins that are not\n           completely empty or completely full to maintain flexibility.\n        5. Random Perturbation: Introduces small randomness to avoid getting stuck\n           in local optima and explore slightly different packing arrangements.\n        6. Item size awareness : Larger Items should fill bins as much as possible\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        waste_penalty_factor: Factor to adjust the impact of waste minimization.\n        bin_fraction_penalty: Factor to adjust the balancing of bin utilization.\n        item_size_weight: Weight of item size awareness.\n        random_perturbation_scale: Scale of random noise.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n\n    # Near-perfect fit bonus (minimize waste)\n    priorities += waste_penalty_factor * np.exp(-waste)  # Exponential decay for increasing waste.\n\n    # Bin utilization balancing - slightly prefers bins that aren't empty or full.  avoids extremities.  Parabolic preference\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += bin_fraction_penalty * -(bin_fraction - 0.5)**2  # Adds a parabolic preference curve.\n    \n    # Item Size Awareness.  Larger items fill bins up\n    priorities += item_size_weight * item/original_capacity\n\n    # Random Perturbation (introduces some \"quantum\" fluctuation). Very small value for numerical stability\n    priorities += np.random.normal(0, random_perturbation_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics:\n        1. Waste Minimization: Prioritizes bins where the remaining space after\n           packing the item is minimal.  A near-perfect fit is highly valued.\n        2. Capacity Threshold: Avoids bins with extremely small remaining\n           capacity to reduce fragmentation. Bins that cannot fit are penalized.\n        3. Bin Level Awareness : Considers the initial capacity of the bins for a more balanced distribution\n        4. Balancing Bin Utilization:  Slight preference for bins that are not\n           completely empty or completely full to maintain flexibility.\n        5. Random Perturbation: Introduces small randomness to avoid getting stuck\n           in local optima and explore slightly different packing arrangements.\n        6. Item size awareness : Larger Items should fill bins as much as possible\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n\n    # Near-perfect fit bonus (minimize waste)\n    priorities += np.exp(-waste)  # Exponential decay for increasing waste.\n\n    # Bin utilization balancing - slightly prefers bins that aren't empty or full.  avoids extremities.  Parabolic preference\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += -(bin_fraction - 0.5)**2  # Adds a parabolic preference curve.\n    \n    # Item Size Awareness.  Larger items fill bins up\n    priorities += item/original_capacity\n\n    # Random Perturbation (introduces some \"quantum\" fluctuation). Very small value for numerical stability\n    priorities += np.random.normal(0, 0.01, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics:\n        1. Waste Minimization: Prioritizes bins where the remaining space after\n           packing the item is minimal.  A near-perfect fit is highly valued.\n        2. Capacity Threshold: Avoids bins with extremely small remaining\n           capacity to reduce fragmentation. Bins that cannot fit are penalized.\n        3. Bin Level Awareness : Considers the initial capacity of the bins for a more balanced distribution\n        4. Balancing Bin Utilization:  Slight preference for bins that are not\n           completely empty or completely full to maintain flexibility.\n        5. Random Perturbation: Introduces small randomness to avoid getting stuck\n           in local optima and explore slightly different packing arrangements.\n        6. Item size awareness : Larger Items should fill bins as much as possible\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n\n    # Near-perfect fit bonus (minimize waste)\n    priorities += np.exp(-waste)  # Exponential decay for increasing waste.\n\n    # Bin utilization balancing - slightly prefers bins that aren't empty or full.  avoids extremities.  Parabolic preference\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += -(bin_fraction - 0.5)**2  # Adds a parabolic preference curve.\n    \n    # Item Size Awareness.  Larger items fill bins up\n    priorities += item/original_capacity\n\n    # Random Perturbation (introduces some \"quantum\" fluctuation). Very small value for numerical stability\n    priorities += np.random.normal(0, 0.01, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics:\n        1. Waste Minimization: Prioritizes bins where the remaining space after\n           packing the item is minimal.  A near-perfect fit is highly valued.\n        2. Capacity Threshold: Avoids bins with extremely small remaining\n           capacity to reduce fragmentation. Bins that cannot fit are penalized.\n        3. Bin Level Awareness : Considers the initial capacity of the bins for a more balanced distribution\n        4. Balancing Bin Utilization:  Slight preference for bins that are not\n           completely empty or completely full to maintain flexibility.\n        5. Random Perturbation: Introduces small randomness to avoid getting stuck\n           in local optima and explore slightly different packing arrangements.\n        6. Item size awareness : Larger Items should fill bins as much as possible\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n\n    # Near-perfect fit bonus (minimize waste)\n    priorities += np.exp(-waste)  # Exponential decay for increasing waste.\n\n    # Bin utilization balancing - slightly prefers bins that aren't empty or full.  avoids extremities.  Parabolic preference\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += -(bin_fraction - 0.5)**2  # Adds a parabolic preference curve.\n    \n    # Item Size Awareness.  Larger items fill bins up\n    priorities += item/original_capacity\n\n    # Random Perturbation (introduces some \"quantum\" fluctuation). Very small value for numerical stability\n    priorities += np.random.normal(0, 0.01, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity, waste, utilization, and gravity for bin priority.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    infeasible = bins_remain_cap < item\n    priorities[infeasible] = -np.inf\n\n    feasible = ~infeasible\n    if np.any(feasible):\n        residual_space = bins_remain_cap[feasible] - item\n        capacity_priority = -np.abs(bins_remain_cap[feasible] - item)\n        waste_priority = -residual_space\n        utilization = 1 - bins_remain_cap[feasible] / np.max(bins_remain_cap)\n        \n        capacity_priority = capacity_priority / np.max(np.abs(capacity_priority))\n        waste_priority = waste_priority / np.max(np.abs(waste_priority))\n\n        priorities[feasible] = 0.4 * capacity_priority + 0.3 * waste_priority + 0.1 * utilization  + 0.2* item * np.log1p(bins_remain_cap[feasible])  / np.max(bins_remain_cap)\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization with a target utilization, penalizing infeasible bins.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    wasted_space = bins_remain_cap - item\n    fit_mask = wasted_space >= 0\n    if np.any(fit_mask):\n        scaling_factor = 2.0\n        priorities[fit_mask] = np.exp(-scaling_factor * wasted_space[fit_mask]**2 / bins_remain_cap[fit_mask].mean())\n        desirable_remaining_space = 0.2 * np.max(bins_remain_cap)\n        priority_values = np.exp(-((wasted_space[fit_mask] ) ** 2) / (2 * (desirable_remaining_space/2)** 2))\n        priorities[fit_mask] = 0.5*priorities[fit_mask]+ 0.5*priority_values\n    priorities[~fit_mask] = -1e9\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on waste, feasibility, and capacity.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    waste = bins_remain_cap - item\n    fits = waste >= 0\n\n    if np.any(fits):\n        # Prioritize fitting bins, smaller waste is better.\n        priorities[fits] = np.exp(-waste[fits]) \n        #Encourage filling bins closer to full by gravity\n        gravity = (item / bins_remain_cap[fits]) ** 2\n        priorities[fits]+= gravity\n        \n    else:\n        # If no bin fits, prioritize based on remaining capacity.\n        priorities = bins_remain_cap / np.sum(bins_remain_cap + 1e-9)\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines utilization, waste minimization, near-perfect fit, small bin preference, and randomness.\"\"\"\n    epsilon = 1e-9\n    available_space = bins_remain_cap + epsilon\n\n    utilization = item / available_space\n    waste = available_space - item\n    normalized_waste = waste / available_space\n    near_perfect_fit = np.exp(-np.abs(waste))\n    small_bins_priority = 1.0 / (available_space + epsilon)\n    temperature = 0.1\n    random_fluctuation = np.random.normal(0, temperature, size=bins_remain_cap.shape)\n\n    # Prioritize feasible bins and scale the waste\n    feasible_bins = bins_remain_cap >= item\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n\n    priority[feasible_bins] = (\n        1.0 * utilization[feasible_bins]\n        -0.5 * normalized_waste[feasible_bins]\n        + 1.0 * near_perfect_fit[feasible_bins]\n        + 0.5 * small_bins_priority[feasible_bins]\n        + 0.1 * random_fluctuation[feasible_bins]\n    )\n\n    return priority\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines utilization, waste minimization, near-perfect fit, small bin preference, and randomness.\"\"\"\n    epsilon = 1e-9\n    available_space = bins_remain_cap + epsilon\n\n    utilization = item / available_space\n    waste = available_space - item\n    normalized_waste = waste / available_space\n    near_perfect_fit = np.exp(-np.abs(waste))\n    small_bins_priority = 1.0 / (available_space + epsilon)\n    temperature = 0.1\n    random_fluctuation = np.random.normal(0, temperature, size=bins_remain_cap.shape)\n\n    # Prioritize feasible bins and scale the waste\n    feasible_bins = bins_remain_cap >= item\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n\n    priority[feasible_bins] = (\n        1.0 * utilization[feasible_bins]\n        -0.5 * normalized_waste[feasible_bins]\n        + 1.0 * near_perfect_fit[feasible_bins]\n        + 0.5 * small_bins_priority[feasible_bins]\n        + 0.1 * random_fluctuation[feasible_bins]\n    )\n\n    return priority\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization with a target utilization, penalizing infeasible bins.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    wasted_space = bins_remain_cap - item\n    fit_mask = wasted_space >= 0\n    if np.any(fit_mask):\n        scaling_factor = 2.0\n        priorities[fit_mask] = np.exp(-scaling_factor * wasted_space[fit_mask]**2 / bins_remain_cap[fit_mask].mean())\n        desirable_remaining_space = 0.2 * np.max(bins_remain_cap)\n        priority_values = np.exp(-((wasted_space[fit_mask] ) ** 2) / (2 * (desirable_remaining_space/2)** 2))\n        priorities[fit_mask] = 0.5*priorities[fit_mask]+ 0.5*priority_values\n    priorities[~fit_mask] = -1e9\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization and bin utilization for priority.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    capacity_diff = np.abs(bins_remain_cap - item)\n    priorities = 1.0 / (1e-6 + capacity_diff)\n    infeasible_mask = item > bins_remain_cap\n    priorities[infeasible_mask] = -np.inf\n    priorities = np.nan_to_num(priorities, neginf=-np.inf)\n    \n    valid_bins = bins_remain_cap > 0\n    fit_mask = (bins_remain_cap >= item) & valid_bins\n    \n    if np.any(fit_mask):\n        fill_ratios = item / bins_remain_cap[fit_mask]\n        gravitational_constant = 1.0\n        priorities[fit_mask] += gravitational_constant * (1 - np.abs(1 - fill_ratios))\n        priorities[fit_mask] += 0.1 * bins_remain_cap[fit_mask] / bins_remain_cap[fit_mask].max()\n    else:\n        empty_mask = (bins_remain_cap == bins_remain_cap.max()) & valid_bins\n        if np.any(empty_mask):\n            priorities[empty_mask] = 0.5\n        else:\n            return np.full(bins_remain_cap.shape, -np.inf)\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics with refined strategies:\n        1.  Waste Minimization with Sigmoid Scaling: Prioritizes near-perfect\n            fits, but uses a sigmoid function to provide a more gradual and\n            context-aware scaling of priorities.  This avoids overly aggressive\n            assignment to bins with tiny waste.\n        2.  Capacity Threshold with Dynamic Adjustment:  Penalizes bins that\n            cannot fit, but dynamically adjusts the penalty based on the\n            item size relative to the average remaining bin capacity.  If the\n            item is large, the penalty is reduced to encourage filling nearly-full\n            bins and reduce fragmentation.\n        3.  Bin Utilization Balancing with Adaptive Range:  Prefers bins that\n            are neither too full nor too empty, but adaptively adjusts the\n            preferred utilization range based on the item size.  Large items\n            shift the preference towards fuller bins.\n        4.  Fragmentation Avoidance: Explicitly discourages creating bins with\n            very small remaining capacity *after* placing the item, further\n            reducing fragmentation.\n        5.  Adaptive Random Perturbation: Adds random noise, but scales the\n            magnitude of the noise based on the item size.  Larger items\n            justify a larger perturbation to explore more diverse packing\n            arrangements.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    avg_cap = np.mean(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold (Sigmoid Scaling & Dynamic Adjustment)\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    penalty_scale = np.clip(item / avg_cap, 0.1, 1.0)  # Dynamic penalty scaling\n    priorities[too_small] = -1e9 * penalty_scale  # Scaled penalty\n    waste[too_small] = np.inf\n\n    # Near-perfect fit bonus (minimize waste) with sigmoid\n    sigmoid_scale = 10  # Controls steepness of sigmoid\n    priorities += 1 / (1 + np.exp(sigmoid_scale * waste)) # Sigmoid scaling\n\n    # Bin utilization balancing (Adaptive Range)\n    bin_fraction = bins_remain_cap / np.max(bins_remain_cap)\n    utilization_center = 0.5 + 0.2 * np.clip(item / avg_cap, 0, 1) # Shifts center\n    priorities += -((bin_fraction - utilization_center) ** 2)\n\n    # Fragmentation Avoidance\n    remaining_fraction = waste / np.max(bins_remain_cap)\n    too_fragmented = (waste > 0) & (remaining_fraction < 0.1)  # Avoid tiny wastes\n    priorities[too_fragmented] -= 0.5  # Discourage fragmented bins, but less aggressive than infeasibility\n\n    # Adaptive Random Perturbation\n    noise_scale = 0.01 + 0.05 * np.clip(item / avg_cap, 0, 1)  # Scale noise\n    priorities += np.random.normal(0, noise_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of enhanced heuristics:\n        1. Waste Minimization with Sigmoid Scaling: Prioritizes bins where the\n           remaining space after packing the item is minimal, but uses a sigmoid\n           function to provide a more nuanced preference.\n        2. Capacity Threshold with Dynamic Adjustment: Avoids bins with\n           extremely small remaining capacity, but dynamically adjusts the\n           threshold based on the average item size.\n        3. Bin Utilization Balancing with Exponential Decay: Prefers bins that\n           are not completely empty or completely full, with an exponential\n           decay to penalize extreme utilization levels.\n        4. Item Size Consideration: Incorporates the item size into the\n           priority calculation.\n        5. Adaptive Random Perturbation: Introduces randomness to avoid getting\n           stuck in local optima, with the perturbation level adapted based on\n           the current iteration or problem characteristics.\n        6. Bin Diversity: Encourages diversity in bin selection to avoid putting all\n           small items in one bin, by adding a bonus to bins that have more empty space.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n\n    # Sigmoid scaling for waste minimization\n    waste_scaling = 5  # Adjust this parameter to control the steepness of sigmoid\n    priorities += 1 / (1 + np.exp(waste_scaling * waste))\n\n    # Dynamic capacity threshold (adapts to item size)\n    threshold = 0.1 * item  # Adjust the scaling factor as needed.\n    near_full = (bins_remain_cap > 0) & (bins_remain_cap < threshold)\n\n    # Bin utilization balancing with exponential decay\n    bin_fraction = bins_remain_cap / np.max(bins_remain_cap)\n    utilization_penalty = np.exp(-5 * (bin_fraction - 0.5)**2)\n    priorities += utilization_penalty\n\n    # Item Size Consideration\n    priorities += item / np.max(bins_remain_cap)  # Give preference to bins that fit the item well based on the bin capacity\n\n    # Adaptive Random Perturbation (small value for numerical stability)\n    perturbation_level = 0.005  # Adjust this parameter based on the problem\n    priorities += np.random.normal(0, perturbation_level, size=bins_remain_cap.shape)\n\n    # Bin Diversity - prefer less full bins\n    priorities += bin_fraction\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of enhanced heuristics:\n        1. Waste Minimization with Sigmoid Scaling: Prioritizes bins where the\n           remaining space after packing the item is minimal, but uses a sigmoid\n           function to provide a more nuanced preference.\n        2. Capacity Threshold with Dynamic Adjustment: Avoids bins with\n           extremely small remaining capacity, but dynamically adjusts the\n           threshold based on the average item size.\n        3. Bin Utilization Balancing with Exponential Decay: Prefers bins that\n           are not completely empty or completely full, with an exponential\n           decay to penalize extreme utilization levels.\n        4. Item Size Consideration: Incorporates the item size into the\n           priority calculation.\n        5. Adaptive Random Perturbation: Introduces randomness to avoid getting\n           stuck in local optima, with the perturbation level adapted based on\n           the current iteration or problem characteristics.\n        6. Bin Diversity: Encourages diversity in bin selection to avoid putting all\n           small items in one bin, by adding a bonus to bins that have more empty space.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n\n    # Sigmoid scaling for waste minimization\n    waste_scaling = 5  # Adjust this parameter to control the steepness of sigmoid\n    priorities += 1 / (1 + np.exp(waste_scaling * waste))\n\n    # Dynamic capacity threshold (adapts to item size)\n    threshold = 0.1 * item  # Adjust the scaling factor as needed.\n    near_full = (bins_remain_cap > 0) & (bins_remain_cap < threshold)\n\n    # Bin utilization balancing with exponential decay\n    bin_fraction = bins_remain_cap / np.max(bins_remain_cap)\n    utilization_penalty = np.exp(-5 * (bin_fraction - 0.5)**2)\n    priorities += utilization_penalty\n\n    # Item Size Consideration\n    priorities += item / np.max(bins_remain_cap)  # Give preference to bins that fit the item well based on the bin capacity\n\n    # Adaptive Random Perturbation (small value for numerical stability)\n    perturbation_level = 0.005  # Adjust this parameter based on the problem\n    priorities += np.random.normal(0, perturbation_level, size=bins_remain_cap.shape)\n\n    # Bin Diversity - prefer less full bins\n    priorities += bin_fraction\n\n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, capacity considerations, and randomness.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Waste Minimization & Infeasibility\n    waste = bins_remain_cap - item\n    infeasible = waste < 0\n    priorities[infeasible] = -np.inf\n    waste[infeasible] = np.inf\n\n    # Capacity considerations - like v1, but refined\n    capacity_diff = np.abs(bins_remain_cap - item)\n    priorities += 1.0 / (1e-6 + capacity_diff)\n\n    # Near-perfect fit bonus with waste\n    priorities += np.exp(-waste)\n\n    # Random Perturbation\n    priorities += np.random.normal(0, 0.01, size=bins_remain_cap.shape)\n\n    priorities = np.nan_to_num(priorities, neginf=-np.inf)\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines snugness, waste minimization, and a penalty for near-fits.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities\n\n    wasted_space = bins_remain_cap - item\n    snugness = item / bins_remain_cap\n\n    buffer = bins_remain_cap - item\n    small_buffer_penalty = np.clip(1 - (buffer / item), a_min=0, a_max=1)\n\n    priorities[feasible_bins] = (2 * np.exp(-wasted_space[feasible_bins]) + snugness[feasible_bins]) * (1-small_buffer_penalty[feasible_bins])\n\n    priorities = (priorities - np.min(priorities)) / (np.max(priorities) - np.min(priorities) + 1e-9)\n\n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines snugness, waste minimization, and a penalty for near-fits.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities\n\n    wasted_space = bins_remain_cap - item\n    snugness = item / bins_remain_cap\n\n    buffer = bins_remain_cap - item\n    small_buffer_penalty = np.clip(1 - (buffer / item), a_min=0, a_max=1)\n\n    priorities[feasible_bins] = (2 * np.exp(-wasted_space[feasible_bins]) + snugness[feasible_bins]) * (1-small_buffer_penalty[feasible_bins])\n\n    priorities = (priorities - np.min(priorities)) / (np.max(priorities) - np.min(priorities) + 1e-9)\n\n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines gravity/velocity with capacity, buffer to select bin.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    infeasible = bins_remain_cap < item\n    priorities[infeasible] = -np.inf\n\n    gravity = (item / bins_remain_cap) ** 2\n    velocity = np.exp(-np.abs(bins_remain_cap - item) / item)\n    priorities[~infeasible] = gravity[~infeasible] * velocity[~infeasible]\n\n    buffer = bins_remain_cap - item\n    small_buffer_penalty = np.clip(1 - (buffer / item), a_min=0, a_max=1)\n    feasible_bins = bins_remain_cap >= item\n    relative_capacity = (bins_remain_cap - item) / np.max(bins_remain_cap)\n    priorities[feasible_bins] += relative_capacity[feasible_bins] * (1-small_buffer_penalty[feasible_bins])\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines gravity/velocity with capacity, buffer to select bin.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    infeasible = bins_remain_cap < item\n    priorities[infeasible] = -np.inf\n\n    gravity = (item / bins_remain_cap) ** 2\n    velocity = np.exp(-np.abs(bins_remain_cap - item) / item)\n    priorities[~infeasible] = gravity[~infeasible] * velocity[~infeasible]\n\n    buffer = bins_remain_cap - item\n    small_buffer_penalty = np.clip(1 - (buffer / item), a_min=0, a_max=1)\n    feasible_bins = bins_remain_cap >= item\n    relative_capacity = (bins_remain_cap - item) / np.max(bins_remain_cap)\n    priorities[feasible_bins] += relative_capacity[feasible_bins] * (1-small_buffer_penalty[feasible_bins])\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}