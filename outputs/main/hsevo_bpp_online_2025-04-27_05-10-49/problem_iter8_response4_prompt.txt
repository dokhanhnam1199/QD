{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines waste minimization, target utilization, and adaptive scaling.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    wasted_space = bins_remain_cap - item\n    fit_mask = wasted_space >= 0\n\n    if np.any(fit_mask):\n        # Adaptive scaling based on item size and remaining capacity distribution.\n        item_fraction = item / np.max(bins_remain_cap)\n        scaling_factor = 1.0 + 2.0 * item_fraction # Increase scaling if item is a significant fraction of bin size\n        priorities[fit_mask] = np.exp(-scaling_factor * wasted_space[fit_mask]**2 / (bins_remain_cap[fit_mask].mean() + 1e-9))\n\n        desirable_remaining_space = 0.2 * np.max(bins_remain_cap)\n        priority_values = np.exp(-((wasted_space[fit_mask] ) ** 2) / (2 * (desirable_remaining_space/2)** 2)) # Target utilization\n        priorities[fit_mask] = 0.6*priorities[fit_mask]+ 0.4*priority_values #Combine with weight\n        \n        # Introduce a small bias to prefer bins with larger remaining capacity if waste is similar. Mitigates fragmentation.\n        capacity_bias = 0.1 * bins_remain_cap[fit_mask] / np.max(bins_remain_cap)\n        priorities[fit_mask] += capacity_bias\n\n        # Small random perturbation for exploration\n        priorities[fit_mask] += np.random.normal(0, 0.01, size=np.sum(fit_mask))\n\n    priorities[~fit_mask] = -1e9 # Penalize infeasible bins severely\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n                bins_remain_cap: np.ndarray,\n                waste_penalty_factor: float = 0.8575834075161552,\n                bin_fraction_penalty: float = 0.4693634879473551,\n                item_size_weight: float = 2.0884148461764993,\n                random_perturbation_scale: float = 0.06717097613120268,\n                capacity_usage_factor: float = 1.5,\n                large_item_threshold: float = 0.7,\n                waste_threshold: float = 0.1) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics with adaptive and interaction considerations:\n\n        1.  Waste Minimization: Prioritizes bins where the remaining space after\n            packing the item is minimal.  A near-perfect fit is highly valued.\n        2.  Capacity Threshold: Avoids bins with extremely small remaining\n            capacity to reduce fragmentation. Bins that cannot fit are penalized.\n        3.  Bin Level Awareness : Considers the initial capacity of the bins for a\n            more balanced distribution\n        4.  Balancing Bin Utilization:  Slight preference for bins that are not\n            completely empty or completely full to maintain flexibility.\n        5.  Random Perturbation: Introduces small randomness to avoid getting stuck\n            in local optima and explore slightly different packing arrangements.\n        6.  Item size awareness : Larger Items should fill bins as much as possible\n        7. Capacity Usage: More emphasis on using bins with higher capacity\n        8. Interaction effects between waste and item size.\n        9. Adaptive Scaling based on Item Size: Dynamically scales the influence of\n            different factors based on whether the item is considered \"large\" or not.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        waste_penalty_factor: Factor to adjust the impact of waste minimization.\n        bin_fraction_penalty: Factor to adjust the balancing of bin utilization.\n        item_size_weight: Weight of item size awareness.\n        random_perturbation_scale: Scale of random noise.\n        capacity_usage_factor: weight to original capacity.\n        large_item_threshold: threshold to determine if item is considered large.\n        waste_threshold: threshold to determine if waste is significant.\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    original_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n\n    # Near-perfect fit bonus (minimize waste)\n    priorities += waste_penalty_factor * np.exp(-waste)  # Exponential decay for increasing waste.\n\n    # Bin utilization balancing - slightly prefers bins that aren't empty or full.  avoids extremities.  Parabolic preference\n    bin_fraction = bins_remain_cap / original_capacity\n    priorities += bin_fraction_penalty * -(bin_fraction - 0.5)**2  # Adds a parabolic preference curve.\n\n    # Item Size Awareness.  Larger items fill bins up\n    priorities += item_size_weight * item / original_capacity\n    \n    # Capacity Usage: Preferentially use bins with higher remaining capacity\n    priorities += capacity_usage_factor * bins_remain_cap/original_capacity\n\n    # Interaction effects: If waste is small and item is large, prioritize more\n    is_large_item = item > large_item_threshold * original_capacity\n    is_small_waste = waste < waste_threshold * original_capacity\n    \n    if is_large_item:\n      priorities += 2*waste_penalty_factor * np.exp(-waste)\n    \n    # Adaptive Scaling: Adjust weights based on item size\n    if is_large_item:\n        # For large items, emphasize packing efficiency and capacity usage\n        waste_penalty_factor *= 1.2  # Increase importance of minimizing waste\n        capacity_usage_factor *= 1.1  # Slight increase in using available space\n        bin_fraction_penalty *= 0.8  # Reduce importance of bin balancing\n    else:\n        # For smaller items, emphasize bin balancing and waste\n        bin_fraction_penalty *= 1.2\n        waste_penalty_factor *= 0.9\n        capacity_usage_factor *= 0.9\n\n    # Random Perturbation (introduces some \"quantum\" fluctuation). Very small value for numerical stability\n    priorities += np.random.normal(0, random_perturbation_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see that the 1st has fewer adaptive components and simpler logic. The 20th has more parameters and conditional adjustments based on item size, making it more complex but not necessarily better.\n\nComparing (2nd) vs (19th), we observe that the 2nd and the 19th are nearly identical. This implies code duplication, and a possible lack of significant experimentation between these versions.\n\nComparing (3rd) vs (4th), the 3rd includes weighted scores for waste, utilization, item size, and a \"gravity\" component, while the 4th simplifies to waste minimization, bin utilization, item size awareness, and adaptive randomness. The 3rd's use of separate weights for each component may offer finer control, but also more parameters to tune.\n\nComparing (second worst) vs (worst), the 19th and 20th focus on adaptive scaling, conditional logic based on item size, and capacity usage. The 20th introduces capacity usage factor, emphasizes interaction effects between waste and item size and adaptive scaling based on item size.\n\nOverall: The better heuristics tend to be simpler and rely on a core set of principles (waste minimization, bin utilization balancing, item size awareness, and randomness), while the worse heuristics try to introduce more complex adaptive mechanisms which is not necessary. Also, duplication might reveal lack of diversity in experiments. Simplicity and core concepts appears more robust.\n- \nOkay, let's refine \"Current Self-Reflection\" to design better heuristics, focusing on actionable advice and avoiding common pitfalls.\n\nHere's a breakdown:\n\n*   **Keywords:** Parsimony, Adaptive Complexity, Diverse Experimentation, Code Reuse Analysis.\n*   **Advice:** Begin with minimal heuristics; incrementally add complexity only when demonstrably beneficial through rigorous testing. Prioritize simple, adaptive mechanisms over complex, heavily parameterized ones.\n*   **Avoid:** Premature complexity, over-parameterization, neglecting code duplication as a sign of limited exploration.\n*   **Explanation:** Start lean, adapt intelligently, test thoroughly, and actively seek out unexplored avenues (indicated by code redundancies).\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}