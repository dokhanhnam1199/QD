{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of heuristics with refined strategies:\n        1.  Waste Minimization with Sigmoid Scaling: Prioritizes near-perfect\n            fits, but uses a sigmoid function to provide a more gradual and\n            context-aware scaling of priorities.  This avoids overly aggressive\n            assignment to bins with tiny waste.\n        2.  Capacity Threshold with Dynamic Adjustment:  Penalizes bins that\n            cannot fit, but dynamically adjusts the penalty based on the\n            item size relative to the average remaining bin capacity.  If the\n            item is large, the penalty is reduced to encourage filling nearly-full\n            bins and reduce fragmentation.\n        3.  Bin Utilization Balancing with Adaptive Range:  Prefers bins that\n            are neither too full nor too empty, but adaptively adjusts the\n            preferred utilization range based on the item size.  Large items\n            shift the preference towards fuller bins.\n        4.  Fragmentation Avoidance: Explicitly discourages creating bins with\n            very small remaining capacity *after* placing the item, further\n            reducing fragmentation.\n        5.  Adaptive Random Perturbation: Adds random noise, but scales the\n            magnitude of the noise based on the item size.  Larger items\n            justify a larger perturbation to explore more diverse packing\n            arrangements.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    avg_cap = np.mean(bins_remain_cap)\n\n    # Waste Minimization & Capacity Threshold (Sigmoid Scaling & Dynamic Adjustment)\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    penalty_scale = np.clip(item / avg_cap, 0.1, 1.0)  # Dynamic penalty scaling\n    priorities[too_small] = -1e9 * penalty_scale  # Scaled penalty\n    waste[too_small] = np.inf\n\n    # Near-perfect fit bonus (minimize waste) with sigmoid\n    sigmoid_scale = 10  # Controls steepness of sigmoid\n    priorities += 1 / (1 + np.exp(sigmoid_scale * waste)) # Sigmoid scaling\n\n    # Bin utilization balancing (Adaptive Range)\n    bin_fraction = bins_remain_cap / np.max(bins_remain_cap)\n    utilization_center = 0.5 + 0.2 * np.clip(item / avg_cap, 0, 1) # Shifts center\n    priorities += -((bin_fraction - utilization_center) ** 2)\n\n    # Fragmentation Avoidance\n    remaining_fraction = waste / np.max(bins_remain_cap)\n    too_fragmented = (waste > 0) & (remaining_fraction < 0.1)  # Avoid tiny wastes\n    priorities[too_fragmented] -= 0.5  # Discourage fragmented bins, but less aggressive than infeasibility\n\n    # Adaptive Random Perturbation\n    noise_scale = 0.01 + 0.05 * np.clip(item / avg_cap, 0, 1)  # Scale noise\n    priorities += np.random.normal(0, noise_scale, size=bins_remain_cap.shape)\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Employs a combination of enhanced heuristics:\n        1. Waste Minimization with Sigmoid Scaling: Prioritizes bins where the\n           remaining space after packing the item is minimal, but uses a sigmoid\n           function to provide a more nuanced preference.\n        2. Capacity Threshold with Dynamic Adjustment: Avoids bins with\n           extremely small remaining capacity, but dynamically adjusts the\n           threshold based on the average item size.\n        3. Bin Utilization Balancing with Exponential Decay: Prefers bins that\n           are not completely empty or completely full, with an exponential\n           decay to penalize extreme utilization levels.\n        4. Item Size Consideration: Incorporates the item size into the\n           priority calculation.\n        5. Adaptive Random Perturbation: Introduces randomness to avoid getting\n           stuck in local optima, with the perturbation level adapted based on\n           the current iteration or problem characteristics.\n        6. Bin Diversity: Encourages diversity in bin selection to avoid putting all\n           small items in one bin, by adding a bonus to bins that have more empty space.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Waste Minimization & Capacity Threshold\n    waste = bins_remain_cap - item\n    too_small = waste < 0\n    priorities[too_small] = -np.inf  # Never put item in bins that are too small.\n    waste[too_small] = np.inf\n\n    # Sigmoid scaling for waste minimization\n    waste_scaling = 5  # Adjust this parameter to control the steepness of sigmoid\n    priorities += 1 / (1 + np.exp(waste_scaling * waste))\n\n    # Dynamic capacity threshold (adapts to item size)\n    threshold = 0.1 * item  # Adjust the scaling factor as needed.\n    near_full = (bins_remain_cap > 0) & (bins_remain_cap < threshold)\n\n    # Bin utilization balancing with exponential decay\n    bin_fraction = bins_remain_cap / np.max(bins_remain_cap)\n    utilization_penalty = np.exp(-5 * (bin_fraction - 0.5)**2)\n    priorities += utilization_penalty\n\n    # Item Size Consideration\n    priorities += item / np.max(bins_remain_cap)  # Give preference to bins that fit the item well based on the bin capacity\n\n    # Adaptive Random Perturbation (small value for numerical stability)\n    perturbation_level = 0.005  # Adjust this parameter based on the problem\n    priorities += np.random.normal(0, perturbation_level, size=bins_remain_cap.shape)\n\n    # Bin Diversity - prefer less full bins\n    priorities += bin_fraction\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see 1st uses waste minimization, capacity threshold, bin utilization balancing, item size awareness, and random perturbation with parameter tuning while the 20th use gravity/velocity with capacity and a buffer, lacking parameter tuning and random perturbation.\nComparing (2nd best) vs (second worst), we see 2nd uses waste minimization, capacity threshold, bin utilization balancing, item size awareness, and random perturbation with parameter tuning while the 19th use gravity/velocity with capacity and a buffer, lacking parameter tuning and random perturbation. Comparing (1st) vs (2nd), we see the two are identical, likely a copy-paste error. (3rd) vs (4th) also shows identical code, likely a copy-paste error. Comparing (second worst) vs (worst), we see the two are identical, likely a copy-paste error. Overall: The best heuristics include multiple factors (waste, capacity, utilization, item size, randomness), use parameter tuning and more sophisticated equations (exponential, sigmoid). Poorer heuristics focus on fewer factors and simpler equations, without parameter tuning. Specifically, the better heuristics prioritize a combination of waste minimization, bin utilization balancing, item size considerations, and random perturbation, often employing techniques like sigmoid scaling and adaptive adjustments. They also include mechanisms to avoid fragmentation. The worse heuristics tend to focus on a smaller subset of these factors, use simpler mathematical formulations, and lack adaptive or dynamic adjustments based on problem characteristics. The penalty terms for infeasible bins are also better handled in the top-ranked functions.\n- \nOkay, let's refine \"Current Self-Reflection\" for designing better heuristics.\n\n*   **Keywords:** Multifactorial Analysis, Adaptive Weights, Exploration, Granularity.\n*   **Advice:** Go beyond simple combinations. Explore interaction effects between factors (e.g., waste & item size). Employ adaptive weight adjustments based on real-time performance feedback.\n*   **Avoid:** Static, linear factor combinations; premature convergence.\n*   **Explanation:** Adaptive learning, fine-grained factor integration, and diversification help us to design better heuristics.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}