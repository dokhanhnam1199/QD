{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a dynamic penalty for excessive remaining capacity,\n    prioritizing tighter fits while penalizing bins that are significantly emptier.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        suitable_bins_caps = bins_remain_cap[can_fit_mask]\n        \n        # Best Fit component: Inverse of the gap (remaining capacity after packing).\n        # Favors bins with smaller gaps.\n        gaps = suitable_bins_caps - item\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Penalty component: Penalize bins that leave a large absolute remaining capacity.\n        # We subtract the remaining capacity directly. This aims to use bins that are\n        # closer to the item size among those that are feasible.\n        # This is a more direct penalty than the ratio-based one, similar to v0.\n        penalty = suitable_bins_caps\n        \n        # Combine scores: Best Fit score minus the penalty.\n        # This prioritizes tight fits but discourages using very large bins\n        # even if they offer a \"best fit\" among a set of large bins.\n        combined_scores = best_fit_scores - penalty\n        \n        priorities[can_fit_mask] = combined_scores\n        \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n### Analyze & experience\n- *   **Heuristic 1 vs. Heuristic 2:** Heuristic 2 introduces parameters (`epsilon`, `penalty_factor`) that are explicitly tuned, suggesting an attempt at calibration. It also uses a ratio-based penalty (`remaining_after_fit / item`), which is more adaptive to item size than the absolute gap used in Heuristic 1. Heuristic 2 also normalizes its final scores to [0, 1], promoting consistency.\n*   **Heuristic 2 vs. Heuristic 3:** Heuristic 2's penalty is logarithmic and ratio-based, offering a smoother and more adaptive penalization compared to Heuristic 3's linear, ratio-based penalty. Heuristic 3's normalization is absent, which can lead to less predictable priority scales.\n*   **Heuristic 3 vs. Heuristic 4:** Heuristic 4 attempts to normalize its \"Best Fit\" scores and uses an exponential penalty (though the implementation seems to use a log-based penalty which is then normalized). Heuristic 3 uses a simpler linear penalty on the ratio. The normalization and smoother penalty in Heuristic 4 are generally preferred.\n*   **Heuristic 4 vs. Heuristic 5:** Heuristic 4 normalizes its best-fit scores and applies a normalized log penalty, aiming for a more balanced approach. Heuristic 5 uses a simpler additive penalty (the remaining capacity itself), which can be overly aggressive in discarding larger bins, and it doesn't normalize its final scores.\n*   **Heuristic 5 vs. Heuristic 6:** Heuristic 6 is identical to Heuristic 1. Heuristic 5 uses an additive penalty (remaining capacity), which is less nuanced than Heuristic 1's logarithmic penalty.\n*   **Heuristic 6 vs. Heuristic 7:** Heuristic 7 is identical to Heuristic 2. Heuristic 6 (same as 1) uses an absolute gap penalty, whereas Heuristic 7 (same as 2) uses a ratio-based, logarithmic penalty, making Heuristic 7's approach more adaptive.\n*   **Heuristic 7 vs. Heuristic 8:** Heuristic 7 focuses on Best Fit and a ratio-based penalty. Heuristic 8 combines Best Fit, First Fit, and a fullness score with fixed weights. While multi-objective is good, the fixed weights and the specific formulation of the \"fullness score\" (potentially assuming fixed bin capacity) make it less robust than Heuristic 7's adaptive ratio penalty. The normalization in Heuristic 7 is also clearer.\n*   **Heuristic 8 vs. Heuristic 9:** Heuristic 9 is identical to Heuristic 8.\n*   **Heuristic 9 vs. Heuristic 10:** Heuristic 10 uses a sigmoid-like penalty, which is a more sophisticated way to penalize large remaining capacities compared to the weighted sum in Heuristic 9. Heuristic 10 also adapts the penalty relative to the item size more directly.\n*   **Heuristic 10 vs. Heuristic 11:** Heuristic 11 is identical to Heuristic 10.\n*   **Heuristic 11 vs. Heuristic 12:** Heuristic 12 is identical to Heuristic 10.\n*   **Heuristic 12 vs. Heuristic 13:** Heuristic 13 is identical to Heuristic 10.\n*   **Heuristic 13 vs. Heuristic 14:** Heuristic 14 is identical to Heuristic 10.\n*   **Heuristic 14 vs. Heuristic 15:** Heuristic 15 is identical to Heuristics 16-20, representing minimal logic. Heuristic 14 (and 10-13) implements a combination of Best Fit and a sophisticated sigmoid-based penalty.\n*   **Heuristics 15-20:** These heuristics are identical and only provide the initial setup (priorities array and mask check), offering no actual scoring logic. They are the worst due to lack of implementation.\n\nOverall, heuristics combining a strong Best Fit component with adaptive, smooth penalties (especially ratio-based or sigmoid-based) that are normalized tend to perform better. Tuned parameters and explicit normalization contribute to a more robust and predictable heuristic.\n- \nHere's a redefined approach to self-reflection for heuristic design, focusing on actionable insights:\n\n*   **Keywords:** Adaptive penalties, normalization, multi-objective weighting, parameter tuning, stability.\n*   **Advice:** Focus on relative performance and feature interactions. Design adaptive penalty functions that scale with the problem's current state (e.g., problem size, constraint violation magnitude). Experiment with different normalization techniques and their impact on score aggregation.\n*   **Avoid:** Rigid, absolute penalty structures. Ignoring the interplay between different objectives when combining them. Overly simplistic aggregation methods for multi-objective problems.\n*   **Explanation:** The goal is to build heuristics that learn and adapt. Instead of fixed penalties, use functions that dynamically adjust. Normalization ensures that disparate metrics contribute meaningfully. Multi-objective design requires careful consideration of how to balance trade-offs, often favoring multiplicative or conditional logic over simple addition.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}