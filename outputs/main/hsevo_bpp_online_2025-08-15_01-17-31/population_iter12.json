[
  {
    "stdout_filepath": "problem_iter11_response0.txt_stdout.txt",
    "code_path": "problem_iter11_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit's precision with an adaptive logarithmic penalty.\n    Prioritizes tight fits while smoothly penalizing underfilled bins,\n    using ratio-based penalty and normalization for robust scoring.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    gaps = suitable_bins_caps - item\n\n    # Best Fit Score: Inverted gap to favor smaller gaps. Add epsilon for numerical stability.\n    best_fit_score = 1.0 / (gaps + 1e-6)\n\n    # Adaptive Logarithmic Penalty: Penalizes bins with significantly more remaining capacity\n    # than the item size, using a logarithmic scale for smoothness.\n    # The penalty is proportional to log(remaining_capacity / item_size).\n    # A small epsilon added to item size prevents division by zero.\n    # A penalty_factor controls the impact of this penalty.\n    penalty_factor = 0.15 # Slightly increased penalty factor for more influence\n    capacity_ratio = suitable_bins_caps / (item + 1e-9)\n    excess_capacity_penalty = penalty_factor * np.log1p(capacity_ratio)\n\n    # Combined Score: Subtract the penalty from the best-fit score.\n    # Higher scores indicate more preferred bins.\n    combined_scores = best_fit_scores - excess_capacity_penalty\n\n    # Normalization: Scale scores to a [0, 1] range for consistent comparison.\n    min_score = np.min(combined_scores)\n    max_score = np.max(combined_scores)\n    if max_score > min_score:\n        normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n    else:\n        # If all scores are the same, assign a neutral score (e.g., 0.5)\n        normalized_scores = np.ones_like(combined_scores) * 0.5\n\n    priorities[suitable_bins_mask] = normalized_scores\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 32, in priority_v2\n    normalized_scores = 1.0 - normalized_penalty\nNameError: name 'best_fit_scores' is not defined. Did you mean: 'best_fit_score'?\n3\n210.99629561116632\n"
  },
  {
    "stdout_filepath": "problem_iter11_response1.txt_stdout.txt",
    "code_path": "problem_iter11_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit strategy with an adaptive logarithmic penalty for remaining capacity.\n    Prioritizes bins that minimize wasted space after packing, with a nuanced penalty\n    for larger remaining capacities to avoid overly aggressive bin selection.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Best Fit: Minimize remaining capacity after packing\n    # Calculate the difference between remaining capacity and item size\n    remaining_after_fit = suitable_bins_cap - item\n    \n    # Adaptive penalty: Use a logarithmic function of the ratio of remaining capacity to item size.\n    # This penalizes larger gaps more, but with diminishing returns (smoother than linear).\n    # Add a small epsilon to the denominator to avoid division by zero if item size is 0 (though unlikely in BPP).\n    # Add 1 to the denominator to ensure values are not excessively large when remaining_after_fit is small.\n    penalty = np.log1p(remaining_after_fit / (item + 1e-9))\n    \n    # Normalize the penalty to be between 0 and 1. Higher penalty should result in lower priority.\n    # We want to invert this relationship, so we use (1 - normalized_penalty).\n    if np.max(penalty) > 0:\n        normalized_penalty = penalty / np.max(penalty)\n        normalized_scores = 1.0 - normalized_penalty\n    else:\n        normalized_scores = np.ones_like(suitable_bins_cap) # All penalties were zero or negative (unlikely with log1p)\n\n    # Assign priorities to the original indices\n    original_indices = np.where(suitable_bins_mask)[0]\n    priorities[original_indices] = normalized_scores\n    \n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 97.70233280920246,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response2.txt_stdout.txt",
    "code_path": "problem_iter11_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a normalized, sigmoid-based penalty for remaining capacity.\n    Prioritizes tight fits while smoothly penalizing bins with excessive empty space.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n    penalty_strength = 5.0 # Tune this parameter to control penalty intensity\n\n    can_fit_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if valid_bins_remain_cap.size > 0:\n        remaining_after_fit = valid_bins_remain_cap - item\n        \n        # Best Fit score: inverse of the remaining space for tighter fits\n        best_fit_scores = 1.0 / (remaining_after_fit + epsilon)\n        \n        # Sigmoid-based penalty: Penalizes large remaining capacities smoothly.\n        # Scales the remaining capacity relative to the item size and bin capacity (implicitly)\n        # The sigmoid function ensures the penalty grows but saturates.\n        normalized_remaining = remaining_after_fit / np.maximum(bins_remain_cap[can_fit_mask], epsilon)\n        penalty_scores = 1.0 / (1.0 + np.exp(penalty_strength * (1.0 - normalized_remaining))) # Penalizes larger remaining space\n        \n        # Combine Best Fit with penalty: subtract penalty from the score\n        combined_priorities = best_fit_scores - penalty_scores\n        \n        # Normalize combined priorities to [0, 1] for consistent comparison\n        min_p, max_p = np.min(combined_priorities), np.max(combined_priorities)\n        if max_p > min_p:\n            normalized_priorities = (combined_priorities - min_p) / (max_p - min_p)\n        else:\n            normalized_priorities = np.ones_like(combined_priorities) * 0.5 # All scores are same\n            \n        original_indices = np.where(can_fit_mask)[0]\n        priorities[original_indices] = normalized_priorities\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 235.53074858920888,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response3.txt_stdout.txt",
    "code_path": "problem_iter11_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a smooth, adaptive penalty for large remaining capacities.\n    Prioritizes tight fits while penalizing significantly underfilled bins gracefully.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    \n    suitable_bins_caps = bins_remain_cap[can_fit_mask]\n    \n    if suitable_bins_caps.size > 0:\n        gaps = suitable_bins_caps - item\n        \n        # Best Fit component: prioritize smaller gaps (higher score for smaller gaps)\n        # Using 1/(gap + epsilon) provides a good base score.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Adaptive penalty for large remaining capacity:\n        # Penalize bins where remaining capacity is much larger than the item size.\n        # Using a sigmoid-like function centered around a ratio (e.g., 2.0)\n        # to smoothly increase penalty as capacity exceeds item size significantly.\n        # A steepness factor controls how quickly the penalty ramps up.\n        penalty_threshold_ratio = 2.0 \n        penalty_steepness = 0.7 \n        \n        large_capacity_penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (suitable_bins_caps / (item + 1e-9) - penalty_threshold_ratio)))\n        \n        # Combine scores: Subtract the penalty from the best-fit score.\n        combined_scores = best_fit_scores - large_capacity_penalty\n        \n        priorities[can_fit_mask] = combined_scores\n        \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 190.19550008653877,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response4.txt_stdout.txt",
    "code_path": "problem_iter11_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a tuned, ratio-based penalty using a sigmoid function\n    for smoother, adaptive penalization, and normalizes scores.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n    \n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Best Fit component: inverse of the gap. Higher score for smaller gaps.\n    epsilon = 1e-9\n    best_fit_scores = 1.0 / (valid_bins_remain_cap - item + epsilon)\n    \n    # Adaptive Penalty component: Penalize bins with large remaining capacity after packing.\n    # We use a sigmoid-like function to create a smooth transition and a more nuanced penalty.\n    # The penalty is based on the ratio of excess capacity to item size.\n    # tunable_penalty_factor controls the steepness of the sigmoid.\n    tunable_penalty_factor = 2.0  # Controls the steepness of the penalty curve\n    \n    # Calculate the \"excess\" capacity relative to the item size.\n    excess_capacity_ratio = (valid_bins_remain_cap - item) / (item + epsilon)\n    \n    # Sigmoid function: sigmoid(x) = 1 / (1 + exp(-k*x))\n    # We apply it to penalize larger ratios.\n    # Here, we use exp(-tunable_penalty_factor * excess_capacity_ratio)\n    # A higher ratio leads to a smaller exp term, thus a higher penalty value (closer to 1).\n    # To convert this to a penalty that *reduces* priority, we can use 1 - sigmoid_value.\n    # A higher excess_capacity_ratio should result in a higher penalty term that reduces priority.\n    # Let's use the direct penalty ratio: exp(-k * excess_capacity_ratio).\n    # This value ranges from 0 (for large ratios) to 1 (for zero or negative ratios).\n    # We want to *subtract* this penalty from the best_fit_score.\n    penalty_scores = np.exp(-tunable_penalty_factor * excess_capacity_ratio)\n    \n    # Combine the best fit score with the penalty.\n    # Score = best_fit_score * (1 - penalty_score) - This would reduce score if penalty is high.\n    # A better combination: Score = best_fit_score - penalty_score * weight\n    # Let's try to make the penalty directly subtract from the score.\n    # The penalty should be higher for larger excess capacity ratios.\n    # The exp(-k * ratio) formulation means penalty_scores are high for low ratios and low for high ratios.\n    # This is the opposite of what we want if we're subtracting.\n    # Let's re-think: we want to reduce priority if excess_capacity_ratio is high.\n    # So, the penalty term should increase with excess_capacity_ratio.\n    # Let's use a penalty that is proportional to the ratio itself, but smoothed.\n    # The exp(-k * ratio) approach is good for *favoring* smaller ratios.\n    # Let's try to combine Best Fit and a penalty that penalizes large *remaining* capacities.\n    # The penalty should be such that it reduces the score of bins with a lot of empty space left.\n    # A simple penalty: (remaining_capacity) / (item + epsilon)\n    # We want to subtract this from the best_fit score.\n    \n    # Let's revert to the idea of prioritizing tight fits (high best_fit_scores)\n    # and then penalizing if there's a lot of space left.\n    # We can use the best_fit_score itself as a base, and then subtract a penalty.\n    # Penalty: A logistic function could map the remaining capacity to a penalty value.\n    # Let's try this:\n    # Penalty = C * logistic_function(remaining_capacity / item)\n    # where logistic_function(x) = 1 / (1 + exp(-k * x))\n    # We want to penalize higher remaining capacities.\n    # If remaining_capacity is large, logistic_function will be close to 1.\n    # If remaining_capacity is small, logistic_function will be close to 0.\n    # So, we want to subtract penalty_strength * logistic_function(excess_capacity_ratio).\n    \n    penalty_strength = 0.5 # Tunable parameter for penalty aggression\n    \n    # Use a logistic function (sigmoid) to map the excess capacity ratio to a penalty between 0 and 1.\n    # The steeper the slope (larger 'k'), the more sensitive the penalty to the ratio.\n    # We use negative excess_capacity_ratio in the sigmoid's exponent to ensure the penalty increases\n    # as excess_capacity_ratio increases (i.e., as the bin becomes less of a \"tight fit\").\n    # sigmoid_penalty = 1 / (1 + exp(-k * (ratio - offset)))\n    # A simpler form: 1 / (1 + exp(-k * ratio)) for ratio > 0.\n    # Let's re-parameterize to make it clearer:\n    # We want a penalty P(ratio) such that P(0) is small, P(large) is large.\n    # P(ratio) = 1 - (1 / (1 + exp(-k * ratio)))  -- This is 1 - sigmoid, which is logistic_complement.\n    # Let's use the standard sigmoid on the negative ratio.\n    # `penalty_value = 1.0 / (1.0 + np.exp(tunable_penalty_factor * excess_capacity_ratio))`\n    # This penalty_value will be close to 0 for large positive ratios (high excess)\n    # and close to 1 for negative ratios (tight fit or overflow).\n    # This is still not what we want. We want to *reduce* the score if the ratio is high.\n    \n    # Let's go back to `best_fit_scores - penalty`.\n    # We want `penalty` to increase with `excess_capacity_ratio`.\n    # `penalty = penalty_strength * excess_capacity_ratio` is a good start.\n    # To make it smoother and adaptive, let's use a function that is bounded or more controlled.\n    # Consider `penalty = penalty_strength * (excess_capacity_ratio / (1 + excess_capacity_ratio))`\n    # This is a saturating penalty, bounded by `penalty_strength`.\n    \n    # Let's combine Best Fit with a penalty that is proportional to the excess capacity,\n    # but using a smooth function like `atan` or `tanh` might be better.\n    # `atan(x)` increases with x but saturates.\n    # `atan(excess_capacity_ratio)` ranges from -pi/2 to pi/2.\n    # We want to penalize positive excess capacity.\n    # `penalty_scores = np.arctan(excess_capacity_ratio)`\n    # This will be close to pi/2 for large positive excess capacity. We want to subtract this.\n    # `priorities = best_fit_scores - penalty_strength * np.arctan(excess_capacity_ratio)`\n    # This would decrease scores for bins with excess capacity.\n    # The epsilon in best_fit_scores handles tight fits.\n    \n    # Let's refine the penalty: we want to penalize bins where `remaining_capacity - item` is large.\n    # The ratio `(remaining_capacity - item) / item` is a good measure.\n    # We can use a scaled `arctan` for a smooth penalty.\n    \n    penalty_scale = 1.0 # Controls how much the penalty affects the score.\n    \n    # Calculate the \"gap\" after packing, relative to item size.\n    gap_ratio = (valid_bins_remain_cap - item) / (item + epsilon)\n    \n    # Use arctan to create a smooth penalty that increases with gap_ratio.\n    # We want to penalize larger gaps, so we subtract this from the best_fit_score.\n    # arctan(gap_ratio) is roughly proportional to gap_ratio for small values,\n    # and approaches pi/2 for large values, providing saturation.\n    # We subtract this scaled penalty.\n    # A positive gap_ratio means wasted space. We want to reduce priority for large positive gap_ratio.\n    # `np.arctan(gap_ratio)` correctly increases for positive `gap_ratio`.\n    \n    combined_priorities = best_fit_scores - penalty_scale * np.arctan(gap_ratio)\n    \n    # Normalize the combined priorities to be in a [0, 1] range for stability.\n    min_p = np.min(combined_priorities)\n    max_p = np.max(combined_priorities)\n    \n    if max_p - min_p > epsilon:\n        normalized_priorities = (combined_priorities - min_p) / (max_p - min_p)\n    else:\n        normalized_priorities = np.zeros_like(combined_priorities) # Or some default if all scores are same\n        if max_p > -np.inf: # If there was at least one valid score\n            normalized_priorities.fill(0.5) # Assign a neutral score if all are same\n\n    # Assign priorities back to the original array structure\n    original_indices = np.where(can_fit_mask)[0]\n    priorities[original_indices] = normalized_priorities\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 323.1448300675329,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response5.txt_stdout.txt",
    "code_path": "problem_iter11_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines normalized Best Fit with a smooth, adaptive penalty for large remaining capacities.\n    Prioritizes tight fits while gracefully penalizing underutilized bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Best Fit component: Prioritize bins that leave minimal remaining space.\n    # Calculate the difference: remaining capacity - item size. Smaller is better.\n    best_fit_diff = suitable_bins_cap - item\n    \n    # Normalize Best Fit scores to [0, 1]. A smaller difference gets a higher score (closer to 1).\n    # Add a small epsilon to prevent division by zero if all suitable bins have the same remaining space.\n    epsilon = 1e-9\n    min_diff = np.min(best_fit_diff)\n    max_diff = np.max(best_fit_diff)\n    \n    if max_diff > min_diff:\n        best_fit_scores = 1.0 - (best_fit_diff - min_diff) / (max_diff - min_diff + epsilon)\n    else:\n        best_fit_scores = np.ones_like(best_fit_diff) # All fits are equally good\n\n    # Adaptive Penalty component: Penalize bins with disproportionately large remaining capacities relative to the item size.\n    # This uses a logarithmic function for smoothness, inspired by heuristics that handle large gaps gracefully.\n    # Calculate the ratio of remaining capacity to item size.\n    # Penalize more when this ratio is high.\n    # We want to *subtract* this penalty from the best_fit_scores. So, the penalty itself should increase with the gap.\n    # Using log1p(excess_capacity) provides a smooth increase and is robust to small values.\n    excess_capacity = suitable_bins_cap - item\n    \n    # Raw penalty based on the log of (1 + excess capacity). Add 1 to avoid log(0).\n    # The +1 is crucial here to handle cases where excess_capacity is 0.\n    log_penalty_raw = np.log1p(excess_capacity) \n    \n    # Normalize the penalty to have a consistent scale across different item/bin sizes.\n    # We scale it relative to the maximum raw penalty among suitable bins.\n    max_log_penalty = np.max(log_penalty_raw)\n    if max_log_penalty > epsilon:\n        normalized_penalty = log_penalty_raw / max_log_penalty\n    else:\n        normalized_penalty = np.zeros_like(log_penalty_raw)\n\n    # Combine scores: Subtract the normalized penalty from the Best Fit score.\n    # This means good fits (high best_fit_scores) are favored, and bins with large excess capacity (high normalized_penalty) are penalized.\n    combined_priorities = best_fit_scores - normalized_penalty\n    \n    # Ensure final priorities are non-negative.\n    combined_priorities = np.maximum(combined_priorities, 0)\n\n    # Assign the calculated priorities back to their original positions in the priorities array.\n    original_indices = np.where(suitable_bins_mask)[0]\n    priorities[original_indices] = combined_priorities\n    \n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 171.8953543301665,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response6.txt_stdout.txt",
    "code_path": "problem_iter11_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a normalized, adaptive sigmoid penalty for remaining capacity.\n    Prioritizes tight fits while smoothly penalizing bins that are significantly emptier.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        suitable_bins_caps = bins_remain_cap[can_fit_mask]\n        \n        # Best Fit component: Inverse of the gap. Higher score for smaller gaps.\n        gaps = suitable_bins_caps - item\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Adaptive sigmoid penalty: Penalizes larger remaining capacities more heavily,\n        # but smoothly, relative to the item size.\n        # The penalty term is designed to be high for bins much larger than the item.\n        # We use a scaled version of the remaining capacity.\n        scaled_remaining = suitable_bins_caps / (np.max(bins_remain_cap) + 1e-9)\n        penalty = 1.0 / (1.0 + np.exp(10 * (scaled_remaining - 0.5))) # Sigmoid centered around 0.5\n        \n        # Combine scores: Best Fit score scaled, minus the penalty.\n        # Normalizing best_fit_scores to [0, 1] and penalty is already in [0, 1]\n        normalized_best_fit = (best_fit_scores - np.min(best_fit_scores)) / (np.max(best_fit_scores) - np.min(best_fit_scores) + 1e-9)\n        \n        combined_scores = normalized_best_fit - penalty\n        \n        priorities[can_fit_mask] = combined_scores\n        \n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 232.84722658818316,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response7.txt_stdout.txt",
    "code_path": "problem_iter11_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a calibrated logarithmic penalty for excessive remaining capacity,\n    favoring bins that offer a tight fit while moderately penalizing large unused space.\n    Also incorporates a slight bias towards bins with less remaining capacity overall.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    gaps = suitable_bins_caps - item\n\n    # Best Fit Score: Higher for smaller gaps (tighter fit). Add epsilon for stability.\n    best_fit_score = 1.0 / (gaps + 1e-6)\n\n    # Calibrated Logarithmic Penalty: Penalizes large gaps more significantly but smoothly.\n    # The penalty increases logarithmically with the gap size.\n    penalty_factor = 0.1\n    excess_capacity_penalty = penalty_factor * np.log1p(gaps)\n\n    # Additional heuristic: Favor bins with less remaining capacity overall (less waste)\n    # This is a simple additive term that slightly boosts bins with less overall capacity.\n    # Normalizing this to be on a similar scale as the best_fit_score.\n    # We subtract it to prefer smaller remaining capacities.\n    # Avoid division by zero by adding a small epsilon.\n    overall_capacity_bias = 1.0 / (suitable_bins_caps + 1e-6)\n    normalized_capacity_bias = (overall_capacity_bias - np.min(overall_capacity_bias)) / (np.max(overall_capacity_bias) - np.min(overall_capacity_bias) + 1e-9)\n\n\n    # Combine scores: Subtract the penalty from the best-fit score, and also subtract the bias for larger capacities.\n    combined_priorities = best_fit_score - excess_capacity_penalty - normalized_capacity_bias * 0.2 # Adjust weight for bias\n\n    # Normalize priorities to be between 0 and 1 for consistent behavior\n    min_priority = np.min(combined_priorities)\n    max_priority = np.max(combined_priorities)\n    if max_priority > min_priority:\n        normalized_priorities = (combined_priorities - min_priority) / (max_priority - min_priority)\n    else:\n        normalized_priorities = np.ones_like(combined_priorities) * 0.5 # Default if all scores are same\n\n    priorities[suitable_bins_mask] = normalized_priorities\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 56.93059433585961,
    "cyclomatic_complexity": 3.0,
    "halstead": 326.8106722817031,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response8.txt_stdout.txt",
    "code_path": "problem_iter11_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a smooth, adaptive penalty for large remaining capacities,\n    enhanced by normalization for stable multi-bin selection.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    \n    suitable_bins_caps = bins_remain_cap[can_fit_mask]\n    \n    if suitable_bins_caps.size > 0:\n        gaps = suitable_bins_caps - item\n        \n        # Best Fit component: prioritize smaller gaps. Use a scaled inverse for emphasis.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Adaptive penalty for large remaining capacity using a sigmoid-like function.\n        # This smoothly penalizes bins with significantly more capacity than needed.\n        penalty_threshold_ratio = 2.0 \n        penalty_steepness = 0.7 \n        \n        # Calculate penalty: high when capacity >> item * ratio, approaches 0 otherwise.\n        large_capacity_penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (suitable_bins_caps / (item + 1e-9) - penalty_threshold_ratio)))\n        \n        # Combine scores: Subtract penalty from Best Fit score.\n        combined_scores = best_fit_scores - large_capacity_penalty\n        \n        # Normalize scores to [0, 1] for stable comparison across different item sizes.\n        # This helps in situations where absolute score differences might vary wildly.\n        if combined_scores.size > 0:\n            min_score = np.min(combined_scores)\n            max_score = np.max(combined_scores)\n            if max_score - min_score > 1e-9:\n                normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n            else:\n                normalized_scores = np.ones_like(combined_scores) * 0.5 # Assign neutral score if all scores are same\n        else:\n            normalized_scores = np.array([])\n\n        priorities[can_fit_mask] = normalized_scores\n        \n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 312.88626403364293,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response9.txt_stdout.txt",
    "code_path": "problem_iter11_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines strong Best Fit with an adaptive, smoothed penalty based on capacity-to-item ratio.\n    Prioritizes tight fits while gracefully penalizing bins with disproportionately large remaining space.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Consider only bins that can fit the item\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if valid_bins_remain_cap.size > 0:\n        # Calculate the remaining capacity after placing the item\n        remaining_after_fit = valid_bins_remain_cap - item\n        \n        # Best Fit component: prioritize smaller gaps (higher score for smaller gaps).\n        # Add epsilon for numerical stability.\n        best_fit_scores = 1.0 / (remaining_after_fit + 1e-9)\n        \n        # Adaptive Penalty component: Penalize based on the ratio of remaining space to the item size.\n        # A higher ratio (more wasted space relative to the item) gets a higher penalty (lower priority).\n        # Using a sigmoid-like function to smoothly increase the penalty as the ratio exceeds a threshold.\n        # This provides a more nuanced penalty than a simple logarithmic function.\n        penalty_threshold_ratio = 1.5 # Ratio where penalty starts to increase significantly\n        penalty_steepness = 0.8 # Controls how quickly the penalty increases\n        \n        # Calculate penalty: sigmoid function ensures values between 0 and 1.\n        # Penalty is high when capacity/item ratio is much larger than threshold.\n        # Add epsilon to item in denominator to prevent division by zero for zero-sized items.\n        capacity_ratio = valid_bins_remain_cap / (item + 1e-9)\n        penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (capacity_ratio - penalty_threshold_ratio)))\n        \n        # Combine Best Fit score with penalty (subtract penalty from score)\n        combined_scores = best_fit_scores - penalty\n        \n        # Normalize the combined scores to be between 0 and 1.\n        # This makes scores comparable across different item/bin configurations and ensures positive priorities.\n        min_score = np.min(combined_scores)\n        max_score = np.max(combined_scores)\n        \n        if max_score > min_score:\n            normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n        else:\n            # If all valid bins have the same combined score, assign a uniform medium priority\n            normalized_scores = np.full_like(combined_scores, 0.5)\n            \n        # Assign the calculated priorities back to the original indices\n        original_indices = np.where(can_fit_mask)[0]\n        priorities[original_indices] = normalized_scores\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 260.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response0.txt_stdout.txt",
    "code_path": "problem_iter12_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # Best Fit heuristic component\n        best_fit_diff = suitable_bins_cap - item\n        min_diff = np.min(best_fit_diff)\n        best_fit_scores = np.exp(-best_fit_diff / np.mean(suitable_bins_cap)) \n        \n        # First Fit Decreasing like component (prioritize fuller bins among suitable ones)\n        # Normalize remaining capacities to give higher scores to bins with less remaining capacity\n        normalized_remain_cap = (suitable_bins_cap - item) / np.max(suitable_bins_cap - item + 1e-9) # Avoid division by zero\n        \n        # Combine heuristics with a weighting mechanism that can be tuned\n        # Here, we'll use a simple additive combination with a focus on Best Fit\n        combined_scores = 0.7 * best_fit_scores + 0.3 * (1 - normalized_remain_cap)\n        \n        # Normalize combined scores to be between 0 and 1\n        if np.max(combined_scores) > 0:\n            normalized_combined_scores = combined_scores / np.max(combined_scores)\n        else:\n            normalized_combined_scores = np.zeros_like(combined_scores)\n            \n        original_indices = np.where(suitable_bins_mask)[0]\n        \n        for i, original_idx in enumerate(original_indices):\n            priorities[original_idx] = normalized_combined_scores[i]\n            \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 199.1772208002305,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response1.txt_stdout.txt",
    "code_path": "problem_iter12_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        best_fit_scores = 1.0 / (suitable_bins_cap - item + 1e-9) \n        \n        normalized_scores = (best_fit_scores - np.min(best_fit_scores)) / (np.max(best_fit_scores) - np.min(best_fit_scores) + 1e-9)\n        \n        \n        original_indices = np.where(suitable_bins_mask)[0]\n        \n        \n        for i, original_idx in enumerate(original_indices):\n            priorities[original_idx] = normalized_scores[i]\n            \n    \n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 100.07820003461549,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response2.txt_stdout.txt",
    "code_path": "problem_iter12_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        best_fit_metric = suitable_bins_cap - item\n        \n        # Normalize the best fit metric to be between 0 and 1\n        # Add a small epsilon to avoid division by zero if all suitable bins are perfectly filled\n        normalized_best_fit = best_fit_metric / (np.max(best_fit_metric) + 1e-9)\n        \n        # Introduce a penalty for bins that are too empty relative to the item size\n        # This encourages using bins that are \"almost full\" to minimize waste\n        # Normalize this penalty too\n        # Larger remaining capacity means a higher penalty score (less desirable)\n        max_remaining_cap = np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 1e-9\n        empty_bin_penalty = (bins_remain_cap[suitable_bins_mask] - item) / max_remaining_cap\n        \n        # Combine metrics: Prioritize smaller differences (better fit) and penalize larger remaining capacities\n        # We want to minimize normalized_best_fit and minimize empty_bin_penalty\n        # So, a lower combined score is better. We'll invert it for higher priority.\n        \n        # Assign a base priority (e.g., 1.0 for all suitable bins) and then adjust\n        base_priority = np.ones_like(suitable_bins_cap)\n        \n        # Lower score in normalized_best_fit is better, so we subtract it from base priority\n        # Lower score in empty_bin_penalty is better, so we subtract it from base priority\n        # Add a small factor to emphasize the best fit\n        combined_score = base_priority - (0.7 * normalized_best_fit + 0.3 * empty_bin_penalty)\n        \n        # Now, assign these scores to the priorities array at the appropriate indices\n        original_indices = np.where(suitable_bins_mask)[0]\n        priorities[original_indices] = combined_score\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 155.11451069865606,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response3.txt_stdout.txt",
    "code_path": "problem_iter12_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        \n        diffs = suitable_bins_cap - item\n        \n        \n        normalized_diffs = 1.0 - (diffs / np.max(suitable_bins_cap))\n        \n        \n        normalized_capacities = suitable_bins_cap / np.max(bins_remain_cap)\n        \n        \n        combined_scores = (0.6 * normalized_diffs) + (0.4 * normalized_capacities)\n        \n        \n        max_score = np.max(combined_scores)\n        best_fit_indices_in_suitable = np.where(combined_scores == max_score)[0]\n        \n        original_indices = np.where(suitable_bins_mask)[0]\n        \n        for idx in best_fit_indices_in_suitable:\n            priorities[original_indices[idx]] = 1.0\n            \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 120.40465370320703,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response4.txt_stdout.txt",
    "code_path": "problem_iter12_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # Best Fit Component: Prioritize bins that leave the least remaining space\n        best_fit_diff = suitable_bins_cap - item\n        min_diff = np.min(best_fit_diff)\n        best_fit_scores = np.exp(-best_fit_diff / (np.mean(bins_remain_cap) + 1e-6)) # Exponential decay, scaled by average remaining capacity\n        \n        # First Fit Component (implicitly handled by order but can be boosted): Prioritize bins that are used earlier\n        # For online, earlier bins are those with lower indices. We can use inverse index for priority.\n        original_indices = np.where(suitable_bins_mask)[0]\n        first_fit_scores = 1.0 / (original_indices + 1.0) \n        \n        # Combine components with adaptive weighting\n        # Weighting can be adaptive based on the distribution of remaining capacities.\n        # If capacities are very spread out, Best Fit might be more important.\n        # If capacities are similar, First Fit might help with fragmentation.\n        \n        capacity_std = np.std(bins_remain_cap)\n        capacity_mean = np.mean(bins_remain_cap)\n        \n        # Heuristic weighting: more weight to first fit if std is low (bins are similar)\n        # more weight to best fit if std is high (bins are diverse)\n        ff_weight = np.exp(-capacity_std / (capacity_mean + 1e-6)) \n        bf_weight = 1.0 - ff_weight\n        \n        combined_scores = bf_weight * best_fit_scores + ff_weight * first_fit_scores\n        \n        # Normalize scores to be between 0 and 1\n        if np.max(combined_scores) > 0:\n            normalized_scores = combined_scores / np.max(combined_scores)\n        else:\n            normalized_scores = combined_scores # Should not happen if suitable bins exist\n\n        # Assign priorities to the original indices\n        priorities[original_indices] = normalized_scores\n        \n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.487435181491823,
    "cyclomatic_complexity": 3.0,
    "halstead": 230.0,
    "exec_success": true
  }
]