{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins by combining Best Fit with adaptive scaling based on bin usage.\n\n    This heuristic favors bins that offer the tightest fit, similar to Best Fit,\n    but also slightly down-weights bins that are already heavily used to encourage\n    opening new bins when appropriate.\n    \"\"\"\n    epsilon = 1e-6\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Best Fit component: inverse of the gap\n    gaps = suitable_bins_cap - item\n    best_fit_score = 1.0 / (gaps + epsilon)\n\n    # Adaptive component: penalize bins that are already close to full (high usage)\n    # This is a simple heuristic: lower priority for bins with less remaining capacity *relative* to the total capacity.\n    # For simplicity, we'll use the inverse of the remaining capacity itself as a penalty,\n    # effectively favoring bins that have more \"room\" in general, even if they don't offer the tightest fit.\n    # However, to combine with Best Fit, we'll apply a *decreasing* function to this.\n    # A simple way is to multiply by a factor that decreases with remaining capacity.\n    # Let's use remaining capacity itself, normalized and then inverted to make larger remaining capacity *less* penalized.\n    \n    # Normalize remaining capacities to get a sense of usage level (0 to 1)\n    # Use a small epsilon in denominator to avoid division by zero if all suitable bins are empty (though unlikely here)\n    max_cap_for_scaling = np.max(suitable_bins_cap) if np.max(suitable_bins_cap) > 0 else 1.0\n    normalized_remain_cap = suitable_bins_cap / max_cap_for_scaling\n    \n    # We want to slightly reduce priority for bins with very *high* normalized remaining capacity (meaning they are less used)\n    # Conversely, we want to slightly *increase* priority for bins with lower normalized remaining capacity (more used).\n    # A simple scaling factor that increases with normalized remaining capacity (i.e., decreases with usage)\n    # could be (1 - normalized_remain_cap). We'll use this as a multiplier.\n    # This factor will be close to 1 for bins with low remaining capacity (high usage)\n    # and close to 0 for bins with high remaining capacity (low usage).\n    # We want the opposite: high usage should have *higher* priority for the adaptive part.\n    # So, let's use `normalized_remain_cap` directly as a factor, which means higher remaining capacity gets lower priority.\n    # To make it work with multiplication, we want high usage -> high priority.\n    # Let's try: priority = best_fit * (1 - normalized_remain_cap)\n    # This would mean bins with lots of remaining space get penalized.\n    # If we want to encourage using bins that are *more* full (but still fit the item), we'd want to multiply by something that\n    # is *higher* for more used bins.\n    # Let's try: priority = best_fit * (1.0 - normalized_remain_cap + epsilon)\n    # This still penalizes bins with lots of remaining capacity.\n    \n    # A better adaptive component might be to prioritize bins that have a reasonable amount of remaining capacity,\n    # but not too much. This could be a Gaussian-like function centered around some ideal remaining capacity.\n    # For this version, let's stick to a simpler idea: Combine best-fit with a preference for not-too-empty bins.\n    # We can multiply the best_fit score by a factor that encourages bins that are not nearly empty.\n    # Let's use the proportion of the item size to the remaining capacity, capped.\n    # `item / suitable_bins_cap` -> ratio of item size to bin's remaining capacity. High value means tight fit.\n    # Let's try: adaptive_factor = suitable_bins_cap / (suitable_bins_cap + item) -> ratio of remaining capacity to total occupied space.\n    # We want bins that are *more full*, so we want a higher factor for bins with *less* remaining capacity (but still fitting).\n    # Let's try: adaptive_factor = 1.0 - (suitable_bins_cap / max_cap_for_scaling)\n    # This gives higher scores to bins with less remaining capacity.\n    \n    adaptive_factor = (1.0 - normalized_remain_cap) + epsilon\n    \n    combined_priorities = best_fit_score * adaptive_factor\n\n    priorities[suitable_bins_mask] = combined_priorities\n\n    # Normalize priorities to be between 0 and 1 for stability, if needed by downstream logic.\n    # This step is optional and depends on how the priorities are used.\n    # if np.any(priorities[suitable_bins_mask]):\n    #     min_p = np.min(priorities[suitable_bins_mask])\n    #     max_p = np.max(priorities[suitable_bins_mask])\n    #     if max_p - min_p > epsilon:\n    #         priorities[suitable_bins_mask] = (priorities[suitable_bins_mask] - min_p) / (max_p - min_p)\n    #     else:\n    #         priorities[suitable_bins_mask] = 0.5 # All suitable bins have same priority\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a penalty for overly large remaining capacity.\n    Prioritizes bins with minimal remaining space after packing, but penalizes\n    bins that would leave excessive space, promoting balanced utilization.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    \n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if valid_bins_remain_cap.size > 0:\n        # Calculate the remaining capacity after placing the item\n        remaining_after_fit = valid_bins_remain_cap - item\n        \n        # Best Fit component: prioritize smaller remaining space\n        # Use inverse of remaining space for higher scores for tighter fits.\n        epsilon = 1e-9\n        best_fit_scores = 1.0 / (remaining_after_fit + epsilon)\n        \n        # Introduce a penalty for leaving too much space (less than \"good enough\" fit)\n        # This component aims to avoid \"overly empty\" bins, promoting better overall distribution.\n        # We can use a sigmoid-like penalty: larger remaining_after_fit leads to a smaller score.\n        # Let's define a threshold for \"too much\" space, e.g., 30% of item size, or a fixed value.\n        # A simple approach is to scale the best_fit_scores down if remaining_after_fit is large.\n        \n        # Let's use a penalty that reduces the priority if remaining_after_fit is large.\n        # A linear penalty might be simple: score = best_fit_score * (1 - alpha * remaining_after_fit)\n        # Or, a capping mechanism: if remaining_after_fit > threshold, reduce score significantly.\n        \n        # Consider a strategy where we want bins that leave very little space,\n        # but also don't want bins that are *almost* empty after packing.\n        # The previous '1 / (remaining_after_fit + epsilon)' strongly favors very small gaps.\n        \n        # Let's refine the 'Best Fit' idea by adding a secondary objective:\n        # prefer bins that become 'moderately' full rather than 'almost completely' full\n        # IF there's a choice between a perfect fit and a slightly less perfect fit that still leaves\n        # a reasonable amount of space. This is tricky in online BPP.\n        \n        # A common strategy is to favor bins that are \"nearly full\" without being *too* full.\n        # The 'Best Fit' strategy by itself already does this by minimizing residual space.\n        \n        # Let's enhance the 'Best Fit' by considering the *original* bin capacity.\n        # Prioritize bins where (remaining_after_fit / original_bin_capacity) is minimized.\n        # This is essentially Best Fit Normalized.\n        \n        # Let's try a hybrid:\n        # 1. Base priority: 1.0 / (remaining_after_fit + epsilon) (Best Fit)\n        # 2. Add a penalty if remaining_after_fit is *too large* relative to the item size.\n        #    This prevents selecting a large bin for a small item just because it's the smallest available.\n        \n        # Let's define a \"good fit\" target, which is a small positive remaining capacity.\n        # We want to reward bins where remaining_after_fit is close to zero.\n        # Let's cap the penalty.\n        \n        # Example:\n        # Item = 0.4, Bins: [0.5, 0.6, 1.0]\n        # Remaining after fit: [0.1, 0.2, 0.6]\n        # Best fit scores: [10, 5, 1.67]\n        \n        # If we want to penalize leaving a lot of space (e.g., 0.6), we could do:\n        # If remaining_after_fit > K * item:\n        #   priority *= penalty_factor (e.g., 0.5)\n        \n        penalty_threshold_factor = 0.5 # Penalize if remaining space is more than 50% of item size\n        penalty_factor = 0.7 # Reduce priority by 30%\n        \n        penalty_mask = remaining_after_fit > (penalty_threshold_factor * item)\n        \n        # Apply the base best fit scores\n        combined_priorities = best_fit_scores.copy()\n        \n        # Apply penalty to bins that leave too much space\n        combined_priorities[penalty_mask] *= penalty_factor\n        \n        priorities[can_fit_mask] = combined_priorities\n    \n    return priorities\n\n### Analyze & experience\n- *   **Comparing (1st) vs (2nd):** Heuristic 1 uses a simple average of 'best fit' and 'slack' scores, aiming to balance immediate fit with fuller bins. Heuristic 2 combines 'best fit' with an 'almost full' boost, prioritizing very tight fits. Heuristic 1's approach seems more balanced by explicitly considering slack.\n*   **Comparing (2nd) vs (3rd):** Heuristic 2 is identical to Heuristic 1, but with different internal calculation names and a slightly different combination logic (addition vs. averaging). Heuristic 3 is identical to Heuristic 1. There seems to be a misunderstanding in the ranking or code provided, as 1, 2, and 3 are very similar. However, focusing on the *intended* logic of Heuristic 2 (adding `almost_full_boost`), it might overemphasize \"almost full\" bins compared to the balanced approach of Heuristic 1/3.\n*   **Comparing (3rd) vs (4th):** Heuristic 3 (and 1) uses a scoring system based on inverse capacities. Heuristic 4 implements a pure \"Best Fit\" by assigning a priority of 1.0 to bins with the minimum gap, and 0 otherwise. Heuristic 3's nuanced scoring is generally better than a binary Best Fit, as it allows for finer selection among good fits.\n*   **Comparing (4th) vs (5th):** Heuristic 4 is pure Best Fit. Heuristic 5 attempts to normalize Best Fit scores and add a \"fullness\" score, but the combination (addition) and re-normalization might lead to unintended weighting. Heuristic 4's simplicity and directness of Best Fit might be more robust than Heuristic 5's complex normalization and combination.\n*   **Comparing (5th) vs (6th):** Heuristic 5 attempts normalization and a weighted combination. Heuristic 6 is identical to Heuristic 4 (pure Best Fit). This reinforces that Heuristic 4 (pure Best Fit) is likely superior to Heuristic 5's complicated approach.\n*   **Comparing (7th) and (11th/12th/13th):** Heuristic 7 proposes a score of `(1.0 / gaps) - bins_that_can_fit_caps`. This attempts to combine Best Fit with a penalty for large bin capacity, which can be good. However, the direct subtraction might lead to negative priorities where the penalty outweighs the Best Fit score, making interpretation and comparison difficult. Heuristics 11-13 implement a similar idea with a conditional penalty for \"too much\" remaining space. This conditional penalty (Heuristics 11-13) is a more controlled way to handle the \"overly large remaining capacity\" issue than Heuristic 7's direct subtraction.\n*   **Comparing (8th) vs (2nd):** Heuristic 8 is identical to Heuristic 2.\n*   **Comparing (9th) vs (1st):** Heuristic 9 weights 'waste reduction' (similar to Best Fit) and 'capacity utilization' (inverse of remaining capacity). This is a reasonable hybrid. Heuristic 1 averages these concepts. Heuristic 9's weighted approach might offer better control than Heuristic 1's simple average.\n*   **Comparing (10th) vs (1st):** Heuristic 10 combines Best Fit with a factor that *reduces* priority for bins with *more* remaining capacity. This aims to penalize lightly used bins. Heuristic 1's \"slack score\" (inverse of remaining capacity) also favors fuller bins. Heuristic 10's multiplicative approach might be more effective at modulating Best Fit scores.\n*   **Comparing (14th-20th) vs (4th):** Heuristics 14-20 are identical. They combine a normalized Best Fit score with an exponential penalty for \"very empty\" bins based on a capacity ratio. This is a sophisticated approach that balances tight fits with avoiding overly empty bins. It's more nuanced than pure Best Fit (Heuristic 4).\n\nOverall: The top heuristics (1-3, 8-10) attempt to combine Best Fit with a secondary objective like favoring fuller bins or penalizing under-utilized bins using additive or multiplicative factors. Heuristics 14-20 represent a more refined version of this by using a normalized Best Fit score combined with an adaptive penalty. Heuristics 7 and 11-13 try to penalize large remaining capacities, but the implementation in 7 is problematic, while 11-13 are better. Pure Best Fit (4, 6) is simpler but less adaptive.\n- \nHere's a redefined self-reflection for designing better heuristics, focusing on actionable insights:\n\n*   **Keywords:** Hybrid heuristics, weighted combination, conditional logic, penalty mechanisms.\n*   **Advice:** Design hybrid heuristics that dynamically adjust their criteria. Explore conditional logic to switch between primary and secondary metrics based on the problem state (e.g., bin fullness).\n*   **Avoid:** Over-reliance on a single, static metric (like inverse of remaining gap) as it can be brittle. Avoid overly complex multiplicative combinations that are hard to tune.\n*   **Explanation:** Pure \"best fit\" is often suboptimal. Combining metrics thoughtfully, perhaps by penalizing certain outcomes or prioritizing states, leads to more robust and adaptable heuristics that can better navigate diverse problem instances.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}