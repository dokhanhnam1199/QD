{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for leaving excessive remaining capacity.\n    Prioritizes bins with minimal remaining space after packing, but penalizes\n    bins that would leave too much space, promoting balanced utilization.\n    This heuristic aims to provide a more balanced approach than pure Best Fit.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Consider only bins that can fit the item\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if valid_bins_remain_cap.size > 0:\n        # Calculate the remaining capacity after placing the item\n        remaining_after_fit = valid_bins_remain_cap - item\n        \n        # Best Fit component: prioritize bins with minimal remaining space.\n        # Use inverse of remaining space for higher scores for tighter fits.\n        # Add a small epsilon to avoid division by zero.\n        epsilon = 1e-9\n        best_fit_scores = 1.0 / (remaining_after_fit + epsilon)\n        \n        # Penalty component: penalize bins that leave a large amount of remaining space.\n        # This discourages selecting very large bins for small items if other options exist.\n        # We use a threshold based on the item size. If remaining_after_fit is larger than\n        # a multiple of the item size, apply a penalty.\n        penalty_threshold_factor = 1.0  # Penalize if remaining space > item size\n        penalty_factor = 0.5            # Reduce priority by 50%\n        \n        penalty_mask = remaining_after_fit > (penalty_threshold_factor * item)\n        \n        # Apply the penalty to the best_fit_scores\n        combined_priorities = best_fit_scores.copy()\n        combined_priorities[penalty_mask] *= penalty_factor\n        \n        # Normalize the combined scores to be between 0 and 1\n        # This ensures scores are comparable across different sets of bins.\n        max_priority = np.max(combined_priorities)\n        if max_priority > 0:\n            final_priorities = combined_priorities / max_priority\n        else:\n            final_priorities = np.zeros_like(combined_priorities)\n            \n        # Assign the calculated priorities back to the original indices\n        original_indices = np.where(can_fit_mask)[0]\n        priorities[original_indices] = final_priorities\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for leaving excessive space,\n    prioritizing tight fits while discouraging overly sparse bin utilization.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # Best Fit component: minimize remaining capacity after packing\n        remaining_after_fit = suitable_bins_cap - item\n        \n        # Score based on inverse of remaining space (higher is better)\n        # Add epsilon to avoid division by zero for perfect fits\n        epsilon = 1e-9\n        best_fit_scores = 1.0 / (remaining_after_fit + epsilon)\n        \n        # Penalty component: Discourage bins that leave a large amount of space\n        # This is a simplified approach inspired by (11th/12th/13th) and (10th) heuristics.\n        # We penalize if the remaining space is significantly larger than the item size.\n        # Let's use a threshold: if remaining_after_fit > factor * item, apply a reduction.\n        penalty_factor = 0.5  # Reduce priority by 50% if condition met\n        penalty_threshold_ratio = 1.5 # Apply penalty if remaining space > 1.5 * item size\n\n        penalty_mask = remaining_after_fit > (penalty_threshold_ratio * item)\n        \n        # Combine scores: Apply penalty multiplicatively\n        combined_scores = best_fit_scores.copy()\n        combined_scores[penalty_mask] *= penalty_factor\n        \n        # Normalize scores to be between 0 and 1 for better weighting/interpretation\n        # This normalization is inspired by (14th-20th) and (5th) heuristics.\n        if np.max(combined_scores) > 0:\n            normalized_scores = combined_scores / np.max(combined_scores)\n        else:\n            normalized_scores = np.zeros_like(combined_scores) # Should not happen if any suitable bin exists\n\n        # Assign priorities: use normalized scores for bins that can fit the item\n        original_indices = np.where(suitable_bins_mask)[0]\n        priorities[original_indices] = normalized_scores\n        \n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for bins that are too large,\n    prioritizing tighter fits while avoiding extremely large remaining capacities.\n    This heuristic balances minimizing leftover space with not opening excessively large bins.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    \n    bins_that_can_fit_caps = bins_remain_cap[can_fit_mask]\n    \n    if bins_that_can_fit_caps.size > 0:\n        gaps = bins_that_can_fit_caps - item\n        \n        # Best Fit component: prioritize bins with the smallest gap.\n        # Using 1/(gap + epsilon) gives higher scores to smaller gaps.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Additional component: Penalize bins that leave very large remaining capacity.\n        # This is achieved by subtracting a value that increases with the remaining capacity.\n        # A simple linear penalty is used here: -(bins_that_can_fit_caps).\n        # This favors using bins that are less empty among those that provide a good fit.\n        combined_scores = best_fit_scores - bins_that_can_fit_caps\n        \n        priorities[can_fit_mask] = combined_scores\n        \n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for excessively large remaining capacities,\n    prioritizing bins that fit the item tightly and are not overly empty.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n    \n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Best Fit component: inverse of the remaining gap after placing the item.\n    # Higher score for smaller gaps (tighter fits).\n    epsilon = 1e-9\n    best_fit_scores = 1.0 / (valid_bins_remain_cap - item + epsilon)\n    \n    # Penalty for \"too much\" remaining capacity.\n    # This penalizes bins that will be left significantly empty after the item is placed.\n    # We use a logarithmic penalty to be less aggressive than exponential.\n    # The penalty is higher for larger remaining capacities.\n    # We subtract 1 to make the penalty focus on capacity substantially larger than the item.\n    # If remaining capacity is close to item size, penalty is small.\n    # If remaining capacity is much larger, penalty is significant.\n    penalty_factor = 0.5 # Tunable parameter for penalty strength\n    # Avoid log(0) or log(negative) by ensuring argument is > 1\n    penalty_arg = (valid_bins_remain_cap / item) if item > 0 else np.inf\n    # We only want to penalize if remaining capacity is significantly larger than the item\n    # Let's define \"significantly larger\" as > 2 * item for instance.\n    # A simpler approach is to penalize based on the absolute remaining capacity if it's large.\n    # Or penalize based on the *proportion* of capacity left.\n    # Let's try penalizing remaining capacity relative to bin's original capacity (if known, but it's not).\n    # Instead, let's penalize based on (remaining_cap_after_fit - item).\n    # If remaining_cap_after_fit - item is large, we want a larger penalty.\n    # Using log((valid_bins_remain_cap - item) + epsilon) can work, but a simpler penalty might be better.\n    # Let's consider the \"slack\" as in priority_v0 but inverted, penalizing large slack.\n    # A linear penalty on slack: -(valid_bins_remain_cap - item)\n    # A better penalty based on \"over-emptiness\":\n    # Penalty is higher if (valid_bins_remain_cap - item) is large compared to 'item'.\n    # Let's adapt Heuristics 11-13 logic but make it simpler.\n    # Penalty is proportional to the remaining capacity AFTER the item is placed, if it's \"too much\".\n    # A threshold could be 'item', meaning if remaining capacity > item, penalize.\n    # Or if remaining capacity > some fraction of bin capacity (unknown).\n    # Let's try to penalize bins where `valid_bins_remain_cap - item` is large.\n    # We want to reduce priority for bins that will be left very empty.\n    # A simple penalty could be proportional to `valid_bins_remain_cap`.\n    # Or `valid_bins_remain_cap / item`.\n    # Let's use a penalty that decreases priority if remaining capacity is large.\n    # The inverse of remaining capacity was used in v0 for \"fullness\". Let's use that in reverse.\n    # Penalize if `valid_bins_remain_cap` is large.\n    # Penalty = f(valid_bins_remain_cap). We want f to be decreasing.\n    # Let's use a simple negative linear term on the remaining capacity itself.\n    # This is related to the 'slack_scores' in v0 but as a penalty.\n    # penalty = penalty_factor * valid_bins_remain_cap\n    \n    # Alternative penalty idea: penalize bins that are \"too empty\" in terms of how much\n    # larger their remaining capacity is compared to the item being packed.\n    # For example, if remaining_cap - item > item, then it's \"too empty\".\n    # Let's create a penalty term that is larger for larger remaining capacities.\n    # Using the inverse of remaining capacity from v0's slack_scores, but subtracting it.\n    # This prioritizes bins that will be fuller.\n    # `slack_scores` from v0: 1.0 / (valid_bins_remain_cap + epsilon)\n    # If we subtract this, we penalize fuller bins. This is not what we want.\n    # We want to penalize *emptier* bins.\n    # Let's try: penalty is proportional to the *amount of wasted space* in the bin *after* fitting.\n    # Wasted space = valid_bins_remain_cap - item\n    # Penalty = penalty_factor * (valid_bins_remain_cap - item)\n    # This would reduce priority for bins with larger remaining space.\n    \n    # Combine Best Fit with a penalty for bins that are likely to be left very empty.\n    # The penalty is stronger for bins with a larger remaining capacity after placing the item.\n    # We want to reduce the score if `valid_bins_remain_cap - item` is large.\n    # Let's use the score `(valid_bins_remain_cap - item)` directly as a penalty.\n    # This is effectively saying: `best_fit_score - penalty_factor * (remaining_capacity_after_fit)`\n    # Where `remaining_capacity_after_fit = valid_bins_remain_cap - item`.\n    \n    # Let's refine the penalty to be based on the \"emptiness ratio\" of the bin after packing.\n    # If a bin has capacity C and we place item I, remaining is C-I.\n    # If C-I is large, we penalize.\n    # Consider the capacity ratio of the *remaining space* to the *item size*.\n    # If `(valid_bins_remain_cap - item) / item` is large, we penalize.\n    # `penalty_term = penalty_factor * ((valid_bins_remain_cap - item) / item)`\n    # This handles cases where `item` is small, leading to large penalties if remaining is large.\n    # Let's clip this ratio to avoid extreme values, perhaps by limiting how much larger the remaining space can be.\n    # A simpler approach: penalize if `valid_bins_remain_cap` itself is large.\n    # Let's use the `slack_scores` from v0 but adjust the combination.\n    # `best_fit_scores` are good. We want to *decrease* priority if `valid_bins_remain_cap` is large.\n    # So, we can subtract a term that increases with `valid_bins_remain_cap`.\n    \n    # Let's combine Best Fit with a penalty for remaining capacity that's much larger than the item size.\n    # This aims to select bins that are \"almost full\" but can still accommodate the item.\n    # The \"gap\" is `valid_bins_remain_cap - item`. Best Fit prioritizes small gaps.\n    # The \"slack\" is `valid_bins_remain_cap`. We want to penalize large slack.\n    # Let's define a penalty that is larger for larger slack.\n    # A simple linear penalty: `penalty_amount = penalty_factor * valid_bins_remain_cap`\n    # This could be too aggressive.\n    \n    # Consider Heuristic 10's approach: `best_fit_score * (1 - penalty_weight * (remaining_capacity / bin_capacity))`\n    # Since bin_capacity is unknown, we can use `item` or a scaled `valid_bins_remain_cap`.\n    # Let's try: `best_fit_score * (1 - penalty_factor * (valid_bins_remain_cap / (item + epsilon)))`\n    # This would penalize bins where remaining capacity is large relative to item size.\n    # If `valid_bins_remain_cap < item`, the term `(valid_bins_remain_cap / (item + epsilon))` is < 1.\n    # `1 - penalty_factor * (...)` would be greater than `1 - penalty_factor`.\n    # This boosts bins that leave less space *relative to item size*.\n    # This is similar to boosting bins that are \"almost full\" when scaled by item size.\n    \n    # Let's combine the Best Fit score with a penalty that reduces priority for bins\n    # with a large amount of remaining capacity *after* the item is placed.\n    # We can use a term that is proportional to the remaining capacity itself.\n    # Penalty = `penalty_factor * (valid_bins_remain_cap - item)`\n    # This makes the priority: `best_fit_score - penalty_factor * (valid_bins_remain_cap - item)`\n    # This is equivalent to prioritizing bins with `1 / (gap) - penalty_factor * (gap)`.\n    \n    # Let's adopt a penalty based on the \"emptiness\" of the bin *after* placement.\n    # We want to penalize bins where `valid_bins_remain_cap` is large.\n    # A simple way is to subtract a fraction of `valid_bins_remain_cap`.\n    # The \"gap\" is `valid_bins_remain_cap - item`.\n    # Let's combine the inverse gap (best fit) with the inverse of remaining capacity (fullness).\n    # v0 used (best_fit + slack)/2.\n    # v1 uses normalized best_fit - penalty.\n    # Let's try a weighted sum of best_fit_scores and a penalty for slack.\n    # Penalty for slack: `penalty_factor * (valid_bins_remain_cap / (item + epsilon))`\n    # This penalizes bins where remaining capacity is large relative to item size.\n    # If `valid_bins_remain_cap = item`, penalty is `penalty_factor`.\n    # If `valid_bins_remain_cap = 2 * item`, penalty is `2 * penalty_factor`.\n    \n    # Final approach: combine the 'best fit' score (inverse gap) with a penalty\n    # that reduces priority for bins that will have a lot of remaining capacity.\n    # The penalty is linear with the remaining capacity `valid_bins_remain_cap`.\n    # This is a balance between tight packing and not leaving bins excessively empty.\n    \n    # Score = BestFit_Score - Penalty_for_Slack\n    # BestFit_Score = 1.0 / (valid_bins_remain_cap - item + epsilon)\n    # Penalty_for_Slack = penalty_factor * valid_bins_remain_cap\n    \n    # Let's try to boost bins that have a small remaining capacity after fitting,\n    # but also consider the \"tightness\" of the fit.\n    # Heuristic 14-20 approach: normalized_best_fit + penalty for empty bins.\n    # Let's simplify it: Best Fit score, but penalize if remaining capacity is very large.\n    \n    # Combining Best Fit (inverse gap) with a penalty for large remaining capacity.\n    # This aims to select bins that are a tight fit and do not leave excessive empty space.\n    # Penalty is applied if `valid_bins_remain_cap` is significantly larger than `item`.\n    \n    # Let's use the `best_fit_scores` and subtract a penalty proportional to the\n    # `valid_bins_remain_cap` to favor fuller bins.\n    penalty_strength = 0.2 # Tunable parameter for penalty\n    \n    # Calculate combined priorities: Best fit score minus a penalty for remaining capacity.\n    # This prioritizes bins that fit the item snugly (high best_fit_scores)\n    # and de-prioritizes bins that will have a lot of remaining space.\n    combined_priorities = best_fit_scores - penalty_strength * (valid_bins_remain_cap - item)\n    \n    # Assign the calculated priorities to the original indices\n    original_indices = np.where(can_fit_mask)[0]\n    priorities[original_indices] = combined_priorities\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for leaving excessive remaining capacity.\n    Prioritizes bins with minimal remaining space after packing, but penalizes\n    bins that would leave too much space, promoting balanced utilization.\n    This heuristic aims to provide a more balanced approach than pure Best Fit.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Consider only bins that can fit the item\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if valid_bins_remain_cap.size > 0:\n        # Calculate the remaining capacity after placing the item\n        remaining_after_fit = valid_bins_remain_cap - item\n        \n        # Best Fit component: prioritize bins with minimal remaining space.\n        # Use inverse of remaining space for higher scores for tighter fits.\n        # Add a small epsilon to avoid division by zero.\n        epsilon = 1e-9\n        best_fit_scores = 1.0 / (remaining_after_fit + epsilon)\n        \n        # Penalty component: penalize bins that leave a large amount of remaining space.\n        # This discourages selecting very large bins for small items if other options exist.\n        # We use a threshold based on the item size. If remaining_after_fit is larger than\n        # a multiple of the item size, apply a penalty.\n        penalty_threshold_factor = 1.0  # Penalize if remaining space > item size\n        penalty_factor = 0.5            # Reduce priority by 50%\n        \n        penalty_mask = remaining_after_fit > (penalty_threshold_factor * item)\n        \n        # Apply the penalty to the best_fit_scores\n        combined_priorities = best_fit_scores.copy()\n        combined_priorities[penalty_mask] *= penalty_factor\n        \n        # Normalize the combined scores to be between 0 and 1\n        # This ensures scores are comparable across different sets of bins.\n        max_priority = np.max(combined_priorities)\n        if max_priority > 0:\n            final_priorities = combined_priorities / max_priority\n        else:\n            final_priorities = np.zeros_like(combined_priorities)\n            \n        # Assign the calculated priorities back to the original indices\n        original_indices = np.where(can_fit_mask)[0]\n        priorities[original_indices] = final_priorities\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        best_fit_diff = suitable_bins_cap - item\n        \n        min_diff = np.min(best_fit_diff)\n        \n        best_fit_indices = np.where(best_fit_diff == min_diff)[0]\n        \n        original_indices = np.where(suitable_bins_mask)[0]\n        \n        for idx in best_fit_indices:\n            priorities[original_indices[idx]] = 1.0\n    \n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for excessively large remaining capacities,\n    prioritizing bins that fit the item tightly and are not overly empty.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n    \n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Best Fit component: inverse of the remaining gap after placing the item.\n    # Higher score for smaller gaps (tighter fits).\n    epsilon = 1e-9\n    best_fit_scores = 1.0 / (valid_bins_remain_cap - item + epsilon)\n    \n    # Penalty for \"too much\" remaining capacity.\n    # This penalizes bins that will be left significantly empty after the item is placed.\n    # We use a logarithmic penalty to be less aggressive than exponential.\n    # The penalty is higher for larger remaining capacities.\n    # We subtract 1 to make the penalty focus on capacity substantially larger than the item.\n    # If remaining capacity is close to item size, penalty is small.\n    # If remaining capacity is much larger, penalty is significant.\n    penalty_factor = 0.5 # Tunable parameter for penalty strength\n    # Avoid log(0) or log(negative) by ensuring argument is > 1\n    penalty_arg = (valid_bins_remain_cap / item) if item > 0 else np.inf\n    # We only want to penalize if remaining capacity is significantly larger than the item\n    # Let's define \"significantly larger\" as > 2 * item for instance.\n    # A simpler approach is to penalize based on the absolute remaining capacity if it's large.\n    # Or penalize based on the *proportion* of capacity left.\n    # Let's try penalizing remaining capacity relative to bin's original capacity (if known, but it's not).\n    # Instead, let's penalize based on (remaining_cap_after_fit - item).\n    # If remaining_cap_after_fit - item is large, we want a larger penalty.\n    # Using log((valid_bins_remain_cap - item) + epsilon) can work, but a simpler penalty might be better.\n    # Let's consider the \"slack\" as in priority_v0 but inverted, penalizing large slack.\n    # A linear penalty on slack: -(valid_bins_remain_cap - item)\n    # A better penalty based on \"over-emptiness\":\n    # Penalty is higher if (valid_bins_remain_cap - item) is large compared to 'item'.\n    # Let's adapt Heuristics 11-13 logic but make it simpler.\n    # Penalty is proportional to the remaining capacity AFTER the item is placed, if it's \"too much\".\n    # A threshold could be 'item', meaning if remaining capacity > item, penalize.\n    # Or if remaining capacity > some fraction of bin capacity (unknown).\n    # Let's try to penalize bins where `valid_bins_remain_cap - item` is large.\n    # We want to reduce priority for bins that will be left very empty.\n    # A simple penalty could be proportional to `valid_bins_remain_cap`.\n    # Or `valid_bins_remain_cap / item`.\n    # Let's use a penalty that decreases priority if remaining capacity is large.\n    # The inverse of remaining capacity was used in v0 for \"fullness\". Let's use that in reverse.\n    # Penalize if `valid_bins_remain_cap` is large.\n    # Penalty = f(valid_bins_remain_cap). We want f to be decreasing.\n    # Let's use a simple negative linear term on the remaining capacity itself.\n    # This is related to the 'slack_scores' in v0 but as a penalty.\n    # penalty = penalty_factor * valid_bins_remain_cap\n    \n    # Alternative penalty idea: penalize bins that are \"too empty\" in terms of how much\n    # larger their remaining capacity is compared to the item being packed.\n    # For example, if remaining_cap - item > item, then it's \"too empty\".\n    # Let's create a penalty term that is larger for larger remaining capacities.\n    # Using the inverse of remaining capacity from v0's slack_scores, but subtracting it.\n    # This prioritizes bins that will be fuller.\n    # `slack_scores` from v0: 1.0 / (valid_bins_remain_cap + epsilon)\n    # If we subtract this, we penalize fuller bins. This is not what we want.\n    # We want to penalize *emptier* bins.\n    # Let's try: penalty is proportional to the *amount of wasted space* in the bin *after* fitting.\n    # Wasted space = valid_bins_remain_cap - item\n    # Penalty = penalty_factor * (valid_bins_remain_cap - item)\n    # This would reduce priority for bins with larger remaining space.\n    \n    # Combine Best Fit with a penalty for bins that are likely to be left very empty.\n    # The penalty is stronger for bins with a larger remaining capacity after placing the item.\n    # We want to reduce the score if `valid_bins_remain_cap - item` is large.\n    # Let's use the score `(valid_bins_remain_cap - item)` directly as a penalty.\n    # This is effectively saying: `best_fit_score - penalty_factor * (remaining_capacity_after_fit)`\n    # Where `remaining_capacity_after_fit = valid_bins_remain_cap - item`.\n    \n    # Let's refine the penalty to be based on the \"emptiness ratio\" of the bin after packing.\n    # If a bin has capacity C and we place item I, remaining is C-I.\n    # If C-I is large, we penalize.\n    # Consider the capacity ratio of the *remaining space* to the *item size*.\n    # If `(valid_bins_remain_cap - item) / item` is large, we penalize.\n    # `penalty_term = penalty_factor * ((valid_bins_remain_cap - item) / item)`\n    # This handles cases where `item` is small, leading to large penalties if remaining is large.\n    # Let's clip this ratio to avoid extreme values, perhaps by limiting how much larger the remaining space can be.\n    # A simpler approach: penalize if `valid_bins_remain_cap` itself is large.\n    # Let's use the `slack_scores` from v0 but adjust the combination.\n    # `best_fit_scores` are good. We want to *decrease* priority if `valid_bins_remain_cap` is large.\n    # So, we can subtract a term that increases with `valid_bins_remain_cap`.\n    \n    # Let's combine Best Fit with a penalty for remaining capacity that's much larger than the item size.\n    # This aims to select bins that are \"almost full\" but can still accommodate the item.\n    # The \"gap\" is `valid_bins_remain_cap - item`. Best Fit prioritizes small gaps.\n    # The \"slack\" is `valid_bins_remain_cap`. We want to penalize large slack.\n    # Let's define a penalty that is larger for larger slack.\n    # A simple linear penalty: `penalty_amount = penalty_factor * valid_bins_remain_cap`\n    # This could be too aggressive.\n    \n    # Consider Heuristic 10's approach: `best_fit_score * (1 - penalty_weight * (remaining_capacity / bin_capacity))`\n    # Since bin_capacity is unknown, we can use `item` or a scaled `valid_bins_remain_cap`.\n    # Let's try: `best_fit_score * (1 - penalty_factor * (valid_bins_remain_cap / (item + epsilon)))`\n    # This would penalize bins where remaining capacity is large relative to item size.\n    # If `valid_bins_remain_cap < item`, the term `(valid_bins_remain_cap / (item + epsilon))` is < 1.\n    # `1 - penalty_factor * (...)` would be greater than `1 - penalty_factor`.\n    # This boosts bins that leave less space *relative to item size*.\n    # This is similar to boosting bins that are \"almost full\" when scaled by item size.\n    \n    # Let's combine the Best Fit score with a penalty that reduces priority for bins\n    # with a large amount of remaining capacity *after* the item is placed.\n    # We can use a term that is proportional to the remaining capacity itself.\n    # Penalty = `penalty_factor * (valid_bins_remain_cap - item)`\n    # This makes the priority: `best_fit_score - penalty_factor * (valid_bins_remain_cap - item)`\n    # This is equivalent to prioritizing bins with `1 / (gap) - penalty_factor * (gap)`.\n    \n    # Let's adopt a penalty based on the \"emptiness\" of the bin *after* placement.\n    # We want to penalize bins where `valid_bins_remain_cap` is large.\n    # A simple way is to subtract a fraction of `valid_bins_remain_cap`.\n    # The \"gap\" is `valid_bins_remain_cap - item`.\n    # Let's combine the inverse gap (best fit) with the inverse of remaining capacity (fullness).\n    # v0 used (best_fit + slack)/2.\n    # v1 uses normalized best_fit - penalty.\n    # Let's try a weighted sum of best_fit_scores and a penalty for slack.\n    # Penalty for slack: `penalty_factor * (valid_bins_remain_cap / (item + epsilon))`\n    # This penalizes bins where remaining capacity is large relative to item size.\n    # If `valid_bins_remain_cap = item`, penalty is `penalty_factor`.\n    # If `valid_bins_remain_cap = 2 * item`, penalty is `2 * penalty_factor`.\n    \n    # Final approach: combine the 'best fit' score (inverse gap) with a penalty\n    # that reduces priority for bins that will have a lot of remaining capacity.\n    # The penalty is linear with the remaining capacity `valid_bins_remain_cap`.\n    # This is a balance between tight packing and not leaving bins excessively empty.\n    \n    # Score = BestFit_Score - Penalty_for_Slack\n    # BestFit_Score = 1.0 / (valid_bins_remain_cap - item + epsilon)\n    # Penalty_for_Slack = penalty_factor * valid_bins_remain_cap\n    \n    # Let's try to boost bins that have a small remaining capacity after fitting,\n    # but also consider the \"tightness\" of the fit.\n    # Heuristic 14-20 approach: normalized_best_fit + penalty for empty bins.\n    # Let's simplify it: Best Fit score, but penalize if remaining capacity is very large.\n    \n    # Combining Best Fit (inverse gap) with a penalty for large remaining capacity.\n    # This aims to select bins that are a tight fit and do not leave excessive empty space.\n    # Penalty is applied if `valid_bins_remain_cap` is significantly larger than `item`.\n    \n    # Let's use the `best_fit_scores` and subtract a penalty proportional to the\n    # `valid_bins_remain_cap` to favor fuller bins.\n    penalty_strength = 0.2 # Tunable parameter for penalty\n    \n    # Calculate combined priorities: Best fit score minus a penalty for remaining capacity.\n    # This prioritizes bins that fit the item snugly (high best_fit_scores)\n    # and de-prioritizes bins that will have a lot of remaining space.\n    combined_priorities = best_fit_scores - penalty_strength * (valid_bins_remain_cap - item)\n    \n    # Assign the calculated priorities to the original indices\n    original_indices = np.where(can_fit_mask)[0]\n    priorities[original_indices] = combined_priorities\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for leaving excessive space,\n    prioritizing tight fits while discouraging overly sparse bin utilization.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # Best Fit component: minimize remaining capacity after packing\n        remaining_after_fit = suitable_bins_cap - item\n        \n        # Score based on inverse of remaining space (higher is better)\n        # Add epsilon to avoid division by zero for perfect fits\n        epsilon = 1e-9\n        best_fit_scores = 1.0 / (remaining_after_fit + epsilon)\n        \n        # Penalty component: Discourage bins that leave a large amount of space\n        # This is a simplified approach inspired by (11th/12th/13th) and (10th) heuristics.\n        # We penalize if the remaining space is significantly larger than the item size.\n        # Let's use a threshold: if remaining_after_fit > factor * item, apply a reduction.\n        penalty_factor = 0.5  # Reduce priority by 50% if condition met\n        penalty_threshold_ratio = 1.5 # Apply penalty if remaining space > 1.5 * item size\n\n        penalty_mask = remaining_after_fit > (penalty_threshold_ratio * item)\n        \n        # Combine scores: Apply penalty multiplicatively\n        combined_scores = best_fit_scores.copy()\n        combined_scores[penalty_mask] *= penalty_factor\n        \n        # Normalize scores to be between 0 and 1 for better weighting/interpretation\n        # This normalization is inspired by (14th-20th) and (5th) heuristics.\n        if np.max(combined_scores) > 0:\n            normalized_scores = combined_scores / np.max(combined_scores)\n        else:\n            normalized_scores = np.zeros_like(combined_scores) # Should not happen if any suitable bin exists\n\n        # Assign priorities: use normalized scores for bins that can fit the item\n        original_indices = np.where(suitable_bins_mask)[0]\n        priorities[original_indices] = normalized_scores\n        \n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for excessively large remaining capacities,\n    prioritizing bins that fit the item tightly and are not overly empty.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n    \n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Best Fit component: inverse of the remaining gap after placing the item.\n    # Higher score for smaller gaps (tighter fits).\n    epsilon = 1e-9\n    best_fit_scores = 1.0 / (valid_bins_remain_cap - item + epsilon)\n    \n    # Penalty for \"too much\" remaining capacity.\n    # This penalizes bins that will be left significantly empty after the item is placed.\n    # We use a logarithmic penalty to be less aggressive than exponential.\n    # The penalty is higher for larger remaining capacities.\n    # We subtract 1 to make the penalty focus on capacity substantially larger than the item.\n    # If remaining capacity is close to item size, penalty is small.\n    # If remaining capacity is much larger, penalty is significant.\n    penalty_factor = 0.5 # Tunable parameter for penalty strength\n    # Avoid log(0) or log(negative) by ensuring argument is > 1\n    penalty_arg = (valid_bins_remain_cap / item) if item > 0 else np.inf\n    # We only want to penalize if remaining capacity is significantly larger than the item\n    # Let's define \"significantly larger\" as > 2 * item for instance.\n    # A simpler approach is to penalize based on the absolute remaining capacity if it's large.\n    # Or penalize based on the *proportion* of capacity left.\n    # Let's try penalizing remaining capacity relative to bin's original capacity (if known, but it's not).\n    # Instead, let's penalize based on (remaining_cap_after_fit - item).\n    # If remaining_cap_after_fit - item is large, we want a larger penalty.\n    # Using log((valid_bins_remain_cap - item) + epsilon) can work, but a simpler penalty might be better.\n    # Let's consider the \"slack\" as in priority_v0 but inverted, penalizing large slack.\n    # A linear penalty on slack: -(valid_bins_remain_cap - item)\n    # A better penalty based on \"over-emptiness\":\n    # Penalty is higher if (valid_bins_remain_cap - item) is large compared to 'item'.\n    # Let's adapt Heuristics 11-13 logic but make it simpler.\n    # Penalty is proportional to the remaining capacity AFTER the item is placed, if it's \"too much\".\n    # A threshold could be 'item', meaning if remaining capacity > item, penalize.\n    # Or if remaining capacity > some fraction of bin capacity (unknown).\n    # Let's try to penalize bins where `valid_bins_remain_cap - item` is large.\n    # We want to reduce priority for bins that will be left very empty.\n    # A simple penalty could be proportional to `valid_bins_remain_cap`.\n    # Or `valid_bins_remain_cap / item`.\n    # Let's use a penalty that decreases priority if remaining capacity is large.\n    # The inverse of remaining capacity was used in v0 for \"fullness\". Let's use that in reverse.\n    # Penalize if `valid_bins_remain_cap` is large.\n    # Penalty = f(valid_bins_remain_cap). We want f to be decreasing.\n    # Let's use a simple negative linear term on the remaining capacity itself.\n    # This is related to the 'slack_scores' in v0 but as a penalty.\n    # penalty = penalty_factor * valid_bins_remain_cap\n    \n    # Alternative penalty idea: penalize bins that are \"too empty\" in terms of how much\n    # larger their remaining capacity is compared to the item being packed.\n    # For example, if remaining_cap - item > item, then it's \"too empty\".\n    # Let's create a penalty term that is larger for larger remaining capacities.\n    # Using the inverse of remaining capacity from v0's slack_scores, but subtracting it.\n    # This prioritizes bins that will be fuller.\n    # `slack_scores` from v0: 1.0 / (valid_bins_remain_cap + epsilon)\n    # If we subtract this, we penalize fuller bins. This is not what we want.\n    # We want to penalize *emptier* bins.\n    # Let's try: penalty is proportional to the *amount of wasted space* in the bin *after* fitting.\n    # Wasted space = valid_bins_remain_cap - item\n    # Penalty = penalty_factor * (valid_bins_remain_cap - item)\n    # This would reduce priority for bins with larger remaining space.\n    \n    # Combine Best Fit with a penalty for bins that are likely to be left very empty.\n    # The penalty is stronger for bins with a larger remaining capacity after placing the item.\n    # We want to reduce the score if `valid_bins_remain_cap - item` is large.\n    # Let's use the score `(valid_bins_remain_cap - item)` directly as a penalty.\n    # This is effectively saying: `best_fit_score - penalty_factor * (remaining_capacity_after_fit)`\n    # Where `remaining_capacity_after_fit = valid_bins_remain_cap - item`.\n    \n    # Let's refine the penalty to be based on the \"emptiness ratio\" of the bin after packing.\n    # If a bin has capacity C and we place item I, remaining is C-I.\n    # If C-I is large, we penalize.\n    # Consider the capacity ratio of the *remaining space* to the *item size*.\n    # If `(valid_bins_remain_cap - item) / item` is large, we penalize.\n    # `penalty_term = penalty_factor * ((valid_bins_remain_cap - item) / item)`\n    # This handles cases where `item` is small, leading to large penalties if remaining is large.\n    # Let's clip this ratio to avoid extreme values, perhaps by limiting how much larger the remaining space can be.\n    # A simpler approach: penalize if `valid_bins_remain_cap` itself is large.\n    # Let's use the `slack_scores` from v0 but adjust the combination.\n    # `best_fit_scores` are good. We want to *decrease* priority if `valid_bins_remain_cap` is large.\n    # So, we can subtract a term that increases with `valid_bins_remain_cap`.\n    \n    # Let's combine Best Fit with a penalty for remaining capacity that's much larger than the item size.\n    # This aims to select bins that are \"almost full\" but can still accommodate the item.\n    # The \"gap\" is `valid_bins_remain_cap - item`. Best Fit prioritizes small gaps.\n    # The \"slack\" is `valid_bins_remain_cap`. We want to penalize large slack.\n    # Let's define a penalty that is larger for larger slack.\n    # A simple linear penalty: `penalty_amount = penalty_factor * valid_bins_remain_cap`\n    # This could be too aggressive.\n    \n    # Consider Heuristic 10's approach: `best_fit_score * (1 - penalty_weight * (remaining_capacity / bin_capacity))`\n    # Since bin_capacity is unknown, we can use `item` or a scaled `valid_bins_remain_cap`.\n    # Let's try: `best_fit_score * (1 - penalty_factor * (valid_bins_remain_cap / (item + epsilon)))`\n    # This would penalize bins where remaining capacity is large relative to item size.\n    # If `valid_bins_remain_cap < item`, the term `(valid_bins_remain_cap / (item + epsilon))` is < 1.\n    # `1 - penalty_factor * (...)` would be greater than `1 - penalty_factor`.\n    # This boosts bins that leave less space *relative to item size*.\n    # This is similar to boosting bins that are \"almost full\" when scaled by item size.\n    \n    # Let's combine the Best Fit score with a penalty that reduces priority for bins\n    # with a large amount of remaining capacity *after* the item is placed.\n    # We can use a term that is proportional to the remaining capacity itself.\n    # Penalty = `penalty_factor * (valid_bins_remain_cap - item)`\n    # This makes the priority: `best_fit_score - penalty_factor * (valid_bins_remain_cap - item)`\n    # This is equivalent to prioritizing bins with `1 / (gap) - penalty_factor * (gap)`.\n    \n    # Let's adopt a penalty based on the \"emptiness\" of the bin *after* placement.\n    # We want to penalize bins where `valid_bins_remain_cap` is large.\n    # A simple way is to subtract a fraction of `valid_bins_remain_cap`.\n    # The \"gap\" is `valid_bins_remain_cap - item`.\n    # Let's combine the inverse gap (best fit) with the inverse of remaining capacity (fullness).\n    # v0 used (best_fit + slack)/2.\n    # v1 uses normalized best_fit - penalty.\n    # Let's try a weighted sum of best_fit_scores and a penalty for slack.\n    # Penalty for slack: `penalty_factor * (valid_bins_remain_cap / (item + epsilon))`\n    # This penalizes bins where remaining capacity is large relative to item size.\n    # If `valid_bins_remain_cap = item`, penalty is `penalty_factor`.\n    # If `valid_bins_remain_cap = 2 * item`, penalty is `2 * penalty_factor`.\n    \n    # Final approach: combine the 'best fit' score (inverse gap) with a penalty\n    # that reduces priority for bins that will have a lot of remaining capacity.\n    # The penalty is linear with the remaining capacity `valid_bins_remain_cap`.\n    # This is a balance between tight packing and not leaving bins excessively empty.\n    \n    # Score = BestFit_Score - Penalty_for_Slack\n    # BestFit_Score = 1.0 / (valid_bins_remain_cap - item + epsilon)\n    # Penalty_for_Slack = penalty_factor * valid_bins_remain_cap\n    \n    # Let's try to boost bins that have a small remaining capacity after fitting,\n    # but also consider the \"tightness\" of the fit.\n    # Heuristic 14-20 approach: normalized_best_fit + penalty for empty bins.\n    # Let's simplify it: Best Fit score, but penalize if remaining capacity is very large.\n    \n    # Combining Best Fit (inverse gap) with a penalty for large remaining capacity.\n    # This aims to select bins that are a tight fit and do not leave excessive empty space.\n    # Penalty is applied if `valid_bins_remain_cap` is significantly larger than `item`.\n    \n    # Let's use the `best_fit_scores` and subtract a penalty proportional to the\n    # `valid_bins_remain_cap` to favor fuller bins.\n    penalty_strength = 0.2 # Tunable parameter for penalty\n    \n    # Calculate combined priorities: Best fit score minus a penalty for remaining capacity.\n    # This prioritizes bins that fit the item snugly (high best_fit_scores)\n    # and de-prioritizes bins that will have a lot of remaining space.\n    combined_priorities = best_fit_scores - penalty_strength * (valid_bins_remain_cap - item)\n    \n    # Assign the calculated priorities to the original indices\n    original_indices = np.where(can_fit_mask)[0]\n    priorities[original_indices] = combined_priorities\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for bins that are too large,\n    prioritizing tighter fits while avoiding extremely large remaining capacities.\n    This heuristic balances minimizing leftover space with not opening excessively large bins.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    \n    bins_that_can_fit_caps = bins_remain_cap[can_fit_mask]\n    \n    if bins_that_can_fit_caps.size > 0:\n        gaps = bins_that_can_fit_caps - item\n        \n        # Best Fit component: prioritize bins with the smallest gap.\n        # Using 1/(gap + epsilon) gives higher scores to smaller gaps.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Additional component: Penalize bins that leave very large remaining capacity.\n        # This is achieved by subtracting a value that increases with the remaining capacity.\n        # A simple linear penalty is used here: -(bins_that_can_fit_caps).\n        # This favors using bins that are less empty among those that provide a good fit.\n        combined_scores = best_fit_scores - bins_that_can_fit_caps\n        \n        priorities[can_fit_mask] = combined_scores\n        \n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for excessive remaining capacity,\n    favoring bins that are nearly full without being wasteful.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[can_fit_mask]\n    gaps = suitable_bins_caps - item\n\n    # Best Fit Score: Higher for smaller gaps (tighter fit)\n    # Add epsilon to prevent division by zero for perfect fits.\n    best_fit_score = 1.0 / (gaps + 1e-6)\n\n    # Penalty for \"too much\" remaining capacity:\n    # Favor bins that don't have a lot of leftover space after packing.\n    # This is a modification inspired by Heuristic 7's idea, but controlled.\n    # We penalize bins where the remaining capacity (after packing) is much larger than the item.\n    # Using log helps to dampen the effect of very large capacities.\n    # A small constant is added to the denominator to prevent log(0) and negative values.\n    # Multiplying by a factor (e.g., 0.5) controls the strength of the penalty.\n    penalty_factor = 0.5\n    # Ensure the term inside log is always positive\n    penalty_term = np.maximum(gaps, 1e-3)\n    excess_capacity_penalty = penalty_factor * np.log(penalty_term + 1e-3) # Added epsilon inside log\n\n    # Combine scores: Subtract the penalty from the best-fit score.\n    # Higher combined score means a better bin choice.\n    combined_priorities = best_fit_score - excess_capacity_penalty\n\n    priorities[can_fit_mask] = combined_priorities\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for excessive remaining capacity,\n    favoring bins that are nearly full without being wasteful.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[can_fit_mask]\n    gaps = suitable_bins_caps - item\n\n    # Best Fit Score: Higher for smaller gaps (tighter fit)\n    # Add epsilon to prevent division by zero for perfect fits.\n    best_fit_score = 1.0 / (gaps + 1e-6)\n\n    # Penalty for \"too much\" remaining capacity:\n    # Favor bins that don't have a lot of leftover space after packing.\n    # This is a modification inspired by Heuristic 7's idea, but controlled.\n    # We penalize bins where the remaining capacity (after packing) is much larger than the item.\n    # Using log helps to dampen the effect of very large capacities.\n    # A small constant is added to the denominator to prevent log(0) and negative values.\n    # Multiplying by a factor (e.g., 0.5) controls the strength of the penalty.\n    penalty_factor = 0.5\n    # Ensure the term inside log is always positive\n    penalty_term = np.maximum(gaps, 1e-3)\n    excess_capacity_penalty = penalty_factor * np.log(penalty_term + 1e-3) # Added epsilon inside log\n\n    # Combine scores: Subtract the penalty from the best-fit score.\n    # Higher combined score means a better bin choice.\n    combined_priorities = best_fit_score - excess_capacity_penalty\n\n    priorities[can_fit_mask] = combined_priorities\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a penalty for excessively large remaining capacity.\n\n    Prioritizes bins that offer a tight fit, but penalizes bins that, after\n    packing, would still have a significantly larger remaining capacity than\n    the item itself, promoting better overall bin utilization.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        suitable_bins_caps = bins_remain_cap[can_fit_mask]\n        \n        # Best Fit component: inverse of the remaining capacity after packing\n        # Higher score for smaller remaining capacity\n        fit_scores = 1.0 / (suitable_bins_caps - item + 1e-9)\n        \n        # Penalty component: penalize bins that, after packing, still have a\n        # much larger capacity than the item. This encourages filling bins more\n        # completely rather than leaving large gaps in partially filled bins.\n        # The penalty is higher for bins with a larger ratio of remaining capacity\n        # to the item size. We use an exponential decay to make the penalty\n        # significant only for substantially larger remaining capacities.\n        capacity_ratio = (suitable_bins_caps - item) / item if item > 0 else np.inf\n        penalty = np.exp(-0.5 * capacity_ratio) * 0.5 # Tunable penalty factor (0.5)\n        \n        # Combine scores: Best Fit score minus the penalty\n        # Higher values indicate better priority\n        combined_priorities = fit_scores - penalty\n        \n        priorities[can_fit_mask] = combined_priorities\n        \n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a penalty for excessively large remaining capacity.\n\n    Prioritizes bins that offer a tight fit, but penalizes bins that, after\n    packing, would still have a significantly larger remaining capacity than\n    the item itself, promoting better overall bin utilization.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        suitable_bins_caps = bins_remain_cap[can_fit_mask]\n        \n        # Best Fit component: inverse of the remaining capacity after packing\n        # Higher score for smaller remaining capacity\n        fit_scores = 1.0 / (suitable_bins_caps - item + 1e-9)\n        \n        # Penalty component: penalize bins that, after packing, still have a\n        # much larger capacity than the item. This encourages filling bins more\n        # completely rather than leaving large gaps in partially filled bins.\n        # The penalty is higher for bins with a larger ratio of remaining capacity\n        # to the item size. We use an exponential decay to make the penalty\n        # significant only for substantially larger remaining capacities.\n        capacity_ratio = (suitable_bins_caps - item) / item if item > 0 else np.inf\n        penalty = np.exp(-0.5 * capacity_ratio) * 0.5 # Tunable penalty factor (0.5)\n        \n        # Combine scores: Best Fit score minus the penalty\n        # Higher values indicate better priority\n        combined_priorities = fit_scores - penalty\n        \n        priorities[can_fit_mask] = combined_priorities\n        \n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a penalty for excessively large remaining capacity.\n\n    Prioritizes bins that offer a tight fit, but penalizes bins that, after\n    packing, would still have a significantly larger remaining capacity than\n    the item itself, promoting better overall bin utilization.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        suitable_bins_caps = bins_remain_cap[can_fit_mask]\n        \n        # Best Fit component: inverse of the remaining capacity after packing\n        # Higher score for smaller remaining capacity\n        fit_scores = 1.0 / (suitable_bins_caps - item + 1e-9)\n        \n        # Penalty component: penalize bins that, after packing, still have a\n        # much larger capacity than the item. This encourages filling bins more\n        # completely rather than leaving large gaps in partially filled bins.\n        # The penalty is higher for bins with a larger ratio of remaining capacity\n        # to the item size. We use an exponential decay to make the penalty\n        # significant only for substantially larger remaining capacities.\n        capacity_ratio = (suitable_bins_caps - item) / item if item > 0 else np.inf\n        penalty = np.exp(-0.5 * capacity_ratio) * 0.5 # Tunable penalty factor (0.5)\n        \n        # Combine scores: Best Fit score minus the penalty\n        # Higher values indicate better priority\n        combined_priorities = fit_scores - penalty\n        \n        priorities[can_fit_mask] = combined_priorities\n        \n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a penalty for excessively large remaining capacity.\n\n    Prioritizes bins that offer a tight fit, but penalizes bins that, after\n    packing, would still have a significantly larger remaining capacity than\n    the item itself, promoting better overall bin utilization.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        suitable_bins_caps = bins_remain_cap[can_fit_mask]\n        \n        # Best Fit component: inverse of the remaining capacity after packing\n        # Higher score for smaller remaining capacity\n        fit_scores = 1.0 / (suitable_bins_caps - item + 1e-9)\n        \n        # Penalty component: penalize bins that, after packing, still have a\n        # much larger capacity than the item. This encourages filling bins more\n        # completely rather than leaving large gaps in partially filled bins.\n        # The penalty is higher for bins with a larger ratio of remaining capacity\n        # to the item size. We use an exponential decay to make the penalty\n        # significant only for substantially larger remaining capacities.\n        capacity_ratio = (suitable_bins_caps - item) / item if item > 0 else np.inf\n        penalty = np.exp(-0.5 * capacity_ratio) * 0.5 # Tunable penalty factor (0.5)\n        \n        # Combine scores: Best Fit score minus the penalty\n        # Higher values indicate better priority\n        combined_priorities = fit_scores - penalty\n        \n        priorities[can_fit_mask] = combined_priorities\n        \n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, penalty_strength: float = 0.004218314821270774, epsilon: float = 7.704559280250573e-09) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for excessively large remaining capacities,\n    prioritizing bins that fit the item tightly and are not overly empty.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by combining Best Fit with a penalty for very large remaining capacities.\n    Favors bins that are a tight fit and discourages using bins that are excessively underfilled.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # Best Fit component: Smaller difference is better\n        best_fit_diff = suitable_bins_cap - item\n        \n        # Normalize the Best Fit scores to be between 0 and 1, higher is better\n        # Add a small epsilon to avoid division by zero if an item perfectly fills a bin.\n        # Invert and normalize: smaller diff -> higher score\n        if np.max(best_fit_diff) > np.min(best_fit_diff):\n            best_fit_scores = 1.0 - (best_fit_diff - np.min(best_fit_diff)) / (np.max(best_fit_diff) - np.min(best_fit_diff) + 1e-9)\n        else:\n            best_fit_scores = np.ones_like(best_fit_diff) # All gaps are the same\n\n        # Penalty for \"too much\" remaining capacity (inspired by Heuristics 11-13)\n        # Penalize bins where remaining capacity is much larger than the item size.\n        # Use a sigmoid-like penalty that is close to 0 for small differences and approaches 1 for large ones.\n        # This is subtracted from the best_fit_score. A high penalty means low priority.\n        # Threshold and steepness can be tuned. Here, penalty increases significantly when remaining capacity > 2*item.\n        penalty_threshold_ratio = 2.0 \n        penalty_steepness = 0.5\n        \n        large_capacity_penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (suitable_bins_cap - item * penalty_threshold_ratio)))\n        \n        # Combine scores: Additive combination, where penalty is subtracted.\n        # This means higher best_fit_score is good, higher penalty is bad.\n        combined_priorities = best_fit_scores - large_capacity_penalty\n\n        # Assign the calculated priorities to the original indices\n        original_indices = np.where(suitable_bins_mask)[0]\n        priorities[original_indices] = combined_priorities\n\n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by combining Best Fit with a penalty for very large remaining capacities.\n    Favors bins that are a tight fit and discourages using bins that are excessively underfilled.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # Best Fit component: Smaller difference is better\n        best_fit_diff = suitable_bins_cap - item\n        \n        # Normalize the Best Fit scores to be between 0 and 1, higher is better\n        # Add a small epsilon to avoid division by zero if an item perfectly fills a bin.\n        # Invert and normalize: smaller diff -> higher score\n        if np.max(best_fit_diff) > np.min(best_fit_diff):\n            best_fit_scores = 1.0 - (best_fit_diff - np.min(best_fit_diff)) / (np.max(best_fit_diff) - np.min(best_fit_diff) + 1e-9)\n        else:\n            best_fit_scores = np.ones_like(best_fit_diff) # All gaps are the same\n\n        # Penalty for \"too much\" remaining capacity (inspired by Heuristics 11-13)\n        # Penalize bins where remaining capacity is much larger than the item size.\n        # Use a sigmoid-like penalty that is close to 0 for small differences and approaches 1 for large ones.\n        # This is subtracted from the best_fit_score. A high penalty means low priority.\n        # Threshold and steepness can be tuned. Here, penalty increases significantly when remaining capacity > 2*item.\n        penalty_threshold_ratio = 2.0 \n        penalty_steepness = 0.5\n        \n        large_capacity_penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (suitable_bins_cap - item * penalty_threshold_ratio)))\n        \n        # Combine scores: Additive combination, where penalty is subtracted.\n        # This means higher best_fit_score is good, higher penalty is bad.\n        combined_priorities = best_fit_scores - large_capacity_penalty\n\n        # Assign the calculated priorities to the original indices\n        original_indices = np.where(suitable_bins_mask)[0]\n        priorities[original_indices] = combined_priorities\n\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by combining Best Fit with a penalty for very large remaining capacities.\n    Favors bins that are a tight fit and discourages using bins that are excessively underfilled.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # Best Fit component: Smaller difference is better\n        best_fit_diff = suitable_bins_cap - item\n        \n        # Normalize the Best Fit scores to be between 0 and 1, higher is better\n        # Add a small epsilon to avoid division by zero if an item perfectly fills a bin.\n        # Invert and normalize: smaller diff -> higher score\n        if np.max(best_fit_diff) > np.min(best_fit_diff):\n            best_fit_scores = 1.0 - (best_fit_diff - np.min(best_fit_diff)) / (np.max(best_fit_diff) - np.min(best_fit_diff) + 1e-9)\n        else:\n            best_fit_scores = np.ones_like(best_fit_diff) # All gaps are the same\n\n        # Penalty for \"too much\" remaining capacity (inspired by Heuristics 11-13)\n        # Penalize bins where remaining capacity is much larger than the item size.\n        # Use a sigmoid-like penalty that is close to 0 for small differences and approaches 1 for large ones.\n        # This is subtracted from the best_fit_score. A high penalty means low priority.\n        # Threshold and steepness can be tuned. Here, penalty increases significantly when remaining capacity > 2*item.\n        penalty_threshold_ratio = 2.0 \n        penalty_steepness = 0.5\n        \n        large_capacity_penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (suitable_bins_cap - item * penalty_threshold_ratio)))\n        \n        # Combine scores: Additive combination, where penalty is subtracted.\n        # This means higher best_fit_score is good, higher penalty is bad.\n        combined_priorities = best_fit_scores - large_capacity_penalty\n\n        # Assign the calculated priorities to the original indices\n        original_indices = np.where(suitable_bins_mask)[0]\n        priorities[original_indices] = combined_priorities\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}