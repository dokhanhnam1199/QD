{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    The Almost Full Fit strategy prioritizes bins that will be almost full after\n    adding the item. A small remaining capacity is preferred.\n    If a bin has exactly enough capacity for the item, it's a perfect fit.\n    If a bin has more capacity than needed, the priority decreases as the remaining\n    capacity increases.\n    If a bin cannot fit the item, its priority is set to a very low value.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    can_fit_mask = bins_remain_cap >= item\n\n    fit_capacities = bins_remain_cap[can_fit_mask] - item\n\n    # Prioritize bins where the remaining capacity after fitting is smallest\n    # A perfect fit (remaining capacity 0) gets the highest priority.\n    # Larger remaining capacities get lower priorities.\n    priorities[can_fit_mask] = -fit_capacities\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask] - item\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1st and 2nd (identical, but listed as distinct): Both implement an \"Almost Full Fit\" strategy by assigning priority as the negative of the remaining capacity after fitting the item (`-fit_capacities`). This correctly prioritizes bins that result in smaller remaining space.\n\nComparing Heuristics 3rd and 8th (identical): These also implement a \"Best Fit\" strategy by assigning priority as the reciprocal of the remaining capacity after fitting (`1.0 / (gaps + epsilon)`). This approach effectively gives higher scores to bins with smaller gaps, aligning with the \"Best Fit\" principle of minimizing wasted space. The detailed comments explain the rationale well.\n\nComparing Heuristics 4th and the \"Best Fit\" variants (3rd/8th): Heuristic 4th uses `1.0 / (diff + epsilon)` for bins that can fit, similar to Best Fit. However, it then normalizes these priorities to be between 0 and 1. Normalization can sometimes obscure the fine-grained differences that exact values might convey, potentially making it less effective for pure prioritization unless the downstream selection mechanism specifically benefits from normalized inputs.\n\nComparing Heuristics 5th, 6th, 7th, and 9th: These heuristics (5th, 6th, 7th, 9th) are largely similar to the \"Best Fit\" approach (3rd/8th), using `1.0 / (bins_remain_cap - item + epsilon)`. Heuristic 9th explicitly sets non-fitting bins to `-float('inf')`, which is a robust way to ensure they are never selected. Heuristic 5th and 7th are identical. Heuristic 6th uses a slightly different epsilon.\n\nComparing Heuristics 10th and 18th: Both attempt a softmax-like approach. Heuristic 10th scales `fit_values` by `max_fit` and then uses `exp(scaled_fit_values) / sum(exp_values)`. Heuristic 18th scales `fit_values` by `max(fit_values)` (handling the case of `max_fit` being 0) and assigns the raw `exp_fit` as priority, without normalizing by the sum. The lack of normalization in 18th means raw exponentiated values are used, which might be less stable or interpretable as probabilities compared to 10th.\n\nComparing Heuristics 11th and 12th: Heuristic 11th uses a sigmoid function applied to a ratio of `(residual / suitable_bins_cap)` and then inverts it, aiming to prioritize bins with less *relative* remaining capacity. Heuristic 12th also uses a sigmoid but aims to prioritize bins where the *absolute* remaining capacity after fitting is small, by using `target_norm_remain_cap - normalized_remain_cap` in the sigmoid input. Both use sigmoid transformations but with different inputs and interpretations, making direct comparison tricky without empirical testing. Heuristic 11th appears to be a more direct mapping towards minimizing waste ratio.\n\nComparing Heuristics 13th, 14th, 15th, 16th, and 17th: Heuristic 13th uses a softmax on the inverse of the remaining capacity, similar to Best Fit but normalized. Heuristics 14th-17th (identical) use a sigmoid function applied to `excess_capacity` (i.e., `bins_remain_cap - item`) with a specific slope and intercept. This sigmoid function penalizes larger excess capacities. The specific choice of slope and intercept (10.0, -5.0) means the function saturates quickly.\n\nComparing Heuristics 19th and 20th (identical): These heuristics set priorities to `bins_remain_cap - item` for bins that can fit, and `-np.inf` for those that cannot. This is essentially a \"Worst Fit\" strategy if selecting the max, or if the negative of this is used, it's a variant of \"Best Fit\" but with linear scaling rather than inverse. The simple subtraction does not give a strong preference for the *tightest* fit as effectively as an inverse.\n\nOverall: The \"Best Fit\" strategies (3rd, 8th, and their similar variants like 5th, 7th) that use the inverse of the remaining gap (`1 / (gap + epsilon)`) are strong contenders for good heuristics as they directly target minimizing waste. Heuristics that normalize or apply sigmoids (4th, 10th, 11th, 12th, 13th, 14th-17th) introduce complexities that may or may not improve performance over simpler \"Best Fit\" without careful tuning or specific problem characteristics. Heuristics 1st and 2nd (\"Almost Full Fit\") are a simpler form of Best Fit. The worst heuristics are those that use simple differences (19th, 20th) or do not effectively prioritize tight fits.\n- \nHere's a redefined approach to self-reflection for heuristic design:\n\n*   **Keywords:** Adaptive prioritization, learning, meta-heuristics, state-space exploration.\n*   **Advice:** Focus on heuristics that *learn* from past decisions and *adapt* their prioritization rules dynamically. Consider meta-heuristics that explore different heuristic strategies or parameter settings.\n*   **Avoid:** Static, single-objective heuristics. Over-reliance on pre-defined rules without mechanisms for adjustment.\n*   **Explanation:** Instead of just minimizing wasted space, aim to improve the *process* of finding good fits. Learning from previous placements can guide future choices, leading to more robust and efficient solutions, especially in complex, dynamic environments.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}