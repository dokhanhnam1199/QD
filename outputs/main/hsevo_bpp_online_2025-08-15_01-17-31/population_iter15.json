[
  {
    "stdout_filepath": "problem_iter14_response0.txt_stdout.txt",
    "code_path": "problem_iter14_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a tight-fit preference (Best Fit) with a preference for bins\n    that are not excessively empty after packing. Uses adaptive scoring.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        gaps = suitable_bins_cap - item\n        \n        # Best Fit component: Higher score for smaller gaps (tighter fits)\n        # Add a small epsilon for numerical stability and to avoid division by zero.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Adaptive capacity utilization component: Penalize bins that will be very empty.\n        # This encourages using bins more fully, avoiding fragmentation.\n        # The score increases as the remaining capacity decreases relative to the item size.\n        # We normalize by the original bin capacity to make it relative.\n        # A small epsilon is added to item size for normalization stability.\n        capacity_utilization_scores = (suitable_bins_cap - item) / (suitable_bins_cap + 1e-9)\n        \n        # Combine scores with a weighting. Prioritize tighter fits more.\n        # The combined score aims to balance \"best fit\" with \"good utilization\".\n        # We invert capacity_utilization_scores to prioritize smaller remaining capacity (higher utilization)\n        combined_scores = (0.7 * best_fit_scores) + (0.3 * (1.0 / (capacity_utilization_scores + 1e-9)))\n        \n        priorities[suitable_bins_mask] = combined_scores\n            \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 174.22857502740396,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response1.txt_stdout.txt",
    "code_path": "problem_iter14_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with an adaptive penalty for overfilled bins,\n    prioritizing tight fits while smoothly penalizing bins with excess capacity.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        gaps = suitable_bins_cap - item\n        \n        # Best Fit component: higher score for smaller gaps (tighter fits)\n        # Adding a small epsilon for numerical stability.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Adaptive penalty for excess capacity:\n        # Penalizes bins where remaining capacity is significantly larger than the item size.\n        # Uses a sigmoid-like function to smoothly increase the penalty.\n        # The penalty is applied if the bin's remaining capacity is more than twice the item size.\n        penalty_threshold_ratio = 2.0 \n        penalty_steepness = 1.0  # Adjusted for potentially stronger penalization\n        \n        # Calculate penalty: lower values for capacities closer to threshold, higher for much larger capacities.\n        # The penalty is subtracted from the best_fit_score.\n        excess_capacity_penalty = 1.0 / (1.0 + np.exp(penalty_steepness * (suitable_bins_cap / (item + 1e-9) - penalty_threshold_ratio)))\n        \n        combined_scores = best_fit_scores - excess_capacity_penalty\n        \n        priorities[suitable_bins_mask] = combined_scores\n            \n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 158.32466846199546,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response2.txt_stdout.txt",
    "code_path": "problem_iter14_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with an adaptive sigmoid penalty for remaining capacity.\n    Prioritizes tight fits and penalizes bins with excessive empty space smoothly.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n    penalty_strength = 7.0  # Increased penalty strength for stronger bias towards tighter fits\n\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_fit = valid_bins_remain_cap - item\n\n    # Best Fit score: inverse of the remaining space for tighter fits\n    best_fit_scores = 1.0 / (remaining_after_fit + epsilon)\n\n    # Adaptive Sigmoid Penalty: Penalizes large remaining capacities smoothly.\n    # Scales remaining capacity relative to the *item's* size to adapt penalty.\n    normalized_remaining_ratio = remaining_after_fit / np.maximum(item, epsilon)\n    penalty_scores = 1.0 / (1.0 + np.exp(penalty_strength * (1.0 - normalized_remaining_ratio)))\n\n    # Combine scores: subtract penalty from best fit score\n    combined_priorities = best_fit_scores - penalty_scores\n\n    # Normalize combined priorities to [0, 1]\n    min_p, max_p = np.min(combined_priorities), np.max(combined_priorities)\n    if max_p > min_p:\n        normalized_priorities = (combined_priorities - min_p) / (max_p - min_p)\n    else:\n        normalized_priorities = np.ones_like(combined_priorities) * 0.5 \n\n    original_indices = np.where(can_fit_mask)[0]\n    priorities[original_indices] = normalized_priorities\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 230.62385799360038,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response3.txt_stdout.txt",
    "code_path": "problem_iter14_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with an adaptive logarithmic penalty on remaining capacity.\n    Prioritizes bins that minimize wasted space, with a nuanced penalty for\n    larger remaining capacities to avoid overly aggressive bin selection.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n    \n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    epsilon = 1e-9\n    log_offset = 1.0\n    \n    # Calculate the difference for Best Fit\n    best_fit_diff = suitable_bins_cap - item\n    \n    # Calculate a penalty based on remaining capacity (logarithmic)\n    # Add log_offset to avoid log(0) or very small numbers.\n    # The larger the remaining capacity, the higher the penalty (lower priority).\n    penalty = np.log(suitable_bins_cap + log_offset)\n    \n    # Normalize the best_fit_diff to get a score between 0 and 1\n    # A smaller difference means a higher score.\n    # Add epsilon to avoid division by zero if all suitable bins have the same capacity.\n    normalized_diff_score = 1.0 - (best_fit_diff / (np.max(suitable_bins_cap) + epsilon))\n    \n    # Combine the Best Fit score and the penalty.\n    # We want to prioritize small differences (high normalized_diff_score)\n    # and penalize large remaining capacities (low penalty value).\n    # A simple additive combination with appropriate scaling or a multiplicative\n    # approach could be used. Here, we invert the penalty to make higher values\n    # correspond to higher priority.\n    # A simple additive combination: priority = best_fit_score - penalty_factor * penalty\n    # Or, priority = best_fit_score * (1 / penalty)\n    \n    # Let's try a formulation that rewards small gaps and penalizes large remaining capacities.\n    # We want small `best_fit_diff` and also want to avoid bins with very large remaining capacity\n    # that would be better used by a much larger item later.\n    # A common approach is to make the priority inversely related to the remaining capacity.\n    # Combining `normalized_diff_score` (high for good fit) and `1/penalty` (high for small remaining capacity)\n    # Since penalty = log(cap + offset), 1/penalty is high for small cap.\n    \n    # We can also add a term that penalizes large remaining capacity directly.\n    # Consider the combined score: normalized_diff_score * (1 / (suitable_bins_cap + epsilon))\n    # This rewards a good fit and a small remaining capacity.\n    \n    # Let's combine Best Fit difference with a penalty on the remaining capacity.\n    # Lower difference is better for Best Fit.\n    # Lower remaining capacity is generally better to keep bins more full.\n    # So, we want to reward bins with `suitable_bins_cap - item` being small AND `suitable_bins_cap` being small.\n    # This suggests a score that is inversely proportional to both.\n    \n    # Consider the inverse of the remaining capacity as a positive score component.\n    # score = normalized_diff_score / (suitable_bins_cap + epsilon)\n    # This gives high scores to bins with small remaining capacity AND small gap.\n    \n    # Let's refine: Best Fit aims to minimize `suitable_bins_cap - item`.\n    # A secondary goal is to leave less \"wasted\" space in the bin if possible.\n    # So, prioritize bins where `suitable_bins_cap - item` is small.\n    # Among those with similar `suitable_bins_cap - item`, prioritize bins with smaller `suitable_bins_cap`.\n    \n    # This can be achieved by giving a higher score to smaller `suitable_bins_cap - item`,\n    # and then a further bonus for smaller `suitable_bins_cap`.\n    \n    # Option 1: Weighted sum of reciprocal values\n    # Score = w1 * (1 / (suitable_bins_cap - item + epsilon)) + w2 * (1 / (suitable_bins_cap + epsilon))\n    \n    # Option 2: Use the difference and penalize based on remaining capacity logarithmically.\n    # priority = (1.0 / (best_fit_diff + epsilon)) * (1.0 / (suitable_bins_cap + log_offset))\n    # This rewards small differences and small remaining capacities.\n    \n    # Let's try a simpler combination that directly reflects the goals:\n    # High priority for bins with small `best_fit_diff`.\n    # Among bins with similar `best_fit_diff`, prefer those with smaller `suitable_bins_cap`.\n    \n    # We can create a composite score. A common heuristic is to combine these terms.\n    # Example: Priority = (Constant - best_fit_diff) * (Constant / suitable_bins_cap)\n    # Or, more directly: Priority = 1 / (best_fit_diff + suitable_bins_cap)\n    \n    # Let's use the Best Fit difference and add a penalty based on the remaining capacity.\n    # We want to MINIMIZE `best_fit_diff` and MINIMIZE `suitable_bins_cap`.\n    # So, a score that is inversely related to both.\n    \n    # A common approach is to use the negative of the remaining capacity as a score component.\n    # Or, a penalty that increases with remaining capacity.\n    \n    # Let's use the normalized difference score and penalize based on the original capacity.\n    # A bin that fits the item snugly (low normalized_diff_score) should have high priority.\n    # Among bins with similar snugness, a bin with less remaining capacity is preferred.\n    \n    # Prioritize bins with small `best_fit_diff`.\n    # Penalize bins with large `suitable_bins_cap`.\n    # Let's use a score that is the inverse of the difference, PLUS an inverse of the capacity.\n    # This favors small gaps and small bins.\n    \n    # The idea from 'Analyze & experience' suggests combining Best Fit with a penalty on remaining capacity.\n    # Heuristic 8-9 uses Best Fit with a logarithmic penalty.\n    # `priority = 1 / (best_fit_diff + epsilon) * (1 / np.log(suitable_bins_cap + log_offset))`\n    # This prioritizes small differences AND small remaining capacities.\n    \n    # Let's try this: higher score for smaller diff, and higher score for smaller remaining capacity.\n    # score_diff = 1.0 / (best_fit_diff + epsilon)\n    # score_cap = 1.0 / (suitable_bins_cap + epsilon)\n    # priority = score_diff * score_cap\n    \n    # Let's adopt the \"Best Fit with a logarithmic penalty\" strategy as described in analysis for Heuristics 8-9.\n    # This combines the tightness of Best Fit with a penalty for leaving too much space.\n    # We want to minimize `best_fit_diff`. So, `1 / (best_fit_diff + epsilon)` is a good component.\n    # We want to penalize large `suitable_bins_cap`. So, `1 / log(suitable_bins_cap + log_offset)` is a good component.\n    \n    combined_score = (1.0 / (best_fit_diff + epsilon)) * (1.0 / np.log(suitable_bins_cap + log_offset))\n    \n    original_indices = np.where(suitable_bins_mask)[0]\n    \n    # Assign the calculated scores to the priorities array\n    priorities[original_indices] = combined_score\n    \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 153.73110979725664,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response4.txt_stdout.txt",
    "code_path": "problem_iter14_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a sigmoid-based penalty on the *ratio* of remaining capacity\n    to item size, favoring tighter fits and penalizing large relative excesses.\n    Also includes a minor bias for bins with less absolute remaining capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n    penalty_strength = 3.0  # Tune this parameter\n\n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if valid_bins_remain_cap.size > 0:\n        remaining_after_fit = valid_bins_remain_cap - item\n\n        # Best Fit score: higher for smaller remaining capacity (tighter fit)\n        # Add epsilon to avoid division by zero\n        best_fit_scores = 1.0 / (remaining_after_fit + epsilon)\n\n        # Adaptive Sigmoid Penalty: Penalizes based on how much *more* capacity is left\n        # compared to the item size. This is relative to the item itself.\n        # The sigmoid penalizes larger ratios smoothly.\n        relative_excess_ratio = remaining_after_fit / (item + epsilon)\n        penalty_scores = 1.0 / (1.0 + np.exp(penalty_strength * relative_excess_ratio))\n\n        # Combine Best Fit with penalty: Subtract penalty from the score.\n        # A smaller penalty (closer to 1) is better.\n        combined_priorities = best_fit_scores * penalty_scores\n\n        # Normalize combined priorities to [0, 1]\n        min_p, max_p = np.min(combined_priorities), np.max(combined_priorities)\n        if max_p > min_p:\n            normalized_priorities = (combined_priorities - min_p) / (max_p - min_p)\n        else:\n            normalized_priorities = np.ones_like(combined_priorities) * 0.5\n\n        original_indices = np.where(can_fit_mask)[0]\n        priorities[original_indices] = normalized_priorities\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 233.1830877661235,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response5.txt_stdout.txt",
    "code_path": "problem_iter14_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a normalized logarithmic penalty for remaining capacity.\n    Prioritizes bins with minimal leftover space, penalizing larger gaps logarithmically.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    best_fit_diff = suitable_bins_cap - item\n    \n    # Use a normalized logarithmic penalty for remaining capacity\n    # Add a small epsilon to avoid log(0) and ensure positive values\n    epsilon = 1e-9\n    \n    # Normalize the differences to be between 0 and 1 (approximately)\n    # This helps in creating a more balanced penalty across different scales of remaining capacity\n    normalized_diff = best_fit_diff / (suitable_bins_cap + epsilon) \n\n    # Logarithmic penalty: smaller differences get higher scores\n    # Use log1p for better numerical stability when normalized_diff is close to 0\n    penalty = -np.log1p(normalized_diff)\n\n    # Apply the penalty to the priorities. Higher penalty means lower priority.\n    # We want to invert this, so we can use the penalty itself as a higher score for better fits.\n    # A simple way is to use the negative of the penalty.\n    priorities[suitable_bins_mask] = -penalty\n\n    # Ensure that bins that exactly fit the item (diff=0) get a high priority\n    # This can be handled by the log1p(0) = 0, so -0 = 0, which might be lower than others.\n    # We can adjust this by ensuring a base score or by modifying the penalty.\n    # For now, let's rely on the fact that smaller diffs lead to higher (less negative) penalty.\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 2.0,
    "halstead": 70.32403072095333,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response6.txt_stdout.txt",
    "code_path": "problem_iter14_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines adaptive sigmoid penalty with a modified Best Fit to balance tightness and space utilization.\n    Prioritizes bins that offer a good fit and penalizes bins with excessive remaining capacity.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Consider only bins that can fit the item\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if valid_bins_remain_cap.size > 0:\n        # Calculate the remaining capacity after placing the item (tightness metric)\n        remaining_after_fit = valid_bins_remain_cap - item\n        \n        # Modified Best Fit: Smaller remaining capacity is better, use inverse for higher score.\n        # Add epsilon for numerical stability and to avoid division by zero.\n        best_fit_scores = 1.0 / (remaining_after_fit + 1e-9)\n        \n        # Adaptive Penalty component: Penalize bins with significantly more space than needed.\n        # Use a sigmoid function on the ratio of bin capacity to item size.\n        # Threshold controls when penalty becomes significant; steepness controls the transition speed.\n        penalty_threshold_ratio = 1.5  # Ratio where penalty starts to increase\n        penalty_steepness = 0.8  # Controls the steepness of the penalty\n        \n        # Calculate the capacity-to-item ratio, ensuring item is not zero for division.\n        capacity_ratio = valid_bins_remain_cap / (item + 1e-9)\n        \n        # Sigmoid function for penalty: penalizes bins with capacity_ratio > penalty_threshold_ratio\n        # Penalty is 0 when ratio is small, approaches 1 when ratio is large.\n        penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (capacity_ratio - penalty_threshold_ratio)))\n        \n        # Combine Best Fit score with penalty: subtract penalty to lower priority for over-spacious bins.\n        combined_scores = best_fit_scores - penalty\n        \n        # Normalize the combined scores to be between 0 and 1 for consistent priority values.\n        min_score = np.min(combined_scores)\n        max_score = np.max(combined_scores)\n        \n        if max_score > min_score:\n            normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n        else:\n            # If all valid bins have the same score, assign a neutral priority.\n            normalized_scores = np.full_like(combined_scores, 0.5)\n            \n        # Assign the calculated priorities back to the original indices.\n        original_indices = np.where(can_fit_mask)[0]\n        priorities[original_indices] = normalized_scores\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 260.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response7.txt_stdout.txt",
    "code_path": "problem_iter14_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with an adaptive logarithmic penalty, prioritizing bins\n    that minimize wasted space while avoiding overly full bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    original_indices = np.where(suitable_bins_mask)[0]\n    \n    # Best Fit Component: Minimize remaining capacity after packing\n    remaining_after_fit = suitable_bins_cap - item\n    \n    # Adaptive Logarithmic Penalty: Penalize larger gaps logarithmically.\n    # Add 1 to the denominator to prevent division by zero and scale penalty.\n    penalty = np.log1p(remaining_after_fit / (item + 1e-9))\n    \n    # Normalize penalties to range [0, 1] where higher penalty means lower priority\n    if np.max(penalty) > 0:\n        normalized_penalty = penalty / np.max(penalty)\n    else:\n        normalized_penalty = np.zeros_like(penalty)\n    \n    # Combined Score: Prioritize bins with lower normalized penalties (better fit)\n    # A higher score indicates higher priority.\n    scores = 1.0 - normalized_penalty\n    \n    # Assign priorities to the original indices\n    priorities[original_indices] = scores\n    \n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 97.70233280920246,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response8.txt_stdout.txt",
    "code_path": "problem_iter14_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with an adaptive, capacity-aware logarithmic penalty.\n    Prioritizes bins that leave minimal space, but scales the penalty based on the\n    relative remaining capacity to encourage fuller bins without being overly greedy.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Best Fit component: Minimize remaining capacity after packing.\n    remaining_after_fit = suitable_bins_cap - item\n    \n    # Adaptive penalty: Logarithmic penalty, scaled by the initial capacity of suitable bins.\n    # This penalizes bins that are large and leave significant space,\n    # but is dampened for bins that are already relatively full.\n    # Add 1 to item size to prevent division by zero and stabilize for small items.\n    # Add 1e-9 to denominator for numerical stability.\n    scaled_penalty = np.log1p(remaining_after_fit / (suitable_bins_cap + 1.0 + 1e-9))\n    \n    # Normalize the scaled penalty to be between 0 and 1. Higher penalty means lower priority.\n    # Invert the normalized penalty to get priority score.\n    if np.max(scaled_penalty) > 0:\n        normalized_penalty = scaled_penalty / np.max(scaled_penalty)\n        # Higher score means higher priority. We want to prioritize smaller 'remaining_after_fit',\n        # which corresponds to smaller 'scaled_penalty'. So, we use 1 - normalized_penalty.\n        scores = 1.0 - normalized_penalty\n    else:\n        # If all scaled penalties are zero or negative (e.g., item perfectly fits or is very small),\n        # all suitable bins get a neutral priority.\n        scores = np.ones_like(suitable_bins_cap)\n\n    # Assign priorities to the original indices\n    original_indices = np.where(suitable_bins_mask)[0]\n    priorities[original_indices] = scores\n    \n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 112.37013046707143,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response9.txt_stdout.txt",
    "code_path": "problem_iter14_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a smooth, adaptive penalty for large remaining capacities,\n    enhanced by normalization for stable multi-bin selection.\n\n    This heuristic balances fitting an item tightly (Best Fit) with a penalty\n    for bins that have excessive remaining capacity, aiming for better overall\n    bin utilization. Normalization ensures scores are comparable across different\n    bin states.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    \n    suitable_bins_caps = bins_remain_cap[can_fit_mask]\n    \n    if suitable_bins_caps.size > 0:\n        gaps = suitable_bins_caps - item\n        \n        # Best Fit component: prioritize smaller gaps. Inverse for higher score.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Adaptive penalty for large remaining capacity using a sigmoid-like function.\n        # Penalizes bins with capacity significantly larger than the item.\n        penalty_threshold_ratio = 2.0  # Threshold for when penalty starts becoming significant\n        penalty_steepness = 0.7      # Controls how quickly penalty increases\n        \n        # Calculate penalty: high when capacity >> item * ratio, approaches 0 otherwise.\n        # The `+ 1e-9` in the denominator prevents division by zero for item sizes close to zero.\n        large_capacity_penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (suitable_bins_caps / (item + 1e-9) - penalty_threshold_ratio)))\n        \n        # Combine scores: Subtract penalty from Best Fit score.\n        combined_scores = best_fit_scores - large_capacity_penalty\n        \n        # Normalize scores to [0, 1] for stable comparison across different item sizes.\n        if combined_scores.size > 0:\n            min_score = np.min(combined_scores)\n            max_score = np.max(combined_scores)\n            if max_score - min_score > 1e-9: # Avoid division by zero if all scores are the same\n                normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n            else:\n                normalized_scores = np.ones_like(combined_scores) * 0.5 # Neutral score if all are identical\n        else:\n            normalized_scores = np.array([])\n\n        priorities[can_fit_mask] = normalized_scores\n        \n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 312.88626403364293,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response0.txt_stdout.txt",
    "code_path": "problem_iter15_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    bin_capacity = bins_remain_cap + item # Assuming this is how it works, if a bin can fit the item\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n    \n    suitable_bins_cap_remaining = bins_remain_cap[suitable_bins_mask]\n    \n    # Objective 1: Tightness of fit (similar to Best Fit)\n    # Calculate remaining capacity after placing the item\n    remaining_after_fit = suitable_bins_cap_remaining - item\n    \n    # Normalize tightness score (smaller remaining capacity is better, so invert and normalize)\n    # Add a small epsilon to avoid division by zero if all remaining capacities are zero\n    normalized_tightness = 1.0 - (remaining_after_fit / (np.max(suitable_bins_cap_remaining) + 1e-9))\n    \n    # Objective 2: Fullness of bin (prioritize bins that are already fuller)\n    # This encourages consolidating items into fewer bins\n    # Normalize by total bin capacity (which is constant or inferred)\n    # Assuming a max bin capacity of 1.0 for normalization if not provided explicitly\n    # If bin_capacity is not available or is dynamic, this part needs adjustment.\n    # For now, let's assume bin capacity is 1.0 for normalization.\n    # The capacity of the bin *before* adding the item is suitable_bins_cap_remaining + item\n    # If we assume a common bin capacity 'C', then it would be (C - suitable_bins_cap_remaining) / C\n    # Let's use the average of the original capacities of suitable bins for normalization if C is unknown.\n    \n    # A more robust approach if bin capacity 'C' is unknown and bins might have different capacities\n    # Or if we want to consider the *current* fullness of the bin.\n    # Let's normalize by the maximum *possible* remaining capacity among suitable bins if we don't have global C\n    \n    # If we assume a standard bin capacity 'C', this would be more effective.\n    # For now, let's try to prioritize bins that are already more full relative to their *potential* remaining capacity\n    # after placing the item.\n    \n    # Let's use the reciprocal of the remaining capacity *after* placing the item, normalized.\n    # A smaller remaining capacity means a fuller bin.\n    # We want to maximize this metric.\n    \n    # Higher score for bins that will have less space left\n    # Add a small epsilon to avoid division by zero\n    bin_fullness_score = 1.0 / (remaining_after_fit + 1e-9)\n    \n    # Normalize the bin_fullness_score. The highest score is the most full bin.\n    # We want to map this to a [0, 1] range where 1 is best.\n    normalized_fullness = bin_fullness_score / np.max(bin_fullness_score + 1e-9)\n    \n    # Combine objectives with adaptive weighting\n    # We can start with equal weights and adjust based on performance if this was part of a learning system.\n    # For a static heuristic, we'll use fixed weights, but the *idea* is adaptivity.\n    # Let's give a slight edge to tightness, but also consider fullness.\n    \n    # Example weighting: 60% tightness, 40% fullness.\n    # This can be tuned. The idea is to combine them smoothly.\n    \n    # Weights can be adjusted, perhaps based on item size (larger items might prioritize tightness more)\n    # For now, let's use fixed weights.\n    weight_tightness = 0.6\n    weight_fullness = 0.4\n    \n    # Composite priority score\n    composite_priority = (weight_tightness * normalized_tightness) + (weight_fullness * normalized_fullness)\n    \n    # Apply priorities to the original bins_remain_cap indices\n    priorities[suitable_bins_mask] = composite_priority\n    \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 190.3981037807637,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response1.txt_stdout.txt",
    "code_path": "problem_iter15_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # Objective 1: Tightness of fit (minimize remaining capacity)\n        fit_tightness = suitable_bins_cap - item\n        \n        # Objective 2: Bin fullness (maximize utilization of the bin that gets the item)\n        # Using the inverse of the remaining capacity after packing. A smaller remaining capacity means higher fullness.\n        bin_fullness = 1.0 / (suitable_bins_cap - item + 1e-9) # Add epsilon for numerical stability\n        \n        # Normalize objectives to a common scale (e.g., [0, 1])\n        # Avoid division by zero if all suitable bins have the same remaining capacity\n        if np.ptp(fit_tightness) > 1e-9:\n            normalized_fit_tightness = (fit_tightness - np.min(fit_tightness)) / np.ptp(fit_tightness)\n        else:\n            normalized_fit_tightness = np.zeros_like(fit_tightness)\n            \n        if np.ptp(bin_fullness) > 1e-9:\n            normalized_bin_fullness = (bin_fullness - np.min(bin_fullness)) / np.ptp(bin_fullness)\n        else:\n            normalized_bin_fullness = np.zeros_like(bin_fullness)\n        \n        # Combine objectives with adaptive weights.\n        # For simplicity, we'll use static weights here, but in a more advanced\n        # scenario, these weights could be learned or adapted based on the problem instance.\n        # A common strategy is to favor tighter fits initially, then consider fullness.\n        # Let's give a slight preference to tighter fits.\n        weight_tightness = 0.6\n        weight_fullness = 0.4\n        \n        composite_score = (weight_tightness * normalized_fit_tightness) + (weight_fullness * normalized_bin_fullness)\n        \n        # Assign high priority to bins that achieve the best composite score\n        max_score = np.max(composite_score)\n        best_bins_mask_in_suitable = (composite_score == max_score)\n        \n        # Map back to original bin indices\n        original_indices_suitable = np.where(suitable_bins_mask)[0]\n        best_original_indices = original_indices_suitable[best_bins_mask_in_suitable]\n        \n        for idx in best_original_indices:\n            priorities[idx] = 1.0\n            \n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 5.0,
    "halstead": 225.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response2.txt_stdout.txt",
    "code_path": "problem_iter15_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Metric 1: Fit Tightness (Best Fit - closer to 0 is better)\n    fit_tightness = suitable_bins_cap - item\n    \n    # Metric 2: Bin Fullness (Inverse of remaining capacity - higher is better)\n    # Add a small epsilon to avoid division by zero if a bin is exactly full and item fits.\n    bin_fullness = 1.0 / (suitable_bins_cap - item + 1e-9) \n\n    # Metric 3: Remaining Capacity (Lower is better, but we want to prioritize bins that are *less* full for future items)\n    # This is less directly about fitting the current item, more about future packing.\n    # We'll give this a lower weight or consider its inverse.\n    # For now, let's focus on the immediate packing.\n\n    # Normalize metrics to be in a similar range and direction (higher priority is better)\n    # For fit_tightness, smaller is better, so we invert it and potentially normalize.\n    # Let's normalize by the range of the suitable bins' capacities to get a relative sense.\n    \n    # Handle cases where all suitable bins have the same capacity.\n    if np.all(suitable_bins_cap == suitable_bins_cap[0]):\n        normalized_fit_tightness = np.ones_like(fit_tightness) * 0.5 # Neutral if all are same\n    else:\n        min_cap_suitable = np.min(suitable_bins_cap)\n        max_cap_suitable = np.max(suitable_bins_cap)\n        range_cap_suitable = max_cap_suitable - min_cap_suitable\n        if range_cap_suitable == 0:\n            normalized_fit_tightness = np.ones_like(fit_tightness) * 0.5\n        else:\n            # Invert and scale: smaller gap -> higher score\n            normalized_fit_tightness = 1.0 - (fit_tightness - np.min(fit_tightness)) / (np.max(fit_tightness) - np.min(fit_tightness) + 1e-9)\n    \n    # Normalize bin_fullness. Higher fullness is better.\n    if np.all(bin_fullness == bin_fullness[0]):\n        normalized_bin_fullness = np.ones_like(bin_fullness) * 0.5\n    else:\n        normalized_bin_fullness = (bin_fullness - np.min(bin_fullness)) / (np.max(bin_fullness) - np.min(bin_fullness) + 1e-9)\n\n    # Combine metrics with weights. Weights can be tuned.\n    # Weighting:\n    # - Fit Tightness: High priority (e.g., 0.6) - want to fill bins efficiently\n    # - Bin Fullness: Medium priority (e.g., 0.4) - contributes to filling bins\n    \n    w_fit = 0.6\n    w_full = 0.4\n    \n    composite_priority = (w_fit * normalized_fit_tightness) + (w_full * normalized_bin_fullness)\n    \n    # Assign the composite priorities to the original indices\n    original_indices = np.where(suitable_bins_mask)[0]\n    priorities[original_indices] = composite_priority\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 5.0,
    "halstead": 408.74358474821895,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response3.txt_stdout.txt",
    "code_path": "problem_iter15_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    \n    remaining_capacity_scores = suitable_bins_cap - item\n    \n    \n    fullness_scores = 1.0 - (suitable_bins_cap / np.max(bins_remain_cap[bins_remain_cap > 0])) if np.any(bins_remain_cap > 0) else np.zeros_like(suitable_bins_cap)\n    \n    \n    if np.any(remaining_capacity_scores):\n        min_diff = np.min(remaining_capacity_scores)\n        best_fit_indicator = (remaining_capacity_scores == min_diff).astype(float)\n    else:\n        best_fit_indicator = np.zeros_like(suitable_bins_cap)\n\n    \n    normalized_fullness = (fullness_scores - np.min(fullness_scores)) / (np.max(fullness_scores) - np.min(fullness_scores) + 1e-9)\n    normalized_remaining_capacity = 1.0 - (remaining_capacity_scores / (np.max(remaining_capacity_scores) + 1e-9))\n    \n    \n    \n    combined_score = 0.6 * normalized_remaining_capacity + 0.4 * normalized_fullness\n\n    \n    \n    \n    priorities[suitable_bins_mask] = combined_score\n    \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 271.8519998980832,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response4.txt_stdout.txt",
    "code_path": "problem_iter15_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # --- Objective 1: Fit Tightness (Best Fit) ---\n    best_fit_diff = suitable_bins_cap - item\n    \n    # Normalize differences: smaller is better, so invert and scale\n    # Add a small epsilon to avoid division by zero if all diffs are 0\n    normalized_fit_tightness = 1.0 / (best_fit_diff + 1e-9)\n    \n    # Scale to a 0-1 range (approximately)\n    min_fit = np.min(normalized_fit_tightness)\n    max_fit = np.max(normalized_fit_tightness)\n    if max_fit - min_fit > 1e-9:\n        normalized_fit_tightness = (normalized_fit_tightness - min_fit) / (max_fit - min_fit)\n    else:\n        normalized_fit_tightness = np.ones_like(normalized_fit_tightness) * 0.5 # Neutral if all are same\n\n    # --- Objective 2: Bin Fullness (Favor fuller bins that can still fit the item) ---\n    # Calculate how full the suitable bins currently are (relative to item size needed)\n    # We want to favor bins that are *already* quite full, so a larger capacity remaining means *less* favorable\n    # So, we can use the inverse of remaining capacity, but capped by item size\n    current_fullness_score = 1.0 / (suitable_bins_cap + 1e-9) \n    \n    # Normalize fullness score: larger is better\n    min_fullness = np.min(current_fullness_score)\n    max_fullness = np.max(current_fullness_score)\n    if max_fullness - min_fullness > 1e-9:\n        normalized_fullness = (current_fullness_score - min_fullness) / (max_fullness - min_fullness)\n    else:\n        normalized_fullness = np.ones_like(current_fullness_score) * 0.5 # Neutral if all are same\n\n    # --- Combine Objectives with Adaptive Weights ---\n    # Weights can be adjusted. Here, we give a slight preference to fit tightness,\n    # but also consider fullness. The weights can be made adaptive based on problem instance properties.\n    weight_fit = 0.6\n    weight_fullness = 0.4\n\n    composite_score = weight_fit * normalized_fit_tightness + weight_fullness * normalized_fullness\n    \n    # Assign scores to the original bins\n    original_indices = np.where(suitable_bins_mask)[0]\n    priorities[original_indices] = composite_score\n    \n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 336.0451250937503,
    "exec_success": true
  }
]