{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a penalty for bins that are too large,\n    prioritizing tighter fits while avoiding extremely large remaining capacities.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    \n    bins_that_can_fit_caps = bins_remain_cap[can_fit_mask]\n    \n    if bins_that_can_fit_caps.size > 0:\n        gaps = bins_that_can_fit_caps - item\n        \n        # Best Fit component: prioritize bins with the smallest gap.\n        # We use 1/(gap + epsilon) which gives higher scores to smaller gaps.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Additional component: Penalize bins that leave very large remaining capacity.\n        # This can be done by considering the capacity relative to the item size or bin capacity.\n        # Let's penalize bins where `bins_remain_cap` is significantly larger than `item`.\n        # A simple penalty could be based on the reciprocal of the remaining capacity after fitting.\n        # Or, inversely related to the capacity AFTER fitting. Smaller post-fit capacity is better.\n        # Let's use a function that increases with smaller `bins_that_can_fit_caps - item`.\n        # The `best_fit_scores` already do this.\n\n        # Let's try to combine a \"Best Fit\" aspect with a \"First Fit Decreasing\" like preference\n        # for using up existing bins first.\n        # The 1/(gap + epsilon) is a strong \"Best Fit\".\n        \n        # A potential improvement: If multiple bins offer a very tight fit (small gap),\n        # we might prefer the one that has been used more or has less absolute capacity\n        # to keep larger bins available for larger items.\n        \n        # Let's modify the score slightly to favor bins that are \"more full\" among the best fits.\n        # This can be achieved by adding a term that increases with `bins_remain_cap`.\n        # However, this might conflict with \"Best Fit\" if a slightly larger bin is a slightly worse fit.\n        \n        # Let's stick to a refined Best Fit. The core idea is to minimize the leftover space.\n        # The previous `best_fit_scores` are good.\n        # To make it more \"adaptive\" or \"robust\", we can consider a secondary criterion if multiple\n        # bins have very similar small gaps.\n        \n        # Example:\n        # Bins capacities: [10, 10, 10, 10]\n        # Item: 3\n        # Bins remain cap: [2, 5, 8, 9]\n        # Item fits in all. Gaps: [2-3, 5-3, 8-3, 9-3] = [-1, 2, 5, 6] - This is wrong, it's `bins_remain_cap - item`.\n        # Bins remain cap: [2, 5, 8, 9]\n        # Item: 3\n        # Bins remaining capacity: [2, 5, 8, 9]\n        # Gaps: [2-3, 5-3, 8-3, 9-3] -> these are only for bins that fit.\n        # Assume bins_remain_cap = [2, 5, 8, 9], item = 3.\n        # can_fit_mask = [False, True, True, True]\n        # bins_that_can_fit_caps = [5, 8, 9]\n        # gaps = [5-3, 8-3, 9-3] = [2, 5, 6]\n        # best_fit_scores = [1/(2+eps), 1/(5+eps), 1/(6+eps)] = [~0.5, ~0.2, ~0.16]\n        # This prioritizes the bin with remaining capacity 5.\n\n        # Consider the case where `item` itself is very large.\n        # If `item` is close to bin capacity, the gap will be small.\n        # The current `1.0 / (gaps + epsilon)` prioritizes these.\n        \n        # Let's try a slight modification to the priority to encourage using bins\n        # that are \"closer\" to fitting the item without being too small.\n        # This is essentially what Best Fit does.\n        \n        # We can also incorporate a slight bias towards bins that are *not* completely empty,\n        # to encourage filling up partially used bins before starting new ones.\n        # This is more of a \"First Fit\" idea, but can be combined.\n        \n        # Let's try prioritizing bins by their remaining capacity AFTER fitting the item.\n        # We want to MINIMIZE this remaining capacity.\n        # Priority = - (bins_remain_cap - item)\n        # This directly makes smaller positive gaps have higher priority.\n        # Example: gaps = [2, 5, 6] -> priorities = [-2, -5, -6]. Max is -2.\n        # This means the bin with the smallest gap is prioritized.\n\n        # Let's combine the \"tight fit\" with a \"less empty\" preference.\n        # A simple way is to add a term that is inversely related to the remaining capacity.\n        # Priority = (1.0 / (gaps + epsilon)) + log(bins_that_can_fit_caps)\n        # This might be too complex.\n        \n        # A simpler combination: Best Fit score with a small bonus for bins that have\n        # less *total* capacity (to use up partially filled bins first).\n        # So, the priority for fitting bins will be:\n        # score = (1.0 / (gaps + epsilon)) + (1.0 / (bins_that_can_fit_caps + epsilon))\n        # This adds a preference for smaller capacity bins among those with similar gaps.\n        \n        # Let's normalize `bins_that_can_fit_caps` to avoid large values dominating.\n        # Or, let's focus on the `gaps` and add a term that penalizes very large `bins_that_can_fit_caps`\n        # when the gap is also not very small.\n        \n        # Revisit the core goal: pick the bin `j` that minimizes `bins_remain_cap[j] - item`.\n        # The `best_fit_scores = 1.0 / (gaps + 1e-9)` achieves this by giving higher scores\n        # to smaller positive gaps.\n        \n        # Let's try to slightly \"flatten\" the advantage of extremely small gaps,\n        # and give a slight boost to bins that are \"mediumly\" sized and fit well.\n        # This can be done by squaring the gap or using a different function.\n        \n        # Let's use a combination of \"Best Fit\" and \"Worst Fit\" (to keep options open).\n        # No, the goal is best fit.\n        \n        # Consider a heuristic that looks at `bins_remain_cap - item` and `bins_remain_cap`.\n        # We want to minimize `bins_remain_cap - item`.\n        # We also implicitly want to use bins that are not excessively large if the fit is similar.\n        \n        # Let's propose a heuristic that prioritizes bins that have a small gap,\n        # AND among those with small gaps, prefers bins that have less total capacity.\n        # The score for a bin that fits:\n        # priority_score = (1.0 / (bins_that_can_fit_caps - item + 1e-9))  # Best Fit part\n        # Let's add a term that rewards using bins with smaller remaining capacity.\n        # This could be `-(bins_that_can_fit_caps)`.\n        # So, priority = (1.0 / (gaps + 1e-9)) - bins_that_can_fit_caps\n        # This would favor smaller gaps, and among equal gaps, it favors smaller `bins_that_can_fit_caps`.\n        \n        priorities[can_fit_mask] = (1.0 / (gaps + 1e-9)) - bins_that_can_fit_caps\n        \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        best_fit_diff = suitable_bins_cap - item\n        \n        # Adaptive prioritization: Higher priority for bins that are almost full,\n        # but can still fit the item. This aims to \"fill up\" bins more efficiently.\n        # We can achieve this by giving a higher score to smaller remaining capacities\n        # after placing the item.\n        \n        # Calculate a \"fit score\" which is inversely related to the remaining capacity after placement.\n        # Add a small epsilon to avoid division by zero if an item perfectly fills a bin.\n        fit_scores = 1.0 / (best_fit_diff + 1e-9)\n        \n        # Normalize scores to prevent extreme values from dominating\n        if np.max(fit_scores) > 0:\n            normalized_fit_scores = fit_scores / np.max(fit_scores)\n        else:\n            normalized_fit_scores = np.zeros_like(fit_scores)\n\n        # Introduce a small penalty for bins that are very empty.\n        # This encourages packing into partially filled bins before opening new ones.\n        # The penalty is higher for bins with much more remaining capacity than the item size.\n        capacity_ratio = suitable_bins_cap / item if item > 0 else np.inf\n        empty_bin_penalty = np.exp(-0.5 * (capacity_ratio - 1)) * 0.2 # Exponential decay penalty\n        \n        # Combine fit score and penalty. Higher values are better.\n        combined_priorities = normalized_fit_scores - empty_bin_penalty\n        \n        # Assign the calculated priorities to the original indices\n        original_indices = np.where(suitable_bins_mask)[0]\n        priorities[original_indices] = combined_priorities\n\n    return priorities\n\n### Analyze & experience\n- *   **Comparing (1st) vs (2nd):** Heuristic 1 uses a simple average of 'best fit' and 'slack' scores, aiming to balance immediate fit with fuller bins. Heuristic 2 combines 'best fit' with an 'almost full' boost, prioritizing very tight fits. Heuristic 1's approach seems more balanced by explicitly considering slack.\n*   **Comparing (2nd) vs (3rd):** Heuristic 2 is identical to Heuristic 1, but with different internal calculation names and a slightly different combination logic (addition vs. averaging). Heuristic 3 is identical to Heuristic 1. There seems to be a misunderstanding in the ranking or code provided, as 1, 2, and 3 are very similar. However, focusing on the *intended* logic of Heuristic 2 (adding `almost_full_boost`), it might overemphasize \"almost full\" bins compared to the balanced approach of Heuristic 1/3.\n*   **Comparing (3rd) vs (4th):** Heuristic 3 (and 1) uses a scoring system based on inverse capacities. Heuristic 4 implements a pure \"Best Fit\" by assigning a priority of 1.0 to bins with the minimum gap, and 0 otherwise. Heuristic 3's nuanced scoring is generally better than a binary Best Fit, as it allows for finer selection among good fits.\n*   **Comparing (4th) vs (5th):** Heuristic 4 is pure Best Fit. Heuristic 5 attempts to normalize Best Fit scores and add a \"fullness\" score, but the combination (addition) and re-normalization might lead to unintended weighting. Heuristic 4's simplicity and directness of Best Fit might be more robust than Heuristic 5's complex normalization and combination.\n*   **Comparing (5th) vs (6th):** Heuristic 5 attempts normalization and a weighted combination. Heuristic 6 is identical to Heuristic 4 (pure Best Fit). This reinforces that Heuristic 4 (pure Best Fit) is likely superior to Heuristic 5's complicated approach.\n*   **Comparing (7th) and (11th/12th/13th):** Heuristic 7 proposes a score of `(1.0 / gaps) - bins_that_can_fit_caps`. This attempts to combine Best Fit with a penalty for large bin capacity, which can be good. However, the direct subtraction might lead to negative priorities where the penalty outweighs the Best Fit score, making interpretation and comparison difficult. Heuristics 11-13 implement a similar idea with a conditional penalty for \"too much\" remaining space. This conditional penalty (Heuristics 11-13) is a more controlled way to handle the \"overly large remaining capacity\" issue than Heuristic 7's direct subtraction.\n*   **Comparing (8th) vs (2nd):** Heuristic 8 is identical to Heuristic 2.\n*   **Comparing (9th) vs (1st):** Heuristic 9 weights 'waste reduction' (similar to Best Fit) and 'capacity utilization' (inverse of remaining capacity). This is a reasonable hybrid. Heuristic 1 averages these concepts. Heuristic 9's weighted approach might offer better control than Heuristic 1's simple average.\n*   **Comparing (10th) vs (1st):** Heuristic 10 combines Best Fit with a factor that *reduces* priority for bins with *more* remaining capacity. This aims to penalize lightly used bins. Heuristic 1's \"slack score\" (inverse of remaining capacity) also favors fuller bins. Heuristic 10's multiplicative approach might be more effective at modulating Best Fit scores.\n*   **Comparing (14th-20th) vs (4th):** Heuristics 14-20 are identical. They combine a normalized Best Fit score with an exponential penalty for \"very empty\" bins based on a capacity ratio. This is a sophisticated approach that balances tight fits with avoiding overly empty bins. It's more nuanced than pure Best Fit (Heuristic 4).\n\nOverall: The top heuristics (1-3, 8-10) attempt to combine Best Fit with a secondary objective like favoring fuller bins or penalizing under-utilized bins using additive or multiplicative factors. Heuristics 14-20 represent a more refined version of this by using a normalized Best Fit score combined with an adaptive penalty. Heuristics 7 and 11-13 try to penalize large remaining capacities, but the implementation in 7 is problematic, while 11-13 are better. Pure Best Fit (4, 6) is simpler but less adaptive.\n- \nHere's a redefined self-reflection for designing better heuristics, focusing on actionable insights:\n\n*   **Keywords:** Hybrid heuristics, weighted combination, conditional logic, penalty mechanisms.\n*   **Advice:** Design hybrid heuristics that dynamically adjust their criteria. Explore conditional logic to switch between primary and secondary metrics based on the problem state (e.g., bin fullness).\n*   **Avoid:** Over-reliance on a single, static metric (like inverse of remaining gap) as it can be brittle. Avoid overly complex multiplicative combinations that are hard to tune.\n*   **Explanation:** Pure \"best fit\" is often suboptimal. Combining metrics thoughtfully, perhaps by penalizing certain outcomes or prioritizing states, leads to more robust and adaptable heuristics that can better navigate diverse problem instances.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}