{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritizes bins using a hybrid approach: Best Fit for immediate fit,\n    and a penalty for remaining capacity to encourage fuller bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # If no bins can fit, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n    \n    # Get remaining capacities for bins that can fit the item\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Calculate 'Best Fit' score: inverse of remaining gap\n    # This prioritizes bins with minimal wasted space (smallest gap)\n    epsilon = 1e-9\n    best_fit_scores = 1.0 / (valid_bins_remain_cap - item + epsilon)\n    \n    # Calculate a 'slack' score: inverse of remaining capacity after fit\n    # This aims to penalize bins that will have a lot of remaining space,\n    # encouraging fuller bins overall.\n    slack_scores = 1.0 / (valid_bins_remain_cap + epsilon)\n    \n    # Combine scores. A simple average can work, or a weighted sum.\n    # Here, we'll use a simple average as a starting point.\n    # This combines the immediate \"tightest fit\" with a tendency towards fuller bins.\n    combined_scores = (best_fit_scores + slack_scores) / 2.0\n    \n    # Assign the combined scores to the priorities array for valid bins\n    priorities[can_fit_mask] = combined_scores\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        best_fit_diff = suitable_bins_cap - item\n        \n        # Adaptive prioritization: Higher priority for bins that are almost full,\n        # but can still fit the item. This aims to \"fill up\" bins more efficiently.\n        # We can achieve this by giving a higher score to smaller remaining capacities\n        # after placing the item.\n        \n        # Calculate a \"fit score\" which is inversely related to the remaining capacity after placement.\n        # Add a small epsilon to avoid division by zero if an item perfectly fills a bin.\n        fit_scores = 1.0 / (best_fit_diff + 1e-9)\n        \n        # Normalize scores to prevent extreme values from dominating\n        if np.max(fit_scores) > 0:\n            normalized_fit_scores = fit_scores / np.max(fit_scores)\n        else:\n            normalized_fit_scores = np.zeros_like(fit_scores)\n\n        # Introduce a small penalty for bins that are very empty.\n        # This encourages packing into partially filled bins before opening new ones.\n        # The penalty is higher for bins with much more remaining capacity than the item size.\n        capacity_ratio = suitable_bins_cap / item if item > 0 else np.inf\n        empty_bin_penalty = np.exp(-0.5 * (capacity_ratio - 1)) * 0.2 # Exponential decay penalty\n        \n        # Combine fit score and penalty. Higher values are better.\n        combined_priorities = normalized_fit_scores - empty_bin_penalty\n        \n        # Assign the calculated priorities to the original indices\n        original_indices = np.where(suitable_bins_mask)[0]\n        priorities[original_indices] = combined_priorities\n\n    return priorities\n\n### Analyze & experience\n- *   **Comparing (1st) vs (2nd):** Heuristic 1 uses a simple average of 'best fit' and 'slack' scores, aiming to balance immediate fit with fuller bins. Heuristic 2 combines 'best fit' with an 'almost full' boost, prioritizing very tight fits. Heuristic 1's approach seems more balanced by explicitly considering slack.\n*   **Comparing (2nd) vs (3rd):** Heuristic 2 is identical to Heuristic 1, but with different internal calculation names and a slightly different combination logic (addition vs. averaging). Heuristic 3 is identical to Heuristic 1. There seems to be a misunderstanding in the ranking or code provided, as 1, 2, and 3 are very similar. However, focusing on the *intended* logic of Heuristic 2 (adding `almost_full_boost`), it might overemphasize \"almost full\" bins compared to the balanced approach of Heuristic 1/3.\n*   **Comparing (3rd) vs (4th):** Heuristic 3 (and 1) uses a scoring system based on inverse capacities. Heuristic 4 implements a pure \"Best Fit\" by assigning a priority of 1.0 to bins with the minimum gap, and 0 otherwise. Heuristic 3's nuanced scoring is generally better than a binary Best Fit, as it allows for finer selection among good fits.\n*   **Comparing (4th) vs (5th):** Heuristic 4 is pure Best Fit. Heuristic 5 attempts to normalize Best Fit scores and add a \"fullness\" score, but the combination (addition) and re-normalization might lead to unintended weighting. Heuristic 4's simplicity and directness of Best Fit might be more robust than Heuristic 5's complex normalization and combination.\n*   **Comparing (5th) vs (6th):** Heuristic 5 attempts normalization and a weighted combination. Heuristic 6 is identical to Heuristic 4 (pure Best Fit). This reinforces that Heuristic 4 (pure Best Fit) is likely superior to Heuristic 5's complicated approach.\n*   **Comparing (7th) and (11th/12th/13th):** Heuristic 7 proposes a score of `(1.0 / gaps) - bins_that_can_fit_caps`. This attempts to combine Best Fit with a penalty for large bin capacity, which can be good. However, the direct subtraction might lead to negative priorities where the penalty outweighs the Best Fit score, making interpretation and comparison difficult. Heuristics 11-13 implement a similar idea with a conditional penalty for \"too much\" remaining space. This conditional penalty (Heuristics 11-13) is a more controlled way to handle the \"overly large remaining capacity\" issue than Heuristic 7's direct subtraction.\n*   **Comparing (8th) vs (2nd):** Heuristic 8 is identical to Heuristic 2.\n*   **Comparing (9th) vs (1st):** Heuristic 9 weights 'waste reduction' (similar to Best Fit) and 'capacity utilization' (inverse of remaining capacity). This is a reasonable hybrid. Heuristic 1 averages these concepts. Heuristic 9's weighted approach might offer better control than Heuristic 1's simple average.\n*   **Comparing (10th) vs (1st):** Heuristic 10 combines Best Fit with a factor that *reduces* priority for bins with *more* remaining capacity. This aims to penalize lightly used bins. Heuristic 1's \"slack score\" (inverse of remaining capacity) also favors fuller bins. Heuristic 10's multiplicative approach might be more effective at modulating Best Fit scores.\n*   **Comparing (14th-20th) vs (4th):** Heuristics 14-20 are identical. They combine a normalized Best Fit score with an exponential penalty for \"very empty\" bins based on a capacity ratio. This is a sophisticated approach that balances tight fits with avoiding overly empty bins. It's more nuanced than pure Best Fit (Heuristic 4).\n\nOverall: The top heuristics (1-3, 8-10) attempt to combine Best Fit with a secondary objective like favoring fuller bins or penalizing under-utilized bins using additive or multiplicative factors. Heuristics 14-20 represent a more refined version of this by using a normalized Best Fit score combined with an adaptive penalty. Heuristics 7 and 11-13 try to penalize large remaining capacities, but the implementation in 7 is problematic, while 11-13 are better. Pure Best Fit (4, 6) is simpler but less adaptive.\n- \nHere's a redefined self-reflection for designing better heuristics, focusing on actionable insights:\n\n*   **Keywords:** Hybrid heuristics, weighted combination, conditional logic, penalty mechanisms.\n*   **Advice:** Design hybrid heuristics that dynamically adjust their criteria. Explore conditional logic to switch between primary and secondary metrics based on the problem state (e.g., bin fullness).\n*   **Avoid:** Over-reliance on a single, static metric (like inverse of remaining gap) as it can be brittle. Avoid overly complex multiplicative combinations that are hard to tune.\n*   **Explanation:** Pure \"best fit\" is often suboptimal. Combining metrics thoughtfully, perhaps by penalizing certain outcomes or prioritizing states, leads to more robust and adaptable heuristics that can better navigate diverse problem instances.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}