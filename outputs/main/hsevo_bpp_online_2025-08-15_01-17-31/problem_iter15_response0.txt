```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    
    bin_capacity = bins_remain_cap + item # Assuming this is how it works, if a bin can fit the item
    
    suitable_bins_mask = bins_remain_cap >= item
    
    if not np.any(suitable_bins_mask):
        return priorities
    
    suitable_bins_cap_remaining = bins_remain_cap[suitable_bins_mask]
    
    # Objective 1: Tightness of fit (similar to Best Fit)
    # Calculate remaining capacity after placing the item
    remaining_after_fit = suitable_bins_cap_remaining - item
    
    # Normalize tightness score (smaller remaining capacity is better, so invert and normalize)
    # Add a small epsilon to avoid division by zero if all remaining capacities are zero
    normalized_tightness = 1.0 - (remaining_after_fit / (np.max(suitable_bins_cap_remaining) + 1e-9))
    
    # Objective 2: Fullness of bin (prioritize bins that are already fuller)
    # This encourages consolidating items into fewer bins
    # Normalize by total bin capacity (which is constant or inferred)
    # Assuming a max bin capacity of 1.0 for normalization if not provided explicitly
    # If bin_capacity is not available or is dynamic, this part needs adjustment.
    # For now, let's assume bin capacity is 1.0 for normalization.
    # The capacity of the bin *before* adding the item is suitable_bins_cap_remaining + item
    # If we assume a common bin capacity 'C', then it would be (C - suitable_bins_cap_remaining) / C
    # Let's use the average of the original capacities of suitable bins for normalization if C is unknown.
    
    # A more robust approach if bin capacity 'C' is unknown and bins might have different capacities
    # Or if we want to consider the *current* fullness of the bin.
    # Let's normalize by the maximum *possible* remaining capacity among suitable bins if we don't have global C
    
    # If we assume a standard bin capacity 'C', this would be more effective.
    # For now, let's try to prioritize bins that are already more full relative to their *potential* remaining capacity
    # after placing the item.
    
    # Let's use the reciprocal of the remaining capacity *after* placing the item, normalized.
    # A smaller remaining capacity means a fuller bin.
    # We want to maximize this metric.
    
    # Higher score for bins that will have less space left
    # Add a small epsilon to avoid division by zero
    bin_fullness_score = 1.0 / (remaining_after_fit + 1e-9)
    
    # Normalize the bin_fullness_score. The highest score is the most full bin.
    # We want to map this to a [0, 1] range where 1 is best.
    normalized_fullness = bin_fullness_score / np.max(bin_fullness_score + 1e-9)
    
    # Combine objectives with adaptive weighting
    # We can start with equal weights and adjust based on performance if this was part of a learning system.
    # For a static heuristic, we'll use fixed weights, but the *idea* is adaptivity.
    # Let's give a slight edge to tightness, but also consider fullness.
    
    # Example weighting: 60% tightness, 40% fullness.
    # This can be tuned. The idea is to combine them smoothly.
    
    # Weights can be adjusted, perhaps based on item size (larger items might prioritize tightness more)
    # For now, let's use fixed weights.
    weight_tightness = 0.6
    weight_fullness = 0.4
    
    # Composite priority score
    composite_priority = (weight_tightness * normalized_tightness) + (weight_fullness * normalized_fullness)
    
    # Apply priorities to the original bins_remain_cap indices
    priorities[suitable_bins_mask] = composite_priority
    
    return priorities
```
