{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit strategy with an adaptive logarithmic penalty for remaining capacity.\n    Prioritizes bins that minimize wasted space after packing, with a nuanced penalty\n    for larger remaining capacities to avoid overly aggressive bin selection.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Best Fit: Minimize remaining capacity after packing\n    # Calculate the difference between remaining capacity and item size\n    remaining_after_fit = suitable_bins_cap - item\n    \n    # Adaptive penalty: Use a logarithmic function of the ratio of remaining capacity to item size.\n    # This penalizes larger gaps more, but with diminishing returns (smoother than linear).\n    # Add a small epsilon to the denominator to avoid division by zero if item size is 0 (though unlikely in BPP).\n    # Add 1 to the denominator to ensure values are not excessively large when remaining_after_fit is small.\n    penalty = np.log1p(remaining_after_fit / (item + 1e-9))\n    \n    # Normalize the penalty to be between 0 and 1. Higher penalty should result in lower priority.\n    # We want to invert this relationship, so we use (1 - normalized_penalty).\n    if np.max(penalty) > 0:\n        normalized_penalty = penalty / np.max(penalty)\n        normalized_scores = 1.0 - normalized_penalty\n    else:\n        normalized_scores = np.ones_like(suitable_bins_cap) # All penalties were zero or negative (unlikely with log1p)\n\n    # Assign priorities to the original indices\n    original_indices = np.where(suitable_bins_mask)[0]\n    priorities[original_indices] = normalized_scores\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit strategy with an adaptive logarithmic penalty for remaining capacity.\n    Prioritizes bins that minimize wasted space after packing, with a nuanced penalty\n    for larger remaining capacities to avoid overly aggressive bin selection.\n\n    Args:\n        item (float): The size of the item to be packed.\n        bins_remain_cap (np.ndarray): A numpy array representing the remaining capacity of each bin.\n        epsilon (float): A small constant added to the denominator to prevent division by zero.\n        log_offset (float): A constant added to the denominator of the log function to ensure values are not excessively large.\n\n    Returns:\n        np.ndarray: A numpy array representing the priority of each bin for the given item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1-2 (identical) vs. Heuristics 4-6 (identical): Heuristics 1-2 use a weighted combination of normalized differences and capacities, while 4-6 implement a strict Best Fit by picking the minimum difference. Heuristics 1-2 are more nuanced by considering the overall bin capacity, potentially leading to better packing density.\n\nComparing Heuristics 3 & 5-6 (identical) vs. Heuristics 7-9 & 13-15 & 18-19 (identical): Heuristics 3, 5-6 use a sigmoid-based penalty for remaining capacity in conjunction with Best Fit. Heuristics 7-9, 13-15, 18-19 use a similar concept but with different formulations (sigmoid vs. log) and parameters. Heuristic 7, in particular, introduces an adaptive penalty based on the capacity-to-item ratio, which is more dynamic than a fixed sigmoid. The ranking suggests that the specific implementation of these combined strategies matters.\n\nComparing Heuristics 7 vs. 10-12 vs. 20: Heuristics 10-12 and 20 refine the Best Fit with penalties, with 10-12 using a sigmoid-like penalty and 20 adding an overall capacity bias. Heuristic 7 uses a sigmoid-like penalty but on the capacity-to-item ratio, which might be more robust than penalizing raw remaining capacity directly. Heuristic 20's explicit addition of a capacity bias might lead to over-optimization for specific scenarios.\n\nComparing Heuristics 16-17 (identical) vs. others: Heuristics 16-17 combine Best Fit and First Fit with adaptive weights based on capacity standard deviation. This is a novel approach that attempts to balance tightness and fragmentation. However, their lower ranking suggests this adaptive weighting might not always be superior or is perhaps less effectively implemented than the refined Best Fit with penalties.\n\nComparing Heuristics 13-15 & 18-19 (identical) vs. Heuristics 8-9 (identical): Heuristics 13-15 and 18-19 seem to be incomplete versions of the strategy in Heuristics 8-9, which implement Best Fit with a logarithmic penalty. The absence of the actual calculation in the former makes them worse.\n\nOverall: The best heuristics (1-3, 7, 10-12, 16-17, 20) generally combine a strong Best Fit component with some form of penalty or score for remaining capacity, often using sigmoid or logarithmic functions for smooth adjustments. Normalization and adaptive weighting (like in 7, 10-12, 16-17) appear to be key for robust performance. Strict Best Fit (4-6) and incomplete implementations are worst.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Adaptive penalties, multi-objective optimization, normalization, weighting, stability, generalization.\n\n*   **Advice:** Design heuristics that balance multiple, potentially conflicting objectives (e.g., fit tightness, bin fullness, minimizing bin usage) using adaptive penalty functions and robust normalization techniques. Focus on creating a composite score that generalizes well across diverse problem instances.\n\n*   **Avoid:** Purely \"greedy\" single-metric optimization (like minimizing only the remaining gap), overly aggressive or linear penalty functions, additive combinations of unnormalized scores, and hardcoded parameters without considering numerical stability or generalization.\n\n*   **Explanation:** By integrating adaptive penalties and normalization, heuristics become more resilient and less susceptible to overfitting on specific data distributions. This allows for a more nuanced trade-off between objectives, leading to better overall performance and broader applicability, rather than relying on simple, potentially brittle, single-criterion decisions.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}