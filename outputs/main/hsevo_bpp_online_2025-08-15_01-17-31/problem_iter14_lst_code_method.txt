{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        \n        diffs = suitable_bins_cap - item\n        \n        \n        normalized_diffs = 1.0 - (diffs / np.max(suitable_bins_cap))\n        \n        \n        normalized_capacities = suitable_bins_cap / np.max(bins_remain_cap)\n        \n        \n        combined_scores = (0.6 * normalized_diffs) + (0.4 * normalized_capacities)\n        \n        \n        max_score = np.max(combined_scores)\n        best_fit_indices_in_suitable = np.where(combined_scores == max_score)[0]\n        \n        original_indices = np.where(suitable_bins_mask)[0]\n        \n        for idx in best_fit_indices_in_suitable:\n            priorities[original_indices[idx]] = 1.0\n            \n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        \n        diffs = suitable_bins_cap - item\n        \n        \n        normalized_diffs = 1.0 - (diffs / np.max(suitable_bins_cap))\n        \n        \n        normalized_capacities = suitable_bins_cap / np.max(bins_remain_cap)\n        \n        \n        combined_scores = (0.6 * normalized_diffs) + (0.4 * normalized_capacities)\n        \n        \n        max_score = np.max(combined_scores)\n        best_fit_indices_in_suitable = np.where(combined_scores == max_score)[0]\n        \n        original_indices = np.where(suitable_bins_mask)[0]\n        \n        for idx in best_fit_indices_in_suitable:\n            priorities[original_indices[idx]] = 1.0\n            \n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a normalized, sigmoid-based penalty for remaining capacity.\n    Prioritizes tight fits while smoothly penalizing bins with excessive empty space.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n    penalty_strength = 5.0 # Tune this parameter to control penalty intensity\n\n    can_fit_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if valid_bins_remain_cap.size > 0:\n        remaining_after_fit = valid_bins_remain_cap - item\n        \n        # Best Fit score: inverse of the remaining space for tighter fits\n        best_fit_scores = 1.0 / (remaining_after_fit + epsilon)\n        \n        # Sigmoid-based penalty: Penalizes large remaining capacities smoothly.\n        # Scales the remaining capacity relative to the item size and bin capacity (implicitly)\n        # The sigmoid function ensures the penalty grows but saturates.\n        normalized_remaining = remaining_after_fit / np.maximum(bins_remain_cap[can_fit_mask], epsilon)\n        penalty_scores = 1.0 / (1.0 + np.exp(penalty_strength * (1.0 - normalized_remaining))) # Penalizes larger remaining space\n        \n        # Combine Best Fit with penalty: subtract penalty from the score\n        combined_priorities = best_fit_scores - penalty_scores\n        \n        # Normalize combined priorities to [0, 1] for consistent comparison\n        min_p, max_p = np.min(combined_priorities), np.max(combined_priorities)\n        if max_p > min_p:\n            normalized_priorities = (combined_priorities - min_p) / (max_p - min_p)\n        else:\n            normalized_priorities = np.ones_like(combined_priorities) * 0.5 # All scores are same\n            \n        original_indices = np.where(can_fit_mask)[0]\n        priorities[original_indices] = normalized_priorities\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        best_fit_diff = suitable_bins_cap - item\n        \n        min_diff = np.min(best_fit_diff)\n        \n        best_fit_indices = np.where(best_fit_diff == min_diff)[0]\n        \n        original_indices = np.where(suitable_bins_mask)[0]\n        \n        for idx in best_fit_indices:\n            priorities[original_indices[idx]] = 1.0\n    \n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a normalized, sigmoid-based penalty for remaining capacity.\n    Prioritizes tight fits while smoothly penalizing bins with excessive empty space.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n    penalty_strength = 5.0 # Tune this parameter to control penalty intensity\n\n    can_fit_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if valid_bins_remain_cap.size > 0:\n        remaining_after_fit = valid_bins_remain_cap - item\n        \n        # Best Fit score: inverse of the remaining space for tighter fits\n        best_fit_scores = 1.0 / (remaining_after_fit + epsilon)\n        \n        # Sigmoid-based penalty: Penalizes large remaining capacities smoothly.\n        # Scales the remaining capacity relative to the item size and bin capacity (implicitly)\n        # The sigmoid function ensures the penalty grows but saturates.\n        normalized_remaining = remaining_after_fit / np.maximum(bins_remain_cap[can_fit_mask], epsilon)\n        penalty_scores = 1.0 / (1.0 + np.exp(penalty_strength * (1.0 - normalized_remaining))) # Penalizes larger remaining space\n        \n        # Combine Best Fit with penalty: subtract penalty from the score\n        combined_priorities = best_fit_scores - penalty_scores\n        \n        # Normalize combined priorities to [0, 1] for consistent comparison\n        min_p, max_p = np.min(combined_priorities), np.max(combined_priorities)\n        if max_p > min_p:\n            normalized_priorities = (combined_priorities - min_p) / (max_p - min_p)\n        else:\n            normalized_priorities = np.ones_like(combined_priorities) * 0.5 # All scores are same\n            \n        original_indices = np.where(can_fit_mask)[0]\n        priorities[original_indices] = normalized_priorities\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        best_fit_diff = suitable_bins_cap - item\n        \n        min_diff = np.min(best_fit_diff)\n        \n        best_fit_indices = np.where(best_fit_diff == min_diff)[0]\n        \n        original_indices = np.where(suitable_bins_mask)[0]\n        \n        for idx in best_fit_indices:\n            priorities[original_indices[idx]] = 1.0\n    \n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines strong Best Fit with an adaptive, smoothed penalty based on capacity-to-item ratio.\n    Prioritizes tight fits while gracefully penalizing bins with disproportionately large remaining space.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Consider only bins that can fit the item\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if valid_bins_remain_cap.size > 0:\n        # Calculate the remaining capacity after placing the item\n        remaining_after_fit = valid_bins_remain_cap - item\n        \n        # Best Fit component: prioritize smaller gaps (higher score for smaller gaps).\n        # Add epsilon for numerical stability.\n        best_fit_scores = 1.0 / (remaining_after_fit + 1e-9)\n        \n        # Adaptive Penalty component: Penalize based on the ratio of remaining space to the item size.\n        # A higher ratio (more wasted space relative to the item) gets a higher penalty (lower priority).\n        # Using a sigmoid-like function to smoothly increase the penalty as the ratio exceeds a threshold.\n        # This provides a more nuanced penalty than a simple logarithmic function.\n        penalty_threshold_ratio = 1.5 # Ratio where penalty starts to increase significantly\n        penalty_steepness = 0.8 # Controls how quickly the penalty increases\n        \n        # Calculate penalty: sigmoid function ensures values between 0 and 1.\n        # Penalty is high when capacity/item ratio is much larger than threshold.\n        # Add epsilon to item in denominator to prevent division by zero for zero-sized items.\n        capacity_ratio = valid_bins_remain_cap / (item + 1e-9)\n        penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (capacity_ratio - penalty_threshold_ratio)))\n        \n        # Combine Best Fit score with penalty (subtract penalty from score)\n        combined_scores = best_fit_scores - penalty\n        \n        # Normalize the combined scores to be between 0 and 1.\n        # This makes scores comparable across different item/bin configurations and ensures positive priorities.\n        min_score = np.min(combined_scores)\n        max_score = np.max(combined_scores)\n        \n        if max_score > min_score:\n            normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n        else:\n            # If all valid bins have the same combined score, assign a uniform medium priority\n            normalized_scores = np.full_like(combined_scores, 0.5)\n            \n        # Assign the calculated priorities back to the original indices\n        original_indices = np.where(can_fit_mask)[0]\n        priorities[original_indices] = normalized_scores\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit strategy with an adaptive logarithmic penalty for remaining capacity.\n    Prioritizes bins that minimize wasted space after packing, with a nuanced penalty\n    for larger remaining capacities to avoid overly aggressive bin selection.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Best Fit: Minimize remaining capacity after packing\n    # Calculate the difference between remaining capacity and item size\n    remaining_after_fit = suitable_bins_cap - item\n    \n    # Adaptive penalty: Use a logarithmic function of the ratio of remaining capacity to item size.\n    # This penalizes larger gaps more, but with diminishing returns (smoother than linear).\n    # Add a small epsilon to the denominator to avoid division by zero if item size is 0 (though unlikely in BPP).\n    # Add 1 to the denominator to ensure values are not excessively large when remaining_after_fit is small.\n    penalty = np.log1p(remaining_after_fit / (item + 1e-9))\n    \n    # Normalize the penalty to be between 0 and 1. Higher penalty should result in lower priority.\n    # We want to invert this relationship, so we use (1 - normalized_penalty).\n    if np.max(penalty) > 0:\n        normalized_penalty = penalty / np.max(penalty)\n        normalized_scores = 1.0 - normalized_penalty\n    else:\n        normalized_scores = np.ones_like(suitable_bins_cap) # All penalties were zero or negative (unlikely with log1p)\n\n    # Assign priorities to the original indices\n    original_indices = np.where(suitable_bins_mask)[0]\n    priorities[original_indices] = normalized_scores\n    \n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit strategy with an adaptive logarithmic penalty for remaining capacity.\n    Prioritizes bins that minimize wasted space after packing, with a nuanced penalty\n    for larger remaining capacities to avoid overly aggressive bin selection.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Best Fit: Minimize remaining capacity after packing\n    # Calculate the difference between remaining capacity and item size\n    remaining_after_fit = suitable_bins_cap - item\n    \n    # Adaptive penalty: Use a logarithmic function of the ratio of remaining capacity to item size.\n    # This penalizes larger gaps more, but with diminishing returns (smoother than linear).\n    # Add a small epsilon to the denominator to avoid division by zero if item size is 0 (though unlikely in BPP).\n    # Add 1 to the denominator to ensure values are not excessively large when remaining_after_fit is small.\n    penalty = np.log1p(remaining_after_fit / (item + 1e-9))\n    \n    # Normalize the penalty to be between 0 and 1. Higher penalty should result in lower priority.\n    # We want to invert this relationship, so we use (1 - normalized_penalty).\n    if np.max(penalty) > 0:\n        normalized_penalty = penalty / np.max(penalty)\n        normalized_scores = 1.0 - normalized_penalty\n    else:\n        normalized_scores = np.ones_like(suitable_bins_cap) # All penalties were zero or negative (unlikely with log1p)\n\n    # Assign priorities to the original indices\n    original_indices = np.where(suitable_bins_mask)[0]\n    priorities[original_indices] = normalized_scores\n    \n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a smooth, adaptive penalty for large remaining capacities,\n    enhanced by normalization for stable multi-bin selection.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    \n    suitable_bins_caps = bins_remain_cap[can_fit_mask]\n    \n    if suitable_bins_caps.size > 0:\n        gaps = suitable_bins_caps - item\n        \n        # Best Fit component: prioritize smaller gaps. Use a scaled inverse for emphasis.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Adaptive penalty for large remaining capacity using a sigmoid-like function.\n        # This smoothly penalizes bins with significantly more capacity than needed.\n        penalty_threshold_ratio = 2.0 \n        penalty_steepness = 0.7 \n        \n        # Calculate penalty: high when capacity >> item * ratio, approaches 0 otherwise.\n        large_capacity_penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (suitable_bins_caps / (item + 1e-9) - penalty_threshold_ratio)))\n        \n        # Combine scores: Subtract penalty from Best Fit score.\n        combined_scores = best_fit_scores - large_capacity_penalty\n        \n        # Normalize scores to [0, 1] for stable comparison across different item sizes.\n        # This helps in situations where absolute score differences might vary wildly.\n        if combined_scores.size > 0:\n            min_score = np.min(combined_scores)\n            max_score = np.max(combined_scores)\n            if max_score - min_score > 1e-9:\n                normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n            else:\n                normalized_scores = np.ones_like(combined_scores) * 0.5 # Assign neutral score if all scores are same\n        else:\n            normalized_scores = np.array([])\n\n        priorities[can_fit_mask] = normalized_scores\n        \n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a smooth, adaptive penalty for large remaining capacities.\n    Prioritizes tight fits while penalizing significantly underfilled bins gracefully.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    \n    suitable_bins_caps = bins_remain_cap[can_fit_mask]\n    \n    if suitable_bins_caps.size > 0:\n        gaps = suitable_bins_caps - item\n        \n        # Best Fit component: prioritize smaller gaps (higher score for smaller gaps)\n        # Using 1/(gap + epsilon) provides a good base score.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Adaptive penalty for large remaining capacity:\n        # Penalize bins where remaining capacity is much larger than the item size.\n        # Using a sigmoid-like function centered around a ratio (e.g., 2.0)\n        # to smoothly increase penalty as capacity exceeds item size significantly.\n        # A steepness factor controls how quickly the penalty ramps up.\n        penalty_threshold_ratio = 2.0 \n        penalty_steepness = 0.7 \n        \n        large_capacity_penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (suitable_bins_caps / (item + 1e-9) - penalty_threshold_ratio)))\n        \n        # Combine scores: Subtract the penalty from the best-fit score.\n        combined_scores = best_fit_scores - large_capacity_penalty\n        \n        priorities[can_fit_mask] = combined_scores\n        \n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a smooth, adaptive penalty for large remaining capacities.\n    Prioritizes tight fits while penalizing significantly underfilled bins gracefully.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    \n    suitable_bins_caps = bins_remain_cap[can_fit_mask]\n    \n    if suitable_bins_caps.size > 0:\n        gaps = suitable_bins_caps - item\n        \n        # Best Fit component: prioritize smaller gaps (higher score for smaller gaps)\n        # Using 1/(gap + epsilon) provides a good base score.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Adaptive penalty for large remaining capacity:\n        # Penalize bins where remaining capacity is much larger than the item size.\n        # Using a sigmoid-like function centered around a ratio (e.g., 2.0)\n        # to smoothly increase penalty as capacity exceeds item size significantly.\n        # A steepness factor controls how quickly the penalty ramps up.\n        penalty_threshold_ratio = 2.0 \n        penalty_steepness = 0.7 \n        \n        large_capacity_penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (suitable_bins_caps / (item + 1e-9) - penalty_threshold_ratio)))\n        \n        # Combine scores: Subtract the penalty from the best-fit score.\n        combined_scores = best_fit_scores - large_capacity_penalty\n        \n        priorities[can_fit_mask] = combined_scores\n        \n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 5.188930652054812e-07, log_offset: float = 4.4238923109801185) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit strategy with an adaptive logarithmic penalty for remaining capacity.\n    Prioritizes bins that minimize wasted space after packing, with a nuanced penalty\n    for larger remaining capacities to avoid overly aggressive bin selection.\n\n    Args:\n        item (float): The size of the item to be packed.\n        bins_remain_cap (np.ndarray): A numpy array representing the remaining capacity of each bin.\n        epsilon (float): A small constant added to the denominator to prevent division by zero.\n        log_offset (float): A constant added to the denominator of the log function to ensure values are not excessively large.\n\n    Returns:\n        np.ndarray: A numpy array representing the priority of each bin for the given item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n[Heuristics 14th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 5.188930652054812e-07, log_offset: float = 4.4238923109801185) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit strategy with an adaptive logarithmic penalty for remaining capacity.\n    Prioritizes bins that minimize wasted space after packing, with a nuanced penalty\n    for larger remaining capacities to avoid overly aggressive bin selection.\n\n    Args:\n        item (float): The size of the item to be packed.\n        bins_remain_cap (np.ndarray): A numpy array representing the remaining capacity of each bin.\n        epsilon (float): A small constant added to the denominator to prevent division by zero.\n        log_offset (float): A constant added to the denominator of the log function to ensure values are not excessively large.\n\n    Returns:\n        np.ndarray: A numpy array representing the priority of each bin for the given item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n[Heuristics 15th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 5.188930652054812e-07, log_offset: float = 4.4238923109801185) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit strategy with an adaptive logarithmic penalty for remaining capacity.\n    Prioritizes bins that minimize wasted space after packing, with a nuanced penalty\n    for larger remaining capacities to avoid overly aggressive bin selection.\n\n    Args:\n        item (float): The size of the item to be packed.\n        bins_remain_cap (np.ndarray): A numpy array representing the remaining capacity of each bin.\n        epsilon (float): A small constant added to the denominator to prevent division by zero.\n        log_offset (float): A constant added to the denominator of the log function to ensure values are not excessively large.\n\n    Returns:\n        np.ndarray: A numpy array representing the priority of each bin for the given item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # Best Fit Component: Prioritize bins that leave the least remaining space\n        best_fit_diff = suitable_bins_cap - item\n        min_diff = np.min(best_fit_diff)\n        best_fit_scores = np.exp(-best_fit_diff / (np.mean(bins_remain_cap) + 1e-6)) # Exponential decay, scaled by average remaining capacity\n        \n        # First Fit Component (implicitly handled by order but can be boosted): Prioritize bins that are used earlier\n        # For online, earlier bins are those with lower indices. We can use inverse index for priority.\n        original_indices = np.where(suitable_bins_mask)[0]\n        first_fit_scores = 1.0 / (original_indices + 1.0) \n        \n        # Combine components with adaptive weighting\n        # Weighting can be adaptive based on the distribution of remaining capacities.\n        # If capacities are very spread out, Best Fit might be more important.\n        # If capacities are similar, First Fit might help with fragmentation.\n        \n        capacity_std = np.std(bins_remain_cap)\n        capacity_mean = np.mean(bins_remain_cap)\n        \n        # Heuristic weighting: more weight to first fit if std is low (bins are similar)\n        # more weight to best fit if std is high (bins are diverse)\n        ff_weight = np.exp(-capacity_std / (capacity_mean + 1e-6)) \n        bf_weight = 1.0 - ff_weight\n        \n        combined_scores = bf_weight * best_fit_scores + ff_weight * first_fit_scores\n        \n        # Normalize scores to be between 0 and 1\n        if np.max(combined_scores) > 0:\n            normalized_scores = combined_scores / np.max(combined_scores)\n        else:\n            normalized_scores = combined_scores # Should not happen if suitable bins exist\n\n        # Assign priorities to the original indices\n        priorities[original_indices] = normalized_scores\n        \n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # Best Fit Component: Prioritize bins that leave the least remaining space\n        best_fit_diff = suitable_bins_cap - item\n        min_diff = np.min(best_fit_diff)\n        best_fit_scores = np.exp(-best_fit_diff / (np.mean(bins_remain_cap) + 1e-6)) # Exponential decay, scaled by average remaining capacity\n        \n        # First Fit Component (implicitly handled by order but can be boosted): Prioritize bins that are used earlier\n        # For online, earlier bins are those with lower indices. We can use inverse index for priority.\n        original_indices = np.where(suitable_bins_mask)[0]\n        first_fit_scores = 1.0 / (original_indices + 1.0) \n        \n        # Combine components with adaptive weighting\n        # Weighting can be adaptive based on the distribution of remaining capacities.\n        # If capacities are very spread out, Best Fit might be more important.\n        # If capacities are similar, First Fit might help with fragmentation.\n        \n        capacity_std = np.std(bins_remain_cap)\n        capacity_mean = np.mean(bins_remain_cap)\n        \n        # Heuristic weighting: more weight to first fit if std is low (bins are similar)\n        # more weight to best fit if std is high (bins are diverse)\n        ff_weight = np.exp(-capacity_std / (capacity_mean + 1e-6)) \n        bf_weight = 1.0 - ff_weight\n        \n        combined_scores = bf_weight * best_fit_scores + ff_weight * first_fit_scores\n        \n        # Normalize scores to be between 0 and 1\n        if np.max(combined_scores) > 0:\n            normalized_scores = combined_scores / np.max(combined_scores)\n        else:\n            normalized_scores = combined_scores # Should not happen if suitable bins exist\n\n        # Assign priorities to the original indices\n        priorities[original_indices] = normalized_scores\n        \n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 5.188930652054812e-07, log_offset: float = 4.4238923109801185) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit strategy with an adaptive logarithmic penalty for remaining capacity.\n    Prioritizes bins that minimize wasted space after packing, with a nuanced penalty\n    for larger remaining capacities to avoid overly aggressive bin selection.\n\n    Args:\n        item (float): The size of the item to be packed.\n        bins_remain_cap (np.ndarray): A numpy array representing the remaining capacity of each bin.\n        epsilon (float): A small constant added to the denominator to prevent division by zero.\n        log_offset (float): A constant added to the denominator of the log function to ensure values are not excessively large.\n\n    Returns:\n        np.ndarray: A numpy array representing the priority of each bin for the given item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n[Heuristics 19th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 5.188930652054812e-07, log_offset: float = 4.4238923109801185) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit strategy with an adaptive logarithmic penalty for remaining capacity.\n    Prioritizes bins that minimize wasted space after packing, with a nuanced penalty\n    for larger remaining capacities to avoid overly aggressive bin selection.\n\n    Args:\n        item (float): The size of the item to be packed.\n        bins_remain_cap (np.ndarray): A numpy array representing the remaining capacity of each bin.\n        epsilon (float): A small constant added to the denominator to prevent division by zero.\n        log_offset (float): A constant added to the denominator of the log function to ensure values are not excessively large.\n\n    Returns:\n        np.ndarray: A numpy array representing the priority of each bin for the given item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a calibrated logarithmic penalty for excessive remaining capacity,\n    favoring bins that offer a tight fit while moderately penalizing large unused space.\n    Also incorporates a slight bias towards bins with less remaining capacity overall.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    gaps = suitable_bins_caps - item\n\n    # Best Fit Score: Higher for smaller gaps (tighter fit). Add epsilon for stability.\n    best_fit_score = 1.0 / (gaps + 1e-6)\n\n    # Calibrated Logarithmic Penalty: Penalizes large gaps more significantly but smoothly.\n    # The penalty increases logarithmically with the gap size.\n    penalty_factor = 0.1\n    excess_capacity_penalty = penalty_factor * np.log1p(gaps)\n\n    # Additional heuristic: Favor bins with less remaining capacity overall (less waste)\n    # This is a simple additive term that slightly boosts bins with less overall capacity.\n    # Normalizing this to be on a similar scale as the best_fit_score.\n    # We subtract it to prefer smaller remaining capacities.\n    # Avoid division by zero by adding a small epsilon.\n    overall_capacity_bias = 1.0 / (suitable_bins_caps + 1e-6)\n    normalized_capacity_bias = (overall_capacity_bias - np.min(overall_capacity_bias)) / (np.max(overall_capacity_bias) - np.min(overall_capacity_bias) + 1e-9)\n\n\n    # Combine scores: Subtract the penalty from the best-fit score, and also subtract the bias for larger capacities.\n    combined_priorities = best_fit_score - excess_capacity_penalty - normalized_capacity_bias * 0.2 # Adjust weight for bias\n\n    # Normalize priorities to be between 0 and 1 for consistent behavior\n    min_priority = np.min(combined_priorities)\n    max_priority = np.max(combined_priorities)\n    if max_priority > min_priority:\n        normalized_priorities = (combined_priorities - min_priority) / (max_priority - min_priority)\n    else:\n        normalized_priorities = np.ones_like(combined_priorities) * 0.5 # Default if all scores are same\n\n    priorities[suitable_bins_mask] = normalized_priorities\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}