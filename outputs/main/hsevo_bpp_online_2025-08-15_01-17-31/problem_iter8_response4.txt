```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines Best Fit with an adaptive exponential penalty for remaining capacity.

    Prioritizes tight fits while adaptively penalizing bins that leave
    excessive, disproportionate remaining space, promoting balanced packing.
    """
    priorities = np.full_like(bins_remain_cap, -np.inf)
    
    can_fit_mask = bins_remain_cap >= item
    
    if np.any(can_fit_mask):
        suitable_bins_caps = bins_remain_cap[can_fit_mask]
        
        # Best Fit component: inverse of the remaining capacity after packing
        # Higher score for smaller remaining capacity (tighter fit)
        epsilon = 1e-9
        fit_scores = 1.0 / (suitable_bins_caps - item + epsilon)
        
        # Adaptive Penalty component: Penalizes bins that leave a large remaining capacity
        # relative to the item size. Uses an exponential decay for smoother penalty.
        # The penalty scales with the ratio of remaining capacity to item size,
        # making it more aggressive for larger items.
        if item > epsilon: # Avoid division by zero or near-zero item sizes
            capacity_ratio = (suitable_bins_caps - item) / item
            # Apply exponential penalty: decay slows down as ratio increases
            # Penalty is higher for smaller ratios (closer fits) which is counter-intuitive to the goal.
            # Let's invert the ratio logic: penalize *larger* ratios more.
            # We want a high score for small remaining capacity, and a low score for large remaining capacity.
            # Best fit score is already good. The penalty should *reduce* the score.
            # Penalize when remaining_capacity - item is large.
            # A simple way is to subtract a value that increases with (remaining_capacity - item).
            # Let's rethink the penalty: we want to *lower* the score of bins with *large* remaining capacity.
            # So, the penalty term should be *added* to the score, and the penalty term itself should be *larger*
            # for bins with large remaining capacity.
            
            # Let's try to penalize based on the absolute remaining capacity and the item size.
            # We want to prioritize bins that are "almost full" after placing the item.
            # A bin that leaves a lot of empty space should be penalized.
            
            # Consider the "gap" after placing the item: gap = suitable_bins_caps - item
            gaps = suitable_bins_caps - item
            
            # We want to penalize large gaps. A linear penalty might be too harsh.
            # An exponential penalty can be smoother.
            # Let's use a penalty that decreases as the gap gets smaller (closer to 0).
            # So the penalty value itself should be high for large gaps.
            # Example: penalty = C * exp(alpha * gap) where C and alpha are positive constants.
            # This penalty should be SUBTRACTED from the fit_scores.
            
            penalty_strength = 0.1 # Tunable factor for penalty magnitude
            penalty_decay = 0.5    # Tunable factor for how quickly penalty decreases with gap
            
            # Higher penalty for larger gaps. Use a logarithmic penalty for smoothness.
            # Ensure gap is non-negative, which it is because of can_fit_mask.
            # Add epsilon to avoid log(0).
            log_penalty = np.log1p(gaps) # log(1 + gap) handles gap=0 and small gaps gracefully
            
            # The combined score should reflect that a good fit (small gap) is good,
            # but a *very* small gap (almost no space left) is also good.
            # A large gap is bad.
            # Let's combine Best Fit score with a penalty that REDUCES the score for large gaps.
            # So, penalty_value should be proportional to the gap.
            
            penalty_term = penalty_strength * log_penalty
            
            # Combine: higher fit_scores are good. Lower penalty_term is good (means smaller gap).
            # We want to subtract the penalty_term from the fit_scores.
            # Score = fit_score - penalty_term
            combined_priorities = fit_scores - penalty_term

        else: # Handle case where item size is zero or very small
            combined_priorities = fit_scores # Pure best fit if item is negligible

        # Normalize priorities to a range, e.g., [0, 1] for easier comparison/selection
        # This makes the heuristic less sensitive to the absolute scale of bin capacities.
        min_priority = np.min(combined_priorities)
        max_priority = np.max(combined_priorities)
        
        if max_priority - min_priority > epsilon:
            normalized_priorities = (combined_priorities - min_priority) / (max_priority - min_priority)
        else:
            normalized_priorities = np.zeros_like(combined_priorities)

        priorities[can_fit_mask] = normalized_priorities

    return priorities
```
