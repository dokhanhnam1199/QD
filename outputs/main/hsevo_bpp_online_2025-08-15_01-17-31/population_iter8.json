[
  {
    "stdout_filepath": "problem_iter8_response0.txt_stdout.txt",
    "code_path": "problem_iter8_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a dynamic penalty based on the ratio of remaining capacity to item size.\n    Prioritizes tight fits while penalizing bins that leave disproportionately large empty space.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Consider only bins that can fit the item\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if valid_bins_remain_cap.size > 0:\n        # Calculate the remaining capacity after placing the item\n        remaining_after_fit = valid_bins_remain_cap - item\n        \n        # Best Fit component: inverse of remaining space for tighter fits.\n        epsilon = 1e-9\n        best_fit_scores = 1.0 / (remaining_after_fit + epsilon)\n        \n        # Dynamic Penalty component: Penalize based on the ratio of remaining space to the item size.\n        # A higher ratio (more wasted space relative to the item) gets a higher penalty (lower priority).\n        # Using log to dampen the effect of very large remaining spaces.\n        # The penalty factor is tuned to be significant but not overwhelming.\n        penalty_factor = 0.5  # Controls the strength of the penalty\n        # Avoid division by zero or log of non-positive values for penalty calculation\n        penalty_terms = np.maximum(remaining_after_fit, epsilon) / np.maximum(item, epsilon)\n        penalty = penalty_factor * np.log1p(penalty_terms) # Use log1p for better numerical stability near 0\n\n        # Combine Best Fit score with penalty (subtract penalty from score)\n        # This effectively reduces the priority of bins with large relative remaining capacity.\n        combined_priorities = best_fit_scores - penalty\n        \n        # Normalize the combined scores to be between 0 and 1\n        # This makes scores comparable across different item/bin configurations.\n        min_priority = np.min(combined_priorities)\n        max_priority = np.max(combined_priorities)\n        \n        if max_priority > min_priority:\n            normalized_priorities = (combined_priorities - min_priority) / (max_priority - min_priority)\n        else:\n            # If all valid bins have the same combined score, assign a uniform priority\n            normalized_priorities = np.ones_like(combined_priorities) * 0.5\n            \n        # Assign the calculated priorities back to the original indices\n        original_indices = np.where(can_fit_mask)[0]\n        priorities[original_indices] = normalized_priorities\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 187.48684196024655,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response1.txt_stdout.txt",
    "code_path": "problem_iter8_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines normalized Best Fit with a sigmoid penalty for large remaining capacity.\n    This balances fitting items tightly while discouraging excessive underutilization.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # Best Fit component: Minimize remaining capacity after packing\n        remaining_after_fit = suitable_bins_cap - item\n        \n        # Normalize Best Fit scores: higher is better (tighter fit)\n        # Invert and scale to [0, 1]\n        min_rem = np.min(remaining_after_fit)\n        max_rem = np.max(remaining_after_fit)\n        if max_rem > min_rem:\n            best_fit_scores = 1.0 - (remaining_after_fit - min_rem) / (max_rem - min_rem)\n        else:\n            best_fit_scores = np.ones_like(remaining_after_fit) # All gaps are the same, equal priority from BF\n\n        # Penalty component: Discourage bins that leave a large amount of space\n        # Use a sigmoid-like penalty that approaches 0 for small remaining capacities and 1 for large ones.\n        # Penalty is applied based on the ratio of remaining capacity to item size.\n        # This is inspired by heuristics using exponential penalties.\n        penalty_threshold_ratio = 1.5 # Start penalizing significantly when remaining > 1.5 * item\n        penalty_steepness = 1.0 # Controls how quickly the penalty ramps up\n        \n        # Calculate capacity ratio relative to the item size\n        capacity_ratios = suitable_bins_cap / item\n        \n        # Sigmoid penalty: penalizes bins with large capacity_ratios\n        # penalty = 1 / (1 + exp(-steepness * (ratio - threshold)))\n        # We want a penalty that is low for good fits and high for bad fits.\n        # So, we want to penalize when ratio is high.\n        # Let's define penalty as something that increases with capacity_ratio.\n        # A simple approach: penalty = ratio itself, then normalize.\n        # Or use a sigmoid on the *inverse* of what we want (i.e., on badness)\n        # We want to penalize when `suitable_bins_cap - item` is large relative to `item`.\n        # So penalize when `suitable_bins_cap / item` is large.\n        # Let's use a penalty that is 0 for ratio=1 and increases.\n        # A simple penalty: (capacity_ratio - 1)\n        # A more refined penalty: sigmoid applied to a scaled difference.\n        # We want penalty to be 0 when ratio is small, and 1 when ratio is large.\n        # Let's try: penalty = 1 - sigmoid(-steepness * (capacity_ratio - penalty_threshold_ratio))\n        # This means penalty is 0 when ratio < threshold, and 1 when ratio is much larger.\n        # Alternative: Directly use the best_fit_score and subtract a penalty based on remaining space.\n        # Heuristic 18/19/20 use `best_fit_scores - large_capacity_penalty`.\n        # Let's model our penalty similar to that, but using the ratio.\n        \n        # Penalty term: large when remaining_after_fit is large relative to item size\n        # We want to reduce the priority if the bin is too empty.\n        # Let's use a sigmoid that approaches 1 as remaining_after_fit / item grows large.\n        # penalty_value = 1.0 / (1.0 + np.exp(-penalty_steepness * (remaining_after_fit - item * penalty_threshold_ratio) / (item + 1e-9))) # Adjusted for item size\n        \n        # Simpler penalty: based on the ratio of remaining capacity to bin capacity, or just remaining capacity.\n        # Let's follow the 18/19/20 approach which subtracts a penalty from best_fit_scores.\n        # The penalty should be higher for bins that leave more space relative to the item.\n        # Consider penalty = (remaining_after_fit / (suitable_bins_cap + 1e-9))\n        # This is the proportion of space left. We want to penalize large proportions.\n        # Normalize this proportion to create a penalty score.\n        \n        # Let's use a penalty that is zero for perfect fits and grows.\n        # Penalty based on remaining space, scaled by item size to be relative.\n        # penalty_component = remaining_after_fit / (item + 1e-9)\n        # Apply a softening/thresholding function to this penalty.\n        # penalty_strength = 0.2 # Scale the penalty\n        # combined_priorities = best_fit_scores - penalty_strength * (np.tanh((penalty_component - 1.0) / 2.0) + 1.0) / 2.0 # Clamp penalty roughly [0, 0.5]\n        \n        # Let's try a combination inspired by 18/19/20: subtract a sigmoid penalty.\n        # The penalty should be low for small remaining_after_fit and high for large ones.\n        # Let's use a sigmoid that maps remaining_after_fit to [0, 1].\n        # Scale the input to sigmoid so that the penalty is significant around a certain excess capacity.\n        # Penalty increases when `remaining_after_fit > threshold * item`.\n        penalty_input = (remaining_after_fit - item * penalty_threshold_ratio) / (item + 1e-9)\n        sigmoid_penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * penalty_input))\n        \n        # Combine: Best Fit score minus the penalty. Higher score is better.\n        # The penalty score is [0, 1], so subtracting it reduces the priority.\n        combined_priorities = best_fit_scores - sigmoid_penalty * 0.7 # Scale penalty effect (0.7 is a tunable parameter)\n        \n        # Ensure priorities are non-negative\n        combined_priorities = np.maximum(combined_priorities, 0)\n        \n        # Assign the calculated priorities to the original indices\n        original_indices = np.where(suitable_bins_mask)[0]\n        priorities[original_indices] = combined_priorities\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 262.5724044505044,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response2.txt_stdout.txt",
    "code_path": "problem_iter8_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a dynamic penalty for excessive remaining capacity,\n    prioritizing tighter fits while penalizing bins that are significantly emptier.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        suitable_bins_caps = bins_remain_cap[can_fit_mask]\n        \n        # Best Fit component: Inverse of the gap (remaining capacity after packing).\n        # Favors bins with smaller gaps.\n        gaps = suitable_bins_caps - item\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Penalty component: Penalize bins that leave a large absolute remaining capacity.\n        # We subtract the remaining capacity directly. This aims to use bins that are\n        # closer to the item size among those that are feasible.\n        # This is a more direct penalty than the ratio-based one, similar to v0.\n        penalty = suitable_bins_caps\n        \n        # Combine scores: Best Fit score minus the penalty.\n        # This prioritizes tight fits but discourages using very large bins\n        # even if they offer a \"best fit\" among a set of large bins.\n        combined_scores = best_fit_scores - penalty\n        \n        priorities[can_fit_mask] = combined_scores\n        \n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 66.41714012534482,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response3.txt_stdout.txt",
    "code_path": "problem_iter8_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a scaled penalty for remaining capacity,\n    prioritizing tight fits and moderately penalizing under-filled bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n    \n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Best Fit component: inverse of the gap. Higher score for smaller gaps.\n    epsilon = 1e-9\n    best_fit_scores = 1.0 / (valid_bins_remain_cap - item + epsilon)\n    \n    # Penalty component: Penalize bins with large remaining capacity after packing.\n    # This penalty is relative to the item size to adapt to different item magnitudes.\n    # We use a simple linear penalty that increases with the remaining capacity\n    # scaled by the item size. This is a refinement from simpler additive penalties.\n    # Penalty = penalty_strength * (remaining_capacity / item_size)\n    # We add 1 to the denominator to avoid division by zero if item is 0 and\n    # to ensure a base penalty for any remaining capacity.\n    penalty_strength = 0.2  # Tunable parameter for penalty aggression\n    \n    # Calculate the \"excess\" capacity relative to the item size.\n    # We are penalizing if this excess is large.\n    excess_capacity_ratio = (valid_bins_remain_cap - item) / (item + epsilon)\n    \n    # We want to reduce the priority if excess_capacity_ratio is high.\n    # A simple penalty is to subtract this ratio, scaled by penalty_strength.\n    # This is similar to priority_v0's linear penalty but scaled by item size.\n    penalty = penalty_strength * excess_capacity_ratio\n    \n    # Combine the best fit score with the penalty.\n    # The score is essentially `best_fit_score - penalty`.\n    # A higher score means a better bin choice.\n    combined_priorities = best_fit_scores - penalty\n    \n    # Assign priorities back to the original array structure\n    original_indices = np.where(can_fit_mask)[0]\n    priorities[original_indices] = combined_priorities\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 125.33591475173351,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response4.txt_stdout.txt",
    "code_path": "problem_iter8_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with an adaptive exponential penalty for remaining capacity.\n\n    Prioritizes tight fits while adaptively penalizing bins that leave\n    excessive, disproportionate remaining space, promoting balanced packing.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        suitable_bins_caps = bins_remain_cap[can_fit_mask]\n        \n        # Best Fit component: inverse of the remaining capacity after packing\n        # Higher score for smaller remaining capacity (tighter fit)\n        epsilon = 1e-9\n        fit_scores = 1.0 / (suitable_bins_caps - item + epsilon)\n        \n        # Adaptive Penalty component: Penalizes bins that leave a large remaining capacity\n        # relative to the item size. Uses an exponential decay for smoother penalty.\n        # The penalty scales with the ratio of remaining capacity to item size,\n        # making it more aggressive for larger items.\n        if item > epsilon: # Avoid division by zero or near-zero item sizes\n            capacity_ratio = (suitable_bins_caps - item) / item\n            # Apply exponential penalty: decay slows down as ratio increases\n            # Penalty is higher for smaller ratios (closer fits) which is counter-intuitive to the goal.\n            # Let's invert the ratio logic: penalize *larger* ratios more.\n            # We want a high score for small remaining capacity, and a low score for large remaining capacity.\n            # Best fit score is already good. The penalty should *reduce* the score.\n            # Penalize when remaining_capacity - item is large.\n            # A simple way is to subtract a value that increases with (remaining_capacity - item).\n            # Let's rethink the penalty: we want to *lower* the score of bins with *large* remaining capacity.\n            # So, the penalty term should be *added* to the score, and the penalty term itself should be *larger*\n            # for bins with large remaining capacity.\n            \n            # Let's try to penalize based on the absolute remaining capacity and the item size.\n            # We want to prioritize bins that are \"almost full\" after placing the item.\n            # A bin that leaves a lot of empty space should be penalized.\n            \n            # Consider the \"gap\" after placing the item: gap = suitable_bins_caps - item\n            gaps = suitable_bins_caps - item\n            \n            # We want to penalize large gaps. A linear penalty might be too harsh.\n            # An exponential penalty can be smoother.\n            # Let's use a penalty that decreases as the gap gets smaller (closer to 0).\n            # So the penalty value itself should be high for large gaps.\n            # Example: penalty = C * exp(alpha * gap) where C and alpha are positive constants.\n            # This penalty should be SUBTRACTED from the fit_scores.\n            \n            penalty_strength = 0.1 # Tunable factor for penalty magnitude\n            penalty_decay = 0.5    # Tunable factor for how quickly penalty decreases with gap\n            \n            # Higher penalty for larger gaps. Use a logarithmic penalty for smoothness.\n            # Ensure gap is non-negative, which it is because of can_fit_mask.\n            # Add epsilon to avoid log(0).\n            log_penalty = np.log1p(gaps) # log(1 + gap) handles gap=0 and small gaps gracefully\n            \n            # The combined score should reflect that a good fit (small gap) is good,\n            # but a *very* small gap (almost no space left) is also good.\n            # A large gap is bad.\n            # Let's combine Best Fit score with a penalty that REDUCES the score for large gaps.\n            # So, penalty_value should be proportional to the gap.\n            \n            penalty_term = penalty_strength * log_penalty\n            \n            # Combine: higher fit_scores are good. Lower penalty_term is good (means smaller gap).\n            # We want to subtract the penalty_term from the fit_scores.\n            # Score = fit_score - penalty_term\n            combined_priorities = fit_scores - penalty_term\n\n        else: # Handle case where item size is zero or very small\n            combined_priorities = fit_scores # Pure best fit if item is negligible\n\n        # Normalize priorities to a range, e.g., [0, 1] for easier comparison/selection\n        # This makes the heuristic less sensitive to the absolute scale of bin capacities.\n        min_priority = np.min(combined_priorities)\n        max_priority = np.max(combined_priorities)\n        \n        if max_priority - min_priority > epsilon:\n            normalized_priorities = (combined_priorities - min_priority) / (max_priority - min_priority)\n        else:\n            normalized_priorities = np.zeros_like(combined_priorities)\n\n        priorities[can_fit_mask] = normalized_priorities\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 220.92066675263135,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response5.txt_stdout.txt",
    "code_path": "problem_iter8_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a calibrated logarithmic penalty for excessive remaining capacity,\n    favoring bins that offer a tight fit while moderately penalizing large unused space.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    gaps = suitable_bins_caps - item\n\n    # Best Fit Score: Higher for smaller gaps (tighter fit). Add epsilon for stability.\n    best_fit_score = 1.0 / (gaps + 1e-6)\n\n    # Calibrated Logarithmic Penalty: Penalizes large gaps more significantly but smoothly.\n    # The penalty increases logarithmically with the gap size.\n    # Using np.log1p(gaps) is numerically stable for small gaps and avoids log(0).\n    # A scaling factor (0.1) controls the penalty's impact.\n    penalty_factor = 0.1\n    excess_capacity_penalty = penalty_factor * np.log1p(gaps)\n\n    # Combine scores: Subtract the penalty from the best-fit score.\n    # Higher combined score indicates a preferred bin.\n    combined_priorities = best_fit_score - excess_capacity_penalty\n\n    # Normalize priorities to be between 0 and 1 for consistent behavior\n    min_priority = np.min(combined_priorities)\n    max_priority = np.max(combined_priorities)\n    if max_priority > min_priority:\n        normalized_priorities = (combined_priorities - min_priority) / (max_priority - min_priority)\n    else:\n        normalized_priorities = np.ones_like(combined_priorities) * 0.5 # Default if all scores are same\n\n    priorities[suitable_bins_mask] = normalized_priorities\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 177.87213211613133,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response6.txt_stdout.txt",
    "code_path": "problem_iter8_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a dynamic penalty for remaining capacity,\n    prioritizing tight fits and bins that are not excessively empty,\n    adapting the penalty based on the item size relative to bin capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n    \n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Best Fit component: inverse of the remaining gap after placing the item.\n    # Higher score for smaller gaps (tighter fits).\n    epsilon = 1e-9\n    best_fit_scores = 1.0 / (valid_bins_remain_cap - item + epsilon)\n    \n    # Penalty for \"too much\" remaining capacity.\n    # This penalty is designed to de-prioritize bins that will be left very empty.\n    # We use a term that scales with the remaining capacity relative to the item size,\n    # but also considers the absolute remaining capacity to avoid overly penalizing\n    # small items in large bins.\n    # The penalty should be higher when `valid_bins_remain_cap` is large and `item` is small.\n    # A simpler approach is to penalize based on `valid_bins_remain_cap` directly,\n    # but scaled to be relative to the item's size.\n    # Consider the \"emptiness\" after packing: `valid_bins_remain_cap - item`.\n    # A penalty proportional to this emptiness, possibly dampened by a log, could work.\n    # Let's try to combine Best Fit with a penalty that is stronger for larger remaining capacities,\n    # but also considers that a larger bin might *inherently* have more remaining capacity.\n    # A penalty based on the ratio of remaining capacity to the item size might be too aggressive for small items.\n    # Instead, let's use a penalty that is primarily based on the absolute remaining capacity,\n    # but modulated by the item size.\n    # The idea is to favor bins that have *just enough* space.\n    \n    # Based on analysis, a combination of Best Fit score and a subtractive penalty\n    # that is linear with the remaining capacity after fitting, but with a tunable strength,\n    # seems effective. This is similar to priority_v0, but we'll try to refine the penalty term.\n    # Let's use a penalty that is proportional to the *slack* (`valid_bins_remain_cap - item`)\n    # but also influenced by the *item size* itself to avoid over-penalizing small items in large bins.\n    \n    # Penalty = penalty_strength * (remaining_capacity_after_fit / item)\n    # However, if item is very small, this penalty can be excessive.\n    # Let's try a penalty that combines the absolute slack with a relative slack.\n    \n    # A common strategy is to combine a \"goodness\" score (like Best Fit) with a \"badness\" penalty.\n    # Goodness: 1 / (gap)\n    # Badness: related to remaining capacity.\n    # Let's use a penalty that increases with the remaining capacity after fitting,\n    # but ensure it's not overly aggressive by bounding its influence.\n    \n    # Inspired by `priority_v0` and `priority_v1`'s idea of combining BF with a penalty.\n    # Let's take the Best Fit score and subtract a penalty proportional to the\n    # remaining capacity *after* the item is placed. This rewards tighter fits\n    # and discourages bins that will be left with a large amount of unused space.\n    \n    penalty_strength = 0.3 # Tunable parameter: Controls how much we penalize remaining capacity.\n    \n    # The penalty term: proportional to the remaining capacity after the item is placed.\n    # We subtract this penalty from the best_fit_scores.\n    # This means bins that fit the item tightly (high best_fit_scores) and will have\n    # less remaining capacity (lower penalty) will get higher priority.\n    penalty_value = penalty_strength * (valid_bins_remain_cap - item)\n    \n    # Combine the best fit score with the penalty.\n    combined_priorities = best_fit_scores - penalty_value\n    \n    # Assign the calculated priorities to the original indices\n    original_indices = np.where(can_fit_mask)[0]\n    priorities[original_indices] = combined_priorities\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 95.90827503317318,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response7.txt_stdout.txt",
    "code_path": "problem_iter8_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines normalized Best Fit with a smooth, exponential penalty for large gaps,\n    prioritizing tight fits and penalizing excessive waste more gracefully.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # Best Fit component: Minimize remaining capacity after packing.\n        # Smaller difference means higher score.\n        best_fit_diff = suitable_bins_cap - item\n        \n        # Normalize Best Fit scores to [0, 1], where 1 is the best fit (smallest diff).\n        # Add epsilon to avoid division by zero if all suitable bins have the same remaining space.\n        epsilon = 1e-9\n        if np.max(best_fit_diff) > np.min(best_fit_diff):\n            best_fit_scores = 1.0 - (best_fit_diff - np.min(best_fit_diff)) / (np.max(best_fit_diff) - np.min(best_fit_diff) + epsilon)\n        else:\n            best_fit_scores = np.ones_like(best_fit_diff) # All fits are equally \"best\"\n\n        # Penalty component: Exponential penalty for large remaining capacities (Heuristic 13-16 inspiration).\n        # Penalizes bins where remaining capacity is significantly larger than the item.\n        # A higher ratio (remaining_cap / item) leads to a higher penalty score.\n        # We want to *reduce* the priority for large gaps, so we use this penalty as a subtractive term.\n        # The exponential function provides a smooth transition.\n        capacity_ratio = suitable_bins_cap / item\n        # Penalty grows smoothly as capacity_ratio increases beyond a certain point (e.g., 1.5).\n        # Using exp(-k * (ratio - threshold)) makes it decrease as ratio increases, so we use exp(k * (ratio - threshold)) or similar form.\n        # Let's use a penalty that *increases* with the ratio and subtract it.\n        # Penalty should be low for small ratios and higher for large ratios.\n        # A simple increasing function: log(ratio) or ratio itself.\n        # A more calibrated approach: using exp to make it more sensitive to larger gaps.\n        # Let's try a penalty that is 0 for ratio <= 1 and grows.\n        # We want to *subtract* this penalty from best_fit_scores.\n        # So penalty should be low for good fits and high for bad fits.\n        # A simple penalty: max(0, capacity_ratio - 1.5) * weight\n        # Let's try a smooth penalty: exp( (suitable_bins_cap - item) / item ) - 1.0, scaled.\n        # A penalty inspired by Heuristic 13-16: exp(-penalty_steepness * (ratio - threshold))\n        # This means penalty is high when ratio < threshold and low when ratio > threshold.\n        # We want to penalize *large* remaining capacities, so the penalty should be high when `suitable_bins_cap - item` is large.\n        # Let's reverse the logic of 13-16 and use a penalty that *increases* with remaining capacity.\n        # Penalty: 1.0 / (1.0 + exp(-steepness * (suitable_bins_cap - item))) scaled.\n        # A simpler approach inspired by 4th and 7th: subtract a value proportional to the gap.\n        # Let's try a normalized gap: (suitable_bins_cap - item) / max_bin_capacity or similar.\n        # The 'Analyze & experience' suggests that multiplicative penalties (like in v0) and smooth exponential penalties (like in 13-16) are good.\n        # Let's combine a normalized best fit with a penalty that *decreases* the score if the gap is large.\n        # Penalty strength should increase with the excess capacity.\n        # Penalty = f(excess_capacity / item_size)\n        # Let's use a penalty inspired by 13-16: `1.0 / (1.0 + np.exp(-penalty_steepness * (suitable_bins_cap - item)))`\n        # This penalty is high for small `suitable_bins_cap - item` and low for large ones.\n        # We want to *reduce* the score if `suitable_bins_cap - item` is large.\n        # So we subtract a penalty that *increases* with the gap.\n        # Penalty = k * (suitable_bins_cap - item)\n        # Let's try a smoothed penalty: `np.log1p(suitable_bins_cap - item)` (inspired by 11th/12th) and normalize it.\n        \n        # Let's combine normalized best-fit with a penalty that decreases the score if the remaining capacity is \"too much\" relative to the item size.\n        # Use a penalty term that is high when `suitable_bins_cap - item` is large, and subtract it.\n        # Penalty function: a scaled version of the excess capacity, perhaps using a log scale for smoothness.\n        # Penalty increases with `suitable_bins_cap - item`.\n        excess_capacity = suitable_bins_cap - item\n        \n        # Use a log-based penalty for smoothness, similar to Heuristics 11/12, but ensure it penalizes *large* gaps.\n        # Penalty should be small for small gaps and large for large gaps.\n        # We will subtract this penalty. So we want a function that grows with excess_capacity.\n        # Add epsilon for log(0).\n        log_penalty_raw = np.log1p(excess_capacity) \n        \n        # Normalize the penalty to avoid overly aggressive subtraction.\n        # Scale it relative to the maximum possible log penalty (or a reasonable upper bound).\n        # A simple normalization: divide by max log penalty across suitable bins.\n        if np.max(log_penalty_raw) > epsilon:\n            normalized_penalty = log_penalty_raw / np.max(log_penalty_raw)\n        else:\n            normalized_penalty = np.zeros_like(log_penalty_raw)\n\n        # Combine: Best Fit score minus the normalized penalty.\n        # This means: high best_fit_score is good, high normalized_penalty (large excess capacity) is bad.\n        combined_priorities = best_fit_scores - normalized_penalty\n        \n        # Ensure priorities don't go below zero (optional, but good practice for scores)\n        combined_priorities = np.maximum(combined_priorities, 0)\n\n        # Assign the calculated priorities to the original indices\n        original_indices = np.where(suitable_bins_mask)[0]\n        priorities[original_indices] = combined_priorities\n        \n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 181.11039140121426,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response8.txt_stdout.txt",
    "code_path": "problem_iter8_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a dynamic penalty for remaining capacity relative to item size.\n    Prioritizes tight fits, penalizing bins that will be left excessively empty\n    in proportion to the item being packed.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities # No bin can fit the item\n    \n    # Filter for bins that can fit the item\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # --- Best Fit component ---\n    # Score is inversely proportional to the remaining gap after placing the item.\n    # Higher score for smaller gaps (tighter fits).\n    # Adding epsilon to prevent division by zero.\n    epsilon = 1e-9\n    best_fit_scores = 1.0 / (valid_bins_remain_cap - item + epsilon)\n    \n    # --- Dynamic Penalty component ---\n    # Penalize bins where the remaining capacity after fitting is large relative to the item.\n    # This encourages filling bins more effectively.\n    # The penalty is designed to be less aggressive when the item is small.\n    # We use a term that increases with the ratio (remaining_capacity_after_fit / item).\n    # `remaining_capacity_after_fit` is `valid_bins_remain_cap - item`.\n    # A penalty factor is applied to control its strength.\n    penalty_factor = 0.3 # Tunable parameter for penalty strength\n    \n    # Calculate the penalty: proportional to the ratio of 'slack' to item size.\n    # Use a small epsilon in the denominator to avoid division by zero if item is 0.\n    # Also, cap the penalty to avoid extreme values if `valid_bins_remain_cap - item` is vastly larger than `item`.\n    # This is similar to Heuristic 13/14's approach of using a capacity ratio.\n    slack = valid_bins_remain_cap - item\n    penalty_ratio = slack / (item + epsilon)\n    \n    # Apply a penalty that grows with the penalty_ratio.\n    # We can use a form similar to Heuristic 13/14's exp(-ratio) but inverted and scaled,\n    # or a simpler proportional penalty. Let's try a penalty that increases with slack/item ratio.\n    # A simple linear penalty on the ratio: `penalty_factor * penalty_ratio`\n    # This means if `slack = item`, penalty is `penalty_factor`. If `slack = 2*item`, penalty is `2*penalty_factor`.\n    # This is similar to what Heuristic 14 and 15 explore, but let's directly use `1.0 / (1.0 + penalty_factor * penalty_ratio)`\n    # as a multiplier that reduces the best_fit_score for bins with high slack-to-item ratio.\n    # Alternatively, subtracting a scaled penalty: `best_fit_scores - penalty_factor * penalty_ratio`\n    # Let's try the subtractive approach with a linear penalty on the ratio.\n    \n    # Combine Best Fit score with the penalty.\n    # The combined score is the Best Fit score minus a penalty proportional to the\n    # ratio of (remaining capacity after fitting) to (item size).\n    # This prioritizes tight fits and discourages leaving large proportional gaps.\n    combined_priorities = best_fit_scores - penalty_factor * penalty_ratio\n    \n    # Assign the calculated priorities back to the original bin indices\n    original_indices = np.where(can_fit_mask)[0]\n    priorities[original_indices] = combined_priorities\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 125.33591475173351,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response9.txt_stdout.txt",
    "code_path": "problem_iter8_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a smooth, adaptive penalty for large remaining capacities.\n    Prioritizes tight fits while penalizing significantly underfilled bins gracefully.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    \n    suitable_bins_caps = bins_remain_cap[can_fit_mask]\n    \n    if suitable_bins_caps.size > 0:\n        gaps = suitable_bins_caps - item\n        \n        # Best Fit component: prioritize smaller gaps (higher score for smaller gaps)\n        # Using 1/(gap + epsilon) provides a good base score.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Adaptive penalty for large remaining capacity:\n        # Penalize bins where remaining capacity is much larger than the item size.\n        # Using a sigmoid-like function centered around a ratio (e.g., 2.0)\n        # to smoothly increase penalty as capacity exceeds item size significantly.\n        # A steepness factor controls how quickly the penalty ramps up.\n        penalty_threshold_ratio = 2.0 \n        penalty_steepness = 0.7 # Slightly steeper than v1 for more pronounced penalty\n        \n        # Penalty is high when capacity is much larger than item * ratio, approaches 0 otherwise.\n        # This encourages using bins that are not excessively empty relative to the item.\n        large_capacity_penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (suitable_bins_caps / (item + 1e-9) - penalty_threshold_ratio)))\n        \n        # Combine scores: Subtract the penalty from the best-fit score.\n        # This means a bin with a good fit (high best_fit_score) but also a large\n        # remaining capacity (high penalty) will have its priority reduced.\n        combined_scores = best_fit_scores - large_capacity_penalty\n        \n        priorities[can_fit_mask] = combined_scores\n        \n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 190.19550008653877,
    "exec_success": true
  }
]