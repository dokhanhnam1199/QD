```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines Best Fit with a tuned, ratio-based penalty using a sigmoid function
    for smoother, adaptive penalization, and normalizes scores.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    
    can_fit_mask = bins_remain_cap >= item
    
    if not np.any(can_fit_mask):
        return priorities
    
    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]
    
    # Best Fit component: inverse of the gap. Higher score for smaller gaps.
    epsilon = 1e-9
    best_fit_scores = 1.0 / (valid_bins_remain_cap - item + epsilon)
    
    # Adaptive Penalty component: Penalize bins with large remaining capacity after packing.
    # We use a sigmoid-like function to create a smooth transition and a more nuanced penalty.
    # The penalty is based on the ratio of excess capacity to item size.
    # tunable_penalty_factor controls the steepness of the sigmoid.
    tunable_penalty_factor = 2.0  # Controls the steepness of the penalty curve
    
    # Calculate the "excess" capacity relative to the item size.
    excess_capacity_ratio = (valid_bins_remain_cap - item) / (item + epsilon)
    
    # Sigmoid function: sigmoid(x) = 1 / (1 + exp(-k*x))
    # We apply it to penalize larger ratios.
    # Here, we use exp(-tunable_penalty_factor * excess_capacity_ratio)
    # A higher ratio leads to a smaller exp term, thus a higher penalty value (closer to 1).
    # To convert this to a penalty that *reduces* priority, we can use 1 - sigmoid_value.
    # A higher excess_capacity_ratio should result in a higher penalty term that reduces priority.
    # Let's use the direct penalty ratio: exp(-k * excess_capacity_ratio).
    # This value ranges from 0 (for large ratios) to 1 (for zero or negative ratios).
    # We want to *subtract* this penalty from the best_fit_score.
    penalty_scores = np.exp(-tunable_penalty_factor * excess_capacity_ratio)
    
    # Combine the best fit score with the penalty.
    # Score = best_fit_score * (1 - penalty_score) - This would reduce score if penalty is high.
    # A better combination: Score = best_fit_score - penalty_score * weight
    # Let's try to make the penalty directly subtract from the score.
    # The penalty should be higher for larger excess capacity ratios.
    # The exp(-k * ratio) formulation means penalty_scores are high for low ratios and low for high ratios.
    # This is the opposite of what we want if we're subtracting.
    # Let's re-think: we want to reduce priority if excess_capacity_ratio is high.
    # So, the penalty term should increase with excess_capacity_ratio.
    # Let's use a penalty that is proportional to the ratio itself, but smoothed.
    # The exp(-k * ratio) approach is good for *favoring* smaller ratios.
    # Let's try to combine Best Fit and a penalty that penalizes large *remaining* capacities.
    # The penalty should be such that it reduces the score of bins with a lot of empty space left.
    # A simple penalty: (remaining_capacity) / (item + epsilon)
    # We want to subtract this from the best_fit score.
    
    # Let's revert to the idea of prioritizing tight fits (high best_fit_scores)
    # and then penalizing if there's a lot of space left.
    # We can use the best_fit_score itself as a base, and then subtract a penalty.
    # Penalty: A logistic function could map the remaining capacity to a penalty value.
    # Let's try this:
    # Penalty = C * logistic_function(remaining_capacity / item)
    # where logistic_function(x) = 1 / (1 + exp(-k * x))
    # We want to penalize higher remaining capacities.
    # If remaining_capacity is large, logistic_function will be close to 1.
    # If remaining_capacity is small, logistic_function will be close to 0.
    # So, we want to subtract penalty_strength * logistic_function(excess_capacity_ratio).
    
    penalty_strength = 0.5 # Tunable parameter for penalty aggression
    
    # Use a logistic function (sigmoid) to map the excess capacity ratio to a penalty between 0 and 1.
    # The steeper the slope (larger 'k'), the more sensitive the penalty to the ratio.
    # We use negative excess_capacity_ratio in the sigmoid's exponent to ensure the penalty increases
    # as excess_capacity_ratio increases (i.e., as the bin becomes less of a "tight fit").
    # sigmoid_penalty = 1 / (1 + exp(-k * (ratio - offset)))
    # A simpler form: 1 / (1 + exp(-k * ratio)) for ratio > 0.
    # Let's re-parameterize to make it clearer:
    # We want a penalty P(ratio) such that P(0) is small, P(large) is large.
    # P(ratio) = 1 - (1 / (1 + exp(-k * ratio)))  -- This is 1 - sigmoid, which is logistic_complement.
    # Let's use the standard sigmoid on the negative ratio.
    # `penalty_value = 1.0 / (1.0 + np.exp(tunable_penalty_factor * excess_capacity_ratio))`
    # This penalty_value will be close to 0 for large positive ratios (high excess)
    # and close to 1 for negative ratios (tight fit or overflow).
    # This is still not what we want. We want to *reduce* the score if the ratio is high.
    
    # Let's go back to `best_fit_scores - penalty`.
    # We want `penalty` to increase with `excess_capacity_ratio`.
    # `penalty = penalty_strength * excess_capacity_ratio` is a good start.
    # To make it smoother and adaptive, let's use a function that is bounded or more controlled.
    # Consider `penalty = penalty_strength * (excess_capacity_ratio / (1 + excess_capacity_ratio))`
    # This is a saturating penalty, bounded by `penalty_strength`.
    
    # Let's combine Best Fit with a penalty that is proportional to the excess capacity,
    # but using a smooth function like `atan` or `tanh` might be better.
    # `atan(x)` increases with x but saturates.
    # `atan(excess_capacity_ratio)` ranges from -pi/2 to pi/2.
    # We want to penalize positive excess capacity.
    # `penalty_scores = np.arctan(excess_capacity_ratio)`
    # This will be close to pi/2 for large positive excess capacity. We want to subtract this.
    # `priorities = best_fit_scores - penalty_strength * np.arctan(excess_capacity_ratio)`
    # This would decrease scores for bins with excess capacity.
    # The epsilon in best_fit_scores handles tight fits.
    
    # Let's refine the penalty: we want to penalize bins where `remaining_capacity - item` is large.
    # The ratio `(remaining_capacity - item) / item` is a good measure.
    # We can use a scaled `arctan` for a smooth penalty.
    
    penalty_scale = 1.0 # Controls how much the penalty affects the score.
    
    # Calculate the "gap" after packing, relative to item size.
    gap_ratio = (valid_bins_remain_cap - item) / (item + epsilon)
    
    # Use arctan to create a smooth penalty that increases with gap_ratio.
    # We want to penalize larger gaps, so we subtract this from the best_fit_score.
    # arctan(gap_ratio) is roughly proportional to gap_ratio for small values,
    # and approaches pi/2 for large values, providing saturation.
    # We subtract this scaled penalty.
    # A positive gap_ratio means wasted space. We want to reduce priority for large positive gap_ratio.
    # `np.arctan(gap_ratio)` correctly increases for positive `gap_ratio`.
    
    combined_priorities = best_fit_scores - penalty_scale * np.arctan(gap_ratio)
    
    # Normalize the combined priorities to be in a [0, 1] range for stability.
    min_p = np.min(combined_priorities)
    max_p = np.max(combined_priorities)
    
    if max_p - min_p > epsilon:
        normalized_priorities = (combined_priorities - min_p) / (max_p - min_p)
    else:
        normalized_priorities = np.zeros_like(combined_priorities) # Or some default if all scores are same
        if max_p > -np.inf: # If there was at least one valid score
            normalized_priorities.fill(0.5) # Assign a neutral score if all are same

    # Assign priorities back to the original array structure
    original_indices = np.where(can_fit_mask)[0]
    priorities[original_indices] = normalized_priorities

    return priorities
```
