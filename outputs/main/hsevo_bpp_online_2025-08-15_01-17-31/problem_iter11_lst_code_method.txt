{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a calibrated logarithmic penalty for excessive remaining capacity,\n    favoring bins that offer a tight fit while moderately penalizing large unused space.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    gaps = suitable_bins_caps - item\n\n    # Best Fit Score: Higher for smaller gaps (tighter fit). Add epsilon for stability.\n    best_fit_score = 1.0 / (gaps + 1e-6)\n\n    # Calibrated Logarithmic Penalty: Penalizes large gaps more significantly but smoothly.\n    # The penalty increases logarithmically with the gap size.\n    # Using np.log1p(gaps) is numerically stable for small gaps and avoids log(0).\n    # A scaling factor (0.1) controls the penalty's impact.\n    penalty_factor = 0.1\n    excess_capacity_penalty = penalty_factor * np.log1p(gaps)\n\n    # Combine scores: Subtract the penalty from the best-fit score.\n    # Higher combined score indicates a preferred bin.\n    combined_priorities = best_fit_score - excess_capacity_penalty\n\n    # Normalize priorities to be between 0 and 1 for consistent behavior\n    min_priority = np.min(combined_priorities)\n    max_priority = np.max(combined_priorities)\n    if max_priority > min_priority:\n        normalized_priorities = (combined_priorities - min_priority) / (max_priority - min_priority)\n    else:\n        normalized_priorities = np.ones_like(combined_priorities) * 0.5 # Default if all scores are same\n\n    priorities[suitable_bins_mask] = normalized_priorities\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 0.000677464406492428, penalty_factor: float = 0.4520655504874903) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a dynamic penalty based on the ratio of remaining capacity to item size.\n    Prioritizes tight fits while penalizing bins that leave disproportionately large empty space.\n    \n    Args:\n        item: The size of the item to be placed.\n        bins_remain_cap: A numpy array representing the remaining capacity of each bin.\n        epsilon: A small float to avoid division by zero or log of non-positive values.\n        penalty_factor: Controls the strength of the penalty.\n    \n    Returns:\n        A numpy array of priorities for each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Consider only bins that can fit the item\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if valid_bins_remain_cap.size > 0:\n        # Calculate the remaining capacity after placing the item\n        remaining_after_fit = valid_bins_remain_cap - item\n        \n        # Best Fit component: inverse of remaining space for tighter fits.\n        best_fit_scores = 1.0 / (remaining_after_fit + epsilon)\n        \n        # Dynamic Penalty component: Penalize based on the ratio of remaining space to the item size.\n        # A higher ratio (more wasted space relative to the item) gets a higher penalty (lower priority).\n        # Using log to dampen the effect of very large remaining spaces.\n        # The penalty factor is tuned to be significant but not overwhelming.\n        \n        # Avoid division by zero or log of non-positive values for penalty calculation\n        penalty_terms = np.maximum(remaining_after_fit, epsilon) / np.maximum(item, epsilon)\n        penalty = penalty_factor * np.log1p(penalty_terms) # Use log1p for better numerical stability near 0\n\n        # Combine Best Fit score with penalty (subtract penalty from score)\n        # This effectively reduces the priority of bins with large relative remaining capacity.\n        combined_priorities = best_fit_scores - penalty\n        \n        # Normalize the combined scores to be between 0 and 1\n        # This makes scores comparable across different item/bin configurations.\n        min_priority = np.min(combined_priorities)\n        max_priority = np.max(combined_priorities)\n        \n        if max_priority > min_priority:\n            normalized_priorities = (combined_priorities - min_priority) / (max_priority - min_priority)\n        else:\n            # If all valid bins have the same combined score, assign a uniform priority\n            normalized_priorities = np.ones_like(combined_priorities) * 0.5\n            \n        # Assign the calculated priorities back to the original indices\n        original_indices = np.where(can_fit_mask)[0]\n        priorities[original_indices] = normalized_priorities\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a scaled penalty for remaining capacity,\n    prioritizing tight fits and moderately penalizing under-filled bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n    \n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Best Fit component: inverse of the gap. Higher score for smaller gaps.\n    epsilon = 1e-9\n    best_fit_scores = 1.0 / (valid_bins_remain_cap - item + epsilon)\n    \n    # Penalty component: Penalize bins with large remaining capacity after packing.\n    # This penalty is relative to the item size to adapt to different item magnitudes.\n    # We use a simple linear penalty that increases with the remaining capacity\n    # scaled by the item size. This is a refinement from simpler additive penalties.\n    # Penalty = penalty_strength * (remaining_capacity / item_size)\n    # We add 1 to the denominator to avoid division by zero if item is 0 and\n    # to ensure a base penalty for any remaining capacity.\n    penalty_strength = 0.2  # Tunable parameter for penalty aggression\n    \n    # Calculate the \"excess\" capacity relative to the item size.\n    # We are penalizing if this excess is large.\n    excess_capacity_ratio = (valid_bins_remain_cap - item) / (item + epsilon)\n    \n    # We want to reduce the priority if excess_capacity_ratio is high.\n    # A simple penalty is to subtract this ratio, scaled by penalty_strength.\n    # This is similar to priority_v0's linear penalty but scaled by item size.\n    penalty = penalty_strength * excess_capacity_ratio\n    \n    # Combine the best fit score with the penalty.\n    # The score is essentially `best_fit_score - penalty`.\n    # A higher score means a better bin choice.\n    combined_priorities = best_fit_scores - penalty\n    \n    # Assign priorities back to the original array structure\n    original_indices = np.where(can_fit_mask)[0]\n    priorities[original_indices] = combined_priorities\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines normalized Best Fit with a smooth, exponential penalty for large gaps,\n    prioritizing tight fits and penalizing excessive waste more gracefully.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # Best Fit component: Minimize remaining capacity after packing.\n        # Smaller difference means higher score.\n        best_fit_diff = suitable_bins_cap - item\n        \n        # Normalize Best Fit scores to [0, 1], where 1 is the best fit (smallest diff).\n        # Add epsilon to avoid division by zero if all suitable bins have the same remaining space.\n        epsilon = 1e-9\n        if np.max(best_fit_diff) > np.min(best_fit_diff):\n            best_fit_scores = 1.0 - (best_fit_diff - np.min(best_fit_diff)) / (np.max(best_fit_diff) - np.min(best_fit_diff) + epsilon)\n        else:\n            best_fit_scores = np.ones_like(best_fit_diff) # All fits are equally \"best\"\n\n        # Penalty component: Exponential penalty for large remaining capacities (Heuristic 13-16 inspiration).\n        # Penalizes bins where remaining capacity is significantly larger than the item.\n        # A higher ratio (remaining_cap / item) leads to a higher penalty score.\n        # We want to *reduce* the priority for large gaps, so we use this penalty as a subtractive term.\n        # The exponential function provides a smooth transition.\n        capacity_ratio = suitable_bins_cap / item\n        # Penalty grows smoothly as capacity_ratio increases beyond a certain point (e.g., 1.5).\n        # Using exp(-k * (ratio - threshold)) makes it decrease as ratio increases, so we use exp(k * (ratio - threshold)) or similar form.\n        # Let's use a penalty that *increases* with the ratio and subtract it.\n        # Penalty should be low for small ratios and higher for large ratios.\n        # A simple increasing function: log(ratio) or ratio itself.\n        # A more calibrated approach: using exp to make it more sensitive to larger gaps.\n        # Let's try a penalty that is 0 for ratio <= 1 and grows.\n        # We want to *subtract* this penalty from best_fit_scores.\n        # So penalty should be low for good fits and high for bad fits.\n        # A simple penalty: max(0, capacity_ratio - 1.5) * weight\n        # Let's try a smooth penalty: exp( (suitable_bins_cap - item) / item ) - 1.0, scaled.\n        # A penalty inspired by Heuristic 13-16: exp(-penalty_steepness * (ratio - threshold))\n        # This means penalty is high when ratio < threshold and low when ratio > threshold.\n        # We want to penalize *large* remaining capacities, so the penalty should be high when `suitable_bins_cap - item` is large.\n        # Let's reverse the logic of 13-16 and use a penalty that *increases* with remaining capacity.\n        # Penalty: 1.0 / (1.0 + exp(-steepness * (suitable_bins_cap - item))) scaled.\n        # A simpler approach inspired by 4th and 7th: subtract a value proportional to the gap.\n        # Let's try a normalized gap: (suitable_bins_cap - item) / max_bin_capacity or similar.\n        # The 'Analyze & experience' suggests that multiplicative penalties (like in v0) and smooth exponential penalties (like in 13-16) are good.\n        # Let's combine a normalized best fit with a penalty that *decreases* the score if the gap is large.\n        # Penalty strength should increase with the excess capacity.\n        # Penalty = f(excess_capacity / item_size)\n        # Let's use a penalty inspired by 13-16: `1.0 / (1.0 + np.exp(-penalty_steepness * (suitable_bins_cap - item)))`\n        # This penalty is high for small `suitable_bins_cap - item` and low for large ones.\n        # We want to *reduce* the score if `suitable_bins_cap - item` is large.\n        # So we subtract a penalty that *increases* with the gap.\n        # Penalty = k * (suitable_bins_cap - item)\n        # Let's try a smoothed penalty: `np.log1p(suitable_bins_cap - item)` (inspired by 11th/12th) and normalize it.\n        \n        # Let's combine normalized best-fit with a penalty that decreases the score if the remaining capacity is \"too much\" relative to the item size.\n        # Use a penalty term that is high when `suitable_bins_cap - item` is large, and subtract it.\n        # Penalty function: a scaled version of the excess capacity, perhaps using a log scale for smoothness.\n        # Penalty increases with `suitable_bins_cap - item`.\n        excess_capacity = suitable_bins_cap - item\n        \n        # Use a log-based penalty for smoothness, similar to Heuristics 11/12, but ensure it penalizes *large* gaps.\n        # Penalty should be small for small gaps and large for large gaps.\n        # We will subtract this penalty. So we want a function that grows with excess_capacity.\n        # Add epsilon for log(0).\n        log_penalty_raw = np.log1p(excess_capacity) \n        \n        # Normalize the penalty to avoid overly aggressive subtraction.\n        # Scale it relative to the maximum possible log penalty (or a reasonable upper bound).\n        # A simple normalization: divide by max log penalty across suitable bins.\n        if np.max(log_penalty_raw) > epsilon:\n            normalized_penalty = log_penalty_raw / np.max(log_penalty_raw)\n        else:\n            normalized_penalty = np.zeros_like(log_penalty_raw)\n\n        # Combine: Best Fit score minus the normalized penalty.\n        # This means: high best_fit_score is good, high normalized_penalty (large excess capacity) is bad.\n        combined_priorities = best_fit_scores - normalized_penalty\n        \n        # Ensure priorities don't go below zero (optional, but good practice for scores)\n        combined_priorities = np.maximum(combined_priorities, 0)\n\n        # Assign the calculated priorities to the original indices\n        original_indices = np.where(suitable_bins_mask)[0]\n        priorities[original_indices] = combined_priorities\n        \n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a dynamic penalty for excessive remaining capacity,\n    prioritizing tighter fits while penalizing bins that are significantly emptier.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        suitable_bins_caps = bins_remain_cap[can_fit_mask]\n        \n        # Best Fit component: Inverse of the gap (remaining capacity after packing).\n        # Favors bins with smaller gaps.\n        gaps = suitable_bins_caps - item\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Penalty component: Penalize bins that leave a large absolute remaining capacity.\n        # We subtract the remaining capacity directly. This aims to use bins that are\n        # closer to the item size among those that are feasible.\n        # This is a more direct penalty than the ratio-based one, similar to v0.\n        penalty = suitable_bins_caps\n        \n        # Combine scores: Best Fit score minus the penalty.\n        # This prioritizes tight fits but discourages using very large bins\n        # even if they offer a \"best fit\" among a set of large bins.\n        combined_scores = best_fit_scores - penalty\n        \n        priorities[can_fit_mask] = combined_scores\n        \n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a calibrated logarithmic penalty for excessive remaining capacity,\n    favoring bins that offer a tight fit while moderately penalizing large unused space.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    gaps = suitable_bins_caps - item\n\n    # Best Fit Score: Higher for smaller gaps (tighter fit). Add epsilon for stability.\n    best_fit_score = 1.0 / (gaps + 1e-6)\n\n    # Calibrated Logarithmic Penalty: Penalizes large gaps more significantly but smoothly.\n    # The penalty increases logarithmically with the gap size.\n    # Using np.log1p(gaps) is numerically stable for small gaps and avoids log(0).\n    # A scaling factor (0.1) controls the penalty's impact.\n    penalty_factor = 0.1\n    excess_capacity_penalty = penalty_factor * np.log1p(gaps)\n\n    # Combine scores: Subtract the penalty from the best-fit score.\n    # Higher combined score indicates a preferred bin.\n    combined_priorities = best_fit_score - excess_capacity_penalty\n\n    # Normalize priorities to be between 0 and 1 for consistent behavior\n    min_priority = np.min(combined_priorities)\n    max_priority = np.max(combined_priorities)\n    if max_priority > min_priority:\n        normalized_priorities = (combined_priorities - min_priority) / (max_priority - min_priority)\n    else:\n        normalized_priorities = np.ones_like(combined_priorities) * 0.5 # Default if all scores are same\n\n    priorities[suitable_bins_mask] = normalized_priorities\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 0.000677464406492428, penalty_factor: float = 0.4520655504874903) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a dynamic penalty based on the ratio of remaining capacity to item size.\n    Prioritizes tight fits while penalizing bins that leave disproportionately large empty space.\n    \n    Args:\n        item: The size of the item to be placed.\n        bins_remain_cap: A numpy array representing the remaining capacity of each bin.\n        epsilon: A small float to avoid division by zero or log of non-positive values.\n        penalty_factor: Controls the strength of the penalty.\n    \n    Returns:\n        A numpy array of priorities for each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Consider only bins that can fit the item\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if valid_bins_remain_cap.size > 0:\n        # Calculate the remaining capacity after placing the item\n        remaining_after_fit = valid_bins_remain_cap - item\n        \n        # Best Fit component: inverse of remaining space for tighter fits.\n        best_fit_scores = 1.0 / (remaining_after_fit + epsilon)\n        \n        # Dynamic Penalty component: Penalize based on the ratio of remaining space to the item size.\n        # A higher ratio (more wasted space relative to the item) gets a higher penalty (lower priority).\n        # Using log to dampen the effect of very large remaining spaces.\n        # The penalty factor is tuned to be significant but not overwhelming.\n        \n        # Avoid division by zero or log of non-positive values for penalty calculation\n        penalty_terms = np.maximum(remaining_after_fit, epsilon) / np.maximum(item, epsilon)\n        penalty = penalty_factor * np.log1p(penalty_terms) # Use log1p for better numerical stability near 0\n\n        # Combine Best Fit score with penalty (subtract penalty from score)\n        # This effectively reduces the priority of bins with large relative remaining capacity.\n        combined_priorities = best_fit_scores - penalty\n        \n        # Normalize the combined scores to be between 0 and 1\n        # This makes scores comparable across different item/bin configurations.\n        min_priority = np.min(combined_priorities)\n        max_priority = np.max(combined_priorities)\n        \n        if max_priority > min_priority:\n            normalized_priorities = (combined_priorities - min_priority) / (max_priority - min_priority)\n        else:\n            # If all valid bins have the same combined score, assign a uniform priority\n            normalized_priorities = np.ones_like(combined_priorities) * 0.5\n            \n        # Assign the calculated priorities back to the original indices\n        original_indices = np.where(can_fit_mask)[0]\n        priorities[original_indices] = normalized_priorities\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # Best Fit: Minimize remaining capacity after packing\n        best_fit_diff = suitable_bins_cap - item\n        min_diff_score = 1.0 / (1.0 + best_fit_diff) # Higher score for smaller diff\n        \n        # First Fit: Prioritize bins that have been used longer (lower index)\n        first_fit_score = 1.0 / (1.0 + np.arange(len(suitable_bins_cap)))\n        \n        # Consider bin fullness: prioritize bins that are more full (less remaining capacity)\n        # This is inverse of remaining capacity, normalized by bin capacity (assuming fixed bin capacity, e.g., 1.0)\n        # If bin capacity varies, you would need that information. Assuming bin capacity is 1.0 for simplicity here.\n        # A more robust approach would involve knowing the original bin capacity.\n        # For now, we can use the reciprocal of remaining capacity if it's not too close to zero.\n        fullness_score = np.zeros_like(suitable_bins_cap)\n        non_zero_remain_cap_mask = suitable_bins_cap > 1e-9 # Avoid division by zero\n        fullness_score[non_zero_remain_cap_mask] = 1.0 / suitable_bins_cap[non_zero_remain_cap_mask]\n        \n        # Combine objectives with adaptive weights. For demonstration, let's use fixed weights,\n        # but in a real adaptive system, these would be learned or adjusted.\n        # For now, let's slightly favor best-fit, then first-fit, then fullness.\n        # These weights could be tuned.\n        weight_best_fit = 0.5\n        weight_first_fit = 0.3\n        weight_fullness = 0.2\n        \n        combined_scores = (weight_best_fit * min_diff_score +\n                           weight_first_fit * first_fit_score +\n                           weight_fullness * fullness_score)\n        \n        # Normalize scores to be between 0 and 1\n        if np.max(combined_scores) > 0:\n            normalized_scores = combined_scores / np.max(combined_scores)\n        else:\n            normalized_scores = combined_scores\n        \n        # Assign priorities to the original indices\n        original_indices = np.where(suitable_bins_mask)[0]\n        priorities[original_indices] = normalized_scores\n        \n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # Best Fit: Minimize remaining capacity after packing\n        best_fit_diff = suitable_bins_cap - item\n        min_diff_score = 1.0 / (1.0 + best_fit_diff) # Higher score for smaller diff\n        \n        # First Fit: Prioritize bins that have been used longer (lower index)\n        first_fit_score = 1.0 / (1.0 + np.arange(len(suitable_bins_cap)))\n        \n        # Consider bin fullness: prioritize bins that are more full (less remaining capacity)\n        # This is inverse of remaining capacity, normalized by bin capacity (assuming fixed bin capacity, e.g., 1.0)\n        # If bin capacity varies, you would need that information. Assuming bin capacity is 1.0 for simplicity here.\n        # A more robust approach would involve knowing the original bin capacity.\n        # For now, we can use the reciprocal of remaining capacity if it's not too close to zero.\n        fullness_score = np.zeros_like(suitable_bins_cap)\n        non_zero_remain_cap_mask = suitable_bins_cap > 1e-9 # Avoid division by zero\n        fullness_score[non_zero_remain_cap_mask] = 1.0 / suitable_bins_cap[non_zero_remain_cap_mask]\n        \n        # Combine objectives with adaptive weights. For demonstration, let's use fixed weights,\n        # but in a real adaptive system, these would be learned or adjusted.\n        # For now, let's slightly favor best-fit, then first-fit, then fullness.\n        # These weights could be tuned.\n        weight_best_fit = 0.5\n        weight_first_fit = 0.3\n        weight_fullness = 0.2\n        \n        combined_scores = (weight_best_fit * min_diff_score +\n                           weight_first_fit * first_fit_score +\n                           weight_fullness * fullness_score)\n        \n        # Normalize scores to be between 0 and 1\n        if np.max(combined_scores) > 0:\n            normalized_scores = combined_scores / np.max(combined_scores)\n        else:\n            normalized_scores = combined_scores\n        \n        # Assign priorities to the original indices\n        original_indices = np.where(suitable_bins_mask)[0]\n        priorities[original_indices] = normalized_scores\n        \n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a smooth, adaptive penalty for large remaining capacities.\n    Prioritizes tight fits while penalizing significantly underfilled bins gracefully.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    \n    suitable_bins_caps = bins_remain_cap[can_fit_mask]\n    \n    if suitable_bins_caps.size > 0:\n        gaps = suitable_bins_caps - item\n        \n        # Best Fit component: prioritize smaller gaps (higher score for smaller gaps)\n        # Using 1/(gap + epsilon) provides a good base score.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Adaptive penalty for large remaining capacity:\n        # Penalize bins where remaining capacity is much larger than the item size.\n        # Using a sigmoid-like function centered around a ratio (e.g., 2.0)\n        # to smoothly increase penalty as capacity exceeds item size significantly.\n        # A steepness factor controls how quickly the penalty ramps up.\n        penalty_threshold_ratio = 2.0 \n        penalty_steepness = 0.7 # Slightly steeper than v1 for more pronounced penalty\n        \n        # Penalty is high when capacity is much larger than item * ratio, approaches 0 otherwise.\n        # This encourages using bins that are not excessively empty relative to the item.\n        large_capacity_penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (suitable_bins_caps / (item + 1e-9) - penalty_threshold_ratio)))\n        \n        # Combine scores: Subtract the penalty from the best-fit score.\n        # This means a bin with a good fit (high best_fit_score) but also a large\n        # remaining capacity (high penalty) will have its priority reduced.\n        combined_scores = best_fit_scores - large_capacity_penalty\n        \n        priorities[can_fit_mask] = combined_scores\n        \n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a smooth, adaptive penalty for large remaining capacities.\n    Prioritizes tight fits while penalizing significantly underfilled bins gracefully.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    \n    suitable_bins_caps = bins_remain_cap[can_fit_mask]\n    \n    if suitable_bins_caps.size > 0:\n        gaps = suitable_bins_caps - item\n        \n        # Best Fit component: prioritize smaller gaps (higher score for smaller gaps)\n        # Using 1/(gap + epsilon) provides a good base score.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Adaptive penalty for large remaining capacity:\n        # Penalize bins where remaining capacity is much larger than the item size.\n        # Using a sigmoid-like function centered around a ratio (e.g., 2.0)\n        # to smoothly increase penalty as capacity exceeds item size significantly.\n        # A steepness factor controls how quickly the penalty ramps up.\n        penalty_threshold_ratio = 2.0 \n        penalty_steepness = 0.7 # Slightly steeper than v1 for more pronounced penalty\n        \n        # Penalty is high when capacity is much larger than item * ratio, approaches 0 otherwise.\n        # This encourages using bins that are not excessively empty relative to the item.\n        large_capacity_penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (suitable_bins_caps / (item + 1e-9) - penalty_threshold_ratio)))\n        \n        # Combine scores: Subtract the penalty from the best-fit score.\n        # This means a bin with a good fit (high best_fit_score) but also a large\n        # remaining capacity (high penalty) will have its priority reduced.\n        combined_scores = best_fit_scores - large_capacity_penalty\n        \n        priorities[can_fit_mask] = combined_scores\n        \n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a smooth, adaptive penalty for large remaining capacities.\n    Prioritizes tight fits while penalizing significantly underfilled bins gracefully.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    \n    suitable_bins_caps = bins_remain_cap[can_fit_mask]\n    \n    if suitable_bins_caps.size > 0:\n        gaps = suitable_bins_caps - item\n        \n        # Best Fit component: prioritize smaller gaps (higher score for smaller gaps)\n        # Using 1/(gap + epsilon) provides a good base score.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Adaptive penalty for large remaining capacity:\n        # Penalize bins where remaining capacity is much larger than the item size.\n        # Using a sigmoid-like function centered around a ratio (e.g., 2.0)\n        # to smoothly increase penalty as capacity exceeds item size significantly.\n        # A steepness factor controls how quickly the penalty ramps up.\n        penalty_threshold_ratio = 2.0 \n        penalty_steepness = 0.7 # Slightly steeper than v1 for more pronounced penalty\n        \n        # Penalty is high when capacity is much larger than item * ratio, approaches 0 otherwise.\n        # This encourages using bins that are not excessively empty relative to the item.\n        large_capacity_penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (suitable_bins_caps / (item + 1e-9) - penalty_threshold_ratio)))\n        \n        # Combine scores: Subtract the penalty from the best-fit score.\n        # This means a bin with a good fit (high best_fit_score) but also a large\n        # remaining capacity (high penalty) will have its priority reduced.\n        combined_scores = best_fit_scores - large_capacity_penalty\n        \n        priorities[can_fit_mask] = combined_scores\n        \n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a smooth, adaptive penalty for large remaining capacities.\n    Prioritizes tight fits while penalizing significantly underfilled bins gracefully.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    \n    suitable_bins_caps = bins_remain_cap[can_fit_mask]\n    \n    if suitable_bins_caps.size > 0:\n        gaps = suitable_bins_caps - item\n        \n        # Best Fit component: prioritize smaller gaps (higher score for smaller gaps)\n        # Using 1/(gap + epsilon) provides a good base score.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Adaptive penalty for large remaining capacity:\n        # Penalize bins where remaining capacity is much larger than the item size.\n        # Using a sigmoid-like function centered around a ratio (e.g., 2.0)\n        # to smoothly increase penalty as capacity exceeds item size significantly.\n        # A steepness factor controls how quickly the penalty ramps up.\n        penalty_threshold_ratio = 2.0 \n        penalty_steepness = 0.7 # Slightly steeper than v1 for more pronounced penalty\n        \n        # Penalty is high when capacity is much larger than item * ratio, approaches 0 otherwise.\n        # This encourages using bins that are not excessively empty relative to the item.\n        large_capacity_penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (suitable_bins_caps / (item + 1e-9) - penalty_threshold_ratio)))\n        \n        # Combine scores: Subtract the penalty from the best-fit score.\n        # This means a bin with a good fit (high best_fit_score) but also a large\n        # remaining capacity (high penalty) will have its priority reduced.\n        combined_scores = best_fit_scores - large_capacity_penalty\n        \n        priorities[can_fit_mask] = combined_scores\n        \n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a smooth, adaptive penalty for large remaining capacities.\n    Prioritizes tight fits while penalizing significantly underfilled bins gracefully.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    \n    suitable_bins_caps = bins_remain_cap[can_fit_mask]\n    \n    if suitable_bins_caps.size > 0:\n        gaps = suitable_bins_caps - item\n        \n        # Best Fit component: prioritize smaller gaps (higher score for smaller gaps)\n        # Using 1/(gap + epsilon) provides a good base score.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Adaptive penalty for large remaining capacity:\n        # Penalize bins where remaining capacity is much larger than the item size.\n        # Using a sigmoid-like function centered around a ratio (e.g., 2.0)\n        # to smoothly increase penalty as capacity exceeds item size significantly.\n        # A steepness factor controls how quickly the penalty ramps up.\n        penalty_threshold_ratio = 2.0 \n        penalty_steepness = 0.7 # Slightly steeper than v1 for more pronounced penalty\n        \n        # Penalty is high when capacity is much larger than item * ratio, approaches 0 otherwise.\n        # This encourages using bins that are not excessively empty relative to the item.\n        large_capacity_penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * (suitable_bins_caps / (item + 1e-9) - penalty_threshold_ratio)))\n        \n        # Combine scores: Subtract the penalty from the best-fit score.\n        # This means a bin with a good fit (high best_fit_score) but also a large\n        # remaining capacity (high penalty) will have its priority reduced.\n        combined_scores = best_fit_scores - large_capacity_penalty\n        \n        priorities[can_fit_mask] = combined_scores\n        \n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n[Heuristics 16th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n[Heuristics 17th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n[Heuristics 18th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n[Heuristics 19th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n[Heuristics 20th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}