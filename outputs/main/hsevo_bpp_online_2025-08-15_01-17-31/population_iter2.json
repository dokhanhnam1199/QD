[
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for bins that are too large,\n    prioritizing tighter fits while avoiding extremely large remaining capacities.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    \n    bins_that_can_fit_caps = bins_remain_cap[can_fit_mask]\n    \n    if bins_that_can_fit_caps.size > 0:\n        gaps = bins_that_can_fit_caps - item\n        \n        # Best Fit component: prioritize bins with the smallest gap.\n        # We use 1/(gap + epsilon) which gives higher scores to smaller gaps.\n        best_fit_scores = 1.0 / (gaps + 1e-9)\n        \n        # Additional component: Penalize bins that leave very large remaining capacity.\n        # This can be done by considering the capacity relative to the item size or bin capacity.\n        # Let's penalize bins where `bins_remain_cap` is significantly larger than `item`.\n        # A simple penalty could be based on the reciprocal of the remaining capacity after fitting.\n        # Or, inversely related to the capacity AFTER fitting. Smaller post-fit capacity is better.\n        # Let's use a function that increases with smaller `bins_that_can_fit_caps - item`.\n        # The `best_fit_scores` already do this.\n\n        # Let's try to combine a \"Best Fit\" aspect with a \"First Fit Decreasing\" like preference\n        # for using up existing bins first.\n        # The 1/(gap + epsilon) is a strong \"Best Fit\".\n        \n        # A potential improvement: If multiple bins offer a very tight fit (small gap),\n        # we might prefer the one that has been used more or has less absolute capacity\n        # to keep larger bins available for larger items.\n        \n        # Let's modify the score slightly to favor bins that are \"more full\" among the best fits.\n        # This can be achieved by adding a term that increases with `bins_remain_cap`.\n        # However, this might conflict with \"Best Fit\" if a slightly larger bin is a slightly worse fit.\n        \n        # Let's stick to a refined Best Fit. The core idea is to minimize the leftover space.\n        # The previous `best_fit_scores` are good.\n        # To make it more \"adaptive\" or \"robust\", we can consider a secondary criterion if multiple\n        # bins have very similar small gaps.\n        \n        # Example:\n        # Bins capacities: [10, 10, 10, 10]\n        # Item: 3\n        # Bins remain cap: [2, 5, 8, 9]\n        # Item fits in all. Gaps: [2-3, 5-3, 8-3, 9-3] = [-1, 2, 5, 6] - This is wrong, it's `bins_remain_cap - item`.\n        # Bins remain cap: [2, 5, 8, 9]\n        # Item: 3\n        # Bins remaining capacity: [2, 5, 8, 9]\n        # Gaps: [2-3, 5-3, 8-3, 9-3] -> these are only for bins that fit.\n        # Assume bins_remain_cap = [2, 5, 8, 9], item = 3.\n        # can_fit_mask = [False, True, True, True]\n        # bins_that_can_fit_caps = [5, 8, 9]\n        # gaps = [5-3, 8-3, 9-3] = [2, 5, 6]\n        # best_fit_scores = [1/(2+eps), 1/(5+eps), 1/(6+eps)] = [~0.5, ~0.2, ~0.16]\n        # This prioritizes the bin with remaining capacity 5.\n\n        # Consider the case where `item` itself is very large.\n        # If `item` is close to bin capacity, the gap will be small.\n        # The current `1.0 / (gaps + epsilon)` prioritizes these.\n        \n        # Let's try a slight modification to the priority to encourage using bins\n        # that are \"closer\" to fitting the item without being too small.\n        # This is essentially what Best Fit does.\n        \n        # We can also incorporate a slight bias towards bins that are *not* completely empty,\n        # to encourage filling up partially used bins before starting new ones.\n        # This is more of a \"First Fit\" idea, but can be combined.\n        \n        # Let's try prioritizing bins by their remaining capacity AFTER fitting the item.\n        # We want to MINIMIZE this remaining capacity.\n        # Priority = - (bins_remain_cap - item)\n        # This directly makes smaller positive gaps have higher priority.\n        # Example: gaps = [2, 5, 6] -> priorities = [-2, -5, -6]. Max is -2.\n        # This means the bin with the smallest gap is prioritized.\n\n        # Let's combine the \"tight fit\" with a \"less empty\" preference.\n        # A simple way is to add a term that is inversely related to the remaining capacity.\n        # Priority = (1.0 / (gaps + epsilon)) + log(bins_that_can_fit_caps)\n        # This might be too complex.\n        \n        # A simpler combination: Best Fit score with a small bonus for bins that have\n        # less *total* capacity (to use up partially filled bins first).\n        # So, the priority for fitting bins will be:\n        # score = (1.0 / (gaps + epsilon)) + (1.0 / (bins_that_can_fit_caps + epsilon))\n        # This adds a preference for smaller capacity bins among those with similar gaps.\n        \n        # Let's normalize `bins_that_can_fit_caps` to avoid large values dominating.\n        # Or, let's focus on the `gaps` and add a term that penalizes very large `bins_that_can_fit_caps`\n        # when the gap is also not very small.\n        \n        # Revisit the core goal: pick the bin `j` that minimizes `bins_remain_cap[j] - item`.\n        # The `best_fit_scores = 1.0 / (gaps + 1e-9)` achieves this by giving higher scores\n        # to smaller positive gaps.\n        \n        # Let's try to slightly \"flatten\" the advantage of extremely small gaps,\n        # and give a slight boost to bins that are \"mediumly\" sized and fit well.\n        # This can be done by squaring the gap or using a different function.\n        \n        # Let's use a combination of \"Best Fit\" and \"Worst Fit\" (to keep options open).\n        # No, the goal is best fit.\n        \n        # Consider a heuristic that looks at `bins_remain_cap - item` and `bins_remain_cap`.\n        # We want to minimize `bins_remain_cap - item`.\n        # We also implicitly want to use bins that are not excessively large if the fit is similar.\n        \n        # Let's propose a heuristic that prioritizes bins that have a small gap,\n        # AND among those with small gaps, prefers bins that have less total capacity.\n        # The score for a bin that fits:\n        # priority_score = (1.0 / (bins_that_can_fit_caps - item + 1e-9))  # Best Fit part\n        # Let's add a term that rewards using bins with smaller remaining capacity.\n        # This could be `-(bins_that_can_fit_caps)`.\n        # So, priority = (1.0 / (gaps + 1e-9)) - bins_that_can_fit_caps\n        # This would favor smaller gaps, and among equal gaps, it favors smaller `bins_that_can_fit_caps`.\n        \n        priorities[can_fit_mask] = (1.0 / (gaps + 1e-9)) - bins_that_can_fit_caps\n        \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 108.41805003750011,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response1.txt_stdout.txt",
    "code_path": "problem_iter2_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins by combining Best Fit with adaptive scaling based on bin usage.\n\n    This heuristic favors bins that offer the tightest fit, similar to Best Fit,\n    but also slightly down-weights bins that are already heavily used to encourage\n    opening new bins when appropriate.\n    \"\"\"\n    epsilon = 1e-6\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # Best Fit component: inverse of the gap\n    gaps = suitable_bins_cap - item\n    best_fit_score = 1.0 / (gaps + epsilon)\n\n    # Adaptive component: penalize bins that are already close to full (high usage)\n    # This is a simple heuristic: lower priority for bins with less remaining capacity *relative* to the total capacity.\n    # For simplicity, we'll use the inverse of the remaining capacity itself as a penalty,\n    # effectively favoring bins that have more \"room\" in general, even if they don't offer the tightest fit.\n    # However, to combine with Best Fit, we'll apply a *decreasing* function to this.\n    # A simple way is to multiply by a factor that decreases with remaining capacity.\n    # Let's use remaining capacity itself, normalized and then inverted to make larger remaining capacity *less* penalized.\n    \n    # Normalize remaining capacities to get a sense of usage level (0 to 1)\n    # Use a small epsilon in denominator to avoid division by zero if all suitable bins are empty (though unlikely here)\n    max_cap_for_scaling = np.max(suitable_bins_cap) if np.max(suitable_bins_cap) > 0 else 1.0\n    normalized_remain_cap = suitable_bins_cap / max_cap_for_scaling\n    \n    # We want to slightly reduce priority for bins with very *high* normalized remaining capacity (meaning they are less used)\n    # Conversely, we want to slightly *increase* priority for bins with lower normalized remaining capacity (more used).\n    # A simple scaling factor that increases with normalized remaining capacity (i.e., decreases with usage)\n    # could be (1 - normalized_remain_cap). We'll use this as a multiplier.\n    # This factor will be close to 1 for bins with low remaining capacity (high usage)\n    # and close to 0 for bins with high remaining capacity (low usage).\n    # We want the opposite: high usage should have *higher* priority for the adaptive part.\n    # So, let's use `normalized_remain_cap` directly as a factor, which means higher remaining capacity gets lower priority.\n    # To make it work with multiplication, we want high usage -> high priority.\n    # Let's try: priority = best_fit * (1 - normalized_remain_cap)\n    # This would mean bins with lots of remaining space get penalized.\n    # If we want to encourage using bins that are *more* full (but still fit the item), we'd want to multiply by something that\n    # is *higher* for more used bins.\n    # Let's try: priority = best_fit * (1.0 - normalized_remain_cap + epsilon)\n    # This still penalizes bins with lots of remaining capacity.\n    \n    # A better adaptive component might be to prioritize bins that have a reasonable amount of remaining capacity,\n    # but not too much. This could be a Gaussian-like function centered around some ideal remaining capacity.\n    # For this version, let's stick to a simpler idea: Combine best-fit with a preference for not-too-empty bins.\n    # We can multiply the best_fit score by a factor that encourages bins that are not nearly empty.\n    # Let's use the proportion of the item size to the remaining capacity, capped.\n    # `item / suitable_bins_cap` -> ratio of item size to bin's remaining capacity. High value means tight fit.\n    # Let's try: adaptive_factor = suitable_bins_cap / (suitable_bins_cap + item) -> ratio of remaining capacity to total occupied space.\n    # We want bins that are *more full*, so we want a higher factor for bins with *less* remaining capacity (but still fitting).\n    # Let's try: adaptive_factor = 1.0 - (suitable_bins_cap / max_cap_for_scaling)\n    # This gives higher scores to bins with less remaining capacity.\n    \n    adaptive_factor = (1.0 - normalized_remain_cap) + epsilon\n    \n    combined_priorities = best_fit_score * adaptive_factor\n\n    priorities[suitable_bins_mask] = combined_priorities\n\n    # Normalize priorities to be between 0 and 1 for stability, if needed by downstream logic.\n    # This step is optional and depends on how the priorities are used.\n    # if np.any(priorities[suitable_bins_mask]):\n    #     min_p = np.min(priorities[suitable_bins_mask])\n    #     max_p = np.max(priorities[suitable_bins_mask])\n    #     if max_p - min_p > epsilon:\n    #         priorities[suitable_bins_mask] = (priorities[suitable_bins_mask] - min_p) / (max_p - min_p)\n    #     else:\n    #         priorities[suitable_bins_mask] = 0.5 # All suitable bins have same priority\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 129.32351694048162,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response2.txt_stdout.txt",
    "code_path": "problem_iter2_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines 'Almost Full Fit' and 'Best Fit' for adaptive bin selection.\n\n    Prioritizes bins that offer a tight fit (minimal remaining capacity) and\n    further favors bins that have been historically good fits (implicitly, by\n    being selected more often due to tight fits).\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    can_fit_mask = bins_remain_cap >= item\n\n    fit_capacities = bins_remain_cap[can_fit_mask] - item\n\n    # Base priority: invert remaining capacity (Best Fit principle)\n    # Add a small epsilon to avoid division by zero.\n    base_priorities = 1.0 / (fit_capacities + 1e-6)\n\n    # Boost priority for bins that become \"almost full\" (low residual capacity)\n    # This acts like the 'Almost Full Fit' and reinforces the Best Fit idea\n    # by giving a strong preference to bins with very little leftover space.\n    # We use a negative of the fit capacity. Higher (less negative) values are better.\n    almost_full_boost = -fit_capacities\n\n    # Combine priorities. The inverse capacity provides the primary ranking,\n    # and the 'almost full' boost further refines it, strongly favoring\n    # bins that are very close to full after packing.\n    priorities[can_fit_mask] = base_priorities + almost_full_boost\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 74.23092131656186,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins using a hybrid approach: Best Fit for immediate fit,\n    and a penalty for remaining capacity to encourage fuller bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # If no bins can fit, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n    \n    # Get remaining capacities for bins that can fit the item\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Calculate 'Best Fit' score: inverse of remaining gap\n    # This prioritizes bins with minimal wasted space (smallest gap)\n    epsilon = 1e-9\n    best_fit_scores = 1.0 / (valid_bins_remain_cap - item + epsilon)\n    \n    # Calculate a 'slack' score: inverse of remaining capacity after fit\n    # This aims to penalize bins that will have a lot of remaining space,\n    # encouraging fuller bins overall.\n    slack_scores = 1.0 / (valid_bins_remain_cap + epsilon)\n    \n    # Combine scores. A simple average can work, or a weighted sum.\n    # Here, we'll use a simple average as a starting point.\n    # This combines the immediate \"tightest fit\" with a tendency towards fuller bins.\n    combined_scores = (best_fit_scores + slack_scores) / 2.0\n    \n    # Assign the combined scores to the priorities array for valid bins\n    priorities[can_fit_mask] = combined_scores\n    \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 108.41805003750011,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response4.txt_stdout.txt",
    "code_path": "problem_iter2_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit and Nearly Full Fit for adaptive bin prioritization.\n\n    Prioritizes bins that are almost full, favoring tighter fits to minimize waste.\n    Bins that perfectly fit or nearly fit receive higher scores.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    \n    fit_capacities = bins_remain_cap[can_fit_mask] - item\n    \n    # Strategy: Prioritize bins that leave minimal remaining capacity (Best Fit)\n    # Add a small bonus for bins that become 'almost full' (e.g., very small residual capacity)\n    # This bonus helps break ties for bins with similar small remaining capacities.\n    epsilon = 1e-6\n    priorities[can_fit_mask] = -fit_capacities \n    \n    # Slightly boost bins with very small remaining capacity to encourage 'almost full' fits\n    # This can be seen as a form of 'almost full fit' preference on top of best fit\n    small_residual_mask = fit_capacities < 0.1 * item # Heuristic threshold for 'almost full'\n    priorities[can_fit_mask][small_residual_mask] += 0.1 * (item / (fit_capacities[small_residual_mask] + epsilon))\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 23, in priority_v2\n    base_scores = 1.0 / (remaining_capacities_after_fit + 1e-9)\nnumpy._core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'\n1\n121.01398665684616\n"
  },
  {
    "stdout_filepath": "problem_iter2_response5.txt_stdout.txt",
    "code_path": "problem_iter2_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a softmax approach for adaptive prioritization.\n    Prioritizes bins with minimal remaining capacity after fitting (Best Fit)\n    and applies a softmax to probabilities, allowing for smoother selection.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the remaining capacity for bins that can fit the item\n    remaining_capacities_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # If no bins can fit the item, return all zeros\n    if remaining_capacities_after_fit.size == 0:\n        return priorities\n    \n    # Use the inverse of remaining capacity as a base score (Best Fit idea)\n    # Add a small epsilon to avoid division by zero\n    base_scores = 1.0 / (remaining_capacities_after_fit + 1e-9)\n    \n    # Apply a softmax-like transformation to create probabilities\n    # This smooths the distribution and can be interpreted as selection probabilities\n    # We use exp of base_scores directly, as their relative magnitudes matter\n    # Normalizing by sum is not strictly necessary if only argmax is used,\n    # but it can be useful for other selection methods or interpretation.\n    # Here, we'll use it to ensure values are in a comparable range for selection.\n    \n    # Avoid potential overflow with large base_scores by subtracting max\n    # This is a common practice in softmax implementations.\n    max_score = np.max(base_scores)\n    exp_scores = np.exp(base_scores - max_score)\n    \n    # Normalize to get probabilities, similar to softmax\n    probabilities = exp_scores / np.sum(exp_scores)\n    \n    # Assign the calculated probabilities to the corresponding bins\n    priorities[can_fit_mask] = probabilities\n    \n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 87.56842503028855,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response6.txt_stdout.txt",
    "code_path": "problem_iter2_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins by combining best-fit with a soft penalty for large residuals.\n\n    This heuristic favors bins with minimal remaining capacity after packing (best-fit)\n    while also down-weighting bins that would have a very large residual,\n    using a sigmoid to smooth the transition.\n    \"\"\"\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n    if suitable_bins_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    residuals = suitable_bins_cap - item\n    \n    epsilon = 1e-6\n    best_fit_score = 1.0 / (residuals + epsilon)\n    \n    slope = 0.5\n    penalty_score = 1 / (1 + np.exp(slope * residuals)) \n    \n    combined_priorities = best_fit_score * penalty_score\n    \n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[suitable_bins_mask] = combined_priorities\n    \n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 116.69205856195879,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response7.txt_stdout.txt",
    "code_path": "problem_iter2_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins using a combination of Best Fit and a sigmoid for penalty.\n\n    Combines the 'Best Fit' inverse strategy with a sigmoid to penalize\n    bins with too much remaining capacity, ensuring a balance.\n    \"\"\"\n    available_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(available_bins_mask):\n        return np.zeros_like(bins_remain_cap)\n    \n    available_bins_cap = bins_remain_cap[available_bins_mask]\n    \n    # \"Best Fit\" component: prioritizes bins with minimal remaining space\n    gaps = available_bins_cap - item\n    best_fit_priorities = 1.0 / (gaps + 1e-9) \n    \n    # Sigmoid component: penalizes bins with large remaining capacity\n    # Use a sigmoid to assign lower priority to bins that are \"too empty\"\n    # This helps to fill bins more evenly and avoid creating many partially filled bins\n    slope = 10.0\n    intercept = -5.0\n    sigmoid_penalty = 1 / (1 + np.exp(-(slope * (available_bins_cap) + intercept)))\n    \n    # Combine the two strategies: high priority for tight fits AND not excessively empty bins\n    # We multiply the 'best_fit_priorities' by the 'sigmoid_penalty' to favor bins\n    # that are both a good fit and not excessively large.\n    combined_priorities = best_fit_priorities * sigmoid_penalty\n    \n    final_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    final_priorities[available_bins_mask] = combined_priorities\n    \n    return final_priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 151.30376252379818,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response8.txt_stdout.txt",
    "code_path": "problem_iter2_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for overly large remaining capacity.\n    Prioritizes bins with minimal remaining space after packing, but penalizes\n    bins that would leave excessive space, promoting balanced utilization.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    \n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if valid_bins_remain_cap.size > 0:\n        # Calculate the remaining capacity after placing the item\n        remaining_after_fit = valid_bins_remain_cap - item\n        \n        # Best Fit component: prioritize smaller remaining space\n        # Use inverse of remaining space for higher scores for tighter fits.\n        epsilon = 1e-9\n        best_fit_scores = 1.0 / (remaining_after_fit + epsilon)\n        \n        # Introduce a penalty for leaving too much space (less than \"good enough\" fit)\n        # This component aims to avoid \"overly empty\" bins, promoting better overall distribution.\n        # We can use a sigmoid-like penalty: larger remaining_after_fit leads to a smaller score.\n        # Let's define a threshold for \"too much\" space, e.g., 30% of item size, or a fixed value.\n        # A simple approach is to scale the best_fit_scores down if remaining_after_fit is large.\n        \n        # Let's use a penalty that reduces the priority if remaining_after_fit is large.\n        # A linear penalty might be simple: score = best_fit_score * (1 - alpha * remaining_after_fit)\n        # Or, a capping mechanism: if remaining_after_fit > threshold, reduce score significantly.\n        \n        # Consider a strategy where we want bins that leave very little space,\n        # but also don't want bins that are *almost* empty after packing.\n        # The previous '1 / (remaining_after_fit + epsilon)' strongly favors very small gaps.\n        \n        # Let's refine the 'Best Fit' idea by adding a secondary objective:\n        # prefer bins that become 'moderately' full rather than 'almost completely' full\n        # IF there's a choice between a perfect fit and a slightly less perfect fit that still leaves\n        # a reasonable amount of space. This is tricky in online BPP.\n        \n        # A common strategy is to favor bins that are \"nearly full\" without being *too* full.\n        # The 'Best Fit' strategy by itself already does this by minimizing residual space.\n        \n        # Let's enhance the 'Best Fit' by considering the *original* bin capacity.\n        # Prioritize bins where (remaining_after_fit / original_bin_capacity) is minimized.\n        # This is essentially Best Fit Normalized.\n        \n        # Let's try a hybrid:\n        # 1. Base priority: 1.0 / (remaining_after_fit + epsilon) (Best Fit)\n        # 2. Add a penalty if remaining_after_fit is *too large* relative to the item size.\n        #    This prevents selecting a large bin for a small item just because it's the smallest available.\n        \n        # Let's define a \"good fit\" target, which is a small positive remaining capacity.\n        # We want to reward bins where remaining_after_fit is close to zero.\n        # Let's cap the penalty.\n        \n        # Example:\n        # Item = 0.4, Bins: [0.5, 0.6, 1.0]\n        # Remaining after fit: [0.1, 0.2, 0.6]\n        # Best fit scores: [10, 5, 1.67]\n        \n        # If we want to penalize leaving a lot of space (e.g., 0.6), we could do:\n        # If remaining_after_fit > K * item:\n        #   priority *= penalty_factor (e.g., 0.5)\n        \n        penalty_threshold_factor = 0.5 # Penalize if remaining space is more than 50% of item size\n        penalty_factor = 0.7 # Reduce priority by 30%\n        \n        penalty_mask = remaining_after_fit > (penalty_threshold_factor * item)\n        \n        # Apply the base best fit scores\n        combined_priorities = best_fit_scores.copy()\n        \n        # Apply penalty to bins that leave too much space\n        combined_priorities[penalty_mask] *= penalty_factor\n        \n        priorities[can_fit_mask] = combined_priorities\n    \n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 114.20025299224778,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for bins that are \"too full\",\n    prioritizing bins that are nearly full but can still accommodate the item.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # \"Best Fit\" component: prioritize smaller gaps\n    gaps = suitable_bins_cap - item\n    best_fit_priorities = 1.0 / (gaps + 1e-9)\n\n    # Add a penalty for bins that are \"too full\" after fitting.\n    # This can be achieved by favoring bins with a larger remaining capacity *after* the item is placed,\n    # but not so large that it's wasteful. Let's use a sigmoid-like approach to dampen high remaining capacities.\n    # We want to penalize large gaps (i.e., low priority) and favor small gaps (high priority).\n    # We can invert the gap and apply a function that still emphasizes smaller gaps more.\n    # The heuristic 14-17 uses a sigmoid. Let's adapt that idea to prioritize bins that are *not* too empty.\n    # A simple inversion of remaining capacity after fit is already good. Let's try to make it more robust.\n    \n    # Instead of just inverse, let's combine inverse with a slight preference for larger remaining capacities\n    # to avoid fragmentation. This is a bit counter to pure best-fit.\n    # Let's stick to Best Fit but refine it. The original Best Fit (v0, 3rd, 8th) is generally strong.\n    # The analysis highlighted that inverse of gap is good. v0 is a good baseline.\n    # Let's try to make it more robust by considering the 'tightness' more aggressively.\n    \n    # Combining v0's simplicity with a consideration for multiple \"good\" fits.\n    # The inverse of the gap is a strong indicator. Let's consider what could make it better.\n    # Maybe a slight boost to bins that are \"almost full\" but still fit.\n    \n    # Let's use the inverse of the gap as a base, and add a bonus if the remaining capacity is small.\n    # This is essentially what the inverse gap already does.\n\n    # Let's re-evaluate based on analysis: \"inverse of the remaining gap (1 / (gap + epsilon))\" is a strong indicator.\n    # Heuristic v0 already does this effectively.\n    # The issue might be in *how* many such bins are prioritized.\n    # The analysis on 10th and 18th suggests scaling and exponentiation.\n    # Let's try a simplified approach: normalize the inverse gaps.\n\n    normalized_best_fit_priorities = best_fit_priorities / np.max(best_fit_priorities + 1e-9)\n    \n    # Let's consider a scenario where we want to avoid very large remaining capacities.\n    # A sigmoid might help here, as seen in heuristics 14-17.\n    # If the gap is very small, it's good. If it's moderately large, it's less good.\n    \n    # Let's use the inverse of the gap as a base, and then apply a function that doesn't over-penalize slightly larger gaps.\n    # The sigmoid in 14-17 uses (slope * excess_capacity + intercept). We want higher values for smaller excess_capacity.\n    # So, the input to sigmoid should be negatively correlated with excess_capacity.\n    # Let's try -excess_capacity.\n    \n    slope = 10.0\n    intercept = 5.0 # Increased intercept to shift sigmoid response towards smaller capacities\n    \n    # We want to prioritize smaller gaps. Sigmoid(x) increases with x.\n    # If we input `-(gaps)` into sigmoid, it will prioritize smaller gaps.\n    # `1 / (1 + exp(-(-slope * gaps + intercept)))`\n    \n    adjusted_gaps = -gaps\n    sigmoid_priorities = 1 / (1 + np.exp(-(slope * adjusted_gaps + intercept)))\n\n    # Combine the direct inverse fit with the sigmoid idea.\n    # The inverse provides a direct measure of \"how well\" it fits.\n    # The sigmoid can then modulate this based on the absolute size of the gap.\n    \n    # Let's try a weighted sum: heavily favouring the inverse gap, with sigmoid as a minor adjustment.\n    # Or, let's use the sigmoid directly but focus on the *inverse* of the gap as input.\n    # This means inputting `1/gap` into a sigmoid. But sigmoid works better on a range like [-inf, inf].\n    \n    # Reverting to a simpler, robust combination:\n    # Best Fit (inverse of gap) is proven effective. Let's stick to that, but ensure it's well-scaled.\n    # The v0 heuristic is already quite good. What if we wanted to be more conservative and prefer\n    # bins that are *just* large enough? This is exactly what 1/(gap + epsilon) does.\n    \n    # Let's consider the \"Almost Full Fit\" idea from 1st and 2nd, combined with Best Fit.\n    # \"Almost Full Fit\" implies a bonus for bins that are close to full.\n    # If a bin has remaining capacity `R` and the item is `I`, the gap is `R-I`.\n    # Small gap is good (Best Fit).\n    # If `R` is very small, the gap is small.\n    \n    # Let's try to give a boost to bins where the remaining capacity AFTER fitting is small, but not too small.\n    # We already have `1.0 / (bins_remain_cap[i] - item + 1e-9)`.\n    \n    # Consider the analysis: \"These heuristics (5th, 6th, 7th, 9th) are largely similar to the \"Best Fit\" approach (3rd/8th),\n    # using 1.0 / (bins_remain_cap - item + epsilon)\". Heuristic 9th explicitly sets non-fitting bins to -float('inf').\n    # This is a good robust strategy.\n    \n    # Let's combine the \"Best Fit\" principle with a secondary criterion.\n    # Primary: Best Fit (inverse of gap).\n    # Secondary: If multiple bins have very similar inverse gaps, pick the one that leaves the least remaining capacity.\n    # This is implicitly handled by the inverse gap already.\n    \n    # Let's re-examine the sigmoid-based approaches.\n    # Heuristics 14-17: `1 / (1 + np.exp(-(slope * (excess_capacity) + intercept)))`\n    # This prioritizes smaller `excess_capacity`.\n    # We can combine this with Best Fit.\n    \n    # Let's try this: Use the Best Fit score (inverse of gap). Then, for bins that are \"almost full\" (small remaining capacity after fit),\n    # give them an additional boost.\n    \n    # Final strategy: Use the inverse of the gap as the primary score.\n    # Then, use a sigmoid function on the *remaining capacity after fit* (which is `gaps`)\n    # to give a penalty for bins that would be left \"too empty\".\n    # This means we want the sigmoid to be high for small gaps, and low for large gaps.\n    # The sigmoid formula `1 / (1 + exp(-x))` increases with `x`.\n    # So, we need to input `f(gap)` where `f` is decreasing. Let's use `-(gap)`.\n    \n    # `sigmoid_penalty = 1 / (1 + np.exp(-(slope * gaps + intercept)))`\n    # This sigmoid_penalty is high for small gaps and low for large gaps.\n    # We want to *boost* bins with small gaps. So, we should *add* this penalty.\n    \n    # Let's try a weighted sum of the inverse gap and the sigmoid penalty.\n    # The inverse gap is the dominant factor. The sigmoid can fine-tune.\n    \n    # Let's define `f(gap) = 1.0 / (gap + 1e-9)` and `g(gap) = 1 / (1 + np.exp(-(slope * gap + intercept)))`.\n    # We want to prioritize small gaps. `f(gap)` does this. `g(gap)` also does this if `slope` is negative, or if we use `-gap`.\n    # Let's use `g(gap)` with `slope` positive and input `-(gap)`: `g_inv_gap(gap) = 1 / (1 + np.exp(-(slope * (-gap) + intercept)))`.\n    # This `g_inv_gap(gap)` is high for small gaps.\n    \n    # Let's combine: `priority = A * f(gap) + B * g_inv_gap(gap)`\n    # A should be larger than B.\n    \n    A = 1.0\n    B = 0.5\n    slope = 10.0\n    intercept = 5.0 # Tunable parameter: higher means stronger penalty for larger gaps.\n    \n    # The inverse gap `1.0 / (gaps + 1e-9)`\n    # The sigmoid on negative gap: `1 / (1 + np.exp(-(slope * (-gaps) + intercept)))`\n\n    combined_priorities = A * (1.0 / (gaps + 1e-9)) + B * (1 / (1 + np.exp(-(slope * (-gaps) + intercept))))\n    \n    priorities[suitable_bins_mask] = combined_priorities\n    \n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 359.45228222839756,
    "exec_success": true
  }
]