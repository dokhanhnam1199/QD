{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Best Fit priority for online Bin Packing.\n    Prioritizes bins that have just enough space for the item.\n    A small negative value is assigned to bins that cannot fit the item.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    bins_that_can_fit = bins_remain_cap[can_fit_mask]\n    \n    if bins_that_can_fit.size > 0:\n        gaps = bins_that_can_fit - item\n        \n        # For bins that can fit, prioritize those with the smallest gap\n        # This encourages tighter packing.\n        # We can transform the gaps to create a descending priority,\n        # so smaller gaps get higher priorities.\n        # A simple way is to invert the gaps (1/gap) but this can lead to division by zero\n        # or very large numbers if gap is close to zero.\n        # A more robust approach is to use a function that maps smaller gaps to higher values.\n        # For instance, exp(-gap) or similar, but let's stick to something simpler\n        # and directly related to \"best fit\".\n        \n        # We want the smallest non-negative gap to have the highest priority.\n        # The priority can be inverse of (gap + epsilon) to avoid division by zero,\n        # or simply a large number for the best fit and decreasing for others.\n        \n        # Let's consider a priority that is inversely proportional to the remaining capacity\n        # AFTER placing the item. The bin that results in the SMALLEST remaining capacity\n        # (closest to zero) is the \"best fit\".\n        \n        # Priority = 1 / (remaining_capacity_after_fit + 1e-9)\n        # Or, to make it simpler and avoid potential overflow with very small gaps:\n        # Priority = -gap, so smaller gaps have larger (less negative) priorities.\n        # But we need to distinguish between different fits.\n        \n        # A common approach for \"best fit\" is to assign a high priority to the bin\n        # where (bin_capacity - item) is minimized.\n        # Let's create a priority that is higher for smaller (bin_capacity - item).\n        \n        # We can simply use the negative of the gap, and then take the reciprocal\n        # to boost smaller gaps significantly.\n        # If gap = 0.1, 1/0.1 = 10. If gap = 0.01, 1/0.01 = 100.\n        # If gap = 1, 1/1 = 1. This seems to work.\n        \n        # Let's ensure a positive priority for fitting bins.\n        # We can use a large base priority and subtract a penalty for larger gaps.\n        # Or, let's directly map smallest gap to highest priority.\n        \n        # Priority = -(gap)\n        # If we have gaps [0.1, 0.5, 0.05], priorities are [-0.1, -0.5, -0.05].\n        # The bin with gap 0.05 is the best fit, but it has the lowest priority (-0.05 is larger than -0.1 and -0.5).\n        # So we need to invert this.\n        \n        # Option 1: Using a penalty for gap\n        # highest_priority_value = 1.0\n        # penalty_per_unit_gap = 0.1\n        # priorities[can_fit_mask] = highest_priority_value - (gaps * penalty_per_unit_gap)\n        \n        # Option 2: Directly use the reciprocal of gap (plus a small constant to avoid zero division)\n        # This gives higher scores to smaller gaps.\n        epsilon = 1e-9\n        priorities[can_fit_mask] = 1.0 / (gaps + epsilon)\n        \n        # Option 3: Maximize the remaining capacity if it's the best fit, otherwise minimize.\n        # This is more \"first fit\" like.\n        \n        # Let's refine Option 2 to ensure clear ranking.\n        # A slightly different approach: assign priority such that smaller gaps get HIGHER scores.\n        # This could be by transforming `gaps` into a decreasing sequence of priorities.\n        # Example: For gaps [0.1, 0.5, 0.05], we want scores like [high, medium, very_high].\n        # The reciprocal of the gap provides this.\n        \n        # Let's make it even more aligned with \"best fit\" as minimizing waste.\n        # The priority of a bin could be seen as how \"tight\" the fit is.\n        # A tighter fit means the remaining capacity is smaller.\n        # We want to maximize the score for the tightest fit.\n        \n        # So, for bins that fit, the priority can be -gap.\n        # Then, we want to pick the bin with the MINIMUM gap.\n        # So, the priority should be something that INCREASES as gap DECREASES.\n        # The score should be inversely proportional to the gap.\n        \n        # Let's try to map gaps to a scoring system:\n        # Gap: 0.01  -> Score: 100\n        # Gap: 0.1   -> Score: 10\n        # Gap: 0.5   -> Score: 2\n        # This suggests a score that is roughly 1/gap.\n        \n        # The previous choice of 1.0 / (gaps + epsilon) works.\n        # However, it might give very large scores to tiny gaps.\n        # Let's make it more linear or bounded.\n        \n        # A simpler approach: subtract the gap from a large constant.\n        # The bin with the smallest gap will have the largest score.\n        # Let M be a sufficiently large number. Priority = M - gap.\n        # If M=100, gaps [0.1, 0.5, 0.05] -> scores [99.9, 99.5, 99.95].\n        # This works well. The smallest gap has the largest priority.\n        \n        # Let's choose a large constant. The range of remaining capacities might influence this.\n        # If bin capacity is 1 and item size is 0.1, gaps can be up to ~0.9.\n        # A constant like 1.0 should be sufficient if we normalize or scale the gaps.\n        \n        # Let's try to create a priority score such that the BEST FIT bin\n        # gets the HIGHEST score.\n        # The \"best fit\" is the bin with the smallest `bins_remain_cap - item`.\n        \n        # So, we want a function f(gap) such that f(gap1) > f(gap2) if gap1 < gap2.\n        # A simple choice is `f(gap) = -gap`. This means smaller gaps have higher priorities.\n        # Let's add an offset to ensure positive priorities or a baseline.\n        # Priority = C - gap.\n        \n        # The actual values of priorities don't matter as much as their relative order.\n        # The primary goal of best fit is to minimize the leftover space in the chosen bin.\n        # So, the bins that leave the least space are preferred.\n        \n        # Let's define priority as inversely proportional to the capacity *after* filling the item.\n        # So, priority = 1 / (remaining_capacity_after_fill).\n        # This will maximize the score for the bin that has the smallest remaining capacity after filling.\n        # This is the definition of best fit.\n        \n        remaining_capacity_after_fill = bins_that_can_fit - item\n        # Use a small epsilon to avoid division by zero if an item perfectly fills a bin.\n        epsilon = 1e-9\n        priorities[can_fit_mask] = 1.0 / (remaining_capacity_after_fill + epsilon)\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Implements a priority function for the online Bin Packing Problem using a\n    sigmoid-based strategy, favoring bins that are nearly full after placing the item,\n    while penalizing bins that would become too full.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array where each element is the remaining capacity of a bin.\n\n    Returns:\n        A NumPy array of the same size as bins_remain_cap, containing the\n        priority score for packing the item into each respective bin. Higher scores\n        indicate a more desirable bin.\n    \"\"\"\n    large_capacity_threshold = 0.8  # Threshold for \"nearly full\"\n    small_capacity_threshold = 0.2  # Threshold for \"too empty\"\n    steepness = 10.0                # Controls the steepness of the sigmoid\n\n    # Calculate the remaining capacity after placing the item\n    potential_remain_cap = bins_remain_cap - item\n\n    # Initialize priorities to a very low value\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify valid bins (those that can actually hold the item)\n    valid_bins_mask = potential_remain_cap >= 0\n\n    # For valid bins, calculate the normalized remaining capacity to apply sigmoid\n    # Avoid division by zero if a bin has exactly zero remaining capacity (edge case)\n    valid_potential_remain_cap = potential_remain_cap[valid_bins_mask]\n    valid_bins_current_cap = bins_remain_cap[valid_bins_mask]\n\n    # Use sigmoid to prioritize bins that result in a near-full state, but not overfull.\n    # We want to push items to bins that are almost full, minimizing wasted space.\n    # The sigmoid maps values around 0 (representing a good fit after item placement)\n    # to values near 0.5. Values far from 0 (too much or too little space remaining)\n    # will be pushed towards 0 or 1.\n    # A bin where potential_remain_cap is close to 0 (i.e., the item almost fills it)\n    # should have a high priority.\n\n    # Calculate a \"fill score\" which is high when the remaining capacity is small\n    fill_score = np.ones_like(valid_bins_current_cap) - (valid_potential_remain_cap / valid_bins_current_cap)\n    # Handle cases where current capacity is zero or item is zero\n    fill_score[valid_bins_current_cap == 0] = 0\n    fill_score[valid_bins_current_cap == item] = 1 # Perfect fit\n\n    # Apply sigmoid to push values towards 0 or 1 based on how close to \"full\" it gets\n    # We want to reward bins that become *nearly full*, so we center the sigmoid\n    # around a \"good fit\" state (where remaining capacity is small).\n    # A bin becoming completely full is good, but slightly less than completely full is also good.\n    # If remaining capacity is exactly 0, the score should be high.\n\n    # Let's define a target remaining capacity. A small positive value is ideal.\n    # Or, more directly, a low \"waste\" score. Waste score = remaining_capacity / bin_capacity\n    # High priority for low waste.\n    # Let's aim for a state where remaining capacity is very small, but not negative.\n    # The sigmoid function helps here: we want to reward states where `potential_remain_cap` is small.\n    # Let's map `potential_remain_cap` to a value that is high when it's near 0.\n\n    # A simple sigmoid where input `x` maps to `1 / (1 + exp(-k * (x - x0)))`\n    # We want high priority when `potential_remain_cap` is small.\n    # So, let's transform `potential_remain_cap` into a metric that's high when small.\n    # For example, `max_capacity - potential_remain_cap` gives a measure of fullness.\n    # Or even better, use `potential_remain_cap` directly but invert the sigmoid's effect.\n\n    # Consider `f(x) = 1 / (1 + exp(-k * (x - threshold)))`\n    # If threshold is small, x near threshold gives 0.5.\n    # If we want small `potential_remain_cap` to be high priority,\n    # we can use `-potential_remain_cap` as input to sigmoid.\n    # Let threshold be a small positive value, say 0.1 * bin_capacity\n\n    # Calculate a normalized \"ideal fill\" state for each valid bin.\n    # We want to maximize the case where remaining capacity is just above 0.\n    normalized_remain_cap = valid_potential_remain_cap / bins_remain_cap[valid_bins_mask]\n\n    # Sigmoid centered around a state of 'almost full' (e.g., normalized_remain_cap close to 0)\n    # The input to sigmoid is `steepness * (normalized_remain_cap - target_norm_remain_cap)`\n    # If `target_norm_remain_cap` is small (e.g., 0.05), then bins with small normalized remaining capacity\n    # will have input close to 0, yielding scores near 0.5.\n    # We want higher priority for lower `normalized_remain_cap`.\n    # Let's use `priorities = 1 - sigmoid(normalized_remain_cap)`\n    # Or more directly, `priorities = sigmoid(-normalized_remain_cap)`\n    # This will give higher scores for smaller `normalized_remain_cap`.\n\n    target_norm_remain_cap = 0.05 # Aim for ~5% remaining capacity\n\n    # Calculate the sigmoid score: higher score for smaller remaining capacity\n    sigmoid_input = steepness * (normalized_remain_cap - target_norm_remain_cap)\n    scores = 1 / (1 + np.exp(-sigmoid_input))\n\n    # Invert scores: we want to prioritize bins that result in SMALLER remaining capacity.\n    # So, a state where `normalized_remain_cap` is low should get a HIGH priority.\n    # Our sigmoid `scores` are high when `sigmoid_input` is high, meaning\n    # `normalized_remain_cap` is high. We need the opposite.\n    # So, let's use `1 - scores` or `sigmoid(-sigmoid_input)`.\n\n    final_scores = 1 / (1 + np.exp(-steepness * (target_norm_remain_cap - normalized_remain_cap)))\n\n    priorities[valid_bins_mask] = final_scores\n\n    # Ensure that bins that cannot fit the item have a very low priority.\n    # This is already handled by initializing to -inf and only updating valid bins.\n\n    # Return priorities, ensuring no NaNs or Infs in the final output if any edge cases slipped through.\n    # Using a very small number for impossible fits would be safer than -inf for some algorithms.\n    priorities[~valid_bins_mask] = 0 # Assign zero priority to invalid bins\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1st and 2nd (identical, but listed as distinct): Both implement an \"Almost Full Fit\" strategy by assigning priority as the negative of the remaining capacity after fitting the item (`-fit_capacities`). This correctly prioritizes bins that result in smaller remaining space.\n\nComparing Heuristics 3rd and 8th (identical): These also implement a \"Best Fit\" strategy by assigning priority as the reciprocal of the remaining capacity after fitting (`1.0 / (gaps + epsilon)`). This approach effectively gives higher scores to bins with smaller gaps, aligning with the \"Best Fit\" principle of minimizing wasted space. The detailed comments explain the rationale well.\n\nComparing Heuristics 4th and the \"Best Fit\" variants (3rd/8th): Heuristic 4th uses `1.0 / (diff + epsilon)` for bins that can fit, similar to Best Fit. However, it then normalizes these priorities to be between 0 and 1. Normalization can sometimes obscure the fine-grained differences that exact values might convey, potentially making it less effective for pure prioritization unless the downstream selection mechanism specifically benefits from normalized inputs.\n\nComparing Heuristics 5th, 6th, 7th, and 9th: These heuristics (5th, 6th, 7th, 9th) are largely similar to the \"Best Fit\" approach (3rd/8th), using `1.0 / (bins_remain_cap - item + epsilon)`. Heuristic 9th explicitly sets non-fitting bins to `-float('inf')`, which is a robust way to ensure they are never selected. Heuristic 5th and 7th are identical. Heuristic 6th uses a slightly different epsilon.\n\nComparing Heuristics 10th and 18th: Both attempt a softmax-like approach. Heuristic 10th scales `fit_values` by `max_fit` and then uses `exp(scaled_fit_values) / sum(exp_values)`. Heuristic 18th scales `fit_values` by `max(fit_values)` (handling the case of `max_fit` being 0) and assigns the raw `exp_fit` as priority, without normalizing by the sum. The lack of normalization in 18th means raw exponentiated values are used, which might be less stable or interpretable as probabilities compared to 10th.\n\nComparing Heuristics 11th and 12th: Heuristic 11th uses a sigmoid function applied to a ratio of `(residual / suitable_bins_cap)` and then inverts it, aiming to prioritize bins with less *relative* remaining capacity. Heuristic 12th also uses a sigmoid but aims to prioritize bins where the *absolute* remaining capacity after fitting is small, by using `target_norm_remain_cap - normalized_remain_cap` in the sigmoid input. Both use sigmoid transformations but with different inputs and interpretations, making direct comparison tricky without empirical testing. Heuristic 11th appears to be a more direct mapping towards minimizing waste ratio.\n\nComparing Heuristics 13th, 14th, 15th, 16th, and 17th: Heuristic 13th uses a softmax on the inverse of the remaining capacity, similar to Best Fit but normalized. Heuristics 14th-17th (identical) use a sigmoid function applied to `excess_capacity` (i.e., `bins_remain_cap - item`) with a specific slope and intercept. This sigmoid function penalizes larger excess capacities. The specific choice of slope and intercept (10.0, -5.0) means the function saturates quickly.\n\nComparing Heuristics 19th and 20th (identical): These heuristics set priorities to `bins_remain_cap - item` for bins that can fit, and `-np.inf` for those that cannot. This is essentially a \"Worst Fit\" strategy if selecting the max, or if the negative of this is used, it's a variant of \"Best Fit\" but with linear scaling rather than inverse. The simple subtraction does not give a strong preference for the *tightest* fit as effectively as an inverse.\n\nOverall: The \"Best Fit\" strategies (3rd, 8th, and their similar variants like 5th, 7th) that use the inverse of the remaining gap (`1 / (gap + epsilon)`) are strong contenders for good heuristics as they directly target minimizing waste. Heuristics that normalize or apply sigmoids (4th, 10th, 11th, 12th, 13th, 14th-17th) introduce complexities that may or may not improve performance over simpler \"Best Fit\" without careful tuning or specific problem characteristics. Heuristics 1st and 2nd (\"Almost Full Fit\") are a simpler form of Best Fit. The worst heuristics are those that use simple differences (19th, 20th) or do not effectively prioritize tight fits.\n- \nHere's a redefined approach to self-reflection for heuristic design:\n\n*   **Keywords:** Adaptive prioritization, learning, meta-heuristics, state-space exploration.\n*   **Advice:** Focus on heuristics that *learn* from past decisions and *adapt* their prioritization rules dynamically. Consider meta-heuristics that explore different heuristic strategies or parameter settings.\n*   **Avoid:** Static, single-objective heuristics. Over-reliance on pre-defined rules without mechanisms for adjustment.\n*   **Explanation:** Instead of just minimizing wasted space, aim to improve the *process* of finding good fits. Learning from previous placements can guide future choices, leading to more robust and efficient solutions, especially in complex, dynamic environments.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}