```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines normalized Best Fit with a sigmoid penalty for large remaining capacity.
    This balances fitting items tightly while discouraging excessive underutilization.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    
    suitable_bins_mask = bins_remain_cap >= item
    
    if np.any(suitable_bins_mask):
        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]
        
        # Best Fit component: Minimize remaining capacity after packing
        remaining_after_fit = suitable_bins_cap - item
        
        # Normalize Best Fit scores: higher is better (tighter fit)
        # Invert and scale to [0, 1]
        min_rem = np.min(remaining_after_fit)
        max_rem = np.max(remaining_after_fit)
        if max_rem > min_rem:
            best_fit_scores = 1.0 - (remaining_after_fit - min_rem) / (max_rem - min_rem)
        else:
            best_fit_scores = np.ones_like(remaining_after_fit) # All gaps are the same, equal priority from BF

        # Penalty component: Discourage bins that leave a large amount of space
        # Use a sigmoid-like penalty that approaches 0 for small remaining capacities and 1 for large ones.
        # Penalty is applied based on the ratio of remaining capacity to item size.
        # This is inspired by heuristics using exponential penalties.
        penalty_threshold_ratio = 1.5 # Start penalizing significantly when remaining > 1.5 * item
        penalty_steepness = 1.0 # Controls how quickly the penalty ramps up
        
        # Calculate capacity ratio relative to the item size
        capacity_ratios = suitable_bins_cap / item
        
        # Sigmoid penalty: penalizes bins with large capacity_ratios
        # penalty = 1 / (1 + exp(-steepness * (ratio - threshold)))
        # We want a penalty that is low for good fits and high for bad fits.
        # So, we want to penalize when ratio is high.
        # Let's define penalty as something that increases with capacity_ratio.
        # A simple approach: penalty = ratio itself, then normalize.
        # Or use a sigmoid on the *inverse* of what we want (i.e., on badness)
        # We want to penalize when `suitable_bins_cap - item` is large relative to `item`.
        # So penalize when `suitable_bins_cap / item` is large.
        # Let's use a penalty that is 0 for ratio=1 and increases.
        # A simple penalty: (capacity_ratio - 1)
        # A more refined penalty: sigmoid applied to a scaled difference.
        # We want penalty to be 0 when ratio is small, and 1 when ratio is large.
        # Let's try: penalty = 1 - sigmoid(-steepness * (capacity_ratio - penalty_threshold_ratio))
        # This means penalty is 0 when ratio < threshold, and 1 when ratio is much larger.
        # Alternative: Directly use the best_fit_score and subtract a penalty based on remaining space.
        # Heuristic 18/19/20 use `best_fit_scores - large_capacity_penalty`.
        # Let's model our penalty similar to that, but using the ratio.
        
        # Penalty term: large when remaining_after_fit is large relative to item size
        # We want to reduce the priority if the bin is too empty.
        # Let's use a sigmoid that approaches 1 as remaining_after_fit / item grows large.
        # penalty_value = 1.0 / (1.0 + np.exp(-penalty_steepness * (remaining_after_fit - item * penalty_threshold_ratio) / (item + 1e-9))) # Adjusted for item size
        
        # Simpler penalty: based on the ratio of remaining capacity to bin capacity, or just remaining capacity.
        # Let's follow the 18/19/20 approach which subtracts a penalty from best_fit_scores.
        # The penalty should be higher for bins that leave more space relative to the item.
        # Consider penalty = (remaining_after_fit / (suitable_bins_cap + 1e-9))
        # This is the proportion of space left. We want to penalize large proportions.
        # Normalize this proportion to create a penalty score.
        
        # Let's use a penalty that is zero for perfect fits and grows.
        # Penalty based on remaining space, scaled by item size to be relative.
        # penalty_component = remaining_after_fit / (item + 1e-9)
        # Apply a softening/thresholding function to this penalty.
        # penalty_strength = 0.2 # Scale the penalty
        # combined_priorities = best_fit_scores - penalty_strength * (np.tanh((penalty_component - 1.0) / 2.0) + 1.0) / 2.0 # Clamp penalty roughly [0, 0.5]
        
        # Let's try a combination inspired by 18/19/20: subtract a sigmoid penalty.
        # The penalty should be low for small remaining_after_fit and high for large ones.
        # Let's use a sigmoid that maps remaining_after_fit to [0, 1].
        # Scale the input to sigmoid so that the penalty is significant around a certain excess capacity.
        # Penalty increases when `remaining_after_fit > threshold * item`.
        penalty_input = (remaining_after_fit - item * penalty_threshold_ratio) / (item + 1e-9)
        sigmoid_penalty = 1.0 / (1.0 + np.exp(-penalty_steepness * penalty_input))
        
        # Combine: Best Fit score minus the penalty. Higher score is better.
        # The penalty score is [0, 1], so subtracting it reduces the priority.
        combined_priorities = best_fit_scores - sigmoid_penalty * 0.7 # Scale penalty effect (0.7 is a tunable parameter)
        
        # Ensure priorities are non-negative
        combined_priorities = np.maximum(combined_priorities, 0)
        
        # Assign the calculated priorities to the original indices
        original_indices = np.where(suitable_bins_mask)[0]
        priorities[original_indices] = combined_priorities

    return priorities
```
