{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Highest priority: Exact fit\n    exact_fit = (bins_remain_cap == item)\n    priorities[exact_fit] = 1000.0\n\n    # High priority: Near perfect fit, scaled by item size\n    near_perfect_fit_threshold = 0.05  # Loosened the near perfect threshold\n    near_perfect_fit = (bins_remain_cap >= item) & (bins_remain_cap <= item * (1 + near_perfect_fit_threshold))\n    priorities[near_perfect_fit] = 900.0 - (bins_remain_cap[near_perfect_fit] - item) / item * 200  # Scale based on waste\n\n    # Feasibility check and HUGE penalty for infeasible bins, scaled by item size\n    infeasible = bins_remain_cap < item\n    priorities[infeasible] = -1e10 * (item + 0.1)  # Scale penalty based on item size\n\n    # Dynamic Fill Ratio Reward: Gaussian around optimal fill, scaled heavily\n    can_accommodate = (bins_remain_cap >= item)\n    if np.any(can_accommodate):\n        fill_ratio = item / bins_remain_cap[can_accommodate]\n        gaussian_std = 0.1 # Increased std for the gaussian curve, allows more flexibility\n        gaussian_reward = np.exp(-0.5 * ((fill_ratio - 1.0) / gaussian_std) ** 2)\n        priorities[can_accommodate] += gaussian_reward * 800.0 # Scale Gaussian reward aggressively\n\n    # Fragmentation Penalty: Penalize creating small remaining spaces, scaled by item size\n    small_space_threshold = 0.15  # Define \"small space\" relative to bin size\n    fragmented_bins = (bins_remain_cap >= item) & ((bins_remain_cap - item) <= small_space_threshold * np.max(bins_remain_cap))\n    if np.any(fragmented_bins):\n        waste = bins_remain_cap[fragmented_bins] - item\n        penalty = (waste / (small_space_threshold * np.max(bins_remain_cap))) * 400 * (item + 0.1) # Scale penalty by item size\n        priorities[fragmented_bins] -= penalty\n\n    # Encourage using existing bins, more heavily, scaled by number of existing bins.\n    non_empty = bins_remain_cap < np.max(bins_remain_cap)\n    num_non_empty = np.sum(non_empty)\n    priorities[non_empty] += 75.0 + num_non_empty * 5 # Reward scaled by how many bins are in use.\n\n    # Similarity Bonus: Reward bins with similar-sized items. This is a basic approach and can be further improved.\n    # This version doesn't track items already in bins, so it uses remaining capacity as a proxy.\n    similar_size = np.abs(bins_remain_cap - item) <= 0.2 * item  # Tolerance for similarity\n    priorities[similar_size & can_accommodate] += 60.0\n\n    # Discourage bins with way more capacity than the item size, more aggressive penalty, scaled by remaining cap\n    large_capacity = (bins_remain_cap >= item) & (bins_remain_cap > 2 * item) # Increased factor\n    priorities[large_capacity] -= 80 + bins_remain_cap[large_capacity] * 2 # Increase the penalty, scaled by how much excess capacity remains.\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    max_bin_cap = np.max(bins_remain_cap)\n\n    # Highest priority: Exact fit\n    exact_fit = (bins_remain_cap == item)\n    priorities[exact_fit] = 1000.0\n\n    # High priority: Near perfect fit, scaled by item size\n    near_perfect_fit_threshold = 0.05  # Define a threshold for \"near perfect\", increased tolerance\n    near_perfect_fit = (bins_remain_cap >= item) & (bins_remain_cap <= item * (1 + near_perfect_fit_threshold))\n    priorities[near_perfect_fit] = 900.0 - (bins_remain_cap[near_perfect_fit] - item) / item * 150  # Scale based on waste, reduced scaling factor\n\n    # Feasibility check and HUGE penalty for infeasible bins\n    infeasible = bins_remain_cap < item\n    priorities[infeasible] = -1e10  # Even stronger penalty\n\n    # Dynamic Fill Ratio Reward: Gaussian around optimal fill, scaled heavily\n    can_accommodate = (bins_remain_cap >= item)\n    if np.any(can_accommodate):\n        fill_ratio = item / bins_remain_cap[can_accommodate]\n        gaussian_std = 0.1 # Adjust std for the gaussian curve, increased std\n        gaussian_reward = np.exp(-0.5 * ((fill_ratio - 1.0) / gaussian_std) ** 2)\n        priorities[can_accommodate] += gaussian_reward * 700.0 # Scale Gaussian reward aggressively, reduced scaling factor\n\n    # Fragmentation Penalty: Penalize creating small remaining spaces, scaled relative to item size\n    small_space_threshold = 0.20  # Define \"small space\" relative to bin size, increased threshold\n    fragmented_bins = (bins_remain_cap >= item) & ((bins_remain_cap - item) <= small_space_threshold * item) # Scaled by item size instead of max_bin_cap\n    if np.any(fragmented_bins):\n        waste = bins_remain_cap[fragmented_bins] - item\n        penalty = (waste / (small_space_threshold * item)) * 450  # Scale penalty, increased scaling factor\n        priorities[fragmented_bins] -= penalty\n\n    # Encourage using existing bins, more heavily\n    non_empty = bins_remain_cap < max_bin_cap\n    priorities[non_empty] += 90.0 # Increased encouragement\n\n    # Similarity Bonus: Reward bins with similar-sized items. This is a basic approach and can be further improved.\n    # This version doesn't track items already in bins, so it uses remaining capacity as a proxy.\n    similar_size = np.abs(bins_remain_cap - item) <= 0.3 * item  # Tolerance for similarity, increased tolerance\n    priorities[similar_size & can_accommodate] += 70.0 # Increased bonus\n\n    # Discourage bins with way more capacity than the item size, more aggressive penalty\n    large_capacity = (bins_remain_cap >= item) & (bins_remain_cap > 2.5 * item) # Increased factor\n    priorities[large_capacity] -= 95 # Increase the penalty\n    return priorities\n\n[Reflection]\nScaling fragmentation penalty by item size and tuning bonus/penalty magnitudes improved performance.\n\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}