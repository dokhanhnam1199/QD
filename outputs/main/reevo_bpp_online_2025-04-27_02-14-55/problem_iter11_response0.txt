```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Highest priority: Exact fit
    exact_fit = (bins_remain_cap == item)
    priorities[exact_fit] = 1000.0

    # Next level priority: Very near fit
    near_exact_fit_threshold = 0.001 * item
    near_exact_fit = (bins_remain_cap >= item - near_exact_fit_threshold) & (bins_remain_cap <= item + near_exact_fit_threshold)
    priorities[near_exact_fit] = 950.0

    # High priority: Near perfect fit, scaled by item size
    near_perfect_fit_threshold = 0.05  # Define a threshold for "near perfect"
    near_perfect_fit = (bins_remain_cap >= item) & (bins_remain_cap <= item * (1 + near_perfect_fit_threshold))
    priorities[near_perfect_fit] = 900.0 - (bins_remain_cap[near_perfect_fit] - item) / item * 100  # Scale based on waste

    # Feasibility check and HUGE penalty for infeasible bins
    infeasible = bins_remain_cap < item
    priorities[infeasible] = -1e10  # Even stronger penalty

    # Dynamic Fill Ratio Reward: Gaussian around optimal fill, scaled heavily
    can_accommodate = (bins_remain_cap >= item)
    if np.any(can_accommodate):
        fill_ratio = item / bins_remain_cap[can_accommodate]
        gaussian_std = 0.1  # Adjust std for the gaussian curve
        gaussian_reward = np.exp(-0.5 * ((fill_ratio - 1.0) / gaussian_std) ** 2)
        priorities[can_accommodate] += gaussian_reward * 700.0  # Scale Gaussian reward aggressively

    # Fragmentation Penalty: Penalize creating small remaining spaces, relative to item size
    small_space_threshold = 0.25  # Define "small space" relative to item size
    fragmented_bins = (bins_remain_cap >= item) & ((bins_remain_cap - item) <= small_space_threshold * item)
    if np.any(fragmented_bins):
        waste = bins_remain_cap[fragmented_bins] - item
        penalty = (waste / (small_space_threshold * item)) * 300  # Scale penalty
        priorities[fragmented_bins] -= penalty

    # Discourage Near-Full bins:
    near_full_threshold = 0.95 #Bins filled to 95% capacity or more are penalized to avoid very high fill rates
    near_full_bins = bins_remain_cap <= (1-near_full_threshold) * np.max(bins_remain_cap)
    priorities[near_full_bins & can_accommodate] -= 200

    # Encourage using existing bins, but less heavily than before, only if it is not close to full.
    non_empty = bins_remain_cap < np.max(bins_remain_cap)
    priorities[non_empty & ~near_full_bins] += 50.0

    # Similarity Bonus: Reward bins with similar-sized items. This is a basic approach and can be further improved.
    # This version doesn't track items already in bins, so it uses remaining capacity as a proxy.
    similar_size = np.abs(bins_remain_cap - item) <= 0.3 * item  # Tolerance for similarity
    priorities[similar_size & can_accommodate] += 40.0

    # Discourage bins with way more capacity than the item size, less aggressive penalty
    large_capacity = (bins_remain_cap >= item) & (bins_remain_cap > 2.5 * item)  # Increased factor
    priorities[large_capacity] -= 60  # Increase the penalty

    return priorities
```
