{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Best-fit slack with capped boost and ratio weighting for online bin packing.\"\"\"\n    n = bins_remain_cap.size\n    feasible = bins_remain_cap >= item\n    scores = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    slack = bins_remain_cap[feasible] - item\n    base = -slack\n    boost_factor = 0.5\n    max_boost = 5.0\n    tiny = 1e-6\n    boost = np.minimum(boost_factor / (slack + tiny), max_boost)\n    ratio = item / bins_remain_cap[feasible]\n    combined = base + boost + 0.1 * ratio\n    idx = np.flatnonzero(feasible)\n    combined += 1e-9 * idx\n    scores[feasible] = combined\n    return scores\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n                bins_remain_cap: np.ndarray,\n                ratio_weight: float = 0.44225129194232105,\n                jitter_scale: float = 0.0007357972103457867,\n                epsilon: float = 0.2595646589647237) -> np.ndarray:\n    \"\"\"Score bins by tightness (best\u2011fit) plus capacity\u2011ratio; optional \u03b5\u2011greedy exploration.\n\n    Parameters\n    ----------\n    item : float\n        Size of the item to place.\n    bins_remain_cap : np.ndarray\n        Remaining capacities of bins.\n    ratio_weight : float, optional\n        Weight applied to the capacity\u2011ratio component of the score.\n    jitter_scale : float, optional\n        Scale of the random jitter added to break ties.\n    epsilon : float, optional\n        Probability of performing random exploration (\u03b5\u2011greedy).\n    \"\"\"\n    n = bins_remain_cap.size\n    feasible = bins_remain_cap >= item\n    priorities = np.full(n, -np.inf, dtype=float)   # infeasible bins get -inf\n    if not np.any(feasible):\n        return priorities\n\n### Analyze & experience\n- Comparing #1 (best) vs #20 (worst), we see #1 implements a full adaptive\u202f\u03b5\u2011greedy best\u2011fit with near\u2011full boost, infeasibility handling and jitter, while #20 is merely a stub with no scoring logic. #2 vs #19 shows the same gap: #2 is a complete adaptive\u2011\u03b5 version, #19 is empty. #3 vs #18: #3 mixes best\u2011fit, ratio weighting, a small fixed \u03b5\u2011greedy and jitter; #18 lacks any functional code. #4 vs #17: #4 uses dead\u2011space avoidance and a ratio\u2011only score but no exploration, whereas #17 is a placeholder. #5 vs #16: #5 features adaptive \u03b5 that shrinks with feasibility, capped inverse\u2011slack boost, exact\u2011fit/near\u2011full bonuses and jitter; #16 uses a static \u03b5, same base but without adaptive decay, making it less flexible. #6 vs #15: #6 offers deterministic scoring with ratio tie\u2011breaker and boosts, no \u03b5\u2011greedy; #15 (duplicate of #14) adds adaptive \u03b5\u2011greedy and jitter on top of the same scoring, giving it a modest edge. #7 vs #14: #7 is identical to #6, while #14 adds adaptive \u03b5\u2011greedy and jitter, improving exploration. #8 vs #13: #8 includes a smooth inverse\u2011slack boost capped at\u202f5, decaying \u03b5\u2011greedy, and a strong random boost; #13 (duplicate of #11) lacks adaptive \u03b5 and uses only static boost and ratio weight. #9 vs #12: #9 avoids tiny dead\u2011space bins and relies on ratio only; #12 adds a near\u2011full bonus and deterministic index tie\u2011breaker, providing finer discrimination. #10 vs #11: #10 duplicates #5 (no \u03b5\u2011greedy, capped boost, bonuses, jitter); #11 omits \u03b5\u2011greedy and uses only a tiny index jitter, so #10 is superior.  \n\nAdjacent comparisons: #1 vs #2 are identical (no distinction). #2 vs #3 \u2013 #2\u2019s adaptive \u03b5 and near\u2011full boost outweigh #3\u2019s fixed \u03b5 and simpler ratio blend. #3 vs #4 \u2013 #3\u2019s exploration and ratio weighting make it more robust than #4\u2019s dead\u2011space\u2011only approach. #4 vs #5 \u2013 #5\u2019s adaptive \u03b5, capped boost, and bonuses dominate #4\u2019s limited heuristic. #5 vs #6 \u2013 #5\u2019s stochastic exploration gives it an advantage over #6\u2019s deterministic scoring. #6 vs #7 are duplicates. #7 vs #8 \u2013 #8\u2019s smooth boost and stronger \u03b5\u2011greedy make it richer. #8 vs #9 \u2013 #9\u2019s narrow focus on dead\u2011space avoidance is far weaker than #8\u2019s multi\u2011component design. #9 vs #10 \u2013 #10 (adaptive \u03b5 version) far exceeds #9\u2019s simple ratio. #10 vs #11 \u2013 #10\u2019s adaptive \u03b5 and capped boost beat #11\u2019s static approach.  \n\nOverall: Top heuristics\u202f(1,\u202f2,\u202f5,\u202f8,\u202f14\u201115) combine adaptive\u202f\u03b5\u2011greedy exploration, multi\u2011component scores (slack, ratio, near\u2011full/exact\u2011fit bonuses), capped boosts, and jitter for tie\u2011breaking. Mid\u2011ranked heuristics drop one or more of these elements, while the lowest tier (17\u201120) lack any functional implementation.\n- \n- **Keywords:** best\u2011fit, fixed \u03b5\u2011greedy, capped additive boost, dead\u2011space filter  \n- **Advice:** Use negative slack as the sole score, apply a small constant \u03b5 for random selection, add a modest capped additive boost to all bins as a tie\u2011breaker, and drop dead\u2011space\u2011only candidates.  \n- **Avoid:** dynamic \u03b5\u2011decay, explicit infeasibility flags, problem\u2011specific near\u2011full or exact\u2011fit bonuses, jitter, layered ratio/fit scoring, multiplicative or over\u2011parameterized boosts, per\u2011item random mixing, softmax compression, adaptive exploration schedules.  \n- **Explanation:** Simpler scoring reduces hidden bias and overhead, static exploration guarantees predictable coverage, capped additive boosts keep decisions interpretable, and removing complex mechanisms improves maintainability and runtime stability.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}