{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n# best\u2011fit with slack, exact\u2011fit boost, jitter, and adaptive epsilon\u2011greedy\n    \"\"\"Combines best\u2011fit, exact\u2011fit boost, near\u2011full boost, jitter, and adaptive epsilon\u2011greedy.\"\"\"\n    valid = bins_remain_cap >= item\n    n = bins_remain_cap.size\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    if np.any(valid):\n        slack = bins_remain_cap[valid] - item\n        base = -slack / bins_remain_cap[valid]\n        base[np.isclose(slack, 0.0, atol=1e-9)] += 1.0\n        base[slack <= 0.05 * bins_remain_cap[valid]] += 0.5\n        base += (np.random.rand(valid.sum()) - 0.5) * 0.01\n        priorities[valid] = base\n        valid_count = np.count_nonzero(valid)\n        epsilon = 0.05 * (1 - valid_count / n)\n        if np.random.rand() < epsilon:\n            idx = np.random.choice(np.flatnonzero(valid))\n            priorities[idx] += 1.0\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    epsilon = 0.05\n    n = bins_remain_cap.size\n    valid = bins_remain_cap >= item\n    if not np.any(valid):\n        return np.full(n, -np.inf, dtype=float)\n    if np.random.rand() < epsilon:\n        rand = np.random.rand(n)\n        rand[~valid] = 0.0\n        return rand / np.sum(rand)\n    scores = np.full(n, -np.inf, dtype=float)\n    scores[valid] = 1.0 - bins_remain_cap[valid] + item\n    max_score = np.max(scores[valid])\n    exp_scores = np.exp(scores - max_score)\n    probs = exp_scores / np.sum(exp_scores)\n    return probs\n\n### Analyze & experience\n- - **1st vs 20th:** The top heuristic uses a *decaying* \u03b5\u2011greedy rate, a fixed near\u2011full slack boost, and returns raw priorities (\u2011slack) for feasible bins, preserving fine\u2011grained ordering. The worst heuristic (20th) only applies a static inverse\u2011slack score, optional \u03b5\u2011greedy, and then **softmax\u2011normalises** the probabilities, which compresses differences and discards the explicit near\u2011full/exact\u2011fit signals.  \n- **2nd vs 19th:** Both share the same static inverse\u2011slack boost, but the 2nd adds an adaptive \u03b5 based on feasibility ratio and a tiny tie\u2011breaker, while the 19th lacks any feasibility\u2011aware \u03b5 schedule and uses the same softmax step, making exploration less informed.  \n- **1st vs 2nd:** Although their code is almost identical, the 1st emphasizes *adaptive decay* of \u03b5 across calls, giving a smoother transition from exploration to exploitation; the 2nd re\u2011initialises \u03b5 each call (no decay), leading to higher stochasticity over time.  \n- **3rd vs 4th:** The 3rd (duplicate of 1st/2nd) retains the simple \u201c\u2011slack + near\u2011full boost\u201d priority, whereas the 4th replaces the fixed boost with an *inverse\u2011slack* term and a static \u03b5 tied to the feasible fraction. The inverse\u2011slack can over\u2011emphasise very small slacks, causing instability compared to the controlled +0.5 boost.  \n- **19th vs 20th:** Both are identical copies; they suffer from the same softmax compression and lack of exact\u2011fit detection, making them indistinguishable in performance and confirming that duplication without variation adds no value.  \n\n**Overall:** The best heuristics combine (i) explicit feasibility checks, (ii) adaptive \u03b5\u2011greedy exploration that decays with experience, (iii) deterministic near\u2011full and exact\u2011fit bonuses, and (iv) minimal jitter to break ties. Lower\u2011ranked versions either omit adaptive \u03b5, replace fixed boosts with aggressive inverse\u2011slack terms, or over\u2011normalize via softmax, eroding priority resolution.\n- \n- **Keywords:** deterministic, monotonic, ratio ranking, reproducibility.  \n- **Advice:** use a single-pass ordering that ranks items by decreasing size\u2011to\u2011remaining capacity ratio; place items only when they satisfy the bin capacity; keep the ranking linear and fixed; set predefined thresholds before execution.  \n- **Avoid:** any nondeterministic adjustments, complex normalizations (softmax, exponential scaling), multi\u2011stage pipelines, any extra ranking beyond the primary ratio.  \n- **Explanation:** Simple monotonic rankings yield transparent bin selections, low overhead, and repeatable results, while avoiding heavy transformations preserves stability and performance.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}