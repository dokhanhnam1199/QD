{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Adaptive epsilon\u2011greedy best\u2011fit with slack inverse, near\u2011full boost, and infeasibility handling.\n    \"\"\"\n    if not hasattr(priority_v2, \"_epsilon\"):\n        priority_v2._epsilon = 0.2\n        priority_v2._epsilon_min = 0.01\n        priority_v2._epsilon_decay = 0.995\n        priority_v2._step = 0\n    priority_v2._step += 1\n    priority_v2._epsilon = max(priority_v2._epsilon_min, priority_v2._epsilon * priority_v2._epsilon_decay)\n    n = bins_remain_cap.size\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    valid = bins_remain_cap >= item\n    if not np.any(valid):\n        return priorities\n    slack = bins_remain_cap[valid] - item\n    base_priority = 1.0 / (slack + 1e-12)\n    near_full_thresh = max(0.02, 0.05 * item)\n    base_priority[slack <= near_full_thresh] += 0.5\n    priorities[valid] = base_priority\n    if np.random.rand() < priority_v2._epsilon:\n        rand_scores = np.random.rand(n)\n        rand_scores[~valid] = -np.inf\n        priorities = rand_scores\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n                bins_remain_cap: np.ndarray,\n                epsilon: float = 0.3029490039237682,\n                epsilon_min: float = 0.07292259727499244,\n                epsilon_decay: float = 0.8704448752801879,\n                near_full_thresh_base: float = 0.06244361354635141,\n                near_full_thresh_multiplier: float = 0.05249682954368101,\n                base_priority_increment: float = 1.2963271146145878,\n                small_eps: float = 7.4448870973632495e-09) -> np.ndarray:\n    \"\"\"\n    Adaptive epsilon\u2011greedy best\u2011fit with slack inverse, near\u2011full boost, and infeasibility handling.\n    \"\"\"\n    # Initialize static attributes on first call\n    if not hasattr(priority_v2, \"_epsilon\"):\n        priority_v2._epsilon = epsilon\n        priority_v2._epsilon_min = epsilon_min\n        priority_v2._epsilon_decay = epsilon_decay\n        priority_v2._step = 0\n\n    priority_v2._step += 1\n    priority_v2._epsilon = max(priority_v2._epsilon_min,\n                               priority_v2._epsilon * priority_v2._epsilon_decay)\n\n    n = bins_remain_cap.size\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    valid = bins_remain_cap >= item\n    if not np.any(valid):\n        return priorities\n\n### Analyze & experience\n- - **1st (best) vs 20th (worst)** \u2013 Both contain a full docstring, but the 1st uses a *simple* best\u2011fit score `\u2011slack` with a modest near\u2011full boost (`+0.5`). Infeasible bins are marked `\u2011inf`, preserving feasibility. The epsilon\u2011greedy exploration only **replaces** the whole score vector, never mixing random with deterministic values, which keeps exploitation strong.  \n  The 20th adds a large `near_full_boost = 2.0`, multiplies the base score, and then injects an index\u2011based penalty (`\u2011idx*1e\u20116`). This over\u2011emphasises near\u2011full bins and biases toward low\u2011index bins, hurting packing quality. The extra arithmetic also obscures the intent, making the code harder to audit.\n\n- **2nd vs 19th** \u2013 The 2nd is virtually identical to the 1st (same docstring, same boost, same epsilon handling). The 19th repeats the same logic but introduces the same index penalty as the 20th and keeps the code duplicated, indicating a design drift without performance gain.\n\n- **3rd vs 18th** \u2013 Again the 3rd mirrors the 1st. The 18th adds a *different* near\u2011full threshold (`0.1`) and a *multiplicative* boost (`*2.0`). It also builds a `bin_scores` array mixing random exploration per\u2011bin (`explore = rand < eps`) rather than a single\u2011vector switch. This granular mixing creates stochastic noise in the deterministic part, reducing the reliability of the best\u2011fit signal.\n\n- **4th vs 17th** \u2013 The 4th switches to an inverse\u2011slack base `1/(slack+\u03b5)` and adds a constant boost. It keeps the epsilon\u2011greedy switch clean. The 17th adds a convex combination of the base score and a random score (`(1\u2011\u03b5)*base + \u03b5*rand`), which dilutes the best\u2011fit ordering and can cause premature exploration, especially early when `\u03b5` is still relatively high.\n\n- **5th vs 16th** \u2013 The 5th is a duplicate of the 4th (same docstring, same logic). The 16th introduces a *global* `near_full_threshold` based on `0.05 * max(bins_remain_cap)` and a boost factor of `1.5`. It then mixes random scores with the base using a weighted sum, similar to 17th, but also uses a fixed `\u2011inf` for infeasible bins. The dynamic threshold can be unstable when bin capacities vary widely.\n\n- **6th (better) vs 15th (worse)** \u2013 The 6th has a clean docstring, uses a modest additive boost (`+3.0` for exact fits, `+0.5` for near\u2011full), and only switches to pure random scores when the epsilon trigger fires. The 15th is incomplete (truncated after validity check) and contains many unused parameters (`near_full_thresh_base`, `base_priority_increment`, etc.), indicating a half\u2011implemented design that would be confusing to maintain.\n\n- **7th vs 14th** \u2013 The 7th uses a compact docstring, explicit static attributes, and a clear epsilon decay (`max(min_epsilon, eps*decay)`). It returns `\u2011inf` for infeasible bins and applies a small boost. The 14th is a copy of the 13th with identical logic, but the comment \u201cAdaptive best\u2011fit with worst\u2011fit bias\u201d is misleading because the bias (`+\u03b1*slack`) is tiny (`\u03b1=0.01`) and has negligible effect, adding unnecessary complexity.\n\n- **8th vs 13th** \u2013 The 8th adds a *worst\u2011fit bias coefficient* (`\u2011\u03b2*slack`) to the inverse\u2011slack score, which can unintentionally favor bins with larger slack, contradicting the best\u2011fit goal. The 13th, despite its \u201cworst\u2011fit bias\u201d label, uses a small additive term (`\u03b1*slack`) that is dominated by the `\u2011slack` term, so the bias is harmless. The 8th\u2019s bias is more aggressive, degrading solution quality.\n\nOverall, the top\u2011ranked heuristics share: concise, accurate docstrings; simple deterministic scoring (best\u2011fit or inverse\u2011slack); modest, additive near\u2011full boosts; clean `\u2011inf` handling for infeasibility; and an epsilon\u2011greedy switch that *replaces* the whole score vector. Lower\u2011ranked versions introduce: large multiplicative boosts, index\u2011based penalties, per\u2011element random mixing, over\u2011parameterization, and ambiguous \u201cbias\u201d terms that dilute the primary heuristic.\n- \n- **Keywords:** simplicity, static \u03b5\u2011greedy, normalized additive scoring, implicit infeasibility penalty.  \n- **Advice:** Use a fixed \u03b5 for exploration, compute a single normalized additive score for each candidate, apply a large negative penalty for infeasible choices, and keep all parameters constant.  \n- **Avoid:** Dynamic \u03b5\u2011decay, problem\u2011specific or stateful boosts, explicit infeasibility flags, multiplicative or per\u2011element random mixing.  \n- **Explanation:** This yields transparent, low\u2011overhead heuristics that generalize without over\u2011parameterization or hidden adaptations.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}