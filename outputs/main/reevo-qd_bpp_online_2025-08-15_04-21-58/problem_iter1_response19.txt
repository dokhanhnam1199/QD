```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Softmax-Based Fit.

    The strategy aims to pack the item into a bin that has just enough remaining
    capacity to fit the item. This is achieved by assigning higher priorities to
    bins where the remaining capacity is close to the item size.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Returns:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Calculate a 'fit score' for each bin: how well the item fits.
    # A perfect fit would result in a score of 0 (remaining_cap - item = 0).
    # Negative scores mean the item doesn't fit.
    fit_scores = bins_remain_cap - item

    # We only consider bins where the item can actually fit (fit_scores >= 0).
    # For bins where the item doesn't fit, assign a very low score so they
    # are unlikely to be chosen by softmax. A large negative number works.
    # We add a small epsilon to prevent division by zero or log of zero if
    # any fit_score happens to be exactly 0.
    epsilon = 1e-9
    valid_fit_scores = np.where(fit_scores >= 0, fit_scores, -np.inf)

    # The core idea of Softmax-Based Fit is to use softmax on some transformed
    # scores. We want bins with smaller 'fit_scores' (closer to zero) to have
    # higher probabilities. A simple transformation is to take the negative
    # of the fit scores. This makes smaller (better) fit scores into larger
    # (more positive) values.
    #
    # Softmax(z_i) = exp(z_i) / sum(exp(z_j))
    # Here, z_i = -(bins_remain_cap[i] - item)
    #
    # We want bins with (bins_remain_cap[i] - item) closer to 0 to have higher
    # probability. So we want to maximize -(bins_remain_cap[i] - item).
    # This is equivalent to minimizing bins_remain_cap[i] - item.
    #
    # To make the softmax output represent priorities directly, we can use:
    # priority_i = exp(-beta * (bins_remain_cap[i] - item)) for valid bins
    # priority_i = 0 for invalid bins.
    # beta is a temperature parameter. A higher beta makes the distribution
    # sharper (more decisive towards the best fit).

    beta = 1.0  # Temperature parameter. Can be tuned.

    # Calculate exponentiated values for valid bins.
    # We add epsilon to valid_fit_scores to ensure that even a perfect fit (0)
    # contributes positively to the exponent, and to avoid issues with exp(0).
    # For invalid bins, exp(-inf) is effectively 0, which is handled by np.exp.
    transformed_scores = -beta * valid_fit_scores
    exp_scores = np.exp(transformed_scores)

    # Apply softmax. The sum of probabilities will be 1.
    # If all valid_fit_scores were -inf (item doesn't fit anywhere),
    # exp_scores will be all zeros, and sum will be zero. In this case,
    # we should return all zeros, or perhaps handle it as an error/special case.
    # However, for typical BPP scenarios, at least one bin will fit.
    sum_exp_scores = np.sum(exp_scores)

    if sum_exp_scores == 0:
        # This might happen if the item is larger than all bin capacities.
        # In a real implementation, you might want to open a new bin.
        # For this function, we return zeros as no bin is prioritized.
        return np.zeros_like(bins_remain_cap)
    else:
        priorities = exp_scores / sum_exp_scores
        return priorities
```
