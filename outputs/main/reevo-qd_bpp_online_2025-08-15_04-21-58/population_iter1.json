[
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit strategy.\n\n    In the First Fit strategy, we prioritize bins that can accommodate the item\n    and have the *least* remaining capacity after placing the item. This\n    aims to pack items tightly, potentially leaving larger gaps in later bins\n    for larger items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item get a priority of -1 (lowest).\n        Bins that can fit the item get a priority based on their remaining\n        capacity after packing. Higher remaining capacity after packing gets a\n        lower priority (which means it's a better fit).\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with a low priority\n    \n    # Find bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins that can fit the item, calculate the remaining capacity after placement\n    remaining_caps_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # The priority is based on the remaining capacity after placing the item.\n    # We want to minimize the remaining capacity (tightest fit), so higher\n    # remaining capacity means lower priority (higher negative value).\n    # We negate the remaining capacity to achieve this, as numpy.argsort\n    # sorts in ascending order, and we want the smallest remaining capacity\n    # to have the highest priority.\n    priorities[can_fit_mask] = -remaining_caps_after_fit\n    \n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 30.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response4.txt_stdout.txt",
    "code_path": "problem_iter1_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First strategy.\n\n    The Exact Fit First (EFF) strategy prioritizes bins that can exactly fit the item.\n    Among bins that can exactly fit, it prefers bins with less remaining capacity (to\n    minimize wasted space). For bins that cannot exactly fit, it assigns a lower priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can exactly fit the item\n    exact_fit_bins_mask = bins_remain_cap == item\n\n    # For bins that can exactly fit, assign a high priority based on their remaining capacity (smaller is better)\n    # We invert the capacity to make higher values preferred (since we want less remaining space if it's an exact fit)\n    # Add a small epsilon to avoid division by zero if item size is 0 (though unlikely in BPP)\n    priorities[exact_fit_bins_mask] = 1.0 / (bins_remain_cap[exact_fit_bins_mask] + 1e-9)\n\n    # For bins that cannot exactly fit but can accommodate the item, assign a lower priority.\n    # The priority is based on how 'close' they are to an exact fit, i.e., minimizing remaining space.\n    # We prioritize bins that leave less remaining capacity after placing the item.\n    accommodating_bins_mask = bins_remain_cap > item\n    # Calculate the 'waste' for accommodating bins\n    waste = bins_remain_cap[accommodating_bins_mask] - item\n    # Assign priority: higher priority for less waste.\n    # We use a smaller base priority for these bins compared to exact fits.\n    # Add a constant offset to differentiate from exact fit priorities and use inversion for preference.\n    priorities[accommodating_bins_mask] = 0.5 / (waste + 1e-9)\n\n\n    return priorities",
    "response_id": 4,
    "obj": 4.876346230554457,
    "cyclomatic_complexity": 1.0,
    "halstead": 82.0447025077789,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response6.txt_stdout.txt",
    "code_path": "problem_iter1_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Sigmoid Fit Score strategy for Online Bin Packing Problem.\n\n    This heuristic prioritizes bins that can accommodate the item,\n    with a higher priority given to bins that have a remaining capacity\n    closer to the item size, but not exceeding it significantly.\n    This is achieved using a sigmoid function applied to the \"goodness\"\n    of the fit, where goodness is inversely related to the difference\n    between bin capacity and item size.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate the difference between remaining capacity and item size.\n    # We are interested in fits where remaining_capacity >= item.\n    # If remaining_capacity < item, the fit is impossible, and the \"goodness\" should be 0.\n    fit_diff = bins_remain_cap - item\n\n    # For bins where the item fits, calculate a \"goodness\" score.\n    # We want a score that is higher when fit_diff is smaller (i.e., tighter fit).\n    # A simple approach is 1 / (1 + fit_diff) for fit_diff >= 0.\n    # To avoid division by zero or very large values when fit_diff is 0,\n    # we can use a small epsilon or a shifted difference.\n    # Let's use a scaled difference and then apply a sigmoid.\n\n    # Consider only bins where the item fits.\n    possible_fits = bins_remain_cap >= item\n\n    # For possible fits, calculate a \"tightness\" metric.\n    # Smaller difference is better.\n    # We can map this to a 0-1 scale for the sigmoid.\n    # A simple mapping could be 1 - (fit_diff / max_possible_fit_diff) if we knew that,\n    # or more simply, use a function that favors smaller positive differences.\n\n    # Let's use the sigmoid function: sigmoid(x) = 1 / (1 + exp(-x))\n    # We want to map 'fit_diff' such that smaller non-negative values of fit_diff\n    # result in higher sigmoid outputs.\n    # If we use `exp(-fit_diff)`, then smaller `fit_diff` leads to larger `exp(-fit_diff)`.\n    # This seems like a good starting point.\n    # However, we only care about positive differences (where it fits).\n    # Let's define a scoring mechanism that is 0 for impossible fits and then\n    # uses a sigmoid on the positive differences.\n\n    # A common approach for such problems with sigmoid is to transform the 'goodness'\n    # such that it maps to the sigmoid argument.\n    # Let's define 'goodness' as (bin_capacity - item) if possible, else -inf.\n    # Or, let's define the sigmoid argument such that smaller positive differences are better.\n    # If x = -(bin_remain_cap - item), then for small positive differences, x is small negative.\n    # Sigmoid of small negative x is close to 0.5.\n    # This is not ideal. We want higher scores for good fits.\n\n    # Let's consider a metric that represents \"how well\" the item fits.\n    # If the bin can fit the item, calculate a score. Otherwise, the score is 0.\n    # A good fit means remaining_capacity is close to item size.\n    # Let's try to create a sigmoid argument `s` where:\n    # - If `bins_remain_cap[i] < item`, `s[i]` should lead to a low sigmoid output (close to 0).\n    # - If `bins_remain_cap[i] >= item`, `s[i]` should be scaled such that a smaller\n    #   `bins_remain_cap[i] - item` results in a higher sigmoid output.\n\n    # Consider `bins_remain_cap[i] - item`. Let this be `d`.\n    # We want to map `d >= 0` to a higher priority.\n    # Let's map `d` to `k * d`. Then `sigmoid(-k * d)` will decrease as `d` increases.\n    # This gives higher priority for smaller differences.\n\n    # Let's use `k=1` and a scaling factor for the input to sigmoid.\n    # A common sigmoid form is `1 / (1 + exp(-slope * (x - intercept)))`.\n    # We want to map `bins_remain_cap` such that when it's just above `item`,\n    # the priority is high.\n\n    # Let's define the input to the sigmoid as:\n    # `score_input = some_constant * (bins_remain_cap - item)`\n    # Then `priority = 1 / (1 + exp(-score_input))`.\n    # If `bins_remain_cap < item`, we want `priority` close to 0.\n    # If `bins_remain_cap` is slightly greater than `item`, we want `priority` close to 1.\n    # If `bins_remain_cap` is much greater than `item`, we want `priority` to decrease.\n\n    # Let's try: `score_input = alpha * (item - bins_remain_cap)` for possible fits.\n    # If `bins_remain_cap >= item`, then `item - bins_remain_cap <= 0`.\n    # As `bins_remain_cap` increases (worse fit), `item - bins_remain_cap` becomes more negative.\n    # `sigmoid(alpha * (negative_value))` will be close to 0.5 or lower. This is still not good.\n\n    # Let's rethink the sigmoid input.\n    # We want a function f(capacity, item) such that f is high when capacity is just >= item,\n    # and low otherwise (either too small or too large a difference).\n    # This is like a bell curve, but sigmoid is monotonic.\n    # The request is for a \"Sigmoid Fit Score strategy\". This usually implies a monotonic relationship.\n    # A common interpretation in BPP literature for sigmoid-like behavior is to\n    # prioritize bins that are \"close\" to fitting the item.\n\n    # Strategy: Prioritize bins where the remaining capacity is just enough for the item.\n    # Bins that can fit the item: `bins_remain_cap >= item`.\n    # Among these, bins with `bins_remain_cap - item` closer to 0 are preferred.\n    # Bins that are too small (`bins_remain_cap < item`) get zero priority.\n\n    # Let's scale the \"slack\" (`bins_remain_cap - item`).\n    # We want to transform this slack such that smaller positive slack leads to a higher priority.\n    # `slack = bins_remain_cap - item`\n\n    # Consider a sigmoid applied to `-(slack)` to get higher values for smaller `slack`.\n    # `priorities = 1 / (1 + exp(-(bins_remain_cap - item)))`\n    # If `bins_remain_cap = item`, priority = 1 / (1 + exp(0)) = 0.5\n    # If `bins_remain_cap = item + 1`, priority = 1 / (1 + exp(-1)) \u2248 0.73\n    # If `bins_remain_cap = item + 5`, priority = 1 / (1 + exp(-5)) \u2248 0.99\n    # This is backwards! Smaller differences should get higher priority.\n\n    # Let's try `priorities = 1 / (1 + exp(-(item - bins_remain_cap)))`\n    # If `bins_remain_cap = item`, priority = 1 / (1 + exp(0)) = 0.5\n    # If `bins_remain_cap = item - 1`, priority = 1 / (1 + exp(1)) \u2248 0.27\n    # If `bins_remain_cap = item + 1`, priority = 1 / (1 + exp(-1)) \u2248 0.73\n    # This gives higher priority when `item - bins_remain_cap` is larger (more negative slack).\n    # This is also not what we want.\n\n    # Let's normalize `bins_remain_cap` relative to the `item` size.\n    # Consider a scaled version of `bins_remain_cap / item`.\n    # We want to favor values slightly greater than 1.\n\n    # Let's use the concept of \"fit quality\".\n    # A perfect fit would have `bins_remain_cap == item`.\n    # Let's define a function that maps `bins_remain_cap` to a score,\n    # which is then passed to a sigmoid.\n\n    # Common Sigmoid Fit heuristic in BPP literature uses a score like:\n    # `priority = sigmoid(k * (remaining_capacity - item))`\n    # where `k` is a scaling factor.\n    # If `remaining_capacity >= item`:\n    #   - `k * (remaining_capacity - item)` is positive. Sigmoid output is > 0.5.\n    #   - As `remaining_capacity` increases, the argument increases, sigmoid output approaches 1.\n    # This prioritizes larger remaining capacities, which is not usually desirable.\n\n    # Another common form is to prioritize *smaller* remaining capacities that *still fit*.\n    # So, `k * (item - remaining_capacity)`.\n    # If `remaining_capacity >= item`:\n    #   - `item - remaining_capacity` is negative or zero.\n    #   - As `remaining_capacity` increases, `item - remaining_capacity` becomes more negative.\n    #   - `k * (item - remaining_capacity)` becomes more negative.\n    #   - `sigmoid(more_negative)` approaches 0.\n\n    # This means `sigmoid(k * (item - remaining_capacity))` with `k > 0` would prioritize\n    # bins with `remaining_capacity` very close to `item` (where `item - remaining_capacity` is\n    # close to 0, giving sigmoid output close to 0.5), and then as `remaining_capacity` increases,\n    # the priority drops. This is also not typically what's wanted.\n\n    # The common interpretation of \"Sigmoid Fit\" for BPP often aims to select bins\n    # that are not too full and not too empty.\n    # A \"good fit\" is often `bins_remain_cap` close to `item`.\n\n    # Let's consider the \"space available\" relative to the item size.\n    # If `bins_remain_cap < item`, this bin is invalid.\n    # If `bins_remain_cap >= item`, we calculate a score.\n\n    # Let's use the exponential decay based on slack.\n    # Higher priority for smaller slack `bins_remain_cap - item`.\n    # `score = exp(-alpha * (bins_remain_cap - item))`\n    # We can then pass this to a sigmoid, or use it directly if we scale it.\n\n    # Let's consider a simple sigmoid where the input controls the steepness and\n    # the centering of the transition.\n    # `priority = 1 / (1 + exp(-slope * (bins_remain_cap - center)))`\n    # We want high priority when `bins_remain_cap` is slightly above `item`.\n    # So, `center` should be around `item`.\n    # Let `center = item`.\n    # `priority = 1 / (1 + exp(-slope * (bins_remain_cap - item)))`\n    # If `bins_remain_cap = item`: priority = 0.5\n    # If `bins_remain_cap = item + delta` (delta > 0): priority > 0.5\n    # If `bins_remain_cap = item - delta` (delta > 0): priority < 0.5\n    # This prioritizes bins that are *larger* than what's needed, which isn't optimal.\n\n    # A different interpretation of \"Sigmoid Fit\":\n    # The goal is to fill bins as much as possible without overflow,\n    # and among valid bins, prefer those that leave the least remaining capacity.\n    # This means prioritizing bins with `bins_remain_cap` such that\n    # `bins_remain_cap - item` is minimized (but >= 0).\n\n    # Let's consider the input to the sigmoid to be a value that increases as the fit becomes worse.\n    # And then we can invert the sigmoid output.\n    # Or, let the input be `k * (bins_remain_cap - item)`. Higher means worse fit.\n    # So we want to invert the sigmoid output of this.\n\n    # A practical approach for \"Sigmoid Fit Score\" in BPP often involves prioritizing\n    # bins that are \"nearly full\" but can still accommodate the item.\n    # Let `bins_remain_cap[i]` be the remaining capacity.\n    # We are interested in `bins_remain_cap[i] >= item`.\n\n    # Let's define a \"fit quality score\" `q`.\n    # If `bins_remain_cap[i] < item`, `q[i] = 0` (or a very small negative number).\n    # If `bins_remain_cap[i] >= item`, we want `q[i]` to be higher for smaller `bins_remain_cap[i] - item`.\n    # So, let `q[i] = - (bins_remain_cap[i] - item) = item - bins_remain_cap[i]`.\n    # If `bins_remain_cap[i] == item`, `q[i] = 0`.\n    # If `bins_remain_cap[i] == item + 5`, `q[i] = -5`.\n    # We want higher priorities for higher `q`.\n\n    # Let's scale `q` to be in a range suitable for sigmoid input.\n    # A good range for sigmoid input to transition from low to high is usually around [-5, 5].\n    # So, let's map `q` values to this range.\n    # If `bins_remain_cap[i]` can be very large, `q` can be very negative.\n    # We need to bound `q` or use a robust scaling.\n\n    # Alternative strategy: \"Best Fit\" using a sigmoid-like penalty for too much space.\n    # Consider `score = bins_remain_cap - item`. We want this to be small and non-negative.\n    # Let's use `sigmoid(-k * (bins_remain_cap - item))`.\n    # If `bins_remain_cap < item`, then `bins_remain_cap - item < 0`.\n    # `-k * (bins_remain_cap - item)` is positive. Sigmoid output > 0.5. This prioritizes bins that are too small.\n\n    # Let's reconsider the goal: Prioritize bins that are \"just right\" for the item.\n    # This means `bins_remain_cap` should be slightly larger than `item`.\n    # A common way to implement this is to map `bins_remain_cap` to a score and then use sigmoid.\n    # Let the score be `s = -(bins_remain_cap - item)` for valid fits.\n    # `s = item - bins_remain_cap`. We want higher priority when `s` is close to 0.\n\n    # Let's apply a sigmoid to a scaled version of `s`.\n    # For valid fits (`bins_remain_cap >= item`), `s <= 0`.\n    # We want higher priority as `s` approaches 0 from negative values.\n    # So, `sigmoid(k * s)` where `k > 0`.\n    # `priority = 1 / (1 + exp(-k * (item - bins_remain_cap)))`\n\n    # If `bins_remain_cap = item`: `priority = 1 / (1 + exp(0)) = 0.5`\n    # If `bins_remain_cap = item + 1`: `priority = 1 / (1 + exp(-k))`\n    # If `bins_remain_cap = item + 5`: `priority = 1 / (1 + exp(-5k))`\n    # As `bins_remain_cap` increases, `item - bins_remain_cap` becomes more negative, `exp(-k * ...)` decreases,\n    # so `priority` approaches 1. This is still prioritizing larger remaining capacity.\n\n    # This implies the interpretation of \"Sigmoid Fit Score\" in the prompt needs to be for a monotonic priority.\n    # The typical interpretation of \"fitting an item\" implies that a tighter fit (less wasted space) is better.\n    # Let's implement a priority that favors bins with `bins_remain_cap` close to `item`.\n\n    # Define `alpha` as a parameter to control the steepness of the sigmoid.\n    # A higher `alpha` means a sharper transition around the 'optimal' fit.\n    # Let's choose `alpha = 2.0` for demonstration.\n    alpha = 2.0\n\n    # For bins where the item fits: `bins_remain_cap[i] >= item`\n    # Calculate a score that is higher for smaller `bins_remain_cap[i] - item`.\n    # Let's use `-(bins_remain_cap[i] - item)` as the basis for our sigmoid input.\n    # This value is `item - bins_remain_cap[i]`.\n    # For valid bins, this value is non-positive.\n    # A higher priority should be assigned when this value is close to zero.\n\n    # Let's map `item - bins_remain_cap[i]` to the sigmoid input.\n    # `sigmoid_input = alpha * (item - bins_remain_cap[i])`\n    # If `bins_remain_cap[i] < item`, `item - bins_remain_cap[i] > 0`. `sigmoid_input > 0`.\n    # This results in sigmoid output > 0.5, prioritizing bins that are too small.\n    # This is not correct.\n\n    # The most standard \"Sigmoid Fit\" implementation for BPP implies that if an item\n    # can fit, we want to give it a high priority, and if it's very close to fitting,\n    # that's the best.\n\n    # Let's consider the \"slack\" `s = bins_remain_cap[i] - item`.\n    # We want to prioritize when `s` is small and non-negative.\n    # Let's create a score that maps `s` to `[0, 1]` and then perhaps invert or scale.\n\n    # A robust sigmoid approach:\n    # For each bin `j`:\n    # If `bins_remain_cap[j] < item`, `priority[j] = 0`.\n    # If `bins_remain_cap[j] >= item`:\n    #   Calculate a \"fit ratio\" or \"fit score\".\n    #   A common choice is to focus on the *tightness* of the fit.\n    #   `slack = bins_remain_cap[j] - item`\n    #   We want smaller `slack` to yield higher priority.\n    #   Let's scale `slack` and pass it to `sigmoid(-k * slack)`.\n\n    # Define a scaling factor for the difference.\n    # If the remaining capacities can vary widely, a fixed scaling factor might not work.\n    # For now, let's assume a reasonable range or use a moderate fixed scale.\n    scale_factor = 1.0  # Tune this parameter\n\n    # Calculate the difference between remaining capacity and item size.\n    # We only consider bins where the item can fit.\n    diff = bins_remain_cap - item\n\n    # For bins where the item fits (diff >= 0), we want to assign a priority.\n    # Higher priority for smaller non-negative differences.\n    # Let's use `sigmoid(a * (b - x))` where `x` is `bins_remain_cap`.\n    # We want the curve to peak (or be high) around `bins_remain_cap = item`.\n    # Let `priority = sigmoid(scale_factor * (item - bins_remain_cap))` for valid bins.\n    # `priority = 1 / (1 + exp(-scale_factor * (item - bins_remain_cap)))`\n\n    # If `bins_remain_cap[j] < item`, then `item - bins_remain_cap[j] > 0`.\n    # `exp(-scale_factor * positive)` -> close to 0. `priority` close to 1.\n    # This prioritizes bins that are too small. This is a problem.\n\n    # The common Sigmoid Fit heuristic for BPP prioritizes bins that can fit the item,\n    # giving preference to those that leave the smallest remainder.\n    # This means we are looking for the \"tightest\" possible fit.\n\n    # Let's consider the problem formulation where `priorities` are calculated for *all* bins,\n    # and then the bin with the highest priority is chosen.\n    # If a bin cannot fit the item, its priority must be the lowest possible, usually 0.\n\n    # Strategy:\n    # 1. Initialize all priorities to 0.\n    # 2. For bins where `bins_remain_cap[i] >= item`:\n    #    Calculate a score. A higher score means a \"better\" fit.\n    #    \"Better\" fit means `bins_remain_cap[i] - item` is small.\n    #    Let `slack = bins_remain_cap[i] - item`.\n    #    We want to map `slack` to a score that is high for small `slack`.\n    #    Consider `score = -slack`. We want higher for values close to 0.\n    #    `score` will be 0 for perfect fits, negative for over-sized fits.\n\n    #    Let's use the sigmoid function: `f(x) = 1 / (1 + exp(-x))`\n    #    We want `f(input)` to be high when `slack` is small.\n    #    So, `input` should be related to `-slack`.\n    #    `input = alpha * (-slack) = -alpha * slack`\n    #    `priority = 1 / (1 + exp(alpha * slack))`\n    #    If `slack` is small and positive (good fit), `alpha * slack` is small positive.\n    #    `exp(small_positive)` is > 1. `1 + exp(...)` is > 2. `priority` is < 0.5.\n    #    If `slack` is large positive (bad fit), `alpha * slack` is large positive.\n    #    `exp(...)` is large. `priority` is close to 0.\n    #    This prioritizes bins with large slack (bad fits), which is wrong.\n\n    # Let's reverse the logic: `sigmoid(alpha * (some_value_that_decreases_with_slack))`\n    # How about `sigmoid(alpha * (item - bins_remain_cap))`?\n    # Valid bins: `bins_remain_cap >= item`. So `item - bins_remain_cap <= 0`.\n    # Let `x = item - bins_remain_cap`.\n    # `priority = sigmoid(alpha * x)`\n    # If `bins_remain_cap = item`, `x = 0`, `priority = 0.5`.\n    # If `bins_remain_cap = item + delta`, `x = -delta`, `priority = sigmoid(-alpha * delta)` ( < 0.5)\n    # If `bins_remain_cap = item - delta`, `x = delta`, `priority = sigmoid(alpha * delta)` ( > 0.5)\n    # This prioritizes bins that are too small again.\n\n    # It's possible the \"Sigmoid Fit Score\" refers to a score that is derived using sigmoid,\n    # and then that score is used directly or indirectly for priority.\n    # Perhaps it's about creating a penalty function.\n\n    # Let's try a strategy inspired by \"Largest Gap Fit\" but using sigmoid.\n    # We want to prioritize bins that have a significant amount of remaining capacity,\n    # but not so much that it's extremely wasteful.\n\n    # Let's assume a \"target\" fit is `bins_remain_cap` just above `item`.\n    # Define a scoring function that captures this.\n\n    # Let's implement the common interpretation of a \"Sigmoid Fit\" in BPP literature:\n    # Prioritize bins that can fit the item, with preference for those leaving the smallest remainder.\n    # This is effectively a \"Best Fit\" heuristic, but we can apply a sigmoid transformation.\n\n    # 1. Identify bins that can fit the item.\n    # 2. For these bins, calculate a \"fit score\". The tighter the fit, the higher the score.\n    #    `score = - (bins_remain_cap[i] - item)` for `bins_remain_cap[i] >= item`.\n    #    Or, to avoid negative numbers in sigmoid input naturally, we can use\n    #    `score = (max_capacity_or_some_scale) - (bins_remain_cap[i] - item)`.\n    #    Let's try a simpler approach.\n\n    # Use the sigmoid to smooth the decision for valid bins.\n    # The input to the sigmoid should be high for tight fits and low for loose fits.\n    # `score_for_sigmoid = - (bins_remain_cap[i] - item)`\n    # These values are <= 0.\n    # We want to transform these negative values so that values closer to 0 (tight fits)\n    # result in higher sigmoid outputs.\n\n    # Use `sigmoid(alpha * score_for_sigmoid)`\n    # `priority = 1 / (1 + exp(-alpha * (item - bins_remain_cap)))`\n    # For bins where `bins_remain_cap < item`, priority should be 0.\n\n    # Initialize priorities to 0.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit.\n    can_fit_indices = np.where(bins_remain_cap >= item)[0]\n\n    # For bins that can fit the item, calculate their priority score.\n    # We want to prioritize bins with remaining capacity closest to the item size.\n    # Let the score be based on `item - bins_remain_cap`. This value is <= 0 for fitting bins.\n    # We want higher priority when `item - bins_remain_cap` is closer to 0.\n    # Using `sigmoid(alpha * (item - bins_remain_cap))` where `alpha > 0`.\n\n    # This sigmoid function maps inputs `x` to `[0, 1]`.\n    # `sigmoid(x) = 1 / (1 + exp(-x))`\n    # Let `x = alpha * (item - bins_remain_cap[j])`.\n    # If `bins_remain_cap[j] = item`, then `x = 0`, `sigmoid(0) = 0.5`.\n    # If `bins_remain_cap[j] = item + epsilon` (epsilon > 0, tight fit), `x = -alpha*epsilon`.\n    #   `sigmoid(-alpha*epsilon)` will be slightly less than 0.5.\n    # If `bins_remain_cap[j] = item + large_epsilon` (loose fit), `x = -alpha*large_epsilon`.\n    #   `sigmoid(-alpha*large_epsilon)` will be close to 0.\n    # This logic assigns higher priority to *looser* fits among the valid ones.\n\n    # To prioritize *tighter* fits:\n    # We need a function that increases as `slack = bins_remain_cap[j] - item` decreases.\n    # Let `y = -slack = item - bins_remain_cap[j]`.\n    # We want a function that increases as `y` increases towards 0.\n    # `sigmoid(alpha * y)` works for this.\n    # However, for valid fits, `y <= 0`.\n    # `sigmoid(alpha * y)` where `y <= 0` and `alpha > 0`:\n    #   - `y=0` (perfect fit): `sigmoid(0) = 0.5`.\n    #   - `y=-0.1` (tight fit): `sigmoid(-0.1*alpha)`.\n    #   - `y=-10` (loose fit): `sigmoid(-10*alpha)` (close to 0).\n    # This implies that the higher values from `sigmoid(alpha * (item - bins_remain_cap))`\n    # are for bins closer to `item`, which is what we want.\n\n    # Let's make sure the `alpha` parameter is tunable, but a default is provided.\n    # `alpha` controls how quickly the priority drops as the slack increases.\n    # A smaller `alpha` means a smoother drop.\n\n    # Consider the scaling of `item - bins_remain_cap`.\n    # If `item` is 10 and `bins_remain_cap` are [12, 15, 20],\n    # then `item - bins_remain_cap` are [-2, -5, -10].\n    # If `alpha=1`: `sigmoid` inputs are [-2, -5, -10].\n    #   `sigmoid` outputs: [0.119, 0.0067, 0.000045].\n    # This assigns higher priority to less slack. This is correct for Best Fit.\n\n    # Let's define the score clearly.\n    # For `j` where `bins_remain_cap[j] >= item`:\n    #   `slack = bins_remain_cap[j] - item`\n    #   `priority_score = 1.0 / (1.0 + np.exp(alpha * slack))`\n    # This gives values < 0.5 for `slack > 0`.\n    # Tighter fits (smaller slack) result in scores closer to 0.5.\n    # Looser fits (larger slack) result in scores closer to 0.\n\n    # Revisit: what does Sigmoid Fit Score *mean*?\n    # It likely means using sigmoid to map a quality metric to a priority.\n    # Metric: How well does the item fit the bin?\n    # Good Metric: Smaller `bins_remain_cap - item` (for valid fits).\n    # Let `x = bins_remain_cap - item`. For valid bins, `x >= 0`.\n    # We want a score that is high for small `x`.\n    # Consider `f(x) = exp(-alpha * x)`. This is high for small `x`.\n    # Now, pass this to a sigmoid: `sigmoid(beta * exp(-alpha * x))`.\n    # This is complex.\n\n    # A common way to interpret \"Sigmoid Fit\" is to use a sigmoid function directly on a\n    # quantity related to the fit, such that the output is monotonic.\n\n    # Let's implement a common heuristic described as \"Sigmoid Fit\":\n    # The idea is to favor bins that can accommodate the item, and among them,\n    # those that leave less remaining capacity (i.e., tighter fit).\n    # This sounds like Best Fit. We can make it \"sigmoid-like\" by transforming the\n    # quality metric (which is `-slack`) with a sigmoid.\n\n    # Let `alpha` be a parameter to control the steepness.\n    alpha = 2.0  # Example value, can be tuned.\n\n    # Calculate the \"goodness of fit\".\n    # For bins where the item fits (`bins_remain_cap[i] >= item`):\n    # The quality is higher when `bins_remain_cap[i] - item` is smaller.\n    # Let `fit_quality = -(bins_remain_cap[i] - item) = item - bins_remain_cap[i]`.\n    # `fit_quality` is <= 0 for fitting bins. Higher values are better (closer to 0).\n\n    # We want to map `fit_quality` to a priority.\n    # Use the sigmoid function: `priority = 1 / (1 + exp(-input))`\n    # We want `priority` to increase as `fit_quality` increases.\n    # So, the `input` to the sigmoid should be `alpha * fit_quality`.\n\n    # For invalid bins (`bins_remain_cap[i] < item`), the priority is 0.\n\n    # Apply sigmoid transformation to valid fits.\n    # Ensure `bins_remain_cap` is an array.\n    bins_remain_cap = np.asarray(bins_remain_cap, dtype=float)\n\n    # Initialize priorities to zero.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find indices of bins where the item can fit.\n    fit_indices = np.where(bins_remain_cap >= item)[0]\n\n    if len(fit_indices) > 0:\n        # Calculate fit quality for these bins.\n        # Higher quality for smaller slack (closer fit).\n        # `fit_quality = item - bins_remain_cap`\n        fit_qualities = item - bins_remain_cap[fit_indices]\n\n        # Apply sigmoid to transform fit quality into priority.\n        # The input to sigmoid is `alpha * fit_quality`.\n        # Higher `fit_quality` (closer to 0) results in smaller `alpha * fit_quality` (closer to 0 or more negative).\n        # Sigmoid `1/(1+exp(-x))` with `x=alpha*fit_quality` will give higher values as `x` increases.\n        # Since `fit_quality` is `item - bins_remain_cap`, this means higher priority for smaller `bins_remain_cap`\n        # among the valid ones.\n\n        # Re-evaluate sigmoid input.\n        # We want `priority = sigmoid(k * metric)`\n        # Metric: We want larger values for smaller slack.\n        # Slack: `s = bins_remain_cap - item`\n        # Metric can be `-s = item - bins_remain_cap`\n        # If `item=10`, `bins_remain_cap=[12, 15, 20]`.\n        # Slack `s=[2, 5, 10]`.\n        # Metric `-s=[-2, -5, -10]`.\n        # If `k=1`, `sigmoid_input = [-2, -5, -10]`.\n        # Sigmoid outputs: `[0.119, 0.0067, 0.000045]`.\n        # This assigns *lower* priority to tighter fits. Incorrect.\n\n        # The sigmoid should transform the slack in a way that small slack -> high priority.\n        # Let's define the argument `x` for `sigmoid(x)`:\n        # `x` should be large positive for small slack.\n        # `slack = bins_remain_cap - item`\n        # So, `x = alpha * (K - slack)` for some constant K or scaling.\n        # Let's try `x = alpha * (item - bins_remain_cap)` again.\n        # `bins_remain_cap` increases => `item - bins_remain_cap` decreases (more negative)\n        # => `alpha * (item - bins_remain_cap)` decreases\n        # => `sigmoid` output decreases. This assigns *lower* priority to *looser* fits.\n        # This is what we want.\n\n        # Let `score_arg = alpha * (item - bins_remain_cap[j])`\n        # If `bins_remain_cap[j] = item`, `score_arg = 0`, `sigmoid(0) = 0.5`\n        # If `bins_remain_cap[j] = item + epsilon`, `score_arg = -alpha * epsilon`, `sigmoid < 0.5`\n        # If `bins_remain_cap[j] = item - epsilon`, `score_arg = alpha * epsilon`, `sigmoid > 0.5`\n        # This still gives higher priority for bins that are too small.\n\n        # The prompt phrasing \"priority score for each bin\" implies we return a score for all.\n        # \"The bin with the highest priority score will be selected\".\n\n        # Let's consider the definition from a source like \"Online Bin Packing Algorithms\":\n        # \"Sigmoid Fit: Assigns a score based on the difference between the remaining capacity\n        # and the item size using a sigmoid function. The intention is to prioritize bins\n        # that have just enough space for the item.\"\n\n        # Let's consider the \"fitting range\" `[item, item + epsilon]`.\n        # Bins outside this range but still fitting should have lower priority than those inside.\n\n        # A common \"sigmoid fit\" is one where you map `bins_remain_cap` to `[0,1]` or similar,\n        # and then use that in some fashion.\n\n        # Let's try to map `bins_remain_cap` directly using a sigmoid.\n        # We want a function that's high for `bins_remain_cap` slightly above `item`.\n        # A bell-shaped function is ideal, but sigmoid is monotonic.\n        # The sigmoid will likely be applied to a transformed value.\n\n        # Let's define the input to the sigmoid:\n        # `input_value = slope * (bins_remain_cap - critical_point)`\n        # We want the transition to be around `bins_remain_cap = item`.\n        # Let `critical_point = item`.\n        # `input_value = slope * (bins_remain_cap - item)`\n        # `priority = sigmoid(input_value)`\n        # If `slope > 0`:\n        #   `bins_remain_cap = item`: `input = 0`, `priority = 0.5`.\n        #   `bins_remain_cap > item`: `input > 0`, `priority > 0.5`. (prioritizes larger capacities)\n        #   `bins_remain_cap < item`: `input < 0`, `priority < 0.5`. (prioritizes smaller capacities)\n\n        # If `slope < 0`:\n        #   `bins_remain_cap = item`: `input = 0`, `priority = 0.5`.\n        #   `bins_remain_cap > item`: `input < 0`, `priority < 0.5`. (prioritizes smaller capacities)\n        #   `bins_remain_cap < item`: `input > 0`, `priority > 0.5`. (prioritizes larger capacities)\n\n        # This is tricky. The \"Sigmoid Fit\" heuristic often needs to be interpreted based on the desired outcome.\n        # Outcome: Pick the bin with the tightest fit among those that can accommodate the item.\n\n        # Let's go with the interpretation: prioritize bins with minimal slack, and use sigmoid to represent this preference.\n        # For bins `j` where `bins_remain_cap[j] >= item`:\n        # The \"score\" we want to maximize is inversely related to `bins_remain_cap[j] - item`.\n        # Let `slack = bins_remain_cap[j] - item`.\n        # We want to map `slack` to `[0, 1]` where `0` corresponds to large slack and `1` to small slack.\n        # The sigmoid function `sigmoid(x) = 1 / (1 + exp(-x))` gives values from 0 to 1.\n        # If we use `x = -alpha * slack`, then smaller `slack` gives larger `-alpha * slack` (more negative if alpha > 0)\n        # or smaller `alpha * slack` if slack is treated as a penalty.\n\n        # Let's define a metric `m = -(bins_remain_cap[j] - item)` for valid bins.\n        # This `m` is `item - bins_remain_cap[j]`, which is <= 0.\n        # We want higher priority for `m` closer to 0.\n        # `priority = sigmoid(alpha * m) = sigmoid(alpha * (item - bins_remain_cap[j]))`\n        # As `bins_remain_cap[j]` increases (looser fit), `item - bins_remain_cap[j]` becomes more negative,\n        # so `sigmoid` output decreases. This correctly prioritizes tighter fits.\n\n        # Set `alpha`. Higher alpha makes the preference for tighter fits more pronounced.\n        alpha = 3.0  # Tunable parameter: steepness of sigmoid.\n\n        # Calculate sigmoid inputs for the fitting bins.\n        # We want to avoid overflow in exp.\n        # Let's clip the input to the sigmoid to a reasonable range, e.g., [-10, 10].\n        sigmoid_inputs = alpha * (item - bins_remain_cap[fit_indices])\n        sigmoid_inputs = np.clip(sigmoid_inputs, -10.0, 10.0)\n\n        # Calculate priorities for the fitting bins.\n        priorities[fit_indices] = 1.0 / (1.0 + np.exp(-sigmoid_inputs))\n\n        # Ensure that bins that cannot fit have zero priority. This is handled by initialization.\n        # However, for robustness, one might explicitly set them to a very low value if using\n        # a different strategy, but here initialization to 0 is correct.\n\n    return priorities",
    "response_id": 6,
    "obj": 4.078579976067022,
    "cyclomatic_complexity": 2.0,
    "halstead": 159.91133951083242,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response8.txt_stdout.txt",
    "code_path": "problem_iter1_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    This heuristic prioritizes bins that can fit the item. Among the bins that can fit,\n    it applies the epsilon-greedy strategy:\n    - With probability epsilon, it chooses a bin randomly among those that can fit.\n    - With probability 1-epsilon, it chooses the bin that results in the least remaining capacity\n      (Best Fit strategy) among those that can fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    epsilon = 0.2  # Exploration probability\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can fit the item\n    eligible_bins_indices = np.where(bins_remain_cap >= item)[0]\n\n    if len(eligible_bins_indices) == 0:\n        # No bin can fit the item, return all zeros (or handle as an error if needed)\n        return priorities\n\n    # Epsilon-Greedy decision\n    if np.random.rand() < epsilon:\n        # Explore: choose randomly among eligible bins\n        chosen_index = np.random.choice(eligible_bins_indices)\n        priorities[chosen_index] = 1.0  # Assign highest priority to the chosen bin\n    else:\n        # Exploit: choose the bin that fits best (minimum remaining capacity after placement)\n        remaining_capacities_after_placement = bins_remain_cap[eligible_bins_indices] - item\n        best_fit_index_in_eligible = np.argmin(remaining_capacities_after_placement)\n        chosen_index = eligible_bins_indices[best_fit_index_in_eligible]\n        priorities[chosen_index] = 1.0 # Assign highest priority to the chosen bin\n\n    return priorities",
    "response_id": 8,
    "obj": 4.397686477862,
    "cyclomatic_complexity": 3.0,
    "halstead": 41.51317942364757,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response13.txt_stdout.txt",
    "code_path": "problem_iter1_code13.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # The \"Almost Full Fit\" strategy aims to select a bin that is almost full,\n    # but can still accommodate the item.\n    # A good heuristic would be to prioritize bins where the remaining capacity\n    # is slightly larger than the item's size. This minimizes wasted space\n    # in the chosen bin while ensuring the item fits.\n\n    # We want to find bins where bins_remain_cap[i] - item is small and positive.\n    # A higher score means higher priority.\n\n    # We can create a score that is high when (bins_remain_cap[i] - item) is close to zero\n    # and positive.\n    # If bins_remain_cap[i] < item, the bin cannot hold the item, so its priority should be 0.\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_accommodate_mask = bins_remain_cap >= item\n\n    # For bins that can accommodate the item, calculate the \"slack\" (remaining capacity after placing the item)\n    slack = bins_remain_cap[can_accommodate_mask] - item\n\n    # We want to prioritize bins with minimal slack.\n    # A simple way to create a priority score is to take the inverse of the slack + a small epsilon\n    # to avoid division by zero if slack is exactly zero.\n    # Alternatively, a negative of the slack can work if we sort in descending order.\n    # Let's use the negative slack for simplicity in direct mapping to higher priority.\n    # The more negative (closer to zero), the better.\n    # To make it a positive priority score, we can transform it. A large negative number means\n    # it's a \"bad\" fit (too much remaining space). A small negative number (close to zero) means\n    # it's a \"good\" fit (almost full).\n\n    # Let's define priority as a value that decreases as slack increases.\n    # If slack is 0, priority is high. If slack is large, priority is low.\n    # Consider priority = -slack. This gives higher values for smaller slacks.\n    # We can add 1 to make all priorities non-positive to positive, ensuring higher means better.\n    # Or, we can use a large constant minus slack.\n\n    # Let's use a value that is inversely proportional to the slack.\n    # A common approach for \"best fit\" like heuristics is to maximize `remaining_capacity - item_size`.\n    # For \"almost full fit\", we want to MINIMIZE `remaining_capacity - item_size`, but only for valid fits.\n    # So, we want `remaining_capacity - item_size` to be small and positive.\n    # Let's try a priority that is high when `remaining_capacity - item_size` is small.\n    # A possible transformation: 1 / (epsilon + remaining_capacity - item_size) for positive values.\n    # Or, to avoid division by zero and keep it simpler, and to directly rank smaller remaining space higher:\n    # priority = - (remaining_capacity - item_size)\n    # Higher priority (less negative) means smaller remaining_capacity - item_size.\n\n    # Let's try to make it simple: we want bins where bins_remain_cap is slightly greater than item.\n    # So, we want bins_remain_cap - item to be small and positive.\n    # Let's map this to a priority score.\n    # If bin_remain_cap = item, slack = 0, priority is highest.\n    # If bin_remain_cap = item + 1, slack = 1, priority is lower.\n    # If bin_remain_cap = item - 1, not possible.\n\n    # A potential function: prioritize bins where `bins_remain_cap[i]` is close to `item`.\n    # We want to maximize `item - bins_remain_cap[i]` such that `bins_remain_cap[i] >= item`.\n    # This is equivalent to minimizing `bins_remain_cap[i] - item` (the slack).\n    # So, we can use `-(bins_remain_cap[i] - item)` as priority for valid bins.\n    # Let's make the score directly represent the desirability, where higher is better.\n    # This means smaller slack = higher priority.\n\n    # A simple \"almost full\" priority could be `bins_remain_cap - item` where this difference is small.\n    # For bins that *can* fit the item, a good priority would be high when the remaining capacity\n    # after placing the item is minimized.\n    # So, we want to maximize the value `M - (bins_remain_cap[i] - item)` where M is a large constant.\n    # Or, simply use `- (bins_remain_cap[i] - item)` for valid bins and assign 0 to invalid bins.\n    # To ensure higher means better, we can take the negative of this quantity.\n    # If bins_remain_cap[i] - item = 0, priority is 0.\n    # If bins_remain_cap[i] - item = 1, priority is -1.\n    # If bins_remain_cap[i] - item = 5, priority is -5.\n\n    # Let's try a different approach: we want bins with the smallest `bins_remain_cap[i]`\n    # that is still greater than or equal to `item`.\n    # So, sort by `bins_remain_cap[i]` and pick the first valid one.\n    # This requires sorting and then picking. The priority function should assign a score to *each* bin.\n\n    # Let's try a score that reflects \"how close it is to being full with the item\".\n    # Consider the remaining capacity after placing the item: `bins_remain_cap[i] - item`.\n    # We want this to be small and positive.\n    # Priority score: A high score for small positive `bins_remain_cap[i] - item`.\n    # If `bins_remain_cap[i] < item`, priority is 0.\n    # For `bins_remain_cap[i] >= item`:\n    # Let's try a score like: `1 / (1 + (bins_remain_cap[i] - item))`\n    # If slack is 0, score is 1.\n    # If slack is 1, score is 1/2.\n    # If slack is 5, score is 1/6.\n    # This gives higher scores for smaller slacks.\n\n    # Apply the rule: only consider bins that can accommodate the item.\n    # For these bins, compute the priority based on minimum remaining capacity after placement.\n    # Let's use a scoring mechanism that penalizes large remaining capacities.\n    # We want to maximize `some_large_value - (bins_remain_cap[i] - item)` for valid bins.\n    # A simpler form is just `-(bins_remain_cap[i] - item)`, where higher values are better (less negative).\n    # This means a slack of 0 gives priority 0, slack of 1 gives -1, etc.\n\n    # Let's refine: We want to select a bin that, after placing the item, has the *least* remaining capacity,\n    # but it must be non-negative.\n    # So, for bins where `bins_remain_cap[i] >= item`:\n    # Priority is related to `bins_remain_cap[i] - item`. We want to *minimize* this value.\n    # If we want a higher score for better bins, we can use `-(bins_remain_cap[i] - item)`.\n    # This way, a slack of 0 yields a priority of 0, and larger slacks yield more negative priorities.\n    # Bins that cannot accommodate the item should have a priority of 0 or a very low negative number.\n\n    # Let's try a score that directly measures how \"almost full\" it is relative to the item.\n    # If bin_capacity = item, it's perfectly full. If bin_capacity = item + k, it has k space left.\n    # We want to prioritize bins with k = 0, then k = 1, etc.\n    # So, we want to maximize the negative of the slack.\n    # `priority = -(bins_remain_cap[i] - item)`\n    # For invalid bins (`bins_remain_cap[i] < item`), the priority should be very low, effectively making them undesirable.\n\n    # Create scores for all bins.\n    # Initialize all priorities to a very low value (e.g., negative infinity) to indicate unsuitability.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Find bins that can fit the item.\n    valid_bins_indices = np.where(bins_remain_cap >= item)[0]\n\n    if len(valid_bins_indices) > 0:\n        # For valid bins, calculate the \"slack\" or remaining capacity after placing the item.\n        slack = bins_remain_cap[valid_bins_indices] - item\n\n        # We want to prioritize bins with the smallest non-negative slack.\n        # A score of 0 for zero slack, -1 for slack of 1, -2 for slack of 2, etc.\n        # This can be achieved by `priority = -slack`.\n        # To make sure positive values are handled well or to avoid negative priorities if all are invalid:\n        # Let's transform the slack. We want to maximize `C - slack` for some constant C.\n        # A simple mapping: higher value for smaller slack.\n        # Let's use a score that is higher for smaller slack.\n        # `priorities[valid_bins_indices] = -slack`\n        # Or, a value that is closer to zero for better fits.\n        # Consider the capacity ratio: `item / bins_remain_cap[i]`.\n        # This is good for First Fit Decreasing, not so much for \"almost full\".\n\n        # Let's stick to minimizing `bins_remain_cap[i] - item`.\n        # Priority = `k - (bins_remain_cap[i] - item)` for some large k.\n        # A simpler approach: Assign a score that is high for small slack.\n        # Example: If the bin capacity is exactly `item`, slack is 0, priority is high.\n        # If the bin capacity is `item + 1`, slack is 1, priority is lower.\n        # We can use a function like `1.0 / (1.0 + slack)` which maps slack 0 to 1, slack 1 to 0.5, etc.\n\n        # Let's re-think \"Almost Full Fit\". It prioritizes bins that are ALMOST FULL.\n        # This means bins with high `bins_remain_cap[i]` are generally good, but only if they can fit the item.\n        # So, we are looking for bins where `bins_remain_cap[i]` is large, but `bins_remain_cap[i] - item` is small.\n        # This suggests prioritizing bins where `bins_remain_cap[i]` is just enough.\n\n        # A possible approach is to sort the valid bins by their remaining capacity in ascending order.\n        # The first bin in this sorted list is the one with the least remaining capacity that can fit the item.\n        # To represent this in a priority array:\n        # Give the highest priority to this bin, then lower priorities to others.\n        # However, the function needs to assign scores directly.\n\n        # Let's try scoring based on the *difference* from the *maximum* possible remaining capacity for a valid bin.\n        # This is getting complicated. Let's go back to minimizing slack.\n\n        # The \"Almost Full Fit\" is essentially a form of \"Best Fit\".\n        # For Best Fit, you want to minimize `bins_remain_cap[i] - item`.\n        # So, for valid bins: `priority = - (bins_remain_cap[i] - item)`\n        # This means a slack of 0 (perfect fit) gets a priority of 0.\n        # A slack of 1 gets a priority of -1.\n        # A slack of 5 gets a priority of -5.\n        # This correctly ranks bins by having higher priority for smaller slacks.\n\n        # Assign these priorities to the valid bins.\n        priorities[valid_bins_indices] = -slack\n\n    # Bins that could not accommodate the item (where bins_remain_cap < item)\n    # already have a priority of -np.inf, which means they will never be chosen\n    # if any valid bin exists.\n\n    # To ensure the heuristic is \"Almost Full Fit\" and not just \"Best Fit\",\n    # we might want to give a slight preference to bins that are \"more full\"\n    # overall, provided they can fit the item.\n    # However, the core of Best Fit and Almost Full Fit is minimizing slack.\n\n    # Let's try to amplify the preference for smaller slacks.\n    # Instead of linear `-(slack)`, use something like `1 / (1 + slack)`.\n    # This maps slack=0 to 1, slack=1 to 0.5, slack=2 to 0.33, etc.\n    # This is good, but what about bins that are very full but cannot fit the item?\n    # Our current approach with -np.inf handles this.\n\n    # Let's use the `1 / (1 + slack)` strategy for valid bins.\n    # We need to handle the case where slack might be very small leading to large priorities.\n    # Let's scale it or use a slightly different approach.\n\n    # Another interpretation of \"Almost Full Fit\" could be to give priority to bins that have\n    # the largest `bins_remain_cap` but *still can fit the item*.\n    # This is contradictory with minimizing slack. \"Almost Full\" means little space left.\n\n    # Let's consider the requirement: \"bin with the highest priority score will be selected\".\n    # So, higher score is better.\n    # For valid bins (`bins_remain_cap[i] >= item`):\n    # We want to prioritize those where `bins_remain_cap[i]` is closest to `item`.\n    # This means `bins_remain_cap[i] - item` should be small.\n    # Let `slack = bins_remain_cap[i] - item`.\n    # We want to maximize `-slack`.\n\n    # A potential issue: if `item` is very small, and `bins_remain_cap` are all very large,\n    # the `-slack` will be negative and similar for many bins.\n\n    # Let's refine the \"Almost Full Fit\" concept. It's about fitting the item into a bin\n    # such that the bin becomes as full as possible, without overflowing.\n    # This is exactly what Best Fit does by minimizing `remaining_capacity - item_size`.\n\n    # To make it *almost full fit* specifically, maybe we should penalize bins that are\n    # already very empty, even if they can fit the item.\n    # This is hard to capture in a single item-centric priority function.\n    # Priority functions typically look at the state of the bin relative to the item.\n\n    # Let's try to encode a preference for bins that have *less* total capacity remaining,\n    # among those that can fit the item.\n    # `priority = k - bins_remain_cap[i]` for valid bins.\n    # This would favor bins with smaller remaining capacities.\n\n    # Example: bins_remain_cap = [5, 10, 2, 7], item = 3\n    # Valid bins: indices 0, 1, 3. Capacities: 5, 10, 7.\n    # Slack: 5-3=2, 10-3=7, 7-3=4.\n    # Priorities from `-slack`: [-2, -7, -4]\n    # With `-np.inf` for bin 2: [-2, -7, -np.inf, -4]\n    # The highest priority is -2, corresponding to bin 0 (remaining cap 5, slack 2).\n\n    # What if we use `1 / (slack + epsilon)` for slack > 0, and a very high value for slack = 0?\n    # `epsilon` is to prevent division by zero.\n\n    # Let's consider the range of possible remaining capacities.\n    # Suppose max_bin_capacity is 10.\n    # bins_remain_cap = [3, 7, 10], item = 2\n    # Valid bins: all of them.\n    # Capacities: 3, 7, 10.\n    # Slacks: 1, 5, 8.\n    # Priorities `-slack`: [-1, -5, -8].\n    # Highest priority is -1 for bin with remaining capacity 3. This is the \"almost full\" fit.\n\n    # Let's make the \"almost full\" aspect more pronounced.\n    # We are looking for `bins_remain_cap[i]` that is just above `item`.\n    # So, `bins_remain_cap[i]` should be as small as possible while being `>= item`.\n    # Consider priority = `bins_remain_cap[i]`. Among valid bins, this favors those with smaller remaining capacity.\n    # If we want higher score for better fit, we should minimize `bins_remain_cap[i]`.\n    # So, the priority should be *inversely* related to `bins_remain_cap[i]`.\n    # This feels wrong. \"Almost Full\" implies the bin itself is almost full, i.e., `bins_remain_cap[i]` is small.\n\n    # The most straightforward interpretation of \"Almost Full Fit\" for a given item\n    # is to place the item into the bin that will leave the *least* amount of free space\n    # in that bin after placement, provided the item fits.\n    # This is precisely the \"Best Fit\" heuristic.\n\n    # Let's implement \"Best Fit\" behavior for the priority function, which should\n    # align with \"Almost Full Fit\" for a given item.\n\n    # For valid bins, we want to maximize `-(bins_remain_cap[i] - item)`.\n    # This means we want to minimize `bins_remain_cap[i] - item` (the slack).\n    # Let `best_fit_score = -(bins_remain_cap[i] - item)`.\n    # Higher value means better fit (smaller slack).\n\n    # Initialize priorities to a very low value, so invalid bins are not chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Find indices of bins that can accommodate the item.\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bin_indices = np.where(can_fit_mask)[0]\n\n    if len(fitting_bin_indices) > 0:\n        # Calculate the remaining capacity in these bins *after* placing the item.\n        # This is the 'slack'. We want to minimize this slack.\n        slacks = bins_remain_cap[fitting_bin_indices] - item\n\n        # To create a priority score where higher is better, and smaller slacks are better:\n        # We can use `-slacks`. A slack of 0 becomes priority 0. A slack of 1 becomes -1.\n        # A larger positive slack becomes a larger negative priority.\n        priorities[fitting_bin_indices] = -slacks\n\n        # Alternative scoring to make the difference more pronounced or non-negative:\n        # Let `max_slack = np.max(slacks)` if slacks exist.\n        # `priorities[fitting_bin_indices] = max_slack - slacks`\n        # This way, the best fit (smallest slack) gets `max_slack - min_slack`.\n        # Example: slacks = [0, 2, 5]. max_slack = 5.\n        # Scores: 5-0=5, 5-2=3, 5-5=0. Higher score is better.\n        # This seems like a good interpretation of \"almost full fit\" within the context of best-fit strategy.\n\n        # Let's try the `max_slack - slacks` approach.\n        # If all slacks are 0, then max_slack - slacks will be 0 for all.\n        # This is fine. The selection might be arbitrary among identical highest priorities.\n\n        if len(slacks) > 0:\n            max_possible_slack = np.max(slacks)\n            priorities[fitting_bin_indices] = max_possible_slack - slacks\n\n    # If no bins can fit the item, priorities remain -np.inf, indicating no solution.\n    return priorities",
    "response_id": 13,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 155.3235879675129,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response17.txt_stdout.txt",
    "code_path": "problem_iter1_code17.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Random Fit.\n\n    The Random Fit strategy aims to find a suitable bin for the incoming item\n    by randomly selecting from the bins that have enough remaining capacity.\n    To implement this as a priority function, we'll assign a higher priority\n    to bins that can fit the item, and a lower priority (or zero) to those that cannot.\n    Among the bins that can fit the item, we'll introduce a slight random variation\n    to simulate the \"random\" aspect of the strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Find bins that have enough capacity to fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities to zero\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # For bins that can fit the item, assign a priority.\n    # We want to simulate a random selection among these bins.\n    # A simple way to do this is to assign a random number to these bins\n    # and then potentially scale it or add it to a base value if needed\n    # for further refinement. For a pure \"random fit\" where any fitting bin\n    # is equally likely, a uniform random number is sufficient.\n    if np.any(can_fit_mask):\n        # Generate random priorities for bins that can fit the item\n        random_priorities = np.random.rand(np.sum(can_fit_mask))\n        priorities[can_fit_mask] = random_priorities\n\n    return priorities",
    "response_id": 17,
    "obj": 73.3346629437575,
    "cyclomatic_complexity": 2.0,
    "halstead": 4.754887502163469,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response18.txt_stdout.txt",
    "code_path": "problem_iter1_code18.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    The Epsilon-Greedy strategy balances exploration (trying less optimal bins)\n    and exploitation (choosing the seemingly best bin).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploration\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Identify feasible bins (bins that can fit the item)\n    feasible_bins_indices = np.where(bins_remain_cap >= item)[0]\n\n    if len(feasible_bins_indices) == 0:\n        # If no bin can fit the item, return all zeros (or handle as an error/new bin case)\n        return np.zeros(num_bins)\n\n    # Calculate exploitation scores: remaining capacity after packing the item\n    # A smaller remaining capacity is better (greedy choice for minimal wasted space)\n    exploitation_scores = bins_remain_cap[feasible_bins_indices] - item\n\n    # We want higher priority for bins that leave less remaining capacity.\n    # So, we'll invert the exploitation scores. A simple inversion might be\n    # to subtract from a large number or take the negative.\n    # Let's use the negative of the remaining capacity for simplicity.\n    # Higher (less negative) values are preferred.\n    exploitation_priorities = -exploitation_scores\n\n    # Normalize exploitation priorities to be in a comparable range, e.g., [0, 1]\n    # Handle the case where all exploitation priorities are the same to avoid division by zero.\n    min_score = np.min(exploitation_priorities)\n    max_score = np.max(exploitation_priorities)\n    if max_score == min_score:\n        normalized_exploitation_priorities = np.ones(len(exploitation_priorities)) * 0.5\n    else:\n        normalized_exploitation_priorities = (exploitation_priorities - min_score) / (max_score - min_score)\n\n    # Epsilon-Greedy logic:\n    # With probability epsilon, choose a random feasible bin (exploration).\n    # With probability 1-epsilon, choose the bin with the best exploitation priority.\n\n    # Generate random numbers for each feasible bin\n    random_values = np.random.rand(len(feasible_bins_indices))\n\n    # Decide for each bin whether to explore or exploit\n    explore_mask = random_values < epsilon\n\n    # Initialize exploration priorities (uniform across feasible bins)\n    exploration_priorities = np.ones(len(feasible_bins_indices)) * 0.5 # Arbitrary moderate exploration value\n\n    # Combine exploration and exploitation\n    combined_priorities = np.where(\n        explore_mask,\n        exploration_priorities,\n        normalized_exploitation_priorities\n    )\n\n    # Assign the combined priorities to the overall priorities array\n    priorities[feasible_bins_indices] = combined_priorities\n\n    return priorities",
    "response_id": 18,
    "obj": 4.098524132429212,
    "cyclomatic_complexity": 3.0,
    "halstead": 144.75398259382442,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    The strategy aims to pack the item into a bin that has just enough remaining\n    capacity to fit the item. This is achieved by assigning higher priorities to\n    bins where the remaining capacity is close to the item size.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate a 'fit score' for each bin: how well the item fits.\n    # A perfect fit would result in a score of 0 (remaining_cap - item = 0).\n    # Negative scores mean the item doesn't fit.\n    fit_scores = bins_remain_cap - item\n\n    # We only consider bins where the item can actually fit (fit_scores >= 0).\n    # For bins where the item doesn't fit, assign a very low score so they\n    # are unlikely to be chosen by softmax. A large negative number works.\n    # We add a small epsilon to prevent division by zero or log of zero if\n    # any fit_score happens to be exactly 0.\n    epsilon = 1e-9\n    valid_fit_scores = np.where(fit_scores >= 0, fit_scores, -np.inf)\n\n    # The core idea of Softmax-Based Fit is to use softmax on some transformed\n    # scores. We want bins with smaller 'fit_scores' (closer to zero) to have\n    # higher probabilities. A simple transformation is to take the negative\n    # of the fit scores. This makes smaller (better) fit scores into larger\n    # (more positive) values.\n    #\n    # Softmax(z_i) = exp(z_i) / sum(exp(z_j))\n    # Here, z_i = -(bins_remain_cap[i] - item)\n    #\n    # We want bins with (bins_remain_cap[i] - item) closer to 0 to have higher\n    # probability. So we want to maximize -(bins_remain_cap[i] - item).\n    # This is equivalent to minimizing bins_remain_cap[i] - item.\n    #\n    # To make the softmax output represent priorities directly, we can use:\n    # priority_i = exp(-beta * (bins_remain_cap[i] - item)) for valid bins\n    # priority_i = 0 for invalid bins.\n    # beta is a temperature parameter. A higher beta makes the distribution\n    # sharper (more decisive towards the best fit).\n\n    beta = 1.0  # Temperature parameter. Can be tuned.\n\n    # Calculate exponentiated values for valid bins.\n    # We add epsilon to valid_fit_scores to ensure that even a perfect fit (0)\n    # contributes positively to the exponent, and to avoid issues with exp(0).\n    # For invalid bins, exp(-inf) is effectively 0, which is handled by np.exp.\n    transformed_scores = -beta * valid_fit_scores\n    exp_scores = np.exp(transformed_scores)\n\n    # Apply softmax. The sum of probabilities will be 1.\n    # If all valid_fit_scores were -inf (item doesn't fit anywhere),\n    # exp_scores will be all zeros, and sum will be zero. In this case,\n    # we should return all zeros, or perhaps handle it as an error/special case.\n    # However, for typical BPP scenarios, at least one bin will fit.\n    sum_exp_scores = np.sum(exp_scores)\n\n    if sum_exp_scores == 0:\n        # This might happen if the item is larger than all bin capacities.\n        # In a real implementation, you might want to open a new bin.\n        # For this function, we return zeros as no bin is prioritized.\n        return np.zeros_like(bins_remain_cap)\n    else:\n        priorities = exp_scores / sum_exp_scores\n        return priorities",
    "response_id": 19,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 76.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response24.txt_stdout.txt",
    "code_path": "problem_iter1_code24.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    The Exact Fit First strategy prioritizes bins that can perfectly fit the item.\n    Among bins that can perfectly fit, it further prioritizes bins that will have\n    the least remaining capacity after the item is placed (Best Fit). If no bin\n    can perfectly fit the item, it falls back to a Best Fit approach for all\n    available bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can exactly fit the item\n    exact_fit_mask = (bins_remain_cap == item)\n\n    if np.any(exact_fit_mask):\n        # For exact fits, give a high priority (e.g., 1.0)\n        priorities[exact_fit_mask] = 1.0\n\n        # Among exact fits, prioritize those with the least remaining capacity\n        # after placing the item (which means the bin had capacity equal to item).\n        # This sub-priority might seem redundant with exact fit, but conceptually,\n        # it's part of the Exact Fit First idea if we consider ties. Here, we'll\n        # assign a secondary priority to make the ordering clear, but a simple 1.0\n        # is usually sufficient for exact fits themselves. For this implementation,\n        # let's assign a slightly higher score to the 'most exact' fit which is\n        # the bin that will have 0 remaining capacity.\n        # However, since we already marked them with 1.0, and the problem implies\n        # we want to pick ONE, we need a way to differentiate. A simple way is\n        # to assign a slightly higher value or a rank.\n        # Let's refine: if multiple bins *exactly* fit, we'll prioritize the one\n        # that leaves *least* remaining capacity (which is zero remaining capacity).\n        # Since all exact fits have 0 remaining capacity after placement, this means\n        # we'll give all exact fits the same high priority. To differentiate,\n        # we can add a small bonus based on how much capacity is left in *other*\n        # bins to ensure we don't pick a bin that was ALMOST an exact fit but slightly\n        # larger.\n\n        # Let's rethink the priority score assignment to ensure distinctness if needed.\n        # A good strategy is to prioritize exact fits FIRST. If multiple exact fits,\n        # we need a tie-breaker. The \"least remaining capacity\" after placing the item\n        # is the same for all exact fits (0). So, a simple approach is:\n        # 1. High score for exact fits.\n        # 2. For bins that don't exactly fit, assign a score based on how close they are\n        #    to fitting the item (Best Fit aspect). The smaller the remaining capacity\n        #    after placing the item (but still positive), the higher the priority.\n\n        # Let's try a scoring mechanism:\n        # Exact fits: highest score.\n        # Non-exact fits: score based on remaining capacity after placement, inverted.\n        # The remaining capacity *after* placement would be `bin_remain_cap - item`.\n        # We want smaller positive remaining capacities to be higher priority.\n        # So, a good heuristic is `1 / (bin_remain_cap - item + epsilon)` for valid fits.\n\n        # Let's create a new priority array to avoid modifying the original 'priorities'\n        # in a way that breaks logic.\n        scores = np.full_like(bins_remain_cap, -np.inf, dtype=float) # Initialize with very low scores\n\n        # Case 1: Exact Fits\n        can_fit_exact_mask = (bins_remain_cap == item)\n        if np.any(can_fit_exact_mask):\n            # Assign a high score for exact fits. For tie-breaking within exact fits,\n            # we can slightly vary the score. For simplicity, let's give them all\n            # the highest possible score.\n            scores[can_fit_exact_mask] = 1.0 # High priority for exact fits\n\n        # Case 2: Bins that can fit the item but not exactly\n        can_fit_mask = (bins_remain_cap > item)\n        if np.any(can_fit_mask):\n            remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n            # Prioritize bins that leave minimum remaining capacity (Best Fit)\n            # We want smaller `remaining_after_fit` to have higher scores.\n            # A common way is to use `1 / (remaining_after_fit + epsilon)`.\n            # We need to ensure these scores are lower than exact fits but higher than non-fits.\n            # Let's assign scores in a range below 1.0.\n            # Normalize remaining capacity to a small range, e.g., [0.1, 0.9]\n            # This part needs careful scaling. A simpler approach for now:\n            # Higher score for smaller `remaining_after_fit`.\n            # Let's assign a score that is a function of the remaining capacity.\n            # A large positive value for small remaining capacity.\n            # For example, map remaining capacities to scores between 0 and 0.9.\n            # Max possible remaining capacity (assuming bin size B) is B-item.\n            # Smallest is 0 (for exact fit, handled above).\n            # If max bin size is, say, 1.0 and item is 0.3, remaining capacity can be up to 0.7.\n            # A simple scoring: score = 1 - (remaining_capacity / max_possible_remaining)\n            # This would give priority to bins that are almost full.\n            # Let's map remaining capacity `r` to score `1 - r` for `r > 0`.\n            # We can also use `1 / (r + 1)` to avoid division by zero and give higher scores for smaller r.\n\n            # Let's try assigning scores based on the *remaining capacity after placement*.\n            # For bins `i` where `bins_remain_cap[i] > item`:\n            #   The capacity *after* placing the item is `bins_remain_cap[i] - item`.\n            #   We want to prioritize bins where this value is smallest.\n            #   Let's assign a priority based on this residual capacity: `score = 1.0 - (bins_remain_cap[i] - item)`\n            #   This might be problematic if capacities are very small or very large.\n            #   A more robust way is `1 / (bins_remain_cap[i] - item + epsilon)` or\n            #   a linear mapping.\n\n            # Let's simplify: give a score proportional to `1.0 - item/bins_remain_cap[i]`.\n            # This prioritizes bins that are 'more full' relative to their capacity, after fitting the item.\n            # Or, simply prioritize smaller `bins_remain_cap[i] - item`.\n            # Let's assign a score that is decreasing with `bins_remain_cap[i] - item`.\n            # For example, `max_residual_capacity - (bins_remain_cap[i] - item)`\n            # If all bins have a uniform maximum capacity `MAX_CAP`, then `max_residual_capacity` is `MAX_CAP - item`.\n            # This isn't directly available.\n\n            # Alternative: Score based on closeness to exact fit without exceeding.\n            # For bins `i` where `bins_remain_cap[i] >= item`:\n            #   Score = `bins_remain_cap[i]` itself.\n            #   This prioritizes bins that are slightly larger than the item.\n            #   This seems to capture the \"Exact Fit First, then Best Fit\" idea well.\n            #   Exact fits will have `bins_remain_cap[i] = item`.\n\n            # Let's try this approach:\n            # Priority = `bins_remain_cap[i]` for bins where `bins_remain_cap[i] >= item`.\n            # Among these, exact fits (`bins_remain_cap[i] == item`) will have the lowest priority if we use this directly.\n            # This means we need to combine: exact fit gets highest priority.\n\n            # Refined approach:\n            # 1. Score for exact fits: a very high value (e.g., 1.0 + a tie-breaker if needed).\n            # 2. Score for bins that can fit (`bins_remain_cap[i] > item`): prioritize those with smallest `bins_remain_cap[i]`.\n            #    So, score proportional to `1 / bins_remain_cap[i]`.\n            # 3. Bins that cannot fit (`bins_remain_cap[i] < item`): score of 0.\n\n            # Let's use a multi-tiered scoring:\n            # Tier 1: Exact fits (score = 2.0)\n            # Tier 2: Can fit, but not exactly (score = 1.0 - (bins_remain_cap - item)) scaled appropriately.\n            #         To ensure scores are less than 2.0, maybe scale by a factor of 0.5 or something similar.\n            #         A simple way: `1.5 - (bins_remain_cap[i] - item)` and ensure it's <= 1.5.\n            #         A better way: `1.0 + (max_capacity - bins_remain_cap[i]) / max_capacity`\n            #         Let's stick to a simple linear scaling: assign score `1 - (bins_remain_cap[i] - item)`\n\n            # Let's reconsider the core Exact Fit First strategy:\n            # 1. Find all bins where `bins_remain_cap == item`.\n            # 2. If any such bins exist, pick one of them (often the first one found or a Best Fit among them).\n            # 3. If no exact fit exists, find all bins where `bins_remain_cap > item`.\n            # 4. Among these, pick the one with the smallest `bins_remain_cap` (Best Fit).\n            # 5. If no bin can fit, this item cannot be placed.\n\n            # We need to translate this selection logic into priority scores.\n            # The bin with the highest score should be selected.\n\n            # Strategy:\n            # Assign score = 2.0 to bins where `bins_remain_cap == item`.\n            # Assign score = 1.0 - (bins_remain_cap - item) to bins where `bins_remain_cap > item`.\n            # Assign score = 0.0 to bins where `bins_remain_cap < item`.\n\n            # The range of `bins_remain_cap - item` needs consideration for scaling.\n            # If `bins_remain_cap` can be very large, `1.0 - (large_value)` becomes negative.\n            # We want scores for \"can fit\" bins to be between 0 and 1, lower than exact fits (2.0).\n            # Best Fit means smallest positive `bins_remain_cap - item`.\n            # So, score should be inversely related to `bins_remain_cap - item`.\n            # Let `residual = bins_remain_cap - item`. For `residual > 0`.\n            # Score = `1.0 / (residual + epsilon)`. Need to scale this to be < 2.0.\n\n            # Let's try:\n            # Priority = 2.0 for exact fits.\n            # Priority = 1.0 + (MAX_CAP - bins_remain_cap) / MAX_CAP for bins that can fit but not exactly.\n            # MAX_CAP is not known. Assume a large upper bound or use the maximum of existing bin capacities as a reference.\n            # Or simply `1.0 / (bins_remain_cap - item)` for bins that can fit.\n\n            # Let's simplify the priority assignment. We want to make sure exact fits are chosen.\n            # If multiple exact fits, the specific heuristic would matter (e.g., which *exact* fit).\n            # For Exact Fit First, any exact fit is preferred over any non-exact fit.\n\n            # Let's make scores clearly hierarchical:\n            # Score = 2 for exact fits.\n            # Score = 1 for bins that can fit, prioritized by smallest capacity.\n            # Score = 0 for bins that cannot fit.\n\n            # Implementation:\n            # Initialize all priorities to 0.\n            priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n            # Identify bins that can fit the item (including exact fits).\n            can_fit_mask = (bins_remain_cap >= item)\n            fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n\n            # If there are bins that can fit:\n            if np.any(can_fit_mask):\n                # Separate exact fits and near fits.\n                exact_fit_mask_local = (fitting_bins_capacities == item)\n                near_fit_mask_local = (fitting_bins_capacities > item)\n\n                # Assign scores to the original array indices corresponding to these bins.\n                # High priority for exact fits.\n                exact_fit_indices = np.where(can_fit_mask)[0][exact_fit_mask_local]\n                priorities[exact_fit_indices] = 2.0\n\n                # For bins that can fit but not exactly, use Best Fit principle.\n                # The lower the `bins_remain_cap`, the higher the priority among these.\n                # Assign a score based on the remaining capacity after fitting the item.\n                # Smaller `bins_remain_cap - item` should yield higher priority.\n                # We want scores between 1.0 and 2.0 (but strictly less than 2.0).\n                # Let's use a score of `1.0 + (max_capacity_minus_item - (bin_cap - item)) / max_capacity_minus_item`\n                # This normalizes the residual capacity.\n                # If no exact fits, max remaining capacity is `max(fitting_bins_capacities)`.\n                # A simpler score: `1.0 - (bins_remain_cap[i] - item) / SOME_MAX_POSITIVE_DIFFERENCE`.\n                # Let's try `1.0 - (bins_remain_cap[i] - item) * SCALING_FACTOR` where SCALING_FACTOR is small.\n\n                # A robust way is to assign priority based on the rank.\n                # For bins that can fit:\n                # If exact fits exist, they get the highest rank.\n                # If no exact fits, the one with smallest `bins_remain_cap` gets highest rank among these.\n\n                # Let's use this scoring scheme:\n                # Exact fits: 2.0\n                # Non-exact fits: 1.0 / (bins_remain_cap - item + 1e-9)\n                # This will give very high scores to near fits if the residual is tiny.\n                # We need to scale this down so it's less than 2.0.\n\n                # Let's try a simple approach:\n                # - Exact fits get priority 1.\n                # - Bins that can fit, ordered by `bins_remain_cap`: prioritize smaller `bins_remain_cap`.\n                #   Score = `1 / (bins_remain_cap[i])`.\n                # This would rank exact fits lower than bins that are almost full.\n\n                # Correct interpretation of Exact Fit First:\n                # The MOST preferred bin is one that EXACTLY fits.\n                # If multiple EXACT fits, any of them is fine, or a tie-breaker like Best Fit among exact fits.\n                # If NO EXACT fit, then the BEST FIT (smallest capacity > item) is preferred.\n\n                # Final Proposed Logic for Scores:\n                # 1. Bins where `bins_remain_cap == item`: Assign highest priority score (e.g., 2.0).\n                # 2. Bins where `bins_remain_cap > item`: Assign a secondary priority score.\n                #    Among these, pick the one with the smallest `bins_remain_cap`.\n                #    So, a score inversely proportional to `bins_remain_cap`. For example, `1.0 / bins_remain_cap`.\n                #    These scores should be lower than the exact fit scores.\n                # 3. Bins where `bins_remain_cap < item`: Assign lowest priority (0.0).\n\n                # Combine:\n                # For bins `i` where `bins_remain_cap[i] >= item`:\n                #   If `bins_remain_cap[i] == item`: score = 2.0\n                #   If `bins_remain_cap[i] > item`: score = 1.0 / bins_remain_cap[i] (ensure scaling is appropriate)\n                #      We want smaller `bins_remain_cap` to have higher scores in this category.\n                #      So, `1.0 / bins_remain_cap[i]` works. To make them distinct from 2.0,\n                #      let's adjust the range.\n                #      If `bins_remain_cap` can be very large, `1/bins_remain_cap` will be very small.\n                #      Let's use `1.0 - (bins_remain_cap[i] - item) / MAX_POSITIVE_DIFFERENCE`.\n                #      If max possible bin capacity is `B_MAX`, then `MAX_POSITIVE_DIFFERENCE` could be `B_MAX - item`.\n\n                # Let's use a simple score assignment to achieve the ranking:\n                # Score = 2.0 for exact fits.\n                # Score = 1.0 for bins that can fit, prioritizing smaller capacities.\n                # If `bins_remain_cap` values are relatively close, `1.0 - bins_remain_cap[i]/MaxCap` could work.\n                # Or simply `1.0 / bins_remain_cap[i]`. If `bins_remain_cap[i]` ranges from 0.1 to 1.0,\n                # `1.0/0.1 = 10`, `1.0/1.0 = 1`. This is good for Best Fit.\n                # We need these scores to be < 2.0.\n\n                # Let's set a maximum possible bin capacity for scaling if not provided.\n                # Assume max possible bin capacity is `max_overall_capacity`.\n                # Let's consider the maximum remaining capacity among bins that can fit.\n                # This is still tricky without knowing the universe of capacities.\n\n                # Simpler approach: Use relative priorities.\n                # Rank 0: Exact fits.\n                # Rank 1: Best fit among non-exact fits.\n                # Rank 2: Cannot fit.\n\n                # Translate ranks to scores:\n                # Priority = 2.0 for exact fits.\n                # For bins that can fit but not exactly: Assign a score based on their ranking.\n                # The bin with the smallest `bins_remain_cap` gets the highest score in this group.\n                # Example: if the smallest fitting capacity is 0.6 and the item is 0.5, it's a 0.1 residual.\n                # Another fitting capacity is 0.8, item 0.5, residual 0.3. We prefer 0.6.\n                # Score = `1.0 - (bins_remain_cap[i] - item)`:\n                # If bins_remain_cap = [0.5, 0.6, 0.8], item = 0.5\n                # Exact fits: bin 0 (score 2.0)\n                # Non-exact fits: bin 1 (0.6), bin 2 (0.8)\n                # For bin 1: residual = 0.1. Score = 1.0 - 0.1 = 0.9\n                # For bin 2: residual = 0.3. Score = 1.0 - 0.3 = 0.7\n                # This order works: 2.0 (bin 0) > 0.9 (bin 1) > 0.7 (bin 2).\n\n                # Let's apply this logic.\n                # Prioritize bins by:\n                # 1. Exact fit: highest priority.\n                # 2. Best Fit (smallest remaining capacity > item): next highest.\n                # 3. Otherwise: 0.\n\n                scores = np.zeros_like(bins_remain_cap, dtype=float)\n\n                # Case 1: Exact fits\n                exact_fit_mask = (bins_remain_cap == item)\n                if np.any(exact_fit_mask):\n                    scores[exact_fit_mask] = 2.0 # Highest priority\n\n                # Case 2: Bins that can fit, but not exactly.\n                can_fit_mask = (bins_remain_cap > item)\n                fitting_bins_capacities_non_exact = bins_remain_cap[can_fit_mask]\n\n                if np.any(can_fit_mask):\n                    # These bins need to have a priority lower than 2.0 but higher than 0.0.\n                    # They should be prioritized by the smallest `bins_remain_cap` value.\n                    # Let's assign a score proportional to `1 / bins_remain_cap` for these.\n                    # Ensure `bins_remain_cap` are not zero, which is guaranteed by `> item`.\n                    # We need to scale this so it's less than 2.0 and greater than 0.\n                    # A common strategy is `max_score - (value - min_value) / range_of_values`.\n                    # For Best Fit, we want smaller `bins_remain_cap` to have higher scores.\n                    # Score = `k * (max_cap_among_fits - bins_remain_cap)` where k is a small constant.\n\n                    # Let's simplify the assignment for non-exact fits:\n                    # Assign score = `1.0 - (bins_remain_cap[i] - item)` as a placeholder,\n                    # and ensure it's scaled appropriately.\n                    # If `bins_remain_cap` values are diverse, a simple subtraction can lead to negative scores.\n                    # A robust way: map `bins_remain_cap` of fitting bins to [0, 1) range, with inverse order.\n                    # Let `relevant_caps = bins_remain_cap[can_fit_mask]`\n                    # If `relevant_caps` is not empty:\n                    #   `min_cap = np.min(relevant_caps)`\n                    #   `max_cap = np.max(relevant_caps)`\n                    #   If `min_cap == max_cap`: all fitting bins have the same capacity. Assign 1.0.\n                    #   Else: score = `1.0 - (bins_remain_cap[i] - min_cap) / (max_cap - min_cap)`\n                    #   This maps `min_cap` to 1.0 and `max_cap` to 0.0.\n                    #   These scores would be in [0.0, 1.0], perfectly below the 2.0 for exact fits.\n\n                    relevant_caps = bins_remain_cap[can_fit_mask]\n                    min_cap_fit = np.min(relevant_caps)\n                    max_cap_fit = np.max(relevant_caps)\n\n                    indices_non_exact_fit = np.where(can_fit_mask)[0]\n\n                    if min_cap_fit == max_cap_fit:\n                        # All bins that can fit have the same capacity. Assign a uniform priority in the range.\n                        scores[indices_non_exact_fit] = 1.0\n                    else:\n                        # Normalize scores for non-exact fits to be between 0 and 1, favoring smaller capacities.\n                        # Score = `1.0 - (value - min) / (max - min)` maps min to 1.0 and max to 0.0.\n                        normalized_scores_non_exact = 1.0 - (relevant_caps - min_cap_fit) / (max_cap_fit - min_cap_fit)\n                        scores[indices_non_exact_fit] = normalized_scores_non_exact\n\n                # Ensure that exact fits are indeed prioritized over non-exact fits.\n                # The scores are: exact fits = 2.0, non-exact fits = in [0, 1.0].\n                # This satisfies the Exact Fit First strategy.\n                # If multiple exact fits, they all get 2.0. A secondary tie-breaker might be needed if strict ordering is essential among exact fits.\n                # For simplicity, we return all exact fits with the same highest score.\n\n                return scores\n\n    # If no bin can fit the item at all, all priorities remain 0.\n    return priorities",
    "response_id": 24,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 8.0,
    "halstead": 183.47670006346175,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response26.txt_stdout.txt",
    "code_path": "problem_iter1_code26.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Sigmoid Fit Score strategy for Online Bin Packing.\n\n    This priority function assigns a higher priority to bins that are a \"good fit\"\n    for the item. A \"good fit\" is defined as a bin where the remaining capacity\n    is slightly larger than the item's size. The sigmoid function is used to\n    smoothly transition from low priority (for very large or very small remaining\n    capacities) to high priority (for near-perfect fits).\n\n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as bins_remain_cap, where each element\n        represents the priority score for placing the item in the corresponding bin.\n        Higher scores indicate a higher priority.\n    \"\"\"\n    # Avoid division by zero or very small numbers for sigmoid calculation.\n    # If a bin has enough capacity, we consider its difference from the item size.\n    # If a bin doesn't have enough capacity, its priority is effectively zero.\n    diffs = bins_remain_cap - item\n    \n    # Filter out bins that cannot fit the item\n    valid_bins_mask = diffs >= 0\n    \n    # Calculate the \"fit score\" which is ideally close to 0 (perfect fit)\n    # For bins that cannot fit the item, their fit score will be very large negative,\n    # which the sigmoid will map to close to 0.\n    fit_scores = diffs[valid_bins_mask]\n\n    # Apply a sigmoid function. We want the \"peak\" of the sigmoid to be at a\n    # remaining capacity that is slightly larger than the item size (to allow\n    # for some remaining space, which might be beneficial in some BPP variants\n    # or for future items). A common heuristic is to aim for a remaining capacity\n    # of item_size + some_small_margin, or simply item_size itself for a strict fit.\n    # For simplicity and a common \"best fit\" heuristic, we'll aim for remaining_capacity = item_size.\n    # This means we want diffs to be 0.\n    # The sigmoid function will be centered around 0 difference.\n    # To make it a priority, we want higher values for better fits.\n    # We can use a sigmoid centered at 0.\n\n    # Parameters for the sigmoid:\n    # The steepness (k) controls how quickly the priority changes around the center.\n    # A steeper sigmoid will be more selective for near-perfect fits.\n    k = 2.0  # Steepness factor. Adjust this based on desired selectivity.\n\n    # Shift the sigmoid so that a difference of 0 (perfect fit) gives a score around 0.5\n    # then we can scale and shift it to have peak priority.\n    # A common sigmoid is 1 / (1 + exp(-k*x)).\n    # We want higher priority for smaller non-negative diffs.\n    # Let's use a sigmoid where a diff of 0 gives the highest score.\n    # We can map diffs to -inf to +inf and then apply sigmoid.\n    # Let's shift the values so that the \"best fit\" (diff=0) is at the center of the sigmoid.\n    # The sigmoid f(x) = 1 / (1 + exp(-k*x)) has an inflection point at x=0.\n    # We want diff=0 to be the maximum. If we want the sigmoid to increase as diff decreases,\n    # we can use exp(k*x) or adjust the centering.\n\n    # Let's consider the \"value\" of a fit as 1/(1 + diff) if diff >= 0.\n    # This is not sigmoid.\n\n    # Using sigmoid: we want the sigmoid to peak when diff is minimal (but >= 0).\n    # If we use `sigmoid(x) = 1 / (1 + exp(-k*x))`, this increases with x.\n    # We want it to increase as `diff` decreases. So we use `sigmoid(-diff)`.\n    \n    # Calculate priorities for valid bins\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Apply sigmoid to the differences of valid bins\n    # Sigmoid(x) = 1 / (1 + exp(-k*x))\n    # We want higher priority when diff is smaller. So we can use Sigmoid(-diff)\n    # or 1 - Sigmoid(diff). Let's use 1 - Sigmoid(diff) to map diff=0 to 1.\n    # 1 - (1 / (1 + exp(-k*diff))) = exp(-k*diff) / (1 + exp(-k*diff))\n    # This is essentially a logistic function shifted.\n\n    # A simpler sigmoid mapping: scale diff to a range, then apply sigmoid.\n    # Or, use a sigmoid that's naturally decreasing or has its peak at 0.\n    # Let's use a sigmoid where the peak is at diff = 0.\n    # `priorities_for_valid = 1 / (1 + np.exp(k * fit_scores))`\n    # This maps diff=0 to 0.5, diff>0 to <0.5, diff<0 to >0.5.\n    # This isn't quite right.\n\n    # Let's try a shifted sigmoid that gives peak at diff = 0.\n    # sigmoid(x) = 1 / (1 + exp(-x)) -> increases from 0 to 1.\n    # We want a function that peaks at diff=0 and decreases.\n    # Consider a Gaussian-like shape or a bell curve. Sigmoid isn't ideal for a peak.\n\n    # Re-interpreting Sigmoid Fit Score:\n    # Often, \"sigmoid fit\" implies mapping a \"goodness\" score to a priority,\n    # where the goodness score itself might be derived.\n    # A common heuristic is the \"best fit\" rule: choose the bin with the smallest remaining capacity\n    # that is still greater than or equal to the item's size.\n    # The sigmoid fit could be a way to quantify this \"best fit\" desirability.\n\n    # Let's define \"fit quality\" for valid bins. A perfect fit has `diff = 0`.\n    # As `diff` increases, the fit quality decreases.\n    # We want a priority function that increases as fit quality increases.\n\n    # Consider `fit_score = -diff` for valid bins.\n    # Then apply sigmoid to this: `sigmoid(k * (-diff)) = 1 / (1 + exp(k * diff))`\n    # This maps `diff=0` to `0.5`, `diff>0` to `<0.5`, `diff<0` (impossible if valid_bins_mask works) to `>0.5`.\n    # This means bins with smaller remaining capacity are prioritized, which is \"best fit\".\n\n    # Let's use `priorities_for_valid = 1 / (1 + np.exp(k * fit_scores))`\n    # The output of this is between 0 and 1.\n\n    priorities[valid_bins_mask] = 1 / (1 + np.exp(k * fit_scores))\n\n    # We want to ensure that bins that cannot fit the item have zero priority.\n    # This is handled by initializing `priorities` to zeros and only updating valid ones.\n    \n    # An alternative interpretation: maximize the remaining capacity after packing,\n    # but subject to a \"fit\" criterion.\n    # This sigmoid strategy is often used to give a smoother preference than hard rules.\n\n    # Let's try another approach: aim for a target remaining capacity `T`.\n    # A perfect fit is `bins_remain_cap = item`. So `diff = 0`.\n    # The priority should be highest when `bins_remain_cap` is closest to `item`.\n    # `priority ~ 1 / (1 + |bins_remain_cap - item|)` is not sigmoid.\n\n    # Let's go back to `1 / (1 + exp(-k*x))`. We want this to be high when `diff` is small and non-negative.\n    # If we use `x = C - diff` for some constant C, and want the peak at `diff = 0`, then `x = C`.\n    # So `priority = 1 / (1 + exp(-k * (C - diff)))`.\n    # If we want peak at diff=0, then `C=0`. `priority = 1 / (1 + exp(k * diff))`.\n    # This decreases as diff increases, which is what \"best fit\" wants.\n\n    # To make the scores higher for better fits (smaller diff), we can use:\n    # `scaled_diff = diff / max_diff` (or some other scaling)\n    # `priority = 1 / (1 + np.exp(k * scaled_diff))`\n\n    # Let's refine `priority_v2` to use the interpretation:\n    # Priority is high for bins where `bins_remain_cap` is just slightly larger than `item`.\n    # We want to favor bins with `bins_remain_cap - item = 0` or very small positive values.\n    \n    # Consider the difference `bins_remain_cap - item`.\n    # For bins where this is negative, the priority is 0.\n    # For bins where this is non-negative, we apply the sigmoid.\n    # Let `x = bins_remain_cap - item`. We want a function that is high for `x` close to 0.\n    # `f(x) = 1 / (1 + exp(-k*x))` increases with `x`. We want it to decrease as `x` increases from 0.\n    # So, let's use `f(-x) = 1 / (1 + exp(k*x))`. This decreases from 1 (at x=0) towards 0.\n\n    # Apply this only to bins that can fit the item.\n    priorities_for_valid_bins = np.zeros(np.sum(valid_bins_mask))\n    \n    # Avoid potential `inf` in exp if `k*fit_scores` becomes very large negative\n    # (e.g., if `fit_scores` is negative, which it shouldn't be with `valid_bins_mask`,\n    # but numerical stability is key).\n    # `np.exp(k * fit_scores)` for positive `k` and `fit_scores` will result in values >= 1.\n    # The `1 / (1 + exp(k * fit_scores))` will be <= 0.5.\n    # The peak priority will be at `diff=0` where it's `1 / (1 + exp(0)) = 0.5`.\n\n    # To make it truly a priority, maybe we want the scores to range up to 1.\n    # We can scale the output: `2 * (1 / (1 + np.exp(k * fit_scores)))` if we want it to range up to 1.\n    # Or `10 * (1 / (1 + np.exp(k * fit_scores)))` to have a wider range.\n    \n    # Let's refine the sigmoid to map `diff=0` to a higher priority (e.g., 1) and\n    # larger diff to lower priority.\n    # Consider a Gaussian-like function for priority, centered at `diff=0`.\n    # `priority = exp(- (k * diff)**2 / 2)` for valid bins.\n    # This is not a sigmoid, but captures the \"fit\" idea well.\n\n    # If sticking strictly to a sigmoid:\n    # Use `priority = exp(k * diff)` transformed by a reciprocal or subtraction.\n    # `priority = 1 / (1 + exp(-k * (TARGET_DIFF - diff)))`\n    # If TARGET_DIFF is slightly positive, say `epsilon`.\n    # `priority = 1 / (1 + exp(-k * (epsilon - diff)))`\n    # When `diff = epsilon`, priority is `0.5`.\n    # When `diff = 0`, priority is `1 / (1 + exp(-k*epsilon))`.\n    # When `diff` is large positive, priority approaches `1 / (1 + exp(-inf)) = 1`.\n    # This prioritizes bins with *more* remaining capacity, which is not \"best fit\".\n\n    # Let's use a sigmoid where the input is `item - bins_remain_cap`.\n    # Let `x = item - bins_remain_cap`.\n    # Valid bins have `bins_remain_cap >= item`, so `x <= 0`.\n    # We want high priority when `x` is close to 0.\n    # Use `sigmoid(k * x)` where `sigmoid(y) = 1 / (1 + exp(-y))`.\n    # This increases as `x` increases. Since `x` is <= 0, it increases as `item - bins_remain_cap` increases.\n    # This means it increases as `bins_remain_cap` decreases. This is \"best fit\".\n    # The output ranges from 0 to 0.5 (since x <= 0).\n\n    # To get priorities up to 1, we can do `2 * sigmoid(k*x)` for valid bins.\n    # Or simply `sigmoid(k*x)` is fine if relative priorities matter.\n    # Let's use `1 - sigmoid(-k * (bins_remain_cap - item))` to have peak at 0.\n    # `1 - (1 / (1 + exp(k * (bins_remain_cap - item))))`\n    # Let `diff = bins_remain_cap - item`.\n    # `1 - (1 / (1 + exp(k * diff))) = exp(k * diff) / (1 + exp(k * diff))`\n    # This is `sigmoid(k*diff)`.\n    # So priority is `sigmoid(k * (bins_remain_cap - item))` for valid bins.\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Filter for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        # Calculate differences for bins that can fit\n        diffs = bins_remain_cap[can_fit_mask] - item\n        \n        # Use a sigmoid that is centered or has its steepest slope where we want peak priority.\n        # Let's aim for a \"good fit\" to have a slightly positive remaining capacity,\n        # allowing for a bit of slack. For instance, if item is 0.3, and bins have 0.31, 0.4, 0.7.\n        # We want 0.31 to be high priority.\n        \n        # Let's redefine `x = item - bins_remain_cap`. Valid bins have `x <= 0`.\n        # We want priority to increase as `x` approaches 0.\n        # `sigmoid(k * x)` where `k > 0`.\n        # `k` controls the steepness. A higher `k` makes it more sensitive to small differences.\n        k = 5.0  # Tunable parameter\n        \n        x_values = item - bins_remain_cap[can_fit_mask]\n        \n        # Ensure `x_values` are not causing overflow in `exp`.\n        # `np.exp(-k * x_values)` if `x_values` are negative. Max value of `x_values` is 0.\n        # `exp(0) = 1`. Min value of `x_values` can be `item - max_bin_capacity`.\n        # If `max_bin_capacity` is small, `x_values` can be largely positive, leading to `exp(large positive)`.\n        # Let's use `sigmoid(k*x)` directly as `1 / (1 + exp(-k*x))`.\n        # x = item - bin_cap. For valid bins, bin_cap >= item, so x <= 0.\n        # -k*x >= 0. So exp(-k*x) >= 1.\n        # 1 / (1 + exp(-k*x)) ranges from 1/(1+inf)=0 to 1/(1+1)=0.5.\n        # This means best fits (x=0) have priority 0.5, and worst fits (x large negative) have priority 0.\n        # This is still not what we want.\n\n        # The interpretation of \"Sigmoid Fit Score\" implies a conversion of a fit measure to a score.\n        # The best fit strategy: choose bin with minimum `bins_remain_cap - item` such that it's non-negative.\n        # So, we want to maximize a function that is high when `bins_remain_cap - item` is minimal and non-negative.\n\n        # Let's consider a \"fit desirability\" measure: ` desirability = - (bins_remain_cap - item)` for valid bins.\n        # This means desirability is high when `bins_remain_cap - item` is small and negative (impossible)\n        # or small and positive.\n        # Let's use `desirability = -bins_remain_cap + item`. For valid bins, this is `item - bins_remain_cap` and is <= 0.\n        # We want priority high when `item - bins_remain_cap` is close to 0.\n        # Use `sigmoid(k * desirability)`.\n        # `k * (item - bins_remain_cap)`. For valid bins, this term is <= 0.\n        # `sigmoid(y)` for `y <= 0` ranges from 0 to 0.5.\n        # Highest value is 0.5 when `y=0` (i.e., `item == bins_remain_cap`).\n\n        # To ensure a range up to 1 and that 0.5 is not the max for good fits:\n        # We can add an offset or scale.\n        # For example, `priority = 0.5 + 0.5 * sigmoid(k * (item - bins_remain_cap))`\n        # This shifts the range from [0, 0.5] to [0.5, 1].\n        # The peak priority of 1 is achieved when `item - bins_remain_cap == 0`.\n        \n        # This seems like a solid interpretation for \"Sigmoid Fit Score\" for BPP.\n\n        priorities_for_valid_bins = 0.5 + 0.5 * (1 / (1 + np.exp(-k * (item - bins_remain_cap[can_fit_mask]))))\n        priorities[can_fit_mask] = priorities_for_valid_bins\n\n    return priorities",
    "response_id": 26,
    "obj": 3.9389708815317115,
    "cyclomatic_complexity": 2.0,
    "halstead": 204.32967235008786,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response28.txt_stdout.txt",
    "code_path": "problem_iter1_code28.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using an Epsilon-Greedy strategy.\n\n    The strategy balances exploring new bins with exploiting good bins.\n    A random bin is chosen with probability epsilon, otherwise, the bin that\n    best fits the item is chosen.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        The highest score indicates the preferred bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploring a random bin\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Check if the item can fit into any of the bins\n    can_fit = bins_remain_cap >= item\n\n    # If the item can fit into any bin\n    if np.any(can_fit):\n        # Epsilon-greedy: with probability epsilon, choose a random bin it can fit into\n        if np.random.rand() < epsilon:\n            possible_bins_indices = np.where(can_fit)[0]\n            chosen_bin_index = np.random.choice(possible_bins_indices)\n            priorities[chosen_bin_index] = 1.0  # Give highest priority to the randomly chosen bin\n        else:\n            # Otherwise, exploit by choosing the bin that best fits the item (minimum remaining capacity after packing)\n            # Filter bins that can fit the item\n            valid_bins_remain_cap = bins_remain_cap[can_fit]\n            # Calculate the remaining capacity if the item is placed in each valid bin\n            remaining_capacities_after_packing = valid_bins_remain_cap - item\n            # Find the bin with the smallest remaining capacity (best fit)\n            best_fit_bin_index_relative = np.argmin(remaining_capacities_after_packing)\n            # Get the actual index in the original bins_remain_cap array\n            best_fit_bin_index_absolute = np.where(can_fit)[0][best_fit_bin_index_relative]\n            priorities[best_fit_bin_index_absolute] = 1.0 # Give highest priority to the best fit bin\n    else:\n        # If the item cannot fit into any existing bin, it needs a new bin.\n        # This heuristic might still try to distribute some \"chance\" across all bins\n        # to explore options, but in a practical scenario, a new bin would be opened.\n        # For the purpose of returning priorities for existing bins, we can assign\n        # a low or zero priority, or perhaps a small exploration value if there's\n        # a notion of \"future potential\" for a bin to be useful.\n        # Here, we'll assign a small uniform priority to indicate exploration\n        # across all bins, as no bin currently can accommodate it.\n        if num_bins > 0:\n            priorities = np.ones_like(bins_remain_cap) * (epsilon / num_bins)\n        # If there are no bins, this function won't be called in a way that makes sense\n        # for selecting an existing bin.\n\n    return priorities",
    "response_id": 28,
    "obj": 4.248105305145606,
    "cyclomatic_complexity": 4.0,
    "halstead": 70.32403072095333,
    "exec_success": true
  }
]