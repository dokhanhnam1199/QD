{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines inverse proximity for tight fits with a sigmoid for smooth preference.\n    Favors bins with minimal remaining capacity after packing, scaled smoothly.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(eligible_bins_mask):\n        eligible_capacities = bins_remain_cap[eligible_bins_mask]\n        \n        # Inverse proximity: smaller gap is better (higher score)\n        # Adding a small epsilon to avoid division by zero\n        inverse_proximity = 1.0 / (eligible_capacities - item + 1e-9)\n\n        # Normalize inverse proximity to a range where sigmoid is effective\n        # Aims to map smaller gaps (higher inverse_proximity) to values around 0.5\n        # and larger gaps to values further from 0.5.\n        # This normalization is heuristic and can be tuned.\n        if np.max(inverse_proximity) > np.min(inverse_proximity):\n            normalized_scores = (inverse_proximity - np.min(inverse_proximity)) / (np.max(inverse_proximity) - np.min(inverse_proximity))\n        else: # All eligible bins have the same inverse proximity\n            normalized_scores = np.ones_like(inverse_proximity) * 0.5\n\n        # Sigmoid function to create a smooth priority distribution\n        # The steepness parameter (e.g., 10) can be tuned.\n        # We want bins with smaller gaps (higher normalized_scores) to have higher sigmoid outputs.\n        # So, we invert the normalized_scores for the sigmoid input to favor smaller gaps.\n        sigmoid_priorities = 1 / (1 + np.exp(-10 * (normalized_scores - 0.5)))\n\n        priorities[eligible_bins_mask] = sigmoid_priorities\n        \n        # Ensure that if all eligible bins are identical in terms of fit, they get a neutral priority\n        if np.all(priorities[eligible_bins_mask] == 0.5) and len(eligible_bins_mask) > 0:\n            priorities[eligible_bins_mask] = 0.5\n\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    suitable_bins = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    if np.any(suitable_bins):\n        remaining_capacities_of_suitable_bins = bins_remain_cap[suitable_bins]\n        \n        gaps = remaining_capacities_of_suitable_bins - item\n        \n        normalized_gaps = gaps / np.max(remaining_capacities_of_suitable_bins)\n        \n        sigmoid_scores = 1 / (1 + np.exp(-10 * (normalized_gaps - 0.5)))\n        \n        priorities[suitable_bins] = sigmoid_scores\n        \n        \n        if np.all(priorities == 0):\n             priorities[suitable_bins] = 0.5\n    \n    return priorities\n\n### Analyze & experience\n- *   **Heuristic 1 vs. Heuristic 13/16:** Heuristic 1 uses a loop for calculation, while 13 and 16 achieve the same result using vectorized NumPy operations (`1 / (cap - item + 1e-9)` applied element-wise). Vectorization is generally more efficient in Python with NumPy. Heuristic 12 and 15 are similar to 13/16 but add comments and slightly different variable names. Heuristic 8 takes absolute difference which is not ideal for prioritizing tighter fits (a bin with -1 difference is worse than a bin with +1 difference, but `abs` makes them equal). Heuristic 8 also applies the `can_fit_mask` *after* calculating inverse absolute difference, which is less clean than filtering first.\n*   **Heuristic 2/6/10 vs. Heuristic 4/14/17:** Heuristics 2, 6, and 10 are identical. They use a sigmoid function applied to normalized inverse proximity scores. Heuristic 4, 14, and 17 are also identical and very similar to 2/6/10, also using sigmoid and normalization. The difference lies in how normalization is performed. 4/14/17 normalize based on `(inverse_proximity - min) / (max - min)`, aiming to center around 0.5 for the sigmoid. 2/6/10 normalize by dividing by the max inverse proximity, effectively scaling to [0, 1] where 1 is the tightest fit. The explicit handling of perfect fits (priority = 1.0) in 2/6/10/4/14/17 is a good addition for ensuring these are always prioritized. The logic in 2/6/10/4/14/17 seems more robust and nuanced than simpler inverse proximity.\n*   **Heuristic 7 vs. Heuristic 11/17:** Heuristic 7 uses a Softmax on negative differences (`-(eligible_capacities - item)`), which effectively prioritizes bins with small remaining capacity after fitting the item. This is a strong approach. Heuristics 11 and 17 are identical and also use a `temperature` parameter with `exp(-diffs / temperature)`. This is conceptually similar to Softmax but returns raw scores proportional to the softmax probabilities, which is often sufficient for selection. Heuristic 17 also includes `1.0 / valid_capacities` logic which is then commented out or seemingly superseded by the `exp(-diffs / temperature)` part. The `temperature` parameter offers a tunable knob.\n*   **Heuristic 3 vs. Heuristic 9:** Heuristic 3 uses `item - bins_remain_cap` for fitting bins, resulting in negative priorities. Higher values (closer to 0) are better fits. Heuristic 9 aims to combine tight fit (`item - bins_that_fit_cap`) with fullness (`1.0 / (bins_that_fit_cap - item + epsilon)`). This combination is more complex and potentially captures more desired behavior by rewarding both tight fits and already full bins (which might imply less residual capacity for future items). Heuristic 3 is simpler but might not exploit the \"already full\" aspect as well.\n*   **Heuristic 5 vs. Others:** Heuristic 5 uses a simple \"best fit\" approach by identifying the minimum difference and assigning a priority of 1.0 only to bins with that minimum difference, and 0.0 otherwise. This is a greedy, non-smooth approach, unlike the graded priorities offered by inverse proximity or sigmoid functions in other heuristics. It doesn't differentiate between multiple \"best fit\" bins, nor does it provide a soft preference for slightly less optimal fits.\n*   **Heuristic 18/19/20 vs. Others:** These are identical and use a sigmoid on normalized gaps. The normalization is `gaps / np.max(remaining_capacities_of_suitable_bins)`. This differs from heuristics 2/6/10/4/14/17 by normalizing based on the *maximum remaining capacity* among suitable bins, rather than the inverse proximity scores. This could lead to different shaping of the priority distribution. The fallback to `0.5` when all priorities are zero is a good fallback for uniform preference in such edge cases.\n*   **Overall Comparison:** Heuristics like 2, 4, 6, 7, 10, 11, 14, 17, 18, 19, 20 offer more sophisticated ways to generate graded priorities, often using sigmoid or exponential functions on derived scores (like inverse proximity or differences). These provide smoother exploration and can balance multiple objectives (tight fit, bin fullness). Simple inverse proximity (1, 12, 13, 15, 16) is a solid baseline. Heuristics like 3 and 9 attempt to combine factors but might be less standard. Heuristic 5 is too simplistic and binary. Heuristics 18/19/20's normalization approach is a variation worth noting. The use of `np.any` and vectorized operations is generally preferred over explicit loops.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Granular scoring, multi-criteria fusion, adaptive parameterization, empirical validation.\n*   **Advice:** Focus on creating finely-grained, multi-dimensional scoring mechanisms that integrate diverse criteria (e.g., fit, fullness, strategic placement) through weighted sums or more complex fusion methods.\n*   **Avoid:** Binary or overly simplistic preference assignments. Avoid introducing complex mathematical transformations without a clear, demonstrable benefit to heuristic performance on the specific problem.\n*   **Explanation:** By moving beyond single metrics and embracing nuanced, combined scoring, heuristics can capture a richer understanding of the problem space, leading to more intelligent and adaptive decision-making, especially when validated against real-world or simulated problem instances.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}