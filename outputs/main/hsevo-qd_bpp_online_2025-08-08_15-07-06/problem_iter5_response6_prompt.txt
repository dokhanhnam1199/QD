{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    A Softmax-based priority function for the online Bin Packing Problem.\n\n    This function calculates the priority of placing an item into each available bin.\n    It considers the remaining capacity of each bin relative to the item size.\n    Bins that can accommodate the item without exceeding their capacity are favored.\n    Among the bins that can accommodate the item, those with less remaining capacity\n    (i.e., tighter fits) are given a higher priority, encouraging fuller bins first.\n    The Softmax function is used to convert these relative preferences into a\n    probability distribution, ensuring that higher priority bins have a greater chance\n    of being selected.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the remaining\n                         capacity of a bin.\n\n    Returns:\n        A numpy array of the same shape as bins_remain_cap, where each element\n        is the priority score for placing the item into the corresponding bin.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    eligible_capacities = bins_remain_cap[eligible_bins_mask]\n\n    if eligible_capacities.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit\" score for eligible bins. A smaller remaining capacity\n    # (tighter fit) results in a higher score. We use the negative difference\n    # to make larger remaining capacities (less good fits) have smaller scores.\n    fit_scores = -(eligible_capacities - item)\n\n    # Apply Softmax to get probabilities (priorities).\n    # Adding a small epsilon to avoid log(0) issues if fit_scores can be zero.\n    epsilon = 1e-9\n    exp_scores = np.exp(fit_scores - np.max(fit_scores)) # Stability trick for softmax\n    priorities = exp_scores / np.sum(exp_scores)\n\n    # Map priorities back to the original bin structure\n    full_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    full_priorities[eligible_bins_mask] = priorities\n\n    return full_priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritizes bins by favoring those with the smallest remaining capacity that can fit the item,\n    using an inverse proximity score for refinement.\n    \"\"\"\n    # Identify bins that can accommodate the item\n    possible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(possible_bins_mask):\n        return np.zeros_like(bins_remain_cap)\n\n    # Get remaining capacities of only the possible bins\n    valid_capacities = bins_remain_cap[possible_bins_mask]\n\n    # Calculate the difference between remaining capacity and item size.\n    # Smaller difference indicates a tighter fit, which is generally preferred\n    # to minimize wasted space and encourage filling bins.\n    differences = valid_capacities - item\n\n    # Assign priority scores: higher score for smaller differences (tighter fits).\n    # Using the inverse of the difference (plus a small epsilon for stability)\n    # makes smaller differences result in larger scores.\n    # If difference is 0 (perfect fit), score becomes very high.\n    epsilon = 1e-9\n    priorities = 1.0 / (differences + epsilon)\n\n    # Create a result array initialized with zeros\n    final_priorities = np.zeros_like(bins_remain_cap)\n    # Place the calculated priorities into the correct positions corresponding to possible bins\n    final_priorities[possible_bins_mask] = priorities\n\n    return final_priorities\n\n### Analyze & experience\n- *   **Heuristic 1 vs. Heuristic 13/16:** Heuristic 1 uses a loop for calculation, while 13 and 16 achieve the same result using vectorized NumPy operations (`1 / (cap - item + 1e-9)` applied element-wise). Vectorization is generally more efficient in Python with NumPy. Heuristic 12 and 15 are similar to 13/16 but add comments and slightly different variable names. Heuristic 8 takes absolute difference which is not ideal for prioritizing tighter fits (a bin with -1 difference is worse than a bin with +1 difference, but `abs` makes them equal). Heuristic 8 also applies the `can_fit_mask` *after* calculating inverse absolute difference, which is less clean than filtering first.\n*   **Heuristic 2/6/10 vs. Heuristic 4/14/17:** Heuristics 2, 6, and 10 are identical. They use a sigmoid function applied to normalized inverse proximity scores. Heuristic 4, 14, and 17 are also identical and very similar to 2/6/10, also using sigmoid and normalization. The difference lies in how normalization is performed. 4/14/17 normalize based on `(inverse_proximity - min) / (max - min)`, aiming to center around 0.5 for the sigmoid. 2/6/10 normalize by dividing by the max inverse proximity, effectively scaling to [0, 1] where 1 is the tightest fit. The explicit handling of perfect fits (priority = 1.0) in 2/6/10/4/14/17 is a good addition for ensuring these are always prioritized. The logic in 2/6/10/4/14/17 seems more robust and nuanced than simpler inverse proximity.\n*   **Heuristic 7 vs. Heuristic 11/17:** Heuristic 7 uses a Softmax on negative differences (`-(eligible_capacities - item)`), which effectively prioritizes bins with small remaining capacity after fitting the item. This is a strong approach. Heuristics 11 and 17 are identical and also use a `temperature` parameter with `exp(-diffs / temperature)`. This is conceptually similar to Softmax but returns raw scores proportional to the softmax probabilities, which is often sufficient for selection. Heuristic 17 also includes `1.0 / valid_capacities` logic which is then commented out or seemingly superseded by the `exp(-diffs / temperature)` part. The `temperature` parameter offers a tunable knob.\n*   **Heuristic 3 vs. Heuristic 9:** Heuristic 3 uses `item - bins_remain_cap` for fitting bins, resulting in negative priorities. Higher values (closer to 0) are better fits. Heuristic 9 aims to combine tight fit (`item - bins_that_fit_cap`) with fullness (`1.0 / (bins_that_fit_cap - item + epsilon)`). This combination is more complex and potentially captures more desired behavior by rewarding both tight fits and already full bins (which might imply less residual capacity for future items). Heuristic 3 is simpler but might not exploit the \"already full\" aspect as well.\n*   **Heuristic 5 vs. Others:** Heuristic 5 uses a simple \"best fit\" approach by identifying the minimum difference and assigning a priority of 1.0 only to bins with that minimum difference, and 0.0 otherwise. This is a greedy, non-smooth approach, unlike the graded priorities offered by inverse proximity or sigmoid functions in other heuristics. It doesn't differentiate between multiple \"best fit\" bins, nor does it provide a soft preference for slightly less optimal fits.\n*   **Heuristic 18/19/20 vs. Others:** These are identical and use a sigmoid on normalized gaps. The normalization is `gaps / np.max(remaining_capacities_of_suitable_bins)`. This differs from heuristics 2/6/10/4/14/17 by normalizing based on the *maximum remaining capacity* among suitable bins, rather than the inverse proximity scores. This could lead to different shaping of the priority distribution. The fallback to `0.5` when all priorities are zero is a good fallback for uniform preference in such edge cases.\n*   **Overall Comparison:** Heuristics like 2, 4, 6, 7, 10, 11, 14, 17, 18, 19, 20 offer more sophisticated ways to generate graded priorities, often using sigmoid or exponential functions on derived scores (like inverse proximity or differences). These provide smoother exploration and can balance multiple objectives (tight fit, bin fullness). Simple inverse proximity (1, 12, 13, 15, 16) is a solid baseline. Heuristics like 3 and 9 attempt to combine factors but might be less standard. Heuristic 5 is too simplistic and binary. Heuristics 18/19/20's normalization approach is a variation worth noting. The use of `np.any` and vectorized operations is generally preferred over explicit loops.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Granular scoring, multi-criteria fusion, adaptive parameterization, empirical validation.\n*   **Advice:** Focus on creating finely-grained, multi-dimensional scoring mechanisms that integrate diverse criteria (e.g., fit, fullness, strategic placement) through weighted sums or more complex fusion methods.\n*   **Avoid:** Binary or overly simplistic preference assignments. Avoid introducing complex mathematical transformations without a clear, demonstrable benefit to heuristic performance on the specific problem.\n*   **Explanation:** By moving beyond single metrics and embracing nuanced, combined scoring, heuristics can capture a richer understanding of the problem space, leading to more intelligent and adaptive decision-making, especially when validated against real-world or simulated problem instances.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}