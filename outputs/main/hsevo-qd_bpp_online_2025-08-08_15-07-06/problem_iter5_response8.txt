```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines Best Fit (minimizing remaining capacity after packing)
    with a penalty for bins that are excessively large, using an exponential scaling.
    """
    fit_mask = bins_remain_cap >= item
    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)
    
    epsilon = 1e-9
    
    if np.any(fit_mask):
        eligible_capacities = bins_remain_cap[fit_mask]
        
        # Calculate the "gap" or remaining capacity after placing the item
        gaps = eligible_capacities - item
        
        # Prioritize bins with smaller gaps (tighter fit).
        # Use inverse of gap + epsilon for a higher score for smaller gaps.
        # Also, apply a penalty for very large gaps by considering the inverse of capacity itself.
        # A simple way to combine is to favor smaller gaps and penalize large capacities.
        # Let's use the exponential of the negative gap to directly map smaller gaps to higher scores,
        # similar to Softmax inputs, and tune with temperature.
        
        # Temperature parameter to control the "aggressiveness" of the heuristic.
        # Lower temperature means stronger preference for the best fit.
        temperature = 0.5 
        
        # Score is based on negative gap (higher score for smaller gap)
        # Adding a small bonus for already fuller bins (lower eligible_capacities)
        # A combined score could be: -gap - (1/eligible_capacity)
        # For exponential scaling, let's use -gap directly, and the exponential will handle the grading.
        # We can further adjust by considering the inverse of capacity.
        # A common approach is to use `exp(-gap / T)`.
        # To also favor fuller bins, we can add a term proportional to `1/capacity`.
        # Let's try: `exp(-(gap - C * (1.0/eligible_capacity)) / T)` where C is a weight.
        # For simplicity, let's focus on the gap primarily and use the exponential.
        # The prompt also mentions penalizing very large remaining capacities.
        # Let's try `exp(-gap)` and see how it behaves.
        # To incorporate "fullness preference": maybe `exp(-(gap - alpha * (1/eligible_capacity)))`
        
        # A robust combination often seen is maximizing `-gap` and `1/capacity`.
        # Let's try `exp( (-gaps - 1.0/eligible_capacities) / temperature )`
        # This prioritizes small gaps and small capacities.
        
        # Simplified approach: Use negative gap scaled by temperature.
        # This prioritizes "best fit" strongly.
        scaled_scores = -gaps / temperature
        
        # Another approach from literature often used with Softmax: maximize `-(gap - C * (1/capacity))`
        # Let's try `exp( -(gaps - 1.0/eligible_capacities) / temperature )`
        # Where `1.0/eligible_capacities` gives higher score to fuller bins.
        
        # Let's select a more established approach: prioritize bins with minimal remaining capacity after packing (Best Fit),
        # and use exponential scaling for graded priorities.
        # This directly aligns with minimizing bin count.
        
        # Calculate priority based on the negative gap, scaled by temperature.
        # Higher priority for smaller gaps (tighter fits).
        priorities[fit_mask] = np.exp(-gaps / temperature)
        
    return priorities
```
