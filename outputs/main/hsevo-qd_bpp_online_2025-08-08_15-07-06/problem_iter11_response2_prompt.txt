{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritizes bins based on a combination of tightest fit and overall bin fullness.\n    Uses a sigmoid function to provide graded priorities for suitable bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        \n        # Metric 1: Tightest fit (inverse of remaining capacity after packing)\n        # Smaller remaining capacity is better (tighter fit)\n        gaps = suitable_bins_caps - item\n        \n        # Avoid division by zero or very small numbers for bins with zero remaining capacity\n        inverse_proximity = 1.0 / (gaps + 1e-9)\n        \n        # Normalize inverse proximity to a [0, 1] range for the sigmoid\n        # Max value corresponds to the tightest fit, min to the loosest fit among suitable bins\n        min_inv_prox = np.min(inverse_proximity)\n        max_inv_prox = np.max(inverse_proximity)\n        \n        if max_inv_prox == min_inv_prox: # All suitable bins have the same tightness\n            normalized_proximity = np.ones_like(inverse_proximity) * 0.5 \n        else:\n            normalized_proximity = (inverse_proximity - min_inv_prox) / (max_inv_prox - min_inv_prox)\n            \n        # Metric 2: Fullness of the bin (inverse of remaining capacity before packing)\n        # Higher fullness is generally better to keep smaller bins for smaller items\n        fullness_scores = 1.0 / (suitable_bins_caps + 1e-9)\n        \n        # Normalize fullness scores\n        min_fullness = np.min(fullness_scores)\n        max_fullness = np.max(fullness_scores)\n        \n        if max_fullness == min_fullness: # All suitable bins have the same fullness\n            normalized_fullness = np.ones_like(fullness_scores) * 0.5\n        else:\n            normalized_fullness = (fullness_scores - min_fullness) / (max_fullness - min_fullness)\n            \n        # Combine metrics using sigmoid for graded preference\n        # We want high proximity score (tight fit) and high fullness score to have higher priority\n        # Sigmoid with a positive slope centered around 0.5 will map higher combined scores to higher priorities\n        combined_score = normalized_proximity * 0.7 + normalized_fullness * 0.3 # Weighted combination\n        \n        # Apply sigmoid to create graded priorities between 0 and 1\n        # A temperature parameter could be added here for tuning (e.g., sigmoid(k * (combined_score - 0.5)))\n        sigmoid_priorities = 1 / (1 + np.exp(-10 * (combined_score - 0.5))) \n        \n        priorities[suitable_bins_mask] = sigmoid_priorities\n        \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    priorities = np.zeros_like(bins_remain_cap)\n    for i, cap in enumerate(bins_remain_cap):\n        if cap >= item:\n            priorities[i] = 1 / (cap - item + 1e-9)\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (multi-criteria with quadratic penalty and linear fullness term) with Heuristic 8 (simple `item - bins_remain_cap`), we see that Heuristic 1 attempts a more nuanced approach by penalizing larger gaps quadratically and considering overall bin fullness, which is a richer scoring mechanism than the linear best-fit of Heuristic 8.\n\nComparing Heuristic 2 (multi-criteria fusion with weights) with Heuristic 5 (normalized multi-criteria fusion with sigmoid), Heuristic 2 uses a direct weighted sum (`item - 0.5 * bins_remain_cap`), while Heuristic 5 normalizes components and applies a sigmoid, aiming for smoother, graded priorities. Heuristic 5's normalization and sigmoid might offer more stable behavior across different input scales than Heuristic 2's direct weighting.\n\nComparing Heuristic 6 (normalized proximity and fullness with sigmoid) with Heuristic 12 (normalized inverse proximity with exponential decay), both use normalization and transformations. Heuristic 6 combines normalized proximity and fullness, whereas Heuristic 12 uses exponential decay on normalized proximity, potentially offering finer control over priority gradients.\n\nComparing Heuristic 11 (simple inverse proximity, infinite for perfect fit) with Heuristic 14 (simple inverse proximity, no special handling for perfect fit), Heuristic 11 explicitly rewards perfect fits with infinite priority, which is a stronger signal than Heuristic 14's consistent inverse proportionality.\n\nComparing Heuristic 13 (combined best-fit and excess capacity penalty via exp and multiplication) with Heuristic 18 (similar logic with `exp(-diffs/temp) * excess_capacity_scores`), both attempt to balance tight fit and penalizing large capacities. Heuristic 18's use of exponential scaling with temperature offers a potentially more controlled way to adjust the sensitivity to these factors compared to the multiplicative approach in Heuristic 13.\n\nOverall: Higher ranked heuristics tend to incorporate multiple criteria (tight fit, bin fullness) and use more sophisticated scoring mechanisms (quadratic penalties, normalization, sigmoid/exponential transformations) for finer-grained control and robustness.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Multi-criteria, Non-linear scoring, Edge cases, Adaptability, Vectorization.\n*   **Advice:** Focus on how multi-criteria scoring can be made *adaptive* through tunable parameters (like temperature) and robust to edge cases.\n*   **Avoid:** Overly simplistic heuristics that fail to capture nuanced trade-offs or are brittle to variations.\n*   **Explanation:** Aim for heuristics that are both computationally efficient (vectorized) and capable of nuanced, adaptable decision-making by leveraging non-linear transformations and explicitly managing extreme scenarios.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}