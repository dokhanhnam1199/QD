```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines tight-fitting preference with a penalty for excess capacity using an exponential scaling.
    Favors bins with minimal remaining capacity after placement, while penalizing bins with much larger capacities.
    """
    eligible_bins_mask = bins_remain_cap >= item
    
    if not np.any(eligible_bins_mask):
        return np.zeros_like(bins_remain_cap)

    eligible_capacities = bins_remain_cap[eligible_bins_mask]
    
    # Calculate the "tightness" score: smaller difference is better.
    # We want to maximize this, so we use the negative difference.
    # A small epsilon is added to prevent division by zero if using inverse difference.
    # Here, we directly use the negative difference for exponential scaling.
    differences = eligible_capacities - item
    tightness_scores = -differences

    # Calculate an "excess capacity" score. We want to penalize bins with significantly
    # more capacity than needed. A simple approach is to relate this to the remaining capacity itself.
    # Bins with very large remaining capacity should have lower priority.
    # We can use a transformation that decreases as capacity increases.
    # Let's use a scaled version of the inverse remaining capacity, or a negative capacity term.
    # For a robust penalty, we can consider (capacity / item) and penalize larger ratios.
    # A simpler approach: use a term that penalizes larger remaining capacities.
    # Let's use a "capacity penalty" that is higher for larger capacities.
    # A linear penalty or inverse penalty might be too simple.
    # Let's try `-(eligible_capacities / item)` to get larger negative values for larger capacities.
    # Or, more directly, let's penalize bins that have significantly more space than the item needs.
    # Consider bins that have more than `item * (1 + tolerance)`.
    # A simple penalty could be `-(eligible_capacities / item)`.
    # We want to combine `tightness_scores` with `excess_capacity_penalty`.
    # Higher `tightness_scores` are good, higher `excess_capacity_penalty` (more negative) are bad.

    # Let's refine the excess capacity penalty to be more sensitive to large gaps.
    # We want to penalize bins where `eligible_capacities` is much larger than `item`.
    # A penalty that grows with `eligible_capacities - item` can be useful.
    # Consider `-(eligible_capacities - item)`. This is the same as `tightness_scores`.
    # We need a different term.
    # How about a term that is small for small capacities and grows?
    # Let's use a scaled negative capacity: `-(eligible_capacities / item)`.
    # The goal is to have a combined score where smaller differences are good, and large capacities are also somewhat penalized.

    # Let's combine the best-fit (tightness) with a penalty on large remaining capacities.
    # A common pattern for combining preferences where one is maximized and another is minimized
    # is to use products or sums with appropriate signs.
    # Using `exp(-diff)` for best fit is good. For excess capacity, we want smaller values to be better.
    # So we might use `exp(-capacity / large_constant)` or similar.

    # Let's re-evaluate the "excess capacity penalty".
    # We want to penalize bins that are "too empty".
    # If a bin has capacity `C` and item `I`, the "waste" is `C - I`. We want to minimize this waste.
    # `tightness_scores` is `- (C - I)`.
    # For excess capacity, we're concerned when `C` is very large relative to `I`.
    # Let's consider `C / I`. We want to penalize large `C / I`.
    # So, a penalty term could be `-(C / I)`.

    # Combine `tightness_scores` and `excess_capacity_penalty`.
    # Let's use `tightness_scores` as the primary driver for "best fit",
    # and add a penalty that reduces the score for bins with large remaining capacities.
    # A simple additive penalty: `tightness_scores - some_function(eligible_capacities)`
    # where `some_function` is increasing.

    # Let's use a simplified combination inspired by best-fit and penalizing large empty bins.
    # The `tightness_scores` already capture the "best fit" by favoring smaller `C - I`.
    # To penalize large capacities, we can add a term that decreases with `C`.
    # Or, we can make the "best fit" score more pronounced for smaller capacities.

    # Let's try to create a score where higher is better, prioritizing tight fits,
    # and among equally tight fits, prefer smaller bins.
    # The `tightness_scores = -(eligible_capacities - item)` already prioritizes tighter fits.
    # To break ties or add a secondary preference for smaller bins, we can add a term proportional to `-eligible_capacities`.
    # Combined score: `-(eligible_capacities - item) - alpha * eligible_capacities`
    # where `alpha` is a small positive constant.

    # Let's simplify and use an exponential approach to create graded priorities.
    # We'll use the `tightness_scores` (negative difference) and scale them.
    # For the excess capacity penalty, we can think about it inversely:
    # Bins that are *almost full* are good, and bins that are *very empty* are bad.
    # `tightness_scores` handles the "almost full" part well.
    # For "very empty" bins, their `eligible_capacities` will be large.

    # Let's combine the logic from priority_v0 (Softmax on differences) and consider
    # penalizing large remaining capacities explicitly.
    # priority_v0 favors tight fits. Let's augment it.
    
    # Using negative differences as scores for exponentiation (higher score for smaller difference).
    # `scores = -(eligible_capacities - item)`
    
    # To penalize large remaining capacities, we can subtract a term proportional to capacity.
    # `adjusted_scores = scores - alpha * eligible_capacities`
    # A typical value for `alpha` might be small, e.g., 0.01 or related to the item size.
    # Let's use `alpha = 0.1` as a starting point, or `alpha = 0.05` to be more gentle.
    
    alpha = 0.05 # Tunable parameter to control the penalty for excess capacity.
    
    combined_scores = tightness_scores - alpha * eligible_capacities
    
    # Apply Softmax-like transformation (exponentiate) to create graded priorities.
    # Use stability trick: subtract max score before exponentiating.
    if combined_scores.size > 0:
        max_score = np.max(combined_scores)
        exp_scores = np.exp(combined_scores - max_score)
        priorities = exp_scores / np.sum(exp_scores)
    else:
        priorities = np.array([])

    # Map priorities back to the original bin structure
    full_priorities = np.zeros_like(bins_remain_cap, dtype=float)
    full_priorities[eligible_bins_mask] = priorities

    return full_priorities
```
