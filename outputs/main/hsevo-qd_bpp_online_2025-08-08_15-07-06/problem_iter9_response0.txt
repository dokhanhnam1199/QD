```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:

    fit_mask = bins_remain_cap >= item
    
    priorities = np.full_like(bins_remain_cap, -np.inf)
    
    fitting_bins_remain_cap = bins_remain_cap[fit_mask]
    
    # Criterion 1: Minimize the remaining space (tightest fit)
    # We want to maximize -(bins_remain_cap - item), which is item - bins_remain_cap.
    # This is similar to priority_v1 but we'll use a non-linear transformation.
    # Using a logistic-like function to provide smoother penalties for larger gaps.
    # Let gap = bins_remain_cap - item. We want to maximize -(gap).
    # Consider a score like tanh(k * -(gap)) or a scaled inverse.
    # Let's try to map the gap to a score where 0 gap is best, and larger gaps are worse.
    # A simple approach is to use a negative quadratic or exponential decay.
    # Let's use a smoothed version of Best Fit: prioritize bins with small positive gap.
    # We want to maximize -(gap). A function that captures this is 1 / (1 + gap) for gap >= 0.
    # If gap is 0, score is 1. If gap is large, score approaches 0.
    # To make it higher is better, we can use a large constant minus this, or just use it as is.
    # Let's scale it to be competitive with other criteria.
    # A good score component that favors smaller gaps is `-(gap)`.
    # To make it more granular, let's use `-(gap**2)`. This penalizes larger gaps more.
    
    gap = fitting_bins_remain_cap - item
    
    # Criterion 2: Maximize the current fullness of the bin (strategic placement)
    # A fuller bin means less remaining capacity. We want to maximize (BIN_CAPACITY - bins_remain_cap).
    # If we assume a fixed BIN_CAPACITY = 1.0 (or normalize), we want to maximize (1.0 - bins_remain_cap).
    # This is equivalent to minimizing bins_remain_cap.
    # So, a score component is `-bins_remain_cap`.
    
    # Fusion: Weighted sum of criteria.
    # Let's prioritize tightness but also give a bonus to fuller bins.
    # weight_tightness: controls how much we penalize gaps. Higher means tighter fit is more important.
    # weight_fullness: controls how much we prefer fuller bins. Higher means we prefer bins that are already full.
    
    weight_tightness = 1.0
    weight_fullness = 0.5
    
    # Score component for tightness: `weight_tightness * -(gap**2)`
    # This rewards a perfect fit (gap=0) with 0, and penalizes larger gaps quadratically.
    
    # Score component for fullness: `weight_fullness * (-fitting_bins_remain_cap)`
    # This rewards bins with less remaining capacity.
    
    scores = weight_tightness * -(gap**2) + weight_fullness * (-fitting_bins_remain_cap)
    
    priorities[fit_mask] = scores
    
    return priorities
```
