```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines Best Fit (prioritizing minimal remaining capacity after packing)
    with a penalty for excessive remaining capacity using exponential scaling.
    This aims for tighter fits while discouraging overly large unused spaces.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    
    # Mask for bins that can accommodate the item
    valid_bins_mask = bins_remain_cap >= item

    if not np.any(valid_bins_mask):
        return priorities

    valid_caps = bins_remain_cap[valid_bins_mask]
    epsilon = 1e-9

    # Heuristic: Best Fit component - Prioritize bins with minimal remaining capacity after packing.
    # Smaller (remaining_cap - item) is better. We want higher scores for smaller differences.
    fit_difference = valid_caps - item
    
    # Heuristic: Penalize excessive remaining capacity.
    # Higher remaining capacity is worse. We want higher scores for smaller valid_caps.
    # We use the inverse of valid_caps.
    excess_capacity_scores = 1.0 / (valid_caps + epsilon)

    # Combine scores: Use exponential scaling on the "fit difference" to create
    # a priority score where tighter fits get exponentially higher scores.
    # A temperature parameter controls the sharpness of this preference.
    # Lower temperature = stronger preference for tightest fit.
    temperature = 0.5  # Tunable parameter

    # Calculate scores: exp(-scaled_difference) gives higher scores for smaller differences.
    # Scale the difference by temperature to control the exponential growth.
    # We use -fit_difference because smaller differences are better.
    scaled_fit_scores = np.exp(-fit_difference / temperature)

    # Combine the tight-fit preference (scaled_fit_scores) with the penalty for excess capacity.
    # Multiplication means both factors contribute positively to the priority.
    # A bin is good if it fits tightly AND doesn't have excessive remaining space.
    combined_scores = scaled_fit_scores * excess_capacity_scores

    # Assign the computed scores back to the original bins array
    priorities[valid_bins_mask] = combined_scores

    # Normalize scores to a reasonable range if necessary, though direct scores are often fine.
    # If all scores are very small, a small boost might be considered, but current combination should work.
    # Ensure no NaNs or Infs if calculations lead to them (though epsilon should prevent most)
    if np.any(np.isnan(priorities)):
        priorities[np.isnan(priorities)] = 0
    if np.any(np.isinf(priorities)):
        priorities[np.isinf(priorities)] = np.finfo(float).max # Assign a very large number

    return priorities
```
