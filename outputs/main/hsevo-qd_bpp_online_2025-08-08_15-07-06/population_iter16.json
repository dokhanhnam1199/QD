[
  {
    "stdout_filepath": "problem_iter1_response17.txt_stdout.txt",
    "code_path": "problem_iter1_code17.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    This is a heuristic for the online bin packing problem.\n    We want to prioritize bins that are \"almost full\" but can still fit the item.\n    This strategy aims to fill bins as much as possible before opening new ones.\n\n    The priority is calculated as follows:\n    1. For bins that can fit the item:\n       - Calculate how much \"space\" is left after fitting the item.\n       - We want to *minimize* this leftover space, but we also don't want to penalize bins that are already very full.\n       - A good measure might be `(bin_capacity - item) / bin_capacity` if we're talking about proportions,\n         but we have remaining capacities. So, `remaining_capacity - item`.\n       - To get a \"priority\" where higher is better, we can take the negative of this difference: `item - remaining_capacity`.\n         This means a bin with `remaining_capacity = 0.9` and `item = 0.8` gives `-0.7` (higher is better than -0.2 for a bin with `remaining_capacity = 0.3`).\n       - To avoid very large negative numbers for bins that are too small, we can set the priority to a very low number (or zero) if the item doesn't fit.\n\n    2. For bins that cannot fit the item:\n       - Assign a very low priority (e.g., 0 or negative infinity effectively, but we'll use 0).\n\n    Let's refine this:\n    We want to put the item into a bin where the remaining capacity is *just enough* or slightly more than the item.\n    If remaining_capacity >= item:\n        Priority = some_function(remaining_capacity - item)\n    Else:\n        Priority = -infinity (effectively 0 for practical purposes if others are positive)\n\n    Consider the difference: `bins_remain_cap - item`.\n    If this difference is negative, the item doesn't fit. We'll assign a very low priority.\n    If this difference is non-negative, we want to prioritize bins where this difference is *smallest* (closest to zero).\n    So, we want to maximize `-(bins_remain_cap - item) = item - bins_remain_cap`.\n    This means if a bin has `rem_cap = 1.0` and `item = 0.5`, priority is `-0.5`.\n    If a bin has `rem_cap = 0.6` and `item = 0.5`, priority is `-0.1`. The latter is higher priority.\n\n    Let's make it simpler. We want the bin where `bins_remain_cap` is *closest to `item`*, but greater than or equal to `item`.\n    This is like finding the minimum of `bins_remain_cap - item` for all `bins_remain_cap >= item`.\n    To turn this into a \"priority\" (higher is better), we can use `-abs(bins_remain_cap - item)` or `item - bins_remain_cap`.\n    Let's stick with `item - bins_remain_cap`.\n\n    Priorities will be negative. Higher values mean a better fit.\n    We need to handle the case where `bins_remain_cap < item`.\n    \"\"\"\n    \n    fit_mask = bins_remain_cap >= item\n    \n    \n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    \n    priorities[fit_mask] = item - bins_remain_cap[fit_mask]\n    \n    \n    return priorities",
    "response_id": 17,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a First Fit strategy variant.\n\n    The priority is higher for bins that can accommodate the item and have a remaining\n    capacity closer to the item's size. This encourages tighter packing.\n    Bins that cannot accommodate the item are given a priority of -1.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_remain_cap.size > 0:\n        differences = fitting_bins_remain_cap - item\n        # Higher priority for smaller differences (tighter fit)\n        # We use -differences to make smaller differences result in higher scores.\n        # Adding a small constant to avoid zero priorities for perfect fits\n        # and to ensure valid bins have a positive priority.\n        priorities[can_fit_mask] = -differences + 1.0\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response4.txt_stdout.txt",
    "code_path": "problem_iter1_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i, cap in enumerate(bins_remain_cap):\n        if cap >= item:\n            priorities[i] = 1 / (cap - item + 1e-9)\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.198244914240141,
    "SLOC": 6.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response5.txt_stdout.txt",
    "code_path": "problem_iter1_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit) strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    for i, remaining_cap in enumerate(bins_remain_cap):\n        if remaining_cap >= item:\n            \n            proximity = remaining_cap - item\n            \n            if proximity == 0:\n                priorities[i] = float('inf') \n            else:\n                priorities[i] = 1.0 / proximity\n                \n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 10.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response8.txt_stdout.txt",
    "code_path": "problem_iter8_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit (inverse proximity) with a penalty for large gaps,\n    favoring bins that minimize waste but are not excessively large.\n    \"\"\"\n    \n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        \n        \n        differences = suitable_bins_caps - item\n        \n        \n        # Assign high priority for tight fits (small differences)\n        # Use inverse proximity, adding a small epsilon to avoid division by zero\n        # and a term to penalize larger differences more heavily (e.g., quadratic)\n        # Combining 1/(diff + eps) and -(diff^2) to balance tight fit and gap penalty.\n        # Weights can be tuned; here, the inverse proximity is primary.\n        scores = (1.0 / (differences + 1e-9)) - (differences**2) \n        \n        priorities[suitable_bins_mask] = scores\n        \n        # Ensure perfect fits get the highest possible finite priority if the formula doesn't naturally yield it.\n        # This is a safeguard and usually covered by the 1/(diff+eps) term if diff is 0.\n        perfect_fit_mask = (differences == 0)\n        if np.any(perfect_fit_mask):\n            priorities[suitable_bins_mask][perfect_fit_mask] = np.finfo(float).max\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit and inverse proximity for Bin Packing priority.\n    Prioritizes bins that are closer fits, with a strong preference for the absolute best fit.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(eligible_bins_mask):\n        eligible_capacities = bins_remain_cap[eligible_bins_mask]\n        differences = eligible_capacities - item\n\n        # Strategy 1: Inverse proximity (favoring smaller remaining capacities)\n        # Add epsilon for numerical stability and to avoid division by zero.\n        inverse_proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Strategy 2: Identify the absolute best fit(s)\n        min_diff = np.min(differences)\n        best_fit_mask_local = (differences == min_diff)\n\n        # Combine strategies: Give a significantly higher priority to the absolute best fit(s)\n        # and use inverse proximity for others.\n        # We scale the best fit scores to be clearly dominant.\n        combined_scores = np.zeros_like(eligible_capacities)\n        combined_scores[best_fit_mask_local] = 100.0  # High priority for best fit\n        combined_scores[~best_fit_mask_local] = inverse_proximity_scores[~best_fit_mask_local]\n\n        # Normalize scores to be in a reasonable range, though not strictly probabilities here.\n        # Using Softmax-like scaling for non-best-fit items to maintain relative preference.\n        non_best_fit_scores = inverse_proximity_scores[~best_fit_mask_local]\n        if non_best_fit_scores.size > 0:\n            exp_scores = np.exp(non_best_fit_scores - np.max(non_best_fit_scores))\n            normalized_non_best_fit = exp_scores / np.sum(exp_scores)\n            combined_scores[~best_fit_mask_local] = normalized_non_best_fit\n\n        # Assign combined scores back to the original array structure\n        priorities[eligible_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 19.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A Softmax-based priority function for the online Bin Packing Problem.\n\n    This function calculates the priority of placing an item into each available bin.\n    It considers the remaining capacity of each bin relative to the item size.\n    Bins that can accommodate the item without exceeding their capacity are favored.\n    Among the bins that can accommodate the item, those with less remaining capacity\n    (i.e., tighter fits) are given a higher priority, encouraging fuller bins first.\n    The Softmax function is used to convert these relative preferences into a\n    probability distribution, ensuring that higher priority bins have a greater chance\n    of being selected.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the remaining\n                         capacity of a bin.\n\n    Returns:\n        A numpy array of the same shape as bins_remain_cap, where each element\n        is the priority score for placing the item into the corresponding bin.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    eligible_capacities = bins_remain_cap[eligible_bins_mask]\n\n    if eligible_capacities.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit\" score for eligible bins. A smaller remaining capacity\n    # (tighter fit) results in a higher score. We use the negative difference\n    # to make larger remaining capacities (less good fits) have smaller scores.\n    fit_scores = -(eligible_capacities - item)\n\n    # Apply Softmax to get probabilities (priorities).\n    # Adding a small epsilon to avoid log(0) issues if fit_scores can be zero.\n    epsilon = 1e-9\n    exp_scores = np.exp(fit_scores - np.max(fit_scores)) # Stability trick for softmax\n    priorities = exp_scores / np.sum(exp_scores)\n\n    # Map priorities back to the original bin structure\n    full_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    full_priorities[eligible_bins_mask] = priorities\n\n    return full_priorities",
    "response_id": 19,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response11.txt_stdout.txt",
    "code_path": "problem_iter1_code11.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    if suitable_bins_caps.size > 0:\n        differences = suitable_bins_caps - item\n        min_diff = np.min(differences)\n        \n        suitable_bin_indices = np.where(suitable_bins_mask)[0]\n        \n        for i, original_index in enumerate(suitable_bin_indices):\n            if bins_remain_cap[original_index] - item == min_diff:\n                priorities[original_index] = 1.0\n            else:\n                priorities[original_index] = 0.0\n    return priorities",
    "response_id": 11,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response8.txt_stdout.txt",
    "code_path": "problem_iter5_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (minimizing remaining capacity after packing)\n    with a penalty for bins that are excessively large, using an exponential scaling.\n    \"\"\"\n    fit_mask = bins_remain_cap >= item\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    epsilon = 1e-9\n    \n    if np.any(fit_mask):\n        eligible_capacities = bins_remain_cap[fit_mask]\n        \n        # Calculate the \"gap\" or remaining capacity after placing the item\n        gaps = eligible_capacities - item\n        \n        # Prioritize bins with smaller gaps (tighter fit).\n        # Use inverse of gap + epsilon for a higher score for smaller gaps.\n        # Also, apply a penalty for very large gaps by considering the inverse of capacity itself.\n        # A simple way to combine is to favor smaller gaps and penalize large capacities.\n        # Let's use the exponential of the negative gap to directly map smaller gaps to higher scores,\n        # similar to Softmax inputs, and tune with temperature.\n        \n        # Temperature parameter to control the \"aggressiveness\" of the heuristic.\n        # Lower temperature means stronger preference for the best fit.\n        temperature = 0.5 \n        \n        # Score is based on negative gap (higher score for smaller gap)\n        # Adding a small bonus for already fuller bins (lower eligible_capacities)\n        # A combined score could be: -gap - (1/eligible_capacity)\n        # For exponential scaling, let's use -gap directly, and the exponential will handle the grading.\n        # We can further adjust by considering the inverse of capacity.\n        # A common approach is to use `exp(-gap / T)`.\n        # To also favor fuller bins, we can add a term proportional to `1/capacity`.\n        # Let's try: `exp(-(gap - C * (1.0/eligible_capacity)) / T)` where C is a weight.\n        # For simplicity, let's focus on the gap primarily and use the exponential.\n        # The prompt also mentions penalizing very large remaining capacities.\n        # Let's try `exp(-gap)` and see how it behaves.\n        # To incorporate \"fullness preference\": maybe `exp(-(gap - alpha * (1/eligible_capacity)))`\n        \n        # A robust combination often seen is maximizing `-gap` and `1/capacity`.\n        # Let's try `exp( (-gaps - 1.0/eligible_capacities) / temperature )`\n        # This prioritizes small gaps and small capacities.\n        \n        # Simplified approach: Use negative gap scaled by temperature.\n        # This prioritizes \"best fit\" strongly.\n        scaled_scores = -gaps / temperature\n        \n        # Another approach from literature often used with Softmax: maximize `-(gap - C * (1/capacity))`\n        # Let's try `exp( -(gaps - 1.0/eligible_capacities) / temperature )`\n        # Where `1.0/eligible_capacities` gives higher score to fuller bins.\n        \n        # Let's select a more established approach: prioritize bins with minimal remaining capacity after packing (Best Fit),\n        # and use exponential scaling for graded priorities.\n        # This directly aligns with minimizing bin count.\n        \n        # Calculate priority based on the negative gap, scaled by temperature.\n        # Higher priority for smaller gaps (tighter fits).\n        priorities[fit_mask] = np.exp(-gaps / temperature)\n        \n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response15.txt_stdout.txt",
    "code_path": "problem_iter1_code15.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the difference between the bin's remaining capacity and the item's size.\n    # A smaller difference means the item fits \"better\" or closer to the bin's capacity.\n    # We add a small epsilon to avoid division by zero if a bin is perfectly full or the item size is 0.\n    diffs = bins_remain_cap - item\n    priorities = 1.0 / (np.abs(diffs) + 1e-9)\n\n    # We want to prioritize bins that can actually fit the item.\n    # If an item cannot fit, its priority should be very low.\n    # We can achieve this by multiplying the inverse difference by a mask\n    # that is 1 for bins that can fit the item and 0 otherwise.\n    can_fit_mask = (bins_remain_cap >= item).astype(float)\n    priorities *= can_fit_mask\n\n    return priorities",
    "response_id": 15,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response29.txt_stdout.txt",
    "code_path": "problem_iter1_code29.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    valid_bins = bins_remain_cap >= item\n    \n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n\n    valid_capacities = bins_remain_cap[valid_bins]\n    \n    # Higher remaining capacity means lower priority (we want to fill bins)\n    # To use Softmax effectively, we want larger values to correspond to higher priority.\n    # So we can use (large_capacity - remaining_capacity) or 1/(remaining_capacity)\n    # Let's try 1/(remaining_capacity) as it penalizes bins that are already very full.\n    \n    inverted_capacities = 1.0 / valid_capacities\n    \n    # To further encourage fitting into bins with *just enough* space, \n    # we can add a term that penalizes bins with very large remaining capacity.\n    # Let's try subtracting the ratio of remaining capacity to total capacity (assuming initial bin capacity is known or can be estimated).\n    # For simplicity here, let's just use the inverse capacity.\n    \n    # Adding a small epsilon to avoid division by zero if a bin has 0 remaining capacity, though valid_bins should prevent this.\n    epsilon = 1e-9\n    scores = 1.0 / (valid_capacities + epsilon)\n    \n    # Softmax is often used to turn scores into probabilities or weights.\n    # A higher score should mean a higher probability of selection.\n    # Let's scale the scores to be positive and somewhat related to \"how well\" it fits.\n    # A common approach in fitting is to maximize the remaining capacity, \n    # but here we want to minimize the number of bins. So we prefer bins that are *almost* full.\n    # Let's try prioritizing bins where item fits snugly.\n    \n    fit_difference = valid_capacities - item\n    # We want to minimize fit_difference. To make it a priority (higher is better), we invert it.\n    # Add a small constant to avoid division by zero if fit_difference is 0.\n    priority_scores = 1.0 / (fit_difference + epsilon)\n    \n    # Apply Softmax to convert scores into probabilities (weights)\n    # We add a small penalty for bins that have much more capacity than needed to discourage very loose fits.\n    # Let's consider the \"waste\" factor. Waste = remaining_capacity - item\n    # We want to minimize waste.\n    \n    # Let's try a heuristic that favors bins that have enough capacity but not excessively more.\n    # We can try a value that increases as remaining_capacity gets closer to item.\n    \n    # Option 1: Prioritize bins that are almost full (minimum remaining capacity that fits item)\n    # We want higher scores for smaller `valid_capacities`. So `1/valid_capacities` or similar.\n    # To be more specific, we want `valid_capacities - item` to be small.\n    # So we can use `1.0 / (valid_capacities - item + epsilon)`\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bins] = 1.0 / (valid_capacities - item + epsilon)\n    \n    # Apply Softmax: exp(score) / sum(exp(scores))\n    # For just returning priority scores for selection, we can directly use the calculated scores\n    # or apply a transformation like Softmax.\n    # If we want to select *one* bin based on highest priority, we can just return the scores directly.\n    # If we want to weight bins for some probabilistic selection, Softmax is good.\n    # For this problem, simply returning the scores that indicate preference is sufficient.\n    \n    # Let's refine to prioritize bins where remaining_capacity - item is minimized.\n    # A simple approach for priority is the inverse of the difference.\n    \n    scores = np.zeros_like(bins_remain_cap)\n    scores[valid_bins] = 1.0 / (valid_capacities - item + epsilon)\n\n    # To make it more \"Softmax-like\" in spirit of distribution, \n    # we can use a sigmoid-like transformation or directly use scaled values.\n    # Let's consider a temperature parameter to control the \"sharpness\" of priorities.\n    temperature = 1.0\n    \n    # Let's make values larger for better fits.\n    # A potential issue is if all valid capacities are very large, leading to small inverse values.\n    # We need to ensure scores are somewhat comparable or scaled.\n    \n    # Consider the \"tightness of fit\" as the primary driver.\n    # Tightest fit = smallest (remaining_capacity - item).\n    # So, priority is inversely proportional to (remaining_capacity - item).\n    \n    normalized_scores = np.zeros_like(bins_remain_cap)\n    if np.any(valid_bins):\n        # Calculate scores: higher score for tighter fit\n        # We want to maximize (1 / (remaining_capacity - item))\n        # Or to avoid issues with very small differences, maybe prioritize directly by minimum remaining capacity that fits.\n        \n        # Let's try a direct mapping:\n        # A bin is \"good\" if remaining_capacity is just enough.\n        # So, priority is high when remaining_capacity is close to item.\n        \n        # Let's use `remaining_capacity` itself as a negative factor for priority\n        # and `item` as a positive factor.\n        # How about prioritizing bins with smaller remaining capacity that can still fit the item?\n        # This aligns with the First Fit Decreasing heuristic's goal of filling bins.\n        \n        # Let's map the difference `valid_capacities - item` to a priority.\n        # Smaller difference should yield higher priority.\n        \n        # Example: item = 3, capacities = [5, 7, 10]\n        # Valid capacities = [5, 7, 10]\n        # Differences = [2, 4, 7]\n        # We want to prioritize bins with difference 2, then 4, then 7.\n        # So, 1/2, 1/4, 1/7 would work.\n        \n        diffs = valid_capacities - item\n        priorities = 1.0 / (diffs + epsilon)\n        \n        # Now, to make it more \"Softmax-like\" if we were to select probabilistically,\n        # we can exponentiate and normalize. But for direct priority score, this is fine.\n        # Let's add a small value to all priorities to avoid negative exponents in a Softmax if we were to use it.\n        # And let's scale them to prevent numerical underflow or overflow with Softmax.\n        \n        # For a direct priority score where higher means better, \n        # this inverse difference works well for \"best fit\" aspect.\n        \n        # Consider what happens if multiple bins have the exact same \"best fit\" difference.\n        # The current approach would give them equal priority.\n        \n        # To incorporate the \"Softmax-Based Fit\" idea, let's interpret it as:\n        # transform the \"fitness\" of a bin (how well it fits the item) into a priority.\n        # The fitness can be related to how close `remaining_capacity` is to `item`.\n        \n        # Let's define fitness as: -(remaining_capacity - item)^2. Higher fitness for smaller squared difference.\n        # Or, more simply, as we did: 1.0 / (remaining_capacity - item + epsilon)\n        \n        # Softmax transformation of these scores to get a distribution if needed.\n        # For now, we just need the scores themselves.\n        \n        # Let's try to directly use the remaining capacity for scaling, \n        # encouraging smaller capacities that fit.\n        \n        # Prioritize bins with the smallest remaining capacity that can fit the item.\n        # So, the priority score should be higher for smaller `valid_capacities`.\n        # Let's try `1.0 / valid_capacities`.\n        \n        # Consider a case: item = 2, bins_remain_cap = [3, 5, 10]\n        # Valid bins = [3, 5, 10]\n        # Option A (inverse diff): 1/(3-2)=1, 1/(5-2)=0.33, 1/(10-2)=0.125. Prioritizes bin with 3. (Best Fit)\n        # Option B (inverse capacity): 1/3=0.33, 1/5=0.2, 1/10=0.1. Prioritizes bin with 3.\n        \n        # If the goal is \"smallest number of bins\", then fitting into a nearly full bin is good.\n        # \"Best Fit\" heuristic is good for this.\n        \n        # Let's combine the \"fit\" (difference) with the \"emptiness\" (remaining capacity).\n        # Maybe penalize very large remaining capacities, even if they fit.\n        \n        # Let's use the difference again, as it directly measures \"how much space is left after fitting\".\n        # Smaller difference is better.\n        \n        diffs = valid_capacities - item\n        \n        # Scale diffs to be more in line with Softmax inputs (e.g., range -inf to +inf for exp)\n        # A common pattern is to use `exp(value)` where larger `value` is better.\n        # We want to maximize `-(diffs)`. So `exp(-diffs)`? No, we want to maximize score for smaller diffs.\n        # Let's use `exp(-diffs)` with `temperature`.\n        \n        temperature = 0.5 # Lower temperature means stronger preference for best fit\n        scaled_diffs = -diffs / temperature\n        \n        # Apply Softmax concept: exp(score) / sum(exp(scores))\n        # We can simply return exp(scaled_diffs) as the priority, which is proportional to softmax output.\n        \n        priorities = np.exp(scaled_diffs)\n        \n    \n    final_priorities = np.zeros_like(bins_remain_cap)\n    final_priorities[valid_bins] = priorities\n    \n    return final_priorities",
    "response_id": 29,
    "tryHS": true,
    "obj": 4.198244914240141,
    "SLOC": 26.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse proximity for tight fits with a sigmoid for smooth preference.\n    Favors bins with minimal remaining capacity after packing, scaled smoothly.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(eligible_bins_mask):\n        eligible_capacities = bins_remain_cap[eligible_bins_mask]\n        \n        # Inverse proximity: smaller gap is better (higher score)\n        # Adding a small epsilon to avoid division by zero\n        inverse_proximity = 1.0 / (eligible_capacities - item + 1e-9)\n\n        # Normalize inverse proximity to a range where sigmoid is effective\n        # Aims to map smaller gaps (higher inverse_proximity) to values around 0.5\n        # and larger gaps to values further from 0.5.\n        # This normalization is heuristic and can be tuned.\n        if np.max(inverse_proximity) > np.min(inverse_proximity):\n            normalized_scores = (inverse_proximity - np.min(inverse_proximity)) / (np.max(inverse_proximity) - np.min(inverse_proximity))\n        else: # All eligible bins have the same inverse proximity\n            normalized_scores = np.ones_like(inverse_proximity) * 0.5\n\n        # Sigmoid function to create a smooth priority distribution\n        # The steepness parameter (e.g., 10) can be tuned.\n        # We want bins with smaller gaps (higher normalized_scores) to have higher sigmoid outputs.\n        # So, we invert the normalized_scores for the sigmoid input to favor smaller gaps.\n        sigmoid_priorities = 1 / (1 + np.exp(-10 * (normalized_scores - 0.5)))\n\n        priorities[eligible_bins_mask] = sigmoid_priorities\n        \n        # Ensure that if all eligible bins are identical in terms of fit, they get a neutral priority\n        if np.all(priorities[eligible_bins_mask] == 0.5) and len(eligible_bins_mask) > 0:\n            priorities[eligible_bins_mask] = 0.5\n\n\n    return priorities",
    "response_id": 3,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response6.txt_stdout.txt",
    "code_path": "problem_iter2_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins that are a tight fit using an inverse proximity measure,\n    giving infinite priority to perfect fits to encourage consolidation.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        differences = suitable_bins_caps - item\n\n        priorities[suitable_bins_mask] = 1.0 / (differences + 1e-9)\n\n        # Assign infinite priority to perfect fits\n        perfect_fit_mask = (differences == 0)\n        if np.any(perfect_fit_mask):\n            priorities[suitable_bins_mask][perfect_fit_mask] = float('inf')\n            \n    return priorities",
    "response_id": 6,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines inverse proximity with a sigmoid for smoother prioritization.\n\n    Favors bins with tight fits, but also provides non-zero priority for\n    less tight fits to encourage exploration.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        \n        # Inverse proximity for tight fits (similar to priority_v0)\n        proximity = suitable_bins_caps - item\n        \n        # Use a scaled sigmoid on proximity to create a smoother distribution\n        # Scaling factor `alpha` controls steepness. Higher alpha means steeper curve.\n        alpha = 10.0 \n        # Add a small epsilon to avoid division by zero for perfect fits\n        inverse_proximity_scores = 1.0 / (proximity + 1e-9)\n        \n        # Normalize inverse proximity scores to be between 0 and 1\n        if np.max(inverse_proximity_scores) > 0:\n            normalized_inverse_proximity = inverse_proximity_scores / np.max(inverse_proximity_scores)\n        else:\n            normalized_inverse_proximity = np.zeros_like(inverse_proximity_scores)\n\n        # Sigmoid transformation to map scores to a [0, 1] range, emphasizing tighter fits\n        # Adjusting the sigmoid's center and steepness can tune behavior.\n        # Here, we center it around a value that would correspond to a \"good\" proximity.\n        # For simplicity, we'll use a sigmoid on the normalized inverse proximity.\n        # A higher score from inverse proximity should map to a higher sigmoid output.\n        sigmoid_scores = 1 / (1 + np.exp(-alpha * (normalized_inverse_proximity - 0.5))) # Adjusted sigmoid\n\n        priorities[suitable_bins_mask] = sigmoid_scores\n        \n        # Ensure perfect fits still get a high priority, potentially capped by sigmoid\n        perfect_fit_mask = (proximity == 0)\n        if np.any(perfect_fit_mask):\n            priorities[suitable_bins_mask][perfect_fit_mask] = 1.0 # Assign max priority for perfect fit\n\n    return priorities",
    "response_id": 9,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response4.txt_stdout.txt",
    "code_path": "problem_iter15_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    This heuristic aims to provide a more sophisticated scoring mechanism than v1\n    by incorporating multiple criteria with non-linear weighting and a tunable penalty\n    for wasted space. It leverages a multi-criteria fusion approach using soft penalties\n    and rewards to create a nuanced priority score.\n\n    The strategy is to prioritize bins based on:\n    1.  How well the item fits the remaining capacity (tightness), with a preference for\n        minimal positive slack, but still accepting perfect fits. A quadratic penalty\n        is used for the gap to ensure diminishing returns for larger gaps.\n    2.  The current fullness of the bin, as a secondary criterion, to encourage\n        using generally fuller bins.\n    3.  A 'graceful degradation' penalty for bins that have a significant amount\n        of remaining capacity, even if they fit the item. This aims to avoid filling\n        bins unnecessarily when a tighter fit is available.\n\n    Scoring logic for bins that can fit the item (`bins_remain_cap >= item`):\n    -   **Tightness Score (Primary):** We want to minimize `bins_remain_cap - item`.\n        To convert this minimization to a maximization problem for the priority score,\n        we can use `-(bins_remain_cap - item)`. To make it more granular and less\n        sensitive to very small gaps, we can square this term. For best fit,\n        `bins_remain_cap - item` should be close to zero. We want to maximize the\n        score from this component. Let's use `-(bins_remain_cap - item)**2`. This\n        term is maximized when `bins_remain_cap - item = 0`.\n\n    -   **Fullness Score (Secondary):** To encourage using generally fuller bins,\n        we can add a term proportional to `bins_remain_cap`. A higher `bins_remain_cap`\n        means the bin was fuller to begin with. Let this be `alpha * bins_remain_cap`.\n\n    -   **Wasted Space Penalty (Tertiary):** We want to penalize leaving *too much*\n        excess space, but in a nuanced way. If `bins_remain_cap` is much larger than\n        `item`, it might be a suboptimal choice. We can introduce a term that becomes\n        more negative as `bins_remain_cap` increases beyond a certain point, relative\n        to `item`.\n        Consider the term `-(bins_remain_cap - item) * (bins_remain_cap / BIN_CAPACITY)`\n        where `BIN_CAPACITY` is the maximum bin capacity. This penalizes large remaining\n        capacities, scaled by how \"full\" the bin is. A simpler approach could be to\n        penalize based on `bins_remain_cap` itself, but we already do that in the\n        fullness score.\n\n        Let's try a simpler penalty: If a bin has `bins_remain_cap` significantly larger\n        than `item`, we want to reduce its priority. The \"waste\" is `bins_remain_cap - item`.\n        A smooth penalty could be something like `-(bins_remain_cap - item)**3` if we want\n        to heavily penalize large wastes, or `-(bins_remain_cap - item)` for a linear penalty.\n        However, `priority_v1` already covers the linear penalty.\n\n        Let's focus on making the \"tightness\" score more useful.\n        Consider a function that rewards small positive gaps and perfect fits, and penalizes\n        negative gaps (non-fits) and large positive gaps.\n        A Gaussian-like function centered at 0 for the gap `bins_remain_cap - item` could work,\n        but it's complex to vectorize.\n\n        Let's refine the combination:\n        We want to maximize `-(bins_remain_cap - item)**2` (tightness).\n        We want to maximize `alpha * bins_remain_cap` (current fullness).\n\n        Combining these: `score = -(bins_remain_cap - item)**2 + alpha * bins_remain_cap`\n        This formulation from the previous thought process seems promising. It prioritizes\n        minimal positive slack and perfect fits, and then uses current bin fullness as\n        a tie-breaker.\n\n        Let's introduce a tunable parameter `beta` to control the sensitivity to the gap.\n        `score = -beta * (bins_remain_cap - item)**2 + alpha * bins_remain_cap`\n\n        Consider the objective: we want to select bins that are not overly empty.\n        If a bin has `bins_remain_cap` much larger than `item`, it's not ideal.\n        Let's introduce a term that penalizes `bins_remain_cap` relative to `item`.\n        A penalty proportional to `bins_remain_cap / (item + epsilon)` could be used,\n        but it's not smooth.\n\n        Alternative approach: Sigmoid-like function.\n        Let `gap = bins_remain_cap - item`. We want to maximize `f(gap)`.\n        We want `f(0)` to be high. `f(small_positive)` to be high. `f(large_positive)` to be lower.\n        We want `f(negative)` to be very low.\n\n        Let's stick to the weighted sum of smooth components.\n        Component 1: Tightness, `-(bins_remain_cap - item)**2`. Max at `gap=0`.\n        Component 2: Fullness, `alpha * bins_remain_cap`. Max at max `bins_remain_cap`.\n\n        We need to combine these thoughtfully. The problem with the previous combination\n        `-(bins_remain_cap - item)**2 + alpha * bins_remain_cap` is that it can\n        favor bins with very large `bins_remain_cap` if `alpha` is sufficiently high,\n        which might contradict the goal of minimizing bins.\n\n        Let's re-evaluate the criteria:\n        1.  **Minimal Waste:** Minimize `bins_remain_cap - item`. This is best achieved by maximizing `-(bins_remain_cap - item)`.\n        2.  **Maximum Current Fill:** Maximize `bins_remain_cap`.\n\n        The conflict arises when a bin that is currently very full (high `bins_remain_cap`)\n        also has a large gap.\n\n        Consider a score that rewards bins for being \"efficiently utilized\".\n        Efficiency could be related to `item / (item + gap)`.\n        This is `item / bins_remain_cap`.\n\n        Let's try a score that is high when `bins_remain_cap` is slightly larger than `item`,\n        and decreases smoothly as `bins_remain_cap` increases further.\n\n        A score component could be `exp(-gamma * (bins_remain_cap - item))`, where `gamma > 0`.\n        This function is maximized at `bins_remain_cap = item`.\n        `score = exp(-gamma * (bins_remain_cap - item)) + alpha * bins_remain_cap`\n\n        Let `gamma = 10` and `alpha = 0.5`.\n        `item = 0.5`\n        Bin X: `bins_remain_cap = 0.5`. Gap = 0. Score = `exp(0) + 0.5 * 0.5 = 1 + 0.25 = 1.25`.\n        Bin Y: `bins_remain_cap = 0.55`. Gap = 0.05. Score = `exp(-10 * 0.05) + 0.5 * 0.55 = exp(-0.5) + 0.275 \u2248 0.6065 + 0.275 = 0.8815`.\n        Bin Z: `bins_remain_cap = 0.8`. Gap = 0.3. Score = `exp(-10 * 0.3) + 0.5 * 0.8 = exp(-3) + 0.4 \u2248 0.0498 + 0.4 = 0.4498`.\n        Bin A: `bins_remain_cap = 0.4` (does not fit). Score = `-inf`.\n\n        In this case, Bin X (perfect fit) gets the highest score. This is similar to `priority_v1`.\n\n        We want to improve by being more \"adaptive\" or \"nuanced\".\n\n        Let's introduce a penalty for being \"too empty\" but also a penalty for being \"too full\".\n        This suggests a multi-modal or more complex scoring function.\n\n        Consider a score that is a weighted sum of:\n        1.  **Tightness Score:** Penalize the gap `bins_remain_cap - item`. Maximize `-(bins_remain_cap - item)`. This is `priority_v1`.\n        2.  **Fullness Score:** Reward higher `bins_remain_cap`. Maximize `bins_remain_cap`.\n\n        Let's try to modulate the tightness score based on overall fullness.\n        If a bin is very full (`bins_remain_cap` is high), even a tight fit might leave a lot of *absolute* space.\n        If a bin is less full (`bins_remain_cap` is low, but still fits), a tight fit might be more valuable.\n\n        Let `gap = bins_remain_cap - item`.\n        Let `current_fill_ratio = (BIN_CAPACITY - bins_remain_cap) / BIN_CAPACITY`. (Assume BIN_CAPACITY = 1 for simplicity)\n        `current_fill_ratio = 1 - bins_remain_cap`.\n\n        We want to maximize `-(gap)` and maximize `current_fill_ratio`.\n        If we combine them: `-(gap) + alpha * current_fill_ratio`\n        `-(bins_remain_cap - item) + alpha * (1 - bins_remain_cap)`\n        `item - bins_remain_cap + alpha - alpha * bins_remain_cap`\n        `item + alpha - (1 + alpha) * bins_remain_cap`\n\n        Let `alpha = 0.5`.\n        `score = item + 0.5 - 1.5 * bins_remain_cap`.\n        This score is maximized when `bins_remain_cap` is minimized.\n        So, among fitting bins, it picks the one with the smallest `bins_remain_cap`.\n\n        `item = 0.5`\n        Bin X: `bins_remain_cap = 0.5`. Score = `0.5 + 0.5 - 1.5 * 0.5 = 1 - 0.75 = 0.25`. (Perfect fit)\n        Bin Y: `bins_remain_cap = 0.55`. Score = `0.5 + 0.5 - 1.5 * 0.55 = 1 - 0.825 = 0.175`.\n        Bin Z: `bins_remain_cap = 0.8`. Score = `0.5 + 0.5 - 1.5 * 0.8 = 1 - 1.2 = -0.2`.\n\n        This heuristic favors the tightest fit (Bin X), and then the next tightest (Bin Y).\n        This is similar to Best Fit.\n\n        To be *better*, we should aim for something that captures more nuance.\n\n        Let's introduce a penalty for \"wasted space\", but a smooth, tunable one.\n        Consider `wasted_space = bins_remain_cap - item`.\n        We want to minimize `wasted_space`.\n        A component could be `exp(-k * wasted_space)` for `k > 0`. This is maximized at `wasted_space = 0`.\n        Let's add `alpha * (1 - bins_remain_cap)` for current fullness.\n\n        `score = exp(-k * (bins_remain_cap - item)) + alpha * (1 - bins_remain_cap)`\n        This is equivalent to `exp(-k*gap) + alpha*(1 - (item+gap))`.\n        To maximize, we want `gap` small and `bins_remain_cap` small.\n\n        Let `k = 10`, `alpha = 0.5`.\n        `item = 0.5`\n        Bin X: `bins_remain_cap = 0.5`. Gap = 0. Score = `exp(0) + 0.5 * (1 - 0.5) = 1 + 0.5 * 0.5 = 1 + 0.25 = 1.25`.\n        Bin Y: `bins_remain_cap = 0.55`. Gap = 0.05. Score = `exp(-10 * 0.05) + 0.5 * (1 - 0.55) = exp(-0.5) + 0.5 * 0.45 \u2248 0.6065 + 0.225 = 0.8315`.\n        Bin Z: `bins_remain_cap = 0.8`. Gap = 0.3. Score = `exp(-10 * 0.3) + 0.5 * (1 - 0.8) = exp(-3) + 0.5 * 0.2 \u2248 0.0498 + 0.1 = 0.1498`.\n\n        This still heavily favors the tightest fit.\n\n        Let's try to reward bins that are \"close\" to fitting, even if not perfectly, and are not \"too empty\".\n        We can use a smooth function that peaks when `bins_remain_cap` is slightly above `item`,\n        and then decreases.\n\n        Consider a score that emphasizes the *ratio* of the item to the bin's capacity.\n        `score = (item / bins_remain_cap) * (1 - (bins_remain_cap - item))`\n        This is not ideal due to division and potential for zero.\n\n        A better approach could be to combine a few factors with tunable weights.\n        1.  **Tightness Metric:** `-(bins_remain_cap - item)` (Maximize for small positive slack)\n        2.  **Fillness Metric:** `bins_remain_cap` (Maximize for fuller bins)\n\n        Let's try a composite score using a tunable parameter to smooth the transition and prioritize different aspects.\n\n        **Key idea:** Create a score that is high for bins where `bins_remain_cap` is slightly larger than `item`, and also considers the overall fullness. We want to avoid bins that are excessively empty.\n\n        Let `fit_score = item / (bins_remain_cap + epsilon)` for fitting bins, where epsilon is a small constant to avoid division by zero. This rewards smaller gaps.\n        Let `fullness_score = bins_remain_cap`.\n\n        Combining: `score = w1 * fit_score + w2 * fullness_score`\n        This still might favor very full bins.\n\n        Let's consider a \"graceful degradation\" approach.\n        For bins that fit:\n        -   Primary objective: Minimize `bins_remain_cap - item`.\n        -   Secondary objective: Maximize `bins_remain_cap`.\n\n        We can achieve this by creating a score that is high when `bins_remain_cap` is close to `item`, and then tapers off as `bins_remain_cap` increases. Additionally, if two bins have similar \"closeness\", we prefer the one that is more full.\n\n        A smoothed version of `priority_v1` might be better. `priority_v1` is `item - bins_remain_cap`.\n        This is maximized when `bins_remain_cap` is smallest (for fitting bins).\n\n        Let's try a score that penalizes the *excess capacity* `bins_remain_cap - item` using a non-linear function, and adds a bonus for the overall fullness `bins_remain_cap`.\n\n        Consider the function: `f(x) = x` for `x <= 0` and `f(x) = exp(-k*x)` for `x > 0`.\n        Let `x = bins_remain_cap - item`.\n        For fitting bins (`x >= 0`): `score_part1 = exp(-k * (bins_remain_cap - item))`.\n        This is maximized when `bins_remain_cap - item = 0`.\n\n        Let's try to create a score that is high for bins that are \"just right\" - not too much space left, but not overly full either.\n\n        **The core idea:** Use a blend of \"Best Fit\" (minimizing `bins_remain_cap - item`) and \"Most Full\" (maximizing `bins_remain_cap`), but with a mechanism to prevent selecting bins that are *too* empty.\n\n        Let `gap = bins_remain_cap - item`.\n        Let `fill_ratio = 1 - bins_remain_cap` (assuming BIN_CAPACITY = 1).\n\n        We want to maximize `-(gap)` and maximize `fill_ratio`.\n        A score that captures this could be a weighted sum where we ensure that `gap` doesn't become too large.\n\n        Consider the term `-(gap)^2`, which is maximized at `gap = 0`.\n        Add `alpha * fill_ratio` for current fullness.\n        `score = -(bins_remain_cap - item)**2 + alpha * (1 - bins_remain_cap)`\n\n        Let's introduce a parameter `nu` to penalize bins that have a *large* amount of remaining capacity relative to their current fill.\n        This is like penalizing \"waste\" in a way that scales with current utilization.\n\n        For fitting bins:\n        `score = -(bins_remain_cap - item)**2 * (1 + nu * (bins_remain_cap - item))`\n        This makes the penalty for a gap increase cubically for larger gaps.\n\n        Let's try a more intuitive approach with tunable parameters.\n        We want to prioritize bins that minimize `bins_remain_cap - item`.\n        We also want to favor bins that are more full.\n\n        Score: `w1 * (item - bins_remain_cap) + w2 * bins_remain_cap`\n        This is similar to the `priority_v1` modification explored earlier.\n        `score = item - bins_remain_cap + alpha * bins_remain_cap = item - (1-alpha) * bins_remain_cap`.\n        This still favors smaller `bins_remain_cap`.\n\n        Let's try a score that is high when `bins_remain_cap` is slightly larger than `item`, and then drops off.\n        We can use a Gaussian-like function centered around `item`.\n        However, vectorizing Gaussians can be tricky.\n\n        A smooth, parameterized function:\n        For fitting bins:\n        `score = exp(-k * (bins_remain_cap - item)) * (1 + alpha * bins_remain_cap)`\n\n        Let `k = 10`, `alpha = 0.5`.\n        `item = 0.5`\n        Bin X: `bins_remain_cap = 0.5`. Score = `exp(0) * (1 + 0.5 * 0.5) = 1 * 1.25 = 1.25`.\n        Bin Y: `bins_remain_cap = 0.55`. Score = `exp(-10 * 0.05) * (1 + 0.5 * 0.55) = exp(-0.5) * (1 + 0.275) \u2248 0.6065 * 1.275 \u2248 0.7733`.\n        Bin Z: `bins_remain_cap = 0.8`. Score = `exp(-10 * 0.3) * (1 + 0.5 * 0.8) = exp(-3) * (1 + 0.4) \u2248 0.0498 * 1.4 \u2248 0.0697`.\n\n        This still favors the perfect fit. The goal is to find a heuristic that is *better*.\n        \"Better\" implies it performs better in terms of the number of bins used.\n\n        Let's consider a score that is sensitive to both small positive slack AND overall bin fullness,\n        but penalizes large slack.\n\n        Consider a score: `-(bins_remain_cap - item)**2 + alpha * (1 - bins_remain_cap)`\n        This prefers small positive gaps and currently fuller bins.\n\n        Let's add a penalty for bins that are \"too empty\".\n        This is handled by the `-inf` for non-fitting bins.\n\n        What if we introduce a parameter that adjusts the \"tightness\" preference?\n        Let `tightness_focus = 1.0`.\n\n        `score = (item - bins_remain_cap) * tightness_focus + alpha * bins_remain_cap`\n        This is `priority_v1` if `alpha=0`.\n\n        Let's try to make the priority score more \"convex\" or \"peak\" near the ideal fit.\n\n        **New Strategy:**\n        1.  **Bin Eligibility:** Only consider bins where `bins_remain_cap >= item`. Assign `-inf` to others.\n        2.  **Fit Quality:** Prioritize bins where `bins_remain_cap` is close to `item`.\n            Use a score component `-(bins_remain_cap - item)**2`. This is maximized when `bins_remain_cap == item`.\n        3.  **Bin Fullness:** As a secondary factor, reward bins that were more full initially.\n            Use a score component `alpha * bins_remain_cap`.\n        4.  **Graceful Degradation Penalty:** If `bins_remain_cap` is significantly larger than `item`, we want to penalize this.\n            This can be achieved by making the `-(bins_remain_cap - item)**2` term decay faster for larger gaps.\n            Alternatively, we can explicitly penalize large `bins_remain_cap` in a way that\n            complements the `alpha * bins_remain_cap` term.\n\n        Let's combine the first two criteria:\n        `tightness_fullness_score = -(bins_remain_cap - item)**2 + alpha * bins_remain_cap`\n\n        Now, how to introduce the \"graceful degradation\" for bins that are too empty?\n        The `-(bins_remain_cap - item)**2` term inherently penalizes large gaps.\n\n        Consider a score that is high when `bins_remain_cap` is just above `item`, and drops off smoothly,\n        and also considers overall fullness.\n\n        Let's use a Gaussian-like component for the gap, modulated by fullness.\n        `score = exp(-k * (bins_remain_cap - item)**2) * (1 + alpha * bins_remain_cap)`\n\n        Here:\n        - `exp(-k * (bins_remain_cap - item)**2)`: Peaks at `bins_remain_cap == item`, drops off quadratically.\n          `k` controls the width of the peak. Larger `k` means tighter preference.\n        - `(1 + alpha * bins_remain_cap)`: This term boosts the score for fuller bins.\n          `alpha` controls the weight of this boost.\n\n        Let's choose parameters:\n        `k = 100` (high sensitivity to tightness)\n        `alpha = 0.5` (moderate boost for fullness)\n\n        `item = 0.5`\n        Bin X: `bins_remain_cap = 0.5`. Gap = 0.\n            Score = `exp(-100 * 0**2) * (1 + 0.5 * 0.5) = exp(0) * (1 + 0.25) = 1 * 1.25 = 1.25`.\n        Bin Y: `bins_remain_cap = 0.55`. Gap = 0.05.\n            Score = `exp(-100 * (0.05)**2) * (1 + 0.5 * 0.55) = exp(-100 * 0.0025) * (1 + 0.275) = exp(-0.25) * 1.275 \u2248 0.7788 * 1.275 \u2248 0.9930`.\n        Bin Z: `bins_remain_cap = 0.8`. Gap = 0.3.\n            Score = `exp(-100 * (0.3)**2) * (1 + 0.5 * 0.8) = exp(-100 * 0.09) * (1 + 0.4) = exp(-9) * 1.4 \u2248 0.000123 * 1.4 \u2248 0.00017`.\n\n        This still favors the perfect fit, but the drop-off is more pronounced.\n        The \"graceful degradation\" aspect implies that a bin that is *slightly* larger but not *too* large might be preferred over a perfect fit if the overall fullness is significantly better.\n\n        Let's invert the gap term's sensitivity to make it less aggressive, or adjust the fullness term.\n        Consider `k=10` instead of `k=100`.\n        `item = 0.5`\n        Bin X: `bins_remain_cap = 0.5`. Gap = 0. Score = `exp(0) * (1 + 0.5 * 0.5) = 1 * 1.25 = 1.25`.\n        Bin Y: `bins_remain_cap = 0.55`. Gap = 0.05.\n            Score = `exp(-10 * (0.05)**2) * (1 + 0.5 * 0.55) = exp(-10 * 0.0025) * (1 + 0.275) = exp(-0.025) * 1.275 \u2248 0.9753 * 1.275 \u2248 1.2435`.\n        Bin Z: `bins_remain_cap = 0.8`. Gap = 0.3.\n            Score = `exp(-10 * (0.3)**2) * (1 + 0.5 * 0.8) = exp(-10 * 0.09) * (1 + 0.4) = exp(-0.9) * 1.4 \u2248 0.4066 * 1.4 \u2248 0.5692`.\n\n        In this case, with `k=10`, Bin X (perfect fit) is still preferred, but Bin Y (slight slack) is very close. This implies a good balance.\n        The `alpha` parameter controls how much we value bins that are generally fuller. If `alpha` is high, Bin Y might even surpass Bin X if `k` is low enough.\n\n        Let's introduce another parameter `beta` to control the \"width\" of the ideal fit.\n        We want to maximize `exp(-beta * (bins_remain_cap - item)**2)`.\n\n        And we want to maximize `alpha * bins_remain_cap`.\n\n        Let's try combining them in a different way:\n        We want to maximize `-(bins_remain_cap - item)**2`.\n        Let's add a term that penalizes `bins_remain_cap` if it's too large.\n        Consider a function `g(x)` that is zero for small `x` and increasingly negative for large `x`.\n        E.g., `g(x) = min(0, -(x - c)**p)` for `p > 0` and some constant `c`.\n\n        A simpler approach: use `max` to select between two good candidates.\n        Candidate 1: Tightest fit (like `priority_v1`).\n        Candidate 2: Fuller bin with a slight slack.\n\n        Let's reconsider the advice: \"finely-grained, multi-dimensional scoring mechanisms that integrate diverse criteria (e.g., fit, fullness, strategic placement) through weighted sums or more complex fusion methods.\"\n\n        The `exp(-k * gap**2) * (1 + alpha * bins_remain_cap)` approach is a fusion of two criteria.\n        The parameter `k` controls the focus on tightness, while `alpha` controls the focus on fullness.\n\n        To make it *better*, we need to address the \"graceful degradation\" or \"avoiding over-emptiness\" more directly.\n\n        Consider the \"Wasted Space Ratio\": `(bins_remain_cap - item) / bins_remain_cap`. We want to minimize this.\n        Consider the \"Current Fill Ratio\": `(BIN_CAPACITY - bins_remain_cap) / BIN_CAPACITY`. We want to maximize this.\n\n        Let `fill_ratio = (1.0 - bins_remain_cap)` (assuming BIN_CAPACITY = 1.0)\n        Let `wasted_ratio = (bins_remain_cap - item) / (bins_remain_cap + 1e-9)`\n\n        Score = `w1 * (1 - wasted_ratio) + w2 * fill_ratio`\n        Score = `w1 * (1 - (bins_remain_cap - item) / (bins_remain_cap + 1e-9)) + w2 * (1 - bins_remain_cap)`\n\n        Let `w1 = 1.0`, `w2 = 0.5`.\n        `score = (bins_remain_cap + 1e-9 - bins_remain_cap + item) / (bins_remain_cap + 1e-9) + 0.5 * (1 - bins_remain_cap)`\n        `score = (item + 1e-9) / (bins_remain_cap + 1e-9) + 0.5 - 0.5 * bins_remain_cap`\n\n        Let `item = 0.5`.\n        Bin X: `bins_remain_cap = 0.5`. Score = `(0.5) / (0.5) + 0.5 - 0.5 * 0.5 = 1 + 0.5 - 0.25 = 1.25`.\n        Bin Y: `bins_remain_cap = 0.55`. Score = `(0.5) / (0.55) + 0.5 - 0.5 * 0.55 \u2248 0.909 + 0.5 - 0.275 = 1.134`.\n        Bin Z: `bins_remain_cap = 0.8`. Score = `(0.5) / (0.8) + 0.5 - 0.5 * 0.8 = 0.625 + 0.5 - 0.4 = 0.725`.\n\n        This heuristic (`(item + 1e-9) / (bins_remain_cap + 1e-9) + 0.5 - 0.5 * bins_remain_cap`)\n        prioritizes the tightest fit (Bin X), then slightly slack (Bin Y), then looser fit (Bin Z).\n        It seems like a robust Best Fit variant.\n\n        To add the \"graceful degradation\" or \"nuance\", we need to avoid bins that are too empty.\n        The current approach implicitly does this via `-inf`.\n\n        Let's try to create a score that is high for bins that are \"almost full\" and \"almost fitting\".\n        Consider a score that is a polynomial or exponential centered around a \"good\" region.\n\n        **Final Proposal (`priority_v2`):**\n        Combine a strong preference for minimal slack with a moderate preference for current bin fullness.\n        The score function will penalize large slack quadratically.\n\n        For fitting bins (`bins_remain_cap >= item`):\n        Score = `w_slack * -(bins_remain_cap - item)**2 + w_fullness * bins_remain_cap`\n\n        Let's tune the weights.\n        If `w_slack = 100`, `w_fullness = 1`.\n        `score = -100 * (bins_remain_cap - item)**2 + bins_remain_cap`\n\n        `item = 0.5`\n        Bin X: `bins_remain_cap = 0.5`. Gap = 0.\n            Score = `-100 * 0**2 + 0.5 = 0.5`.\n        Bin Y: `bins_remain_cap = 0.55`. Gap = 0.05.\n            Score = `-100 * (0.05)**2 + 0.55 = -100 * 0.0025 + 0.55 = -0.25 + 0.55 = 0.3`.\n        Bin Z: `bins_remain_cap = 0.8`. Gap = 0.3.\n            Score = `-100 * (0.3)**2 + 0.8 = -100 * 0.09 + 0.8 = -9 + 0.8 = -8.2`.\n\n        This heuristic strongly favors the perfect fit (Bin X), then the slightly slack (Bin Y).\n        This is effectively a sharpened version of Best Fit.\n\n        Let's introduce a penalty for \"emptiness\" more directly.\n        Consider the inverse of the current bin fill: `1 / bins_remain_cap`.\n        We want to penalize bins that have too much remaining capacity.\n\n        Revised Score:\n        For fitting bins:\n        `score = -bins_remain_cap + item + alpha * (1 - bins_remain_cap)`\n        This gives: `item + alpha - (1 + alpha) * bins_remain_cap`.\n        This prioritizes minimum `bins_remain_cap`.\n\n        Let's try this:\n        The score aims to balance tight fitting with overall bin fullness.\n        It penalizes large remaining capacities more severely.\n\n        For fitting bins:\n        `score = (item - bins_remain_cap) * k1 + bins_remain_cap * k2`\n        where `k1` heavily favors tight fits, and `k2` favors fuller bins.\n\n        Let's try the exponential function approach again but with different framing.\n        We want to maximize a function that peaks at `bins_remain_cap = item`.\n        Let `f(x) = exp(-k * (x - item)**2)` where `x = bins_remain_cap`.\n        This peaks at `x = item`.\n\n        Now, combine with fullness `x`.\n        `score = exp(-k * (bins_remain_cap - item)**2) * (1 + alpha * bins_remain_cap)`\n        This was analyzed earlier. `k=10`, `alpha=0.5` seemed to work well.\n\n        Let's try to implement this.\n        Parameters: `k` (tightness sensitivity), `alpha` (fullness weight).\n    \"\"\"\n\n    # Assign a very low priority to bins that cannot fit the item\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can fit the item\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate scores for fitting bins\n    fitting_bins_remain_cap = bins_remain_cap[fit_mask]\n    gap = fitting_bins_remain_cap - item\n\n    # Tunable parameters\n    k = 10.0  # Sensitivity to tightness (higher k means stronger preference for tight fit)\n    alpha = 0.5 # Weight for current bin fullness (higher alpha means stronger preference for fuller bins)\n\n    # Score calculation:\n    # The first term `exp(-k * gap**2)` peaks at gap=0 (perfect fit) and decays quadratically.\n    # The second term `(1 + alpha * fitting_bins_remain_cap)` boosts the score for bins that are\n    # fuller to begin with, acting as a tie-breaker or secondary preference.\n    # The `1 + ...` ensures the multiplier is always positive.\n    scores = np.exp(-k * (gap**2)) * (1 + alpha * fitting_bins_remain_cap)\n\n    # Assign calculated scores to the eligible bins\n    priorities[fit_mask] = scores\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.008775428799367,
    "SLOC": 10.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response0.txt_stdout.txt",
    "code_path": "problem_iter5_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit and a penalty for excessive remaining capacity.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins_mask = bins_remain_cap >= item\n\n    if not np.any(valid_bins_mask):\n        return priorities\n\n    valid_caps = bins_remain_cap[valid_bins_mask]\n    epsilon = 1e-9\n\n    # Heuristic 1: Best Fit (prioritize bins with minimal remaining capacity after packing)\n    # This is achieved by using the inverse of the difference.\n    # A smaller difference (remaining_cap - item) means a tighter fit and higher priority.\n    fit_difference = valid_caps - item\n    best_fit_scores = 1.0 / (fit_difference + epsilon)\n\n    # Heuristic 18/19/20 inspired: Penalize bins with excessively large remaining capacity.\n    # We can scale the remaining capacity itself. Higher remaining capacity should have lower priority.\n    # Using inverse of remaining capacity to reflect this.\n    # A larger remaining capacity means a lower score here.\n    excess_capacity_scores = 1.0 / (valid_caps + epsilon)\n\n    # Combine scores: We want both a tight fit AND not too much excess capacity.\n    # Multiplying the scores gives a combined priority that favors bins that are both\n    # close to fitting the item AND not overly large.\n    # A small positive constant is added to `fit_difference` to avoid division by zero.\n    # A small positive constant is added to `valid_caps` for similar reasons.\n    combined_scores = best_fit_scores * excess_capacity_scores\n\n    # Normalize scores to a range that might be useful for probabilistic selection or just for relative comparison.\n    # Using Softmax-like scaling (exponential) can highlight preferences more strongly.\n    # Let's use exp(-scaled_difference) to map smaller differences to higher scores, and then scale by excess capacity.\n    # A temperature parameter can control the sharpness of the priority.\n    temperature = 0.5  # Tunable parameter: lower value for stronger preference to best fit\n    \n    # Re-calculating scaled difference for exponential mapping to emphasize tight fits\n    # Map differences to exponents: smaller difference -> larger exponent -> larger priority\n    scaled_exponents = -fit_difference / temperature\n\n    # Combine the \"tight fit\" exponential score with the \"less excess capacity\" inverse score.\n    # Multiplication here means we want high scores in both aspects.\n    final_scores_for_valid_bins = np.exp(scaled_exponents) * excess_capacity_scores\n\n    # Assign the computed scores back to the original bins array\n    priorities[valid_bins_mask] = final_scores_for_valid_bins\n\n    # Ensure no negative priorities and handle edge cases (e.g., all priorities are zero)\n    # If all valid bins resulted in NaN or Inf, or very low scores, this can be a fallback.\n    # If all scores are zero (e.g., no valid bins, handled earlier, or all resulted in ~0 scores)\n    # we might want a uniform preference, but the zero initialization already covers this.\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response1.txt_stdout.txt",
    "code_path": "problem_iter5_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines inverse proximity with exponential decay for nuanced bin selection.\n\n    Favors tighter fits while allowing some preference for less tight bins,\n    tuned by an exponential decay parameter.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n        # Inverse proximity for tight fits\n        proximity = suitable_bins_caps - item\n        inverse_proximity_scores = 1.0 / (proximity + 1e-9)\n\n        # Normalize inverse proximity scores to a [0, 1] range\n        if np.max(inverse_proximity_scores) > 0:\n            normalized_inverse_proximity = inverse_proximity_scores / np.max(inverse_proximity_scores)\n        else:\n            normalized_inverse_proximity = np.zeros_like(inverse_proximity_scores)\n\n        # Use exponential decay on normalized inverse proximity.\n        # A higher normalized inverse proximity (tighter fit) results in a score closer to 1.\n        # The temperature parameter controls the decay rate. Smaller temperature -> steeper decay.\n        temperature = 0.5\n        exponential_scores = np.exp(normalized_inverse_proximity / temperature)\n\n        # Normalize exponential scores to a [0, 1] range.\n        if np.max(exponential_scores) > 0:\n            final_scores = exponential_scores / np.max(exponential_scores)\n        else:\n            final_scores = np.zeros_like(exponential_scores)\n\n        priorities[suitable_bins_mask] = final_scores\n\n        # Ensure perfect fits receive the maximum priority (1.0)\n        perfect_fit_mask = (proximity == 0)\n        if np.any(perfect_fit_mask):\n            priorities[suitable_bins_mask][perfect_fit_mask] = 1.0\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 22.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response6.txt_stdout.txt",
    "code_path": "problem_iter5_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fitting preference with a Softmax-based distribution.\n    Favors bins with minimal remaining capacity after placement, using Softmax\n    to create a graded priority, encouraging exploration of good fits.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    eligible_capacities = bins_remain_cap[eligible_bins_mask]\n\n    if eligible_capacities.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Prioritize tighter fits: smaller (capacity - item) is better.\n    # We use the negative difference for Softmax, so smaller differences\n    # lead to larger (more positive) exponents.\n    # This is inspired by Heuristic 7 (Softmax on negative differences).\n    differences = eligible_capacities - item\n    scores = -differences\n\n    # Use Softmax to generate a probability distribution over eligible bins.\n    # This creates graded priorities, favoring tighter fits more strongly.\n    # Stability trick: subtract max score before exponentiating.\n    # This is the core of Heuristic 7.\n    if np.max(scores) - np.min(scores) > 1e6: # Heuristic for numerical stability if scores vary extremely\n        scaled_scores = (scores - np.min(scores)) / (np.max(scores) - np.min(scores) + 1e-9)\n    else:\n        scaled_scores = scores\n\n    exp_scores = np.exp(scaled_scores - np.max(scaled_scores))\n    priorities = exp_scores / np.sum(exp_scores)\n\n    # Map priorities back to the original bin structure\n    full_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    full_priorities[eligible_bins_mask] = priorities\n\n    return full_priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response4.txt_stdout.txt",
    "code_path": "problem_iter5_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins based on a combination of tightest fit and overall bin fullness.\n    Uses a sigmoid function to provide graded priorities for suitable bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        \n        # Metric 1: Tightest fit (inverse of remaining capacity after packing)\n        # Smaller remaining capacity is better (tighter fit)\n        gaps = suitable_bins_caps - item\n        \n        # Avoid division by zero or very small numbers for bins with zero remaining capacity\n        inverse_proximity = 1.0 / (gaps + 1e-9)\n        \n        # Normalize inverse proximity to a [0, 1] range for the sigmoid\n        # Max value corresponds to the tightest fit, min to the loosest fit among suitable bins\n        min_inv_prox = np.min(inverse_proximity)\n        max_inv_prox = np.max(inverse_proximity)\n        \n        if max_inv_prox == min_inv_prox: # All suitable bins have the same tightness\n            normalized_proximity = np.ones_like(inverse_proximity) * 0.5 \n        else:\n            normalized_proximity = (inverse_proximity - min_inv_prox) / (max_inv_prox - min_inv_prox)\n            \n        # Metric 2: Fullness of the bin (inverse of remaining capacity before packing)\n        # Higher fullness is generally better to keep smaller bins for smaller items\n        fullness_scores = 1.0 / (suitable_bins_caps + 1e-9)\n        \n        # Normalize fullness scores\n        min_fullness = np.min(fullness_scores)\n        max_fullness = np.max(fullness_scores)\n        \n        if max_fullness == min_fullness: # All suitable bins have the same fullness\n            normalized_fullness = np.ones_like(fullness_scores) * 0.5\n        else:\n            normalized_fullness = (fullness_scores - min_fullness) / (max_fullness - min_fullness)\n            \n        # Combine metrics using sigmoid for graded preference\n        # We want high proximity score (tight fit) and high fullness score to have higher priority\n        # Sigmoid with a positive slope centered around 0.5 will map higher combined scores to higher priorities\n        combined_score = normalized_proximity * 0.7 + normalized_fullness * 0.3 # Weighted combination\n        \n        # Apply sigmoid to create graded priorities between 0 and 1\n        # A temperature parameter could be added here for tuning (e.g., sigmoid(k * (combined_score - 0.5)))\n        sigmoid_priorities = 1 / (1 + np.exp(-10 * (combined_score - 0.5))) \n        \n        priorities[suitable_bins_mask] = sigmoid_priorities\n        \n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 24.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response9.txt_stdout.txt",
    "code_path": "problem_iter5_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight fit score with bin fullness score using a weighted sum and sigmoid.\n    Prioritizes bins that minimize remaining capacity after packing, and also favor bins that are already fuller.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(eligible_bins_mask):\n        eligible_capacities = bins_remain_cap[eligible_bins_mask]\n        \n        # Score 1: Tight fit (inverse proximity)\n        # Smaller gap (eligible_capacities - item) means higher score. Add epsilon for stability.\n        tight_fit_scores = 1.0 / (eligible_capacities - item + 1e-9)\n\n        # Score 2: Bin fullness (inverse of remaining capacity)\n        # Fuller bins (smaller remaining capacity) get higher scores. Add epsilon.\n        fullness_scores = 1.0 / (eligible_capacities + 1e-9)\n\n        # Normalize scores to a common range (e.g., 0 to 1) for combination\n        # Normalize tight_fit_scores\n        min_tf, max_tf = np.min(tight_fit_scores), np.max(tight_fit_scores)\n        if max_tf > min_tf:\n            normalized_tight_fit = (tight_fit_scores - min_tf) / (max_tf - min_tf)\n        else:\n            normalized_tight_fit = np.ones_like(tight_fit_scores) * 0.5\n\n        # Normalize fullness_scores\n        min_fs, max_fs = np.min(fullness_scores), np.max(fullness_scores)\n        if max_fs > min_fs:\n            normalized_fullness = (fullness_scores - min_fs) / (max_fs - min_fs)\n        else:\n            normalized_fullness = np.ones_like(fullness_scores) * 0.5\n\n        # Combine scores with weights (can be tuned)\n        # Weights determine the relative importance of tight fit vs. fullness\n        weight_tight_fit = 0.6\n        weight_fullness = 0.4\n        combined_scores = (weight_tight_fit * normalized_tight_fit) + (weight_fullness * normalized_fullness)\n\n        # Apply sigmoid to the combined scores to get smooth priorities\n        # A steeper sigmoid (e.g., 10) emphasizes differences more.\n        # We want higher combined_scores to map to higher sigmoid outputs.\n        sigmoid_priorities = 1 / (1 + np.exp(-10 * (combined_scores - 0.5)))\n        \n        priorities[eligible_bins_mask] = sigmoid_priorities\n        \n        # If all eligible bins result in identical sigmoid priorities (e.g., all scores are same),\n        # assign a neutral priority of 0.5 to ensure some differentiation if possible.\n        # This also handles cases where combined_scores are all exactly 0.5.\n        if np.all(priorities[eligible_bins_mask] == 0.5) and np.any(eligible_bins_mask):\n             priorities[eligible_bins_mask] = 0.5 # Assign neutral priority if all are same\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 25.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response6.txt_stdout.txt",
    "code_path": "problem_iter8_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority for bins to pack an item.\n    Prioritizes bins with minimal remaining capacity that can fit the item,\n    using an exponential scaling for graded preferences.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n    \n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    differences = suitable_bins_caps - item\n    \n    if suitable_bins_caps.size > 0:\n        min_diff = np.min(differences)\n        \n        \n        scaled_diffs = differences - min_diff\n        \n        \n        temperature = 0.1\n        exp_scores = np.exp(-scaled_diffs / temperature)\n        \n        \n        normalized_exp_scores = exp_scores / np.max(exp_scores)\n        \n        priorities[suitable_bins_mask] = normalized_exp_scores\n        \n        \n        if np.all(priorities == 0):\n            priorities[suitable_bins_mask] = 0.5\n            \n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 17.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response2.txt_stdout.txt",
    "code_path": "problem_iter11_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit and bin fullness using normalized scores and exponential decay.\n    Prioritizes bins that are a good fit and are already relatively full.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        \n        # Metric 1: Tightest fit (proximity)\n        # Smaller remaining capacity after packing is better. Add small epsilon to avoid division by zero.\n        gaps = suitable_bins_caps - item\n        proximity_scores = 1.0 / (gaps + 1e-9)\n        \n        # Normalize proximity scores to [0, 1]\n        min_prox = np.min(proximity_scores)\n        max_prox = np.max(proximity_scores)\n        if max_prox == min_prox:\n            normalized_proximity = np.ones_like(proximity_scores) * 0.5\n        else:\n            normalized_proximity = (proximity_scores - min_prox) / (max_prox - min_prox)\n            \n        # Metric 2: Fullness of the bin before packing\n        # Higher fullness (lower remaining capacity) is generally better.\n        fullness_scores = 1.0 / (suitable_bins_caps + 1e-9)\n        \n        # Normalize fullness scores to [0, 1]\n        min_fullness = np.min(fullness_scores)\n        max_fullness = np.max(fullness_scores)\n        if max_fullness == min_fullness:\n            normalized_fullness = np.ones_like(fullness_scores) * 0.5\n        else:\n            normalized_fullness = (fullness_scores - min_fullness) / (max_fullness - min_fullness)\n            \n        # Combine metrics with a preference for tightness, modulated by fullness\n        # Use exponential decay on the combined score to create a graded priority\n        # A higher combined score (good fit and high fullness) will result in a higher priority\n        combined_score = normalized_proximity * 0.7 + normalized_fullness * 0.3\n        \n        # Apply exponential decay to create graded priorities, similar to Heuristic 12's approach\n        # Here, we map the combined score to a priority. A higher combined score should yield a higher priority.\n        # Using an exponential function (e.g., exp(x)) naturally produces a graded response.\n        # We scale and shift to get a reasonable range, e.g., mapping [0,1] combined_score to a positive range.\n        # An exponential function like exp(k * combined_score) where k is a scaling factor.\n        # Let's use exp(5 * combined_score) for a steeper curve.\n        exponential_priorities = np.exp(5 * combined_score)\n        \n        # Normalize these exponential priorities to [0, 1]\n        min_exp_prio = np.min(exponential_priorities)\n        max_exp_prio = np.max(exponential_priorities)\n        \n        if max_exp_prio == min_exp_prio:\n            final_priorities = np.ones_like(exponential_priorities) * 0.5\n        else:\n            final_priorities = (exponential_priorities - min_exp_prio) / (max_exp_prio - min_exp_prio)\n        \n        priorities[suitable_bins_mask] = final_priorities\n        \n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 30.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response8.txt_stdout.txt",
    "code_path": "problem_iter11_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines normalized inverse proximity with a linear penalty on excess capacity.\n\n    Favors tighter fits using inverse proximity, tempered by a penalty\n    for bins with significantly more remaining capacity than needed.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n        # Inverse proximity for tight fits, normalized\n        proximity = suitable_bins_caps - item\n        inverse_proximity_scores = 1.0 / (proximity + 1e-9)\n\n        if np.max(inverse_proximity_scores) > 0:\n            normalized_inverse_proximity = inverse_proximity_scores / np.max(inverse_proximity_scores)\n        else:\n            normalized_inverse_proximity = np.zeros_like(inverse_proximity_scores)\n\n        # Linear penalty on excess capacity, normalized\n        excess_capacity = suitable_bins_caps - item\n        if np.max(excess_capacity) > 0:\n            normalized_excess_capacity = excess_capacity / np.max(excess_capacity)\n        else:\n            normalized_excess_capacity = np.zeros_like(excess_capacity)\n\n        # Combine normalized inverse proximity with a penalty for excess capacity\n        # A higher score is better. We want tight fits (high normalized_inverse_proximity)\n        # and we penalize bins with more excess capacity (lower normalized_excess_capacity).\n        # A simple combination: normalized_inverse_proximity - normalized_excess_capacity\n        # This prioritizes tight fits, but slightly de-prioritizes bins with large excess.\n        combined_scores = normalized_inverse_proximity - normalized_excess_capacity\n\n        # Ensure perfect fits (where proximity is 0) get maximum priority if they exist\n        # This also ensures their score isn't penalized by the excess capacity term.\n        perfect_fit_mask = (proximity == 0)\n        combined_scores[perfect_fit_mask] = np.max(normalized_inverse_proximity) + 1 # Give a slight boost\n\n        # Normalize the combined scores to be in a reasonable range for priority\n        if np.ptp(combined_scores) > 0: # ptp is peak-to-peak range\n            priorities[suitable_bins_mask] = (combined_scores - np.min(combined_scores)) / np.ptp(combined_scores)\n        else:\n            priorities[suitable_bins_mask] = np.zeros_like(combined_scores) # All scores are the same\n\n        # Ensure perfect fits still have the absolute highest priority after normalization\n        priorities[suitable_bins_mask][perfect_fit_mask] = 1.0\n\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 25.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response3.txt_stdout.txt",
    "code_path": "problem_iter12_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    This heuristic aims to improve upon previous versions by employing a more\n    sophisticated multi-criteria scoring mechanism that considers both the\n    tightness of the fit and the overall \"strategy\" of bin usage. It incorporates\n    non-linear scoring and adaptive weighting to handle different scenarios.\n\n    Scoring strategy:\n    1.  **Tightness Score (Primary):** Prioritize bins that leave the minimal positive\n        remaining capacity after placing the item. This is similar to the Best Fit\n        strategy, but we'll use a non-linear transformation to allow for more\n        granularity. Specifically, we want to maximize `item - bins_remain_cap`\n        for fitting bins, but we'll transform this to penalize larger gaps more\n        severely using a quadratic function.\n    2.  **Fullness Score (Secondary):** Favor bins that are already relatively full\n        before the item is placed. This encourages consolidation and can help\n        reduce the total number of bins used, especially for smaller items.\n\n    The heuristic aims to balance these two criteria. For bins that cannot fit the\n    item, a very low priority score is assigned.\n\n    For bins that *can* fit the item (`bins_remain_cap >= item`):\n    -   Calculate the 'gap': `gap = bins_remain_cap - item`. We want to minimize this positive gap.\n    -   Calculate a 'tightness score component': `tightness_score = -gap**2`. This strongly penalizes larger gaps.\n    -   Calculate a 'fullness score component': `fullness_score = bins_remain_cap`. This rewards bins that are already more full.\n\n    We combine these with tunable weights. Let's use `w_tight` for tightness and\n    `w_full` for fullness. A common approach is to prioritize tightness, but\n    also give a boost to fuller bins if the tightness is comparable.\n\n    A potential combined score for fitting bins could be:\n    `score = w_tight * (-gap**2) + w_full * bins_remain_cap`\n\n    The goal is to maximize this score.\n    Let's set `w_tight = 1.0` and `w_full = 0.5`.\n    `score = -(bins_remain_cap - item)**2 + 0.5 * bins_remain_cap`\n\n    This means a perfect fit (`gap = 0`) gets `0.5 * bins_remain_cap`.\n    A slightly larger gap (`gap = 0.1`) gets `-(0.01) + 0.5 * (item + 0.1)`.\n    If `item=0.5`, `bins_remain_cap=0.55`: `score = -(0.05)**2 + 0.5 * 0.55 = -0.0025 + 0.275 = 0.2725`\n    If `item=0.5`, `bins_remain_cap=0.5`: `score = -(0.0)**2 + 0.5 * 0.5 = 0.25`\n    This heuristic would prefer the bin with a small gap over a perfect fit, which\n    might be beneficial for distributing items more evenly.\n\n    To add more dynamism and potentially adapt to the context of the problem (e.g.,\n    item sizes or bin capacities), we can introduce an adaptive parameter.\n    Let's consider a \"sensitivity\" parameter `s` that influences how much we\n    penalize gaps. A higher `s` means we are more sensitive to gaps.\n\n    Revised score for fitting bins:\n    `score = -(s * gap)**2 + lambda_param * bins_remain_cap`\n    Let `s` be a function of the item size relative to bin capacity, or a global parameter.\n    For simplicity, let's use a fixed `s` and `lambda_param`.\n\n    Let `s = 1.0` (for now, can be tuned or made adaptive)\n    Let `lambda_param = 0.3` (prioritizing fuller bins slightly more)\n\n    `score = -(bins_remain_cap - item)**2 + 0.3 * bins_remain_cap`\n\n    Example: `item = 0.4`\n    Bin A: `bins_remain_cap = 0.4`. Gap = 0. `score = -(0)**2 + 0.3 * 0.4 = 0.12`.\n    Bin B: `bins_remain_cap = 0.45`. Gap = 0.05. `score = -(0.05)**2 + 0.3 * 0.45 = -0.0025 + 0.135 = 0.1325`.\n    Bin C: `bins_remain_cap = 0.7`. Gap = 0.3. `score = -(0.3)**2 + 0.3 * 0.7 = -0.09 + 0.21 = 0.12`.\n    Bin D: `bins_remain_cap = 0.3`. Cannot fit. Score = -inf.\n\n    Bin B is preferred (tightest fit among the slightly-gapped ones), then Bin A (perfect fit), then Bin C.\n    This heuristic is more nuanced than simply picking the tightest fit.\n\n    Consider an edge case: what if all fitting bins are much larger than the item?\n    `item = 0.1`\n    Bin X: `bins_remain_cap = 0.9`. Gap = 0.8. `score = -(0.8)**2 + 0.3 * 0.9 = -0.64 + 0.27 = -0.37`.\n    Bin Y: `bins_remain_cap = 0.5`. Gap = 0.4. `score = -(0.4)**2 + 0.3 * 0.5 = -0.16 + 0.15 = -0.01`.\n    Bin Z: `bins_remain_cap = 0.15`. Gap = 0.05. `score = -(0.05)**2 + 0.3 * 0.15 = -0.0025 + 0.045 = 0.0425`.\n\n    Bin Z is preferred, then Bin Y, then Bin X. This seems reasonable.\n\n    To make it *better* than `priority_v1`, which effectively is `max(item - bins_remain_cap)`,\n    we can focus on making the score more sensitive to the 'almost perfect' fits\n    while still penalizing large gaps.\n    The quadratic term `-(gap**2)` already does this. The `lambda_param * bins_remain_cap`\n    adds the secondary criterion of bin fullness.\n\n    Let's introduce another element: penalizing bins that are *so* full that only\n    very small items could fit into them. This might be captured by the `-gap**2`\n    term, as a very full bin (`bins_remain_cap` is small) implies a small gap for a small item.\n\n    Consider a scenario where we want to avoid leaving *too much* empty space,\n    but also not fill bins *too* tightly if it means leaving very little room for future items.\n    The current score `-(gap**2) + lambda_param * bins_remain_cap` tries to balance this.\n\n    Let's consider an alternative non-linear transformation for the gap.\n    Instead of `-(gap**2)`, maybe something like `-(gap / bins_remain_cap)**2` or `-(gap / BIN_CAPACITY)**2`\n    to normalize the gap. However, `BIN_CAPACITY` is not given.\n\n    Let's stick with the current structure but adjust weights and perhaps add\n    a small penalty for extremely large bins that fit the item, to encourage\n    using bins that are \"just right\".\n    The `lambda_param * bins_remain_cap` term already discourages very empty bins.\n\n    Let's make the penalty for gap more pronounced for larger gaps by squaring.\n    We also want to reward bins that are fuller overall, hence `lambda_param * bins_remain_cap`.\n    To be better than `priority_v1`, which is equivalent to `item - bins_remain_cap`,\n    we need to ensure our heuristic offers a different trade-off.\n\n    The score `-(bins_remain_cap - item)**2 + lambda_param * bins_remain_cap`\n    is designed to:\n    1. Strongly favor minimizing the gap `bins_remain_cap - item`.\n    2. Provide a secondary boost to bins that are more full.\n\n    This is a good candidate for `priority_v2`.\n\n    Parameters:\n    - `gap_penalty_factor`: Controls the strength of the penalty for larger gaps.\n    - `fullness_bonus_factor`: Controls the bonus for fuller bins.\n    \"\"\"\n\n    # Define weights for the scoring components\n    gap_penalty_factor = 1.0  # Higher value means stronger penalty for gaps\n    fullness_bonus_factor = 0.4 # Higher value means stronger bonus for fuller bins\n\n    # Initialize priorities with a very low value for bins that cannot fit the item\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Create a mask for bins that can accommodate the item\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate scores only for the bins that can fit the item\n    fitting_bins_remain_cap = bins_remain_cap[fit_mask]\n\n    # Calculate the gap (remaining capacity after placing the item)\n    gap = fitting_bins_remain_cap - item\n\n    # Calculate the tightness score component: penalize larger gaps quadratically\n    # We want to maximize -(gap^2), meaning minimize gap^2\n    tightness_score_component = -(gap_penalty_factor * gap)**2\n\n    # Calculate the fullness score component: reward bins that are already fuller\n    # This term is simply the remaining capacity itself\n    fullness_score_component = fullness_bonus_factor * fitting_bins_remain_cap\n\n    # Combine the scores\n    combined_scores = tightness_score_component + fullness_score_component\n\n    # Assign the calculated scores to the corresponding bins\n    priorities[fit_mask] = combined_scores\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter13_response0.txt_stdout.txt",
    "code_path": "problem_iter13_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 8.887281177181691e-06, sigmoid_steepness: float = 12.141015575843076, neutral_priority: float = 0.5002145562071864) -> np.ndarray:\n    \"\"\"\n    Combines inverse proximity for tight fits with a sigmoid for smooth preference.\n    Favors bins with minimal remaining capacity after packing, scaled smoothly.\n\n    Args:\n        item (float): The size of the item to be packed.\n        bins_remain_cap (np.ndarray): A numpy array representing the remaining capacity of each bin.\n        epsilon (float): A small value to prevent division by zero. Defaults to 1e-9.\n        sigmoid_steepness (float): Controls the steepness of the sigmoid function. Higher values make the transition sharper. Defaults to 10.0.\n        neutral_priority (float): The priority assigned to bins that are equidistant in terms of fit. Defaults to 0.5.\n\n    Returns:\n        np.ndarray: A numpy array of priorities for each bin.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(eligible_bins_mask):\n        eligible_capacities = bins_remain_cap[eligible_bins_mask]\n        \n        # Inverse proximity: smaller gap is better (higher score)\n        # Adding a small epsilon to avoid division by zero\n        inverse_proximity = 1.0 / (eligible_capacities - item + epsilon)\n\n        # Normalize inverse proximity to a range where sigmoid is effective\n        # Aims to map smaller gaps (higher inverse_proximity) to values around 0.5\n        # and larger gaps to values further from 0.5.\n        # This normalization is heuristic and can be tuned.\n        min_inv_proximity = np.min(inverse_proximity)\n        max_inv_proximity = np.max(inverse_proximity)\n        \n        if max_inv_proximity > min_inv_proximity:\n            normalized_scores = (inverse_proximity - min_inv_proximity) / (max_inv_proximity - min_inv_proximity)\n        else: # All eligible bins have the same inverse proximity\n            normalized_scores = np.ones_like(inverse_proximity) * neutral_priority\n\n        # Sigmoid function to create a smooth priority distribution\n        # The steepness parameter (e.g., 10) can be tuned.\n        # We want bins with smaller gaps (higher normalized_scores) to have higher sigmoid outputs.\n        # So, we invert the normalized_scores for the sigmoid input to favor smaller gaps.\n        # The input to sigmoid is centered around 0 for the neutral_priority.\n        sigmoid_input = sigmoid_steepness * (normalized_scores - neutral_priority)\n        sigmoid_priorities = 1 / (1 + np.exp(-sigmoid_input))\n\n        priorities[eligible_bins_mask] = sigmoid_priorities\n        \n        # Ensure that if all eligible bins are identical in terms of fit, they get a neutral priority\n        if np.all(priorities[eligible_bins_mask] == neutral_priority) and len(eligible_bins_mask) > 0:\n            priorities[eligible_bins_mask] = neutral_priority\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter16_response0.txt_stdout.txt",
    "code_path": "problem_iter16_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 4.228630226255814e-05, temperature: float = 1.4830507458244795) -> np.ndarray:\n    valid_bins = bins_remain_cap >= item\n    \n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.487435181491823,
    "SLOC": 4.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  }
]