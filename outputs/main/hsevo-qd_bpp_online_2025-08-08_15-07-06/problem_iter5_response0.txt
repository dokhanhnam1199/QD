```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Combines Best Fit and a penalty for excessive remaining capacity."""

    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    valid_bins_mask = bins_remain_cap >= item

    if not np.any(valid_bins_mask):
        return priorities

    valid_caps = bins_remain_cap[valid_bins_mask]
    epsilon = 1e-9

    # Heuristic 1: Best Fit (prioritize bins with minimal remaining capacity after packing)
    # This is achieved by using the inverse of the difference.
    # A smaller difference (remaining_cap - item) means a tighter fit and higher priority.
    fit_difference = valid_caps - item
    best_fit_scores = 1.0 / (fit_difference + epsilon)

    # Heuristic 18/19/20 inspired: Penalize bins with excessively large remaining capacity.
    # We can scale the remaining capacity itself. Higher remaining capacity should have lower priority.
    # Using inverse of remaining capacity to reflect this.
    # A larger remaining capacity means a lower score here.
    excess_capacity_scores = 1.0 / (valid_caps + epsilon)

    # Combine scores: We want both a tight fit AND not too much excess capacity.
    # Multiplying the scores gives a combined priority that favors bins that are both
    # close to fitting the item AND not overly large.
    # A small positive constant is added to `fit_difference` to avoid division by zero.
    # A small positive constant is added to `valid_caps` for similar reasons.
    combined_scores = best_fit_scores * excess_capacity_scores

    # Normalize scores to a range that might be useful for probabilistic selection or just for relative comparison.
    # Using Softmax-like scaling (exponential) can highlight preferences more strongly.
    # Let's use exp(-scaled_difference) to map smaller differences to higher scores, and then scale by excess capacity.
    # A temperature parameter can control the sharpness of the priority.
    temperature = 0.5  # Tunable parameter: lower value for stronger preference to best fit
    
    # Re-calculating scaled difference for exponential mapping to emphasize tight fits
    # Map differences to exponents: smaller difference -> larger exponent -> larger priority
    scaled_exponents = -fit_difference / temperature

    # Combine the "tight fit" exponential score with the "less excess capacity" inverse score.
    # Multiplication here means we want high scores in both aspects.
    final_scores_for_valid_bins = np.exp(scaled_exponents) * excess_capacity_scores

    # Assign the computed scores back to the original bins array
    priorities[valid_bins_mask] = final_scores_for_valid_bins

    # Ensure no negative priorities and handle edge cases (e.g., all priorities are zero)
    # If all valid bins resulted in NaN or Inf, or very low scores, this can be a fallback.
    # If all scores are zero (e.g., no valid bins, handled earlier, or all resulted in ~0 scores)
    # we might want a uniform preference, but the zero initialization already covers this.

    return priorities
```
