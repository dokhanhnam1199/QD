```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    valid_bins = bins_remain_cap >= item
    
    if not np.any(valid_bins):
        return np.zeros_like(bins_remain_cap)

    valid_capacities = bins_remain_cap[valid_bins]
    
    # Higher remaining capacity means lower priority (we want to fill bins)
    # To use Softmax effectively, we want larger values to correspond to higher priority.
    # So we can use (large_capacity - remaining_capacity) or 1/(remaining_capacity)
    # Let's try 1/(remaining_capacity) as it penalizes bins that are already very full.
    
    inverted_capacities = 1.0 / valid_capacities
    
    # To further encourage fitting into bins with *just enough* space, 
    # we can add a term that penalizes bins with very large remaining capacity.
    # Let's try subtracting the ratio of remaining capacity to total capacity (assuming initial bin capacity is known or can be estimated).
    # For simplicity here, let's just use the inverse capacity.
    
    # Adding a small epsilon to avoid division by zero if a bin has 0 remaining capacity, though valid_bins should prevent this.
    epsilon = 1e-9
    scores = 1.0 / (valid_capacities + epsilon)
    
    # Softmax is often used to turn scores into probabilities or weights.
    # A higher score should mean a higher probability of selection.
    # Let's scale the scores to be positive and somewhat related to "how well" it fits.
    # A common approach in fitting is to maximize the remaining capacity, 
    # but here we want to minimize the number of bins. So we prefer bins that are *almost* full.
    # Let's try prioritizing bins where item fits snugly.
    
    fit_difference = valid_capacities - item
    # We want to minimize fit_difference. To make it a priority (higher is better), we invert it.
    # Add a small constant to avoid division by zero if fit_difference is 0.
    priority_scores = 1.0 / (fit_difference + epsilon)
    
    # Apply Softmax to convert scores into probabilities (weights)
    # We add a small penalty for bins that have much more capacity than needed to discourage very loose fits.
    # Let's consider the "waste" factor. Waste = remaining_capacity - item
    # We want to minimize waste.
    
    # Let's try a heuristic that favors bins that have enough capacity but not excessively more.
    # We can try a value that increases as remaining_capacity gets closer to item.
    
    # Option 1: Prioritize bins that are almost full (minimum remaining capacity that fits item)
    # We want higher scores for smaller `valid_capacities`. So `1/valid_capacities` or similar.
    # To be more specific, we want `valid_capacities - item` to be small.
    # So we can use `1.0 / (valid_capacities - item + epsilon)`
    
    priorities = np.zeros_like(bins_remain_cap)
    priorities[valid_bins] = 1.0 / (valid_capacities - item + epsilon)
    
    # Apply Softmax: exp(score) / sum(exp(scores))
    # For just returning priority scores for selection, we can directly use the calculated scores
    # or apply a transformation like Softmax.
    # If we want to select *one* bin based on highest priority, we can just return the scores directly.
    # If we want to weight bins for some probabilistic selection, Softmax is good.
    # For this problem, simply returning the scores that indicate preference is sufficient.
    
    # Let's refine to prioritize bins where remaining_capacity - item is minimized.
    # A simple approach for priority is the inverse of the difference.
    
    scores = np.zeros_like(bins_remain_cap)
    scores[valid_bins] = 1.0 / (valid_capacities - item + epsilon)

    # To make it more "Softmax-like" in spirit of distribution, 
    # we can use a sigmoid-like transformation or directly use scaled values.
    # Let's consider a temperature parameter to control the "sharpness" of priorities.
    temperature = 1.0
    
    # Let's make values larger for better fits.
    # A potential issue is if all valid capacities are very large, leading to small inverse values.
    # We need to ensure scores are somewhat comparable or scaled.
    
    # Consider the "tightness of fit" as the primary driver.
    # Tightest fit = smallest (remaining_capacity - item).
    # So, priority is inversely proportional to (remaining_capacity - item).
    
    normalized_scores = np.zeros_like(bins_remain_cap)
    if np.any(valid_bins):
        # Calculate scores: higher score for tighter fit
        # We want to maximize (1 / (remaining_capacity - item))
        # Or to avoid issues with very small differences, maybe prioritize directly by minimum remaining capacity that fits.
        
        # Let's try a direct mapping:
        # A bin is "good" if remaining_capacity is just enough.
        # So, priority is high when remaining_capacity is close to item.
        
        # Let's use `remaining_capacity` itself as a negative factor for priority
        # and `item` as a positive factor.
        # How about prioritizing bins with smaller remaining capacity that can still fit the item?
        # This aligns with the First Fit Decreasing heuristic's goal of filling bins.
        
        # Let's map the difference `valid_capacities - item` to a priority.
        # Smaller difference should yield higher priority.
        
        # Example: item = 3, capacities = [5, 7, 10]
        # Valid capacities = [5, 7, 10]
        # Differences = [2, 4, 7]
        # We want to prioritize bins with difference 2, then 4, then 7.
        # So, 1/2, 1/4, 1/7 would work.
        
        diffs = valid_capacities - item
        priorities = 1.0 / (diffs + epsilon)
        
        # Now, to make it more "Softmax-like" if we were to select probabilistically,
        # we can exponentiate and normalize. But for direct priority score, this is fine.
        # Let's add a small value to all priorities to avoid negative exponents in a Softmax if we were to use it.
        # And let's scale them to prevent numerical underflow or overflow with Softmax.
        
        # For a direct priority score where higher means better, 
        # this inverse difference works well for "best fit" aspect.
        
        # Consider what happens if multiple bins have the exact same "best fit" difference.
        # The current approach would give them equal priority.
        
        # To incorporate the "Softmax-Based Fit" idea, let's interpret it as:
        # transform the "fitness" of a bin (how well it fits the item) into a priority.
        # The fitness can be related to how close `remaining_capacity` is to `item`.
        
        # Let's define fitness as: -(remaining_capacity - item)^2. Higher fitness for smaller squared difference.
        # Or, more simply, as we did: 1.0 / (remaining_capacity - item + epsilon)
        
        # Softmax transformation of these scores to get a distribution if needed.
        # For now, we just need the scores themselves.
        
        # Let's try to directly use the remaining capacity for scaling, 
        # encouraging smaller capacities that fit.
        
        # Prioritize bins with the smallest remaining capacity that can fit the item.
        # So, the priority score should be higher for smaller `valid_capacities`.
        # Let's try `1.0 / valid_capacities`.
        
        # Consider a case: item = 2, bins_remain_cap = [3, 5, 10]
        # Valid bins = [3, 5, 10]
        # Option A (inverse diff): 1/(3-2)=1, 1/(5-2)=0.33, 1/(10-2)=0.125. Prioritizes bin with 3. (Best Fit)
        # Option B (inverse capacity): 1/3=0.33, 1/5=0.2, 1/10=0.1. Prioritizes bin with 3.
        
        # If the goal is "smallest number of bins", then fitting into a nearly full bin is good.
        # "Best Fit" heuristic is good for this.
        
        # Let's combine the "fit" (difference) with the "emptiness" (remaining capacity).
        # Maybe penalize very large remaining capacities, even if they fit.
        
        # Let's use the difference again, as it directly measures "how much space is left after fitting".
        # Smaller difference is better.
        
        diffs = valid_capacities - item
        
        # Scale diffs to be more in line with Softmax inputs (e.g., range -inf to +inf for exp)
        # A common pattern is to use `exp(value)` where larger `value` is better.
        # We want to maximize `-(diffs)`. So `exp(-diffs)`? No, we want to maximize score for smaller diffs.
        # Let's use `exp(-diffs)` with `temperature`.
        
        temperature = 0.5 # Lower temperature means stronger preference for best fit
        scaled_diffs = -diffs / temperature
        
        # Apply Softmax concept: exp(score) / sum(exp(scores))
        # We can simply return exp(scaled_diffs) as the priority, which is proportional to softmax output.
        
        priorities = np.exp(scaled_diffs)
        
    
    final_priorities = np.zeros_like(bins_remain_cap)
    final_priorities[valid_bins] = priorities
    
    return final_priorities
```
