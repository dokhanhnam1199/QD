{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A Softmax-based priority function for the online Bin Packing Problem.\n\n    This function calculates the priority of placing an item into each available bin.\n    It considers the remaining capacity of each bin relative to the item size.\n    Bins that can accommodate the item without exceeding their capacity are favored.\n    Among the bins that can accommodate the item, those with less remaining capacity\n    (i.e., tighter fits) are given a higher priority, encouraging fuller bins first.\n    The Softmax function is used to convert these relative preferences into a\n    probability distribution, ensuring that higher priority bins have a greater chance\n    of being selected.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the remaining\n                         capacity of a bin.\n\n    Returns:\n        A numpy array of the same shape as bins_remain_cap, where each element\n        is the priority score for placing the item into the corresponding bin.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    eligible_capacities = bins_remain_cap[eligible_bins_mask]\n\n    if eligible_capacities.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit\" score for eligible bins. A smaller remaining capacity\n    # (tighter fit) results in a higher score. We use the negative difference\n    # to make larger remaining capacities (less good fits) have smaller scores.\n    fit_scores = -(eligible_capacities - item)\n\n    # Apply Softmax to get probabilities (priorities).\n    # Adding a small epsilon to avoid log(0) issues if fit_scores can be zero.\n    epsilon = 1e-9\n    exp_scores = np.exp(fit_scores - np.max(fit_scores)) # Stability trick for softmax\n    priorities = exp_scores / np.sum(exp_scores)\n\n    # Map priorities back to the original bin structure\n    full_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    full_priorities[eligible_bins_mask] = priorities\n\n    return full_priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit) strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    for i, remaining_cap in enumerate(bins_remain_cap):\n        if remaining_cap >= item:\n            \n            proximity = remaining_cap - item\n            \n            if proximity == 0:\n                priorities[i] = float('inf') \n            else:\n                priorities[i] = 1.0 / proximity\n                \n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the difference between the bin's remaining capacity and the item's size.\n    # A smaller difference means the item fits \"better\" or closer to the bin's capacity.\n    # We add a small epsilon to avoid division by zero if a bin is perfectly full or the item size is 0.\n    diffs = bins_remain_cap - item\n    priorities = 1.0 / (np.abs(diffs) + 1e-9)\n\n    # We want to prioritize bins that can actually fit the item.\n    # If an item cannot fit, its priority should be very low.\n    # We can achieve this by multiplying the inverse difference by a mask\n    # that is 1 for bins that can fit the item and 0 otherwise.\n    can_fit_mask = (bins_remain_cap >= item).astype(float)\n    priorities *= can_fit_mask\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A Softmax-based priority function for the online Bin Packing Problem.\n\n    This function calculates the priority of placing an item into each available bin.\n    It considers the remaining capacity of each bin relative to the item size.\n    Bins that can accommodate the item without exceeding their capacity are favored.\n    Among the bins that can accommodate the item, those with less remaining capacity\n    (i.e., tighter fits) are given a higher priority, encouraging fuller bins first.\n    The Softmax function is used to convert these relative preferences into a\n    probability distribution, ensuring that higher priority bins have a greater chance\n    of being selected.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the remaining\n                         capacity of a bin.\n\n    Returns:\n        A numpy array of the same shape as bins_remain_cap, where each element\n        is the priority score for placing the item into the corresponding bin.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    eligible_capacities = bins_remain_cap[eligible_bins_mask]\n\n    if eligible_capacities.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit\" score for eligible bins. A smaller remaining capacity\n    # (tighter fit) results in a higher score. We use the negative difference\n    # to make larger remaining capacities (less good fits) have smaller scores.\n    fit_scores = -(eligible_capacities - item)\n\n    # Apply Softmax to get probabilities (priorities).\n    # Adding a small epsilon to avoid log(0) issues if fit_scores can be zero.\n    epsilon = 1e-9\n    exp_scores = np.exp(fit_scores - np.max(fit_scores)) # Stability trick for softmax\n    priorities = exp_scores / np.sum(exp_scores)\n\n    # Map priorities back to the original bin structure\n    full_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    full_priorities[eligible_bins_mask] = priorities\n\n    return full_priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a First Fit strategy variant.\n\n    The priority is higher for bins that can accommodate the item and have a remaining\n    capacity closer to the item's size. This encourages tighter packing.\n    Bins that cannot accommodate the item are given a priority of -1.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_remain_cap.size > 0:\n        differences = fitting_bins_remain_cap - item\n        # Higher priority for smaller differences (tighter fit)\n        # We use -differences to make smaller differences result in higher scores.\n        # Adding a small constant to avoid zero priorities for perfect fits\n        # and to ensure valid bins have a positive priority.\n        priorities[can_fit_mask] = -differences + 1.0\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    if suitable_bins_caps.size > 0:\n        differences = suitable_bins_caps - item\n        min_diff = np.min(differences)\n        \n        suitable_bin_indices = np.where(suitable_bins_mask)[0]\n        \n        for i, original_index in enumerate(suitable_bin_indices):\n            if bins_remain_cap[original_index] - item == min_diff:\n                priorities[original_index] = 1.0\n            else:\n                priorities[original_index] = 0.0\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    if suitable_bins_caps.size > 0:\n        differences = suitable_bins_caps - item\n        min_diff = np.min(differences)\n        \n        suitable_bin_indices = np.where(suitable_bins_mask)[0]\n        \n        for i, original_index in enumerate(suitable_bin_indices):\n            if bins_remain_cap[original_index] - item == min_diff:\n                priorities[original_index] = 1.0\n            else:\n                priorities[original_index] = 0.0\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit) strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    for i, remaining_cap in enumerate(bins_remain_cap):\n        if remaining_cap >= item:\n            \n            proximity = remaining_cap - item\n            \n            if proximity == 0:\n                priorities[i] = float('inf') \n            else:\n                priorities[i] = 1.0 / proximity\n                \n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Epsilon-Greedy strategy for Online Bin Packing Problem.\n\n    Prioritizes bins that can fit the item and have the least remaining capacity\n    to minimize wasted space. With a small probability (epsilon), it chooses a\n    random bin to explore less optimal but potentially better future placements.\n    \"\"\"\n    epsilon = 0.1  # Probability of choosing a random bin\n    \n    possible_bins = bins_remain_cap >= item\n    \n    if not np.any(possible_bins):\n        return np.zeros_like(bins_remain_cap)\n        \n    priorities = np.zeros_like(bins_remain_cap)\n    \n    if np.random.rand() < epsilon:\n        # Explore: choose a random bin that can fit the item\n        candidate_indices = np.where(possible_bins)[0]\n        chosen_index = np.random.choice(candidate_indices)\n        priorities[chosen_index] = 1.0\n    else:\n        # Exploit: choose the bin with the least remaining capacity that fits the item\n        bins_to_consider = bins_remain_cap[possible_bins]\n        remaining_capacity_for_valid_bins = bins_to_consider - item\n        \n        best_bin_relative_index = np.argmin(remaining_capacity_for_valid_bins)\n        \n        valid_bin_indices = np.where(possible_bins)[0]\n        best_bin_absolute_index = valid_bin_indices[best_bin_relative_index]\n        \n        priorities[best_bin_absolute_index] = 1.0\n        \n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Epsilon-Greedy strategy for Online Bin Packing Problem.\n\n    Prioritizes bins that can fit the item and have the least remaining capacity\n    to minimize wasted space. With a small probability (epsilon), it chooses a\n    random bin to explore less optimal but potentially better future placements.\n    \"\"\"\n    epsilon = 0.1  # Probability of choosing a random bin\n    \n    possible_bins = bins_remain_cap >= item\n    \n    if not np.any(possible_bins):\n        return np.zeros_like(bins_remain_cap)\n        \n    priorities = np.zeros_like(bins_remain_cap)\n    \n    if np.random.rand() < epsilon:\n        # Explore: choose a random bin that can fit the item\n        candidate_indices = np.where(possible_bins)[0]\n        chosen_index = np.random.choice(candidate_indices)\n        priorities[chosen_index] = 1.0\n    else:\n        # Exploit: choose the bin with the least remaining capacity that fits the item\n        bins_to_consider = bins_remain_cap[possible_bins]\n        remaining_capacity_for_valid_bins = bins_to_consider - item\n        \n        best_bin_relative_index = np.argmin(remaining_capacity_for_valid_bins)\n        \n        valid_bin_indices = np.where(possible_bins)[0]\n        best_bin_absolute_index = valid_bin_indices[best_bin_relative_index]\n        \n        priorities[best_bin_absolute_index] = 1.0\n        \n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i, cap in enumerate(bins_remain_cap):\n        if cap >= item:\n            priorities[i] = 1 / (cap - item + 1e-9)\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    valid_bins = bins_remain_cap >= item\n    \n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n\n    valid_capacities = bins_remain_cap[valid_bins]\n    \n    # Higher remaining capacity means lower priority (we want to fill bins)\n    # To use Softmax effectively, we want larger values to correspond to higher priority.\n    # So we can use (large_capacity - remaining_capacity) or 1/(remaining_capacity)\n    # Let's try 1/(remaining_capacity) as it penalizes bins that are already very full.\n    \n    inverted_capacities = 1.0 / valid_capacities\n    \n    # To further encourage fitting into bins with *just enough* space, \n    # we can add a term that penalizes bins with very large remaining capacity.\n    # Let's try subtracting the ratio of remaining capacity to total capacity (assuming initial bin capacity is known or can be estimated).\n    # For simplicity here, let's just use the inverse capacity.\n    \n    # Adding a small epsilon to avoid division by zero if a bin has 0 remaining capacity, though valid_bins should prevent this.\n    epsilon = 1e-9\n    scores = 1.0 / (valid_capacities + epsilon)\n    \n    # Softmax is often used to turn scores into probabilities or weights.\n    # A higher score should mean a higher probability of selection.\n    # Let's scale the scores to be positive and somewhat related to \"how well\" it fits.\n    # A common approach in fitting is to maximize the remaining capacity, \n    # but here we want to minimize the number of bins. So we prefer bins that are *almost* full.\n    # Let's try prioritizing bins where item fits snugly.\n    \n    fit_difference = valid_capacities - item\n    # We want to minimize fit_difference. To make it a priority (higher is better), we invert it.\n    # Add a small constant to avoid division by zero if fit_difference is 0.\n    priority_scores = 1.0 / (fit_difference + epsilon)\n    \n    # Apply Softmax to convert scores into probabilities (weights)\n    # We add a small penalty for bins that have much more capacity than needed to discourage very loose fits.\n    # Let's consider the \"waste\" factor. Waste = remaining_capacity - item\n    # We want to minimize waste.\n    \n    # Let's try a heuristic that favors bins that have enough capacity but not excessively more.\n    # We can try a value that increases as remaining_capacity gets closer to item.\n    \n    # Option 1: Prioritize bins that are almost full (minimum remaining capacity that fits item)\n    # We want higher scores for smaller `valid_capacities`. So `1/valid_capacities` or similar.\n    # To be more specific, we want `valid_capacities - item` to be small.\n    # So we can use `1.0 / (valid_capacities - item + epsilon)`\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bins] = 1.0 / (valid_capacities - item + epsilon)\n    \n    # Apply Softmax: exp(score) / sum(exp(scores))\n    # For just returning priority scores for selection, we can directly use the calculated scores\n    # or apply a transformation like Softmax.\n    # If we want to select *one* bin based on highest priority, we can just return the scores directly.\n    # If we want to weight bins for some probabilistic selection, Softmax is good.\n    # For this problem, simply returning the scores that indicate preference is sufficient.\n    \n    # Let's refine to prioritize bins where remaining_capacity - item is minimized.\n    # A simple approach for priority is the inverse of the difference.\n    \n    scores = np.zeros_like(bins_remain_cap)\n    scores[valid_bins] = 1.0 / (valid_capacities - item + epsilon)\n\n    # To make it more \"Softmax-like\" in spirit of distribution, \n    # we can use a sigmoid-like transformation or directly use scaled values.\n    # Let's consider a temperature parameter to control the \"sharpness\" of priorities.\n    temperature = 1.0\n    \n    # Let's make values larger for better fits.\n    # A potential issue is if all valid capacities are very large, leading to small inverse values.\n    # We need to ensure scores are somewhat comparable or scaled.\n    \n    # Consider the \"tightness of fit\" as the primary driver.\n    # Tightest fit = smallest (remaining_capacity - item).\n    # So, priority is inversely proportional to (remaining_capacity - item).\n    \n    normalized_scores = np.zeros_like(bins_remain_cap)\n    if np.any(valid_bins):\n        # Calculate scores: higher score for tighter fit\n        # We want to maximize (1 / (remaining_capacity - item))\n        # Or to avoid issues with very small differences, maybe prioritize directly by minimum remaining capacity that fits.\n        \n        # Let's try a direct mapping:\n        # A bin is \"good\" if remaining_capacity is just enough.\n        # So, priority is high when remaining_capacity is close to item.\n        \n        # Let's use `remaining_capacity` itself as a negative factor for priority\n        # and `item` as a positive factor.\n        # How about prioritizing bins with smaller remaining capacity that can still fit the item?\n        # This aligns with the First Fit Decreasing heuristic's goal of filling bins.\n        \n        # Let's map the difference `valid_capacities - item` to a priority.\n        # Smaller difference should yield higher priority.\n        \n        # Example: item = 3, capacities = [5, 7, 10]\n        # Valid capacities = [5, 7, 10]\n        # Differences = [2, 4, 7]\n        # We want to prioritize bins with difference 2, then 4, then 7.\n        # So, 1/2, 1/4, 1/7 would work.\n        \n        diffs = valid_capacities - item\n        priorities = 1.0 / (diffs + epsilon)\n        \n        # Now, to make it more \"Softmax-like\" if we were to select probabilistically,\n        # we can exponentiate and normalize. But for direct priority score, this is fine.\n        # Let's add a small value to all priorities to avoid negative exponents in a Softmax if we were to use it.\n        # And let's scale them to prevent numerical underflow or overflow with Softmax.\n        \n        # For a direct priority score where higher means better, \n        # this inverse difference works well for \"best fit\" aspect.\n        \n        # Consider what happens if multiple bins have the exact same \"best fit\" difference.\n        # The current approach would give them equal priority.\n        \n        # To incorporate the \"Softmax-Based Fit\" idea, let's interpret it as:\n        # transform the \"fitness\" of a bin (how well it fits the item) into a priority.\n        # The fitness can be related to how close `remaining_capacity` is to `item`.\n        \n        # Let's define fitness as: -(remaining_capacity - item)^2. Higher fitness for smaller squared difference.\n        # Or, more simply, as we did: 1.0 / (remaining_capacity - item + epsilon)\n        \n        # Softmax transformation of these scores to get a distribution if needed.\n        # For now, we just need the scores themselves.\n        \n        # Let's try to directly use the remaining capacity for scaling, \n        # encouraging smaller capacities that fit.\n        \n        # Prioritize bins with the smallest remaining capacity that can fit the item.\n        # So, the priority score should be higher for smaller `valid_capacities`.\n        # Let's try `1.0 / valid_capacities`.\n        \n        # Consider a case: item = 2, bins_remain_cap = [3, 5, 10]\n        # Valid bins = [3, 5, 10]\n        # Option A (inverse diff): 1/(3-2)=1, 1/(5-2)=0.33, 1/(10-2)=0.125. Prioritizes bin with 3. (Best Fit)\n        # Option B (inverse capacity): 1/3=0.33, 1/5=0.2, 1/10=0.1. Prioritizes bin with 3.\n        \n        # If the goal is \"smallest number of bins\", then fitting into a nearly full bin is good.\n        # \"Best Fit\" heuristic is good for this.\n        \n        # Let's combine the \"fit\" (difference) with the \"emptiness\" (remaining capacity).\n        # Maybe penalize very large remaining capacities, even if they fit.\n        \n        # Let's use the difference again, as it directly measures \"how much space is left after fitting\".\n        # Smaller difference is better.\n        \n        diffs = valid_capacities - item\n        \n        # Scale diffs to be more in line with Softmax inputs (e.g., range -inf to +inf for exp)\n        # A common pattern is to use `exp(value)` where larger `value` is better.\n        # We want to maximize `-(diffs)`. So `exp(-diffs)`? No, we want to maximize score for smaller diffs.\n        # Let's use `exp(-diffs)` with `temperature`.\n        \n        temperature = 0.5 # Lower temperature means stronger preference for best fit\n        scaled_diffs = -diffs / temperature\n        \n        # Apply Softmax concept: exp(score) / sum(exp(scores))\n        # We can simply return exp(scaled_diffs) as the priority, which is proportional to softmax output.\n        \n        priorities = np.exp(scaled_diffs)\n        \n    \n    final_priorities = np.zeros_like(bins_remain_cap)\n    final_priorities[valid_bins] = priorities\n    \n    return final_priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    valid_bins = bins_remain_cap >= item\n    \n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n\n    valid_capacities = bins_remain_cap[valid_bins]\n    \n    # Higher remaining capacity means lower priority (we want to fill bins)\n    # To use Softmax effectively, we want larger values to correspond to higher priority.\n    # So we can use (large_capacity - remaining_capacity) or 1/(remaining_capacity)\n    # Let's try 1/(remaining_capacity) as it penalizes bins that are already very full.\n    \n    inverted_capacities = 1.0 / valid_capacities\n    \n    # To further encourage fitting into bins with *just enough* space, \n    # we can add a term that penalizes bins with very large remaining capacity.\n    # Let's try subtracting the ratio of remaining capacity to total capacity (assuming initial bin capacity is known or can be estimated).\n    # For simplicity here, let's just use the inverse capacity.\n    \n    # Adding a small epsilon to avoid division by zero if a bin has 0 remaining capacity, though valid_bins should prevent this.\n    epsilon = 1e-9\n    scores = 1.0 / (valid_capacities + epsilon)\n    \n    # Softmax is often used to turn scores into probabilities or weights.\n    # A higher score should mean a higher probability of selection.\n    # Let's scale the scores to be positive and somewhat related to \"how well\" it fits.\n    # A common approach in fitting is to maximize the remaining capacity, \n    # but here we want to minimize the number of bins. So we prefer bins that are *almost* full.\n    # Let's try prioritizing bins where item fits snugly.\n    \n    fit_difference = valid_capacities - item\n    # We want to minimize fit_difference. To make it a priority (higher is better), we invert it.\n    # Add a small constant to avoid division by zero if fit_difference is 0.\n    priority_scores = 1.0 / (fit_difference + epsilon)\n    \n    # Apply Softmax to convert scores into probabilities (weights)\n    # We add a small penalty for bins that have much more capacity than needed to discourage very loose fits.\n    # Let's consider the \"waste\" factor. Waste = remaining_capacity - item\n    # We want to minimize waste.\n    \n    # Let's try a heuristic that favors bins that have enough capacity but not excessively more.\n    # We can try a value that increases as remaining_capacity gets closer to item.\n    \n    # Option 1: Prioritize bins that are almost full (minimum remaining capacity that fits item)\n    # We want higher scores for smaller `valid_capacities`. So `1/valid_capacities` or similar.\n    # To be more specific, we want `valid_capacities - item` to be small.\n    # So we can use `1.0 / (valid_capacities - item + epsilon)`\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bins] = 1.0 / (valid_capacities - item + epsilon)\n    \n    # Apply Softmax: exp(score) / sum(exp(scores))\n    # For just returning priority scores for selection, we can directly use the calculated scores\n    # or apply a transformation like Softmax.\n    # If we want to select *one* bin based on highest priority, we can just return the scores directly.\n    # If we want to weight bins for some probabilistic selection, Softmax is good.\n    # For this problem, simply returning the scores that indicate preference is sufficient.\n    \n    # Let's refine to prioritize bins where remaining_capacity - item is minimized.\n    # A simple approach for priority is the inverse of the difference.\n    \n    scores = np.zeros_like(bins_remain_cap)\n    scores[valid_bins] = 1.0 / (valid_capacities - item + epsilon)\n\n    # To make it more \"Softmax-like\" in spirit of distribution, \n    # we can use a sigmoid-like transformation or directly use scaled values.\n    # Let's consider a temperature parameter to control the \"sharpness\" of priorities.\n    temperature = 1.0\n    \n    # Let's make values larger for better fits.\n    # A potential issue is if all valid capacities are very large, leading to small inverse values.\n    # We need to ensure scores are somewhat comparable or scaled.\n    \n    # Consider the \"tightness of fit\" as the primary driver.\n    # Tightest fit = smallest (remaining_capacity - item).\n    # So, priority is inversely proportional to (remaining_capacity - item).\n    \n    normalized_scores = np.zeros_like(bins_remain_cap)\n    if np.any(valid_bins):\n        # Calculate scores: higher score for tighter fit\n        # We want to maximize (1 / (remaining_capacity - item))\n        # Or to avoid issues with very small differences, maybe prioritize directly by minimum remaining capacity that fits.\n        \n        # Let's try a direct mapping:\n        # A bin is \"good\" if remaining_capacity is just enough.\n        # So, priority is high when remaining_capacity is close to item.\n        \n        # Let's use `remaining_capacity` itself as a negative factor for priority\n        # and `item` as a positive factor.\n        # How about prioritizing bins with smaller remaining capacity that can still fit the item?\n        # This aligns with the First Fit Decreasing heuristic's goal of filling bins.\n        \n        # Let's map the difference `valid_capacities - item` to a priority.\n        # Smaller difference should yield higher priority.\n        \n        # Example: item = 3, capacities = [5, 7, 10]\n        # Valid capacities = [5, 7, 10]\n        # Differences = [2, 4, 7]\n        # We want to prioritize bins with difference 2, then 4, then 7.\n        # So, 1/2, 1/4, 1/7 would work.\n        \n        diffs = valid_capacities - item\n        priorities = 1.0 / (diffs + epsilon)\n        \n        # Now, to make it more \"Softmax-like\" if we were to select probabilistically,\n        # we can exponentiate and normalize. But for direct priority score, this is fine.\n        # Let's add a small value to all priorities to avoid negative exponents in a Softmax if we were to use it.\n        # And let's scale them to prevent numerical underflow or overflow with Softmax.\n        \n        # For a direct priority score where higher means better, \n        # this inverse difference works well for \"best fit\" aspect.\n        \n        # Consider what happens if multiple bins have the exact same \"best fit\" difference.\n        # The current approach would give them equal priority.\n        \n        # To incorporate the \"Softmax-Based Fit\" idea, let's interpret it as:\n        # transform the \"fitness\" of a bin (how well it fits the item) into a priority.\n        # The fitness can be related to how close `remaining_capacity` is to `item`.\n        \n        # Let's define fitness as: -(remaining_capacity - item)^2. Higher fitness for smaller squared difference.\n        # Or, more simply, as we did: 1.0 / (remaining_capacity - item + epsilon)\n        \n        # Softmax transformation of these scores to get a distribution if needed.\n        # For now, we just need the scores themselves.\n        \n        # Let's try to directly use the remaining capacity for scaling, \n        # encouraging smaller capacities that fit.\n        \n        # Prioritize bins with the smallest remaining capacity that can fit the item.\n        # So, the priority score should be higher for smaller `valid_capacities`.\n        # Let's try `1.0 / valid_capacities`.\n        \n        # Consider a case: item = 2, bins_remain_cap = [3, 5, 10]\n        # Valid bins = [3, 5, 10]\n        # Option A (inverse diff): 1/(3-2)=1, 1/(5-2)=0.33, 1/(10-2)=0.125. Prioritizes bin with 3. (Best Fit)\n        # Option B (inverse capacity): 1/3=0.33, 1/5=0.2, 1/10=0.1. Prioritizes bin with 3.\n        \n        # If the goal is \"smallest number of bins\", then fitting into a nearly full bin is good.\n        # \"Best Fit\" heuristic is good for this.\n        \n        # Let's combine the \"fit\" (difference) with the \"emptiness\" (remaining capacity).\n        # Maybe penalize very large remaining capacities, even if they fit.\n        \n        # Let's use the difference again, as it directly measures \"how much space is left after fitting\".\n        # Smaller difference is better.\n        \n        diffs = valid_capacities - item\n        \n        # Scale diffs to be more in line with Softmax inputs (e.g., range -inf to +inf for exp)\n        # A common pattern is to use `exp(value)` where larger `value` is better.\n        # We want to maximize `-(diffs)`. So `exp(-diffs)`? No, we want to maximize score for smaller diffs.\n        # Let's use `exp(-diffs)` with `temperature`.\n        \n        temperature = 0.5 # Lower temperature means stronger preference for best fit\n        scaled_diffs = -diffs / temperature\n        \n        # Apply Softmax concept: exp(score) / sum(exp(scores))\n        # We can simply return exp(scaled_diffs) as the priority, which is proportional to softmax output.\n        \n        priorities = np.exp(scaled_diffs)\n        \n    \n    final_priorities = np.zeros_like(bins_remain_cap)\n    final_priorities[valid_bins] = priorities\n    \n    return final_priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i, cap in enumerate(bins_remain_cap):\n        if cap >= item:\n            priorities[i] = 1 / (cap - item + 1e-9)\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    valid_bins = bins_remain_cap >= item\n    \n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n\n    valid_capacities = bins_remain_cap[valid_bins]\n    \n    # Higher remaining capacity means lower priority (we want to fill bins)\n    # To use Softmax effectively, we want larger values to correspond to higher priority.\n    # So we can use (large_capacity - remaining_capacity) or 1/(remaining_capacity)\n    # Let's try 1/(remaining_capacity) as it penalizes bins that are already very full.\n    \n    inverted_capacities = 1.0 / valid_capacities\n    \n    # To further encourage fitting into bins with *just enough* space, \n    # we can add a term that penalizes bins with very large remaining capacity.\n    # Let's try subtracting the ratio of remaining capacity to total capacity (assuming initial bin capacity is known or can be estimated).\n    # For simplicity here, let's just use the inverse capacity.\n    \n    # Adding a small epsilon to avoid division by zero if a bin has 0 remaining capacity, though valid_bins should prevent this.\n    epsilon = 1e-9\n    scores = 1.0 / (valid_capacities + epsilon)\n    \n    # Softmax is often used to turn scores into probabilities or weights.\n    # A higher score should mean a higher probability of selection.\n    # Let's scale the scores to be positive and somewhat related to \"how well\" it fits.\n    # A common approach in fitting is to maximize the remaining capacity, \n    # but here we want to minimize the number of bins. So we prefer bins that are *almost* full.\n    # Let's try prioritizing bins where item fits snugly.\n    \n    fit_difference = valid_capacities - item\n    # We want to minimize fit_difference. To make it a priority (higher is better), we invert it.\n    # Add a small constant to avoid division by zero if fit_difference is 0.\n    priority_scores = 1.0 / (fit_difference + epsilon)\n    \n    # Apply Softmax to convert scores into probabilities (weights)\n    # We add a small penalty for bins that have much more capacity than needed to discourage very loose fits.\n    # Let's consider the \"waste\" factor. Waste = remaining_capacity - item\n    # We want to minimize waste.\n    \n    # Let's try a heuristic that favors bins that have enough capacity but not excessively more.\n    # We can try a value that increases as remaining_capacity gets closer to item.\n    \n    # Option 1: Prioritize bins that are almost full (minimum remaining capacity that fits item)\n    # We want higher scores for smaller `valid_capacities`. So `1/valid_capacities` or similar.\n    # To be more specific, we want `valid_capacities - item` to be small.\n    # So we can use `1.0 / (valid_capacities - item + epsilon)`\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bins] = 1.0 / (valid_capacities - item + epsilon)\n    \n    # Apply Softmax: exp(score) / sum(exp(scores))\n    # For just returning priority scores for selection, we can directly use the calculated scores\n    # or apply a transformation like Softmax.\n    # If we want to select *one* bin based on highest priority, we can just return the scores directly.\n    # If we want to weight bins for some probabilistic selection, Softmax is good.\n    # For this problem, simply returning the scores that indicate preference is sufficient.\n    \n    # Let's refine to prioritize bins where remaining_capacity - item is minimized.\n    # A simple approach for priority is the inverse of the difference.\n    \n    scores = np.zeros_like(bins_remain_cap)\n    scores[valid_bins] = 1.0 / (valid_capacities - item + epsilon)\n\n    # To make it more \"Softmax-like\" in spirit of distribution, \n    # we can use a sigmoid-like transformation or directly use scaled values.\n    # Let's consider a temperature parameter to control the \"sharpness\" of priorities.\n    temperature = 1.0\n    \n    # Let's make values larger for better fits.\n    # A potential issue is if all valid capacities are very large, leading to small inverse values.\n    # We need to ensure scores are somewhat comparable or scaled.\n    \n    # Consider the \"tightness of fit\" as the primary driver.\n    # Tightest fit = smallest (remaining_capacity - item).\n    # So, priority is inversely proportional to (remaining_capacity - item).\n    \n    normalized_scores = np.zeros_like(bins_remain_cap)\n    if np.any(valid_bins):\n        # Calculate scores: higher score for tighter fit\n        # We want to maximize (1 / (remaining_capacity - item))\n        # Or to avoid issues with very small differences, maybe prioritize directly by minimum remaining capacity that fits.\n        \n        # Let's try a direct mapping:\n        # A bin is \"good\" if remaining_capacity is just enough.\n        # So, priority is high when remaining_capacity is close to item.\n        \n        # Let's use `remaining_capacity` itself as a negative factor for priority\n        # and `item` as a positive factor.\n        # How about prioritizing bins with smaller remaining capacity that can still fit the item?\n        # This aligns with the First Fit Decreasing heuristic's goal of filling bins.\n        \n        # Let's map the difference `valid_capacities - item` to a priority.\n        # Smaller difference should yield higher priority.\n        \n        # Example: item = 3, capacities = [5, 7, 10]\n        # Valid capacities = [5, 7, 10]\n        # Differences = [2, 4, 7]\n        # We want to prioritize bins with difference 2, then 4, then 7.\n        # So, 1/2, 1/4, 1/7 would work.\n        \n        diffs = valid_capacities - item\n        priorities = 1.0 / (diffs + epsilon)\n        \n        # Now, to make it more \"Softmax-like\" if we were to select probabilistically,\n        # we can exponentiate and normalize. But for direct priority score, this is fine.\n        # Let's add a small value to all priorities to avoid negative exponents in a Softmax if we were to use it.\n        # And let's scale them to prevent numerical underflow or overflow with Softmax.\n        \n        # For a direct priority score where higher means better, \n        # this inverse difference works well for \"best fit\" aspect.\n        \n        # Consider what happens if multiple bins have the exact same \"best fit\" difference.\n        # The current approach would give them equal priority.\n        \n        # To incorporate the \"Softmax-Based Fit\" idea, let's interpret it as:\n        # transform the \"fitness\" of a bin (how well it fits the item) into a priority.\n        # The fitness can be related to how close `remaining_capacity` is to `item`.\n        \n        # Let's define fitness as: -(remaining_capacity - item)^2. Higher fitness for smaller squared difference.\n        # Or, more simply, as we did: 1.0 / (remaining_capacity - item + epsilon)\n        \n        # Softmax transformation of these scores to get a distribution if needed.\n        # For now, we just need the scores themselves.\n        \n        # Let's try to directly use the remaining capacity for scaling, \n        # encouraging smaller capacities that fit.\n        \n        # Prioritize bins with the smallest remaining capacity that can fit the item.\n        # So, the priority score should be higher for smaller `valid_capacities`.\n        # Let's try `1.0 / valid_capacities`.\n        \n        # Consider a case: item = 2, bins_remain_cap = [3, 5, 10]\n        # Valid bins = [3, 5, 10]\n        # Option A (inverse diff): 1/(3-2)=1, 1/(5-2)=0.33, 1/(10-2)=0.125. Prioritizes bin with 3. (Best Fit)\n        # Option B (inverse capacity): 1/3=0.33, 1/5=0.2, 1/10=0.1. Prioritizes bin with 3.\n        \n        # If the goal is \"smallest number of bins\", then fitting into a nearly full bin is good.\n        # \"Best Fit\" heuristic is good for this.\n        \n        # Let's combine the \"fit\" (difference) with the \"emptiness\" (remaining capacity).\n        # Maybe penalize very large remaining capacities, even if they fit.\n        \n        # Let's use the difference again, as it directly measures \"how much space is left after fitting\".\n        # Smaller difference is better.\n        \n        diffs = valid_capacities - item\n        \n        # Scale diffs to be more in line with Softmax inputs (e.g., range -inf to +inf for exp)\n        # A common pattern is to use `exp(value)` where larger `value` is better.\n        # We want to maximize `-(diffs)`. So `exp(-diffs)`? No, we want to maximize score for smaller diffs.\n        # Let's use `exp(-diffs)` with `temperature`.\n        \n        temperature = 0.5 # Lower temperature means stronger preference for best fit\n        scaled_diffs = -diffs / temperature\n        \n        # Apply Softmax concept: exp(score) / sum(exp(scores))\n        # We can simply return exp(scaled_diffs) as the priority, which is proportional to softmax output.\n        \n        priorities = np.exp(scaled_diffs)\n        \n    \n    final_priorities = np.zeros_like(bins_remain_cap)\n    final_priorities[valid_bins] = priorities\n    \n    return final_priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i, cap in enumerate(bins_remain_cap):\n        if cap >= item:\n            priorities[i] = 1 / (cap - item + 1e-9)\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    fitting_bins_mask = bins_remain_cap >= item\n    fitting_bins = bins_remain_cap[fitting_bins_mask]\n    if fitting_bins.size > 0:\n        differences = fitting_bins - item\n        best_fit_indices = np.where(bins_remain_cap == np.min(differences))[0]\n        priorities[best_fit_indices] = 1.0\n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    fitting_bins_mask = bins_remain_cap >= item\n    fitting_bins = bins_remain_cap[fitting_bins_mask]\n    if fitting_bins.size > 0:\n        differences = fitting_bins - item\n        best_fit_indices = np.where(bins_remain_cap == np.min(differences))[0]\n        priorities[best_fit_indices] = 1.0\n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    suitable_bins = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    if np.any(suitable_bins):\n        remaining_capacities_of_suitable_bins = bins_remain_cap[suitable_bins]\n        \n        gaps = remaining_capacities_of_suitable_bins - item\n        \n        normalized_gaps = gaps / np.max(remaining_capacities_of_suitable_bins)\n        \n        sigmoid_scores = 1 / (1 + np.exp(-10 * (normalized_gaps - 0.5)))\n        \n        priorities[suitable_bins] = sigmoid_scores\n        \n        \n        if np.all(priorities == 0):\n             priorities[suitable_bins] = 0.5\n    \n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    suitable_bins = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    if np.any(suitable_bins):\n        remaining_capacities_of_suitable_bins = bins_remain_cap[suitable_bins]\n        \n        gaps = remaining_capacities_of_suitable_bins - item\n        \n        normalized_gaps = gaps / np.max(remaining_capacities_of_suitable_bins)\n        \n        sigmoid_scores = 1 / (1 + np.exp(-10 * (normalized_gaps - 0.5)))\n        \n        priorities[suitable_bins] = sigmoid_scores\n        \n        \n        if np.all(priorities == 0):\n             priorities[suitable_bins] = 0.5\n    \n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}