[
  {
    "stdout_filepath": "problem_iter1_response17.txt_stdout.txt",
    "code_path": "problem_iter1_code17.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    This is a heuristic for the online bin packing problem.\n    We want to prioritize bins that are \"almost full\" but can still fit the item.\n    This strategy aims to fill bins as much as possible before opening new ones.\n\n    The priority is calculated as follows:\n    1. For bins that can fit the item:\n       - Calculate how much \"space\" is left after fitting the item.\n       - We want to *minimize* this leftover space, but we also don't want to penalize bins that are already very full.\n       - A good measure might be `(bin_capacity - item) / bin_capacity` if we're talking about proportions,\n         but we have remaining capacities. So, `remaining_capacity - item`.\n       - To get a \"priority\" where higher is better, we can take the negative of this difference: `item - remaining_capacity`.\n         This means a bin with `remaining_capacity = 0.9` and `item = 0.8` gives `-0.7` (higher is better than -0.2 for a bin with `remaining_capacity = 0.3`).\n       - To avoid very large negative numbers for bins that are too small, we can set the priority to a very low number (or zero) if the item doesn't fit.\n\n    2. For bins that cannot fit the item:\n       - Assign a very low priority (e.g., 0 or negative infinity effectively, but we'll use 0).\n\n    Let's refine this:\n    We want to put the item into a bin where the remaining capacity is *just enough* or slightly more than the item.\n    If remaining_capacity >= item:\n        Priority = some_function(remaining_capacity - item)\n    Else:\n        Priority = -infinity (effectively 0 for practical purposes if others are positive)\n\n    Consider the difference: `bins_remain_cap - item`.\n    If this difference is negative, the item doesn't fit. We'll assign a very low priority.\n    If this difference is non-negative, we want to prioritize bins where this difference is *smallest* (closest to zero).\n    So, we want to maximize `-(bins_remain_cap - item) = item - bins_remain_cap`.\n    This means if a bin has `rem_cap = 1.0` and `item = 0.5`, priority is `-0.5`.\n    If a bin has `rem_cap = 0.6` and `item = 0.5`, priority is `-0.1`. The latter is higher priority.\n\n    Let's make it simpler. We want the bin where `bins_remain_cap` is *closest to `item`*, but greater than or equal to `item`.\n    This is like finding the minimum of `bins_remain_cap - item` for all `bins_remain_cap >= item`.\n    To turn this into a \"priority\" (higher is better), we can use `-abs(bins_remain_cap - item)` or `item - bins_remain_cap`.\n    Let's stick with `item - bins_remain_cap`.\n\n    Priorities will be negative. Higher values mean a better fit.\n    We need to handle the case where `bins_remain_cap < item`.\n    \"\"\"\n    \n    fit_mask = bins_remain_cap >= item\n    \n    \n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    \n    priorities[fit_mask] = item - bins_remain_cap[fit_mask]\n    \n    \n    return priorities",
    "response_id": 17,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a First Fit strategy variant.\n\n    The priority is higher for bins that can accommodate the item and have a remaining\n    capacity closer to the item's size. This encourages tighter packing.\n    Bins that cannot accommodate the item are given a priority of -1.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_remain_cap.size > 0:\n        differences = fitting_bins_remain_cap - item\n        # Higher priority for smaller differences (tighter fit)\n        # We use -differences to make smaller differences result in higher scores.\n        # Adding a small constant to avoid zero priorities for perfect fits\n        # and to ensure valid bins have a positive priority.\n        priorities[can_fit_mask] = -differences + 1.0\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response4.txt_stdout.txt",
    "code_path": "problem_iter1_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i, cap in enumerate(bins_remain_cap):\n        if cap >= item:\n            priorities[i] = 1 / (cap - item + 1e-9)\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.198244914240141,
    "SLOC": 6.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response5.txt_stdout.txt",
    "code_path": "problem_iter1_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit) strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    for i, remaining_cap in enumerate(bins_remain_cap):\n        if remaining_cap >= item:\n            \n            proximity = remaining_cap - item\n            \n            if proximity == 0:\n                priorities[i] = float('inf') \n            else:\n                priorities[i] = 1.0 / proximity\n                \n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 10.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response6.txt_stdout.txt",
    "code_path": "problem_iter1_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    suitable_bins = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    if np.any(suitable_bins):\n        remaining_capacities_of_suitable_bins = bins_remain_cap[suitable_bins]\n        \n        gaps = remaining_capacities_of_suitable_bins - item\n        \n        normalized_gaps = gaps / np.max(remaining_capacities_of_suitable_bins)\n        \n        sigmoid_scores = 1 / (1 + np.exp(-10 * (normalized_gaps - 0.5)))\n        \n        priorities[suitable_bins] = sigmoid_scores\n        \n        \n        if np.all(priorities == 0):\n             priorities[suitable_bins] = 0.5\n    \n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 149.30195452732352,
    "SLOC": 12.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit and inverse proximity for Bin Packing priority.\n    Prioritizes bins that are closer fits, with a strong preference for the absolute best fit.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(eligible_bins_mask):\n        eligible_capacities = bins_remain_cap[eligible_bins_mask]\n        differences = eligible_capacities - item\n\n        # Strategy 1: Inverse proximity (favoring smaller remaining capacities)\n        # Add epsilon for numerical stability and to avoid division by zero.\n        inverse_proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Strategy 2: Identify the absolute best fit(s)\n        min_diff = np.min(differences)\n        best_fit_mask_local = (differences == min_diff)\n\n        # Combine strategies: Give a significantly higher priority to the absolute best fit(s)\n        # and use inverse proximity for others.\n        # We scale the best fit scores to be clearly dominant.\n        combined_scores = np.zeros_like(eligible_capacities)\n        combined_scores[best_fit_mask_local] = 100.0  # High priority for best fit\n        combined_scores[~best_fit_mask_local] = inverse_proximity_scores[~best_fit_mask_local]\n\n        # Normalize scores to be in a reasonable range, though not strictly probabilities here.\n        # Using Softmax-like scaling for non-best-fit items to maintain relative preference.\n        non_best_fit_scores = inverse_proximity_scores[~best_fit_mask_local]\n        if non_best_fit_scores.size > 0:\n            exp_scores = np.exp(non_best_fit_scores - np.max(non_best_fit_scores))\n            normalized_non_best_fit = exp_scores / np.sum(exp_scores)\n            combined_scores[~best_fit_mask_local] = normalized_non_best_fit\n\n        # Assign combined scores back to the original array structure\n        priorities[eligible_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 19.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A Softmax-based priority function for the online Bin Packing Problem.\n\n    This function calculates the priority of placing an item into each available bin.\n    It considers the remaining capacity of each bin relative to the item size.\n    Bins that can accommodate the item without exceeding their capacity are favored.\n    Among the bins that can accommodate the item, those with less remaining capacity\n    (i.e., tighter fits) are given a higher priority, encouraging fuller bins first.\n    The Softmax function is used to convert these relative preferences into a\n    probability distribution, ensuring that higher priority bins have a greater chance\n    of being selected.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the remaining\n                         capacity of a bin.\n\n    Returns:\n        A numpy array of the same shape as bins_remain_cap, where each element\n        is the priority score for placing the item into the corresponding bin.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    eligible_capacities = bins_remain_cap[eligible_bins_mask]\n\n    if eligible_capacities.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit\" score for eligible bins. A smaller remaining capacity\n    # (tighter fit) results in a higher score. We use the negative difference\n    # to make larger remaining capacities (less good fits) have smaller scores.\n    fit_scores = -(eligible_capacities - item)\n\n    # Apply Softmax to get probabilities (priorities).\n    # Adding a small epsilon to avoid log(0) issues if fit_scores can be zero.\n    epsilon = 1e-9\n    exp_scores = np.exp(fit_scores - np.max(fit_scores)) # Stability trick for softmax\n    priorities = exp_scores / np.sum(exp_scores)\n\n    # Map priorities back to the original bin structure\n    full_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    full_priorities[eligible_bins_mask] = priorities\n\n    return full_priorities",
    "response_id": 19,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response11.txt_stdout.txt",
    "code_path": "problem_iter1_code11.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    if suitable_bins_caps.size > 0:\n        differences = suitable_bins_caps - item\n        min_diff = np.min(differences)\n        \n        suitable_bin_indices = np.where(suitable_bins_mask)[0]\n        \n        for i, original_index in enumerate(suitable_bin_indices):\n            if bins_remain_cap[original_index] - item == min_diff:\n                priorities[original_index] = 1.0\n            else:\n                priorities[original_index] = 0.0\n    return priorities",
    "response_id": 11,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response8.txt_stdout.txt",
    "code_path": "problem_iter5_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (minimizing remaining capacity after packing)\n    with a penalty for bins that are excessively large, using an exponential scaling.\n    \"\"\"\n    fit_mask = bins_remain_cap >= item\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    epsilon = 1e-9\n    \n    if np.any(fit_mask):\n        eligible_capacities = bins_remain_cap[fit_mask]\n        \n        # Calculate the \"gap\" or remaining capacity after placing the item\n        gaps = eligible_capacities - item\n        \n        # Prioritize bins with smaller gaps (tighter fit).\n        # Use inverse of gap + epsilon for a higher score for smaller gaps.\n        # Also, apply a penalty for very large gaps by considering the inverse of capacity itself.\n        # A simple way to combine is to favor smaller gaps and penalize large capacities.\n        # Let's use the exponential of the negative gap to directly map smaller gaps to higher scores,\n        # similar to Softmax inputs, and tune with temperature.\n        \n        # Temperature parameter to control the \"aggressiveness\" of the heuristic.\n        # Lower temperature means stronger preference for the best fit.\n        temperature = 0.5 \n        \n        # Score is based on negative gap (higher score for smaller gap)\n        # Adding a small bonus for already fuller bins (lower eligible_capacities)\n        # A combined score could be: -gap - (1/eligible_capacity)\n        # For exponential scaling, let's use -gap directly, and the exponential will handle the grading.\n        # We can further adjust by considering the inverse of capacity.\n        # A common approach is to use `exp(-gap / T)`.\n        # To also favor fuller bins, we can add a term proportional to `1/capacity`.\n        # Let's try: `exp(-(gap - C * (1.0/eligible_capacity)) / T)` where C is a weight.\n        # For simplicity, let's focus on the gap primarily and use the exponential.\n        # The prompt also mentions penalizing very large remaining capacities.\n        # Let's try `exp(-gap)` and see how it behaves.\n        # To incorporate \"fullness preference\": maybe `exp(-(gap - alpha * (1/eligible_capacity)))`\n        \n        # A robust combination often seen is maximizing `-gap` and `1/capacity`.\n        # Let's try `exp( (-gaps - 1.0/eligible_capacities) / temperature )`\n        # This prioritizes small gaps and small capacities.\n        \n        # Simplified approach: Use negative gap scaled by temperature.\n        # This prioritizes \"best fit\" strongly.\n        scaled_scores = -gaps / temperature\n        \n        # Another approach from literature often used with Softmax: maximize `-(gap - C * (1/capacity))`\n        # Let's try `exp( -(gaps - 1.0/eligible_capacities) / temperature )`\n        # Where `1.0/eligible_capacities` gives higher score to fuller bins.\n        \n        # Let's select a more established approach: prioritize bins with minimal remaining capacity after packing (Best Fit),\n        # and use exponential scaling for graded priorities.\n        # This directly aligns with minimizing bin count.\n        \n        # Calculate priority based on the negative gap, scaled by temperature.\n        # Higher priority for smaller gaps (tighter fits).\n        priorities[fit_mask] = np.exp(-gaps / temperature)\n        \n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response15.txt_stdout.txt",
    "code_path": "problem_iter1_code15.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the difference between the bin's remaining capacity and the item's size.\n    # A smaller difference means the item fits \"better\" or closer to the bin's capacity.\n    # We add a small epsilon to avoid division by zero if a bin is perfectly full or the item size is 0.\n    diffs = bins_remain_cap - item\n    priorities = 1.0 / (np.abs(diffs) + 1e-9)\n\n    # We want to prioritize bins that can actually fit the item.\n    # If an item cannot fit, its priority should be very low.\n    # We can achieve this by multiplying the inverse difference by a mask\n    # that is 1 for bins that can fit the item and 0 otherwise.\n    can_fit_mask = (bins_remain_cap >= item).astype(float)\n    priorities *= can_fit_mask\n\n    return priorities",
    "response_id": 15,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response29.txt_stdout.txt",
    "code_path": "problem_iter1_code29.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    valid_bins = bins_remain_cap >= item\n    \n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n\n    valid_capacities = bins_remain_cap[valid_bins]\n    \n    # Higher remaining capacity means lower priority (we want to fill bins)\n    # To use Softmax effectively, we want larger values to correspond to higher priority.\n    # So we can use (large_capacity - remaining_capacity) or 1/(remaining_capacity)\n    # Let's try 1/(remaining_capacity) as it penalizes bins that are already very full.\n    \n    inverted_capacities = 1.0 / valid_capacities\n    \n    # To further encourage fitting into bins with *just enough* space, \n    # we can add a term that penalizes bins with very large remaining capacity.\n    # Let's try subtracting the ratio of remaining capacity to total capacity (assuming initial bin capacity is known or can be estimated).\n    # For simplicity here, let's just use the inverse capacity.\n    \n    # Adding a small epsilon to avoid division by zero if a bin has 0 remaining capacity, though valid_bins should prevent this.\n    epsilon = 1e-9\n    scores = 1.0 / (valid_capacities + epsilon)\n    \n    # Softmax is often used to turn scores into probabilities or weights.\n    # A higher score should mean a higher probability of selection.\n    # Let's scale the scores to be positive and somewhat related to \"how well\" it fits.\n    # A common approach in fitting is to maximize the remaining capacity, \n    # but here we want to minimize the number of bins. So we prefer bins that are *almost* full.\n    # Let's try prioritizing bins where item fits snugly.\n    \n    fit_difference = valid_capacities - item\n    # We want to minimize fit_difference. To make it a priority (higher is better), we invert it.\n    # Add a small constant to avoid division by zero if fit_difference is 0.\n    priority_scores = 1.0 / (fit_difference + epsilon)\n    \n    # Apply Softmax to convert scores into probabilities (weights)\n    # We add a small penalty for bins that have much more capacity than needed to discourage very loose fits.\n    # Let's consider the \"waste\" factor. Waste = remaining_capacity - item\n    # We want to minimize waste.\n    \n    # Let's try a heuristic that favors bins that have enough capacity but not excessively more.\n    # We can try a value that increases as remaining_capacity gets closer to item.\n    \n    # Option 1: Prioritize bins that are almost full (minimum remaining capacity that fits item)\n    # We want higher scores for smaller `valid_capacities`. So `1/valid_capacities` or similar.\n    # To be more specific, we want `valid_capacities - item` to be small.\n    # So we can use `1.0 / (valid_capacities - item + epsilon)`\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bins] = 1.0 / (valid_capacities - item + epsilon)\n    \n    # Apply Softmax: exp(score) / sum(exp(scores))\n    # For just returning priority scores for selection, we can directly use the calculated scores\n    # or apply a transformation like Softmax.\n    # If we want to select *one* bin based on highest priority, we can just return the scores directly.\n    # If we want to weight bins for some probabilistic selection, Softmax is good.\n    # For this problem, simply returning the scores that indicate preference is sufficient.\n    \n    # Let's refine to prioritize bins where remaining_capacity - item is minimized.\n    # A simple approach for priority is the inverse of the difference.\n    \n    scores = np.zeros_like(bins_remain_cap)\n    scores[valid_bins] = 1.0 / (valid_capacities - item + epsilon)\n\n    # To make it more \"Softmax-like\" in spirit of distribution, \n    # we can use a sigmoid-like transformation or directly use scaled values.\n    # Let's consider a temperature parameter to control the \"sharpness\" of priorities.\n    temperature = 1.0\n    \n    # Let's make values larger for better fits.\n    # A potential issue is if all valid capacities are very large, leading to small inverse values.\n    # We need to ensure scores are somewhat comparable or scaled.\n    \n    # Consider the \"tightness of fit\" as the primary driver.\n    # Tightest fit = smallest (remaining_capacity - item).\n    # So, priority is inversely proportional to (remaining_capacity - item).\n    \n    normalized_scores = np.zeros_like(bins_remain_cap)\n    if np.any(valid_bins):\n        # Calculate scores: higher score for tighter fit\n        # We want to maximize (1 / (remaining_capacity - item))\n        # Or to avoid issues with very small differences, maybe prioritize directly by minimum remaining capacity that fits.\n        \n        # Let's try a direct mapping:\n        # A bin is \"good\" if remaining_capacity is just enough.\n        # So, priority is high when remaining_capacity is close to item.\n        \n        # Let's use `remaining_capacity` itself as a negative factor for priority\n        # and `item` as a positive factor.\n        # How about prioritizing bins with smaller remaining capacity that can still fit the item?\n        # This aligns with the First Fit Decreasing heuristic's goal of filling bins.\n        \n        # Let's map the difference `valid_capacities - item` to a priority.\n        # Smaller difference should yield higher priority.\n        \n        # Example: item = 3, capacities = [5, 7, 10]\n        # Valid capacities = [5, 7, 10]\n        # Differences = [2, 4, 7]\n        # We want to prioritize bins with difference 2, then 4, then 7.\n        # So, 1/2, 1/4, 1/7 would work.\n        \n        diffs = valid_capacities - item\n        priorities = 1.0 / (diffs + epsilon)\n        \n        # Now, to make it more \"Softmax-like\" if we were to select probabilistically,\n        # we can exponentiate and normalize. But for direct priority score, this is fine.\n        # Let's add a small value to all priorities to avoid negative exponents in a Softmax if we were to use it.\n        # And let's scale them to prevent numerical underflow or overflow with Softmax.\n        \n        # For a direct priority score where higher means better, \n        # this inverse difference works well for \"best fit\" aspect.\n        \n        # Consider what happens if multiple bins have the exact same \"best fit\" difference.\n        # The current approach would give them equal priority.\n        \n        # To incorporate the \"Softmax-Based Fit\" idea, let's interpret it as:\n        # transform the \"fitness\" of a bin (how well it fits the item) into a priority.\n        # The fitness can be related to how close `remaining_capacity` is to `item`.\n        \n        # Let's define fitness as: -(remaining_capacity - item)^2. Higher fitness for smaller squared difference.\n        # Or, more simply, as we did: 1.0 / (remaining_capacity - item + epsilon)\n        \n        # Softmax transformation of these scores to get a distribution if needed.\n        # For now, we just need the scores themselves.\n        \n        # Let's try to directly use the remaining capacity for scaling, \n        # encouraging smaller capacities that fit.\n        \n        # Prioritize bins with the smallest remaining capacity that can fit the item.\n        # So, the priority score should be higher for smaller `valid_capacities`.\n        # Let's try `1.0 / valid_capacities`.\n        \n        # Consider a case: item = 2, bins_remain_cap = [3, 5, 10]\n        # Valid bins = [3, 5, 10]\n        # Option A (inverse diff): 1/(3-2)=1, 1/(5-2)=0.33, 1/(10-2)=0.125. Prioritizes bin with 3. (Best Fit)\n        # Option B (inverse capacity): 1/3=0.33, 1/5=0.2, 1/10=0.1. Prioritizes bin with 3.\n        \n        # If the goal is \"smallest number of bins\", then fitting into a nearly full bin is good.\n        # \"Best Fit\" heuristic is good for this.\n        \n        # Let's combine the \"fit\" (difference) with the \"emptiness\" (remaining capacity).\n        # Maybe penalize very large remaining capacities, even if they fit.\n        \n        # Let's use the difference again, as it directly measures \"how much space is left after fitting\".\n        # Smaller difference is better.\n        \n        diffs = valid_capacities - item\n        \n        # Scale diffs to be more in line with Softmax inputs (e.g., range -inf to +inf for exp)\n        # A common pattern is to use `exp(value)` where larger `value` is better.\n        # We want to maximize `-(diffs)`. So `exp(-diffs)`? No, we want to maximize score for smaller diffs.\n        # Let's use `exp(-diffs)` with `temperature`.\n        \n        temperature = 0.5 # Lower temperature means stronger preference for best fit\n        scaled_diffs = -diffs / temperature\n        \n        # Apply Softmax concept: exp(score) / sum(exp(scores))\n        # We can simply return exp(scaled_diffs) as the priority, which is proportional to softmax output.\n        \n        priorities = np.exp(scaled_diffs)\n        \n    \n    final_priorities = np.zeros_like(bins_remain_cap)\n    final_priorities[valid_bins] = priorities\n    \n    return final_priorities",
    "response_id": 29,
    "tryHS": false,
    "obj": 4.198244914240141,
    "SLOC": 26.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse proximity for tight fits with a sigmoid for smooth preference.\n    Favors bins with minimal remaining capacity after packing, scaled smoothly.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(eligible_bins_mask):\n        eligible_capacities = bins_remain_cap[eligible_bins_mask]\n        \n        # Inverse proximity: smaller gap is better (higher score)\n        # Adding a small epsilon to avoid division by zero\n        inverse_proximity = 1.0 / (eligible_capacities - item + 1e-9)\n\n        # Normalize inverse proximity to a range where sigmoid is effective\n        # Aims to map smaller gaps (higher inverse_proximity) to values around 0.5\n        # and larger gaps to values further from 0.5.\n        # This normalization is heuristic and can be tuned.\n        if np.max(inverse_proximity) > np.min(inverse_proximity):\n            normalized_scores = (inverse_proximity - np.min(inverse_proximity)) / (np.max(inverse_proximity) - np.min(inverse_proximity))\n        else: # All eligible bins have the same inverse proximity\n            normalized_scores = np.ones_like(inverse_proximity) * 0.5\n\n        # Sigmoid function to create a smooth priority distribution\n        # The steepness parameter (e.g., 10) can be tuned.\n        # We want bins with smaller gaps (higher normalized_scores) to have higher sigmoid outputs.\n        # So, we invert the normalized_scores for the sigmoid input to favor smaller gaps.\n        sigmoid_priorities = 1 / (1 + np.exp(-10 * (normalized_scores - 0.5)))\n\n        priorities[eligible_bins_mask] = sigmoid_priorities\n        \n        # Ensure that if all eligible bins are identical in terms of fit, they get a neutral priority\n        if np.all(priorities[eligible_bins_mask] == 0.5) and len(eligible_bins_mask) > 0:\n            priorities[eligible_bins_mask] = 0.5\n\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response6.txt_stdout.txt",
    "code_path": "problem_iter2_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins that are a tight fit using an inverse proximity measure,\n    giving infinite priority to perfect fits to encourage consolidation.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        differences = suitable_bins_caps - item\n\n        priorities[suitable_bins_mask] = 1.0 / (differences + 1e-9)\n\n        # Assign infinite priority to perfect fits\n        perfect_fit_mask = (differences == 0)\n        if np.any(perfect_fit_mask):\n            priorities[suitable_bins_mask][perfect_fit_mask] = float('inf')\n            \n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines inverse proximity with a sigmoid for smoother prioritization.\n\n    Favors bins with tight fits, but also provides non-zero priority for\n    less tight fits to encourage exploration.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        \n        # Inverse proximity for tight fits (similar to priority_v0)\n        proximity = suitable_bins_caps - item\n        \n        # Use a scaled sigmoid on proximity to create a smoother distribution\n        # Scaling factor `alpha` controls steepness. Higher alpha means steeper curve.\n        alpha = 10.0 \n        # Add a small epsilon to avoid division by zero for perfect fits\n        inverse_proximity_scores = 1.0 / (proximity + 1e-9)\n        \n        # Normalize inverse proximity scores to be between 0 and 1\n        if np.max(inverse_proximity_scores) > 0:\n            normalized_inverse_proximity = inverse_proximity_scores / np.max(inverse_proximity_scores)\n        else:\n            normalized_inverse_proximity = np.zeros_like(inverse_proximity_scores)\n\n        # Sigmoid transformation to map scores to a [0, 1] range, emphasizing tighter fits\n        # Adjusting the sigmoid's center and steepness can tune behavior.\n        # Here, we center it around a value that would correspond to a \"good\" proximity.\n        # For simplicity, we'll use a sigmoid on the normalized inverse proximity.\n        # A higher score from inverse proximity should map to a higher sigmoid output.\n        sigmoid_scores = 1 / (1 + np.exp(-alpha * (normalized_inverse_proximity - 0.5))) # Adjusted sigmoid\n\n        priorities[suitable_bins_mask] = sigmoid_scores\n        \n        # Ensure perfect fits still get a high priority, potentially capped by sigmoid\n        perfect_fit_mask = (proximity == 0)\n        if np.any(perfect_fit_mask):\n            priorities[suitable_bins_mask][perfect_fit_mask] = 1.0 # Assign max priority for perfect fit\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response3.txt_stdout.txt",
    "code_path": "problem_iter3_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    This heuristic aims to improve upon priority_v1 by considering the \"waste\"\n    of a bin after placing the item, and also giving a slight preference\n    to bins that are already relatively full.\n\n    The priority is calculated as follows:\n    1. For bins that can fit the item (bins_remain_cap >= item):\n       - Calculate the remaining capacity after placing the item: `remaining_capacity - item`.\n       - To prioritize bins with less remaining capacity (i.e., more full), we want to minimize this difference.\n         A good way to convert minimization to maximization (for priority) is to use the negative of the difference: `item - remaining_capacity`.\n       - Additionally, we can add a small bonus for bins that are already quite full. A bin with `remaining_capacity` closer to 0 (but still >= item) is generally preferred.\n         We can achieve this by adding a term that increases as `remaining_capacity` decreases. For example, we can use `1 / (bins_remain_cap + epsilon)` where epsilon is a small constant to avoid division by zero.\n         A simpler approach might be to add a term like `(bin_capacity - bins_remain_cap)`. Since we don't have bin_capacity, we can approximate this by considering how \"full\" the bin is relative to the item itself.\n         Let's consider the relative \"fullness\" of the bin *after* placing the item. A bin that becomes `0.1` full is better than one that becomes `0.5` full, if the item is the same.\n         So, we want to maximize `-(remaining_capacity - item)`. This is `item - remaining_capacity`.\n         To also favor bins that are already more full, we can add a term related to how much space is *left* relative to the item's size. A bin with `remaining_capacity = 0.5` and `item = 0.4` has `0.1` space left. A bin with `remaining_capacity = 0.9` and `item = 0.4` has `0.5` space left. We prefer the former.\n         So `item - remaining_capacity` seems good.\n         To incorporate the \"already full\" aspect, we can consider the *inverse* of the remaining capacity *after* packing. A bin with very little capacity left is good.\n         Let's try `(item - bins_remain_cap) + C * (1 / (bins_remain_cap - item + epsilon))` where C is a small constant.\n         A simpler heuristic that balances fitting tightly and preferring fuller bins could be:\n         Maximize `(item - bins_remain_cap)` (tight fit) + `(1 / (bins_remain_cap + epsilon))` (already full).\n         Let's use `epsilon = 1e-6` to avoid division by zero.\n\n    2. For bins that cannot fit the item:\n       - Assign a very low priority (e.g., -np.inf).\n    \"\"\"\n    \n    fit_mask = bins_remain_cap >= item\n    \n    \n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    \n    epsilon = 1e-6\n    \n    \n    bins_that_fit_cap = bins_remain_cap[fit_mask]\n    \n    \n    tight_fit_score = item - bins_that_fit_cap\n    \n    \n    fullness_score = 1.0 / (bins_that_fit_cap - item + epsilon)\n    \n    \n    priorities[fit_mask] = tight_fit_score + fullness_score\n    \n    \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response0.txt_stdout.txt",
    "code_path": "problem_iter5_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit and a penalty for excessive remaining capacity.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins_mask = bins_remain_cap >= item\n\n    if not np.any(valid_bins_mask):\n        return priorities\n\n    valid_caps = bins_remain_cap[valid_bins_mask]\n    epsilon = 1e-9\n\n    # Heuristic 1: Best Fit (prioritize bins with minimal remaining capacity after packing)\n    # This is achieved by using the inverse of the difference.\n    # A smaller difference (remaining_cap - item) means a tighter fit and higher priority.\n    fit_difference = valid_caps - item\n    best_fit_scores = 1.0 / (fit_difference + epsilon)\n\n    # Heuristic 18/19/20 inspired: Penalize bins with excessively large remaining capacity.\n    # We can scale the remaining capacity itself. Higher remaining capacity should have lower priority.\n    # Using inverse of remaining capacity to reflect this.\n    # A larger remaining capacity means a lower score here.\n    excess_capacity_scores = 1.0 / (valid_caps + epsilon)\n\n    # Combine scores: We want both a tight fit AND not too much excess capacity.\n    # Multiplying the scores gives a combined priority that favors bins that are both\n    # close to fitting the item AND not overly large.\n    # A small positive constant is added to `fit_difference` to avoid division by zero.\n    # A small positive constant is added to `valid_caps` for similar reasons.\n    combined_scores = best_fit_scores * excess_capacity_scores\n\n    # Normalize scores to a range that might be useful for probabilistic selection or just for relative comparison.\n    # Using Softmax-like scaling (exponential) can highlight preferences more strongly.\n    # Let's use exp(-scaled_difference) to map smaller differences to higher scores, and then scale by excess capacity.\n    # A temperature parameter can control the sharpness of the priority.\n    temperature = 0.5  # Tunable parameter: lower value for stronger preference to best fit\n    \n    # Re-calculating scaled difference for exponential mapping to emphasize tight fits\n    # Map differences to exponents: smaller difference -> larger exponent -> larger priority\n    scaled_exponents = -fit_difference / temperature\n\n    # Combine the \"tight fit\" exponential score with the \"less excess capacity\" inverse score.\n    # Multiplication here means we want high scores in both aspects.\n    final_scores_for_valid_bins = np.exp(scaled_exponents) * excess_capacity_scores\n\n    # Assign the computed scores back to the original bins array\n    priorities[valid_bins_mask] = final_scores_for_valid_bins\n\n    # Ensure no negative priorities and handle edge cases (e.g., all priorities are zero)\n    # If all valid bins resulted in NaN or Inf, or very low scores, this can be a fallback.\n    # If all scores are zero (e.g., no valid bins, handled earlier, or all resulted in ~0 scores)\n    # we might want a uniform preference, but the zero initialization already covers this.\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response1.txt_stdout.txt",
    "code_path": "problem_iter5_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines inverse proximity with exponential decay for nuanced bin selection.\n\n    Favors tighter fits while allowing some preference for less tight bins,\n    tuned by an exponential decay parameter.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n        # Inverse proximity for tight fits\n        proximity = suitable_bins_caps - item\n        inverse_proximity_scores = 1.0 / (proximity + 1e-9)\n\n        # Normalize inverse proximity scores to a [0, 1] range\n        if np.max(inverse_proximity_scores) > 0:\n            normalized_inverse_proximity = inverse_proximity_scores / np.max(inverse_proximity_scores)\n        else:\n            normalized_inverse_proximity = np.zeros_like(inverse_proximity_scores)\n\n        # Use exponential decay on normalized inverse proximity.\n        # A higher normalized inverse proximity (tighter fit) results in a score closer to 1.\n        # The temperature parameter controls the decay rate. Smaller temperature -> steeper decay.\n        temperature = 0.5\n        exponential_scores = np.exp(normalized_inverse_proximity / temperature)\n\n        # Normalize exponential scores to a [0, 1] range.\n        if np.max(exponential_scores) > 0:\n            final_scores = exponential_scores / np.max(exponential_scores)\n        else:\n            final_scores = np.zeros_like(exponential_scores)\n\n        priorities[suitable_bins_mask] = final_scores\n\n        # Ensure perfect fits receive the maximum priority (1.0)\n        perfect_fit_mask = (proximity == 0)\n        if np.any(perfect_fit_mask):\n            priorities[suitable_bins_mask][perfect_fit_mask] = 1.0\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 22.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response6.txt_stdout.txt",
    "code_path": "problem_iter5_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fitting preference with a Softmax-based distribution.\n    Favors bins with minimal remaining capacity after placement, using Softmax\n    to create a graded priority, encouraging exploration of good fits.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    eligible_capacities = bins_remain_cap[eligible_bins_mask]\n\n    if eligible_capacities.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Prioritize tighter fits: smaller (capacity - item) is better.\n    # We use the negative difference for Softmax, so smaller differences\n    # lead to larger (more positive) exponents.\n    # This is inspired by Heuristic 7 (Softmax on negative differences).\n    differences = eligible_capacities - item\n    scores = -differences\n\n    # Use Softmax to generate a probability distribution over eligible bins.\n    # This creates graded priorities, favoring tighter fits more strongly.\n    # Stability trick: subtract max score before exponentiating.\n    # This is the core of Heuristic 7.\n    if np.max(scores) - np.min(scores) > 1e6: # Heuristic for numerical stability if scores vary extremely\n        scaled_scores = (scores - np.min(scores)) / (np.max(scores) - np.min(scores) + 1e-9)\n    else:\n        scaled_scores = scores\n\n    exp_scores = np.exp(scaled_scores - np.max(scaled_scores))\n    priorities = exp_scores / np.sum(exp_scores)\n\n    # Map priorities back to the original bin structure\n    full_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    full_priorities[eligible_bins_mask] = priorities\n\n    return full_priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response4.txt_stdout.txt",
    "code_path": "problem_iter5_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins based on a combination of tightest fit and overall bin fullness.\n    Uses a sigmoid function to provide graded priorities for suitable bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        \n        # Metric 1: Tightest fit (inverse of remaining capacity after packing)\n        # Smaller remaining capacity is better (tighter fit)\n        gaps = suitable_bins_caps - item\n        \n        # Avoid division by zero or very small numbers for bins with zero remaining capacity\n        inverse_proximity = 1.0 / (gaps + 1e-9)\n        \n        # Normalize inverse proximity to a [0, 1] range for the sigmoid\n        # Max value corresponds to the tightest fit, min to the loosest fit among suitable bins\n        min_inv_prox = np.min(inverse_proximity)\n        max_inv_prox = np.max(inverse_proximity)\n        \n        if max_inv_prox == min_inv_prox: # All suitable bins have the same tightness\n            normalized_proximity = np.ones_like(inverse_proximity) * 0.5 \n        else:\n            normalized_proximity = (inverse_proximity - min_inv_prox) / (max_inv_prox - min_inv_prox)\n            \n        # Metric 2: Fullness of the bin (inverse of remaining capacity before packing)\n        # Higher fullness is generally better to keep smaller bins for smaller items\n        fullness_scores = 1.0 / (suitable_bins_caps + 1e-9)\n        \n        # Normalize fullness scores\n        min_fullness = np.min(fullness_scores)\n        max_fullness = np.max(fullness_scores)\n        \n        if max_fullness == min_fullness: # All suitable bins have the same fullness\n            normalized_fullness = np.ones_like(fullness_scores) * 0.5\n        else:\n            normalized_fullness = (fullness_scores - min_fullness) / (max_fullness - min_fullness)\n            \n        # Combine metrics using sigmoid for graded preference\n        # We want high proximity score (tight fit) and high fullness score to have higher priority\n        # Sigmoid with a positive slope centered around 0.5 will map higher combined scores to higher priorities\n        combined_score = normalized_proximity * 0.7 + normalized_fullness * 0.3 # Weighted combination\n        \n        # Apply sigmoid to create graded priorities between 0 and 1\n        # A temperature parameter could be added here for tuning (e.g., sigmoid(k * (combined_score - 0.5)))\n        sigmoid_priorities = 1 / (1 + np.exp(-10 * (combined_score - 0.5))) \n        \n        priorities[suitable_bins_mask] = sigmoid_priorities\n        \n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 24.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response9.txt_stdout.txt",
    "code_path": "problem_iter5_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight fit score with bin fullness score using a weighted sum and sigmoid.\n    Prioritizes bins that minimize remaining capacity after packing, and also favor bins that are already fuller.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(eligible_bins_mask):\n        eligible_capacities = bins_remain_cap[eligible_bins_mask]\n        \n        # Score 1: Tight fit (inverse proximity)\n        # Smaller gap (eligible_capacities - item) means higher score. Add epsilon for stability.\n        tight_fit_scores = 1.0 / (eligible_capacities - item + 1e-9)\n\n        # Score 2: Bin fullness (inverse of remaining capacity)\n        # Fuller bins (smaller remaining capacity) get higher scores. Add epsilon.\n        fullness_scores = 1.0 / (eligible_capacities + 1e-9)\n\n        # Normalize scores to a common range (e.g., 0 to 1) for combination\n        # Normalize tight_fit_scores\n        min_tf, max_tf = np.min(tight_fit_scores), np.max(tight_fit_scores)\n        if max_tf > min_tf:\n            normalized_tight_fit = (tight_fit_scores - min_tf) / (max_tf - min_tf)\n        else:\n            normalized_tight_fit = np.ones_like(tight_fit_scores) * 0.5\n\n        # Normalize fullness_scores\n        min_fs, max_fs = np.min(fullness_scores), np.max(fullness_scores)\n        if max_fs > min_fs:\n            normalized_fullness = (fullness_scores - min_fs) / (max_fs - min_fs)\n        else:\n            normalized_fullness = np.ones_like(fullness_scores) * 0.5\n\n        # Combine scores with weights (can be tuned)\n        # Weights determine the relative importance of tight fit vs. fullness\n        weight_tight_fit = 0.6\n        weight_fullness = 0.4\n        combined_scores = (weight_tight_fit * normalized_tight_fit) + (weight_fullness * normalized_fullness)\n\n        # Apply sigmoid to the combined scores to get smooth priorities\n        # A steeper sigmoid (e.g., 10) emphasizes differences more.\n        # We want higher combined_scores to map to higher sigmoid outputs.\n        sigmoid_priorities = 1 / (1 + np.exp(-10 * (combined_scores - 0.5)))\n        \n        priorities[eligible_bins_mask] = sigmoid_priorities\n        \n        # If all eligible bins result in identical sigmoid priorities (e.g., all scores are same),\n        # assign a neutral priority of 0.5 to ensure some differentiation if possible.\n        # This also handles cases where combined_scores are all exactly 0.5.\n        if np.all(priorities[eligible_bins_mask] == 0.5) and np.any(eligible_bins_mask):\n             priorities[eligible_bins_mask] = 0.5 # Assign neutral priority if all are same\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 25.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  }
]