{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    This heuristic aims to improve upon priority_v1 by introducing a multi-criteria approach\n    and a more nuanced scoring for bins that are not perfectly fitting but are still viable.\n\n    The strategy is to prioritize bins based on two main factors:\n    1. How well the item fits the remaining capacity (tightest fit).\n    2. How \"full\" the bin is after the item is placed (to encourage fuller bins).\n\n    Scoring logic:\n    - For bins that can fit the item:\n        - Calculate the \"tightness\" of the fit: `item - bins_remain_cap`. A smaller (less negative) value is better,\n          meaning the remaining capacity is closer to the item size.\n        - Calculate a \"fullness score\" which is a measure of how much capacity is left *after* placing the item.\n          We want to penalize leaving too much space, so we can use `-(bins_remain_cap - item)`.\n          A higher value here means less space is left, which is good.\n        - Combine these two scores. A simple weighted sum can be used.\n          Let `tightness_score = item - bins_remain_cap`.\n          Let `fullness_score = -(bins_remain_cap - item)`.\n          The combined score could be `w1 * tightness_score + w2 * fullness_score`.\n          Here, we want to prioritize bins where `bins_remain_cap - item` is minimized (closest to 0).\n          This means `item - bins_remain_cap` is maximized (closest to 0 from the negative side).\n          Let's consider `bins_remain_cap - item`. We want to minimize this for fitting bins.\n          To convert minimization to maximization for a priority score, we can use `- (bins_remain_cap - item)`.\n          So, `priority_component_1 = bins_remain_cap - item`. We want to maximize this.\n          If `bins_remain_cap = 1.0` and `item = 0.5`, `priority_component_1 = 0.5`.\n          If `bins_remain_cap = 0.6` and `item = 0.5`, `priority_component_1 = 0.1`.\n          This `priority_component_1` directly represents how much *extra* space is left. We want to minimize this extra space.\n          So, to maximize, we use `-(bins_remain_cap - item)`.\n\n        - Let's reconsider the goal: Find a bin where `bins_remain_cap >= item`.\n          We want the `bins_remain_cap` to be as close to `item` as possible.\n          This means minimizing `bins_remain_cap - item`.\n          So, a good score component is `-(bins_remain_cap - item)`.\n\n        - Let's also consider the \"wasted space\" after placing the item. This is `bins_remain_cap - item`.\n          We want to minimize this. So, `-(bins_remain_cap - item)` is a good candidate.\n\n        - Consider the *absolute* difference: `abs(bins_remain_cap - item)`. We want to minimize this.\n          So, `-abs(bins_remain_cap - item)` is a good score.\n\n        - What if we want to prioritize bins that are generally fuller, even if they aren't the *tightest* fit?\n          A bin with `bins_remain_cap = 0.9` and `item = 0.1` leaves `0.8` space.\n          A bin with `bins_remain_cap = 0.5` and `item = 0.4` leaves `0.1` space.\n          The second is a tighter fit and leaves less wasted space.\n\n        - Let's introduce a parameter `alpha` to balance two criteria:\n            1. Minimizing the \"slack\" or remaining space: `bins_remain_cap - item`. We want to maximize `-(bins_remain_cap - item)`.\n            2. Maximizing the overall remaining capacity of the bin *before* packing, as a secondary factor to encourage using generally fuller bins. This is `bins_remain_cap`.\n\n        - So, for bins that fit:\n            Score = `alpha * (bins_remain_cap - item) + (1 - alpha) * bins_remain_cap`\n            To maximize this score, we want `bins_remain_cap - item` to be small and `bins_remain_cap` to be large.\n            Let's try a different combination: Prioritize bins that leave the *least* amount of space.\n            This means minimizing `bins_remain_cap - item`.\n            So, a primary score component is `-(bins_remain_cap - item)`.\n\n            A secondary consideration could be the overall fullness of the bin.\n            Perhaps we can use the remaining capacity `bins_remain_cap` itself as a secondary score.\n            If `bins_remain_cap = 0.9` and `item = 0.5`, the difference is `0.4`.\n            If `bins_remain_cap = 0.55` and `item = 0.5`, the difference is `0.05`.\n            The second case is a tighter fit. `-(0.4)` vs `-(0.05)`. The second is higher.\n\n            What if we want to combine the tightness with a penalty for being *too* empty?\n            Consider `bins_remain_cap >= item`.\n            Score = `-(bins_remain_cap - item)` for tightness.\n            We could add a small bonus for `bins_remain_cap` itself, weighted.\n            Score = `-(bins_remain_cap - item) + beta * bins_remain_cap`\n            Let `beta = 0.1`.\n            Bin 1: `bins_remain_cap = 0.9`, `item = 0.5`. Score = `-(0.4) + 0.1 * 0.9 = -0.4 + 0.09 = -0.31`\n            Bin 2: `bins_remain_cap = 0.55`, `item = 0.5`. Score = `-(0.05) + 0.1 * 0.55 = -0.05 + 0.055 = 0.005`\n            Bin 2 has higher priority.\n\n            Let's try maximizing `bins_remain_cap - item` for fitting bins. This represents the smallest positive slack.\n            To turn it into a higher-is-better score, we can use `-(bins_remain_cap - item)`.\n            For bins that don't fit, we assign a very low score.\n\n            Let's refine the scoring:\n            For bins where `bins_remain_cap >= item`:\n            We want to prioritize bins that minimize `bins_remain_cap - item`.\n            This is equivalent to maximizing `item - bins_remain_cap`.\n            This is the same as `priority_v1`.\n\n            Let's think about \"granular scoring\" and \"multi-criteria fusion\".\n            We can create two components:\n            1. Tightness: `score_tight = -(bins_remain_cap - item)` for fitting bins, `-inf` otherwise.\n               This favors bins that are almost full.\n            2. Fillness: `score_fill = bins_remain_cap` for fitting bins, `-inf` otherwise.\n               This favors bins that have more capacity in general (potentially leading to fewer bins overall if items are small).\n\n            We can fuse these: `priority = w_tight * score_tight + w_fill * score_fill`.\n            Let's choose weights. If we want to primarily prioritize tightness, `w_tight` should be larger.\n            Let `w_tight = 1.0` and `w_fill = 0.5`.\n\n            So, for fitting bins:\n            `score = -(bins_remain_cap - item) + 0.5 * bins_remain_cap`\n            `score = -bins_remain_cap + item + 0.5 * bins_remain_cap`\n            `score = item - 0.5 * bins_remain_cap`\n\n            Example:\n            `item = 0.5`\n            Bin A: `bins_remain_cap = 0.9`. Fits.\n                `score_tight = -(0.9 - 0.5) = -0.4`\n                `score_fill = 0.9`\n                `score_A = -0.4 + 0.5 * 0.9 = -0.4 + 0.45 = 0.05`\n            Bin B: `bins_remain_cap = 0.55`. Fits.\n                `score_tight = -(0.55 - 0.5) = -0.05`\n                `score_fill = 0.55`\n                `score_B = -0.05 + 0.5 * 0.55 = -0.05 + 0.275 = 0.225`\n            Bin C: `bins_remain_cap = 0.5`. Fits.\n                `score_tight = -(0.5 - 0.5) = 0.0`\n                `score_fill = 0.5`\n                `score_C = 0.0 + 0.5 * 0.5 = 0.25`\n\n            In this example, Bin C has the highest priority, followed by Bin B, then Bin A.\n            This prioritizes the tightest fit, but also gives a boost to bins that have more capacity overall if the tightness is similar.\n\n            Let's adjust the weights. If we want to more strongly favor overall fullness, increase `w_fill`.\n            Let `w_tight = 1.0`, `w_fill = 1.0`.\n            `score = item - bins_remain_cap`\n            This is exactly `priority_v1`.\n\n            The goal is to be *better* than `priority_v1`.\n            `priority_v1` maximizes `item - bins_remain_cap`. This strongly favors the tightest fit.\n\n            Let's reconsider the advice: \"finely-grained, multi-dimensional scoring mechanisms that integrate diverse criteria (e.g., fit, fullness, strategic placement) through weighted sums or more complex fusion methods.\"\n\n            What if we use a non-linear function for tightness? Or a different way to combine?\n            A common strategy is \"Best Fit Decreasing\" (BFD), which sorts items first. Here it's online.\n            For online, \"First Fit\" (FF), \"Best Fit\" (BF), \"Worst Fit\" (WF), \"Most Full\" are common.\n            BF priority is `-(bins_remain_cap - item)`. This is what `priority_v1` does.\n\n            Let's try to explicitly penalize bins that are *too large* but still fit.\n            Suppose `item = 0.3`.\n            Bin X: `bins_remain_cap = 0.3`. `score_v1 = 0.3 - 0.3 = 0`.\n            Bin Y: `bins_remain_cap = 0.8`. `score_v1 = 0.3 - 0.8 = -0.5`.\n            `priority_v1` would pick Bin X.\n\n            What if we also want to consider the *current* fullness of the bin, not just how much space is left *after* packing?\n            Consider the ratio `item / bins_remain_cap`. High ratio means it's a good fit for a nearly full bin.\n            But this can lead to division by zero or very small numbers.\n\n            Let's try a score that is sensitive to both:\n            1. The gap: `gap = bins_remain_cap - item`. We want to minimize this gap.\n            2. The current bin fullness: `current_fill_ratio = (BIN_CAPACITY - bins_remain_cap) / BIN_CAPACITY`. We want this to be high.\n\n            If we assume `BIN_CAPACITY` is a constant, let's say `C`.\n            We want to minimize `bins_remain_cap - item`.\n            We want to maximize `(C - bins_remain_cap) / C`. This is equivalent to minimizing `bins_remain_cap`.\n\n            So, for fitting bins:\n            We want to prioritize bins with small `bins_remain_cap - item`.\n            And among those, prioritize bins with smaller `bins_remain_cap`.\n\n            This suggests a lexicographical ordering or a weighted sum.\n            Let's use a weighted sum:\n            `score = w1 * -(bins_remain_cap - item) + w2 * -(bins_remain_cap)`\n            Where `w1 > 0` and `w2 > 0`.\n            `score = w1 * (item - bins_remain_cap) - w2 * bins_remain_cap`\n\n            If `w1=1`, `w2=1`: `score = item - bins_remain_cap - bins_remain_cap = item - 2 * bins_remain_cap`.\n                `item = 0.5`\n                Bin X: `bins_remain_cap = 0.5`. `score = 0.5 - 2 * 0.5 = -0.5`.\n                Bin Y: `bins_remain_cap = 0.8`. `score = 0.5 - 2 * 0.8 = 0.5 - 1.6 = -1.1`.\n                Bin Z: `bins_remain_cap = 0.55`. `score = 0.5 - 2 * 0.55 = 0.5 - 1.1 = -0.6`.\n                Here, Bin X (tightest fit) is still best.\n\n            Let's try `w1=1`, `w2=0.5`: `score = item - bins_remain_cap - 0.5 * bins_remain_cap = item - 1.5 * bins_remain_cap`.\n                `item = 0.5`\n                Bin X: `bins_remain_cap = 0.5`. `score = 0.5 - 1.5 * 0.5 = 0.5 - 0.75 = -0.25`.\n                Bin Y: `bins_remain_cap = 0.8`. `score = 0.5 - 1.5 * 0.8 = 0.5 - 1.2 = -0.7`.\n                Bin Z: `bins_remain_cap = 0.55`. `score = 0.5 - 1.5 * 0.55 = 0.5 - 0.825 = -0.325`.\n                Bin X (tightest fit) is still best.\n\n            The current heuristic `priority_v1` already focuses on the tightest fit.\n            To be *better*, we might need to consider scenarios where the tightest fit is not optimal, or introduce a more robust scoring.\n\n            Let's introduce a factor that penalizes leaving *too much* excess space, but less severely than `priority_v1` does implicitly.\n\n            Consider the \"waste\" `bins_remain_cap - item`.\n            `priority_v1` is `- (bins_remain_cap - item)`.\n            This gives a score of 0 for a perfect fit, and increasingly negative scores as the excess space increases.\n\n            What if we use a score that is higher for a perfect fit, and then drops, but not as steeply?\n            For example, a function like `max(0, item - bins_remain_cap)` is what we want to maximize.\n            This is `priority_v1`.\n\n            Let's try to make the score more \"granular\".\n            Instead of just penalizing negative differences, let's reward positive differences (slack) up to a certain point.\n\n            For bins that can fit (`bins_remain_cap >= item`):\n            Primary goal: Minimize `bins_remain_cap - item`.\n            Secondary goal: If two bins have similar small gaps, pick the one that is fuller (smaller `bins_remain_cap`).\n\n            Let's define a score function for fitting bins:\n            `score = -(bins_remain_cap - item)**2 + lambda * bins_remain_cap`\n            The `-(bins_remain_cap - item)**2` term penalizes larger gaps quadratically.\n            The `lambda * bins_remain_cap` term favors fuller bins.\n\n            Let `lambda = 0.1`.\n            `item = 0.5`\n            Bin X: `bins_remain_cap = 0.5`. Gap = 0. Score = `-0**2 + 0.1 * 0.5 = 0.05`.\n            Bin Y: `bins_remain_cap = 0.55`. Gap = 0.05. Score = `-(0.05)**2 + 0.1 * 0.55 = -0.0025 + 0.055 = 0.0525`.\n            Bin Z: `bins_remain_cap = 0.8`. Gap = 0.3. Score = `-(0.3)**2 + 0.1 * 0.8 = -0.09 + 0.08 = -0.01`.\n\n            Here, Bin Y is preferred, then Bin X, then Bin Z.\n            This might be an improvement over `priority_v1` which would strongly prefer X over Y and Z.\n            This new heuristic prioritizes a small positive gap (like Bin Y) slightly over a perfect fit (Bin X),\n            while still penalizing larger gaps (Bin Z).\n            The parameter `lambda` controls how much we care about overall fullness.\n\n            Let's refine the functional form. We want to maximize the score.\n            The `-(bins_remain_cap - item)**2` term aims to keep `bins_remain_cap - item` close to zero.\n            To maximize this term, we want `bins_remain_cap - item` to be as close to 0 as possible.\n            The `lambda * bins_remain_cap` term favors smaller `bins_remain_cap`.\n\n            So, the combined score: `-(bins_remain_cap - item)**2 + lambda * bins_remain_cap`\n            We want to maximize this.\n            Let's set `lambda = 0.1`.\n\n            We need to handle the `bins_remain_cap < item` case. Assign a very low score (e.g., -infinity).\n    \"\"\"\n    \n    fit_mask = bins_remain_cap >= item\n    \n    \n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    \n    fitting_bins_remain_cap = bins_remain_cap[fit_mask]\n    \n    \n    gap = fitting_bins_remain_cap - item\n    \n    \n    lambda_param = 0.1 # Parameter to balance tightness and overall bin fullness\n    \n    \n    scores = -(gap**2) + lambda_param * fitting_bins_remain_cap\n    \n    \n    priorities[fit_mask] = scores\n    \n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines tight fit score with bin fullness score using a weighted sum and sigmoid.\n    Prioritizes bins that minimize remaining capacity after packing, and also favor bins that are already fuller.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(eligible_bins_mask):\n        eligible_capacities = bins_remain_cap[eligible_bins_mask]\n        \n        # Score 1: Tight fit (inverse proximity)\n        # Smaller gap (eligible_capacities - item) means higher score. Add epsilon for stability.\n        tight_fit_scores = 1.0 / (eligible_capacities - item + 1e-9)\n\n        # Score 2: Bin fullness (inverse of remaining capacity)\n        # Fuller bins (smaller remaining capacity) get higher scores. Add epsilon.\n        fullness_scores = 1.0 / (eligible_capacities + 1e-9)\n\n        # Normalize scores to a common range (e.g., 0 to 1) for combination\n        # Normalize tight_fit_scores\n        min_tf, max_tf = np.min(tight_fit_scores), np.max(tight_fit_scores)\n        if max_tf > min_tf:\n            normalized_tight_fit = (tight_fit_scores - min_tf) / (max_tf - min_tf)\n        else:\n            normalized_tight_fit = np.ones_like(tight_fit_scores) * 0.5\n\n        # Normalize fullness_scores\n        min_fs, max_fs = np.min(fullness_scores), np.max(fullness_scores)\n        if max_fs > min_fs:\n            normalized_fullness = (fullness_scores - min_fs) / (max_fs - min_fs)\n        else:\n            normalized_fullness = np.ones_like(fullness_scores) * 0.5\n\n        # Combine scores with weights (can be tuned)\n        # Weights determine the relative importance of tight fit vs. fullness\n        weight_tight_fit = 0.6\n        weight_fullness = 0.4\n        combined_scores = (weight_tight_fit * normalized_tight_fit) + (weight_fullness * normalized_fullness)\n\n        # Apply sigmoid to the combined scores to get smooth priorities\n        # A steeper sigmoid (e.g., 10) emphasizes differences more.\n        # We want higher combined_scores to map to higher sigmoid outputs.\n        sigmoid_priorities = 1 / (1 + np.exp(-10 * (combined_scores - 0.5)))\n        \n        priorities[eligible_bins_mask] = sigmoid_priorities\n        \n        # If all eligible bins result in identical sigmoid priorities (e.g., all scores are same),\n        # assign a neutral priority of 0.5 to ensure some differentiation if possible.\n        # This also handles cases where combined_scores are all exactly 0.5.\n        if np.all(priorities[eligible_bins_mask] == 0.5) and np.any(eligible_bins_mask):\n             priorities[eligible_bins_mask] = 0.5 # Assign neutral priority if all are same\n\n    return priorities\n\n### Analyze & experience\n- *   **Comparing (1st) vs (2nd):** Heuristic 1st introduces a quadratic penalty `-(gap**2)` and a linear term `lambda_param * fitting_bins_remain_cap`, aiming for a more nuanced score that balances tight fit with overall bin fullness. Heuristic 2nd also aims for multi-criteria but its explanation is more exploratory and less concrete in its final scoring logic within the docstring. The implementation of 1st seems to better reflect a specific, refined scoring strategy.\n\n*   **Comparing (3rd) vs (4th):** Heuristic 3rd implements a simple binary \"best fit\" logic (1.0 for minimum difference, 0.0 otherwise), failing to provide graded priorities or consider overall bin fullness. Heuristic 4th attempts a multi-criteria approach with normalization and sigmoid activation, aiming for smoother, graded priorities, but its normalization and weighting logic can be complex and might not always yield the most intuitive results.\n\n*   **Comparing (5th) vs (6th):** Heuristic 5th uses an exponential score based on the difference from the minimum difference, with a temperature parameter. Heuristic 6th uses Softmax on the negative differences, creating a probability distribution, which is a more standard approach for graded priorities in similar contexts. Heuristic 6th's Softmax implementation is generally more robust for generating graded priorities.\n\n*   **Comparing (7th) vs (8th):** Heuristic 7th uses a simple `item - bins_remain_cap` score, which is a direct representation of tightest fit but can lead to large negative values. Heuristic 8th uses `1.0 / (differences + 1e-9)` and explicitly assigns `inf` to perfect fits, offering a clearer \"best fit\" prioritization.\n\n*   **Comparing (9th) vs (10th):** Heuristic 9th uses a simple `1.0 / proximity` for fitting bins, prioritizing the tightest fit. Heuristic 10th attempts to combine this with a quadratic penalty `-(differences**2)`, aiming to penalize larger gaps more heavily while still favoring tight fits. Heuristic 10th's multi-faceted scoring is more sophisticated.\n\n*   **Comparing (11th) vs (12th):** Heuristic 11th is identical to Heuristic 10th. Heuristic 12th is identical to Heuristic 6th.\n\n*   **Comparing (13th) vs (14th):** Heuristic 13th and 14th are identical. They use `exp(-diffs / temperature)` which is a Softmax-like approach focused on the tightest fit, controlled by temperature.\n\n*   **Comparing (15th) vs (16th):** Heuristic 15th, 17th, 18th, 19th, 20th are all identical implementations of a simple \"best fit\" heuristic (`1 / (cap - item + 1e-9)`). Heuristic 16th (and its identical counterparts 13th/14th) uses a temperature-controlled exponential function on negative differences, which provides a smoother, graded priority distribution compared to the simple inverse of the difference.\n\n*   **Overall:** Heuristics 1st, 10th/11th, and 13th/14th/16th/17th/18th/19th/20th show a progression. Heuristic 1st offers a good balance of tight fit and bin fullness. Heuristics 10th/11th attempt to penalize gaps quadratically. Heuristics 13th/14th/16th/etc. use a Softmax-like approach for graded priorities based on tightness. The simplest (and thus arguably worst for nuanced optimization) are the basic \"best fit\" heuristics (15th onwards). Heuristic 4th tries complex normalization and sigmoid, which might be overly complicated. Heuristic 3rd is too simplistic.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Multi-criteria, graded scoring, tunable parameters, nuanced penalties, vectorized operations.\n*   **Advice:** Focus on composite scoring functions that blend multiple criteria (fit, fullness, etc.) using smooth, non-linear mappings (e.g., exponentials, sigmoid). Tune parameters to adapt to problem characteristics. Leverage vectorized operations for performance.\n*   **Avoid:** Overtly simple binary or linear scoring. Unjustified complex transformations without empirical evidence. Ignoring edge cases or bin eligibility.\n*   **Explanation:** Nuanced scoring and tunable parameters allow heuristics to capture subtle trade-offs and adapt to diverse problem instances, leading to more effective and robust solutions than blunt, oversimplified approaches.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}