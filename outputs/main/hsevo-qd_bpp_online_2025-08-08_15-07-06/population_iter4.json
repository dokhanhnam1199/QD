[
  {
    "stdout_filepath": "problem_iter1_response17.txt_stdout.txt",
    "code_path": "problem_iter1_code17.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    This is a heuristic for the online bin packing problem.\n    We want to prioritize bins that are \"almost full\" but can still fit the item.\n    This strategy aims to fill bins as much as possible before opening new ones.\n\n    The priority is calculated as follows:\n    1. For bins that can fit the item:\n       - Calculate how much \"space\" is left after fitting the item.\n       - We want to *minimize* this leftover space, but we also don't want to penalize bins that are already very full.\n       - A good measure might be `(bin_capacity - item) / bin_capacity` if we're talking about proportions,\n         but we have remaining capacities. So, `remaining_capacity - item`.\n       - To get a \"priority\" where higher is better, we can take the negative of this difference: `item - remaining_capacity`.\n         This means a bin with `remaining_capacity = 0.9` and `item = 0.8` gives `-0.7` (higher is better than -0.2 for a bin with `remaining_capacity = 0.3`).\n       - To avoid very large negative numbers for bins that are too small, we can set the priority to a very low number (or zero) if the item doesn't fit.\n\n    2. For bins that cannot fit the item:\n       - Assign a very low priority (e.g., 0 or negative infinity effectively, but we'll use 0).\n\n    Let's refine this:\n    We want to put the item into a bin where the remaining capacity is *just enough* or slightly more than the item.\n    If remaining_capacity >= item:\n        Priority = some_function(remaining_capacity - item)\n    Else:\n        Priority = -infinity (effectively 0 for practical purposes if others are positive)\n\n    Consider the difference: `bins_remain_cap - item`.\n    If this difference is negative, the item doesn't fit. We'll assign a very low priority.\n    If this difference is non-negative, we want to prioritize bins where this difference is *smallest* (closest to zero).\n    So, we want to maximize `-(bins_remain_cap - item) = item - bins_remain_cap`.\n    This means if a bin has `rem_cap = 1.0` and `item = 0.5`, priority is `-0.5`.\n    If a bin has `rem_cap = 0.6` and `item = 0.5`, priority is `-0.1`. The latter is higher priority.\n\n    Let's make it simpler. We want the bin where `bins_remain_cap` is *closest to `item`*, but greater than or equal to `item`.\n    This is like finding the minimum of `bins_remain_cap - item` for all `bins_remain_cap >= item`.\n    To turn this into a \"priority\" (higher is better), we can use `-abs(bins_remain_cap - item)` or `item - bins_remain_cap`.\n    Let's stick with `item - bins_remain_cap`.\n\n    Priorities will be negative. Higher values mean a better fit.\n    We need to handle the case where `bins_remain_cap < item`.\n    \"\"\"\n    \n    fit_mask = bins_remain_cap >= item\n    \n    \n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    \n    priorities[fit_mask] = item - bins_remain_cap[fit_mask]\n    \n    \n    return priorities",
    "response_id": 17,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a First Fit strategy variant.\n\n    The priority is higher for bins that can accommodate the item and have a remaining\n    capacity closer to the item's size. This encourages tighter packing.\n    Bins that cannot accommodate the item are given a priority of -1.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_remain_cap.size > 0:\n        differences = fitting_bins_remain_cap - item\n        # Higher priority for smaller differences (tighter fit)\n        # We use -differences to make smaller differences result in higher scores.\n        # Adding a small constant to avoid zero priorities for perfect fits\n        # and to ensure valid bins have a positive priority.\n        priorities[can_fit_mask] = -differences + 1.0\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response4.txt_stdout.txt",
    "code_path": "problem_iter1_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i, cap in enumerate(bins_remain_cap):\n        if cap >= item:\n            priorities[i] = 1 / (cap - item + 1e-9)\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.198244914240141,
    "SLOC": 6.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response5.txt_stdout.txt",
    "code_path": "problem_iter1_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit) strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    for i, remaining_cap in enumerate(bins_remain_cap):\n        if remaining_cap >= item:\n            \n            proximity = remaining_cap - item\n            \n            if proximity == 0:\n                priorities[i] = float('inf') \n            else:\n                priorities[i] = 1.0 / proximity\n                \n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 10.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response6.txt_stdout.txt",
    "code_path": "problem_iter1_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    suitable_bins = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    if np.any(suitable_bins):\n        remaining_capacities_of_suitable_bins = bins_remain_cap[suitable_bins]\n        \n        gaps = remaining_capacities_of_suitable_bins - item\n        \n        normalized_gaps = gaps / np.max(remaining_capacities_of_suitable_bins)\n        \n        sigmoid_scores = 1 / (1 + np.exp(-10 * (normalized_gaps - 0.5)))\n        \n        priorities[suitable_bins] = sigmoid_scores\n        \n        \n        if np.all(priorities == 0):\n             priorities[suitable_bins] = 0.5\n    \n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 149.30195452732352,
    "SLOC": 12.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit and inverse proximity for Bin Packing priority.\n    Prioritizes bins that are closer fits, with a strong preference for the absolute best fit.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(eligible_bins_mask):\n        eligible_capacities = bins_remain_cap[eligible_bins_mask]\n        differences = eligible_capacities - item\n\n        # Strategy 1: Inverse proximity (favoring smaller remaining capacities)\n        # Add epsilon for numerical stability and to avoid division by zero.\n        inverse_proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Strategy 2: Identify the absolute best fit(s)\n        min_diff = np.min(differences)\n        best_fit_mask_local = (differences == min_diff)\n\n        # Combine strategies: Give a significantly higher priority to the absolute best fit(s)\n        # and use inverse proximity for others.\n        # We scale the best fit scores to be clearly dominant.\n        combined_scores = np.zeros_like(eligible_capacities)\n        combined_scores[best_fit_mask_local] = 100.0  # High priority for best fit\n        combined_scores[~best_fit_mask_local] = inverse_proximity_scores[~best_fit_mask_local]\n\n        # Normalize scores to be in a reasonable range, though not strictly probabilities here.\n        # Using Softmax-like scaling for non-best-fit items to maintain relative preference.\n        non_best_fit_scores = inverse_proximity_scores[~best_fit_mask_local]\n        if non_best_fit_scores.size > 0:\n            exp_scores = np.exp(non_best_fit_scores - np.max(non_best_fit_scores))\n            normalized_non_best_fit = exp_scores / np.sum(exp_scores)\n            combined_scores[~best_fit_mask_local] = normalized_non_best_fit\n\n        # Assign combined scores back to the original array structure\n        priorities[eligible_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 19.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A Softmax-based priority function for the online Bin Packing Problem.\n\n    This function calculates the priority of placing an item into each available bin.\n    It considers the remaining capacity of each bin relative to the item size.\n    Bins that can accommodate the item without exceeding their capacity are favored.\n    Among the bins that can accommodate the item, those with less remaining capacity\n    (i.e., tighter fits) are given a higher priority, encouraging fuller bins first.\n    The Softmax function is used to convert these relative preferences into a\n    probability distribution, ensuring that higher priority bins have a greater chance\n    of being selected.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the remaining\n                         capacity of a bin.\n\n    Returns:\n        A numpy array of the same shape as bins_remain_cap, where each element\n        is the priority score for placing the item into the corresponding bin.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    eligible_capacities = bins_remain_cap[eligible_bins_mask]\n\n    if eligible_capacities.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit\" score for eligible bins. A smaller remaining capacity\n    # (tighter fit) results in a higher score. We use the negative difference\n    # to make larger remaining capacities (less good fits) have smaller scores.\n    fit_scores = -(eligible_capacities - item)\n\n    # Apply Softmax to get probabilities (priorities).\n    # Adding a small epsilon to avoid log(0) issues if fit_scores can be zero.\n    epsilon = 1e-9\n    exp_scores = np.exp(fit_scores - np.max(fit_scores)) # Stability trick for softmax\n    priorities = exp_scores / np.sum(exp_scores)\n\n    # Map priorities back to the original bin structure\n    full_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    full_priorities[eligible_bins_mask] = priorities\n\n    return full_priorities",
    "response_id": 19,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response11.txt_stdout.txt",
    "code_path": "problem_iter1_code11.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    if suitable_bins_caps.size > 0:\n        differences = suitable_bins_caps - item\n        min_diff = np.min(differences)\n        \n        suitable_bin_indices = np.where(suitable_bins_mask)[0]\n        \n        for i, original_index in enumerate(suitable_bin_indices):\n            if bins_remain_cap[original_index] - item == min_diff:\n                priorities[original_index] = 1.0\n            else:\n                priorities[original_index] = 0.0\n    return priorities",
    "response_id": 11,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response5.txt_stdout.txt",
    "code_path": "problem_iter2_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by favoring those with the smallest remaining capacity that can fit the item,\n    using an inverse proximity score for refinement.\n    \"\"\"\n    # Identify bins that can accommodate the item\n    possible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(possible_bins_mask):\n        return np.zeros_like(bins_remain_cap)\n\n    # Get remaining capacities of only the possible bins\n    valid_capacities = bins_remain_cap[possible_bins_mask]\n\n    # Calculate the difference between remaining capacity and item size.\n    # Smaller difference indicates a tighter fit, which is generally preferred\n    # to minimize wasted space and encourage filling bins.\n    differences = valid_capacities - item\n\n    # Assign priority scores: higher score for smaller differences (tighter fits).\n    # Using the inverse of the difference (plus a small epsilon for stability)\n    # makes smaller differences result in larger scores.\n    # If difference is 0 (perfect fit), score becomes very high.\n    epsilon = 1e-9\n    priorities = 1.0 / (differences + epsilon)\n\n    # Create a result array initialized with zeros\n    final_priorities = np.zeros_like(bins_remain_cap)\n    # Place the calculated priorities into the correct positions corresponding to possible bins\n    final_priorities[possible_bins_mask] = priorities\n\n    return final_priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.198244914240141,
    "SLOC": 11.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response15.txt_stdout.txt",
    "code_path": "problem_iter1_code15.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the difference between the bin's remaining capacity and the item's size.\n    # A smaller difference means the item fits \"better\" or closer to the bin's capacity.\n    # We add a small epsilon to avoid division by zero if a bin is perfectly full or the item size is 0.\n    diffs = bins_remain_cap - item\n    priorities = 1.0 / (np.abs(diffs) + 1e-9)\n\n    # We want to prioritize bins that can actually fit the item.\n    # If an item cannot fit, its priority should be very low.\n    # We can achieve this by multiplying the inverse difference by a mask\n    # that is 1 for bins that can fit the item and 0 otherwise.\n    can_fit_mask = (bins_remain_cap >= item).astype(float)\n    priorities *= can_fit_mask\n\n    return priorities",
    "response_id": 15,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response29.txt_stdout.txt",
    "code_path": "problem_iter1_code29.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    valid_bins = bins_remain_cap >= item\n    \n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n\n    valid_capacities = bins_remain_cap[valid_bins]\n    \n    # Higher remaining capacity means lower priority (we want to fill bins)\n    # To use Softmax effectively, we want larger values to correspond to higher priority.\n    # So we can use (large_capacity - remaining_capacity) or 1/(remaining_capacity)\n    # Let's try 1/(remaining_capacity) as it penalizes bins that are already very full.\n    \n    inverted_capacities = 1.0 / valid_capacities\n    \n    # To further encourage fitting into bins with *just enough* space, \n    # we can add a term that penalizes bins with very large remaining capacity.\n    # Let's try subtracting the ratio of remaining capacity to total capacity (assuming initial bin capacity is known or can be estimated).\n    # For simplicity here, let's just use the inverse capacity.\n    \n    # Adding a small epsilon to avoid division by zero if a bin has 0 remaining capacity, though valid_bins should prevent this.\n    epsilon = 1e-9\n    scores = 1.0 / (valid_capacities + epsilon)\n    \n    # Softmax is often used to turn scores into probabilities or weights.\n    # A higher score should mean a higher probability of selection.\n    # Let's scale the scores to be positive and somewhat related to \"how well\" it fits.\n    # A common approach in fitting is to maximize the remaining capacity, \n    # but here we want to minimize the number of bins. So we prefer bins that are *almost* full.\n    # Let's try prioritizing bins where item fits snugly.\n    \n    fit_difference = valid_capacities - item\n    # We want to minimize fit_difference. To make it a priority (higher is better), we invert it.\n    # Add a small constant to avoid division by zero if fit_difference is 0.\n    priority_scores = 1.0 / (fit_difference + epsilon)\n    \n    # Apply Softmax to convert scores into probabilities (weights)\n    # We add a small penalty for bins that have much more capacity than needed to discourage very loose fits.\n    # Let's consider the \"waste\" factor. Waste = remaining_capacity - item\n    # We want to minimize waste.\n    \n    # Let's try a heuristic that favors bins that have enough capacity but not excessively more.\n    # We can try a value that increases as remaining_capacity gets closer to item.\n    \n    # Option 1: Prioritize bins that are almost full (minimum remaining capacity that fits item)\n    # We want higher scores for smaller `valid_capacities`. So `1/valid_capacities` or similar.\n    # To be more specific, we want `valid_capacities - item` to be small.\n    # So we can use `1.0 / (valid_capacities - item + epsilon)`\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bins] = 1.0 / (valid_capacities - item + epsilon)\n    \n    # Apply Softmax: exp(score) / sum(exp(scores))\n    # For just returning priority scores for selection, we can directly use the calculated scores\n    # or apply a transformation like Softmax.\n    # If we want to select *one* bin based on highest priority, we can just return the scores directly.\n    # If we want to weight bins for some probabilistic selection, Softmax is good.\n    # For this problem, simply returning the scores that indicate preference is sufficient.\n    \n    # Let's refine to prioritize bins where remaining_capacity - item is minimized.\n    # A simple approach for priority is the inverse of the difference.\n    \n    scores = np.zeros_like(bins_remain_cap)\n    scores[valid_bins] = 1.0 / (valid_capacities - item + epsilon)\n\n    # To make it more \"Softmax-like\" in spirit of distribution, \n    # we can use a sigmoid-like transformation or directly use scaled values.\n    # Let's consider a temperature parameter to control the \"sharpness\" of priorities.\n    temperature = 1.0\n    \n    # Let's make values larger for better fits.\n    # A potential issue is if all valid capacities are very large, leading to small inverse values.\n    # We need to ensure scores are somewhat comparable or scaled.\n    \n    # Consider the \"tightness of fit\" as the primary driver.\n    # Tightest fit = smallest (remaining_capacity - item).\n    # So, priority is inversely proportional to (remaining_capacity - item).\n    \n    normalized_scores = np.zeros_like(bins_remain_cap)\n    if np.any(valid_bins):\n        # Calculate scores: higher score for tighter fit\n        # We want to maximize (1 / (remaining_capacity - item))\n        # Or to avoid issues with very small differences, maybe prioritize directly by minimum remaining capacity that fits.\n        \n        # Let's try a direct mapping:\n        # A bin is \"good\" if remaining_capacity is just enough.\n        # So, priority is high when remaining_capacity is close to item.\n        \n        # Let's use `remaining_capacity` itself as a negative factor for priority\n        # and `item` as a positive factor.\n        # How about prioritizing bins with smaller remaining capacity that can still fit the item?\n        # This aligns with the First Fit Decreasing heuristic's goal of filling bins.\n        \n        # Let's map the difference `valid_capacities - item` to a priority.\n        # Smaller difference should yield higher priority.\n        \n        # Example: item = 3, capacities = [5, 7, 10]\n        # Valid capacities = [5, 7, 10]\n        # Differences = [2, 4, 7]\n        # We want to prioritize bins with difference 2, then 4, then 7.\n        # So, 1/2, 1/4, 1/7 would work.\n        \n        diffs = valid_capacities - item\n        priorities = 1.0 / (diffs + epsilon)\n        \n        # Now, to make it more \"Softmax-like\" if we were to select probabilistically,\n        # we can exponentiate and normalize. But for direct priority score, this is fine.\n        # Let's add a small value to all priorities to avoid negative exponents in a Softmax if we were to use it.\n        # And let's scale them to prevent numerical underflow or overflow with Softmax.\n        \n        # For a direct priority score where higher means better, \n        # this inverse difference works well for \"best fit\" aspect.\n        \n        # Consider what happens if multiple bins have the exact same \"best fit\" difference.\n        # The current approach would give them equal priority.\n        \n        # To incorporate the \"Softmax-Based Fit\" idea, let's interpret it as:\n        # transform the \"fitness\" of a bin (how well it fits the item) into a priority.\n        # The fitness can be related to how close `remaining_capacity` is to `item`.\n        \n        # Let's define fitness as: -(remaining_capacity - item)^2. Higher fitness for smaller squared difference.\n        # Or, more simply, as we did: 1.0 / (remaining_capacity - item + epsilon)\n        \n        # Softmax transformation of these scores to get a distribution if needed.\n        # For now, we just need the scores themselves.\n        \n        # Let's try to directly use the remaining capacity for scaling, \n        # encouraging smaller capacities that fit.\n        \n        # Prioritize bins with the smallest remaining capacity that can fit the item.\n        # So, the priority score should be higher for smaller `valid_capacities`.\n        # Let's try `1.0 / valid_capacities`.\n        \n        # Consider a case: item = 2, bins_remain_cap = [3, 5, 10]\n        # Valid bins = [3, 5, 10]\n        # Option A (inverse diff): 1/(3-2)=1, 1/(5-2)=0.33, 1/(10-2)=0.125. Prioritizes bin with 3. (Best Fit)\n        # Option B (inverse capacity): 1/3=0.33, 1/5=0.2, 1/10=0.1. Prioritizes bin with 3.\n        \n        # If the goal is \"smallest number of bins\", then fitting into a nearly full bin is good.\n        # \"Best Fit\" heuristic is good for this.\n        \n        # Let's combine the \"fit\" (difference) with the \"emptiness\" (remaining capacity).\n        # Maybe penalize very large remaining capacities, even if they fit.\n        \n        # Let's use the difference again, as it directly measures \"how much space is left after fitting\".\n        # Smaller difference is better.\n        \n        diffs = valid_capacities - item\n        \n        # Scale diffs to be more in line with Softmax inputs (e.g., range -inf to +inf for exp)\n        # A common pattern is to use `exp(value)` where larger `value` is better.\n        # We want to maximize `-(diffs)`. So `exp(-diffs)`? No, we want to maximize score for smaller diffs.\n        # Let's use `exp(-diffs)` with `temperature`.\n        \n        temperature = 0.5 # Lower temperature means stronger preference for best fit\n        scaled_diffs = -diffs / temperature\n        \n        # Apply Softmax concept: exp(score) / sum(exp(scores))\n        # We can simply return exp(scaled_diffs) as the priority, which is proportional to softmax output.\n        \n        priorities = np.exp(scaled_diffs)\n        \n    \n    final_priorities = np.zeros_like(bins_remain_cap)\n    final_priorities[valid_bins] = priorities\n    \n    return final_priorities",
    "response_id": 29,
    "tryHS": false,
    "obj": 4.198244914240141,
    "SLOC": 26.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse proximity for tight fits with a sigmoid for smooth preference.\n    Favors bins with minimal remaining capacity after packing, scaled smoothly.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(eligible_bins_mask):\n        eligible_capacities = bins_remain_cap[eligible_bins_mask]\n        \n        # Inverse proximity: smaller gap is better (higher score)\n        # Adding a small epsilon to avoid division by zero\n        inverse_proximity = 1.0 / (eligible_capacities - item + 1e-9)\n\n        # Normalize inverse proximity to a range where sigmoid is effective\n        # Aims to map smaller gaps (higher inverse_proximity) to values around 0.5\n        # and larger gaps to values further from 0.5.\n        # This normalization is heuristic and can be tuned.\n        if np.max(inverse_proximity) > np.min(inverse_proximity):\n            normalized_scores = (inverse_proximity - np.min(inverse_proximity)) / (np.max(inverse_proximity) - np.min(inverse_proximity))\n        else: # All eligible bins have the same inverse proximity\n            normalized_scores = np.ones_like(inverse_proximity) * 0.5\n\n        # Sigmoid function to create a smooth priority distribution\n        # The steepness parameter (e.g., 10) can be tuned.\n        # We want bins with smaller gaps (higher normalized_scores) to have higher sigmoid outputs.\n        # So, we invert the normalized_scores for the sigmoid input to favor smaller gaps.\n        sigmoid_priorities = 1 / (1 + np.exp(-10 * (normalized_scores - 0.5)))\n\n        priorities[eligible_bins_mask] = sigmoid_priorities\n        \n        # Ensure that if all eligible bins are identical in terms of fit, they get a neutral priority\n        if np.all(priorities[eligible_bins_mask] == 0.5) and len(eligible_bins_mask) > 0:\n            priorities[eligible_bins_mask] = 0.5\n\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response6.txt_stdout.txt",
    "code_path": "problem_iter2_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins that are a tight fit using an inverse proximity measure,\n    giving infinite priority to perfect fits to encourage consolidation.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        differences = suitable_bins_caps - item\n\n        priorities[suitable_bins_mask] = 1.0 / (differences + 1e-9)\n\n        # Assign infinite priority to perfect fits\n        perfect_fit_mask = (differences == 0)\n        if np.any(perfect_fit_mask):\n            priorities[suitable_bins_mask][perfect_fit_mask] = float('inf')\n            \n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines inverse proximity with a sigmoid for smoother prioritization.\n\n    Favors bins with tight fits, but also provides non-zero priority for\n    less tight fits to encourage exploration.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        \n        # Inverse proximity for tight fits (similar to priority_v0)\n        proximity = suitable_bins_caps - item\n        \n        # Use a scaled sigmoid on proximity to create a smoother distribution\n        # Scaling factor `alpha` controls steepness. Higher alpha means steeper curve.\n        alpha = 10.0 \n        # Add a small epsilon to avoid division by zero for perfect fits\n        inverse_proximity_scores = 1.0 / (proximity + 1e-9)\n        \n        # Normalize inverse proximity scores to be between 0 and 1\n        if np.max(inverse_proximity_scores) > 0:\n            normalized_inverse_proximity = inverse_proximity_scores / np.max(inverse_proximity_scores)\n        else:\n            normalized_inverse_proximity = np.zeros_like(inverse_proximity_scores)\n\n        # Sigmoid transformation to map scores to a [0, 1] range, emphasizing tighter fits\n        # Adjusting the sigmoid's center and steepness can tune behavior.\n        # Here, we center it around a value that would correspond to a \"good\" proximity.\n        # For simplicity, we'll use a sigmoid on the normalized inverse proximity.\n        # A higher score from inverse proximity should map to a higher sigmoid output.\n        sigmoid_scores = 1 / (1 + np.exp(-alpha * (normalized_inverse_proximity - 0.5))) # Adjusted sigmoid\n\n        priorities[suitable_bins_mask] = sigmoid_scores\n        \n        # Ensure perfect fits still get a high priority, potentially capped by sigmoid\n        perfect_fit_mask = (proximity == 0)\n        if np.any(perfect_fit_mask):\n            priorities[suitable_bins_mask][perfect_fit_mask] = 1.0 # Assign max priority for perfect fit\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response3.txt_stdout.txt",
    "code_path": "problem_iter3_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    This heuristic aims to improve upon priority_v1 by considering the \"waste\"\n    of a bin after placing the item, and also giving a slight preference\n    to bins that are already relatively full.\n\n    The priority is calculated as follows:\n    1. For bins that can fit the item (bins_remain_cap >= item):\n       - Calculate the remaining capacity after placing the item: `remaining_capacity - item`.\n       - To prioritize bins with less remaining capacity (i.e., more full), we want to minimize this difference.\n         A good way to convert minimization to maximization (for priority) is to use the negative of the difference: `item - remaining_capacity`.\n       - Additionally, we can add a small bonus for bins that are already quite full. A bin with `remaining_capacity` closer to 0 (but still >= item) is generally preferred.\n         We can achieve this by adding a term that increases as `remaining_capacity` decreases. For example, we can use `1 / (bins_remain_cap + epsilon)` where epsilon is a small constant to avoid division by zero.\n         A simpler approach might be to add a term like `(bin_capacity - bins_remain_cap)`. Since we don't have bin_capacity, we can approximate this by considering how \"full\" the bin is relative to the item itself.\n         Let's consider the relative \"fullness\" of the bin *after* placing the item. A bin that becomes `0.1` full is better than one that becomes `0.5` full, if the item is the same.\n         So, we want to maximize `-(remaining_capacity - item)`. This is `item - remaining_capacity`.\n         To also favor bins that are already more full, we can add a term related to how much space is *left* relative to the item's size. A bin with `remaining_capacity = 0.5` and `item = 0.4` has `0.1` space left. A bin with `remaining_capacity = 0.9` and `item = 0.4` has `0.5` space left. We prefer the former.\n         So `item - remaining_capacity` seems good.\n         To incorporate the \"already full\" aspect, we can consider the *inverse* of the remaining capacity *after* packing. A bin with very little capacity left is good.\n         Let's try `(item - bins_remain_cap) + C * (1 / (bins_remain_cap - item + epsilon))` where C is a small constant.\n         A simpler heuristic that balances fitting tightly and preferring fuller bins could be:\n         Maximize `(item - bins_remain_cap)` (tight fit) + `(1 / (bins_remain_cap + epsilon))` (already full).\n         Let's use `epsilon = 1e-6` to avoid division by zero.\n\n    2. For bins that cannot fit the item:\n       - Assign a very low priority (e.g., -np.inf).\n    \"\"\"\n    \n    fit_mask = bins_remain_cap >= item\n    \n    \n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    \n    epsilon = 1e-6\n    \n    \n    bins_that_fit_cap = bins_remain_cap[fit_mask]\n    \n    \n    tight_fit_score = item - bins_that_fit_cap\n    \n    \n    fullness_score = 1.0 / (bins_that_fit_cap - item + epsilon)\n    \n    \n    priorities[fit_mask] = tight_fit_score + fullness_score\n    \n    \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  }
]