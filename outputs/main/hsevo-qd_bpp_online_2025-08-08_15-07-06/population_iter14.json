[
  {
    "stdout_filepath": "problem_iter1_response17.txt_stdout.txt",
    "code_path": "problem_iter1_code17.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    This is a heuristic for the online bin packing problem.\n    We want to prioritize bins that are \"almost full\" but can still fit the item.\n    This strategy aims to fill bins as much as possible before opening new ones.\n\n    The priority is calculated as follows:\n    1. For bins that can fit the item:\n       - Calculate how much \"space\" is left after fitting the item.\n       - We want to *minimize* this leftover space, but we also don't want to penalize bins that are already very full.\n       - A good measure might be `(bin_capacity - item) / bin_capacity` if we're talking about proportions,\n         but we have remaining capacities. So, `remaining_capacity - item`.\n       - To get a \"priority\" where higher is better, we can take the negative of this difference: `item - remaining_capacity`.\n         This means a bin with `remaining_capacity = 0.9` and `item = 0.8` gives `-0.7` (higher is better than -0.2 for a bin with `remaining_capacity = 0.3`).\n       - To avoid very large negative numbers for bins that are too small, we can set the priority to a very low number (or zero) if the item doesn't fit.\n\n    2. For bins that cannot fit the item:\n       - Assign a very low priority (e.g., 0 or negative infinity effectively, but we'll use 0).\n\n    Let's refine this:\n    We want to put the item into a bin where the remaining capacity is *just enough* or slightly more than the item.\n    If remaining_capacity >= item:\n        Priority = some_function(remaining_capacity - item)\n    Else:\n        Priority = -infinity (effectively 0 for practical purposes if others are positive)\n\n    Consider the difference: `bins_remain_cap - item`.\n    If this difference is negative, the item doesn't fit. We'll assign a very low priority.\n    If this difference is non-negative, we want to prioritize bins where this difference is *smallest* (closest to zero).\n    So, we want to maximize `-(bins_remain_cap - item) = item - bins_remain_cap`.\n    This means if a bin has `rem_cap = 1.0` and `item = 0.5`, priority is `-0.5`.\n    If a bin has `rem_cap = 0.6` and `item = 0.5`, priority is `-0.1`. The latter is higher priority.\n\n    Let's make it simpler. We want the bin where `bins_remain_cap` is *closest to `item`*, but greater than or equal to `item`.\n    This is like finding the minimum of `bins_remain_cap - item` for all `bins_remain_cap >= item`.\n    To turn this into a \"priority\" (higher is better), we can use `-abs(bins_remain_cap - item)` or `item - bins_remain_cap`.\n    Let's stick with `item - bins_remain_cap`.\n\n    Priorities will be negative. Higher values mean a better fit.\n    We need to handle the case where `bins_remain_cap < item`.\n    \"\"\"\n    \n    fit_mask = bins_remain_cap >= item\n    \n    \n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    \n    priorities[fit_mask] = item - bins_remain_cap[fit_mask]\n    \n    \n    return priorities",
    "response_id": 17,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a First Fit strategy variant.\n\n    The priority is higher for bins that can accommodate the item and have a remaining\n    capacity closer to the item's size. This encourages tighter packing.\n    Bins that cannot accommodate the item are given a priority of -1.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_remain_cap.size > 0:\n        differences = fitting_bins_remain_cap - item\n        # Higher priority for smaller differences (tighter fit)\n        # We use -differences to make smaller differences result in higher scores.\n        # Adding a small constant to avoid zero priorities for perfect fits\n        # and to ensure valid bins have a positive priority.\n        priorities[can_fit_mask] = -differences + 1.0\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response4.txt_stdout.txt",
    "code_path": "problem_iter1_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i, cap in enumerate(bins_remain_cap):\n        if cap >= item:\n            priorities[i] = 1 / (cap - item + 1e-9)\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.198244914240141,
    "SLOC": 6.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response5.txt_stdout.txt",
    "code_path": "problem_iter1_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit) strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    for i, remaining_cap in enumerate(bins_remain_cap):\n        if remaining_cap >= item:\n            \n            proximity = remaining_cap - item\n            \n            if proximity == 0:\n                priorities[i] = float('inf') \n            else:\n                priorities[i] = 1.0 / proximity\n                \n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 10.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response8.txt_stdout.txt",
    "code_path": "problem_iter8_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit (inverse proximity) with a penalty for large gaps,\n    favoring bins that minimize waste but are not excessively large.\n    \"\"\"\n    \n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        \n        \n        differences = suitable_bins_caps - item\n        \n        \n        # Assign high priority for tight fits (small differences)\n        # Use inverse proximity, adding a small epsilon to avoid division by zero\n        # and a term to penalize larger differences more heavily (e.g., quadratic)\n        # Combining 1/(diff + eps) and -(diff^2) to balance tight fit and gap penalty.\n        # Weights can be tuned; here, the inverse proximity is primary.\n        scores = (1.0 / (differences + 1e-9)) - (differences**2) \n        \n        priorities[suitable_bins_mask] = scores\n        \n        # Ensure perfect fits get the highest possible finite priority if the formula doesn't naturally yield it.\n        # This is a safeguard and usually covered by the 1/(diff+eps) term if diff is 0.\n        perfect_fit_mask = (differences == 0)\n        if np.any(perfect_fit_mask):\n            priorities[suitable_bins_mask][perfect_fit_mask] = np.finfo(float).max\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit and inverse proximity for Bin Packing priority.\n    Prioritizes bins that are closer fits, with a strong preference for the absolute best fit.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(eligible_bins_mask):\n        eligible_capacities = bins_remain_cap[eligible_bins_mask]\n        differences = eligible_capacities - item\n\n        # Strategy 1: Inverse proximity (favoring smaller remaining capacities)\n        # Add epsilon for numerical stability and to avoid division by zero.\n        inverse_proximity_scores = 1.0 / (differences + 1e-9)\n\n        # Strategy 2: Identify the absolute best fit(s)\n        min_diff = np.min(differences)\n        best_fit_mask_local = (differences == min_diff)\n\n        # Combine strategies: Give a significantly higher priority to the absolute best fit(s)\n        # and use inverse proximity for others.\n        # We scale the best fit scores to be clearly dominant.\n        combined_scores = np.zeros_like(eligible_capacities)\n        combined_scores[best_fit_mask_local] = 100.0  # High priority for best fit\n        combined_scores[~best_fit_mask_local] = inverse_proximity_scores[~best_fit_mask_local]\n\n        # Normalize scores to be in a reasonable range, though not strictly probabilities here.\n        # Using Softmax-like scaling for non-best-fit items to maintain relative preference.\n        non_best_fit_scores = inverse_proximity_scores[~best_fit_mask_local]\n        if non_best_fit_scores.size > 0:\n            exp_scores = np.exp(non_best_fit_scores - np.max(non_best_fit_scores))\n            normalized_non_best_fit = exp_scores / np.sum(exp_scores)\n            combined_scores[~best_fit_mask_local] = normalized_non_best_fit\n\n        # Assign combined scores back to the original array structure\n        priorities[eligible_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 19.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A Softmax-based priority function for the online Bin Packing Problem.\n\n    This function calculates the priority of placing an item into each available bin.\n    It considers the remaining capacity of each bin relative to the item size.\n    Bins that can accommodate the item without exceeding their capacity are favored.\n    Among the bins that can accommodate the item, those with less remaining capacity\n    (i.e., tighter fits) are given a higher priority, encouraging fuller bins first.\n    The Softmax function is used to convert these relative preferences into a\n    probability distribution, ensuring that higher priority bins have a greater chance\n    of being selected.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element represents the remaining\n                         capacity of a bin.\n\n    Returns:\n        A numpy array of the same shape as bins_remain_cap, where each element\n        is the priority score for placing the item into the corresponding bin.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    eligible_capacities = bins_remain_cap[eligible_bins_mask]\n\n    if eligible_capacities.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit\" score for eligible bins. A smaller remaining capacity\n    # (tighter fit) results in a higher score. We use the negative difference\n    # to make larger remaining capacities (less good fits) have smaller scores.\n    fit_scores = -(eligible_capacities - item)\n\n    # Apply Softmax to get probabilities (priorities).\n    # Adding a small epsilon to avoid log(0) issues if fit_scores can be zero.\n    epsilon = 1e-9\n    exp_scores = np.exp(fit_scores - np.max(fit_scores)) # Stability trick for softmax\n    priorities = exp_scores / np.sum(exp_scores)\n\n    # Map priorities back to the original bin structure\n    full_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    full_priorities[eligible_bins_mask] = priorities\n\n    return full_priorities",
    "response_id": 19,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response11.txt_stdout.txt",
    "code_path": "problem_iter1_code11.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    if suitable_bins_caps.size > 0:\n        differences = suitable_bins_caps - item\n        min_diff = np.min(differences)\n        \n        suitable_bin_indices = np.where(suitable_bins_mask)[0]\n        \n        for i, original_index in enumerate(suitable_bin_indices):\n            if bins_remain_cap[original_index] - item == min_diff:\n                priorities[original_index] = 1.0\n            else:\n                priorities[original_index] = 0.0\n    return priorities",
    "response_id": 11,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response8.txt_stdout.txt",
    "code_path": "problem_iter5_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (minimizing remaining capacity after packing)\n    with a penalty for bins that are excessively large, using an exponential scaling.\n    \"\"\"\n    fit_mask = bins_remain_cap >= item\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    epsilon = 1e-9\n    \n    if np.any(fit_mask):\n        eligible_capacities = bins_remain_cap[fit_mask]\n        \n        # Calculate the \"gap\" or remaining capacity after placing the item\n        gaps = eligible_capacities - item\n        \n        # Prioritize bins with smaller gaps (tighter fit).\n        # Use inverse of gap + epsilon for a higher score for smaller gaps.\n        # Also, apply a penalty for very large gaps by considering the inverse of capacity itself.\n        # A simple way to combine is to favor smaller gaps and penalize large capacities.\n        # Let's use the exponential of the negative gap to directly map smaller gaps to higher scores,\n        # similar to Softmax inputs, and tune with temperature.\n        \n        # Temperature parameter to control the \"aggressiveness\" of the heuristic.\n        # Lower temperature means stronger preference for the best fit.\n        temperature = 0.5 \n        \n        # Score is based on negative gap (higher score for smaller gap)\n        # Adding a small bonus for already fuller bins (lower eligible_capacities)\n        # A combined score could be: -gap - (1/eligible_capacity)\n        # For exponential scaling, let's use -gap directly, and the exponential will handle the grading.\n        # We can further adjust by considering the inverse of capacity.\n        # A common approach is to use `exp(-gap / T)`.\n        # To also favor fuller bins, we can add a term proportional to `1/capacity`.\n        # Let's try: `exp(-(gap - C * (1.0/eligible_capacity)) / T)` where C is a weight.\n        # For simplicity, let's focus on the gap primarily and use the exponential.\n        # The prompt also mentions penalizing very large remaining capacities.\n        # Let's try `exp(-gap)` and see how it behaves.\n        # To incorporate \"fullness preference\": maybe `exp(-(gap - alpha * (1/eligible_capacity)))`\n        \n        # A robust combination often seen is maximizing `-gap` and `1/capacity`.\n        # Let's try `exp( (-gaps - 1.0/eligible_capacities) / temperature )`\n        # This prioritizes small gaps and small capacities.\n        \n        # Simplified approach: Use negative gap scaled by temperature.\n        # This prioritizes \"best fit\" strongly.\n        scaled_scores = -gaps / temperature\n        \n        # Another approach from literature often used with Softmax: maximize `-(gap - C * (1/capacity))`\n        # Let's try `exp( -(gaps - 1.0/eligible_capacities) / temperature )`\n        # Where `1.0/eligible_capacities` gives higher score to fuller bins.\n        \n        # Let's select a more established approach: prioritize bins with minimal remaining capacity after packing (Best Fit),\n        # and use exponential scaling for graded priorities.\n        # This directly aligns with minimizing bin count.\n        \n        # Calculate priority based on the negative gap, scaled by temperature.\n        # Higher priority for smaller gaps (tighter fits).\n        priorities[fit_mask] = np.exp(-gaps / temperature)\n        \n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response15.txt_stdout.txt",
    "code_path": "problem_iter1_code15.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the difference between the bin's remaining capacity and the item's size.\n    # A smaller difference means the item fits \"better\" or closer to the bin's capacity.\n    # We add a small epsilon to avoid division by zero if a bin is perfectly full or the item size is 0.\n    diffs = bins_remain_cap - item\n    priorities = 1.0 / (np.abs(diffs) + 1e-9)\n\n    # We want to prioritize bins that can actually fit the item.\n    # If an item cannot fit, its priority should be very low.\n    # We can achieve this by multiplying the inverse difference by a mask\n    # that is 1 for bins that can fit the item and 0 otherwise.\n    can_fit_mask = (bins_remain_cap >= item).astype(float)\n    priorities *= can_fit_mask\n\n    return priorities",
    "response_id": 15,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response29.txt_stdout.txt",
    "code_path": "problem_iter1_code29.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    valid_bins = bins_remain_cap >= item\n    \n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n\n    valid_capacities = bins_remain_cap[valid_bins]\n    \n    # Higher remaining capacity means lower priority (we want to fill bins)\n    # To use Softmax effectively, we want larger values to correspond to higher priority.\n    # So we can use (large_capacity - remaining_capacity) or 1/(remaining_capacity)\n    # Let's try 1/(remaining_capacity) as it penalizes bins that are already very full.\n    \n    inverted_capacities = 1.0 / valid_capacities\n    \n    # To further encourage fitting into bins with *just enough* space, \n    # we can add a term that penalizes bins with very large remaining capacity.\n    # Let's try subtracting the ratio of remaining capacity to total capacity (assuming initial bin capacity is known or can be estimated).\n    # For simplicity here, let's just use the inverse capacity.\n    \n    # Adding a small epsilon to avoid division by zero if a bin has 0 remaining capacity, though valid_bins should prevent this.\n    epsilon = 1e-9\n    scores = 1.0 / (valid_capacities + epsilon)\n    \n    # Softmax is often used to turn scores into probabilities or weights.\n    # A higher score should mean a higher probability of selection.\n    # Let's scale the scores to be positive and somewhat related to \"how well\" it fits.\n    # A common approach in fitting is to maximize the remaining capacity, \n    # but here we want to minimize the number of bins. So we prefer bins that are *almost* full.\n    # Let's try prioritizing bins where item fits snugly.\n    \n    fit_difference = valid_capacities - item\n    # We want to minimize fit_difference. To make it a priority (higher is better), we invert it.\n    # Add a small constant to avoid division by zero if fit_difference is 0.\n    priority_scores = 1.0 / (fit_difference + epsilon)\n    \n    # Apply Softmax to convert scores into probabilities (weights)\n    # We add a small penalty for bins that have much more capacity than needed to discourage very loose fits.\n    # Let's consider the \"waste\" factor. Waste = remaining_capacity - item\n    # We want to minimize waste.\n    \n    # Let's try a heuristic that favors bins that have enough capacity but not excessively more.\n    # We can try a value that increases as remaining_capacity gets closer to item.\n    \n    # Option 1: Prioritize bins that are almost full (minimum remaining capacity that fits item)\n    # We want higher scores for smaller `valid_capacities`. So `1/valid_capacities` or similar.\n    # To be more specific, we want `valid_capacities - item` to be small.\n    # So we can use `1.0 / (valid_capacities - item + epsilon)`\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bins] = 1.0 / (valid_capacities - item + epsilon)\n    \n    # Apply Softmax: exp(score) / sum(exp(scores))\n    # For just returning priority scores for selection, we can directly use the calculated scores\n    # or apply a transformation like Softmax.\n    # If we want to select *one* bin based on highest priority, we can just return the scores directly.\n    # If we want to weight bins for some probabilistic selection, Softmax is good.\n    # For this problem, simply returning the scores that indicate preference is sufficient.\n    \n    # Let's refine to prioritize bins where remaining_capacity - item is minimized.\n    # A simple approach for priority is the inverse of the difference.\n    \n    scores = np.zeros_like(bins_remain_cap)\n    scores[valid_bins] = 1.0 / (valid_capacities - item + epsilon)\n\n    # To make it more \"Softmax-like\" in spirit of distribution, \n    # we can use a sigmoid-like transformation or directly use scaled values.\n    # Let's consider a temperature parameter to control the \"sharpness\" of priorities.\n    temperature = 1.0\n    \n    # Let's make values larger for better fits.\n    # A potential issue is if all valid capacities are very large, leading to small inverse values.\n    # We need to ensure scores are somewhat comparable or scaled.\n    \n    # Consider the \"tightness of fit\" as the primary driver.\n    # Tightest fit = smallest (remaining_capacity - item).\n    # So, priority is inversely proportional to (remaining_capacity - item).\n    \n    normalized_scores = np.zeros_like(bins_remain_cap)\n    if np.any(valid_bins):\n        # Calculate scores: higher score for tighter fit\n        # We want to maximize (1 / (remaining_capacity - item))\n        # Or to avoid issues with very small differences, maybe prioritize directly by minimum remaining capacity that fits.\n        \n        # Let's try a direct mapping:\n        # A bin is \"good\" if remaining_capacity is just enough.\n        # So, priority is high when remaining_capacity is close to item.\n        \n        # Let's use `remaining_capacity` itself as a negative factor for priority\n        # and `item` as a positive factor.\n        # How about prioritizing bins with smaller remaining capacity that can still fit the item?\n        # This aligns with the First Fit Decreasing heuristic's goal of filling bins.\n        \n        # Let's map the difference `valid_capacities - item` to a priority.\n        # Smaller difference should yield higher priority.\n        \n        # Example: item = 3, capacities = [5, 7, 10]\n        # Valid capacities = [5, 7, 10]\n        # Differences = [2, 4, 7]\n        # We want to prioritize bins with difference 2, then 4, then 7.\n        # So, 1/2, 1/4, 1/7 would work.\n        \n        diffs = valid_capacities - item\n        priorities = 1.0 / (diffs + epsilon)\n        \n        # Now, to make it more \"Softmax-like\" if we were to select probabilistically,\n        # we can exponentiate and normalize. But for direct priority score, this is fine.\n        # Let's add a small value to all priorities to avoid negative exponents in a Softmax if we were to use it.\n        # And let's scale them to prevent numerical underflow or overflow with Softmax.\n        \n        # For a direct priority score where higher means better, \n        # this inverse difference works well for \"best fit\" aspect.\n        \n        # Consider what happens if multiple bins have the exact same \"best fit\" difference.\n        # The current approach would give them equal priority.\n        \n        # To incorporate the \"Softmax-Based Fit\" idea, let's interpret it as:\n        # transform the \"fitness\" of a bin (how well it fits the item) into a priority.\n        # The fitness can be related to how close `remaining_capacity` is to `item`.\n        \n        # Let's define fitness as: -(remaining_capacity - item)^2. Higher fitness for smaller squared difference.\n        # Or, more simply, as we did: 1.0 / (remaining_capacity - item + epsilon)\n        \n        # Softmax transformation of these scores to get a distribution if needed.\n        # For now, we just need the scores themselves.\n        \n        # Let's try to directly use the remaining capacity for scaling, \n        # encouraging smaller capacities that fit.\n        \n        # Prioritize bins with the smallest remaining capacity that can fit the item.\n        # So, the priority score should be higher for smaller `valid_capacities`.\n        # Let's try `1.0 / valid_capacities`.\n        \n        # Consider a case: item = 2, bins_remain_cap = [3, 5, 10]\n        # Valid bins = [3, 5, 10]\n        # Option A (inverse diff): 1/(3-2)=1, 1/(5-2)=0.33, 1/(10-2)=0.125. Prioritizes bin with 3. (Best Fit)\n        # Option B (inverse capacity): 1/3=0.33, 1/5=0.2, 1/10=0.1. Prioritizes bin with 3.\n        \n        # If the goal is \"smallest number of bins\", then fitting into a nearly full bin is good.\n        # \"Best Fit\" heuristic is good for this.\n        \n        # Let's combine the \"fit\" (difference) with the \"emptiness\" (remaining capacity).\n        # Maybe penalize very large remaining capacities, even if they fit.\n        \n        # Let's use the difference again, as it directly measures \"how much space is left after fitting\".\n        # Smaller difference is better.\n        \n        diffs = valid_capacities - item\n        \n        # Scale diffs to be more in line with Softmax inputs (e.g., range -inf to +inf for exp)\n        # A common pattern is to use `exp(value)` where larger `value` is better.\n        # We want to maximize `-(diffs)`. So `exp(-diffs)`? No, we want to maximize score for smaller diffs.\n        # Let's use `exp(-diffs)` with `temperature`.\n        \n        temperature = 0.5 # Lower temperature means stronger preference for best fit\n        scaled_diffs = -diffs / temperature\n        \n        # Apply Softmax concept: exp(score) / sum(exp(scores))\n        # We can simply return exp(scaled_diffs) as the priority, which is proportional to softmax output.\n        \n        priorities = np.exp(scaled_diffs)\n        \n    \n    final_priorities = np.zeros_like(bins_remain_cap)\n    final_priorities[valid_bins] = priorities\n    \n    return final_priorities",
    "response_id": 29,
    "tryHS": false,
    "obj": 4.198244914240141,
    "SLOC": 26.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse proximity for tight fits with a sigmoid for smooth preference.\n    Favors bins with minimal remaining capacity after packing, scaled smoothly.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(eligible_bins_mask):\n        eligible_capacities = bins_remain_cap[eligible_bins_mask]\n        \n        # Inverse proximity: smaller gap is better (higher score)\n        # Adding a small epsilon to avoid division by zero\n        inverse_proximity = 1.0 / (eligible_capacities - item + 1e-9)\n\n        # Normalize inverse proximity to a range where sigmoid is effective\n        # Aims to map smaller gaps (higher inverse_proximity) to values around 0.5\n        # and larger gaps to values further from 0.5.\n        # This normalization is heuristic and can be tuned.\n        if np.max(inverse_proximity) > np.min(inverse_proximity):\n            normalized_scores = (inverse_proximity - np.min(inverse_proximity)) / (np.max(inverse_proximity) - np.min(inverse_proximity))\n        else: # All eligible bins have the same inverse proximity\n            normalized_scores = np.ones_like(inverse_proximity) * 0.5\n\n        # Sigmoid function to create a smooth priority distribution\n        # The steepness parameter (e.g., 10) can be tuned.\n        # We want bins with smaller gaps (higher normalized_scores) to have higher sigmoid outputs.\n        # So, we invert the normalized_scores for the sigmoid input to favor smaller gaps.\n        sigmoid_priorities = 1 / (1 + np.exp(-10 * (normalized_scores - 0.5)))\n\n        priorities[eligible_bins_mask] = sigmoid_priorities\n        \n        # Ensure that if all eligible bins are identical in terms of fit, they get a neutral priority\n        if np.all(priorities[eligible_bins_mask] == 0.5) and len(eligible_bins_mask) > 0:\n            priorities[eligible_bins_mask] = 0.5\n\n\n    return priorities",
    "response_id": 3,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response6.txt_stdout.txt",
    "code_path": "problem_iter2_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins that are a tight fit using an inverse proximity measure,\n    giving infinite priority to perfect fits to encourage consolidation.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        differences = suitable_bins_caps - item\n\n        priorities[suitable_bins_mask] = 1.0 / (differences + 1e-9)\n\n        # Assign infinite priority to perfect fits\n        perfect_fit_mask = (differences == 0)\n        if np.any(perfect_fit_mask):\n            priorities[suitable_bins_mask][perfect_fit_mask] = float('inf')\n            \n    return priorities",
    "response_id": 6,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines inverse proximity with a sigmoid for smoother prioritization.\n\n    Favors bins with tight fits, but also provides non-zero priority for\n    less tight fits to encourage exploration.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        \n        # Inverse proximity for tight fits (similar to priority_v0)\n        proximity = suitable_bins_caps - item\n        \n        # Use a scaled sigmoid on proximity to create a smoother distribution\n        # Scaling factor `alpha` controls steepness. Higher alpha means steeper curve.\n        alpha = 10.0 \n        # Add a small epsilon to avoid division by zero for perfect fits\n        inverse_proximity_scores = 1.0 / (proximity + 1e-9)\n        \n        # Normalize inverse proximity scores to be between 0 and 1\n        if np.max(inverse_proximity_scores) > 0:\n            normalized_inverse_proximity = inverse_proximity_scores / np.max(inverse_proximity_scores)\n        else:\n            normalized_inverse_proximity = np.zeros_like(inverse_proximity_scores)\n\n        # Sigmoid transformation to map scores to a [0, 1] range, emphasizing tighter fits\n        # Adjusting the sigmoid's center and steepness can tune behavior.\n        # Here, we center it around a value that would correspond to a \"good\" proximity.\n        # For simplicity, we'll use a sigmoid on the normalized inverse proximity.\n        # A higher score from inverse proximity should map to a higher sigmoid output.\n        sigmoid_scores = 1 / (1 + np.exp(-alpha * (normalized_inverse_proximity - 0.5))) # Adjusted sigmoid\n\n        priorities[suitable_bins_mask] = sigmoid_scores\n        \n        # Ensure perfect fits still get a high priority, potentially capped by sigmoid\n        perfect_fit_mask = (proximity == 0)\n        if np.any(perfect_fit_mask):\n            priorities[suitable_bins_mask][perfect_fit_mask] = 1.0 # Assign max priority for perfect fit\n\n    return priorities",
    "response_id": 9,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response1.txt_stdout.txt",
    "code_path": "problem_iter6_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    This heuristic aims to improve upon priority_v1 by introducing a multi-criteria approach\n    and a more nuanced scoring for bins that are not perfectly fitting but are still viable.\n\n    The strategy is to prioritize bins based on two main factors:\n    1. How well the item fits the remaining capacity (tightest fit).\n    2. How \"full\" the bin is after the item is placed (to encourage fuller bins).\n\n    Scoring logic:\n    - For bins that can fit the item:\n        - Calculate the \"tightness\" of the fit: `item - bins_remain_cap`. A smaller (less negative) value is better,\n          meaning the remaining capacity is closer to the item size.\n        - Calculate a \"fullness score\" which is a measure of how much capacity is left *after* placing the item.\n          We want to penalize leaving too much space, so we can use `-(bins_remain_cap - item)`.\n          A higher value here means less space is left, which is good.\n        - Combine these two scores. A simple weighted sum can be used.\n          Let `tightness_score = item - bins_remain_cap`.\n          Let `fullness_score = -(bins_remain_cap - item)`.\n          The combined score could be `w1 * tightness_score + w2 * fullness_score`.\n          Here, we want to prioritize bins where `bins_remain_cap - item` is minimized (closest to 0).\n          This means `item - bins_remain_cap` is maximized (closest to 0 from the negative side).\n          Let's consider `bins_remain_cap - item`. We want to minimize this for fitting bins.\n          To convert minimization to maximization for a priority score, we can use `- (bins_remain_cap - item)`.\n          So, `priority_component_1 = bins_remain_cap - item`. We want to maximize this.\n          If `bins_remain_cap = 1.0` and `item = 0.5`, `priority_component_1 = 0.5`.\n          If `bins_remain_cap = 0.6` and `item = 0.5`, `priority_component_1 = 0.1`.\n          This `priority_component_1` directly represents how much *extra* space is left. We want to minimize this extra space.\n          So, to maximize, we use `-(bins_remain_cap - item)`.\n\n        - Let's reconsider the goal: Find a bin where `bins_remain_cap >= item`.\n          We want the `bins_remain_cap` to be as close to `item` as possible.\n          This means minimizing `bins_remain_cap - item`.\n          So, a good score component is `-(bins_remain_cap - item)`.\n\n        - Let's also consider the \"wasted space\" after placing the item. This is `bins_remain_cap - item`.\n          We want to minimize this. So, `-(bins_remain_cap - item)` is a good candidate.\n\n        - Consider the *absolute* difference: `abs(bins_remain_cap - item)`. We want to minimize this.\n          So, `-abs(bins_remain_cap - item)` is a good score.\n\n        - What if we want to prioritize bins that are generally fuller, even if they aren't the *tightest* fit?\n          A bin with `bins_remain_cap = 0.9` and `item = 0.1` leaves `0.8` space.\n          A bin with `bins_remain_cap = 0.5` and `item = 0.4` leaves `0.1` space.\n          The second is a tighter fit and leaves less wasted space.\n\n        - Let's introduce a parameter `alpha` to balance two criteria:\n            1. Minimizing the \"slack\" or remaining space: `bins_remain_cap - item`. We want to maximize `-(bins_remain_cap - item)`.\n            2. Maximizing the overall remaining capacity of the bin *before* packing, as a secondary factor to encourage using generally fuller bins. This is `bins_remain_cap`.\n\n        - So, for bins that fit:\n            Score = `alpha * (bins_remain_cap - item) + (1 - alpha) * bins_remain_cap`\n            To maximize this score, we want `bins_remain_cap - item` to be small and `bins_remain_cap` to be large.\n            Let's try a different combination: Prioritize bins that leave the *least* amount of space.\n            This means minimizing `bins_remain_cap - item`.\n            So, a primary score component is `-(bins_remain_cap - item)`.\n\n            A secondary consideration could be the overall fullness of the bin.\n            Perhaps we can use the remaining capacity `bins_remain_cap` itself as a secondary score.\n            If `bins_remain_cap = 0.9` and `item = 0.5`, the difference is `0.4`.\n            If `bins_remain_cap = 0.55` and `item = 0.5`, the difference is `0.05`.\n            The second case is a tighter fit. `-(0.4)` vs `-(0.05)`. The second is higher.\n\n            What if we want to combine the tightness with a penalty for being *too* empty?\n            Consider `bins_remain_cap >= item`.\n            Score = `-(bins_remain_cap - item)` for tightness.\n            We could add a small bonus for `bins_remain_cap` itself, weighted.\n            Score = `-(bins_remain_cap - item) + beta * bins_remain_cap`\n            Let `beta = 0.1`.\n            Bin 1: `bins_remain_cap = 0.9`, `item = 0.5`. Score = `-(0.4) + 0.1 * 0.9 = -0.4 + 0.09 = -0.31`\n            Bin 2: `bins_remain_cap = 0.55`, `item = 0.5`. Score = `-(0.05) + 0.1 * 0.55 = -0.05 + 0.055 = 0.005`\n            Bin 2 has higher priority.\n\n            Let's try maximizing `bins_remain_cap - item` for fitting bins. This represents the smallest positive slack.\n            To turn it into a higher-is-better score, we can use `-(bins_remain_cap - item)`.\n            For bins that don't fit, we assign a very low score.\n\n            Let's refine the scoring:\n            For bins where `bins_remain_cap >= item`:\n            We want to prioritize bins that minimize `bins_remain_cap - item`.\n            This is equivalent to maximizing `item - bins_remain_cap`.\n            This is the same as `priority_v1`.\n\n            Let's think about \"granular scoring\" and \"multi-criteria fusion\".\n            We can create two components:\n            1. Tightness: `score_tight = -(bins_remain_cap - item)` for fitting bins, `-inf` otherwise.\n               This favors bins that are almost full.\n            2. Fillness: `score_fill = bins_remain_cap` for fitting bins, `-inf` otherwise.\n               This favors bins that have more capacity in general (potentially leading to fewer bins overall if items are small).\n\n            We can fuse these: `priority = w_tight * score_tight + w_fill * score_fill`.\n            Let's choose weights. If we want to primarily prioritize tightness, `w_tight` should be larger.\n            Let `w_tight = 1.0` and `w_fill = 0.5`.\n\n            So, for fitting bins:\n            `score = -(bins_remain_cap - item) + 0.5 * bins_remain_cap`\n            `score = -bins_remain_cap + item + 0.5 * bins_remain_cap`\n            `score = item - 0.5 * bins_remain_cap`\n\n            Example:\n            `item = 0.5`\n            Bin A: `bins_remain_cap = 0.9`. Fits.\n                `score_tight = -(0.9 - 0.5) = -0.4`\n                `score_fill = 0.9`\n                `score_A = -0.4 + 0.5 * 0.9 = -0.4 + 0.45 = 0.05`\n            Bin B: `bins_remain_cap = 0.55`. Fits.\n                `score_tight = -(0.55 - 0.5) = -0.05`\n                `score_fill = 0.55`\n                `score_B = -0.05 + 0.5 * 0.55 = -0.05 + 0.275 = 0.225`\n            Bin C: `bins_remain_cap = 0.5`. Fits.\n                `score_tight = -(0.5 - 0.5) = 0.0`\n                `score_fill = 0.5`\n                `score_C = 0.0 + 0.5 * 0.5 = 0.25`\n\n            In this example, Bin C has the highest priority, followed by Bin B, then Bin A.\n            This prioritizes the tightest fit, but also gives a boost to bins that have more capacity overall if the tightness is similar.\n\n            Let's adjust the weights. If we want to more strongly favor overall fullness, increase `w_fill`.\n            Let `w_tight = 1.0`, `w_fill = 1.0`.\n            `score = item - bins_remain_cap`\n            This is exactly `priority_v1`.\n\n            The goal is to be *better* than `priority_v1`.\n            `priority_v1` maximizes `item - bins_remain_cap`. This strongly favors the tightest fit.\n\n            Let's reconsider the advice: \"finely-grained, multi-dimensional scoring mechanisms that integrate diverse criteria (e.g., fit, fullness, strategic placement) through weighted sums or more complex fusion methods.\"\n\n            What if we use a non-linear function for tightness? Or a different way to combine?\n            A common strategy is \"Best Fit Decreasing\" (BFD), which sorts items first. Here it's online.\n            For online, \"First Fit\" (FF), \"Best Fit\" (BF), \"Worst Fit\" (WF), \"Most Full\" are common.\n            BF priority is `-(bins_remain_cap - item)`. This is what `priority_v1` does.\n\n            Let's try to explicitly penalize bins that are *too large* but still fit.\n            Suppose `item = 0.3`.\n            Bin X: `bins_remain_cap = 0.3`. `score_v1 = 0.3 - 0.3 = 0`.\n            Bin Y: `bins_remain_cap = 0.8`. `score_v1 = 0.3 - 0.8 = -0.5`.\n            `priority_v1` would pick Bin X.\n\n            What if we also want to consider the *current* fullness of the bin, not just how much space is left *after* packing?\n            Consider the ratio `item / bins_remain_cap`. High ratio means it's a good fit for a nearly full bin.\n            But this can lead to division by zero or very small numbers.\n\n            Let's try a score that is sensitive to both:\n            1. The gap: `gap = bins_remain_cap - item`. We want to minimize this gap.\n            2. The current bin fullness: `current_fill_ratio = (BIN_CAPACITY - bins_remain_cap) / BIN_CAPACITY`. We want this to be high.\n\n            If we assume `BIN_CAPACITY` is a constant, let's say `C`.\n            We want to minimize `bins_remain_cap - item`.\n            We want to maximize `(C - bins_remain_cap) / C`. This is equivalent to minimizing `bins_remain_cap`.\n\n            So, for fitting bins:\n            We want to prioritize bins with small `bins_remain_cap - item`.\n            And among those, prioritize bins with smaller `bins_remain_cap`.\n\n            This suggests a lexicographical ordering or a weighted sum.\n            Let's use a weighted sum:\n            `score = w1 * -(bins_remain_cap - item) + w2 * -(bins_remain_cap)`\n            Where `w1 > 0` and `w2 > 0`.\n            `score = w1 * (item - bins_remain_cap) - w2 * bins_remain_cap`\n\n            If `w1=1`, `w2=1`: `score = item - bins_remain_cap - bins_remain_cap = item - 2 * bins_remain_cap`.\n                `item = 0.5`\n                Bin X: `bins_remain_cap = 0.5`. `score = 0.5 - 2 * 0.5 = -0.5`.\n                Bin Y: `bins_remain_cap = 0.8`. `score = 0.5 - 2 * 0.8 = 0.5 - 1.6 = -1.1`.\n                Bin Z: `bins_remain_cap = 0.55`. `score = 0.5 - 2 * 0.55 = 0.5 - 1.1 = -0.6`.\n                Here, Bin X (tightest fit) is still best.\n\n            Let's try `w1=1`, `w2=0.5`: `score = item - bins_remain_cap - 0.5 * bins_remain_cap = item - 1.5 * bins_remain_cap`.\n                `item = 0.5`\n                Bin X: `bins_remain_cap = 0.5`. `score = 0.5 - 1.5 * 0.5 = 0.5 - 0.75 = -0.25`.\n                Bin Y: `bins_remain_cap = 0.8`. `score = 0.5 - 1.5 * 0.8 = 0.5 - 1.2 = -0.7`.\n                Bin Z: `bins_remain_cap = 0.55`. `score = 0.5 - 1.5 * 0.55 = 0.5 - 0.825 = -0.325`.\n                Bin X (tightest fit) is still best.\n\n            The current heuristic `priority_v1` already focuses on the tightest fit.\n            To be *better*, we might need to consider scenarios where the tightest fit is not optimal, or introduce a more robust scoring.\n\n            Let's introduce a factor that penalizes leaving *too much* excess space, but less severely than `priority_v1` does implicitly.\n\n            Consider the \"waste\" `bins_remain_cap - item`.\n            `priority_v1` is `- (bins_remain_cap - item)`.\n            This gives a score of 0 for a perfect fit, and increasingly negative scores as the excess space increases.\n\n            What if we use a score that is higher for a perfect fit, and then drops, but not as steeply?\n            For example, a function like `max(0, item - bins_remain_cap)` is what we want to maximize.\n            This is `priority_v1`.\n\n            Let's try to make the score more \"granular\".\n            Instead of just penalizing negative differences, let's reward positive differences (slack) up to a certain point.\n\n            For bins that can fit (`bins_remain_cap >= item`):\n            Primary goal: Minimize `bins_remain_cap - item`.\n            Secondary goal: If two bins have similar small gaps, pick the one that is fuller (smaller `bins_remain_cap`).\n\n            Let's define a score function for fitting bins:\n            `score = -(bins_remain_cap - item)**2 + lambda * bins_remain_cap`\n            The `-(bins_remain_cap - item)**2` term penalizes larger gaps quadratically.\n            The `lambda * bins_remain_cap` term favors fuller bins.\n\n            Let `lambda = 0.1`.\n            `item = 0.5`\n            Bin X: `bins_remain_cap = 0.5`. Gap = 0. Score = `-0**2 + 0.1 * 0.5 = 0.05`.\n            Bin Y: `bins_remain_cap = 0.55`. Gap = 0.05. Score = `-(0.05)**2 + 0.1 * 0.55 = -0.0025 + 0.055 = 0.0525`.\n            Bin Z: `bins_remain_cap = 0.8`. Gap = 0.3. Score = `-(0.3)**2 + 0.1 * 0.8 = -0.09 + 0.08 = -0.01`.\n\n            Here, Bin Y is preferred, then Bin X, then Bin Z.\n            This might be an improvement over `priority_v1` which would strongly prefer X over Y and Z.\n            This new heuristic prioritizes a small positive gap (like Bin Y) slightly over a perfect fit (Bin X),\n            while still penalizing larger gaps (Bin Z).\n            The parameter `lambda` controls how much we care about overall fullness.\n\n            Let's refine the functional form. We want to maximize the score.\n            The `-(bins_remain_cap - item)**2` term aims to keep `bins_remain_cap - item` close to zero.\n            To maximize this term, we want `bins_remain_cap - item` to be as close to 0 as possible.\n            The `lambda * bins_remain_cap` term favors smaller `bins_remain_cap`.\n\n            So, the combined score: `-(bins_remain_cap - item)**2 + lambda * bins_remain_cap`\n            We want to maximize this.\n            Let's set `lambda = 0.1`.\n\n            We need to handle the `bins_remain_cap < item` case. Assign a very low score (e.g., -infinity).\n    \"\"\"\n    \n    fit_mask = bins_remain_cap >= item\n    \n    \n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    \n    fitting_bins_remain_cap = bins_remain_cap[fit_mask]\n    \n    \n    gap = fitting_bins_remain_cap - item\n    \n    \n    lambda_param = 0.1 # Parameter to balance tightness and overall bin fullness\n    \n    \n    scores = -(gap**2) + lambda_param * fitting_bins_remain_cap\n    \n    \n    priorities[fit_mask] = scores\n    \n    \n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.038691663342641,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response0.txt_stdout.txt",
    "code_path": "problem_iter5_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit and a penalty for excessive remaining capacity.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins_mask = bins_remain_cap >= item\n\n    if not np.any(valid_bins_mask):\n        return priorities\n\n    valid_caps = bins_remain_cap[valid_bins_mask]\n    epsilon = 1e-9\n\n    # Heuristic 1: Best Fit (prioritize bins with minimal remaining capacity after packing)\n    # This is achieved by using the inverse of the difference.\n    # A smaller difference (remaining_cap - item) means a tighter fit and higher priority.\n    fit_difference = valid_caps - item\n    best_fit_scores = 1.0 / (fit_difference + epsilon)\n\n    # Heuristic 18/19/20 inspired: Penalize bins with excessively large remaining capacity.\n    # We can scale the remaining capacity itself. Higher remaining capacity should have lower priority.\n    # Using inverse of remaining capacity to reflect this.\n    # A larger remaining capacity means a lower score here.\n    excess_capacity_scores = 1.0 / (valid_caps + epsilon)\n\n    # Combine scores: We want both a tight fit AND not too much excess capacity.\n    # Multiplying the scores gives a combined priority that favors bins that are both\n    # close to fitting the item AND not overly large.\n    # A small positive constant is added to `fit_difference` to avoid division by zero.\n    # A small positive constant is added to `valid_caps` for similar reasons.\n    combined_scores = best_fit_scores * excess_capacity_scores\n\n    # Normalize scores to a range that might be useful for probabilistic selection or just for relative comparison.\n    # Using Softmax-like scaling (exponential) can highlight preferences more strongly.\n    # Let's use exp(-scaled_difference) to map smaller differences to higher scores, and then scale by excess capacity.\n    # A temperature parameter can control the sharpness of the priority.\n    temperature = 0.5  # Tunable parameter: lower value for stronger preference to best fit\n    \n    # Re-calculating scaled difference for exponential mapping to emphasize tight fits\n    # Map differences to exponents: smaller difference -> larger exponent -> larger priority\n    scaled_exponents = -fit_difference / temperature\n\n    # Combine the \"tight fit\" exponential score with the \"less excess capacity\" inverse score.\n    # Multiplication here means we want high scores in both aspects.\n    final_scores_for_valid_bins = np.exp(scaled_exponents) * excess_capacity_scores\n\n    # Assign the computed scores back to the original bins array\n    priorities[valid_bins_mask] = final_scores_for_valid_bins\n\n    # Ensure no negative priorities and handle edge cases (e.g., all priorities are zero)\n    # If all valid bins resulted in NaN or Inf, or very low scores, this can be a fallback.\n    # If all scores are zero (e.g., no valid bins, handled earlier, or all resulted in ~0 scores)\n    # we might want a uniform preference, but the zero initialization already covers this.\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response1.txt_stdout.txt",
    "code_path": "problem_iter5_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines inverse proximity with exponential decay for nuanced bin selection.\n\n    Favors tighter fits while allowing some preference for less tight bins,\n    tuned by an exponential decay parameter.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n        # Inverse proximity for tight fits\n        proximity = suitable_bins_caps - item\n        inverse_proximity_scores = 1.0 / (proximity + 1e-9)\n\n        # Normalize inverse proximity scores to a [0, 1] range\n        if np.max(inverse_proximity_scores) > 0:\n            normalized_inverse_proximity = inverse_proximity_scores / np.max(inverse_proximity_scores)\n        else:\n            normalized_inverse_proximity = np.zeros_like(inverse_proximity_scores)\n\n        # Use exponential decay on normalized inverse proximity.\n        # A higher normalized inverse proximity (tighter fit) results in a score closer to 1.\n        # The temperature parameter controls the decay rate. Smaller temperature -> steeper decay.\n        temperature = 0.5\n        exponential_scores = np.exp(normalized_inverse_proximity / temperature)\n\n        # Normalize exponential scores to a [0, 1] range.\n        if np.max(exponential_scores) > 0:\n            final_scores = exponential_scores / np.max(exponential_scores)\n        else:\n            final_scores = np.zeros_like(exponential_scores)\n\n        priorities[suitable_bins_mask] = final_scores\n\n        # Ensure perfect fits receive the maximum priority (1.0)\n        perfect_fit_mask = (proximity == 0)\n        if np.any(perfect_fit_mask):\n            priorities[suitable_bins_mask][perfect_fit_mask] = 1.0\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 22.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response6.txt_stdout.txt",
    "code_path": "problem_iter5_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fitting preference with a Softmax-based distribution.\n    Favors bins with minimal remaining capacity after placement, using Softmax\n    to create a graded priority, encouraging exploration of good fits.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    eligible_capacities = bins_remain_cap[eligible_bins_mask]\n\n    if eligible_capacities.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Prioritize tighter fits: smaller (capacity - item) is better.\n    # We use the negative difference for Softmax, so smaller differences\n    # lead to larger (more positive) exponents.\n    # This is inspired by Heuristic 7 (Softmax on negative differences).\n    differences = eligible_capacities - item\n    scores = -differences\n\n    # Use Softmax to generate a probability distribution over eligible bins.\n    # This creates graded priorities, favoring tighter fits more strongly.\n    # Stability trick: subtract max score before exponentiating.\n    # This is the core of Heuristic 7.\n    if np.max(scores) - np.min(scores) > 1e6: # Heuristic for numerical stability if scores vary extremely\n        scaled_scores = (scores - np.min(scores)) / (np.max(scores) - np.min(scores) + 1e-9)\n    else:\n        scaled_scores = scores\n\n    exp_scores = np.exp(scaled_scores - np.max(scaled_scores))\n    priorities = exp_scores / np.sum(exp_scores)\n\n    # Map priorities back to the original bin structure\n    full_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    full_priorities[eligible_bins_mask] = priorities\n\n    return full_priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response4.txt_stdout.txt",
    "code_path": "problem_iter5_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins based on a combination of tightest fit and overall bin fullness.\n    Uses a sigmoid function to provide graded priorities for suitable bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        \n        # Metric 1: Tightest fit (inverse of remaining capacity after packing)\n        # Smaller remaining capacity is better (tighter fit)\n        gaps = suitable_bins_caps - item\n        \n        # Avoid division by zero or very small numbers for bins with zero remaining capacity\n        inverse_proximity = 1.0 / (gaps + 1e-9)\n        \n        # Normalize inverse proximity to a [0, 1] range for the sigmoid\n        # Max value corresponds to the tightest fit, min to the loosest fit among suitable bins\n        min_inv_prox = np.min(inverse_proximity)\n        max_inv_prox = np.max(inverse_proximity)\n        \n        if max_inv_prox == min_inv_prox: # All suitable bins have the same tightness\n            normalized_proximity = np.ones_like(inverse_proximity) * 0.5 \n        else:\n            normalized_proximity = (inverse_proximity - min_inv_prox) / (max_inv_prox - min_inv_prox)\n            \n        # Metric 2: Fullness of the bin (inverse of remaining capacity before packing)\n        # Higher fullness is generally better to keep smaller bins for smaller items\n        fullness_scores = 1.0 / (suitable_bins_caps + 1e-9)\n        \n        # Normalize fullness scores\n        min_fullness = np.min(fullness_scores)\n        max_fullness = np.max(fullness_scores)\n        \n        if max_fullness == min_fullness: # All suitable bins have the same fullness\n            normalized_fullness = np.ones_like(fullness_scores) * 0.5\n        else:\n            normalized_fullness = (fullness_scores - min_fullness) / (max_fullness - min_fullness)\n            \n        # Combine metrics using sigmoid for graded preference\n        # We want high proximity score (tight fit) and high fullness score to have higher priority\n        # Sigmoid with a positive slope centered around 0.5 will map higher combined scores to higher priorities\n        combined_score = normalized_proximity * 0.7 + normalized_fullness * 0.3 # Weighted combination\n        \n        # Apply sigmoid to create graded priorities between 0 and 1\n        # A temperature parameter could be added here for tuning (e.g., sigmoid(k * (combined_score - 0.5)))\n        sigmoid_priorities = 1 / (1 + np.exp(-10 * (combined_score - 0.5))) \n        \n        priorities[suitable_bins_mask] = sigmoid_priorities\n        \n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 24.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response9.txt_stdout.txt",
    "code_path": "problem_iter5_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight fit score with bin fullness score using a weighted sum and sigmoid.\n    Prioritizes bins that minimize remaining capacity after packing, and also favor bins that are already fuller.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(eligible_bins_mask):\n        eligible_capacities = bins_remain_cap[eligible_bins_mask]\n        \n        # Score 1: Tight fit (inverse proximity)\n        # Smaller gap (eligible_capacities - item) means higher score. Add epsilon for stability.\n        tight_fit_scores = 1.0 / (eligible_capacities - item + 1e-9)\n\n        # Score 2: Bin fullness (inverse of remaining capacity)\n        # Fuller bins (smaller remaining capacity) get higher scores. Add epsilon.\n        fullness_scores = 1.0 / (eligible_capacities + 1e-9)\n\n        # Normalize scores to a common range (e.g., 0 to 1) for combination\n        # Normalize tight_fit_scores\n        min_tf, max_tf = np.min(tight_fit_scores), np.max(tight_fit_scores)\n        if max_tf > min_tf:\n            normalized_tight_fit = (tight_fit_scores - min_tf) / (max_tf - min_tf)\n        else:\n            normalized_tight_fit = np.ones_like(tight_fit_scores) * 0.5\n\n        # Normalize fullness_scores\n        min_fs, max_fs = np.min(fullness_scores), np.max(fullness_scores)\n        if max_fs > min_fs:\n            normalized_fullness = (fullness_scores - min_fs) / (max_fs - min_fs)\n        else:\n            normalized_fullness = np.ones_like(fullness_scores) * 0.5\n\n        # Combine scores with weights (can be tuned)\n        # Weights determine the relative importance of tight fit vs. fullness\n        weight_tight_fit = 0.6\n        weight_fullness = 0.4\n        combined_scores = (weight_tight_fit * normalized_tight_fit) + (weight_fullness * normalized_fullness)\n\n        # Apply sigmoid to the combined scores to get smooth priorities\n        # A steeper sigmoid (e.g., 10) emphasizes differences more.\n        # We want higher combined_scores to map to higher sigmoid outputs.\n        sigmoid_priorities = 1 / (1 + np.exp(-10 * (combined_scores - 0.5)))\n        \n        priorities[eligible_bins_mask] = sigmoid_priorities\n        \n        # If all eligible bins result in identical sigmoid priorities (e.g., all scores are same),\n        # assign a neutral priority of 0.5 to ensure some differentiation if possible.\n        # This also handles cases where combined_scores are all exactly 0.5.\n        if np.all(priorities[eligible_bins_mask] == 0.5) and np.any(eligible_bins_mask):\n             priorities[eligible_bins_mask] = 0.5 # Assign neutral priority if all are same\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 25.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response6.txt_stdout.txt",
    "code_path": "problem_iter8_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priority for bins to pack an item.\n    Prioritizes bins with minimal remaining capacity that can fit the item,\n    using an exponential scaling for graded preferences.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n    \n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    differences = suitable_bins_caps - item\n    \n    if suitable_bins_caps.size > 0:\n        min_diff = np.min(differences)\n        \n        \n        scaled_diffs = differences - min_diff\n        \n        \n        temperature = 0.1\n        exp_scores = np.exp(-scaled_diffs / temperature)\n        \n        \n        normalized_exp_scores = exp_scores / np.max(exp_scores)\n        \n        priorities[suitable_bins_mask] = normalized_exp_scores\n        \n        \n        if np.all(priorities == 0):\n            priorities[suitable_bins_mask] = 0.5\n            \n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 17.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response2.txt_stdout.txt",
    "code_path": "problem_iter11_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit and bin fullness using normalized scores and exponential decay.\n    Prioritizes bins that are a good fit and are already relatively full.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        \n        # Metric 1: Tightest fit (proximity)\n        # Smaller remaining capacity after packing is better. Add small epsilon to avoid division by zero.\n        gaps = suitable_bins_caps - item\n        proximity_scores = 1.0 / (gaps + 1e-9)\n        \n        # Normalize proximity scores to [0, 1]\n        min_prox = np.min(proximity_scores)\n        max_prox = np.max(proximity_scores)\n        if max_prox == min_prox:\n            normalized_proximity = np.ones_like(proximity_scores) * 0.5\n        else:\n            normalized_proximity = (proximity_scores - min_prox) / (max_prox - min_prox)\n            \n        # Metric 2: Fullness of the bin before packing\n        # Higher fullness (lower remaining capacity) is generally better.\n        fullness_scores = 1.0 / (suitable_bins_caps + 1e-9)\n        \n        # Normalize fullness scores to [0, 1]\n        min_fullness = np.min(fullness_scores)\n        max_fullness = np.max(fullness_scores)\n        if max_fullness == min_fullness:\n            normalized_fullness = np.ones_like(fullness_scores) * 0.5\n        else:\n            normalized_fullness = (fullness_scores - min_fullness) / (max_fullness - min_fullness)\n            \n        # Combine metrics with a preference for tightness, modulated by fullness\n        # Use exponential decay on the combined score to create a graded priority\n        # A higher combined score (good fit and high fullness) will result in a higher priority\n        combined_score = normalized_proximity * 0.7 + normalized_fullness * 0.3\n        \n        # Apply exponential decay to create graded priorities, similar to Heuristic 12's approach\n        # Here, we map the combined score to a priority. A higher combined score should yield a higher priority.\n        # Using an exponential function (e.g., exp(x)) naturally produces a graded response.\n        # We scale and shift to get a reasonable range, e.g., mapping [0,1] combined_score to a positive range.\n        # An exponential function like exp(k * combined_score) where k is a scaling factor.\n        # Let's use exp(5 * combined_score) for a steeper curve.\n        exponential_priorities = np.exp(5 * combined_score)\n        \n        # Normalize these exponential priorities to [0, 1]\n        min_exp_prio = np.min(exponential_priorities)\n        max_exp_prio = np.max(exponential_priorities)\n        \n        if max_exp_prio == min_exp_prio:\n            final_priorities = np.ones_like(exponential_priorities) * 0.5\n        else:\n            final_priorities = (exponential_priorities - min_exp_prio) / (max_exp_prio - min_exp_prio)\n        \n        priorities[suitable_bins_mask] = final_priorities\n        \n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 30.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response8.txt_stdout.txt",
    "code_path": "problem_iter11_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines normalized inverse proximity with a linear penalty on excess capacity.\n\n    Favors tighter fits using inverse proximity, tempered by a penalty\n    for bins with significantly more remaining capacity than needed.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n        # Inverse proximity for tight fits, normalized\n        proximity = suitable_bins_caps - item\n        inverse_proximity_scores = 1.0 / (proximity + 1e-9)\n\n        if np.max(inverse_proximity_scores) > 0:\n            normalized_inverse_proximity = inverse_proximity_scores / np.max(inverse_proximity_scores)\n        else:\n            normalized_inverse_proximity = np.zeros_like(inverse_proximity_scores)\n\n        # Linear penalty on excess capacity, normalized\n        excess_capacity = suitable_bins_caps - item\n        if np.max(excess_capacity) > 0:\n            normalized_excess_capacity = excess_capacity / np.max(excess_capacity)\n        else:\n            normalized_excess_capacity = np.zeros_like(excess_capacity)\n\n        # Combine normalized inverse proximity with a penalty for excess capacity\n        # A higher score is better. We want tight fits (high normalized_inverse_proximity)\n        # and we penalize bins with more excess capacity (lower normalized_excess_capacity).\n        # A simple combination: normalized_inverse_proximity - normalized_excess_capacity\n        # This prioritizes tight fits, but slightly de-prioritizes bins with large excess.\n        combined_scores = normalized_inverse_proximity - normalized_excess_capacity\n\n        # Ensure perfect fits (where proximity is 0) get maximum priority if they exist\n        # This also ensures their score isn't penalized by the excess capacity term.\n        perfect_fit_mask = (proximity == 0)\n        combined_scores[perfect_fit_mask] = np.max(normalized_inverse_proximity) + 1 # Give a slight boost\n\n        # Normalize the combined scores to be in a reasonable range for priority\n        if np.ptp(combined_scores) > 0: # ptp is peak-to-peak range\n            priorities[suitable_bins_mask] = (combined_scores - np.min(combined_scores)) / np.ptp(combined_scores)\n        else:\n            priorities[suitable_bins_mask] = np.zeros_like(combined_scores) # All scores are the same\n\n        # Ensure perfect fits still have the absolute highest priority after normalization\n        priorities[suitable_bins_mask][perfect_fit_mask] = 1.0\n\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 25.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response3.txt_stdout.txt",
    "code_path": "problem_iter12_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    This heuristic aims to improve upon previous versions by employing a more\n    sophisticated multi-criteria scoring mechanism that considers both the\n    tightness of the fit and the overall \"strategy\" of bin usage. It incorporates\n    non-linear scoring and adaptive weighting to handle different scenarios.\n\n    Scoring strategy:\n    1.  **Tightness Score (Primary):** Prioritize bins that leave the minimal positive\n        remaining capacity after placing the item. This is similar to the Best Fit\n        strategy, but we'll use a non-linear transformation to allow for more\n        granularity. Specifically, we want to maximize `item - bins_remain_cap`\n        for fitting bins, but we'll transform this to penalize larger gaps more\n        severely using a quadratic function.\n    2.  **Fullness Score (Secondary):** Favor bins that are already relatively full\n        before the item is placed. This encourages consolidation and can help\n        reduce the total number of bins used, especially for smaller items.\n\n    The heuristic aims to balance these two criteria. For bins that cannot fit the\n    item, a very low priority score is assigned.\n\n    For bins that *can* fit the item (`bins_remain_cap >= item`):\n    -   Calculate the 'gap': `gap = bins_remain_cap - item`. We want to minimize this positive gap.\n    -   Calculate a 'tightness score component': `tightness_score = -gap**2`. This strongly penalizes larger gaps.\n    -   Calculate a 'fullness score component': `fullness_score = bins_remain_cap`. This rewards bins that are already more full.\n\n    We combine these with tunable weights. Let's use `w_tight` for tightness and\n    `w_full` for fullness. A common approach is to prioritize tightness, but\n    also give a boost to fuller bins if the tightness is comparable.\n\n    A potential combined score for fitting bins could be:\n    `score = w_tight * (-gap**2) + w_full * bins_remain_cap`\n\n    The goal is to maximize this score.\n    Let's set `w_tight = 1.0` and `w_full = 0.5`.\n    `score = -(bins_remain_cap - item)**2 + 0.5 * bins_remain_cap`\n\n    This means a perfect fit (`gap = 0`) gets `0.5 * bins_remain_cap`.\n    A slightly larger gap (`gap = 0.1`) gets `-(0.01) + 0.5 * (item + 0.1)`.\n    If `item=0.5`, `bins_remain_cap=0.55`: `score = -(0.05)**2 + 0.5 * 0.55 = -0.0025 + 0.275 = 0.2725`\n    If `item=0.5`, `bins_remain_cap=0.5`: `score = -(0.0)**2 + 0.5 * 0.5 = 0.25`\n    This heuristic would prefer the bin with a small gap over a perfect fit, which\n    might be beneficial for distributing items more evenly.\n\n    To add more dynamism and potentially adapt to the context of the problem (e.g.,\n    item sizes or bin capacities), we can introduce an adaptive parameter.\n    Let's consider a \"sensitivity\" parameter `s` that influences how much we\n    penalize gaps. A higher `s` means we are more sensitive to gaps.\n\n    Revised score for fitting bins:\n    `score = -(s * gap)**2 + lambda_param * bins_remain_cap`\n    Let `s` be a function of the item size relative to bin capacity, or a global parameter.\n    For simplicity, let's use a fixed `s` and `lambda_param`.\n\n    Let `s = 1.0` (for now, can be tuned or made adaptive)\n    Let `lambda_param = 0.3` (prioritizing fuller bins slightly more)\n\n    `score = -(bins_remain_cap - item)**2 + 0.3 * bins_remain_cap`\n\n    Example: `item = 0.4`\n    Bin A: `bins_remain_cap = 0.4`. Gap = 0. `score = -(0)**2 + 0.3 * 0.4 = 0.12`.\n    Bin B: `bins_remain_cap = 0.45`. Gap = 0.05. `score = -(0.05)**2 + 0.3 * 0.45 = -0.0025 + 0.135 = 0.1325`.\n    Bin C: `bins_remain_cap = 0.7`. Gap = 0.3. `score = -(0.3)**2 + 0.3 * 0.7 = -0.09 + 0.21 = 0.12`.\n    Bin D: `bins_remain_cap = 0.3`. Cannot fit. Score = -inf.\n\n    Bin B is preferred (tightest fit among the slightly-gapped ones), then Bin A (perfect fit), then Bin C.\n    This heuristic is more nuanced than simply picking the tightest fit.\n\n    Consider an edge case: what if all fitting bins are much larger than the item?\n    `item = 0.1`\n    Bin X: `bins_remain_cap = 0.9`. Gap = 0.8. `score = -(0.8)**2 + 0.3 * 0.9 = -0.64 + 0.27 = -0.37`.\n    Bin Y: `bins_remain_cap = 0.5`. Gap = 0.4. `score = -(0.4)**2 + 0.3 * 0.5 = -0.16 + 0.15 = -0.01`.\n    Bin Z: `bins_remain_cap = 0.15`. Gap = 0.05. `score = -(0.05)**2 + 0.3 * 0.15 = -0.0025 + 0.045 = 0.0425`.\n\n    Bin Z is preferred, then Bin Y, then Bin X. This seems reasonable.\n\n    To make it *better* than `priority_v1`, which effectively is `max(item - bins_remain_cap)`,\n    we can focus on making the score more sensitive to the 'almost perfect' fits\n    while still penalizing large gaps.\n    The quadratic term `-(gap**2)` already does this. The `lambda_param * bins_remain_cap`\n    adds the secondary criterion of bin fullness.\n\n    Let's introduce another element: penalizing bins that are *so* full that only\n    very small items could fit into them. This might be captured by the `-gap**2`\n    term, as a very full bin (`bins_remain_cap` is small) implies a small gap for a small item.\n\n    Consider a scenario where we want to avoid leaving *too much* empty space,\n    but also not fill bins *too* tightly if it means leaving very little room for future items.\n    The current score `-(gap**2) + lambda_param * bins_remain_cap` tries to balance this.\n\n    Let's consider an alternative non-linear transformation for the gap.\n    Instead of `-(gap**2)`, maybe something like `-(gap / bins_remain_cap)**2` or `-(gap / BIN_CAPACITY)**2`\n    to normalize the gap. However, `BIN_CAPACITY` is not given.\n\n    Let's stick with the current structure but adjust weights and perhaps add\n    a small penalty for extremely large bins that fit the item, to encourage\n    using bins that are \"just right\".\n    The `lambda_param * bins_remain_cap` term already discourages very empty bins.\n\n    Let's make the penalty for gap more pronounced for larger gaps by squaring.\n    We also want to reward bins that are fuller overall, hence `lambda_param * bins_remain_cap`.\n    To be better than `priority_v1`, which is equivalent to `item - bins_remain_cap`,\n    we need to ensure our heuristic offers a different trade-off.\n\n    The score `-(bins_remain_cap - item)**2 + lambda_param * bins_remain_cap`\n    is designed to:\n    1. Strongly favor minimizing the gap `bins_remain_cap - item`.\n    2. Provide a secondary boost to bins that are more full.\n\n    This is a good candidate for `priority_v2`.\n\n    Parameters:\n    - `gap_penalty_factor`: Controls the strength of the penalty for larger gaps.\n    - `fullness_bonus_factor`: Controls the bonus for fuller bins.\n    \"\"\"\n\n    # Define weights for the scoring components\n    gap_penalty_factor = 1.0  # Higher value means stronger penalty for gaps\n    fullness_bonus_factor = 0.4 # Higher value means stronger bonus for fuller bins\n\n    # Initialize priorities with a very low value for bins that cannot fit the item\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Create a mask for bins that can accommodate the item\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate scores only for the bins that can fit the item\n    fitting_bins_remain_cap = bins_remain_cap[fit_mask]\n\n    # Calculate the gap (remaining capacity after placing the item)\n    gap = fitting_bins_remain_cap - item\n\n    # Calculate the tightness score component: penalize larger gaps quadratically\n    # We want to maximize -(gap^2), meaning minimize gap^2\n    tightness_score_component = -(gap_penalty_factor * gap)**2\n\n    # Calculate the fullness score component: reward bins that are already fuller\n    # This term is simply the remaining capacity itself\n    fullness_score_component = fullness_bonus_factor * fitting_bins_remain_cap\n\n    # Combine the scores\n    combined_scores = tightness_score_component + fullness_score_component\n\n    # Assign the calculated scores to the corresponding bins\n    priorities[fit_mask] = combined_scores\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter13_response0.txt_stdout.txt",
    "code_path": "problem_iter13_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 8.887281177181691e-06, sigmoid_steepness: float = 12.141015575843076, neutral_priority: float = 0.5002145562071864) -> np.ndarray:\n    \"\"\"\n    Combines inverse proximity for tight fits with a sigmoid for smooth preference.\n    Favors bins with minimal remaining capacity after packing, scaled smoothly.\n\n    Args:\n        item (float): The size of the item to be packed.\n        bins_remain_cap (np.ndarray): A numpy array representing the remaining capacity of each bin.\n        epsilon (float): A small value to prevent division by zero. Defaults to 1e-9.\n        sigmoid_steepness (float): Controls the steepness of the sigmoid function. Higher values make the transition sharper. Defaults to 10.0.\n        neutral_priority (float): The priority assigned to bins that are equidistant in terms of fit. Defaults to 0.5.\n\n    Returns:\n        np.ndarray: A numpy array of priorities for each bin.\n    \"\"\"\n    eligible_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(eligible_bins_mask):\n        eligible_capacities = bins_remain_cap[eligible_bins_mask]\n        \n        # Inverse proximity: smaller gap is better (higher score)\n        # Adding a small epsilon to avoid division by zero\n        inverse_proximity = 1.0 / (eligible_capacities - item + epsilon)\n\n        # Normalize inverse proximity to a range where sigmoid is effective\n        # Aims to map smaller gaps (higher inverse_proximity) to values around 0.5\n        # and larger gaps to values further from 0.5.\n        # This normalization is heuristic and can be tuned.\n        min_inv_proximity = np.min(inverse_proximity)\n        max_inv_proximity = np.max(inverse_proximity)\n        \n        if max_inv_proximity > min_inv_proximity:\n            normalized_scores = (inverse_proximity - min_inv_proximity) / (max_inv_proximity - min_inv_proximity)\n        else: # All eligible bins have the same inverse proximity\n            normalized_scores = np.ones_like(inverse_proximity) * neutral_priority\n\n        # Sigmoid function to create a smooth priority distribution\n        # The steepness parameter (e.g., 10) can be tuned.\n        # We want bins with smaller gaps (higher normalized_scores) to have higher sigmoid outputs.\n        # So, we invert the normalized_scores for the sigmoid input to favor smaller gaps.\n        # The input to sigmoid is centered around 0 for the neutral_priority.\n        sigmoid_input = sigmoid_steepness * (normalized_scores - neutral_priority)\n        sigmoid_priorities = 1 / (1 + np.exp(-sigmoid_input))\n\n        priorities[eligible_bins_mask] = sigmoid_priorities\n        \n        # Ensure that if all eligible bins are identical in terms of fit, they get a neutral priority\n        if np.all(priorities[eligible_bins_mask] == neutral_priority) and len(eligible_bins_mask) > 0:\n            priorities[eligible_bins_mask] = neutral_priority\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  }
]