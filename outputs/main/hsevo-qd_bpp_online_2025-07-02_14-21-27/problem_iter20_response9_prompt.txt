{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    An enhanced priority function that incorporates adaptive weighting, temperature-based exploration,\n    a refined capacity ratio, and penalties for near-misses.\n    \"\"\"\n\n    valid_bins = bins_remain_cap > 0\n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n\n    safe_bins_remain_cap = np.where(valid_bins, bins_remain_cap, np.inf)\n\n    # 1. Capacity-Aware Newtonian Potential (with softened denominator)\n    potential = safe_bins_remain_cap / (np.abs(safe_bins_remain_cap - item) + 0.1)\n    newtonian_priority = np.where(safe_bins_remain_cap >= item, potential, -np.inf)\n\n    # 2. Refined Capacity Ratio: Emphasize bins slightly larger than the item.\n    capacity_ratio = safe_bins_remain_cap / (item + 1e-9)\n    capacity_priority = np.where(\n        safe_bins_remain_cap >= item,\n        np.exp(-(capacity_ratio - 1)**2 / 0.5),  # Gaussian-like preference around ratio=1\n        -np.inf\n    )\n\n    # 3. Bin Utilization (Encourages filling partially full bins)\n    bin_utilization = 1 - (safe_bins_remain_cap / np.max(bins_remain_cap))\n    utilization_priority = np.where(safe_bins_remain_cap >= item, bin_utilization, -np.inf)\n\n    # 4. Near-Miss Penalty: Discourage bins that are almost big enough.\n    near_miss_penalty = np.where(\n        (safe_bins_remain_cap < item) & (safe_bins_remain_cap > item * 0.75),  # Tune 0.75\n        -10,  # Significant penalty\n        0\n    )\n\n    # 5. Adaptive Weighting based on Item Size and Available Capacity\n    avg_capacity = np.mean(bins_remain_cap[valid_bins]) if np.any(valid_bins) else 1e-9\n    item_size_ratio = np.clip(item / (avg_capacity + 1e-9), 0.0, 1.0)\n    weight_newtonian = 1 - item_size_ratio\n    weight_capacity = item_size_ratio\n\n    # 6. Temperature-Based Stochastic Exploration (Simulated Annealing)\n    temperature = 0.1  # Adjust cooling schedule as needed\n    exploration_factor = temperature * np.exp(-item_size_ratio) # Reduce exploration as item size increase\n    random_noise = np.random.normal(0, exploration_factor, size=bins_remain_cap.shape)\n\n    # 7. Combine Priorities with Adaptive Weights\n    combined_priority = (\n        weight_newtonian * newtonian_priority +\n        weight_capacity * capacity_priority +\n        0.2 * utilization_priority +\n        near_miss_penalty + # Directly add penalty\n        random_noise # Add the noise\n    )\n\n    # 8. Handle Invalid Bins and NaN values\n    combined_priority = np.nan_to_num(combined_priority, neginf=-np.inf)\n    priorities = np.where(~valid_bins, -np.inf, combined_priority)\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    A multifactorial and adaptive priority function for online bin packing.\n\n    Combines multiple heuristics with adaptive weighting based on bin fill level and item size.\n    Prioritizes bins that are neither too full nor too empty and encourages exploration of less utilized bins.\n    \"\"\"\n\n    valid_bins = bins_remain_cap > 0\n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n\n    safe_bins_remain_cap = np.where(valid_bins, bins_remain_cap, np.inf)\n    bin_size = np.max(bins_remain_cap) # Assuming all bins have the same max capacity\n\n    # Heuristic 1: Remaining Capacity Ratio (Focus: Utilization)\n    capacity_ratio = safe_bins_remain_cap / bin_size\n    capacity_priority = capacity_ratio  # Higher remaining capacity is initially better.\n\n    # Heuristic 2: Item Fit Score (Focus: Avoiding Fragmentation)\n    fit_score = np.exp(-np.abs(safe_bins_remain_cap - item) / (bin_size / 4)) # Gaussian-like preference for close fits. Adjusted standard deviation to bin_size /4\n    fit_priority = fit_score\n\n    # Heuristic 3: Bin Usage (Focus: Exploration)\n    usage_level = 1 - capacity_ratio\n    usage_priority = np.sqrt(usage_level) # Preferentially selects bins with higher utilization for exploration. Square root dampens the effect.\n\n    # Heuristic 4: Modified Newtonian (Focus: Packing Efficiency)\n    newtonian_component = safe_bins_remain_cap / np.abs(safe_bins_remain_cap - item + 1e-9)\n    newtonian_priority = np.where(safe_bins_remain_cap >= item, newtonian_component, -np.inf)\n\n\n    # Adaptive Weighting based on item size\n    item_size_ratio = item / bin_size\n    weight_capacity = 0.2 if item_size_ratio < 0.3 else (0.1 if item_size_ratio < 0.6 else 0.05)  # Smaller items: prioritize filling almost empty bins.\n    weight_fit = 0.5 if item_size_ratio < 0.3 else (0.4 if item_size_ratio < 0.6 else 0.3 ) # Moderate item fit is more important for small items\n    weight_usage = 0.2 if item_size_ratio < 0.3 else (0.3 if item_size_ratio < 0.6 else 0.4)  # larger items focus on usage\n    weight_newtonian = 0.1 if item_size_ratio < 0.3 else (0.2 if item_size_ratio < 0.6 else 0.25) # packing efficiency\n\n    # Combined Priority\n    combined_priority = (\n        weight_capacity * capacity_priority +\n        weight_fit * fit_priority +\n        weight_usage * usage_priority +\n        weight_newtonian * newtonian_priority\n    )\n\n    combined_priority = np.nan_to_num(combined_priority, neginf=-np.inf)\n    priorities = np.where(~valid_bins, -np.inf, combined_priority)\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see a vast difference in complexity. The best heuristic incorporates Newtonian potential, capacity ratio, bin utilization, adaptive weighting, and stochastic exploration, while the worst only uses a simple log ratio.\n\nComparing (1st) vs (19th), we observe that the 1st heuristic refines the ratio-based approach by adding Newtonian potential, bin utilization consideration, adaptive weighting based on item size, and stochastic exploration for diversity. The 19th heuristic uses the same approach as the 20th.\n\nComparing (1st) vs (18th), we note that 1st heuristic includes checks for valid bins and handles edge cases (like division by zero). The 18th is incomplete.\n\nComparing (2nd) vs (17th), the 2nd combines Newtonian potential, capacity ratio, and fragmentation penalty with adaptive weighting, while 17th takes into consideration bin size to calculate its components such as capacity ratio, item fit score, bin usage and modified Newtonian.\n\nComparing (14th) vs (15th), these are identical.\n\nComparing (15th) vs (16th), 15th uses only Newtonian potential, while 16th uses Newtonian potential and log ratios.\n\nComparing (11th) vs (12th), they are identical.\n\nComparing (12th) vs (13th), they are identical.\n\nComparing (6th) vs (7th), they are identical.\n\nComparing (7th) vs (8th), they are identical.\n\nComparing (8th) vs (9th), they are identical.\n\nComparing (9th) vs (10th), they are different.\n\nOverall: The best heuristics incorporate a multi-faceted approach, combining several factors like Newtonian potential, capacity ratio, and bin utilization. Adaptive weighting based on item size and available capacity appears crucial. Stochastic exploration adds robustness. Penalties for fragmentation or near-misses can also improve performance. The simpler heuristics rely only on capacity ratios and perform the worst. Handling edge cases (division by zero, invalid bins) robustly is critical for stability. The middle-ranked heuristics attempt to combine Newtonian and ratio-based methods, sometimes with a fragmentation penalty.\n- \nOkay, here's a redefined approach to self-reflection for heuristic design, focusing on actionable insights and avoiding common pitfalls:\n\n*   **Keywords:** Adaptive weights, edge case handling, stochastic exploration, multi-factor analysis, robustness, iterative refinement, performance evaluation, problem complexity.\n\n*   **Advice:** Focus on capturing underlying problem complexities by intelligently combining multiple relevant factors and adapting their influence. Prioritize robust handling of edge cases and numerical instability. Systematically evaluate and refine heuristics through experimentation.\n\n*   **Avoid:** Over-simplification, premature optimization, neglecting edge cases, redundant heuristics, and relying solely on intuition without proper testing and evaluation.\n\n*   **Explanation:** Move beyond simply \"combining factors.\" Emphasize capturing the *essence* of the problem. Prioritize *robustness* and systematic *evaluation*. This involves not just addressing edge cases as afterthoughts, but designing for them from the start. Avoid unnecessary complexity, focusing on targeted refinements based on empirical data and problem-specific insights.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}