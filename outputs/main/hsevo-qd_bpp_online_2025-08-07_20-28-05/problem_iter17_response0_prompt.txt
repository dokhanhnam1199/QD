{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    slack = bins_remain_cap - item\n    fits = slack >= 0\n    eps = np.finfo(slack.dtype).eps\n    return np.where(fits, -slack + eps, -np.inf)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    if not hasattr(priority_v2, \"initialized\"):\n        priority_v2.num_rules = 4\n        priority_v2.sum_waste = np.zeros(priority_v2.num_rules, dtype=float)\n        priority_v2.count = np.zeros(priority_v2.num_rules, dtype=int)\n        priority_v2.step = 0\n        priority_v2.epsilon = 1e-9\n        priority_v2.initialized = True\n    feasible = bins_remain_cap >= item\n    if not feasible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    leftovers = bins_remain_cap - item\n    best_fit_score = np.where(feasible, -leftovers, -np.inf)\n    worst_fit_score = np.where(feasible, leftovers, -np.inf)\n    indices = np.arange(bins_remain_cap.shape[0] if hasattr(bins_remain_cap, \"shape\") else len(bins_remain_cap))\n    first_fit_score = np.where(feasible, -indices, -np.inf)\n    target = bins_remain_cap.mean()\n    fill_bal_score = np.where(feasible, -np.abs(leftovers - target), -np.inf)\n    rule_scores = [best_fit_score, worst_fit_score, first_fit_score, fill_bal_score]\n    waste_per_rule = np.empty(priority_v2.num_rules, dtype=float)\n    for i, scores in enumerate(rule_scores):\n        if not np.isfinite(scores).any():\n            waste_per_rule[i] = np.inf\n        else:\n            idx = np.argmax(scores)\n            waste_per_rule[i] = bins_remain_cap[idx] - item\n    for i in range(priority_v2.num_rules):\n        w = waste_per_rule[i]\n        if np.isfinite(w):\n            priority_v2.sum_waste[i] += w\n            priority_v2.count[i] += 1\n    avg_waste = np.where(priority_v2.count > 0, priority_v2.sum_waste / priority_v2.count, np.inf)\n    inv = 1.0 / (avg_waste + priority_v2.epsilon)\n    boost = np.zeros(priority_v2.num_rules, dtype=float)\n    boost_idx = priority_v2.step % priority_v2.num_rules\n    boost[boost_idx] = 1.0 / (priority_v2.step + 1)\n    raw_weights = inv + boost\n    total = raw_weights.sum()\n    if total > 0:\n        weights = raw_weights / total\n    else:\n        weights = np.full(priority_v2.num_rules, 1.0 / priority_v2.num_rules)\n    combined_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    for w, scores in zip(weights, rule_scores):\n        combined_scores += w * scores\n    combined_scores[~feasible] = -np.inf\n    priority_v2.step += 1\n    return combined_scores\n\n### Analyze & experience\n- - **(Best)\u202fHeuristic\u202f1 vs (Worst)\u202fHeuristic\u202f20** \u2013\u202fHeuristic\u202f1 is a *stateless* inverse\u2011slack score with a tiny \u03b5\u2011random perturbation. It is O(n) with virtually no bookkeeping. Heuristic\u202f20 augments the same core with **global counters**, **UCB\u2011style exploration**, a **median\u2011based diversity term**, and **reward updates** after every placement. This yields a far richer model but introduces heavy state, many hyper\u2011parameters (c, \u03b5, weights), and non\u2011trivial update cost. In practice the extra complexity can cause high variance and over\u2011fitting on short streams, while the simple heuristic remains robust and fast.\n\n- **(2nd\u202fBest)\u202fHeuristic\u202f2 vs (2nd\u202fWorst)\u202fHeuristic\u202f19** \u2013\u202fHeuristic\u202f2 is identical to the best one (pure inverse\u2011slack\u202f+\u202f\u03b5\u2011noise). Heuristic\u202f19 only defines a signature and docstring; the body stops after allocating a zero\u2011filled array and never computes a meaningful score. Consequently it cannot guide any placement, making it dramatically weaker.\n\n- **1st vs 2nd** \u2013\u202fBoth are byte\u2011for\u2011byte copies; the ranking difference likely reflects stochastic performance variations rather than code differences.\n\n- **3rd vs 4th** \u2013\u202fHeuristic\u202f3 mirrors the baseline (no state). Heuristic\u202f4 adds **online averaging of rewards** per bin and a **reward weight** (0.3). This gives the algorithm a learning signal that can improve fit over long horizons, but it also requires correct initialization, handling of sparse updates, and careful tuning of the reward weight to avoid destabilising the deterministic slack term.\n\n- **(Second\u202fWorst)\u202fHeuristic\u202f19 vs (Worst)\u202fHeuristic\u202f20** \u2013\u202fHeuristic\u202f19 is essentially a stub (returns zeros), while Heuristic\u202f20 implements a full **UCB\u2011plus\u2011diversity** scheme. Despite being more sophisticated, Heuristic\u202f20 still ranks lower than many intermediate designs because its added machinery can outweigh benefits on typical benchmark streams, especially when the exploration bonus dominates the core inverse\u2011slack signal.\n\n- **Overall patterns** \u2013\u202fThe top\u2011ranked heuristics are *minimalistic*: a deterministic inverse\u2011slack component (or slack\u2011based linear penalty) combined with a modest \u03b5\u2011random term. Mid\u2011ranked methods introduce **softmax probabilities**, **linear\u2011plus\u2011quadratic slack penalties**, or **multi\u2011rule weighted sums** (e.g., Heuristic\u202f10), offering more flexibility but demanding extra parameters. The lowest\u2011ranked approaches either lack a functional core (Heuristic\u202f18,\u202f19) or overload the decision with many learned components (Heuristics\u202f17,\u202f20) that are hard to tune and may degrade performance without ample data.\n- \n- **Keywords:** feasibility filter, surrogate cost model, normalized profit\u2011to\u2011resource ratio, offline calibration  \n- **Advice:** Apply a two\u2011stage pipeline: prune infeasible options with deterministic rules, then rank the remainder by a single normalized efficiency metric derived from an offline\u2011trained surrogate; retrain the surrogate periodically without runtime state.  \n- **Avoid:** Random exploration terms, online state accumulation, multi\u2011parameter softmax scaling, heavy hyper\u2011parameter tuning, duplicated code, ad\u2011hoc reward averaging.  \n- **Explanation:** This yields a fast, transparent heuristic; feasibility is guaranteed, ranking is data\u2011grounded, and offline learning limits runtime overhead while preserving interpretability.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}