
- **Keywords:** adaptive scoring, multi‑feature state, Thompson sampling, constraint‑aware learning.  
- **Advice:** Build a lightweight feature vector (slack, item size, bin fill ratio, historical success) and train an online linear model or bandit; use Thompson sampling for exploration; update only with sufficient data; keep the model stateless between episodes.  
- **Avoid:** Pure inverse‑slack baselines, fixed ε‑greedy, heavy per‑bin state, ad‑hoc softmax, duplicated logic, and unchecked randomness.  
- **Explanation:** A principled, data‑driven score balances feasibility and utilization, explores efficiently, and stays computationally cheap without over‑parameterizing or fragile heuristics.