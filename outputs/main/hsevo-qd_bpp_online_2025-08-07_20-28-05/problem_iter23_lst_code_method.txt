{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\n_selection_counts = None\n_total_rewards = None\n_total_calls = 0\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Inverse slack + epsilon random + avg reward for online bin packing.\"\"\"\n    global _selection_counts, _total_rewards, _total_calls\n    bins_remain_cap = np.asarray(bins_remain_cap, dtype=float)\n    n = bins_remain_cap.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if _selection_counts is None or _selection_counts.shape[0] != n:\n        _selection_counts = np.zeros(n, dtype=float)\n        _total_rewards = np.zeros(n, dtype=float)\n    feasible = bins_remain_cap >= item\n    scores = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    leftovers = bins_remain_cap[feasible] - item\n    deterministic = 1.0 / (leftovers + 1.0)\n    epsilon = 0.2\n    random_part = np.random.rand(feasible.sum())\n    base_score = (1 - epsilon) * deterministic + epsilon * random_part\n    avg_reward = np.zeros_like(base_score)\n    mask = _selection_counts[feasible] > 0\n    avg_reward[mask] = _total_rewards[feasible][mask] / _selection_counts[feasible][mask]\n    reward_weight = 0.3\n    combined = base_score + reward_weight * avg_reward\n    scores[feasible] = combined\n    best = int(np.argmax(scores))\n    _total_calls += 1\n    reward = - (bins_remain_cap[best] - item)\n    _selection_counts[best] += 1\n    _total_rewards[best] += reward\n    return scores\n\n[Heuristics 2nd]\nimport numpy as np\n\n_selection_counts = None\n_total_rewards = None\n_total_calls = 0\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Inverse slack + epsilon random + avg reward for online bin packing.\"\"\"\n    global _selection_counts, _total_rewards, _total_calls\n    bins_remain_cap = np.asarray(bins_remain_cap, dtype=float)\n    n = bins_remain_cap.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if _selection_counts is None or _selection_counts.shape[0] != n:\n        _selection_counts = np.zeros(n, dtype=float)\n        _total_rewards = np.zeros(n, dtype=float)\n    feasible = bins_remain_cap >= item\n    scores = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    leftovers = bins_remain_cap[feasible] - item\n    deterministic = 1.0 / (leftovers + 1.0)\n    epsilon = 0.2\n    random_part = np.random.rand(feasible.sum())\n    base_score = (1 - epsilon) * deterministic + epsilon * random_part\n    avg_reward = np.zeros_like(base_score)\n    mask = _selection_counts[feasible] > 0\n    avg_reward[mask] = _total_rewards[feasible][mask] / _selection_counts[feasible][mask]\n    reward_weight = 0.3\n    combined = base_score + reward_weight * avg_reward\n    scores[feasible] = combined\n    best = int(np.argmax(scores))\n    _total_calls += 1\n    reward = - (bins_remain_cap[best] - item)\n    _selection_counts[best] += 1\n    _total_rewards[best] += reward\n    return scores\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    epsilon = 0.2\n    feasible = bins_remain_cap >= item\n    if not np.any(feasible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    leftovers = bins_remain_cap[feasible] - item\n    deterministic = 1.0 / (leftovers + 1.0)\n    random_part = np.random.rand(feasible.sum())\n    combined = (1 - epsilon) * deterministic + epsilon * random_part\n    scores[feasible] = combined\n    max_score = np.max(scores[feasible])\n    exp_scores = np.exp(scores - max_score)\n    probabilities = exp_scores / exp_scores.sum()\n    return probabilities\n\n[Heuristics 4th]\nimport numpy as np\n\n_selection_counts = None\n_total_rewards = None\n_total_calls = 0\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combine inverse slack, \u03b5\u2011greedy noise, per\u2011bin reward, and UCB bonus.\"\"\"\n    global _selection_counts, _total_rewards, _total_calls\n    caps = np.asarray(bins_remain_cap, dtype=float)\n    n = caps.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if _selection_counts is None or _selection_counts.shape[0] != n:\n        _selection_counts = np.zeros(n, dtype=float)\n        _total_rewards = np.zeros(n, dtype=float)\n        _total_calls = 0\n    feasible = caps >= item\n    scores = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    leftovers = caps[feasible] - item\n    deterministic = 1.0 / (leftovers + 1.0)\n    epsilon = 0.15\n    random_part = np.random.rand(feasible.sum())\n    base_score = (1 - epsilon) * deterministic + epsilon * random_part\n    avg_reward = np.zeros_like(base_score)\n    mask = _selection_counts[feasible] > 0\n    avg_reward[mask] = _total_rewards[feasible][mask] / _selection_counts[feasible][mask]\n    reward_weight = 0.1\n    reward_component = reward_weight * avg_reward\n    c = 0.2\n    total = _total_calls + 1\n    sel_counts = _selection_counts[feasible] + 1e-9\n    ucb_bonus = c * np.sqrt(np.log(total) / sel_counts)\n    combined = base_score + reward_component + ucb_bonus\n    scores[feasible] = combined\n    best = int(np.argmax(scores))\n    _total_calls += 1\n    reward = - (caps[best] - item)\n    _selection_counts[best] += 1\n    _total_rewards[best] += reward\n    return scores\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive scoring: inverse slack, linear slack, per-bin reward, UCB bonus, \u03b5-exploration.\"\"\"\n    if not hasattr(priority_v2, \"_initialized\"):\n        priority_v2._sum_reward = None\n        priority_v2._count = None\n        priority_v2._total_calls = 0\n        priority_v2._epsilon = 0.12\n        priority_v2._w_inv = 0.55\n        priority_v2._w_lin = 0.25\n        priority_v2._w_reward = 0.15\n        priority_v2._w_ucb = 0.05\n        priority_v2._initialized = True\n    n = bins_remain_cap.shape[0]\n    if priority_v2._sum_reward is None or priority_v2._sum_reward.shape[0] != n:\n        priority_v2._sum_reward = np.zeros(n, dtype=float)\n        priority_v2._count = np.zeros(n, dtype=int)\n    feasible = bins_remain_cap >= item\n    scores = np.full(n, -np.inf, dtype=float)\n    if not feasible.any():\n        priority_v2._initialized = True\n        priority_v2._total_calls += 1\n        return scores\n    slack = bins_remain_cap[feasible] - item\n    base = priority_v2._w_inv * (1.0 / (slack + 1.0)) + priority_v2._w_lin * (-slack)\n    scores[feasible] = base\n    avg_reward = np.zeros(n, dtype=float)\n    mask = priority_v2._count > 0\n    avg_reward[mask] = priority_v2._sum_reward[mask] / priority_v2._count[mask]\n    scores[feasible] += priority_v2._w_reward * avg_reward[feasible]\n    total = priority_v2._total_calls + 1\n    ucb = np.sqrt(2.0 * np.log(total) / (priority_v2._count + 1e-6))\n    scores[feasible] += priority_v2._w_ucb * ucb[feasible]\n    rand_vals = np.random.rand(n)\n    scores[feasible] += priority_v2._epsilon * rand_vals[feasible]\n    chosen = int(np.argmax(scores))\n    if feasible[chosen]:\n        reward = - (0) if False else - (bins_remain_cap[chosen] - item)\n        priority_v2._sum_reward[chosen] += reward\n        priority_v2._count[chosen] += 1\n    priority_v2._total_calls += 1\n    return scores\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combine inverse slack and negative squared slack with epsilon\u2011greedy randomness for balanced tight\u2011fit and exploration.\"\"\"\n    feasible = bins_remain_cap >= item\n    if not feasible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    leftover = bins_remain_cap[feasible] - item\n    epsilon = 0.2\n    beta = 0.01\n    deterministic = 1.0 / (leftover + 1e-9) - beta * (leftover ** 2)\n    random_part = np.random.rand(feasible.sum())\n    scores = (1 - epsilon) * deterministic + epsilon * random_part\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    priorities[feasible] = scores\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\n_selection_counts = None\n_total_rewards = None\n_total_calls = 0\n\n# Priority combines inverse slack, best\u2011fit, first\u2011fit, fill\u2011balance and simple reward learning.\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Score bins using slack, fit, index and reward heuristics for online bin packing.\"\"\"\n    global _selection_counts, _total_rewards, _total_calls\n    caps = np.asarray(bins_remain_cap, dtype=float)\n    n = caps.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if _selection_counts is None or _selection_counts.shape[0] != n:\n        _selection_counts = np.zeros(n, dtype=float)\n        _total_rewards = np.zeros(n, dtype=float)\n    feasible = caps >= item\n    if not np.any(feasible):\n        return np.full(n, -np.inf, dtype=float)\n    leftovers_all = caps - item\n    leftovers = leftovers_all[feasible]\n    inv_slack = 1.0 / (leftovers + 1.0)\n    best_fit = -leftovers\n    first_fit = -np.arange(n)[feasible]\n    target = caps.mean()\n    fill_bal = -np.abs(leftovers - target)\n    avg_reward = np.zeros_like(leftovers)\n    mask = _selection_counts[feasible] > 0\n    avg_reward[mask] = _total_rewards[feasible][mask] / _selection_counts[feasible][mask]\n    w_inv, w_best, w_first, w_bal, w_rew = 0.4, 0.3, 0.1, 0.1, 0.1\n    combined = (w_inv * inv_slack + w_best * best_fit + w_first * first_fit +\n                w_bal * fill_bal + w_rew * avg_reward)\n    scores = np.full(n, -np.inf, dtype=float)\n    scores[feasible] = combined\n    best = int(np.argmax(scores))\n    reward = -(caps[best] - item)\n    _selection_counts[best] += 1\n    _total_rewards[best] += reward\n    _total_calls += 1\n    return scores\n\n[Heuristics 8th]\nimport numpy as np\n\n# Combine tight\u2011fit incentive, slack penalty, random exploration, and softmax scaling.\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Score bins by inverse slack, squared\u2011slack penalty, \u03b5\u2011greedy noise, and softmax.\"\"\"\n    feasible = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    if not np.any(feasible):\n        return priorities\n    slack = bins_remain_cap[feasible] - item\n    inv_slack = 1.0 / (slack + 1.0)\n    beta = 0.1\n    penalty = -beta * (slack ** 2)\n    deterministic = inv_slack + penalty\n    epsilon = 0.05\n    random_noise = epsilon * np.random.rand(deterministic.shape[0])\n    raw = deterministic + random_noise\n    temperature = 0.5\n    exp_vals = np.exp(raw / temperature)\n    total = exp_vals.sum()\n    if total > 0:\n        priorities[feasible] = exp_vals / total\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\n# Combines multiple slack components with epsilon\u2011greedy exploration and softmax scaling.\ndef priority_v2(item: float, bins_remain_cap: np.ndarray,\n                epsilon: float = 0.1, w_inv: float = 0.5,\n                w_lin: float = 0.3, w_sq: float = 0.2,\n                temperature: float = 1.0) -> np.ndarray:\n    \"\"\"\n    Epsilon\u2011greedy softmax priority blending inverse slack, linear slack penalty,\n    and squared slack penalty for balanced tight fits and exploration.\n    \"\"\"\n    feasible = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    if np.any(feasible):\n        slack = bins_remain_cap[feasible] - item\n        base = w_inv * (1.0 / (slack + 1.0)) - w_lin * slack - w_sq * slack**2\n        scores[feasible] = base\n    if epsilon > 0:\n        scores += epsilon * np.random.rand(bins_remain_cap.size)\n    finite_mask = np.isfinite(scores)\n    if not np.any(finite_mask):\n        return scores\n    exp_scores = np.exp((scores[finite_mask] / temperature) -\n                        np.max(scores[finite_mask] / temperature))\n    probs = np.zeros_like(scores)\n    probs[finite_mask] = exp_scores / exp_scores.sum()\n    return probs\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray,\n                epsilon: float = 0.2, beta: float = 0.02) -> np.ndarray:\n    \"\"\"Epsilon\u2011greedy tight\u2011fit with squared\u2011slack penalty for waste reduction.\"\"\"\n    feasible = bins_remain_cap >= item\n    scores = np.full(bins_remain_cap.shape, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    leftovers = bins_remain_cap[feasible] - item\n    det = 1.0/(leftovers + 1.0) - beta * leftovers**2\n    rand_part = np.random.rand(bins_remain_cap.size)\n    scores[feasible] = (1 - epsilon) * det + epsilon * rand_part[feasible]\n    return scores\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    epsilon = 0.15\n    w_inv = 0.5\n    w_lin = 0.3\n    w_sq = 0.2\n    temperature = 1.0\n    feasible = bins_remain_cap >= item\n    if not np.any(feasible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    slack = bins_remain_cap[feasible] - item\n    core = w_inv / (slack + 1.0) - w_lin * slack - w_sq * slack**2\n    rand = np.random.rand(feasible.sum())\n    scores_feasible = (1 - epsilon) * core + epsilon * rand\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    scores[feasible] = scores_feasible\n    max_score = np.max(scores_feasible)\n    exp_scores = np.exp((scores_feasible - max_score) / temperature)\n    probs_feasible = exp_scores / exp_scores.sum()\n    probs = np.zeros_like(bins_remain_cap, dtype=float)\n    probs[feasible] = probs_feasible\n    return probs\n\n[Heuristics 12th]\nimport numpy as np\n\n# Combine inverse\u2011slack and linear\u2011slack with \u03b5\u2011greedy randomness.\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    \u03b5\u2011greedy scoring: weighted sum of inverse slack and negative slack, with random tie\u2011breaker.\n    \"\"\"\n    epsilon = 0.15          # exploration factor\n    w_inv = 0.6             # weight for reciprocal slack\n    w_lin = 0.4             # weight for linear slack penalty\n    feasible = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    slack = bins_remain_cap[feasible] - item\n    base = w_inv * (1.0 / (slack + 1.0)) + w_lin * (-slack)\n    scores[feasible] = base\n    scores = (1 - epsilon) * scores + epsilon * np.random.rand(bins_remain_cap.shape[0])\n    # softmax scaling for probability\u2011like priorities\n    finite_mask = np.isfinite(scores)\n    if not np.any(finite_mask):\n        return scores\n    max_score = np.max(scores[finite_mask])\n    exp_scores = np.exp(scores - max_score)\n    exp_scores[~finite_mask] = 0.0\n    probabilities = exp_scores / exp_scores.sum()\n    return probabilities\n\n[Heuristics 13th]\ndef priority_v2(item, bins_remain_cap):\n    \"\"\"\n    Combines inverse slack, UCB, reward learning, and diversity for adaptive bin selection.\n    \"\"\"\n    import numpy as np\n    bins_remain_cap = np.asarray(bins_remain_cap, dtype=float)\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.array([], dtype=float)\n    if not hasattr(priority_v2, \"_selection_counts\") or priority_v2._bins_len != n:\n        priority_v2._selection_counts = np.zeros(n, dtype=int)\n        priority_v2._total_rewards = np.zeros(n, dtype=float)\n        priority_v2._bins_len = n\n    epsilon = 0.05\n    offset = 1e-6\n    score_scale = 1.0\n    reward_weight = 0.1\n    ucb_weight = 1.0\n    diversity_weight = 0.1\n    slack = bins_remain_cap - item\n    feasible = slack >= 0\n    det = np.zeros(n, dtype=float)\n    det[feasible] = 1.0 / (slack[feasible] + offset)\n    avg_reward = np.zeros(n, dtype=float)\n    nonzero = priority_v2._selection_counts > 0\n    avg_reward[nonzero] = priority_v2._total_rewards[nonzero] / priority_v2._selection_counts[nonzero]\n    total_counts = np.sum(priority_v2._selection_counts)\n    if total_counts == 0:\n        base = 1.0\n    else:\n        base = np.sqrt(2 * np.log(total_counts + 1))\n    ucb = np.zeros(n, dtype=float)\n    ucb[nonzero] = np.sqrt(2 * np.log(total_counts + 1) / priority_v2._selection_counts[nonzero])\n    ucb[~nonzero] = base\n    if feasible.any():\n        median_slack = np.median(slack[feasible])\n    else:\n        median_slack = 0.0\n    diversity = np.zeros(n, dtype=float)\n    diversity[feasible] = median_slack - slack[feasible]\n    combined = score_scale * det + reward_weight * avg_reward + ucb_weight * ucb + diversity_weight * diversity\n    combined[~feasible] = -np.inf\n    random_term = np.random.rand(n)\n    scores = (1 - epsilon) * combined + epsilon * random_term\n    return scores\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    slack = bins_remain_cap - item\n    fits = slack >= 0\n    eps = np.finfo(slack.dtype).eps\n    return np.where(fits, -slack + eps, -np.inf)\n\n[Heuristics 15th]\nimport numpy as np\n\n_initialized = False\n_model_mu = None\n_model_cov = None\n_model_precision = None\n_bin_capacity = 1.0\n_noise_var = 0.5\n_prior_var = 10.0\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    global _initialized, _model_mu, _model_cov, _model_precision, _bin_capacity, _noise_var, _prior_var\n    caps = np.asarray(bins_remain_cap, dtype=float)\n    n = caps.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if not _initialized:\n        _bin_capacity = np.max(caps) if caps.size > 0 else 1.0\n        d = 3\n        _model_mu = np.zeros(d)\n        _model_cov = np.eye(d) * _prior_var\n        _model_precision = np.linalg.inv(_model_cov)\n        _initialized = True\n    else:\n        max_cap = np.max(caps)\n        if max_cap > _bin_capacity:\n            _bin_capacity = max_cap\n    feasible = caps >= item\n    scores = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    slack = caps[feasible] - item\n    fill = 1.0 - slack / _bin_capacity\n    X = np.column_stack((np.ones_like(slack), slack, fill))\n    w = np.random.multivariate_normal(_model_mu, _model_cov)\n    scores[feasible] = X @ w\n    best = int(np.argmax(scores))\n    if feasible[best]:\n        slack_selected = caps[best] - item\n        reward = -slack_selected\n        x = np.array([1.0, slack_selected, 1.0 - slack_selected / _bin_capacity])\n        precision_update = _model_precision + np.outer(x, x) / _noise_var\n        cov_new = np.linalg.inv(precision_update)\n        mu_new = cov_new @ (_model_precision @ _model_mu + x * reward / _noise_var)\n        _model_cov = cov_new\n        _model_precision = precision_update\n        _model_mu = mu_new\n    return scores\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if not hasattr(priority_v2, \"initialized\"):\n        priority_v2.num_rules = 4\n        priority_v2.sum_waste = np.zeros(priority_v2.num_rules, dtype=float)\n        priority_v2.count = np.zeros(priority_v2.num_rules, dtype=int)\n        priority_v2.step = 0\n        priority_v2.epsilon = 1e-9\n        priority_v2.initialized = True\n    feasible = bins_remain_cap >= item\n    if not feasible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    leftovers = bins_remain_cap - item\n    best_fit_score = np.where(feasible, -leftovers, -np.inf)\n    worst_fit_score = np.where(feasible, leftovers, -np.inf)\n    indices = np.arange(bins_remain_cap.shape[0] if hasattr(bins_remain_cap, \"shape\") else len(bins_remain_cap))\n    first_fit_score = np.where(feasible, -indices, -np.inf)\n    target = bins_remain_cap.mean()\n    fill_bal_score = np.where(feasible, -np.abs(leftovers - target), -np.inf)\n    rule_scores = [best_fit_score, worst_fit_score, first_fit_score, fill_bal_score]\n    waste_per_rule = np.empty(priority_v2.num_rules, dtype=float)\n    for i, scores in enumerate(rule_scores):\n        if not np.isfinite(scores).any():\n            waste_per_rule[i] = np.inf\n        else:\n            idx = np.argmax(scores)\n            waste_per_rule[i] = bins_remain_cap[idx] - item\n    for i in range(priority_v2.num_rules):\n        w = waste_per_rule[i]\n        if np.isfinite(w):\n            priority_v2.sum_waste[i] += w\n            priority_v2.count[i] += 1\n    avg_waste = np.where(priority_v2.count > 0, priority_v2.sum_waste / priority_v2.count, np.inf)\n    inv = 1.0 / (avg_waste + priority_v2.epsilon)\n    boost = np.zeros(priority_v2.num_rules, dtype=float)\n    boost_idx = priority_v2.step % priority_v2.num_rules\n    boost[boost_idx] = 1.0 / (priority_v2.step + 1)\n    raw_weights = inv + boost\n    total = raw_weights.sum()\n    if total > 0:\n        weights = raw_weights / total\n    else:\n        weights = np.full(priority_v2.num_rules, 1.0 / priority_v2.num_rules)\n    combined_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    for w, scores in zip(weights, rule_scores):\n        combined_scores += w * scores\n    combined_scores[~feasible] = -np.inf\n    priority_v2.step += 1\n    return combined_scores\n\n[Heuristics 17th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                temperature: float = 8.21777003404979,\n                total_threshold: float = 0.7518899527010219) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        temperature: Temperature controlling the softness of the exponential weighting.\n        total_threshold: Minimum sum of exponential values required to assign non\u2011zero priorities.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    feasible = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    if not np.any(feasible):\n        return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\nclass PriorityV2:\n    def __init__(self):\n        self.total_calls = 0\n        self.selection_counts = None\n        self.total_rewards = None\n        self.temperature = 1.0\n        self.decay = 0.995\n\n    def priority(self, item, bins_remain_cap):\n        n = bins_remain_cap.shape[0]\n        if self.selection_counts is None or self.selection_counts.shape[0] != n:\n            self.selection_counts = np.zeros(n, dtype=float)\n            self.total_rewards = np.zeros(n, dtype=float)\n        feasible = bins_remain_cap >= item\n        leftovers = np.where(feasible, bins_remain_cap - item, np.inf)\n        base_score = np.where(feasible, 1.0 / (leftovers + 1.0), 0.0)\n        median_leftover = np.median(leftovers[feasible]) if np.any(feasible) else 0.0\n        diversity = np.where(feasible, 1.0 / (np.abs(leftovers - median_leftover) + 1.0), 0.0)\n        avg_reward = np.where(self.selection_counts > 0, self.total_rewards / self.selection_counts, 0.0)\n        c = 1.0\n        ucb = avg_reward + c * np.sqrt(np.log(self.total_calls + 1) / (self.selection_counts + 1))\n        combined_feas = 0.5 * base_score + 0.3 * diversity + 0.2 * ucb\n        combined = np.full(n, -np.inf, dtype=float)\n        combined[feasible] = combined_feas[feasible]\n        noise = np.random.rand(n)\n        combined[feasible] = (1 - self.temperature) * combined[feasible] + self.temperature * noise[feasible]\n        self.total_calls += 1\n        self.temperature *= self.decay\n        if np.any(feasible):\n            chosen = np.argmax(combined)\n            reward = - (bins_remain_cap[chosen] - item)\n            self.selection_counts[chosen] += 1\n            self.total_rewards[chosen] += reward\n        return combined\n\n_priority_v2_instance = PriorityV2()\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    return _priority_v2_instance.priority(item, bins_remain_cap)\n\n[Heuristics 19th]\nimport numpy as np\n\nclass PriorityV2:\n    def __init__(self):\n        self.total_calls = 0\n        self.selection_counts = None\n        self.total_rewards = None\n        self.temperature = 1.0\n        self.decay = 0.995\n\n    def priority(self, item, bins_remain_cap):\n        n = bins_remain_cap.shape[0]\n        if self.selection_counts is None or self.selection_counts.shape[0] != n:\n            self.selection_counts = np.zeros(n, dtype=float)\n            self.total_rewards = np.zeros(n, dtype=float)\n        feasible = bins_remain_cap >= item\n        leftovers = np.where(feasible, bins_remain_cap - item, np.inf)\n        base_score = np.where(feasible, 1.0 / (leftovers + 1.0), 0.0)\n        median_leftover = np.median(leftovers[feasible]) if np.any(feasible) else 0.0\n        diversity = np.where(feasible, 1.0 / (np.abs(leftovers - median_leftover) + 1.0), 0.0)\n        avg_reward = np.where(self.selection_counts > 0, self.total_rewards / self.selection_counts, 0.0)\n        c = 1.0\n        ucb = avg_reward + c * np.sqrt(np.log(self.total_calls + 1) / (self.selection_counts + 1))\n        combined_feas = 0.5 * base_score + 0.3 * diversity + 0.2 * ucb\n        combined = np.full(n, -np.inf, dtype=float)\n        combined[feasible] = combined_feas[feasible]\n        noise = np.random.rand(n)\n        combined[feasible] = (1 - self.temperature) * combined[feasible] + self.temperature * noise[feasible]\n        self.total_calls += 1\n        self.temperature *= self.decay\n        if np.any(feasible):\n            chosen = np.argmax(combined)\n            reward = - (bins_remain_cap[chosen] - item)\n            self.selection_counts[chosen] += 1\n            self.total_rewards[chosen] += reward\n        return combined\n\n_priority_v2_instance = PriorityV2()\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    return _priority_v2_instance.priority(item, bins_remain_cap)\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray,\n                epsilon: float = 0.2,\n                temperature: float = 1.0,\n                total_threshold: float = 0.0) -> np.ndarray:\n    \"\"\"Inverse slack scored, temperature\u2011scaled softmax, then blended with \u03b5\u2011noise for tie\u2011breaking.\"\"\"\n    feasible = bins_remain_cap >= item\n    scores = np.full(bins_remain_cap.shape, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    leftovers = bins_remain_cap[feasible] - item\n    base = 1.0 / (leftovers + 1.0)\n    exp_input = base / temperature\n    exp_input = np.clip(exp_input, a_min=None, a_max=50.0)\n    exp_scores = np.exp(exp_input)\n    total = exp_scores.sum()\n    if total >= total_threshold:\n        scores[feasible] = exp_scores / total\n    else:\n        scores[feasible] = 0.0\n    rand = np.random.rand(bins_remain_cap.shape[0])\n    rand[~feasible] = 0.0\n    return (1.0 - epsilon) * scores + epsilon * rand\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}