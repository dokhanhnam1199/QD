{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n_selection_counts = None\n_total_rewards = None\n\n    global _selection_counts, _total_rewards\n    caps = np.asarray(bins_remain_cap, dtype=float)\n    n = caps.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if _selection_counts is None or _selection_counts.shape[0] != n:\n        _selection_counts = np.zeros(n, dtype=float)\n        _total_rewards = np.zeros(n, dtype=float)\n    feasible = caps >= item\n    scores = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    leftovers = caps[feasible] - item\n    deterministic = 1.0 / (leftovers + 1.0)\n    epsilon = 0.2\n    random_part = np.random.rand(feasible.sum())\n    base_score = (1 - epsilon) * deterministic + epsilon * random_part\n    avg_reward = np.zeros_like(base_score)\n    mask = _selection_counts[feasible] > 0\n    avg_reward[mask] = _total_rewards[feasible][mask] / _selection_counts[feasible][mask]\n    reward_weight = 0.1\n    combined = base_score + reward_weight * avg_reward\n    scores[feasible] = combined\n    best = int(np.argmax(scores))\n    if feasible[best]:\n        reward = - (caps[best] - item)\n        _selection_counts[best] += 1\n        _total_rewards[best] += reward\n    return scores\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n_selection_counts = None\n_total_rewards = None\n_total_calls = 0\n\n    \"\"\"Inverse slack + epsilon random + avg reward for online bin packing.\"\"\"\n    global _selection_counts, _total_rewards, _total_calls\n    bins_remain_cap = np.asarray(bins_remain_cap, dtype=float)\n    n = bins_remain_cap.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if _selection_counts is None or _selection_counts.shape[0] != n:\n        _selection_counts = np.zeros(n, dtype=float)\n        _total_rewards = np.zeros(n, dtype=float)\n    feasible = bins_remain_cap >= item\n    scores = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    leftovers = bins_remain_cap[feasible] - item\n    deterministic = 1.0 / (leftovers + 1.0)\n    epsilon = 0.2\n    random_part = np.random.rand(feasible.sum())\n    base_score = (1 - epsilon) * deterministic + epsilon * random_part\n    avg_reward = np.zeros_like(base_score)\n    mask = _selection_counts[feasible] > 0\n    avg_reward[mask] = _total_rewards[feasible][mask] / _selection_counts[feasible][mask]\n    reward_weight = 0.3\n    combined = base_score + reward_weight * avg_reward\n    scores[feasible] = combined\n    best = int(np.argmax(scores))\n    _total_calls += 1\n    reward = - (bins_remain_cap[best] - item)\n    _selection_counts[best] += 1\n    _total_rewards[best] += reward\n    return scores\n\n### Analyze & experience\n- Comparing **(best)**\u202f(#1) vs **(worst)**\u202f(#20), we see the best heuristic maintains global state (`_selection_counts`, `_total_rewards`) to learn per\u2011bin rewards, combines deterministic inverse\u2011slack with an \u03b5\u2011greedy random term, and updates the reward after each placement. The worst heuristic merely computes inverse\u2011slack, applies a temperature\u2011scaled softmax, adds \u03b5\u2011noise, and never learns or adapts.  \n\n**(second best)**\u202f(#2) vs **(second worst)**\u202f(#19), both contain a docstring\u2011free reward learning core, but #2 lacks any exploration beyond \u03b5\u2011greedy randomness. #19 enriches the base score with a UCB exploration bonus, a median\u2011based diversity term, and weighted blending, thereby balancing exploitation with more sophisticated exploration.  \n\nComparing **(1st)**\u202f(#1) vs **(2nd)**\u202f(#2), the code is virtually identical: both initialize global counters, compute the same deterministic\u202f+\u202f\u03b5\u2011random base, add a small reward\u2011weight term (`reward_weight = 0.1`), and update the same global statistics after the chosen bin. No functional difference is evident.  \n\n**(3rd)**\u202f(#3) vs **(4th)**\u202f(#4), #3 retains the reward learning machinery, raises `reward_weight` to\u202f0.3, tracks `_total_calls`, and still returns raw scores. #4 abandons all learning, instead normalizing scores with a softmax (probability) transform and returning probabilities, making it a pure static scoring rule.  \n\nComparing **(second worst)**\u202f(#19) vs **(worst)**\u202f(#20), #19 blends deterministic inverse\u2011slack, \u03b5\u2011randomness, UCB, and diversity, and updates per\u2011bin reward statistics. #20 reduces everything to an inverse\u2011slack softmax with \u03b5\u2011noise, with no history, no UCB, and no diversity term.  \n\n**Overall:** The top heuristics consistently (i) use inverse\u2011slack as a deterministic fit measure, (ii) augment it with learned per\u2011bin reward signals, (iii) control exploration via \u03b5\u2011greedy, UCB, or diversity weighting, and (iv) keep state lightweight. Lower\u2011ranked versions strip away learning and exploration, relying on static transforms (softmax, random tie\u2011breakers) that cannot adapt to instance dynamics.\n- \n- **Keywords:** adaptive scoring, multi\u2011feature state, Thompson sampling, constraint\u2011aware learning.  \n- **Advice:** Build a lightweight feature vector (slack, item size, bin fill ratio, historical success) and train an online linear model or bandit; use Thompson sampling for exploration; update only with sufficient data; keep the model stateless between episodes.  \n- **Avoid:** Pure inverse\u2011slack baselines, fixed \u03b5\u2011greedy, heavy per\u2011bin state, ad\u2011hoc softmax, duplicated logic, and unchecked randomness.  \n- **Explanation:** A principled, data\u2011driven score balances feasibility and utilization, explores efficiently, and stays computationally cheap without over\u2011parameterizing or fragile heuristics.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}