{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n_selection_counts = None\n_total_rewards = None\n_total_calls = 0\n\n# Priority combines inverse slack, best\u2011fit, first\u2011fit, fill\u2011balance and simple reward learning.\n    \"\"\"Score bins using slack, fit, index and reward heuristics for online bin packing.\"\"\"\n    global _selection_counts, _total_rewards, _total_calls\n    caps = np.asarray(bins_remain_cap, dtype=float)\n    n = caps.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if _selection_counts is None or _selection_counts.shape[0] != n:\n        _selection_counts = np.zeros(n, dtype=float)\n        _total_rewards = np.zeros(n, dtype=float)\n    feasible = caps >= item\n    if not np.any(feasible):\n        return np.full(n, -np.inf, dtype=float)\n    leftovers_all = caps - item\n    leftovers = leftovers_all[feasible]\n    inv_slack = 1.0 / (leftovers + 1.0)\n    best_fit = -leftovers\n    first_fit = -np.arange(n)[feasible]\n    target = caps.mean()\n    fill_bal = -np.abs(leftovers - target)\n    avg_reward = np.zeros_like(leftovers)\n    mask = _selection_counts[feasible] > 0\n    avg_reward[mask] = _total_rewards[feasible][mask] / _selection_counts[feasible][mask]\n    w_inv, w_best, w_first, w_bal, w_rew = 0.4, 0.3, 0.1, 0.1, 0.1\n    combined = (w_inv * inv_slack + w_best * best_fit + w_first * first_fit +\n                w_bal * fill_bal + w_rew * avg_reward)\n    scores = np.full(n, -np.inf, dtype=float)\n    scores[feasible] = combined\n    best = int(np.argmax(scores))\n    reward = -(caps[best] - item)\n    _selection_counts[best] += 1\n    _total_rewards[best] += reward\n    _total_calls += 1\n    return scores\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    if not hasattr(priority_v2, \"initialized\"):\n        priority_v2.num_rules = 4\n        priority_v2.sum_waste = np.zeros(priority_v2.num_rules, dtype=float)\n        priority_v2.count = np.zeros(priority_v2.num_rules, dtype=int)\n        priority_v2.step = 0\n        priority_v2.epsilon = 1e-9\n        priority_v2.initialized = True\n    feasible = bins_remain_cap >= item\n    if not feasible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    leftovers = bins_remain_cap - item\n    best_fit_score = np.where(feasible, -leftovers, -np.inf)\n    worst_fit_score = np.where(feasible, leftovers, -np.inf)\n    indices = np.arange(bins_remain_cap.shape[0] if hasattr(bins_remain_cap, \"shape\") else len(bins_remain_cap))\n    first_fit_score = np.where(feasible, -indices, -np.inf)\n    target = bins_remain_cap.mean()\n    fill_bal_score = np.where(feasible, -np.abs(leftovers - target), -np.inf)\n    rule_scores = [best_fit_score, worst_fit_score, first_fit_score, fill_bal_score]\n    waste_per_rule = np.empty(priority_v2.num_rules, dtype=float)\n    for i, scores in enumerate(rule_scores):\n        if not np.isfinite(scores).any():\n            waste_per_rule[i] = np.inf\n        else:\n            idx = np.argmax(scores)\n            waste_per_rule[i] = bins_remain_cap[idx] - item\n    for i in range(priority_v2.num_rules):\n        w = waste_per_rule[i]\n        if np.isfinite(w):\n            priority_v2.sum_waste[i] += w\n            priority_v2.count[i] += 1\n    avg_waste = np.where(priority_v2.count > 0, priority_v2.sum_waste / priority_v2.count, np.inf)\n    inv = 1.0 / (avg_waste + priority_v2.epsilon)\n    boost = np.zeros(priority_v2.num_rules, dtype=float)\n    boost_idx = priority_v2.step % priority_v2.num_rules\n    boost[boost_idx] = 1.0 / (priority_v2.step + 1)\n    raw_weights = inv + boost\n    total = raw_weights.sum()\n    if total > 0:\n        weights = raw_weights / total\n    else:\n        weights = np.full(priority_v2.num_rules, 1.0 / priority_v2.num_rules)\n    combined_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    for w, scores in zip(weights, rule_scores):\n        combined_scores += w * scores\n    combined_scores[~feasible] = -np.inf\n    priority_v2.step += 1\n    return combined_scores\n\n### Analyze & experience\n- - **(1st) vs (20th):** The top heuristic keeps per\u2011bin selection counts and cumulative rewards, adds a modest \u03b5\u2011greedy term, and updates only after a feasible best\u2011bin is chosen. The worst version merely computes 1/(slack+1), applies a temperature\u2011scaled softmax and \u03b5\u2011noise, with no state or learning.  \n- **(2nd) vs (19th):** The second\u2011best also tracks counts/rewards and a total\u2011call counter, but uses a larger reward weight (0.3) and updates unconditionally after argmax. The second\u2011worst is identical to the 18th/20th design: static inverse\u2011slack scoring, softmax, \u03b5\u2011noise, no learning.  \n- **(1st) vs (2nd):** Both share the deterministic inverse\u2011slack\u202f+\u202f\u03b5\u2011random core and per\u2011bin reward averaging. Differences: the 2nd uses a higher reward weight (0.3) and records total calls (unused), while the 1st applies a lower reward weight (0.1) and safeguards updates by confirming the selected bin is feasible.  \n- **(3rd) vs (4th):** The 3rd adds a UCB exploration bonus, explicit total\u2011call tracking, and a smaller \u03b5 (0.15), blending deterministic, reward, and UCB terms. The 4th drops learning entirely, weighting deterministic slack by a static reward factor based on max capacity, then returns softmax probabilities.  \n- **(19th) vs (20th):** These two are identical duplicates of the simple softmax\u202f+\u202f\u03b5\u2011noise scheme; no distinction in behavior or performance.  \n- **Overall:** The highest\u2011ranked heuristics combine a lightweight online learning loop (counts, avg reward) with inverse\u2011slack scoring and modest \u03b5\u2011exploration, occasionally enriched by UCB. Lower\u2011ranked versions either discard learning, over\u2011engineer without clear benefit, or duplicate earlier ideas, leading to stagnation.\n- \n- **Keywords:** Inverse\u2011slack, \u03b5\u2011greedy, lightweight UCB, feasibility flag, softmax normalization, minimal per\u2011bin state.  \n- **Advice:** Start with deterministic inverse\u2011slack\u202f+\u202f\u03b5\u2011greedy, add lightweight UCB only when enough data, keep per\u2011bin stats minimal, update only after successful placements, and expose a single scoring function.  \n- **Avoid:** Over\u2011engineering mix of metrics, heavy state, duplicate code, excessive randomness, hidden logic, extra hyper\u2011parameters, waste\u2011promoting reward structures.  \n- **Explanation:** This keeps exploration\u2011exploitation balance, low runtime, reproducible outcomes, and a maintainable, extensible codebase.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}