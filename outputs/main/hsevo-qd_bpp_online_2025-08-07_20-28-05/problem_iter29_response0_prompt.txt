{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    if not hasattr(priority_v2, \"_init\"):\n        priority_v2._selection_counts = None\n        priority_v2._total_rewards = None\n        priority_v2._total_calls = 0\n        priority_v2._epsilon = 0.15\n        priority_v2._reward_weight = 0.3\n        priority_v2._ucb_weight = 0.05\n        priority_v2._init = True\n    bins_remain_cap = np.asarray(bins_v2, dtype=float) if False else np.asarray(bins_remain_cap, dtype=float)\n    n = bins_remain_cap.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if priority_v2._selection_counts is None or priority_v2._selection_counts.shape[0] != n:\n        priority_v2._selection_counts = np.zeros(n, dtype=float)\n        priority_v2._total_rewards = np.zeros(n, dtype=float)\n    feasible = bins_remain_cap >= item\n    scores = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        priority_v2._total_calls += 1\n        return scores\n    leftovers = bins_remain_cap[feasible] - item\n    deterministic = 1.0 / (leftovers + 1.0)\n    random_part = np.random.rand(feasible.sum())\n    base_score = (1 - priority_v2._epsilon) * deterministic + priority_v2._epsilon * random_part\n    avg_reward = np.zeros_like(base_score)\n    mask = priority_v2._selection_counts[feasible] > 0\n    avg_reward[mask] = priority_v2._total_rewards[feasible][mask] / priority_v2._selection_counts[feasible][mask]\n    total = priority_v2._total_calls + 1\n    ucb = np.sqrt(2.0 * np.log(total) / (priority_v2._selection_counts + 1e-6))\n    ucb_feas = ucb[feasible]\n    combined = base_score + priority_v2._reward_weight * avg_reward + priority_v2._ucb_weight * ucb_feas\n    scores[feasible] = combined\n    best = int(np.argmax(scores))\n    priority_v2._total_calls += 1\n    reward = - (bins_remain_cap[best] - item)\n    priority_v2._selection_counts[best] += 1\n    priority_v2._total_rewards[best] += reward\n    return scores\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n_initialized = False\n_model_mu = None\n_model_cov = None\n_model_precision = None\n_bin_capacity = 1.0\n_noise_var = 0.5\n_prior_var = 10.0\n\n    global _initialized, _model_mu, _model_cov, _model_precision, _bin_capacity, _noise_var, _prior_var\n    caps = np.asarray(bins_remain_cap, dtype=float)\n    n = caps.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if not _initialized:\n        _bin_capacity = np.max(caps) if caps.size > 0 else 1.0\n        d = 3\n        _model_mu = np.zeros(d)\n        _model_cov = np.eye(d) * _prior_var\n        _model_precision = np.linalg.inv(_model_cov)\n        _initialized = True\n    else:\n        max_cap = np.max(caps)\n        if max_cap > _bin_capacity:\n            _bin_capacity = max_cap\n    feasible = caps >= item\n    scores = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    slack = caps[feasible] - item\n    fill = 1.0 - slack / _bin_capacity\n    X = np.column_stack((np.ones_like(slack), slack, fill))\n    w = np.random.multivariate_normal(_model_mu, _model_cov)\n    scores[feasible] = X @ w\n    best = int(np.argmax(scores))\n    if feasible[best]:\n        slack_selected = caps[best] - item\n        reward = -slack_selected\n        x = np.array([1.0, slack_selected, 1.0 - slack_selected / _bin_capacity])\n        precision_update = _model_precision + np.outer(x, x) / _noise_var\n        cov_new = np.linalg.inv(precision_update)\n        mu_new = cov_new @ (_model_precision @ _model_mu + x * reward / _noise_var)\n        _model_cov = cov_new\n        _model_precision = precision_update\n        _model_mu = mu_new\n    return scores\n\n### Analyze & experience\n- Comparing (best) vs (worst), we see that the best (Heuristic\u202f1) is a compact, vectorized implementation that cleanly separates the inverse\u2011slack deterministic term and the \u03b5\u2011greedy random term, with no hidden state or side\u2011effects; the worst (Heuristic\u202f20) mixes many ad\u2011hoc bonuses (median diversity, UCB, diversity weight), contains inconsistent naming (`_pv2_counts` vs `bins_remain_cap`), and uses a clunky `best_idx` fallback, making it hard to read, maintain, and debug.  \n\n(second best) vs (second worst), we see that Heuristic\u202f2 includes a clear docstring, explicit handling of empty inputs, consistent shape checks, and a modest learning component (average reward); Heuristic\u202f19 is essentially a stub with no docstring, no scoring beyond \u2212\u221e, and no useful logic, offering no guidance to a caller.  \n\nComparing (1st) vs (2nd), we see that Heuristic\u202f1 favors simplicity and deterministic behavior, while Heuristic\u202f2 adds stateful online learning (selection counts, total rewards) and a reward\u2011weight term. The added complexity can improve adaptivity but also introduces mutable globals and extra bookkeeping, which may affect reproducibility.  \n\n(3rd) vs (4th), we see that Heuristic\u202f3 returns a proper probability distribution via softmax, enabling stochastic selection, but lacks a docstring; Heuristic\u202f4 provides a full docstring with parameter explanations and allows configurable offset and scaling, yet still returns raw scores rather than probabilities, limiting its direct use for sampling.  \n\n(second worst) vs (worst), we see that Heuristic\u202f19 offers only a minimal placeholder with no scoring logic, while Heuristic\u202f20 attempts a richer model (median\u2011based diversity, UCB, epsilon random) but suffers from duplicated code, unclear variable intent, and a fragile best\u2011index default, making it less reliable than the trivial stub.  \n\nOverall, the top heuristics prioritize clear vectorized math, minimal side\u2011effects, and concise documentation; lower\u2011ranked versions either add unnecessary complexity, lack proper comments/docstrings, or contain bugs that outweigh any theoretical benefit.\n- \n- **Keywords:** modularity, type\u2011safety, offline analysis, domain\u2011specific metrics, vectorization, reproducibility.  \n- **Advice:** decompose the heuristic into steps (feasibility test, cost estimate, selection); compute scores from metrics such as space utilization, geometry fit, and load balance; use immutable data structures and type hints; rely on vectorized NumPy ops and memoize sub\u2011computations; document each step with unit tests.  \n- **Avoid:** ad\u2011hoc metric blends, hidden mutable state, unnecessary hyper\u2011parameters, duplicated logic, non\u2011deterministic randomness.  \n- **Explanation:** A clear, modular pipeline with immutable, typed data and analytically derived scores stays fast, testable, and easy to maintain while leveraging rich problem\u2011specific information.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}