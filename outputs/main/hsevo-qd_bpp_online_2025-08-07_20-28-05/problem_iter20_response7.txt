```python
import numpy as np

# Global learning state (per‑bin counts, cumulative rewards, total calls)
_pv2_counts = None
_pv2_rewards = None
_pv2_calls = 0

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Score bins using inverse‑slack, ε‑noise, UCB bonus and median‑diversity for online BPP."""
    global _pv2_counts, _pv2_rewards, _pv2_calls

    caps = np.asarray(bins_remain_cap, dtype=float)
    n = caps.size
    if n == 0:
        return np.empty(0, dtype=float)

    # Initialise learning structures when size changes or first call
    if _pv2_counts is None or _pv2_counts.shape[0] != n:
        _pv2_counts = np.zeros(n, dtype=float)
        _pv2_rewards = np.zeros(n, dtype=float)

    # Feasibility mask
    feasible = caps >= item
    if not np.any(feasible):
        return np.full(n, -np.inf, dtype=float)

    # Deterministic slack component (tightness)
    leftovers = caps[feasible] - item
    det = 1.0 / (leftovers + 1.0)

    # ε‑greedy random perturbation
    epsilon = 0.15
    rand = np.random.rand(leftovers.size)
    base = (1 - epsilon) * det + epsilon * rand

    # Historical reward term (average reward per bin)
    counts_feas = _pv2_counts[feasible]
    avg_reward = np.where(counts_feas > 0,
                          _pv2_rewards[feasible] / counts_feas,
                          0.0)
    reward_weight = 0.1

    # UCB exploration bonus
    c = 0.5
    ucb = c * np.sqrt(np.log(_pv2_calls + 1) / (counts_feas + 1))

    # Diversity term based on median leftover
    median_left = np.median(leftovers) if leftovers.size > 0 else 0.0
    diversity = 1.0 / (np.abs(leftovers - median_left) + 1.0)
    diversity_weight = 0.05

    # Combine all components into final score for feasible bins
    score_feas = base + reward_weight * avg_reward + ucb + diversity_weight * diversity

    # Assemble full score vector (infeasible bins get -inf)
    scores = np.full(n, -np.inf, dtype=float)
    scores[feasible] = score_feas

    # Update learning statistics for the chosen bin
    best = np.argmax(scores)
    reward = - (caps[best] - item)          # negative leftover encourages tighter packing
    _pv2_counts[best] += 1
    _pv2_rewards[best] += reward
    _pv2_calls += 0  # increment after using current call count in UCB
    _pv2_calls += 1

    return scores
```
