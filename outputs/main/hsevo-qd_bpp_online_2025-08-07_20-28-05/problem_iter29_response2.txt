```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Score bins for online BPP using inverse slack, ε‑greedy, avg reward, and UCB."""
    # Initialize static state on first call
    if not hasattr(priority_v2, "_init"):
        priority_v2._counts = None          # selection counts per bin
        priority_v2._rewards = None         # cumulative rewards per bin
        priority_v2._calls = 0              # total calls
        priority_v2._epsilon = 0.1          # exploration rate
        priority_v2._reward_weight = 0.3    # weight for avg reward term
        priority_v2._ucb_weight = 0.05      # weight for UCB term
        priority_v2._init = True

    caps = np.asarray(bins_remain_cap, dtype=float)
    n = caps.shape[0]
    if n == 0:
        return np.array([], dtype=float)

    # Resize state if number of bins changes
    if priority_v2._counts is None or priority_v2._counts.shape[0] != n:
        priority_v2._counts = np.zeros(n, dtype=float)
        priority_v2._rewards = np.zeros(n, dtype=float)

    feasible = caps >= item
    scores = np.full(n, -np.inf, dtype=float)

    if not np.any(feasible):
        priority_v2._calls += 1
        return scores

    # --- deterministic part: inverse slack (more remaining space → lower score) ---
    leftover = caps[feasible] - item
    inv_slack = 1.0 / (leftover + 1.0)

    # --- exploration part: ε‑greedy random noise ---
    rand_part = np.random.rand(leftover.shape[0])
    base_score = (1 - priority_v2._epsilon) * inv_slack + priority_v2._epsilon * rand_part

    # --- average reward term (exploitation of learned bin quality) ---
    idx_feas = np.where(feasible)[0]
    counts_feas = priority_v2._counts[idx_feas]
    rewards_feas = priority_v2._rewards[idx_feas]
    avg_reward = np.zeros_like(base_score)
    mask = counts_feas > 0
    avg_reward[mask] = rewards_feas[mask] / counts_feas[mask]

    # --- Upper Confidence Bound term (balance exploration/exploitation) ---
    total_calls = priority_v2._calls + 1
    ucb = np.sqrt(2.0 * np.log(total_calls) / (counts_feas + 1e-6))

    # --- combine all components ---
    scores_feas = base_score + priority_v2._reward_weight * avg_reward + priority_v2._ucb_weight * ucb
    scores[idx_feas] = scores_feas

    # Update online statistics for the selected bin
    best = int(np.argmax(scores))
    priority_v2._calls += 1
    reward = - (caps[best] - item)          # negative leftover as reward
    priority_v2._counts[best] += 1
    priority_v2._rewards[best] += reward

    return scores
```
