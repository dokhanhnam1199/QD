{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    slack = bins_remain_cap - item\n    fits = slack >= 0\n    eps = np.finfo(slack.dtype).eps\n    return np.where(fits, -slack + eps, -np.inf)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\nclass PriorityV2:\n    def __init__(self):\n        self.total_calls = 0\n        self.selection_counts = None\n        self.total_rewards = None\n        self.temperature = 1.0\n        self.decay = 0.995\n\n    def priority(self, item, bins_remain_cap):\n        n = bins_remain_cap.shape[0]\n        if self.selection_counts is None or self.selection_counts.shape[0] != n:\n            self.selection_counts = np.zeros(n, dtype=float)\n            self.total_rewards = np.zeros(n, dtype=float)\n        feasible = bins_remain_cap >= item\n        leftovers = np.where(feasible, bins_remain_cap - item, np.inf)\n        base_score = np.where(feasible, 1.0 / (leftovers + 1.0), 0.0)\n        median_leftover = np.median(leftovers[feasible]) if np.any(feasible) else 0.0\n        diversity = np.where(feasible, 1.0 / (np.abs(leftovers - median_leftover) + 1.0), 0.0)\n        avg_reward = np.where(self.selection_counts > 0, self.total_rewards / self.selection_counts, 0.0)\n        c = 1.0\n        ucb = avg_reward + c * np.sqrt(np.log(self.total_calls + 1) / (self.selection_counts + 1))\n        combined_feas = 0.5 * base_score + 0.3 * diversity + 0.2 * ucb\n        combined = np.full(n, -np.inf, dtype=float)\n        combined[feasible] = combined_feas[feasible]\n        noise = np.random.rand(n)\n        combined[feasible] = (1 - self.temperature) * combined[feasible] + self.temperature * noise[feasible]\n        self.total_calls += 1\n        self.temperature *= self.decay\n        if np.any(feasible):\n            chosen = np.argmax(combined)\n            reward = - (bins_remain_cap[chosen] - item)\n            self.selection_counts[chosen] += 1\n            self.total_rewards[chosen] += reward\n        return combined\n\n_priority_v2_instance = PriorityV2()\n\n    return _priority_v2_instance.priority(item, bins_remain_cap)\n\n### Analyze & experience\n- Comparing (best) vs (worst), we see that Heuristics\u202f1\ufe0f\u20e3 (global\u202f_selection_counts/_total_rewards, \u03b5\u2011greedy inverse\u2011slack, reward weighting) adapts via per\u2011bin learning, while Heuristics\u202f20\ufe0f\u20e3 only applies inverse slack with softmax and \u03b5\u2011noise, lacking any feedback loop.  \n(second best) vs (second worst), Heuristics\u202f2\ufe0f\u20e3 (simple avg\u2011reward update, deterministic\u2011plus\u2011random score) beats Heuristics\u202f19\ufe0f\u20e3 (class with UCB, diversity term, temperature decay, many hyper\u2011parameters) \u2013 the leaner reward estimator is more stable than a heavily tuned UCB scheme.  \nComparing (1st) vs (2nd), the code is virtually identical (same global state, same scoring); the ranking difference likely stems from marginal documentation or initialization nuances rather than algorithmic change.  \n(3rd) vs (4th), Heuristics\u202f3\ufe0f\u20e3 returns softmax probabilities from inverse\u2011slack\u202f+\u202f\u03b5\u2011noise, whereas Heuristics\u202f4\ufe0f\u20e3 adds per\u2011bin avg\u2011reward and a UCB bonus. The extra UCB term can cause over\u2011exploration and noisy updates, making the simpler softmax version superior.  \n(second worst) vs (worst), Heuristics\u202f19\ufe0f\u20e3 (UCB, diversity, temperature decay, learning updates) outperforms Heuristics\u202f20\ufe0f\u20e3 (static inverse\u2011slack softmax) by retaining a modest adaptive component.  \nOverall: lightweight adaptive heuristics that blend deterministic fit with modest \u03b5\u2011exploration and a single reward statistic consistently beat overly complex designs with multiple bonuses, aggressive UCB terms, or excessive randomness.\n- \n- **Keywords**: inverse\u2011slack, \u03b5\u2011greedy, stateless; **Advice**: use inverse\u2011slack as core score with \u03b5\u2011randomness; **Avoid**: UCB or heavy tuning before data; **Explanation**: ensures fast, predictable decisions.  \n- **Keywords**: reward\u2011averaging, batch; **Advice**: accumulate rewards over a window then update average; **Avoid**: per\u2011step volatile updates; **Explanation**: lowers variance with minimal overhead.  \n- **Keywords**: softmax, probability; **Advice**: apply softmax only when downstream needs probabilities; **Avoid**: unconditional scaling that hides raw scores; **Explanation**: retains interpretability and proper distribution.  \n- **Keywords**: documentation, modular code; **Advice**: expose all hyper\u2011parameters and keep functions pure; **Avoid**: hidden state, duplicated logic, unused imports; **Explanation**: boosts maintainability and safe extension.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}