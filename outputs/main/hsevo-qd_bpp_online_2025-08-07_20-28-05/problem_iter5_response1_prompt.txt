{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    feasible = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    if not np.any(feasible):\n        return priorities\n    slack = bins_remain_cap[feasible] - item\n    temperature = 1.0\n    exp_vals = np.exp(-slack / temperature)\n    total = exp_vals.sum()\n    if total > 0:\n        priorities[feasible] = exp_vals / total\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Priority for Worst-Fit: higher value for bins leaving most empty after packing.\"\"\"\n    return np.where(bins_remain_cap >= item, (bins_remain_cap - item) ** 2, -np.inf)\n\n### Analyze & experience\n- - **1st vs 20th:** The best heuristic uses an \u03b5\u2011greedy inverse\u2011slack score (`1/(leftover+1)`) with a random term and returns raw scores, encouraging tight fits and exploration. The worst heuristic (worst\u2011fit) returns squared leftover (`(slack)\u00b2`) with no randomness, promoting wasteful packing.  \n- **2nd vs 19th:** Identical to the 1st/20th comparison; second\u2011best also uses \u03b5\u2011greedy inverse\u2011slack, while second\u2011worst repeats the pure worst\u2011fit rule.  \n- **3rd vs 18th:** The 3rd heuristic adds a feasibility check, combines deterministic and random parts, then applies a softmax to produce a probability distribution, improving robustness. The 18th remains deterministic worst\u2011fit with no exploration.  \n- **4th vs 17th:** 4th is essentially the same softmax\u2011based \u03b5\u2011greedy approach as 3rd, while 17th is a plain \u03b5\u2011greedy inverse\u2011slack without softmax, offering less calibrated stochastic selection.  \n- **5th vs 16th:** Mirrors 4th vs 17th; 5th retains the softmax probability conversion, 16th repeats the simple \u03b5\u2011greedy raw score.  \n- **6th vs 15th:** Same pattern; 6th uses softmax probabilities, 15th provides raw \u03b5\u2011greedy scores.  \n- **7th vs 14th:** 7th introduces configurable `offset` and `score_scale` with a docstring, allowing tuning of the deterministic term while still using \u03b5\u2011greedy randomness. 14th (a duplicate of 11th) uses a weighted mix of inverse and linear slack plus softmax, but lacks the extra tunable parameters.  \n- **8th vs 13th:** 8th blends inverse slack with a negative squared\u2011slack penalty (`-\u03b2\u00b7slack\u00b2`) and \u03b5\u2011greedy randomness, improving discrimination between very tight and moderately tight fits. 13th simply returns `-slack+\u03b5`, a deterministic tight\u2011fit score without exploration.  \n- **9th vs 12th:** Both compute softmax of `exp(-slack/temperature)`, yielding a probabilistic preference for tighter bins; they include docstrings clarifying purpose.  \n- **10th vs 11th:** 10th returns `-slack+\u03b5` (deterministic tight\u2011fit). 11th augments this with a weighted sum of inverse and linear slack, \u03b5\u2011greedy randomness, and softmax, providing richer scoring.  \n- **Comparisons among adjacent ranks:** 1st vs 2nd are identical (no difference). 3rd, 4th, 5th, 6th are repeats of the same softmax\u2011based \u03b5\u2011greedy design, showing no degradation. 15th\u201117th repeat the simple \u03b5\u2011greedy raw score, indicating a step down from the probability\u2011scaled versions. 18th\u201120th are identical worst\u2011fit implementations, confirming the lowest rank.\n- \n- **Keywords:** predictive ranking, utilization. **Advice:** Train a model to forecast bin impact and rank bins. **Avoid:** static single\u2011metric scores. **Explanation:** Captures long\u2011term balance.  \n- **Keywords:** bandit, regret. **Advice:** Use a bandit algorithm that updates exploration probability from observed regret. **Avoid:** fixed random perturbations. **Explanation:** Targets high\u2011uncertainty bins.  \n- **Keywords:** simulated annealing, temperature. **Advice:** Apply SA to reassign items, cooling temperature to reduce randomness. **Avoid:** one\u2011shot deterministic scoring. **Explanation:** Escapes local minima.  \n- **Keywords:** constraint propagation, diversity. **Advice:** Combine CP pruning with a diversity metric that spreads items across bins. **Avoid:** worst\u2011fit heuristics. **Explanation:** Ensures feasibility and avoids over\u2011concentration.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}