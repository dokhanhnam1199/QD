```python
import numpy as np

_selection_counts = None
_total_rewards = None
_total_calls = 0

# Priority combines inverse slack, best‑fit, first‑fit, fill‑balance and simple reward learning.
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Score bins using slack, fit, index and reward heuristics for online bin packing."""
    global _selection_counts, _total_rewards, _total_calls
    caps = np.asarray(bins_remain_cap, dtype=float)
    n = caps.shape[0]
    if n == 0:
        return np.array([], dtype=float)
    if _selection_counts is None or _selection_counts.shape[0] != n:
        _selection_counts = np.zeros(n, dtype=float)
        _total_rewards = np.zeros(n, dtype=float)
    feasible = caps >= item
    if not np.any(feasible):
        return np.full(n, -np.inf, dtype=float)
    leftovers_all = caps - item
    leftovers = leftovers_all[feasible]
    inv_slack = 1.0 / (leftovers + 1.0)
    best_fit = -leftovers
    first_fit = -np.arange(n)[feasible]
    target = caps.mean()
    fill_bal = -np.abs(leftovers - target)
    avg_reward = np.zeros_like(leftovers)
    mask = _selection_counts[feasible] > 0
    avg_reward[mask] = _total_rewards[feasible][mask] / _selection_counts[feasible][mask]
    w_inv, w_best, w_first, w_bal, w_rew = 0.4, 0.3, 0.1, 0.1, 0.1
    combined = (w_inv * inv_slack + w_best * best_fit + w_first * first_fit +
                w_bal * fill_bal + w_rew * avg_reward)
    scores = np.full(n, -np.inf, dtype=float)
    scores[feasible] = combined
    best = int(np.argmax(scores))
    reward = -(caps[best] - item)
    _selection_counts[best] += 1
    _total_rewards[best] += reward
    _total_calls += 1
    return scores
```
