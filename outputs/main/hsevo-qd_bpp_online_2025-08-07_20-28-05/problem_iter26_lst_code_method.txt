{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\n_selection_counts = None\n_total_rewards = None\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    global _selection_counts, _total_rewards\n    caps = np.asarray(bins_remain_cap, dtype=float)\n    n = caps.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if _selection_counts is None or _selection_counts.shape[0] != n:\n        _selection_counts = np.zeros(n, dtype=float)\n        _total_rewards = np.zeros(n, dtype=float)\n    feasible = caps >= item\n    scores = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    leftovers = caps[feasible] - item\n    deterministic = 1.0 / (leftovers + 1.0)\n    epsilon = 0.2\n    random_part = np.random.rand(feasible.sum())\n    base_score = (1 - epsilon) * deterministic + epsilon * random_part\n    avg_reward = np.zeros_like(base_score)\n    mask = _selection_counts[feasible] > 0\n    avg_reward[mask] = _total_rewards[feasible][mask] / _selection_counts[feasible][mask]\n    reward_weight = 0.1\n    combined = base_score + reward_weight * avg_reward\n    scores[feasible] = combined\n    best = int(np.argmax(scores))\n    if feasible[best]:\n        reward = - (caps[best] - item)\n        _selection_counts[best] += 1\n        _total_rewards[best] += reward\n    return scores\n\n[Heuristics 2nd]\nimport numpy as np\n\n_selection_counts = None\n_total_rewards = None\n_total_calls = 0\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Inverse slack + epsilon random + avg reward for online bin packing.\"\"\"\n    global _selection_counts, _total_rewards, _total_calls\n    bins_remain_cap = np.asarray(bins_remain_cap, dtype=float)\n    n = bins_remain_cap.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if _selection_counts is None or _selection_counts.shape[0] != n:\n        _selection_counts = np.zeros(n, dtype=float)\n        _total_rewards = np.zeros(n, dtype=float)\n    feasible = bins_remain_cap >= item\n    scores = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    leftovers = bins_remain_cap[feasible] - item\n    deterministic = 1.0 / (leftovers + 1.0)\n    epsilon = 0.2\n    random_part = np.random.rand(feasible.sum())\n    base_score = (1 - epsilon) * deterministic + epsilon * random_part\n    avg_reward = np.zeros_like(base_score)\n    mask = _selection_counts[feasible] > 0\n    avg_reward[mask] = _total_rewards[feasible][mask] / _selection_counts[feasible][mask]\n    reward_weight = 0.3\n    combined = base_score + reward_weight * avg_reward\n    scores[feasible] = combined\n    best = int(np.argmax(scores))\n    _total_calls += 1\n    reward = - (bins_remain_cap[best] - item)\n    _selection_counts[best] += 1\n    _total_rewards[best] += reward\n    return scores\n\n[Heuristics 3rd]\nimport numpy as np\n\n_selection_counts = None\n_total_rewards = None\n_total_calls = 0\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combine inverse slack, \u03b5\u2011greedy noise, per\u2011bin reward, and UCB bonus.\"\"\"\n    global _selection_counts, _total_rewards, _total_calls\n    caps = np.asarray(bins_remain_cap, dtype=float)\n    n = caps.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if _selection_counts is None or _selection_counts.shape[0] != n:\n        _selection_counts = np.zeros(n, dtype=float)\n        _total_rewards = np.zeros(n, dtype=float)\n        _total_calls = 0\n    feasible = caps >= item\n    scores = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    leftovers = caps[feasible] - item\n    deterministic = 1.0 / (leftovers + 1.0)\n    epsilon = 0.15\n    random_part = np.random.rand(feasible.sum())\n    base_score = (1 - epsilon) * deterministic + epsilon * random_part\n    avg_reward = np.zeros_like(base_score)\n    mask = _selection_counts[feasible] > 0\n    avg_reward[mask] = _total_rewards[feasible][mask] / _selection_counts[feasible][mask]\n    reward_weight = 0.1\n    reward_component = reward_weight * avg_reward\n    c = 0.2\n    total = _total_calls + 1\n    sel_counts = _selection_counts[feasible] + 1e-9\n    ucb_bonus = c * np.sqrt(np.log(total) / sel_counts)\n    combined = base_score + reward_component + ucb_bonus\n    scores[feasible] = combined\n    best = int(np.argmax(scores))\n    _total_calls += 1\n    reward = - (caps[best] - item)\n    _selection_counts[best] += 1\n    _total_rewards[best] += reward\n    return scores\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines inverse\u2011slack, \u03b5\u2011greedy exploration, and reward weighting for online bin packing.\"\"\"\n    epsilon = 0.2\n    feasible = bins_remain_cap >= item\n    if not np.any(feasible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    leftovers = bins_remain_cap[feasible] - item\n    deterministic = 1.0 / (leftovers + 1.0)\n    max_cap = bins_remain_cap.max()\n    reward = 1.0 - leftovers / max_cap\n    reward = np.clip(reward, 0.0, 1.0)\n    weighted = deterministic * (1.0 + reward)\n    random_part = np.random.rand(feasible.sum())\n    combined = (1 - epsilon) * weighted + epsilon * random_part\n    scores[feasible] = combined\n    max_score = np.max(scores[feasible])\n    exp_scores = np.exp(scores - max_score)\n    probabilities = exp_scores / exp_scores.sum()\n    return probabilities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines inverse\u2011slack, \u03b5\u2011greedy exploration, and reward weighting for online bin packing.\"\"\"\n    epsilon = 0.2\n    feasible = bins_remain_cap >= item\n    if not np.any(feasible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    leftovers = bins_remain_cap[feasible] - item\n    deterministic = 1.0 / (leftovers + 1.0)\n    max_cap = bins_remain_cap.max()\n    reward = 1.0 - leftovers / max_cap\n    reward = np.clip(reward, 0.0, 1.0)\n    weighted = deterministic * (1.0 + reward)\n    random_part = np.random.rand(feasible.sum())\n    combined = (1 - epsilon) * weighted + epsilon * random_part\n    scores[feasible] = combined\n    max_score = np.max(scores[feasible])\n    exp_scores = np.exp(scores - max_score)\n    probabilities = exp_scores / exp_scores.sum()\n    return probabilities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combine inverse slack and negative squared slack with epsilon\u2011greedy randomness for balanced tight\u2011fit and exploration.\"\"\"\n    feasible = bins_remain_cap >= item\n    if not feasible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    leftover = bins_remain_cap[feasible] - item\n    epsilon = 0.2\n    beta = 0.01\n    deterministic = 1.0 / (leftover + 1e-9) - beta * (leftover ** 2)\n    random_part = np.random.rand(feasible.sum())\n    scores = (1 - epsilon) * deterministic + epsilon * random_part\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    priorities[feasible] = scores\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combine inverse slack and negative squared slack with epsilon\u2011greedy randomness for balanced tight\u2011fit and exploration.\"\"\"\n    feasible = bins_remain_cap >= item\n    if not feasible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    leftover = bins_remain_cap[feasible] - item\n    epsilon = 0.2\n    beta = 0.01\n    deterministic = 1.0 / (leftover + 1e-9) - beta * (leftover ** 2)\n    random_part = np.random.rand(feasible.sum())\n    scores = (1 - epsilon) * deterministic + epsilon * random_part\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    priorities[feasible] = scores\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\n_selection_counts = None\n_total_rewards = None\n_total_calls = 0\n\n# Priority combines inverse slack, best\u2011fit, first\u2011fit, fill\u2011balance and simple reward learning.\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Score bins using slack, fit, index and reward heuristics for online bin packing.\"\"\"\n    global _selection_counts, _total_rewards, _total_calls\n    caps = np.asarray(bins_remain_cap, dtype=float)\n    n = caps.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if _selection_counts is None or _selection_counts.shape[0] != n:\n        _selection_counts = np.zeros(n, dtype=float)\n        _total_rewards = np.zeros(n, dtype=float)\n    feasible = caps >= item\n    if not np.any(feasible):\n        return np.full(n, -np.inf, dtype=float)\n    leftovers_all = caps - item\n    leftovers = leftovers_all[feasible]\n    inv_slack = 1.0 / (leftovers + 1.0)\n    best_fit = -leftovers\n    first_fit = -np.arange(n)[feasible]\n    target = caps.mean()\n    fill_bal = -np.abs(leftovers - target)\n    avg_reward = np.zeros_like(leftovers)\n    mask = _selection_counts[feasible] > 0\n    avg_reward[mask] = _total_rewards[feasible][mask] / _selection_counts[feasible][mask]\n    w_inv, w_best, w_first, w_bal, w_rew = 0.4, 0.3, 0.1, 0.1, 0.1\n    combined = (w_inv * inv_slack + w_best * best_fit + w_first * first_fit +\n                w_bal * fill_bal + w_rew * avg_reward)\n    scores = np.full(n, -np.inf, dtype=float)\n    scores[feasible] = combined\n    best = int(np.argmax(scores))\n    reward = -(caps[best] - item)\n    _selection_counts[best] += 1\n    _total_rewards[best] += reward\n    _total_calls += 1\n    return scores\n\n[Heuristics 9th]\ndef priority_v2(item, bins_remain_cap):\n    \"\"\"\n    Combines inverse slack, UCB, reward learning, and diversity for adaptive bin selection.\n    \"\"\"\n    import numpy as np\n    bins_remain_cap = np.asarray(bins_remain_cap, dtype=float)\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.array([], dtype=float)\n    if not hasattr(priority_v2, \"_selection_counts\") or priority_v2._bins_len != n:\n        priority_v2._selection_counts = np.zeros(n, dtype=int)\n        priority_v2._total_rewards = np.zeros(n, dtype=float)\n        priority_v2._bins_len = n\n    epsilon = 0.05\n    offset = 1e-6\n    score_scale = 1.0\n    reward_weight = 0.1\n    ucb_weight = 1.0\n    diversity_weight = 0.1\n    slack = bins_remain_cap - item\n    feasible = slack >= 0\n    det = np.zeros(n, dtype=float)\n    det[feasible] = 1.0 / (slack[feasible] + offset)\n    avg_reward = np.zeros(n, dtype=float)\n    nonzero = priority_v2._selection_counts > 0\n    avg_reward[nonzero] = priority_v2._total_rewards[nonzero] / priority_v2._selection_counts[nonzero]\n    total_counts = np.sum(priority_v2._selection_counts)\n    if total_counts == 0:\n        base = 1.0\n    else:\n        base = np.sqrt(2 * np.log(total_counts + 1))\n    ucb = np.zeros(n, dtype=float)\n    ucb[nonzero] = np.sqrt(2 * np.log(total_counts + 1) / priority_v2._selection_counts[nonzero])\n    ucb[~nonzero] = base\n    if feasible.any():\n        median_slack = np.median(slack[feasible])\n    else:\n        median_slack = 0.0\n    diversity = np.zeros(n, dtype=float)\n    diversity[feasible] = median_slack - slack[feasible]\n    combined = score_scale * det + reward_weight * avg_reward + ucb_weight * ucb + diversity_weight * diversity\n    combined[~feasible] = -np.inf\n    random_term = np.random.rand(n)\n    scores = (1 - epsilon) * combined + epsilon * random_term\n    return scores\n\n[Heuristics 10th]\nimport numpy as np\n\n_epsilon = 0.2\n_beta = 0.01\n_gamma = 0.5\n_delta = 0.1\n_bin_capacity = None\n_avg_slack = None\n_slack_counts = None\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Score bins using inverse slack, fill bias, avg slack, and \u03b5\u2011exploration.\"\"\"\n    global _bin_capacity, _avg_slack, _slack_counts\n    caps = np.asarray(bins_remain_cap, dtype=float)\n    n = caps.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if _bin_capacity is None:\n        _bin_capacity = caps.max() if caps.size > 0 else 1.0\n    elif caps.max() > _bin_capacity:\n        _bin_capacity = caps.max()\n    if _avg_slack is None or _avg_slack.shape[0] != n:\n        _avg_slack = np.zeros(n, dtype=float)\n        _slack_counts = np.zeros(n, dtype=int)\n    feasible = caps >= item\n    if not feasible.any():\n        return np.full_like(caps, -np.inf)\n    slack = caps[feasible] - item\n    inv_slack = 1.0 / (slack + 1e-9)\n    fill = 1.0 - slack / _bin_capacity\n    avg_slack = _avg_slack[feasible]\n    deterministic = inv_slack - _beta * slack**2 + _gamma * fill - _delta * avg_slack\n    random_part = np.random.rand(slack.size)\n    scores = (1 - _epsilon) * deterministic + _epsilon * random_part\n    priorities = np.full_like(caps, -np.inf)\n    priorities[feasible] = scores\n    idx = np.where(feasible)[0]\n    _avg_slack[idx] = (_avg_slack[idx] * _slack_counts[idx] + slack) / (_slack_counts[idx] + 1)\n    _slack_counts[idx] += 1\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\n_initialized = False\n_model_mu = None\n_model_cov = None\n_model_precision = None\n_bin_capacity = 1.0\n_noise_var = 0.5\n_prior_var = 10.0\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    global _initialized, _model_mu, _model_cov, _model_precision, _bin_capacity, _noise_var, _prior_var\n    caps = np.asarray(bins_remain_cap, dtype=float)\n    n = caps.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if not _initialized:\n        _bin_capacity = np.max(caps) if caps.size > 0 else 1.0\n        d = 3\n        _model_mu = np.zeros(d)\n        _model_cov = np.eye(d) * _prior_var\n        _model_precision = np.linalg.inv(_model_cov)\n        _initialized = True\n    else:\n        max_cap = np.max(caps)\n        if max_cap > _bin_capacity:\n            _bin_capacity = max_cap\n    feasible = caps >= item\n    scores = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    slack = caps[feasible] - item\n    fill = 1.0 - slack / _bin_capacity\n    X = np.column_stack((np.ones_like(slack), slack, fill))\n    w = np.random.multivariate_normal(_model_mu, _model_cov)\n    scores[feasible] = X @ w\n    best = int(np.argmax(scores))\n    if feasible[best]:\n        slack_selected = caps[best] - item\n        reward = -slack_selected\n        x = np.array([1.0, slack_selected, 1.0 - slack_selected / _bin_capacity])\n        precision_update = _model_precision + np.outer(x, x) / _noise_var\n        cov_new = np.linalg.inv(precision_update)\n        mu_new = cov_new @ (_model_precision @ _model_mu + x * reward / _noise_var)\n        _model_cov = cov_new\n        _model_precision = precision_update\n        _model_mu = mu_new\n    return scores\n\n[Heuristics 12th]\nimport numpy as np\n\n# Combine tight\u2011fit incentive, slack penalty, random exploration, and softmax scaling.\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Score bins by inverse slack, squared\u2011slack penalty, \u03b5\u2011greedy noise, and softmax.\"\"\"\n    feasible = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    if not np.any(feasible):\n        return priorities\n    slack = bins_remain_cap[feasible] - item\n    inv_slack = 1.0 / (slack + 1.0)\n    beta = 0.1\n    penalty = -beta * (slack ** 2)\n    deterministic = inv_slack + penalty\n    epsilon = 0.05\n    random_noise = epsilon * np.random.rand(deterministic.shape[0])\n    raw = deterministic + random_noise\n    temperature = 0.5\n    exp_vals = np.exp(raw / temperature)\n    total = exp_vals.sum()\n    if total > 0:\n        priorities[feasible] = exp_vals / total\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if not hasattr(priority_v2, \"initialized\"):\n        priority_v2.num_rules = 4\n        priority_v2.sum_waste = np.zeros(priority_v2.num_rules, dtype=float)\n        priority_v2.count = np.zeros(priority_v2.num_rules, dtype=int)\n        priority_v2.step = 0\n        priority_v2.epsilon = 1e-9\n        priority_v2.initialized = True\n    feasible = bins_remain_cap >= item\n    if not feasible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    leftovers = bins_remain_cap - item\n    best_fit_score = np.where(feasible, -leftovers, -np.inf)\n    worst_fit_score = np.where(feasible, leftovers, -np.inf)\n    indices = np.arange(bins_remain_cap.shape[0] if hasattr(bins_remain_cap, \"shape\") else len(bins_remain_cap))\n    first_fit_score = np.where(feasible, -indices, -np.inf)\n    target = bins_remain_cap.mean()\n    fill_bal_score = np.where(feasible, -np.abs(leftovers - target), -np.inf)\n    rule_scores = [best_fit_score, worst_fit_score, first_fit_score, fill_bal_score]\n    waste_per_rule = np.empty(priority_v2.num_rules, dtype=float)\n    for i, scores in enumerate(rule_scores):\n        if not np.isfinite(scores).any():\n            waste_per_rule[i] = np.inf\n        else:\n            idx = np.argmax(scores)\n            waste_per_rule[i] = bins_remain_cap[idx] - item\n    for i in range(priority_v2.num_rules):\n        w = waste_per_rule[i]\n        if np.isfinite(w):\n            priority_v2.sum_waste[i] += w\n            priority_v2.count[i] += 1\n    avg_waste = np.where(priority_v2.count > 0, priority_v2.sum_waste / priority_v2.count, np.inf)\n    inv = 1.0 / (avg_waste + priority_v2.epsilon)\n    boost = np.zeros(priority_v2.num_rules, dtype=float)\n    boost_idx = priority_v2.step % priority_v2.num_rules\n    boost[boost_idx] = 1.0 / (priority_v2.step + 1)\n    raw_weights = inv + boost\n    total = raw_weights.sum()\n    if total > 0:\n        weights = raw_weights / total\n    else:\n        weights = np.full(priority_v2.num_rules, 1.0 / priority_v2.num_rules)\n    combined_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    for w, scores in zip(weights, rule_scores):\n        combined_scores += w * scores\n    combined_scores[~feasible] = -np.inf\n    priority_v2.step += 1\n    return combined_scores\n\n[Heuristics 14th]\nimport numpy as np\n\n# Global state for learning per bin\n_selection_counts = None\n_total_rewards = None\n_total_calls = None\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive scorer: inverse slack, UCB, diversity, reward learning, \u03b5\u2011greedy.\n    \"\"\"\n    global _selection_counts, _total_rewards, _total_calls\n    caps = np.asarray(bins_remain_cap, dtype=float)\n    n = caps.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if _selection_counts is None or _selection_counts.shape[0] != n:\n        _selection_counts = np.zeros(n, dtype=float)\n        _total_rewards = np.zeros(n, dtype=float)\n        _total_calls = 0.0\n    feasible = caps >= item\n    scores = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    leftovers = caps[feasible] - item\n    deterministic = 1.0 / (leftovers + 1.0)\n    epsilon = 0.2\n    random_part = np.random.rand(feasible.sum())\n    base_score = (1 - epsilon) * deterministic + epsilon * random_part\n    avg_reward = np.zeros_like(base_score)\n    mask = _selection_counts[feasible] > 0\n    avg_reward[mask] = _total_rewards[feasible][mask] / _selection_counts[feasible][mask]\n    reward_weight = 0.1\n    total_calls = _total_calls + 1e-6\n    ucb = np.sqrt(2 * np.log(total_calls) / (_selection_counts[feasible] + 1e-6))\n    ucb_weight = 0.05\n    median_leftover = np.median(leftovers)\n    diversity = 1.0 / (np.abs(leftovers - median_leftover) + 1.0)\n    diversity_weight = 0.05\n    combined = base_score + reward_weight * avg_reward + ucb_weight * ucb + diversity_weight * diversity\n    scores[feasible] = combined\n    best = int(np.argmax(scores))\n    if feasible[best]:\n        reward = -(caps[best] - item)\n        _selection_counts[best] += 1\n        _total_rewards[best] += reward\n        _total_calls += 1\n    return scores\n\n[Heuristics 15th]\nimport numpy as np\n\nclass PriorityV2:\n    def __init__(self):\n        self.total_calls = 0\n        self.selection_counts = None\n        self.total_rewards = None\n        self.temperature = 1.0\n        self.decay = 0.995\n\n    def priority(self, item, bins_remain_cap):\n        n = bins_remain_cap.shape[0]\n        if self.selection_counts is None or self.selection_counts.shape[0] != n:\n            self.selection_counts = np.zeros(n, dtype=float)\n            self.total_rewards = np.zeros(n, dtype=float)\n        feasible = bins_remain_cap >= item\n        leftovers = np.where(feasible, bins_remain_cap - item, np.inf)\n        base_score = np.where(feasible, 1.0 / (leftovers + 1.0), 0.0)\n        median_leftover = np.median(leftovers[feasible]) if np.any(feasible) else 0.0\n        diversity = np.where(feasible, 1.0 / (np.abs(leftovers - median_leftover) + 1.0), 0.0)\n        avg_reward = np.where(self.selection_counts > 0, self.total_rewards / self.selection_counts, 0.0)\n        c = 1.0\n        ucb = avg_reward + c * np.sqrt(np.log(self.total_calls + 1) / (self.selection_counts + 1))\n        combined_feas = 0.5 * base_score + 0.3 * diversity + 0.2 * ucb\n        combined = np.full(n, -np.inf, dtype=float)\n        combined[feasible] = combined_feas[feasible]\n        noise = np.random.rand(n)\n        combined[feasible] = (1 - self.temperature) * combined[feasible] + self.temperature * noise[feasible]\n        self.total_calls += 1\n        self.temperature *= self.decay\n        if np.any(feasible):\n            chosen = np.argmax(combined)\n            reward = - (bins_remain_cap[chosen] - item)\n            self.selection_counts[chosen] += 1\n            self.total_rewards[chosen] += reward\n        return combined\n\n_priority_v2_instance = PriorityV2()\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    return _priority_v2_instance.priority(item, bins_remain_cap)\n\n[Heuristics 16th]\nimport numpy as np\n\nclass PriorityV2:\n    def __init__(self):\n        self.total_calls = 0\n        self.selection_counts = None\n        self.total_rewards = None\n        self.temperature = 1.0\n        self.decay = 0.995\n\n    def priority(self, item, bins_remain_cap):\n        n = bins_remain_cap.shape[0]\n        if self.selection_counts is None or self.selection_counts.shape[0] != n:\n            self.selection_counts = np.zeros(n, dtype=float)\n            self.total_rewards = np.zeros(n, dtype=float)\n        feasible = bins_remain_cap >= item\n        leftovers = np.where(feasible, bins_remain_cap - item, np.inf)\n        base_score = np.where(feasible, 1.0 / (leftovers + 1.0), 0.0)\n        median_leftover = np.median(leftovers[feasible]) if np.any(feasible) else 0.0\n        diversity = np.where(feasible, 1.0 / (np.abs(leftovers - median_leftover) + 1.0), 0.0)\n        avg_reward = np.where(self.selection_counts > 0, self.total_rewards / self.selection_counts, 0.0)\n        c = 1.0\n        ucb = avg_reward + c * np.sqrt(np.log(self.total_calls + 1) / (self.selection_counts + 1))\n        combined_feas = 0.5 * base_score + 0.3 * diversity + 0.2 * ucb\n        combined = np.full(n, -np.inf, dtype=float)\n        combined[feasible] = combined_feas[feasible]\n        noise = np.random.rand(n)\n        combined[feasible] = (1 - self.temperature) * combined[feasible] + self.temperature * noise[feasible]\n        self.total_calls += 1\n        self.temperature *= self.decay\n        if np.any(feasible):\n            chosen = np.argmax(combined)\n            reward = - (bins_remain_cap[chosen] - item)\n            self.selection_counts[chosen] += 1\n            self.total_rewards[chosen] += reward\n        return combined\n\n_priority_v2_instance = PriorityV2()\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    return _priority_v2_instance.priority(item, bins_remain_cap)\n\n[Heuristics 17th]\nimport numpy as np\n\n_pv2_counts = None\n_pv2_rewards = None\n_pv2_total_calls = 0\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    global _pv2_counts, _pv2_rewards, _pv2_total_calls\n    bins_remain_cap = np.asarray(bins_remain_cap, dtype=float)\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n    if _pv2_counts is None or _pv2_counts.shape[0] != n:\n        _pv2_counts = np.zeros(n, dtype=float)\n        _pv2_rewards = np.zeros(n, dtype=float)\n    feasible = bins_remain_cap >= item\n    scores = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    leftovers = bins_remain_cap[feasible] - item\n    epsilon = 0.2\n    inv_slack = 1.0 / (leftovers + 1.0)\n    rand_part = np.random.rand(leftovers.shape[0])\n    base_score = (1 - epsilon) * inv_slack + epsilon * rand_part\n    median_leftover = np.median(leftovers) if leftovers.size > 0 else 0.0\n    diversity = 1.0 / (np.abs(leftovers - median_leftover) + 1.0)\n    diversity_weight = 0.1\n    counts_feasible = _pv2_counts[feasible]\n    rewards_feasible = _pv2_rewards[feasible]\n    avg_reward = np.where(counts_feasible > 0, rewards_feasible / counts_feasible, 0.0)\n    c = 0.5\n    exploration_bonus = c * np.sqrt(np.log(_pv2_total_calls + 1) / (counts_feasible + 1))\n    ucb_score = avg_reward + exploration_bonus\n    ucb_weight = 0.3\n    final_score = base_score + ucb_weight * ucb_score + diversity_weight * diversity\n    scores[feasible] = final_score\n    best_idx = int(0) if n == 0 else int(np.argmax(scores))\n    if scores[best_idx] != -np.inf:\n        chosen_leftover = bins_remain_cap[best_idx] - item\n        reward = -chosen_leftover\n        _pv2_counts[best_idx] += 1\n        _pv2_rewards[best_idx] += reward\n        _pv2_total_calls += 1\n    return scores\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray,\n                epsilon: float = 0.2,\n                temperature: float = 1.0,\n                total_threshold: float = 0.0) -> np.ndarray:\n    \"\"\"Inverse slack scored, temperature\u2011scaled softmax, then blended with \u03b5\u2011noise for tie\u2011breaking.\"\"\"\n    feasible = bins_remain_cap >= item\n    scores = np.full(bins_remain_cap.shape, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    leftovers = bins_remain_cap[feasible] - item\n    base = 1.0 / (leftovers + 1.0)\n    exp_input = base / temperature\n    exp_input = np.clip(exp_input, a_min=None, a_max=50.0)\n    exp_scores = np.exp(exp_input)\n    total = exp_scores.sum()\n    if total >= total_threshold:\n        scores[feasible] = exp_scores / total\n    else:\n        scores[feasible] = 0.0\n    rand = np.random.rand(bins_remain_cap.shape[0])\n    rand[~feasible] = 0.0\n    return (1.0 - epsilon) * scores + epsilon * rand\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray,\n                epsilon: float = 0.2,\n                temperature: float = 1.0,\n                total_threshold: float = 0.0) -> np.ndarray:\n    \"\"\"Inverse slack scored, temperature\u2011scaled softmax, then blended with \u03b5\u2011noise for tie\u2011breaking.\"\"\"\n    feasible = bins_remain_cap >= item\n    scores = np.full(bins_remain_cap.shape, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    leftovers = bins_remain_cap[feasible] - item\n    base = 1.0 / (leftovers + 1.0)\n    exp_input = base / temperature\n    exp_input = np.clip(exp_input, a_min=None, a_max=50.0)\n    exp_scores = np.exp(exp_input)\n    total = exp_scores.sum()\n    if total >= total_threshold:\n        scores[feasible] = exp_scores / total\n    else:\n        scores[feasible] = 0.0\n    rand = np.random.rand(bins_remain_cap.shape[0])\n    rand[~feasible] = 0.0\n    return (1.0 - epsilon) * scores + epsilon * rand\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray,\n                epsilon: float = 0.2,\n                temperature: float = 1.0,\n                total_threshold: float = 0.0) -> np.ndarray:\n    \"\"\"Inverse slack scored, temperature\u2011scaled softmax, then blended with \u03b5\u2011noise for tie\u2011breaking.\"\"\"\n    feasible = bins_remain_cap >= item\n    scores = np.full(bins_remain_cap.shape, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n    leftovers = bins_remain_cap[feasible] - item\n    base = 1.0 / (leftovers + 1.0)\n    exp_input = base / temperature\n    exp_input = np.clip(exp_input, a_min=None, a_max=50.0)\n    exp_scores = np.exp(exp_input)\n    total = exp_scores.sum()\n    if total >= total_threshold:\n        scores[feasible] = exp_scores / total\n    else:\n        scores[feasible] = 0.0\n    rand = np.random.rand(bins_remain_cap.shape[0])\n    rand[~feasible] = 0.0\n    return (1.0 - epsilon) * scores + epsilon * rand\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}