```python
import numpy as np

# Global state for learning per bin
_selection_counts = None
_total_rewards = None
_total_calls = None

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Adaptive scorer: inverse slack, UCB, diversity, reward learning, ε‑greedy.
    """
    global _selection_counts, _total_rewards, _total_calls
    caps = np.asarray(bins_remain_cap, dtype=float)
    n = caps.shape[0]
    if n == 0:
        return np.array([], dtype=float)
    if _selection_counts is None or _selection_counts.shape[0] != n:
        _selection_counts = np.zeros(n, dtype=float)
        _total_rewards = np.zeros(n, dtype=float)
        _total_calls = 0.0
    feasible = caps >= item
    scores = np.full(n, -np.inf, dtype=float)
    if not np.any(feasible):
        return scores
    leftovers = caps[feasible] - item
    deterministic = 1.0 / (leftovers + 1.0)
    epsilon = 0.2
    random_part = np.random.rand(feasible.sum())
    base_score = (1 - epsilon) * deterministic + epsilon * random_part
    avg_reward = np.zeros_like(base_score)
    mask = _selection_counts[feasible] > 0
    avg_reward[mask] = _total_rewards[feasible][mask] / _selection_counts[feasible][mask]
    reward_weight = 0.1
    total_calls = _total_calls + 1e-6
    ucb = np.sqrt(2 * np.log(total_calls) / (_selection_counts[feasible] + 1e-6))
    ucb_weight = 0.05
    median_leftover = np.median(leftovers)
    diversity = 1.0 / (np.abs(leftovers - median_leftover) + 1.0)
    diversity_weight = 0.05
    combined = base_score + reward_weight * avg_reward + ucb_weight * ucb + diversity_weight * diversity
    scores[feasible] = combined
    best = int(np.argmax(scores))
    if feasible[best]:
        reward = -(caps[best] - item)
        _selection_counts[best] += 1
        _total_rewards[best] += reward
        _total_calls += 1
    return scores
```
