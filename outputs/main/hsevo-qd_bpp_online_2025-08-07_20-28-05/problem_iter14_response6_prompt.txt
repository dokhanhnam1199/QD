{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n_selection_counts = None\n_total_rewards = None\n_total_calls = 0\n\n    global _selection_counts, _total_rewards, _total_calls\n    bins_remain_cap = np.asarray(bins_remain_cap, dtype=float)\n    n = bins_remain_cap.shape[0]\n    if n == 0:\n        return np.array([], dtype=float)\n    if _selection_counts is None or _selection_counts.shape[0] != n:\n        _selection_counts = np.zeros(n, dtype=float)\n        _total_rewards = np.zeros(n, dtype=float)\n    feasible = bins_remain_cap >= item\n    if not np.any(feasible):\n        return np.full(n, -np.inf, dtype=float)\n    scores = np.full(n, -np.inf, dtype=float)\n    leftovers = bins_remain_cap[feasible] - item\n    deterministic = 1.0 / (leftovers + 1.0)\n    epsilon = 0.2\n    random_part = np.random.rand(feasible.sum())\n    base_score = (1 - epsilon) * deterministic + epsilon * random_part\n    avg_reward_feas = np.zeros_like(base_score)\n    mask_counts_feas = _selection_counts[feasible] > 0\n    avg_reward_feas[mask_counts_feas] = _total_rewards[feasible][mask_counts_feas] / _selection_counts[feasible][mask_counts_feas]\n    reward_weight = 0.3\n    combined_feas = base_score + reward_weight * avg_reward_feas\n    scores[feasible] = combined_feas\n    best = np.argmax(scores)\n    _total_calls += 1\n    leftover = bins_remain_cap[best] - item\n    reward = -leftover\n    _selection_counts[best] += 1\n    _total_rewards[best] += reward\n    return scores\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    if not hasattr(priority_v2, \"initialized\"):\n        priority_v2.num_rules = 4\n        priority_v2.sum_waste = np.zeros(priority_v2.num_rules, dtype=float)\n        priority_v2.count = np.zeros(priority_v2.num_rules, dtype=int)\n        priority_v2.step = 0\n        priority_v2.epsilon = 1e-9\n        priority_v2.initialized = True\n    feasible = bins_remain_cap >= item\n    if not feasible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    leftovers = bins_remain_cap - item\n    best_fit_score = np.where(feasible, -leftovers, -np.inf)\n    worst_fit_score = np.where(feasible, leftovers, -np.inf)\n    indices = np.arange(bins_remain_cap.shape[0] if hasattr(bins_remain_cap, \"shape\") else len(bins_remain_cap))\n    first_fit_score = np.where(feasible, -indices, -np.inf)\n    target = bins_remain_cap.mean()\n    fill_bal_score = np.where(feasible, -np.abs(leftovers - target), -np.inf)\n    rule_scores = [best_fit_score, worst_fit_score, first_fit_score, fill_bal_score]\n    waste_per_rule = np.empty(priority_v2.num_rules, dtype=float)\n    for i, scores in enumerate(rule_scores):\n        if not np.isfinite(scores).any():\n            waste_per_rule[i] = np.inf\n        else:\n            idx = np.argmax(scores)\n            waste_per_rule[i] = bins_remain_cap[idx] - item\n    for i in range(priority_v2.num_rules):\n        w = waste_per_rule[i]\n        if np.isfinite(w):\n            priority_v2.sum_waste[i] += w\n            priority_v2.count[i] += 1\n    avg_waste = np.where(priority_v2.count > 0, priority_v2.sum_waste / priority_v2.count, np.inf)\n    inv = 1.0 / (avg_waste + priority_v2.epsilon)\n    boost = np.zeros(priority_v2.num_rules, dtype=float)\n    boost_idx = priority_v2.step % priority_v2.num_rules\n    boost[boost_idx] = 1.0 / (priority_v2.step + 1)\n    raw_weights = inv + boost\n    total = raw_weights.sum()\n    if total > 0:\n        weights = raw_weights / total\n    else:\n        weights = np.full(priority_v2.num_rules, 1.0 / priority_v2.num_rules)\n    combined_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    for w, scores in zip(weights, rule_scores):\n        combined_scores += w * scores\n    combined_scores[~feasible] = -np.inf\n    priority_v2.step += 1\n    return combined_scores\n\n### Analyze & experience\n- - **(Best) vs (Worst):** Heuristic\u202f1st is a concise \u03b5\u2011greedy inverse\u2011slack scorer with explicit infeasibility handling (\u2011inf). Heuristic\u202f20th wraps a full class using UCB, median\u2011based diversity, temperature decay, and extensive state. While more adaptive, it introduces substantial overhead and complexity, risking stale statistics and harder debugging.  \n- **(Second best) vs (Second worst):** Heuristic\u202f2nd duplicates the 1st implementation\u2014still robust despite lacking a docstring. Heuristic\u202f19th is a stub that returns a zero array for feasible bins, providing no meaningful priority.  \n- **(1st) vs (2nd):** The source code is identical; the ranking likely reflects external factors (e.g., runtime, hidden bug fixes) not evident here.  \n- **(3rd) vs (4th):** Both add global counters, reward tracking, and a 0.3\u2011weighted average\u2011reward term to the base score, enabling lightweight online learning. Their duplication suggests ranking differences stem from subtle initialization or performance nuances.  \n- **(Second worst) vs (Worst):** Heuristic\u202f19th (stub) offers no discrimination between bins, while Heuristic\u202f20th (complex adaptive) at least attempts to learn; the stub is thus worse.  \n- **Overall:** Top heuristics prioritize simplicity, clear \u03b5\u2011greedy inverse\u2011slack scoring, and minimal state. Mid\u2011ranked methods enrich this with linear/squared slack penalties, softmax scaling, or modest reward tracking, achieving a better balance of fit and exploration. The lowest ranks either provide no guidance (stubs) or incur excessive overhead without clear benefit.\n- \n- Keywords: size class grouping, descending order, first\u2011fit, single\u2011pass swap, rule\u2011based.  \n- Advice: Split items into large, medium, small buckets; pack each bucket by first\u2011fit in descending size; then run one pass swapping a small item from an emptier bin with a larger item from a fuller bin if it reduces the max empty space.  \n- Avoid: Random exploration, multi\u2011metric scoring, online learning, extra hyper\u2011parameters, placeholder or duplicated code, and any heavy state tracking.  \n- Explanation: This simple, rule\u2011based approach uses only item size, runs in linear time, and improves bin utilization without complex scores or hidden randomness.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}