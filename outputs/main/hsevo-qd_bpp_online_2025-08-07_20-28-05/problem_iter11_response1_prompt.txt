{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n# Combine inverse\u2011slack and linear\u2011slack with \u03b5\u2011greedy randomness.\n    \"\"\"\n    \u03b5\u2011greedy scoring: weighted sum of inverse slack and negative slack, with random tie\u2011breaker.\n    \"\"\"\n    epsilon = 0.15          # exploration factor\n    w_inv = 0.6             # weight for reciprocal slack\n    w_lin = 0.4             # weight for linear slack penalty\n    feasible = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    slack = bins_remain_cap[feasible] - item\n    base = w_inv * (1.0 / (slack + 1.0)) + w_lin * (-slack)\n    scores[feasible] = base\n    scores = (1 - epsilon) * scores + epsilon * np.random.rand(bins_remain_cap.shape[0])\n    # softmax scaling for probability\u2011like priorities\n    finite_mask = np.isfinite(scores)\n    if not np.any(finite_mask):\n        return scores\n    max_score = np.max(scores[finite_mask])\n    exp_scores = np.exp(scores - max_score)\n    exp_scores[~finite_mask] = 0.0\n    probabilities = exp_scores / exp_scores.sum()\n    return probabilities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\nclass PriorityV2:\n    def __init__(self):\n        self.total_calls = 0\n        self.selection_counts = None\n        self.total_rewards = None\n        self.temperature = 1.0\n        self.decay = 0.995\n\n    def priority(self, item, bins_remain_cap):\n        n = bins_remain_cap.shape[0]\n        if self.selection_counts is None or self.selection_counts.shape[0] != n:\n            self.selection_counts = np.zeros(n, dtype=float)\n            self.total_rewards = np.zeros(n, dtype=float)\n        feasible = bins_remain_cap >= item\n        leftovers = np.where(feasible, bins_remain_cap - item, np.inf)\n        base_score = np.where(feasible, 1.0 / (leftovers + 1.0), 0.0)\n        median_leftover = np.median(leftovers[feasible]) if np.any(feasible) else 0.0\n        diversity = np.where(feasible, 1.0 / (np.abs(leftovers - median_leftover) + 1.0), 0.0)\n        avg_reward = np.where(self.selection_counts > 0, self.total_rewards / self.selection_counts, 0.0)\n        c = 1.0\n        ucb = avg_reward + c * np.sqrt(np.log(self.total_calls + 1) / (self.selection_counts + 1))\n        combined_feas = 0.5 * base_score + 0.3 * diversity + 0.2 * ucb\n        combined = np.full(n, -np.inf, dtype=float)\n        combined[feasible] = combined_feas[feasible]\n        noise = np.random.rand(n)\n        combined[feasible] = (1 - self.temperature) * combined[feasible] + self.temperature * noise[feasible]\n        self.total_calls += 1\n        self.temperature *= self.decay\n        if np.any(feasible):\n            chosen = np.argmax(combined)\n            reward = - (bins_remain_cap[chosen] - item)\n            self.selection_counts[chosen] += 1\n            self.total_rewards[chosen] += reward\n        return combined\n\n_priority_v2_instance = PriorityV2()\n\n    return _priority_v2_instance.priority(item, bins_remain_cap)\n\n### Analyze & experience\n- Comparing (best) vs (worst), we see the best heuristic (#1) uses a lightweight deterministic inverse\u2011slack score with a simple epsilon\u2011greedy reward adjustment and clear state updates, whereas the worst (#20) blends UCB, temperature decay, noise, and multiple slack metrics, adding unnecessary complexity and risk of overfitting;  \n(Second best) vs (second worst), we see #2 (identical to #1) again shows the simplicity advantage, while #19 (UCB\u2011based) mirrors the worst in complexity, reinforcing that added components do not necessarily improve outcomes;  \nComparing (1st) vs (2nd), we see they are functionally identical, exposing duplicate implementation and a maintainability issue;  \n(3rd) vs (4th), the two are identical as well, again highlighting redundant code;  \nComparing (second worst) vs (worst), we see no difference\u2014they implement the same UCB/temperature logic\u2014illustrating lack of diversity in the lower\u2011ranked heuristics;  \nOverall: The top heuristics are simple, score\u2011direct, and learn from empirical reward; the bottom heuristics over\u2011engineer with multi\u2011metric blending and hidden selection logic, leading to reduced clarity and potential performance loss.\n- \n- **Keywords:** simplicity, modularity, deterministic, explainability, balanced scoring.  \n- **Advice:** Build a single, stateless scoring function that linearly combines normalized slack and empirical reward, separate feature extraction, and enforce explicit infeasibility handling.  \n- **Avoid:** Random \u03b5\u2011greedy tweaks, softmax probability layers, hidden state, duplicated code, and excess hyper\u2011parameters.  \n- **Explanation:** A deterministic, modular pipeline yields transparent heuristics that are easy to maintain, debug, and compose while still capturing key trade\u2011offs.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}