```python
import numpy as np

# Mix per‑bin reward learning, adaptive rule weighting, and UCB exploration.
_selection_counts = None
_total_rewards = None
_total_calls = 0
_rule_waste_sum = None
_rule_waste_count = None
_epsilon = 1e-9

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Blend adaptive rule scores, per‑bin avg reward, and UCB for online BPP."""
    global _selection_counts, _total_rewards, _total_calls
    global _rule_waste_sum, _rule_waste_count

    caps = np.asarray(bins_remain_cap, dtype=float)
    n = caps.shape[0]
    if n == 0:
        return np.array([], dtype=float)

    if _selection_counts is None or _selection_counts.shape[0] != n:
        _selection_counts = np.zeros(n, dtype=float)
        _total_rewards = np.zeros(n, dtype=float)
        _rule_waste_sum = np.zeros(4, dtype=float)
        _rule_waste_count = np.zeros(4, dtype=int)

    feasible = caps >= item
    if not np.any(feasible):
        return np.full(n, -np.inf, dtype=float)

    leftovers = caps - item
    indices = np.arange(n)
    target = caps.mean()

    # Rule scores
    best_fit = -leftovers
    worst_fit = leftovers
    first_fit = -indices
    fill_bal = -np.abs(leftovers - target)
    rule_scores = np.stack([best_fit, worst_fit, first_fit, fill_bal], axis=0)

    # Update waste statistics for each rule
    for i in range(4):
        scores_i = rule_scores[i].copy()
        scores_i[~feasible] = -np.inf
        best_idx = int(np.argmax(scores_i))
        if feasible[best_idx]:
            waste = leftovers[best_idx]
            _rule_waste_sum[i] += waste
            _rule_waste_count[i] += 1

    # Adaptive rule weights (inverse average waste)
    avg_waste = np.where(_rule_waste_count > 0,
                         _rule_waste_sum / _rule_waste_count,
                         np.inf)
    inv_waste = 1.0 / (avg_waste + _epsilon)
    finite_mask = np.isfinite(inv_waste)
    if np.any(finite_mask):
        rule_weights = np.zeros_like(inv_waste)
        rule_weights[finite_mask] = inv_waste[finite_mask] / np.sum(inv_waste[finite_mask])
    else:
        rule_weights = np.full_like(inv_waste, 1.0 / inv_waste.size)

    # Combine rule scores
    combined = np.zeros(n, dtype=float)
    for i, w in enumerate(rule_weights):
        combined += w * rule_scores[i]

    # Per‑bin average reward (negative waste)
    avg_reward = np.zeros(n, dtype=float)
    mask = _selection_counts > 0
    avg_reward[mask] = _total_rewards[mask] / _selection_counts[mask]

    # UCB exploration term
    ucb = np.sqrt(np.log(_total_calls + 1) / np.maximum(_selection_counts, 1.0))

    # Final score mixture
    w_rule = 0.7
    w_reward = 0.15
    w_ucb = 0.15
    scores = combined * w_rule + avg_reward * w_reward + ucb * w_ucb
    scores[~feasible] = -np.inf

    # Update per‑bin stats after placement
    best = int(np.argmax(scores))
    reward = -(caps[best] - item)  # negative waste
    _selection_counts[best] += 1
    _total_rewards[best] += reward
    _total_calls += 1

    return scores
```
