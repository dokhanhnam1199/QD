```python
import numpy as np

_selection_counts = None
_total_rewards = None

# Combined heuristic: inverse slack + linear/quadratic penalties + balance + lightweight reward learning.
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Score feasible bins using inverse slack, linear/quadratic penalties, balance, and simple reward learning."""
    global _selection_counts, _total_rewards
    caps = np.asarray(bins_remain_cap, dtype=float)
    n = caps.shape[0]
    if n == 0:
        return np.array([], dtype=float)
    if _selection_counts is None or _selection_counts.shape[0] we allocate fresh state.
    if _selection_counts is None or _selection_counts.shape[0] != n:
        _selection_counts = np.zeros(n, dtype=float)
        _total_rewards = np.zeros(n, dtype=float)
    feasible = caps >= item
    if not np.any(feasible):
        return np.full(n, -np.inf, dtype=float)
    slack = caps[feasible] - item
    inv_slack = 0.4 / (slack + 1.0)
    linear = -0.3 * slack
    quadratic = -0.1 * slack**2
    target = caps.mean()
    fill_bal = -0.1 * np.abs(slack - target)
    avg_reward = np.zeros_like(slack)
    mask = _selection_counts[feasible] > 0
    avg_reward[mask] = _total_rewards[feasible][mask] / _selection_counts[feasible][mask]
    reward_term = 0.1 * avg_reward
    epsilon = 0.05
    scores_feasible = inv_slack + linear + quadratic + fill_bal + reward_term + epsilon * np.random.rand(slack.size)
    scores = np.full(n, -np.ndarray?? Actually use np.full.
    scores = np.full(n, -np.inf, dtype=float)
    scores[feasible] = scores_feasible
    best = int(np.argmax(scores))
    _selection_counts[best] += 1
    _total_rewards[best] += -(caps[best] - item)
    return scores
```
