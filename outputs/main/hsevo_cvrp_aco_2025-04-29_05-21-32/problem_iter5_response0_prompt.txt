{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Capacitated Vehicle Routing Problem (CVRP) via stochastic solution sampling. CVRP requires finding the shortest path that visits all given nodes and returns to the starting node. Each node has a demand and each vehicle has a capacity. The total demand of the nodes visited by a vehicle cannot exceed the vehicle capacity. When the total demand exceeds the vehicle capacity, the vehicle must return to the starting node.\nThe `heuristics` function takes as input a distance matrix (shape: n by n), Euclidean coordinates of nodes (shape: n by 2), a vector of customer demands (shape: n), and the integer capacity of vehicle capacity. It returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the distance_matrix. The depot node is indexed by 0.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray, coordinates: np.ndarray, demands: np.ndarray, capacity: int) -> np.ndarray:\n\n    \"\"\"Combines distance, savings, demand, and coordinate proximity for CVRP.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix)\n\n    # Distance and Savings (Clarke-Wright)\n    savings_matrix = np.zeros_like(distance_matrix)\n    for i in range(1, n):\n        for j in range(i + 1, n):\n            savings_matrix[i, j] = distance_matrix[0, i] + distance_matrix[0, j] - distance_matrix[i, j]\n            savings_matrix[j, i] = savings_matrix[i, j]\n\n    # Normalize savings and combine with inverse distance\n    heuristic_matrix = (1 / (distance_matrix + np.eye(n))) + (savings_matrix / np.max(savings_matrix))\n\n    # Demand penalty:  Penalize edges connecting nodes with high demand.\n    demand_penalty = np.zeros(n)\n    for i in range(1, n):\n        demand_penalty[i] = demands[i] / capacity\n\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                heuristic_matrix[i, j] = 0  # No loops\n            else:\n                heuristic_matrix[i, j] /= (1 + demand_penalty[i] + demand_penalty[j]) # Slightly penalize high demand nodes.\n\n    # Coordinate proximity: Favor edges connecting nearby nodes in space.\n    coordinate_distances = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            coordinate_distances[i, j] = np.linalg.norm(coordinates[i] - coordinates[j])\n\n    heuristic_matrix += (1 / (coordinate_distances + np.eye(n)))  # Prefer shorter coordinate distance\n    heuristic_matrix[np.isinf(heuristic_matrix)] = 0  # Handle infinities\n    heuristic_matrix[np.isnan(heuristic_matrix)] = 0  # Handle NaN values\n\n    return heuristic_matrix\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray, coordinates: np.ndarray, demands: np.ndarray, capacity: int) -> np.ndarray:\n\n    \"\"\"\n    Heuristic combining distance, demand, and angle for CVRP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros((n, n))\n\n    distance_importance = 1.0\n    demand_importance = 0.5\n    angle_importance = 0.2\n\n    depot_index = 0\n\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                heuristic_matrix[i, j] = 0\n                continue\n\n            distance_heuristic = 1 / distance_matrix[i, j] if distance_matrix[i, j] > 0 else 0\n\n            demand_heuristic = 1 - (demands[j] / capacity) if demands[j] < capacity else 0\n\n            vector_ij = coordinates[j] - coordinates[i]\n            vector_dj = coordinates[j] - coordinates[depot_index]\n\n            norm_ij = np.linalg.norm(vector_ij)\n            norm_dj = np.linalg.norm(vector_dj)\n\n            if norm_ij > 0 and norm_dj > 0:\n                cos_angle = np.dot(vector_ij, vector_dj) / (norm_ij * norm_dj)\n                cos_angle = np.clip(cos_angle, -1.0, 1.0)\n                angle = np.arccos(cos_angle)\n            else:\n                angle = np.pi / 2\n\n            angle_heuristic = 1 - (angle / np.pi)\n\n            heuristic_matrix[i, j] = (\n                distance_importance * distance_heuristic +\n                demand_importance * demand_heuristic +\n                angle_importance * angle_heuristic\n            )\n    return heuristic_matrix\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic utilizes a combination of inverse distance, demand considerations (with depot-specific adjustments), depot proximity, k-nearest neighbors, and sparsification, followed by normalization. The 20th heuristic uses a similar approach but lacks specific scaling factors for depot departures based on remaining capacity and uses hardcoded boosts.\n(2nd best) vs (second worst): (2nd) uses a weighted combination of distance, demand, and depot proximity. (19th) uses inverse distance, demand factor adjusted for depot location, depot proximity factor, k-nearest neighbors, and sparsification. The primary difference lies in the explicit inclusion of k-NN and sparsification in (1st) which seems to be essential for better performance.\nComparing (1st) vs (2nd), we see (1st) employs a more complex and refined approach by integrating k-NN bonus, a more aggressive sparsification based on a dynamic threshold, and specific boost from the depot alongside normalization, whereas (2nd) relies on a simpler, weighted combination of distance, demand, and depot proximity. The sophisticated tuning and edge pruning in (1st) likely contribute to its superior performance.\n(3rd) vs (4th): Both implementations are nearly identical, meaning there's little to differentiate between them in terms of design.\nComparing (second worst) vs (worst), we see a similar approach that considers inverse distance, demand and depot proximity, but weights them differently and does not include k-NN or sparsification. The normalization step remains consistent. Overall, the lack of k-NN and sparsification leads to decreased performance.\n\nOverall: The best heuristics in the list combine several factors: inverse distance as a base, demand considerations with capacity constraints, depot proximity encouragements, k-nearest neighbor bonuses, and sparsification to prune unpromising edges. The key to better performance involves fine-tuning the weights and thresholds of these components and incorporating more problem-specific information to make better routing decisions. The weaker heuristics often lack the k-NN and sparsification components and have less refined scaling for different factors. Normalization appears to be a standard practice.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, let's refine \"Current Self-Reflection\" to design better heuristics, avoiding common pitfalls.\n\nHere's a more effective redefinition:\n\n*   **Keywords:** Incremental refinement, component-wise addition, adaptive intensification, simplification, performance evaluation.\n*   **Advice:** Build heuristics iteratively, adding components based on performance impact. Focus on *why* a component improves solutions. Use limited components.\n*   **Avoid:** Premature complexity, over-tuning, black-box approaches, relying on overly specific or intricate formulas. Do not blindly use weights.\n*   **Explanation:** Start with a minimal viable heuristic. Systematically add and evaluate new components, understanding their individual contributions. Keep the design interpretable.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}