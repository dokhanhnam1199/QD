```python
import numpy as np

def heuristics_v2(distance_matrix: np.ndarray, coordinates: np.ndarray, demands: np.ndarray, capacity: int) -> np.ndarray:
    """
    Heuristics for CVRP edge selection, considering distance, demand, and vehicle capacity.

    Args:
        distance_matrix (np.ndarray): Distance matrix between nodes (n x n).
        coordinates (np.ndarray): Euclidean coordinates of nodes (n x 2).
        demands (np.ndarray): Demand of each node (n).
        capacity (int): Vehicle capacity.

    Returns:
        np.ndarray: Prior indicators of how promising each edge is (n x n).
    """
    n = distance_matrix.shape[0]
    heuristics = np.zeros_like(distance_matrix)

    # Start with inverse distance
    heuristics = 1 / (distance_matrix + 1e-9)  # Adding a small constant to avoid division by zero

    # Modify based on demand and proximity to depot
    for i in range(n):
        for j in range(n):
            if i == j:
                heuristics[i, j] = 0  # No self-loops

            # Penalize edges that might overload vehicles
            avg_demand = (demands[i] + demands[j]) / 2
            if avg_demand > capacity / 3:  # Adjusted threshold
                heuristics[i, j] *= 0.5 # Moderate Penalization

            # Reward edges closer to the depot to encourage route starts near the depot
            depot_distance_penalty = 0.0
            depot_distance_penalty = (distance_matrix[0, i] + distance_matrix[0, j]) / (2 * np.mean(distance_matrix))

            heuristics[i, j] /= (1 + depot_distance_penalty)  # The closer to depot, the larger the divisor

            # Encourage edges leading away from the depot initially.
            # After the first vehicle visit some customers, it may return to the depot again.
            # Thus we focus more on connecting customers instead of returning depot.
            # initial_reward = np.exp(-distance_matrix[0,i] * distance_matrix[0,j]/(np.mean(distance_matrix)**2))
            # heuristics[i,j] *= (1 + initial_reward)

    # Normalize heuristics (optional, but can improve performance)
    heuristics = heuristics / np.max(heuristics)

    return heuristics
```
