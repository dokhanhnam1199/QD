{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Capacitated Vehicle Routing Problem (CVRP) via stochastic solution sampling. CVRP requires finding the shortest path that visits all given nodes and returns to the starting node. Each node has a demand and each vehicle has a capacity. The total demand of the nodes visited by a vehicle cannot exceed the vehicle capacity. When the total demand exceeds the vehicle capacity, the vehicle must return to the starting node.\nThe `heuristics` function takes as input a distance matrix (shape: n by n), Euclidean coordinates of nodes (shape: n by 2), a vector of customer demands (shape: n), and the integer capacity of vehicle capacity. It returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the distance_matrix. The depot node is indexed by 0.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray, coordinates: np.ndarray, demands: np.ndarray, capacity: int) -> np.ndarray:\n\n    \"\"\"\n    Heuristics function for solving Capacitated Vehicle Routing Problem (CVRP) via stochastic solution sampling.\n    This version incorporates several improvements over v1, including a more refined demand consideration,\n    depot proximity enhancements, and sparsification of unpromising edges.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix between nodes (shape: n x n).\n        coordinates (np.ndarray): Euclidean coordinates of nodes (shape: n x 2).\n        demands (np.ndarray): Vector of customer demands (shape: n).\n        capacity (int): Vehicle capacity.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution (shape: n x n).\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    depot_index = 0\n\n    # Heuristic 1: Inverse distance (primary factor)\n    heuristics = 1 / (distance_matrix + 1e-9)\n\n    # Heuristic 2: Demand consideration, penalizing edges that risk exceeding capacity\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                heuristics[i, j] = 0\n                continue\n\n            # Depot departures boost: only apply to edges *from* the depot. Scale inversely with demand.\n            if i == depot_index and j != depot_index:\n                remaining_capacity = capacity - demands[j]\n                if remaining_capacity > 0:\n                    heuristics[i, j] += 0.75 * (remaining_capacity / capacity)  # Scale boost by remaining capacity\n\n            # Penalize high-demand edges to discourage routes that fill up too quickly. Don't penalize edges *to* the depot.\n            if j != depot_index:\n                demand_penalty = demands[j] / capacity # Higher demand leads to higher penalty\n                heuristics[i, j] *= max(0, 1 - demand_penalty) #scale values to avoid negatives\n                if demands[j] > capacity:\n                    heuristics[i, j] = 0 #impossible node visits\n\n    # Heuristic 3: Depot proximity. Encourages connections to nodes close to the depot.\n    # Adjust strength based on distance *from* the depot. Only do this for edges *from* a node.\n    for i in range(n):\n      if i != depot_index:\n        depot_distance = distance_matrix[i, depot_index]\n        proximity_bonus = max(0, 1 - (depot_distance / np.max(distance_matrix))) # Closer = larger bonus\n        heuristics[i, :] += 0.25 * proximity_bonus  # Apply bonus to *all* outbound edges from node i\n\n    # Heuristic 4: Sparsification - remove very unpromising edges to focus search\n    # Identify and zero out edges below a threshold based on mean heuristic value.\n\n    mean_heuristic = np.mean(heuristics)\n    sparsification_threshold = 0.25 * mean_heuristic  # Adjust this factor as needed\n\n    heuristics[heuristics < sparsification_threshold] = 0\n\n    # Heuristic 5: k-Nearest Neighbors encouragement (Similar to v1, but with fine-tuning)\n    k_nearest_neighbors = 4\n    for i in range(n):\n        neighbors_idx = np.argsort(distance_matrix[i, :])[1:k_nearest_neighbors + 1]  # Exclude self\n\n        for nn in neighbors_idx:\n            heuristics[i, nn] += 0.15  # Slight preference for neighbors\n\n\n    # Normalize\n    max_heuristic = np.max(heuristics)\n    if max_heuristic > 0:\n        heuristics = heuristics / max_heuristic\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray, coordinates: np.ndarray, demands: np.ndarray, capacity: int) -> np.ndarray:\n\n    \"\"\"\n    Combines inverse distance with demand and depot proximity for CVRP heuristic.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # Base: Inverse distance\n    heuristics = 1 / (distance_matrix + 1e-9)\n\n    # Demand penalization\n    max_demand = np.max(demands)\n    normalized_demands = demands / max_demand if max_demand > 0 else np.zeros_like(demands)\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                heuristics[i, j] = 0\n                continue\n            if i!= 0 and j != 0:\n                demand_penalty = normalized_demands[i] + normalized_demands[j]\n                heuristics[i, j] /= (1 + demand_penalty)\n\n\n    #Depot proximity reward/penalty\n    for i in range(n):\n        for j in range(n):\n\n            depot_distance_penalty = 0.0\n            depot_distance_penalty = (distance_matrix[0, i] + distance_matrix[0, j]) / (2 * np.mean(distance_matrix))\n\n            heuristics[i, j] /= (1 + depot_distance_penalty)\n    # Normalize\n    heuristics = heuristics / np.max(heuristics)\n\n    return heuristics\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic utilizes a combination of inverse distance, demand considerations (with depot-specific adjustments), depot proximity, k-nearest neighbors, and sparsification, followed by normalization. The 20th heuristic uses a similar approach but lacks specific scaling factors for depot departures based on remaining capacity and uses hardcoded boosts.\n(2nd best) vs (second worst): (2nd) uses a weighted combination of distance, demand, and depot proximity. (19th) uses inverse distance, demand factor adjusted for depot location, depot proximity factor, k-nearest neighbors, and sparsification. The primary difference lies in the explicit inclusion of k-NN and sparsification in (1st) which seems to be essential for better performance.\nComparing (1st) vs (2nd), we see (1st) employs a more complex and refined approach by integrating k-NN bonus, a more aggressive sparsification based on a dynamic threshold, and specific boost from the depot alongside normalization, whereas (2nd) relies on a simpler, weighted combination of distance, demand, and depot proximity. The sophisticated tuning and edge pruning in (1st) likely contribute to its superior performance.\n(3rd) vs (4th): Both implementations are nearly identical, meaning there's little to differentiate between them in terms of design.\nComparing (second worst) vs (worst), we see a similar approach that considers inverse distance, demand and depot proximity, but weights them differently and does not include k-NN or sparsification. The normalization step remains consistent. Overall, the lack of k-NN and sparsification leads to decreased performance.\n\nOverall: The best heuristics in the list combine several factors: inverse distance as a base, demand considerations with capacity constraints, depot proximity encouragements, k-nearest neighbor bonuses, and sparsification to prune unpromising edges. The key to better performance involves fine-tuning the weights and thresholds of these components and incorporating more problem-specific information to make better routing decisions. The weaker heuristics often lack the k-NN and sparsification components and have less refined scaling for different factors. Normalization appears to be a standard practice.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, let's refine \"Current Self-Reflection\" to design better heuristics, avoiding common pitfalls.\n\nHere's a more effective redefinition:\n\n*   **Keywords:** Incremental refinement, component-wise addition, adaptive intensification, simplification, performance evaluation.\n*   **Advice:** Build heuristics iteratively, adding components based on performance impact. Focus on *why* a component improves solutions. Use limited components.\n*   **Avoid:** Premature complexity, over-tuning, black-box approaches, relying on overly specific or intricate formulas. Do not blindly use weights.\n*   **Explanation:** Start with a minimal viable heuristic. Systematically add and evaluate new components, understanding their individual contributions. Keep the design interpretable.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}