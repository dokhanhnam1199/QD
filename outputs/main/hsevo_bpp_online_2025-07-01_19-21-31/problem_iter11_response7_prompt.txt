{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Adaptive weights: remaining capacity distribution & item size.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_after_add = bins_remain_cap - item\n    epsilon = 1e-9\n\n    # Infeasible bins get strongly negative priority\n    infeasible_mask = remaining_after_add < 0\n    priorities[infeasible_mask] = -np.inf\n    \n    # Waste priority, use reciprocal of waste\n    waste = np.abs(remaining_after_add)\n    waste_priority = 1.0 / (waste + epsilon)\n\n    # Near-full bonus, give bonus to near full bins\n    near_full_bonus = np.where((bins_remain_cap > item) & (bins_remain_cap <= 2 * item), item / bins_remain_cap * 10, 0)\n    \n    # Capacity based priority, favor larger remaining capacity\n    capacity_priority = np.sqrt(bins_remain_cap) / (item + epsilon)\n\n    # Adaptive weighting based on remaining capacity distribution\n    capacity_mean = np.mean(bins_remain_cap[bins_remain_cap >= item]) if np.any(bins_remain_cap >= item) else np.mean(bins_remain_cap) #Avoid empty array\n    capacity_std = np.std(bins_remain_cap[bins_remain_cap >= item]) if np.any(bins_remain_cap >= item) and len(bins_remain_cap[bins_remain_cap >= item]) > 1 else 0.1 # Avoid empty array and zero std\n    \n    #weights - Item-size adaptive\n    if item < 0.2:  # Small items, prioritize waste minimization and capacity\n        waste_weight = 0.6\n        near_full_weight = 0.1\n        capacity_weight = 0.3\n    elif item > 0.8:  # Large items, prioritize near-full and avoid fragmentation\n        waste_weight = 0.1\n        near_full_weight = 0.6\n        capacity_weight = 0.3\n    else:  # Medium items, balance all factors\n        waste_weight = 0.3\n        near_full_weight = 0.4\n        capacity_weight = 0.3\n\n    #capacity_normalized = (bins_remain_cap - capacity_mean) / capacity_std\n    #capacity_weight2 = 1.0 + np.tanh(capacity_normalized) if capacity_std > 0 else 1.0\n\n    # Combine all priorities\n    valid_mask = remaining_after_add >= 0\n    priorities[valid_mask] = waste_weight * waste_priority[valid_mask] + near_full_weight * near_full_bonus[valid_mask] + capacity_weight * capacity_priority[valid_mask]\n\n    # Introduce a penalty for bins that become too empty after adding the item\n    empty_penalty = np.where(remaining_after_add > 0.7, -0.5 * (remaining_after_add - 0.7), 0)  #if remaining cap > 0.7, penalize\n    priorities = np.where(remaining_after_add >= 0, priorities + empty_penalty, priorities)\n\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines adaptive weighting based on capacity and waste minimization.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_after_add = bins_remain_cap - item\n    epsilon = 1e-9\n\n    # Infeasible bins get strongly negative priority\n    infeasible_mask = remaining_after_add < 0\n    priorities[infeasible_mask] = -np.inf\n\n    # Waste priority, use reciprocal of waste\n    waste = np.abs(remaining_after_add)\n    waste_priority = 1.0 / (waste + epsilon)\n\n    # Near-full penalty, give penalty to near full bins\n    near_full_penalty = np.where((bins_remain_cap > item) & (bins_remain_cap <= 2 * item), -item / bins_remain_cap * 10, 0)\n    \n    # Capacity based priority, favor larger remaining capacity\n    capacity_priority = np.sqrt(bins_remain_cap) / (item + epsilon)\n\n    # Adaptive weighting based on total capacity\n    total_capacity = np.sum(bins_remain_cap)\n    waste_weight = 0.4\n    near_full_weight = 0.3\n    capacity_weight = 0.3\n    \n    if total_capacity < 5 * item:\n        waste_weight = 0.2\n        near_full_weight = 0.5\n        capacity_weight = 0.3\n    elif total_capacity > 20 * item:\n        waste_weight = 0.5\n        near_full_weight = 0.2\n        capacity_weight = 0.3\n\n    # Combine all priorities with adaptive weights\n    priorities = np.where(~infeasible_mask, waste_weight * waste_priority + near_full_weight * near_full_penalty + capacity_weight * capacity_priority, priorities)\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic incorporates waste minimization, near-full bonuses, adaptive weighting based on capacity distribution, and a fill percentage score, whereas the worst only considers waste, near-full penalty, and capacity, with adaptive weighting based on total capacity. (2nd best) vs (second worst) reveals similar attributes. Comparing (1st) vs (2nd), no code differences between the two. (3rd) vs (4th), the 4th introduces a larger infeasibility penalty, adjusts the near-full bonus with power, reduces capacity weight, sharpens the fill percentage peak, and adds a penalty for bins with very little remaining capacity. (second worst) vs (worst), little change in functionality. Overall: The better performing heuristics combine multiple factors (waste, near-fullness, capacity distribution, fill percentage) with carefully tuned weights and penalties, while the worse performing ones use fewer factors and simpler weighting schemes. The best heuristics also show refinement in handling infeasibility and edge cases.\n- \nOkay, let's refine \"Current Self-Reflection\" for designing better heuristics. Aim for actionable insights that avoid the pitfalls of ineffective self-reflection.\n\nHere's a breakdown:\n\n*   **Keywords:** Adaptive weights, factor combination, infeasibility penalties, experimental calibration, numerical stability, problem context.\n\n*   **Advice:** Design heuristics that intelligently combine relevant factors using adaptive weighting schemes informed by data characteristics and problem context. Prioritize penalties for infeasibility.\n\n*   **Avoid:** Simple implementations that overlook problem nuances, relying solely on penalties, ignoring potential rewards, skipping data-driven parameter tuning.\n\n*   **Explanation:** Move beyond basic implementations. Adaptive weighting, calibrated by experimentation and problem context, allows for better heuristic performance. Consider rewards, not *just* penalties.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}