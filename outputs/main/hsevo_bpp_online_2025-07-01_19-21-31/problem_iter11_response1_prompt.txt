{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines waste minimization, near-full bonuses, adaptive weighting,\n    and fill percentage, with robust infeasibility handling and calibrated weights.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_after_add = bins_remain_cap - item\n    epsilon = 1e-9\n\n    # Strong infeasibility penalty\n    infeasible_mask = remaining_after_add < 0\n    priorities[infeasible_mask] = -1e9  # Significantly larger penalty\n\n    valid_mask = remaining_after_add >= 0\n\n    if np.any(valid_mask):\n        # Waste priority (reciprocal, stabilized)\n        waste = remaining_after_add[valid_mask]\n        waste_priority = 1.0 / (waste + 0.1 * item + epsilon)  # Stabilized by item size\n\n        # Near-full bonus (only when item fits reasonably well)\n        near_full_bonus = np.where(\n            (bins_remain_cap[valid_mask] > item) & (bins_remain_cap[valid_mask] <= 2 * item),\n            (item / bins_remain_cap[valid_mask])**2 * 5,  # Reduced magnitude, polynomial\n            0\n        )\n\n        # Fill percentage score\n        fill_percentage = item / bins_remain_cap[valid_mask]\n        fill_score = np.exp(-10 * np.abs(fill_percentage - 0.8)) #Stronger penalty\n\n        # Adaptive weighting based on remaining capacity distribution (dampened)\n        capacity_mean = np.mean(bins_remain_cap[valid_mask]) if np.any(valid_mask) else np.mean(bins_remain_cap)\n        capacity_std = np.std(bins_remain_cap[valid_mask]) if np.any(valid_mask) and len(bins_remain_cap[valid_mask]) > 1 else 0.1\n        capacity_normalized = (bins_remain_cap[valid_mask] - capacity_mean) / (capacity_std + epsilon)\n        capacity_weight = 0.5 + 0.5 * np.tanh(capacity_normalized)  # Scaled tanh for stability\n\n\n        # Calibrated weights (data-driven tuning is essential)\n        waste_weight = 1.0\n        near_full_weight = 0.75 # Reduced near-full weight\n        fill_weight = 0.6 # Reduced fill weight\n        capacity_weight_final = 0.25\n\n\n        priorities[valid_mask] = (\n            waste_weight * waste_priority\n            + near_full_weight * near_full_bonus\n            + fill_weight * fill_score\n            + capacity_weight_final*capacity_weight\n        )\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines adaptive weighting based on capacity and waste minimization.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_after_add = bins_remain_cap - item\n    epsilon = 1e-9\n\n    # Infeasible bins get strongly negative priority\n    infeasible_mask = remaining_after_add < 0\n    priorities[infeasible_mask] = -np.inf\n\n    # Waste priority, use reciprocal of waste\n    waste = np.abs(remaining_after_add)\n    waste_priority = 1.0 / (waste + epsilon)\n\n    # Near-full penalty, give penalty to near full bins\n    near_full_penalty = np.where((bins_remain_cap > item) & (bins_remain_cap <= 2 * item), -item / bins_remain_cap * 10, 0)\n    \n    # Capacity based priority, favor larger remaining capacity\n    capacity_priority = np.sqrt(bins_remain_cap) / (item + epsilon)\n\n    # Adaptive weighting based on total capacity\n    total_capacity = np.sum(bins_remain_cap)\n    waste_weight = 0.4\n    near_full_weight = 0.3\n    capacity_weight = 0.3\n    \n    if total_capacity < 5 * item:\n        waste_weight = 0.2\n        near_full_weight = 0.5\n        capacity_weight = 0.3\n    elif total_capacity > 20 * item:\n        waste_weight = 0.5\n        near_full_weight = 0.2\n        capacity_weight = 0.3\n\n    # Combine all priorities with adaptive weights\n    priorities = np.where(~infeasible_mask, waste_weight * waste_priority + near_full_weight * near_full_penalty + capacity_weight * capacity_priority, priorities)\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic incorporates waste minimization, near-full bonuses, adaptive weighting based on capacity distribution, and a fill percentage score, whereas the worst only considers waste, near-full penalty, and capacity, with adaptive weighting based on total capacity. (2nd best) vs (second worst) reveals similar attributes. Comparing (1st) vs (2nd), no code differences between the two. (3rd) vs (4th), the 4th introduces a larger infeasibility penalty, adjusts the near-full bonus with power, reduces capacity weight, sharpens the fill percentage peak, and adds a penalty for bins with very little remaining capacity. (second worst) vs (worst), little change in functionality. Overall: The better performing heuristics combine multiple factors (waste, near-fullness, capacity distribution, fill percentage) with carefully tuned weights and penalties, while the worse performing ones use fewer factors and simpler weighting schemes. The best heuristics also show refinement in handling infeasibility and edge cases.\n- \nOkay, let's refine \"Current Self-Reflection\" for designing better heuristics. Aim for actionable insights that avoid the pitfalls of ineffective self-reflection.\n\nHere's a breakdown:\n\n*   **Keywords:** Adaptive weights, factor combination, infeasibility penalties, experimental calibration, numerical stability, problem context.\n\n*   **Advice:** Design heuristics that intelligently combine relevant factors using adaptive weighting schemes informed by data characteristics and problem context. Prioritize penalties for infeasibility.\n\n*   **Avoid:** Simple implementations that overlook problem nuances, relying solely on penalties, ignoring potential rewards, skipping data-driven parameter tuning.\n\n*   **Explanation:** Move beyond basic implementations. Adaptive weighting, calibrated by experimentation and problem context, allows for better heuristic performance. Consider rewards, not *just* penalties.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}