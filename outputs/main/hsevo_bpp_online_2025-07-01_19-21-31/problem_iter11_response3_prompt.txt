{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Adaptive weights based on capacity distribution.\n    Combines waste, fill percentage, and near-full penalties.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap >= item\n    epsilon = 1e-9\n\n    if np.any(valid_bins):\n        waste = bins_remain_cap[valid_bins] - item\n        priorities[valid_bins] = 1 / (waste + epsilon)\n\n        new_capacities = bins_remain_cap[valid_bins] - item\n        near_full_penalty = np.zeros_like(new_capacities)\n\n        near_full_mask = new_capacities < 0.1\n        near_full_penalty[near_full_mask] = -100\n\n        near_full_mask2 = (new_capacities >= 0.1) & (new_capacities < 0.3)\n        near_full_penalty[near_full_mask2] = -2 * (0.3 - new_capacities[near_full_mask2])**2\n        \n        priorities[valid_bins] += near_full_penalty\n        \n        fill_percentage = item / bins_remain_cap[valid_bins]\n        fill_score = np.exp(-np.abs(fill_percentage - 0.8))\n        \n        # Adaptive Weighting\n        total_capacity = np.sum(bins_remain_cap)\n        if total_capacity < 5 * item:\n            fill_weight = 0.3\n            waste_weight = 0.7\n        elif total_capacity > 20 * item:\n            fill_weight = 0.7\n            waste_weight = 0.3\n        else:\n            fill_weight = 0.5\n            waste_weight = 0.5\n\n        priorities[valid_bins] = waste_weight * (1 / (waste + epsilon)) + fill_weight * fill_score #balance between waste and fill\n\n    else:\n        priorities = np.full_like(bins_remain_cap, -1000.0)\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines waste minimization, near-full bonuses, adaptive weighting based on bin utilization,\n    and penalty for bins that will be overfilled. Aims for a more balanced and robust approach.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_after_add = bins_remain_cap - item\n    epsilon = 1e-9\n\n    # Strong penalty for infeasible bins\n    infeasible_mask = remaining_after_add < 0\n    priorities[infeasible_mask] = -1e9  # Significantly stronger penalty than before\n\n    valid_mask = remaining_after_add >= 0\n\n    if np.any(valid_mask):\n        # Waste priority (reciprocal of waste, as before)\n        waste = np.abs(remaining_after_add[valid_mask])\n        waste_priority = 1.0 / (waste + epsilon)\n\n        # Near-full bonus, but more conservative and adaptive\n        near_full_bonus = np.where(\n            (bins_remain_cap[valid_mask] > item) & (bins_remain_cap[valid_mask] <= 2 * item),\n            item / bins_remain_cap[valid_mask] * 5,  # Reduced magnitude of bonus\n            0\n        )\n\n        # Adaptive weighting based on bin utilization\n        bin_utilization = (bins_remain_cap - remaining_after_add) / bins_remain_cap # calculate how much space we will be using in the bin\n        bin_utilization = bin_utilization[valid_mask]\n\n        # Scale the values for better control over the weights\n        utilization_mean = np.mean(bin_utilization) if np.any(valid_mask) else 0.5\n        utilization_std = np.std(bin_utilization) if np.any(valid_mask) and len(bin_utilization) > 1 else 0.1\n\n        waste_weight = 1.0\n        near_full_weight = 0.75  # Slightly reduce the importance of near-full bonus\n        utilization_weight = 0.5 # Weight for utilization factor\n\n        # Prioritize bins with lower utilization if item is small\n        if item < np.mean(bins_remain_cap):\n            utilization_priority = 1 - bin_utilization\n        else: # Prioritize bins with higher utilization if item is large\n            utilization_priority = bin_utilization\n\n        # Combine priorities with adaptive weights\n        priorities[valid_mask] = (\n            waste_weight * waste_priority +\n            near_full_weight * near_full_bonus +\n            utilization_weight * utilization_priority\n        )\n\n        # Reduce priority if the item will make the bin nearly full (risk of fragmentation)\n        nearly_full_penalty = np.where(remaining_after_add[valid_mask] < 0.1 * bins_remain_cap[valid_mask], -10, 0)\n        priorities[valid_mask] += nearly_full_penalty\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic incorporates waste minimization, near-full bonuses, adaptive weighting based on capacity distribution, and a fill percentage score, whereas the worst only considers waste, near-full penalty, and capacity, with adaptive weighting based on total capacity. (2nd best) vs (second worst) reveals similar attributes. Comparing (1st) vs (2nd), no code differences between the two. (3rd) vs (4th), the 4th introduces a larger infeasibility penalty, adjusts the near-full bonus with power, reduces capacity weight, sharpens the fill percentage peak, and adds a penalty for bins with very little remaining capacity. (second worst) vs (worst), little change in functionality. Overall: The better performing heuristics combine multiple factors (waste, near-fullness, capacity distribution, fill percentage) with carefully tuned weights and penalties, while the worse performing ones use fewer factors and simpler weighting schemes. The best heuristics also show refinement in handling infeasibility and edge cases.\n- \nOkay, let's refine \"Current Self-Reflection\" for designing better heuristics. Aim for actionable insights that avoid the pitfalls of ineffective self-reflection.\n\nHere's a breakdown:\n\n*   **Keywords:** Adaptive weights, factor combination, infeasibility penalties, experimental calibration, numerical stability, problem context.\n\n*   **Advice:** Design heuristics that intelligently combine relevant factors using adaptive weighting schemes informed by data characteristics and problem context. Prioritize penalties for infeasibility.\n\n*   **Avoid:** Simple implementations that overlook problem nuances, relying solely on penalties, ignoring potential rewards, skipping data-driven parameter tuning.\n\n*   **Explanation:** Move beyond basic implementations. Adaptive weighting, calibrated by experimentation and problem context, allows for better heuristic performance. Consider rewards, not *just* penalties.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}