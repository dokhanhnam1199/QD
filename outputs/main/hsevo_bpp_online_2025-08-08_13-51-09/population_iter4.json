[
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines the 'tight fit' prioritization of inverse difference with a sigmoid\n    function to normalize priorities, favoring bins that are a near-perfect fit\n    while maintaining a reasonable range.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(valid_bins_mask):\n        return priorities\n\n    valid_bins_cap = bins_remain_cap[valid_bins_mask]\n    \n    # Calculate inverse difference for valid bins: smaller difference is better\n    # Adding a small epsilon to avoid division by zero and extreme values\n    inverse_diff = 1.0 / (valid_bins_cap - item + 1e-9)\n    \n    # Use sigmoid to normalize and shape the priorities.\n    # The sigmoid will map the inverse differences to a [0, 1] range.\n    # We can center the sigmoid around a typical \"good fit\" or use the min/max\n    # of the calculated inverse differences to create a more adaptive scaling.\n    \n    min_inv_diff = np.min(inverse_diff)\n    max_inv_diff = np.max(inverse_diff)\n    \n    # Normalize inverse_diff to [0, 1] before applying sigmoid for more stable results\n    if max_inv_diff - min_inv_diff > 1e-9:\n        normalized_inv_diff = (inverse_diff - min_inv_diff) / (max_inv_diff - min_inv_diff)\n    else:\n        normalized_inv_diff = np.zeros_like(inverse_diff)\n\n    # Apply sigmoid. A sigmoid centered around 0.5 (e.g., 2 * x - 1 for normalized input)\n    # will map [0, 1] to roughly [0, 1], with a steep rise in the middle.\n    # Here, we use a simple sigmoid form that maps values to [0, 1].\n    # A common sigmoid form: 1 / (1 + exp(-k * (x - x0)))\n    # Let's use a simpler form for demonstration, similar to a normalized inverse:\n    # We want smaller differences (larger inverse_diff) to have higher priority.\n    # A high inverse_diff should map to a high sigmoid output.\n    # Using normalized_inv_diff, a higher value means a tighter fit.\n    # Let's use a sigmoid that emphasizes the middle range.\n    \n    # Option 1: Simple sigmoid on normalized inverse difference\n    # This will give higher priority to bins that are \"moderately\" good fits\n    # relative to the best fits.\n    scaled_priorities = 1 / (1 + np.exp(-10 * (normalized_inv_diff - 0.5))) # steep sigmoid\n\n    # Option 2: Direct sigmoid on inverse difference, scaled and shifted.\n    # This approach might be more sensitive to extreme inverse_diff values.\n    # We can scale inverse_diff to a reasonable range for sigmoid.\n    # Let's use the inverse difference directly, but clip extreme values or scale carefully.\n    # For simplicity and robustness, sticking with normalized inverse difference.\n\n    priorities[valid_bins_mask] = scaled_priorities\n    \n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response1.txt_stdout.txt",
    "code_path": "problem_iter2_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a Sigmoid-based preference for tighter fits.\n\n    Prioritizes bins that fit the item, favoring those with minimal remaining capacity.\n    A sigmoid function is used to smooth the priority for bins with very close fits.\n    \"\"\"\n    available_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(available_bins_mask):\n        valid_capacities = bins_remain_cap[available_bins_mask]\n        \n        # Calculate the difference between capacity and item size\n        differences = valid_capacities - item\n        \n        # Use a sigmoid function on the negative differences. \n        # Smaller differences (tighter fits) will result in values closer to 1.\n        # We invert the difference to make smaller differences yield higher sigmoid outputs.\n        # Adding a small epsilon to avoid division by zero if differences are very close to zero.\n        # Scaling factor can be tuned for sensitivity.\n        scaling_factor = 10.0  # More aggressive preference for tight fits\n        # Using negative differences as input for sigmoid to map smaller differences to higher outputs.\n        sigmoided_priorities = 1 / (1 + np.exp(scaling_factor * differences)) \n        \n        priorities[available_bins_mask] = sigmoided_priorities\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 10.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response2.txt_stdout.txt",
    "code_path": "problem_iter2_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins based on a combination of 'best fit' and 'least remaining capacity'.\n    It favors bins that closely fit the item while also considering those that will have\n    the least remaining space after packing, encouraging fuller bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    available_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(available_bins_mask):\n        return priorities  # No bins available\n\n    available_caps = bins_remain_cap[available_bins_mask]\n    \n    # --- Heuristic 1: Sigmoid on Fit Ratio (modified) ---\n    # Metric 1: How well the item fits the bin's remaining capacity.\n    # We use item / available_caps to represent how full the bin *will become*.\n    # A smaller ratio means a tighter fit (higher priority).\n    # We invert this for the sigmoid input to favor smaller ratios.\n    fit_ratios = item / available_caps\n    \n    # Sigmoid for fit ratio: -fit_ratios emphasizes smaller ratios.\n    # Scale and shift to center the sigmoid around a 'good fit' point (e.g., ratio close to 0).\n    # Adding a small epsilon to avoid division by zero or log(0) issues if scaling is applied later.\n    sigmoid_fit_input = -fit_ratios * 5.0 # Steepness parameter\n    priorities[available_bins_mask] = 1 / (1 + np.exp(-sigmoid_fit_input))\n\n    # --- Heuristic 11: Inverse Distance (modified for remaining capacity) ---\n    # Metric 2: Prioritize bins with less remaining capacity *after* packing.\n    # This is similar to \"best fit\" by minimizing leftover space.\n    remaining_capacities_after_fit = available_caps - item\n    \n    # Use inverse of remaining capacity, adding a small epsilon to avoid division by zero.\n    # Smaller remaining capacity should lead to higher priority.\n    inverse_remaining_cap = 1.0 / (remaining_capacities_after_fit + 1e-9)\n\n    # Normalize inverse remaining capacities to combine with fit priorities.\n    # This ensures that the remaining capacity metric is on a similar scale.\n    max_inv_rem_cap = np.max(inverse_remaining_cap)\n    if max_inv_rem_cap > 0:\n        normalized_inverse_remaining_cap = inverse_remaining_cap / max_inv_rem_cap\n    else:\n        normalized_inverse_remaining_cap = np.zeros_like(inverse_remaining_cap)\n\n    # --- Combination ---\n    # Combine the two metrics. We can use a weighted sum or a multiplication.\n    # Multiplication can emphasize bins that are good in *both* aspects.\n    # Let's use a weighted sum for more flexibility.\n    \n    # Assign weights to each heuristic. These can be tuned.\n    weight_fit = 0.7\n    weight_remaining = 0.3\n    \n    combined_priorities = (weight_fit * priorities[available_bins_mask] + \n                           weight_remaining * normalized_inverse_remaining_cap)\n\n    # Normalize the final combined priorities to be between 0 and 1,\n    # ensuring the highest priority bin is clearly selected.\n    max_combined_priority = np.max(combined_priorities)\n    if max_combined_priority > 0:\n        priorities[available_bins_mask] = combined_priorities / max_combined_priority\n    else:\n        priorities[available_bins_mask] = np.zeros_like(combined_priorities)\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.11846828879138,
    "SLOC": 26.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a Sigmoid-based penalty for large remaining capacities.\n\n    Prioritizes bins that closely fit the item, with a smooth penalty for bins\n    that would have a significantly larger remaining capacity after placement.\n    \"\"\"\n    available_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(available_bins_mask):\n        valid_capacities = bins_remain_cap[available_bins_mask]\n        \n        # Heuristic 1: \"Best Fit\" component using inverse of difference\n        # Higher score for smaller differences (tighter fit)\n        differences = valid_capacities - item\n        best_fit_scores = 1.0 / (differences + 1e-9)\n\n        # Heuristic 2: Sigmoid applied to the *remaining* capacity after fitting\n        # Penalizes bins that will have a lot of space left, but smoothly.\n        # Centered to give higher scores to smaller remaining capacities.\n        scaling_factor = 10.0 # Tune this to control sensitivity to remaining space\n        potential_remaining_capacities = valid_capacities - item\n        sigmoided_penalty = 1 / (1 + np.exp(scaling_factor * (potential_remaining_capacities - 0.1))) # Penalty for remaining capacity > 0.1\n\n        # Combine scores: Prioritize good fits and penalize large remaining capacities\n        # A simple multiplication or weighted sum can be used. Here, we multiply\n        # to ensure that both conditions (good fit AND small remaining capacity) are met.\n        combined_scores = best_fit_scores * sigmoided_penalty\n\n        priorities[available_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response4.txt_stdout.txt",
    "code_path": "problem_iter2_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with an exploration bonus for less full bins.\n\n    Prioritizes bins that offer a tight fit (small remaining capacity) but\n    also encourages exploring bins with more empty space to avoid premature\n    bin exhaustion.\n    \"\"\"\n    epsilon = 0.1\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n        # Best Fit component: prioritize bins with minimal remaining capacity after fit\n        # Add a small constant to avoid division by zero.\n        best_fit_score = 1.0 / (suitable_bins_cap - item + 1e-9)\n\n        # Exploration component: bonus for bins with larger remaining capacity\n        # Normalize the bonus to prevent it from dominating the best fit score.\n        # Using min-max scaling on the remaining capacities of suitable bins.\n        if suitable_bins_cap.size > 1:\n            min_cap = np.min(suitable_bins_cap)\n            max_cap = np.max(suitable_bins_cap)\n            normalized_remaining_cap = (suitable_bins_cap - min_cap) / (max_cap - min_cap + 1e-9)\n            exploration_bonus = epsilon * normalized_remaining_cap\n        else:\n            # If only one suitable bin, no exploration bonus needed relative to others\n            exploration_bonus = np.zeros_like(suitable_bins_cap)\n\n        # Combine scores\n        priorities[suitable_bins_mask] = best_fit_score + exploration_bonus\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.198244914240141,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response5.txt_stdout.txt",
    "code_path": "problem_iter2_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by combining a tight fit score (inverse of remaining capacity after fitting)\n    with a diversification bonus for bins with less capacity.\n    This aims to favor bins that are almost full for the current item while also\n    exploring less utilized bins to prevent premature overcrowding.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Filter bins that can accommodate the item\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n    \n    if valid_bins_remain_cap.size > 0:\n        # Calculate \"tight fit\" score: inverse of remaining capacity after fitting\n        # A smaller remaining capacity means a tighter fit, hence higher priority.\n        # Add a small epsilon to avoid division by zero.\n        fit_scores = 1.0 / (valid_bins_remain_cap - item + 1e-9)\n        \n        # Calculate a \"diversification bonus\" based on the inverse of the remaining capacity\n        # This slightly favors bins that are less full, promoting exploration.\n        # We use the original remaining capacity here to gauge overall fullness.\n        diversification_bonus = 1.0 / (bins_remain_cap[valid_bins_mask] + 1e-9)\n        \n        # Combine fit score and diversification bonus.\n        # A simple additive combination, scaled to give reasonable influence to both.\n        # The scaling factor can be tuned. Here, we'll give a slight edge to fit_scores.\n        combined_priorities = fit_scores + 0.5 * diversification_bonus\n        \n        # Assign the calculated priorities back to the original array indices\n        priorities[valid_bins_mask] = combined_priorities\n        \n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 10.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response6.txt_stdout.txt",
    "code_path": "problem_iter2_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with an exploration bonus for less full bins.\n\n    Prioritizes bins that are a tight fit (best fit) but also explores\n    less full bins to potentially improve overall packing.\n    \"\"\"\n    epsilon = 0.1\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n        # Best Fit component: inverse of remaining capacity after fitting\n        best_fit_scores = 1.0 / (suitable_bins_cap - item + 1e-9)\n\n        # Exploration component: bonus for bins with more remaining capacity (less full)\n        avg_remaining_capacity = np.mean(suitable_bins_cap)\n        exploration_bonus = np.maximum(0, avg_remaining_capacity - suitable_bins_cap) * epsilon\n\n        priorities[suitable_bins_mask] = best_fit_scores + exploration_bonus\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.11846828879138,
    "SLOC": 11.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response7.txt_stdout.txt",
    "code_path": "problem_iter2_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (inverse difference) with an exploration bonus\n    for less full bins, inspired by Epsilon-Greedy.\n\n    Args:\n        item: Size of item to be packed.\n        bins_remain_cap: Array of remaining capacities of each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    epsilon = 0.1\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        \n        # Best Fit component: Prioritize bins with minimal remaining capacity after packing\n        # Using inverse of remaining capacity after fitting for a \"tighter fit\" score\n        best_fit_scores = 1.0 / (suitable_bins_caps - item + 1e-9)\n        \n        # Exploration component: Bonus for bins with more remaining capacity\n        # This encourages trying bins that are not necessarily the tightest fit\n        avg_suitable_cap = np.mean(suitable_bins_caps)\n        exploration_bonus = np.maximum(0, avg_suitable_cap - suitable_bins_caps) * epsilon\n        \n        priorities[suitable_bins_mask] = best_fit_scores + exploration_bonus\n        \n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.11846828879138,
    "SLOC": 11.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response8.txt_stdout.txt",
    "code_path": "problem_iter2_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins using a combination of inverse difference and remaining capacity.\n\n    This heuristic combines the \"best fit\" aspect of inverse difference with a\n    penalty for bins with excessively large remaining capacity, promoting tighter fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins_mask = bins_remain_cap >= item\n    \n    if np.any(valid_bins_mask):\n        valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n        \n        differences = valid_bins_remain_cap - item\n        \n        # Inverse difference for best fit, scaled by inverse of remaining capacity to penalize large gaps\n        # Adding a small epsilon to the denominator to prevent division by zero\n        scaled_inverse_differences = 1.0 / (differences + 1e-9) / (valid_bins_remain_cap + 1e-9)\n        \n        priorities[valid_bins_mask] = scaled_inverse_differences\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines sigmoid fit score with an exploration bonus for diverse bin selection.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Calculate potential remaining capacity for valid bins\n    potential_remaining_cap = bins_remain_cap - item\n    valid_bins_mask = potential_remaining_cap >= 0\n\n    # If no bins can accommodate the item, return zeros\n    if not np.any(valid_bins_mask):\n        return np.zeros_like(bins_remain_cap)\n\n    # Normalize remaining capacities of valid bins for sigmoid calculation\n    valid_capacities = bins_remain_cap[valid_bins_mask]\n    \n    # Use a sigmoid function to get a \"tight fit\" score.\n    # We want bins that leave minimal remaining capacity, so we use the negative\n    # of the remaining capacity. The scaling factor controls steepness.\n    scaling_factor = 2.0\n    tight_fit_scores = 1 / (1 + np.exp(-scaling_factor * (-potential_remaining_cap[valid_bins_mask])))\n\n    # Introduce an exploration bonus: bins with more remaining capacity get a small bonus\n    # to encourage exploring less full bins. Normalize capacities for bonus calculation.\n    # Add a small epsilon to avoid division by zero if a bin is already full (though filtered by valid_bins_mask)\n    exploration_bonus = 0.1 * (valid_capacities / (np.mean(valid_capacities) + 1e-9))\n\n    # Combine tight fit score with exploration bonus.\n    # The exploration bonus is added to the tight fit score.\n    combined_scores = tight_fit_scores + exploration_bonus\n\n    # Initialize priorities with zeros\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Assign the combined scores to the valid bins\n    priorities[valid_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.487435181491823,
    "SLOC": 13.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response0.txt_stdout.txt",
    "code_path": "problem_iter3_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin.\n    This version aims for a tighter fit by considering the remaining capacity\n    after placing the item, penalizing bins that become too empty, and\n    favoring bins that are already relatively full. It also introduces a\n    slight diversification by not always picking the absolute best fit,\n    but rather a set of good fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Identify bins that can fit the item\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    # Calculate the remaining capacity if the item is placed in a suitable bin\n    remaining_caps_after_placement = bins_remain_cap[suitable_bins_mask] - item\n    \n    # --- Heuristic Logic ---\n    # Metric 1: Favor bins that result in a smaller remaining capacity (tighter fit)\n    # We invert and add a small constant to avoid division by zero if remaining_caps_after_placement is 0.\n    tight_fit_score = 1.0 / (remaining_caps_after_placement + 1e-6)\n    \n    # Metric 2: Penalize bins that become too empty. This encourages filling bins more evenly.\n    # We use a function that decreases as remaining capacity increases.\n    # A quadratic penalty or exponential decay could also be explored.\n    empty_penalty_score = np.exp(-remaining_caps_after_placement / 5.0) # Adjust the divisor for sensitivity\n\n    # Metric 3: Favor bins that are already more full.\n    # This is inversely related to current remaining capacity.\n    fullness_score = 1.0 / (bins_remain_cap[suitable_bins_mask] + 1e-6)\n    \n    # Combine scores. Weights can be tuned.\n    # Here, we give more weight to the tight fit and fullness, and less to the empty penalty.\n    combined_score = (0.5 * tight_fit_score) + (0.3 * fullness_score) + (0.2 * empty_penalty_score)\n\n    # Assign the calculated scores to the suitable bins\n    priorities[suitable_bins_mask] = combined_score\n    \n    # --- Diversification ---\n    # Instead of strictly picking the argmin, let's slightly perturb the priorities\n    # to encourage exploration of near-optimal bins.\n    # We can add a small random noise, or select from the top-k bins.\n    # For simplicity here, let's just make the absolute best bin slightly more prominent\n    # but ensure other good fits have a reasonable chance.\n    \n    if np.any(priorities > 0):\n        min_priority = np.min(priorities[priorities > 0])\n        # Add a small positive value to all suitable bins to avoid zero priority\n        priorities[priorities > 0] += min_priority * 0.01 \n        \n        # Find the index of the bin with the maximum priority\n        best_bin_index = np.argmax(priorities)\n        \n        # Set the priority of the best bin to a higher value\n        priorities[best_bin_index] = priorities[best_bin_index] * 1.1 # Boost the best bin slightly\n        \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 58, in priority_v2\nnumpy._core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'\n15\n2\n"
  },
  {
    "stdout_filepath": "problem_iter3_response1.txt_stdout.txt",
    "code_path": "problem_iter3_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Filter bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    # Calculate potential remaining capacity if item is placed\n    potential_remaining_cap = bins_remain_cap[suitable_bins_mask] - item\n    \n    # Heuristic 1: Prioritize bins with the smallest remaining capacity after placement (Best Fit)\n    # Lower remaining capacity means a tighter fit. We want to prioritize these.\n    # We invert the remaining capacity to get a higher score for smaller remaining capacity.\n    # Add a small epsilon to avoid division by zero if remaining capacity is 0.\n    best_fit_score = 1.0 / (potential_remaining_cap + 1e-9)\n    \n    # Heuristic 2: Consider the original remaining capacity for diversification.\n    # Bins with larger original remaining capacity might offer more flexibility for future items.\n    # We use a logarithmic scale to dampen the effect of very large capacities.\n    original_capacity_score = np.log1p(bins_remain_cap[suitable_bins_mask])\n    \n    # Heuristic 3: Introduce a slight penalty for bins that are already very full.\n    # This encourages using slightly less full bins to leave more room for future items.\n    # The penalty is higher for bins that are closer to being full.\n    fullness_penalty = 1.0 / (bins_remain_cap[suitable_bins_mask] + 1e-9)\n    \n    # Combine heuristics: A weighted sum of the scores.\n    # The weights can be tuned. Here, we give a slightly higher weight to Best Fit.\n    combined_score = (0.5 * best_fit_score + 0.3 * original_capacity_score - 0.2 * fullness_penalty)\n    \n    # Normalize scores to be between 0 and 1 for better stability and comparability\n    if combined_score.size > 0:\n        min_score = np.min(combined_score)\n        max_score = np.max(combined_score)\n        if max_score - min_score > 1e-9:\n            normalized_scores = (combined_score - min_score) / (max_score - min_score)\n        else:\n            normalized_scores = np.ones_like(combined_score) * 0.5 # If all scores are the same, assign a neutral score\n    else:\n        normalized_scores = np.array([])\n\n    # Assign the calculated priorities to the suitable bins\n    priorities[suitable_bins_mask] = normalized_scores\n    \n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 83.56601515755884,
    "SLOC": 19.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response2.txt_stdout.txt",
    "code_path": "problem_iter3_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin,\n    prioritizing bins that result in a tighter fit and considering\n    the potential for future packing.\n\n    This heuristic aims to improve upon First Fit Decreasing by:\n    1. Prioritizing bins that leave a smaller remaining capacity (tightest fit).\n    2. Favoring bins that have been used less to encourage wider bin utilization (diversification).\n    3. Introducing a small random perturbation to explore different packing configurations.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Mask for bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    # Calculate a score for suitable bins\n    # Lower remaining capacity is generally better (tighter fit)\n    # Higher number of bins used so far (inverse of remaining capacity sum) is worse (encourages diversification)\n    \n    # Avoid division by zero or very small numbers\n    non_zero_caps = bins_remain_cap[suitable_bins_mask]\n    if non_zero_caps.size > 0:\n        # Tighter fit: inverse of remaining capacity\n        tight_fit_score = 1.0 / (non_zero_caps - item + 1e-9)\n        \n        # Diversification score: penalize bins that are already very full (lower capacity means more utilized)\n        # We want to encourage filling bins that are already somewhat utilized to avoid many partially filled bins.\n        # However, we also don't want to completely neglect under-utilized bins.\n        # Let's consider the inverse of the *original* capacity used, which is harder to track without more context.\n        # A simpler approach: penalize bins with very high remaining capacity, which implies they are under-utilized.\n        # Let's use the inverse of remaining capacity (higher means more used) as a positive term to encourage filling.\n        # This is a bit contradictory to the diversification goal of using many bins.\n        # A better diversification: encourage using bins that have *already* accepted items, so they are not completely empty.\n        # If we don't know how many items are in each bin, we can infer usage from remaining capacity relative to bin size.\n        # Let's assume a default bin capacity if not provided, or infer from context. For now, focus on remaining capacity.\n        \n        # A good diversification proxy: penalize bins with a lot of remaining capacity relative to the item size.\n        # This encourages using bins that are \"just right\" for the item.\n        # Alternatively, penalize bins that are too empty (very high remaining capacity).\n        # Let's try to assign a higher priority to bins that are \"almost full\" after packing the item.\n        \n        # Combine tight fit and a penalty for very large remaining capacities (under-utilization)\n        # The higher the score, the better the bin.\n        # We want smaller (non_zero_caps - item), so we invert it.\n        # We also want to penalize very large (non_zero_caps - item), so we might add a term that decreases with large differences.\n        \n        # Let's try a simple heuristic: prioritize bins that leave the least remaining space.\n        # Add a small diversification factor by slightly favoring bins that are not completely empty.\n        # If we assume all bins initially have the same capacity (e.g., `max_capacity`), we can use that.\n        # Without `max_capacity`, let's use the fact that `bins_remain_cap` are values themselves.\n        # A bin with `remaining_cap = X` has used `total_cap - X`.\n        # If we want to diversify, we might want to avoid filling up a few bins completely if many bins are available.\n        # This is tricky without knowing the total capacity or number of items in each bin.\n        \n        # Let's stick to a refined \"tight fit\" and add a diversification term based on how \"full\" the bin *would be*.\n        # 'Fullness' after packing: (total_capacity - (remaining_capacity - item)) / total_capacity\n        # Assuming total_capacity is constant or implicitly handled, we can focus on (remaining_capacity - item).\n        # Smaller (remaining_capacity - item) is better for tightness.\n        \n        # Let's try: priority = 1 / (remaining_capacity_after_packing + epsilon) + small_diversification_bonus_for_not_empty_bins\n        # If we don't know empty/non-empty status:\n        # The goal is to select a bin. We want to pick the \"best\" bin.\n        # The best bin is one that leaves the least space, making it a tight fit.\n        # Consider the *ratio* of remaining capacity to item size. A smaller ratio is better for tight fit.\n        # `(bins_remain_cap[suitable_bins_mask] - item) / item`\n        \n        # Let's go back to the idea of local search neighborhoods.\n        # A neighborhood could be:\n        # 1. Move an item to a different bin.\n        # 2. Swap two items between bins.\n        # Our priority function needs to guide this process.\n        # For online BPP, we don't have future items.\n        \n        # Consider a hybrid approach:\n        # Primary goal: Minimize remaining capacity after packing (tight fit).\n        # Secondary goal: Encourage using bins that are not too empty (diversification).\n        \n        # Tighter fit: higher priority for smaller (remaining_capacity - item)\n        # A score that increases as (remaining_capacity - item) decreases.\n        tightness_score = 1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)\n        \n        # Diversification: favor bins that are already somewhat utilized.\n        # If a bin has a lot of remaining capacity, it's less utilized.\n        # We can penalize bins with very high remaining capacity.\n        # Let's assume bin capacities are somewhat normalized or we can infer a sense of \"emptiness\".\n        # A simple proxy for \"emptiness\" is the value of `bins_remain_cap` itself.\n        # Higher `bins_remain_cap` means more empty space.\n        # So, we want to give *lower* priority to bins with very high `bins_remain_cap`.\n        # This means we want a score that decreases as `bins_remain_cap` increases.\n        # A possible term: `- bins_remain_cap[suitable_bins_mask]` or `1.0 / bins_remain_cap[suitable_bins_mask]`\n        # Using `1.0 / bins_remain_cap` could lead to division by zero if a bin has 0 remaining capacity (though this wouldn't be suitable).\n        # Let's use `bins_remain_cap` directly as a penalty term, perhaps scaled.\n        \n        # Let's try to combine:\n        # Priority = (tightness_score) - (diversification_penalty)\n        # Where tightness_score is high for small `remaining_capacity - item`.\n        # And diversification_penalty is high for large `bins_remain_cap`.\n        \n        # Let's normalize `bins_remain_cap` to get a sense of \"emptiness\" relative to potential capacity.\n        # This is hard without knowing max capacity.\n        # If we consider `bins_remain_cap` as absolute values:\n        # A bin with 50 remaining capacity is \"more empty\" than a bin with 10.\n        # So, to diversify, we might want to avoid picking bins with very high remaining capacity if possible.\n        \n        # A simple combined metric for suitable bins:\n        # We want to minimize `remaining_capacity_after_packing` (which is `bins_remain_cap[suitable_bins_mask] - item`).\n        # Let's make this the primary driver for priority: `-(bins_remain_cap[suitable_bins_mask] - item)`\n        # For diversification, we want to avoid bins with very high `bins_remain_cap`.\n        # So, add a penalty for high `bins_remain_cap`: `- bins_remain_cap[suitable_bins_mask]`\n        \n        # Combined score: `-(bins_remain_cap[suitable_bins_mask] - item) - bins_remain_cap[suitable_bins_mask]`\n        # This simplifies to: `item - 2 * bins_remain_cap[suitable_bins_mask] + item` which is not right.\n        # It should be: `-(bins_remain_cap[suitable_bins_mask] - item)` (maximize this) for tightness.\n        # For diversification, we want to select bins that are not *too* empty.\n        # \"Too empty\" means having a large `bins_remain_cap`.\n        # So, we want to prefer bins with *smaller* `bins_remain_cap`.\n        # This seems counter-intuitive to diversification which wants to use *more* bins.\n        \n        # Let's rethink diversification: Using more bins means not filling up a few bins too quickly.\n        # This implies we should prefer bins that are *less* full.\n        # This is directly opposed to tight fit. This is the core dilemma.\n        \n        # Alternative interpretation of diversification: Avoid creating \"too many\" partially filled bins.\n        # This means filling up existing bins as much as possible. This leads back to tight fit.\n        \n        # Let's consider the advice: \"explore diverse metrics beyond simple inverse relationships\".\n        # And \"how different neighborhood structures in local search can expose novel packing solutions\".\n        # For an online setting, we don't have local search on a static solution.\n        # We need a heuristic that makes a good choice *now*.\n        \n        # Let's try a metric that balances tightness with the *volume available for future small items*.\n        # If a bin has just enough space for the current item, that's a tight fit.\n        # If a bin has *much more* space than the current item, it might be able to hold several more small items.\n        \n        # Consider a bin's \"utility\":\n        # Utility = (1 / (remaining_capacity_after_packing + epsilon))  # For tight fit\n        # Utility = some_function(remaining_capacity_after_packing)     # For future packing\n        \n        # If `bins_remain_cap[suitable_bins_mask] - item` is very small, the bin is tight.\n        # If `bins_remain_cap[suitable_bins_mask]` is large, the bin has a lot of room.\n        \n        # Let's try a score that favors bins that are *almost full* but can still take the item.\n        # The \"fullness\" of a bin after packing is related to `total_capacity - (bins_remain_cap - item)`.\n        # Without `total_capacity`, let's focus on the remaining gap.\n        \n        # A more sophisticated \"tight fit\" might consider the \"waste\" created.\n        # Waste = `bins_remain_cap[suitable_bins_mask] - item`.\n        # We want to minimize waste.\n        \n        # Let's introduce a penalty for \"too much\" remaining capacity.\n        # A bin with remaining capacity `R` and item size `I`.\n        # Gap `G = R - I`. We want to minimize `G`.\n        # Also, if `R` is very large, it might be better to leave it for future small items.\n        # So, we want to pick bins where `G` is small, but `R` isn't excessively large (to avoid having many \"almost empty\" bins).\n        \n        # Let's try a combined score for suitable bins:\n        # `score = (1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)) - (bins_remain_cap[suitable_bins_mask] / max_possible_capacity_or_some_scale)`\n        # Without `max_possible_capacity`, let's use a dynamic scale based on current `bins_remain_cap`.\n        # Perhaps scale by the average remaining capacity?\n        \n        # Let's try a score that prioritizes bins that leave a small gap, but also slightly penalizes bins that leave a *huge* gap.\n        # `score = (1 / (gap + epsilon)) - (gap / some_large_constant)`\n        # Or `score = (1 / (gap + epsilon)) * (1 / (1 + gap / some_scale))`\n        \n        # Let's try a simpler combination that still reflects the advice:\n        # 1. Tight fit: Prioritize small `gap = bins_remain_cap[suitable_bins_mask] - item`.\n        #    Score component: `1.0 / (gap + 1e-9)`\n        # 2. Diversification (avoiding *too* empty bins): Penalize large `bins_remain_cap`.\n        #    Score component: `-(bins_remain_cap[suitable_bins_mask] * small_weight)`\n        \n        # Combine them:\n        # `priorities[suitable_bins_mask] = (1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)) - (bins_remain_cap[suitable_bins_mask] * 0.01)`\n        # The weight 0.01 is arbitrary and needs tuning.\n        \n        # Let's try another approach inspired by local search neighborhoods.\n        # Imagine we could move items. A good bin is one that, after receiving the item,\n        # allows for easy packing of subsequent items.\n        # This means leaving a \"useful\" amount of space, not too little, not too much.\n        \n        # What if we consider the \"entropy\" of the remaining capacities?\n        # Or the variance of remaining capacities?\n        \n        # Let's simplify and focus on refining the \"tight fit\" and adding a diversification hint.\n        \n        # For suitable bins:\n        # `remaining_after_packing = bins_remain_cap[suitable_bins_mask] - item`\n        \n        # Priority metric 1: Smallest `remaining_after_packing`.\n        #   `p1 = 1.0 / (remaining_after_packing + 1e-9)`\n        \n        # Priority metric 2: Moderate `bins_remain_cap`.\n        #   This is tricky without a reference. Let's consider the \"fullness ratio\" if we had a max capacity `C`.\n        #   Fullness = `(C - bins_remain_cap) / C`.\n        #   We want to avoid bins where `bins_remain_cap` is very large, meaning low fullness.\n        #   If we don't know `C`, we can use the current `bins_remain_cap` values themselves.\n        #   A simple way to penalize large `bins_remain_cap`: use a term like `-bins_remain_cap[suitable_bins_mask]`.\n        #   This makes bins with less remaining capacity more attractive.\n        \n        # Let's combine these:\n        # We want to maximize `p1` and maximize (or not penalize) the second metric.\n        # Let's make both terms positive and combined.\n        # Term 1 (tightness): `1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)`\n        # Term 2 (diversification by avoiding very empty bins):\n        #   We want to favor bins with *less* remaining capacity if they are not too full.\n        #   Let's try to favor bins that have a moderate amount of remaining capacity after packing.\n        #   This means `bins_remain_cap[suitable_bins_mask] - item` is neither too small nor too large.\n        #   This sounds like a Gaussian-like function centered around some ideal gap.\n        \n        # A pragmatic approach:\n        # Prioritize bins that leave the *smallest* non-negative remainder.\n        # Among those with the same smallest remainder, pick one that isn't *entirely* empty.\n        # If all have the same remainder and are entirely empty, pick one randomly.\n        \n        # Let's try to improve upon the \"tightest fit\" by adding a diversification element that\n        # encourages filling bins that are already in use, rather than always picking a new empty bin.\n        # This is hard to do without knowing bin usage history.\n        \n        # Consider the ratio: `item / bins_remain_cap[suitable_bins_mask]`\n        # Higher ratio means the item is a larger fraction of the remaining capacity.\n        # This suggests a tighter fit.\n        # `ratios = item / bins_remain_cap[suitable_bins_mask]`\n        \n        # Let's combine tightness and a \"fill-up\" incentive.\n        # Metric: `(bins_remain_cap[suitable_bins_mask] - item)` -> minimize this.\n        # Metric: `bins_remain_cap[suitable_bins_mask]` -> if too high, penalize.\n        \n        # Try this:\n        # For suitable bins, calculate `gap = bins_remain_cap[suitable_bins_mask] - item`\n        # Score = `1.0 / (gap + 1e-9)` # Maximizes tightness\n        # Add a small penalty for large gaps to encourage some slack for future small items?\n        # No, the advice says \"explore diverse metrics beyond simple inverse relationships\".\n        \n        # Let's use a combination inspired by \"least wasted space\" and \"most used bin\".\n        # For suitable bins:\n        # `remaining_after_packing = bins_remain_cap[suitable_bins_mask] - item`\n        \n        # Priority Score = `(1.0 / (remaining_after_packing + 1e-9)) + (1.0 / (bins_remain_cap[suitable_bins_mask] + 1e-9))`\n        # The first term favors tight fits.\n        # The second term favors bins with *less* remaining capacity (i.e., more used), which encourages filling up bins.\n        # This second term acts as a diversification by trying to fill existing bins rather than always starting new ones.\n        \n        # Let's refine this: The second term `1.0 / bins_remain_cap` might heavily favor bins that are almost full.\n        # We want to avoid bins that are *too* empty.\n        # So, penalize large `bins_remain_cap`.\n        \n        # New proposal:\n        # Score for suitable bins = `(1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)) - (bins_remain_cap[suitable_bins_mask] * 0.05)`\n        # This prioritizes tightness, and then slightly penalizes bins that have a lot of remaining capacity.\n        # The `- (bins_remain_cap * 0.05)` term is a heuristic diversification to avoid very empty bins dominating.\n        \n        # Let's try a simpler, more direct interpretation of the advice:\n        # \"refining the quality of 'tight fit' assessment by exploring diverse metrics beyond simple inverse relationships\"\n        # Instead of just `1/gap`, consider a function that has a peak when `gap` is small but not zero.\n        # Or consider the *ratio* of item size to remaining capacity.\n        \n        # Metric: `item / bins_remain_cap[suitable_bins_mask]`\n        # This ratio is higher when the item takes up a larger portion of the remaining capacity, indicating a tighter fit.\n        # `tightness_score_ratio = item / bins_remain_cap[suitable_bins_mask]`\n        \n        # Now, for diversification. Avoid selecting bins that are *too* empty.\n        # This means avoiding bins where `bins_remain_cap[suitable_bins_mask]` is very large.\n        # So, we want to penalize large `bins_remain_cap`.\n        # A simple penalty: `- bins_remain_cap[suitable_bins_mask]`.\n        \n        # Combined score: `tightness_score_ratio - (bins_remain_cap[suitable_bins_mask] * weight)`\n        # `priorities[suitable_bins_mask] = (item / bins_remain_cap[suitable_bins_mask]) - (bins_remain_cap[suitable_bins_mask] * 0.01)`\n        \n        # Let's try something that captures \"best fit\" and \"next best fit\" interaction.\n        # The \"best fit\" is the bin that minimizes `bins_remain_cap - item`.\n        # The \"next best fit\" indicates if other bins are also quite suitable.\n        \n        # How about this: for each suitable bin, calculate a score.\n        # Score = (1 / (remaining_capacity_after_packing + epsilon))  # Tightness\n        # Add a small random noise for exploration:\n        # `noise = np.random.normal(0, 0.01 * (bins_remain_cap[suitable_bins_mask] - item + 1e-9))`\n        # `priorities[suitable_bins_mask] = (1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)) + noise`\n        \n        # Let's try to make it directly optimized towards \"goodness\" rather than a combined penalty.\n        # The goal is to select *one* bin.\n        # The \"best\" bin is the one that leaves the least waste.\n        # `waste = bins_remain_cap[suitable_bins_mask] - item`\n        \n        # For diversification, we want to avoid situations where all remaining capacity is clustered.\n        # Consider the variance of `bins_remain_cap`.\n        \n        # A pragmatic heuristic that balances tight fit and diversification:\n        # For each suitable bin, calculate a \"tightness score\":\n        # `tightness = 1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)`\n        # Then, consider a \"diversification score\":\n        # If a bin has a lot of remaining capacity, it's less \"used\".\n        # We want to encourage using bins that are already somewhat filled.\n        # Let's use `(1.0 / (bins_remain_cap[suitable_bins_mask] + 1e-9))` as a proxy for \"how much has been used\".\n        # Higher value means less remaining capacity, thus more used.\n        \n        # Combined score: `tightness + diversification_score_proxy`\n        # `priorities[suitable_bins_mask] = (1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)) + (1.0 / (bins_remain_cap[suitable_bins_mask] + 1e-9))`\n        \n        # This still feels like it might over-prioritize nearly empty bins due to the second term.\n        \n        # Let's consider the advice: \"refining the quality of 'tight fit' assessment by exploring diverse metrics\".\n        # Instead of just `1/gap`, let's use a function that is high for small `gap`, but not too aggressive.\n        # Maybe `(1 - (gap / (gap + small_constant)))` which is `small_constant / (gap + small_constant)`.\n        # This is still an inverse relationship.\n        \n        # Let's focus on the \"neighborhood exploration\" hint.\n        # A good neighborhood explorer for BPP might try to pack an item into the bin that\n        # is \"most full\" among those that can fit it, and then check if this packing\n        # creates a very \"unbalanced\" state in the remaining capacities.\n        \n        # How about prioritizing bins that are \"almost full\" (meaning small `bins_remain_cap`)\n        # *and* leave a small gap after packing.\n        \n        # Let's try a metric that is proportional to tightness and inversely proportional to remaining capacity after packing.\n        # We want to maximize `item / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)`.\n        # This prioritizes bins where the item fills a large fraction of the *remaining* space.\n        \n        # For diversification, we can add a small random perturbation to break ties and explore.\n        \n        # Final proposed heuristic:\n        # For each suitable bin, calculate a score based on:\n        # 1. How tightly the item fits: prioritize smaller remaining capacity after packing.\n        #    Use `1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)`.\n        # 2. Diversification: Encourage using bins that aren't excessively empty.\n        #    Penalize bins with very large remaining capacity.\n        #    Use `- bins_remain_cap[suitable_bins_mask] * weight`.\n        # Add a small random noise to scores for exploration.\n        \n        tightness_score = 1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)\n        diversification_penalty = bins_remain_cap[suitable_bins_mask] * 0.02 # Small penalty for very empty bins\n        \n        scores = tightness_score - diversification_penalty\n        \n        # Add a small random perturbation to the scores to introduce exploration\n        noise = np.random.normal(0, 0.05 * np.mean(scores[scores > -np.inf])) # Scale noise by mean score\n        \n        priorities[suitable_bins_mask] = scores + noise\n        \n        # Handle potential NaN/inf from noise or calculations\n        priorities[np.isinf(priorities)] = np.finfo(float).max # Set inf to very large number\n        priorities[np.isnan(priorities)] = -np.finfo(float).max # Set NaN to very small number\n        \n        # Ensure no negative priorities if not intended, or clip if needed.\n        # For selection, we want the maximum score.\n        \n    else:\n        # If no suitable bins are found, priorities remain zero.\n        pass # The caller will handle creating a new bin if no bin has priority > 0\n\n    # The bin with the highest priority score will be selected.\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 344, in priority_v2\n    # This is related to \"First Fit Decreasing\" logic.\n  File \"numpy/random/mtrand.pyx\", line 1576, in numpy.random.mtrand.RandomState.normal\n  File \"numpy/random/_common.pyx\", line 637, in numpy.random._common.cont\n  File \"numpy/random/_common.pyx\", line 435, in numpy.random._common.check_constraint\nValueError: scale < 0\n17\n2\n"
  },
  {
    "stdout_filepath": "problem_iter3_response3.txt_stdout.txt",
    "code_path": "problem_iter3_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin,\n    aiming for a tighter fit while considering overall bin utilization.\n\n    This heuristic attempts to balance the \"best fit\" idea with a more\n    global view of bin utilization and potential for future packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can fit the item\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        # No suitable bin found, return all zeros\n        return priorities\n\n    # Calculate a \"tightness\" score for suitable bins\n    # We want bins with smaller remaining capacity after placing the item,\n    # but with a penalty for being *too* small if that leads to fragmentation.\n    # Let's consider remaining capacity and the ratio of item size to bin capacity.\n    \n    # Remaining capacity after placing the item\n    remaining_after_placement = bins_remain_cap[suitable_bins_mask] - item\n    \n    # Calculate a score that favors smaller remaining capacity (tighter fit)\n    # We invert the remaining capacity and add a small epsilon to avoid division by zero\n    # We also add a term that penalizes very small remaining capacities if they are\n    # too small to fit common future items, to avoid premature fragmentation.\n    # A simple approach is to use the inverse of remaining capacity.\n    tightness_score = 1.0 / (remaining_after_placement + 1e-6)\n    \n    # A potential diversification factor: consider the relative size of the item\n    # compared to the bin's current capacity. Placing a large item into a bin\n    # that almost fits it might be more valuable than placing a small item.\n    # This can be thought of as a form of \"best fit\" for the current item.\n    relative_fit_score = bins_remain_cap[suitable_bins_mask] / item\n    \n    # Combine scores: prioritize tighter fits (high tightness_score)\n    # and also consider bins where the item fits \"better\" relative to current capacity.\n    # We can use a weighted sum, or a more complex combination.\n    # Let's try to boost bins that have a good fit but still substantial remaining capacity\n    # to avoid creating very nearly empty bins too quickly.\n    \n    # A score that favors bins with a good fit, but not bins that are now almost full\n    # and cannot fit much else.\n    # We can use a function like exp(-x) where x is remaining capacity,\n    # to give higher scores to bins with less remaining capacity.\n    utilization_score = np.exp(-remaining_after_placement / np.mean(bins_remain_cap[suitable_bins_mask]))\n    \n    # Let's combine tightness and utilization in a way that gives a higher priority\n    # to bins with small remaining capacity *after* placement, but not zero.\n    # The idea is to make bins as full as possible without overflowing.\n    # A simple approach is to use the inverse of remaining capacity.\n    \n    # For bins that can fit the item, we want to prioritize those that will have\n    # the least remaining capacity after the item is placed.\n    # This is the core of \"Best Fit\".\n    \n    # Let's use a score that is the negative of the remaining capacity after placement.\n    # Higher score means smaller remaining capacity (better fit).\n    scores = -remaining_after_placement\n    \n    # To add some diversification and avoid always picking the absolute tightest,\n    # we can add a small random perturbation, or consider other factors.\n    # However, for a priority function, deterministic is usually preferred.\n    \n    # A slight modification to \"Best Fit\" could be to prioritize bins that\n    # are already relatively full. This can be captured by considering the inverse\n    # of the current remaining capacity.\n    inverse_current_capacity_score = 1.0 / (bins_remain_cap[suitable_bins_mask] + 1e-6)\n    \n    # Let's try a combination that prioritizes tight fits and bins that are generally fuller.\n    # We want to maximize -(remaining_after_placement).\n    # Let's also boost bins that have less remaining capacity *before* placing the item.\n    # This is equivalent to prioritizing bins that are already quite full.\n    \n    # Score: prioritize bins with the smallest remaining capacity AFTER placing the item.\n    # This is the Best Fit criteria.\n    \n    # Let's construct a score that's higher for bins with smaller remaining capacity\n    # after the item is placed.\n    # For example, we can use a measure related to how \"full\" the bin will be.\n    # If a bin has capacity C and we place item I, it will have C-I remaining.\n    # We want C-I to be minimal.\n    \n    # Let's calculate a priority based on the remaining capacity after placement.\n    # We want to maximize the negative of the remaining capacity.\n    \n    # A potential improvement could be to look at the \"gaps\" created.\n    # A bin that is almost full and then has an item placed, creating a very small gap,\n    # is generally good.\n    \n    # Let's define the priority as the negative of the remaining capacity after placement.\n    # Higher priority for smaller remaining capacity.\n    priorities[suitable_bins_mask] = -remaining_after_placement\n    \n    # To encourage filling bins more generally, we can add a term proportional\n    # to how \"full\" the bin is.\n    # This could be `bins_remain_cap[suitable_bins_mask]`.\n    # However, this might counteract the tightest fit.\n    \n    # Let's consider a score that is high when `remaining_after_placement` is small.\n    # A simple inverted relationship: `1 / (remaining_after_placement + epsilon)`\n    # This favors bins where the item leaves the least space.\n    \n    # Let's try to combine the Best Fit idea with a penalty for creating very small gaps.\n    # Instead of `1 / (remaining_after_placement)`, which can be very large for small remaining capacity,\n    # let's use a score that is high for small remaining capacity.\n    \n    # A simple, robust approach that favors \"tightness\":\n    # Assign a score that is the negative of the remaining capacity after placement.\n    # This means bins that are nearly full after placement get higher scores.\n    priorities[suitable_bins_mask] = -remaining_after_placement\n    \n    # To add a slight diversification or consideration of overall bin state,\n    # we could also consider the inverse of the current remaining capacity.\n    # This would boost bins that are already quite full.\n    # However, this might conflict with pure Best Fit.\n    \n    # Let's stick to refining the \"tight fit\" aspect.\n    # The current `priorities[suitable_bins_mask] = -remaining_after_placement`\n    # IS the Best Fit heuristic.\n\n    # To \"think outside the box\" and improve upon simple Best Fit:\n    # Consider a metric that penalizes creating small, unusable gaps more explicitly.\n    # For instance, if remaining_after_placement is very small (e.g., < 0.1 * bin_capacity),\n    # perhaps we want to slightly de-prioritize it if there's another bin that fits nearly as well.\n    \n    # Let's try to balance \"tightness\" with \"avoiding fragmentation into tiny spaces\".\n    # We can assign a score that is high for small `remaining_after_placement`,\n    # but then apply a decreasing function to this score as `remaining_after_placement` gets even smaller.\n    \n    # Score = f(remaining_after_placement) where f is decreasing.\n    # Example: f(x) = 1 / (x + epsilon). This is what we've essentially explored.\n    \n    # Alternative thought: What if we penalize bins that are *almost* full,\n    # if the item itself is small relative to the bin capacity?\n    # This is getting complicated for a simple priority function.\n\n    # Let's go back to the core of improving \"tight fit assessment\" and local search concepts.\n    # In local search, we explore neighborhoods. Here, we are defining a greedy choice.\n    # The \"neighborhood\" is implicitly the set of suitable bins.\n    \n    # Consider a metric that is sensitive to the *ratio* of remaining capacity to original capacity.\n    # Or, how much of the remaining capacity is being used by this item.\n    \n    # Let's try a score that is higher for bins that have small remaining capacity\n    # AFTER placement, but with a slight penalty for becoming *too* full if it means\n    # the bin can't accommodate future items of moderate size.\n    \n    # If `remaining_after_placement` is very small, the bin is almost full.\n    # We want to reward this, but maybe not excessively if it creates a tiny leftover space.\n    \n    # Let's try this score:\n    # Higher score = more preferred bin.\n    # We want to minimize `remaining_after_placement`.\n    # So, a higher priority should come from smaller `remaining_after_placement`.\n    \n    # Priority = BaseScore - PenaltyForRemainingCapacity\n    # BaseScore could be related to how \"full\" the bin is.\n    \n    # Let's try:\n    # Score = (bin_capacity - item) - alpha * (bin_capacity - item)^2\n    # This penalizes very small remaining capacities.\n    # No, we want to *reward* small remaining capacities.\n\n    # Let's try to boost bins that, after placing the item, leave a relatively small gap,\n    # but not a gap so small that it's almost useless.\n    \n    # Consider `remaining_after_placement`. We want this to be small.\n    # If `remaining_after_placement` is close to 0, it's good.\n    # If `remaining_after_placement` is very large, it's bad.\n    \n    # Let's try a score that is higher for smaller `remaining_after_placement`.\n    # And let's try to add a factor that considers how \"full\" the bin becomes.\n    \n    # A balanced approach: Prioritize bins that, after placing the item,\n    # have a small remaining capacity, but also ensure that we don't\n    # create bins that are *extremely* full if there are alternatives.\n    \n    # Let's reconsider the inverse: `1.0 / (remaining_after_placement + epsilon)`.\n    # This is a good start for \"tight fit\".\n    \n    # To improve this, let's consider a term that captures the \"waste\" created.\n    # Waste = `remaining_after_placement`. We want to minimize this.\n    \n    # Let's modify the inverse relationship.\n    # Instead of `1/x`, maybe `log(1+1/x)` or something similar.\n    \n    # A more structured approach inspired by local search neighborhood exploration:\n    # We can think of different \"types\" of fits.\n    # 1. Perfect fit: remaining_after_placement == 0. Highest priority.\n    # 2. Tight fit: remaining_after_placement is small. High priority.\n    # 3. Moderate fit: remaining_after_placement is moderate. Medium priority.\n    \n    # How to quantify \"small\" vs \"moderate\"? Relative to the item size or bin capacity.\n    \n    # Let's try a scoring function based on the negative remaining capacity,\n    # but then apply a non-linear transformation to emphasize smaller values.\n    \n    # Let `r = remaining_after_placement`. We want to maximize a function `f(r)` that decreases with `r`.\n    # Simple `f(r) = -r`.\n    # How about `f(r) = -r^2`? This penalizes larger `r` more, and favors very small `r`.\n    # This is similar to `1/r` in terms of favoring small `r`.\n    \n    # Let's try a score that is higher for smaller remaining capacity.\n    # `score = (bin_capacity_after_placement + 1) / (bin_capacity_after_placement + epsilon)`\n    # where bin_capacity_after_placement = bins_remain_cap[suitable_bins_mask] - item\n    # This score is always > 1 and approaches 1 as remaining capacity increases.\n    # So, a higher score means a smaller remaining capacity.\n    \n    # Let's use this:\n    score_for_suitable = (bins_remain_cap[suitable_bins_mask] + 1) / (remaining_after_placement + 1e-6)\n    \n    # This score is high when remaining_after_placement is small.\n    # Example:\n    # Bin capacity: 10, Item: 7. Remaining after: 3. Score = (10+1)/(3+eps) = 11/3 = 3.67\n    # Bin capacity: 10, Item: 9. Remaining after: 1. Score = (10+1)/(1+eps) = 11/1 = 11\n    # Bin capacity: 10, Item: 5. Remaining after: 5. Score = (10+1)/(5+eps) = 11/5 = 2.2\n    \n    # This looks promising. It favors tighter fits.\n    # Now, consider the \"diversification\" or \"avoiding fragmentation\" aspect.\n    # If `remaining_after_placement` is very small, the bin is almost full.\n    # This could be good, but if it's *too* small, it might be hard to fit future items.\n    \n    # Let's try to slightly penalize extremely small `remaining_after_placement`.\n    # If `remaining_after_placement < threshold`, reduce the score.\n    \n    # Let's combine the previous score with a penalty if the bin becomes excessively full.\n    # If `remaining_after_placement` is very small relative to the *original* bin capacity,\n    # we might want to slightly reduce its priority.\n    \n    # Let's try a score based on `remaining_after_placement` and also the `original_bin_capacity`.\n    \n    # New score idea:\n    # We want to minimize `remaining_after_placement`.\n    # Let's use a score where higher is better.\n    # Score = `f(remaining_after_placement)` where `f` is a decreasing function.\n    # We want `f` to be steep for small `r` and shallower for larger `r`.\n    \n    # Let's use `f(r) = exp(-r / average_remaining_capacity)`. This is like utilization.\n    # Or `f(r) = 1 / (r^2 + epsilon)` for a stronger emphasis on small `r`.\n    \n    # Let's try a composite score:\n    # 1. Prioritize tight fit: `1.0 / (remaining_after_placement + epsilon)`\n    # 2. Consider overall bin fullness: `1.0 / (bins_remain_cap[suitable_bins_mask] + epsilon)`\n    #    This would prefer bins that are already more full.\n    \n    # Let's combine these two:\n    # `priority_score = w1 * (1.0 / (remaining_after_placement + epsilon)) + w2 * (1.0 / (bins_remain_cap[suitable_bins_mask] + epsilon))`\n    # where w1 and w2 are weights.\n    \n    # A simpler approach to encourage tighter fits without extreme penalization of small gaps:\n    # Use the negative of the remaining capacity, but then \"clip\" the highest scores.\n    # Or, map the remaining capacity to a priority using a function that is steep at small values.\n    \n    # Consider the function `f(x) = exp(-x)` where x is `remaining_after_placement`.\n    # This gives high scores for small x.\n    # `priorities[suitable_bins_mask] = np.exp(-remaining_after_placement / np.mean(bins_remain_cap[suitable_bins_mask]))`\n    # This normalizes the remaining capacity by the average.\n    \n    # Let's go with a score that is higher for smaller `remaining_after_placement`,\n    # but also considers the overall \"emptiness\" of the bin.\n    # A bin that is nearly full and has a tight fit is good.\n    \n    # Let's use the negative of the remaining capacity for Best Fit.\n    # Then, let's add a term that penalizes leaving a very small gap IF the bin was already quite full.\n    \n    # This suggests a multi-objective optimization, which is hard for a simple priority function.\n    \n    # Let's refine the `(capacity + 1) / (remaining + epsilon)` idea.\n    # This favors small `remaining_after_placement`.\n    # Let's call `remaining_after_placement` as `gap`.\n    # Score = `(bin_cap - item + 1) / (gap + epsilon)`\n    \n    # How to add diversification or avoid extreme fits?\n    # If `gap` is very small (e.g., `gap < 0.05 * original_bin_cap`), perhaps reduce the score.\n    \n    # Let's define a \"goodness\" metric:\n    # Higher means better.\n    # We want `gap` to be small.\n    \n    # Consider the reciprocal of the gap: `1/gap`.\n    # To avoid infinities, `1/(gap + epsilon)`.\n    \n    # Let's try a score that emphasizes small gaps, but also considers the overall size of the bin.\n    # A bin that is larger and filled tightly might be preferred over a smaller bin filled equally tightly.\n    \n    # Score = (bin_capacity - item + epsilon) / item\n    # This is the inverse of \"how much space is left relative to the item size\".\n    # If item is 9, capacity is 10, remaining is 1. Score = (10-9+eps)/9 = 1/9. Low score.\n    # If item is 5, capacity is 10, remaining is 5. Score = (10-5+eps)/5 = 5/5 = 1. Higher score.\n    # This is not quite right.\n    \n    # Let's go back to the negative remaining capacity for Best Fit.\n    # `priorities[suitable_bins_mask] = -remaining_after_placement`\n    \n    # To improve \"tightness assessment\", we can normalize or transform this.\n    # A common technique is to use the inverse of the remaining capacity.\n    \n    # `priorities[suitable_bins_mask] = 1.0 / (remaining_after_placement + 1e-6)`\n    # This gives very high scores for very small remaining capacities.\n    \n    # To avoid extreme values and maybe introduce a bit of a \"smoothing\" or\n    # \"avoid extreme fragmentation\" effect, let's consider a transformation of the gap.\n    \n    # Try this: Score = `log(1 + 1 / (remaining_after_placement + epsilon))`\n    # This grows slower than `1/x`.\n    \n    # Or, `score = sqrt(1 / (remaining_after_placement + epsilon))`\n    \n    # Let's combine \"tight fit\" with a measure of \"how much of the bin is utilized\".\n    # A bin that is already mostly full and can accommodate the item tightly is good.\n    \n    # Let's consider `remaining_after_placement`. We want this to be small.\n    # Let's use a score that is higher for smaller remaining capacity,\n    # but also considers how \"full\" the bin is.\n    \n    # Score = `(current_bin_capacity - item) - alpha * (current_bin_capacity - item)^2`\n    # This is not right.\n    \n    # Let's try to prioritize bins that, after placing the item, have the smallest remaining capacity.\n    # This is the Best Fit heuristic.\n    # `priorities[suitable_bins_mask] = -remaining_after_placement`\n    \n    # To make it \"better\" or \"think outside the box\":\n    # We can introduce a non-linearity to the \"tightness\".\n    # Instead of `-x`, let's use `-x^2` or `1/(x+epsilon)`.\n    # Using `1.0 / (remaining_after_placement + 1e-6)` favors very small remaining capacities strongly.\n    \n    # Let's consider the ratio of the item size to the bin's current capacity.\n    # `item / bins_remain_cap[suitable_bins_mask]`\n    # High values here mean the item is large relative to the bin.\n    # This is related to \"First Fit Decreasing\" logic.\n    \n    # Let's try to combine Best Fit with a measure that encourages fuller bins.\n    # Score = `-(remaining_after_placement)` + `lambda * (bin_capacity - remaining_after_placement)`\n    # where `bin_capacity` is the initial remaining capacity of that bin.\n    # `lambda` is a weighting factor.\n    # The second term encourages filling the bin more.\n    \n    # Let's use a score that is high when `remaining_after_placement` is small.\n    # We can use the negative of `remaining_after_placement`.\n    # `priorities[suitable_bins_mask] = -remaining_after_placement`\n    \n    # To add diversification, or to smooth the preference, we can add a small random noise.\n    # But this is usually not preferred for deterministic heuristics.\n    \n    # Let's focus on improving the \"tight fit\" metric without adding randomness.\n    # The advice mentions \"neighborhood exploration\" and \"diversification\".\n    # For a greedy priority function, this translates to how we define the \"best\" neighbor (bin).\n    \n    # Consider the \"waste\" produced by a bin: `remaining_after_placement`.\n    # We want to minimize waste.\n    \n    # Let's try a score that is higher for smaller waste, but with diminishing returns as waste gets very small.\n    # This is to avoid making a bin that's almost completely full have *drastically* higher priority\n    # than a bin that is just slightly less full, if that leads to very awkward residual spaces.\n    \n    # Function `g(waste)`: we want `g(waste)` to be decreasing and steep for small `waste`.\n    # `g(waste) = 1 / (waste + epsilon)` is a good candidate.\n    \n    # Let's try a slight modification:\n    # Score = `(current_bin_capacity - item)`\n    # Higher score for smaller remaining capacity.\n    \n    # Let's consider the problem statement again: \"refining the quality of 'tight fit' assessment by exploring diverse metrics beyond simple inverse relationships.\"\n    # \"Consider how different neighborhood structures in local search can expose novel packing solutions.\"\n    \n    # This implies we might want to look at properties of the *bin itself* or the *item itself* in relation to the bin.\n    \n    # Let's try a score that is inversely proportional to the remaining capacity,\n    # but also considers how much of the bin is used by the current item.\n    \n    # Score = `(bin_capacity_after_placement) / (item_size)`\n    # Higher score means smaller remaining capacity relative to item size.\n    # `score = (bins_remain_cap[suitable_bins_mask] - item) / item`\n    # Example: Bin 10, Item 7. Rem_after = 3. Score = 3/7.\n    # Example: Bin 10, Item 9. Rem_after = 1. Score = 1/9.\n    # This favors larger *absolute* remaining capacities, which is the opposite of Best Fit.\n    \n    # Let's flip it: `item / (bin_capacity_after_placement + epsilon)`\n    # Example: Bin 10, Item 7. Rem_after = 3. Score = 7/3 = 2.33.\n    # Example: Bin 10, Item 9. Rem_after = 1. Score = 9/1 = 9.\n    # This favors smaller absolute remaining capacities. This is better.\n    \n    # Let's call `remaining_after_placement` as `residual_capacity`.\n    # Score = `item / (residual_capacity + epsilon)`\n    \n    # This score is high when `residual_capacity` is small, and also when `item` is large.\n    # This means we prefer bins where the item fills it up a lot, leaving little space.\n    \n    # Let's compare `1.0 / (residual_capacity + epsilon)` and `item / (residual_capacity + epsilon)`.\n    # The first one is pure Best Fit.\n    # The second one adds the item size.\n    # If we have Bin A (cap 10, rem 1) and Bin B (cap 20, rem 1) and item is 5:\n    # Bin A: residual=9, item=5. Score=5/9.\n    # Bin B: residual=15, item=5. Score=5/15.\n    # This favors Bin A, which has less absolute remaining capacity.\n    \n    # If we have Bin A (cap 10, rem 1) and Bin B (cap 20, rem 1) and item is 15:\n    # Bin A: residual= -5 (not suitable)\n    # Bin B: residual = 5, item = 15. Score = 15/5 = 3.\n    \n    # Let's go back to the negative remaining capacity as the base Best Fit.\n    # `priorities[suitable_bins_mask] = -remaining_after_placement`\n    \n    # To refine this: Instead of just the residual, consider the ratio of residual to current capacity.\n    # `ratio = remaining_after_placement / bins_remain_cap[suitable_bins_mask]`\n    # We want this ratio to be small. So, we want to maximize `-ratio`.\n    \n    # `priorities[suitable_bins_mask] = -(remaining_after_placement / (bins_remain_cap[suitable_bins_mask] + 1e-6))`\n    # Example:\n    # Bin 10, Item 7. Rem_after = 3. Ratio = 3/10 = 0.3. Score = -0.3.\n    # Bin 10, Item 9. Rem_after = 1. Ratio = 1/10 = 0.1. Score = -0.1.\n    # This favors Bin 10 (item 9) as it has a smaller relative residual. This is good.\n    \n    # Let's consider the advice: \"exploring diverse metrics beyond simple inverse relationships\"\n    # and \"local search, neighborhood exploration, diversification\".\n    \n    # The current priority function determines which bin is the \"best neighbor\" in a greedy sense.\n    # The \"neighborhood\" is the set of suitable bins.\n    \n    # A metric that might be useful: \"how much of the remaining capacity is 'wasted' by this item?\"\n    # Waste_per_unit_capacity = `remaining_after_placement / bins_remain_cap[suitable_bins_mask]`\n    # We want to minimize this. So, priority = - Waste_per_unit_capacity.\n    \n    priorities[suitable_bins_mask] = -(remaining_after_placement / (bins_remain_cap[suitable_bins_mask] + 1e-6))\n    \n    # This heuristic tries to find bins where the item fits snugly,\n    # meaning the remaining capacity is small relative to the bin's original capacity.\n    # This encourages fuller bins and potentially fewer bins overall.\n    # It's a refinement of Best Fit, looking at the relative waste.\n    \n    # To incorporate \"diversification\" or \"avoiding poor local optima\":\n    # For a greedy heuristic, this often means adding a small random factor, or\n    # using a meta-heuristic. But for just the priority function, we want a robust deterministic score.\n    \n    # Let's consider the \"avoid neglecting the impact of initial solutions; consider a diversification strategy for starting points.\"\n    # This is more for metaheuristics. For a priority function, it means the function itself should\n    # guide towards good solutions.\n    \n    # What if we also consider the \"cost\" of placing an item in a bin that is already very full?\n    # If a bin has very little capacity left, and we place an item there, even if it fits tightly,\n    # it might prevent future, smaller items from being packed efficiently.\n    \n    # Let's refine the score:\n    # We want `remaining_after_placement` to be small.\n    # Let's consider the score `1.0 / (remaining_after_placement + epsilon)` again.\n    # This gives high values for small remaining capacities.\n    \n    # To diversify or add a \"local search\" flavor (conceptually):\n    # Think about how different packing strategies affect future options.\n    # A \"tight fit\" might be good, but if it leaves a very awkward small gap, that might be bad.\n    \n    # Let's consider a score based on the \"gap\" left:\n    # `gap = remaining_after_placement`\n    # We want small `gap`.\n    # Score = `1 / (gap + epsilon)`\n    \n    # Now, how to differentiate between `gap = 0.1` and `gap = 0.01`?\n    # `1/0.1 = 10`, `1/0.01 = 100`. The difference is significant.\n    \n    # What if we add a term related to the *item size* and the *original bin capacity*?\n    # Consider the \"percentage fill\" of the bin if the item is placed.\n    # `fill_percentage = (bins_remain_cap[suitable_bins_mask] - remaining_after_placement) / bins_remain_cap[suitable_bins_mask]`\n    # We want this to be high. So, `fill_percentage` itself can be a priority.\n    \n    # `priorities[suitable_bins_mask] = (bins_remain_cap[suitable_bins_mask] - item) / (bins_remain_cap[suitable_bins_mask] + 1e-6)`\n    # This is `remaining_after_placement / bins_remain_cap[suitable_bins_mask]`.\n    # We want this to be SMALL. So, priority = - this ratio.\n    \n    # This is what we had before: `priorities[suitable_bins_mask] = -(remaining_after_placement / (bins_remain_cap[suitable_bins_mask] + 1e-6))`\n    \n    # Let's try a score that combines \"tight fit\" (low residual) and \"good utilization\" (high fill percentage).\n    \n    # Score = `k1 * (1.0 / (remaining_after_placement + epsilon)) + k2 * (item / (bins_remain_cap[suitable_bins_mask] + epsilon))`\n    # The second term `item / bins_remain_cap[suitable_bins_mask]` is the fill percentage if item is placed.\n    # We want to maximize both.\n    \n    # Let's try a score that is maximized when `remaining_after_placement` is small.\n    # `score = 1.0 / (remaining_after_placement + epsilon)`\n    \n    # To improve \"tightness assessment\" and incorporate \"local search thinking\":\n    # Consider a penalty for creating bins that are now *almost full* (very small remaining capacity).\n    # If `remaining_after_placement` is very small, its reciprocal is very large.\n    # We can apply a function that grows less steeply for very small values.\n    \n    # Example: `f(x) = 1 / (x + epsilon)`.\n    # `f(0.1) = 10`\n    # `f(0.01) = 100`\n    # `f(0.001) = 1000`\n    \n    # Consider `f(x) = sqrt(1 / (x + epsilon))`.\n    # `f(0.1) = sqrt(10) approx 3.16`\n    # `f(0.01) = sqrt(100) = 10`\n    # `f(0.001) = sqrt(1000) approx 31.6`\n    # This is still very steep.\n    \n    # What if we modify the score for bins that are *too* full?\n    # If `remaining_after_placement < some_small_threshold`:\n    #   `score = score_from_before - penalty_for_being_too_full`\n    \n    # Let's try a score that is the negative of the remaining capacity, but then we transform it.\n    # `score = -remaining_after_placement`\n    \n    # To favor tighter fits, we can use `score = -remaining_after_placement^2`.\n    # This penalizes larger remaining capacities more.\n    \n    # Let's try a score that combines tight fit with a consideration of the item size relative to the bin.\n    # A tight fit for a large item is generally better than a tight fit for a small item,\n    # in terms of overall utilization.\n    \n    # Score = `(item_size / bins_remain_cap[suitable_bins_mask]) * (1.0 / (remaining_after_placement + epsilon))`\n    # This means: higher fill ratio AND smaller residual capacity.\n    \n    # Let's refine the \"tight fit\" and \"diversification\" aspects.\n    # The advice suggests looking beyond simple inverse relationships.\n    # Consider the \"gap\" `g = remaining_after_placement`.\n    # We want small `g`.\n    # A score could be `1/(g + epsilon)`.\n    \n    # To avoid extreme values, let's map `g` to a priority using a function that's steep for small `g`.\n    # Consider the reciprocal of the gap, but capped at some maximum value to avoid extreme priorities.\n    # Or, use a function like `tanh(k/g)` or `log(1 + k/g)`.\n    \n    # Let's try `log(1 + item / (remaining_after_placement + epsilon))`.\n    # This gives higher scores for larger items that fit tightly.\n    \n    # Let's consider the score `(current_bin_capacity - item) / current_bin_capacity`. We want this to be small.\n    # So, priority = `- (current_bin_capacity - item) / current_bin_capacity`.\n    # This is `-(1 - item / current_bin_capacity)`.\n    # This is `item / current_bin_capacity - 1`.\n    # This prioritizes bins where the item is a large fraction of the bin's remaining capacity.\n    \n    # Let's use this:\n    # `priorities[suitable_bins_mask] = item / (bins_remain_cap[suitable_bins_mask] + 1e-6)`\n    # This rewards filling bins more.\n    \n    # Let's try to combine this with the tightest fit idea.\n    # The tightest fit is when `remaining_after_placement` is minimal.\n    # `priorities[suitable_bins_mask] = -remaining_after_placement`\n    \n    # Let's combine these two ideas: prioritize small remaining capacity,\n    # and also prioritize bins that are more \"filled\" by this item.\n    \n    # A robust approach for \"tight fit\": prioritize bins where the remaining capacity after placement is minimal.\n    # This is achieved by maximizing `-remaining_after_placement`.\n    \n    # To differentiate from simple Best Fit, let's consider the \"quality\" of the fit in relation to the bin's size.\n    # A fit that leaves 1 unit remaining in a bin of capacity 10 (residual ratio 0.1) might be better than\n    # a fit that leaves 1 unit in a bin of capacity 20 (residual ratio 0.05).\n    # However, the advice is about \"diverse metrics beyond simple inverse relationships\".\n    \n    # Let's try a score that penalizes bins that are already almost full, if the item being placed is small.\n    # This is to prevent creating many bins that are very nearly full but can't fit anything else.\n    \n    # Consider the score: `remaining_after_placement`. We want this to be minimized.\n    # Let's try to add a penalty if `remaining_after_placement` is very small AND `bins_remain_cap[suitable_bins_mask]` is large.\n    # This is getting complex.\n    \n    # Let's stick to a clear improvement on Best Fit that incorporates \"tightness\".\n    # The core idea is to give higher priority to bins that, after placing the item,\n    # have the least remaining capacity.\n    \n    # Simple Best Fit: `priority = -remaining_after_placement`\n    \n    # A refined version: `priority = 1.0 / (remaining_after_placement + epsilon)`\n    # This gives stronger preference to very tight fits.\n    \n    # Let's consider the prompt's advice: \"refining the quality of 'tight fit' assessment by exploring diverse metrics beyond simple inverse relationships.\"\n    # \"Consider how different neighborhood structures in local search can expose novel packing solutions.\"\n    \n    # The advice implies we might want to look at properties of the bins or items that aren't just about the residual space.\n    \n    # Let's try a score that is higher if the item fills a larger proportion of the bin's *current* capacity.\n    # Score = `item / bins_remain_cap[suitable_bins_mask]`\n    \n    # Let's combine this with the tightest fit:\n    # `priority_score = (item / bins_remain_cap[suitable_bins_mask]) * (1.0 / (remaining_after_placement + epsilon))`\n    \n    # This score is high when:\n    # 1. The item is large relative to the bin's current capacity.\n    # 2. The remaining capacity after placement is very small.\n    \n    # Example:\n    # Bin A: cap=10, item=7. Rem_after=3. Fill ratio = 7/10. Score = (7/10) * (1/3) = 0.7 * 0.333 = 0.233\n    # Bin B: cap=10, item=9. Rem_after=1. Fill ratio = 9/10. Score = (9/10) * (1/1) = 0.9 * 1 = 0.9\n    # Bin C: cap=20, item=18. Rem_after=2. Fill ratio = 18/20. Score = (18/20) * (1/2) = 0.9 * 0.5 = 0.45\n    \n    # This heuristic favors bins where the item takes up a large proportion of the bin, AND leaves little space.\n    # It's a form of \"best fit\" that considers the item's impact more explicitly.\n    \n    priorities[suitable_bins_mask] = (item / (bins_remain_cap[suitable_bins_mask] + 1e-6)) * (1.0 / (remaining_after_placement + 1e-6))\n    \n    # This seems like a reasonable \"outside the box\" improvement on pure Best Fit,\n    # as it combines the \"fill ratio\" of the item with the \"tightness\" of the fit.\n    # It's a weighted Best Fit, where the weight is the fill ratio.\n    \n    # To ensure it's always positive and reflects preference, higher values are better.\n    # The current calculation already ensures higher values for better fits.\n    \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 17.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response4.txt_stdout.txt",
    "code_path": "problem_iter3_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin,\n    aiming for a tighter fit and better overall utilization, inspired by\n    local search neighborhood exploration and diversification.\n\n    This heuristic prioritizes bins that offer a tight fit, but also\n    considers bins that might be less full but offer future flexibility\n    or create \"adjacent\" spaces. It uses a combination of remaining\n    capacity and a measure of \"waste\" to guide the selection.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities  # No suitable bins\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Tightness of fit (prioritize bins where item fits snugly)\n    # Lower value of (bin_capacity - item) is better\n    tightness_score = suitable_bins_cap - item\n\n    # Metric 2: Future utilization (prioritize bins that are not overly full,\n    # leaving some space for future items, but not too empty either)\n    # This is a bit more exploratory. We can introduce a slight penalty for\n    # very large remaining capacities, encouraging filling up bins more.\n    # A simple inverse relationship could be problematic, so let's use a\n    # capped approach or a \"sweet spot\" concept.\n    # For simplicity here, let's consider bins that are not too empty,\n    # but also not too full.\n    # Let's use a score that favors remaining capacity that is closer to 'item'\n    # but not excessively small.\n    # A Gaussian-like function centered around 'item' or slightly larger might work.\n    # Here, a simple inverse of remaining capacity (higher score for less remaining)\n    # can be used as a baseline for utilization, but let's combine it.\n\n    # Let's try a more robust approach for \"future utilization\" by considering\n    # how much space is left *after* placing the item. We want to minimize\n    # the remaining space in a bin, but not to the extreme of making it\n    # unusable for small items.\n    # A measure of \"potential waste\" if this bin is chosen.\n    # Prioritize bins that, after placing the item, leave a small but usable remainder.\n    # A simple inverse of (remaining_capacity - item) could be too aggressive.\n    # Let's try a score that is high for small remaining capacity, and decreases\n    # as remaining capacity increases. A function like 1 / (1 + (capacity - item))\n    # or a capped inverse.\n\n    # Let's define a score that favors tighter fits more strongly.\n    # Higher score means more desirable.\n    # We can combine tightness with a measure of how \"useful\" the remaining space is.\n\n    # Option 1: Prioritize bins that leave the *least* amount of space after packing.\n    # This is essentially Best Fit.\n    # We want to give higher priority to bins where (bin_remain_cap - item) is minimal.\n    # So, inverse of (bin_remain_cap - item) plus a small epsilon to avoid division by zero.\n    # Let's invert the tightness_score: higher means better fit.\n    # We can use 1 / (1 + tightness_score) so that tightness_score=0 gets highest priority.\n    fit_priority = 1.0 / (1.0 + tightness_score)\n\n    # Option 2: Consider overall bin fullness. Bins that are already quite full\n    # might be preferable to consolidate items.\n    # This can be measured by (bin_capacity - bins_remain_cap) / bin_capacity.\n    # Let's assume a standard bin capacity for comparison, or use the item size.\n    # A simpler approach might be to penalize very empty bins.\n    # Let's consider a score based on how \"full\" the bin becomes *after* packing.\n    # Higher score for bins that are more full after packing.\n    # This is (total_capacity - (bin_remain_cap - item)) / total_capacity.\n    # For simplicity in an online setting without knowing total capacity, we can\n    # infer a \"target fullness\" or use the item size as a reference for the remaining space.\n\n    # Let's refine `fit_priority` to also consider the \"quality\" of the remaining space.\n    # We want small remaining space (tight fit), but if the remaining space is *too* small,\n    # it might be less useful for subsequent items.\n    # So, we prefer remaining space that is small but still substantial enough to fit\n    # at least some smaller items.\n\n    # Let's try a combined score:\n    # - Primary: Tightness (low (bin_cap - item))\n    # - Secondary: Avoid extremely large remaining capacities (encourages consolidation)\n\n    # The tightness score directly maps to how \"good\" the fit is for the current item.\n    # We want to maximize this. So, higher values of (1 / (1 + tightness_score)) are better.\n\n    # Let's introduce a small diversification aspect or a secondary objective.\n    # For instance, if multiple bins offer a very tight fit, we might break ties\n    # by picking the bin that was already less full, to spread the load slightly.\n    # Or, conversely, pick the one that is already more full to encourage consolidation.\n\n    # Let's stick to the primary goal of tight fit, and for tie-breaking,\n    # we can consider how much \"waste\" is generated.\n    # The waste is (bin_remain_cap - item). We want to minimize this.\n    # So, for bins with the same tightness score, we might prefer one that\n    # has a slightly larger remaining capacity (meaning it was less full to begin with).\n    # This is counter-intuitive to \"tight fit\" but could be a diversification.\n\n    # Let's focus on the \"tight fit\" and \"useful remaining space\" idea.\n    # Score = tightness_value + some_function_of_remaining_space.\n    # We want to minimize (remaining_capacity - item).\n    # So, let's consider the inverse of that as a priority.\n    # priority = 1 / (epsilon + (bin_remain_cap - item))\n\n    # Let's try a more direct heuristic that is known to be effective:\n    # Prioritize bins that leave the *smallest* remaining capacity after packing.\n    # This is Best Fit. The priority is inversely proportional to (bin_remain_cap - item).\n    # We want to maximize priority for bins with minimum (bin_remain_cap - item).\n    # So, priority = C - (bin_remain_cap - item), where C is a large constant.\n    # Or simply, priority = -(bin_remain_cap - item).\n    # The higher the priority value, the better.\n\n    # Let's re-think the prompt's advice: \"refining the quality of 'tight fit'\n    # assessment by exploring diverse metrics beyond simple inverse relationships.\n    # Consider how different neighborhood structures in local search can expose\n    # novel packing solutions.\"\n\n    # \"Diverse metrics beyond simple inverse relationships\": Instead of just\n    # (bin_remain_cap - item), let's also consider how \"close\" bin_remain_cap\n    # is to a \"good\" size for the item, perhaps slightly larger than the item.\n    # This is related to the \"sweet spot\" idea.\n\n    # Let's try a score that has two components:\n    # 1. Penalty for not fitting: Very low score if item doesn't fit.\n    # 2. Reward for tight fit: Higher score for smaller (bin_remain_cap - item).\n    # 3. Reward for leaving a \"useful\" remainder: Not too much, not too little.\n\n    # Let's define a \"desirability\" score for each suitable bin.\n    # A good bin is one that is nearly full but still accommodates the item.\n    # We want to maximize the desirability.\n    # Desirability_score = 1 / (1 + (bin_remain_cap - item))  # Reward for small remainder\n\n    # To avoid bins that become *too* full and might be useless for future items,\n    # we can add a penalty for bins that are already very full.\n    # Or, focus on the remaining capacity being \"just right\".\n    # A simple way is to penalize very large remainders.\n\n    # Let's create a score that is high for small positive (bin_remain_cap - item)\n    # and decreases as (bin_remain_cap - item) increases.\n    # For a bin to be suitable, bin_remain_cap >= item.\n    # The \"waste\" or \"slack\" is `slack = bin_remain_cap - item`.\n    # We want to minimize `slack`.\n\n    # A primary priority could be `1 / (1 + slack)` for suitable bins.\n    # This gives highest priority to slack = 0.\n\n    # Now, let's consider the \"neighborhood exploration\" idea.\n    # What if we also give a slight bonus to bins that are not *completely* full,\n    # or that have a remaining capacity that is a \"good\" size for common item sizes?\n    # This is hard to define without knowing future items.\n\n    # Let's try a heuristic that balances tight fit with not making bins \"too full\".\n    # For a suitable bin, we want to minimize `slack = bin_remain_cap - item`.\n    # However, if `slack` is very small, it might be hard to fit other items.\n    # So, we want `slack` to be small, but not zero.\n\n    # Let's consider the remaining capacity *after* placing the item.\n    # `post_placement_remain_cap = bin_remain_cap - item`.\n    # We want `post_placement_remain_cap` to be small.\n    # However, if `post_placement_remain_cap` is too small (e.g., < item/2), it might be undesirable.\n    # This hints at a \"sweet spot\" for the remaining capacity.\n\n    # Let's try a score that is high for `post_placement_remain_cap` close to 0,\n    # but also consider bins where `post_placement_remain_cap` is moderately sized.\n    # This can be achieved by a quadratic penalty or a function that has a peak.\n\n    # Example: Score = 1 - (slack / bin_capacity)  -- aims for high utilization.\n    # Or, Score = 1 / (1 + slack) -- Best Fit.\n\n    # Let's try a heuristic that combines Best Fit with a penalty for\n    # extremely low remaining capacity *after* packing.\n    # The \"ideal\" remaining capacity after packing might be something like\n    # the size of a small item.\n\n    # For each suitable bin:\n    # Calculate slack = bin_remain_cap - item.\n    # Calculate a \"fit score\": Higher for smaller slack.\n    # Let's use `1.0 / (1.0 + slack)` as the primary component.\n\n    # Now, let's introduce a secondary component that penalizes very small slacks\n    # (making the bin nearly full) or very large slacks (making the bin under-utilized).\n    # We want a sweet spot for slack. A simple way is to penalize slack if it's too small.\n    # Let's say slack < item / 4 is penalized.\n\n    # Consider `slack_score = slack / item`. We want this to be small.\n    # If `slack_score` is very small (e.g., < 0.25), let's penalize it.\n    # A penalty could be multiplying the priority by a factor < 1.\n\n    # Let's simplify. We want to minimize slack.\n    # The function should return higher values for more preferred bins.\n    # Prioritize bins where `bin_remain_cap - item` is minimal and non-negative.\n\n    # Let's use the Best Fit principle but with a slight modification to\n    # encourage diversification or prevent over-filling.\n\n    # For suitable bins:\n    # Calculate the \"goodness\" of the fit as `goodness = bin_remain_cap - item`.\n    # We want to minimize `goodness`. Higher priority for smaller `goodness`.\n    # `priority = 1.0 / (1.0 + goodness)` gives highest priority for `goodness = 0`.\n\n    # To incorporate the idea of \"useful remaining space\" or avoiding extreme filling:\n    # Let's consider a score that peaks when `bin_remain_cap` is slightly larger than `item`.\n    # For example, if the bin capacity was originally `C`, we might prefer a bin\n    # where `bin_remain_cap` is around `item` or `item + small_margin`.\n    # This is difficult without knowing the original bin capacity or typical item sizes.\n\n    # Let's try a score that:\n    # 1. Rewards tight fits (`slack` is small).\n    # 2. Penalizes bins that become extremely full (very small `slack`).\n    # 3. Penalizes bins that remain very empty (large `slack` but not fitting the item).\n\n    # For suitable bins:\n    # `slack = bin_remain_cap - item`\n    # Primary scoring: minimize `slack`. `primary_score = 1.0 / (1.0 + slack)`\n    # Secondary scoring: if `slack` is very small (e.g., < item / 2), reduce priority.\n    # Let's define a \"small slack penalty\".\n    # `small_slack_penalty_factor = max(0.0, 1.0 - (slack / (item / 2.0)))` if item > 0 else 1.0\n    # This factor is 1 when slack >= item/2, and decreases towards 0 as slack approaches 0.\n\n    # `priority_score = primary_score * small_slack_penalty_factor`\n\n    # Let's refine this. The penalty factor should be applied only if slack is small.\n    # If slack is small, we want to reduce its priority.\n\n    scores = np.zeros_like(bins_remain_cap)\n    suitable_indices = np.where(suitable_bins_mask)[0]\n\n    if len(suitable_indices) == 0:\n        return scores\n\n    for i in suitable_indices:\n        slack = bins_remain_cap[i] - item\n\n        # Primary component: prioritize minimal slack (tight fit)\n        # Use a small epsilon to avoid division by zero if slack is negative (shouldn't happen here)\n        # and to give a high score for slack = 0.\n        primary_score = 1.0 / (1.0 + slack)\n\n        # Secondary component: penalize very small slacks (to avoid over-filling)\n        # If slack is very small (e.g., less than half the item size), reduce its desirability.\n        # This encourages leaving some usable space.\n        # The threshold `slack_threshold` can be tuned. Let's use `item / 2.0`.\n        # If slack is very small, we want to decrease its score.\n        # Let's introduce a penalty factor that is close to 1 for larger slacks,\n        # and decreases as slack gets very small.\n        # A simple approach: If slack is very small relative to the item, penalize.\n        # For example, if `slack < item * 0.25`, apply a penalty.\n\n        # Let's try a score that is high when slack is small, but not zero.\n        # We want to avoid slack = 0 if possible, by preferring slack = 1 (if bin_capacity was 10, item = 9, remaining = 1).\n        # This is like saying we prefer a remaining capacity of 1 over 0.\n\n        # Let's reconsider the prompt's keywords: \"local search\", \"neighborhood exploration\", \"diversification\".\n        # This implies that simply picking the best fit might not be optimal.\n        # We might want to explore slightly \"sub-optimal\" bins to open up better packing opportunities later.\n\n        # Let's design a score that favors tighter fits, but also considers\n        # bins that offer a \"balanced\" remaining capacity.\n\n        # Score for a suitable bin `i`:\n        # `slack = bins_remain_cap[i] - item`\n        # We want to minimize `slack`.\n        # Let's consider a function that rewards smaller slacks.\n        # `reward = exp(-slack)` would give high rewards for small slacks.\n        # Or `reward = 1.0 / (1.0 + slack)`.\n\n        # To penalize very small slacks (overfilling):\n        # If `slack < item * 0.2`: apply a penalty.\n        # Let's use a function that is high when `slack` is small, but also\n        # has a slight dip or reduced slope when `slack` is near zero.\n\n        # Consider a score that is a function of `slack`.\n        # Let's try `score = -slack**2` - this peaks at slack=0, but doesn't penalize slack=0.\n        # What about `score = -slack`? Maximizing this means minimizing slack. This is BF.\n\n        # Let's introduce a penalty for very small slacks (e.g., slack < 0.1 * item).\n        # This suggests that a bin that has `slack = 0.5` might be preferred over `slack = 0.05`.\n        # This is a deviation from pure Best Fit.\n\n        # Let `slack = bins_remain_cap[i] - item`.\n        # Primary preference: minimize slack.\n        # If we have `slack1 = 0.1` and `slack2 = 0.2`, we prefer `slack1`.\n        # If we have `slack1 = 0.01` and `slack2 = 0.1`, we prefer `slack2` slightly,\n        # because `0.01` is too small.\n\n        # Let's try a score that is high when `slack` is small, but decreases\n        # if `slack` becomes critically small.\n        # A function like `f(slack) = 1 / (1 + slack) - penalty_for_small_slack`\n\n        # Consider `slack_normalized = slack / item` (if item > 0).\n        # We want `slack_normalized` to be small.\n        # If `slack_normalized` is very small (e.g., < 0.2), we want to reduce its score.\n        # Let's define `small_slack_penalty = max(0.0, 1.0 - (slack_normalized / 0.2))`\n        # This penalty is 1 if `slack_normalized >= 0.2` and goes to 0 as `slack_normalized` approaches 0.\n\n        # `priority = (1.0 / (1.0 + slack)) * small_slack_penalty`\n\n        # Let's be careful with item = 0 case.\n        item_val = max(item, 1e-9) # Avoid division by zero if item is 0.\n\n        slack = bins_remain_cap[i] - item\n        slack_normalized = slack / item_val\n\n        # Primary scoring: prioritize minimal slack\n        # Higher score for smaller slack\n        primary_score = 1.0 / (1.0 + slack)\n\n        # Secondary penalty for very small slack (e.g., less than 20% of item size)\n        # This encourages leaving some usable space.\n        # If slack_normalized < 0.2, we apply a penalty.\n        # The penalty factor is 1.0 for slack_normalized >= 0.2, and decreases to 0 as slack_normalized -> 0.\n        # We want a smooth transition. A sigmoid-like function or a simple linear ramp.\n        # Let's use a linear ramp that starts penalizing significantly below 0.2.\n        # Penalty factor: `max(0.0, 1.0 - (0.2 - slack_normalized) / 0.2)` for slack_normalized < 0.2.\n        # Simplified: `max(0.0, slack_normalized / 0.2)` for slack_normalized < 0.2.\n        # Better: `max(0.0, 1.0 - (0.2 - slack_normalized) / 0.2)` is `max(0.0, slack_normalized / 0.2)`.\n        # Let's make the penalty more gradual.\n        # A Gaussian-like penalty centered at 0, or a decaying exponential.\n\n        # Let's try a score that favors slack in a certain range.\n        # The ideal slack could be around `item / 4` or `item / 2`.\n        # This is hard to capture simply.\n\n        # Back to the prompt's hint: \"refining the quality of 'tight fit' by exploring diverse metrics\".\n        # What if we prioritize bins that are \"almost full\", and among those, pick the best fit?\n        # \"Almost full\" could mean remaining capacity is less than a certain fraction of the bin.\n        # But we don't know the bin capacity.\n\n        # Let's stick to prioritizing minimal slack, but with a twist:\n        # If multiple bins have the *same* minimal slack (e.g., both have slack = 0.1),\n        # how do we break ties?\n        # The prompt suggests \"neighborhood exploration\" and \"diversification\".\n        # This implies we might not *always* pick the absolute best fit.\n\n        # Let's try a heuristic that prioritizes bins that leave a small, but positive,\n        # amount of remaining capacity.\n        # This means minimizing `slack = bin_remain_cap - item`, but we might prefer\n        # `slack = 0.1` over `slack = 0.0` if the latter makes the bin too full.\n\n        # Let's define a \"desirability\" score for the remaining capacity after packing.\n        # `post_pack_remain = bin_remain_cap - item`\n        # We want `post_pack_remain` to be small.\n        # Let's consider a score that is high when `post_pack_remain` is small,\n        # but not *too* small.\n        # A function that peaks at a small positive value, e.g., at `post_pack_remain = 0.5` or `1.0`.\n        # This is complex to implement without more context on bin sizes or item sizes distribution.\n\n        # Simpler approach:\n        # 1. Primary objective: Minimize `slack = bin_remain_cap - item`.\n        # 2. Secondary objective: Among bins with similar slack, prefer those that were less full initially.\n        # This is \"Loose Fit\" tie-breaking. It can help distribute items.\n\n        # Let's try this:\n        # Calculate `slack = bin_remain_cap - item` for all suitable bins.\n        # Calculate `priority = 1.0 / (1.0 + slack)`. This is Best Fit priority.\n        # To introduce diversification or \"loosening\", we can add a small\n        # random perturbation or a systematic slight \"loosening\" factor.\n\n        # Let's stick to refining the \"tight fit\" quality.\n        # We want to minimize `slack`.\n        # What if we consider the *relative* slack? `slack / bin_remain_cap`\n        # This is problematic if `bin_remain_cap` is small.\n\n        # Let's use the `1.0 / (1.0 + slack)` as a base.\n        # Now, how to penalize over-filling?\n        # If `slack < epsilon` (a small threshold), reduce the score.\n        # Let `epsilon = 0.1 * item_val`.\n        # If `slack < epsilon`, then `penalty_factor = slack / epsilon`.\n        # Otherwise, `penalty_factor = 1.0`.\n\n        slack = bins_remain_cap[i] - item\n        # Ensure slack is non-negative for calculations.\n        effective_slack = max(0.0, slack)\n\n        # Base priority: higher for smaller effective slack (Best Fit)\n        # Add a small value to the denominator to prevent division by zero and ensure positive scores.\n        base_priority = 1.0 / (1.0 + effective_slack)\n\n        # Refinement: Penalize very small positive slacks to encourage leaving usable space.\n        # Define a \"critical slack\" threshold, e.g., 10% of the item's size.\n        # This threshold can be adjusted.\n        critical_slack_threshold = 0.15 * item_val # Example: 15% of item size\n\n        # Calculate a penalty factor. It should be less than 1 if slack is below the threshold.\n        # For slack below threshold, the penalty factor decreases as slack approaches 0.\n        # For slack at or above threshold, penalty factor is 1.0.\n        if effective_slack < critical_slack_threshold:\n            # Linear penalty: if slack is 0, penalty is 0. If slack is critical_slack_threshold, penalty is 1.\n            # We want to penalize slack values *below* the threshold, meaning their priority should be reduced.\n            # So, if slack is small, we want the penalty factor to be small.\n            # If slack = 0, penalty_factor = 0.\n            # If slack = critical_slack_threshold, penalty_factor = 1.0.\n            penalty_factor = effective_slack / critical_slack_threshold\n            # Clamp penalty factor to be at most 1.0 (in case effective_slack exceeds threshold during calculation)\n            penalty_factor = min(penalty_factor, 1.0)\n        else:\n            penalty_factor = 1.0\n\n        # Final score is the base priority modulated by the penalty factor.\n        # This means bins with very small slack will have their priority reduced.\n        final_score = base_priority * penalty_factor\n\n        scores[i] = final_score\n\n    # The bin with the highest score is the most preferred.\n    return scores",
    "response_id": 4,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Command '['python3', '-u', '/home/dokhanhnam1199/QD/problems/bpp_online/eval.py', '5000', '/home/dokhanhnam1199/QD', 'train']' timed out after 49.999975410988554 seconds"
  },
  {
    "stdout_filepath": "problem_iter4_response0.txt_stdout.txt",
    "code_path": "problem_iter4_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 0.0004745363277406069, sigmoid_k: float = 6.294290879552339, sigmoid_center_offset: float = 0.6538583089803157) -> np.ndarray:\n    \"\"\"\n    Combines the 'tight fit' prioritization of inverse difference with a sigmoid\n    function to normalize priorities, favoring bins that are a near-perfect fit\n    while maintaining a reasonable range.\n\n    Args:\n        item (float): The item size to fit.\n        bins_remain_cap (np.ndarray): A numpy array representing the remaining capacity of each bin.\n        epsilon (float): A small value added to the denominator to prevent division by zero.\n        sigmoid_k (float): The steepness parameter for the sigmoid function.\n        sigmoid_center_offset (float): The offset to center the sigmoid curve.\n\n    Returns:\n        np.ndarray: An array of priorities for each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(valid_bins_mask):\n        return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.487435181491823,
    "SLOC": 5.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  }
]