[
  {
    "stdout_filepath": "problem_iter14_response0.txt_stdout.txt",
    "code_path": "problem_iter14_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a refined best-fit metric that penalizes both extreme remaining capacities\n    with a dynamic weighting strategy that adapts to item size relative to bin availability.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    # Metric 1: Refined Best Fit (using log of inverse residual)\n    # Penalizes bins that are too full (small residual) or too empty (large residual) after placement.\n    remaining_after_placement = suitable_bins_caps - item\n    # Use log1p of the inverse of remaining capacity + epsilon. Higher score for residuals closer to 0.\n    # Adding 1 to the denominator ensures that even for perfect fits (residual=0), we don't get division by zero.\n    # A small epsilon is added to the denominator to prevent division by zero if remaining_after_placement is 0.\n    best_fit_scores = np.log1p(1.0 / (remaining_after_placement + 1e-6))\n\n    # Metric 2: Exploration/Spread (using log of current remaining capacity)\n    # Favors bins with larger initial remaining capacities, promoting spreading items.\n    # Use log1p of remaining capacity. Higher score for larger remaining capacities.\n    exploration_scores = np.log1p(suitable_bins_caps)\n\n    # Normalize scores to be in a comparable range [0, 1] for combining.\n    # Avoid division by zero if all scores for a metric are zero.\n    max_best_fit = np.max(best_fit_scores)\n    normalized_best_fit = best_fit_scores / max_best_fit if max_best_fit > 1e-6 else np.zeros_like(best_fit_scores)\n\n    max_exploration = np.max(exploration_scores)\n    normalized_exploration = exploration_scores / max_exploration if max_exploration > 1e-6 else np.zeros_like(exploration_scores)\n\n    # Dynamic Weighting: Adjust weights based on item size relative to the maximum remaining capacity of suitable bins.\n    # This aims to balance \"best fit\" for larger items with \"exploration\" for smaller items.\n    max_suitable_cap = np.max(suitable_bins_caps)\n    relative_item_size = item / (max_suitable_cap + 1e-6)\n\n    # Weight for Best Fit: Higher for larger items, lower for smaller items.\n    # This ensures that for large items, we prioritize a tight fit.\n    weight_best_fit = 0.5 + 0.4 * relative_item_size\n    weight_best_fit = np.clip(weight_best_fit, 0.5, 0.9)\n\n    # Weight for Exploration: Lower for larger items, higher for smaller items.\n    # This encourages spreading smaller items into less utilized bins.\n    weight_exploration = 0.5 - 0.4 * relative_item_size\n    weight_exploration = np.clip(weight_exploration, 0.1, 0.5)\n\n    # Ensure weights sum to 1\n    total_weight = weight_best_fit + weight_exploration\n    if total_weight < 1e-6:\n        total_weight = 1.0\n    \n    weight_best_fit /= total_weight\n    weight_exploration /= total_weight\n\n    combined_scores = (weight_best_fit * normalized_best_fit +\n                       weight_exploration * normalized_exploration)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 28.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response1.txt_stdout.txt",
    "code_path": "problem_iter14_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A hybrid heuristic combining Best Fit (tightness) with dynamic exploration,\n    adapting weights based on item size to balance packing efficiency and bin utilization.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_caps - item\n\n    # Metric 1: Refined Best Fit - favors bins with smallest remaining capacity after placement.\n    # Logarithmic scaling to compress larger gaps and emphasize smaller ones. Add epsilon for stability.\n    best_fit_scores = np.log1p(1.0 / (remaining_after_placement + 1e-6))\n\n    # Metric 2: Exploration Bonus - favors bins with more remaining capacity.\n    # Min-max scaling to normalize the exploration score between 0 and 1.\n    min_rem_after = np.min(remaining_after_placement)\n    max_rem_after = np.max(remaining_after_placement)\n    \n    exploration_scores = np.zeros_like(remaining_after_placement)\n    if max_rem_after > min_rem_after:\n        exploration_scores = (remaining_after_placement - min_rem_after) / (max_rem_after - min_rem_after)\n    elif suitable_bins_caps.size > 0: # If all suitable bins have same remaining space, give equal exploration bonus if any\n        exploration_scores = np.ones_like(remaining_after_placement) * 0.5\n\n    # Dynamic Weighting based on item size relative to a conceptual bin capacity of 1.0.\n    # This aims to use Best Fit more for larger items and Exploration for smaller ones.\n    # A smoother transition using a sigmoid-like approach or clipping.\n    # Assume item size is already normalized or scaled appropriately.\n    \n    # Define a soft transition point (e.g., 0.5 for half-full bins)\n    transition_point = 0.5\n    \n    # Calculate weights: higher weight for best_fit for larger items, higher for exploration for smaller items.\n    # Use clipping to ensure weights stay within a reasonable range and sum approximately to 1.\n    weight_best_fit = np.clip(item / transition_point, 0.2, 0.9)\n    weight_exploration = 1.0 - weight_best_fit\n\n    # Ensure weights are sensible and sum to 1\n    total_weight = weight_best_fit + weight_exploration\n    if total_weight > 1e-6:\n        weight_best_fit /= total_weight\n        weight_exploration /= total_weight\n    else: # Fallback if weights are zero\n        weight_best_fit = 0.5\n        weight_exploration = 0.5\n\n    # Combine scores using dynamic weights\n    combined_scores = (weight_best_fit * best_fit_scores +\n                       weight_exploration * exploration_scores)\n\n    # Assign combined scores to the priorities array\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 73.7534902273634,
    "SLOC": 29.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response2.txt_stdout.txt",
    "code_path": "problem_iter14_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (tightness) with a \"Gap Fill Ratio\" metric.\n    Dynamic weights favor Best Fit for items that are a significant portion\n    of the remaining space, and Gap Fill Ratio for items that are smaller\n    relative to the available space.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_caps - item\n\n    # Metric 1: Best Fit - Score based on the tightness of the fit.\n    # Using the reciprocal of (1 + remaining capacity after placement)\n    # This prioritizes bins where the remaining space is minimized.\n    best_fit_scores = 1.0 / (1.0 + remaining_after_placement)\n\n    # Metric 2: Gap Fill Ratio - Score based on how much the item fills the *current available space*.\n    # High ratio means item is a large fraction of available space, good for utilizing larger gaps.\n    # Add epsilon for stability.\n    gap_fill_ratio_scores = item / (suitable_bins_caps + 1e-6)\n    # Clip to avoid extreme values and ensure scores are somewhat bounded.\n    gap_fill_ratio_scores = np.clip(gap_fill_ratio_scores, 0, 2.0)\n\n    # Normalize scores to [0, 1] for combination. Avoid division by zero.\n    max_best_fit = np.max(best_fit_scores)\n    normalized_best_fit = best_fit_scores / max_best_fit if max_best_fit > 1e-6 else np.zeros_like(best_fit_scores)\n\n    max_gap_fill = np.max(gap_fill_ratio_scores)\n    normalized_gap_fill = gap_fill_ratio_scores / max_gap_fill if max_gap_fill > 1e-6 else np.zeros_like(gap_fill_ratio_scores)\n\n    # Dynamic Weighting:\n    # Aim to balance between fitting tightly (Best Fit) and utilizing larger gaps effectively (Gap Fill Ratio).\n    # If an item is large relative to the available space in a bin, Best Fit becomes more important.\n    # If an item is small relative to the available space, the Gap Fill Ratio (how much it contributes to filling that gap) is more relevant.\n\n    # Consider the ratio of the item size to the maximum remaining capacity among suitable bins.\n    # This gives a sense of whether the item is \"large\" or \"small\" compared to the best available space.\n    max_suitable_cap = np.max(suitable_bins_caps)\n    relative_item_size = item / (max_suitable_cap + 1e-6)\n\n    # Weighting scheme:\n    # For larger relative items (closer to 1), prioritize Best Fit.\n    # For smaller relative items (closer to 0), prioritize Gap Fill Ratio.\n    # Use a sigmoid-like shape for smooth transition.\n\n    # Weight for Best Fit: increases with relative item size.\n    # Range: [0.4, 0.9]\n    weight_best_fit = 0.4 + 0.5 * relative_item_size\n    weight_best_fit = np.clip(weight_best_fit, 0.4, 0.9)\n\n    # Weight for Gap Fill Ratio: decreases with relative item size.\n    # Range: [0.1, 0.6]\n    weight_gap_fill = 0.6 - 0.5 * relative_item_size\n    weight_gap_fill = np.clip(weight_gap_fill, 0.1, 0.6)\n\n    # Ensure weights sum to 1.\n    total_weight = weight_best_fit + weight_gap_fill\n    if total_weight > 1e-6:\n        weight_best_fit /= total_weight\n        weight_gap_fill /= total_weight\n    else: # Fallback if total_weight is near zero\n        weight_best_fit = 0.5\n        weight_gap_fill = 0.5\n\n    combined_scores = (weight_best_fit * normalized_best_fit +\n                       weight_gap_fill * normalized_gap_fill)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 31.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response3.txt_stdout.txt",
    "code_path": "problem_iter14_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a nuanced 'sweet spot' best-fit with exploration favoring less-used bins,\n    weighted to prioritize good fits while encouraging diversification.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    # Metric 1: Nuanced Best Fit - Prioritize bins leaving a \"sweet spot\" of residual capacity.\n    # This helps avoid very small unusable gaps. Using a Gaussian-like shape.\n    remaining_after_placement = suitable_bins_caps - item\n    # Define a target residual capacity, e.g., 20% of the item size, to aim for.\n    target_residual = item * 0.2\n    # Score is highest when remaining_after_placement is close to target_residual.\n    # Adding a small epsilon to the denominator for numerical stability, especially if target_residual is 0.\n    best_fit_scores = np.exp(-((remaining_after_placement - target_residual) / (target_residual + 1e-6))**2)\n\n    # Metric 2: Exploration/Less Used Bin Preference - Favor bins that are less full *after* placement,\n    # relative to other suitable bins. This encourages spreading items.\n    # Calculate the range of remaining capacities after placing the item in suitable bins.\n    min_rem_after_m2 = np.min(remaining_after_placement)\n    max_rem_after_m2 = np.max(remaining_after_placement)\n    \n    exploration_scores = np.zeros_like(suitable_bins_caps)\n    if max_rem_after_m2 > min_rem_after_m2:\n        # Normalize remaining capacity after placement. Higher score for more empty bins.\n        # This is a form of min-max scaling for the post-placement residual.\n        exploration_scores = (remaining_after_placement - min_rem_after_m2) / (max_rem_after_m2 - min_rem_after_m2)\n    elif suitable_bins_caps.size > 0:\n        # If all suitable bins result in the same remaining capacity, this metric doesn't differentiate.\n        # Assign a neutral score (e.g., 0.5) to avoid bias.\n        exploration_scores = np.ones_like(suitable_bins_caps) * 0.5\n\n    # Combine scores with a weighted sum.\n    # Give a strong weight to the nuanced best-fit as it directly impacts packing efficiency.\n    # Give a moderate weight to exploration to balance against potential fragmentation.\n    # Weights are chosen to prioritize good fits while still allowing for diversification.\n    # Example weights: 0.7 for Best Fit, 0.3 for Exploration.\n    combined_scores = 0.7 * best_fit_scores + 0.3 * exploration_scores\n    \n    # Ensure scores are within a reasonable range and handle potential NaNs/Infs.\n    # NaN values can occur if all suitable bins have the same remaining capacity after placement\n    # and the denominator in exploration_scores becomes zero.\n    combined_scores = np.nan_to_num(combined_scores, nan=0.5, posinf=1.0, neginf=0.0)\n    # Clip scores to [0, 1] to maintain a consistent range.\n    combined_scores = np.clip(combined_scores, 0.0, 1.0)\n\n    priorities[suitable_bins_mask] = combined_scores\n    \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 37.654567211806956,
    "SLOC": 21.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response4.txt_stdout.txt",
    "code_path": "problem_iter14_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit's tightness with an exploration bonus favoring less utilized bins,\n    using a balanced approach with dynamic weighting based on item size.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return np.zeros_like(bins_remain_cap)\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Best Fit Component: Prioritize bins with minimal remaining capacity after placement.\n    # Use inverse of remaining capacity for higher scores for tighter fits.\n    best_fit_scores = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Exploration Component: Favor bins with larger original capacity (less utilized).\n    # Normalize remaining capacities of suitable bins using min-max scaling.\n    min_cap = np.min(suitable_bins_remain_cap)\n    max_cap = np.max(suitable_bins_remain_cap)\n    if max_cap - min_cap > 1e-9:\n        exploration_scores = (suitable_bins_remain_cap - min_cap) / (max_cap - min_cap)\n    else:\n        exploration_scores = np.zeros_like(suitable_bins_remain_cap)\n        \n    # Dynamic Weighting: Adjust weights based on item size relative to max suitable capacity.\n    # For larger items, lean more towards Best Fit. For smaller items, give more weight to exploration.\n    max_suitable_cap = np.max(suitable_bins_remain_cap)\n    weight_bf = 0.5 + 0.5 * (item / max_suitable_cap) if max_suitable_cap > 1e-9 else 0.5\n    weight_exp = 1.0 - weight_bf\n    \n    # Combine scores with dynamic weights. Higher combined scores indicate better bins.\n    combined_scores = weight_bf * best_fit_scores + weight_exp * exploration_scores\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 83.51615476665337,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response5.txt_stdout.txt",
    "code_path": "problem_iter14_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit and Fullness Prioritization with adaptive weighting.\n    This heuristic balances finding a snug fit for the item with utilizing fuller bins,\n    adjusting emphasis based on the item's size relative to available capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit (Minimize residual capacity)\n    # Score is inversely proportional to the remaining capacity after placement.\n    # Higher score for bins that leave less space.\n    remaining_after_placement = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-6)\n\n    # Metric 2: Bin Fullness Prioritization (Favor fuller bins)\n    # Score is inversely proportional to the bin's current remaining capacity.\n    # Higher score for bins that are more full.\n    fullness_scores = 1.0 / (suitable_bins_caps + 1e-6)\n\n    # --- Adaptive Weighting ---\n    # Determine the context: item size relative to average suitable bin capacity.\n    avg_suitable_cap = np.mean(suitable_bins_caps)\n    if avg_suitable_cap > 1e-9:\n        item_vs_avg_cap_ratio = item / avg_suitable_cap\n    else:\n        item_vs_avg_cap_ratio = 0.5 # Default if no suitable bins or very small capacity\n\n    # Base weights: BF for tight fit, F for consolidation.\n    w_bf = 0.6\n    w_f = 0.4\n\n    # Adjust weights dynamically:\n    # If item is relatively large, prioritize Best Fit more.\n    # If item is relatively small, give more weight to Fullness to encourage consolidation.\n    if item_vs_avg_cap_ratio > 1.0: # Item is larger than average remaining capacity\n        w_bf += 0.2 * (item_vs_avg_cap_ratio - 1.0)\n        w_f -= 0.2 * (item_vs_avg_cap_ratio - 1.0)\n    else: # Item is smaller than average remaining capacity\n        w_f += 0.2 * (1.0 - item_vs_avg_cap_ratio)\n        w_bf -= 0.2 * (1.0 - item_vs_avg_cap_ratio)\n\n    # Ensure weights remain valid and sum to 1.\n    w_bf = max(0, w_bf)\n    w_f = max(0, w_f)\n    total_w = w_bf + w_f\n    if total_w > 1e-9:\n        w_bf /= total_w\n        w_f /= total_w\n    else: # Fallback if weights become zero\n        w_bf, w_f = 0.5, 0.5\n\n    # --- Normalization and Combination ---\n    # Normalize scores to be in a comparable range [0, 1]\n    # Normalize Best Fit scores: higher score is better.\n    if np.max(best_fit_scores) > 1e-9:\n        norm_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        norm_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Fullness scores: higher score is better.\n    if np.max(fullness_scores) > 1e-9:\n        norm_fullness = fullness_scores / np.max(fullness_scores)\n    else:\n        norm_fullness = np.zeros_like(fullness_scores)\n\n    # Combine normalized scores with dynamic weights.\n    combined_scores = (w_bf * norm_best_fit + w_f * norm_fullness)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 41.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response6.txt_stdout.txt",
    "code_path": "problem_iter14_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit and Bin Fullness metrics with adaptive weights.\n    Prioritizes tighter fits for larger items and fuller bins for all items,\n    while normalizing scores for balanced contribution.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit (minimize remaining space after packing)\n    # Higher score for bins with less remaining space after placing the item.\n    remaining_after_placement = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-9)\n\n    # Metric 2: Bin Fullness (prioritize fuller bins)\n    # Higher score for bins that are already more full (less remaining capacity).\n    fullness_scores = 1.0 / (suitable_bins_caps + 1e-9)\n\n    # Normalize scores to ensure comparability between metrics.\n    # Max-min normalization is applied to each metric independently.\n    if np.max(best_fit_scores) > 1e-9:\n        norm_best_fit = (best_fit_scores - np.min(best_fit_scores)) / (np.max(best_fit_scores) - np.min(best_fit_scores) + 1e-9)\n    else:\n        norm_best_fit = np.zeros_like(best_fit_scores)\n\n    if np.max(fullness_scores) > 1e-9:\n        norm_fullness = (fullness_scores - np.min(fullness_scores)) / (np.max(fullness_scores) - np.min(fullness_scores) + 1e-9)\n    else:\n        norm_fullness = np.zeros_like(fullness_scores)\n\n    # Adaptive Weighting: Adjust weights based on item size relative to the average suitable bin capacity.\n    # This strategy emphasizes \"Best Fit\" for larger items and \"Fullness\" for smaller items.\n    avg_suitable_cap = np.mean(suitable_bins_caps)\n    if avg_suitable_cap > 1e-9:\n        item_vs_avg_cap_ratio = item / avg_suitable_cap\n    else:\n        item_vs_avg_cap_ratio = 0.5 # Default for very small capacities or no suitable bins\n\n    # Base weights that can be tuned.\n    w_bf = 0.6\n    w_f = 0.4\n\n    # Dynamically adjust weights: if item is large relative to average capacity, boost Best Fit.\n    # Otherwise, slightly boost Fullness.\n    if item_vs_avg_cap_ratio > 1.2: # Threshold for considering item \"large\"\n        w_bf += 0.2 * (item_vs_avg_cap_ratio - 1.2)\n        w_f -= 0.2 * (item_vs_avg_cap_ratio - 1.2)\n    elif item_vs_avg_cap_ratio < 0.8: # Threshold for considering item \"small\"\n        w_f += 0.1 * (0.8 - item_vs_avg_cap_ratio)\n        w_bf -= 0.1 * (0.8 - item_vs_avg_cap_ratio)\n\n    # Ensure weights remain valid (non-negative and sum to 1).\n    w_bf = max(0, w_bf)\n    w_f = max(0, w_f)\n    total_w = w_bf + w_f\n    if total_w > 1e-9:\n        w_bf /= total_w\n        w_f /= total_w\n    else: # Fallback to equal weights if calculation results in zero total weight\n        w_bf, w_f = 0.5, 0.5\n\n    # Combine normalized scores using the adaptive weights.\n    combined_scores = w_bf * norm_best_fit + w_f * norm_fullness\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 41.0,
    "cyclomatic_complexity": 8.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response7.txt_stdout.txt",
    "code_path": "problem_iter14_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (tightest residual) with Bin Fullness (prioritizing fuller bins)\n    using dynamic weights based on item size. Aims for efficient packing by\n    balancing tight fits with better overall bin utilization.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit (BF) - Prioritizes bins with minimum remaining capacity after placement.\n    # Higher score for smaller `remaining_capacity = suitable_bins_caps - item`.\n    remaining_capacity = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_capacity + 1e-9)\n\n    # Metric 2: Bin Fullness (F) - Prioritizes bins that are already more full.\n    # Higher score for smaller `suitable_bins_caps`.\n    fullness_scores = 1.0 / (suitable_bins_caps + 1e-9)\n\n    # Normalize scores for each metric to ensure they are in a comparable range.\n    # Normalization helps in combining metrics with different scales.\n\n    # Normalize Best Fit scores (0 to 1, higher is better)\n    max_bf = np.max(best_fit_scores)\n    norm_best_fit = best_fit_scores / max_bf if max_bf > 1e-9 else np.zeros_like(best_fit_scores)\n\n    # Normalize Fullness scores (0 to 1, higher is better)\n    max_f = np.max(fullness_scores)\n    norm_fullness = fullness_scores / max_f if max_f > 1e-9 else np.zeros_like(fullness_scores)\n\n    # --- Dynamic Weighting ---\n    # Determine weights based on the item's size relative to the average suitable bin capacity.\n    # This strategy adapts the heuristic's focus.\n    avg_suitable_cap = np.mean(suitable_bins_caps)\n    if avg_suitable_cap > 1e-9:\n        item_vs_avg_cap_ratio = item / avg_suitable_cap\n    else:\n        item_vs_avg_cap_ratio = 1.0 # Default ratio if average capacity is zero/negligible\n\n    # Base weights: Balanced approach\n    w_bf_base = 0.6\n    w_f_base = 0.4\n\n    # Adjust weights:\n    # If item is large relative to average suitable capacity, boost Best Fit.\n    # If item is small relative to average suitable capacity, slightly boost Fullness (to use slightly fuller bins).\n    # The goal is to make BF dominant for larger items and F still relevant for smaller ones.\n    w_bf = w_bf_base + 0.3 * max(0, item_vs_avg_cap_ratio - 1.0)\n    w_f = w_f_base - 0.3 * max(0, item_vs_avg_cap_ratio - 1.0)\n\n    # Ensure weights are non-negative and sum to 1.\n    w_bf = max(0.1, w_bf) # Ensure at least some weight for BF\n    w_f = max(0.1, w_f)  # Ensure at least some weight for F\n\n    total_w = w_bf + w_f\n    w_bf /= total_w\n    w_f /= total_w\n\n    # Combine normalized scores with dynamically adjusted weights\n    combined_scores = (w_bf * norm_best_fit + w_f * norm_fullness)\n\n    # Assign the calculated combined scores to the priorities array for suitable bins.\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 30.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response8.txt_stdout.txt",
    "code_path": "problem_iter14_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a refined Best Fit with a dynamic Exploration bonus,\n    balancing tightness and spreading load based on item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_caps - item\n\n    # Metric 1: Refined Best Fit - favors bins with smallest remaining capacity after placement.\n    # Logarithmic scaling emphasizes smaller remaining capacities. Add epsilon for stability.\n    best_fit_scores = np.log1p(1.0 / (remaining_after_placement + 1e-6))\n\n    # Metric 2: Exploration Bonus - favors bins that have significantly more remaining capacity.\n    # Min-max scaling of remaining capacity after placement creates a [0, 1] score.\n    min_rem_after = np.min(remaining_after_placement)\n    max_rem_after = np.max(remaining_after_placement)\n\n    exploration_scores = np.zeros_like(remaining_after_placement)\n    if max_rem_after > min_rem_after:\n        exploration_scores = (remaining_after_placement - min_rem_after) / (max_rem_after - min_rem_after)\n    else:\n        # If all suitable bins leave the same remaining capacity, exploration score is uniform.\n        exploration_scores = np.ones_like(remaining_after_placement) * 0.5 # Neutral exploration\n\n    # Dynamic Weighting based on item size.\n    # Larger items benefit more from a precise fit (Best Fit).\n    # Smaller items can afford to explore less utilized bins (Exploration Bonus).\n    # Assumes 'item' is scaled relative to typical bin capacity (e.g., 0 to 1).\n    \n    # Threshold for item size to switch weighting strategy.\n    threshold_medium = 0.5 \n    \n    # Calculate weights smoothly based on item size relative to threshold.\n    # For small items (item < threshold), exploration weight is higher.\n    # For large items (item > threshold), best_fit weight is higher.\n    weight_best_fit = np.clip(item / threshold_medium, 0.1, 1.0) \n    weight_exploration = np.clip((threshold_medium - item) / threshold_medium, 0.1, 1.0) \n\n    # Normalize weights to ensure they sum to 1, preventing issues if they fall outside intended ranges.\n    total_weight = weight_best_fit + weight_exploration\n    if total_weight > 1e-6:\n        weight_best_fit /= total_weight\n        weight_exploration /= total_weight\n    else: \n        # Fallback to equal weights if calculation results in near-zero total weight.\n        weight_best_fit = 0.5\n        weight_exploration = 0.5\n\n    # Combine scores using the dynamically determined weights.\n    combined_scores = (weight_best_fit * best_fit_scores +\n                       weight_exploration * exploration_scores)\n\n    # Assign the calculated combined scores to the priorities array for suitable bins.\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 72.92580773833267,
    "SLOC": 29.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response9.txt_stdout.txt",
    "code_path": "problem_iter14_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a Bin Fullness metric, dynamically weighting them\n    based on item size relative to bin capacities to balance tight fits and\n    bin utilization.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit (Tightness)\n    # Prioritize bins that leave minimum remaining capacity after packing.\n    # Score is inverse of remaining capacity (smaller remaining = higher score).\n    remaining_capacity = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_capacity + 1e-9)\n\n    # Metric 2: Bin Fullness\n    # Prioritize bins that are already more full, encouraging consolidation.\n    # Score is inverse of current remaining capacity (more full = higher score).\n    fullness_scores = 1.0 / (suitable_bins_caps + 1e-9)\n\n    # --- Dynamic Weighting Strategy ---\n    # Determine item's significance relative to available bin space.\n    # Calculate average remaining capacity among suitable bins.\n    avg_suitable_cap = np.mean(suitable_bins_caps)\n    if avg_suitable_cap > 1e-9:\n        # Ratio of item size to average suitable bin capacity.\n        # > 1 suggests item is large relative to available space.\n        # < 1 suggests item is small relative to available space.\n        item_vs_avg_cap_ratio = item / avg_suitable_cap\n    else:\n        # Default if no suitable bins or average capacity is zero/negligible.\n        item_vs_avg_cap_ratio = 0.5\n\n    # Adjust weights:\n    # For larger items (item_vs_avg_cap_ratio > 1.0):\n    #   Emphasize Best Fit (tightness) and Bin Fullness.\n    #   Reduce emphasis on Bin Fullness slightly as tightness is key.\n    # For smaller items (item_vs_avg_cap_ratio <= 1.0):\n    #   Emphasize Bin Fullness to use partially filled bins.\n    #   Slightly reduce emphasis on Best Fit as exact tightness is less critical.\n    if item_vs_avg_cap_ratio > 1.0:\n        w_bf = 0.6  # More weight on tight fit for larger items\n        w_f = 0.4   # Moderate weight on fullness\n    else:\n        w_bf = 0.4  # Moderate weight on tight fit for smaller items\n        w_f = 0.6   # More weight on fullness for smaller items\n\n    # Normalize weights to ensure they sum to 1 (though fixed here for clarity)\n    # Total weight = w_bf + w_f\n\n    # Normalize individual metric scores to a [0, 1] range for consistent combination.\n    # Normalize Best Fit scores\n    if np.max(best_fit_scores) > 1e-9:\n        norm_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        norm_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Fullness scores\n    if np.max(fullness_scores) > 1e-9:\n        norm_fullness = fullness_scores / np.max(fullness_scores)\n    else:\n        norm_fullness = np.zeros_like(fullness_scores)\n\n    # Combine normalized scores using dynamic weights\n    combined_scores = (w_bf * norm_best_fit + w_f * norm_fullness)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 31.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  }
]