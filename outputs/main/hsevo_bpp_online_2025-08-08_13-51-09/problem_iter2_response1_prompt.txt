{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a Sigmoid Fit Score strategy.\n\n    The priority is higher for bins that have just enough remaining capacity to fit the item.\n    This encourages fuller bins and potentially fewer overall bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    available_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(available_bins_mask):\n        valid_capacities = bins_remain_cap[available_bins_mask]\n        \n        # Calculate the \"fit score\" - how close the remaining capacity is to the item size.\n        # We want a high score when remaining_capacity is just slightly larger than item.\n        # A good proxy is item / remaining_capacity for available bins.\n        # If remaining_capacity is exactly item, this is 1. If much larger, it's close to 0.\n        fit_scores = item / valid_capacities\n\n        # Apply sigmoid to compress the fit scores into a [0, 1] range.\n        # We can use a scaling factor to tune the steepness of the sigmoid.\n        # A higher scaling factor makes the sigmoid steeper, more sensitive to small differences.\n        # We can also add an offset to shift the sigmoid, but for this problem, a simple sigmoid is sufficient.\n        scaling_factor = 5.0  # Tunable parameter\n        sigmoided_scores = 1 / (1 + np.exp(-scaling_factor * (fit_scores - 0.8))) # Centered around a fit score of 0.8\n\n        priorities[available_bins_mask] = sigmoided_scores\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Prioritize bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Heuristic: Prioritize bins with least remaining capacity that can still fit the item\n    # This is a \"best fit\" approach.\n    fitting_bins_capacity = bins_remain_cap[can_fit_mask]\n    if fitting_bins_capacity.size > 0:\n        # Calculate the \"waste\" if the item is placed in these bins\n        waste = fitting_bins_capacity - item\n        # Higher priority for bins with less waste (i.e., tighter fit)\n        # We invert the waste because we want the smallest waste to have the highest priority\n        # Add a small epsilon to avoid division by zero or very large negative numbers if waste is 0\n        priorities[can_fit_mask] = 1.0 / (waste + 1e-9)\n    \n    # A small random component can be added for exploration (epsilon-greedy like behavior)\n    # For simplicity in this priority function, we are directly implementing the greedy part.\n    # The epsilon-greedy strategy would then decide whether to pick the best fit (greedy)\n    # or a random bin. This priority function is solely for the greedy selection.\n    \n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Sigmoid Fit Score) with Heuristic 8 (simple inverse distance loop): Heuristic 1 uses a sigmoid function to map the \"fit score\" (item/remaining_capacity) to a priority between 0 and 1, with a tunable scaling factor. This provides a more nuanced prioritization, favoring bins that are *just right* for the item. Heuristic 8 simply calculates the inverse of the difference, which can lead to extremely high priorities for bins with very little remaining capacity, potentially causing instability or suboptimal packing.\n\nComparing Heuristic 1 (Sigmoid Fit Score) with Heuristic 4 (Inverse Distance): Both aim to prioritize bins with a tighter fit. Heuristic 1 uses a sigmoid to normalize and shape the priority, making it less sensitive to extreme differences than Heuristic 4's direct inverse distance. The sigmoid in Heuristic 1 can also be centered, as seen in the implementation (e.g., `fit_scores - 0.8`), allowing for more control over what constitutes a \"good fit.\" Heuristic 4's `1.0 / (differences + 1e-9)` can still produce very large values.\n\nComparing Heuristic 2 (Sigmoid on Differences) with Heuristic 1: Heuristic 2 applies a sigmoid to the *difference* between available capacity and item size. It normalizes these differences before applying the sigmoid, aiming for higher priority with smaller differences. While conceptually similar in using a sigmoid, Heuristic 1's approach of using `item / valid_capacities` as the base score is more directly tied to the concept of \"how full\" the bin would be, which is a common objective in bin packing. Heuristic 2's normalization `(np.max(diff) - np.min(diff) + 1e-9)` can be sensitive to outliers in the available capacities.\n\nComparing Heuristic 5/6/7 (Epsilon-Greedy) with Heuristic 9/10 (Best Fit with potential exploration): These heuristics introduce an element of exploration by considering bins that are not necessarily the best fit. Heuristic 5/6/7 add an \"exploration bonus\" based on the average remaining capacity, aiming to balance using nearly full bins with exploring less full ones. Heuristic 9/10 directly implement a \"best fit\" priority, with comments suggesting an epsilon-greedy *strategy* that would use this priority. The explicit integration of exploration within the priority calculation (as in 5/6/7) is a more direct approach to balancing exploration and exploitation within the priority scoring itself.\n\nComparing Heuristic 11/12/13/14 (Simple Inverse Distance Loops) with Heuristic 9/10 (Vectorized Inverse Distance): The vectorized versions (9/10) are generally preferred for performance in Python with NumPy due to avoiding explicit Python loops. The logic is identical.\n\nComparing Heuristic 16 (Sigmoid on Remaining Capacity) with Heuristic 1 (Sigmoid on Fit Ratio): Heuristic 16 uses the sigmoid on the *remaining capacity* after fitting, effectively minimizing it. This is similar to Heuristic 1 but uses `-valid_potential_remaining_cap` as input to the sigmoid, pushing values towards 1 for smaller remaining capacities. Heuristic 1's approach of `item / valid_capacities` is perhaps a more direct representation of \"how full\" the bin will be relative to its capacity.\n\nComparing Heuristic 20 (Softmax on Differences) with Heuristic 4 (Inverse Distance): Heuristic 20 uses `exp(fit_ratios)` and normalizes via softmax. This can lead to very large priorities if any bin has a large remaining capacity, potentially dominating the selection. Heuristic 4's inverse distance is also prone to large values, but the sigmoid approach of Heuristic 1 offers better control.\n\nComparing Heuristic 17/18/19 (Remaining Difference) with Heuristic 8/11/12/13/14 (Inverse Difference): The simple difference (`cap - item`) as a priority (Heuristics 17, 19) means that *larger* remaining differences are prioritized, which is the opposite of what's usually desired for minimizing bins. Heuristic 18 adds a -1 for invalid bins, which is a reasonable way to disqualify them. Heuristics 8, 11, 12, 13, 14 prioritize bins with *smaller* differences (inverse relationship), which is more aligned with the \"best fit\" principle.\n\nOverall: Heuristics using sigmoid functions (1, 2, 16) offer controlled and nuanced prioritization, mapping different \"fit\" metrics to a predictable range. Heuristics incorporating exploration (5, 6, 7) add a valuable dimension for improving overall packing. Simple inverse distance (4, 8, 9, 10, 11, 12, 13, 14) is a good baseline but can be sensitive to extreme values. Prioritizing by simply the remaining difference (17, 18, 19) is generally counterproductive. Softmax (20) can be unstable.\n- \n*   **Keywords:** Bin packing, heuristic design, local search, neighborhood exploration, diversification.\n*   **Advice:** Focus on refining the *quality* of \"tight fit\" assessment by exploring diverse metrics beyond simple inverse relationships. Consider how different neighborhood structures in local search can expose novel packing solutions.\n*   **Avoid:** Over-reliance on fixed scaling functions. Avoid neglecting the impact of initial solutions; consider a diversification strategy for starting points.\n*   **Explanation:** \"Tight fit\" can be multifaceted. Instead of just inverse relationships, explore metrics that capture volume utilization *and* adjacency benefits. Different local search neighborhoods explore different parts of the solution space, preventing getting stuck in poor local optima. Diverse starting points ensure a broader exploration.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}