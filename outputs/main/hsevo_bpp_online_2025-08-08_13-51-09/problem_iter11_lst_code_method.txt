{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit - score based on how tightly the item fits\n    # Using log to compress larger gaps, making smaller gaps relatively more important\n    remaining_after_placement = suitable_bins_caps - item\n    # Add a small epsilon to avoid log(0) and to differentiate very tight fits\n    best_fit_scores = np.log1p(1.0 / (remaining_after_placement + 1e-6))\n\n    # Metric 2: Exploration Bonus - score based on the \"openness\" of the bin\n    # Favor bins that have a significant amount of remaining capacity, but not excessively so.\n    # This is a sigmoid-like approach to reward moderately open bins.\n    # Normalize capacity to a 0-1 range based on the maximum possible capacity (assumed to be large, e.g., 100 for typical bin packing)\n    # or based on the max capacity among suitable bins if that's more contextually relevant.\n    # Let's use max of suitable bins as it's adaptive.\n    max_suitable_cap = np.max(suitable_bins_caps)\n    if max_suitable_cap > 1e-6:\n        normalized_suitable_caps = suitable_bins_caps / max_suitable_cap\n        # Sigmoid-like function to reward bins that are not too empty, not too full.\n        # Parameters can be tuned. Here, we aim to reward bins with roughly 50-75% remaining capacity.\n        # Example: exp(-(x-0.6)^2) where x is normalized_suitable_caps\n        exploration_scores = np.exp(-((normalized_suitable_caps - 0.6)**2) / 0.2)\n    else:\n        exploration_scores = np.zeros_like(suitable_bins_caps)\n\n    # Metric 3: Uniformity Bonus - Reward bins that are already partially filled,\n    # aiming to create more uniformly filled bins overall.\n    # This is inversely related to how \"empty\" the bin is.\n    # We can score based on how much has *already* been placed in the bin.\n    # Let's estimate initial fill based on remaining capacity relative to a hypothetical 'full' capacity.\n    # For simplicity, assume max bin capacity is 1.0, or use a value derived from data.\n    # Here, we'll use remaining capacity relative to the *item's* size to penalize bins that are *almost* empty\n    # and would be significantly \"wasted\" by a small item.\n    # More generally, consider the ratio of remaining capacity to the item size.\n    # A higher ratio means the item is small relative to the bin's open space.\n    # We want to prioritize bins where the item represents a larger fraction of the remaining space,\n    # which is counter to simple exploration.\n    # Let's reframe: reward bins that have *some* capacity already used.\n    # We can proxy this by 1 - normalized_suitable_caps, then apply a similar sigmoid.\n    # However, a simpler approach is to reward bins that are not \"too empty\".\n    # Let's use the inverse of normalized_suitable_caps, scaled to avoid large values.\n    # Consider `1 - normalized_suitable_caps` as a measure of \"fill level\".\n    # We want to slightly penalize very low fill levels.\n    fill_level = 1.0 - normalized_suitable_caps\n    # Add a small penalty for bins that are very empty (i.e., fill_level is close to 0)\n    # Using a power function to make the penalty more pronounced for emptier bins.\n    uniformity_scores = np.where(fill_level < 0.3, fill_level * 5, fill_level) # Penalize very empty bins more\n    uniformity_scores = np.clip(uniformity_scores, 0, 1) # Clip to 0-1 range\n\n\n    # Normalize scores to be in a comparable range [0, 1]\n    if np.max(best_fit_scores) > 1e-6:\n        normalized_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        normalized_best_fit = np.zeros_like(best_fit_scores)\n\n    if np.max(exploration_scores) > 1e-6:\n        normalized_exploration = exploration_scores / np.max(exploration_scores)\n    else:\n        normalized_exploration = np.zeros_like(exploration_scores)\n\n    if np.max(uniformity_scores) > 1e-6:\n        normalized_uniformity = uniformity_scores / np.max(uniformity_scores)\n    else:\n        normalized_uniformity = np.zeros_like(uniformity_scores)\n\n\n    # Combine scores with dynamic weights.\n    # The weights can be adjusted based on the item size relative to the bin capacity.\n    # If the item is large, prioritize \"best fit\". If the item is small, prioritize \"exploration\" and \"uniformity\".\n\n    # Example dynamic weighting:\n    # For smaller items, give more weight to exploration and uniformity.\n    # For larger items, give more weight to best fit.\n    # Let's define a threshold for \"small\" item, e.g., item size < 0.5 * max_bin_capacity. Assume max_bin_capacity = 1.0 for normalization.\n    item_size_normalized = item # assuming item is already normalized or scaled appropriately\n\n    weight_best_fit = 0.5 + 0.4 * item_size_normalized # weight increases with item size\n    weight_exploration = 0.3 - 0.2 * item_size_normalized # weight decreases with item size\n    weight_uniformity = 0.2 - 0.1 * item_size_normalized # weight decreases with item size\n\n    # Ensure weights sum to 1 (or handle normalization if not exactly 1)\n    total_weight = weight_best_fit + weight_exploration + weight_uniformity\n    weight_best_fit /= total_weight\n    weight_exploration /= total_weight\n    weight_uniformity /= total_weight\n\n\n    combined_scores = (weight_best_fit * normalized_best_fit +\n                       weight_exploration * normalized_exploration +\n                       weight_uniformity * normalized_uniformity)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit - score based on how tightly the item fits\n    # Using log to compress larger gaps, making smaller gaps relatively more important\n    remaining_after_placement = suitable_bins_caps - item\n    # Add a small epsilon to avoid log(0) and to differentiate very tight fits\n    best_fit_scores = np.log1p(1.0 / (remaining_after_placement + 1e-6))\n\n    # Metric 2: Exploration Bonus - score based on the \"openness\" of the bin\n    # Favor bins that have a significant amount of remaining capacity, but not excessively so.\n    # This is a sigmoid-like approach to reward moderately open bins.\n    # Normalize capacity to a 0-1 range based on the maximum possible capacity (assumed to be large, e.g., 100 for typical bin packing)\n    # or based on the max capacity among suitable bins if that's more contextually relevant.\n    # Let's use max of suitable bins as it's adaptive.\n    max_suitable_cap = np.max(suitable_bins_caps)\n    if max_suitable_cap > 1e-6:\n        normalized_suitable_caps = suitable_bins_caps / max_suitable_cap\n        # Sigmoid-like function to reward bins that are not too empty, not too full.\n        # Parameters can be tuned. Here, we aim to reward bins with roughly 50-75% remaining capacity.\n        # Example: exp(-(x-0.6)^2) where x is normalized_suitable_caps\n        exploration_scores = np.exp(-((normalized_suitable_caps - 0.6)**2) / 0.2)\n    else:\n        exploration_scores = np.zeros_like(suitable_bins_caps)\n\n    # Metric 3: Uniformity Bonus - Reward bins that are already partially filled,\n    # aiming to create more uniformly filled bins overall.\n    # This is inversely related to how \"empty\" the bin is.\n    # We can score based on how much has *already* been placed in the bin.\n    # Let's estimate initial fill based on remaining capacity relative to a hypothetical 'full' capacity.\n    # For simplicity, assume max bin capacity is 1.0, or use a value derived from data.\n    # Here, we'll use remaining capacity relative to the *item's* size to penalize bins that are *almost* empty\n    # and would be significantly \"wasted\" by a small item.\n    # More generally, consider the ratio of remaining capacity to the item size.\n    # A higher ratio means the item is small relative to the bin's open space.\n    # We want to prioritize bins where the item represents a larger fraction of the remaining space,\n    # which is counter to simple exploration.\n    # Let's reframe: reward bins that have *some* capacity already used.\n    # We can proxy this by 1 - normalized_suitable_caps, then apply a similar sigmoid.\n    # However, a simpler approach is to reward bins that are not \"too empty\".\n    # Let's use the inverse of normalized_suitable_caps, scaled to avoid large values.\n    # Consider `1 - normalized_suitable_caps` as a measure of \"fill level\".\n    # We want to slightly penalize very low fill levels.\n    fill_level = 1.0 - normalized_suitable_caps\n    # Add a small penalty for bins that are very empty (i.e., fill_level is close to 0)\n    # Using a power function to make the penalty more pronounced for emptier bins.\n    uniformity_scores = np.where(fill_level < 0.3, fill_level * 5, fill_level) # Penalize very empty bins more\n    uniformity_scores = np.clip(uniformity_scores, 0, 1) # Clip to 0-1 range\n\n\n    # Normalize scores to be in a comparable range [0, 1]\n    if np.max(best_fit_scores) > 1e-6:\n        normalized_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        normalized_best_fit = np.zeros_like(best_fit_scores)\n\n    if np.max(exploration_scores) > 1e-6:\n        normalized_exploration = exploration_scores / np.max(exploration_scores)\n    else:\n        normalized_exploration = np.zeros_like(exploration_scores)\n\n    if np.max(uniformity_scores) > 1e-6:\n        normalized_uniformity = uniformity_scores / np.max(uniformity_scores)\n    else:\n        normalized_uniformity = np.zeros_like(uniformity_scores)\n\n\n    # Combine scores with dynamic weights.\n    # The weights can be adjusted based on the item size relative to the bin capacity.\n    # If the item is large, prioritize \"best fit\". If the item is small, prioritize \"exploration\" and \"uniformity\".\n\n    # Example dynamic weighting:\n    # For smaller items, give more weight to exploration and uniformity.\n    # For larger items, give more weight to best fit.\n    # Let's define a threshold for \"small\" item, e.g., item size < 0.5 * max_bin_capacity. Assume max_bin_capacity = 1.0 for normalization.\n    item_size_normalized = item # assuming item is already normalized or scaled appropriately\n\n    weight_best_fit = 0.5 + 0.4 * item_size_normalized # weight increases with item size\n    weight_exploration = 0.3 - 0.2 * item_size_normalized # weight decreases with item size\n    weight_uniformity = 0.2 - 0.1 * item_size_normalized # weight decreases with item size\n\n    # Ensure weights sum to 1 (or handle normalization if not exactly 1)\n    total_weight = weight_best_fit + weight_exploration + weight_uniformity\n    weight_best_fit /= total_weight\n    weight_exploration /= total_weight\n    weight_uniformity /= total_weight\n\n\n    combined_scores = (weight_best_fit * normalized_best_fit +\n                       weight_exploration * normalized_exploration +\n                       weight_uniformity * normalized_uniformity)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a normalized bonus for larger remaining capacity,\n    using a weighted sum for balanced decision-making.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    # Metric 1: Best Fit - prioritize bins with minimal remaining capacity after placement.\n    remaining_after_placement = suitable_bins_caps - item\n    # Using inverse of remaining space (plus epsilon for numerical stability)\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-6)\n    \n    # Metric 2: Favor larger bins (diversification) - prioritize bins with more capacity initially.\n    # This encourages not always picking the absolute tightest.\n    large_bin_scores = suitable_bins_caps\n    \n    # Normalize Best Fit scores\n    if np.max(best_fit_scores) > 1e-6:\n        normalized_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        normalized_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Large Bin scores (using min-max scaling on the capacities of suitable bins)\n    if suitable_bins_caps.size > 1:\n        min_cap = np.min(suitable_bins_caps)\n        max_cap = np.max(suitable_bins_caps)\n        range_cap = max_cap - min_cap\n        if range_cap > 1e-6:\n            normalized_large_bin = (suitable_bins_caps - min_cap) / range_cap\n        else:\n            normalized_large_bin = np.zeros_like(suitable_bins_caps) # All suitable bins have same capacity\n    elif suitable_bins_caps.size == 1:\n        normalized_large_bin = np.array([1.0]) # If only one bin, it's maximally \"large\" in this context\n    else:\n        normalized_large_bin = np.zeros_like(suitable_bins_caps)\n\n    # Combine normalized scores using a weighted sum.\n    # Weights can be tuned. Here, 70% Best Fit, 30% favor larger bins.\n    combined_scores = 0.7 * normalized_best_fit + 0.3 * normalized_large_bin\n    \n    priorities[suitable_bins_mask] = combined_scores\n    \n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a normalized bonus for larger remaining capacity,\n    using a weighted sum for balanced decision-making.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    # Metric 1: Best Fit - prioritize bins with minimal remaining capacity after placement.\n    remaining_after_placement = suitable_bins_caps - item\n    # Using inverse of remaining space (plus epsilon for numerical stability)\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-6)\n    \n    # Metric 2: Favor larger bins (diversification) - prioritize bins with more capacity initially.\n    # This encourages not always picking the absolute tightest.\n    large_bin_scores = suitable_bins_caps\n    \n    # Normalize Best Fit scores\n    if np.max(best_fit_scores) > 1e-6:\n        normalized_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        normalized_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Large Bin scores (using min-max scaling on the capacities of suitable bins)\n    if suitable_bins_caps.size > 1:\n        min_cap = np.min(suitable_bins_caps)\n        max_cap = np.max(suitable_bins_caps)\n        range_cap = max_cap - min_cap\n        if range_cap > 1e-6:\n            normalized_large_bin = (suitable_bins_caps - min_cap) / range_cap\n        else:\n            normalized_large_bin = np.zeros_like(suitable_bins_caps) # All suitable bins have same capacity\n    elif suitable_bins_caps.size == 1:\n        normalized_large_bin = np.array([1.0]) # If only one bin, it's maximally \"large\" in this context\n    else:\n        normalized_large_bin = np.zeros_like(suitable_bins_caps)\n\n    # Combine normalized scores using a weighted sum.\n    # Weights can be tuned. Here, 70% Best Fit, 30% favor larger bins.\n    combined_scores = 0.7 * normalized_best_fit + 0.3 * normalized_large_bin\n    \n    priorities[suitable_bins_mask] = combined_scores\n    \n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a normalized bonus for larger remaining capacity,\n    using a weighted sum for balanced decision-making.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    # Metric 1: Best Fit - prioritize bins with minimal remaining capacity after placement.\n    remaining_after_placement = suitable_bins_caps - item\n    # Using inverse of remaining space (plus epsilon for numerical stability)\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-6)\n    \n    # Metric 2: Favor larger bins (diversification) - prioritize bins with more capacity initially.\n    # This encourages not always picking the absolute tightest.\n    large_bin_scores = suitable_bins_caps\n    \n    # Normalize Best Fit scores\n    if np.max(best_fit_scores) > 1e-6:\n        normalized_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        normalized_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Large Bin scores (using min-max scaling on the capacities of suitable bins)\n    if suitable_bins_caps.size > 1:\n        min_cap = np.min(suitable_bins_caps)\n        max_cap = np.max(suitable_bins_caps)\n        range_cap = max_cap - min_cap\n        if range_cap > 1e-6:\n            normalized_large_bin = (suitable_bins_caps - min_cap) / range_cap\n        else:\n            normalized_large_bin = np.zeros_like(suitable_bins_caps) # All suitable bins have same capacity\n    elif suitable_bins_caps.size == 1:\n        normalized_large_bin = np.array([1.0]) # If only one bin, it's maximally \"large\" in this context\n    else:\n        normalized_large_bin = np.zeros_like(suitable_bins_caps)\n\n    # Combine normalized scores using a weighted sum.\n    # Weights can be tuned. Here, 70% Best Fit, 30% favor larger bins.\n    combined_scores = 0.7 * normalized_best_fit + 0.3 * normalized_large_bin\n    \n    priorities[suitable_bins_mask] = combined_scores\n    \n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with an exploration bonus favoring less utilized bins.\n\n    This heuristic balances packing tightly (Best Fit) with spreading items,\n    aiming for better overall bin utilization and diversification.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return np.zeros_like(bins_remain_cap)\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Best Fit Component: Smaller remaining capacity after placing the item is better.\n    # We use the negative of the remaining capacity to make larger negative values (tighter fits)\n    # have higher priority in argmin later.\n    best_fit_scores = -(suitable_bins_remain_cap - item)\n\n    # Exploration Bonus Component: Favor bins that are less utilized (larger original capacity).\n    # Use min-max scaling to normalize the remaining capacities of *suitable* bins.\n    # Higher remaining capacity (before placing item) gets a bonus.\n    min_cap = np.min(suitable_bins_remain_cap)\n    max_cap = np.max(suitable_bins_remain_cap)\n    if max_cap - min_cap > 1e-9: # Avoid division by zero if all suitable bins have same capacity\n        exploration_scores = (suitable_bins_remain_cap - min_cap) / (max_cap - min_cap)\n    else:\n        exploration_scores = np.zeros_like(suitable_bins_remain_cap)\n\n    # Combine scores: Prioritize Best Fit, then exploration bonus.\n    # A simple weighted sum can work, but we can also directly combine them.\n    # Let's prioritize best-fit by giving it a larger weight.\n    # The 'best_fit_scores' are negative, so a higher value (closer to 0) is better.\n    # The 'exploration_scores' are positive, higher is better.\n    # To combine, we can add them, ensuring best_fit_scores are scaled appropriately.\n    # A common approach is to make the best-fit component dominant.\n    # Let's try: priority = best_fit_component + weight * exploration_component\n    # Since best_fit_scores are negative, we can add a large constant to make them positive\n    # or ensure that the best_fit_scores are the primary driver.\n    # Let's make the best_fit_scores dominant and add exploration as a tie-breaker or secondary factor.\n    # We want smaller (more negative) best_fit_scores to be prioritized, so we add the exploration.\n    combined_scores = best_fit_scores + 0.5 * exploration_scores # Weight for exploration\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    # Ensure no -inf remains if all suitable bins were considered.\n    # If all suitable bins have the same priority, np.argmin will pick the first one.\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(\n    item: float,\n    bins_remain_cap: np.ndarray,\n    best_fit_epsilon: float = 4.4681445231767004e-05,\n    exploration_target_fill_ratio: float = 0.4114913606900349,\n    exploration_spread_factor: float = 0.38679557957577443,\n    uniformity_penalty_threshold: float = 0.3177427210749225,\n    uniformity_penalty_multiplier: float = 3.7570027711933562,\n    item_size_for_weight_transition: float = 0.6361631512367136,\n    weight_best_fit_scale: float = 0.21769049426920228,\n    weight_exploration_scale: float = 0.3142842299424705,\n    weight_uniformity_scale: float = 0.2216621137118452,\n    score_normalization_epsilon: float = 8.513245871612468e-05) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n[Heuristics 8th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(\n    item: float,\n    bins_remain_cap: np.ndarray,\n    best_fit_epsilon: float = 4.4681445231767004e-05,\n    exploration_target_fill_ratio: float = 0.4114913606900349,\n    exploration_spread_factor: float = 0.38679557957577443,\n    uniformity_penalty_threshold: float = 0.3177427210749225,\n    uniformity_penalty_multiplier: float = 3.7570027711933562,\n    item_size_for_weight_transition: float = 0.6361631512367136,\n    weight_best_fit_scale: float = 0.21769049426920228,\n    weight_exploration_scale: float = 0.3142842299424705,\n    weight_uniformity_scale: float = 0.2216621137118452,\n    score_normalization_epsilon: float = 8.513245871612468e-05) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Modified Best Fit - Prioritize bins that leave a small but not necessarily zero residual capacity.\n    # This aims to avoid leaving extremely small, unusable gaps.\n    remaining_after_placement = suitable_bins_caps - item\n    # Penalize very small residuals (e.g., < 10% of item size) slightly less than larger residuals.\n    # Reward residuals that are substantial enough to be potentially useful for future small items.\n    # We use a function that is high for residuals close to a \"sweet spot\" and decreases as it deviates.\n    # A quadratic function centered around a target residual (e.g., 20% of item size) can work.\n    target_residual = item * 0.2\n    best_fit_scores = np.exp(-((remaining_after_placement - target_residual) / (target_residual + 1e-6))**2)\n\n    # Metric 2: Exploration Bonus (Refined) - Favor bins that have a significant amount of remaining capacity,\n    # but also consider the \"quality\" of the space. Bins that are nearly full are still good for consolidation.\n    # We can use a sigmoid-like function to reward bins with moderate to high remaining capacity,\n    # but also give a slight boost to bins that are already quite full (consolidated).\n    min_cap_all = np.min(bins_remain_cap)\n    max_cap_all = np.max(bins_remain_cap)\n\n    if max_cap_all - min_cap_all > 1e-6:\n        normalized_caps = (suitable_bins_caps - min_cap_all) / (max_cap_all - min_cap_all)\n    else:\n        normalized_caps = np.ones_like(suitable_bins_caps) * 0.5\n\n    # Exploration score: favors moderately full bins (e.g., 30-70% full) and very full bins.\n    # Using a combination of a linear increase and a slight boost for higher capacities.\n    exploration_scores = normalized_caps * 0.6 + (normalized_caps**2) * 0.4\n\n    # Metric 3: Bin Age/Usage - Prioritize bins that have been used more recently or have been filled to a greater extent over time.\n    # This is an implicit metric. In an online setting, we can approximate this by looking at the\n    # \"age\" or number of items placed in a bin. For this function, we'll use remaining capacity as a proxy for \"emptiness\"\n    # and inversely relate it to \"usage\". Bins with less remaining capacity are considered \"more used\".\n    # We can factor this in by slightly boosting bins that are less empty (more filled).\n    # This is already somewhat captured by the Best Fit metric. Let's add a subtle bias for bins with less remaining capacity overall.\n    # Lower remaining capacity (i.e., higher usage) should get a slight preference.\n    usage_scores = 1.0 - normalized_caps\n    usage_scores = usage_scores / (np.max(usage_scores) + 1e-6) # Normalize to [0, 1]\n\n    # Combine scores with adjusted weights. Focus on balancing fit and exploration.\n    # 60% for Best Fit (nuanced), 30% for Exploration (rewarding moderate to high capacity), 10% for Usage.\n    # The interaction between these is more nuanced: Best Fit finds the tightest, Exploration uses less full bins,\n    # Usage encourages consolidation.\n    combined_scores = 0.6 * best_fit_scores + 0.3 * exploration_scores + 0.1 * usage_scores\n\n    # Ensure scores are within [0, 1] and handle potential NaNs or Infs\n    combined_scores = np.nan_to_num(combined_scores, nan=0.0, posinf=1.0, neginf=0.0)\n    combined_scores = np.clip(combined_scores, 0.0, 1.0)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Balances Best Fit (tightness) with an exploration bonus for emptier bins.\n    This heuristic aims for efficient packing by favoring tight fits while also\n    promoting distribution and potentially accommodating future items.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Best Fit component: prioritize bins with minimal remaining capacity after placing the item.\n    # Using inverse of the difference: smaller difference means higher priority.\n    tightness_scores = 1.0 / (valid_bins_remain_cap - item + 1e-9)\n\n    # Exploration component: reward bins that are currently emptier.\n    # This encourages spreading items across bins. We'll use the log of remaining capacity\n    # to dampen the effect of very large capacities, and add a small epsilon to avoid log(0).\n    # Higher log(remaining_capacity) means emptier, so we want to add this as a bonus.\n    # A multiplier is used to balance exploration with the best-fit objective.\n    emptiness_bonus = np.log(valid_bins_remain_cap + 1e-9) * 0.1 # Tunable parameter\n\n    # Combine scores: Sum of Best Fit and exploration bonus.\n    # Higher combined score indicates a more desirable bin.\n    combined_scores = tightness_scores + emptiness_bonus\n\n    # Normalize scores to be between 0 and 1 for easier interpretation and comparison.\n    # This ensures that the highest score is 1, and others are scaled proportionally.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9: # Avoid division by zero if all scores are effectively zero\n        priorities[suitable_bins_mask] = combined_scores / max_score\n    elif np.any(suitable_bins_mask): # If all suitable bins have similar low scores\n        # Distribute priority equally among suitable bins if all scores are near zero.\n        priorities[suitable_bins_mask] = 1.0 / np.sum(suitable_bins_mask)\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Balances Best Fit (tightness) with an exploration bonus for emptier bins.\n    This heuristic aims for efficient packing by favoring tight fits while also\n    promoting distribution and potentially accommodating future items.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Best Fit component: prioritize bins with minimal remaining capacity after placing the item.\n    # Using inverse of the difference: smaller difference means higher priority.\n    tightness_scores = 1.0 / (valid_bins_remain_cap - item + 1e-9)\n\n    # Exploration component: reward bins that are currently emptier.\n    # This encourages spreading items across bins. We'll use the log of remaining capacity\n    # to dampen the effect of very large capacities, and add a small epsilon to avoid log(0).\n    # Higher log(remaining_capacity) means emptier, so we want to add this as a bonus.\n    # A multiplier is used to balance exploration with the best-fit objective.\n    emptiness_bonus = np.log(valid_bins_remain_cap + 1e-9) * 0.1 # Tunable parameter\n\n    # Combine scores: Sum of Best Fit and exploration bonus.\n    # Higher combined score indicates a more desirable bin.\n    combined_scores = tightness_scores + emptiness_bonus\n\n    # Normalize scores to be between 0 and 1 for easier interpretation and comparison.\n    # This ensures that the highest score is 1, and others are scaled proportionally.\n    max_score = np.max(combined_scores)\n    if max_score > 1e-9: # Avoid division by zero if all scores are effectively zero\n        priorities[suitable_bins_mask] = combined_scores / max_score\n    elif np.any(suitable_bins_mask): # If all suitable bins have similar low scores\n        # Distribute priority equally among suitable bins if all scores are near zero.\n        priorities[suitable_bins_mask] = 1.0 / np.sum(suitable_bins_mask)\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight packing (Best Fit) with a diversification bonus favoring\n    less utilized bins. Balances fitting tightly with spreading load.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_cap - item\n\n    # Primary objective: Tight Fit (Best Fit)\n    # Higher score for smaller remaining capacity. Use reciprocal for emphasis.\n    tight_fit_score = 1.0 / (remaining_after_placement + 1e-9)\n\n    # Secondary objective: Diversification (Favor less utilized bins)\n    # This encourages using bins that are not already very full.\n    # We can measure this by the *remaining capacity after placement* relative to the\n    # *total capacity of the bin*. A higher ratio here means the bin was less full.\n    # Use min-max scaling for a robust normalized score between 0 and 1.\n    min_rem_after = np.min(remaining_after_placement)\n    max_rem_after = np.max(remaining_after_placement)\n\n    exploration_bonus = np.zeros_like(remaining_after_placement)\n    if max_rem_after > min_rem_after:\n        # Normalize remaining capacity after placement to get exploration score\n        exploration_bonus = (remaining_after_placement - min_rem_after) / (max_rem_after - min_rem_after)\n    else:\n        # If all suitable bins result in the same remaining capacity, no bonus from this metric.\n        pass\n\n    # Combine scores: Primarily driven by tight fit, with an additive exploration bonus.\n    # The exploration bonus is scaled down to ensure tight fit remains dominant,\n    # but it serves as a tie-breaker and encourages exploration.\n    # A weight of 0.1 is empirically chosen to give exploration a modest influence.\n    final_priorities = tight_fit_score + 0.1 * exploration_bonus\n\n    priorities[suitable_bins_mask] = final_priorities\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight packing (Best Fit) with a diversification bonus favoring\n    less utilized bins. Balances fitting tightly with spreading load.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_cap - item\n\n    # Primary objective: Tight Fit (Best Fit)\n    # Higher score for smaller remaining capacity. Use reciprocal for emphasis.\n    tight_fit_score = 1.0 / (remaining_after_placement + 1e-9)\n\n    # Secondary objective: Diversification (Favor less utilized bins)\n    # This encourages using bins that are not already very full.\n    # We can measure this by the *remaining capacity after placement* relative to the\n    # *total capacity of the bin*. A higher ratio here means the bin was less full.\n    # Use min-max scaling for a robust normalized score between 0 and 1.\n    min_rem_after = np.min(remaining_after_placement)\n    max_rem_after = np.max(remaining_after_placement)\n\n    exploration_bonus = np.zeros_like(remaining_after_placement)\n    if max_rem_after > min_rem_after:\n        # Normalize remaining capacity after placement to get exploration score\n        exploration_bonus = (remaining_after_placement - min_rem_after) / (max_rem_after - min_rem_after)\n    else:\n        # If all suitable bins result in the same remaining capacity, no bonus from this metric.\n        pass\n\n    # Combine scores: Primarily driven by tight fit, with an additive exploration bonus.\n    # The exploration bonus is scaled down to ensure tight fit remains dominant,\n    # but it serves as a tie-breaker and encourages exploration.\n    # A weight of 0.1 is empirically chosen to give exploration a modest influence.\n    final_priorities = tight_fit_score + 0.1 * exploration_bonus\n\n    priorities[suitable_bins_mask] = final_priorities\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight packing (Best Fit) with a diversification bonus favoring\n    less utilized bins. Balances fitting tightly with spreading load.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_cap - item\n\n    # Primary objective: Tight Fit (Best Fit)\n    # Higher score for smaller remaining capacity. Use reciprocal for emphasis.\n    tight_fit_score = 1.0 / (remaining_after_placement + 1e-9)\n\n    # Secondary objective: Diversification (Favor less utilized bins)\n    # This encourages using bins that are not already very full.\n    # We can measure this by the *remaining capacity after placement* relative to the\n    # *total capacity of the bin*. A higher ratio here means the bin was less full.\n    # Use min-max scaling for a robust normalized score between 0 and 1.\n    min_rem_after = np.min(remaining_after_placement)\n    max_rem_after = np.max(remaining_after_placement)\n\n    exploration_bonus = np.zeros_like(remaining_after_placement)\n    if max_rem_after > min_rem_after:\n        # Normalize remaining capacity after placement to get exploration score\n        exploration_bonus = (remaining_after_placement - min_rem_after) / (max_rem_after - min_rem_after)\n    else:\n        # If all suitable bins result in the same remaining capacity, no bonus from this metric.\n        pass\n\n    # Combine scores: Primarily driven by tight fit, with an additive exploration bonus.\n    # The exploration bonus is scaled down to ensure tight fit remains dominant,\n    # but it serves as a tie-breaker and encourages exploration.\n    # A weight of 0.1 is empirically chosen to give exploration a modest influence.\n    final_priorities = tight_fit_score + 0.1 * exploration_bonus\n\n    priorities[suitable_bins_mask] = final_priorities\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with an exploration bonus favoring less utilized bins using a log-transformed score.\n    This aims for efficient packing and better distribution of items across bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Best Fit component: Inverse of the remaining space after packing\n    # Smaller remaining space yields a higher score. Add epsilon for stability.\n    best_fit_scores = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Exploration component: Reward bins with more remaining capacity (less utilized)\n    # Use log transformation to dampen the effect of very large remaining capacities\n    # and provide a smoother exploration bonus. Add 1 to avoid log(0).\n    exploration_scores = np.log1p(suitable_bins_remain_cap)\n\n    # Normalize exploration scores using min-max scaling to ensure they are comparable\n    min_exp_score = np.min(exploration_scores)\n    max_exp_score = np.max(exploration_scores)\n    if max_exp_score - min_exp_score > 1e-9:\n        normalized_exploration_scores = (exploration_scores - min_exp_score) / (max_exp_score - min_exp_score)\n    else:\n        normalized_exploration_scores = np.zeros_like(exploration_scores)\n\n    # Combine Best Fit and Exploration scores\n    # Weighting can be tuned. Prioritizing Best Fit slightly.\n    w_best_fit = 0.7\n    w_exploration = 0.3\n    combined_scores = w_best_fit * best_fit_scores + w_exploration * normalized_exploration_scores\n\n    # Assign combined scores to the priority array for suitable bins\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Balances Best Fit with an exploration bonus favoring bins with more remaining capacity,\n    using a weighted sum of inverse difference and normalized remaining capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Heuristic 1 component: Tightness score (higher for smaller remaining space)\n    # This is the inverse of the difference, so smaller difference is better (higher score)\n    tightness_scores = 1.0 / (suitable_bins_remain_cap - item + 1e-9) # Add epsilon for stability\n\n    # Heuristic 15/19 component: Exploration bonus (favor bins with more remaining capacity)\n    # Normalize remaining capacities of suitable bins to a [0, 1] range\n    min_rem_cap = np.min(suitable_bins_remain_cap)\n    max_rem_cap = np.max(suitable_bins_remain_cap)\n    if max_rem_cap - min_rem_cap > 1e-9: # Avoid division by zero if all remaining capacities are same\n        exploration_bonus = (suitable_bins_remain_cap - min_rem_cap) / (max_rem_cap - min_rem_cap)\n    else:\n        exploration_bonus = np.zeros_like(suitable_bins_remain_cap)\n\n    # Combine scores. Prioritize tightness but add exploration bonus.\n    # Weights can be tuned. Let's prioritize tightness slightly more.\n    combined_scores = 0.7 * tightness_scores + 0.3 * exploration_bonus\n\n    # Assign the combined scores to the original priority array for suitable bins\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Balances Best Fit with an exploration bonus favoring bins with more remaining capacity,\n    using a weighted sum of inverse difference and normalized remaining capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Heuristic 1 component: Tightness score (higher for smaller remaining space)\n    # This is the inverse of the difference, so smaller difference is better (higher score)\n    tightness_scores = 1.0 / (suitable_bins_remain_cap - item + 1e-9) # Add epsilon for stability\n\n    # Heuristic 15/19 component: Exploration bonus (favor bins with more remaining capacity)\n    # Normalize remaining capacities of suitable bins to a [0, 1] range\n    min_rem_cap = np.min(suitable_bins_remain_cap)\n    max_rem_cap = np.max(suitable_bins_remain_cap)\n    if max_rem_cap - min_rem_cap > 1e-9: # Avoid division by zero if all remaining capacities are same\n        exploration_bonus = (suitable_bins_remain_cap - min_rem_cap) / (max_rem_cap - min_rem_cap)\n    else:\n        exploration_bonus = np.zeros_like(suitable_bins_remain_cap)\n\n    # Combine scores. Prioritize tightness but add exploration bonus.\n    # Weights can be tuned. Let's prioritize tightness slightly more.\n    combined_scores = 0.7 * tightness_scores + 0.3 * exploration_bonus\n\n    # Assign the combined scores to the original priority array for suitable bins\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by balancing a tight fit (minimal residual space)\n    with an exploration bonus (favoring less utilized bins) using a\n    logarithmic transformation for better distribution.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Tight Fit (minimize remaining space after packing)\n    # Use inverse of remaining capacity after packing, add epsilon for stability.\n    # Higher score for smaller remaining space.\n    tightness_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Metric 2: Exploration Bonus (favor less utilized bins)\n    # Use log of remaining capacity. Higher score for bins with more remaining capacity.\n    # This encourages using less full bins to potentially improve overall packing.\n    exploration_score = np.log(suitable_bins_remain_cap + 1e-9)\n\n    # Combine scores with weights. Prioritize tightness slightly more.\n    # Weights can be tuned.\n    combined_scores = 0.6 * tightness_score + 0.4 * exploration_score\n\n    # Normalize scores to a [0, 1] range to ensure comparable priorities.\n    # Handle cases where all scores are identical to avoid division by zero.\n    min_score = np.min(combined_scores)\n    max_score = np.max(combined_scores)\n    if max_score - min_score > 1e-9:\n        normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n    else:\n        normalized_scores = np.ones_like(combined_scores) * 0.5 # Mid-range if all equal\n\n    priorities[suitable_bins_mask] = normalized_scores\n\n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by balancing a tight fit (minimal residual space)\n    with an exploration bonus (favoring less utilized bins) using a\n    logarithmic transformation for better distribution.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Tight Fit (minimize remaining space after packing)\n    # Use inverse of remaining capacity after packing, add epsilon for stability.\n    # Higher score for smaller remaining space.\n    tightness_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Metric 2: Exploration Bonus (favor less utilized bins)\n    # Use log of remaining capacity. Higher score for bins with more remaining capacity.\n    # This encourages using less full bins to potentially improve overall packing.\n    exploration_score = np.log(suitable_bins_remain_cap + 1e-9)\n\n    # Combine scores with weights. Prioritize tightness slightly more.\n    # Weights can be tuned.\n    combined_scores = 0.6 * tightness_score + 0.4 * exploration_score\n\n    # Normalize scores to a [0, 1] range to ensure comparable priorities.\n    # Handle cases where all scores are identical to avoid division by zero.\n    min_score = np.min(combined_scores)\n    max_score = np.max(combined_scores)\n    if max_score - min_score > 1e-9:\n        normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n    else:\n        normalized_scores = np.ones_like(combined_scores) * 0.5 # Mid-range if all equal\n\n    priorities[suitable_bins_mask] = normalized_scores\n\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: First Fit Decreasing Inspired - Prioritize bins that have enough capacity\n    # but are not excessively large compared to the item. This aims for a balance\n    # between fitting the item and leaving sufficient space for future items.\n    # We use the ratio of bin capacity to item size as a proxy for \"good fit\".\n    # A higher ratio means the bin is \"just enough\" or slightly larger.\n    fit_quality_scores = suitable_bins_caps / item\n    \n    # Metric 2: Remaining Capacity Variance - Prioritize bins that reduce the variance\n    # of remaining capacities among suitable bins. This encourages a more uniform\n    # distribution of remaining capacities, potentially leading to better overall packing.\n    # We want to favor bins that, after placing the item, result in remaining capacities\n    # that are closer to the average remaining capacity.\n    remaining_after_placement = suitable_bins_caps - item\n    \n    if remaining_after_placement.size > 0:\n        mean_remaining = np.mean(remaining_after_placement)\n        # We want bins that result in remaining capacity closer to the mean.\n        # So, the score is inversely proportional to the absolute difference from the mean.\n        variance_reduction_scores = 1.0 / (np.abs(remaining_after_placement - mean_remaining) + 1e-6)\n        \n        # Normalize variance reduction scores to [0, 1]\n        max_vr_score = np.max(variance_reduction_scores)\n        if max_vr_score > 1e-6:\n            normalized_variance_reduction = variance_reduction_scores / max_vr_score\n        else:\n            normalized_variance_reduction = np.ones_like(variance_reduction_scores) * 0.5\n    else:\n        normalized_variance_reduction = np.array([])\n\n    # Normalize fit quality scores to [0, 1]\n    min_fq = np.min(fit_quality_scores)\n    max_fq = np.max(fit_quality_scores)\n    if max_fq - min_fq > 1e-6:\n        normalized_fit_quality = (fit_quality_scores - min_fq) / (max_fq - min_fq)\n    else:\n        normalized_fit_quality = np.ones_like(fit_quality_scores) * 0.5\n\n    # Combine scores with weights.\n    # Giving more weight to fit quality (0.6) as it directly relates to how well\n    # the item fits. The variance reduction (0.4) acts as a secondary objective\n    # to promote better long-term packing.\n    combined_scores = 0.6 * normalized_fit_quality + 0.4 * normalized_variance_reduction\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}