[
  {
    "stdout_filepath": "problem_iter11_response0.txt_stdout.txt",
    "code_path": "problem_iter11_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit (inverse residual) with a normalized exploration bonus\n    (log-transformed remaining capacity), balancing efficiency and spread.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit - inverse of remaining space after placement.\n    best_fit_scores = 1.0 / (suitable_bins_caps - item + 1e-9)\n\n    # Metric 2: Exploration - log-transformed remaining capacity to favor less utilized bins.\n    # Add 1 to avoid log(0) and provide a smoother bonus.\n    exploration_scores = np.log1p(suitable_bins_caps)\n\n    # Normalize exploration scores using min-max scaling.\n    min_exp_score = np.min(exploration_scores)\n    max_exp_score = np.max(exploration_scores)\n    if max_exp_score - min_exp_score > 1e-9:\n        normalized_exploration_scores = (exploration_scores - min_exp_score) / (max_exp_score - min_exp_score)\n    else:\n        normalized_exploration_scores = np.zeros_like(exploration_scores)\n\n    # Combine scores with a focus on Best Fit (0.7) and balanced exploration (0.3).\n    # This combination aims for efficient packing while encouraging better bin distribution.\n    combined_scores = 0.7 * best_fit_scores + 0.3 * normalized_exploration_scores\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 83.62584762664541,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response1.txt_stdout.txt",
    "code_path": "problem_iter11_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a normalized bonus for larger remaining capacity,\n    using a weighted sum for balanced decision-making. Favors bins that are\n    almost full but can still accommodate the item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    # Metric 1: Best Fit - prioritize bins with minimal remaining capacity after placement.\n    # Higher score for smaller residual space.\n    remaining_after_placement = suitable_bins_caps - item\n    # Inverse relationship: smaller residual -> higher score. Add epsilon for stability.\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-9)\n    \n    # Metric 2: Exploration/Larger Bin Preference - favor bins with more capacity initially.\n    # This encourages not always picking the absolute tightest, promoting diversification.\n    # We'll use a logarithmic scale for remaining capacity to de-emphasize very large bins\n    # and focus on bins that are \"reasonably\" large but not excessively so.\n    # log1p is used to handle cases where remaining capacity is 0 after placement,\n    # and to provide a smoother scaling than a simple linear approach.\n    exploration_scores = np.log1p(suitable_bins_caps)\n    \n    # Normalize Best Fit scores (min-max scaling)\n    if suitable_bins_caps.size > 1:\n        min_bf = np.min(best_fit_scores)\n        max_bf = np.max(best_fit_scores)\n        range_bf = max_bf - min_bf\n        if range_bf > 1e-9:\n            normalized_best_fit = (best_fit_scores - min_bf) / range_bf\n        else:\n            normalized_best_fit = np.ones_like(best_fit_scores) # All suitable bins offer same tightness score\n    elif suitable_bins_caps.size == 1:\n        normalized_best_fit = np.array([1.0])\n    else:\n        normalized_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Exploration scores (min-max scaling)\n    if suitable_bins_caps.size > 1:\n        min_exp = np.min(exploration_scores)\n        max_exp = np.max(exploration_scores)\n        range_exp = max_exp - min_exp\n        if range_exp > 1e-9:\n            normalized_exploration = (exploration_scores - min_exp) / range_exp\n        else:\n            normalized_exploration = np.zeros_like(exploration_scores) # All suitable bins have same initial capacity\n    elif suitable_bins_caps.size == 1:\n        normalized_exploration = np.array([1.0]) # If only one bin, it's maximally \"exploratory\" in this context\n    else:\n        normalized_exploration = np.zeros_like(exploration_scores)\n\n    # Combine normalized scores using a weighted sum.\n    # We give a slightly higher weight to Best Fit, as tight packing is crucial for BPP.\n    # The exploration bonus helps to prevent premature fragmentation.\n    combined_scores = 0.7 * normalized_best_fit + 0.3 * normalized_exploration\n    \n    priorities[suitable_bins_mask] = combined_scores\n    \n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 36.0,
    "cyclomatic_complexity": 8.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response2.txt_stdout.txt",
    "code_path": "problem_iter11_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (tightness) with a 'fair share' exploration bonus,\n    prioritizing bins that are neither too full nor too empty, relative to others.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Best Fit component: Inverse of remaining capacity after placing the item.\n    # Smaller difference implies a tighter fit and higher score.\n    tightness_scores = 1.0 / (valid_bins_remain_cap - item + 1e-9)\n\n    # Exploration component: Penalize bins that are excessively full or empty\n    # relative to the average remaining capacity of suitable bins. This encourages\n    # a more balanced distribution. We use a quadratic penalty.\n    avg_remain_cap = np.mean(valid_bins_remain_cap)\n    # Deviation from average, squared to penalize larger deviations more.\n    # We add a small constant to avoid zero deviation resulting in zero penalty.\n    fairness_penalty = (valid_bins_remain_cap - avg_remain_cap)**2 / (avg_remain_cap + 1e-9)\n\n    # Combine scores: Higher tightness is good, lower penalty (closer to avg) is good.\n    # We subtract the penalty as it's a negative aspect.\n    # Weights can be tuned; here, tightness is prioritized.\n    combined_scores = tightness_scores - fairness_penalty * 0.2 # Tunable parameter for penalty influence\n\n    # Normalize scores to be between 0 and 1.\n    min_score = np.min(combined_scores)\n    max_score = np.max(combined_scores)\n\n    if max_score - min_score > 1e-9:\n        priorities[suitable_bins_mask] = (combined_scores - min_score) / (max_score - min_score)\n    elif np.any(suitable_bins_mask):\n        # If all suitable bins have very similar combined scores, distribute equally.\n        priorities[suitable_bins_mask] = 1.0 / np.sum(suitable_bins_mask)\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 39.39968089349822,
    "SLOC": 17.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response3.txt_stdout.txt",
    "code_path": "problem_iter11_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit's tightness with an exploration bonus favoring less utilized bins,\n    using a balanced approach to combine these factors.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return np.zeros_like(bins_remain_cap)\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Best Fit Component: Prioritize bins with minimal remaining capacity after placement.\n    # Negative of remaining capacity to favor smaller (more negative) values for minimization.\n    best_fit_scores = -(suitable_bins_remain_cap - item)\n\n    # Exploration Component: Favor bins that are less utilized (larger original capacity).\n    # Normalize remaining capacities of suitable bins using min-max scaling.\n    min_cap = np.min(suitable_bins_remain_cap)\n    max_cap = np.max(suitable_bins_remain_cap)\n    if max_cap - min_cap > 1e-9:\n        exploration_scores = (suitable_bins_remain_cap - min_cap) / (max_cap - min_cap)\n    else:\n        exploration_scores = np.zeros_like(suitable_bins_remain_cap)\n\n    # Combined Score: Balance Best Fit and Exploration.\n    # A weighted sum is used. We give a slightly higher weight to Best Fit (tightness)\n    # as it's generally a primary goal in BPP, while exploration acts as a tie-breaker\n    # or secondary optimization.\n    # We add exploration_scores to best_fit_scores. Higher values (closer to zero for BF) are better.\n    # Exploration scores are positive and higher is better.\n    combined_scores = best_fit_scores + 0.7 * exploration_scores # Weight for exploration\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    # If all suitable bins are identical in terms of combined score, argmin will pick the first.\n    # This heuristic aims to find a good balance, leaning towards tight fits but\n    # considering bin utilization as a secondary factor.\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response4.txt_stdout.txt",
    "code_path": "problem_iter11_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a tight fit metric with an exploration bonus, favoring bins\n    that minimize remaining space while also considering less utilized bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Tight Fit (similar to priority_v0)\n    # Prioritize bins that leave minimal remaining space after packing.\n    # Add epsilon for numerical stability. Higher score for smaller remaining space.\n    tightness_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Metric 2: Exploration Bonus (inspired by priority_v0 and priority_v10/11/12)\n    # Favor bins that are less full (more remaining capacity).\n    # Using log1p for slightly better distribution at lower capacities.\n    # Higher score for bins with more remaining capacity.\n    exploration_score = np.log1p(suitable_bins_remain_cap)\n\n    # Combine scores with weights.\n    # Giving a slight edge to tightness, but exploration is also important.\n    # These weights can be tuned based on empirical performance.\n    combined_scores = 0.55 * tightness_score + 0.45 * exploration_score\n\n    # Normalize combined scores to a [0, 1] range.\n    # This ensures that the relative priorities are maintained even with different\n    # scales of the individual metrics. Handle cases where all scores are equal.\n    min_score = np.min(combined_scores)\n    max_score = np.max(combined_scores)\n    if max_score - min_score > 1e-9:\n        normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n    else:\n        normalized_scores = np.ones_like(combined_scores) * 0.5\n\n    priorities[suitable_bins_mask] = normalized_scores\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 84.95213402473077,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response5.txt_stdout.txt",
    "code_path": "problem_iter11_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with an exploration bonus favoring less utilized bins,\n    using logarithmic scaling for exploration to enhance bin distribution.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit (minimize remaining space after packing)\n    # Higher score for bins with less remaining space after packing the item.\n    # Adding a small epsilon to avoid division by zero.\n    tightness_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Metric 2: Exploration Bonus (favor less utilized bins)\n    # Logarithmic scaling of remaining capacity. Favors bins that are less full,\n    # encouraging a more even distribution of items across bins.\n    exploration_score = np.log(suitable_bins_remain_cap + 1e-9)\n\n    # Combine scores. Weighting favors tightness slightly, but exploration\n    # provides a bonus for less-used bins. These weights are subject to tuning.\n    combined_scores = 0.55 * tightness_score + 0.45 * exploration_score\n\n    # Normalize the combined scores to a [0, 1] range for consistent priority.\n    # Avoid division by zero if all combined scores are identical.\n    min_score = np.min(combined_scores)\n    max_score = np.max(combined_scores)\n    if max_score - min_score > 1e-9:\n        normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n    else:\n        normalized_scores = np.ones_like(combined_scores) * 0.5 # Default to mid-range if all scores are equal\n\n    priorities[suitable_bins_mask] = normalized_scores\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 84.95213402473077,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response6.txt_stdout.txt",
    "code_path": "problem_iter11_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a refined Best Fit with a dynamic Exploration bonus.\n    Prioritizes tight fits for larger items and exploration for smaller items,\n    adapting the strategy based on item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_caps - item\n\n    # Metric 1: Refined Best Fit - favors bins with smallest remaining capacity after placement.\n    # Using log1p to compress larger gaps and emphasize smaller ones. Add epsilon for stability.\n    best_fit_scores = np.log1p(1.0 / (remaining_after_placement + 1e-6))\n\n    # Metric 2: Exploration Bonus - favors bins that have significantly more remaining capacity.\n    # This is achieved by rewarding bins that are further from the minimum possible remaining capacity\n    # (after placing the item). Using min-max scaling on the remaining space after placement.\n    min_rem_after = np.min(remaining_after_placement)\n    max_rem_after = np.max(remaining_after_placement)\n\n    exploration_scores = np.zeros_like(remaining_after_placement)\n    if max_rem_after > min_rem_after:\n        # Normalize remaining capacity after placement to get exploration score (0 to 1)\n        exploration_scores = (remaining_after_placement - min_rem_after) / (max_rem_after - min_rem_after)\n    else:\n        # If all suitable bins leave the same remaining capacity, no exploration bonus from this metric.\n        pass\n\n    # Dynamic Weighting based on item size.\n    # Larger items benefit more from a precise fit (Best Fit).\n    # Smaller items can afford to explore less utilized bins (Exploration Bonus).\n    # Assume bin capacity is normalized to 1.0 for a relative item size assessment.\n    # If bin capacities vary significantly, a different normalization might be needed.\n    # For simplicity, we'll use item size directly, assuming it's scaled appropriately.\n    # Let's assume `item` is on a scale where 0.5 means it's half the typical bin capacity.\n    \n    # A simple heuristic: if item is more than 50% of typical capacity, prioritize best fit.\n    # If item is less than 20%, prioritize exploration. In between, a mix.\n    # Using item size as a proxy for its \"impact\" on bin fullness.\n    \n    # Weights sum to 1.0.\n    # For small items (e.g., item < 0.3): higher exploration, lower best fit.\n    # For large items (e.g., item > 0.7): higher best fit, lower exploration.\n    \n    # Example: Item size normalized to [0, 1] range, representing proportion of bin capacity.\n    # If actual item sizes are larger, they would need to be scaled.\n    # Let's assume `item` is already scaled relative to a standard bin capacity.\n\n    # Define a threshold, e.g., 0.5, for medium-sized items.\n    threshold_medium = 0.5 \n    \n    # Smooth transition for weights\n    weight_best_fit = np.clip(item / threshold_medium, 0.1, 1.0) # Favors best fit for larger items\n    weight_exploration = np.clip((threshold_medium - item) / threshold_medium, 0.1, 1.0) # Favors exploration for smaller items\n\n    # Normalize weights to ensure they sum to 1 if they cross thresholds or are outside bounds.\n    total_weight = weight_best_fit + weight_exploration\n    if total_weight > 1e-6:\n        weight_best_fit /= total_weight\n        weight_exploration /= total_weight\n    else: # Fallback if both are effectively zero\n        weight_best_fit = 0.5\n        weight_exploration = 0.5\n\n    # Combine scores\n    combined_scores = (weight_best_fit * best_fit_scores +\n                       weight_exploration * exploration_scores)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 72.92580773833267,
    "SLOC": 29.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response7.txt_stdout.txt",
    "code_path": "problem_iter11_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a strong 'Best Fit' strategy with a 'Logarithmic Exploration' bonus.\n    Prioritizes snugly fitting items while also giving a slight preference\n    to bins with more remaining capacity to encourage spread.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_caps - item\n\n    # Metric 1: Best Fit - favors bins with minimum remaining space after placement.\n    # Using reciprocal emphasizes smaller remaining capacities.\n    # Add epsilon to avoid division by zero.\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-6)\n\n    # Metric 2: Exploration/Uniformity - favors bins with more open space.\n    # Using log1p compresses the range of remaining capacities, giving a boost\n    # to moderately open bins without overly favoring extremely empty ones.\n    exploration_scores = np.log1p(suitable_bins_caps)\n\n    # Combine scores with weights.\n    # Weighting: Give more importance to Best Fit, but include Exploration.\n    # A weight of 0.7 for Best Fit and 0.3 for Exploration offers a good balance.\n    weight_best_fit = 0.7\n    weight_exploration = 0.3\n\n    combined_scores = (weight_best_fit * best_fit_scores +\n                       weight_exploration * exploration_scores)\n\n    # Apply combined scores to the priorities array\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 81.91065017949742,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response8.txt_stdout.txt",
    "code_path": "problem_iter11_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a normalized bonus for larger remaining capacity,\n    using a weighted sum for balanced decision-making. Favors bins that are\n    nearly full but can still accommodate the item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    # Metric 1: Best Fit - prioritize bins with minimal remaining capacity after placement.\n    # Add a small epsilon to avoid division by zero and ensure non-zero scores for tight fits.\n    remaining_after_placement = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-6)\n    \n    # Metric 2: Exploration/Emptiness Bonus - reward bins that are less utilized initially.\n    # Using log1p to handle zero capacities gracefully and provide diminishing returns.\n    emptiness_bonus_scores = np.log1p(suitable_bins_caps)\n    \n    # Normalize Best Fit scores using min-max scaling.\n    if suitable_bins_caps.size > 1:\n        min_bf = np.min(best_fit_scores)\n        max_bf = np.max(best_fit_scores)\n        range_bf = max_bf - min_bf\n        if range_bf > 1e-6:\n            normalized_best_fit = (best_fit_scores - min_bf) / range_bf\n        else:\n            normalized_best_fit = np.zeros_like(best_fit_scores) # All suitable bins offer same tightness\n    elif suitable_bins_caps.size == 1:\n        normalized_best_fit = np.array([1.0]) # Only one suitable bin, highest possible score\n    else:\n        normalized_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Emptiness Bonus scores using min-max scaling.\n    if suitable_bins_caps.size > 1:\n        min_eb = np.min(emptiness_bonus_scores)\n        max_eb = np.max(emptiness_bonus_scores)\n        range_eb = max_eb - min_eb\n        if range_eb > 1e-6:\n            normalized_emptiness_bonus = (emptiness_bonus_scores - min_eb) / range_eb\n        else:\n            normalized_emptiness_bonus = np.zeros_like(emptiness_bonus_scores) # All suitable bins have same emptiness\n    elif suitable_bins_caps.size == 1:\n        normalized_emptiness_bonus = np.array([1.0]) # Only one suitable bin, highest possible score\n    else:\n        normalized_emptiness_bonus = np.zeros_like(emptiness_bonus_scores)\n\n    # Combine normalized scores using a weighted sum.\n    # Increased weight on best fit, while still providing a bonus for less full bins.\n    # Weights are tuned to balance immediate fit with longer-term space utilization.\n    combined_scores = 0.7 * normalized_best_fit + 0.3 * normalized_emptiness_bonus\n    \n    priorities[suitable_bins_mask] = combined_scores\n    \n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 36.0,
    "cyclomatic_complexity": 8.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response9.txt_stdout.txt",
    "code_path": "problem_iter11_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a nuanced best-fit approach with an exploration bonus favoring less utilized bins,\n    and a subtle preference for bins with more overall remaining capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Modified Best Fit (from priority_v0)\n    # Prioritizes bins leaving a \"sweet spot\" residual capacity to avoid tiny unusable gaps.\n    remaining_after_placement_m1 = suitable_bins_caps - item\n    target_residual = item * 0.2  # Target residual capacity ~20% of item size\n    # Gaussian-like function: higher score for residuals closer to target_residual\n    # Add small epsilon to avoid division by zero if target_residual is 0.\n    best_fit_scores = np.exp(-((remaining_after_placement_m1 - target_residual) / (target_residual + 1e-6))**2)\n\n    # Metric 2: Exploration Bonus (inspired by priority_v1, simpler version)\n    # Favors bins that are less full *after* placement, relative to other suitable bins.\n    min_rem_after_m2 = np.min(remaining_after_placement_m1)\n    max_rem_after_m2 = np.max(remaining_after_placement_m1)\n    \n    exploration_scores = np.zeros_like(suitable_bins_caps)\n    if max_rem_after_m2 > min_rem_after_m2:\n        # Normalized remaining capacity after placement: higher for more empty bins\n        exploration_scores = (remaining_after_placement_m1 - min_rem_after_m2) / (max_rem_after_m2 - min_rem_after_m2)\n    else:\n        # If all suitable bins result in the same remaining capacity, no exploration bonus from this diff.\n        # Default to 0.5 for any such bins to avoid bias.\n        exploration_scores = np.ones_like(suitable_bins_caps) * 0.5\n\n    # Metric 3: Usage Proxy (favors bins with more total remaining capacity)\n    # This is a simple proxy for less-used bins.\n    min_cap_all = np.min(bins_remain_cap)\n    max_cap_all = np.max(bins_remain_cap)\n    \n    usage_scores = np.zeros_like(suitable_bins_caps)\n    if max_cap_all > min_cap_all:\n        # Normalize current remaining capacities. Higher score for more remaining capacity.\n        normalized_current_caps = (suitable_bins_caps - min_cap_all) / (max_cap_all - min_cap_all)\n        usage_scores = normalized_current_caps\n    else:\n        # If all bins have same capacity, this metric doesn't differentiate.\n        usage_scores = np.ones_like(suitable_bins_caps) * 0.5\n\n    # Combine scores: Heavy emphasis on nuanced best-fit, moderate on exploration, light on usage.\n    # Weights are chosen to balance finding good fits with spreading items.\n    # 0.6 for Best Fit (primary, quality of fit)\n    # 0.3 for Exploration (secondary, diversity)\n    # 0.1 for Usage (tertiary, simple preference for emptier bins)\n    combined_scores = 0.6 * best_fit_scores + 0.3 * exploration_scores + 0.1 * usage_scores\n\n    # Ensure scores are within a reasonable range and handle potential NaNs/Infs\n    combined_scores = np.nan_to_num(combined_scores, nan=0.0, posinf=1.0, neginf=0.0)\n    combined_scores = np.clip(combined_scores, 0.0, 1.0)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 43.87714399680894,
    "SLOC": 29.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  }
]