[
  {
    "stdout_filepath": "problem_iter5_response0.txt_stdout.txt",
    "code_path": "problem_iter5_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit's tight packing with an epsilon-greedy exploration bonus\n    using min-max scaling for robust exploration of less full bins.\n    \"\"\"\n    epsilon = 0.1\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if np.any(suitable_bins_mask):\n        suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n        \n        # Best Fit component: Prioritize bins with minimal remaining capacity after packing\n        best_fit_scores = 1.0 / (suitable_bins_caps - item + 1e-9)\n        \n        # Exploration component: Bonus for bins with more remaining capacity, normalized\n        if suitable_bins_caps.size > 1:\n            min_cap = np.min(suitable_bins_caps)\n            max_cap = np.max(suitable_bins_caps)\n            # Normalize remaining capacities to encourage exploration of emptier bins\n            normalized_remaining_cap = (suitable_bins_caps - min_cap) / (max_cap - min_cap + 1e-9)\n            exploration_bonus = epsilon * normalized_remaining_cap\n        else:\n            # If only one suitable bin, no relative exploration bonus\n            exploration_bonus = np.zeros_like(suitable_bins_caps)\n        \n        priorities[suitable_bins_mask] = best_fit_scores + exploration_bonus\n        \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.198244914240141,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response1.txt_stdout.txt",
    "code_path": "problem_iter5_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight fit with diversified exploration using scaled inverse remaining capacity\n    and a sigmoid-based penalty for bins that are too full.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n    \n    if valid_bins_remain_cap.size > 0:\n        # Metric 1: Tight Fit Score (inverse of remaining capacity after fitting)\n        # Favors bins that will have less space left after item is placed.\n        fit_scores = 1.0 / (valid_bins_remain_cap - item + 1e-9)\n        \n        # Metric 2: Diversification Bonus (inverse of current remaining capacity)\n        # Favors less utilized bins to prevent premature concentration.\n        # Use original remaining capacity for this, scaled.\n        diversification_scores = 1.0 / (bins_remain_cap[valid_bins_mask] + 1e-9)\n        \n        # Combine using a weighted sum. Heuristic 19/20's balanced approach inspires this.\n        # We give slightly more weight to the tight fit, but diversification is significant.\n        combined_priorities = 0.6 * fit_scores + 0.4 * diversification_scores\n        \n        # Assign priorities back to the original array\n        priorities[valid_bins_mask] = combined_priorities\n        \n        # Normalize priorities to ensure the highest score is clearly dominant\n        max_priority = np.max(priorities[valid_bins_mask])\n        if max_priority > 0:\n            priorities[valid_bins_mask] /= max_priority\n            \n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response2.txt_stdout.txt",
    "code_path": "problem_iter5_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a normalized exploration bonus for less full bins.\n\n    Prioritizes bins that offer a tight fit, while also exploring less utilized\n    bins through a normalized bonus, balancing immediate fit with future options.\n    \"\"\"\n    epsilon = 0.1  # Weight for the exploration bonus\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n        # Best Fit component: Prioritize bins with minimal remaining capacity after placing the item\n        # Adding a small epsilon to the denominator to prevent division by zero.\n        best_fit_score = 1.0 / (suitable_bins_cap - item + 1e-9)\n\n        # Exploration component: Bonus for bins with larger remaining capacity (less full)\n        # Normalize the remaining capacity using min-max scaling to provide a consistent bonus range.\n        if suitable_bins_cap.size > 1:\n            min_cap = np.min(suitable_bins_cap)\n            max_cap = np.max(suitable_bins_cap)\n            # Normalize remaining capacity: 0 for the fullest, 1 for the emptiest among suitable bins\n            normalized_exploration_score = (suitable_bins_cap - min_cap) / (max_cap - min_cap + 1e-9)\n            exploration_bonus = epsilon * normalized_exploration_score\n        else:\n            # If only one suitable bin, no relative exploration bonus is meaningful\n            exploration_bonus = np.zeros_like(suitable_bins_cap)\n\n        # Combine the Best Fit score with the exploration bonus\n        priorities[suitable_bins_mask] = best_fit_score + exploration_bonus\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 71.27044276027125,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response3.txt_stdout.txt",
    "code_path": "problem_iter5_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (tightest fit) with an exploration bonus for less utilized bins.\n    It prioritizes bins that are nearly full but also gives a chance to less full bins.\n    \"\"\"\n    epsilon = 0.05 # Exploration factor\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Best Fit component: inverse of the remaining capacity after placing the item.\n    # Smaller remaining capacity is better (tighter fit). Add epsilon for stability.\n    best_fit_score = 1.0 / (suitable_bins_cap - item + 1e-9)\n\n    # Exploration component: bonus for bins that are less full (more remaining capacity).\n    # This is a scaled difference from the average remaining capacity of suitable bins.\n    # Higher remaining capacity gets a higher exploration bonus.\n    avg_suitable_cap = np.mean(suitable_bins_cap)\n    exploration_bonus = np.maximum(0, suitable_bins_cap - avg_suitable_cap) * epsilon\n\n    # Combine Best Fit and Exploration.\n    # The exploration bonus adds to the best fit score.\n    # A small exploration factor prevents it from overpowering the best fit.\n    priorities[suitable_bins_mask] = best_fit_score + exploration_bonus\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response4.txt_stdout.txt",
    "code_path": "problem_iter5_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a smoothed penalty for large remaining capacities and normalization.\n\n    Prioritizes bins that tightly fit the item, while smoothly penalizing\n    bins that would leave excessive empty space, ensuring a balanced approach.\n    \"\"\"\n    available_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(available_bins_mask):\n        valid_capacities = bins_remain_cap[available_bins_mask]\n\n        # Heuristic 1: \"Best Fit\" component using inverse of difference\n        # Higher score for smaller differences (tighter fit)\n        differences = valid_capacities - item\n        best_fit_scores = 1.0 / (differences + 1e-9)\n\n        # Heuristic 2: Sigmoid applied to the *remaining* capacity after fitting\n        # Penalizes bins that will have a lot of space left, but smoothly.\n        # Centered to give higher scores to smaller remaining capacities.\n        scaling_factor = 15.0 # Increased scaling factor for a sharper penalty\n        potential_remaining_capacities = valid_capacities - item\n        # Using a slightly different sigmoid formulation for a smoother penalty curve\n        sigmoided_penalty = 1 / (1 + np.exp(scaling_factor * (potential_remaining_capacities - 0.2)))\n\n        # Combine scores: Prioritize good fits and penalize large remaining capacities\n        combined_scores = best_fit_scores * sigmoided_penalty\n\n        # Normalize the combined scores to prevent extreme values and ensure comparability\n        if combined_scores.size > 0:\n            min_score = np.min(combined_scores)\n            max_score = np.max(combined_scores)\n            if max_score - min_score > 1e-9:\n                normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n            else:\n                normalized_scores = np.ones_like(combined_scores) * 0.5\n        else:\n            normalized_scores = np.array([])\n\n        priorities[available_bins_mask] = normalized_scores\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.198244914240141,
    "SLOC": 22.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response5.txt_stdout.txt",
    "code_path": "problem_iter5_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines best-fit tight packing with an exploration bonus for less utilized bins.\n    Prioritizes bins that leave minimal residual space after packing, while also\n    offering a slight preference for less full bins to encourage exploration.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_cap - item\n\n    # Core \"Best Fit\" score: Higher score for smaller remaining capacity (tighter fit)\n    # Using reciprocal to emphasize very small remaining capacities.\n    tight_fit_score = 1.0 / (remaining_after_placement + 1e-9)\n\n    # Exploration/Diversification component: Bonus for bins that are less full.\n    # This is inspired by epsilon-greedy strategies where we sometimes explore\n    # non-optimal choices to potentially find better overall solutions.\n    # We'll scale this bonus by a small factor to ensure \"best fit\" is primary.\n    \n    # Calculate a measure of how \"full\" each suitable bin is.\n    fill_ratio = (suitable_bins_cap - remaining_after_placement) / (suitable_bins_cap + 1e-9)\n    \n    # A simple exploration bonus based on how much space is left relative to initial capacity.\n    # We want to favor bins with *more* remaining capacity, as an exploration incentive.\n    # The score should be higher for larger remaining capacities.\n    exploration_score = remaining_after_placement / (suitable_bins_cap + 1e-9)\n    \n    # Combine scores: Prioritize tight fits, but add a small bonus for exploration.\n    # The tight_fit_score is the primary driver. The exploration_score is a secondary boost.\n    # A simple weighted sum:\n    # We want to heavily favor tight fits, so the weight for tight_fit_score should be higher.\n    # Let's use weights that reflect this. For example, weight = 1 for tight fit, and a smaller weight for exploration.\n    \n    # The idea from Heuristics 7/8/9/11/12/15/16 which used an epsilon-greedy approach:\n    # They assigned a primary score (like best fit) and then added an exploration bonus.\n    # Let's adapt that. The `tight_fit_score` is our primary score.\n    # For the exploration bonus, we want to favor bins that are NOT the tightest fit.\n    # Heuristics 11/12/15/16 used min-max scaling for the exploration bonus, which is robust.\n    # Let's use a simplified exploration bonus that is higher for bins with larger remaining capacity.\n    \n    # Consider the normalized remaining capacity:\n    # Min-max scaling for exploration bonus:\n    min_rem = np.min(remaining_after_placement)\n    max_rem = np.max(remaining_after_placement)\n    \n    exploration_bonus = np.zeros_like(remaining_after_placement)\n    if max_rem > min_rem: # Avoid division by zero if all remaining capacities are the same\n        exploration_bonus = (remaining_after_placement - min_rem) / (max_rem - min_rem)\n    else:\n        # If all remaining capacities are the same, no exploration bonus based on difference.\n        # Or assign a small constant bonus if needed, but zero is fine for diversity.\n        pass # exploration_bonus remains zeros\n\n    # Combine: Primarily driven by tight fit, with an additive exploration bonus.\n    # The exploration bonus is scaled to be less influential than the tight fit.\n    # A small multiplier for the exploration bonus ensures it acts as a tie-breaker or diversification element.\n    \n    final_priorities = tight_fit_score + 0.1 * exploration_bonus # 0.1 is an arbitrary small weight for exploration\n\n    priorities[suitable_bins_mask] = final_priorities\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 71.27044276027125,
    "SLOC": 20.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response6.txt_stdout.txt",
    "code_path": "problem_iter5_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a scaled exploration bonus for less utilized bins,\n    promoting both tight fits and exploring under-filled bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins_mask = bins_remain_cap >= item\n\n    if not np.any(valid_bins_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n    \n    # Best Fit component: Inverse difference\n    differences = valid_bins_remain_cap - item\n    best_fit_scores = 1.0 / (differences + 1e-9)\n\n    # Exploration component: Reward less utilized bins (higher original capacity)\n    # We'll use min-max scaling on the *original* capacities of valid bins\n    # to create an exploration bonus. Assuming bins_remain_cap is derived from\n    # an initial bin capacity, we need to infer or have access to original capacities.\n    # For this example, let's assume initial capacity is related to remaining capacity\n    # before any items were placed, or we can infer it. A simple proxy is to assume\n    # bins with larger remaining capacity *might* have had larger original capacity.\n    # A more robust implementation would pass initial bin capacities.\n    # Let's simulate inferring original capacity by assuming a maximum possible capacity\n    # or a distribution if not provided. For simplicity, we'll use a proxy for\n    # original capacity: if we don't have it, we can use remaining capacity as a weak proxy\n    # or assume a default large capacity for all bins initially.\n    # A better approach: assume bins_remain_cap is from a fixed initial capacity C.\n    # Then original_capacity = C - (total_item_size_in_bin). This is complex for online.\n    # Let's simplify: Prioritize bins that are NOT too full.\n    # Higher remaining capacity means less utilized. Let's reward that.\n\n    # Using remaining capacity itself as a proxy for \"less utilized\"\n    # Scale remaining capacity to give a bonus. Min-max scale it.\n    min_rem_cap = np.min(valid_bins_remain_cap)\n    max_rem_cap = np.max(valid_bins_remain_cap)\n    \n    if max_rem_cap - min_rem_cap > 1e-9:\n        exploration_bonus = (valid_bins_remain_cap - min_rem_cap) / (max_rem_cap - min_rem_cap)\n    else:\n        exploration_bonus = np.zeros_like(valid_bins_remain_cap)\n\n\n    # Combine Best Fit and Exploration: weighted sum\n    # Weights can be tuned. Let's give slightly more weight to Best Fit.\n    w_best_fit = 0.7\n    w_exploration = 0.3\n\n    combined_scores = w_best_fit * best_fit_scores + w_exploration * exploration_bonus\n    \n    priorities[valid_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 83.62584762664541,
    "SLOC": 19.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response7.txt_stdout.txt",
    "code_path": "problem_iter5_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (tightest residual space) with a bonus for bins that are\n    larger (more potential for future items), using a normalized score.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    # Metric 1: Best Fit - prioritize bins with minimal remaining capacity after placement.\n    # Higher score for smaller remaining capacity.\n    remaining_after_placement = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-6)\n    \n    # Metric 2: Favor larger bins - prioritize bins that have more capacity initially.\n    # This provides a diversification aspect, not always picking the absolute tightest.\n    large_bin_scores = suitable_bins_caps\n    \n    # Combine metrics: A weighted sum to balance tight fit and larger bin preference.\n    # The weights can be tuned, but let's assume equal importance for now.\n    # We want to normalize these scores to have a comparable range.\n    \n    # Normalize Best Fit scores (they are already positive and higher is better)\n    if np.max(best_fit_scores) > 1e-6:\n        normalized_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        normalized_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Large Bin scores\n    if np.max(large_bin_scores) > 1e-6:\n        normalized_large_bin = large_bin_scores / np.max(large_bin_scores)\n    else:\n        normalized_large_bin = np.zeros_like(large_bin_scores)\n\n    # Combine normalized scores (e.g., simple average)\n    # Higher priority indicates a better bin choice.\n    combined_scores = 0.7 * normalized_best_fit + 0.3 * normalized_large_bin # Example weights\n    \n    priorities[suitable_bins_mask] = combined_scores\n    \n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 20.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response8.txt_stdout.txt",
    "code_path": "problem_iter5_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a normalized exploration bonus based on remaining capacity.\n    Prioritizes tight fits and encourages exploring less utilized bins.\n    \"\"\"\n    epsilon = 0.1\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n        # Best Fit component: Inverse of remaining capacity after placement.\n        # Higher score for bins with less remaining space after fitting the item.\n        best_fit_score = 1.0 / (suitable_bins_cap - item + 1e-9)\n\n        # Exploration component: Normalized bonus for larger remaining capacity.\n        # Uses min-max scaling to provide a bonus to bins with more free space,\n        # relative to other suitable bins. This encourages diversification.\n        exploration_bonus = np.zeros_like(suitable_bins_cap, dtype=float)\n        if suitable_bins_cap.size > 1:\n            min_cap_suitable = np.min(suitable_bins_cap)\n            max_cap_suitable = np.max(suitable_bins_cap)\n            range_cap_suitable = max_cap_suitable - min_cap_suitable\n            if range_cap_suitable > 1e-9:\n                exploration_bonus = epsilon * (suitable_bins_cap - min_cap_suitable) / range_cap_suitable\n            else:\n                exploration_bonus = np.zeros_like(suitable_bins_cap) # All suitable bins have same capacity\n\n        # Combine scores: Weighted sum of Best Fit and Exploration Bonus.\n        # This balances the preference for tight fits with the need to explore.\n        combined_score = best_fit_score + exploration_bonus\n        priorities[suitable_bins_mask] = combined_score\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 71.27044276027125,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response9.txt_stdout.txt",
    "code_path": "problem_iter5_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit's tightest fit with a fill ratio consideration,\n    prioritizing bins where the item creates a small residual gap relative to\n    the bin's capacity, and also considering the item's fill percentage.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_caps - item\n\n    # Metric 1: Tightest Fit (inverse of remaining capacity after placement)\n    # Favors bins with minimal residual space. High score for small residual.\n    tight_fit_score = 1.0 / (remaining_after_placement + 1e-9)\n\n    # Metric 2: Fill Ratio (item size relative to current bin capacity)\n    # Favors bins where the item takes up a larger proportion of available space.\n    # High score for larger item/bin ratio.\n    fill_ratio_score = item / (suitable_bins_caps + 1e-9)\n\n    # Combine metrics: Weighted sum.\n    # The fill ratio acts as a multiplier for the tight fit score,\n    # giving higher priority to tight fits of larger items relative to bin size.\n    # This implicitly balances 'tightness' with 'impact'.\n    combined_score = tight_fit_score * fill_ratio_score\n    \n    priorities[suitable_bins_mask] = combined_score\n    \n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response0.txt_stdout.txt",
    "code_path": "problem_iter6_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin, favoring bins\n    that leave minimal remaining space, but also considering bins that are\n    nearly full to encourage consolidation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Calculate \"tightness\" score: how much space is left after packing\n    # We want to minimize remaining space, so higher score for smaller remaining space\n    tightness_scores = - (suitable_bins_remain_cap - item)\n\n    # Calculate \"consolidation\" score: favor bins that are already quite full\n    # This encourages using up existing bins before opening new ones.\n    # A bin is considered \"nearly full\" if its remaining capacity is small relative to the bin size.\n    # We'll assume a standard bin size for this example, say 1.0. If bin size varies,\n    # this would need to be adjusted or passed as an argument.\n    bin_capacity = 1.0 # Assuming a standard bin capacity of 1.0\n    consolidation_scores = (bin_capacity - suitable_bins_remain_cap) / bin_capacity\n\n    # Combine scores. We want bins that are both tight (minimize remaining space)\n    # and encourage consolidation (bins that are already full).\n    # A simple approach is to sum them. We can use weights to tune their importance.\n    # Let's give slightly more weight to tightness as it's the primary goal of BPP.\n    combined_scores = 0.7 * tightness_scores + 0.3 * consolidation_scores\n\n    # Normalize combined_scores to be between 0 and 1 for better comparison across items/bin states\n    if np.ptp(combined_scores) > 0: # Avoid division by zero if all scores are the same\n        normalized_scores = (combined_scores - np.min(combined_scores)) / np.ptp(combined_scores)\n    else:\n        normalized_scores = np.zeros_like(combined_scores)\n\n\n    # Assign the normalized scores to the original priority array for suitable bins\n    priorities[suitable_bins_mask] = normalized_scores\n\n    # To ensure we pick the *best* bin according to these priorities, we want to\n    # maximize the priority score.\n    # For the First Fit Decreasing logic (which the original v1 hinted at),\n    # the *best* bin is the one that minimizes remaining capacity.\n    # Our `priority_v2` is designed to assign higher scores to such bins.\n    # So, the bin with the highest priority score is the one we select.\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response1.txt_stdout.txt",
    "code_path": "problem_iter6_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a heuristic that\n    balances Best Fit and considering the \"emptiness\" of bins.\n\n    This heuristic aims to find a bin that is a \"tight fit\" for the current item,\n    minimizing wasted space. It also gives a slight preference to bins that are\n    less full overall, promoting better distribution and potentially allowing\n    for better packing of future larger items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities # No suitable bin found\n\n    # Calculate the \"tightness\" of the fit (smaller is better)\n    tightness = bins_remain_cap[suitable_bins_mask] - item\n\n    # Calculate a measure of \"emptiness\" for suitable bins.\n    # A higher value here means the bin is emptier (more remaining capacity).\n    # We use log to dampen the effect of very large capacities.\n    # Adding a small epsilon to avoid log(0).\n    emptiness = np.log(bins_remain_cap[suitable_bins_mask] + 1e-9)\n\n    # Combine tightness and emptiness.\n    # We want small tightness (good fit) and high emptiness (less full bin).\n    # So, we can subtract emptiness from tightness, or use a weighted sum.\n    # Here, we prioritize a good fit, but give a bonus to emptier bins.\n    # The negative sign for emptiness means higher emptiness leads to a higher score.\n    scores = -tightness + emptiness * 0.1 # Adjust the multiplier (0.1) for balancing\n\n    # Assign the calculated scores to the corresponding suitable bins\n    priorities[suitable_bins_mask] = scores\n\n    # Normalize priorities so the highest score is 1 and others are scaled accordingly.\n    # This makes the scores more interpretable as a probability or preference.\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities /= max_priority\n    elif np.any(suitable_bins_mask): # If all suitable bins have the same (non-positive) score\n        priorities[suitable_bins_mask] = 1.0 / np.sum(suitable_bins_mask) # Distribute equally\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.198244914240141,
    "SLOC": 15.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response2.txt_stdout.txt",
    "code_path": "problem_iter6_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    \n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    \n    if suitable_bins_cap.size == 0:\n        return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.487435181491823,
    "SLOC": 6.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response3.txt_stdout.txt",
    "code_path": "problem_iter6_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a refined heuristic.\n\n    This heuristic aims to balance fitting the item snugly (similar to Best Fit)\n    while also considering bins that have more remaining capacity to potentially\n    accommodate future items more effectively. It prioritizes bins where the\n    remaining capacity after placing the item is maximized.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Calculate potential remaining capacity for each bin if the item fits\n    potential_remaining_caps = bins_remain_cap - item\n    \n    # Only consider bins that can actually fit the item\n    suitable_bins_mask = potential_remaining_caps >= 0\n    \n    if np.any(suitable_bins_mask):\n        # For suitable bins, assign a priority based on the potential remaining capacity.\n        # Higher remaining capacity gets a higher priority.\n        # We add a small epsilon to avoid division by zero or extremely large values\n        # if a bin has exactly 0 remaining capacity after fitting.\n        priorities[suitable_bins_mask] = potential_remaining_caps[suitable_bins_mask] + 1e-9\n        \n        # Normalize priorities to a range (optional, but can make interpretation easier)\n        # If all suitable bins have the same remaining capacity, this would give them equal priority.\n        min_priority = np.min(priorities[suitable_bins_mask])\n        max_priority = np.max(priorities[suitable_bins_mask])\n        \n        if max_priority > min_priority:\n            priorities[suitable_bins_mask] = (priorities[suitable_bins_mask] - min_priority) / (max_priority - min_priority)\n        else:\n            # If all suitable bins have the same priority value, assign a uniform priority\n            priorities[suitable_bins_mask] = 1.0\n\n        # Apply a slight penalty for bins that are almost full after placing the item\n        # This encourages using bins that leave more \"room\" for future items.\n        # For example, if remaining capacity is < 10% of original bin capacity (assuming bins are of a fixed total capacity)\n        # or simply a small absolute value if bin capacity isn't readily available.\n        # For now, let's consider a small absolute threshold.\n        small_remaining_threshold = 1.0 # This is an arbitrary threshold, can be tuned.\n        almost_full_mask = (potential_remaining_caps >= 0) & (potential_remaining_caps < small_remaining_threshold)\n        priorities[almost_full_mask] *= 0.8 # Reduce priority by 20%\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 52, in priority_v2\nnumpy._core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'multiply' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'\n16\n3\n"
  },
  {
    "stdout_filepath": "problem_iter6_response4.txt_stdout.txt",
    "code_path": "problem_iter6_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a refined logic\n    that prioritizes bins with less remaining capacity after packing, but also\n    considers the \"tightness\" of the fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.487435181491823,
    "SLOC": 5.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  }
]