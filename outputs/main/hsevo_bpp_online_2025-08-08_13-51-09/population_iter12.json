[
  {
    "stdout_filepath": "problem_iter11_response0.txt_stdout.txt",
    "code_path": "problem_iter11_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit (inverse residual) with a normalized exploration bonus\n    (log-transformed remaining capacity), balancing efficiency and spread.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit - inverse of remaining space after placement.\n    best_fit_scores = 1.0 / (suitable_bins_caps - item + 1e-9)\n\n    # Metric 2: Exploration - log-transformed remaining capacity to favor less utilized bins.\n    # Add 1 to avoid log(0) and provide a smoother bonus.\n    exploration_scores = np.log1p(suitable_bins_caps)\n\n    # Normalize exploration scores using min-max scaling.\n    min_exp_score = np.min(exploration_scores)\n    max_exp_score = np.max(exploration_scores)\n    if max_exp_score - min_exp_score > 1e-9:\n        normalized_exploration_scores = (exploration_scores - min_exp_score) / (max_exp_score - min_exp_score)\n    else:\n        normalized_exploration_scores = np.zeros_like(exploration_scores)\n\n    # Combine scores with a focus on Best Fit (0.7) and balanced exploration (0.3).\n    # This combination aims for efficient packing while encouraging better bin distribution.\n    combined_scores = 0.7 * best_fit_scores + 0.3 * normalized_exploration_scores\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 83.62584762664541,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response1.txt_stdout.txt",
    "code_path": "problem_iter11_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a normalized bonus for larger remaining capacity,\n    using a weighted sum for balanced decision-making. Favors bins that are\n    almost full but can still accommodate the item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    # Metric 1: Best Fit - prioritize bins with minimal remaining capacity after placement.\n    # Higher score for smaller residual space.\n    remaining_after_placement = suitable_bins_caps - item\n    # Inverse relationship: smaller residual -> higher score. Add epsilon for stability.\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-9)\n    \n    # Metric 2: Exploration/Larger Bin Preference - favor bins with more capacity initially.\n    # This encourages not always picking the absolute tightest, promoting diversification.\n    # We'll use a logarithmic scale for remaining capacity to de-emphasize very large bins\n    # and focus on bins that are \"reasonably\" large but not excessively so.\n    # log1p is used to handle cases where remaining capacity is 0 after placement,\n    # and to provide a smoother scaling than a simple linear approach.\n    exploration_scores = np.log1p(suitable_bins_caps)\n    \n    # Normalize Best Fit scores (min-max scaling)\n    if suitable_bins_caps.size > 1:\n        min_bf = np.min(best_fit_scores)\n        max_bf = np.max(best_fit_scores)\n        range_bf = max_bf - min_bf\n        if range_bf > 1e-9:\n            normalized_best_fit = (best_fit_scores - min_bf) / range_bf\n        else:\n            normalized_best_fit = np.ones_like(best_fit_scores) # All suitable bins offer same tightness score\n    elif suitable_bins_caps.size == 1:\n        normalized_best_fit = np.array([1.0])\n    else:\n        normalized_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Exploration scores (min-max scaling)\n    if suitable_bins_caps.size > 1:\n        min_exp = np.min(exploration_scores)\n        max_exp = np.max(exploration_scores)\n        range_exp = max_exp - min_exp\n        if range_exp > 1e-9:\n            normalized_exploration = (exploration_scores - min_exp) / range_exp\n        else:\n            normalized_exploration = np.zeros_like(exploration_scores) # All suitable bins have same initial capacity\n    elif suitable_bins_caps.size == 1:\n        normalized_exploration = np.array([1.0]) # If only one bin, it's maximally \"exploratory\" in this context\n    else:\n        normalized_exploration = np.zeros_like(exploration_scores)\n\n    # Combine normalized scores using a weighted sum.\n    # We give a slightly higher weight to Best Fit, as tight packing is crucial for BPP.\n    # The exploration bonus helps to prevent premature fragmentation.\n    combined_scores = 0.7 * normalized_best_fit + 0.3 * normalized_exploration\n    \n    priorities[suitable_bins_mask] = combined_scores\n    \n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 36.0,
    "cyclomatic_complexity": 8.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response2.txt_stdout.txt",
    "code_path": "problem_iter11_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (tightness) with a 'fair share' exploration bonus,\n    prioritizing bins that are neither too full nor too empty, relative to others.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Best Fit component: Inverse of remaining capacity after placing the item.\n    # Smaller difference implies a tighter fit and higher score.\n    tightness_scores = 1.0 / (valid_bins_remain_cap - item + 1e-9)\n\n    # Exploration component: Penalize bins that are excessively full or empty\n    # relative to the average remaining capacity of suitable bins. This encourages\n    # a more balanced distribution. We use a quadratic penalty.\n    avg_remain_cap = np.mean(valid_bins_remain_cap)\n    # Deviation from average, squared to penalize larger deviations more.\n    # We add a small constant to avoid zero deviation resulting in zero penalty.\n    fairness_penalty = (valid_bins_remain_cap - avg_remain_cap)**2 / (avg_remain_cap + 1e-9)\n\n    # Combine scores: Higher tightness is good, lower penalty (closer to avg) is good.\n    # We subtract the penalty as it's a negative aspect.\n    # Weights can be tuned; here, tightness is prioritized.\n    combined_scores = tightness_scores - fairness_penalty * 0.2 # Tunable parameter for penalty influence\n\n    # Normalize scores to be between 0 and 1.\n    min_score = np.min(combined_scores)\n    max_score = np.max(combined_scores)\n\n    if max_score - min_score > 1e-9:\n        priorities[suitable_bins_mask] = (combined_scores - min_score) / (max_score - min_score)\n    elif np.any(suitable_bins_mask):\n        # If all suitable bins have very similar combined scores, distribute equally.\n        priorities[suitable_bins_mask] = 1.0 / np.sum(suitable_bins_mask)\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 39.39968089349822,
    "SLOC": 17.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response3.txt_stdout.txt",
    "code_path": "problem_iter11_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit's tightness with an exploration bonus favoring less utilized bins,\n    using a balanced approach to combine these factors.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return np.zeros_like(bins_remain_cap)\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Best Fit Component: Prioritize bins with minimal remaining capacity after placement.\n    # Negative of remaining capacity to favor smaller (more negative) values for minimization.\n    best_fit_scores = -(suitable_bins_remain_cap - item)\n\n    # Exploration Component: Favor bins that are less utilized (larger original capacity).\n    # Normalize remaining capacities of suitable bins using min-max scaling.\n    min_cap = np.min(suitable_bins_remain_cap)\n    max_cap = np.max(suitable_bins_remain_cap)\n    if max_cap - min_cap > 1e-9:\n        exploration_scores = (suitable_bins_remain_cap - min_cap) / (max_cap - min_cap)\n    else:\n        exploration_scores = np.zeros_like(suitable_bins_remain_cap)\n\n    # Combined Score: Balance Best Fit and Exploration.\n    # A weighted sum is used. We give a slightly higher weight to Best Fit (tightness)\n    # as it's generally a primary goal in BPP, while exploration acts as a tie-breaker\n    # or secondary optimization.\n    # We add exploration_scores to best_fit_scores. Higher values (closer to zero for BF) are better.\n    # Exploration scores are positive and higher is better.\n    combined_scores = best_fit_scores + 0.7 * exploration_scores # Weight for exploration\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    # If all suitable bins are identical in terms of combined score, argmin will pick the first.\n    # This heuristic aims to find a good balance, leaning towards tight fits but\n    # considering bin utilization as a secondary factor.\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response4.txt_stdout.txt",
    "code_path": "problem_iter11_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a tight fit metric with an exploration bonus, favoring bins\n    that minimize remaining space while also considering less utilized bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Tight Fit (similar to priority_v0)\n    # Prioritize bins that leave minimal remaining space after packing.\n    # Add epsilon for numerical stability. Higher score for smaller remaining space.\n    tightness_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Metric 2: Exploration Bonus (inspired by priority_v0 and priority_v10/11/12)\n    # Favor bins that are less full (more remaining capacity).\n    # Using log1p for slightly better distribution at lower capacities.\n    # Higher score for bins with more remaining capacity.\n    exploration_score = np.log1p(suitable_bins_remain_cap)\n\n    # Combine scores with weights.\n    # Giving a slight edge to tightness, but exploration is also important.\n    # These weights can be tuned based on empirical performance.\n    combined_scores = 0.55 * tightness_score + 0.45 * exploration_score\n\n    # Normalize combined scores to a [0, 1] range.\n    # This ensures that the relative priorities are maintained even with different\n    # scales of the individual metrics. Handle cases where all scores are equal.\n    min_score = np.min(combined_scores)\n    max_score = np.max(combined_scores)\n    if max_score - min_score > 1e-9:\n        normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n    else:\n        normalized_scores = np.ones_like(combined_scores) * 0.5\n\n    priorities[suitable_bins_mask] = normalized_scores\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 84.95213402473077,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response5.txt_stdout.txt",
    "code_path": "problem_iter11_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with an exploration bonus favoring less utilized bins,\n    using logarithmic scaling for exploration to enhance bin distribution.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit (minimize remaining space after packing)\n    # Higher score for bins with less remaining space after packing the item.\n    # Adding a small epsilon to avoid division by zero.\n    tightness_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Metric 2: Exploration Bonus (favor less utilized bins)\n    # Logarithmic scaling of remaining capacity. Favors bins that are less full,\n    # encouraging a more even distribution of items across bins.\n    exploration_score = np.log(suitable_bins_remain_cap + 1e-9)\n\n    # Combine scores. Weighting favors tightness slightly, but exploration\n    # provides a bonus for less-used bins. These weights are subject to tuning.\n    combined_scores = 0.55 * tightness_score + 0.45 * exploration_score\n\n    # Normalize the combined scores to a [0, 1] range for consistent priority.\n    # Avoid division by zero if all combined scores are identical.\n    min_score = np.min(combined_scores)\n    max_score = np.max(combined_scores)\n    if max_score - min_score > 1e-9:\n        normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n    else:\n        normalized_scores = np.ones_like(combined_scores) * 0.5 # Default to mid-range if all scores are equal\n\n    priorities[suitable_bins_mask] = normalized_scores\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 84.95213402473077,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response6.txt_stdout.txt",
    "code_path": "problem_iter11_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a refined Best Fit with a dynamic Exploration bonus.\n    Prioritizes tight fits for larger items and exploration for smaller items,\n    adapting the strategy based on item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_caps - item\n\n    # Metric 1: Refined Best Fit - favors bins with smallest remaining capacity after placement.\n    # Using log1p to compress larger gaps and emphasize smaller ones. Add epsilon for stability.\n    best_fit_scores = np.log1p(1.0 / (remaining_after_placement + 1e-6))\n\n    # Metric 2: Exploration Bonus - favors bins that have significantly more remaining capacity.\n    # This is achieved by rewarding bins that are further from the minimum possible remaining capacity\n    # (after placing the item). Using min-max scaling on the remaining space after placement.\n    min_rem_after = np.min(remaining_after_placement)\n    max_rem_after = np.max(remaining_after_placement)\n\n    exploration_scores = np.zeros_like(remaining_after_placement)\n    if max_rem_after > min_rem_after:\n        # Normalize remaining capacity after placement to get exploration score (0 to 1)\n        exploration_scores = (remaining_after_placement - min_rem_after) / (max_rem_after - min_rem_after)\n    else:\n        # If all suitable bins leave the same remaining capacity, no exploration bonus from this metric.\n        pass\n\n    # Dynamic Weighting based on item size.\n    # Larger items benefit more from a precise fit (Best Fit).\n    # Smaller items can afford to explore less utilized bins (Exploration Bonus).\n    # Assume bin capacity is normalized to 1.0 for a relative item size assessment.\n    # If bin capacities vary significantly, a different normalization might be needed.\n    # For simplicity, we'll use item size directly, assuming it's scaled appropriately.\n    # Let's assume `item` is on a scale where 0.5 means it's half the typical bin capacity.\n    \n    # A simple heuristic: if item is more than 50% of typical capacity, prioritize best fit.\n    # If item is less than 20%, prioritize exploration. In between, a mix.\n    # Using item size as a proxy for its \"impact\" on bin fullness.\n    \n    # Weights sum to 1.0.\n    # For small items (e.g., item < 0.3): higher exploration, lower best fit.\n    # For large items (e.g., item > 0.7): higher best fit, lower exploration.\n    \n    # Example: Item size normalized to [0, 1] range, representing proportion of bin capacity.\n    # If actual item sizes are larger, they would need to be scaled.\n    # Let's assume `item` is already scaled relative to a standard bin capacity.\n\n    # Define a threshold, e.g., 0.5, for medium-sized items.\n    threshold_medium = 0.5 \n    \n    # Smooth transition for weights\n    weight_best_fit = np.clip(item / threshold_medium, 0.1, 1.0) # Favors best fit for larger items\n    weight_exploration = np.clip((threshold_medium - item) / threshold_medium, 0.1, 1.0) # Favors exploration for smaller items\n\n    # Normalize weights to ensure they sum to 1 if they cross thresholds or are outside bounds.\n    total_weight = weight_best_fit + weight_exploration\n    if total_weight > 1e-6:\n        weight_best_fit /= total_weight\n        weight_exploration /= total_weight\n    else: # Fallback if both are effectively zero\n        weight_best_fit = 0.5\n        weight_exploration = 0.5\n\n    # Combine scores\n    combined_scores = (weight_best_fit * best_fit_scores +\n                       weight_exploration * exploration_scores)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 72.92580773833267,
    "SLOC": 29.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response7.txt_stdout.txt",
    "code_path": "problem_iter11_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a strong 'Best Fit' strategy with a 'Logarithmic Exploration' bonus.\n    Prioritizes snugly fitting items while also giving a slight preference\n    to bins with more remaining capacity to encourage spread.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_caps - item\n\n    # Metric 1: Best Fit - favors bins with minimum remaining space after placement.\n    # Using reciprocal emphasizes smaller remaining capacities.\n    # Add epsilon to avoid division by zero.\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-6)\n\n    # Metric 2: Exploration/Uniformity - favors bins with more open space.\n    # Using log1p compresses the range of remaining capacities, giving a boost\n    # to moderately open bins without overly favoring extremely empty ones.\n    exploration_scores = np.log1p(suitable_bins_caps)\n\n    # Combine scores with weights.\n    # Weighting: Give more importance to Best Fit, but include Exploration.\n    # A weight of 0.7 for Best Fit and 0.3 for Exploration offers a good balance.\n    weight_best_fit = 0.7\n    weight_exploration = 0.3\n\n    combined_scores = (weight_best_fit * best_fit_scores +\n                       weight_exploration * exploration_scores)\n\n    # Apply combined scores to the priorities array\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 81.91065017949742,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response8.txt_stdout.txt",
    "code_path": "problem_iter11_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a normalized bonus for larger remaining capacity,\n    using a weighted sum for balanced decision-making. Favors bins that are\n    nearly full but can still accommodate the item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    # Metric 1: Best Fit - prioritize bins with minimal remaining capacity after placement.\n    # Add a small epsilon to avoid division by zero and ensure non-zero scores for tight fits.\n    remaining_after_placement = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-6)\n    \n    # Metric 2: Exploration/Emptiness Bonus - reward bins that are less utilized initially.\n    # Using log1p to handle zero capacities gracefully and provide diminishing returns.\n    emptiness_bonus_scores = np.log1p(suitable_bins_caps)\n    \n    # Normalize Best Fit scores using min-max scaling.\n    if suitable_bins_caps.size > 1:\n        min_bf = np.min(best_fit_scores)\n        max_bf = np.max(best_fit_scores)\n        range_bf = max_bf - min_bf\n        if range_bf > 1e-6:\n            normalized_best_fit = (best_fit_scores - min_bf) / range_bf\n        else:\n            normalized_best_fit = np.zeros_like(best_fit_scores) # All suitable bins offer same tightness\n    elif suitable_bins_caps.size == 1:\n        normalized_best_fit = np.array([1.0]) # Only one suitable bin, highest possible score\n    else:\n        normalized_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Emptiness Bonus scores using min-max scaling.\n    if suitable_bins_caps.size > 1:\n        min_eb = np.min(emptiness_bonus_scores)\n        max_eb = np.max(emptiness_bonus_scores)\n        range_eb = max_eb - min_eb\n        if range_eb > 1e-6:\n            normalized_emptiness_bonus = (emptiness_bonus_scores - min_eb) / range_eb\n        else:\n            normalized_emptiness_bonus = np.zeros_like(emptiness_bonus_scores) # All suitable bins have same emptiness\n    elif suitable_bins_caps.size == 1:\n        normalized_emptiness_bonus = np.array([1.0]) # Only one suitable bin, highest possible score\n    else:\n        normalized_emptiness_bonus = np.zeros_like(emptiness_bonus_scores)\n\n    # Combine normalized scores using a weighted sum.\n    # Increased weight on best fit, while still providing a bonus for less full bins.\n    # Weights are tuned to balance immediate fit with longer-term space utilization.\n    combined_scores = 0.7 * normalized_best_fit + 0.3 * normalized_emptiness_bonus\n    \n    priorities[suitable_bins_mask] = combined_scores\n    \n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 36.0,
    "cyclomatic_complexity": 8.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response9.txt_stdout.txt",
    "code_path": "problem_iter11_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a nuanced best-fit approach with an exploration bonus favoring less utilized bins,\n    and a subtle preference for bins with more overall remaining capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Modified Best Fit (from priority_v0)\n    # Prioritizes bins leaving a \"sweet spot\" residual capacity to avoid tiny unusable gaps.\n    remaining_after_placement_m1 = suitable_bins_caps - item\n    target_residual = item * 0.2  # Target residual capacity ~20% of item size\n    # Gaussian-like function: higher score for residuals closer to target_residual\n    # Add small epsilon to avoid division by zero if target_residual is 0.\n    best_fit_scores = np.exp(-((remaining_after_placement_m1 - target_residual) / (target_residual + 1e-6))**2)\n\n    # Metric 2: Exploration Bonus (inspired by priority_v1, simpler version)\n    # Favors bins that are less full *after* placement, relative to other suitable bins.\n    min_rem_after_m2 = np.min(remaining_after_placement_m1)\n    max_rem_after_m2 = np.max(remaining_after_placement_m1)\n    \n    exploration_scores = np.zeros_like(suitable_bins_caps)\n    if max_rem_after_m2 > min_rem_after_m2:\n        # Normalized remaining capacity after placement: higher for more empty bins\n        exploration_scores = (remaining_after_placement_m1 - min_rem_after_m2) / (max_rem_after_m2 - min_rem_after_m2)\n    else:\n        # If all suitable bins result in the same remaining capacity, no exploration bonus from this diff.\n        # Default to 0.5 for any such bins to avoid bias.\n        exploration_scores = np.ones_like(suitable_bins_caps) * 0.5\n\n    # Metric 3: Usage Proxy (favors bins with more total remaining capacity)\n    # This is a simple proxy for less-used bins.\n    min_cap_all = np.min(bins_remain_cap)\n    max_cap_all = np.max(bins_remain_cap)\n    \n    usage_scores = np.zeros_like(suitable_bins_caps)\n    if max_cap_all > min_cap_all:\n        # Normalize current remaining capacities. Higher score for more remaining capacity.\n        normalized_current_caps = (suitable_bins_caps - min_cap_all) / (max_cap_all - min_cap_all)\n        usage_scores = normalized_current_caps\n    else:\n        # If all bins have same capacity, this metric doesn't differentiate.\n        usage_scores = np.ones_like(suitable_bins_caps) * 0.5\n\n    # Combine scores: Heavy emphasis on nuanced best-fit, moderate on exploration, light on usage.\n    # Weights are chosen to balance finding good fits with spreading items.\n    # 0.6 for Best Fit (primary, quality of fit)\n    # 0.3 for Exploration (secondary, diversity)\n    # 0.1 for Usage (tertiary, simple preference for emptier bins)\n    combined_scores = 0.6 * best_fit_scores + 0.3 * exploration_scores + 0.1 * usage_scores\n\n    # Ensure scores are within a reasonable range and handle potential NaNs/Infs\n    combined_scores = np.nan_to_num(combined_scores, nan=0.0, posinf=1.0, neginf=0.0)\n    combined_scores = np.clip(combined_scores, 0.0, 1.0)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 43.87714399680894,
    "SLOC": 29.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response0.txt_stdout.txt",
    "code_path": "problem_iter12_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit - Prioritize bins that leave minimal remaining capacity.\n    # Using inverse of remaining capacity, scaled to avoid extreme values.\n    # A small epsilon is added to prevent division by zero and to ensure\n    # even very tight fits (0 remaining) get a high score.\n    remaining_after_placement = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-6)\n\n    # Metric 2: Bin Fullness - Reward bins that are already more full.\n    # This encourages filling bins more completely before opening new ones.\n    # We can approximate fullness by (bin_capacity - remaining_capacity) / bin_capacity.\n    # For simplicity and adaptivity, let's use the remaining capacity relative to the maximum\n    # capacity encountered among suitable bins. A smaller remaining capacity implies higher fullness.\n    max_suitable_cap = np.max(suitable_bins_caps)\n    if max_suitable_cap > 1e-6:\n        # Lower remaining capacity relative to max suitable capacity means it's fuller.\n        # We want to score higher for lower remaining capacity.\n        fullness_scores = 1.0 - (suitable_bins_caps / max_suitable_cap)\n    else:\n        fullness_scores = np.zeros_like(suitable_bins_caps)\n\n    # Metric 3: Item Size Ratio - Favor bins where the item takes up a significant portion\n    # of the remaining capacity. This helps in reducing fragmentation for larger items.\n    # Calculated as item_size / remaining_capacity. A higher ratio is better.\n    # Add epsilon to denominator to avoid division by zero if remaining_capacity is zero.\n    item_size_ratio_scores = item / (suitable_bins_caps + 1e-6)\n\n    # Normalize scores to be in a comparable range [0, 1] for combination.\n    # Use min-max scaling for robustness.\n\n    if np.ptp(best_fit_scores) > 1e-6:\n        normalized_best_fit = (best_fit_scores - np.min(best_fit_scores)) / np.ptp(best_fit_scores)\n    else:\n        normalized_best_fit = np.zeros_like(best_fit_scores)\n\n    if np.ptp(fullness_scores) > 1e-6:\n        normalized_fullness = (fullness_scores - np.min(fullness_scores)) / np.ptp(fullness_scores)\n    else:\n        normalized_fullness = np.zeros_like(fullness_scores)\n\n    if np.ptp(item_size_ratio_scores) > 1e-6:\n        normalized_item_ratio = (item_size_ratio_scores - np.min(item_size_ratio_scores)) / np.ptp(item_size_ratio_scores)\n    else:\n        normalized_item_ratio = np.zeros_like(item_size_ratio_scores)\n\n\n    # Combine scores with dynamic weighting.\n    # The weights are adjusted based on the item size relative to the bin capacity.\n    # If the item is large relative to typical bin capacities, prioritize \"best fit\" and \"item size ratio\".\n    # If the item is small, prioritize \"fullness\" to consolidate.\n\n    # Assume a typical normalized bin capacity is 1.0 for scale.\n    # The choice of scale can be an average item size or a predefined max capacity.\n    # Let's use a soft threshold based on item size proportion to a conceptual \"average\" remaining capacity.\n    # A simple heuristic for scale: consider average remaining capacity of suitable bins.\n    avg_suitable_cap = np.mean(suitable_bins_caps) if suitable_bins_caps.size > 0 else 1.0\n    item_ratio_to_avg_cap = item / (avg_suitable_cap + 1e-6)\n\n    # Weights adjust based on how \"large\" the item is relative to the available space.\n    # If item_ratio_to_avg_cap is high (item is relatively large), boost best_fit and item_ratio.\n    # If item_ratio_to_avg_cap is low (item is relatively small), boost fullness.\n\n    weight_best_fit = 0.3 + 0.4 * min(item_ratio_to_avg_cap, 1.0)\n    weight_fullness = 0.4 - 0.3 * min(item_ratio_to_avg_cap, 1.0)\n    weight_item_ratio = 0.3 + 0.3 * min(item_ratio_to_avg_cap, 1.0)\n\n    # Ensure weights sum to 1\n    total_weight = weight_best_fit + weight_fullness + weight_item_ratio\n    if total_weight > 1e-6:\n        weight_best_fit /= total_weight\n        weight_fullness /= total_weight\n        weight_item_ratio /= total_weight\n    else: # Fallback if all weights are zero\n        weight_best_fit, weight_fullness, weight_item_ratio = 1/3, 1/3, 1/3\n\n\n    combined_scores = (weight_best_fit * normalized_best_fit +\n                       weight_fullness * normalized_fullness +\n                       weight_item_ratio * normalized_item_ratio)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 43.0,
    "cyclomatic_complexity": 8.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response1.txt_stdout.txt",
    "code_path": "problem_iter12_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    # Metric 1: Refined Best Fit - prioritize bins that leave minimal empty space, but with a twist\n    # We use a hyperbola-like function (1/x) which amplifies smaller remaining capacities.\n    # Adding the item size to the denominator helps to ensure that larger items are less penalized\n    # for having slightly larger absolute remaining gaps compared to smaller items.\n    remaining_after_placement = suitable_bins_caps - item\n    # Use a small epsilon to avoid division by zero for perfect fits.\n    # Add item size to the denominator to slightly favor bins where the item takes a larger proportion of the remaining space.\n    best_fit_scores = 1.0 / (remaining_after_placement + item * 0.1 + 1e-6)\n\n    # Metric 2: Adaptive Exploration/Openness - favor bins that have substantial capacity, but not too much.\n    # Instead of a fixed sigmoid, use a function that adapts to the distribution of suitable bin capacities.\n    # We want to reward bins that are neither nearly full nor nearly empty relative to the current item.\n    # Consider the ratio of remaining capacity to the item size.\n    # A high ratio means the bin is very open relative to the item.\n    # A low ratio means the bin is quite full relative to the item.\n    # We want to reward a \"middle ground\".\n    # Let's use a Gaussian-like function centered around a preferred ratio.\n    # A preferred ratio could be 2-4 times the item size, meaning the bin has enough space for the current item\n    # and potentially a few more similar items, but not so much space that it's wasted.\n    preferred_ratio_low = 2.0\n    preferred_ratio_high = 5.0\n    ratio_remaining_to_item = suitable_bins_caps / item\n\n    # Score is high when ratio_remaining_to_item is between preferred_ratio_low and preferred_ratio_high\n    # and decays outside this range.\n    # We can use a smooth function like a quadratic or a difference of sigmoids.\n    # Let's use a \"hat\" function (linear ramp up, linear ramp down) for simplicity and interpretability.\n    exploration_scores = np.zeros_like(suitable_bins_caps)\n    within_range_mask = (ratio_remaining_to_item >= preferred_ratio_low) & (ratio_remaining_to_item <= preferred_ratio_high)\n    exploration_scores[within_range_mask] = 1.0\n\n    # Ramp up for ratios between 0 and preferred_ratio_low\n    ramp_up_mask = ratio_remaining_to_item < preferred_ratio_low\n    ramp_up_ratios = ratio_remaining_to_item[ramp_up_mask]\n    exploration_scores[ramp_up_mask] = ramp_up_ratios / preferred_ratio_low\n\n    # Ramp down for ratios between preferred_ratio_high and a higher threshold (e.g., 10x item size)\n    ramp_down_threshold = preferred_ratio_high * 2.0\n    ramp_down_mask = (ratio_remaining_to_item > preferred_ratio_high) & (ratio_remaining_to_item <= ramp_down_threshold)\n    ramp_down_ratios = ratio_remaining_to_item[ramp_down_mask]\n    exploration_scores[ramp_down_mask] = 1.0 - (ramp_down_ratios - preferred_ratio_high) / (ramp_down_threshold - preferred_ratio_high)\n\n    # Metric 3: Fill Uniformity - encourage filling bins that are already partially occupied.\n    # Instead of using an assumed max capacity, let's consider the distribution of *all* remaining capacities.\n    # A bin is \"less empty\" if its remaining capacity is small relative to the average remaining capacity of suitable bins.\n    # Or, more directly, reward bins that have *less* remaining capacity.\n    # This is somewhat contradictory to exploration, so we need a balance.\n    # Let's score based on the inverse of remaining capacity, but scaled to be comparable to other metrics.\n    # A simple approach: penalize bins with very large remaining capacity.\n    # Consider the proportion of capacity remaining in the bin relative to its original capacity.\n    # However, we don't know original capacity. Let's use relative remaining capacity compared to the *largest* suitable remaining capacity.\n    max_suitable_cap = np.max(suitable_bins_caps)\n    if max_suitable_cap > 1e-6:\n        uniformity_scores = 1.0 - (suitable_bins_caps / max_suitable_cap)\n        # Add a small boost for bins that are actually used (i.e., not completely empty before placing item)\n        # This is implicit if suitable_bins_caps are not very large.\n        # To further encourage using partially filled bins: penalize bins that are \"too open\" more strongly.\n        # Let's use a power function on the normalized remaining capacity (0 to 1).\n        # We want to penalize values close to 1 (very empty bins).\n        normalized_remaining = suitable_bins_caps / max_suitable_cap\n        # A quadratic penalty for high remaining capacity.\n        uniformity_scores = 1.0 - normalized_remaining**2\n    else:\n        uniformity_scores = np.zeros_like(suitable_bins_caps)\n\n\n    # Normalization: Scale all metrics to a [0, 1] range to combine them.\n    # Avoid division by zero by checking if the max is positive.\n    if np.max(best_fit_scores) > 1e-6:\n        normalized_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        normalized_best_fit = np.zeros_like(best_fit_scores)\n\n    if np.max(exploration_scores) > 1e-6:\n        normalized_exploration = exploration_scores / np.max(exploration_scores)\n    else:\n        normalized_exploration = np.zeros_like(exploration_scores)\n\n    if np.max(uniformity_scores) > 1e-6:\n        normalized_uniformity = uniformity_scores / np.max(uniformity_scores)\n    else:\n        normalized_uniformity = np.zeros_like(uniformity_scores)\n\n    # Dynamic Weighting based on item size relative to bin capacity.\n    # Assume a reference bin capacity, e.g., the maximum possible capacity in the problem, or a typical capacity.\n    # For simplicity, let's assume a standard bin capacity is 1.0 for relative item size.\n    # If items are larger than 0.5 of bin capacity, prioritize \"best fit\".\n    # If items are smaller, balance between exploration and uniformity.\n\n    # Define item \"largeness\" relative to a typical bin capacity (e.g., 1.0)\n    item_largeness = item # assuming item is scaled relative to bin capacity\n\n    # Weights that shift based on item size.\n    # For small items, favor exploration and uniformity. For large items, favor best fit.\n    # We want a smooth transition.\n    # Let's use a sigmoid-like function for the weight of best_fit.\n    # A common approach is to use `sigmoid(k * (item_largeness - midpoint))`\n    # Let midpoint be 0.4 (items smaller than 0.4 of bin capacity get less BF weight)\n    # Let k be a steepness factor, e.g., 10.\n    midpoint = 0.4\n    steepness = 10.0\n    weight_best_fit = 1 / (1 + np.exp(-steepness * (item_largeness - midpoint)))\n\n    # The remaining weight will be split between exploration and uniformity.\n    # Let's give slightly more weight to exploration for medium items, and uniformity for smaller items.\n    # If item_largeness is small, favor uniformity. If item_largeness is medium, favor exploration.\n    # For larger items, BF dominates and exploration/uniformity get less.\n\n    # If item_largeness < 0.2: High uniformity, moderate exploration\n    # If 0.2 <= item_largeness < 0.5: Moderate uniformity, high exploration\n    # If item_largeness >= 0.5: Low uniformity, low exploration\n\n    weight_exploration = (1 - weight_best_fit) * (0.5 + 0.3 * np.exp(-steepness * (item_largeness - 0.3)))\n    weight_uniformity = (1 - weight_best_fit) * (0.5 - 0.3 * np.exp(-steepness * (item_largeness - 0.3)))\n\n\n    # Ensure weights sum to 1\n    total_weight = weight_best_fit + weight_exploration + weight_uniformity\n    weight_best_fit /= total_weight\n    weight_exploration /= total_weight\n    weight_uniformity /= total_weight\n\n    combined_scores = (weight_best_fit * normalized_best_fit +\n                       weight_exploration * normalized_exploration +\n                       weight_uniformity * normalized_uniformity)\n\n    # Assign scores to the original bins array\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 56.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response2.txt_stdout.txt",
    "code_path": "problem_iter12_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    num_suitable_bins = len(suitable_bins_caps)\n\n    # Metric 1: Modified Best Fit - Prioritize bins that leave minimal but positive remaining space.\n    # We want to penalize bins that leave a very large gap, but also not over-penalize bins that are almost full.\n    # Using the inverse of the remaining capacity, but capping the maximum remaining space to avoid extreme penalties.\n    max_gap_consideration = 1.0 # Assume a typical maximum gap we'd want to consider, e.g., 1.0 if bin capacity is 1.0\n    capped_remaining_after_placement = np.minimum(suitable_bins_caps - item, max_gap_consideration)\n    # Avoid division by zero and heavily favor bins with very little remaining space.\n    best_fit_scores = 1.0 / (capped_remaining_after_placement + 1e-6)\n\n    # Metric 2: \"Tightness\" or \"Sufficiency\" Score - Reward bins where the item fills a significant portion of the *current* remaining capacity.\n    # This encourages using bins that are already somewhat filled rather than almost empty ones.\n    # This is complementary to exploration.\n    # Score is item_size / current_remaining_capacity\n    sufficiency_scores = item / (suitable_bins_caps + 1e-6)\n    # Clamp scores to prevent excessively high values for bins with very little capacity left.\n    sufficiency_scores = np.clip(sufficiency_scores, 0, 3.0) # Cap at 3.0, meaning item is 3x the remaining capacity (unlikely but safe)\n\n    # Metric 3: \"Bin Longevity\" or \"Future-proofing\" Score - Favor bins that have a larger absolute remaining capacity *after* placement.\n    # This is the opposite of best fit, aiming to keep more options open for future items.\n    # We want to reward larger remaining capacities, but not to the point where it ignores good fits.\n    # Normalize by the maximum possible remaining capacity (e.g., original bin capacity, assume 1.0 for normalization)\n    # Or, normalize by the maximum remaining capacity among *all* bins (not just suitable ones) to give context.\n    # Let's use a proxy for the maximum possible capacity, e.g., the largest initial capacity encountered or a fixed large value.\n    # For simplicity, assume a conceptual \"full bin capacity\" of 1.0 for normalization.\n    conceptual_full_capacity = 1.0\n    bin_longevity_scores = (suitable_bins_caps - item) / conceptual_full_capacity\n    bin_longevity_scores = np.clip(bin_longevity_scores, 0, 1.0)\n\n\n    # Normalize scores for each metric to be in a [0, 1] range\n    def normalize_scores(scores):\n        max_score = np.max(scores)\n        if max_score > 1e-6:\n            return scores / max_score\n        return np.zeros_like(scores)\n\n    norm_best_fit = normalize_scores(best_fit_scores)\n    norm_sufficiency = normalize_scores(sufficiency_scores)\n    norm_longevity = normalize_scores(bin_longevity_scores)\n\n    # Combine scores with adaptive weights.\n    # Weights are influenced by the item's size relative to a typical bin capacity (assume 1.0 for normalization).\n    # Heuristic:\n    # - If item is large (e.g., > 0.5), prioritize fitting it well (Best Fit) and using existing partially filled bins (Sufficiency). Longevity is less critical.\n    # - If item is small (e.g., < 0.3), prioritize keeping bins open (Longevity) and fitting into less full bins (balancing fill). Sufficiency is less critical.\n    # - For medium items, balance all three.\n\n    item_norm = item # assuming item is already scaled appropriately relative to bin capacity\n\n    # Define weight functions that transition smoothly\n    # Weight for Best Fit increases with item size\n    w_bf = 0.4 + 0.4 * item_norm\n    # Weight for Sufficiency also increases with item size, but peaks earlier\n    w_suf = 0.3 + 0.3 * (1 - np.exp(-5 * item_norm))\n    # Weight for Longevity decreases with item size\n    w_lon = 0.3 - 0.3 * item_norm\n\n    # Ensure weights sum to 1\n    total_w = w_bf + w_suf + w_lon\n    w_bf /= total_w\n    w_suf /= total_w\n    w_lon /= total_w\n\n    # Weighted sum of normalized scores\n    combined_scores = (w_bf * norm_best_fit +\n                       w_suf * norm_sufficiency +\n                       w_lon * norm_longevity)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 36.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response3.txt_stdout.txt",
    "code_path": "problem_iter12_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit (Refined) - Score based on the tightness of the fit.\n    # Using the reciprocal of the remaining capacity after placement.\n    # This naturally prioritizes bins where the remaining space is minimized.\n    # Add a small epsilon to avoid division by zero for perfect fits and to differentiate near-perfect fits.\n    remaining_after_placement = suitable_bins_caps - item\n    # Using 1 / (1 + remaining_capacity) to ensure positive scores and to slightly favor bins with less remaining space.\n    # This is similar to log1p but less sensitive to very small remaining spaces.\n    best_fit_scores = 1.0 / (1.0 + remaining_after_placement)\n\n    # Metric 2: Gap Exploitation - Reward bins that have a significant amount of remaining capacity\n    # but not so much that the item feels \"lost\".\n    # This metric aims to utilize larger \"gaps\" effectively without being overly greedy.\n    # We can score based on the ratio of item size to remaining capacity in the bin.\n    # A higher ratio means the item fills a larger portion of the remaining space, which is good for utilizing larger gaps.\n    # Add a small epsilon to the denominator to avoid division by zero.\n    gap_exploitation_scores = item / (suitable_bins_caps + 1e-6)\n    # Clip scores to avoid excessively high values if item is much larger than bin capacity (though this shouldn't happen with suitable_bins_mask).\n    gap_exploitation_scores = np.clip(gap_exploitation_scores, 0, 2.0) # Cap at 2.0 as a reasonable max ratio.\n\n\n    # Metric 3: Bin Fill Similarity - Aim to make bins have similar fill levels to promote better packing density.\n    # This means preferring bins that are already somewhat full, or bins where adding this item\n    # will bring its fill level closer to other partially filled bins.\n    # A proxy for \"already somewhat full\" is the inverse of remaining capacity relative to a baseline (e.g., max capacity, or average capacity).\n    # Let's normalize remaining capacity by the maximum possible capacity of *any* bin (assuming a global max capacity, e.g., 1.0 for normalized problems).\n    # If a global max is not available, we can use the maximum of all bins' initial capacities.\n    # For this example, let's assume a standard bin capacity of 1.0 as a reference.\n    # The \"fill level\" of a suitable bin would be (1.0 - remaining_capacity) / 1.0.\n    # We want to favor bins with fill levels that are not too close to 0 (very empty) and not too close to 1 (almost full, but not best fit).\n    # A Gaussian-like function centered around a moderate fill level (e.g., 0.6) is suitable.\n    # Let's use the inverse of remaining capacity relative to the item size itself. This measures how \"tight\" the current remaining space is relative to the item.\n    # If the remaining capacity is much larger than the item, the ratio is small, meaning the item is small relative to the space.\n    # If remaining capacity is close to item size, the ratio is near 1, meaning the item is a good fit for the *remaining* space.\n    # We want to favor bins where the item takes up a good fraction of the remaining space.\n    # A higher score for item / remaining_capacity (if remaining_capacity > item) is desired.\n    # This is essentially a variation of best fit, but focuses on the item's proportion of the *remaining* space.\n    # Let's re-think this: Bin Fill Similarity. We want bins to be filled to a similar degree *after* placement.\n    # This implies we want to pick bins that are not too empty and not too full.\n    # Let's consider the \"unused potential\" of a bin. This is the remaining capacity.\n    # We want to pick bins with *moderate* remaining capacity.\n    # Let's normalize the remaining capacity by the item's size. A ratio around 1 is ideal (best fit).\n    # For similarity, we want to avoid extreme remaining capacities.\n    # Consider `remaining_after_placement / item`. We want this to be moderate.\n    # A good heuristic is to penalize bins with very small or very large `remaining_after_placement`.\n    # Let's use `1 - exp(-k * (remaining_after_placement / item))` for values where remaining_after_placement > 0.\n    # A simpler approach: consider the 'fullness' of the bin.\n    # Fullness is approximately (BinCapacity - remaining_capacity) / BinCapacity.\n    # Let's use the current remaining capacity to infer a \"fill state\".\n    # Consider `(item / suitable_bins_caps)` as a measure of how much the item contributes to filling the *current gap*.\n    # A higher value here means the item is a larger portion of the available space, which can be good for utilizing larger gaps.\n    # Let's focus on making bins more \"balanced\".\n    # We can score bins based on how much they \"resemble\" an average fill level.\n    # Average fill level can be estimated by (TotalItemSize / NumberOfBins) / BinCapacity.\n    # For online, this is harder. Let's try to encourage bins to be moderately filled, not too empty, not too full.\n    # Use a score that is high for intermediate remaining capacities.\n    # Let's try a sigmoid-like function on the inverse of remaining capacity.\n    # The \"emptiness\" of the bin is roughly `suitable_bins_caps`.\n    # We want to penalize very small `suitable_bins_caps` (already full) and very large `suitable_bins_caps` (very empty).\n    # Let's use the inverse of remaining capacity scaled by item size.\n    # `suitable_bins_caps / item` ratio. We want this to be moderate.\n    # If `suitable_bins_caps / item` is very small, bin is almost full. If very large, bin is very empty.\n    # Let's use a bell-shaped curve centered around a desired ratio, say 2 (meaning remaining capacity is twice the item size).\n    # This encourages bins that are not too full, not too empty.\n    desired_ratio = 2.0\n    current_ratio = suitable_bins_caps / item\n    # Gaussian-like function: exp(-((ratio - desired_ratio)^2) / variance)\n    # Variance controls the width of the bell curve. A smaller variance means we are pickier.\n    variance = 1.0 # Tunable parameter\n    bin_fill_similarity_scores = np.exp(-((current_ratio - desired_ratio)**2) / (2 * variance))\n\n\n    # Normalize scores to be in a comparable range [0, 1] for combining.\n    # Avoid division by zero if all scores for a metric are zero.\n    max_best_fit = np.max(best_fit_scores)\n    normalized_best_fit = best_fit_scores / max_best_fit if max_best_fit > 1e-6 else np.zeros_like(best_fit_scores)\n\n    max_gap_exploitation = np.max(gap_exploitation_scores)\n    normalized_gap_exploitation = gap_exploitation_scores / max_gap_exploitation if max_gap_exploitation > 1e-6 else np.zeros_like(gap_exploitation_scores)\n\n    max_bin_fill_similarity = np.max(bin_fill_similarity_scores)\n    normalized_bin_fill_similarity = bin_fill_similarity_scores / max_bin_fill_similarity if max_bin_fill_similarity > 1e-6 else np.zeros_like(bin_fill_similarity_scores)\n\n    # Combine scores with dynamic weights.\n    # The weights should adapt based on the item's size relative to the *average* remaining capacity\n    # or the *maximum* remaining capacity among suitable bins.\n    # Let's use the maximum remaining capacity as a reference for \"how open\" the bins are.\n    max_suitable_cap = np.max(suitable_bins_caps)\n\n    # If item is large relative to max suitable capacity, prioritize best fit.\n    # If item is small relative to max suitable capacity, prioritize gap exploitation and fill similarity.\n    # Define a threshold for \"large\" item. Let's use 0.5 * max_suitable_cap.\n    # Normalize item size by max_suitable_cap to get a relative size.\n    relative_item_size = item / (max_suitable_cap + 1e-6)\n\n    # Weighting scheme:\n    # Best Fit is always important, especially for larger items.\n    # Gap Exploitation is good for items that can fill up larger available spaces.\n    # Bin Fill Similarity aims for balanced bins.\n\n    # Weight for Best Fit: increases with item size relative to available space.\n    weight_best_fit = 0.4 + 0.5 * relative_item_size\n    weight_best_fit = np.clip(weight_best_fit, 0.4, 0.9) # Ensure it's not too dominant for small items.\n\n    # Weight for Gap Exploitation: decreases with item size, favors smaller items filling larger gaps.\n    weight_gap_exploitation = 0.4 - 0.3 * relative_item_size\n    weight_gap_exploitation = np.clip(weight_gap_exploitation, 0.1, 0.4)\n\n    # Weight for Bin Fill Similarity: generally useful, moderate weight.\n    weight_bin_fill_similarity = 0.2 # Constant or slightly adjusted\n\n    # Ensure weights sum to 1 (approximately, or re-normalize if needed)\n    # A simpler approach for weights that sum to 1:\n    total_weight = weight_best_fit + weight_gap_exploitation + weight_bin_fill_similarity\n    # If total_weight is 0 (unlikely here), set to 1.\n    if total_weight < 1e-6:\n        total_weight = 1.0\n\n    weight_best_fit /= total_weight\n    weight_gap_exploitation /= total_weight\n    weight_bin_fill_similarity /= total_weight\n\n\n    combined_scores = (weight_best_fit * normalized_best_fit +\n                       weight_gap_exploitation * normalized_gap_exploitation +\n                       weight_bin_fill_similarity * normalized_bin_fill_similarity)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 38.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response4.txt_stdout.txt",
    "code_path": "problem_iter12_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    num_suitable_bins = suitable_bins_caps.shape[0]\n\n    # Metric 1: Best Fit (revisited)\n    # Focus on the *relative* remaining capacity after placement.\n    # Smaller relative remaining capacity is better (tighter fit).\n    # Use inverse of (1 + relative remaining capacity) to give higher scores to tighter fits.\n    # Added a small constant to the denominator to avoid division by zero and to differentiate very tight fits.\n    remaining_after_placement = suitable_bins_caps - item\n    relative_remaining = remaining_after_placement / suitable_bins_caps # How much capacity is left relative to the bin's current state\n    best_fit_scores = 1.0 / (1.0 + relative_remaining + 1e-6) # Higher score for smaller relative_remaining\n\n    # Metric 2: Fill Level Favorability\n    # Favor bins that are neither too empty nor too full. This promotes better space utilization.\n    # We can define a \"target fill level\". Let's assume a target fill level that is high but leaves some room, e.g., 80-90%.\n    # We can use a Gaussian-like function centered around a desired fill ratio.\n    # Let's assume bin capacity is implicitly normalized to 1.0 for this metric, and item size is also normalized.\n    # The actual capacity of the bin might be more relevant. Let's use remaining capacity relative to a hypothetical maximum bin capacity (e.g., 1.0 for normalized context, or inferable).\n    # For simplicity and adaptability, let's consider the ratio of *item size* to the *bin's remaining capacity*.\n    # A higher ratio means the item makes a bigger dent in the remaining capacity.\n    # We want to avoid bins that are almost empty, as a small item would leave them very inefficiently filled.\n    # A bin that is mostly full and can still accommodate the item is also less desirable for exploration.\n    # Let's score based on how much capacity is *already used* (1 - normalized remaining capacity).\n    # This encourages using bins that have some items already.\n    # Max capacity of a bin isn't explicitly given, assume it's large enough to hold any item.\n    # Let's use the item size relative to the *current remaining capacity* of the bin.\n    # If item/suitable_bins_caps is high, it means the item is a large fraction of what's left -> good for utilization.\n    # If item/suitable_bins_caps is low, it means the item is small compared to what's left -> might leave it too empty.\n    # We want to reward bins where the item fills a significant portion of the *remaining* space.\n    fill_score_component = item / (suitable_bins_caps + 1e-6)\n    # We also want to reward bins that are not excessively empty. Let's use the inverse of normalized remaining capacity.\n    # Normalized remaining capacity: suitable_bins_caps / MAX_BIN_CAPACITY (assume MAX_BIN_CAPACITY=1 for normalized context)\n    # So, 1 - suitable_bins_caps is a measure of \"fill level\". We want to prioritize bins with moderate fill levels.\n    # Let's combine these: prefer bins where the item fits snugly, and the bin isn't too empty.\n    # A bin is \"good\" if `suitable_bins_caps` is not too large, and `item` is not too small relative to it.\n    # Let's define a \"gap score\" based on `suitable_bins_caps - item`. Small gaps are good.\n    gap_scores = suitable_bins_caps - item\n    # We want to penalize bins that are too empty, meaning `suitable_bins_caps` is large.\n    # Let's consider the inverse of the *absolute* remaining capacity. Larger remaining capacity = lower score.\n    # And also the inverse of `gap_scores`. Smaller gaps = higher score.\n    # Combine `1 / (gap_scores + epsilon)` with `1 / (suitable_bins_caps + epsilon)`.\n    # This is effectively prioritizing bins with small remaining capacity after placement AND small total capacity.\n    # This seems too restrictive. Let's rethink.\n\n    # Metric 2 (Revised): Balanced Fill\n    # Aim for bins that are \"moderately full\" but also accommodate the item well.\n    # Consider the ratio of item size to bin capacity.\n    # `item / bin_capacity`. This is not directly available.\n    # Let's use `item / suitable_bins_caps` as a proxy for how much of the *current remaining space* the item occupies.\n    # High values are good (item fills a lot). Low values are bad (item is small relative to space).\n    # Also, consider the *absolute* remaining capacity after placement: `suitable_bins_caps - item`.\n    # Small absolute remaining capacity is good (tight fit). Large absolute remaining capacity is bad.\n    # Let's combine these: higher score if `item / suitable_bins_caps` is high AND `suitable_bins_caps - item` is low.\n    # This is somewhat redundant with Best Fit.\n\n    # Let's introduce a metric for \"bin efficiency\" or \"fairness\".\n    # Metric 2 (New): Bin Efficiency Prioritization\n    # Prioritize bins that have capacity close to the item size, but slightly larger.\n    # This helps consolidate items without leaving excessive \"slack\".\n    # We are looking for `suitable_bins_caps` that are \"just enough\".\n    # This can be modeled as `1 / (suitable_bins_caps - item + epsilon)` BUT we also want to avoid very large capacities.\n    # Let's use a metric that is high when `suitable_bins_caps` is close to `item`.\n    # We can use a Gaussian-like function centered around `item`.\n    # The \"variance\" of this Gaussian could adapt to the item size. Larger items might tolerate larger gaps.\n    # Let's keep it simpler: reward bins where `suitable_bins_caps` is \"just above\" `item`.\n    # This means `suitable_bins_caps - item` should be small and positive.\n    # `1 / (suitable_bins_caps - item + epsilon)` is similar to best-fit.\n\n    # Let's try a simpler, more direct approach for diversification and better space utilization.\n    # Metric 2: Space Utilization Fairness\n    # We want to use bins that have a reasonable amount of space left but are not excessively empty.\n    # A bin that is almost full and can fit the item is good.\n    # A bin that is almost empty and can fit the item is less good (leaves a lot of slack).\n    # Let's focus on the \"fill state\" of the bin *before* placing the item.\n    # We need to infer the \"original\" capacity or the \"current fill\".\n    # If we assume a fixed bin capacity (e.g., 1.0 for normalization), then `1 - suitable_bins_caps` is the \"fill ratio\".\n    # We want to avoid bins that are very empty (fill ratio close to 0).\n    # Let's define a \"fill penalty\": higher penalty for very empty bins.\n    # `fill_penalty = max(0, 0.5 - (1 - suitable_bins_caps))` -> penalize bins where remaining capacity > 0.5\n    # This is inverted. We want to *reward* bins that are not too empty.\n    # Let's consider `1 - suitable_bins_caps` as \"current fill\".\n    # We can use a function that peaks at a moderate fill level, e.g., 0.7-0.9.\n    # A simple sigmoid-like function can achieve this.\n    # `fill_score = 1 / (1 + exp(-(fill_ratio - target_fill) / steepness))`\n    # Let's use a simpler approach: reward bins whose remaining capacity is not too large.\n    # Inverse of remaining capacity: `1 / (suitable_bins_caps + epsilon)`.\n    # This prioritizes bins that are more full.\n    # However, this is counter to \"exploration\" or giving smaller items space.\n\n    # Let's try to balance \"tight fit\" with \"not too empty\".\n    # Metric 2: Optimized Capacity Usage\n    # Prioritize bins where the remaining capacity is just enough for the item,\n    # AND the bin wasn't excessively empty to begin with.\n    # Consider `suitable_bins_caps` relative to the item size.\n    # We want `suitable_bins_caps` to be slightly larger than `item`.\n    # Let's score based on the inverse of `suitable_bins_caps` (favoring fuller bins) and also penalize very large `suitable_bins_caps`.\n    # Consider the ratio: `item / suitable_bins_caps`. High is good.\n    # And the gap: `suitable_bins_caps - item`. Low is good.\n    # Let's combine them: `(item / suitable_bins_caps) * (1 / (suitable_bins_caps - item + epsilon))`\n    # This might become unstable if `suitable_bins_caps` is very close to `item`.\n\n    # Rethinking the goal: dynamic adaptation and avoiding suboptimal choices early on.\n    # Instead of multiple, possibly conflicting metrics with complex weighting, let's try a more robust, adaptive metric.\n    # A common issue is creating many nearly empty bins or very full bins.\n    # We want to find a balance.\n    # Let's consider the \"waste\" created by placing the item.\n    # Waste = `suitable_bins_caps - item`. Lower waste is better (Best Fit).\n    # However, if a bin has very little capacity remaining overall, even a small waste might be significant.\n    # Let's consider the \"quality\" of the bin itself.\n    # Metric 2: Quality-Aware Best Fit\n    # Prioritize bins that offer a good fit, but also consider the overall \"quality\" of the bin.\n    # A \"quality\" bin might be one that is not too empty, nor too full,\n    # and has already accommodated a few items (though we don't have this info directly).\n    # Let's proxy \"quality\" by the *inverse* of the remaining capacity.\n    # A bin with less remaining capacity is \"more full\" and might be considered of higher quality in terms of utilization.\n    # So, we want to reward:\n    # 1. Small gap (`suitable_bins_caps - item` is small).\n    # 2. High initial fill (small `suitable_bins_caps`).\n    # This can be combined as: `1 / (suitable_bins_caps - item + epsilon) * (1 / (suitable_bins_caps + epsilon))`.\n    # This effectively prioritizes bins that have *just enough* capacity and are already quite full.\n\n    # Let's try to be more nuanced. What if the item is very small?\n    # A very small item should preferably go into a bin that has a moderate amount of remaining capacity,\n    # to avoid making it too empty.\n    # What if the item is very large?\n    # A very large item should ideally go into a bin that has a capacity *just* over the item size.\n    # This suggests a weight that adapts not just to the item size, but also to the distribution of `suitable_bins_caps`.\n\n    # Metric 2: Adaptive Fit Quality\n    # We want to favor bins where `suitable_bins_caps` is slightly larger than `item`.\n    # Let's consider the \"excess capacity\" `excess = suitable_bins_caps - item`.\n    # We want `excess` to be small.\n    # Additionally, we want to avoid using bins that are already extremely full or extremely empty.\n    # Let's define a \"bin desirability\" based on its remaining capacity `suitable_bins_caps`.\n    # High desirability for bins with moderate remaining capacity (e.g., 50% to 80% full).\n    # Let's simplify and focus on the \"slack\" created.\n    # `slack = suitable_bins_caps - item`.\n    # We want `slack` to be small.\n    # However, if `suitable_bins_caps` is already very small, a small `slack` might be undesirable.\n    # Let's try a score that combines the inverse of `suitable_bins_caps` (favoring fuller bins)\n    # and penalizes large slack.\n    # Consider `score = (1 - suitable_bins_caps) / (suitable_bins_caps - item + epsilon)`\n    # This would be high if the bin is quite full AND the slack is small.\n    # It might be unstable if `suitable_bins_caps` is small.\n\n    # Let's introduce a penalty for bins that are *too* empty relative to the item.\n    # If `suitable_bins_caps` is much larger than `item`, it might lead to poor packing.\n    # A metric for \"over-capacity\": `max(0, suitable_bins_caps - item - target_slack)`\n    # where `target_slack` could be a small constant or a fraction of `item`.\n    # Let's aim for a specific target remaining capacity after placement, say `target_rem`.\n    # `target_rem` could be related to the item size, e.g., `0.1 * item` or `0.2 * BIN_CAPACITY`.\n    # For simplicity, let's make `target_rem` a small constant like 0.1.\n    # Then, we penalize bins where `suitable_bins_caps - item` is much larger than `target_rem`.\n    # `penalty = max(0, (suitable_bins_caps - item) - target_rem)`\n    # The score would be `1 / (penalty + 1)`.\n\n    # Metric 2: Balanced Slack Minimization\n    # Prioritize bins that minimize slack, but also penalize placing items in very empty bins\n    # where the item occupies a small fraction of the remaining capacity.\n    # Let `slack = suitable_bins_caps - item`. We want small slack.\n    # Consider the ratio `item / suitable_bins_caps`. We want this to be reasonably high.\n    # A combination could be: `(1 / (slack + epsilon)) * (item / (suitable_bins_caps + epsilon))`\n    # This score is high when slack is small AND item occupies a good portion of bin's remaining capacity.\n\n    # Let's try to integrate the \"exploration\" idea more subtly.\n    # Exploration might mean not always picking the absolute \"best\" fit if it means creating a very specialized bin.\n    # We want to keep options open.\n    # This might mean slightly favoring bins that are neither too full nor too empty.\n\n    # Metric 2 (Re-Revisited): Dynamic Target Fit\n    # The \"ideal\" remaining capacity after placement might depend on the item size and the overall bin fullness.\n    # If a bin is very full, we want the item to fit snugly (small residual capacity).\n    # If a bin is quite empty, we might want the item to fill a larger portion of it.\n    # Let's try a score that rewards bins where `suitable_bins_caps - item` is minimized,\n    # but with a twist: the \"penalty\" for slack depends on how \"full\" the bin is.\n    # Let's use the inverse of `suitable_bins_caps` to represent \"fullness\".\n    # `fullness_score = 1 / (suitable_bins_caps + epsilon)`.\n    # We want to penalize slack `suitable_bins_caps - item`.\n    # Combine: `score = fullness_score / (suitable_bins_caps - item + epsilon)`\n    # This would prioritize bins that are full and have small slack.\n\n    # Let's go with a simpler, more interpretable combination that balances tight fits and bin usage.\n    # Metric 2: Fill State Aware Best Fit\n    # We want a tight fit (Best Fit) and we want to avoid using bins that are too empty.\n    # `best_fit_contribution = 1.0 / (suitable_bins_caps - item + 1e-6)`\n    # How to penalize bins that are too empty?\n    # We can use `1 - suitable_bins_caps` as a proxy for \"fill level\".\n    # We want to avoid bins where `suitable_bins_caps` is large.\n    # So, we want to reward bins with low `suitable_bins_caps`.\n    # Let's try `fill_awareness = 1.0 / (suitable_bins_caps + 1e-6)`.\n    # This rewards fuller bins.\n    # Combining them: `score = best_fit_contribution * fill_awareness`\n    # `score = (1.0 / (suitable_bins_caps - item + 1e-6)) * (1.0 / (suitable_bins_caps + 1e-6))`\n    # This prioritizes bins that are both nearly full and have just enough capacity for the item.\n\n    # Let's normalize the components to ensure comparability.\n    # Metric 1: Best Fit (again, but more robust)\n    # Prioritize bins that leave minimum remaining capacity.\n    # `remaining_capacity = suitable_bins_caps - item`\n    # `best_fit_score = 1.0 / (remaining_capacity + 1e-6)`\n    # Normalize this score by the maximum possible best fit score.\n    remaining_capacity = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_capacity + 1e-6)\n\n    # Metric 2: Bin Fullness Prioritization\n    # Prioritize bins that are already more full. This encourages consolidating items.\n    # Use inverse of remaining capacity as a proxy for fullness.\n    # `fullness_score = 1.0 / (suitable_bins_caps + 1e-6)`\n    # Normalize this score by the maximum possible fullness score.\n    fullness_scores = 1.0 / (suitable_bins_caps + 1e-6)\n\n    # Metric 3: Item Size Ratio\n    # For smaller items, it's often beneficial to place them in bins that are not too empty,\n    # to avoid creating many sparsely filled bins.\n    # For larger items, it's crucial to find a good fit.\n    # Let's consider the ratio of the item size to the bin's remaining capacity.\n    # `item_ratio = item / suitable_bins_caps`. A higher ratio means the item makes a larger dent.\n    # This metric is good for smaller items in moderately full bins.\n    # For very large items, this ratio might be close to 1.\n    item_ratio_scores = item / (suitable_bins_caps + 1e-6)\n\n    # Normalization of components\n    # Normalize Best Fit scores: higher score for tighter fits.\n    if np.max(best_fit_scores) > 1e-9:\n        norm_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        norm_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Fullness scores: higher score for fuller bins.\n    if np.max(fullness_scores) > 1e-9:\n        norm_fullness = fullness_scores / np.max(fullness_scores)\n    else:\n        norm_fullness = np.zeros_like(fullness_scores)\n\n    # Normalize Item Ratio scores: higher score for items filling more of the bin's remaining capacity.\n    if np.max(item_ratio_scores) > 1e-9:\n        norm_item_ratio = item_ratio_scores / np.max(item_ratio_scores)\n    else:\n        norm_item_ratio = np.zeros_like(item_ratio_scores)\n\n    # Adaptive Weighting Strategy:\n    # The importance of each metric can depend on the item's size relative to the bin capacity.\n    # Assume a standard bin capacity (e.g., 1.0 for normalization purposes if item is scaled).\n    # If `item` is large (close to 1.0), 'Best Fit' and 'Fullness' are critical.\n    # If `item` is small, 'Item Ratio' and 'Fullness' are important to avoid waste.\n\n    # Let's define weights based on the item's size relative to the *average* suitable bin capacity.\n    # This provides a context for the item.\n    avg_suitable_cap = np.mean(suitable_bins_caps)\n    if avg_suitable_cap > 1e-9:\n        item_vs_avg_cap_ratio = item / avg_suitable_cap\n    else:\n        item_vs_avg_cap_ratio = 0.5 # Default if no suitable bins or very small capacity\n\n    # Weights adjustment:\n    # If item is large relative to average capacity, emphasize Best Fit and Fullness.\n    # If item is small relative to average capacity, emphasize Item Ratio and Fullness.\n\n    # Base weights, can be tuned.\n    w_bf = 0.4\n    w_f = 0.4\n    w_ir = 0.2\n\n    # Adjust weights dynamically.\n    # Example: if item is large, increase weight for BF and F.\n    if item_vs_avg_cap_ratio > 1.0: # Item is larger than average remaining capacity\n        w_bf += 0.2 * (item_vs_avg_cap_ratio - 1.0) # Boost BF for large items\n        w_f += 0.1 * (item_vs_avg_cap_ratio - 1.0) # Boost F for large items\n        w_ir -= 0.3 * (item_vs_avg_cap_ratio - 1.0) # Decrease IR for large items\n    else: # Item is smaller than average remaining capacity\n        w_ir += 0.3 * (1.0 - item_vs_avg_cap_ratio) # Boost IR for small items\n        w_f += 0.1 * (1.0 - item_vs_avg_cap_ratio) # Boost F for small items\n        w_bf -= 0.2 * (1.0 - item_vs_avg_cap_ratio) # Decrease BF for small items\n\n    # Ensure weights are non-negative and sum to 1 (or close enough, then re-normalize).\n    w_bf = max(0, w_bf)\n    w_f = max(0, w_f)\n    w_ir = max(0, w_ir)\n\n    total_w = w_bf + w_f + w_ir\n    if total_w > 1e-9:\n        w_bf /= total_w\n        w_f /= total_w\n        w_ir /= total_w\n    else: # Fallback to equal weights if something goes wrong\n        w_bf, w_f, w_ir = 1/3, 1/3, 1/3\n\n    # Combine normalized scores with dynamic weights\n    combined_scores = (w_bf * norm_best_fit +\n                       w_f * norm_fullness +\n                       w_ir * norm_item_ratio)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 59.0,
    "cyclomatic_complexity": 8.0,
    "exec_success": true
  }
]