{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines tightest fit with a normalized bonus for larger remaining capacity,\n    using a weighted sum for balanced decision-making. Favors bins that are\n    almost full but can still accommodate the item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    # Metric 1: Best Fit - prioritize bins with minimal remaining capacity after placement.\n    # Higher score for smaller residual space.\n    remaining_after_placement = suitable_bins_caps - item\n    # Inverse relationship: smaller residual -> higher score. Add epsilon for stability.\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-9)\n    \n    # Metric 2: Exploration/Larger Bin Preference - favor bins with more capacity initially.\n    # This encourages not always picking the absolute tightest, promoting diversification.\n    # We'll use a logarithmic scale for remaining capacity to de-emphasize very large bins\n    # and focus on bins that are \"reasonably\" large but not excessively so.\n    # log1p is used to handle cases where remaining capacity is 0 after placement,\n    # and to provide a smoother scaling than a simple linear approach.\n    exploration_scores = np.log1p(suitable_bins_caps)\n    \n    # Normalize Best Fit scores (min-max scaling)\n    if suitable_bins_caps.size > 1:\n        min_bf = np.min(best_fit_scores)\n        max_bf = np.max(best_fit_scores)\n        range_bf = max_bf - min_bf\n        if range_bf > 1e-9:\n            normalized_best_fit = (best_fit_scores - min_bf) / range_bf\n        else:\n            normalized_best_fit = np.ones_like(best_fit_scores) # All suitable bins offer same tightness score\n    elif suitable_bins_caps.size == 1:\n        normalized_best_fit = np.array([1.0])\n    else:\n        normalized_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Exploration scores (min-max scaling)\n    if suitable_bins_caps.size > 1:\n        min_exp = np.min(exploration_scores)\n        max_exp = np.max(exploration_scores)\n        range_exp = max_exp - min_exp\n        if range_exp > 1e-9:\n            normalized_exploration = (exploration_scores - min_exp) / range_exp\n        else:\n            normalized_exploration = np.zeros_like(exploration_scores) # All suitable bins have same initial capacity\n    elif suitable_bins_caps.size == 1:\n        normalized_exploration = np.array([1.0]) # If only one bin, it's maximally \"exploratory\" in this context\n    else:\n        normalized_exploration = np.zeros_like(exploration_scores)\n\n    # Combine normalized scores using a weighted sum.\n    # We give a slightly higher weight to Best Fit, as tight packing is crucial for BPP.\n    # The exploration bonus helps to prevent premature fragmentation.\n    combined_scores = 0.7 * normalized_best_fit + 0.3 * normalized_exploration\n    \n    priorities[suitable_bins_mask] = combined_scores\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines a nuanced best-fit approach with an exploration bonus favoring less utilized bins,\n    and a subtle preference for bins with more overall remaining capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Modified Best Fit (from priority_v0)\n    # Prioritizes bins leaving a \"sweet spot\" residual capacity to avoid tiny unusable gaps.\n    remaining_after_placement_m1 = suitable_bins_caps - item\n    target_residual = item * 0.2  # Target residual capacity ~20% of item size\n    # Gaussian-like function: higher score for residuals closer to target_residual\n    # Add small epsilon to avoid division by zero if target_residual is 0.\n    best_fit_scores = np.exp(-((remaining_after_placement_m1 - target_residual) / (target_residual + 1e-6))**2)\n\n    # Metric 2: Exploration Bonus (inspired by priority_v1, simpler version)\n    # Favors bins that are less full *after* placement, relative to other suitable bins.\n    min_rem_after_m2 = np.min(remaining_after_placement_m1)\n    max_rem_after_m2 = np.max(remaining_after_placement_m1)\n    \n    exploration_scores = np.zeros_like(suitable_bins_caps)\n    if max_rem_after_m2 > min_rem_after_m2:\n        # Normalized remaining capacity after placement: higher for more empty bins\n        exploration_scores = (remaining_after_placement_m1 - min_rem_after_m2) / (max_rem_after_m2 - min_rem_after_m2)\n    else:\n        # If all suitable bins result in the same remaining capacity, no exploration bonus from this diff.\n        # Default to 0.5 for any such bins to avoid bias.\n        exploration_scores = np.ones_like(suitable_bins_caps) * 0.5\n\n    # Metric 3: Usage Proxy (favors bins with more total remaining capacity)\n    # This is a simple proxy for less-used bins.\n    min_cap_all = np.min(bins_remain_cap)\n    max_cap_all = np.max(bins_remain_cap)\n    \n    usage_scores = np.zeros_like(suitable_bins_caps)\n    if max_cap_all > min_cap_all:\n        # Normalize current remaining capacities. Higher score for more remaining capacity.\n        normalized_current_caps = (suitable_bins_caps - min_cap_all) / (max_cap_all - min_cap_all)\n        usage_scores = normalized_current_caps\n    else:\n        # If all bins have same capacity, this metric doesn't differentiate.\n        usage_scores = np.ones_like(suitable_bins_caps) * 0.5\n\n    # Combine scores: Heavy emphasis on nuanced best-fit, moderate on exploration, light on usage.\n    # Weights are chosen to balance finding good fits with spreading items.\n    # 0.6 for Best Fit (primary, quality of fit)\n    # 0.3 for Exploration (secondary, diversity)\n    # 0.1 for Usage (tertiary, simple preference for emptier bins)\n    combined_scores = 0.6 * best_fit_scores + 0.3 * exploration_scores + 0.1 * usage_scores\n\n    # Ensure scores are within a reasonable range and handle potential NaNs/Infs\n    combined_scores = np.nan_to_num(combined_scores, nan=0.0, posinf=1.0, neginf=0.0)\n    combined_scores = np.clip(combined_scores, 0.0, 1.0)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1 and 2 (identical): They implement three metrics: Best Fit, Gap Exploitation, and Bin Fill Similarity, with dynamic weighting based on item size relative to max suitable capacity. The weights adapt to prioritize Best Fit for larger items and Gap Exploitation/Fill Similarity for smaller ones. Normalization is applied to each metric before combining.\n\nComparing Heuristics 3 and 4: Heuristic 3 combines Best Fit (tightness) with Exploration (larger initial capacity) using a fixed weighted sum. Heuristic 4 combines Best Fit (negative remaining capacity) with Exploration (normalized remaining capacity) using a weighted sum, favoring Best Fit. Heuristic 3 normalizes component scores via min-max scaling, while Heuristic 4 uses direct combination.\n\nComparing Heuristics 5, 6, 7, 8 (identical): These heuristics also use Best Fit (relative remaining capacity), Bin Fullness (inverse of remaining capacity), and Item Size Ratio. They employ dynamic weighting based on the item's size relative to the average suitable bin capacity. The weights adapt to prioritize Best Fit and Fullness for larger items, and Item Ratio/Fullness for smaller items. Normalization is applied to each metric before weighted combination.\n\nComparing Heuristics 9 and 10, 11, 12 (identical): Heuristic 9 combines Best Fit (tightness) with a fairness penalty (deviation from average remaining capacity). Heuristics 10-12 combine Modified Best Fit (sweet spot residual), Exploration (normalized remaining capacity after placement), and Usage Proxy (normalized current remaining capacity) with fixed weights.\n\nComparing Heuristics 13, 14, 15 (identical): These combine Refined Best Fit (log1p of inverse residual) with Exploration (min-max scaled remaining capacity after placement), using dynamic weights based on item size relative to a threshold.\n\nComparing Heuristics 16, 17, 18, 19, 20 (identical): These combine Best Fit (inverse residual) with Exploration (log1p or log of remaining capacity). They use fixed weights and normalize the combined score. Heuristics 17-18 have slightly different weights (0.55/0.45) than 19-20 (0.55/0.45) and 16 (0.7/0.3) for Best Fit vs. Exploration.\n\nOverall: The best heuristics (1-8) tend to use multiple metrics and adapt their weighting dynamically based on item characteristics or bin states, often involving normalization of individual metrics or the final combined score. Simpler fixed-weight combinations of Best Fit and Exploration (like 16-20) are less sophisticated but might be more robust. The use of logarithmic or Gaussian-like functions for scoring can provide smoother preference curves.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Metric selection, dynamic weighting, normalization, balance, robustness, simplicity.\n*   **Advice:** Focus on a core set of well-defined, interpretable metrics. Systematically explore dynamic weighting and robust normalization techniques. Aim for a balance between primary objectives and secondary goals like diversification or exploration.\n*   **Avoid:** Overly complex or opaque mathematical transformations, unstable calculations, and prioritizing single, overly simplistic metrics.\n*   **Explanation:** Effective self-reflection identifies *how* and *why* certain metric combinations and adjustments lead to improved performance by understanding their impact on the problem's core objectives.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}