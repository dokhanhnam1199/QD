{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit - score based on how tightly the item fits\n    # Using log to compress larger gaps, making smaller gaps relatively more important\n    remaining_after_placement = suitable_bins_caps - item\n    # Add a small epsilon to avoid log(0) and to differentiate very tight fits\n    best_fit_scores = np.log1p(1.0 / (remaining_after_placement + 1e-6))\n\n    # Metric 2: Exploration Bonus - score based on the \"openness\" of the bin\n    # Favor bins that have a significant amount of remaining capacity, but not excessively so.\n    # This is a sigmoid-like approach to reward moderately open bins.\n    # Normalize capacity to a 0-1 range based on the maximum possible capacity (assumed to be large, e.g., 100 for typical bin packing)\n    # or based on the max capacity among suitable bins if that's more contextually relevant.\n    # Let's use max of suitable bins as it's adaptive.\n    max_suitable_cap = np.max(suitable_bins_caps)\n    if max_suitable_cap > 1e-6:\n        normalized_suitable_caps = suitable_bins_caps / max_suitable_cap\n        # Sigmoid-like function to reward bins that are not too empty, not too full.\n        # Parameters can be tuned. Here, we aim to reward bins with roughly 50-75% remaining capacity.\n        # Example: exp(-(x-0.6)^2) where x is normalized_suitable_caps\n        exploration_scores = np.exp(-((normalized_suitable_caps - 0.6)**2) / 0.2)\n    else:\n        exploration_scores = np.zeros_like(suitable_bins_caps)\n\n    # Metric 3: Uniformity Bonus - Reward bins that are already partially filled,\n    # aiming to create more uniformly filled bins overall.\n    # This is inversely related to how \"empty\" the bin is.\n    # We can score based on how much has *already* been placed in the bin.\n    # Let's estimate initial fill based on remaining capacity relative to a hypothetical 'full' capacity.\n    # For simplicity, assume max bin capacity is 1.0, or use a value derived from data.\n    # Here, we'll use remaining capacity relative to the *item's* size to penalize bins that are *almost* empty\n    # and would be significantly \"wasted\" by a small item.\n    # More generally, consider the ratio of remaining capacity to the item size.\n    # A higher ratio means the item is small relative to the bin's open space.\n    # We want to prioritize bins where the item represents a larger fraction of the remaining space,\n    # which is counter to simple exploration.\n    # Let's reframe: reward bins that have *some* capacity already used.\n    # We can proxy this by 1 - normalized_suitable_caps, then apply a similar sigmoid.\n    # However, a simpler approach is to reward bins that are not \"too empty\".\n    # Let's use the inverse of normalized_suitable_caps, scaled to avoid large values.\n    # Consider `1 - normalized_suitable_caps` as a measure of \"fill level\".\n    # We want to slightly penalize very low fill levels.\n    fill_level = 1.0 - normalized_suitable_caps\n    # Add a small penalty for bins that are very empty (i.e., fill_level is close to 0)\n    # Using a power function to make the penalty more pronounced for emptier bins.\n    uniformity_scores = np.where(fill_level < 0.3, fill_level * 5, fill_level) # Penalize very empty bins more\n    uniformity_scores = np.clip(uniformity_scores, 0, 1) # Clip to 0-1 range\n\n\n    # Normalize scores to be in a comparable range [0, 1]\n    if np.max(best_fit_scores) > 1e-6:\n        normalized_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        normalized_best_fit = np.zeros_like(best_fit_scores)\n\n    if np.max(exploration_scores) > 1e-6:\n        normalized_exploration = exploration_scores / np.max(exploration_scores)\n    else:\n        normalized_exploration = np.zeros_like(exploration_scores)\n\n    if np.max(uniformity_scores) > 1e-6:\n        normalized_uniformity = uniformity_scores / np.max(uniformity_scores)\n    else:\n        normalized_uniformity = np.zeros_like(uniformity_scores)\n\n\n    # Combine scores with dynamic weights.\n    # The weights can be adjusted based on the item size relative to the bin capacity.\n    # If the item is large, prioritize \"best fit\". If the item is small, prioritize \"exploration\" and \"uniformity\".\n\n    # Example dynamic weighting:\n    # For smaller items, give more weight to exploration and uniformity.\n    # For larger items, give more weight to best fit.\n    # Let's define a threshold for \"small\" item, e.g., item size < 0.5 * max_bin_capacity. Assume max_bin_capacity = 1.0 for normalization.\n    item_size_normalized = item # assuming item is already normalized or scaled appropriately\n\n    weight_best_fit = 0.5 + 0.4 * item_size_normalized # weight increases with item size\n    weight_exploration = 0.3 - 0.2 * item_size_normalized # weight decreases with item size\n    weight_uniformity = 0.2 - 0.1 * item_size_normalized # weight decreases with item size\n\n    # Ensure weights sum to 1 (or handle normalization if not exactly 1)\n    total_weight = weight_best_fit + weight_exploration + weight_uniformity\n    weight_best_fit /= total_weight\n    weight_exploration /= total_weight\n    weight_uniformity /= total_weight\n\n\n    combined_scores = (weight_best_fit * normalized_best_fit +\n                       weight_exploration * normalized_exploration +\n                       weight_uniformity * normalized_uniformity)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines tight packing (Best Fit) with a diversification bonus favoring\n    less utilized bins. Balances fitting tightly with spreading load.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_cap - item\n\n    # Primary objective: Tight Fit (Best Fit)\n    # Higher score for smaller remaining capacity. Use reciprocal for emphasis.\n    tight_fit_score = 1.0 / (remaining_after_placement + 1e-9)\n\n    # Secondary objective: Diversification (Favor less utilized bins)\n    # This encourages using bins that are not already very full.\n    # We can measure this by the *remaining capacity after placement* relative to the\n    # *total capacity of the bin*. A higher ratio here means the bin was less full.\n    # Use min-max scaling for a robust normalized score between 0 and 1.\n    min_rem_after = np.min(remaining_after_placement)\n    max_rem_after = np.max(remaining_after_placement)\n\n    exploration_bonus = np.zeros_like(remaining_after_placement)\n    if max_rem_after > min_rem_after:\n        # Normalize remaining capacity after placement to get exploration score\n        exploration_bonus = (remaining_after_placement - min_rem_after) / (max_rem_after - min_rem_after)\n    else:\n        # If all suitable bins result in the same remaining capacity, no bonus from this metric.\n        pass\n\n    # Combine scores: Primarily driven by tight fit, with an additive exploration bonus.\n    # The exploration bonus is scaled down to ensure tight fit remains dominant,\n    # but it serves as a tie-breaker and encourages exploration.\n    # A weight of 0.1 is empirically chosen to give exploration a modest influence.\n    final_priorities = tight_fit_score + 0.1 * exploration_bonus\n\n    priorities[suitable_bins_mask] = final_priorities\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1st and 2nd: They are identical, indicating no difference in performance.\n\nComparing Heuristics 3rd, 4th, and 5th: These three heuristics are identical, suggesting they represent a single approach with consistent performance. They focus on a weighted sum of \"Best Fit\" and \"Favor Larger Bins\" using min-max scaling for the latter.\n\nComparing Heuristics 6th with 3rd/4th/5th: Heuristic 6th uses a different approach for exploration (less utilized bins) by directly using the difference from min/max suitable bin capacities for its score, and combines it with a negative best-fit score. This suggests a variation in how \"exploration\" is defined and combined.\n\nComparing Heuristics 9th with 1st/2nd: Heuristic 9th introduces a \"Modified Best Fit\" using a quadratic function around a target residual, and a refined \"Exploration Bonus\" with a sigmoid-like function, plus a \"Bin Age/Usage\" proxy. This is a more complex multi-metric approach compared to the simpler linear combinations in 1st/2nd.\n\nComparing Heuristics 10th/11th with 1st/2nd: Heuristics 10th and 11th are identical. They combine a \"tightness_scores\" (inverse of remaining space) with an \"emptiness_bonus\" (log of remaining capacity). This is a different combination than the more complex metrics in Heuristic 1.\n\nComparing Heuristics 12th/13th/14th with 10th/11th: These three heuristics are identical. They are very similar to Heuristics 10th/11th, using inverse of remaining space for tightness, and a normalized remaining capacity (min-max scaled) for exploration. The key difference is the normalization method for exploration and the weighting.\n\nComparing Heuristics 15th with 12th/13th/14th: Heuristic 15th is identical to Heuristics 16th and 17th. It uses inverse of remaining space for Best Fit and log1p of remaining capacity for exploration, then normalizes the exploration score and combines them.\n\nComparing Heuristics 18th/19th with 15th/16th/17th: Heuristics 18th and 19th are identical. They are similar to 15th/16th/17th by using inverse remaining space for tightness and log of remaining capacity for exploration, but normalize the *combined* scores instead of just the exploration component.\n\nComparing Heuristics 20th with others: Heuristic 20th introduces \"Fit Quality\" (ratio of bin capacity to item size) and \"Remaining Capacity Variance\" reduction. This is a distinct approach that doesn't directly use \"Best Fit\" in the same way.\n\nOverall: The heuristics generally explore variations of \"Best Fit\" (minimizing remaining space) and \"Exploration\" (favoring less full bins, or bins with specific properties). Normalization techniques and weighting schemes are varied. The complexity of metrics and their combination seems to loosely correlate with rank, with simpler combinations often appearing higher. Heuristics 7th and 8th are truncated and thus not fully comparable.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Balance, Nuance, Justification, Adaptability, Simplicity (with purpose).\n*   **Advice:** Focus on *why* metrics are combined and *how* they interact. Justify normalization and weighting choices. Design heuristics that can adapt to different problem instances or stages of the search.\n*   **Avoid:** Arbitrary metric combinations, overly complex mathematical transformations without clear performance benefits, neglecting the *interpretability* of the heuristic's decisions, and assuming a single \"best\" metric.\n*   **Explanation:** True self-reflection goes beyond listing techniques. It requires critically assessing the *causal link* between a heuristic's design choices and its observed performance, aiming for elegant, well-reasoned solutions rather than just technically complex ones.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}