{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    num_suitable_bins = suitable_bins_caps.shape[0]\n\n    # Metric 1: Best Fit (revisited)\n    # Focus on the *relative* remaining capacity after placement.\n    # Smaller relative remaining capacity is better (tighter fit).\n    # Use inverse of (1 + relative remaining capacity) to give higher scores to tighter fits.\n    # Added a small constant to the denominator to avoid division by zero and to differentiate very tight fits.\n    remaining_after_placement = suitable_bins_caps - item\n    relative_remaining = remaining_after_placement / suitable_bins_caps # How much capacity is left relative to the bin's current state\n    best_fit_scores = 1.0 / (1.0 + relative_remaining + 1e-6) # Higher score for smaller relative_remaining\n\n    # Metric 2: Fill Level Favorability\n    # Favor bins that are neither too empty nor too full. This promotes better space utilization.\n    # We can define a \"target fill level\". Let's assume a target fill level that is high but leaves some room, e.g., 80-90%.\n    # We can use a Gaussian-like function centered around a desired fill ratio.\n    # Let's assume bin capacity is implicitly normalized to 1.0 for this metric, and item size is also normalized.\n    # The actual capacity of the bin might be more relevant. Let's use remaining capacity relative to a hypothetical maximum bin capacity (e.g., 1.0 for normalized context, or inferable).\n    # For simplicity and adaptability, let's consider the ratio of *item size* to the *bin's remaining capacity*.\n    # A higher ratio means the item makes a bigger dent in the remaining capacity.\n    # We want to avoid bins that are almost empty, as a small item would leave them very inefficiently filled.\n    # A bin that is mostly full and can still accommodate the item is also less desirable for exploration.\n    # Let's score based on how much capacity is *already used* (1 - normalized remaining capacity).\n    # This encourages using bins that have some items already.\n    # Max capacity of a bin isn't explicitly given, assume it's large enough to hold any item.\n    # Let's use the item size relative to the *current remaining capacity* of the bin.\n    # If item/suitable_bins_caps is high, it means the item is a large fraction of what's left -> good for utilization.\n    # If item/suitable_bins_caps is low, it means the item is small compared to what's left -> might leave it too empty.\n    # We want to reward bins where the item fills a significant portion of the *remaining* space.\n    fill_score_component = item / (suitable_bins_caps + 1e-6)\n    # We also want to reward bins that are not excessively empty. Let's use the inverse of normalized remaining capacity.\n    # Normalized remaining capacity: suitable_bins_caps / MAX_BIN_CAPACITY (assume MAX_BIN_CAPACITY=1 for normalized context)\n    # So, 1 - suitable_bins_caps is a measure of \"fill level\". We want to prioritize bins with moderate fill levels.\n    # Let's combine these: prefer bins where the item fits snugly, and the bin isn't too empty.\n    # A bin is \"good\" if `suitable_bins_caps` is not too large, and `item` is not too small relative to it.\n    # Let's define a \"gap score\" based on `suitable_bins_caps - item`. Small gaps are good.\n    gap_scores = suitable_bins_caps - item\n    # We want to penalize bins that are too empty, meaning `suitable_bins_caps` is large.\n    # Let's consider the inverse of the *absolute* remaining capacity. Larger remaining capacity = lower score.\n    # And also the inverse of `gap_scores`. Smaller gaps = higher score.\n    # Combine `1 / (gap_scores + epsilon)` with `1 / (suitable_bins_caps + epsilon)`.\n    # This is effectively prioritizing bins with small remaining capacity after placement AND small total capacity.\n    # This seems too restrictive. Let's rethink.\n\n    # Metric 2 (Revised): Balanced Fill\n    # Aim for bins that are \"moderately full\" but also accommodate the item well.\n    # Consider the ratio of item size to bin capacity.\n    # `item / bin_capacity`. This is not directly available.\n    # Let's use `item / suitable_bins_caps` as a proxy for how much of the *current remaining space* the item occupies.\n    # High values are good (item fills a lot). Low values are bad (item is small relative to space).\n    # Also, consider the *absolute* remaining capacity after placement: `suitable_bins_caps - item`.\n    # Small absolute remaining capacity is good (tight fit). Large absolute remaining capacity is bad.\n    # Let's combine these: higher score if `item / suitable_bins_caps` is high AND `suitable_bins_caps - item` is low.\n    # This is somewhat redundant with Best Fit.\n\n    # Let's introduce a metric for \"bin efficiency\" or \"fairness\".\n    # Metric 2 (New): Bin Efficiency Prioritization\n    # Prioritize bins that have capacity close to the item size, but slightly larger.\n    # This helps consolidate items without leaving excessive \"slack\".\n    # We are looking for `suitable_bins_caps` that are \"just enough\".\n    # This can be modeled as `1 / (suitable_bins_caps - item + epsilon)` BUT we also want to avoid very large capacities.\n    # Let's use a metric that is high when `suitable_bins_caps` is close to `item`.\n    # We can use a Gaussian-like function centered around `item`.\n    # The \"variance\" of this Gaussian could adapt to the item size. Larger items might tolerate larger gaps.\n    # Let's keep it simpler: reward bins where `suitable_bins_caps` is \"just above\" `item`.\n    # This means `suitable_bins_caps - item` should be small and positive.\n    # `1 / (suitable_bins_caps - item + epsilon)` is similar to best-fit.\n\n    # Let's try a simpler, more direct approach for diversification and better space utilization.\n    # Metric 2: Space Utilization Fairness\n    # We want to use bins that have a reasonable amount of space left but are not excessively empty.\n    # A bin that is almost full and can fit the item is good.\n    # A bin that is almost empty and can fit the item is less good (leaves a lot of slack).\n    # Let's focus on the \"fill state\" of the bin *before* placing the item.\n    # We need to infer the \"original\" capacity or the \"current fill\".\n    # If we assume a fixed bin capacity (e.g., 1.0 for normalization), then `1 - suitable_bins_caps` is the \"fill ratio\".\n    # We want to avoid bins that are very empty (fill ratio close to 0).\n    # Let's define a \"fill penalty\": higher penalty for very empty bins.\n    # `fill_penalty = max(0, 0.5 - (1 - suitable_bins_caps))` -> penalize bins where remaining capacity > 0.5\n    # This is inverted. We want to *reward* bins that are not too empty.\n    # Let's consider `1 - suitable_bins_caps` as \"current fill\".\n    # We can use a function that peaks at a moderate fill level, e.g., 0.7-0.9.\n    # A simple sigmoid-like function can achieve this.\n    # `fill_score = 1 / (1 + exp(-(fill_ratio - target_fill) / steepness))`\n    # Let's use a simpler approach: reward bins whose remaining capacity is not too large.\n    # Inverse of remaining capacity: `1 / (suitable_bins_caps + epsilon)`.\n    # This prioritizes bins that are more full.\n    # However, this is counter to \"exploration\" or giving smaller items space.\n\n    # Let's try to balance \"tight fit\" with \"not too empty\".\n    # Metric 2: Optimized Capacity Usage\n    # Prioritize bins where the remaining capacity is just enough for the item,\n    # AND the bin wasn't excessively empty to begin with.\n    # Consider `suitable_bins_caps` relative to the item size.\n    # We want `suitable_bins_caps` to be slightly larger than `item`.\n    # Let's score based on the inverse of `suitable_bins_caps` (favoring fuller bins) and also penalize very large `suitable_bins_caps`.\n    # Consider the ratio: `item / suitable_bins_caps`. High is good.\n    # And the gap: `suitable_bins_caps - item`. Low is good.\n    # Let's combine them: `(item / suitable_bins_caps) * (1 / (suitable_bins_caps - item + epsilon))`\n    # This might become unstable if `suitable_bins_caps` is very close to `item`.\n\n    # Rethinking the goal: dynamic adaptation and avoiding suboptimal choices early on.\n    # Instead of multiple, possibly conflicting metrics with complex weighting, let's try a more robust, adaptive metric.\n    # A common issue is creating many nearly empty bins or very full bins.\n    # We want to find a balance.\n    # Let's consider the \"waste\" created by placing the item.\n    # Waste = `suitable_bins_caps - item`. Lower waste is better (Best Fit).\n    # However, if a bin has very little capacity remaining overall, even a small waste might be significant.\n    # Let's consider the \"quality\" of the bin itself.\n    # Metric 2: Quality-Aware Best Fit\n    # Prioritize bins that offer a good fit, but also consider the overall \"quality\" of the bin.\n    # A \"quality\" bin might be one that is not too empty, nor too full,\n    # and has already accommodated a few items (though we don't have this info directly).\n    # Let's proxy \"quality\" by the *inverse* of the remaining capacity.\n    # A bin with less remaining capacity is \"more full\" and might be considered of higher quality in terms of utilization.\n    # So, we want to reward:\n    # 1. Small gap (`suitable_bins_caps - item` is small).\n    # 2. High initial fill (small `suitable_bins_caps`).\n    # This can be combined as: `1 / (suitable_bins_caps - item + epsilon) * (1 / (suitable_bins_caps + epsilon))`.\n    # This effectively prioritizes bins that have *just enough* capacity and are already quite full.\n\n    # Let's try to be more nuanced. What if the item is very small?\n    # A very small item should preferably go into a bin that has a moderate amount of remaining capacity,\n    # to avoid making it too empty.\n    # What if the item is very large?\n    # A very large item should ideally go into a bin that has a capacity *just* over the item size.\n    # This suggests a weight that adapts not just to the item size, but also to the distribution of `suitable_bins_caps`.\n\n    # Metric 2: Adaptive Fit Quality\n    # We want to favor bins where `suitable_bins_caps` is slightly larger than `item`.\n    # Let's consider the \"excess capacity\" `excess = suitable_bins_caps - item`.\n    # We want `excess` to be small.\n    # Additionally, we want to avoid using bins that are already extremely full or extremely empty.\n    # Let's define a \"bin desirability\" based on its remaining capacity `suitable_bins_caps`.\n    # High desirability for bins with moderate remaining capacity (e.g., 50% to 80% full).\n    # Let's simplify and focus on the \"slack\" created.\n    # `slack = suitable_bins_caps - item`.\n    # We want `slack` to be small.\n    # However, if `suitable_bins_caps` is already very small, a small `slack` might be undesirable.\n    # Let's try a score that combines the inverse of `suitable_bins_caps` (favoring fuller bins)\n    # and penalizes large slack.\n    # Consider `score = (1 - suitable_bins_caps) / (suitable_bins_caps - item + epsilon)`\n    # This would be high if the bin is quite full AND the slack is small.\n    # It might be unstable if `suitable_bins_caps` is small.\n\n    # Let's introduce a penalty for bins that are *too* empty relative to the item.\n    # If `suitable_bins_caps` is much larger than `item`, it might lead to poor packing.\n    # A metric for \"over-capacity\": `max(0, suitable_bins_caps - item - target_slack)`\n    # where `target_slack` could be a small constant or a fraction of `item`.\n    # Let's aim for a specific target remaining capacity after placement, say `target_rem`.\n    # `target_rem` could be related to the item size, e.g., `0.1 * item` or `0.2 * BIN_CAPACITY`.\n    # For simplicity, let's make `target_rem` a small constant like 0.1.\n    # Then, we penalize bins where `suitable_bins_caps - item` is much larger than `target_rem`.\n    # `penalty = max(0, (suitable_bins_caps - item) - target_rem)`\n    # The score would be `1 / (penalty + 1)`.\n\n    # Metric 2: Balanced Slack Minimization\n    # Prioritize bins that minimize slack, but also penalize placing items in very empty bins\n    # where the item occupies a small fraction of the remaining capacity.\n    # Let `slack = suitable_bins_caps - item`. We want small slack.\n    # Consider the ratio `item / suitable_bins_caps`. We want this to be reasonably high.\n    # A combination could be: `(1 / (slack + epsilon)) * (item / (suitable_bins_caps + epsilon))`\n    # This score is high when slack is small AND item occupies a good portion of bin's remaining capacity.\n\n    # Let's try to integrate the \"exploration\" idea more subtly.\n    # Exploration might mean not always picking the absolute \"best\" fit if it means creating a very specialized bin.\n    # We want to keep options open.\n    # This might mean slightly favoring bins that are neither too full nor too empty.\n\n    # Metric 2 (Re-Revisited): Dynamic Target Fit\n    # The \"ideal\" remaining capacity after placement might depend on the item size and the overall bin fullness.\n    # If a bin is very full, we want the item to fit snugly (small residual capacity).\n    # If a bin is quite empty, we might want the item to fill a larger portion of it.\n    # Let's try a score that rewards bins where `suitable_bins_caps - item` is minimized,\n    # but with a twist: the \"penalty\" for slack depends on how \"full\" the bin is.\n    # Let's use the inverse of `suitable_bins_caps` to represent \"fullness\".\n    # `fullness_score = 1 / (suitable_bins_caps + epsilon)`.\n    # We want to penalize slack `suitable_bins_caps - item`.\n    # Combine: `score = fullness_score / (suitable_bins_caps - item + epsilon)`\n    # This would prioritize bins that are full and have small slack.\n\n    # Let's go with a simpler, more interpretable combination that balances tight fits and bin usage.\n    # Metric 2: Fill State Aware Best Fit\n    # We want a tight fit (Best Fit) and we want to avoid using bins that are too empty.\n    # `best_fit_contribution = 1.0 / (suitable_bins_caps - item + 1e-6)`\n    # How to penalize bins that are too empty?\n    # We can use `1 - suitable_bins_caps` as a proxy for \"fill level\".\n    # We want to avoid bins where `suitable_bins_caps` is large.\n    # So, we want to reward bins with low `suitable_bins_caps`.\n    # Let's try `fill_awareness = 1.0 / (suitable_bins_caps + 1e-6)`.\n    # This rewards fuller bins.\n    # Combining them: `score = best_fit_contribution * fill_awareness`\n    # `score = (1.0 / (suitable_bins_caps - item + 1e-6)) * (1.0 / (suitable_bins_caps + 1e-6))`\n    # This prioritizes bins that are both nearly full and have just enough capacity for the item.\n\n    # Let's normalize the components to ensure comparability.\n    # Metric 1: Best Fit (again, but more robust)\n    # Prioritize bins that leave minimum remaining capacity.\n    # `remaining_capacity = suitable_bins_caps - item`\n    # `best_fit_score = 1.0 / (remaining_capacity + 1e-6)`\n    # Normalize this score by the maximum possible best fit score.\n    remaining_capacity = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_capacity + 1e-6)\n\n    # Metric 2: Bin Fullness Prioritization\n    # Prioritize bins that are already more full. This encourages consolidating items.\n    # Use inverse of remaining capacity as a proxy for fullness.\n    # `fullness_score = 1.0 / (suitable_bins_caps + 1e-6)`\n    # Normalize this score by the maximum possible fullness score.\n    fullness_scores = 1.0 / (suitable_bins_caps + 1e-6)\n\n    # Metric 3: Item Size Ratio\n    # For smaller items, it's often beneficial to place them in bins that are not too empty,\n    # to avoid creating many sparsely filled bins.\n    # For larger items, it's crucial to find a good fit.\n    # Let's consider the ratio of the item size to the bin's remaining capacity.\n    # `item_ratio = item / suitable_bins_caps`. A higher ratio means the item makes a larger dent.\n    # This metric is good for smaller items in moderately full bins.\n    # For very large items, this ratio might be close to 1.\n    item_ratio_scores = item / (suitable_bins_caps + 1e-6)\n\n    # Normalization of components\n    # Normalize Best Fit scores: higher score for tighter fits.\n    if np.max(best_fit_scores) > 1e-9:\n        norm_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        norm_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Fullness scores: higher score for fuller bins.\n    if np.max(fullness_scores) > 1e-9:\n        norm_fullness = fullness_scores / np.max(fullness_scores)\n    else:\n        norm_fullness = np.zeros_like(fullness_scores)\n\n    # Normalize Item Ratio scores: higher score for items filling more of the bin's remaining capacity.\n    if np.max(item_ratio_scores) > 1e-9:\n        norm_item_ratio = item_ratio_scores / np.max(item_ratio_scores)\n    else:\n        norm_item_ratio = np.zeros_like(item_ratio_scores)\n\n    # Adaptive Weighting Strategy:\n    # The importance of each metric can depend on the item's size relative to the bin capacity.\n    # Assume a standard bin capacity (e.g., 1.0 for normalization purposes if item is scaled).\n    # If `item` is large (close to 1.0), 'Best Fit' and 'Fullness' are critical.\n    # If `item` is small, 'Item Ratio' and 'Fullness' are important to avoid waste.\n\n    # Let's define weights based on the item's size relative to the *average* suitable bin capacity.\n    # This provides a context for the item.\n    avg_suitable_cap = np.mean(suitable_bins_caps)\n    if avg_suitable_cap > 1e-9:\n        item_vs_avg_cap_ratio = item / avg_suitable_cap\n    else:\n        item_vs_avg_cap_ratio = 0.5 # Default if no suitable bins or very small capacity\n\n    # Weights adjustment:\n    # If item is large relative to average capacity, emphasize Best Fit and Fullness.\n    # If item is small relative to average capacity, emphasize Item Ratio and Fullness.\n\n    # Base weights, can be tuned.\n    w_bf = 0.4\n    w_f = 0.4\n    w_ir = 0.2\n\n    # Adjust weights dynamically.\n    # Example: if item is large, increase weight for BF and F.\n    if item_vs_avg_cap_ratio > 1.0: # Item is larger than average remaining capacity\n        w_bf += 0.2 * (item_vs_avg_cap_ratio - 1.0) # Boost BF for large items\n        w_f += 0.1 * (item_vs_avg_cap_ratio - 1.0) # Boost F for large items\n        w_ir -= 0.3 * (item_vs_avg_cap_ratio - 1.0) # Decrease IR for large items\n    else: # Item is smaller than average remaining capacity\n        w_ir += 0.3 * (1.0 - item_vs_avg_cap_ratio) # Boost IR for small items\n        w_f += 0.1 * (1.0 - item_vs_avg_cap_ratio) # Boost F for small items\n        w_bf -= 0.2 * (1.0 - item_vs_avg_cap_ratio) # Decrease BF for small items\n\n    # Ensure weights are non-negative and sum to 1 (or close enough, then re-normalize).\n    w_bf = max(0, w_bf)\n    w_f = max(0, w_f)\n    w_ir = max(0, w_ir)\n\n    total_w = w_bf + w_f + w_ir\n    if total_w > 1e-9:\n        w_bf /= total_w\n        w_f /= total_w\n        w_ir /= total_w\n    else: # Fallback to equal weights if something goes wrong\n        w_bf, w_f, w_ir = 1/3, 1/3, 1/3\n\n    # Combine normalized scores with dynamic weights\n    combined_scores = (w_bf * norm_best_fit +\n                       w_f * norm_fullness +\n                       w_ir * norm_item_ratio)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit (tightness) with a 'fair share' exploration bonus,\n    prioritizing bins that are neither too full nor too empty, relative to others.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Best Fit component: Inverse of remaining capacity after placing the item.\n    # Smaller difference implies a tighter fit and higher score.\n    tightness_scores = 1.0 / (valid_bins_remain_cap - item + 1e-9)\n\n    # Exploration component: Penalize bins that are excessively full or empty\n    # relative to the average remaining capacity of suitable bins. This encourages\n    # a more balanced distribution. We use a quadratic penalty.\n    avg_remain_cap = np.mean(valid_bins_remain_cap)\n    # Deviation from average, squared to penalize larger deviations more.\n    # We add a small constant to avoid zero deviation resulting in zero penalty.\n    fairness_penalty = (valid_bins_remain_cap - avg_remain_cap)**2 / (avg_remain_cap + 1e-9)\n\n    # Combine scores: Higher tightness is good, lower penalty (closer to avg) is good.\n    # We subtract the penalty as it's a negative aspect.\n    # Weights can be tuned; here, tightness is prioritized.\n    combined_scores = tightness_scores - fairness_penalty * 0.2 # Tunable parameter for penalty influence\n\n    # Normalize scores to be between 0 and 1.\n    min_score = np.min(combined_scores)\n    max_score = np.max(combined_scores)\n\n    if max_score - min_score > 1e-9:\n        priorities[suitable_bins_mask] = (combined_scores - min_score) / (max_score - min_score)\n    elif np.any(suitable_bins_mask):\n        # If all suitable bins have very similar combined scores, distribute equally.\n        priorities[suitable_bins_mask] = 1.0 / np.sum(suitable_bins_mask)\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1 and 2 (identical): They implement three metrics: Best Fit, Gap Exploitation, and Bin Fill Similarity, with dynamic weighting based on item size relative to max suitable capacity. The weights adapt to prioritize Best Fit for larger items and Gap Exploitation/Fill Similarity for smaller ones. Normalization is applied to each metric before combining.\n\nComparing Heuristics 3 and 4: Heuristic 3 combines Best Fit (tightness) with Exploration (larger initial capacity) using a fixed weighted sum. Heuristic 4 combines Best Fit (negative remaining capacity) with Exploration (normalized remaining capacity) using a weighted sum, favoring Best Fit. Heuristic 3 normalizes component scores via min-max scaling, while Heuristic 4 uses direct combination.\n\nComparing Heuristics 5, 6, 7, 8 (identical): These heuristics also use Best Fit (relative remaining capacity), Bin Fullness (inverse of remaining capacity), and Item Size Ratio. They employ dynamic weighting based on the item's size relative to the average suitable bin capacity. The weights adapt to prioritize Best Fit and Fullness for larger items, and Item Ratio/Fullness for smaller items. Normalization is applied to each metric before weighted combination.\n\nComparing Heuristics 9 and 10, 11, 12 (identical): Heuristic 9 combines Best Fit (tightness) with a fairness penalty (deviation from average remaining capacity). Heuristics 10-12 combine Modified Best Fit (sweet spot residual), Exploration (normalized remaining capacity after placement), and Usage Proxy (normalized current remaining capacity) with fixed weights.\n\nComparing Heuristics 13, 14, 15 (identical): These combine Refined Best Fit (log1p of inverse residual) with Exploration (min-max scaled remaining capacity after placement), using dynamic weights based on item size relative to a threshold.\n\nComparing Heuristics 16, 17, 18, 19, 20 (identical): These combine Best Fit (inverse residual) with Exploration (log1p or log of remaining capacity). They use fixed weights and normalize the combined score. Heuristics 17-18 have slightly different weights (0.55/0.45) than 19-20 (0.55/0.45) and 16 (0.7/0.3) for Best Fit vs. Exploration.\n\nOverall: The best heuristics (1-8) tend to use multiple metrics and adapt their weighting dynamically based on item characteristics or bin states, often involving normalization of individual metrics or the final combined score. Simpler fixed-weight combinations of Best Fit and Exploration (like 16-20) are less sophisticated but might be more robust. The use of logarithmic or Gaussian-like functions for scoring can provide smoother preference curves.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Metric selection, dynamic weighting, normalization, balance, robustness, simplicity.\n*   **Advice:** Focus on a core set of well-defined, interpretable metrics. Systematically explore dynamic weighting and robust normalization techniques. Aim for a balance between primary objectives and secondary goals like diversification or exploration.\n*   **Avoid:** Overly complex or opaque mathematical transformations, unstable calculations, and prioritizing single, overly simplistic metrics.\n*   **Explanation:** Effective self-reflection identifies *how* and *why* certain metric combinations and adjustments lead to improved performance by understanding their impact on the problem's core objectives.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}