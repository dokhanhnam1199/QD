{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    \n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    \n    if suitable_bins_cap.size == 0:\n        return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines best-fit tight packing with an exploration bonus for less utilized bins.\n    Prioritizes bins that leave minimal residual space after packing, while also\n    offering a slight preference for less full bins to encourage exploration.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_cap - item\n\n    # Core \"Best Fit\" score: Higher score for smaller remaining capacity (tighter fit)\n    # Using reciprocal to emphasize very small remaining capacities.\n    tight_fit_score = 1.0 / (remaining_after_placement + 1e-9)\n\n    # Exploration/Diversification component: Bonus for bins that are less full.\n    # This is inspired by epsilon-greedy strategies where we sometimes explore\n    # non-optimal choices to potentially find better overall solutions.\n    # We'll scale this bonus by a small factor to ensure \"best fit\" is primary.\n    \n    # Calculate a measure of how \"full\" each suitable bin is.\n    fill_ratio = (suitable_bins_cap - remaining_after_placement) / (suitable_bins_cap + 1e-9)\n    \n    # A simple exploration bonus based on how much space is left relative to initial capacity.\n    # We want to favor bins with *more* remaining capacity, as an exploration incentive.\n    # The score should be higher for larger remaining capacities.\n    exploration_score = remaining_after_placement / (suitable_bins_cap + 1e-9)\n    \n    # Combine scores: Prioritize tight fits, but add a small bonus for exploration.\n    # The tight_fit_score is the primary driver. The exploration_score is a secondary boost.\n    # A simple weighted sum:\n    # We want to heavily favor tight fits, so the weight for tight_fit_score should be higher.\n    # Let's use weights that reflect this. For example, weight = 1 for tight fit, and a smaller weight for exploration.\n    \n    # The idea from Heuristics 7/8/9/11/12/15/16 which used an epsilon-greedy approach:\n    # They assigned a primary score (like best fit) and then added an exploration bonus.\n    # Let's adapt that. The `tight_fit_score` is our primary score.\n    # For the exploration bonus, we want to favor bins that are NOT the tightest fit.\n    # Heuristics 11/12/15/16 used min-max scaling for the exploration bonus, which is robust.\n    # Let's use a simplified exploration bonus that is higher for bins with larger remaining capacity.\n    \n    # Consider the normalized remaining capacity:\n    # Min-max scaling for exploration bonus:\n    min_rem = np.min(remaining_after_placement)\n    max_rem = np.max(remaining_after_placement)\n    \n    exploration_bonus = np.zeros_like(remaining_after_placement)\n    if max_rem > min_rem: # Avoid division by zero if all remaining capacities are the same\n        exploration_bonus = (remaining_after_placement - min_rem) / (max_rem - min_rem)\n    else:\n        # If all remaining capacities are the same, no exploration bonus based on difference.\n        # Or assign a small constant bonus if needed, but zero is fine for diversity.\n        pass # exploration_bonus remains zeros\n\n    # Combine: Primarily driven by tight fit, with an additive exploration bonus.\n    # The exploration bonus is scaled to be less influential than the tight fit.\n    # A small multiplier for the exploration bonus ensures it acts as a tie-breaker or diversification element.\n    \n    final_priorities = tight_fit_score + 0.1 * exploration_bonus # 0.1 is an arbitrary small weight for exploration\n\n    priorities[suitable_bins_mask] = final_priorities\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Best) vs Heuristic 2 (Worst): Heuristic 1 calculates complex priority scores considering tightness and consolidation, aiming for nuanced bin selection. Heuristic 2 directly implements a Best Fit strategy by setting the priority of the single best-fit bin to 1 and others to 0, which is a deterministic choice rather than a nuanced priority.\n\nComparing Heuristic 1 vs Heuristic 3: Heuristic 1 focuses on minimizing remaining space and encouraging consolidation with a weighted sum. Heuristic 3 also combines \"Best Fit\" with favoring larger bins, but its normalization approach and combination might be less robust than Heuristic 1's focus on tightness.\n\nComparing Heuristic 3 vs Heuristic 4 (identical): Both use \"Best Fit\" and \"favor larger bins\" with similar normalization and weighting.\n\nComparing Heuristic 3 vs Heuristic 5 (identical): Same as above.\n\nComparing Heuristic 5 vs Heuristic 6: Heuristic 5 combines \"Best Fit\" with \"favor larger bins\" using normalization and weights. Heuristic 6 uses \"tight fit\" and \"diversification bonus\" with a simpler inverse scaling for both, and normalizes the final combined scores. Heuristic 6's approach to diversification using inverse remaining capacity is a good addition.\n\nComparing Heuristic 6 vs Heuristic 7 (similar to 1): Heuristic 7 is very similar to Heuristic 1, focusing on tightness and consolidation. Heuristic 6's \"diversification\" metric offers a different angle.\n\nComparing Heuristic 7 vs Heuristic 8 (identical): These are identical to Heuristic 1.\n\nComparing Heuristic 8 vs Heuristic 9: Heuristic 8 (and its duplicates) focuses on tightness and consolidation. Heuristic 9 combines \"Best Fit\" with a sigmoid penalty for large remaining capacities, offering a smoother penalty. The sigmoid approach in Heuristic 9 is an interesting variation for managing large remaining spaces.\n\nComparing Heuristic 9 vs Heuristic 10: Heuristic 9 uses a sigmoid penalty. Heuristic 10 balances \"Best Fit\" with \"emptiness\" using a log transformation, aiming to distribute items more evenly. The log transformation for emptiness in Heuristic 10 is a novel approach for diversification.\n\nComparing Heuristic 10 vs Heuristic 11 (identical): Both balance \"Best Fit\" and \"emptiness\" using log transformation.\n\nComparing Heuristic 11 vs Heuristic 12 (identical): Same as above.\n\nComparing Heuristic 12 vs Heuristic 13 (incomplete): Heuristic 13 is cut off and only initializes variables without logic.\n\nComparing Heuristic 13 vs Heuristic 14 (identical, incomplete): Same as Heuristic 13.\n\nComparing Heuristic 14 vs Heuristic 15: Heuristic 15 combines \"Best Fit\" (tight packing) with an \"exploration bonus\" using normalized remaining capacity. It uses a reciprocal for tight fit and min-max scaling for exploration. This is a more refined version of balancing fit and exploration.\n\nComparing Heuristic 15 vs Heuristic 16: Heuristic 16 also combines \"Best Fit\" and an \"exploration bonus,\" using inverse for best fit and min-max scaling of the bin capacity itself for exploration. Heuristic 15's exploration bonus is based on remaining capacity after placement, while Heuristic 16's is based on the absolute remaining capacity of suitable bins. Heuristic 16's use of `-np.inf` initialization is also a good practice for clarity.\n\nComparing Heuristic 16 vs Heuristic 17 (identical): Same as above.\n\nComparing Heuristic 17 vs Heuristic 18: Heuristic 18 is a more refined version of 16/17, with clearer variable naming and explicit handling of the single suitable bin case for exploration bonus. The normalization for exploration bonus is correctly applied to the capacity itself, not remaining after placement.\n\nComparing Heuristic 18 vs Heuristic 19: Heuristic 18 combines best-fit (inverse difference) with an exploration bonus based on normalized bin capacity. Heuristic 19 uses a weighted sum of best-fit (inverse difference) and an exploration bonus derived from min-max scaling of remaining capacity. Heuristic 19's exploration bonus based on remaining capacity is a better proxy for \"less utilized\" than absolute capacity.\n\nComparing Heuristic 19 vs Heuristic 20 (identical): Both combine best-fit and exploration bonus on remaining capacity with weighted sum.\n\nOverall: The best heuristics (1, 7, 8) focus on minimizing remaining space and encouraging consolidation. The next tier (3, 5, 6, 9, 10, 11, 12, 15, 19, 20) blend \"Best Fit\" with diversification/exploration strategies using various normalization and transformation techniques. Heuristics 2, 13, 14 are either too simplistic or incomplete. Heuristics 16, 17, 18 use a good approach for exploration bonus on bin capacity but might be slightly less nuanced than using remaining capacity after fit.\n- \nHere's a redefined approach to self-reflection for designing better heuristics, focusing on avoiding ineffective practices:\n\n*   **Keywords:** Objective balancing, nuanced scoring, robust metrics, avoiding simplicity.\n*   **Advice:** Focus on how multiple objectives (primary/secondary) interact. Develop score combinations that reflect these interactions, not just additive weighting. Ensure metrics are sensitive to problem specifics.\n*   **Avoid:** Sigmoid/inverse functions *solely* for scaling without justification. Blindly prioritizing \"tight fit\" over exploration. Over-reliance on simplistic, single-metric evaluations.\n*   **Explanation:** The goal is to create heuristics that learn and adapt by understanding trade-offs, not by applying pre-defined, inflexible transformations. Complex interactions between objectives require thoughtful metric design and combination.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}