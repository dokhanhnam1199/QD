{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit (Refined) - Score based on the tightness of the fit.\n    # Using the reciprocal of the remaining capacity after placement.\n    # This naturally prioritizes bins where the remaining space is minimized.\n    # Add a small epsilon to avoid division by zero for perfect fits and to differentiate near-perfect fits.\n    remaining_after_placement = suitable_bins_caps - item\n    # Using 1 / (1 + remaining_capacity) to ensure positive scores and to slightly favor bins with less remaining space.\n    # This is similar to log1p but less sensitive to very small remaining spaces.\n    best_fit_scores = 1.0 / (1.0 + remaining_after_placement)\n\n    # Metric 2: Gap Exploitation - Reward bins that have a significant amount of remaining capacity\n    # but not so much that the item feels \"lost\".\n    # This metric aims to utilize larger \"gaps\" effectively without being overly greedy.\n    # We can score based on the ratio of item size to remaining capacity in the bin.\n    # A higher ratio means the item fills a larger portion of the remaining space, which is good for utilizing larger gaps.\n    # Add a small epsilon to the denominator to avoid division by zero.\n    gap_exploitation_scores = item / (suitable_bins_caps + 1e-6)\n    # Clip scores to avoid excessively high values if item is much larger than bin capacity (though this shouldn't happen with suitable_bins_mask).\n    gap_exploitation_scores = np.clip(gap_exploitation_scores, 0, 2.0) # Cap at 2.0 as a reasonable max ratio.\n\n\n    # Metric 3: Bin Fill Similarity - Aim to make bins have similar fill levels to promote better packing density.\n    # This means preferring bins that are already somewhat full, or bins where adding this item\n    # will bring its fill level closer to other partially filled bins.\n    # A proxy for \"already somewhat full\" is the inverse of remaining capacity relative to a baseline (e.g., max capacity, or average capacity).\n    # Let's normalize remaining capacity by the maximum possible capacity of *any* bin (assuming a global max capacity, e.g., 1.0 for normalized problems).\n    # If a global max is not available, we can use the maximum of all bins' initial capacities.\n    # For this example, let's assume a standard bin capacity of 1.0 as a reference.\n    # The \"fill level\" of a suitable bin would be (1.0 - remaining_capacity) / 1.0.\n    # We want to favor bins with fill levels that are not too close to 0 (very empty) and not too close to 1 (almost full, but not best fit).\n    # A Gaussian-like function centered around a moderate fill level (e.g., 0.6) is suitable.\n    # Let's use the inverse of remaining capacity relative to the item size itself. This measures how \"tight\" the current remaining space is relative to the item.\n    # If the remaining capacity is much larger than the item, the ratio is small, meaning the item is small relative to the space.\n    # If remaining capacity is close to item size, the ratio is near 1, meaning the item is a good fit for the *remaining* space.\n    # We want to favor bins where the item takes up a good fraction of the remaining space.\n    # A higher score for item / remaining_capacity (if remaining_capacity > item) is desired.\n    # This is essentially a variation of best fit, but focuses on the item's proportion of the *remaining* space.\n    # Let's re-think this: Bin Fill Similarity. We want bins to be filled to a similar degree *after* placement.\n    # This implies we want to pick bins that are not too empty and not too full.\n    # Let's consider the \"unused potential\" of a bin. This is the remaining capacity.\n    # We want to pick bins with *moderate* remaining capacity.\n    # Let's normalize the remaining capacity by the item's size. A ratio around 1 is ideal (best fit).\n    # For similarity, we want to avoid extreme remaining capacities.\n    # Consider `remaining_after_placement / item`. We want this to be moderate.\n    # A good heuristic is to penalize bins with very small or very large `remaining_after_placement`.\n    # Let's use `1 - exp(-k * (remaining_after_placement / item))` for values where remaining_after_placement > 0.\n    # A simpler approach: consider the 'fullness' of the bin.\n    # Fullness is approximately (BinCapacity - remaining_capacity) / BinCapacity.\n    # Let's use the current remaining capacity to infer a \"fill state\".\n    # Consider `(item / suitable_bins_caps)` as a measure of how much the item contributes to filling the *current gap*.\n    # A higher value here means the item is a larger portion of the available space, which can be good for utilizing larger gaps.\n    # Let's focus on making bins more \"balanced\".\n    # We can score bins based on how much they \"resemble\" an average fill level.\n    # Average fill level can be estimated by (TotalItemSize / NumberOfBins) / BinCapacity.\n    # For online, this is harder. Let's try to encourage bins to be moderately filled, not too empty, not too full.\n    # Use a score that is high for intermediate remaining capacities.\n    # Let's try a sigmoid-like function on the inverse of remaining capacity.\n    # The \"emptiness\" of the bin is roughly `suitable_bins_caps`.\n    # We want to penalize very small `suitable_bins_caps` (already full) and very large `suitable_bins_caps` (very empty).\n    # Let's use the inverse of remaining capacity scaled by item size.\n    # `suitable_bins_caps / item` ratio. We want this to be moderate.\n    # If `suitable_bins_caps / item` is very small, bin is almost full. If very large, bin is very empty.\n    # Let's use a bell-shaped curve centered around a desired ratio, say 2 (meaning remaining capacity is twice the item size).\n    # This encourages bins that are not too full, not too empty.\n    desired_ratio = 2.0\n    current_ratio = suitable_bins_caps / item\n    # Gaussian-like function: exp(-((ratio - desired_ratio)^2) / variance)\n    # Variance controls the width of the bell curve. A smaller variance means we are pickier.\n    variance = 1.0 # Tunable parameter\n    bin_fill_similarity_scores = np.exp(-((current_ratio - desired_ratio)**2) / (2 * variance))\n\n\n    # Normalize scores to be in a comparable range [0, 1] for combining.\n    # Avoid division by zero if all scores for a metric are zero.\n    max_best_fit = np.max(best_fit_scores)\n    normalized_best_fit = best_fit_scores / max_best_fit if max_best_fit > 1e-6 else np.zeros_like(best_fit_scores)\n\n    max_gap_exploitation = np.max(gap_exploitation_scores)\n    normalized_gap_exploitation = gap_exploitation_scores / max_gap_exploitation if max_gap_exploitation > 1e-6 else np.zeros_like(gap_exploitation_scores)\n\n    max_bin_fill_similarity = np.max(bin_fill_similarity_scores)\n    normalized_bin_fill_similarity = bin_fill_similarity_scores / max_bin_fill_similarity if max_bin_fill_similarity > 1e-6 else np.zeros_like(bin_fill_similarity_scores)\n\n    # Combine scores with dynamic weights.\n    # The weights should adapt based on the item's size relative to the *average* remaining capacity\n    # or the *maximum* remaining capacity among suitable bins.\n    # Let's use the maximum remaining capacity as a reference for \"how open\" the bins are.\n    max_suitable_cap = np.max(suitable_bins_caps)\n\n    # If item is large relative to max suitable capacity, prioritize best fit.\n    # If item is small relative to max suitable capacity, prioritize gap exploitation and fill similarity.\n    # Define a threshold for \"large\" item. Let's use 0.5 * max_suitable_cap.\n    # Normalize item size by max_suitable_cap to get a relative size.\n    relative_item_size = item / (max_suitable_cap + 1e-6)\n\n    # Weighting scheme:\n    # Best Fit is always important, especially for larger items.\n    # Gap Exploitation is good for items that can fill up larger available spaces.\n    # Bin Fill Similarity aims for balanced bins.\n\n    # Weight for Best Fit: increases with item size relative to available space.\n    weight_best_fit = 0.4 + 0.5 * relative_item_size\n    weight_best_fit = np.clip(weight_best_fit, 0.4, 0.9) # Ensure it's not too dominant for small items.\n\n    # Weight for Gap Exploitation: decreases with item size, favors smaller items filling larger gaps.\n    weight_gap_exploitation = 0.4 - 0.3 * relative_item_size\n    weight_gap_exploitation = np.clip(weight_gap_exploitation, 0.1, 0.4)\n\n    # Weight for Bin Fill Similarity: generally useful, moderate weight.\n    weight_bin_fill_similarity = 0.2 # Constant or slightly adjusted\n\n    # Ensure weights sum to 1 (approximately, or re-normalize if needed)\n    # A simpler approach for weights that sum to 1:\n    total_weight = weight_best_fit + weight_gap_exploitation + weight_bin_fill_similarity\n    # If total_weight is 0 (unlikely here), set to 1.\n    if total_weight < 1e-6:\n        total_weight = 1.0\n\n    weight_best_fit /= total_weight\n    weight_gap_exploitation /= total_weight\n    weight_bin_fill_similarity /= total_weight\n\n\n    combined_scores = (weight_best_fit * normalized_best_fit +\n                       weight_gap_exploitation * normalized_gap_exploitation +\n                       weight_bin_fill_similarity * normalized_bin_fill_similarity)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit (Refined) - Score based on the tightness of the fit.\n    # Using the reciprocal of the remaining capacity after placement.\n    # This naturally prioritizes bins where the remaining space is minimized.\n    # Add a small epsilon to avoid division by zero for perfect fits and to differentiate near-perfect fits.\n    remaining_after_placement = suitable_bins_caps - item\n    # Using 1 / (1 + remaining_capacity) to ensure positive scores and to slightly favor bins with less remaining space.\n    # This is similar to log1p but less sensitive to very small remaining spaces.\n    best_fit_scores = 1.0 / (1.0 + remaining_after_placement)\n\n    # Metric 2: Gap Exploitation - Reward bins that have a significant amount of remaining capacity\n    # but not so much that the item feels \"lost\".\n    # This metric aims to utilize larger \"gaps\" effectively without being overly greedy.\n    # We can score based on the ratio of item size to remaining capacity in the bin.\n    # A higher ratio means the item fills a larger portion of the remaining space, which is good for utilizing larger gaps.\n    # Add a small epsilon to the denominator to avoid division by zero.\n    gap_exploitation_scores = item / (suitable_bins_caps + 1e-6)\n    # Clip scores to avoid excessively high values if item is much larger than bin capacity (though this shouldn't happen with suitable_bins_mask).\n    gap_exploitation_scores = np.clip(gap_exploitation_scores, 0, 2.0) # Cap at 2.0 as a reasonable max ratio.\n\n\n    # Metric 3: Bin Fill Similarity - Aim to make bins have similar fill levels to promote better packing density.\n    # This means preferring bins that are already somewhat full, or bins where adding this item\n    # will bring its fill level closer to other partially filled bins.\n    # A proxy for \"already somewhat full\" is the inverse of remaining capacity relative to a baseline (e.g., max capacity, or average capacity).\n    # Let's normalize remaining capacity by the maximum possible capacity of *any* bin (assuming a global max capacity, e.g., 1.0 for normalized problems).\n    # If a global max is not available, we can use the maximum of all bins' initial capacities.\n    # For this example, let's assume a standard bin capacity of 1.0 as a reference.\n    # The \"fill level\" of a suitable bin would be (1.0 - remaining_capacity) / 1.0.\n    # We want to favor bins with fill levels that are not too close to 0 (very empty) and not too close to 1 (almost full, but not best fit).\n    # A Gaussian-like function centered around a moderate fill level (e.g., 0.6) is suitable.\n    # Let's use the inverse of remaining capacity relative to the item size itself. This measures how \"tight\" the current remaining space is relative to the item.\n    # If the remaining capacity is much larger than the item, the ratio is small, meaning the item is small relative to the space.\n    # If remaining capacity is close to item size, the ratio is near 1, meaning the item is a good fit for the *remaining* space.\n    # We want to favor bins where the item takes up a good fraction of the remaining space.\n    # A higher score for item / remaining_capacity (if remaining_capacity > item) is desired.\n    # This is essentially a variation of best fit, but focuses on the item's proportion of the *remaining* space.\n    # Let's re-think this: Bin Fill Similarity. We want bins to be filled to a similar degree *after* placement.\n    # This implies we want to pick bins that are not too empty and not too full.\n    # Let's consider the \"unused potential\" of a bin. This is the remaining capacity.\n    # We want to pick bins with *moderate* remaining capacity.\n    # Let's normalize the remaining capacity by the item's size. A ratio around 1 is ideal (best fit).\n    # For similarity, we want to avoid extreme remaining capacities.\n    # Consider `remaining_after_placement / item`. We want this to be moderate.\n    # A good heuristic is to penalize bins with very small or very large `remaining_after_placement`.\n    # Let's use `1 - exp(-k * (remaining_after_placement / item))` for values where remaining_after_placement > 0.\n    # A simpler approach: consider the 'fullness' of the bin.\n    # Fullness is approximately (BinCapacity - remaining_capacity) / BinCapacity.\n    # Let's use the current remaining capacity to infer a \"fill state\".\n    # Consider `(item / suitable_bins_caps)` as a measure of how much the item contributes to filling the *current gap*.\n    # A higher value here means the item is a larger portion of the available space, which can be good for utilizing larger gaps.\n    # Let's focus on making bins more \"balanced\".\n    # We can score bins based on how much they \"resemble\" an average fill level.\n    # Average fill level can be estimated by (TotalItemSize / NumberOfBins) / BinCapacity.\n    # For online, this is harder. Let's try to encourage bins to be moderately filled, not too empty, not too full.\n    # Use a score that is high for intermediate remaining capacities.\n    # Let's try a sigmoid-like function on the inverse of remaining capacity.\n    # The \"emptiness\" of the bin is roughly `suitable_bins_caps`.\n    # We want to penalize very small `suitable_bins_caps` (already full) and very large `suitable_bins_caps` (very empty).\n    # Let's use the inverse of remaining capacity scaled by item size.\n    # `suitable_bins_caps / item` ratio. We want this to be moderate.\n    # If `suitable_bins_caps / item` is very small, bin is almost full. If very large, bin is very empty.\n    # Let's use a bell-shaped curve centered around a desired ratio, say 2 (meaning remaining capacity is twice the item size).\n    # This encourages bins that are not too full, not too empty.\n    desired_ratio = 2.0\n    current_ratio = suitable_bins_caps / item\n    # Gaussian-like function: exp(-((ratio - desired_ratio)^2) / variance)\n    # Variance controls the width of the bell curve. A smaller variance means we are pickier.\n    variance = 1.0 # Tunable parameter\n    bin_fill_similarity_scores = np.exp(-((current_ratio - desired_ratio)**2) / (2 * variance))\n\n\n    # Normalize scores to be in a comparable range [0, 1] for combining.\n    # Avoid division by zero if all scores for a metric are zero.\n    max_best_fit = np.max(best_fit_scores)\n    normalized_best_fit = best_fit_scores / max_best_fit if max_best_fit > 1e-6 else np.zeros_like(best_fit_scores)\n\n    max_gap_exploitation = np.max(gap_exploitation_scores)\n    normalized_gap_exploitation = gap_exploitation_scores / max_gap_exploitation if max_gap_exploitation > 1e-6 else np.zeros_like(gap_exploitation_scores)\n\n    max_bin_fill_similarity = np.max(bin_fill_similarity_scores)\n    normalized_bin_fill_similarity = bin_fill_similarity_scores / max_bin_fill_similarity if max_bin_fill_similarity > 1e-6 else np.zeros_like(bin_fill_similarity_scores)\n\n    # Combine scores with dynamic weights.\n    # The weights should adapt based on the item's size relative to the *average* remaining capacity\n    # or the *maximum* remaining capacity among suitable bins.\n    # Let's use the maximum remaining capacity as a reference for \"how open\" the bins are.\n    max_suitable_cap = np.max(suitable_bins_caps)\n\n    # If item is large relative to max suitable capacity, prioritize best fit.\n    # If item is small relative to max suitable capacity, prioritize gap exploitation and fill similarity.\n    # Define a threshold for \"large\" item. Let's use 0.5 * max_suitable_cap.\n    # Normalize item size by max_suitable_cap to get a relative size.\n    relative_item_size = item / (max_suitable_cap + 1e-6)\n\n    # Weighting scheme:\n    # Best Fit is always important, especially for larger items.\n    # Gap Exploitation is good for items that can fill up larger available spaces.\n    # Bin Fill Similarity aims for balanced bins.\n\n    # Weight for Best Fit: increases with item size relative to available space.\n    weight_best_fit = 0.4 + 0.5 * relative_item_size\n    weight_best_fit = np.clip(weight_best_fit, 0.4, 0.9) # Ensure it's not too dominant for small items.\n\n    # Weight for Gap Exploitation: decreases with item size, favors smaller items filling larger gaps.\n    weight_gap_exploitation = 0.4 - 0.3 * relative_item_size\n    weight_gap_exploitation = np.clip(weight_gap_exploitation, 0.1, 0.4)\n\n    # Weight for Bin Fill Similarity: generally useful, moderate weight.\n    weight_bin_fill_similarity = 0.2 # Constant or slightly adjusted\n\n    # Ensure weights sum to 1 (approximately, or re-normalize if needed)\n    # A simpler approach for weights that sum to 1:\n    total_weight = weight_best_fit + weight_gap_exploitation + weight_bin_fill_similarity\n    # If total_weight is 0 (unlikely here), set to 1.\n    if total_weight < 1e-6:\n        total_weight = 1.0\n\n    weight_best_fit /= total_weight\n    weight_gap_exploitation /= total_weight\n    weight_bin_fill_similarity /= total_weight\n\n\n    combined_scores = (weight_best_fit * normalized_best_fit +\n                       weight_gap_exploitation * normalized_gap_exploitation +\n                       weight_bin_fill_similarity * normalized_bin_fill_similarity)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a normalized bonus for larger remaining capacity,\n    using a weighted sum for balanced decision-making. Favors bins that are\n    almost full but can still accommodate the item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    # Metric 1: Best Fit - prioritize bins with minimal remaining capacity after placement.\n    # Higher score for smaller residual space.\n    remaining_after_placement = suitable_bins_caps - item\n    # Inverse relationship: smaller residual -> higher score. Add epsilon for stability.\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-9)\n    \n    # Metric 2: Exploration/Larger Bin Preference - favor bins with more capacity initially.\n    # This encourages not always picking the absolute tightest, promoting diversification.\n    # We'll use a logarithmic scale for remaining capacity to de-emphasize very large bins\n    # and focus on bins that are \"reasonably\" large but not excessively so.\n    # log1p is used to handle cases where remaining capacity is 0 after placement,\n    # and to provide a smoother scaling than a simple linear approach.\n    exploration_scores = np.log1p(suitable_bins_caps)\n    \n    # Normalize Best Fit scores (min-max scaling)\n    if suitable_bins_caps.size > 1:\n        min_bf = np.min(best_fit_scores)\n        max_bf = np.max(best_fit_scores)\n        range_bf = max_bf - min_bf\n        if range_bf > 1e-9:\n            normalized_best_fit = (best_fit_scores - min_bf) / range_bf\n        else:\n            normalized_best_fit = np.ones_like(best_fit_scores) # All suitable bins offer same tightness score\n    elif suitable_bins_caps.size == 1:\n        normalized_best_fit = np.array([1.0])\n    else:\n        normalized_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Exploration scores (min-max scaling)\n    if suitable_bins_caps.size > 1:\n        min_exp = np.min(exploration_scores)\n        max_exp = np.max(exploration_scores)\n        range_exp = max_exp - min_exp\n        if range_exp > 1e-9:\n            normalized_exploration = (exploration_scores - min_exp) / range_exp\n        else:\n            normalized_exploration = np.zeros_like(exploration_scores) # All suitable bins have same initial capacity\n    elif suitable_bins_caps.size == 1:\n        normalized_exploration = np.array([1.0]) # If only one bin, it's maximally \"exploratory\" in this context\n    else:\n        normalized_exploration = np.zeros_like(exploration_scores)\n\n    # Combine normalized scores using a weighted sum.\n    # We give a slightly higher weight to Best Fit, as tight packing is crucial for BPP.\n    # The exploration bonus helps to prevent premature fragmentation.\n    combined_scores = 0.7 * normalized_best_fit + 0.3 * normalized_exploration\n    \n    priorities[suitable_bins_mask] = combined_scores\n    \n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit's tightness with an exploration bonus favoring less utilized bins,\n    using a balanced approach to combine these factors.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return np.zeros_like(bins_remain_cap)\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Best Fit Component: Prioritize bins with minimal remaining capacity after placement.\n    # Negative of remaining capacity to favor smaller (more negative) values for minimization.\n    best_fit_scores = -(suitable_bins_remain_cap - item)\n\n    # Exploration Component: Favor bins that are less utilized (larger original capacity).\n    # Normalize remaining capacities of suitable bins using min-max scaling.\n    min_cap = np.min(suitable_bins_remain_cap)\n    max_cap = np.max(suitable_bins_remain_cap)\n    if max_cap - min_cap > 1e-9:\n        exploration_scores = (suitable_bins_remain_cap - min_cap) / (max_cap - min_cap)\n    else:\n        exploration_scores = np.zeros_like(suitable_bins_remain_cap)\n\n    # Combined Score: Balance Best Fit and Exploration.\n    # A weighted sum is used. We give a slightly higher weight to Best Fit (tightness)\n    # as it's generally a primary goal in BPP, while exploration acts as a tie-breaker\n    # or secondary optimization.\n    # We add exploration_scores to best_fit_scores. Higher values (closer to zero for BF) are better.\n    # Exploration scores are positive and higher is better.\n    combined_scores = best_fit_scores + 0.7 * exploration_scores # Weight for exploration\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    # If all suitable bins are identical in terms of combined score, argmin will pick the first.\n    # This heuristic aims to find a good balance, leaning towards tight fits but\n    # considering bin utilization as a secondary factor.\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    num_suitable_bins = suitable_bins_caps.shape[0]\n\n    # Metric 1: Best Fit (revisited)\n    # Focus on the *relative* remaining capacity after placement.\n    # Smaller relative remaining capacity is better (tighter fit).\n    # Use inverse of (1 + relative remaining capacity) to give higher scores to tighter fits.\n    # Added a small constant to the denominator to avoid division by zero and to differentiate very tight fits.\n    remaining_after_placement = suitable_bins_caps - item\n    relative_remaining = remaining_after_placement / suitable_bins_caps # How much capacity is left relative to the bin's current state\n    best_fit_scores = 1.0 / (1.0 + relative_remaining + 1e-6) # Higher score for smaller relative_remaining\n\n    # Metric 2: Fill Level Favorability\n    # Favor bins that are neither too empty nor too full. This promotes better space utilization.\n    # We can define a \"target fill level\". Let's assume a target fill level that is high but leaves some room, e.g., 80-90%.\n    # We can use a Gaussian-like function centered around a desired fill ratio.\n    # Let's assume bin capacity is implicitly normalized to 1.0 for this metric, and item size is also normalized.\n    # The actual capacity of the bin might be more relevant. Let's use remaining capacity relative to a hypothetical maximum bin capacity (e.g., 1.0 for normalized context, or inferable).\n    # For simplicity and adaptability, let's consider the ratio of *item size* to the *bin's remaining capacity*.\n    # A higher ratio means the item makes a bigger dent in the remaining capacity.\n    # We want to avoid bins that are almost empty, as a small item would leave them very inefficiently filled.\n    # A bin that is mostly full and can still accommodate the item is also less desirable for exploration.\n    # Let's score based on how much capacity is *already used* (1 - normalized remaining capacity).\n    # This encourages using bins that have some items already.\n    # Max capacity of a bin isn't explicitly given, assume it's large enough to hold any item.\n    # Let's use the item size relative to the *current remaining capacity* of the bin.\n    # If item/suitable_bins_caps is high, it means the item is a large fraction of what's left -> good for utilization.\n    # If item/suitable_bins_caps is low, it means the item is small compared to what's left -> might leave it too empty.\n    # We want to reward bins where the item fills a significant portion of the *remaining* space.\n    fill_score_component = item / (suitable_bins_caps + 1e-6)\n    # We also want to reward bins that are not excessively empty. Let's use the inverse of normalized remaining capacity.\n    # Normalized remaining capacity: suitable_bins_caps / MAX_BIN_CAPACITY (assume MAX_BIN_CAPACITY=1 for normalized context)\n    # So, 1 - suitable_bins_caps is a measure of \"fill level\". We want to prioritize bins with moderate fill levels.\n    # Let's combine these: prefer bins where the item fits snugly, and the bin isn't too empty.\n    # A bin is \"good\" if `suitable_bins_caps` is not too large, and `item` is not too small relative to it.\n    # Let's define a \"gap score\" based on `suitable_bins_caps - item`. Small gaps are good.\n    gap_scores = suitable_bins_caps - item\n    # We want to penalize bins that are too empty, meaning `suitable_bins_caps` is large.\n    # Let's consider the inverse of the *absolute* remaining capacity. Larger remaining capacity = lower score.\n    # And also the inverse of `gap_scores`. Smaller gaps = higher score.\n    # Combine `1 / (gap_scores + epsilon)` with `1 / (suitable_bins_caps + epsilon)`.\n    # This is effectively prioritizing bins with small remaining capacity after placement AND small total capacity.\n    # This seems too restrictive. Let's rethink.\n\n    # Metric 2 (Revised): Balanced Fill\n    # Aim for bins that are \"moderately full\" but also accommodate the item well.\n    # Consider the ratio of item size to bin capacity.\n    # `item / bin_capacity`. This is not directly available.\n    # Let's use `item / suitable_bins_caps` as a proxy for how much of the *current remaining space* the item occupies.\n    # High values are good (item fills a lot). Low values are bad (item is small relative to space).\n    # Also, consider the *absolute* remaining capacity after placement: `suitable_bins_caps - item`.\n    # Small absolute remaining capacity is good (tight fit). Large absolute remaining capacity is bad.\n    # Let's combine these: higher score if `item / suitable_bins_caps` is high AND `suitable_bins_caps - item` is low.\n    # This is somewhat redundant with Best Fit.\n\n    # Let's introduce a metric for \"bin efficiency\" or \"fairness\".\n    # Metric 2 (New): Bin Efficiency Prioritization\n    # Prioritize bins that have capacity close to the item size, but slightly larger.\n    # This helps consolidate items without leaving excessive \"slack\".\n    # We are looking for `suitable_bins_caps` that are \"just enough\".\n    # This can be modeled as `1 / (suitable_bins_caps - item + epsilon)` BUT we also want to avoid very large capacities.\n    # Let's use a metric that is high when `suitable_bins_caps` is close to `item`.\n    # We can use a Gaussian-like function centered around `item`.\n    # The \"variance\" of this Gaussian could adapt to the item size. Larger items might tolerate larger gaps.\n    # Let's keep it simpler: reward bins where `suitable_bins_caps` is \"just above\" `item`.\n    # This means `suitable_bins_caps - item` should be small and positive.\n    # `1 / (suitable_bins_caps - item + epsilon)` is similar to best-fit.\n\n    # Let's try a simpler, more direct approach for diversification and better space utilization.\n    # Metric 2: Space Utilization Fairness\n    # We want to use bins that have a reasonable amount of space left but are not excessively empty.\n    # A bin that is almost full and can fit the item is good.\n    # A bin that is almost empty and can fit the item is less good (leaves a lot of slack).\n    # Let's focus on the \"fill state\" of the bin *before* placing the item.\n    # We need to infer the \"original\" capacity or the \"current fill\".\n    # If we assume a fixed bin capacity (e.g., 1.0 for normalization), then `1 - suitable_bins_caps` is the \"fill ratio\".\n    # We want to avoid bins that are very empty (fill ratio close to 0).\n    # Let's define a \"fill penalty\": higher penalty for very empty bins.\n    # `fill_penalty = max(0, 0.5 - (1 - suitable_bins_caps))` -> penalize bins where remaining capacity > 0.5\n    # This is inverted. We want to *reward* bins that are not too empty.\n    # Let's consider `1 - suitable_bins_caps` as \"current fill\".\n    # We can use a function that peaks at a moderate fill level, e.g., 0.7-0.9.\n    # A simple sigmoid-like function can achieve this.\n    # `fill_score = 1 / (1 + exp(-(fill_ratio - target_fill) / steepness))`\n    # Let's use a simpler approach: reward bins whose remaining capacity is not too large.\n    # Inverse of remaining capacity: `1 / (suitable_bins_caps + epsilon)`.\n    # This prioritizes bins that are more full.\n    # However, this is counter to \"exploration\" or giving smaller items space.\n\n    # Let's try to balance \"tight fit\" with \"not too empty\".\n    # Metric 2: Optimized Capacity Usage\n    # Prioritize bins where the remaining capacity is just enough for the item,\n    # AND the bin wasn't excessively empty to begin with.\n    # Consider `suitable_bins_caps` relative to the item size.\n    # We want `suitable_bins_caps` to be slightly larger than `item`.\n    # Let's score based on the inverse of `suitable_bins_caps` (favoring fuller bins) and also penalize very large `suitable_bins_caps`.\n    # Consider the ratio: `item / suitable_bins_caps`. High is good.\n    # And the gap: `suitable_bins_caps - item`. Low is good.\n    # Let's combine them: `(item / suitable_bins_caps) * (1 / (suitable_bins_caps - item + epsilon))`\n    # This might become unstable if `suitable_bins_caps` is very close to `item`.\n\n    # Rethinking the goal: dynamic adaptation and avoiding suboptimal choices early on.\n    # Instead of multiple, possibly conflicting metrics with complex weighting, let's try a more robust, adaptive metric.\n    # A common issue is creating many nearly empty bins or very full bins.\n    # We want to find a balance.\n    # Let's consider the \"waste\" created by placing the item.\n    # Waste = `suitable_bins_caps - item`. Lower waste is better (Best Fit).\n    # However, if a bin has very little capacity remaining overall, even a small waste might be significant.\n    # Let's consider the \"quality\" of the bin itself.\n    # Metric 2: Quality-Aware Best Fit\n    # Prioritize bins that offer a good fit, but also consider the overall \"quality\" of the bin.\n    # A \"quality\" bin might be one that is not too empty, nor too full,\n    # and has already accommodated a few items (though we don't have this info directly).\n    # Let's proxy \"quality\" by the *inverse* of the remaining capacity.\n    # A bin with less remaining capacity is \"more full\" and might be considered of higher quality in terms of utilization.\n    # So, we want to reward:\n    # 1. Small gap (`suitable_bins_caps - item` is small).\n    # 2. High initial fill (small `suitable_bins_caps`).\n    # This can be combined as: `1 / (suitable_bins_caps - item + epsilon) * (1 / (suitable_bins_caps + epsilon))`.\n    # This effectively prioritizes bins that have *just enough* capacity and are already quite full.\n\n    # Let's try to be more nuanced. What if the item is very small?\n    # A very small item should preferably go into a bin that has a moderate amount of remaining capacity,\n    # to avoid making it too empty.\n    # What if the item is very large?\n    # A very large item should ideally go into a bin that has a capacity *just* over the item size.\n    # This suggests a weight that adapts not just to the item size, but also to the distribution of `suitable_bins_caps`.\n\n    # Metric 2: Adaptive Fit Quality\n    # We want to favor bins where `suitable_bins_caps` is slightly larger than `item`.\n    # Let's consider the \"excess capacity\" `excess = suitable_bins_caps - item`.\n    # We want `excess` to be small.\n    # Additionally, we want to avoid using bins that are already extremely full or extremely empty.\n    # Let's define a \"bin desirability\" based on its remaining capacity `suitable_bins_caps`.\n    # High desirability for bins with moderate remaining capacity (e.g., 50% to 80% full).\n    # Let's simplify and focus on the \"slack\" created.\n    # `slack = suitable_bins_caps - item`.\n    # We want `slack` to be small.\n    # However, if `suitable_bins_caps` is already very small, a small `slack` might be undesirable.\n    # Let's try a score that combines the inverse of `suitable_bins_caps` (favoring fuller bins)\n    # and penalizes large slack.\n    # Consider `score = (1 - suitable_bins_caps) / (suitable_bins_caps - item + epsilon)`\n    # This would be high if the bin is quite full AND the slack is small.\n    # It might be unstable if `suitable_bins_caps` is small.\n\n    # Let's introduce a penalty for bins that are *too* empty relative to the item.\n    # If `suitable_bins_caps` is much larger than `item`, it might lead to poor packing.\n    # A metric for \"over-capacity\": `max(0, suitable_bins_caps - item - target_slack)`\n    # where `target_slack` could be a small constant or a fraction of `item`.\n    # Let's aim for a specific target remaining capacity after placement, say `target_rem`.\n    # `target_rem` could be related to the item size, e.g., `0.1 * item` or `0.2 * BIN_CAPACITY`.\n    # For simplicity, let's make `target_rem` a small constant like 0.1.\n    # Then, we penalize bins where `suitable_bins_caps - item` is much larger than `target_rem`.\n    # `penalty = max(0, (suitable_bins_caps - item) - target_rem)`\n    # The score would be `1 / (penalty + 1)`.\n\n    # Metric 2: Balanced Slack Minimization\n    # Prioritize bins that minimize slack, but also penalize placing items in very empty bins\n    # where the item occupies a small fraction of the remaining capacity.\n    # Let `slack = suitable_bins_caps - item`. We want small slack.\n    # Consider the ratio `item / suitable_bins_caps`. We want this to be reasonably high.\n    # A combination could be: `(1 / (slack + epsilon)) * (item / (suitable_bins_caps + epsilon))`\n    # This score is high when slack is small AND item occupies a good portion of bin's remaining capacity.\n\n    # Let's try to integrate the \"exploration\" idea more subtly.\n    # Exploration might mean not always picking the absolute \"best\" fit if it means creating a very specialized bin.\n    # We want to keep options open.\n    # This might mean slightly favoring bins that are neither too full nor too empty.\n\n    # Metric 2 (Re-Revisited): Dynamic Target Fit\n    # The \"ideal\" remaining capacity after placement might depend on the item size and the overall bin fullness.\n    # If a bin is very full, we want the item to fit snugly (small residual capacity).\n    # If a bin is quite empty, we might want the item to fill a larger portion of it.\n    # Let's try a score that rewards bins where `suitable_bins_caps - item` is minimized,\n    # but with a twist: the \"penalty\" for slack depends on how \"full\" the bin is.\n    # Let's use the inverse of `suitable_bins_caps` to represent \"fullness\".\n    # `fullness_score = 1 / (suitable_bins_caps + epsilon)`.\n    # We want to penalize slack `suitable_bins_caps - item`.\n    # Combine: `score = fullness_score / (suitable_bins_caps - item + epsilon)`\n    # This would prioritize bins that are full and have small slack.\n\n    # Let's go with a simpler, more interpretable combination that balances tight fits and bin usage.\n    # Metric 2: Fill State Aware Best Fit\n    # We want a tight fit (Best Fit) and we want to avoid using bins that are too empty.\n    # `best_fit_contribution = 1.0 / (suitable_bins_caps - item + 1e-6)`\n    # How to penalize bins that are too empty?\n    # We can use `1 - suitable_bins_caps` as a proxy for \"fill level\".\n    # We want to avoid bins where `suitable_bins_caps` is large.\n    # So, we want to reward bins with low `suitable_bins_caps`.\n    # Let's try `fill_awareness = 1.0 / (suitable_bins_caps + 1e-6)`.\n    # This rewards fuller bins.\n    # Combining them: `score = best_fit_contribution * fill_awareness`\n    # `score = (1.0 / (suitable_bins_caps - item + 1e-6)) * (1.0 / (suitable_bins_caps + 1e-6))`\n    # This prioritizes bins that are both nearly full and have just enough capacity for the item.\n\n    # Let's normalize the components to ensure comparability.\n    # Metric 1: Best Fit (again, but more robust)\n    # Prioritize bins that leave minimum remaining capacity.\n    # `remaining_capacity = suitable_bins_caps - item`\n    # `best_fit_score = 1.0 / (remaining_capacity + 1e-6)`\n    # Normalize this score by the maximum possible best fit score.\n    remaining_capacity = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_capacity + 1e-6)\n\n    # Metric 2: Bin Fullness Prioritization\n    # Prioritize bins that are already more full. This encourages consolidating items.\n    # Use inverse of remaining capacity as a proxy for fullness.\n    # `fullness_score = 1.0 / (suitable_bins_caps + 1e-6)`\n    # Normalize this score by the maximum possible fullness score.\n    fullness_scores = 1.0 / (suitable_bins_caps + 1e-6)\n\n    # Metric 3: Item Size Ratio\n    # For smaller items, it's often beneficial to place them in bins that are not too empty,\n    # to avoid creating many sparsely filled bins.\n    # For larger items, it's crucial to find a good fit.\n    # Let's consider the ratio of the item size to the bin's remaining capacity.\n    # `item_ratio = item / suitable_bins_caps`. A higher ratio means the item makes a larger dent.\n    # This metric is good for smaller items in moderately full bins.\n    # For very large items, this ratio might be close to 1.\n    item_ratio_scores = item / (suitable_bins_caps + 1e-6)\n\n    # Normalization of components\n    # Normalize Best Fit scores: higher score for tighter fits.\n    if np.max(best_fit_scores) > 1e-9:\n        norm_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        norm_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Fullness scores: higher score for fuller bins.\n    if np.max(fullness_scores) > 1e-9:\n        norm_fullness = fullness_scores / np.max(fullness_scores)\n    else:\n        norm_fullness = np.zeros_like(fullness_scores)\n\n    # Normalize Item Ratio scores: higher score for items filling more of the bin's remaining capacity.\n    if np.max(item_ratio_scores) > 1e-9:\n        norm_item_ratio = item_ratio_scores / np.max(item_ratio_scores)\n    else:\n        norm_item_ratio = np.zeros_like(item_ratio_scores)\n\n    # Adaptive Weighting Strategy:\n    # The importance of each metric can depend on the item's size relative to the bin capacity.\n    # Assume a standard bin capacity (e.g., 1.0 for normalization purposes if item is scaled).\n    # If `item` is large (close to 1.0), 'Best Fit' and 'Fullness' are critical.\n    # If `item` is small, 'Item Ratio' and 'Fullness' are important to avoid waste.\n\n    # Let's define weights based on the item's size relative to the *average* suitable bin capacity.\n    # This provides a context for the item.\n    avg_suitable_cap = np.mean(suitable_bins_caps)\n    if avg_suitable_cap > 1e-9:\n        item_vs_avg_cap_ratio = item / avg_suitable_cap\n    else:\n        item_vs_avg_cap_ratio = 0.5 # Default if no suitable bins or very small capacity\n\n    # Weights adjustment:\n    # If item is large relative to average capacity, emphasize Best Fit and Fullness.\n    # If item is small relative to average capacity, emphasize Item Ratio and Fullness.\n\n    # Base weights, can be tuned.\n    w_bf = 0.4\n    w_f = 0.4\n    w_ir = 0.2\n\n    # Adjust weights dynamically.\n    # Example: if item is large, increase weight for BF and F.\n    if item_vs_avg_cap_ratio > 1.0: # Item is larger than average remaining capacity\n        w_bf += 0.2 * (item_vs_avg_cap_ratio - 1.0) # Boost BF for large items\n        w_f += 0.1 * (item_vs_avg_cap_ratio - 1.0) # Boost F for large items\n        w_ir -= 0.3 * (item_vs_avg_cap_ratio - 1.0) # Decrease IR for large items\n    else: # Item is smaller than average remaining capacity\n        w_ir += 0.3 * (1.0 - item_vs_avg_cap_ratio) # Boost IR for small items\n        w_f += 0.1 * (1.0 - item_vs_avg_cap_ratio) # Boost F for small items\n        w_bf -= 0.2 * (1.0 - item_vs_avg_cap_ratio) # Decrease BF for small items\n\n    # Ensure weights are non-negative and sum to 1 (or close enough, then re-normalize).\n    w_bf = max(0, w_bf)\n    w_f = max(0, w_f)\n    w_ir = max(0, w_ir)\n\n    total_w = w_bf + w_f + w_ir\n    if total_w > 1e-9:\n        w_bf /= total_w\n        w_f /= total_w\n        w_ir /= total_w\n    else: # Fallback to equal weights if something goes wrong\n        w_bf, w_f, w_ir = 1/3, 1/3, 1/3\n\n    # Combine normalized scores with dynamic weights\n    combined_scores = (w_bf * norm_best_fit +\n                       w_f * norm_fullness +\n                       w_ir * norm_item_ratio)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    num_suitable_bins = suitable_bins_caps.shape[0]\n\n    # Metric 1: Best Fit (revisited)\n    # Focus on the *relative* remaining capacity after placement.\n    # Smaller relative remaining capacity is better (tighter fit).\n    # Use inverse of (1 + relative remaining capacity) to give higher scores to tighter fits.\n    # Added a small constant to the denominator to avoid division by zero and to differentiate very tight fits.\n    remaining_after_placement = suitable_bins_caps - item\n    relative_remaining = remaining_after_placement / suitable_bins_caps # How much capacity is left relative to the bin's current state\n    best_fit_scores = 1.0 / (1.0 + relative_remaining + 1e-6) # Higher score for smaller relative_remaining\n\n    # Metric 2: Fill Level Favorability\n    # Favor bins that are neither too empty nor too full. This promotes better space utilization.\n    # We can define a \"target fill level\". Let's assume a target fill level that is high but leaves some room, e.g., 80-90%.\n    # We can use a Gaussian-like function centered around a desired fill ratio.\n    # Let's assume bin capacity is implicitly normalized to 1.0 for this metric, and item size is also normalized.\n    # The actual capacity of the bin might be more relevant. Let's use remaining capacity relative to a hypothetical maximum bin capacity (e.g., 1.0 for normalized context, or inferable).\n    # For simplicity and adaptability, let's consider the ratio of *item size* to the *bin's remaining capacity*.\n    # A higher ratio means the item makes a bigger dent in the remaining capacity.\n    # We want to avoid bins that are almost empty, as a small item would leave them very inefficiently filled.\n    # A bin that is mostly full and can still accommodate the item is also less desirable for exploration.\n    # Let's score based on how much capacity is *already used* (1 - normalized remaining capacity).\n    # This encourages using bins that have some items already.\n    # Max capacity of a bin isn't explicitly given, assume it's large enough to hold any item.\n    # Let's use the item size relative to the *current remaining capacity* of the bin.\n    # If item/suitable_bins_caps is high, it means the item is a large fraction of what's left -> good for utilization.\n    # If item/suitable_bins_caps is low, it means the item is small compared to what's left -> might leave it too empty.\n    # We want to reward bins where the item fills a significant portion of the *remaining* space.\n    fill_score_component = item / (suitable_bins_caps + 1e-6)\n    # We also want to reward bins that are not excessively empty. Let's use the inverse of normalized remaining capacity.\n    # Normalized remaining capacity: suitable_bins_caps / MAX_BIN_CAPACITY (assume MAX_BIN_CAPACITY=1 for normalized context)\n    # So, 1 - suitable_bins_caps is a measure of \"fill level\". We want to prioritize bins with moderate fill levels.\n    # Let's combine these: prefer bins where the item fits snugly, and the bin isn't too empty.\n    # A bin is \"good\" if `suitable_bins_caps` is not too large, and `item` is not too small relative to it.\n    # Let's define a \"gap score\" based on `suitable_bins_caps - item`. Small gaps are good.\n    gap_scores = suitable_bins_caps - item\n    # We want to penalize bins that are too empty, meaning `suitable_bins_caps` is large.\n    # Let's consider the inverse of the *absolute* remaining capacity. Larger remaining capacity = lower score.\n    # And also the inverse of `gap_scores`. Smaller gaps = higher score.\n    # Combine `1 / (gap_scores + epsilon)` with `1 / (suitable_bins_caps + epsilon)`.\n    # This is effectively prioritizing bins with small remaining capacity after placement AND small total capacity.\n    # This seems too restrictive. Let's rethink.\n\n    # Metric 2 (Revised): Balanced Fill\n    # Aim for bins that are \"moderately full\" but also accommodate the item well.\n    # Consider the ratio of item size to bin capacity.\n    # `item / bin_capacity`. This is not directly available.\n    # Let's use `item / suitable_bins_caps` as a proxy for how much of the *current remaining space* the item occupies.\n    # High values are good (item fills a lot). Low values are bad (item is small relative to space).\n    # Also, consider the *absolute* remaining capacity after placement: `suitable_bins_caps - item`.\n    # Small absolute remaining capacity is good (tight fit). Large absolute remaining capacity is bad.\n    # Let's combine these: higher score if `item / suitable_bins_caps` is high AND `suitable_bins_caps - item` is low.\n    # This is somewhat redundant with Best Fit.\n\n    # Let's introduce a metric for \"bin efficiency\" or \"fairness\".\n    # Metric 2 (New): Bin Efficiency Prioritization\n    # Prioritize bins that have capacity close to the item size, but slightly larger.\n    # This helps consolidate items without leaving excessive \"slack\".\n    # We are looking for `suitable_bins_caps` that are \"just enough\".\n    # This can be modeled as `1 / (suitable_bins_caps - item + epsilon)` BUT we also want to avoid very large capacities.\n    # Let's use a metric that is high when `suitable_bins_caps` is close to `item`.\n    # We can use a Gaussian-like function centered around `item`.\n    # The \"variance\" of this Gaussian could adapt to the item size. Larger items might tolerate larger gaps.\n    # Let's keep it simpler: reward bins where `suitable_bins_caps` is \"just above\" `item`.\n    # This means `suitable_bins_caps - item` should be small and positive.\n    # `1 / (suitable_bins_caps - item + epsilon)` is similar to best-fit.\n\n    # Let's try a simpler, more direct approach for diversification and better space utilization.\n    # Metric 2: Space Utilization Fairness\n    # We want to use bins that have a reasonable amount of space left but are not excessively empty.\n    # A bin that is almost full and can fit the item is good.\n    # A bin that is almost empty and can fit the item is less good (leaves a lot of slack).\n    # Let's focus on the \"fill state\" of the bin *before* placing the item.\n    # We need to infer the \"original\" capacity or the \"current fill\".\n    # If we assume a fixed bin capacity (e.g., 1.0 for normalization), then `1 - suitable_bins_caps` is the \"fill ratio\".\n    # We want to avoid bins that are very empty (fill ratio close to 0).\n    # Let's define a \"fill penalty\": higher penalty for very empty bins.\n    # `fill_penalty = max(0, 0.5 - (1 - suitable_bins_caps))` -> penalize bins where remaining capacity > 0.5\n    # This is inverted. We want to *reward* bins that are not too empty.\n    # Let's consider `1 - suitable_bins_caps` as \"current fill\".\n    # We can use a function that peaks at a moderate fill level, e.g., 0.7-0.9.\n    # A simple sigmoid-like function can achieve this.\n    # `fill_score = 1 / (1 + exp(-(fill_ratio - target_fill) / steepness))`\n    # Let's use a simpler approach: reward bins whose remaining capacity is not too large.\n    # Inverse of remaining capacity: `1 / (suitable_bins_caps + epsilon)`.\n    # This prioritizes bins that are more full.\n    # However, this is counter to \"exploration\" or giving smaller items space.\n\n    # Let's try to balance \"tight fit\" with \"not too empty\".\n    # Metric 2: Optimized Capacity Usage\n    # Prioritize bins where the remaining capacity is just enough for the item,\n    # AND the bin wasn't excessively empty to begin with.\n    # Consider `suitable_bins_caps` relative to the item size.\n    # We want `suitable_bins_caps` to be slightly larger than `item`.\n    # Let's score based on the inverse of `suitable_bins_caps` (favoring fuller bins) and also penalize very large `suitable_bins_caps`.\n    # Consider the ratio: `item / suitable_bins_caps`. High is good.\n    # And the gap: `suitable_bins_caps - item`. Low is good.\n    # Let's combine them: `(item / suitable_bins_caps) * (1 / (suitable_bins_caps - item + epsilon))`\n    # This might become unstable if `suitable_bins_caps` is very close to `item`.\n\n    # Rethinking the goal: dynamic adaptation and avoiding suboptimal choices early on.\n    # Instead of multiple, possibly conflicting metrics with complex weighting, let's try a more robust, adaptive metric.\n    # A common issue is creating many nearly empty bins or very full bins.\n    # We want to find a balance.\n    # Let's consider the \"waste\" created by placing the item.\n    # Waste = `suitable_bins_caps - item`. Lower waste is better (Best Fit).\n    # However, if a bin has very little capacity remaining overall, even a small waste might be significant.\n    # Let's consider the \"quality\" of the bin itself.\n    # Metric 2: Quality-Aware Best Fit\n    # Prioritize bins that offer a good fit, but also consider the overall \"quality\" of the bin.\n    # A \"quality\" bin might be one that is not too empty, nor too full,\n    # and has already accommodated a few items (though we don't have this info directly).\n    # Let's proxy \"quality\" by the *inverse* of the remaining capacity.\n    # A bin with less remaining capacity is \"more full\" and might be considered of higher quality in terms of utilization.\n    # So, we want to reward:\n    # 1. Small gap (`suitable_bins_caps - item` is small).\n    # 2. High initial fill (small `suitable_bins_caps`).\n    # This can be combined as: `1 / (suitable_bins_caps - item + epsilon) * (1 / (suitable_bins_caps + epsilon))`.\n    # This effectively prioritizes bins that have *just enough* capacity and are already quite full.\n\n    # Let's try to be more nuanced. What if the item is very small?\n    # A very small item should preferably go into a bin that has a moderate amount of remaining capacity,\n    # to avoid making it too empty.\n    # What if the item is very large?\n    # A very large item should ideally go into a bin that has a capacity *just* over the item size.\n    # This suggests a weight that adapts not just to the item size, but also to the distribution of `suitable_bins_caps`.\n\n    # Metric 2: Adaptive Fit Quality\n    # We want to favor bins where `suitable_bins_caps` is slightly larger than `item`.\n    # Let's consider the \"excess capacity\" `excess = suitable_bins_caps - item`.\n    # We want `excess` to be small.\n    # Additionally, we want to avoid using bins that are already extremely full or extremely empty.\n    # Let's define a \"bin desirability\" based on its remaining capacity `suitable_bins_caps`.\n    # High desirability for bins with moderate remaining capacity (e.g., 50% to 80% full).\n    # Let's simplify and focus on the \"slack\" created.\n    # `slack = suitable_bins_caps - item`.\n    # We want `slack` to be small.\n    # However, if `suitable_bins_caps` is already very small, a small `slack` might be undesirable.\n    # Let's try a score that combines the inverse of `suitable_bins_caps` (favoring fuller bins)\n    # and penalizes large slack.\n    # Consider `score = (1 - suitable_bins_caps) / (suitable_bins_caps - item + epsilon)`\n    # This would be high if the bin is quite full AND the slack is small.\n    # It might be unstable if `suitable_bins_caps` is small.\n\n    # Let's introduce a penalty for bins that are *too* empty relative to the item.\n    # If `suitable_bins_caps` is much larger than `item`, it might lead to poor packing.\n    # A metric for \"over-capacity\": `max(0, suitable_bins_caps - item - target_slack)`\n    # where `target_slack` could be a small constant or a fraction of `item`.\n    # Let's aim for a specific target remaining capacity after placement, say `target_rem`.\n    # `target_rem` could be related to the item size, e.g., `0.1 * item` or `0.2 * BIN_CAPACITY`.\n    # For simplicity, let's make `target_rem` a small constant like 0.1.\n    # Then, we penalize bins where `suitable_bins_caps - item` is much larger than `target_rem`.\n    # `penalty = max(0, (suitable_bins_caps - item) - target_rem)`\n    # The score would be `1 / (penalty + 1)`.\n\n    # Metric 2: Balanced Slack Minimization\n    # Prioritize bins that minimize slack, but also penalize placing items in very empty bins\n    # where the item occupies a small fraction of the remaining capacity.\n    # Let `slack = suitable_bins_caps - item`. We want small slack.\n    # Consider the ratio `item / suitable_bins_caps`. We want this to be reasonably high.\n    # A combination could be: `(1 / (slack + epsilon)) * (item / (suitable_bins_caps + epsilon))`\n    # This score is high when slack is small AND item occupies a good portion of bin's remaining capacity.\n\n    # Let's try to integrate the \"exploration\" idea more subtly.\n    # Exploration might mean not always picking the absolute \"best\" fit if it means creating a very specialized bin.\n    # We want to keep options open.\n    # This might mean slightly favoring bins that are neither too full nor too empty.\n\n    # Metric 2 (Re-Revisited): Dynamic Target Fit\n    # The \"ideal\" remaining capacity after placement might depend on the item size and the overall bin fullness.\n    # If a bin is very full, we want the item to fit snugly (small residual capacity).\n    # If a bin is quite empty, we might want the item to fill a larger portion of it.\n    # Let's try a score that rewards bins where `suitable_bins_caps - item` is minimized,\n    # but with a twist: the \"penalty\" for slack depends on how \"full\" the bin is.\n    # Let's use the inverse of `suitable_bins_caps` to represent \"fullness\".\n    # `fullness_score = 1 / (suitable_bins_caps + epsilon)`.\n    # We want to penalize slack `suitable_bins_caps - item`.\n    # Combine: `score = fullness_score / (suitable_bins_caps - item + epsilon)`\n    # This would prioritize bins that are full and have small slack.\n\n    # Let's go with a simpler, more interpretable combination that balances tight fits and bin usage.\n    # Metric 2: Fill State Aware Best Fit\n    # We want a tight fit (Best Fit) and we want to avoid using bins that are too empty.\n    # `best_fit_contribution = 1.0 / (suitable_bins_caps - item + 1e-6)`\n    # How to penalize bins that are too empty?\n    # We can use `1 - suitable_bins_caps` as a proxy for \"fill level\".\n    # We want to avoid bins where `suitable_bins_caps` is large.\n    # So, we want to reward bins with low `suitable_bins_caps`.\n    # Let's try `fill_awareness = 1.0 / (suitable_bins_caps + 1e-6)`.\n    # This rewards fuller bins.\n    # Combining them: `score = best_fit_contribution * fill_awareness`\n    # `score = (1.0 / (suitable_bins_caps - item + 1e-6)) * (1.0 / (suitable_bins_caps + 1e-6))`\n    # This prioritizes bins that are both nearly full and have just enough capacity for the item.\n\n    # Let's normalize the components to ensure comparability.\n    # Metric 1: Best Fit (again, but more robust)\n    # Prioritize bins that leave minimum remaining capacity.\n    # `remaining_capacity = suitable_bins_caps - item`\n    # `best_fit_score = 1.0 / (remaining_capacity + 1e-6)`\n    # Normalize this score by the maximum possible best fit score.\n    remaining_capacity = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_capacity + 1e-6)\n\n    # Metric 2: Bin Fullness Prioritization\n    # Prioritize bins that are already more full. This encourages consolidating items.\n    # Use inverse of remaining capacity as a proxy for fullness.\n    # `fullness_score = 1.0 / (suitable_bins_caps + 1e-6)`\n    # Normalize this score by the maximum possible fullness score.\n    fullness_scores = 1.0 / (suitable_bins_caps + 1e-6)\n\n    # Metric 3: Item Size Ratio\n    # For smaller items, it's often beneficial to place them in bins that are not too empty,\n    # to avoid creating many sparsely filled bins.\n    # For larger items, it's crucial to find a good fit.\n    # Let's consider the ratio of the item size to the bin's remaining capacity.\n    # `item_ratio = item / suitable_bins_caps`. A higher ratio means the item makes a larger dent.\n    # This metric is good for smaller items in moderately full bins.\n    # For very large items, this ratio might be close to 1.\n    item_ratio_scores = item / (suitable_bins_caps + 1e-6)\n\n    # Normalization of components\n    # Normalize Best Fit scores: higher score for tighter fits.\n    if np.max(best_fit_scores) > 1e-9:\n        norm_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        norm_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Fullness scores: higher score for fuller bins.\n    if np.max(fullness_scores) > 1e-9:\n        norm_fullness = fullness_scores / np.max(fullness_scores)\n    else:\n        norm_fullness = np.zeros_like(fullness_scores)\n\n    # Normalize Item Ratio scores: higher score for items filling more of the bin's remaining capacity.\n    if np.max(item_ratio_scores) > 1e-9:\n        norm_item_ratio = item_ratio_scores / np.max(item_ratio_scores)\n    else:\n        norm_item_ratio = np.zeros_like(item_ratio_scores)\n\n    # Adaptive Weighting Strategy:\n    # The importance of each metric can depend on the item's size relative to the bin capacity.\n    # Assume a standard bin capacity (e.g., 1.0 for normalization purposes if item is scaled).\n    # If `item` is large (close to 1.0), 'Best Fit' and 'Fullness' are critical.\n    # If `item` is small, 'Item Ratio' and 'Fullness' are important to avoid waste.\n\n    # Let's define weights based on the item's size relative to the *average* suitable bin capacity.\n    # This provides a context for the item.\n    avg_suitable_cap = np.mean(suitable_bins_caps)\n    if avg_suitable_cap > 1e-9:\n        item_vs_avg_cap_ratio = item / avg_suitable_cap\n    else:\n        item_vs_avg_cap_ratio = 0.5 # Default if no suitable bins or very small capacity\n\n    # Weights adjustment:\n    # If item is large relative to average capacity, emphasize Best Fit and Fullness.\n    # If item is small relative to average capacity, emphasize Item Ratio and Fullness.\n\n    # Base weights, can be tuned.\n    w_bf = 0.4\n    w_f = 0.4\n    w_ir = 0.2\n\n    # Adjust weights dynamically.\n    # Example: if item is large, increase weight for BF and F.\n    if item_vs_avg_cap_ratio > 1.0: # Item is larger than average remaining capacity\n        w_bf += 0.2 * (item_vs_avg_cap_ratio - 1.0) # Boost BF for large items\n        w_f += 0.1 * (item_vs_avg_cap_ratio - 1.0) # Boost F for large items\n        w_ir -= 0.3 * (item_vs_avg_cap_ratio - 1.0) # Decrease IR for large items\n    else: # Item is smaller than average remaining capacity\n        w_ir += 0.3 * (1.0 - item_vs_avg_cap_ratio) # Boost IR for small items\n        w_f += 0.1 * (1.0 - item_vs_avg_cap_ratio) # Boost F for small items\n        w_bf -= 0.2 * (1.0 - item_vs_avg_cap_ratio) # Decrease BF for small items\n\n    # Ensure weights are non-negative and sum to 1 (or close enough, then re-normalize).\n    w_bf = max(0, w_bf)\n    w_f = max(0, w_f)\n    w_ir = max(0, w_ir)\n\n    total_w = w_bf + w_f + w_ir\n    if total_w > 1e-9:\n        w_bf /= total_w\n        w_f /= total_w\n        w_ir /= total_w\n    else: # Fallback to equal weights if something goes wrong\n        w_bf, w_f, w_ir = 1/3, 1/3, 1/3\n\n    # Combine normalized scores with dynamic weights\n    combined_scores = (w_bf * norm_best_fit +\n                       w_f * norm_fullness +\n                       w_ir * norm_item_ratio)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    num_suitable_bins = suitable_bins_caps.shape[0]\n\n    # Metric 1: Best Fit (revisited)\n    # Focus on the *relative* remaining capacity after placement.\n    # Smaller relative remaining capacity is better (tighter fit).\n    # Use inverse of (1 + relative remaining capacity) to give higher scores to tighter fits.\n    # Added a small constant to the denominator to avoid division by zero and to differentiate very tight fits.\n    remaining_after_placement = suitable_bins_caps - item\n    relative_remaining = remaining_after_placement / suitable_bins_caps # How much capacity is left relative to the bin's current state\n    best_fit_scores = 1.0 / (1.0 + relative_remaining + 1e-6) # Higher score for smaller relative_remaining\n\n    # Metric 2: Fill Level Favorability\n    # Favor bins that are neither too empty nor too full. This promotes better space utilization.\n    # We can define a \"target fill level\". Let's assume a target fill level that is high but leaves some room, e.g., 80-90%.\n    # We can use a Gaussian-like function centered around a desired fill ratio.\n    # Let's assume bin capacity is implicitly normalized to 1.0 for this metric, and item size is also normalized.\n    # The actual capacity of the bin might be more relevant. Let's use remaining capacity relative to a hypothetical maximum bin capacity (e.g., 1.0 for normalized context, or inferable).\n    # For simplicity and adaptability, let's consider the ratio of *item size* to the *bin's remaining capacity*.\n    # A higher ratio means the item makes a bigger dent in the remaining capacity.\n    # We want to avoid bins that are almost empty, as a small item would leave them very inefficiently filled.\n    # A bin that is mostly full and can still accommodate the item is also less desirable for exploration.\n    # Let's score based on how much capacity is *already used* (1 - normalized remaining capacity).\n    # This encourages using bins that have some items already.\n    # Max capacity of a bin isn't explicitly given, assume it's large enough to hold any item.\n    # Let's use the item size relative to the *current remaining capacity* of the bin.\n    # If item/suitable_bins_caps is high, it means the item is a large fraction of what's left -> good for utilization.\n    # If item/suitable_bins_caps is low, it means the item is small compared to what's left -> might leave it too empty.\n    # We want to reward bins where the item fills a significant portion of the *remaining* space.\n    fill_score_component = item / (suitable_bins_caps + 1e-6)\n    # We also want to reward bins that are not excessively empty. Let's use the inverse of normalized remaining capacity.\n    # Normalized remaining capacity: suitable_bins_caps / MAX_BIN_CAPACITY (assume MAX_BIN_CAPACITY=1 for normalized context)\n    # So, 1 - suitable_bins_caps is a measure of \"fill level\". We want to prioritize bins with moderate fill levels.\n    # Let's combine these: prefer bins where the item fits snugly, and the bin isn't too empty.\n    # A bin is \"good\" if `suitable_bins_caps` is not too large, and `item` is not too small relative to it.\n    # Let's define a \"gap score\" based on `suitable_bins_caps - item`. Small gaps are good.\n    gap_scores = suitable_bins_caps - item\n    # We want to penalize bins that are too empty, meaning `suitable_bins_caps` is large.\n    # Let's consider the inverse of the *absolute* remaining capacity. Larger remaining capacity = lower score.\n    # And also the inverse of `gap_scores`. Smaller gaps = higher score.\n    # Combine `1 / (gap_scores + epsilon)` with `1 / (suitable_bins_caps + epsilon)`.\n    # This is effectively prioritizing bins with small remaining capacity after placement AND small total capacity.\n    # This seems too restrictive. Let's rethink.\n\n    # Metric 2 (Revised): Balanced Fill\n    # Aim for bins that are \"moderately full\" but also accommodate the item well.\n    # Consider the ratio of item size to bin capacity.\n    # `item / bin_capacity`. This is not directly available.\n    # Let's use `item / suitable_bins_caps` as a proxy for how much of the *current remaining space* the item occupies.\n    # High values are good (item fills a lot). Low values are bad (item is small relative to space).\n    # Also, consider the *absolute* remaining capacity after placement: `suitable_bins_caps - item`.\n    # Small absolute remaining capacity is good (tight fit). Large absolute remaining capacity is bad.\n    # Let's combine these: higher score if `item / suitable_bins_caps` is high AND `suitable_bins_caps - item` is low.\n    # This is somewhat redundant with Best Fit.\n\n    # Let's introduce a metric for \"bin efficiency\" or \"fairness\".\n    # Metric 2 (New): Bin Efficiency Prioritization\n    # Prioritize bins that have capacity close to the item size, but slightly larger.\n    # This helps consolidate items without leaving excessive \"slack\".\n    # We are looking for `suitable_bins_caps` that are \"just enough\".\n    # This can be modeled as `1 / (suitable_bins_caps - item + epsilon)` BUT we also want to avoid very large capacities.\n    # Let's use a metric that is high when `suitable_bins_caps` is close to `item`.\n    # We can use a Gaussian-like function centered around `item`.\n    # The \"variance\" of this Gaussian could adapt to the item size. Larger items might tolerate larger gaps.\n    # Let's keep it simpler: reward bins where `suitable_bins_caps` is \"just above\" `item`.\n    # This means `suitable_bins_caps - item` should be small and positive.\n    # `1 / (suitable_bins_caps - item + epsilon)` is similar to best-fit.\n\n    # Let's try a simpler, more direct approach for diversification and better space utilization.\n    # Metric 2: Space Utilization Fairness\n    # We want to use bins that have a reasonable amount of space left but are not excessively empty.\n    # A bin that is almost full and can fit the item is good.\n    # A bin that is almost empty and can fit the item is less good (leaves a lot of slack).\n    # Let's focus on the \"fill state\" of the bin *before* placing the item.\n    # We need to infer the \"original\" capacity or the \"current fill\".\n    # If we assume a fixed bin capacity (e.g., 1.0 for normalization), then `1 - suitable_bins_caps` is the \"fill ratio\".\n    # We want to avoid bins that are very empty (fill ratio close to 0).\n    # Let's define a \"fill penalty\": higher penalty for very empty bins.\n    # `fill_penalty = max(0, 0.5 - (1 - suitable_bins_caps))` -> penalize bins where remaining capacity > 0.5\n    # This is inverted. We want to *reward* bins that are not too empty.\n    # Let's consider `1 - suitable_bins_caps` as \"current fill\".\n    # We can use a function that peaks at a moderate fill level, e.g., 0.7-0.9.\n    # A simple sigmoid-like function can achieve this.\n    # `fill_score = 1 / (1 + exp(-(fill_ratio - target_fill) / steepness))`\n    # Let's use a simpler approach: reward bins whose remaining capacity is not too large.\n    # Inverse of remaining capacity: `1 / (suitable_bins_caps + epsilon)`.\n    # This prioritizes bins that are more full.\n    # However, this is counter to \"exploration\" or giving smaller items space.\n\n    # Let's try to balance \"tight fit\" with \"not too empty\".\n    # Metric 2: Optimized Capacity Usage\n    # Prioritize bins where the remaining capacity is just enough for the item,\n    # AND the bin wasn't excessively empty to begin with.\n    # Consider `suitable_bins_caps` relative to the item size.\n    # We want `suitable_bins_caps` to be slightly larger than `item`.\n    # Let's score based on the inverse of `suitable_bins_caps` (favoring fuller bins) and also penalize very large `suitable_bins_caps`.\n    # Consider the ratio: `item / suitable_bins_caps`. High is good.\n    # And the gap: `suitable_bins_caps - item`. Low is good.\n    # Let's combine them: `(item / suitable_bins_caps) * (1 / (suitable_bins_caps - item + epsilon))`\n    # This might become unstable if `suitable_bins_caps` is very close to `item`.\n\n    # Rethinking the goal: dynamic adaptation and avoiding suboptimal choices early on.\n    # Instead of multiple, possibly conflicting metrics with complex weighting, let's try a more robust, adaptive metric.\n    # A common issue is creating many nearly empty bins or very full bins.\n    # We want to find a balance.\n    # Let's consider the \"waste\" created by placing the item.\n    # Waste = `suitable_bins_caps - item`. Lower waste is better (Best Fit).\n    # However, if a bin has very little capacity remaining overall, even a small waste might be significant.\n    # Let's consider the \"quality\" of the bin itself.\n    # Metric 2: Quality-Aware Best Fit\n    # Prioritize bins that offer a good fit, but also consider the overall \"quality\" of the bin.\n    # A \"quality\" bin might be one that is not too empty, nor too full,\n    # and has already accommodated a few items (though we don't have this info directly).\n    # Let's proxy \"quality\" by the *inverse* of the remaining capacity.\n    # A bin with less remaining capacity is \"more full\" and might be considered of higher quality in terms of utilization.\n    # So, we want to reward:\n    # 1. Small gap (`suitable_bins_caps - item` is small).\n    # 2. High initial fill (small `suitable_bins_caps`).\n    # This can be combined as: `1 / (suitable_bins_caps - item + epsilon) * (1 / (suitable_bins_caps + epsilon))`.\n    # This effectively prioritizes bins that have *just enough* capacity and are already quite full.\n\n    # Let's try to be more nuanced. What if the item is very small?\n    # A very small item should preferably go into a bin that has a moderate amount of remaining capacity,\n    # to avoid making it too empty.\n    # What if the item is very large?\n    # A very large item should ideally go into a bin that has a capacity *just* over the item size.\n    # This suggests a weight that adapts not just to the item size, but also to the distribution of `suitable_bins_caps`.\n\n    # Metric 2: Adaptive Fit Quality\n    # We want to favor bins where `suitable_bins_caps` is slightly larger than `item`.\n    # Let's consider the \"excess capacity\" `excess = suitable_bins_caps - item`.\n    # We want `excess` to be small.\n    # Additionally, we want to avoid using bins that are already extremely full or extremely empty.\n    # Let's define a \"bin desirability\" based on its remaining capacity `suitable_bins_caps`.\n    # High desirability for bins with moderate remaining capacity (e.g., 50% to 80% full).\n    # Let's simplify and focus on the \"slack\" created.\n    # `slack = suitable_bins_caps - item`.\n    # We want `slack` to be small.\n    # However, if `suitable_bins_caps` is already very small, a small `slack` might be undesirable.\n    # Let's try a score that combines the inverse of `suitable_bins_caps` (favoring fuller bins)\n    # and penalizes large slack.\n    # Consider `score = (1 - suitable_bins_caps) / (suitable_bins_caps - item + epsilon)`\n    # This would be high if the bin is quite full AND the slack is small.\n    # It might be unstable if `suitable_bins_caps` is small.\n\n    # Let's introduce a penalty for bins that are *too* empty relative to the item.\n    # If `suitable_bins_caps` is much larger than `item`, it might lead to poor packing.\n    # A metric for \"over-capacity\": `max(0, suitable_bins_caps - item - target_slack)`\n    # where `target_slack` could be a small constant or a fraction of `item`.\n    # Let's aim for a specific target remaining capacity after placement, say `target_rem`.\n    # `target_rem` could be related to the item size, e.g., `0.1 * item` or `0.2 * BIN_CAPACITY`.\n    # For simplicity, let's make `target_rem` a small constant like 0.1.\n    # Then, we penalize bins where `suitable_bins_caps - item` is much larger than `target_rem`.\n    # `penalty = max(0, (suitable_bins_caps - item) - target_rem)`\n    # The score would be `1 / (penalty + 1)`.\n\n    # Metric 2: Balanced Slack Minimization\n    # Prioritize bins that minimize slack, but also penalize placing items in very empty bins\n    # where the item occupies a small fraction of the remaining capacity.\n    # Let `slack = suitable_bins_caps - item`. We want small slack.\n    # Consider the ratio `item / suitable_bins_caps`. We want this to be reasonably high.\n    # A combination could be: `(1 / (slack + epsilon)) * (item / (suitable_bins_caps + epsilon))`\n    # This score is high when slack is small AND item occupies a good portion of bin's remaining capacity.\n\n    # Let's try to integrate the \"exploration\" idea more subtly.\n    # Exploration might mean not always picking the absolute \"best\" fit if it means creating a very specialized bin.\n    # We want to keep options open.\n    # This might mean slightly favoring bins that are neither too full nor too empty.\n\n    # Metric 2 (Re-Revisited): Dynamic Target Fit\n    # The \"ideal\" remaining capacity after placement might depend on the item size and the overall bin fullness.\n    # If a bin is very full, we want the item to fit snugly (small residual capacity).\n    # If a bin is quite empty, we might want the item to fill a larger portion of it.\n    # Let's try a score that rewards bins where `suitable_bins_caps - item` is minimized,\n    # but with a twist: the \"penalty\" for slack depends on how \"full\" the bin is.\n    # Let's use the inverse of `suitable_bins_caps` to represent \"fullness\".\n    # `fullness_score = 1 / (suitable_bins_caps + epsilon)`.\n    # We want to penalize slack `suitable_bins_caps - item`.\n    # Combine: `score = fullness_score / (suitable_bins_caps - item + epsilon)`\n    # This would prioritize bins that are full and have small slack.\n\n    # Let's go with a simpler, more interpretable combination that balances tight fits and bin usage.\n    # Metric 2: Fill State Aware Best Fit\n    # We want a tight fit (Best Fit) and we want to avoid using bins that are too empty.\n    # `best_fit_contribution = 1.0 / (suitable_bins_caps - item + 1e-6)`\n    # How to penalize bins that are too empty?\n    # We can use `1 - suitable_bins_caps` as a proxy for \"fill level\".\n    # We want to avoid bins where `suitable_bins_caps` is large.\n    # So, we want to reward bins with low `suitable_bins_caps`.\n    # Let's try `fill_awareness = 1.0 / (suitable_bins_caps + 1e-6)`.\n    # This rewards fuller bins.\n    # Combining them: `score = best_fit_contribution * fill_awareness`\n    # `score = (1.0 / (suitable_bins_caps - item + 1e-6)) * (1.0 / (suitable_bins_caps + 1e-6))`\n    # This prioritizes bins that are both nearly full and have just enough capacity for the item.\n\n    # Let's normalize the components to ensure comparability.\n    # Metric 1: Best Fit (again, but more robust)\n    # Prioritize bins that leave minimum remaining capacity.\n    # `remaining_capacity = suitable_bins_caps - item`\n    # `best_fit_score = 1.0 / (remaining_capacity + 1e-6)`\n    # Normalize this score by the maximum possible best fit score.\n    remaining_capacity = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_capacity + 1e-6)\n\n    # Metric 2: Bin Fullness Prioritization\n    # Prioritize bins that are already more full. This encourages consolidating items.\n    # Use inverse of remaining capacity as a proxy for fullness.\n    # `fullness_score = 1.0 / (suitable_bins_caps + 1e-6)`\n    # Normalize this score by the maximum possible fullness score.\n    fullness_scores = 1.0 / (suitable_bins_caps + 1e-6)\n\n    # Metric 3: Item Size Ratio\n    # For smaller items, it's often beneficial to place them in bins that are not too empty,\n    # to avoid creating many sparsely filled bins.\n    # For larger items, it's crucial to find a good fit.\n    # Let's consider the ratio of the item size to the bin's remaining capacity.\n    # `item_ratio = item / suitable_bins_caps`. A higher ratio means the item makes a larger dent.\n    # This metric is good for smaller items in moderately full bins.\n    # For very large items, this ratio might be close to 1.\n    item_ratio_scores = item / (suitable_bins_caps + 1e-6)\n\n    # Normalization of components\n    # Normalize Best Fit scores: higher score for tighter fits.\n    if np.max(best_fit_scores) > 1e-9:\n        norm_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        norm_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Fullness scores: higher score for fuller bins.\n    if np.max(fullness_scores) > 1e-9:\n        norm_fullness = fullness_scores / np.max(fullness_scores)\n    else:\n        norm_fullness = np.zeros_like(fullness_scores)\n\n    # Normalize Item Ratio scores: higher score for items filling more of the bin's remaining capacity.\n    if np.max(item_ratio_scores) > 1e-9:\n        norm_item_ratio = item_ratio_scores / np.max(item_ratio_scores)\n    else:\n        norm_item_ratio = np.zeros_like(item_ratio_scores)\n\n    # Adaptive Weighting Strategy:\n    # The importance of each metric can depend on the item's size relative to the bin capacity.\n    # Assume a standard bin capacity (e.g., 1.0 for normalization purposes if item is scaled).\n    # If `item` is large (close to 1.0), 'Best Fit' and 'Fullness' are critical.\n    # If `item` is small, 'Item Ratio' and 'Fullness' are important to avoid waste.\n\n    # Let's define weights based on the item's size relative to the *average* suitable bin capacity.\n    # This provides a context for the item.\n    avg_suitable_cap = np.mean(suitable_bins_caps)\n    if avg_suitable_cap > 1e-9:\n        item_vs_avg_cap_ratio = item / avg_suitable_cap\n    else:\n        item_vs_avg_cap_ratio = 0.5 # Default if no suitable bins or very small capacity\n\n    # Weights adjustment:\n    # If item is large relative to average capacity, emphasize Best Fit and Fullness.\n    # If item is small relative to average capacity, emphasize Item Ratio and Fullness.\n\n    # Base weights, can be tuned.\n    w_bf = 0.4\n    w_f = 0.4\n    w_ir = 0.2\n\n    # Adjust weights dynamically.\n    # Example: if item is large, increase weight for BF and F.\n    if item_vs_avg_cap_ratio > 1.0: # Item is larger than average remaining capacity\n        w_bf += 0.2 * (item_vs_avg_cap_ratio - 1.0) # Boost BF for large items\n        w_f += 0.1 * (item_vs_avg_cap_ratio - 1.0) # Boost F for large items\n        w_ir -= 0.3 * (item_vs_avg_cap_ratio - 1.0) # Decrease IR for large items\n    else: # Item is smaller than average remaining capacity\n        w_ir += 0.3 * (1.0 - item_vs_avg_cap_ratio) # Boost IR for small items\n        w_f += 0.1 * (1.0 - item_vs_avg_cap_ratio) # Boost F for small items\n        w_bf -= 0.2 * (1.0 - item_vs_avg_cap_ratio) # Decrease BF for small items\n\n    # Ensure weights are non-negative and sum to 1 (or close enough, then re-normalize).\n    w_bf = max(0, w_bf)\n    w_f = max(0, w_f)\n    w_ir = max(0, w_ir)\n\n    total_w = w_bf + w_f + w_ir\n    if total_w > 1e-9:\n        w_bf /= total_w\n        w_f /= total_w\n        w_ir /= total_w\n    else: # Fallback to equal weights if something goes wrong\n        w_bf, w_f, w_ir = 1/3, 1/3, 1/3\n\n    # Combine normalized scores with dynamic weights\n    combined_scores = (w_bf * norm_best_fit +\n                       w_f * norm_fullness +\n                       w_ir * norm_item_ratio)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    num_suitable_bins = suitable_bins_caps.shape[0]\n\n    # Metric 1: Best Fit (revisited)\n    # Focus on the *relative* remaining capacity after placement.\n    # Smaller relative remaining capacity is better (tighter fit).\n    # Use inverse of (1 + relative remaining capacity) to give higher scores to tighter fits.\n    # Added a small constant to the denominator to avoid division by zero and to differentiate very tight fits.\n    remaining_after_placement = suitable_bins_caps - item\n    relative_remaining = remaining_after_placement / suitable_bins_caps # How much capacity is left relative to the bin's current state\n    best_fit_scores = 1.0 / (1.0 + relative_remaining + 1e-6) # Higher score for smaller relative_remaining\n\n    # Metric 2: Fill Level Favorability\n    # Favor bins that are neither too empty nor too full. This promotes better space utilization.\n    # We can define a \"target fill level\". Let's assume a target fill level that is high but leaves some room, e.g., 80-90%.\n    # We can use a Gaussian-like function centered around a desired fill ratio.\n    # Let's assume bin capacity is implicitly normalized to 1.0 for this metric, and item size is also normalized.\n    # The actual capacity of the bin might be more relevant. Let's use remaining capacity relative to a hypothetical maximum bin capacity (e.g., 1.0 for normalized context, or inferable).\n    # For simplicity and adaptability, let's consider the ratio of *item size* to the *bin's remaining capacity*.\n    # A higher ratio means the item makes a bigger dent in the remaining capacity.\n    # We want to avoid bins that are almost empty, as a small item would leave them very inefficiently filled.\n    # A bin that is mostly full and can still accommodate the item is also less desirable for exploration.\n    # Let's score based on how much capacity is *already used* (1 - normalized remaining capacity).\n    # This encourages using bins that have some items already.\n    # Max capacity of a bin isn't explicitly given, assume it's large enough to hold any item.\n    # Let's use the item size relative to the *current remaining capacity* of the bin.\n    # If item/suitable_bins_caps is high, it means the item is a large fraction of what's left -> good for utilization.\n    # If item/suitable_bins_caps is low, it means the item is small compared to what's left -> might leave it too empty.\n    # We want to reward bins where the item fills a significant portion of the *remaining* space.\n    fill_score_component = item / (suitable_bins_caps + 1e-6)\n    # We also want to reward bins that are not excessively empty. Let's use the inverse of normalized remaining capacity.\n    # Normalized remaining capacity: suitable_bins_caps / MAX_BIN_CAPACITY (assume MAX_BIN_CAPACITY=1 for normalized context)\n    # So, 1 - suitable_bins_caps is a measure of \"fill level\". We want to prioritize bins with moderate fill levels.\n    # Let's combine these: prefer bins where the item fits snugly, and the bin isn't too empty.\n    # A bin is \"good\" if `suitable_bins_caps` is not too large, and `item` is not too small relative to it.\n    # Let's define a \"gap score\" based on `suitable_bins_caps - item`. Small gaps are good.\n    gap_scores = suitable_bins_caps - item\n    # We want to penalize bins that are too empty, meaning `suitable_bins_caps` is large.\n    # Let's consider the inverse of the *absolute* remaining capacity. Larger remaining capacity = lower score.\n    # And also the inverse of `gap_scores`. Smaller gaps = higher score.\n    # Combine `1 / (gap_scores + epsilon)` with `1 / (suitable_bins_caps + epsilon)`.\n    # This is effectively prioritizing bins with small remaining capacity after placement AND small total capacity.\n    # This seems too restrictive. Let's rethink.\n\n    # Metric 2 (Revised): Balanced Fill\n    # Aim for bins that are \"moderately full\" but also accommodate the item well.\n    # Consider the ratio of item size to bin capacity.\n    # `item / bin_capacity`. This is not directly available.\n    # Let's use `item / suitable_bins_caps` as a proxy for how much of the *current remaining space* the item occupies.\n    # High values are good (item fills a lot). Low values are bad (item is small relative to space).\n    # Also, consider the *absolute* remaining capacity after placement: `suitable_bins_caps - item`.\n    # Small absolute remaining capacity is good (tight fit). Large absolute remaining capacity is bad.\n    # Let's combine these: higher score if `item / suitable_bins_caps` is high AND `suitable_bins_caps - item` is low.\n    # This is somewhat redundant with Best Fit.\n\n    # Let's introduce a metric for \"bin efficiency\" or \"fairness\".\n    # Metric 2 (New): Bin Efficiency Prioritization\n    # Prioritize bins that have capacity close to the item size, but slightly larger.\n    # This helps consolidate items without leaving excessive \"slack\".\n    # We are looking for `suitable_bins_caps` that are \"just enough\".\n    # This can be modeled as `1 / (suitable_bins_caps - item + epsilon)` BUT we also want to avoid very large capacities.\n    # Let's use a metric that is high when `suitable_bins_caps` is close to `item`.\n    # We can use a Gaussian-like function centered around `item`.\n    # The \"variance\" of this Gaussian could adapt to the item size. Larger items might tolerate larger gaps.\n    # Let's keep it simpler: reward bins where `suitable_bins_caps` is \"just above\" `item`.\n    # This means `suitable_bins_caps - item` should be small and positive.\n    # `1 / (suitable_bins_caps - item + epsilon)` is similar to best-fit.\n\n    # Let's try a simpler, more direct approach for diversification and better space utilization.\n    # Metric 2: Space Utilization Fairness\n    # We want to use bins that have a reasonable amount of space left but are not excessively empty.\n    # A bin that is almost full and can fit the item is good.\n    # A bin that is almost empty and can fit the item is less good (leaves a lot of slack).\n    # Let's focus on the \"fill state\" of the bin *before* placing the item.\n    # We need to infer the \"original\" capacity or the \"current fill\".\n    # If we assume a fixed bin capacity (e.g., 1.0 for normalization), then `1 - suitable_bins_caps` is the \"fill ratio\".\n    # We want to avoid bins that are very empty (fill ratio close to 0).\n    # Let's define a \"fill penalty\": higher penalty for very empty bins.\n    # `fill_penalty = max(0, 0.5 - (1 - suitable_bins_caps))` -> penalize bins where remaining capacity > 0.5\n    # This is inverted. We want to *reward* bins that are not too empty.\n    # Let's consider `1 - suitable_bins_caps` as \"current fill\".\n    # We can use a function that peaks at a moderate fill level, e.g., 0.7-0.9.\n    # A simple sigmoid-like function can achieve this.\n    # `fill_score = 1 / (1 + exp(-(fill_ratio - target_fill) / steepness))`\n    # Let's use a simpler approach: reward bins whose remaining capacity is not too large.\n    # Inverse of remaining capacity: `1 / (suitable_bins_caps + epsilon)`.\n    # This prioritizes bins that are more full.\n    # However, this is counter to \"exploration\" or giving smaller items space.\n\n    # Let's try to balance \"tight fit\" with \"not too empty\".\n    # Metric 2: Optimized Capacity Usage\n    # Prioritize bins where the remaining capacity is just enough for the item,\n    # AND the bin wasn't excessively empty to begin with.\n    # Consider `suitable_bins_caps` relative to the item size.\n    # We want `suitable_bins_caps` to be slightly larger than `item`.\n    # Let's score based on the inverse of `suitable_bins_caps` (favoring fuller bins) and also penalize very large `suitable_bins_caps`.\n    # Consider the ratio: `item / suitable_bins_caps`. High is good.\n    # And the gap: `suitable_bins_caps - item`. Low is good.\n    # Let's combine them: `(item / suitable_bins_caps) * (1 / (suitable_bins_caps - item + epsilon))`\n    # This might become unstable if `suitable_bins_caps` is very close to `item`.\n\n    # Rethinking the goal: dynamic adaptation and avoiding suboptimal choices early on.\n    # Instead of multiple, possibly conflicting metrics with complex weighting, let's try a more robust, adaptive metric.\n    # A common issue is creating many nearly empty bins or very full bins.\n    # We want to find a balance.\n    # Let's consider the \"waste\" created by placing the item.\n    # Waste = `suitable_bins_caps - item`. Lower waste is better (Best Fit).\n    # However, if a bin has very little capacity remaining overall, even a small waste might be significant.\n    # Let's consider the \"quality\" of the bin itself.\n    # Metric 2: Quality-Aware Best Fit\n    # Prioritize bins that offer a good fit, but also consider the overall \"quality\" of the bin.\n    # A \"quality\" bin might be one that is not too empty, nor too full,\n    # and has already accommodated a few items (though we don't have this info directly).\n    # Let's proxy \"quality\" by the *inverse* of the remaining capacity.\n    # A bin with less remaining capacity is \"more full\" and might be considered of higher quality in terms of utilization.\n    # So, we want to reward:\n    # 1. Small gap (`suitable_bins_caps - item` is small).\n    # 2. High initial fill (small `suitable_bins_caps`).\n    # This can be combined as: `1 / (suitable_bins_caps - item + epsilon) * (1 / (suitable_bins_caps + epsilon))`.\n    # This effectively prioritizes bins that have *just enough* capacity and are already quite full.\n\n    # Let's try to be more nuanced. What if the item is very small?\n    # A very small item should preferably go into a bin that has a moderate amount of remaining capacity,\n    # to avoid making it too empty.\n    # What if the item is very large?\n    # A very large item should ideally go into a bin that has a capacity *just* over the item size.\n    # This suggests a weight that adapts not just to the item size, but also to the distribution of `suitable_bins_caps`.\n\n    # Metric 2: Adaptive Fit Quality\n    # We want to favor bins where `suitable_bins_caps` is slightly larger than `item`.\n    # Let's consider the \"excess capacity\" `excess = suitable_bins_caps - item`.\n    # We want `excess` to be small.\n    # Additionally, we want to avoid using bins that are already extremely full or extremely empty.\n    # Let's define a \"bin desirability\" based on its remaining capacity `suitable_bins_caps`.\n    # High desirability for bins with moderate remaining capacity (e.g., 50% to 80% full).\n    # Let's simplify and focus on the \"slack\" created.\n    # `slack = suitable_bins_caps - item`.\n    # We want `slack` to be small.\n    # However, if `suitable_bins_caps` is already very small, a small `slack` might be undesirable.\n    # Let's try a score that combines the inverse of `suitable_bins_caps` (favoring fuller bins)\n    # and penalizes large slack.\n    # Consider `score = (1 - suitable_bins_caps) / (suitable_bins_caps - item + epsilon)`\n    # This would be high if the bin is quite full AND the slack is small.\n    # It might be unstable if `suitable_bins_caps` is small.\n\n    # Let's introduce a penalty for bins that are *too* empty relative to the item.\n    # If `suitable_bins_caps` is much larger than `item`, it might lead to poor packing.\n    # A metric for \"over-capacity\": `max(0, suitable_bins_caps - item - target_slack)`\n    # where `target_slack` could be a small constant or a fraction of `item`.\n    # Let's aim for a specific target remaining capacity after placement, say `target_rem`.\n    # `target_rem` could be related to the item size, e.g., `0.1 * item` or `0.2 * BIN_CAPACITY`.\n    # For simplicity, let's make `target_rem` a small constant like 0.1.\n    # Then, we penalize bins where `suitable_bins_caps - item` is much larger than `target_rem`.\n    # `penalty = max(0, (suitable_bins_caps - item) - target_rem)`\n    # The score would be `1 / (penalty + 1)`.\n\n    # Metric 2: Balanced Slack Minimization\n    # Prioritize bins that minimize slack, but also penalize placing items in very empty bins\n    # where the item occupies a small fraction of the remaining capacity.\n    # Let `slack = suitable_bins_caps - item`. We want small slack.\n    # Consider the ratio `item / suitable_bins_caps`. We want this to be reasonably high.\n    # A combination could be: `(1 / (slack + epsilon)) * (item / (suitable_bins_caps + epsilon))`\n    # This score is high when slack is small AND item occupies a good portion of bin's remaining capacity.\n\n    # Let's try to integrate the \"exploration\" idea more subtly.\n    # Exploration might mean not always picking the absolute \"best\" fit if it means creating a very specialized bin.\n    # We want to keep options open.\n    # This might mean slightly favoring bins that are neither too full nor too empty.\n\n    # Metric 2 (Re-Revisited): Dynamic Target Fit\n    # The \"ideal\" remaining capacity after placement might depend on the item size and the overall bin fullness.\n    # If a bin is very full, we want the item to fit snugly (small residual capacity).\n    # If a bin is quite empty, we might want the item to fill a larger portion of it.\n    # Let's try a score that rewards bins where `suitable_bins_caps - item` is minimized,\n    # but with a twist: the \"penalty\" for slack depends on how \"full\" the bin is.\n    # Let's use the inverse of `suitable_bins_caps` to represent \"fullness\".\n    # `fullness_score = 1 / (suitable_bins_caps + epsilon)`.\n    # We want to penalize slack `suitable_bins_caps - item`.\n    # Combine: `score = fullness_score / (suitable_bins_caps - item + epsilon)`\n    # This would prioritize bins that are full and have small slack.\n\n    # Let's go with a simpler, more interpretable combination that balances tight fits and bin usage.\n    # Metric 2: Fill State Aware Best Fit\n    # We want a tight fit (Best Fit) and we want to avoid using bins that are too empty.\n    # `best_fit_contribution = 1.0 / (suitable_bins_caps - item + 1e-6)`\n    # How to penalize bins that are too empty?\n    # We can use `1 - suitable_bins_caps` as a proxy for \"fill level\".\n    # We want to avoid bins where `suitable_bins_caps` is large.\n    # So, we want to reward bins with low `suitable_bins_caps`.\n    # Let's try `fill_awareness = 1.0 / (suitable_bins_caps + 1e-6)`.\n    # This rewards fuller bins.\n    # Combining them: `score = best_fit_contribution * fill_awareness`\n    # `score = (1.0 / (suitable_bins_caps - item + 1e-6)) * (1.0 / (suitable_bins_caps + 1e-6))`\n    # This prioritizes bins that are both nearly full and have just enough capacity for the item.\n\n    # Let's normalize the components to ensure comparability.\n    # Metric 1: Best Fit (again, but more robust)\n    # Prioritize bins that leave minimum remaining capacity.\n    # `remaining_capacity = suitable_bins_caps - item`\n    # `best_fit_score = 1.0 / (remaining_capacity + 1e-6)`\n    # Normalize this score by the maximum possible best fit score.\n    remaining_capacity = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_capacity + 1e-6)\n\n    # Metric 2: Bin Fullness Prioritization\n    # Prioritize bins that are already more full. This encourages consolidating items.\n    # Use inverse of remaining capacity as a proxy for fullness.\n    # `fullness_score = 1.0 / (suitable_bins_caps + 1e-6)`\n    # Normalize this score by the maximum possible fullness score.\n    fullness_scores = 1.0 / (suitable_bins_caps + 1e-6)\n\n    # Metric 3: Item Size Ratio\n    # For smaller items, it's often beneficial to place them in bins that are not too empty,\n    # to avoid creating many sparsely filled bins.\n    # For larger items, it's crucial to find a good fit.\n    # Let's consider the ratio of the item size to the bin's remaining capacity.\n    # `item_ratio = item / suitable_bins_caps`. A higher ratio means the item makes a larger dent.\n    # This metric is good for smaller items in moderately full bins.\n    # For very large items, this ratio might be close to 1.\n    item_ratio_scores = item / (suitable_bins_caps + 1e-6)\n\n    # Normalization of components\n    # Normalize Best Fit scores: higher score for tighter fits.\n    if np.max(best_fit_scores) > 1e-9:\n        norm_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        norm_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Fullness scores: higher score for fuller bins.\n    if np.max(fullness_scores) > 1e-9:\n        norm_fullness = fullness_scores / np.max(fullness_scores)\n    else:\n        norm_fullness = np.zeros_like(fullness_scores)\n\n    # Normalize Item Ratio scores: higher score for items filling more of the bin's remaining capacity.\n    if np.max(item_ratio_scores) > 1e-9:\n        norm_item_ratio = item_ratio_scores / np.max(item_ratio_scores)\n    else:\n        norm_item_ratio = np.zeros_like(item_ratio_scores)\n\n    # Adaptive Weighting Strategy:\n    # The importance of each metric can depend on the item's size relative to the bin capacity.\n    # Assume a standard bin capacity (e.g., 1.0 for normalization purposes if item is scaled).\n    # If `item` is large (close to 1.0), 'Best Fit' and 'Fullness' are critical.\n    # If `item` is small, 'Item Ratio' and 'Fullness' are important to avoid waste.\n\n    # Let's define weights based on the item's size relative to the *average* suitable bin capacity.\n    # This provides a context for the item.\n    avg_suitable_cap = np.mean(suitable_bins_caps)\n    if avg_suitable_cap > 1e-9:\n        item_vs_avg_cap_ratio = item / avg_suitable_cap\n    else:\n        item_vs_avg_cap_ratio = 0.5 # Default if no suitable bins or very small capacity\n\n    # Weights adjustment:\n    # If item is large relative to average capacity, emphasize Best Fit and Fullness.\n    # If item is small relative to average capacity, emphasize Item Ratio and Fullness.\n\n    # Base weights, can be tuned.\n    w_bf = 0.4\n    w_f = 0.4\n    w_ir = 0.2\n\n    # Adjust weights dynamically.\n    # Example: if item is large, increase weight for BF and F.\n    if item_vs_avg_cap_ratio > 1.0: # Item is larger than average remaining capacity\n        w_bf += 0.2 * (item_vs_avg_cap_ratio - 1.0) # Boost BF for large items\n        w_f += 0.1 * (item_vs_avg_cap_ratio - 1.0) # Boost F for large items\n        w_ir -= 0.3 * (item_vs_avg_cap_ratio - 1.0) # Decrease IR for large items\n    else: # Item is smaller than average remaining capacity\n        w_ir += 0.3 * (1.0 - item_vs_avg_cap_ratio) # Boost IR for small items\n        w_f += 0.1 * (1.0 - item_vs_avg_cap_ratio) # Boost F for small items\n        w_bf -= 0.2 * (1.0 - item_vs_avg_cap_ratio) # Decrease BF for small items\n\n    # Ensure weights are non-negative and sum to 1 (or close enough, then re-normalize).\n    w_bf = max(0, w_bf)\n    w_f = max(0, w_f)\n    w_ir = max(0, w_ir)\n\n    total_w = w_bf + w_f + w_ir\n    if total_w > 1e-9:\n        w_bf /= total_w\n        w_f /= total_w\n        w_ir /= total_w\n    else: # Fallback to equal weights if something goes wrong\n        w_bf, w_f, w_ir = 1/3, 1/3, 1/3\n\n    # Combine normalized scores with dynamic weights\n    combined_scores = (w_bf * norm_best_fit +\n                       w_f * norm_fullness +\n                       w_ir * norm_item_ratio)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (tightness) with a 'fair share' exploration bonus,\n    prioritizing bins that are neither too full nor too empty, relative to others.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Best Fit component: Inverse of remaining capacity after placing the item.\n    # Smaller difference implies a tighter fit and higher score.\n    tightness_scores = 1.0 / (valid_bins_remain_cap - item + 1e-9)\n\n    # Exploration component: Penalize bins that are excessively full or empty\n    # relative to the average remaining capacity of suitable bins. This encourages\n    # a more balanced distribution. We use a quadratic penalty.\n    avg_remain_cap = np.mean(valid_bins_remain_cap)\n    # Deviation from average, squared to penalize larger deviations more.\n    # We add a small constant to avoid zero deviation resulting in zero penalty.\n    fairness_penalty = (valid_bins_remain_cap - avg_remain_cap)**2 / (avg_remain_cap + 1e-9)\n\n    # Combine scores: Higher tightness is good, lower penalty (closer to avg) is good.\n    # We subtract the penalty as it's a negative aspect.\n    # Weights can be tuned; here, tightness is prioritized.\n    combined_scores = tightness_scores - fairness_penalty * 0.2 # Tunable parameter for penalty influence\n\n    # Normalize scores to be between 0 and 1.\n    min_score = np.min(combined_scores)\n    max_score = np.max(combined_scores)\n\n    if max_score - min_score > 1e-9:\n        priorities[suitable_bins_mask] = (combined_scores - min_score) / (max_score - min_score)\n    elif np.any(suitable_bins_mask):\n        # If all suitable bins have very similar combined scores, distribute equally.\n        priorities[suitable_bins_mask] = 1.0 / np.sum(suitable_bins_mask)\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a nuanced best-fit approach with an exploration bonus favoring less utilized bins,\n    and a subtle preference for bins with more overall remaining capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Modified Best Fit (from priority_v0)\n    # Prioritizes bins leaving a \"sweet spot\" residual capacity to avoid tiny unusable gaps.\n    remaining_after_placement_m1 = suitable_bins_caps - item\n    target_residual = item * 0.2  # Target residual capacity ~20% of item size\n    # Gaussian-like function: higher score for residuals closer to target_residual\n    # Add small epsilon to avoid division by zero if target_residual is 0.\n    best_fit_scores = np.exp(-((remaining_after_placement_m1 - target_residual) / (target_residual + 1e-6))**2)\n\n    # Metric 2: Exploration Bonus (inspired by priority_v1, simpler version)\n    # Favors bins that are less full *after* placement, relative to other suitable bins.\n    min_rem_after_m2 = np.min(remaining_after_placement_m1)\n    max_rem_after_m2 = np.max(remaining_after_placement_m1)\n    \n    exploration_scores = np.zeros_like(suitable_bins_caps)\n    if max_rem_after_m2 > min_rem_after_m2:\n        # Normalized remaining capacity after placement: higher for more empty bins\n        exploration_scores = (remaining_after_placement_m1 - min_rem_after_m2) / (max_rem_after_m2 - min_rem_after_m2)\n    else:\n        # If all suitable bins result in the same remaining capacity, no exploration bonus from this diff.\n        # Default to 0.5 for any such bins to avoid bias.\n        exploration_scores = np.ones_like(suitable_bins_caps) * 0.5\n\n    # Metric 3: Usage Proxy (favors bins with more total remaining capacity)\n    # This is a simple proxy for less-used bins.\n    min_cap_all = np.min(bins_remain_cap)\n    max_cap_all = np.max(bins_remain_cap)\n    \n    usage_scores = np.zeros_like(suitable_bins_caps)\n    if max_cap_all > min_cap_all:\n        # Normalize current remaining capacities. Higher score for more remaining capacity.\n        normalized_current_caps = (suitable_bins_caps - min_cap_all) / (max_cap_all - min_cap_all)\n        usage_scores = normalized_current_caps\n    else:\n        # If all bins have same capacity, this metric doesn't differentiate.\n        usage_scores = np.ones_like(suitable_bins_caps) * 0.5\n\n    # Combine scores: Heavy emphasis on nuanced best-fit, moderate on exploration, light on usage.\n    # Weights are chosen to balance finding good fits with spreading items.\n    # 0.6 for Best Fit (primary, quality of fit)\n    # 0.3 for Exploration (secondary, diversity)\n    # 0.1 for Usage (tertiary, simple preference for emptier bins)\n    combined_scores = 0.6 * best_fit_scores + 0.3 * exploration_scores + 0.1 * usage_scores\n\n    # Ensure scores are within a reasonable range and handle potential NaNs/Infs\n    combined_scores = np.nan_to_num(combined_scores, nan=0.0, posinf=1.0, neginf=0.0)\n    combined_scores = np.clip(combined_scores, 0.0, 1.0)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a nuanced best-fit approach with an exploration bonus favoring less utilized bins,\n    and a subtle preference for bins with more overall remaining capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Modified Best Fit (from priority_v0)\n    # Prioritizes bins leaving a \"sweet spot\" residual capacity to avoid tiny unusable gaps.\n    remaining_after_placement_m1 = suitable_bins_caps - item\n    target_residual = item * 0.2  # Target residual capacity ~20% of item size\n    # Gaussian-like function: higher score for residuals closer to target_residual\n    # Add small epsilon to avoid division by zero if target_residual is 0.\n    best_fit_scores = np.exp(-((remaining_after_placement_m1 - target_residual) / (target_residual + 1e-6))**2)\n\n    # Metric 2: Exploration Bonus (inspired by priority_v1, simpler version)\n    # Favors bins that are less full *after* placement, relative to other suitable bins.\n    min_rem_after_m2 = np.min(remaining_after_placement_m1)\n    max_rem_after_m2 = np.max(remaining_after_placement_m1)\n    \n    exploration_scores = np.zeros_like(suitable_bins_caps)\n    if max_rem_after_m2 > min_rem_after_m2:\n        # Normalized remaining capacity after placement: higher for more empty bins\n        exploration_scores = (remaining_after_placement_m1 - min_rem_after_m2) / (max_rem_after_m2 - min_rem_after_m2)\n    else:\n        # If all suitable bins result in the same remaining capacity, no exploration bonus from this diff.\n        # Default to 0.5 for any such bins to avoid bias.\n        exploration_scores = np.ones_like(suitable_bins_caps) * 0.5\n\n    # Metric 3: Usage Proxy (favors bins with more total remaining capacity)\n    # This is a simple proxy for less-used bins.\n    min_cap_all = np.min(bins_remain_cap)\n    max_cap_all = np.max(bins_remain_cap)\n    \n    usage_scores = np.zeros_like(suitable_bins_caps)\n    if max_cap_all > min_cap_all:\n        # Normalize current remaining capacities. Higher score for more remaining capacity.\n        normalized_current_caps = (suitable_bins_caps - min_cap_all) / (max_cap_all - min_cap_all)\n        usage_scores = normalized_current_caps\n    else:\n        # If all bins have same capacity, this metric doesn't differentiate.\n        usage_scores = np.ones_like(suitable_bins_caps) * 0.5\n\n    # Combine scores: Heavy emphasis on nuanced best-fit, moderate on exploration, light on usage.\n    # Weights are chosen to balance finding good fits with spreading items.\n    # 0.6 for Best Fit (primary, quality of fit)\n    # 0.3 for Exploration (secondary, diversity)\n    # 0.1 for Usage (tertiary, simple preference for emptier bins)\n    combined_scores = 0.6 * best_fit_scores + 0.3 * exploration_scores + 0.1 * usage_scores\n\n    # Ensure scores are within a reasonable range and handle potential NaNs/Infs\n    combined_scores = np.nan_to_num(combined_scores, nan=0.0, posinf=1.0, neginf=0.0)\n    combined_scores = np.clip(combined_scores, 0.0, 1.0)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a nuanced best-fit approach with an exploration bonus favoring less utilized bins,\n    and a subtle preference for bins with more overall remaining capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Modified Best Fit (from priority_v0)\n    # Prioritizes bins leaving a \"sweet spot\" residual capacity to avoid tiny unusable gaps.\n    remaining_after_placement_m1 = suitable_bins_caps - item\n    target_residual = item * 0.2  # Target residual capacity ~20% of item size\n    # Gaussian-like function: higher score for residuals closer to target_residual\n    # Add small epsilon to avoid division by zero if target_residual is 0.\n    best_fit_scores = np.exp(-((remaining_after_placement_m1 - target_residual) / (target_residual + 1e-6))**2)\n\n    # Metric 2: Exploration Bonus (inspired by priority_v1, simpler version)\n    # Favors bins that are less full *after* placement, relative to other suitable bins.\n    min_rem_after_m2 = np.min(remaining_after_placement_m1)\n    max_rem_after_m2 = np.max(remaining_after_placement_m1)\n    \n    exploration_scores = np.zeros_like(suitable_bins_caps)\n    if max_rem_after_m2 > min_rem_after_m2:\n        # Normalized remaining capacity after placement: higher for more empty bins\n        exploration_scores = (remaining_after_placement_m1 - min_rem_after_m2) / (max_rem_after_m2 - min_rem_after_m2)\n    else:\n        # If all suitable bins result in the same remaining capacity, no exploration bonus from this diff.\n        # Default to 0.5 for any such bins to avoid bias.\n        exploration_scores = np.ones_like(suitable_bins_caps) * 0.5\n\n    # Metric 3: Usage Proxy (favors bins with more total remaining capacity)\n    # This is a simple proxy for less-used bins.\n    min_cap_all = np.min(bins_remain_cap)\n    max_cap_all = np.max(bins_remain_cap)\n    \n    usage_scores = np.zeros_like(suitable_bins_caps)\n    if max_cap_all > min_cap_all:\n        # Normalize current remaining capacities. Higher score for more remaining capacity.\n        normalized_current_caps = (suitable_bins_caps - min_cap_all) / (max_cap_all - min_cap_all)\n        usage_scores = normalized_current_caps\n    else:\n        # If all bins have same capacity, this metric doesn't differentiate.\n        usage_scores = np.ones_like(suitable_bins_caps) * 0.5\n\n    # Combine scores: Heavy emphasis on nuanced best-fit, moderate on exploration, light on usage.\n    # Weights are chosen to balance finding good fits with spreading items.\n    # 0.6 for Best Fit (primary, quality of fit)\n    # 0.3 for Exploration (secondary, diversity)\n    # 0.1 for Usage (tertiary, simple preference for emptier bins)\n    combined_scores = 0.6 * best_fit_scores + 0.3 * exploration_scores + 0.1 * usage_scores\n\n    # Ensure scores are within a reasonable range and handle potential NaNs/Infs\n    combined_scores = np.nan_to_num(combined_scores, nan=0.0, posinf=1.0, neginf=0.0)\n    combined_scores = np.clip(combined_scores, 0.0, 1.0)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a refined Best Fit with a dynamic Exploration bonus.\n    Prioritizes tight fits for larger items and exploration for smaller items,\n    adapting the strategy based on item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_caps - item\n\n    # Metric 1: Refined Best Fit - favors bins with smallest remaining capacity after placement.\n    # Using log1p to compress larger gaps and emphasize smaller ones. Add epsilon for stability.\n    best_fit_scores = np.log1p(1.0 / (remaining_after_placement + 1e-6))\n\n    # Metric 2: Exploration Bonus - favors bins that have significantly more remaining capacity.\n    # This is achieved by rewarding bins that are further from the minimum possible remaining capacity\n    # (after placing the item). Using min-max scaling on the remaining space after placement.\n    min_rem_after = np.min(remaining_after_placement)\n    max_rem_after = np.max(remaining_after_placement)\n\n    exploration_scores = np.zeros_like(remaining_after_placement)\n    if max_rem_after > min_rem_after:\n        # Normalize remaining capacity after placement to get exploration score (0 to 1)\n        exploration_scores = (remaining_after_placement - min_rem_after) / (max_rem_after - min_rem_after)\n    else:\n        # If all suitable bins leave the same remaining capacity, no exploration bonus from this metric.\n        pass\n\n    # Dynamic Weighting based on item size.\n    # Larger items benefit more from a precise fit (Best Fit).\n    # Smaller items can afford to explore less utilized bins (Exploration Bonus).\n    # Assume bin capacity is normalized to 1.0 for a relative item size assessment.\n    # If bin capacities vary significantly, a different normalization might be needed.\n    # For simplicity, we'll use item size directly, assuming it's scaled appropriately.\n    # Let's assume `item` is on a scale where 0.5 means it's half the typical bin capacity.\n    \n    # A simple heuristic: if item is more than 50% of typical capacity, prioritize best fit.\n    # If item is less than 20%, prioritize exploration. In between, a mix.\n    # Using item size as a proxy for its \"impact\" on bin fullness.\n    \n    # Weights sum to 1.0.\n    # For small items (e.g., item < 0.3): higher exploration, lower best fit.\n    # For large items (e.g., item > 0.7): higher best fit, lower exploration.\n    \n    # Example: Item size normalized to [0, 1] range, representing proportion of bin capacity.\n    # If actual item sizes are larger, they would need to be scaled.\n    # Let's assume `item` is already scaled relative to a standard bin capacity.\n\n    # Define a threshold, e.g., 0.5, for medium-sized items.\n    threshold_medium = 0.5 \n    \n    # Smooth transition for weights\n    weight_best_fit = np.clip(item / threshold_medium, 0.1, 1.0) # Favors best fit for larger items\n    weight_exploration = np.clip((threshold_medium - item) / threshold_medium, 0.1, 1.0) # Favors exploration for smaller items\n\n    # Normalize weights to ensure they sum to 1 if they cross thresholds or are outside bounds.\n    total_weight = weight_best_fit + weight_exploration\n    if total_weight > 1e-6:\n        weight_best_fit /= total_weight\n        weight_exploration /= total_weight\n    else: # Fallback if both are effectively zero\n        weight_best_fit = 0.5\n        weight_exploration = 0.5\n\n    # Combine scores\n    combined_scores = (weight_best_fit * best_fit_scores +\n                       weight_exploration * exploration_scores)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a refined Best Fit with a dynamic Exploration bonus.\n    Prioritizes tight fits for larger items and exploration for smaller items,\n    adapting the strategy based on item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_caps - item\n\n    # Metric 1: Refined Best Fit - favors bins with smallest remaining capacity after placement.\n    # Using log1p to compress larger gaps and emphasize smaller ones. Add epsilon for stability.\n    best_fit_scores = np.log1p(1.0 / (remaining_after_placement + 1e-6))\n\n    # Metric 2: Exploration Bonus - favors bins that have significantly more remaining capacity.\n    # This is achieved by rewarding bins that are further from the minimum possible remaining capacity\n    # (after placing the item). Using min-max scaling on the remaining space after placement.\n    min_rem_after = np.min(remaining_after_placement)\n    max_rem_after = np.max(remaining_after_placement)\n\n    exploration_scores = np.zeros_like(remaining_after_placement)\n    if max_rem_after > min_rem_after:\n        # Normalize remaining capacity after placement to get exploration score (0 to 1)\n        exploration_scores = (remaining_after_placement - min_rem_after) / (max_rem_after - min_rem_after)\n    else:\n        # If all suitable bins leave the same remaining capacity, no exploration bonus from this metric.\n        pass\n\n    # Dynamic Weighting based on item size.\n    # Larger items benefit more from a precise fit (Best Fit).\n    # Smaller items can afford to explore less utilized bins (Exploration Bonus).\n    # Assume bin capacity is normalized to 1.0 for a relative item size assessment.\n    # If bin capacities vary significantly, a different normalization might be needed.\n    # For simplicity, we'll use item size directly, assuming it's scaled appropriately.\n    # Let's assume `item` is on a scale where 0.5 means it's half the typical bin capacity.\n    \n    # A simple heuristic: if item is more than 50% of typical capacity, prioritize best fit.\n    # If item is less than 20%, prioritize exploration. In between, a mix.\n    # Using item size as a proxy for its \"impact\" on bin fullness.\n    \n    # Weights sum to 1.0.\n    # For small items (e.g., item < 0.3): higher exploration, lower best fit.\n    # For large items (e.g., item > 0.7): higher best fit, lower exploration.\n    \n    # Example: Item size normalized to [0, 1] range, representing proportion of bin capacity.\n    # If actual item sizes are larger, they would need to be scaled.\n    # Let's assume `item` is already scaled relative to a standard bin capacity.\n\n    # Define a threshold, e.g., 0.5, for medium-sized items.\n    threshold_medium = 0.5 \n    \n    # Smooth transition for weights\n    weight_best_fit = np.clip(item / threshold_medium, 0.1, 1.0) # Favors best fit for larger items\n    weight_exploration = np.clip((threshold_medium - item) / threshold_medium, 0.1, 1.0) # Favors exploration for smaller items\n\n    # Normalize weights to ensure they sum to 1 if they cross thresholds or are outside bounds.\n    total_weight = weight_best_fit + weight_exploration\n    if total_weight > 1e-6:\n        weight_best_fit /= total_weight\n        weight_exploration /= total_weight\n    else: # Fallback if both are effectively zero\n        weight_best_fit = 0.5\n        weight_exploration = 0.5\n\n    # Combine scores\n    combined_scores = (weight_best_fit * best_fit_scores +\n                       weight_exploration * exploration_scores)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a refined Best Fit with a dynamic Exploration bonus.\n    Prioritizes tight fits for larger items and exploration for smaller items,\n    adapting the strategy based on item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_caps - item\n\n    # Metric 1: Refined Best Fit - favors bins with smallest remaining capacity after placement.\n    # Using log1p to compress larger gaps and emphasize smaller ones. Add epsilon for stability.\n    best_fit_scores = np.log1p(1.0 / (remaining_after_placement + 1e-6))\n\n    # Metric 2: Exploration Bonus - favors bins that have significantly more remaining capacity.\n    # This is achieved by rewarding bins that are further from the minimum possible remaining capacity\n    # (after placing the item). Using min-max scaling on the remaining space after placement.\n    min_rem_after = np.min(remaining_after_placement)\n    max_rem_after = np.max(remaining_after_placement)\n\n    exploration_scores = np.zeros_like(remaining_after_placement)\n    if max_rem_after > min_rem_after:\n        # Normalize remaining capacity after placement to get exploration score (0 to 1)\n        exploration_scores = (remaining_after_placement - min_rem_after) / (max_rem_after - min_rem_after)\n    else:\n        # If all suitable bins leave the same remaining capacity, no exploration bonus from this metric.\n        pass\n\n    # Dynamic Weighting based on item size.\n    # Larger items benefit more from a precise fit (Best Fit).\n    # Smaller items can afford to explore less utilized bins (Exploration Bonus).\n    # Assume bin capacity is normalized to 1.0 for a relative item size assessment.\n    # If bin capacities vary significantly, a different normalization might be needed.\n    # For simplicity, we'll use item size directly, assuming it's scaled appropriately.\n    # Let's assume `item` is on a scale where 0.5 means it's half the typical bin capacity.\n    \n    # A simple heuristic: if item is more than 50% of typical capacity, prioritize best fit.\n    # If item is less than 20%, prioritize exploration. In between, a mix.\n    # Using item size as a proxy for its \"impact\" on bin fullness.\n    \n    # Weights sum to 1.0.\n    # For small items (e.g., item < 0.3): higher exploration, lower best fit.\n    # For large items (e.g., item > 0.7): higher best fit, lower exploration.\n    \n    # Example: Item size normalized to [0, 1] range, representing proportion of bin capacity.\n    # If actual item sizes are larger, they would need to be scaled.\n    # Let's assume `item` is already scaled relative to a standard bin capacity.\n\n    # Define a threshold, e.g., 0.5, for medium-sized items.\n    threshold_medium = 0.5 \n    \n    # Smooth transition for weights\n    weight_best_fit = np.clip(item / threshold_medium, 0.1, 1.0) # Favors best fit for larger items\n    weight_exploration = np.clip((threshold_medium - item) / threshold_medium, 0.1, 1.0) # Favors exploration for smaller items\n\n    # Normalize weights to ensure they sum to 1 if they cross thresholds or are outside bounds.\n    total_weight = weight_best_fit + weight_exploration\n    if total_weight > 1e-6:\n        weight_best_fit /= total_weight\n        weight_exploration /= total_weight\n    else: # Fallback if both are effectively zero\n        weight_best_fit = 0.5\n        weight_exploration = 0.5\n\n    # Combine scores\n    combined_scores = (weight_best_fit * best_fit_scores +\n                       weight_exploration * exploration_scores)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit (inverse residual) with a normalized exploration bonus\n    (log-transformed remaining capacity), balancing efficiency and spread.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit - inverse of remaining space after placement.\n    best_fit_scores = 1.0 / (suitable_bins_caps - item + 1e-9)\n\n    # Metric 2: Exploration - log-transformed remaining capacity to favor less utilized bins.\n    # Add 1 to avoid log(0) and provide a smoother bonus.\n    exploration_scores = np.log1p(suitable_bins_caps)\n\n    # Normalize exploration scores using min-max scaling.\n    min_exp_score = np.min(exploration_scores)\n    max_exp_score = np.max(exploration_scores)\n    if max_exp_score - min_exp_score > 1e-9:\n        normalized_exploration_scores = (exploration_scores - min_exp_score) / (max_exp_score - min_exp_score)\n    else:\n        normalized_exploration_scores = np.zeros_like(exploration_scores)\n\n    # Combine scores with a focus on Best Fit (0.7) and balanced exploration (0.3).\n    # This combination aims for efficient packing while encouraging better bin distribution.\n    combined_scores = 0.7 * best_fit_scores + 0.3 * normalized_exploration_scores\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with an exploration bonus favoring less utilized bins,\n    using logarithmic scaling for exploration to enhance bin distribution.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit (minimize remaining space after packing)\n    # Higher score for bins with less remaining space after packing the item.\n    # Adding a small epsilon to avoid division by zero.\n    tightness_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Metric 2: Exploration Bonus (favor less utilized bins)\n    # Logarithmic scaling of remaining capacity. Favors bins that are less full,\n    # encouraging a more even distribution of items across bins.\n    exploration_score = np.log(suitable_bins_remain_cap + 1e-9)\n\n    # Combine scores. Weighting favors tightness slightly, but exploration\n    # provides a bonus for less-used bins. These weights are subject to tuning.\n    combined_scores = 0.55 * tightness_score + 0.45 * exploration_score\n\n    # Normalize the combined scores to a [0, 1] range for consistent priority.\n    # Avoid division by zero if all combined scores are identical.\n    min_score = np.min(combined_scores)\n    max_score = np.max(combined_scores)\n    if max_score - min_score > 1e-9:\n        normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n    else:\n        normalized_scores = np.ones_like(combined_scores) * 0.5 # Default to mid-range if all scores are equal\n\n    priorities[suitable_bins_mask] = normalized_scores\n\n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with an exploration bonus favoring less utilized bins,\n    using logarithmic scaling for exploration to enhance bin distribution.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit (minimize remaining space after packing)\n    # Higher score for bins with less remaining space after packing the item.\n    # Adding a small epsilon to avoid division by zero.\n    tightness_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Metric 2: Exploration Bonus (favor less utilized bins)\n    # Logarithmic scaling of remaining capacity. Favors bins that are less full,\n    # encouraging a more even distribution of items across bins.\n    exploration_score = np.log(suitable_bins_remain_cap + 1e-9)\n\n    # Combine scores. Weighting favors tightness slightly, but exploration\n    # provides a bonus for less-used bins. These weights are subject to tuning.\n    combined_scores = 0.55 * tightness_score + 0.45 * exploration_score\n\n    # Normalize the combined scores to a [0, 1] range for consistent priority.\n    # Avoid division by zero if all combined scores are identical.\n    min_score = np.min(combined_scores)\n    max_score = np.max(combined_scores)\n    if max_score - min_score > 1e-9:\n        normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n    else:\n        normalized_scores = np.ones_like(combined_scores) * 0.5 # Default to mid-range if all scores are equal\n\n    priorities[suitable_bins_mask] = normalized_scores\n\n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a tight fit metric with an exploration bonus, favoring bins\n    that minimize remaining space while also considering less utilized bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Tight Fit (similar to priority_v0)\n    # Prioritize bins that leave minimal remaining space after packing.\n    # Add epsilon for numerical stability. Higher score for smaller remaining space.\n    tightness_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Metric 2: Exploration Bonus (inspired by priority_v0 and priority_v10/11/12)\n    # Favor bins that are less full (more remaining capacity).\n    # Using log1p for slightly better distribution at lower capacities.\n    # Higher score for bins with more remaining capacity.\n    exploration_score = np.log1p(suitable_bins_remain_cap)\n\n    # Combine scores with weights.\n    # Giving a slight edge to tightness, but exploration is also important.\n    # These weights can be tuned based on empirical performance.\n    combined_scores = 0.55 * tightness_score + 0.45 * exploration_score\n\n    # Normalize combined scores to a [0, 1] range.\n    # This ensures that the relative priorities are maintained even with different\n    # scales of the individual metrics. Handle cases where all scores are equal.\n    min_score = np.min(combined_scores)\n    max_score = np.max(combined_scores)\n    if max_score - min_score > 1e-9:\n        normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n    else:\n        normalized_scores = np.ones_like(combined_scores) * 0.5\n\n    priorities[suitable_bins_mask] = normalized_scores\n\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a tight fit metric with an exploration bonus, favoring bins\n    that minimize remaining space while also considering less utilized bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Tight Fit (similar to priority_v0)\n    # Prioritize bins that leave minimal remaining space after packing.\n    # Add epsilon for numerical stability. Higher score for smaller remaining space.\n    tightness_score = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Metric 2: Exploration Bonus (inspired by priority_v0 and priority_v10/11/12)\n    # Favor bins that are less full (more remaining capacity).\n    # Using log1p for slightly better distribution at lower capacities.\n    # Higher score for bins with more remaining capacity.\n    exploration_score = np.log1p(suitable_bins_remain_cap)\n\n    # Combine scores with weights.\n    # Giving a slight edge to tightness, but exploration is also important.\n    # These weights can be tuned based on empirical performance.\n    combined_scores = 0.55 * tightness_score + 0.45 * exploration_score\n\n    # Normalize combined scores to a [0, 1] range.\n    # This ensures that the relative priorities are maintained even with different\n    # scales of the individual metrics. Handle cases where all scores are equal.\n    min_score = np.min(combined_scores)\n    max_score = np.max(combined_scores)\n    if max_score - min_score > 1e-9:\n        normalized_scores = (combined_scores - min_score) / (max_score - min_score)\n    else:\n        normalized_scores = np.ones_like(combined_scores) * 0.5\n\n    priorities[suitable_bins_mask] = normalized_scores\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}