{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit (Refined) - Score based on the tightness of the fit.\n    # Using the reciprocal of the remaining capacity after placement.\n    # This naturally prioritizes bins where the remaining space is minimized.\n    # Add a small epsilon to avoid division by zero for perfect fits and to differentiate near-perfect fits.\n    remaining_after_placement = suitable_bins_caps - item\n    # Using 1 / (1 + remaining_capacity) to ensure positive scores and to slightly favor bins with less remaining space.\n    # This is similar to log1p but less sensitive to very small remaining spaces.\n    best_fit_scores = 1.0 / (1.0 + remaining_after_placement)\n\n    # Metric 2: Gap Exploitation - Reward bins that have a significant amount of remaining capacity\n    # but not so much that the item feels \"lost\".\n    # This metric aims to utilize larger \"gaps\" effectively without being overly greedy.\n    # We can score based on the ratio of item size to remaining capacity in the bin.\n    # A higher ratio means the item fills a larger portion of the remaining space, which is good for utilizing larger gaps.\n    # Add a small epsilon to the denominator to avoid division by zero.\n    gap_exploitation_scores = item / (suitable_bins_caps + 1e-6)\n    # Clip scores to avoid excessively high values if item is much larger than bin capacity (though this shouldn't happen with suitable_bins_mask).\n    gap_exploitation_scores = np.clip(gap_exploitation_scores, 0, 2.0) # Cap at 2.0 as a reasonable max ratio.\n\n\n    # Metric 3: Bin Fill Similarity - Aim to make bins have similar fill levels to promote better packing density.\n    # This means preferring bins that are already somewhat full, or bins where adding this item\n    # will bring its fill level closer to other partially filled bins.\n    # A proxy for \"already somewhat full\" is the inverse of remaining capacity relative to a baseline (e.g., max capacity, or average capacity).\n    # Let's normalize remaining capacity by the maximum possible capacity of *any* bin (assuming a global max capacity, e.g., 1.0 for normalized problems).\n    # If a global max is not available, we can use the maximum of all bins' initial capacities.\n    # For this example, let's assume a standard bin capacity of 1.0 as a reference.\n    # The \"fill level\" of a suitable bin would be (1.0 - remaining_capacity) / 1.0.\n    # We want to favor bins with fill levels that are not too close to 0 (very empty) and not too close to 1 (almost full, but not best fit).\n    # A Gaussian-like function centered around a moderate fill level (e.g., 0.6) is suitable.\n    # Let's use the inverse of remaining capacity relative to the item size itself. This measures how \"tight\" the current remaining space is relative to the item.\n    # If the remaining capacity is much larger than the item, the ratio is small, meaning the item is small relative to the space.\n    # If remaining capacity is close to item size, the ratio is near 1, meaning the item is a good fit for the *remaining* space.\n    # We want to favor bins where the item takes up a good fraction of the remaining space.\n    # A higher score for item / remaining_capacity (if remaining_capacity > item) is desired.\n    # This is essentially a variation of best fit, but focuses on the item's proportion of the *remaining* space.\n    # Let's re-think this: Bin Fill Similarity. We want bins to be filled to a similar degree *after* placement.\n    # This implies we want to pick bins that are not too empty and not too full.\n    # Let's consider the \"unused potential\" of a bin. This is the remaining capacity.\n    # We want to pick bins with *moderate* remaining capacity.\n    # Let's normalize the remaining capacity by the item's size. A ratio around 1 is ideal (best fit).\n    # For similarity, we want to avoid extreme remaining capacities.\n    # Consider `remaining_after_placement / item`. We want this to be moderate.\n    # A good heuristic is to penalize bins with very small or very large `remaining_after_placement`.\n    # Let's use `1 - exp(-k * (remaining_after_placement / item))` for values where remaining_after_placement > 0.\n    # A simpler approach: consider the 'fullness' of the bin.\n    # Fullness is approximately (BinCapacity - remaining_capacity) / BinCapacity.\n    # Let's use the current remaining capacity to infer a \"fill state\".\n    # Consider `(item / suitable_bins_caps)` as a measure of how much the item contributes to filling the *current gap*.\n    # A higher value here means the item is a larger portion of the available space, which can be good for utilizing larger gaps.\n    # Let's focus on making bins more \"balanced\".\n    # We can score bins based on how much they \"resemble\" an average fill level.\n    # Average fill level can be estimated by (TotalItemSize / NumberOfBins) / BinCapacity.\n    # For online, this is harder. Let's try to encourage bins to be moderately filled, not too empty, not too full.\n    # Use a score that is high for intermediate remaining capacities.\n    # Let's try a sigmoid-like function on the inverse of remaining capacity.\n    # The \"emptiness\" of the bin is roughly `suitable_bins_caps`.\n    # We want to penalize very small `suitable_bins_caps` (already full) and very large `suitable_bins_caps` (very empty).\n    # Let's use the inverse of remaining capacity scaled by item size.\n    # `suitable_bins_caps / item` ratio. We want this to be moderate.\n    # If `suitable_bins_caps / item` is very small, bin is almost full. If very large, bin is very empty.\n    # Let's use a bell-shaped curve centered around a desired ratio, say 2 (meaning remaining capacity is twice the item size).\n    # This encourages bins that are not too full, not too empty.\n    desired_ratio = 2.0\n    current_ratio = suitable_bins_caps / item\n    # Gaussian-like function: exp(-((ratio - desired_ratio)^2) / variance)\n    # Variance controls the width of the bell curve. A smaller variance means we are pickier.\n    variance = 1.0 # Tunable parameter\n    bin_fill_similarity_scores = np.exp(-((current_ratio - desired_ratio)**2) / (2 * variance))\n\n\n    # Normalize scores to be in a comparable range [0, 1] for combining.\n    # Avoid division by zero if all scores for a metric are zero.\n    max_best_fit = np.max(best_fit_scores)\n    normalized_best_fit = best_fit_scores / max_best_fit if max_best_fit > 1e-6 else np.zeros_like(best_fit_scores)\n\n    max_gap_exploitation = np.max(gap_exploitation_scores)\n    normalized_gap_exploitation = gap_exploitation_scores / max_gap_exploitation if max_gap_exploitation > 1e-6 else np.zeros_like(gap_exploitation_scores)\n\n    max_bin_fill_similarity = np.max(bin_fill_similarity_scores)\n    normalized_bin_fill_similarity = bin_fill_similarity_scores / max_bin_fill_similarity if max_bin_fill_similarity > 1e-6 else np.zeros_like(bin_fill_similarity_scores)\n\n    # Combine scores with dynamic weights.\n    # The weights should adapt based on the item's size relative to the *average* remaining capacity\n    # or the *maximum* remaining capacity among suitable bins.\n    # Let's use the maximum remaining capacity as a reference for \"how open\" the bins are.\n    max_suitable_cap = np.max(suitable_bins_caps)\n\n    # If item is large relative to max suitable capacity, prioritize best fit.\n    # If item is small relative to max suitable capacity, prioritize gap exploitation and fill similarity.\n    # Define a threshold for \"large\" item. Let's use 0.5 * max_suitable_cap.\n    # Normalize item size by max_suitable_cap to get a relative size.\n    relative_item_size = item / (max_suitable_cap + 1e-6)\n\n    # Weighting scheme:\n    # Best Fit is always important, especially for larger items.\n    # Gap Exploitation is good for items that can fill up larger available spaces.\n    # Bin Fill Similarity aims for balanced bins.\n\n    # Weight for Best Fit: increases with item size relative to available space.\n    weight_best_fit = 0.4 + 0.5 * relative_item_size\n    weight_best_fit = np.clip(weight_best_fit, 0.4, 0.9) # Ensure it's not too dominant for small items.\n\n    # Weight for Gap Exploitation: decreases with item size, favors smaller items filling larger gaps.\n    weight_gap_exploitation = 0.4 - 0.3 * relative_item_size\n    weight_gap_exploitation = np.clip(weight_gap_exploitation, 0.1, 0.4)\n\n    # Weight for Bin Fill Similarity: generally useful, moderate weight.\n    weight_bin_fill_similarity = 0.2 # Constant or slightly adjusted\n\n    # Ensure weights sum to 1 (approximately, or re-normalize if needed)\n    # A simpler approach for weights that sum to 1:\n    total_weight = weight_best_fit + weight_gap_exploitation + weight_bin_fill_similarity\n    # If total_weight is 0 (unlikely here), set to 1.\n    if total_weight < 1e-6:\n        total_weight = 1.0\n\n    weight_best_fit /= total_weight\n    weight_gap_exploitation /= total_weight\n    weight_bin_fill_similarity /= total_weight\n\n\n    combined_scores = (weight_best_fit * normalized_best_fit +\n                       weight_gap_exploitation * normalized_gap_exploitation +\n                       weight_bin_fill_similarity * normalized_bin_fill_similarity)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines a refined Best Fit with a dynamic Exploration bonus.\n    Prioritizes tight fits for larger items and exploration for smaller items,\n    adapting the strategy based on item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_caps - item\n\n    # Metric 1: Refined Best Fit - favors bins with smallest remaining capacity after placement.\n    # Using log1p to compress larger gaps and emphasize smaller ones. Add epsilon for stability.\n    best_fit_scores = np.log1p(1.0 / (remaining_after_placement + 1e-6))\n\n    # Metric 2: Exploration Bonus - favors bins that have significantly more remaining capacity.\n    # This is achieved by rewarding bins that are further from the minimum possible remaining capacity\n    # (after placing the item). Using min-max scaling on the remaining space after placement.\n    min_rem_after = np.min(remaining_after_placement)\n    max_rem_after = np.max(remaining_after_placement)\n\n    exploration_scores = np.zeros_like(remaining_after_placement)\n    if max_rem_after > min_rem_after:\n        # Normalize remaining capacity after placement to get exploration score (0 to 1)\n        exploration_scores = (remaining_after_placement - min_rem_after) / (max_rem_after - min_rem_after)\n    else:\n        # If all suitable bins leave the same remaining capacity, no exploration bonus from this metric.\n        pass\n\n    # Dynamic Weighting based on item size.\n    # Larger items benefit more from a precise fit (Best Fit).\n    # Smaller items can afford to explore less utilized bins (Exploration Bonus).\n    # Assume bin capacity is normalized to 1.0 for a relative item size assessment.\n    # If bin capacities vary significantly, a different normalization might be needed.\n    # For simplicity, we'll use item size directly, assuming it's scaled appropriately.\n    # Let's assume `item` is on a scale where 0.5 means it's half the typical bin capacity.\n    \n    # A simple heuristic: if item is more than 50% of typical capacity, prioritize best fit.\n    # If item is less than 20%, prioritize exploration. In between, a mix.\n    # Using item size as a proxy for its \"impact\" on bin fullness.\n    \n    # Weights sum to 1.0.\n    # For small items (e.g., item < 0.3): higher exploration, lower best fit.\n    # For large items (e.g., item > 0.7): higher best fit, lower exploration.\n    \n    # Example: Item size normalized to [0, 1] range, representing proportion of bin capacity.\n    # If actual item sizes are larger, they would need to be scaled.\n    # Let's assume `item` is already scaled relative to a standard bin capacity.\n\n    # Define a threshold, e.g., 0.5, for medium-sized items.\n    threshold_medium = 0.5 \n    \n    # Smooth transition for weights\n    weight_best_fit = np.clip(item / threshold_medium, 0.1, 1.0) # Favors best fit for larger items\n    weight_exploration = np.clip((threshold_medium - item) / threshold_medium, 0.1, 1.0) # Favors exploration for smaller items\n\n    # Normalize weights to ensure they sum to 1 if they cross thresholds or are outside bounds.\n    total_weight = weight_best_fit + weight_exploration\n    if total_weight > 1e-6:\n        weight_best_fit /= total_weight\n        weight_exploration /= total_weight\n    else: # Fallback if both are effectively zero\n        weight_best_fit = 0.5\n        weight_exploration = 0.5\n\n    # Combine scores\n    combined_scores = (weight_best_fit * best_fit_scores +\n                       weight_exploration * exploration_scores)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1 and 2 (identical): They implement three metrics: Best Fit, Gap Exploitation, and Bin Fill Similarity, with dynamic weighting based on item size relative to max suitable capacity. The weights adapt to prioritize Best Fit for larger items and Gap Exploitation/Fill Similarity for smaller ones. Normalization is applied to each metric before combining.\n\nComparing Heuristics 3 and 4: Heuristic 3 combines Best Fit (tightness) with Exploration (larger initial capacity) using a fixed weighted sum. Heuristic 4 combines Best Fit (negative remaining capacity) with Exploration (normalized remaining capacity) using a weighted sum, favoring Best Fit. Heuristic 3 normalizes component scores via min-max scaling, while Heuristic 4 uses direct combination.\n\nComparing Heuristics 5, 6, 7, 8 (identical): These heuristics also use Best Fit (relative remaining capacity), Bin Fullness (inverse of remaining capacity), and Item Size Ratio. They employ dynamic weighting based on the item's size relative to the average suitable bin capacity. The weights adapt to prioritize Best Fit and Fullness for larger items, and Item Ratio/Fullness for smaller items. Normalization is applied to each metric before weighted combination.\n\nComparing Heuristics 9 and 10, 11, 12 (identical): Heuristic 9 combines Best Fit (tightness) with a fairness penalty (deviation from average remaining capacity). Heuristics 10-12 combine Modified Best Fit (sweet spot residual), Exploration (normalized remaining capacity after placement), and Usage Proxy (normalized current remaining capacity) with fixed weights.\n\nComparing Heuristics 13, 14, 15 (identical): These combine Refined Best Fit (log1p of inverse residual) with Exploration (min-max scaled remaining capacity after placement), using dynamic weights based on item size relative to a threshold.\n\nComparing Heuristics 16, 17, 18, 19, 20 (identical): These combine Best Fit (inverse residual) with Exploration (log1p or log of remaining capacity). They use fixed weights and normalize the combined score. Heuristics 17-18 have slightly different weights (0.55/0.45) than 19-20 (0.55/0.45) and 16 (0.7/0.3) for Best Fit vs. Exploration.\n\nOverall: The best heuristics (1-8) tend to use multiple metrics and adapt their weighting dynamically based on item characteristics or bin states, often involving normalization of individual metrics or the final combined score. Simpler fixed-weight combinations of Best Fit and Exploration (like 16-20) are less sophisticated but might be more robust. The use of logarithmic or Gaussian-like functions for scoring can provide smoother preference curves.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Metric selection, dynamic weighting, normalization, balance, robustness, simplicity.\n*   **Advice:** Focus on a core set of well-defined, interpretable metrics. Systematically explore dynamic weighting and robust normalization techniques. Aim for a balance between primary objectives and secondary goals like diversification or exploration.\n*   **Avoid:** Overly complex or opaque mathematical transformations, unstable calculations, and prioritizing single, overly simplistic metrics.\n*   **Explanation:** Effective self-reflection identifies *how* and *why* certain metric combinations and adjustments lead to improved performance by understanding their impact on the problem's core objectives.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}