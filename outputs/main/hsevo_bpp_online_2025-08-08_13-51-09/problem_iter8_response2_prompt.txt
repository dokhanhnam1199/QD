{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit decreasing heuristic logic.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    suitable_bins_mask = bins_remain_cap >= item\n    priorities[suitable_bins_mask] = bins_remain_cap[suitable_bins_mask] - item\n    best_fit_bin_index = np.argmin(priorities)\n    priorities[:] = 0\n    priorities[best_fit_bin_index] = 1\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    \n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    \n    if suitable_bins_cap.size == 0:\n        return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Best) vs Heuristic 2 (Worst): Heuristic 1 calculates complex priority scores considering tightness and consolidation, aiming for nuanced bin selection. Heuristic 2 directly implements a Best Fit strategy by setting the priority of the single best-fit bin to 1 and others to 0, which is a deterministic choice rather than a nuanced priority.\n\nComparing Heuristic 1 vs Heuristic 3: Heuristic 1 focuses on minimizing remaining space and encouraging consolidation with a weighted sum. Heuristic 3 also combines \"Best Fit\" with favoring larger bins, but its normalization approach and combination might be less robust than Heuristic 1's focus on tightness.\n\nComparing Heuristic 3 vs Heuristic 4 (identical): Both use \"Best Fit\" and \"favor larger bins\" with similar normalization and weighting.\n\nComparing Heuristic 3 vs Heuristic 5 (identical): Same as above.\n\nComparing Heuristic 5 vs Heuristic 6: Heuristic 5 combines \"Best Fit\" with \"favor larger bins\" using normalization and weights. Heuristic 6 uses \"tight fit\" and \"diversification bonus\" with a simpler inverse scaling for both, and normalizes the final combined scores. Heuristic 6's approach to diversification using inverse remaining capacity is a good addition.\n\nComparing Heuristic 6 vs Heuristic 7 (similar to 1): Heuristic 7 is very similar to Heuristic 1, focusing on tightness and consolidation. Heuristic 6's \"diversification\" metric offers a different angle.\n\nComparing Heuristic 7 vs Heuristic 8 (identical): These are identical to Heuristic 1.\n\nComparing Heuristic 8 vs Heuristic 9: Heuristic 8 (and its duplicates) focuses on tightness and consolidation. Heuristic 9 combines \"Best Fit\" with a sigmoid penalty for large remaining capacities, offering a smoother penalty. The sigmoid approach in Heuristic 9 is an interesting variation for managing large remaining spaces.\n\nComparing Heuristic 9 vs Heuristic 10: Heuristic 9 uses a sigmoid penalty. Heuristic 10 balances \"Best Fit\" with \"emptiness\" using a log transformation, aiming to distribute items more evenly. The log transformation for emptiness in Heuristic 10 is a novel approach for diversification.\n\nComparing Heuristic 10 vs Heuristic 11 (identical): Both balance \"Best Fit\" and \"emptiness\" using log transformation.\n\nComparing Heuristic 11 vs Heuristic 12 (identical): Same as above.\n\nComparing Heuristic 12 vs Heuristic 13 (incomplete): Heuristic 13 is cut off and only initializes variables without logic.\n\nComparing Heuristic 13 vs Heuristic 14 (identical, incomplete): Same as Heuristic 13.\n\nComparing Heuristic 14 vs Heuristic 15: Heuristic 15 combines \"Best Fit\" (tight packing) with an \"exploration bonus\" using normalized remaining capacity. It uses a reciprocal for tight fit and min-max scaling for exploration. This is a more refined version of balancing fit and exploration.\n\nComparing Heuristic 15 vs Heuristic 16: Heuristic 16 also combines \"Best Fit\" and an \"exploration bonus,\" using inverse for best fit and min-max scaling of the bin capacity itself for exploration. Heuristic 15's exploration bonus is based on remaining capacity after placement, while Heuristic 16's is based on the absolute remaining capacity of suitable bins. Heuristic 16's use of `-np.inf` initialization is also a good practice for clarity.\n\nComparing Heuristic 16 vs Heuristic 17 (identical): Same as above.\n\nComparing Heuristic 17 vs Heuristic 18: Heuristic 18 is a more refined version of 16/17, with clearer variable naming and explicit handling of the single suitable bin case for exploration bonus. The normalization for exploration bonus is correctly applied to the capacity itself, not remaining after placement.\n\nComparing Heuristic 18 vs Heuristic 19: Heuristic 18 combines best-fit (inverse difference) with an exploration bonus based on normalized bin capacity. Heuristic 19 uses a weighted sum of best-fit (inverse difference) and an exploration bonus derived from min-max scaling of remaining capacity. Heuristic 19's exploration bonus based on remaining capacity is a better proxy for \"less utilized\" than absolute capacity.\n\nComparing Heuristic 19 vs Heuristic 20 (identical): Both combine best-fit and exploration bonus on remaining capacity with weighted sum.\n\nOverall: The best heuristics (1, 7, 8) focus on minimizing remaining space and encouraging consolidation. The next tier (3, 5, 6, 9, 10, 11, 12, 15, 19, 20) blend \"Best Fit\" with diversification/exploration strategies using various normalization and transformation techniques. Heuristics 2, 13, 14 are either too simplistic or incomplete. Heuristics 16, 17, 18 use a good approach for exploration bonus on bin capacity but might be slightly less nuanced than using remaining capacity after fit.\n- \nHere's a redefined approach to self-reflection for designing better heuristics, focusing on avoiding ineffective practices:\n\n*   **Keywords:** Objective balancing, nuanced scoring, robust metrics, avoiding simplicity.\n*   **Advice:** Focus on how multiple objectives (primary/secondary) interact. Develop score combinations that reflect these interactions, not just additive weighting. Ensure metrics are sensitive to problem specifics.\n*   **Avoid:** Sigmoid/inverse functions *solely* for scaling without justification. Blindly prioritizing \"tight fit\" over exploration. Over-reliance on simplistic, single-metric evaluations.\n*   **Explanation:** The goal is to create heuristics that learn and adapt by understanding trade-offs, not by applying pre-defined, inflexible transformations. Complex interactions between objectives require thoughtful metric design and combination.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}