[
  {
    "stdout_filepath": "problem_iter14_response0.txt_stdout.txt",
    "code_path": "problem_iter14_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a refined best-fit metric that penalizes both extreme remaining capacities\n    with a dynamic weighting strategy that adapts to item size relative to bin availability.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    # Metric 1: Refined Best Fit (using log of inverse residual)\n    # Penalizes bins that are too full (small residual) or too empty (large residual) after placement.\n    remaining_after_placement = suitable_bins_caps - item\n    # Use log1p of the inverse of remaining capacity + epsilon. Higher score for residuals closer to 0.\n    # Adding 1 to the denominator ensures that even for perfect fits (residual=0), we don't get division by zero.\n    # A small epsilon is added to the denominator to prevent division by zero if remaining_after_placement is 0.\n    best_fit_scores = np.log1p(1.0 / (remaining_after_placement + 1e-6))\n\n    # Metric 2: Exploration/Spread (using log of current remaining capacity)\n    # Favors bins with larger initial remaining capacities, promoting spreading items.\n    # Use log1p of remaining capacity. Higher score for larger remaining capacities.\n    exploration_scores = np.log1p(suitable_bins_caps)\n\n    # Normalize scores to be in a comparable range [0, 1] for combining.\n    # Avoid division by zero if all scores for a metric are zero.\n    max_best_fit = np.max(best_fit_scores)\n    normalized_best_fit = best_fit_scores / max_best_fit if max_best_fit > 1e-6 else np.zeros_like(best_fit_scores)\n\n    max_exploration = np.max(exploration_scores)\n    normalized_exploration = exploration_scores / max_exploration if max_exploration > 1e-6 else np.zeros_like(exploration_scores)\n\n    # Dynamic Weighting: Adjust weights based on item size relative to the maximum remaining capacity of suitable bins.\n    # This aims to balance \"best fit\" for larger items with \"exploration\" for smaller items.\n    max_suitable_cap = np.max(suitable_bins_caps)\n    relative_item_size = item / (max_suitable_cap + 1e-6)\n\n    # Weight for Best Fit: Higher for larger items, lower for smaller items.\n    # This ensures that for large items, we prioritize a tight fit.\n    weight_best_fit = 0.5 + 0.4 * relative_item_size\n    weight_best_fit = np.clip(weight_best_fit, 0.5, 0.9)\n\n    # Weight for Exploration: Lower for larger items, higher for smaller items.\n    # This encourages spreading smaller items into less utilized bins.\n    weight_exploration = 0.5 - 0.4 * relative_item_size\n    weight_exploration = np.clip(weight_exploration, 0.1, 0.5)\n\n    # Ensure weights sum to 1\n    total_weight = weight_best_fit + weight_exploration\n    if total_weight < 1e-6:\n        total_weight = 1.0\n    \n    weight_best_fit /= total_weight\n    weight_exploration /= total_weight\n\n    combined_scores = (weight_best_fit * normalized_best_fit +\n                       weight_exploration * normalized_exploration)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 28.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response1.txt_stdout.txt",
    "code_path": "problem_iter14_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A hybrid heuristic combining Best Fit (tightness) with dynamic exploration,\n    adapting weights based on item size to balance packing efficiency and bin utilization.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_caps - item\n\n    # Metric 1: Refined Best Fit - favors bins with smallest remaining capacity after placement.\n    # Logarithmic scaling to compress larger gaps and emphasize smaller ones. Add epsilon for stability.\n    best_fit_scores = np.log1p(1.0 / (remaining_after_placement + 1e-6))\n\n    # Metric 2: Exploration Bonus - favors bins with more remaining capacity.\n    # Min-max scaling to normalize the exploration score between 0 and 1.\n    min_rem_after = np.min(remaining_after_placement)\n    max_rem_after = np.max(remaining_after_placement)\n    \n    exploration_scores = np.zeros_like(remaining_after_placement)\n    if max_rem_after > min_rem_after:\n        exploration_scores = (remaining_after_placement - min_rem_after) / (max_rem_after - min_rem_after)\n    elif suitable_bins_caps.size > 0: # If all suitable bins have same remaining space, give equal exploration bonus if any\n        exploration_scores = np.ones_like(remaining_after_placement) * 0.5\n\n    # Dynamic Weighting based on item size relative to a conceptual bin capacity of 1.0.\n    # This aims to use Best Fit more for larger items and Exploration for smaller ones.\n    # A smoother transition using a sigmoid-like approach or clipping.\n    # Assume item size is already normalized or scaled appropriately.\n    \n    # Define a soft transition point (e.g., 0.5 for half-full bins)\n    transition_point = 0.5\n    \n    # Calculate weights: higher weight for best_fit for larger items, higher for exploration for smaller items.\n    # Use clipping to ensure weights stay within a reasonable range and sum approximately to 1.\n    weight_best_fit = np.clip(item / transition_point, 0.2, 0.9)\n    weight_exploration = 1.0 - weight_best_fit\n\n    # Ensure weights are sensible and sum to 1\n    total_weight = weight_best_fit + weight_exploration\n    if total_weight > 1e-6:\n        weight_best_fit /= total_weight\n        weight_exploration /= total_weight\n    else: # Fallback if weights are zero\n        weight_best_fit = 0.5\n        weight_exploration = 0.5\n\n    # Combine scores using dynamic weights\n    combined_scores = (weight_best_fit * best_fit_scores +\n                       weight_exploration * exploration_scores)\n\n    # Assign combined scores to the priorities array\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 73.7534902273634,
    "SLOC": 29.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response2.txt_stdout.txt",
    "code_path": "problem_iter14_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (tightness) with a \"Gap Fill Ratio\" metric.\n    Dynamic weights favor Best Fit for items that are a significant portion\n    of the remaining space, and Gap Fill Ratio for items that are smaller\n    relative to the available space.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_caps - item\n\n    # Metric 1: Best Fit - Score based on the tightness of the fit.\n    # Using the reciprocal of (1 + remaining capacity after placement)\n    # This prioritizes bins where the remaining space is minimized.\n    best_fit_scores = 1.0 / (1.0 + remaining_after_placement)\n\n    # Metric 2: Gap Fill Ratio - Score based on how much the item fills the *current available space*.\n    # High ratio means item is a large fraction of available space, good for utilizing larger gaps.\n    # Add epsilon for stability.\n    gap_fill_ratio_scores = item / (suitable_bins_caps + 1e-6)\n    # Clip to avoid extreme values and ensure scores are somewhat bounded.\n    gap_fill_ratio_scores = np.clip(gap_fill_ratio_scores, 0, 2.0)\n\n    # Normalize scores to [0, 1] for combination. Avoid division by zero.\n    max_best_fit = np.max(best_fit_scores)\n    normalized_best_fit = best_fit_scores / max_best_fit if max_best_fit > 1e-6 else np.zeros_like(best_fit_scores)\n\n    max_gap_fill = np.max(gap_fill_ratio_scores)\n    normalized_gap_fill = gap_fill_ratio_scores / max_gap_fill if max_gap_fill > 1e-6 else np.zeros_like(gap_fill_ratio_scores)\n\n    # Dynamic Weighting:\n    # Aim to balance between fitting tightly (Best Fit) and utilizing larger gaps effectively (Gap Fill Ratio).\n    # If an item is large relative to the available space in a bin, Best Fit becomes more important.\n    # If an item is small relative to the available space, the Gap Fill Ratio (how much it contributes to filling that gap) is more relevant.\n\n    # Consider the ratio of the item size to the maximum remaining capacity among suitable bins.\n    # This gives a sense of whether the item is \"large\" or \"small\" compared to the best available space.\n    max_suitable_cap = np.max(suitable_bins_caps)\n    relative_item_size = item / (max_suitable_cap + 1e-6)\n\n    # Weighting scheme:\n    # For larger relative items (closer to 1), prioritize Best Fit.\n    # For smaller relative items (closer to 0), prioritize Gap Fill Ratio.\n    # Use a sigmoid-like shape for smooth transition.\n\n    # Weight for Best Fit: increases with relative item size.\n    # Range: [0.4, 0.9]\n    weight_best_fit = 0.4 + 0.5 * relative_item_size\n    weight_best_fit = np.clip(weight_best_fit, 0.4, 0.9)\n\n    # Weight for Gap Fill Ratio: decreases with relative item size.\n    # Range: [0.1, 0.6]\n    weight_gap_fill = 0.6 - 0.5 * relative_item_size\n    weight_gap_fill = np.clip(weight_gap_fill, 0.1, 0.6)\n\n    # Ensure weights sum to 1.\n    total_weight = weight_best_fit + weight_gap_fill\n    if total_weight > 1e-6:\n        weight_best_fit /= total_weight\n        weight_gap_fill /= total_weight\n    else: # Fallback if total_weight is near zero\n        weight_best_fit = 0.5\n        weight_gap_fill = 0.5\n\n    combined_scores = (weight_best_fit * normalized_best_fit +\n                       weight_gap_fill * normalized_gap_fill)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 31.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response3.txt_stdout.txt",
    "code_path": "problem_iter14_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a nuanced 'sweet spot' best-fit with exploration favoring less-used bins,\n    weighted to prioritize good fits while encouraging diversification.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    # Metric 1: Nuanced Best Fit - Prioritize bins leaving a \"sweet spot\" of residual capacity.\n    # This helps avoid very small unusable gaps. Using a Gaussian-like shape.\n    remaining_after_placement = suitable_bins_caps - item\n    # Define a target residual capacity, e.g., 20% of the item size, to aim for.\n    target_residual = item * 0.2\n    # Score is highest when remaining_after_placement is close to target_residual.\n    # Adding a small epsilon to the denominator for numerical stability, especially if target_residual is 0.\n    best_fit_scores = np.exp(-((remaining_after_placement - target_residual) / (target_residual + 1e-6))**2)\n\n    # Metric 2: Exploration/Less Used Bin Preference - Favor bins that are less full *after* placement,\n    # relative to other suitable bins. This encourages spreading items.\n    # Calculate the range of remaining capacities after placing the item in suitable bins.\n    min_rem_after_m2 = np.min(remaining_after_placement)\n    max_rem_after_m2 = np.max(remaining_after_placement)\n    \n    exploration_scores = np.zeros_like(suitable_bins_caps)\n    if max_rem_after_m2 > min_rem_after_m2:\n        # Normalize remaining capacity after placement. Higher score for more empty bins.\n        # This is a form of min-max scaling for the post-placement residual.\n        exploration_scores = (remaining_after_placement - min_rem_after_m2) / (max_rem_after_m2 - min_rem_after_m2)\n    elif suitable_bins_caps.size > 0:\n        # If all suitable bins result in the same remaining capacity, this metric doesn't differentiate.\n        # Assign a neutral score (e.g., 0.5) to avoid bias.\n        exploration_scores = np.ones_like(suitable_bins_caps) * 0.5\n\n    # Combine scores with a weighted sum.\n    # Give a strong weight to the nuanced best-fit as it directly impacts packing efficiency.\n    # Give a moderate weight to exploration to balance against potential fragmentation.\n    # Weights are chosen to prioritize good fits while still allowing for diversification.\n    # Example weights: 0.7 for Best Fit, 0.3 for Exploration.\n    combined_scores = 0.7 * best_fit_scores + 0.3 * exploration_scores\n    \n    # Ensure scores are within a reasonable range and handle potential NaNs/Infs.\n    # NaN values can occur if all suitable bins have the same remaining capacity after placement\n    # and the denominator in exploration_scores becomes zero.\n    combined_scores = np.nan_to_num(combined_scores, nan=0.5, posinf=1.0, neginf=0.0)\n    # Clip scores to [0, 1] to maintain a consistent range.\n    combined_scores = np.clip(combined_scores, 0.0, 1.0)\n\n    priorities[suitable_bins_mask] = combined_scores\n    \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 37.654567211806956,
    "SLOC": 21.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response4.txt_stdout.txt",
    "code_path": "problem_iter14_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit's tightness with an exploration bonus favoring less utilized bins,\n    using a balanced approach with dynamic weighting based on item size.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return np.zeros_like(bins_remain_cap)\n\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    # Best Fit Component: Prioritize bins with minimal remaining capacity after placement.\n    # Use inverse of remaining capacity for higher scores for tighter fits.\n    best_fit_scores = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n\n    # Exploration Component: Favor bins with larger original capacity (less utilized).\n    # Normalize remaining capacities of suitable bins using min-max scaling.\n    min_cap = np.min(suitable_bins_remain_cap)\n    max_cap = np.max(suitable_bins_remain_cap)\n    if max_cap - min_cap > 1e-9:\n        exploration_scores = (suitable_bins_remain_cap - min_cap) / (max_cap - min_cap)\n    else:\n        exploration_scores = np.zeros_like(suitable_bins_remain_cap)\n        \n    # Dynamic Weighting: Adjust weights based on item size relative to max suitable capacity.\n    # For larger items, lean more towards Best Fit. For smaller items, give more weight to exploration.\n    max_suitable_cap = np.max(suitable_bins_remain_cap)\n    weight_bf = 0.5 + 0.5 * (item / max_suitable_cap) if max_suitable_cap > 1e-9 else 0.5\n    weight_exp = 1.0 - weight_bf\n    \n    # Combine scores with dynamic weights. Higher combined scores indicate better bins.\n    combined_scores = weight_bf * best_fit_scores + weight_exp * exploration_scores\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 83.51615476665337,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response5.txt_stdout.txt",
    "code_path": "problem_iter14_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit and Fullness Prioritization with adaptive weighting.\n    This heuristic balances finding a snug fit for the item with utilizing fuller bins,\n    adjusting emphasis based on the item's size relative to available capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit (Minimize residual capacity)\n    # Score is inversely proportional to the remaining capacity after placement.\n    # Higher score for bins that leave less space.\n    remaining_after_placement = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-6)\n\n    # Metric 2: Bin Fullness Prioritization (Favor fuller bins)\n    # Score is inversely proportional to the bin's current remaining capacity.\n    # Higher score for bins that are more full.\n    fullness_scores = 1.0 / (suitable_bins_caps + 1e-6)\n\n    # --- Adaptive Weighting ---\n    # Determine the context: item size relative to average suitable bin capacity.\n    avg_suitable_cap = np.mean(suitable_bins_caps)\n    if avg_suitable_cap > 1e-9:\n        item_vs_avg_cap_ratio = item / avg_suitable_cap\n    else:\n        item_vs_avg_cap_ratio = 0.5 # Default if no suitable bins or very small capacity\n\n    # Base weights: BF for tight fit, F for consolidation.\n    w_bf = 0.6\n    w_f = 0.4\n\n    # Adjust weights dynamically:\n    # If item is relatively large, prioritize Best Fit more.\n    # If item is relatively small, give more weight to Fullness to encourage consolidation.\n    if item_vs_avg_cap_ratio > 1.0: # Item is larger than average remaining capacity\n        w_bf += 0.2 * (item_vs_avg_cap_ratio - 1.0)\n        w_f -= 0.2 * (item_vs_avg_cap_ratio - 1.0)\n    else: # Item is smaller than average remaining capacity\n        w_f += 0.2 * (1.0 - item_vs_avg_cap_ratio)\n        w_bf -= 0.2 * (1.0 - item_vs_avg_cap_ratio)\n\n    # Ensure weights remain valid and sum to 1.\n    w_bf = max(0, w_bf)\n    w_f = max(0, w_f)\n    total_w = w_bf + w_f\n    if total_w > 1e-9:\n        w_bf /= total_w\n        w_f /= total_w\n    else: # Fallback if weights become zero\n        w_bf, w_f = 0.5, 0.5\n\n    # --- Normalization and Combination ---\n    # Normalize scores to be in a comparable range [0, 1]\n    # Normalize Best Fit scores: higher score is better.\n    if np.max(best_fit_scores) > 1e-9:\n        norm_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        norm_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Fullness scores: higher score is better.\n    if np.max(fullness_scores) > 1e-9:\n        norm_fullness = fullness_scores / np.max(fullness_scores)\n    else:\n        norm_fullness = np.zeros_like(fullness_scores)\n\n    # Combine normalized scores with dynamic weights.\n    combined_scores = (w_bf * norm_best_fit + w_f * norm_fullness)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 41.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response6.txt_stdout.txt",
    "code_path": "problem_iter14_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit and Bin Fullness metrics with adaptive weights.\n    Prioritizes tighter fits for larger items and fuller bins for all items,\n    while normalizing scores for balanced contribution.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit (minimize remaining space after packing)\n    # Higher score for bins with less remaining space after placing the item.\n    remaining_after_placement = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-9)\n\n    # Metric 2: Bin Fullness (prioritize fuller bins)\n    # Higher score for bins that are already more full (less remaining capacity).\n    fullness_scores = 1.0 / (suitable_bins_caps + 1e-9)\n\n    # Normalize scores to ensure comparability between metrics.\n    # Max-min normalization is applied to each metric independently.\n    if np.max(best_fit_scores) > 1e-9:\n        norm_best_fit = (best_fit_scores - np.min(best_fit_scores)) / (np.max(best_fit_scores) - np.min(best_fit_scores) + 1e-9)\n    else:\n        norm_best_fit = np.zeros_like(best_fit_scores)\n\n    if np.max(fullness_scores) > 1e-9:\n        norm_fullness = (fullness_scores - np.min(fullness_scores)) / (np.max(fullness_scores) - np.min(fullness_scores) + 1e-9)\n    else:\n        norm_fullness = np.zeros_like(fullness_scores)\n\n    # Adaptive Weighting: Adjust weights based on item size relative to the average suitable bin capacity.\n    # This strategy emphasizes \"Best Fit\" for larger items and \"Fullness\" for smaller items.\n    avg_suitable_cap = np.mean(suitable_bins_caps)\n    if avg_suitable_cap > 1e-9:\n        item_vs_avg_cap_ratio = item / avg_suitable_cap\n    else:\n        item_vs_avg_cap_ratio = 0.5 # Default for very small capacities or no suitable bins\n\n    # Base weights that can be tuned.\n    w_bf = 0.6\n    w_f = 0.4\n\n    # Dynamically adjust weights: if item is large relative to average capacity, boost Best Fit.\n    # Otherwise, slightly boost Fullness.\n    if item_vs_avg_cap_ratio > 1.2: # Threshold for considering item \"large\"\n        w_bf += 0.2 * (item_vs_avg_cap_ratio - 1.2)\n        w_f -= 0.2 * (item_vs_avg_cap_ratio - 1.2)\n    elif item_vs_avg_cap_ratio < 0.8: # Threshold for considering item \"small\"\n        w_f += 0.1 * (0.8 - item_vs_avg_cap_ratio)\n        w_bf -= 0.1 * (0.8 - item_vs_avg_cap_ratio)\n\n    # Ensure weights remain valid (non-negative and sum to 1).\n    w_bf = max(0, w_bf)\n    w_f = max(0, w_f)\n    total_w = w_bf + w_f\n    if total_w > 1e-9:\n        w_bf /= total_w\n        w_f /= total_w\n    else: # Fallback to equal weights if calculation results in zero total weight\n        w_bf, w_f = 0.5, 0.5\n\n    # Combine normalized scores using the adaptive weights.\n    combined_scores = w_bf * norm_best_fit + w_f * norm_fullness\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 41.0,
    "cyclomatic_complexity": 8.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response7.txt_stdout.txt",
    "code_path": "problem_iter14_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (tightest residual) with Bin Fullness (prioritizing fuller bins)\n    using dynamic weights based on item size. Aims for efficient packing by\n    balancing tight fits with better overall bin utilization.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit (BF) - Prioritizes bins with minimum remaining capacity after placement.\n    # Higher score for smaller `remaining_capacity = suitable_bins_caps - item`.\n    remaining_capacity = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_capacity + 1e-9)\n\n    # Metric 2: Bin Fullness (F) - Prioritizes bins that are already more full.\n    # Higher score for smaller `suitable_bins_caps`.\n    fullness_scores = 1.0 / (suitable_bins_caps + 1e-9)\n\n    # Normalize scores for each metric to ensure they are in a comparable range.\n    # Normalization helps in combining metrics with different scales.\n\n    # Normalize Best Fit scores (0 to 1, higher is better)\n    max_bf = np.max(best_fit_scores)\n    norm_best_fit = best_fit_scores / max_bf if max_bf > 1e-9 else np.zeros_like(best_fit_scores)\n\n    # Normalize Fullness scores (0 to 1, higher is better)\n    max_f = np.max(fullness_scores)\n    norm_fullness = fullness_scores / max_f if max_f > 1e-9 else np.zeros_like(fullness_scores)\n\n    # --- Dynamic Weighting ---\n    # Determine weights based on the item's size relative to the average suitable bin capacity.\n    # This strategy adapts the heuristic's focus.\n    avg_suitable_cap = np.mean(suitable_bins_caps)\n    if avg_suitable_cap > 1e-9:\n        item_vs_avg_cap_ratio = item / avg_suitable_cap\n    else:\n        item_vs_avg_cap_ratio = 1.0 # Default ratio if average capacity is zero/negligible\n\n    # Base weights: Balanced approach\n    w_bf_base = 0.6\n    w_f_base = 0.4\n\n    # Adjust weights:\n    # If item is large relative to average suitable capacity, boost Best Fit.\n    # If item is small relative to average suitable capacity, slightly boost Fullness (to use slightly fuller bins).\n    # The goal is to make BF dominant for larger items and F still relevant for smaller ones.\n    w_bf = w_bf_base + 0.3 * max(0, item_vs_avg_cap_ratio - 1.0)\n    w_f = w_f_base - 0.3 * max(0, item_vs_avg_cap_ratio - 1.0)\n\n    # Ensure weights are non-negative and sum to 1.\n    w_bf = max(0.1, w_bf) # Ensure at least some weight for BF\n    w_f = max(0.1, w_f)  # Ensure at least some weight for F\n\n    total_w = w_bf + w_f\n    w_bf /= total_w\n    w_f /= total_w\n\n    # Combine normalized scores with dynamically adjusted weights\n    combined_scores = (w_bf * norm_best_fit + w_f * norm_fullness)\n\n    # Assign the calculated combined scores to the priorities array for suitable bins.\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 30.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response8.txt_stdout.txt",
    "code_path": "problem_iter14_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a refined Best Fit with a dynamic Exploration bonus,\n    balancing tightness and spreading load based on item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    remaining_after_placement = suitable_bins_caps - item\n\n    # Metric 1: Refined Best Fit - favors bins with smallest remaining capacity after placement.\n    # Logarithmic scaling emphasizes smaller remaining capacities. Add epsilon for stability.\n    best_fit_scores = np.log1p(1.0 / (remaining_after_placement + 1e-6))\n\n    # Metric 2: Exploration Bonus - favors bins that have significantly more remaining capacity.\n    # Min-max scaling of remaining capacity after placement creates a [0, 1] score.\n    min_rem_after = np.min(remaining_after_placement)\n    max_rem_after = np.max(remaining_after_placement)\n\n    exploration_scores = np.zeros_like(remaining_after_placement)\n    if max_rem_after > min_rem_after:\n        exploration_scores = (remaining_after_placement - min_rem_after) / (max_rem_after - min_rem_after)\n    else:\n        # If all suitable bins leave the same remaining capacity, exploration score is uniform.\n        exploration_scores = np.ones_like(remaining_after_placement) * 0.5 # Neutral exploration\n\n    # Dynamic Weighting based on item size.\n    # Larger items benefit more from a precise fit (Best Fit).\n    # Smaller items can afford to explore less utilized bins (Exploration Bonus).\n    # Assumes 'item' is scaled relative to typical bin capacity (e.g., 0 to 1).\n    \n    # Threshold for item size to switch weighting strategy.\n    threshold_medium = 0.5 \n    \n    # Calculate weights smoothly based on item size relative to threshold.\n    # For small items (item < threshold), exploration weight is higher.\n    # For large items (item > threshold), best_fit weight is higher.\n    weight_best_fit = np.clip(item / threshold_medium, 0.1, 1.0) \n    weight_exploration = np.clip((threshold_medium - item) / threshold_medium, 0.1, 1.0) \n\n    # Normalize weights to ensure they sum to 1, preventing issues if they fall outside intended ranges.\n    total_weight = weight_best_fit + weight_exploration\n    if total_weight > 1e-6:\n        weight_best_fit /= total_weight\n        weight_exploration /= total_weight\n    else: \n        # Fallback to equal weights if calculation results in near-zero total weight.\n        weight_best_fit = 0.5\n        weight_exploration = 0.5\n\n    # Combine scores using the dynamically determined weights.\n    combined_scores = (weight_best_fit * best_fit_scores +\n                       weight_exploration * exploration_scores)\n\n    # Assign the calculated combined scores to the priorities array for suitable bins.\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 72.92580773833267,
    "SLOC": 29.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response9.txt_stdout.txt",
    "code_path": "problem_iter14_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a Bin Fullness metric, dynamically weighting them\n    based on item size relative to bin capacities to balance tight fits and\n    bin utilization.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit (Tightness)\n    # Prioritize bins that leave minimum remaining capacity after packing.\n    # Score is inverse of remaining capacity (smaller remaining = higher score).\n    remaining_capacity = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_capacity + 1e-9)\n\n    # Metric 2: Bin Fullness\n    # Prioritize bins that are already more full, encouraging consolidation.\n    # Score is inverse of current remaining capacity (more full = higher score).\n    fullness_scores = 1.0 / (suitable_bins_caps + 1e-9)\n\n    # --- Dynamic Weighting Strategy ---\n    # Determine item's significance relative to available bin space.\n    # Calculate average remaining capacity among suitable bins.\n    avg_suitable_cap = np.mean(suitable_bins_caps)\n    if avg_suitable_cap > 1e-9:\n        # Ratio of item size to average suitable bin capacity.\n        # > 1 suggests item is large relative to available space.\n        # < 1 suggests item is small relative to available space.\n        item_vs_avg_cap_ratio = item / avg_suitable_cap\n    else:\n        # Default if no suitable bins or average capacity is zero/negligible.\n        item_vs_avg_cap_ratio = 0.5\n\n    # Adjust weights:\n    # For larger items (item_vs_avg_cap_ratio > 1.0):\n    #   Emphasize Best Fit (tightness) and Bin Fullness.\n    #   Reduce emphasis on Bin Fullness slightly as tightness is key.\n    # For smaller items (item_vs_avg_cap_ratio <= 1.0):\n    #   Emphasize Bin Fullness to use partially filled bins.\n    #   Slightly reduce emphasis on Best Fit as exact tightness is less critical.\n    if item_vs_avg_cap_ratio > 1.0:\n        w_bf = 0.6  # More weight on tight fit for larger items\n        w_f = 0.4   # Moderate weight on fullness\n    else:\n        w_bf = 0.4  # Moderate weight on tight fit for smaller items\n        w_f = 0.6   # More weight on fullness for smaller items\n\n    # Normalize weights to ensure they sum to 1 (though fixed here for clarity)\n    # Total weight = w_bf + w_f\n\n    # Normalize individual metric scores to a [0, 1] range for consistent combination.\n    # Normalize Best Fit scores\n    if np.max(best_fit_scores) > 1e-9:\n        norm_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        norm_best_fit = np.zeros_like(best_fit_scores)\n\n    # Normalize Fullness scores\n    if np.max(fullness_scores) > 1e-9:\n        norm_fullness = fullness_scores / np.max(fullness_scores)\n    else:\n        norm_fullness = np.zeros_like(fullness_scores)\n\n    # Combine normalized scores using dynamic weights\n    combined_scores = (w_bf * norm_best_fit + w_f * norm_fullness)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 31.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response0.txt_stdout.txt",
    "code_path": "problem_iter15_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    # Metric 1: Best Fit - score based on how tightly the item fits.\n    # Use inverse of remaining capacity after placement. Add epsilon for stability.\n    remaining_after_placement = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-9)\n\n    # Metric 2: Utilization - encourage bins that are already utilized to some extent.\n    # This is the inverse of how much capacity is *remaining*.\n    # We normalize the *used* capacity relative to the *original* capacity of the bin.\n    # For simplicity, let's assume a target 'fullness' and score based on proximity to it.\n    # A simpler heuristic: reward bins with less remaining capacity (i.e., higher utilization)\n    # without being too tight. This is a balance.\n    # Let's score based on how much is *left* after placing the item, but penalize very large remaining capacities.\n    # A smaller remaining capacity after placement is generally good for 'best fit',\n    # but here we want to balance with utilization.\n    # Let's consider the ratio of the item size to the bin's remaining capacity *before* placement.\n    # A higher ratio suggests the item is filling up a significant portion of what's available.\n    utilization_scores = item / (suitable_bins_caps + 1e-9)\n    \n    # Metric 3: Diversity/Exploration - Favor bins that are less full, but not completely empty.\n    # This aims to keep options open for future, possibly larger items.\n    # Normalize the remaining capacity and use a function that peaks at some intermediate value.\n    # Let's use remaining capacity relative to the item's size.\n    # If remaining_after_placement is large, it means the bin is quite empty.\n    # We want to slightly favor bins that are not excessively empty.\n    # A simple approach: score based on remaining capacity, but capped or dampened.\n    # Let's try a score that increases with remaining capacity but saturates.\n    # A simple linear scaling, then capping, or using a mild sigmoid.\n    # Let's normalize remaining capacity relative to item size. High value means very empty.\n    # We want to reward bins that have some remaining capacity but not an overwhelming amount.\n    # Max remaining capacity could be a reference.\n    # Let's define \"emptiness\" as (bin_capacity - item_size) / bin_capacity.\n    # We want to avoid bins that are too empty.\n    # A score inversely proportional to remaining capacity, but also considering the overall bin capacity.\n    # Let's consider the *gap* between the item and the bin's remaining capacity.\n    # We want to avoid very large gaps after placement if possible, but also avoid bins that are too full.\n    # Let's re-evaluate: for diversity, we want bins that are not too full.\n    # Score based on (suitable_bins_caps - item) but normalized.\n    # Consider the ratio: (remaining_after_placement) / (suitable_bins_caps)\n    # This is the proportion of space left *after* placing the item.\n    # We want to avoid this proportion being too large (meaning the bin is still very empty).\n    # So, we invert this ratio.\n    diversity_scores = suitable_bins_caps / (remaining_after_placement + 1e-9)\n    \n    # --- Normalization and Combination ---\n    \n    # Robust normalization: scale each metric to [0, 1] using min-max scaling,\n    # but use a small epsilon to prevent division by zero if all values are the same.\n    \n    def robust_scale(scores):\n        min_val = np.min(scores)\n        max_val = np.max(scores)\n        if max_val - min_val < 1e-9:\n            return np.zeros_like(scores)\n        return (scores - min_val) / (max_val - min_val)\n\n    normalized_best_fit = robust_scale(best_fit_scores)\n    normalized_utilization = robust_scale(utilization_scores)\n    normalized_diversity = robust_scale(diversity_scores)\n    \n    # Dynamic weighting:\n    # The weights should adapt to the item size.\n    # For small items: Prioritize diversity and utilization.\n    # For large items: Prioritize best fit.\n    # Let's use a simple linear interpolation based on item size relative to a typical bin capacity (e.g., 1.0).\n    # Assume item is in a normalized range, e.g., 0 to 1.\n    \n    # Weight for best fit increases with item size.\n    # Weight for diversity decreases with item size.\n    # Weight for utilization can be moderate and perhaps less sensitive to item size,\n    # or also decrease slightly with item size as the \"choice\" of bins for large items is more restricted.\n    \n    # Let's refine weights:\n    # Best Fit: Should be higher for larger items.\n    # Utilization: Moderate, maybe slightly favoring items that fit well into partially filled bins.\n    # Diversity: Should be higher for smaller items to keep bins open.\n    \n    # Linear interpolation between weights\n    # Small item (e.g., size ~0.1): BF=0.3, Util=0.4, Div=0.3\n    # Large item (e.g., size ~0.9): BF=0.7, Util=0.2, Div=0.1\n    \n    # Define weights based on item size relative to a conceptual maximum bin capacity (e.g., 1.0)\n    # Clamp item size to a reasonable range if not guaranteed.\n    clamped_item_size = np.clip(item, 0.01, 1.0) # Assuming item size is normalized between 0 and 1\n    \n    # Interpolation factor (0 for small item, 1 for large item)\n    interp_factor = clamped_item_size\n    \n    # Weights for a 'small' item scenario\n    w_bf_small, w_util_small, w_div_small = 0.3, 0.4, 0.3\n    # Weights for a 'large' item scenario\n    w_bf_large, w_util_large, w_div_large = 0.7, 0.2, 0.1\n    \n    # Interpolate weights\n    weight_best_fit = w_bf_small + (w_bf_large - w_bf_small) * interp_factor\n    weight_utilization = w_util_small + (w_util_large - w_util_small) * interp_factor\n    weight_diversity = w_div_small + (w_div_large - w_div_small) * interp_factor\n    \n    # Ensure weights sum to 1 (normalize if needed, though interpolation should keep it close)\n    total_w = weight_best_fit + weight_utilization + weight_diversity\n    weight_best_fit /= total_w\n    weight_utilization /= total_w\n    weight_diversity /= total_w\n    \n    combined_scores = (weight_best_fit * normalized_best_fit +\n                       weight_utilization * normalized_utilization +\n                       weight_diversity * normalized_diversity)\n    \n    priorities[suitable_bins_mask] = combined_scores\n    \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 35.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response1.txt_stdout.txt",
    "code_path": "problem_iter15_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    \n    # Metric 1: Best Fit - score based on how tightly the item fits\n    # Uses inverse of remaining capacity plus a small constant to avoid division by zero\n    # and to ensure that smaller remaining capacities get higher scores.\n    remaining_after_placement = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-6)\n\n    # Metric 2: Exploration - score based on the \"openness\" of the bin.\n    # Favors bins with a good amount of remaining capacity, but not so much that it's \"wasted\".\n    # We use a scaled inverse of remaining capacity, capping it to avoid extremely high scores for very empty bins.\n    # The scaling factor (e.g., 10.0) can be tuned. The addition of `item` before inversion\n    # helps to ensure that bins with little remaining capacity (relative to the item) don't dominate.\n    exploration_scores = np.clip(10.0 * (1.0 / (suitable_bins_caps + 1e-6)), 0, 5.0)\n\n    # Metric 3: Capacity Utilization - score based on how full the bin is *before* this item.\n    # This is approximated by (max_capacity - remaining_capacity). For simplicity,\n    # we can use a reference maximum capacity (e.g., 1.0) or an adaptive max capacity.\n    # Let's use the average remaining capacity of all suitable bins as a reference point\n    # for what a \"typical\" suitable bin looks like. Bins closer to this average are favored.\n    \n    # Using bin capacity relative to item size as a proxy for fill state when item is placed\n    # Maximize fill state = minimize remaining capacity after placement\n    # To incentivize utilization of partially filled bins, we can also consider how much\n    # capacity is already used. Let's use the inverse of remaining capacity, but\n    # penalize bins that are too empty relative to the item.\n    \n    # Consider the \"slack\" ratio: (bin_capacity - item) / bin_capacity. We want to minimize this.\n    # Or, consider (bin_capacity - item) / item. Small values are good for best fit.\n    # For uniformity, let's focus on bins that are not too empty.\n    # We can reward bins whose remaining capacity is closer to the *average* remaining capacity\n    # of all suitable bins, up to a certain point.\n    \n    if len(suitable_bins_caps) > 1:\n        avg_suitable_cap = np.mean(suitable_bins_caps)\n        # Score based on proximity to the average remaining capacity.\n        # Use a Gaussian-like function centered around the average.\n        uniformity_scores = np.exp(-((suitable_bins_caps - avg_suitable_cap)**2) / (2 * (avg_suitable_cap**2 + 1e-6)))\n    else:\n        uniformity_scores = np.ones_like(suitable_bins_caps) # If only one suitable bin, uniformity doesn't apply strongly.\n\n    # Normalize scores to be in a comparable range [0, 1]\n    # Robust normalization: scale by max, but add epsilon to avoid division by zero and ensure scores are not all zero if max is zero.\n    max_bf = np.max(best_fit_scores)\n    normalized_best_fit = best_fit_scores / (max_bf + 1e-6)\n\n    max_exp = np.max(exploration_scores)\n    normalized_exploration = exploration_scores / (max_exp + 1e-6)\n\n    max_uni = np.max(uniformity_scores)\n    normalized_uniformity = uniformity_scores / (max_uni + 1e-6)\n\n    # Dynamic weighting:\n    # Weighting based on item size relative to a typical bin capacity.\n    # Assume a reference bin capacity (e.g., 1.0 if items are normalized, or a typical large value).\n    # Let's use the maximum remaining capacity among suitable bins as a reference for \"large\".\n    \n    # If item is large relative to available space, prioritize fitting it tightly (Best Fit).\n    # If item is small, explore more and try to fill bins moderately (Exploration, Uniformity).\n    \n    # A simple dynamic weighting scheme:\n    # Normalize item size by the maximum remaining capacity found among suitable bins.\n    max_suitable_cap_overall = np.max(bins_remain_cap) # Use overall max for consistent scaling\n    if max_suitable_cap_overall > 1e-6:\n        normalized_item_size = item / max_suitable_cap_overall\n    else:\n        normalized_item_size = 0.5 # Default if no capacity\n\n    # Weights that shift based on item size.\n    # When item is larger (closer to 1), BF gets more weight.\n    # When item is smaller (closer to 0), Exploration and Uniformity get more weight.\n    weight_best_fit = 0.4 + 0.5 * normalized_item_size\n    weight_exploration = 0.3 * (1.0 - normalized_item_size)\n    weight_uniformity = 0.3 * (1.0 - normalized_item_size * 0.5) # Slightly less penalty for uniformity with large items\n\n    # Ensure weights sum to approximately 1, or re-normalize\n    total_weight = weight_best_fit + weight_exploration + weight_uniformity\n    weight_best_fit /= total_weight\n    weight_exploration /= total_weight\n    weight_uniformity /= total_weight\n    \n    combined_scores = (weight_best_fit * normalized_best_fit +\n                       weight_exploration * normalized_exploration +\n                       weight_uniformity * normalized_uniformity)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 37.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response2.txt_stdout.txt",
    "code_path": "problem_iter15_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    # Metric 1: Modified Best Fit (MF) - Penalize bins that would leave a very large remaining capacity.\n    # This encourages using space more efficiently by avoiding \"too much\" leftover.\n    # We use the logarithm of the remaining capacity after placement. Lower values are better.\n    remaining_after_placement = suitable_bins_caps - item\n    mf_scores = -np.log1p(remaining_after_placement) # Higher negative scores mean better fit\n\n    # Metric 2: Adaptive Gap Penalty (AGP) - Penalize bins that are \"almost full\" but not quite.\n    # This aims to prevent fragmentation by not packing items into bins that are already very close to capacity.\n    # We define \"almost full\" based on the item size itself. If the item is large, a bin with 90% capacity\n    # might be considered \"almost full\". If the item is small, a bin with 50% capacity might be \"almost full\".\n    # Let's use a threshold relative to the bin's *current* remaining capacity.\n    # A bin that has `bin_cap - item` which is small relative to `bin_cap`.\n    # We want to *reward* bins where `item / bin_cap` is high (meaning the item fills a large portion of what's available).\n    # This is the inverse of leaving a large gap relative to what's available.\n    # We want to penalize bins where `(bin_cap - item) / bin_cap` is small.\n    # So, we reward `1 - (bin_cap - item) / bin_cap` which simplifies to `item / bin_cap`.\n    # However, we want to reward bins that are *not too empty* relative to the item.\n    # A bin that is almost empty, and an item takes up a small portion of it, is not ideal for AGP.\n    # Let's consider the ratio of the item size to the bin's remaining capacity.\n    # Higher ratio means the item is a significant part of the available space.\n    # We use a transformation to favor ratios closer to 1 (item fills most of the remaining space).\n    # This also helps to avoid packing small items into very large remaining capacity bins.\n    agp_scores = np.zeros_like(suitable_bins_caps)\n    non_zero_suitable_caps_mask = suitable_bins_caps > 1e-9\n    if np.any(non_zero_suitable_caps_mask):\n        ratios = item / suitable_bins_caps[non_zero_suitable_caps_mask]\n        # Sigmoid-like function to reward ratios close to 1.\n        # exp(-(x-1)^2)\n        agp_scores[non_zero_suitable_caps_mask] = np.exp(-((ratios - 1.0)**2) / 0.2) # Peak at ratio=1\n\n    # Metric 3: First Fit Descending (FFD) preference - Tie-breaking or preference for bins that have been used more.\n    # This can be proxied by the total capacity already used in the bin.\n    # Since we only have remaining capacity, we can assume a maximum bin capacity (e.g., 1.0 if normalized)\n    # and calculate used capacity as `max_capacity - remaining_capacity`.\n    # For simplicity here, let's use the inverse of remaining capacity as a proxy for \"how full\" it is.\n    # This favors bins that are less empty.\n    # Use a transformation to avoid very large values for almost empty bins.\n    # `1 / (remaining_capacity + epsilon)`\n    max_bin_capacity_guess = 1.0 # Assume a default max capacity if not provided, or derive from problem context\n    # If problem has context of max bin capacity, it should be passed. Using 1.0 as common normalization.\n    used_capacity_proxy = max_bin_capacity_guess - suitable_bins_caps\n    # Normalize used_capacity_proxy to [0,1] relative to max capacity.\n    used_capacity_proxy_normalized = used_capacity_proxy / max_bin_capacity_guess\n    # We want to reward bins that have more used capacity.\n    # A simple approach is to directly use the normalized used capacity.\n    # Let's add a small constant to avoid 0 if max_bin_capacity_guess is 1 and suitable_bins_caps is 1.\n    ffs_scores = used_capacity_proxy_normalized + 1e-6 # Higher is better\n\n    # --- Normalization and Combination ---\n\n    # Normalize scores to [0, 1] range for each metric\n    # Avoid division by zero if all values in a metric are the same or zero.\n    def normalize(scores):\n        max_score = np.max(scores)\n        min_score = np.min(scores)\n        if max_score == min_score:\n            return np.zeros_like(scores) # All values are the same, no relative preference\n        return (scores - min_score) / (max_score - min_score)\n\n    normalized_mf = normalize(mf_scores)\n    normalized_agp = normalize(agp_scores)\n    normalized_ffs = normalize(ffs_scores)\n\n    # --- Dynamic Weighting ---\n    # Weights are adjusted based on the item size relative to the *average* remaining capacity of suitable bins.\n    # This makes the weighting adaptive to the current state of the bins.\n\n    avg_suitable_cap = np.mean(suitable_bins_caps) if suitable_bins_caps.size > 0 else 1.0\n    # Normalize item size by average suitable capacity to get a sense of \"item size relative to opportunity\"\n    relative_item_size = item / (avg_suitable_cap + 1e-9)\n\n    # Weighting strategy:\n    # If item is large relative to available space (relative_item_size > 1):\n    #   Prioritize Best Fit (MF) heavily.\n    #   Penalize leaving large gaps (AGP less important).\n    #   Used capacity (FFS) still moderately important.\n    # If item is small relative to available space (relative_item_size < 1):\n    #   Prioritize filling bins more (FFS more important).\n    #   Avoid placing small items into very large empty bins (AGP more important).\n    #   Best Fit (MF) is less critical as gaps are less likely to be problematic.\n\n    # Define base weights\n    w_mf_base = 0.4\n    w_agp_base = 0.3\n    w_ffs_base = 0.3\n\n    # Adjust weights based on relative_item_size\n    # For relative_item_size > 1 (large item relative to capacity):\n    # Increase MF, decrease AGP\n    w_mf = w_mf_base + 0.3 * np.clip(relative_item_size - 1, 0, 1)\n    w_agp = w_agp_base - 0.2 * np.clip(relative_item_size - 1, 0, 1)\n    w_ffs = w_ffs_base # Keep FFS stable or slightly increase\n\n    # For relative_item_size < 1 (small item relative to capacity):\n    # Decrease MF, increase AGP and FFS\n    w_mf = w_mf_base - 0.2 * np.clip(1 - relative_item_size, 0, 1)\n    w_agp = w_agp_base + 0.3 * np.clip(1 - relative_item_size, 0, 1)\n    w_ffs = w_ffs_base + 0.2 * np.clip(1 - relative_item_size, 0, 1)\n\n    # Ensure weights are within a reasonable range and sum to 1\n    w_mf = np.clip(w_mf, 0.1, 0.8)\n    w_agp = np.clip(w_agp, 0.1, 0.6)\n    w_ffs = np.clip(w_ffs, 0.1, 0.6)\n\n    total_weight = w_mf + w_agp + w_ffs\n    w_mf /= total_weight\n    w_agp /= total_weight\n    w_ffs /= total_weight\n\n    # Combine normalized scores\n    combined_scores = (w_mf * normalized_mf +\n                       w_agp * normalized_agp +\n                       w_ffs * normalized_ffs)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 50.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response3.txt_stdout.txt",
    "code_path": "problem_iter15_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit (Tightness)\n    # Score based on how tightly the item fits. Prefer bins where remaining capacity is minimal but sufficient.\n    # Use inverse of remaining capacity, clamped to avoid division by zero and to cap extreme values.\n    remaining_after_placement = suitable_bins_caps - item\n    # We want smaller remaining_after_placement to have higher score.\n    # Using 1/(x+epsilon) or similar. Let's use a clipped inverse.\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-6)\n    # Cap the score to prevent overly large values from very small remaining capacities.\n    best_fit_scores = np.clip(best_fit_scores, 0, 10) # Arbitrary cap, can be tuned\n\n    # Metric 2: Fill Ratio (Uniformity/Exploration Balance)\n    # Reward bins that are not too empty, encouraging fuller bins first.\n    # This balances filling existing bins to some extent before opening new ones.\n    # Calculate the fill ratio if the item is placed. Assume max bin capacity is 1.0 for normalization.\n    # If bin capacities vary significantly, using a context-aware max capacity could be better,\n    # but for simplicity, we'll assume a standard bin capacity (e.g., 1.0 or a known max).\n    # Let's assume a theoretical max capacity of 1.0 for normalization of 'emptiness'.\n    # A higher fill ratio (closer to 1) is preferred.\n    # Consider the capacity *before* placing the item for this metric.\n    # We want to prefer bins that are already somewhat filled.\n    # Let's use remaining_after_placement as a proxy for emptiness.\n    # Normalized \"emptiness\" would be remaining_after_placement / max_bin_capacity.\n    # We want to penalize high emptiness. So, score should be inversely related to emptiness.\n    # Let's use 1 - (remaining_after_placement / max_suitable_cap_before_placement) where max_suitable_cap_before_placement is the capacity of the fullest suitable bin.\n    # This aims to reward bins that are already somewhat utilized.\n\n    # Let's simplify and focus on the remaining capacity itself.\n    # We want bins that are not excessively empty, but also not almost full (to leave space).\n    # This suggests a \"sweet spot\".\n    # Let's score based on the remaining capacity *after* placement relative to the item size.\n    # A bin that has just enough for the item (small remaining_after_placement) gets a high best_fit score.\n    # For fill ratio, let's reward bins that have a moderate amount of remaining capacity.\n    # This helps to leave space for future items and avoid packing items too tightly too early.\n    # We can use a Gaussian-like function centered around a preferred remaining capacity.\n    # Preferred remaining capacity could be item_size * some_factor, or a fixed percentage.\n    # Let's try scoring based on how much space is left *relative to the bin's current capacity*.\n    # More intuitively, reward bins that are not \"too empty\".\n    # Consider the capacity before placement: suitable_bins_caps.\n    # We want to encourage using bins that are not at their absolute minimum, but also not nearly full.\n    # Let's use remaining capacity relative to item size: suitable_bins_caps / item\n    # Higher ratio means the bin has more extra space compared to the item.\n    # We want a moderate amount of extra space.\n    # Let's define a \"target\" remaining capacity: perhaps related to the item size.\n    # Target_remaining = item * 0.5 (or some other factor)\n    # Score is high if abs(remaining_after_placement - Target_remaining) is small.\n    # Or, more simply, reward bins with a good amount of remaining capacity, but not too much.\n    # Let's use a \"gap fill\" metric: remaining_after_placement / suitable_bins_caps.\n    # We want this to be small for \"best fit\", but not too small for \"exploration\".\n    # Let's try a metric that rewards bins that are not too empty, using the *original* remaining capacity.\n    # We want to avoid bins that are almost empty.\n    # Let's use the inverse of the *original* remaining capacity (as a proxy for \"fill level\").\n    # Higher original remaining capacity (less filled) gets a lower score here.\n    # We want to favor bins that are *not* excessively empty.\n    # Fill_score = 1 / (suitable_bins_caps + 1e-6) - this rewards fuller bins.\n    # Let's focus on the remaining capacity *after* packing, normalized by the *original* capacity of that bin.\n    # This tells us how much space is left relative to how much was available.\n    # We want this ratio to be small for best fit, and moderate for exploration.\n\n    # Let's try a simpler, more robust approach: prioritize bins that offer a good balance of fitting the item snugly AND leaving reasonable space for future items.\n    # Metric 1: Fit Tightness (similar to Best Fit)\n    # Score = 1 / (remaining_after_placement + epsilon)\n    fit_tightness = 1.0 / (remaining_after_placement + 1e-6)\n    fit_tightness = np.clip(fit_tightness, 0, 15) # Cap to avoid extreme values\n\n    # Metric 2: Space Utilization (Balance)\n    # Reward bins that leave a \"useful\" amount of space.\n    # This means not too little (handled by fit_tightness) and not too much.\n    # Let's score based on the ratio of remaining capacity *after* placement to the *item size*.\n    # A ratio of 0 means it fits perfectly. A high ratio means a lot of space is left.\n    # We want a moderate ratio.\n    # Let's use a Gaussian-like function centered around a preferred remaining space.\n    # Preferred remaining space could be item_size * 1.0 (i.e., double the item size left)\n    # Or, a fixed percentage of bin capacity.\n    # Let's try scoring based on the remaining capacity *relative to the bin's original capacity*.\n    # This is (remaining_after_placement / suitable_bins_caps)\n    # We want this ratio to be small, but not zero.\n    # Let's try a simpler \"minimum remaining capacity\" concept.\n    # Reward bins that have *at least* a certain amount of space left, e.g., 0.2 * bin_capacity.\n    # This encourages leaving some buffer.\n    # Let's consider the \"slack\" relative to the item size.\n    # Slack = remaining_after_placement\n    # We want slack to be positive but not excessively large.\n    # Consider the ratio: suitable_bins_caps / item.\n    # This ratio indicates how many times the item fits into the current bin capacity.\n    # A ratio close to 1 is good for fitting.\n    # A ratio much larger than 1 means a lot of space is left.\n    # Let's try to reward bins where the *remaining capacity after placement* is a reasonable fraction of the *original capacity*.\n    # (remaining_after_placement / suitable_bins_caps)\n    # We want this to be in a middle range.\n    # Let's use a function that peaks at a certain \"excess capacity\" relative to the item.\n    # Excess capacity = remaining_after_placement\n    # We want Excess capacity to be not too small (handled by fit_tightness) and not too large.\n    # Let's score based on the remaining capacity *after* placement, normalized by the *item size*.\n    # `remaining_after_placement / item`\n    # High score for moderate values.\n    # Let's try a Gumbel-like distribution for the remaining capacity.\n    # Score = exp(-exp(-(remaining_after_placement - mu) / sigma))\n    # Let mu be a target remaining capacity. For example, mu = item_size * 0.5\n    # This rewards bins that leave roughly half the item's size in remaining space.\n    mu = item * 0.5  # Target remaining space\n    sigma = item * 0.2 # Spread of the distribution\n\n    # Ensure sigma is not zero\n    if sigma < 1e-6:\n        sigma = 1e-6\n    space_utilization = np.exp(-np.exp(-(remaining_after_placement - mu) / sigma))\n    space_utilization = np.clip(space_utilization, 0, 1) # Scale to [0, 1]\n\n\n    # Metric 3: Bin Age/Usage (Exploration/Diversification)\n    # Reward bins that have been used less, to spread items and avoid creating many nearly full bins and one very empty bin.\n    # This can be proxied by the *original* remaining capacity. Higher original remaining capacity means less used.\n    # We want to avoid bins that are *too* empty, but also encourage using bins that aren't already nearly full.\n    # Let's score based on the *original* remaining capacity, normalized by max suitable bin capacity.\n    # This rewards bins that have more \"room\" in an absolute sense, but we need to temper this.\n    # A better approach is to reward bins that are not at their absolute maximum capacity, but also not at their minimum.\n    # Let's use the *inverse* of the original remaining capacity as a proxy for \"fill level\".\n    # Fill_level_proxy = 1 / (suitable_bins_caps + 1e-6)\n    # We want to reward moderately filled bins.\n    # Let's use the inverse of the original remaining capacity, but capped.\n    # This rewards bins that are less empty.\n    less_empty_score = 1.0 / (suitable_bins_caps + 1e-6)\n    less_empty_score = np.clip(less_empty_score, 0, 5) # Cap to avoid extreme values\n\n    # Combine scores with dynamic weights.\n    # Weights should adapt based on the item size relative to the typical bin capacity.\n    # Assume a normalized bin capacity of 1.0.\n    # If item size is large (e.g., > 0.5), 'fit_tightness' becomes more important.\n    # If item size is small, 'space_utilization' and 'less_empty_score' can be more important.\n\n    # Normalize item size relative to a typical max capacity (e.g., 1.0)\n    item_normalized = item # Assuming item is already scaled or typical max capacity is 1.0\n\n    # Dynamic weighting:\n    # For smaller items, we might want to ensure they go into bins that are not extremely empty.\n    # For larger items, fitting them snugly is crucial.\n    \n    # Weight for fit_tightness increases with item size\n    w_fit = 0.4 + 0.5 * item_normalized\n    w_fit = np.clip(w_fit, 0.4, 0.9)\n\n    # Weight for space_utilization (leaving moderate space) can be moderate, perhaps peaking for medium items.\n    w_space = 0.3 + 0.4 * (1 - item_normalized) # Higher for smaller items\n    w_space = np.clip(w_space, 0.3, 0.7)\n\n    # Weight for less_empty_score (avoiding very empty bins) is important for small items.\n    w_less_empty = 0.3 + 0.4 * (1 - item_normalized) # Higher for smaller items\n    w_less_empty = np.clip(w_less_empty, 0.3, 0.7)\n\n    # Re-normalize weights to sum to 1\n    total_w = w_fit + w_space + w_less_empty\n    w_fit /= total_w\n    w_space /= total_w\n    w_less_empty /= total_w\n    \n\n    # Normalize each metric's scores before combining\n    # Avoid division by zero if a metric yields all zeros\n    if np.max(fit_tightness) > 1e-6:\n        norm_fit = fit_tightness / np.max(fit_tightness)\n    else:\n        norm_fit = np.zeros_like(fit_tightness)\n\n    if np.max(space_utilization) > 1e-6:\n        norm_space = space_utilization / np.max(space_utilization)\n    else:\n        norm_space = np.zeros_like(space_utilization)\n\n    if np.max(less_empty_score) > 1e-6:\n        norm_less_empty = less_empty_score / np.max(less_empty_score)\n    else:\n        norm_less_empty = np.zeros_like(less_empty_score)\n\n    combined_scores = (w_fit * norm_fit +\n                       w_space * norm_space +\n                       w_less_empty * norm_less_empty)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 47.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response4.txt_stdout.txt",
    "code_path": "problem_iter15_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_caps = bins_remain_cap[suitable_bins_mask]\n\n    # Metric 1: Best Fit - favors bins where the remaining capacity is minimized after placing the item.\n    # We use the inverse of the remaining capacity plus a small epsilon to avoid division by zero,\n    # effectively prioritizing bins with less slack.\n    remaining_after_placement = suitable_bins_caps - item\n    best_fit_scores = 1.0 / (remaining_after_placement + 1e-9)\n\n    # Metric 2: First Fit Decreasing-like - prioritizes bins that have been used more, encouraging fuller bins.\n    # This is approximated by looking at how much space is *not* remaining.\n    # We consider the capacity of the bin relative to the item size. Bins that can accommodate the item\n    # and have a larger capacity relative to the item are considered \"more utilized\" in this context.\n    # This metric aims to balance the bins by filling up larger available spaces that are still suitable.\n    # We use a score that increases with the ratio of bin capacity to item size, but capped to avoid extreme values.\n    # This encourages packing into bins that can take the item with relative ease, without being excessively large.\n    utilization_scores = np.clip(suitable_bins_caps / item, 0, 5) # Cap to prevent overly favoring very large bins\n\n    # Metric 3: Evenness/Spread - aims to distribute items somewhat evenly.\n    # This can be achieved by favoring bins that are \"moderately\" filled, meaning they are not too empty and not too full.\n    # We can use a Gaussian-like function centered around a target fill level.\n    # A simple heuristic is to favor bins whose remaining capacity is neither too small (best-fit already handles this)\n    # nor too large (which would be heavily penalized by utilization_scores).\n    # We'll define a \"sweet spot\" for remaining capacity, e.g., roughly 25-50% of the item's size.\n    # Or, more robustly, relative to the bin's *original* capacity, if that were known or estimable.\n    # Without original capacity, we can use remaining capacity relative to the item size to indicate \"slack\".\n    # We want bins with a moderate amount of slack, not too little (best fit) and not too much.\n    # Let's use a score that peaks when remaining_after_placement is roughly equal to the item size.\n    # This means the bin was about twice the item's size.\n    # Using a Gaussian centered at `item` for `remaining_after_placement`.\n    center = item\n    spread = max(item * 2, 0.1) # Adjust spread based on item size, with a minimum\n    evenness_scores = np.exp(-0.5 * ((remaining_after_placement - center) / spread)**2)\n\n\n    # Normalize scores to be in a comparable range [0, 1]\n    # Robust normalization using median and IQR for outliers, or simple max normalization if values are well-behaved.\n    # Max normalization is simpler and often sufficient.\n\n    if np.max(best_fit_scores) > 1e-9:\n        normalized_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        normalized_best_fit = np.zeros_like(best_fit_scores)\n\n    if np.max(utilization_scores) > 1e-9:\n        normalized_utilization = utilization_scores / np.max(utilization_scores)\n    else:\n        normalized_utilization = np.zeros_like(utilization_scores)\n\n    if np.max(evenness_scores) > 1e-9:\n        normalized_evenness = evenness_scores / np.max(evenness_scores)\n    else:\n        normalized_evenness = np.zeros_like(evenness_scores)\n\n\n    # Combine scores with dynamic weights.\n    # The weights are adjusted based on the item size.\n    # For smaller items, we might favor utilization and evenness to distribute them.\n    # For larger items, best fit becomes more critical.\n\n    # Normalize item size against a typical maximum capacity (e.g., 1.0 or an average bin capacity).\n    # Assuming a normalized item size is already provided or can be estimated.\n    # If item is not normalized, consider `item / max_expected_capacity`. Let's assume `item` is already scaled.\n    item_normalized_scale = item # Or item / max_bin_capacity if max_bin_capacity is known\n\n    # Weights that balance the metrics.\n    # More weight on best_fit for larger items.\n    # More weight on utilization and evenness for smaller items.\n    weight_best_fit = 0.3 + 0.6 * item_normalized_scale\n    weight_utilization = 0.4 - 0.3 * item_normalized_scale\n    weight_evenness = 0.3 - 0.1 * item_normalized_scale\n\n    # Ensure weights are non-negative and sum to 1 (or close to it).\n    # Clip weights to be within [0, 1] and re-normalize if necessary.\n    weights = np.array([weight_best_fit, weight_utilization, weight_evenness])\n    weights = np.clip(weights, 0, 1)\n    weights /= np.sum(weights)\n\n    final_weights = {\n        'best_fit': weights[0],\n        'utilization': weights[1],\n        'evenness': weights[2]\n    }\n\n    combined_scores = (final_weights['best_fit'] * normalized_best_fit +\n                       final_weights['utilization'] * normalized_utilization +\n                       final_weights['evenness'] * normalized_evenness)\n\n    priorities[suitable_bins_mask] = combined_scores\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 41.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  }
]