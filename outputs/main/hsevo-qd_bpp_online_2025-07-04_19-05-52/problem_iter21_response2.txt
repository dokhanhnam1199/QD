```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin.

    This version prioritizes bins based on a combination of fill ratio,
    fragmentation penalty, and a reward for bins approaching a target utilization.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    eligible_bins = bins_remain_cap >= item
    bin_capacity = np.max(bins_remain_cap)

    if not np.any(eligible_bins):
        return np.zeros_like(bins_remain_cap) - np.inf

    # 1. Fill Ratio: Prioritize bins where the item fills a significant portion.
    fill_ratio = item / bins_remain_cap
    fill_ratio[~eligible_bins] = -np.inf  # Ineligible bins have negative infinity priority
    priorities = 1.2 * fill_ratio  # Adjust weight as needed

    # 2. Fragmentation Penalty: Discourage creating very small gaps.  Adaptively adjust the penalty.
    gap_size = bins_remain_cap - item
    avg_item_size = np.mean(item) if isinstance(item, np.ndarray) else item # use average item size.
    frag_threshold = 0.2 * bin_capacity #Dynamic Fragmentation threshold based on average item size
    small_gap = gap_size < frag_threshold

    penalty = 0.3 * (frag_threshold - gap_size) / frag_threshold # penalty is proportional to how small is the gap.
    penalty[gap_size > frag_threshold] = 0
    priorities[small_gap & eligible_bins] -= penalty[small_gap & eligible_bins]

    # 3. Target Utilization Reward: Reward bins approaching a target utilization.
    target_utilization = 0.75 * bin_capacity #Aim for 75% utilization
    utilization_range = 0.1 * bin_capacity  # A small range around the target utilization

    lower_bound = target_utilization - utilization_range/2
    upper_bound = target_utilization + utilization_range/2

    approaching_target = (bins_remain_cap >= item + lower_bound) & (bins_remain_cap <= item + upper_bound)
    reward = 0.2 * (1- np.abs(bins_remain_cap[approaching_target] - (item + target_utilization)) / utilization_range) #The reward is proportional to how close to the target is the bin after insertion
    priorities[approaching_target & eligible_bins] += reward

    # 4. Larger capacity reward (for better choices in First-Fit-Decreasing like approaches)
    priorities[eligible_bins] += 0.03 * (bins_remain_cap[eligible_bins] / bin_capacity)


    # 5. Prevent bins from becoming too full
    nearly_full_threshold = 0.1 * bin_capacity  # e.g., less than 10% remaining
    nearly_full = gap_size < nearly_full_threshold
    priorities[nearly_full & eligible_bins] -= 0.4 #Strong penalty here.

    # 6. Final Clipping
    priorities = np.clip(priorities, -1, 1)

    return priorities
```
