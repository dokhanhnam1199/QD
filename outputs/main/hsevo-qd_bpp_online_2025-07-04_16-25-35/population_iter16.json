[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    ratios = item / bins_remain_cap\n    log_ratios = np.log(ratios)\n    priorities = -log_ratios\n    return priorities",
    "response_id": 0,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 11.60964047443681,
    "mi": 94.04446327225541,
    "token_count": 47.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    priority[sufficient_capacity] += 1  # Base priority if bin can fit item\n\n    # Heuristic 2: Minimize Waste (Maximize filled space).\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0] # Consider only valid waste\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += 2\n\n    # Heuristic 3: Prevent Fragmentation (Bins near full have high priority).\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= 2 * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += 1.5\n\n    # Heuristic 4: Small amount of available capacity.\n    small_cap = (bins_remain_cap >= item)\n    priority[small_cap] += (bins_remain_cap[small_cap] - item) / np.max(bins_remain_cap) # Add scaled value based on relative small capacity.\n    \n    # Heuristic 5: Give higher preference if it is an empty bin.\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += 0.5\n\n    return priority",
    "response_id": 1,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 53.77443751081735,
    "mi": 67.91077893705676,
    "token_count": 88.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity ratios with waste minimization for bin prioritization.\"\"\"\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += 1\n\n    # Heuristic 2: Minimize Waste (Only for bins with sufficient capacity)\n    waste = bins_remain_cap - item\n    min_waste = np.min(waste[sufficient_capacity]) if np.any(sufficient_capacity) else np.inf\n    min_waste_bins_idx = np.where((waste == min_waste) & sufficient_capacity)[0]\n    priority[min_waste_bins_idx] += 2\n\n    # Heuristic 3: Capacity Ratio (Only for bins with sufficient capacity)\n    ratios = item / bins_remain_cap\n    ratios[~sufficient_capacity] = 0  # Ignore bins without sufficient capacity\n    priority += ratios\n\n    return priority",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 144.75398259382442,
    "mi": 86.58537321553837,
    "token_count": 155.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response0.txt_stdout.txt",
    "code_path": "problem_iter3_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Heuristic 1: Feasibility (Essential)\n    feasible = bins_remain_cap >= item\n    if not np.any(feasible):\n        return priority  # No feasible bin, all priorities remain 0\n\n    priority[feasible] += 1.0 # Base priority for feasible bins\n\n    # Heuristic 2: Minimize Waste (First-Fit Decreasing variant)\n    waste = bins_remain_cap - item\n    positive_waste = waste[feasible] # Only consider waste in feasible bins\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where((waste == min_waste) & feasible)[0]  #Ensure we choose within the feasible set\n        priority[min_waste_bins_idx] += 3.0 # Higher priority for minimizing waste\n\n    # Heuristic 3: Avoid Fragmentation (Balancing Bin Usage)\n    # Give a slightly higher priority to bins that, after placing the item,\n    # will have a remaining capacity in the middle range of possible remaining capacities.\n    post_fill_capacities = bins_remain_cap - item\n    post_fill_capacities_feasible = post_fill_capacities[feasible]\n    if len(post_fill_capacities_feasible) > 0:\n        median_capacity = np.median(post_fill_capacities_feasible)\n        capacity_diff = np.abs(post_fill_capacities - median_capacity)\n        # Scale the priority boost inversely proportional to the capacity difference.\n        # Bins closer to the median capacity get a higher boost.\n        priority[feasible] += np.clip(1.0 - capacity_diff[feasible] / np.max(bins_remain_cap), 0, 1) #Scale and clip. Prevents negative values\n        \n\n    # Heuristic 4: Prioritize emptier bins if the item is relatively large.\n    # This helps distribute items more evenly across bins initially.\n    if item > np.mean(bins_remain_cap): # Scale by item size relative to average capacity.\n        empty_bin_indices = np.where(bins_remain_cap == np.max(bins_remain_cap))[0]\n        priority[empty_bin_indices] += 1.5\n\n    # Heuristic 5: Add a slight randomness to break ties and explore the solution space.\n    priority[feasible] += np.random.rand(np.sum(feasible)) * 0.1\n\n    return priority",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 26.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 385.74374433250205,
    "mi": 76.827568796314,
    "token_count": 299.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response1.txt_stdout.txt",
    "code_path": "problem_iter7_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray,\n                 sufficient_capacity_weight: float = 1.9596220015949373,\n                 min_waste_weight: float = 4.237844712440302,\n                 nearly_full_threshold_multiplier: float = 2.0406124655988833,\n                 nearly_full_weight: float = 4.224367911090436,\n                 empty_bin_weight: float = 4.8338513308698) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        sufficient_capacity_weight: Weight for bins with sufficient capacity.\n        min_waste_weight: Weight for bins that minimize waste.\n        nearly_full_threshold_multiplier: Multiplier for item size to determine nearly full threshold.\n        nearly_full_weight: Weight for bins that are nearly full.\n        empty_bin_weight: Weight for empty bins.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    priority[sufficient_capacity] += sufficient_capacity_weight  # Base priority if bin can fit item\n\n    # Heuristic 2: Minimize Waste (Maximize filled space).\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0] # Consider only valid waste\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += min_waste_weight\n\n    # Heuristic 3: Prevent Fragmentation (Bins near full have high priority).\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= nearly_full_threshold_multiplier * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += nearly_full_weight\n\n    # Heuristic 4: Small amount of available capacity.\n    small_cap = (bins_remain_cap >= item)\n    priority[small_cap] += (bins_remain_cap[small_cap] - item) / np.max(bins_remain_cap) # Add scaled value based on relative small capacity.\n    \n    # Heuristic 5: Give higher preference if it is an empty bin.\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += empty_bin_weight\n\n    return priority",
    "response_id": 1,
    "tryHS": true,
    "obj": 3.8791384124451627,
    "SLOC": 22.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 307.756981016698,
    "mi": 81.80209096280826,
    "token_count": 238.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response0.txt_stdout.txt",
    "code_path": "problem_iter5_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity ratios, waste minimization, and fragmentation avoidance.\"\"\"\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += 1\n\n    # Heuristic 2: Minimize Waste (Only for bins with sufficient capacity)\n    waste = bins_remain_cap - item\n    positive_waste = waste[sufficient_capacity]\n    min_waste = np.min(positive_waste) if positive_waste.size > 0 else np.inf\n    min_waste_bins_idx = np.where((waste == min_waste) & sufficient_capacity)[0]\n    priority[min_waste_bins_idx] += 2\n\n    # Heuristic 3: Capacity Ratio (Only for bins with sufficient capacity)\n    ratios = item / bins_remain_cap\n    ratios[~sufficient_capacity] = 0  # Ignore bins without sufficient capacity\n    priority += ratios\n\n    # Heuristic 4: Prevent Fragmentation (Bins near full have high priority).\n    nearly_full_threshold = 1.2 * item  # Adjust as needed\n    nearly_full = (bins_remain_cap > 0) & (bins_remain_cap <= nearly_full_threshold) & (bins_remain_cap >= item)\n    priority[nearly_full] += 1.5\n\n    # Heuristic 5: Prioritize Emptier Bins for Larger Items\n    empty_bin_threshold = np.max(bins_remain_cap)\n    is_empty = bins_remain_cap == empty_bin_threshold\n    priority[is_empty] += 0.8   # Slightly favor empty bins\n\n    return priority",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 309.6277232931715,
    "mi": 79.19005722310375,
    "token_count": 233.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response0.txt_stdout.txt",
    "code_path": "problem_iter6_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray,\n                  sufficient_capacity_weight: float = 1.8066941616372154,\n                  min_waste_weight: float = 1.2369759055947915,\n                  nearly_full_threshold_multiplier: float = 1.164912650607926,\n                  nearly_full_weight: float = 2.4304538888032448,\n                  empty_bin_weight: float = 0.7700931706376539,\n                  waste_capacity_ratio_weight: float = 0.5,\n                  bin_utilization_weight: float = 1.0) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        sufficient_capacity_weight: Weight for bins with sufficient capacity.\n        min_waste_weight: Weight for bins that minimize waste.\n        nearly_full_threshold_multiplier: Multiplier for item size to determine nearly full threshold.\n        nearly_full_weight: Weight for bins that are nearly full.\n        empty_bin_weight: Weight for empty bins.\n        waste_capacity_ratio_weight: Weight for the ratio of waste to bin capacity\n        bin_utilization_weight: Weight for the current bin utilization\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    bin_capacity = np.max(bins_remain_cap)\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    priority[sufficient_capacity] += sufficient_capacity_weight  # Base priority if bin can fit item\n\n    # Heuristic 2: Minimize Waste (Maximize filled space).\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0] # Consider only valid waste\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += min_waste_weight\n\n    # Heuristic 3: Prevent Fragmentation (Bins near full have high priority).\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= nearly_full_threshold_multiplier * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += nearly_full_weight\n\n    # Heuristic 4: Small amount of available capacity and weight by normalized waste\n    sufficient_bins = bins_remain_cap >= item\n    if np.any(sufficient_bins):\n        waste_ratios = waste[sufficient_bins] / bin_capacity\n        priority[sufficient_bins] += waste_capacity_ratio_weight * (1 - waste_ratios)\n    \n    # Heuristic 5: Give higher preference if it is an empty bin.\n    is_empty = bins_remain_cap == bin_capacity\n    priority[is_empty] += empty_bin_weight\n\n    # Heuristic 6: Bin Utilization\n    bin_utilization = (bin_capacity - bins_remain_cap) / bin_capacity\n    priority += bin_utilization_weight * bin_utilization\n\n    return priority",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 31.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 433.2579304308557,
    "mi": 79.47398562180626,
    "token_count": 331.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response9.txt_stdout.txt",
    "code_path": "problem_iter8_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity, waste, fragmentation, and empty bin considerations.\"\"\"\n\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Sufficient capacity boost\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += 1.8 #Sufficient cap weight\n\n    # Minimize waste boost\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0]\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += 1.2  #Min waste weight\n\n    # Nearly full bin boost\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= 1.1 * item) & (bins_remain_cap >= item) #threshold multiplier = 1.1\n    priority[nearly_full] += 2.4 #Nearly full weight\n\n    #Small capacity based on relative availability\n    small_cap = (bins_remain_cap >= item)\n    priority[small_cap] += (bins_remain_cap[small_cap] - item) / np.max(bins_remain_cap)\n\n    #Emptiness boost\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += 0.7 #Empty bins weight\n\n    return priority",
    "response_id": 9,
    "tryHS": false,
    "obj": 2.6924611088950936,
    "SLOC": 17.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 307.756981016698,
    "mi": 84.16086853866355,
    "token_count": 207.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response1.txt_stdout.txt",
    "code_path": "problem_iter9_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity, waste, fragmentation, and empty bin considerations with dynamic adjustments.\"\"\"\n\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    num_bins = len(bins_remain_cap)\n\n    # 1. Sufficient Capacity Boost with Item Size Consideration\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += 1.8\n\n    # 2. Minimize Waste Boost with Adaptive Waste Threshold\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0]\n\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += 1.2\n\n        #Adaptive waste threshold based on the item size\n        adaptive_waste_threshold = 0.2 * item\n        small_waste_bins = (waste >= 0) & (waste <= adaptive_waste_threshold)\n        priority[small_waste_bins] += 0.6 # Added small waste weight\n\n    # 3. Nearly Full Bin Boost with Dynamic Threshold\n    nearly_full_threshold = 1.1 # Initial threshold\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= nearly_full_threshold * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += 2.4\n\n    # 4. Small Capacity Based on Relative Availability (Normalized)\n    sufficient_bins = bins_remain_cap >= item\n    if np.any(sufficient_bins):\n        max_remaining_cap = np.max(bins_remain_cap[sufficient_bins])\n        priority[sufficient_bins] += (bins_remain_cap[sufficient_bins] - item) / max_remaining_cap #Normalizing\n    \n    # 5. Empty Bin Consideration with bin utilization\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += 0.7\n\n    #6. Penalize bins close to full but can't fit the item\n\n    cannot_fit = (bins_remain_cap < item) & (bins_remain_cap > 0)\n    priority[cannot_fit] -= 0.5\n\n    #7. Reward higher bin utilization globally (Encourage packing)\n\n    total_capacity = np.sum(bins_remain_cap)\n    bin_utilization_score = (num_bins - (total_capacity/np.max(bins_remain_cap)))/num_bins #Scale between 0 and 1\n    priority += 0.2 * bin_utilization_score\n\n    return priority",
    "response_id": 1,
    "tryHS": false,
    "obj": 2.9417630634224174,
    "SLOC": 31.0,
    "cyclomatic_complexity": 5.0,
    "halstead": 856.9045644567952,
    "mi": 74.43441032100107,
    "token_count": 376.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter10_response0.txt_stdout.txt",
    "code_path": "problem_iter10_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, nearly_full_threshold_multiplier: float = 1.4998962572658903, empty_bin_priority_boost: float = 0.5231127815881131, min_waste_priority: float = 2.4493668660249956, sufficient_capacity_priority: float = 1.4708959522600125, prevent_fragmentation_priority: float = 1.8570691214094701) -> np.ndarray:\n    \"\"\"Combines capacity ratios, waste minimization, and fragmentation avoidance.\"\"\"\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += sufficient_capacity_priority\n\n    # Heuristic 2: Minimize Waste (Only for bins with sufficient capacity)\n    waste = bins_remain_cap - item\n    positive_waste = waste[sufficient_capacity]\n    min_waste = np.min(positive_waste) if positive_waste.size > 0 else np.inf\n    min_waste_bins_idx = np.where((waste == min_waste) & sufficient_capacity)[0]\n    priority[min_waste_bins_idx] += min_waste_priority\n\n    # Heuristic 3: Capacity Ratio (Only for bins with sufficient capacity)\n    ratios = item / bins_remain_cap\n    ratios[~sufficient_capacity] = 0  # Ignore bins without sufficient capacity\n    priority += ratios\n\n    # Heuristic 4: Prevent Fragmentation (Bins near full have high priority).\n    nearly_full_threshold = nearly_full_threshold_multiplier * item  # Adjust as needed\n    nearly_full = (bins_remain_cap > 0) & (bins_remain_cap <= nearly_full_threshold) & (bins_remain_cap >= item)\n    priority[nearly_full] += prevent_fragmentation_priority\n\n    # Heuristic 5: Prioritize Emptier Bins for Larger Items\n    empty_bin_threshold = np.max(bins_remain_cap)\n    is_empty = bins_remain_cap == empty_bin_threshold\n    priority[is_empty] += empty_bin_priority_boost   # Slightly favor empty bins\n\n    return priority",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 19.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 287.2398489489181,
    "mi": 82.63225868936881,
    "token_count": 232.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response2.txt_stdout.txt",
    "code_path": "problem_iter11_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity, waste, fragmentation, and bin utilization for online bin packing.\"\"\"\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    num_bins = len(bins_remain_cap)\n\n    # 1. Sufficient Capacity Boost\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += 2.0\n\n    # 2. Minimize Waste\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0]\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += 1.5\n\n        adaptive_waste_threshold = 0.2 * item\n        small_waste_bins = (waste >= 0) & (waste <= adaptive_waste_threshold)\n        priority[small_waste_bins] += 0.7\n\n    # 3. Nearly Full Bin Boost\n    nearly_full_threshold = 1.1\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= nearly_full_threshold * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += 2.5\n\n    # 4. Small Capacity Based on Relative Availability\n    sufficient_bins = bins_remain_cap >= item\n    if np.any(sufficient_bins):\n        max_remaining_cap = np.max(bins_remain_cap[sufficient_bins])\n        priority[sufficient_bins] += (bins_remain_cap[sufficient_bins] - item) / max_remaining_cap\n\n    # 5. Empty Bin Consideration\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += 0.8\n\n    # 6. Penalize bins close to full but can't fit the item\n    cannot_fit = (bins_remain_cap < item) & (bins_remain_cap > 0)\n    priority[cannot_fit] -= 0.6\n\n    # 7. Reward higher bin utilization globally\n    total_capacity = np.sum(bins_remain_cap)\n    bin_utilization_score = (num_bins - (total_capacity/np.max(bins_remain_cap)))/num_bins\n    priority += 0.3 * bin_utilization_score\n\n    return priority",
    "response_id": 2,
    "tryHS": false,
    "obj": 3.948942959712818,
    "SLOC": 27.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 499.9129966509874,
    "mi": 73.36164052411662,
    "token_count": 274.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response0.txt_stdout.txt",
    "code_path": "problem_iter12_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"A refined priority function for online bin packing, focusing on balanced bin utilization and adaptive strategies.\"\"\"\n\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    bin_count = len(bins_remain_cap)\n    \n    # 1. Essential Fit: Guarantee capacity\n    sufficient_capacity = bins_remain_cap >= item\n    if not np.any(sufficient_capacity):\n        # If no bin fits, prioritize the least insufficient\n        priority = -np.abs(bins_remain_cap - item)  # Assign negative priority based on how much it overflows\n        return priority\n\n    priority[sufficient_capacity] += 1.0  # Base priority for fitting\n\n    # 2. Waste Optimization:\n    waste = bins_remain_cap - item\n    positive_waste = waste[sufficient_capacity]  # Consider only bins that fit\n    \n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where((waste == min_waste) & sufficient_capacity)[0]\n        priority[min_waste_bins_idx] += 2.0  # Strong preference for minimal waste\n\n    # 3. Near-Full Incentive (Adaptive threshold based on item size):\n    near_full_threshold = 1.1 * item  #Dynamic threshold\n    nearly_full = (bins_remain_cap >= item) & (bins_remain_cap <= near_full_threshold)\n    priority[nearly_full] += 1.5\n\n    # 4. Balancing: Reward bins that are neither too full nor too empty\n    bin_utilization = (np.max(bins_remain_cap) - bins_remain_cap) / np.max(bins_remain_cap)  # Calculate how full each bin is relative to fullest bin\n    \n    #Give higher score if the utilization is in an appropriate range\n    good_utilization = (bin_utilization >= 0.2) & (bin_utilization <= 0.8) #prevent overly empty or full bins\n    priority[good_utilization] += 0.8 #medium preference\n\n    # 5. Empty Bin Consideration (But penalize overuse)\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    if np.any(is_empty) and np.sum(bins_remain_cap < np.max(bins_remain_cap)) > bin_count // 3 : #ensure that you do not overuse empty bins. At least one-third of all bins need to have some value\n         priority[is_empty] += 0.5\n\n\n    return priority",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 27.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 527.5355568033491,
    "mi": 74.47480425348022,
    "token_count": 328.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter13_hs3.txt_stdout.txt",
    "code_path": "problem_iter13_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                 bins_remain_cap: np.ndarray,\n                 sufficient_capacity_weight: float = 4.164098163938444,\n                 min_waste_weight: float = 1.7981704950118487,\n                 nearly_full_threshold_multiplier: float = 1.4532477027434771,\n                 nearly_full_weight: float = 2.2997831001478364,\n                 empty_bin_weight: float = 1.19369803054251,\n                 min_bin_cap: float = 0.9392883625784341) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        sufficient_capacity_weight: Weight for bins with sufficient capacity.\n        min_waste_weight: Weight for bins that minimize waste.\n        nearly_full_threshold_multiplier: Multiplier for item size to determine nearly full threshold.\n        nearly_full_weight: Weight for bins that are nearly full.\n        empty_bin_weight: Weight for empty bins.\n        min_bin_cap: Minimum capacity of a bin to be considered.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    priority[sufficient_capacity] += sufficient_capacity_weight  # Base priority if bin can fit item\n\n    # Heuristic 2: Minimize Waste (Maximize filled space).\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0] # Consider only valid waste\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += min_waste_weight\n\n    # Heuristic 3: Prevent Fragmentation (Bins near full have high priority).\n    nearly_full = (bins_remain_cap > min_bin_cap) & (bins_remain_cap <= nearly_full_threshold_multiplier * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += nearly_full_weight\n\n    # Heuristic 4: Small amount of available capacity.\n    small_cap = (bins_remain_cap >= item)\n    priority[small_cap] += (bins_remain_cap[small_cap] - item) / np.max(bins_remain_cap) # Add scaled value based on relative small capacity.\n    \n    # Heuristic 5: Give higher preference if it is an empty bin.\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += empty_bin_weight\n\n    return priority",
    "response_id": 0,
    "tryHS": true,
    "obj": 3.8691663342640563,
    "SLOC": 24.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 310.19550008653874,
    "mi": 82.1899070972932,
    "token_count": 246.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response0.txt_stdout.txt",
    "code_path": "problem_iter14_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity, waste, fragmentation, and bin utilization with tunable weights.\"\"\"\n\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    num_bins = len(bins_remain_cap)\n\n    # Tunable weights\n    sufficient_cap_weight = 1.9\n    min_waste_weight = 1.3\n    small_waste_weight = 0.6\n    nearly_full_weight = 2.6\n    relative_cap_weight = 1.0\n    empty_bin_weight = 0.9\n    cannot_fit_penalty = 0.5\n    bin_utilization_weight = 0.4\n\n    # 1. Sufficient Capacity Boost\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += sufficient_cap_weight\n\n    # 2. Minimize Waste\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0]\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += min_waste_weight\n\n        adaptive_waste_threshold = 0.2 * item\n        small_waste_bins = (waste >= 0) & (waste <= adaptive_waste_threshold)\n        priority[small_waste_bins] += small_waste_weight\n\n    # 3. Nearly Full Bin Boost\n    nearly_full_threshold = 1.1\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= nearly_full_threshold * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += nearly_full_weight\n\n    # 4. Small Capacity Based on Relative Availability\n    sufficient_bins = bins_remain_cap >= item\n    if np.any(sufficient_bins):\n        max_remaining_cap = np.max(bins_remain_cap[sufficient_bins])\n        priority[sufficient_bins] += (bins_remain_cap[sufficient_bins] - item) / max_remaining_cap * relative_cap_weight\n\n    # 5. Empty Bin Consideration\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += empty_bin_weight\n\n    # 6. Penalize bins close to full but can't fit the item\n    cannot_fit = (bins_remain_cap < item) & (bins_remain_cap > 0)\n    priority[cannot_fit] -= cannot_fit_penalty\n\n    # 7. Reward higher bin utilization globally\n    total_capacity = np.sum(bins_remain_cap)\n    bin_utilization_score = (num_bins - (total_capacity/np.max(bins_remain_cap)))/num_bins\n    priority += bin_utilization_weight * bin_utilization_score\n\n    return priority",
    "response_id": 0,
    "tryHS": false,
    "obj": 2.9716792979656916,
    "SLOC": 18.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 307.35774857210805,
    "mi": 82.52988615426216,
    "token_count": 197.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response1.txt_stdout.txt",
    "code_path": "problem_iter15_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Enhanced priority function combining capacity, waste, fragmentation, and bin utilization,\n    with adaptive weighting based on item size and bin availability.\"\"\"\n\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    num_bins = len(bins_remain_cap)\n\n    # 1. Sufficient Capacity Boost (Adaptive)\n    sufficient_capacity = bins_remain_cap >= item\n    if np.any(sufficient_capacity):\n        priority[sufficient_capacity] += 2.0  # Increased weight for sufficient capacity\n    else:\n        # If no bin has sufficient capacity, slightly penalize all bins to encourage opening a new one\n        priority -= 0.1\n    \n\n    # 2. Minimize Waste (Refined)\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0]\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += 1.5 # Increased weight for min waste\n    \n\n    # 3. Nearly Full Bin Boost (Context-Aware)\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= 1.1 * item) & (bins_remain_cap >= item)\n    if np.any(nearly_full):\n         priority[nearly_full] += 2.5 # Further increased weight for nearly full\n    \n\n    # 4. Bin Utilization (Proportional) - Encourages packing into bins that are already somewhat full\n    bin_utilization = (np.max(bins_remain_cap) - bins_remain_cap) / np.max(bins_remain_cap)\n    priority += 0.8 * bin_utilization  # Weight the utilization boost\n\n\n    # 5. Empty Bin Consideration (Strategic)\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    if np.any(is_empty) and item > 0.5 * np.max(bins_remain_cap): # only if item is large enough\n        priority[is_empty] += 0.9 # Increased weight for empty bins for larger items.\n    elif np.any(is_empty):\n        priority[is_empty] -= 0.2 # slight penalty if item is small, let other bin be filled first\n\n\n    # 6. Fragmentation Avoidance (Dynamic Penalty)\n    # Penalize bins that, after packing, would leave a small, unusable space\n    remaining_after_pack = bins_remain_cap - item\n    highly_fragmented = (remaining_after_pack > 0) & (remaining_after_pack < 0.1 * np.max(bins_remain_cap))\n    priority[highly_fragmented] -= 1.3  # Apply a penalty for high fragmentation\n   \n    return priority",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.038691663342641,
    "SLOC": 22.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 532.6966022743776,
    "mi": 78.83913287808235,
    "token_count": 268.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter16_hs4.txt_stdout.txt",
    "code_path": "problem_iter16_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                 bins_remain_cap: np.ndarray,\n                 sufficient_capacity_weight: float = 3.23500175425229,\n                 min_waste_weight: float = 2.509575567352772,\n                 nearly_full_threshold_multiplier: float = 1.7419906104563585,\n                 nearly_full_weight: float = 4.445492173021169,\n                 empty_bin_weight: float = 3.763034250316775,\n                 min_bin_capacity: float = 0.2711488311096898) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        sufficient_capacity_weight: Weight for bins with sufficient capacity.\n        min_waste_weight: Weight for bins that minimize waste.\n        nearly_full_threshold_multiplier: Multiplier for item size to determine nearly full threshold.\n        nearly_full_weight: Weight for bins that are nearly full.\n        empty_bin_weight: Weight for empty bins.\n        min_bin_capacity: Minimum bin capacity to consider.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    priority[sufficient_capacity] += sufficient_capacity_weight  # Base priority if bin can fit item\n\n    # Heuristic 2: Minimize Waste (Maximize filled space).\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0] # Consider only valid waste\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += min_waste_weight\n\n    # Heuristic 3: Prevent Fragmentation (Bins near full have high priority).\n    nearly_full = (bins_remain_cap > min_bin_capacity) & (bins_remain_cap <= nearly_full_threshold_multiplier * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += nearly_full_weight\n\n    # Heuristic 4: Small amount of available capacity.\n    small_cap = (bins_remain_cap >= item)\n    priority[small_cap] += (bins_remain_cap[small_cap] - item) / np.max(bins_remain_cap) # Add scaled value based on relative small capacity.\n    \n    # Heuristic 5: Give higher preference if it is an empty bin.\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += empty_bin_weight\n\n    return priority",
    "response_id": 0,
    "tryHS": true,
    "obj": 3.9589150378939015,
    "SLOC": 24.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 310.19550008653874,
    "mi": 82.1899070972932,
    "token_count": 246.0,
    "exec_success": true
  }
]