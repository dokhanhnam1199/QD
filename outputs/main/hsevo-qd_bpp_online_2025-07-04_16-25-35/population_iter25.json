[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    ratios = item / bins_remain_cap\n    log_ratios = np.log(ratios)\n    priorities = -log_ratios\n    return priorities",
    "response_id": 0,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 11.60964047443681,
    "mi": 94.04446327225541,
    "token_count": 47.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    priority[sufficient_capacity] += 1  # Base priority if bin can fit item\n\n    # Heuristic 2: Minimize Waste (Maximize filled space).\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0] # Consider only valid waste\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += 2\n\n    # Heuristic 3: Prevent Fragmentation (Bins near full have high priority).\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= 2 * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += 1.5\n\n    # Heuristic 4: Small amount of available capacity.\n    small_cap = (bins_remain_cap >= item)\n    priority[small_cap] += (bins_remain_cap[small_cap] - item) / np.max(bins_remain_cap) # Add scaled value based on relative small capacity.\n    \n    # Heuristic 5: Give higher preference if it is an empty bin.\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += 0.5\n\n    return priority",
    "response_id": 1,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 53.77443751081735,
    "mi": 67.91077893705676,
    "token_count": 88.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity ratios with waste minimization for bin prioritization.\"\"\"\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += 1\n\n    # Heuristic 2: Minimize Waste (Only for bins with sufficient capacity)\n    waste = bins_remain_cap - item\n    min_waste = np.min(waste[sufficient_capacity]) if np.any(sufficient_capacity) else np.inf\n    min_waste_bins_idx = np.where((waste == min_waste) & sufficient_capacity)[0]\n    priority[min_waste_bins_idx] += 2\n\n    # Heuristic 3: Capacity Ratio (Only for bins with sufficient capacity)\n    ratios = item / bins_remain_cap\n    ratios[~sufficient_capacity] = 0  # Ignore bins without sufficient capacity\n    priority += ratios\n\n    return priority",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 144.75398259382442,
    "mi": 86.58537321553837,
    "token_count": 155.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response0.txt_stdout.txt",
    "code_path": "problem_iter3_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Heuristic 1: Feasibility (Essential)\n    feasible = bins_remain_cap >= item\n    if not np.any(feasible):\n        return priority  # No feasible bin, all priorities remain 0\n\n    priority[feasible] += 1.0 # Base priority for feasible bins\n\n    # Heuristic 2: Minimize Waste (First-Fit Decreasing variant)\n    waste = bins_remain_cap - item\n    positive_waste = waste[feasible] # Only consider waste in feasible bins\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where((waste == min_waste) & feasible)[0]  #Ensure we choose within the feasible set\n        priority[min_waste_bins_idx] += 3.0 # Higher priority for minimizing waste\n\n    # Heuristic 3: Avoid Fragmentation (Balancing Bin Usage)\n    # Give a slightly higher priority to bins that, after placing the item,\n    # will have a remaining capacity in the middle range of possible remaining capacities.\n    post_fill_capacities = bins_remain_cap - item\n    post_fill_capacities_feasible = post_fill_capacities[feasible]\n    if len(post_fill_capacities_feasible) > 0:\n        median_capacity = np.median(post_fill_capacities_feasible)\n        capacity_diff = np.abs(post_fill_capacities - median_capacity)\n        # Scale the priority boost inversely proportional to the capacity difference.\n        # Bins closer to the median capacity get a higher boost.\n        priority[feasible] += np.clip(1.0 - capacity_diff[feasible] / np.max(bins_remain_cap), 0, 1) #Scale and clip. Prevents negative values\n        \n\n    # Heuristic 4: Prioritize emptier bins if the item is relatively large.\n    # This helps distribute items more evenly across bins initially.\n    if item > np.mean(bins_remain_cap): # Scale by item size relative to average capacity.\n        empty_bin_indices = np.where(bins_remain_cap == np.max(bins_remain_cap))[0]\n        priority[empty_bin_indices] += 1.5\n\n    # Heuristic 5: Add a slight randomness to break ties and explore the solution space.\n    priority[feasible] += np.random.rand(np.sum(feasible)) * 0.1\n\n    return priority",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 26.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 385.74374433250205,
    "mi": 76.827568796314,
    "token_count": 299.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response1.txt_stdout.txt",
    "code_path": "problem_iter7_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray,\n                 sufficient_capacity_weight: float = 1.9596220015949373,\n                 min_waste_weight: float = 4.237844712440302,\n                 nearly_full_threshold_multiplier: float = 2.0406124655988833,\n                 nearly_full_weight: float = 4.224367911090436,\n                 empty_bin_weight: float = 4.8338513308698) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        sufficient_capacity_weight: Weight for bins with sufficient capacity.\n        min_waste_weight: Weight for bins that minimize waste.\n        nearly_full_threshold_multiplier: Multiplier for item size to determine nearly full threshold.\n        nearly_full_weight: Weight for bins that are nearly full.\n        empty_bin_weight: Weight for empty bins.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    priority[sufficient_capacity] += sufficient_capacity_weight  # Base priority if bin can fit item\n\n    # Heuristic 2: Minimize Waste (Maximize filled space).\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0] # Consider only valid waste\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += min_waste_weight\n\n    # Heuristic 3: Prevent Fragmentation (Bins near full have high priority).\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= nearly_full_threshold_multiplier * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += nearly_full_weight\n\n    # Heuristic 4: Small amount of available capacity.\n    small_cap = (bins_remain_cap >= item)\n    priority[small_cap] += (bins_remain_cap[small_cap] - item) / np.max(bins_remain_cap) # Add scaled value based on relative small capacity.\n    \n    # Heuristic 5: Give higher preference if it is an empty bin.\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += empty_bin_weight\n\n    return priority",
    "response_id": 1,
    "tryHS": true,
    "obj": 3.8791384124451627,
    "SLOC": 22.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 307.756981016698,
    "mi": 81.80209096280826,
    "token_count": 238.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response0.txt_stdout.txt",
    "code_path": "problem_iter5_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity ratios, waste minimization, and fragmentation avoidance.\"\"\"\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += 1\n\n    # Heuristic 2: Minimize Waste (Only for bins with sufficient capacity)\n    waste = bins_remain_cap - item\n    positive_waste = waste[sufficient_capacity]\n    min_waste = np.min(positive_waste) if positive_waste.size > 0 else np.inf\n    min_waste_bins_idx = np.where((waste == min_waste) & sufficient_capacity)[0]\n    priority[min_waste_bins_idx] += 2\n\n    # Heuristic 3: Capacity Ratio (Only for bins with sufficient capacity)\n    ratios = item / bins_remain_cap\n    ratios[~sufficient_capacity] = 0  # Ignore bins without sufficient capacity\n    priority += ratios\n\n    # Heuristic 4: Prevent Fragmentation (Bins near full have high priority).\n    nearly_full_threshold = 1.2 * item  # Adjust as needed\n    nearly_full = (bins_remain_cap > 0) & (bins_remain_cap <= nearly_full_threshold) & (bins_remain_cap >= item)\n    priority[nearly_full] += 1.5\n\n    # Heuristic 5: Prioritize Emptier Bins for Larger Items\n    empty_bin_threshold = np.max(bins_remain_cap)\n    is_empty = bins_remain_cap == empty_bin_threshold\n    priority[is_empty] += 0.8   # Slightly favor empty bins\n\n    return priority",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 309.6277232931715,
    "mi": 79.19005722310375,
    "token_count": 233.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response0.txt_stdout.txt",
    "code_path": "problem_iter6_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray,\n                  sufficient_capacity_weight: float = 1.8066941616372154,\n                  min_waste_weight: float = 1.2369759055947915,\n                  nearly_full_threshold_multiplier: float = 1.164912650607926,\n                  nearly_full_weight: float = 2.4304538888032448,\n                  empty_bin_weight: float = 0.7700931706376539,\n                  waste_capacity_ratio_weight: float = 0.5,\n                  bin_utilization_weight: float = 1.0) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        sufficient_capacity_weight: Weight for bins with sufficient capacity.\n        min_waste_weight: Weight for bins that minimize waste.\n        nearly_full_threshold_multiplier: Multiplier for item size to determine nearly full threshold.\n        nearly_full_weight: Weight for bins that are nearly full.\n        empty_bin_weight: Weight for empty bins.\n        waste_capacity_ratio_weight: Weight for the ratio of waste to bin capacity\n        bin_utilization_weight: Weight for the current bin utilization\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    bin_capacity = np.max(bins_remain_cap)\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    priority[sufficient_capacity] += sufficient_capacity_weight  # Base priority if bin can fit item\n\n    # Heuristic 2: Minimize Waste (Maximize filled space).\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0] # Consider only valid waste\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += min_waste_weight\n\n    # Heuristic 3: Prevent Fragmentation (Bins near full have high priority).\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= nearly_full_threshold_multiplier * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += nearly_full_weight\n\n    # Heuristic 4: Small amount of available capacity and weight by normalized waste\n    sufficient_bins = bins_remain_cap >= item\n    if np.any(sufficient_bins):\n        waste_ratios = waste[sufficient_bins] / bin_capacity\n        priority[sufficient_bins] += waste_capacity_ratio_weight * (1 - waste_ratios)\n    \n    # Heuristic 5: Give higher preference if it is an empty bin.\n    is_empty = bins_remain_cap == bin_capacity\n    priority[is_empty] += empty_bin_weight\n\n    # Heuristic 6: Bin Utilization\n    bin_utilization = (bin_capacity - bins_remain_cap) / bin_capacity\n    priority += bin_utilization_weight * bin_utilization\n\n    return priority",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 31.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 433.2579304308557,
    "mi": 79.47398562180626,
    "token_count": 331.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response9.txt_stdout.txt",
    "code_path": "problem_iter8_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity, waste, fragmentation, and empty bin considerations.\"\"\"\n\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Sufficient capacity boost\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += 1.8 #Sufficient cap weight\n\n    # Minimize waste boost\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0]\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += 1.2  #Min waste weight\n\n    # Nearly full bin boost\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= 1.1 * item) & (bins_remain_cap >= item) #threshold multiplier = 1.1\n    priority[nearly_full] += 2.4 #Nearly full weight\n\n    #Small capacity based on relative availability\n    small_cap = (bins_remain_cap >= item)\n    priority[small_cap] += (bins_remain_cap[small_cap] - item) / np.max(bins_remain_cap)\n\n    #Emptiness boost\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += 0.7 #Empty bins weight\n\n    return priority",
    "response_id": 9,
    "tryHS": false,
    "obj": 2.6924611088950936,
    "SLOC": 17.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 307.756981016698,
    "mi": 84.16086853866355,
    "token_count": 207.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response1.txt_stdout.txt",
    "code_path": "problem_iter9_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity, waste, fragmentation, and empty bin considerations with dynamic adjustments.\"\"\"\n\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    num_bins = len(bins_remain_cap)\n\n    # 1. Sufficient Capacity Boost with Item Size Consideration\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += 1.8\n\n    # 2. Minimize Waste Boost with Adaptive Waste Threshold\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0]\n\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += 1.2\n\n        #Adaptive waste threshold based on the item size\n        adaptive_waste_threshold = 0.2 * item\n        small_waste_bins = (waste >= 0) & (waste <= adaptive_waste_threshold)\n        priority[small_waste_bins] += 0.6 # Added small waste weight\n\n    # 3. Nearly Full Bin Boost with Dynamic Threshold\n    nearly_full_threshold = 1.1 # Initial threshold\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= nearly_full_threshold * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += 2.4\n\n    # 4. Small Capacity Based on Relative Availability (Normalized)\n    sufficient_bins = bins_remain_cap >= item\n    if np.any(sufficient_bins):\n        max_remaining_cap = np.max(bins_remain_cap[sufficient_bins])\n        priority[sufficient_bins] += (bins_remain_cap[sufficient_bins] - item) / max_remaining_cap #Normalizing\n    \n    # 5. Empty Bin Consideration with bin utilization\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += 0.7\n\n    #6. Penalize bins close to full but can't fit the item\n\n    cannot_fit = (bins_remain_cap < item) & (bins_remain_cap > 0)\n    priority[cannot_fit] -= 0.5\n\n    #7. Reward higher bin utilization globally (Encourage packing)\n\n    total_capacity = np.sum(bins_remain_cap)\n    bin_utilization_score = (num_bins - (total_capacity/np.max(bins_remain_cap)))/num_bins #Scale between 0 and 1\n    priority += 0.2 * bin_utilization_score\n\n    return priority",
    "response_id": 1,
    "tryHS": false,
    "obj": 2.9417630634224174,
    "SLOC": 31.0,
    "cyclomatic_complexity": 5.0,
    "halstead": 856.9045644567952,
    "mi": 74.43441032100107,
    "token_count": 376.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter10_response0.txt_stdout.txt",
    "code_path": "problem_iter10_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, nearly_full_threshold_multiplier: float = 1.4998962572658903, empty_bin_priority_boost: float = 0.5231127815881131, min_waste_priority: float = 2.4493668660249956, sufficient_capacity_priority: float = 1.4708959522600125, prevent_fragmentation_priority: float = 1.8570691214094701) -> np.ndarray:\n    \"\"\"Combines capacity ratios, waste minimization, and fragmentation avoidance.\"\"\"\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += sufficient_capacity_priority\n\n    # Heuristic 2: Minimize Waste (Only for bins with sufficient capacity)\n    waste = bins_remain_cap - item\n    positive_waste = waste[sufficient_capacity]\n    min_waste = np.min(positive_waste) if positive_waste.size > 0 else np.inf\n    min_waste_bins_idx = np.where((waste == min_waste) & sufficient_capacity)[0]\n    priority[min_waste_bins_idx] += min_waste_priority\n\n    # Heuristic 3: Capacity Ratio (Only for bins with sufficient capacity)\n    ratios = item / bins_remain_cap\n    ratios[~sufficient_capacity] = 0  # Ignore bins without sufficient capacity\n    priority += ratios\n\n    # Heuristic 4: Prevent Fragmentation (Bins near full have high priority).\n    nearly_full_threshold = nearly_full_threshold_multiplier * item  # Adjust as needed\n    nearly_full = (bins_remain_cap > 0) & (bins_remain_cap <= nearly_full_threshold) & (bins_remain_cap >= item)\n    priority[nearly_full] += prevent_fragmentation_priority\n\n    # Heuristic 5: Prioritize Emptier Bins for Larger Items\n    empty_bin_threshold = np.max(bins_remain_cap)\n    is_empty = bins_remain_cap == empty_bin_threshold\n    priority[is_empty] += empty_bin_priority_boost   # Slightly favor empty bins\n\n    return priority",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 19.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 287.2398489489181,
    "mi": 82.63225868936881,
    "token_count": 232.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response2.txt_stdout.txt",
    "code_path": "problem_iter11_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity, waste, fragmentation, and bin utilization for online bin packing.\"\"\"\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    num_bins = len(bins_remain_cap)\n\n    # 1. Sufficient Capacity Boost\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += 2.0\n\n    # 2. Minimize Waste\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0]\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += 1.5\n\n        adaptive_waste_threshold = 0.2 * item\n        small_waste_bins = (waste >= 0) & (waste <= adaptive_waste_threshold)\n        priority[small_waste_bins] += 0.7\n\n    # 3. Nearly Full Bin Boost\n    nearly_full_threshold = 1.1\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= nearly_full_threshold * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += 2.5\n\n    # 4. Small Capacity Based on Relative Availability\n    sufficient_bins = bins_remain_cap >= item\n    if np.any(sufficient_bins):\n        max_remaining_cap = np.max(bins_remain_cap[sufficient_bins])\n        priority[sufficient_bins] += (bins_remain_cap[sufficient_bins] - item) / max_remaining_cap\n\n    # 5. Empty Bin Consideration\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += 0.8\n\n    # 6. Penalize bins close to full but can't fit the item\n    cannot_fit = (bins_remain_cap < item) & (bins_remain_cap > 0)\n    priority[cannot_fit] -= 0.6\n\n    # 7. Reward higher bin utilization globally\n    total_capacity = np.sum(bins_remain_cap)\n    bin_utilization_score = (num_bins - (total_capacity/np.max(bins_remain_cap)))/num_bins\n    priority += 0.3 * bin_utilization_score\n\n    return priority",
    "response_id": 2,
    "tryHS": false,
    "obj": 3.948942959712818,
    "SLOC": 27.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 499.9129966509874,
    "mi": 73.36164052411662,
    "token_count": 274.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response0.txt_stdout.txt",
    "code_path": "problem_iter12_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"A refined priority function for online bin packing, focusing on balanced bin utilization and adaptive strategies.\"\"\"\n\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    bin_count = len(bins_remain_cap)\n    \n    # 1. Essential Fit: Guarantee capacity\n    sufficient_capacity = bins_remain_cap >= item\n    if not np.any(sufficient_capacity):\n        # If no bin fits, prioritize the least insufficient\n        priority = -np.abs(bins_remain_cap - item)  # Assign negative priority based on how much it overflows\n        return priority\n\n    priority[sufficient_capacity] += 1.0  # Base priority for fitting\n\n    # 2. Waste Optimization:\n    waste = bins_remain_cap - item\n    positive_waste = waste[sufficient_capacity]  # Consider only bins that fit\n    \n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where((waste == min_waste) & sufficient_capacity)[0]\n        priority[min_waste_bins_idx] += 2.0  # Strong preference for minimal waste\n\n    # 3. Near-Full Incentive (Adaptive threshold based on item size):\n    near_full_threshold = 1.1 * item  #Dynamic threshold\n    nearly_full = (bins_remain_cap >= item) & (bins_remain_cap <= near_full_threshold)\n    priority[nearly_full] += 1.5\n\n    # 4. Balancing: Reward bins that are neither too full nor too empty\n    bin_utilization = (np.max(bins_remain_cap) - bins_remain_cap) / np.max(bins_remain_cap)  # Calculate how full each bin is relative to fullest bin\n    \n    #Give higher score if the utilization is in an appropriate range\n    good_utilization = (bin_utilization >= 0.2) & (bin_utilization <= 0.8) #prevent overly empty or full bins\n    priority[good_utilization] += 0.8 #medium preference\n\n    # 5. Empty Bin Consideration (But penalize overuse)\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    if np.any(is_empty) and np.sum(bins_remain_cap < np.max(bins_remain_cap)) > bin_count // 3 : #ensure that you do not overuse empty bins. At least one-third of all bins need to have some value\n         priority[is_empty] += 0.5\n\n\n    return priority",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 27.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 527.5355568033491,
    "mi": 74.47480425348022,
    "token_count": 328.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter22_response0.txt_stdout.txt",
    "code_path": "problem_iter22_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                 bins_remain_cap: np.ndarray,\n                 sufficient_capacity_weight: float = 1.1647927566871803,\n                 min_waste_weight: float = 3.6867759800227633,\n                 nearly_full_threshold_multiplier: float = 1.5742237894667817,\n                 nearly_full_weight: float = 3.54118135517663,\n                 empty_bin_weight: float = 0.616318967352163,\n                 min_bin_cap_threshold: float = 0.20093388504503917) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        sufficient_capacity_weight: Weight for bins with sufficient capacity.\n        min_waste_weight: Weight for bins that minimize waste.\n        nearly_full_threshold_multiplier: Multiplier for item size to determine nearly full threshold.\n        nearly_full_weight: Weight for bins that are nearly full.\n        empty_bin_weight: Weight for empty bins.\n        min_bin_cap_threshold: Minimum capacity of the bin to be considered.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    priority[sufficient_capacity] += sufficient_capacity_weight  # Base priority if bin can fit item\n\n    # Heuristic 2: Minimize Waste (Maximize filled space).\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0] # Consider only valid waste\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += min_waste_weight\n\n    # Heuristic 3: Prevent Fragmentation (Bins near full have high priority).\n    nearly_full = (bins_remain_cap > min_bin_cap_threshold) & (bins_remain_cap <= nearly_full_threshold_multiplier * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += nearly_full_weight\n\n    # Heuristic 4: Small amount of available capacity.\n    small_cap = (bins_remain_cap >= item)\n    priority[small_cap] += (bins_remain_cap[small_cap] - item) / np.max(bins_remain_cap) # Add scaled value based on relative small capacity.\n    \n    # Heuristic 5: Give higher preference if it is an empty bin.\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += empty_bin_weight\n\n    return priority",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 24.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 310.19550008653874,
    "mi": 82.1899070972932,
    "token_count": 246.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response0.txt_stdout.txt",
    "code_path": "problem_iter14_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity, waste, fragmentation, and bin utilization with tunable weights.\"\"\"\n\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    num_bins = len(bins_remain_cap)\n\n    # Tunable weights\n    sufficient_cap_weight = 1.9\n    min_waste_weight = 1.3\n    small_waste_weight = 0.6\n    nearly_full_weight = 2.6\n    relative_cap_weight = 1.0\n    empty_bin_weight = 0.9\n    cannot_fit_penalty = 0.5\n    bin_utilization_weight = 0.4\n\n    # 1. Sufficient Capacity Boost\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += sufficient_cap_weight\n\n    # 2. Minimize Waste\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0]\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += min_waste_weight\n\n        adaptive_waste_threshold = 0.2 * item\n        small_waste_bins = (waste >= 0) & (waste <= adaptive_waste_threshold)\n        priority[small_waste_bins] += small_waste_weight\n\n    # 3. Nearly Full Bin Boost\n    nearly_full_threshold = 1.1\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= nearly_full_threshold * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += nearly_full_weight\n\n    # 4. Small Capacity Based on Relative Availability\n    sufficient_bins = bins_remain_cap >= item\n    if np.any(sufficient_bins):\n        max_remaining_cap = np.max(bins_remain_cap[sufficient_bins])\n        priority[sufficient_bins] += (bins_remain_cap[sufficient_bins] - item) / max_remaining_cap * relative_cap_weight\n\n    # 5. Empty Bin Consideration\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += empty_bin_weight\n\n    # 6. Penalize bins close to full but can't fit the item\n    cannot_fit = (bins_remain_cap < item) & (bins_remain_cap > 0)\n    priority[cannot_fit] -= cannot_fit_penalty\n\n    # 7. Reward higher bin utilization globally\n    total_capacity = np.sum(bins_remain_cap)\n    bin_utilization_score = (num_bins - (total_capacity/np.max(bins_remain_cap)))/num_bins\n    priority += bin_utilization_weight * bin_utilization_score\n\n    return priority",
    "response_id": 0,
    "tryHS": false,
    "obj": 2.9716792979656916,
    "SLOC": 18.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 307.35774857210805,
    "mi": 82.52988615426216,
    "token_count": 197.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response6.txt_stdout.txt",
    "code_path": "problem_iter17_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity, waste, fragmentation, and adaptive weighting based on bin stats.\"\"\"\n\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Adaptive weights based on bin utilization\n    avg_cap = np.mean(bins_remain_cap)\n    cap_std = np.std(bins_remain_cap)\n\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += 1.0  # Base priority\n\n    # Heuristic 2: Minimize Waste (with adaptive waste threshold)\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0]\n\n    if len(positive_waste) > 0:\n        # Adaptive waste threshold based on average remaining capacity\n        adaptive_waste_threshold = max(0.1 * avg_cap, 0.05) # Minimum waste size considered \"good\"\n        valid_waste = positive_waste[positive_waste <= adaptive_waste_threshold]\n\n        if len(valid_waste) > 0:\n            min_valid_waste = np.min(valid_waste)\n            min_waste_bins_idx = np.where(waste == min_valid_waste)[0]\n            priority[min_waste_bins_idx] += 2.0  # Higher priority for truly small waste\n\n    # Heuristic 3: Prevent Fragmentation (Bins near full have high priority)\n    nearly_full_threshold = 2 * item # Dynamic threshold relative to item size\n    nearly_full = (bins_remain_cap >= item) & (bins_remain_cap <= nearly_full_threshold)\n    priority[nearly_full] += 1.5\n\n    # Heuristic 4: Empty bin consideration\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += 0.5\n\n    # Adaptive Scaling to prevent domination by any single heuristic\n    priority /= np.max(priority) if np.max(priority) > 0 else 1.0\n\n    return priority",
    "response_id": 6,
    "tryHS": true,
    "obj": 3.5700039888312682,
    "SLOC": 22.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 546.8326336496443,
    "mi": 79.65507543291712,
    "token_count": 258.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter18_response0.txt_stdout.txt",
    "code_path": "problem_iter18_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_history: np.ndarray = None) -> np.ndarray:\n    \"\"\"\n    An enhanced priority function for online bin packing, incorporating dynamic adjustments\n    based on bin history and item size relative to bin capacity.  It adaptively prioritizes\n    bins based on remaining capacity, waste minimization, fragmentation avoidance, and\n    bin utilization history.  The weights are tuned to balance these competing objectives.\n\n    Args:\n        item (float): The size of the item to be packed.\n        bins_remain_cap (np.ndarray): A NumPy array containing the remaining capacity of each bin.\n        bin_history (np.ndarray, optional): A 2D NumPy array containing historical data about bins utilization. Defaults to None.\n\n    Returns:\n        np.ndarray: A NumPy array containing the priority score for each bin.\n    \"\"\"\n\n    num_bins = len(bins_remain_cap)\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # 1. Capacity-Based Prioritization (Adaptive to Item Size)\n    capacity_ratio = item / bins_remain_cap\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += 1.5 * (1 - capacity_ratio[sufficient_capacity])  # Higher priority for bins only slightly larger than the item\n\n    # 2. Waste Minimization (Optimized Waste Range)\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0]\n\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += 1.3  # Boost for bins minimizing immediate waste\n\n        # Scale waste priority based on relative waste amount\n        waste_normalized = positive_waste / np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else np.zeros_like(positive_waste)\n        waste_priority = 1.0 - waste_normalized #Invert values, smaller waste = higher value\n        waste_bins_idx = np.where(waste >=0)[0]\n        priority[waste_bins_idx] += waste_priority\n\n    # 3. Fragmentation Avoidance (Targeting Near-Full Bins, Tuned Threshold)\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= 1.05 * item) & (bins_remain_cap >= item) # Reduced threshold\n    priority[nearly_full] += 2.2  # Prioritize filling bins close to full\n\n    # 4. Empty Bin Consideration (Delayed Gratification)\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += 0.6  # Slightly less aggressive, to avoid premature commitment\n\n    # 5. Bin History Integration (If available) - Encouraging Balanced Usage\n    if bin_history is not None:\n        # Assuming bin_history contains load counts per bin. This part is dummy.\n        # Replace with real logic if you can generate or access that data\n        bin_utilization = np.sum(bin_history, axis=1) if bin_history.ndim > 1 else bin_history #Dummy usage, replace with actual counts\n        normalized_utilization = (bin_utilization - np.min(bin_utilization)) / (np.max(bin_utilization) - np.min(bin_utilization) + 1e-9)  # Normalize to [0, 1]\n\n        # Bias towards less utilized bins\n        priority += (1.0 - normalized_utilization) * 0.4 #Scale factor\n\n    return priority",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 29.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 547.242212941545,
    "mi": 76.95087827432599,
    "token_count": 314.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter19_hs4.txt_stdout.txt",
    "code_path": "problem_iter19_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray,\n                 sufficient_capacity_weight: float = 3.7931009388935824,\n                 min_waste_weight: float = 3.2195618083045288,\n                 nearly_full_threshold_multiplier: float = 2.082935278278309,\n                 nearly_full_weight: float = 4.327860682772133,\n                 empty_bin_weight: float = 1.7726770254941546,\n                 min_bins_remain_cap: float = 0.14805011680396876) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        sufficient_capacity_weight: Weight for bins with sufficient capacity.\n        min_waste_weight: Weight for bins that minimize waste.\n        nearly_full_threshold_multiplier: Multiplier for item size to determine nearly full threshold.\n        nearly_full_weight: Weight for bins that are nearly full.\n        empty_bin_weight: Weight for empty bins.\n        min_bins_remain_cap: Minimum value of bins_remain_cap\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    priority[sufficient_capacity] += sufficient_capacity_weight  # Base priority if bin can fit item\n\n    # Heuristic 2: Minimize Waste (Maximize filled space).\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0] # Consider only valid waste\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += min_waste_weight\n\n    # Heuristic 3: Prevent Fragmentation (Bins near full have high priority).\n    nearly_full = (bins_remain_cap > min_bins_remain_cap) & (bins_remain_cap <= nearly_full_threshold_multiplier * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += nearly_full_weight\n\n    # Heuristic 4: Small amount of available capacity.\n    small_cap = (bins_remain_cap >= item)\n    priority[small_cap] += (bins_remain_cap[small_cap] - item) / np.max(bins_remain_cap) # Add scaled value based on relative small capacity.\n    \n    # Heuristic 5: Give higher preference if it is an empty bin.\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += empty_bin_weight\n\n    return priority",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 23.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 310.19550008653874,
    "mi": 81.72843291011073,
    "token_count": 245.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response3.txt_stdout.txt",
    "code_path": "problem_iter20_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on capacity, waste, fragmentation, and utilization.\n    Weights are tuned for balanced performance and adapts to item size.\"\"\"\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    num_bins = len(bins_remain_cap)\n\n    # Tunable weights\n    sufficient_cap_weight = 1.9\n    min_waste_weight = 1.3\n    small_waste_weight = 0.6\n    nearly_full_weight = 2.6\n    relative_cap_weight = 1.0\n    empty_bin_weight = 0.9\n    cannot_fit_penalty = 0.5\n    bin_utilization_weight = 0.4\n\n    # 1. Sufficient Capacity Boost\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += sufficient_cap_weight\n\n    # 2. Minimize Waste\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0]\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += min_waste_weight\n\n        adaptive_waste_threshold = 0.2 * item\n        small_waste_bins = (waste >= 0) & (waste <= adaptive_waste_threshold)\n        priority[small_waste_bins] += small_waste_weight\n\n    # 3. Nearly Full Bin Boost\n    nearly_full_threshold = 1.1\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= nearly_full_threshold * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += nearly_full_weight\n\n    # 4. Small Capacity Based on Relative Availability\n    sufficient_bins = bins_remain_cap >= item\n    if np.any(sufficient_bins):\n        max_remaining_cap = np.max(bins_remain_cap[sufficient_bins])\n        priority[sufficient_bins] += (bins_remain_cap[sufficient_bins] - item) / max_remaining_cap * relative_cap_weight\n\n    # 5. Empty Bin Consideration\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += empty_bin_weight\n\n    # 6. Penalize bins close to full but can't fit the item\n    cannot_fit = (bins_remain_cap < item) & (bins_remain_cap > 0)\n    priority[cannot_fit] -= cannot_fit_penalty\n\n    # 7. Reward higher bin utilization globally\n    total_capacity = np.sum(bins_remain_cap)\n    bin_utilization_score = (num_bins - (total_capacity/np.max(bins_remain_cap)))/num_bins\n    priority += bin_utilization_weight * bin_utilization_score\n\n    return priority",
    "response_id": 3,
    "tryHS": false,
    "obj": 2.9716792979656916,
    "SLOC": 29.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 589.6986907795153,
    "mi": 73.87649203705188,
    "token_count": 318.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter21_response0.txt_stdout.txt",
    "code_path": "problem_iter21_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity, waste, fragmentation, and empty bin considerations with adaptive weights.\"\"\"\n\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Adaptive weights based on item size (example: smaller items favor minimal waste)\n    item_size_factor = min(1.0, item)  # Scale down influence for larger items\n\n    # Sufficient capacity boost (slightly increased weight)\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += 2.0 * (1 + 0.2 * item_size_factor)  # Increased weight + item size influence\n\n    # Minimize waste boost (adaptive weight)\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0]\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += 1.5 * (1 - 0.3 * item_size_factor)  # Adjusted weight, favors smaller items\n\n        # Add a slight penalty for larger waste (discourages creating very fragmented bins)\n        large_waste = waste > 0.5 #tuneable threshold\n        priority[large_waste] -= 0.1 * item_size_factor  # Item size influence, smaller item less fragmented\n    \n    # Nearly full bin boost (strongest boost, less adaptive)\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= 1.1 * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += 2.8\n\n    # Small capacity based on relative availability (normalized and scaled)\n    small_cap = (bins_remain_cap >= item)\n    if np.any(small_cap):  # Avoid division by zero if no bins have enough capacity\n        max_cap = np.max(bins_remain_cap)\n        priority[small_cap] += ((bins_remain_cap[small_cap] - item) / max_cap) * 0.8  # Scaled contribution\n\n    # Emptiness boost (decreased weight, less important to fill empty bins immediately)\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += 0.5 * (1 + 0.1 * item_size_factor) # Empty bins weight, scaled\n\n    # Diversity encouragement: random nudge to avoid getting stuck in local optima\n    priority += np.random.rand(len(bins_remain_cap)) * 0.01\n\n    return priority",
    "response_id": 0,
    "tryHS": false,
    "obj": 3.181092939768657,
    "SLOC": 29.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 527.5355568033491,
    "mi": 77.32379313111034,
    "token_count": 326.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response3.txt_stdout.txt",
    "code_path": "problem_iter23_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines heuristics for bin packing, prioritizing sufficient capacity,\n    waste minimization, and nearly full bins.\"\"\"\n\n    bin_capacity = np.max(bins_remain_cap)\n\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += 1.9  # Weight if bin can fit item\n\n    # Minimize Waste\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0]\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += 4.2  # Weight for minimal waste\n\n    # Nearly Full\n    nearly_full = (bins_remain_cap > 0) & (bins_remain_cap <= 2.0 * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += 4.2 # Weight for nearly full bins\n\n    # Small amount of available capacity, normalized by max capacity\n    sufficient_bins = bins_remain_cap >= item\n    if np.any(sufficient_bins):\n        waste_ratios = waste[sufficient_bins] / bin_capacity\n        priority[sufficient_bins] += 0.5 * (1 - waste_ratios) # weight by normalized waste\n    \n    # Give higher preference if it is an empty bin.\n    is_empty = bins_remain_cap == bin_capacity\n    priority[is_empty] += 4.8\n\n    #Bin utilization weight\n    bin_utilization = (bin_capacity - bins_remain_cap) / bin_capacity\n    priority += 1.0 * bin_utilization\n\n    return priority",
    "response_id": 3,
    "tryHS": false,
    "obj": 3.599920223374565,
    "SLOC": 16.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 232.84722658818316,
    "mi": 85.4892857443056,
    "token_count": 177.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter24_response3.txt_stdout.txt",
    "code_path": "problem_iter24_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity, waste, fragmentation, and empty bin considerations with dynamic adjustments.\"\"\"\n\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n    n_bins = len(bins_remain_cap)\n\n    # Sufficient capacity boost\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += 1.8\n\n    # Minimize waste boost, focus on very small waste\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0]\n    if len(positive_waste) > 0:\n        min_waste = np.min(positive_waste)\n        min_waste_bins_idx = np.where(waste == min_waste)[0]\n        priority[min_waste_bins_idx] += 1.2 + (1 - min_waste) * 0.5 # Reward extremely small waste even more\n\n    # Nearly full bin boost, dynamically adjust threshold based on item size\n    nearly_full_threshold = 1.05 # Reduced multiplier\n    nearly_full = (bins_remain_cap > 0.0) & (bins_remain_cap <= nearly_full_threshold * item) & (bins_remain_cap >= item)\n    priority[nearly_full] += 2.4\n\n    # Prioritize bins with remaining capacity close to item size, relative to mean remaining capacity\n    mean_remaining = np.mean(bins_remain_cap)\n    capacity_difference = np.abs(bins_remain_cap - item)\n    priority[sufficient_capacity] += np.exp(-capacity_difference[sufficient_capacity] / (mean_remaining + 0.001)) * 0.8 # Avoid division by zero\n\n    # Emptiness boost, but only if there are not many empty bins\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    num_empty = np.sum(is_empty)\n    if num_empty <= n_bins // 2:  # Limit empty bin boost\n        priority[is_empty] += 0.7\n\n    # Penalize bins with very large remaining capacity when item is small\n    if item < 0.2:\n        large_capacity = bins_remain_cap > 0.8\n        priority[large_capacity] -= 0.3 * (bins_remain_cap[large_capacity] - 0.8)\n\n    return priority",
    "response_id": 3,
    "tryHS": false,
    "obj": 2.4331870761866865,
    "SLOC": 26.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 696.6409683152644,
    "mi": 77.54282537985364,
    "token_count": 342.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter25_hs3.txt_stdout.txt",
    "code_path": "problem_iter25_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray,\n                sufficient_capacity_weight: float = 0.31980367699914636,\n                min_waste_priority_weight: float = 0.5863170663017462,\n                nearly_full_weight: float = 2.2339697684468587,\n                empty_bin_weight: float = 0.4243603531228519,\n                adaptive_waste_threshold_factor: float = 0.010025274520129518,\n                min_adaptive_waste_threshold: float = 0.005051387230875593,\n                nearly_full_threshold_factor: float = 2.6223981714577644) -> np.ndarray:\n    \"\"\"Combines capacity, waste, fragmentation, and adaptive weighting based on bin stats.\"\"\"\n\n    priority = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Adaptive weights based on bin utilization\n    avg_cap = np.mean(bins_remain_cap)\n    cap_std = np.std(bins_remain_cap)\n\n    # Heuristic 1: Sufficient Capacity\n    sufficient_capacity = bins_remain_cap >= item\n    priority[sufficient_capacity] += sufficient_capacity_weight  # Base priority\n\n    # Heuristic 2: Minimize Waste (with adaptive waste threshold)\n    waste = bins_remain_cap - item\n    positive_waste = waste[waste >= 0]\n\n    if len(positive_waste) > 0:\n        # Adaptive waste threshold based on average remaining capacity\n        adaptive_waste_threshold = max(adaptive_waste_threshold_factor * avg_cap, min_adaptive_waste_threshold) # Minimum waste size considered \"good\"\n        valid_waste = positive_waste[positive_waste <= adaptive_waste_threshold]\n\n        if len(valid_waste) > 0:\n            min_valid_waste = np.min(valid_waste)\n            min_waste_bins_idx = np.where(waste == min_valid_waste)[0]\n            priority[min_waste_bins_idx] += min_waste_priority_weight  # Higher priority for truly small waste\n\n    # Heuristic 3: Prevent Fragmentation (Bins near full have high priority)\n    nearly_full_threshold = nearly_full_threshold_factor * item # Dynamic threshold relative to item size\n    nearly_full = (bins_remain_cap >= item) & (bins_remain_cap <= nearly_full_threshold)\n    priority[nearly_full] += nearly_full_weight\n\n    # Heuristic 4: Empty bin consideration\n    is_empty = bins_remain_cap == np.max(bins_remain_cap)\n    priority[is_empty] += empty_bin_weight\n\n    # Adaptive Scaling to prevent domination by any single heuristic\n    priority /= np.max(priority) if np.max(priority) > 0 else 1.0\n\n    return priority",
    "response_id": 0,
    "tryHS": true,
    "obj": 1.3960909453530117,
    "SLOC": 29.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 294.6857250822118,
    "mi": 80.44654340774136,
    "token_count": 300.0,
    "exec_success": true
  }
]