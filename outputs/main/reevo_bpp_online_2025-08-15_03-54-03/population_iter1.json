[
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit strategy.\n\n    The First Fit strategy prioritizes bins that can accommodate the item and\n    then selects the first one encountered (which implies prioritizing bins\n    with smaller remaining capacity that are still sufficient).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the\n        item are given a score of -1. Bins that can fit the item are scored\n        based on their remaining capacity, favoring smaller remaining capacities.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with low priority (-1)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, assign a priority score.\n    # We want to prioritize bins with *less* remaining capacity first (First Fit logic).\n    # So, we assign a score that is inversely proportional to the remaining capacity.\n    # A simple way is to use the negative of the remaining capacity, so smaller\n    # remaining capacities (closer to zero) get higher (less negative) scores.\n    # Adding a small constant to ensure scores are non-negative for fitting bins,\n    # while still maintaining the relative order.\n    fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n    if fitting_bins_capacities.size > 0:\n        # Create a score where smaller remaining capacity is better.\n        # (max_capacity - remaining_capacity) + epsilon can also work, but this is simpler for relative ordering.\n        # A large number minus remaining capacity will prioritize smaller remaining capacities.\n        # We use a large number to ensure these priorities are significantly higher than the default -1.\n        max_possible_capacity = np.max(bins_remain_cap) + item # A value larger than any possible remaining capacity\n        priorities[can_fit_mask] = max_possible_capacity - fitting_bins_capacities\n\n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 51.80615605397529,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit strategy.\n\n    The Best Fit strategy aims to minimize wasted space by placing the item in the bin\n    that leaves the least remaining capacity. However, in an online setting, we need\n    to ensure the item can actually fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a more suitable bin. Bins that cannot accommodate\n        the item are given a priority of -1.\n    \"\"\"\n    # Initialize priorities to 0\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity after packing\n    # The best fit will have the smallest positive remaining capacity\n    remaining_after_fit = bins_remain_cap[fit_mask] - item\n\n    # Assign priorities. Higher priority for bins with less remaining capacity (best fit).\n    # We use negative remaining capacity as a proxy for \"goodness\" of fit.\n    # A smaller positive remaining capacity results in a larger negative value,\n    # thus a higher priority.\n    priorities[fit_mask] = -remaining_after_fit\n\n    # Bins that cannot fit the item are assigned a very low priority (-1)\n    priorities[~fit_mask] = -1.0\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 39.863137138648355,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response2.txt_stdout.txt",
    "code_path": "problem_iter1_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, using Worst Fit strategy.\n\n    The Worst Fit strategy prioritizes bins that have the largest remaining capacity.\n    The priority is calculated as the remaining capacity of the bin minus the item size.\n    This encourages placing the item in the bin where it leaves the largest remaining\n    space, thus trying to \"waste\" the most space in a single bin, leaving smaller\n    bins for potentially smaller items later.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the item\n        will have a priority of negative infinity.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)  # Initialize with a very low priority\n\n    # Calculate priority for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask] - item\n\n    return priorities",
    "response_id": 2,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 1.0,
    "halstead": 22.458839376460833,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response3.txt_stdout.txt",
    "code_path": "problem_iter1_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    The Almost Full Fit strategy prioritizes bins that are nearly full and can still\n    accommodate the item. Bins that are too small to fit the item are given a very low\n    priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate a \"tightness\" score for bins that can fit the item.\n    # A higher tightness means the bin is closer to being full (after adding the item).\n    # We want to pick bins that are almost full, so maximizing the remaining capacity\n    # after fitting the item, but only among bins that can fit the item.\n    \n    # Find indices of bins that can accommodate the item\n    can_fit_indices = np.where(bins_remain_cap >= item)[0]\n    \n    if len(can_fit_indices) > 0:\n        # For bins that can fit the item, the priority is based on how much space\n        # will be left. We want to leave as little space as possible, implying\n        # the bin was already quite full. So, prioritize bins where\n        # (bins_remain_cap - item) is minimized.\n        # To make it a priority score where higher is better, we can use\n        # the negative of the remaining capacity, or some other transformation.\n        # A simple approach is to use the remaining capacity after packing.\n        # Higher remaining capacity after packing means the bin was less full initially.\n        # So, to prioritize 'almost full' bins, we want bins with SMALLER remaining capacity\n        # AFTER packing. Thus, we assign a higher priority to bins with SMALLER (bins_remain_cap - item).\n        \n        remaining_after_packing = bins_remain_cap[can_fit_indices] - item\n        \n        # To rank, we can invert the remaining space. A common way is to use 1 / remaining_after_packing.\n        # However, if remaining_after_packing is zero, this causes division by zero.\n        # A safer approach is to use a large negative number for those that are exactly full after packing\n        # and smaller (but positive) numbers for those with more space.\n        \n        # Let's use a score that is inversely related to the remaining space after packing.\n        # The goal is to penalize bins that will have a lot of space left.\n        # We can use 1 / (remaining_after_packing + epsilon) to avoid division by zero.\n        # However, a simpler approach for prioritization that aligns with \"almost full\"\n        # is to prioritize bins with the smallest remaining capacity AFTER fitting the item.\n        # So, if bins are [5, 8, 10] and item is 3:\n        # Bin 0: remaining 2, score related to 2\n        # Bin 1: remaining 5, score related to 5\n        # Bin 2: remaining 7, score related to 7\n        # We want to pick the one with the smallest remaining space (Bin 0), so its score should be highest.\n        \n        # Let's assign a priority score that is inversely proportional to the space left.\n        # A very small amount of space left after packing (close to zero) should have the highest priority.\n        # So, consider (1.0 / (remaining_after_packing + 1e-9)) for positive values.\n        # For bins that fit, we want the one with least remaining capacity.\n        # So, highest priority for bins_remain_cap[i] - item = 0.\n        \n        # Let's try prioritizing bins with the smallest remaining capacity after packing.\n        # The priority can be set as -remaining_after_packing. Higher values mean lower remaining capacity, thus higher priority.\n        priorities[can_fit_indices] = -remaining_after_packing\n        \n        # Alternative: A different heuristic might prioritize bins where the item fills it more \"completely\".\n        # This is essentially the same as minimizing remaining capacity.\n        # If we want \"almost full\" to be more specific, we could also add a penalty for being too full such that\n        # the remaining space is very small, but not zero.\n        # For this \"Almost Full Fit\" I'll stick to minimizing remaining capacity.\n        \n        # To make the \"almost full\" aspect more explicit, we can consider a distribution\n        # where the peak priority is for bins that will have a small positive remaining capacity.\n        # However, standard \"almost full fit\" typically aims for minimal waste.\n        \n        # Another way to think about \"Almost Full Fit\": we want bins that are NOT empty, but can still take the item.\n        # Prioritize bins that are already utilized to some extent.\n        # The remaining capacity is a good proxy for utilization. Lower remaining capacity means higher utilization.\n        \n        # Let's refine the priority to be positive, where higher means more preferred.\n        # We can use a score like: (max_capacity - item) - bins_remain_cap[i]  where max_capacity is the bin's original capacity.\n        # However, we only have remaining capacities.\n        \n        # Simple approach: prioritize bins with the smallest remaining capacity after packing.\n        # Assign priority based on inverse of remaining space: 1 / (remaining_after_packing + epsilon)\n        # Higher values of this will correspond to smaller remaining_after_packing.\n        epsilon = 1e-9\n        priorities[can_fit_indices] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Bins that cannot fit the item receive a priority of 0 or a very low value.\n    # Our initialization already sets them to 0.\n    \n    # Let's adjust the scale so that the priority is positive and reflects \"nearness to full\".\n    # Bins that are slightly too small to fit the item should have zero priority.\n    # Bins that fit the item should have positive priority.\n    # Among bins that fit, those that will have less remaining capacity are preferred.\n    \n    # Final strategy:\n    # If a bin `b` has `bins_remain_cap[b] >= item`:\n    #   Priority = 1.0 / (bins_remain_cap[b] - item + epsilon)\n    # Else:\n    #   Priority = 0.0\n    \n    # This means bins with smallest `bins_remain_cap[b] - item` (i.e., almost full) get highest priority.\n    \n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 66.41714012534482,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response4.txt_stdout.txt",
    "code_path": "problem_iter1_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This heuristic implements the \"Exact Fit First\" strategy.\n    It prioritizes bins that can *exactly* fit the item. If multiple bins\n    can exactly fit the item, it prioritizes the one with the least remaining\n    capacity after fitting (to keep larger remaining capacities available for\n    potentially larger future items).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can exactly fit the item\n    exact_fit_bins_mask = (bins_remain_cap == item)\n\n    # If there are exact fit bins, prioritize them.\n    # Among exact fit bins, prefer those with less remaining capacity\n    # (which after fitting the item, will have even less remaining capacity).\n    # A higher priority score means more preferred.\n    # We assign a high base score (e.g., 100) for exact fits.\n    # To differentiate between exact fit bins, we can use the inverse\n    # of their remaining capacity (larger capacity gets lower score,\n    # smaller capacity gets higher score among exact fits).\n    if np.any(exact_fit_bins_mask):\n        priorities[exact_fit_bins_mask] = 100 + (1.0 / (bins_remain_cap[exact_fit_bins_mask] - item + 1e-6)) # Add small epsilon to avoid division by zero if item == bin_capacity\n\n    # If no exact fit bins, consider bins that can fit the item.\n    # For non-exact fits, we want to prioritize bins that leave the smallest\n    # remaining capacity after placing the item. This is a \"best fit\" idea.\n    # Bins that are \"too large\" are less preferred.\n    # A common way to achieve this is to prioritize bins with the smallest\n    # remaining capacity that is still greater than the item.\n    # We assign a lower base score for these non-exact fits.\n    else:\n        can_fit_mask = (bins_remain_cap > item)\n        if np.any(can_fit_mask):\n            # Prioritize bins with the smallest remaining capacity that can fit the item\n            # A higher priority for smaller remaining capacity.\n            priorities[can_fit_mask] = 10 + (1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-6))\n\n    return priorities",
    "response_id": 4,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 3.0,
    "halstead": 127.43782540330756,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response5.txt_stdout.txt",
    "code_path": "problem_iter1_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance strategy.\n\n    The priority is calculated as 1 / (remaining_capacity - item_size + epsilon)\n    for bins where the item can fit, and 0 otherwise. Epsilon is added to\n    avoid division by zero. A higher score means the bin is a better fit (closer to full after packing).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    epsilon = 1e-9  # To prevent division by zero\n\n    # Find bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate priority only for bins where the item can fit\n    # Inverse distance to being full after placing the item\n    priorities[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + epsilon)\n\n    return priorities",
    "response_id": 5,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 1.0,
    "halstead": 41.51317942364757,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response6.txt_stdout.txt",
    "code_path": "problem_iter1_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score strategy.\n\n    The Sigmoid Fit Score strategy prioritizes bins that are \"almost full\"\n    but can still accommodate the item. This aims to keep bins as full as possible\n    without causing immediate overflow.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We only consider bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate a \"fit score\"\n    # A good score would be for bins that are nearly full, but not too full.\n    # We can use a sigmoid function to map the remaining capacity to a score.\n    # A smaller remaining capacity (closer to 'item') should have a higher score.\n    # Let's transform remaining capacity into a \"wastefulness\" metric: waste = bin_capacity - item_size\n    # So, bins_remain_cap - item are potential remaining capacities if the item fits.\n    # We want to maximize the likelihood of a good fit.\n    # A common approach for \"almost full\" is to look at the ratio of item size to remaining capacity,\n    # or conversely, remaining capacity to bin capacity (though bin capacity isn't given here, only remaining).\n    # A simpler heuristic is to consider the difference: `bins_remain_cap - item`.\n    # We want to prioritize bins where this difference is small (but non-negative).\n\n    # Let's normalize the potential remaining capacity after placing the item.\n    # A value close to 0 (but positive) is desirable.\n    # We can use the inverse of the potential remaining capacity.\n    # However, we must handle cases where potential remaining capacity is 0 or very small\n    # to avoid division by zero or extremely large numbers.\n\n    # A more robust approach using sigmoid:\n    # Consider the 'slack' or 'residual capacity' after placing the item: residual = bins_remain_cap - item\n    # We want to maximize the probability of fitting.\n    # Let's try to map `residual` to a score where smaller positive values get higher scores.\n    # The sigmoid function `1 / (1 + exp(-x))` maps values to (0, 1).\n    # If we use `-(residual)`, then `exp(-residual)` will be small for large residuals\n    # and large for small residuals.\n    # So, `1 / (1 + exp(-(residual)))` will be closer to 1 for small positive residuals and closer to 0 for large residuals.\n    # We want small positive residuals to have HIGH priority.\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate the residual capacity if the item fits\n    residual_capacities = bins_remain_cap[can_fit_mask] - item\n\n    # We want to give higher priority to smaller *positive* residuals.\n    # A residual of 0 is ideal (perfect fit).\n    # A small positive residual is good.\n    # A large positive residual is less ideal.\n\n    # Using sigmoid:\n    # The sigmoid function typically maps values from -inf to +inf to (0, 1).\n    # If we want small positive residuals to have high priority, we can map them\n    # to positive values and then apply sigmoid.\n    # Let's consider the \"efficiency\" of the fit: `item / bins_remain_cap`\n    # This ratio is high for full bins and low for empty bins.\n    # We want to prioritize bins where this ratio is high (but the item must fit).\n\n    # Alternative Sigmoid approach:\n    # Let's consider `bins_remain_cap - item`. We want this value to be small and non-negative.\n    # We can transform `bins_remain_cap - item` into a variable `x` such that\n    # `sigmoid(x)` is high for small non-negative `bins_remain_cap - item`.\n    # Consider `y = -(bins_remain_cap - item)`. If `bins_remain_cap - item` is small positive, `y` is small negative.\n    # `sigmoid(y)` will be close to 0.5. Not what we want.\n\n    # Let's try a different mapping.\n    # The goal is to find bins that are \"just right\".\n    # Consider the gap: `gap = bins_remain_cap - item` for bins that can fit.\n    # We want to maximize priority for `gap` close to 0.\n    # Let's define a scoring function that peaks at gap = 0 and decreases as gap increases.\n    # We can use a sigmoid centered around 0, but inverted.\n    # `sigmoid(-x)` where x is the gap.\n    # `sigmoid(-(bins_remain_cap[can_fit_mask] - item))`\n\n    # Let `gap = bins_remain_cap - item`.\n    # We are interested in `gap >= 0`.\n    # Let `x = -(gap) = item - bins_remain_cap`.\n    # We want to maximize priority when `item - bins_remain_cap` is close to 0 (i.e., `bins_remain_cap` close to `item`).\n    # Consider `x = -(bins_remain_cap[can_fit_mask] - item) = item - bins_remain_cap[can_fit_mask]`\n    # If `bins_remain_cap = item`, `x = 0`, `sigmoid(0) = 0.5`.\n    # If `bins_remain_cap > item`, `x < 0`, `sigmoid(x)` is < 0.5.\n    # If `bins_remain_cap < item`, `x > 0`, `sigmoid(x)` is > 0.5. This is for items that don't fit.\n\n    # The standard \"Sigmoid Fit\" or \"Best Fit Decreasing\" often aims to fill bins as much as possible.\n    # For online BPP, \"First Fit Decreasing\" prioritizes the first bin that fits.\n    # \"Best Fit\" would prioritize the bin with the smallest remaining capacity that can still fit the item.\n\n    # Let's re-evaluate the \"Sigmoid Fit Score\" strategy for online BPP.\n    # The idea is to prioritize bins that are \"almost full\" but can still accept the item.\n    # This implies we want to minimize `bins_remain_cap - item` (for bins where it's >= 0).\n    # Let `residual = bins_remain_cap[can_fit_mask] - item`.\n    # We want to give higher priority as `residual` approaches 0.\n\n    # Let's try to map the `residual` to a score using sigmoid.\n    # We want a high score for small `residual` (close to 0) and lower scores for larger `residual`.\n    # Consider a transformation `y = -(residual)`.\n    # `sigmoid(y) = 1 / (1 + exp(-y)) = 1 / (1 + exp(-(residual)))`.\n    # If `residual = 0`, `exp(0) = 1`, `sigmoid = 0.5`.\n    # If `residual` is small positive, `-(residual)` is small negative, `exp(-(residual))` is slightly less than 1, `sigmoid` is slightly less than 0.5.\n    # If `residual` is large positive, `-(residual)` is large negative, `exp(-(residual))` is close to 0, `sigmoid` is close to 1.\n\n    # This sigmoid `1 / (1 + exp(-(bins_remain_cap - item)))` actually prioritizes bins with *larger* remaining capacity after fitting the item.\n    # This is counter-intuitive for \"almost full\".\n\n    # Let's try a sigmoid where smaller values of `residual` yield higher scores.\n    # Consider `sigmoid(-k * residual)` for some `k > 0`.\n    # If `residual = 0`, `sigmoid(0) = 0.5`.\n    # If `residual` is small positive, `-k * residual` is small negative, `sigmoid` is < 0.5.\n    # If `residual` is large positive, `-k * residual` is large negative, `sigmoid` is close to 0.\n    # This gives higher scores to bins with *smaller* remaining capacity after fitting (i.e., more full bins).\n\n    # Let's use `k = 1` for simplicity.\n    # Score for fitting bins: `sigmoid(-(bins_remain_cap[can_fit_mask] - item))`\n\n    # We need to scale the `residual` to make the sigmoid curve steeper or shallower.\n    # A common approach is to scale `bins_remain_cap - item` by some factor.\n    # Let's use the item size itself as a scaling factor. Or the median remaining capacity?\n    # For simplicity, let's use a fixed steepness factor, say `alpha`.\n\n    alpha = 10  # Steepness parameter for the sigmoid\n\n    # Calculate the negative of the residual for sigmoid input\n    # We want to map `bins_remain_cap - item` to `x` such that `sigmoid(f(x))` is high for small `x`.\n    # If we use `f(x) = -x`, then `sigmoid(-x)` has higher values for smaller `x`.\n    # So, let `x = bins_remain_cap[can_fit_mask] - item`.\n    # The scores will be `np.exp(-(alpha * x))`. No, that's not sigmoid.\n\n    # Let's use the form: `1 / (1 + exp(-k * (target - value)))`\n    # We want high priority when `bins_remain_cap - item` is close to 0.\n    # So, let `target = 0`. Let `value = bins_remain_cap - item`.\n    # Priority for fitting bins: `1 / (1 + exp(-alpha * (0 - (bins_remain_cap[can_fit_mask] - item))))`\n    # = `1 / (1 + exp(alpha * (bins_remain_cap[can_fit_mask] - item)))`\n\n    # Let's test this:\n    # If `bins_remain_cap - item = 0` (perfect fit): `1 / (1 + exp(0)) = 0.5`\n    # If `bins_remain_cap - item = small positive` (e.g., 0.1): `exp(0.1 * alpha)` is > 1. Denominator > 2. Score < 0.5.\n    # If `bins_remain_cap - item = large positive` (e.g., 1.0): `exp(1.0 * alpha)` is very large. Score close to 0.\n    # This means bins with larger remaining capacity *after* fitting get lower scores. This is good.\n\n    # If `bins_remain_cap - item = small negative` (item doesn't fit): We are masking this out.\n\n    # The problem is that the sigmoid output for the \"best\" case (residual = 0) is 0.5.\n    # We want the highest priority to be close to 1.\n    # This sigmoid form maps values to (0, 1).\n    # Maybe the interpretation of \"Sigmoid Fit Score\" implies mapping the *degree of fit* to a score.\n    # A good fit is when `bins_remain_cap` is `item` or slightly larger.\n    # The degree of \"emptiness\" after fitting is `bins_remain_cap - item`.\n    # We want to penalize large \"emptiness\".\n\n    # Consider mapping `bins_remain_cap` directly.\n    # Bins that can fit the item are those where `bins_remain_cap >= item`.\n    # Among these, we want those that are closest to `item`.\n    # This means we want to maximize `1 / (1 + exp(-alpha * (bins_remain_cap - item)))` for `bins_remain_cap >= item`.\n    # If `bins_remain_cap = item`, `exp(0) = 1`, score is 0.5.\n    # If `bins_remain_cap` is slightly larger than `item`, `exp(positive)` is > 1, score is < 0.5.\n    # If `bins_remain_cap` is much larger than `item`, `exp(much positive)` is very large, score is close to 0.\n\n    # This actually seems to prioritize bins with *less* remaining capacity after fitting,\n    # which is what \"best fit\" does. However, the peak is at 0.5, not 1.\n\n    # Let's try to shift and scale the sigmoid.\n    # We want a peak at `bins_remain_cap = item`.\n    # Let `x = bins_remain_cap[can_fit_mask] - item`.\n    # We want a function `f(x)` such that `f(0)` is high, `f(small_positive)` is slightly lower,\n    # and `f(large_positive)` is low.\n\n    # What if we consider the *ratio* of remaining capacity to bin capacity (if we knew it)?\n    # Since we don't know bin capacity, let's focus on the `item` and `bins_remain_cap`.\n\n    # Let's try a different formulation for sigmoid: `sigmoid(x) = 1 / (1 + exp(-x))`.\n    # We want to maximize `f(bins_remain_cap)`.\n    # For bins that can fit, `bins_remain_cap >= item`.\n    # Consider `value = item / bins_remain_cap[can_fit_mask]`. This is the fill ratio.\n    # We want to prioritize higher fill ratios.\n    # `sigmoid(k * (item / bins_remain_cap[can_fit_mask]))`\n    # If `bins_remain_cap = item`, ratio is 1. Sigmoid is `1 / (1 + exp(-k))`.\n    # If `bins_remain_cap` is slightly larger, ratio is slightly less than 1. Sigmoid is slightly less.\n    # If `bins_remain_cap` is much larger, ratio is close to 0. Sigmoid is close to 0.5.\n    # This prioritizes bins that are almost full (high fill ratio).\n\n    # Let's use this. We need to handle potential division by zero if `bins_remain_cap` is 0,\n    # but our `can_fit_mask` already filters for `bins_remain_cap >= item`.\n    # If `item > 0`, then `bins_remain_cap` will be > 0.\n    # If `item = 0`, then `bins_remain_cap` can be 0, but `0/0` is NaN.\n    # If `item = 0`, it fits in any bin, and has ratio 0.\n\n    fill_ratios = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate fill ratio for bins that can fit the item\n    # Avoid division by zero for bins with remaining capacity 0, although our mask should prevent this if item > 0.\n    # If item is 0, then the fill ratio is 0 for any bin that can fit it (all bins).\n    positive_capacities_mask = bins_remain_cap > 0\n    can_fit_and_positive_cap_mask = can_fit_mask & positive_capacities_mask\n\n    # For bins that can fit and have positive capacity, calculate the fill ratio.\n    # fill_ratio = item / bins_remain_cap\n    fill_ratios[can_fit_and_positive_cap_mask] = item / bins_remain_cap[can_fit_and_positive_cap_mask]\n\n    # If item is 0, it fits everywhere. The fill ratio is effectively 0.\n    # Our `fill_ratios` is initialized to 0, so this is handled.\n\n    # Apply sigmoid to the fill ratio to get priority scores.\n    # Higher fill ratios should result in higher scores.\n    # Sigmoid function: `1 / (1 + exp(-k * x))` where x is the fill ratio.\n    # A larger k makes the transition steeper.\n    k = 15  # Steepness factor for the sigmoid. Higher means it's more sensitive to small changes in fill ratio.\n\n    # The fill ratio is between 0 and 1.\n    # `sigmoid(k * fill_ratio)`:\n    # If fill_ratio = 0, sigmoid = 0.5\n    # If fill_ratio = 1, sigmoid = 1 / (1 + exp(-k)) which is close to 1 for large k.\n    # This prioritizes bins with higher fill ratios.\n\n    # Let's ensure we don't get NaN or Inf if `bins_remain_cap` is exactly 0 and `item` is > 0.\n    # `can_fit_mask` ensures `bins_remain_cap >= item`.\n    # If `item > 0`, then `bins_remain_cap` must be > 0.\n    # So `bins_remain_cap[can_fit_mask]` will not be zero.\n\n    # We need to apply sigmoid to the `fill_ratios`.\n    # For bins that *cannot* fit the item, their priority should be 0.\n    # The `fill_ratios` are already 0 for these bins due to initialization and mask logic.\n\n    # Calculate the priority using sigmoid\n    # For bins where `can_fit_mask` is True, the `fill_ratios` have been calculated.\n    # For bins where `can_fit_mask` is False, `fill_ratios` remain 0.\n    # Applying sigmoid to 0 gives 0.5. This is not desired for bins that cannot fit.\n    # We need to apply sigmoid *only* to the bins that can fit.\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate fill ratios only for bins that can fit\n    if np.any(can_fit_mask):\n        # Ensure we only divide by positive capacities if item > 0\n        # If item is 0, fill ratio is 0 for any bin it fits in.\n        if item > 0:\n            valid_capacities_for_fit = bins_remain_cap[can_fit_mask]\n            # We already ensured valid_capacities_for_fit >= item > 0.\n            # So division by zero is not an issue here.\n            current_fill_ratios = item / valid_capacities_for_fit\n        else: # item is 0\n            current_fill_ratios = np.zeros(np.sum(can_fit_mask)) # Fill ratio is 0 for zero-sized items.\n\n        # Apply sigmoid to the fill ratios.\n        # `sigmoid(k * current_fill_ratios)`\n        # This maps fill_ratio (0 to 1) to sigmoid values (0.5 to ~1).\n        # We want higher scores for higher fill ratios.\n        # Let's use `sigmoid(k * (current_fill_ratios - offset))` to shift the peak or make it more sensitive.\n        # If we want the peak at fill_ratio = 1 (i.e., best fit), we need the sigmoid input to be large positive at fill_ratio = 1.\n        # `k * fill_ratio` achieves this.\n\n        # `sigmoid(k * x)` where `x` is the fill ratio.\n        # To make the 'peak' at fill_ratio = 1 more pronounced and the drop-off steeper for lower fill ratios,\n        # we can simply use a large `k`.\n\n        # Let's use `alpha * (item / bins_remain_cap)` as input to sigmoid.\n        # This results in scores between `sigmoid(0)` (0.5) and `sigmoid(alpha)` (~1).\n        # A slightly different strategy is to prioritize bins with *least* remaining capacity that *can* fit the item.\n        # This is the \"Best Fit\" strategy. How to achieve this with sigmoid?\n\n        # Let's try a different approach using sigmoid to emphasize \"almost full\".\n        # Consider the gap: `gap = bins_remain_cap - item`. We want small, non-negative gaps to have high priority.\n        # Let `x = -(gap) = item - bins_remain_cap`.\n        # For bins that can fit, `gap >= 0`, so `x <= 0`.\n        # `sigmoid(k * x)`:\n        # If `gap = 0`, `x = 0`, `sigmoid(0) = 0.5`.\n        # If `gap` is small positive, `x` is small negative, `sigmoid` < 0.5.\n        # If `gap` is large positive, `x` is large negative, `sigmoid` close to 0.\n        # This prioritizes bins with *larger* gaps (less full bins), which is NOT what we want.\n\n        # Let's invert the sigmoid input.\n        # `sigmoid(-k * x)` where `x = bins_remain_cap - item`.\n        # `sigmoid(-k * (bins_remain_cap[can_fit_mask] - item))`\n        # If `bins_remain_cap - item = 0`: `sigmoid(0) = 0.5`.\n        # If `bins_remain_cap - item = small positive`: `sigmoid(-k * small_positive)` is < 0.5.\n        # If `bins_remain_cap - item = large positive`: `sigmoid(-k * large_positive)` is close to 0.\n        # This means bins with larger remaining capacity after fitting get lower scores. This IS what we want!\n        # The priority scores will be in the range (0, 1), with the highest score being 0.5 for a perfect fit.\n        # To get scores closer to 1 for perfect fits, we can modify the sigmoid or the input.\n\n        # Let's refine the input:\n        # `value = (bins_remain_cap[can_fit_mask] - item)`\n        # We want `value=0` to give the highest score.\n        # We can use `sigmoid(k * (1 - normalized_value))`\n        # Normalization is tricky.\n\n        # Let's go back to the fill ratio: `fill_ratio = item / bins_remain_cap` for bins that can fit.\n        # We want to prioritize `fill_ratio` close to 1.\n        # Let `x = fill_ratio`. We want `sigmoid(k * x)` to be high for high `x`.\n        # If `fill_ratio` is always <= 1, then `sigmoid(k * fill_ratio)` has its highest value at `fill_ratio = 1`.\n        # `sigmoid(k)` which is close to 1.\n        # For `fill_ratio = 0`, `sigmoid(0) = 0.5`.\n        # This mapping is good: 0.5 for bins that are empty-ish after fitting, ~1 for bins that are full.\n\n        # Re-implementing the fill ratio approach:\n        fill_ratios_for_fitting = np.zeros(np.sum(can_fit_mask))\n        \n        # Case: item size is 0\n        if item == 0:\n            # A zero-sized item fits into any bin. It doesn't consume capacity.\n            # The \"fill ratio\" could be considered 0 as it doesn't fill anything.\n            # Or, perhaps it makes bins that *can* fit the item have a score.\n            # If we use the current sigmoid, 0 fill ratio maps to 0.5 priority.\n            # So, any bin that can fit a zero item would get a priority of 0.5.\n            # This seems reasonable: a zero item can be placed in any bin with equal preference based on capacity.\n            # Let's proceed with the fill ratio calculation as if item > 0, and handle the item=0 case gracefully.\n            # If item is 0, `current_fill_ratios = 0 / bins_remain_cap` (if bin_remain_cap > 0).\n            # If bin_remain_cap is also 0, it's 0/0. But can_fit_mask requires `bins_remain_cap >= 0`.\n            # If item is 0, it fits in any bin.\n            # Let's define fill ratio for item=0 as 0.\n            fill_ratios_for_fitting = np.zeros(np.sum(can_fit_mask)) # fill ratio is 0\n        else:\n            # Item size > 0\n            bins_with_positive_capacity_that_fit = bins_remain_cap[can_fit_mask]\n            \n            # Filter for positive remaining capacities to avoid division by zero.\n            # Our `can_fit_mask` ensures `bins_remain_cap >= item`.\n            # If `item > 0`, then `bins_remain_cap` must be > 0 for `can_fit_mask` to be true.\n            # So, `bins_with_positive_capacity_that_fit` is guaranteed to be > 0.\n            \n            fill_ratios_for_fitting = item / bins_with_positive_capacity_that_fit\n\n        # Now apply sigmoid to these fill ratios.\n        # `k` controls the steepness. A higher `k` means the priority rapidly increases as fill ratio approaches 1.\n        # This strategy \"Sigmoid Fit Score\" aims to pack items into bins that are almost full.\n        # So, we want higher priority for higher fill ratios.\n        # `sigmoid(k * fill_ratio)`\n        # fill_ratio is between 0 and 1.\n        # sigmoid(0) = 0.5\n        # sigmoid(k) is close to 1 for k large.\n        # This provides scores in the range (0.5, ~1) for fitting bins.\n\n        k = 10.0 # Tuning parameter for sigmoid steepness. Adjust this for desired behavior.\n\n        # Ensure the input to sigmoid is within reasonable bounds to avoid overflow/underflow of exp.\n        # fill_ratios are between 0 and 1. k=10 is fine.\n        # If k is very large, k*fill_ratio can exceed typical float limits.\n        # Clip k * fill_ratios to prevent overflow.\n        sigmoid_input = k * fill_ratios_for_fitting\n        \n        # To ensure that the output is always positive for fitting bins, and zero for non-fitting,\n        # we apply the sigmoid to `fill_ratios_for_fitting`.\n        # Bins that cannot fit the item will have their priority remain 0 from initialization.\n        priorities[can_fit_mask] = 1 / (1 + np.exp(-sigmoid_input))\n\n    return priorities",
    "response_id": 6,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 174.22857502740396,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response7.txt_stdout.txt",
    "code_path": "problem_iter1_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This priority function implements the \"Random Fit\" strategy for the online Bin Packing Problem.\n    For the online BPP, the strategy is to assign the item to the first bin that can accommodate it.\n    However, the prompt requests a \"priority score for each bin\" and that \"the bin with the highest priority score will be selected\".\n    Since Random Fit doesn't inherently have a priority scoring mechanism beyond \"can it fit?\",\n    we will simulate a priority based on a random selection among eligible bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    eligible_bins_indices = np.where(bins_remain_cap >= item)[0]\n\n    # If there are eligible bins, assign a high random priority to them.\n    # The \"random\" aspect comes from the fact that if multiple bins are eligible,\n    # we'll randomly pick one to have a slightly higher priority by assigning a\n    # higher random number. This mimics a non-deterministic selection among\n    # valid choices in a \"random fit\" spirit.\n    if len(eligible_bins_indices) > 0:\n        # Assign random values to eligible bins.\n        # To simulate a random pick among suitable bins, we can assign\n        # random numbers only to those that can fit the item.\n        # For simplicity and to clearly signal which are preferred,\n        # we'll assign higher values to eligible bins and lower to others.\n        # A truly \"random\" fit would then just pick one of these highest.\n        # Here, we assign higher random values to indicate preference.\n        random_priorities = np.random.rand(len(eligible_bins_indices))\n        priorities[eligible_bins_indices] = 1.0 + random_priorities # Add 1.0 to ensure higher priority than non-eligible bins\n\n    # In a true \"random fit\", if there are multiple bins that can fit,\n    # any of them could be chosen randomly. By assigning random values to\n    # all eligible bins and ensuring they are higher than non-eligible bins,\n    # the selection of the bin with the highest priority (which will be one\n    # of the eligible bins) effectively acts as a random selection among them.\n\n    return priorities",
    "response_id": 7,
    "obj": 73.5939369764659,
    "cyclomatic_complexity": 2.0,
    "halstead": 28.529325012980813,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response8.txt_stdout.txt",
    "code_path": "problem_iter1_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using an Epsilon-Greedy strategy.\n\n    The strategy aims to balance exploration (trying less occupied bins) with exploitation\n    (favoring bins that have a tight fit for the item).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploring a random bin\n\n    # Identify feasible bins where the item can fit\n    feasible_bins_mask = bins_remain_cap >= item\n    \n    # If no bins are feasible, return all zeros (no valid placement)\n    if not np.any(feasible_bins_mask):\n        return np.zeros_like(bins_remain_cap)\n\n    # Initialize priorities for all bins\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Greedy (Exploitation) part: Prioritize bins that are a \"tight fit\"\n    # We want bins where remaining capacity - item size is small.\n    # To avoid division by zero or very small numbers, we can add a small constant.\n    # Also, to make smaller remaining capacities more dominant, we can use the inverse.\n    # However, a simple approach is to prioritize bins with the smallest remaining capacity\n    # that can still fit the item. This is equivalent to prioritizing bins where the\n    # residual capacity (after placing the item) is minimized.\n    \n    # Calculate the residual capacity if the item is placed in a feasible bin\n    residual_capacities = bins_remain_cap[feasible_bins_mask] - item\n    \n    # For feasible bins, calculate a greedy score. Lower residual capacity is better.\n    # We can use the inverse of (residual_capacity + 1) to ensure higher scores for better fits.\n    # Add a small epsilon to avoid division by zero and to make smaller residuals more impactful.\n    greedy_scores = 1.0 / (residual_capacities + 1e-6)\n\n    # Apply the greedy scores to the feasible bins\n    priorities[feasible_bins_mask] = greedy_scores\n\n    # Epsilon-Greedy (Exploration) part: With probability epsilon, choose a random feasible bin\n    # This helps to avoid getting stuck in local optima by sometimes trying bins\n    # that are not the \"best fit\" according to the greedy strategy.\n\n    # Generate random numbers for each bin\n    random_values = np.random.rand(len(bins_remain_cap))\n\n    # For feasible bins, if the random value is less than epsilon, give it a high exploration score.\n    # We can assign a constant high score, or a score based on randomness.\n    # Let's assign a score that makes it competitive but not necessarily dominant.\n    # A simple approach is to assign a score that is slightly better than the \"average\"\n    # greedy score or a random high value.\n    \n    # We want to randomly pick among the *feasible* bins.\n    num_feasible_bins = np.sum(feasible_bins_mask)\n    \n    if num_feasible_bins > 0:\n        # Get indices of feasible bins\n        feasible_bin_indices = np.where(feasible_bins_mask)[0]\n        \n        # Determine how many bins to \"explore\" based on epsilon\n        num_to_explore = int(np.floor(epsilon * num_feasible_bins))\n        \n        if num_to_explore > 0:\n            # Randomly select indices of feasible bins to give an exploration boost\n            exploration_indices = np.random.choice(feasible_bin_indices, num_to_explore, replace=False)\n            \n            # Assign a high exploration score to these bins.\n            # A score that is slightly higher than the current best greedy score\n            # would be reasonable. For simplicity, let's use a constant that ensures\n            # it's considered.\n            exploration_boost = np.max(greedy_scores) * 1.1 if np.any(greedy_scores) else 1.0\n            priorities[exploration_indices] = exploration_boost\n\n    return priorities",
    "response_id": 8,
    "obj": 5.95333067411249,
    "cyclomatic_complexity": 5.0,
    "halstead": 114.20025299224778,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response9.txt_stdout.txt",
    "code_path": "problem_iter1_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    The priority is calculated as follows:\n    For each bin, calculate the 'fit' which is the remaining capacity after placing the item.\n    If the item fits, the 'fit' is bins_remain_cap - item.\n    If the item does not fit, assign a very low fit value (negative infinity) to disincentivize choosing this bin.\n    Apply the softmax function to these 'fit' values to get probabilities (priorities).\n    A higher 'fit' value (meaning less remaining capacity after packing) results in a higher priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity if the item is placed in each bin.\n    # If the item doesn't fit, assign a very small negative number (-inf)\n    # to ensure it gets a very low priority after softmax.\n    potential_fits = np.where(bins_remain_cap >= item, bins_remain_cap - item, -np.inf)\n\n    # Apply softmax to convert the 'fit' values into probabilities (priorities).\n    # Adding a small epsilon to the denominator to avoid division by zero if all potentials are -inf.\n    # However, softmax naturally handles this by producing a uniform distribution if all inputs are equal\n    # or very small negative numbers result in probabilities close to zero for those bins.\n    # A simple softmax formula: exp(x_i) / sum(exp(x_j))\n    exp_fits = np.exp(potential_fits)\n    sum_exp_fits = np.sum(exp_fits)\n\n    # Handle the case where sum_exp_fits is zero (all potential_fits were -inf)\n    if sum_exp_fits == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    priorities = exp_fits / sum_exp_fits\n\n    return priorities",
    "response_id": 9,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 2.0,
    "halstead": 48.43204266092217,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response10.txt_stdout.txt",
    "code_path": "problem_iter1_code10.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit strategy.\n\n    This heuristic prioritizes bins that can accommodate the item and have the\n    least remaining capacity *after* the item is placed. This aims to fill\n    bins more tightly.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can actually fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins that can fit the item, assign a priority.\n    # We want to prioritize bins that, after placing the item,\n    # will have the *least* remaining capacity. This is equivalent to\n    # prioritizing bins with the smallest current remaining capacity that can fit the item.\n    #\n    # The priority score will be the negative of the remaining capacity\n    # if the item fits. This way, a smaller remaining capacity (more negative)\n    # leads to a higher (less negative) priority score.\n    #\n    # For bins that cannot fit the item, their priority remains 0,\n    # effectively making them unselectable.\n    \n    priorities[can_fit_mask] = -bins_remain_cap[can_fit_mask] + item\n    \n    return priorities",
    "response_id": 10,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 22.458839376460833,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response11.txt_stdout.txt",
    "code_path": "problem_iter1_code11.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit strategy.\n\n    The Best Fit strategy aims to place the item into the bin where it fits\n    most snugly, leaving the smallest remaining capacity. This often leads\n    to better packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate a better fit.\n    \"\"\"\n    # We want to prioritize bins where the item fits snugly.\n    # A good measure of snugness is how much capacity is left AFTER placing the item.\n    # We prefer bins that result in a smaller remaining capacity, but only if the item fits.\n\n    # Calculate potential remaining capacity if the item is placed in each bin.\n    # If the item doesn't fit, the remaining capacity is considered infinity (or a very large number).\n    potential_remaining_cap = np.where(bins_remain_cap >= item,\n                                       bins_remain_cap - item,\n                                       np.inf)\n\n    # We want to maximize the priority for bins with the smallest *positive* remaining capacity.\n    # If potential_remaining_cap is 0, it's a perfect fit and should have high priority.\n    # If potential_remaining_cap is positive, smaller values are better.\n    # If potential_remaining_cap is inf, it means the item doesn't fit, so it should have very low priority.\n\n    # A common heuristic is to use the inverse of the remaining capacity.\n    # To handle the case where remaining capacity is zero (perfect fit), we can add a small epsilon or\n    # use a strategy that maps 0 to a very high value.\n    # For 'Best Fit', we want to prioritize bins that have just enough capacity, or a bit more.\n    # The difference (bins_remain_cap - item) represents the 'waste' if the item fits. We want to minimize waste.\n\n    # Let's define priority as the negative of the remaining capacity if it fits.\n    # The highest priority will be for the smallest negative number (closest to zero),\n    # which corresponds to the smallest positive remaining capacity.\n    # For bins where the item doesn't fit (np.inf), we'll assign a very low priority.\n\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # For bins where the item fits\n    fits_mask = bins_remain_cap >= item\n    if np.any(fits_mask):\n        remaining_after_fit = bins_remain_cap[fits_mask] - item\n        # We want to prioritize bins with smaller remaining capacity.\n        # So, we use the negative of the remaining capacity.\n        # The most negative value corresponds to the smallest remaining capacity, hence the best fit.\n        # Adding a small constant to the remaining capacity ensures that the minimum remaining capacity (0)\n        # gets a higher score than larger remaining capacities, but avoids division by zero issues\n        # if we were to use 1 / (remaining_capacity + epsilon).\n        # Here, we'll simply use the negative remaining capacity directly.\n        priorities[fits_mask] = -remaining_after_fit\n\n    # For bins where the item doesn't fit, assign a very low priority (large negative number)\n    # This ensures they are selected only if no other bins are available.\n    # A large negative number like -infinity will be correctly handled by argmax.\n    priorities[~fits_mask] = -np.inf\n\n    return priorities",
    "response_id": 11,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 62, in priority_v2\nOverflowError: cannot convert float infinity to integer\n2\n59.794705707972525\n"
  },
  {
    "stdout_filepath": "problem_iter1_response12.txt_stdout.txt",
    "code_path": "problem_iter1_code12.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    The Worst Fit strategy aims to place the item into the bin with the largest remaining capacity.\n    This is often done to leave smaller gaps in other bins, potentially allowing for better packing later.\n    The priority score is therefore directly proportional to the remaining capacity of the bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to a very small negative number to indicate that bins\n    # that cannot accommodate the item should have the lowest priority.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can accommodate the current item.\n    valid_bins_mask = bins_remain_cap >= item\n\n    # For valid bins, the priority is simply their remaining capacity.\n    # The higher the remaining capacity, the higher the priority (Worst Fit).\n    priorities[valid_bins_mask] = bins_remain_cap[valid_bins_mask]\n\n    return priorities",
    "response_id": 12,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 1.0,
    "halstead": 11.60964047443681,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response13.txt_stdout.txt",
    "code_path": "problem_iter1_code13.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    The Almost Full Fit strategy prioritizes bins that have just enough remaining\n    capacity to fit the item, aiming to leave larger remaining capacities in other\n    bins for potentially larger future items. This implementation gives a higher\n    priority to bins where the remaining capacity is just slightly larger than the\n    item size.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to a very small negative number (or zero)\n    # to ensure that bins that cannot fit the item get a low priority.\n    priorities = np.full_like(bins_remain_cap, -float('inf'))\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the priority.\n    # The priority is inversely related to the remaining capacity after placing the item.\n    # A smaller remaining capacity (i.e., item fits \"snugly\") gets a higher priority.\n    # To achieve \"almost full\", we can consider the difference between remaining capacity\n    # and the item size. A smaller difference indicates the bin is closer to being full\n    # after fitting the item.\n    # We can use a penalty or bonus based on how \"close\" it is.\n    # A simple approach is to take the reciprocal of the remaining capacity\n    # after fitting the item, but this might lead to very large numbers if\n    # remaining capacity is very small.\n    # A more controlled approach is to reward bins where (bins_remain_cap - item) is small.\n    # We can use the inverse of (bins_remain_cap - item + epsilon) to avoid division by zero,\n    # or a function that peaks at 0.\n    # Let's try rewarding bins where the difference (bins_remain_cap - item) is close to 0.\n\n    # Calculate the remaining capacity if the item is placed in the bin\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # The priority will be higher for smaller `remaining_after_fit` values.\n    # We can use a function like 1 / (remaining_after_fit + epsilon) or\n    # an exponential function like exp(-k * remaining_after_fit).\n    # Let's use an exponential decay, where smaller remaining capacity means higher priority.\n    # We want to maximize the priority.\n    # A small epsilon is added to avoid division by zero if remaining_after_fit is 0.\n    epsilon = 1e-9\n    priorities[can_fit_mask] = 1.0 / (remaining_after_fit + epsilon)\n\n    # Alternative strategy: Prioritize bins where remaining capacity is 'just enough'\n    # This means bins where bins_remain_cap - item is small.\n    # We can define a threshold for 'just enough'.\n    # For example, if bin_capacity - item < bin_capacity * tolerance\n    # A simpler approach for \"Almost Full Fit\" is to prioritize bins that are closer to being full.\n    # The 'almost full' aspect can be captured by prioritizing bins with *less* remaining capacity\n    # that can still fit the item.\n    # So, the priority should be inversely related to `bins_remain_cap` among those that can fit the item.\n\n    # Let's refine the priority:\n    # For bins that can fit: prioritize those with smaller `bins_remain_cap`.\n    # This means the priority should be `1 / bins_remain_cap` for fitting bins.\n    # Or, to give a stronger preference to \"almost full\" bins, we can make the priority\n    # even more sensitive to smaller remaining capacities.\n    # Let's try prioritizing bins with smallest `bins_remain_cap` that can fit the item.\n\n    # Reset priorities to calculate a new scheme\n    priorities = np.full_like(bins_remain_cap, -float('inf'))\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, assign priority based on their current remaining capacity.\n    # Smaller remaining capacity means higher priority (closer to \"almost full\").\n    # We can use a function that produces larger values for smaller `bins_remain_cap`.\n    # For example: 1 / (bins_remain_cap[can_fit_mask])\n    # To give a more pronounced \"almost full\" effect, let's use an inverse relationship\n    # that amplifies smaller values.\n    # Consider `1 / (bins_remain_cap[can_fit_mask] - item + epsilon)` as before, which focuses\n    # on the space left *after* packing. This aligns well with aiming for fuller bins.\n\n    # Let's stick to the idea of penalizing larger remaining capacities when an item fits.\n    # The difference `bins_remain_cap - item` should be minimized.\n    # A larger value of `1 / (bins_remain_cap - item + epsilon)` means smaller `bins_remain_cap - item`.\n    priorities[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + epsilon)\n\n    # Another interpretation of Almost Full Fit: find a bin where `bins_remain_cap` is\n    # as close as possible to `item`, but still greater than or equal to `item`.\n    # This means minimizing `bins_remain_cap - item`.\n    # So, higher priority for smaller positive differences.\n\n    # Let's try a heuristic that emphasizes bins where `bins_remain_cap` is slightly larger than `item`.\n    # We want to prioritize bins where `bins_remain_cap` is close to `item`.\n    # We can assign a high score if `bins_remain_cap` is just above `item` and lower scores\n    # as `bins_remain_cap` increases further.\n\n    # Initialize priorities for all bins\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Find indices of bins that can fit the item\n    fitting_indices = np.where(bins_remain_cap >= item)[0]\n\n    if fitting_indices.size > 0:\n        # For bins that can fit, calculate a \"tightness\" score.\n        # A tighter fit means smaller `bins_remain_cap`.\n        # Let's use the inverse of `bins_remain_cap` for fitting bins.\n        # To ensure a greater emphasis on 'almost full', we can amplify this.\n        # A simple approach: reward bins that are \"closer\" to needing the item.\n        # Consider `bins_remain_cap / item`. We want this ratio to be small but >= 1.\n        # Or, `bins_remain_cap - item`. We want this to be small and non-negative.\n        # Priority = 1 / (bins_remain_cap - item + epsilon) works well for minimizing the gap.\n\n        # Let's consider the \"almost full\" aspect as being within a certain tolerance of full.\n        # For simplicity, let's define priority as the inverse of remaining capacity for fitting bins.\n        # This prioritizes bins with less remaining space.\n        # The \"almost\" part can be achieved by not selecting the absolute *smallest* remaining capacity,\n        # but one that's \"sufficiently small\".\n\n        # A more explicit \"Almost Full Fit\" strategy:\n        # Prioritize bins where `bins_remain_cap` is closest to `item` from above.\n        # We can compute the difference `diff = bins_remain_cap - item`.\n        # We want to prioritize bins where `diff` is small and non-negative.\n        # Higher priority for smaller non-negative `diff`.\n\n        diffs = bins_remain_cap[fitting_indices] - item\n        # To get higher priority for smaller `diffs`, we can use `1 / (diffs + epsilon)`.\n        # This rewards bins where the leftover space is minimal.\n        priorities[fitting_indices] = 1.0 / (diffs + epsilon)\n\n    return priorities",
    "response_id": 13,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 194.51316411045156,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response14.txt_stdout.txt",
    "code_path": "problem_iter1_code14.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First strategy.\n\n    The Exact Fit First strategy prioritizes bins that can accommodate the item exactly.\n    If multiple bins can fit the item exactly, the one with the least remaining capacity\n    after placing the item is chosen (which is the same for all exact fits).\n    If no bin can fit the item exactly, then it prioritizes bins that can fit the item,\n    ranking them by how little space is left after fitting.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find bins that can exactly fit the item\n    exact_fit_mask = (bins_remain_cap == item)\n    \n    # If there are exact fits, give them the highest priority (e.g., a large positive value)\n    # and for all other bins, give a lower priority.\n    if np.any(exact_fit_mask):\n        priorities[exact_fit_mask] = 1.0\n        # For non-exact fits, we can assign a slightly lower priority,\n        # or even 0, as we prioritize exact fits first.\n        # Here, we assign 0 to non-exact fits.\n        priorities[~exact_fit_mask] = 0.0 \n    else:\n        # If no exact fit is found, consider all bins that can fit the item.\n        # Prioritize bins where the remaining capacity after placing the item is minimal.\n        # This is equivalent to prioritizing bins with the smallest remaining capacity\n        # that is still greater than or equal to the item size.\n        can_fit_mask = (bins_remain_cap >= item)\n        \n        if np.any(can_fit_mask):\n            # For bins that can fit, the priority is inversely related to the\n            # remaining capacity after placing the item. A smaller remaining\n            # capacity is better. We can use 1 / (remaining_capacity - item + epsilon)\n            # to ensure a higher value for bins that fit 'tighter'.\n            # We add a small epsilon to avoid division by zero if remaining_capacity == item.\n            # Since we've already handled the exact fit case, remaining_capacity will be > item here.\n            # So, a simpler way is to rank by the negative of the remaining capacity.\n            # The smaller the remaining capacity, the larger (less negative) the priority.\n            priorities[can_fit_mask] = -bins_remain_cap[can_fit_mask]\n            # Bins that cannot fit get a priority of 0 (or negative infinity if we wanted to be strict).\n            # Since we initialized with zeros, they remain zero, effectively giving them lowest priority.\n            \n        # If no bin can fit the item, all priorities remain 0, indicating no valid placement.\n\n    return priorities",
    "response_id": 14,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 30.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response15.txt_stdout.txt",
    "code_path": "problem_iter1_code15.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit) strategy.\n\n    The strategy prioritizes bins that are \"closer\" to fitting the item.\n    Specifically, it calculates the \"distance\" as the remaining capacity minus the item size.\n    A smaller positive distance indicates a better fit.\n    To make it a priority score (higher is better), we use the inverse of (distance + a small epsilon to avoid division by zero)\n    if the item fits, otherwise the priority is 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    # Find bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the \"distance\" to fit for bins where the item can fit\n    # Distance is defined as remaining capacity - item size\n    distances = bins_remain_cap[can_fit_mask] - item\n\n    # Use a small epsilon to avoid division by zero and handle perfect fits gracefully\n    epsilon = 1e-6\n    \n    # Calculate inverse distance. Smaller distances (better fits) get higher priority.\n    # We invert the distance + epsilon to ensure higher values for better fits.\n    # Adding epsilon ensures we don't divide by zero for a perfect fit (remaining_cap == item).\n    priorities[can_fit_mask] = 1.0 / (distances + epsilon)\n\n    return priorities",
    "response_id": 15,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 41.51317942364757,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response16.txt_stdout.txt",
    "code_path": "problem_iter1_code16.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score prioritizes bins that are a \"good fit\" for the item.\n    A \"good fit\" is defined as a bin whose remaining capacity is slightly larger than the item size.\n    The sigmoid function is used to smooth this preference, giving higher scores to bins\n    where `bins_remain_cap - item` is close to zero (positive side).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We want to find bins where remaining_capacity >= item.\n    # If remaining_capacity is much larger, it's less preferred.\n    # If remaining_capacity < item, it's not a valid fit, so score should be zero.\n\n    # Calculate the \"gap\" between bin capacity and item size.\n    # We only consider bins where the item can fit (remaining_capacity >= item)\n    gaps = bins_remain_cap - item\n\n    # Apply sigmoid function to the gaps.\n    # The sigmoid function f(x) = 1 / (1 + exp(-x)) maps any real value to (0, 1).\n    # To prioritize bins where the gap is small and positive (close to 0),\n    # we can use the gap directly as the input to sigmoid.\n    # For gaps < 0 (item doesn't fit), exp(-gap) will be very large, making sigmoid close to 0.\n    # For gaps = 0, exp(0) = 1, sigmoid = 1 / (1 + 1) = 0.5. This is the center.\n    # For positive gaps, exp(-gap) decreases, making sigmoid closer to 1.\n    # This is not ideal. We want higher scores for smaller *positive* gaps.\n\n    # Let's try a sigmoid where the \"sweet spot\" is when `gap` is 0.\n    # A common approach for \"good fit\" is to maximize the ratio `item / remaining_capacity`\n    # but we need to avoid division by zero or very small capacities.\n    # Another approach: prioritize bins with minimum remaining capacity that can fit the item.\n    # This is the essence of \"First Fit Decreasing\" logic, but applied online.\n\n    # Let's reinterpret \"Sigmoid Fit Score\" to prioritize bins with small remaining capacity\n    # as long as the item fits. We want bins where `bins_remain_cap` is just slightly larger than `item`.\n    # This means `bins_remain_cap - item` should be small and non-negative.\n\n    # We can transform the `gap` to be centered around 0 for ideal fit.\n    # A good fit is when `bins_remain_cap` is as close as possible to `item`.\n    # Consider `x = bins_remain_cap - item`. We want `x` to be small and >= 0.\n    # Sigmoid function `1 / (1 + exp(-k * x))` where `k` is a scaling factor.\n    # If k > 0, as x increases, sigmoid increases. This prioritizes larger gaps.\n    # If k < 0, as x increases, sigmoid decreases. This prioritizes smaller gaps.\n    # We want to penalize bins where the item *doesn't* fit (gap < 0).\n\n    # Let's use a sigmoid that is close to 0 for invalid fits and increases towards 1\n    # as the remaining capacity gets closer to the item size from above.\n    # We can achieve this by using a negative scaling factor.\n\n    # First, set invalid bins (where item doesn't fit) to a very low priority (e.g., 0).\n    # This is a soft constraint, allowing exploration even for poor fits if no good ones exist.\n    # Or, we can explicitly mask them out before applying sigmoid.\n\n    valid_bins_mask = bins_remain_cap >= item\n\n    # Calculate the \"fit score\" for valid bins.\n    # We want bins where `bins_remain_cap` is close to `item`.\n    # So, `bins_remain_cap - item` should be small and positive.\n    # Let `x = bins_remain_cap - item`.\n    # We want to map small positive `x` to high values, and larger positive `x` to lower values.\n    # A sigmoid with a negative slope centered at 0 (or a small positive value) can work.\n\n    # Let's define a metric that is 0 for perfect fit and increases as the gap increases.\n    # Then, apply `1 - sigmoid` or a similar transformation.\n\n    # Consider the inverse of the remaining capacity. This is not quite right.\n\n    # Let's aim for a heuristic that's akin to \"Best Fit\" but using sigmoid.\n    # Best Fit aims to minimize the remaining capacity after packing.\n    # So, we want to prioritize bins with `bins_remain_cap` such that `bins_remain_cap - item` is minimized and non-negative.\n\n    # Let `y = bins_remain_cap`. We want to maximize a function `f(y)` where `f(y)` is high when `y` is slightly above `item`.\n    # A Gaussian-like shape centered at `item` might be ideal, but sigmoid is requested.\n\n    # Let's try this: map `bins_remain_cap` directly.\n    # We want higher scores for smaller `bins_remain_cap` (if valid).\n    # The sigmoid function `sigmoid(x) = 1 / (1 + exp(-x))` increases with x.\n    # So, if we want to prioritize smaller `bins_remain_cap`, we need `x` to be *inversely* related to `bins_remain_cap`.\n    # `x = -bins_remain_cap` would invert the trend.\n\n    # However, we need to handle invalid bins.\n    # Let's transform `bins_remain_cap` to create a \"fitness\" score.\n    # For a valid bin, we want the score to be high if `bins_remain_cap` is small but >= `item`.\n    # This means `bins_remain_cap - item` is small and >= 0.\n\n    # Consider the term `1.0 / (bins_remain_cap - item + epsilon)` for valid bins, where epsilon avoids division by zero.\n    # This term is large for small positive differences and smaller for larger differences.\n    # We can then apply sigmoid to this term to bound it and smooth it.\n    # `sigmoid(alpha * (1.0 / (bins_remain_cap - item + epsilon) - offset))`\n\n    # A simpler Sigmoid Fit Score approach:\n    # Consider the remaining capacity as the input to the sigmoid.\n    # To prioritize *smaller* valid remaining capacities, we need the sigmoid argument to *decrease* as `bins_remain_cap` increases.\n    # This implies a negative slope.\n    # `score = 1 / (1 + exp(-k * (bins_remain_cap - item)))` with `k > 0` gives higher score for larger gaps.\n    # `score = 1 / (1 + exp(k * (bins_remain_cap - item)))` with `k > 0` gives higher score for smaller gaps.\n\n    # Let's use the second form.\n    # `k` controls the steepness of the sigmoid. A higher `k` makes the transition sharper.\n    # For bins where `bins_remain_cap < item`, `bins_remain_cap - item` is negative.\n    # If `k > 0`, `k * (bins_remain_cap - item)` will be negative and large in magnitude.\n    # `exp(large_negative_number)` is close to 0.\n    # So, `score` will be `1 / (1 + 0)`, which is 1. This is not good, it prioritizes invalid bins.\n\n    # We must ensure invalid bins get a very low score, ideally 0.\n    # Let's use a sigmoid on the gap, but with a transformation that results in high scores for small *positive* gaps.\n\n    # Method:\n    # 1. For bins where `bins_remain_cap < item`, assign a score of 0.\n    # 2. For bins where `bins_remain_cap >= item`, calculate a \"fitness value\".\n    #    This fitness value should be high when `bins_remain_cap - item` is small and positive.\n    #    Let `fit_value = bins_remain_cap - item`.\n    # 3. Apply a sigmoid transformation to `fit_value` such that the output is high for small `fit_value`.\n    #    This can be achieved by `sigmoid(-k * fit_value)` where `k` is a positive constant.\n    #    As `fit_value` increases (gap gets larger), `-k * fit_value` decreases, and sigmoid decreases.\n\n    # Set a base scale factor for the sigmoid. This determines how sensitive the score is to the gap size.\n    # A larger `k` means the \"best fit\" range is narrower.\n    scale_factor = 2.0  # Tune this parameter. A higher value means we strongly prefer bins with smaller remaining capacity.\n\n    # Create an array to hold priorities. Initialize with zeros.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find indices of bins that can accommodate the item.\n    can_fit_indices = np.where(bins_remain_cap >= item)[0]\n\n    # If there are no bins that can fit the item, return all zeros.\n    if len(can_fit_indices) == 0:\n        return priorities\n\n    # Calculate the \"gap\" for the bins that can fit the item.\n    gaps_for_fitting_bins = bins_remain_cap[can_fit_indices] - item\n\n    # Apply the sigmoid function to the negative of the gaps.\n    # This way, smaller gaps (closer to 0) will result in higher sigmoid values.\n    # The term `scale_factor * gaps_for_fitting_bins` will be close to 0 for small gaps.\n    # `exp(-x)` for `x` near 0 is around 1. So `1 / (1 + 1)` is 0.5. This is not ideal.\n\n    # Let's refine the sigmoid application.\n    # We want scores to be high when `bins_remain_cap - item` is small.\n    # Consider the term `alpha - (bins_remain_cap - item)`.\n    # Or `-(bins_remain_cap - item)`.\n    # Let's map `bins_remain_cap` directly to a score such that values near `item` get high scores.\n\n    # Idea: Normalize remaining capacities and then apply sigmoid.\n    # Or, more directly, use `bins_remain_cap` as input to sigmoid, but with an inverted scale.\n\n    # Let `x = bins_remain_cap`.\n    # We want a function `f(x)` where `f(x)` is high for `x` slightly above `item`.\n    # Sigmoid: `1 / (1 + exp(-z))`. For it to be high, `z` needs to be large and positive.\n    # So, we need `z` to be a decreasing function of `bins_remain_cap`.\n    # `z = -scale_factor * (bins_remain_cap - item)`\n    # `score = 1 / (1 + exp(- (-scale_factor * (bins_remain_cap - item))))`\n    # `score = 1 / (1 + exp(scale_factor * (bins_remain_cap - item)))`\n\n    # This works for valid bins:\n    # If `bins_remain_cap = item`, `gap = 0`, `score = 1 / (1 + exp(0)) = 0.5`.\n    # If `bins_remain_cap = item + epsilon` (small epsilon), `gap = epsilon`.\n    # `score = 1 / (1 + exp(scale_factor * epsilon))`. For small epsilon and positive scale_factor, exp is slightly > 1. Score is slightly < 0.5. This is the opposite of what we want.\n\n    # Let's reverse the argument to sigmoid.\n    # `score = 1 / (1 + exp(-(scale_factor * (bins_remain_cap - item))))`\n    # `score = 1 / (1 + exp(-scale_factor * bins_remain_cap + scale_factor * item))`\n\n    # If `bins_remain_cap = item`, `score = 1 / (1 + exp(0)) = 0.5`.\n    # If `bins_remain_cap = item + epsilon`, `score = 1 / (1 + exp(-scale_factor * epsilon))`.\n    # For small epsilon and positive scale_factor, `exp(-scale_factor * epsilon)` is slightly less than 1.\n    # So, `score` is slightly greater than 0.5. This is good.\n    # If `bins_remain_cap = item + large_value`, `exp(-large_positive)` is close to 0. Score is close to 1.\n    # This prioritizes bins with *large* remaining capacity, which is usually *not* the goal for BPP heuristics.\n\n    # The \"Sigmoid Fit Score\" implies we want to score how well an item *fits*.\n    # A common interpretation of \"fit\" in BPP is minimizing wasted space.\n    # This means prioritizing bins where `bins_remain_cap - item` is minimal and non-negative.\n\n    # Let's try a different sigmoid argument that achieves this:\n    # We want a high score when `bins_remain_cap - item` is close to 0.\n    # Consider `f(x) = 1 / (1 + exp(-k * x))` where `x = -(bins_remain_cap - item)` i.e., `item - bins_remain_cap`.\n    # This term `item - bins_remain_cap` is non-positive for valid bins.\n    # If `bins_remain_cap = item`, `item - bins_remain_cap = 0`. `exp(0) = 1`. Score = `1 / (1+1) = 0.5`.\n    # If `bins_remain_cap = item + epsilon` (small epsilon), `item - bins_remain_cap = -epsilon`.\n    # `exp(-k * epsilon)` is slightly less than 1. Score is slightly more than 0.5.\n    # If `bins_remain_cap = item - epsilon` (invalid bin), `item - bins_remain_cap = epsilon`.\n    # `exp(k * epsilon)` is slightly more than 1. Score is slightly less than 0.5. This is still not good for invalid bins.\n\n    # A key part of online BPP is handling invalid placements. They should have the lowest priority.\n    # Let's assign a penalty to invalid bins, and a score based on \"fit\" for valid ones.\n\n    # We want a score that is:\n    # - Close to 1 for bins where `bins_remain_cap` is just slightly larger than `item`.\n    # - Decreases as `bins_remain_cap` increases beyond `item`.\n    # - Close to 0 for bins where `bins_remain_cap < item`.\n\n    # Sigmoid `1 / (1 + exp(-x))` increases with `x`.\n    # Let's construct `x` such that it's large and positive for small `bins_remain_cap - item` (when non-negative),\n    # and small/negative for `bins_remain_cap < item`.\n\n    # Let `adjusted_capacity = bins_remain_cap - item`.\n    # For valid bins (`adjusted_capacity >= 0`), we want high scores when `adjusted_capacity` is small.\n    # For invalid bins (`adjusted_capacity < 0`), we want low scores.\n\n    # We can use a sigmoid on a transformed version of `adjusted_capacity`.\n    # Let's consider the reciprocal of `adjusted_capacity + epsilon` for valid bins.\n    # `reciprocal_gap = 1.0 / (adjusted_capacity + 1e-6)`\n    # This is large when `adjusted_capacity` is small, and small when `adjusted_capacity` is large.\n\n    # Now, apply sigmoid to `reciprocal_gap`. The sigmoid function `1/(1+exp(-x))` outputs between 0 and 1.\n    # If `reciprocal_gap` is very large, `exp(-large_number)` is near 0, sigmoid is near 1.\n    # If `reciprocal_gap` is small, `exp(-small_number)` is near 1, sigmoid is near 0.5.\n    # This is again inverted. We need the argument to sigmoid to be larger for smaller gaps.\n\n    # Alternative Sigmoid strategy:\n    # Maximize `bins_remain_cap / item` subject to `bins_remain_cap >= item`. This is \"Worst Fit\".\n    # Minimize `bins_remain_cap / item` subject to `bins_remain_cap >= item`. This is related to \"Best Fit\".\n\n    # Let's focus on minimizing `bins_remain_cap - item` for valid bins.\n    # We can define a score related to the \"tightness\" of the fit.\n    # `tightness = item / bins_remain_cap` for `bins_remain_cap >= item`. High `tightness` is good.\n\n    # To incorporate sigmoid for \"Sigmoid Fit Score\":\n    # We want a smooth transition. Let's use the property that `1 - sigmoid(x)` decreases with `x`.\n    # If we can make `x` a monotonically increasing function of `bins_remain_cap` (for valid bins),\n    # then `1 - sigmoid(x)` will be a decreasing function, which is what we want for smaller remaining capacities.\n\n    # Consider `x = bins_remain_cap`.\n    # `1 - sigmoid(scale_factor * (bins_remain_cap - item))`\n    # If `bins_remain_cap = item`, `x = 0`. `1 - sigmoid(0) = 1 - 0.5 = 0.5`.\n    # If `bins_remain_cap = item + epsilon`, `x = epsilon`. `1 - sigmoid(scale_factor * epsilon)`\n    #   For positive `scale_factor`, `sigmoid(positive)` is > 0.5. So `1 - sigmoid` is < 0.5. This is again inverted.\n\n    # Let's try another transformation for the sigmoid argument.\n    # We want a function that peaks when `bins_remain_cap` is around `item` + a small epsilon.\n    # A centered sigmoid would be `1 / (1 + exp(-k * (bins_remain_cap - C)))`.\n    # If `C = item`, it peaks at 0.5 when `bins_remain_cap = item`.\n\n    # Let's try to use the sigmoid to squash values into a range, and then simply return the value.\n    # `score = sigmoid(A - B * bins_remain_cap)`\n    # If `A` and `B` are positive, this score decreases as `bins_remain_cap` increases.\n    # We also need to handle the `bins_remain_cap < item` case.\n\n    # Let's combine a \"validity check\" with a \"fit quality score\".\n    # Use sigmoid to represent the quality of fit for valid bins.\n\n    # `priority_score_for_valid_bins = sigmoid(k * (MAX_CAPACITY - bins_remain_cap))`\n    # Where `MAX_CAPACITY` is the bin's total capacity (not remaining capacity). This doesn't fit the input.\n\n    # Let's reconsider the core idea of \"Sigmoid Fit Score\". It's often used in scheduling/resource allocation to express preference based on certain criteria smoothly.\n    # For BPP, a good fit means minimizing waste, i.e., `bins_remain_cap` is close to `item` (but >= `item`).\n\n    # Let's define a \"fitness measure\" for valid bins: `f(bins_remain_cap) = -(bins_remain_cap - item)`\n    # This measure is 0 for a perfect fit and becomes more negative as `bins_remain_cap` increases.\n    # Now, apply a sigmoid that makes this measure yield high scores for values near 0 and decreasing for negative values.\n    # Sigmoid: `1 / (1 + exp(-x))`. For high scores, `x` needs to be positive.\n    # We need to transform `f(bins_remain_cap)` into something that is positive for good fits.\n\n    # Let `gap = bins_remain_cap - item`.\n    # If `gap >= 0`, we want a high score for small `gap`.\n    # If `gap < 0`, we want a score of 0.\n\n    # Try `sigmoid(K * (some_value))`\n    # Let `some_value` be related to `1.0 / (gap + epsilon)` for valid bins.\n\n    # Let's try to model \"Best Fit\" using sigmoid.\n    # Best Fit selects the bin with the minimum `bins_remain_cap` that can still fit the item.\n    # So we want to prioritize bins with small `bins_remain_cap`.\n\n    # Consider `priorities = sigmoid(C - k * bins_remain_cap)`\n    # Here `C` and `k` are constants.\n    # For valid bins, `bins_remain_cap >= item`.\n    # If `bins_remain_cap` is small, `C - k * bins_remain_cap` is larger, so sigmoid is higher. This matches Best Fit.\n    # However, we need to ensure invalid bins are penalized.\n    # If `bins_remain_cap < item`, `C - k * bins_remain_cap` is even larger if `k > 0`.\n    # This would mean invalid bins can get higher scores than valid bins with slightly larger remaining capacities. This is bad.\n\n    # We need a term that's zero or very low for invalid bins and then ramps up and potentially down for valid bins.\n\n    # Let's create a custom function that embodies this:\n    # `score(capacity) = max(0, sigmoid(some_func(capacity)))`\n    # Or, `score = valid_mask * sigmoid(some_func(capacity))`\n\n    # Let `adjusted_capacity = bins_remain_cap - item`.\n    # For valid bins, `adjusted_capacity >= 0`. We want high scores when `adjusted_capacity` is small.\n    # Let `transformed_value = -scale_factor * adjusted_capacity`.\n    # If `adjusted_capacity` is small positive, `transformed_value` is small negative. Sigmoid will be < 0.5.\n    # If `adjusted_capacity` is large positive, `transformed_value` is large negative. Sigmoid will be near 0.\n    # This is inverted again!\n\n    # The correct sigmoid argument for prioritizing small positive values is `1 / (val + epsilon)` or similar transformation before sigmoid.\n    # Or, use sigmoid in a way that maps smaller values to larger outputs.\n    # `sigmoid(A - B*val)` maps smaller `val` to higher output.\n\n    # Let's try this structure for priority:\n    # priority_for_bin_i = sigmoid(\n    #     some_base_score - factor * (bins_remain_cap[i] - item)\n    # )\n    # We need to ensure that bins with `bins_remain_cap[i] < item` get a score close to zero.\n\n    # Let's scale the `bins_remain_cap` relative to the item size to make the sigmoid argument more robust.\n    # `normalized_fit = bins_remain_cap / item` (handle item = 0 if necessary, but BPP items are > 0)\n    # For valid bins, `normalized_fit >= 1.0`. We want values close to 1.0.\n\n    # `transformed_for_sigmoid = 1.0 / normalized_fit` (for valid bins)\n    # This value is close to 1.0 for `normalized_fit` near 1.0.\n    # As `normalized_fit` increases, `transformed_for_sigmoid` decreases.\n    # This is what we want! Higher scores for smaller `normalized_fit` (closer to 1.0).\n\n    # Apply sigmoid to this `transformed_for_sigmoid`.\n    # `score = sigmoid(k * transformed_for_sigmoid)`\n    # `score = sigmoid(k * (item / bins_remain_cap))` for valid bins.\n\n    # Now, how to handle invalid bins?\n    # We can make the sigmoid argument `-(large_number)` if `bins_remain_cap < item`.\n    # This makes the score `sigmoid(-large_number)` which is close to 0.\n\n    # Let's formalize:\n    # If `bins_remain_cap[i] < item`: priority = 0.0\n    # If `bins_remain_cap[i] >= item`:\n    #   Let `metric = item / bins_remain_cap[i]`\n    #   We want `metric` to be close to 1.0 for best fit.\n    #   Let's transform `metric` so it's higher when close to 1.0 and then use sigmoid.\n    #   Consider `transformed = 1.0 / metric = bins_remain_cap[i] / item`. This is increasing.\n    #   If we use `sigmoid(k * (C - transformed))`, it will be higher for smaller `transformed`.\n\n    # Final attempt with a clear logic:\n    # Prioritize bins that have the smallest remaining capacity that is still greater than or equal to the item size.\n    # This is \"Best Fit\" logic. We need to represent this with a sigmoid.\n\n    # For each bin `i`:\n    # If `bins_remain_cap[i] < item`: priority[i] = 0.0\n    # If `bins_remain_cap[i] >= item`:\n    #    Let `gap = bins_remain_cap[i] - item`.\n    #    We want to assign a score based on `gap`, where smaller `gap` is better.\n    #    Let's use the sigmoid to map `gap` values.\n    #    A sigmoid function `f(x) = 1 / (1 + exp(-x))` is monotonic increasing.\n    #    To make scores higher for smaller `gap`, we need the argument to sigmoid to be a decreasing function of `gap`.\n    #    Let `argument = -scale_factor * gap`.\n    #    `priority[i] = sigmoid(-scale_factor * (bins_remain_cap[i] - item))`\n\n    # Now, consider the range of `bins_remain_cap`. The `scale_factor` needs tuning.\n    # What if `bins_remain_cap[i] - item` is very large?\n    # E.g., `bins_remain_cap[i] = 100`, `item = 10`. `gap = 90`.\n    # `sigmoid(-scale_factor * 90)` will be close to 0.\n    # What if `bins_remain_cap[i] = 11`, `item = 10`. `gap = 1`.\n    # `sigmoid(-scale_factor * 1)` will be closer to 0.5 if scale_factor is reasonable.\n    # What if `bins_remain_cap[i] = 10`, `item = 10`. `gap = 0`.\n    # `sigmoid(0)` is 0.5.\n\n    # This `sigmoid(-scale_factor * gap)` formulation gives:\n    # - 0.5 for perfect fit (`gap=0`)\n    # - Values slightly > 0.5 for small positive gaps\n    # - Values decreasing towards 0 for larger positive gaps.\n    # This prioritizes bins that are \"over-full\" and decreasingly prioritizes bins with larger remaining capacity.\n    # This aligns with \"Best Fit\" where we seek the tightest fit.\n\n    # The issue is the invalid bins.\n    # We need to ensure `priority[i] = 0` when `bins_remain_cap[i] < item`.\n\n    # Let's use the validity mask.\n    # We calculate the sigmoid score only for valid bins and multiply by the mask.\n\n    # Set a scale factor. This parameter controls how aggressive the \"best fit\" is.\n    # A larger scale_factor means the difference between a perfect fit and a slightly larger gap\n    # will result in a more pronounced score difference.\n    # Let's choose a value that might spread scores for typical gaps.\n    # If item is 10, and capacities are 10, 12, 15, 20.\n    # Gaps are 0, 2, 5, 10.\n    # With `scale_factor = 1.0`:\n    #   sigmoid(0) = 0.5\n    #   sigmoid(-2) \u2248 0.12\n    #   sigmoid(-5) \u2248 0.0067\n    #   sigmoid(-10) \u2248 0.000045\n    # This is completely inverted again. Higher score for smaller negative input.\n\n    # OK, final refined strategy:\n    # The goal is to score bins based on \"goodness of fit\" for an item.\n    # A \"good fit\" means the bin can accommodate the item, and the remaining capacity after packing is minimized.\n    # This means `bins_remain_cap - item` should be small and non-negative.\n\n    # We can define a desirability function `D(capacity, item)`.\n    # We want `D` to be high when `capacity` is close to `item` and `capacity >= item`.\n    # We want `D` to be low when `capacity < item`.\n\n    # Sigmoid form: `sigmoid(arg)` outputs values in (0, 1).\n    # To get scores from 0 up to 1, we can do `mask * sigmoid(arg)`.\n    # Or `sigmoid(arg)` can handle the penalty implicitly.\n\n    # Let's craft `arg` such that:\n    # - If `bins_remain_cap < item`, `arg` is a large negative number.\n    # - If `bins_remain_cap == item`, `arg` is moderate.\n    # - If `bins_remain_cap` is slightly above `item`, `arg` is larger.\n    # - If `bins_remain_cap` is much larger than `item`, `arg` is small negative or close to zero.\n\n    # Consider the function `1.0 / (bins_remain_cap - item + epsilon)` for valid bins.\n    # This is large when `bins_remain_cap - item` is small.\n    # Let's map this to the argument of sigmoid:\n    # `arg = scale_factor * (1.0 / (bins_remain_cap - item + epsilon))` for valid bins.\n    # Then `sigmoid(arg)` will be high when `gap` is small.\n\n    # Let's implement this carefully:\n\n    # Set a small epsilon to avoid division by zero or extreme values for perfect fits.\n    epsilon = 1e-6\n    # Scale factor to tune the sensitivity of the sigmoid.\n    # A larger scale_factor makes the function more sensitive to small differences in fit.\n    # For example, if scale_factor = 5, and item = 10, capacities are 10, 11, 12.\n    # Gaps are 0, 1, 2.\n    # 1/(gap+epsilon): large, ~1, ~0.5\n    # Sigmoid(5 * these_values):\n    #   5*large -> sigmoid(very large) -> near 1\n    #   5*1 -> sigmoid(5) -> ~0.993\n    #   5*0.5 -> sigmoid(2.5) -> ~0.924\n    # This looks good. Prioritizes perfect fit, then very close fits.\n\n    # Need to handle invalid bins explicitly or design the transformation to yield 0.\n    # The `1.0 / (bins_remain_cap - item + epsilon)` formulation only works for valid bins.\n\n    # Let's apply the sigmoid to a measure that directly relates to \"tightness\".\n    # For valid bins: `item / bins_remain_cap`. This ratio is high for tight fits.\n    # Let `fit_ratio = item / bins_remain_cap`.\n    # We want to apply sigmoid to `fit_ratio` to get scores between 0 and 1.\n    # `score = sigmoid(k * fit_ratio)`\n    # If `bins_remain_cap = item`: `fit_ratio = 1.0`. `score = sigmoid(k)`.\n    # If `bins_remain_cap = item + epsilon`: `fit_ratio = item / (item+epsilon) < 1.0`. `score = sigmoid(k * ratio) < sigmoid(k)`.\n    # This is inverted.\n\n    # Let's use the `sigmoid(A - B * value)` pattern.\n    # We want higher scores for smaller `bins_remain_cap` (if valid).\n    # Let `score = sigmoid(constant_A - scale_factor * bins_remain_cap)`.\n    # This will make scores higher for smaller `bins_remain_cap`.\n\n    # How to ensure invalid bins get 0?\n    # Let's define `priorities` to be 0 initially.\n    # Then, for valid bins, calculate a score and update `priorities[valid_indices]`.\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_indices = np.where(bins_remain_cap >= item)[0]\n\n    if len(valid_indices) > 0:\n        valid_bins_remain_cap = bins_remain_cap[valid_indices]\n\n        # Strategy: Maximize the \"density\" of packing, meaning prioritize bins\n        # where the item takes up a larger fraction of the *remaining* capacity.\n        # This is `item / bins_remain_cap`.\n        # To use sigmoid for this, we want the argument to be higher when `item / bins_remain_cap` is higher.\n        # Let `fit_metric = item / valid_bins_remain_cap`.\n\n        # Now apply sigmoid. We want higher scores for higher `fit_metric`.\n        # `sigmoid(scale_factor * fit_metric)` will do this.\n        # However, `fit_metric` can be very large if `bins_remain_cap` is slightly larger than `item`.\n        # E.g., item=10, capacity=10.1. fit_metric = 10/10.1 \u2248 0.99.\n        # E.g., item=10, capacity=20. fit_metric = 10/20 = 0.5.\n\n        # Let's use a slightly different approach to make the sigmoid argument centered or scaled.\n        # Consider `adjusted_capacity = bins_remain_cap - item`.\n        # We want small, non-negative `adjusted_capacity` to get high scores.\n\n        # Let's consider the inverse of `adjusted_capacity` to achieve inversion.\n        # `inverse_gap = 1.0 / (bins_remain_cap - item + epsilon)` for valid bins.\n        # This value is high when the gap is small.\n\n        # Now, we want to map these `inverse_gap` values using sigmoid.\n        # To make it simpler, let's center the values around some baseline.\n        # A common approach is to shift and scale such that the desired \"best fit\" region maps to the middle of the sigmoid (0) or positive values.\n\n        # Let's try a form like: `sigmoid(offset - scale * value)` where `value` is `bins_remain_cap`.\n        # This makes the score higher for smaller `bins_remain_cap`.\n\n        # Let's use `bins_remain_cap` directly.\n        # We want to model \"Best Fit\", which means choosing the bin with the smallest remaining capacity that can accommodate the item.\n        # So, if `bins_remain_cap` is small and valid, score should be high.\n        # If `bins_remain_cap` is large and valid, score should be lower.\n        # If `bins_remain_cap` is invalid, score should be zero.\n\n        # We can define a score function:\n        # `score(capacity) = (capacity >= item) ? sigmoid(some_func(capacity)) : 0`\n\n        # Let's define `some_func(capacity)` such that it makes small `capacity` yield higher sigmoid outputs.\n        # `sigmoid(C - K * capacity)` achieves this.\n        # `C` and `K` need tuning.\n\n        # A common sigmoid fitting strategy maps a performance metric `p` (e.g., error rate) to a score.\n        # If `p` is low, score should be high.\n        # `score = sigmoid(A - B * p)`.\n\n        # Here, the performance metric is `bins_remain_cap` itself. Lower is better.\n        # So, `p = bins_remain_cap`.\n        # `score = sigmoid(C - K * bins_remain_cap)`.\n\n        # Let's select `C` and `K` such that scores are reasonable.\n        # Example: bin capacities range from 1 to 100. Item size from 1 to 50.\n        # If item=20, and bins have remaining capacities [20, 25, 30, 50, 100].\n        # We want highest score for 20, then 25, etc.\n        # `score(cap) = sigmoid(C - K * cap)`.\n        # To make 20 yield highest score, we need `C - K*20` to be largest among valid ones.\n\n        # Let's try to make the \"ideal\" capacity map to the center of sigmoid (0 for arg).\n        # What is ideal? Maybe not the item size itself, but slightly larger.\n        # Let's consider the range of available capacities.\n        # If capacities are typically between `min_cap` and `max_cap`.\n\n        # Simplified strategy:\n        # A bin is prioritized if it fits the item (`bins_remain_cap >= item`).\n        # Among the bins that fit, prioritize those with smaller `bins_remain_cap`.\n        # This is \"Best Fit\".\n        # The sigmoid function can transform this preference into a score.\n\n        # Consider `bins_remain_cap` as the input to a function that produces a \"goodness\" score.\n        # The function should be monotonically decreasing for values >= item.\n        # Let `score_raw = bins_remain_cap`.\n        # We want a high score when `score_raw` is small (and valid).\n        # `priorities[valid_indices] = sigmoid(BASE - SCALE * valid_bins_remain_cap)`\n\n        # Let's choose BASE and SCALE.\n        # If `valid_bins_remain_cap` is small (e.g., close to `item`), we want `BASE - SCALE * small_cap` to be large.\n        # If `valid_bins_remain_cap` is large, we want `BASE - SCALE * large_cap` to be small.\n\n        # Example: Let MAX_CAP be the maximum possible capacity of any bin.\n        # We can normalize `bins_remain_cap` relative to `MAX_CAP` or relative to `item`.\n\n        # A robust approach that tries to map a \"goodness\" metric to a sigmoid:\n        # \"Goodness\" metric: Higher for smaller `bins_remain_cap` (if valid).\n        # Let `metric = -bins_remain_cap`. Higher is better.\n        # Apply sigmoid to this `metric` with scaling.\n        # `priority = sigmoid(scale * metric)`\n\n        # Let's try to make the scores meaningful.\n        # If item=10, capacities=[10, 12, 15, 50]. Valid capacities=[10, 12, 15, 50].\n        # We want priority for 10 > 12 > 15 > 50.\n\n        # Consider a target value `T`. If `bins_remain_cap` is near `T`, score is high.\n        # For BPP, `T` should be around `item`.\n        # Let `adjusted_cap = bins_remain_cap - item`. We want small non-negative `adjusted_cap`.\n        # `sigmoid(offset - scale * adjusted_cap)`\n        # `sigmoid(offset - scale * (bins_remain_cap - item))`\n\n        # To make scores fall between 0 and 1:\n        # If `bins_remain_cap < item`, score should be 0.\n        # Otherwise, score should be `sigmoid(a - b * (bins_remain_cap - item))` for some `a`, `b > 0`.\n\n        # Let's pick `a` and `b` so the transition is meaningful.\n        # If `bins_remain_cap == item`, `score = sigmoid(a)`.\n        # If `bins_remain_cap = item + Delta`, `score = sigmoid(a - b*Delta)`.\n        # We want `a - b*Delta` to decrease as `Delta` increases.\n\n        # Let's choose `a` such that the score at perfect fit (`Delta=0`) is not too extreme.\n        # Maybe `a = 0` for `sigmoid(0) = 0.5`.\n        # Then `score = sigmoid(-b * (bins_remain_cap - item))`\n        # `score = 1 / (1 + exp(b * (bins_remain_cap - item)))`\n\n        # Test: item=10, capacities=[10, 12, 15, 50]. Let b=1.0.\n        # cap=10: gap=0, score = 1/(1+exp(0)) = 0.5\n        # cap=12: gap=2, score = 1/(1+exp(2)) \u2248 0.119\n        # cap=15: gap=5, score = 1/(1+exp(5)) \u2248 0.0067\n        # cap=50: gap=40, score = 1/(1+exp(40)) \u2248 0.00000000000000000000000000000000000000008\n\n        # This gives high scores for perfect fit, decreasing scores for larger gaps.\n        # This is exactly what \"Best Fit\" aims for.\n\n        # Now, combine with the invalidity mask.\n\n        scale_factor = 2.0  # Controls the steepness of the sigmoid; how quickly preference drops off with increasing remaining capacity.\n\n        # Calculate the argument for the sigmoid function.\n        # For valid bins, we want to penalize larger remaining capacities.\n        # The expression `-(valid_bins_remain_cap - item)` makes larger capacities result in more negative numbers.\n        # When passed to sigmoid(x), smaller x values yield smaller outputs.\n        # So we need argument to be `scale_factor * (item - valid_bins_remain_cap)` or `scale_factor * valid_bins_remain_cap` with inverted sigmoid.\n\n        # Let's use the form: `1 / (1 + exp(k * (capacity - item)))`\n        # This form makes `1/(1+exp(0))=0.5` for perfect fit, and decreases for larger capacities.\n        # This captures \"Best Fit\".\n\n        fit_arg = scale_factor * (valid_bins_remain_cap - item)\n        priorities[valid_indices] = 1.0 / (1.0 + np.exp(-fit_arg)) # Using -fit_arg to reverse the exponential decay effect to a rise towards 1\n\n        # Let's re-evaluate the argument to sigmoid:\n        # We want:\n        # 1. Small `bins_remain_cap` (if >= item) -> High Score\n        # 2. Large `bins_remain_cap` (if >= item) -> Low Score\n        # 3. `bins_remain_cap < item` -> Score = 0\n\n        # Consider `score = sigmoid(k * (item - bins_remain_cap))`\n        # item = 10, cap = [10, 12, 15, 50], k=1.0\n        # cap=10: item-cap = 0. sigmoid(0) = 0.5\n        # cap=12: item-cap = -2. sigmoid(-2) \u2248 0.12\n        # cap=15: item-cap = -5. sigmoid(-5) \u2248 0.0067\n        # cap=50: item-cap = -40. sigmoid(-40) \u2248 8e-18 (close to 0)\n\n        # This formula prioritizes bins with small `bins_remain_cap` from the *valid* set.\n        # But it produces scores like 0.5 for perfect fit, not necessarily close to 1.\n        # And it requires explicit zeroing for invalid bins.\n\n        # The prompt mentions \"Sigmoid Fit Score strategy\". This suggests using sigmoid for the score itself.\n        # If we want scores to range from 0 to 1, and highest for perfect fit:\n        # Score = sigmoid(a - b * gap)\n        # If gap = 0, score = sigmoid(a).\n        # If gap = delta, score = sigmoid(a - b*delta).\n        # For gap=0 to be max, need `a` to be largest.\n\n        # Let's try to map the inverse of remaining capacity to sigmoid:\n        # `inverted_cap = 1.0 / (bins_remain_cap + epsilon)`\n        # Higher `inverted_cap` is better.\n        # `score = sigmoid(scale * inverted_cap)` ? No, still not right.\n\n        # The most intuitive \"Sigmoid Fit Score\" for Best Fit logic would be:\n        # For a bin `i`:\n        #   If `bins_remain_cap[i] < item`: priority = 0\n        #   Else: priority = SigmoidFunction(bins_remain_cap[i], item)\n        # Where SigmoidFunction decreases as `bins_remain_cap[i]` increases.\n        # e.g., `sigmoid(a - b * bins_remain_cap[i])` or `1 - sigmoid(a + b * bins_remain_cap[i])`.\n\n        # Let's try `sigmoid(k * (1.0 / (bins_remain_cap - item + epsilon) - C))`\n        # This would map small gaps to large positive arguments in sigmoid.\n\n        # The simplest and most direct interpretation of \"Sigmoid Fit Score\" for Best Fit is probably:\n        # `score(capacity) = sigmoid(K * (max_capacity_or_item - capacity))`\n        # where `max_capacity_or_item` is a reference point.\n\n        # Let's use `item` as a reference.\n        # If `bins_remain_cap[i] < item`, priority = 0.\n        # Else, priority = `sigmoid(scale * (item - bins_remain_cap[i]))`.\n        # This implies:\n        # - `item - bins_remain_cap[i]` is negative for valid bins.\n        # - `sigmoid(negative)` yields values < 0.5.\n        # - `scale * (item - bins_remain_cap[i])` is more negative for larger `bins_remain_cap[i]`.\n        # - So, `sigmoid(...)` will be smaller for larger `bins_remain_cap[i]`.\n\n        # This means:\n        # - Perfect fit (`bins_remain_cap[i] == item`): `item - bins_remain_cap[i] = 0`. `sigmoid(0) = 0.5`.\n        # - Slightly larger capacity (`bins_remain_cap[i] = item + d`): `item - bins_remain_cap[i] = -d`. `sigmoid(-scale * d)` < 0.5.\n        # - Much larger capacity: `sigmoid` gets very close to 0.\n\n        # This results in scores for valid bins between (0, 0.5].\n        # To get scores from 0 to 1, we need to map the output appropriately.\n        # We can scale and shift this. `score_mapped = 0.5 + 0.5 * sigmoid(scale * (item - bins_remain_cap[i]))`\n        # This will map 0.5 -> 0.75, 0.12 -> 0.56, close to 0 -> 0.5. This is also not what we want.\n\n        # The issue is that `sigmoid(x)` naturally maps a monotonic function of capacity to (0, 1).\n        # If we want decreasing function (Best Fit), the argument needs to be decreasing.\n\n        # Let's make the argument such that:\n        # - When `bins_remain_cap = item`, argument is `M`.\n        # - When `bins_remain_cap = item + large`, argument is `M - large_value`.\n        # - When `bins_remain_cap < item`, we want score to be 0.\n\n        # Simplest way to incorporate Best Fit with sigmoid is to map the *quality* of fit.\n        # Quality = `item / bins_remain_cap` for valid bins. Higher is better.\n        # Let `metric = item / valid_bins_remain_cap`.\n        # We want higher scores for higher `metric`.\n        # `priorities[valid_indices] = sigmoid(scale_factor * metric)`\n\n        # With scale_factor = 2.0:\n        # item = 10. valid_caps = [10, 12, 15, 50].\n        # metrics = [1.0, 0.833, 0.667, 0.2].\n        # sigmoid(2 * 1.0) = sigmoid(2) \u2248 0.88\n        # sigmoid(2 * 0.833) = sigmoid(1.666) \u2248 0.84\n        # sigmoid(2 * 0.667) = sigmoid(1.334) \u2248 0.79\n        # sigmoid(2 * 0.2) = sigmoid(0.4) \u2248 0.59\n\n        # This prioritizes bins with a higher `item / bins_remain_cap` ratio, which means smaller remaining capacity relative to item size.\n        # This aligns with \"Best Fit\".\n        # The scores are between ~0.59 and ~0.88 for valid bins, which is a reasonable range.\n\n        # Let's adopt this last strategy.\n\n        valid_capacities = bins_remain_cap[valid_indices]\n\n        # Calculate a \"fit ratio\" - how much of the remaining capacity the item would take.\n        # Higher ratio implies a better fit for Best Fit strategy.\n        # Add epsilon to denominator to avoid division by zero if item is 0 (though it shouldn't be in BPP).\n        fit_ratio = item / (valid_capacities + epsilon)\n\n        # Apply sigmoid to the fit ratio.\n        # We scale the fit_ratio before applying sigmoid. This tuning parameter (scale_factor)\n        # determines how sensitive the priority is to the fit ratio.\n        # A larger scale_factor means a smaller difference in fit ratio will result in a larger difference in priority score.\n        # `sigmoid(x) = 1 / (1 + exp(-x))`\n        # If `fit_ratio` is high, `scale_factor * fit_ratio` is higher, `exp(-large)` is small, sigmoid is close to 1.\n        # If `fit_ratio` is low, `scale_factor * fit_ratio` is lower, `exp(-small)` is larger, sigmoid is closer to 0.5.\n        priorities[valid_indices] = 1.0 / (1.0 + np.exp(-(scale_factor * fit_ratio)))\n\n    return priorities",
    "response_id": 16,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 249.9824559469954,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response17.txt_stdout.txt",
    "code_path": "problem_iter1_code17.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Random Fit strategy.\n\n    The Random Fit strategy aims to randomly select a valid bin for the item.\n    This implementation assigns a higher priority to bins that can fit the item,\n    and among those, it gives a slight random boost to encourage exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to a very low value (negative infinity is often used for no-fit)\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Assign a base priority to bins that can fit the item.\n    # A simple approach is to assign a value of 1, signifying \"can fit\".\n    # The randomness will then distinguish between them.\n    priorities[can_fit_mask] = 1.0\n\n    # Add a random component to the priority of bins that can fit the item.\n    # This ensures that any of the fitting bins has a chance of being selected,\n    # mimicking the \"random\" aspect of Random Fit. The magnitude of the random\n    # number can be small to not overly influence the decision if other criteria\n    # were to be introduced. Here, we use a uniform distribution between 0 and 1.\n    random_boost = np.random.rand(len(bins_remain_cap))\n    priorities[can_fit_mask] += random_boost[can_fit_mask]\n\n    # Bins that cannot fit the item will retain their -np.inf priority.\n    return priorities",
    "response_id": 17,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 34, in priority_v2\n    # Exploration: Assign equal probability to all valid bins\nnumpy._core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'\n1\n24.0\n"
  },
  {
    "stdout_filepath": "problem_iter1_response18.txt_stdout.txt",
    "code_path": "problem_iter1_code18.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using an Epsilon-Greedy strategy.\n\n    This heuristic aims to balance exploration (trying new bins) and exploitation\n    (using bins that are a good fit).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploration\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Filter out bins that cannot fit the item\n    valid_bin_indices = np.where(bins_remain_cap >= item)[0]\n\n    if len(valid_bin_indices) == 0:\n        # No bin can fit the item, return zeros\n        return priorities\n\n    # Exploitation: Calculate \"goodness\" of fitting the item into valid bins\n    # We prioritize bins that leave minimal remaining capacity (First Fit Decreasing-like intuition)\n    # Adding a small constant to avoid division by zero if a bin is exactly the item size\n    remaining_after_fit = bins_remain_cap[valid_bin_indices] - item + 1e-9\n    # We want to minimize remaining capacity, so we take the inverse\n    exploitation_scores = 1.0 / remaining_after_fit\n\n    # Exploration: Assign equal probability to all valid bins\n    # For exploration, all valid bins are equally good candidates.\n    # We use a small positive value to ensure they are considered.\n    exploration_scores = np.ones_like(valid_bin_indices, dtype=float) * epsilon / len(valid_bin_indices)\n\n    # Combine exploitation and exploration\n    # The priority is a weighted sum. For exploitation, the weight is (1-epsilon).\n    # For exploration, the weight is epsilon.\n    # We want to maximize the combined score.\n    combined_scores = (1 - epsilon) * exploitation_scores + exploration_scores\n\n    # Assign combined scores to the correct indices in the full priorities array\n    priorities[valid_bin_indices] = combined_scores\n\n    # Normalize priorities to sum to 1 (optional, but can be useful for some selection methods)\n    # However, for simply picking the max, normalization isn't strictly necessary.\n    # Let's keep it unnormalized for now, as we will likely pick the max directly.\n\n    return priorities",
    "response_id": 18,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 135.7068586817104,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    This heuristic aims to balance two goals:\n    1. Fit the item into a bin with minimal remaining capacity (tight fit) to avoid\n       leaving small, unusable gaps.\n    2. Favor bins that have enough capacity for the current item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to a very small negative number for bins that cannot fit the item.\n    # This ensures they will have a low probability in the softmax.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n\n    if suitable_bins_remain_cap.size == 0:\n        return priorities # No bin can fit the item\n\n    # Calculate a \"fit score\" for suitable bins.\n    # A smaller remaining capacity after fitting the item is preferred.\n    # We add a small epsilon to avoid division by zero or log(0) issues if remaining capacity is 0.\n    # The intuition is to make the remaining capacity as small as possible *after* placing the item.\n    # So, if a bin has capacity C and the item has size I, the remaining capacity is C-I.\n    # We want to minimize C-I.\n    # A simple approach is to use 1/(C-I+epsilon) as a score, or to take a negative of it.\n    # Here, we want to maximize the probability for bins with *smaller* remaining capacity after fitting.\n    # Let's use a score that is inversely proportional to the remaining capacity *after* fitting.\n    # So, if a bin has capacity `cap` and item `item`, the new remaining capacity is `cap - item`.\n    # We want to prioritize bins where `cap - item` is small.\n    # A score of `-(cap - item)` would mean smaller remaining capacity gives higher score.\n    # To make it work with softmax, we want values that are not extremely large negative.\n    # Let's consider the \"wasted space\" if the item is placed, which is `cap - item`.\n    # We want to prioritize bins that minimize this wasted space.\n    # A score like `-abs(cap - item)` or `-(cap - item)` achieves this.\n    # To keep values in a reasonable range for softmax and to promote diversity,\n    # we can transform it.\n    # Let's use a score that is inversely related to the remaining capacity AFTER fitting.\n    # For example, `1 / (bins_remain_cap[i] - item + epsilon)` where epsilon is small.\n    # Higher value means smaller remaining capacity.\n    # Or, even simpler, prioritize bins with the smallest remaining capacity *after* the item is placed.\n    # Let `remaining_after_fit = bins_remain_cap[i] - item`.\n    # We want to maximize the priority for smaller `remaining_after_fit`.\n    # A score of `-remaining_after_fit` works, so we want to maximize `-(bins_remain_cap[i] - item)`.\n    # This is equivalent to minimizing `bins_remain_cap[i] - item`.\n    # For softmax, we usually want positive values or values that are not excessively negative.\n    # Let's consider the 'tightness' of the fit. A tighter fit means `bins_remain_cap[i] - item` is small.\n    # We can represent this by `1.0 / (bins_remain_cap[i] - item + 1e-9)` or `-(bins_remain_cap[i] - item)`.\n    # Let's use `-(bins_remain_cap[i] - item)` as the raw \"fit preference\".\n    # To make it suitable for softmax, we can scale it or shift it if needed.\n    # A direct approach is `bins_remain_cap[i] - item`. If we want to maximize this, it means\n    # we prefer bins with *more* space left. That's not the \"tight fit\" strategy.\n    #\n    # Let's re-evaluate: Softmax assigns probability P(bin_i) = exp(score_i) / sum(exp(score_j)).\n    # We want bins with less remaining capacity *after* placing the item to have higher scores.\n    # So, if remaining capacity is `R = bins_remain_cap - item`, we want to prioritize smaller R.\n    # A function `f(R)` such that `f(R)` is increasing for decreasing `R`.\n    # Examples: `-R`, `1/R`, `exp(-R)`.\n    # Using `-R` or `-(bins_remain_cap - item)` is a good candidate.\n    # Let's normalize `bins_remain_cap[i] - item`.\n    # For Softmax, it's better if scores are not too far apart initially to avoid one probability being\n    # overwhelmingly dominant, unless that's intended.\n    #\n    # Alternative: Consider the \"tightness\" directly. How much \"waste\" is there if we put it in?\n    # Waste = `bins_remain_cap[i] - item`. We want to MINIMIZE waste.\n    # So, a higher score should correspond to lower waste.\n    # Let's use `score = -(bins_remain_cap[i] - item)`.\n    # However, this can lead to very negative scores if `bins_remain_cap[i] - item` is large.\n    #\n    # Let's try to map the remaining capacity *after* fit to a desirability score.\n    # Smaller remaining capacity => higher desirability.\n    # The range of `bins_remain_cap[i] - item` can be from 0 up to `max(bins_remain_cap) - item`.\n    # Let `post_fit_capacity = bins_remain_cap[suitable_bins_mask] - item`.\n    # We want to convert `post_fit_capacity` into priorities.\n    # A simple mapping to make smaller values map to larger priorities:\n    # `priority = C - post_fit_capacity` for some constant C, or `1.0 / (post_fit_capacity + epsilon)`.\n    # Let's use a linear transformation that maps small remaining capacities to larger scores.\n    # Consider the range of `post_fit_capacity`. We want the smallest values of this range to get the highest scores.\n    # Let `min_post_fit = 0` and `max_post_fit = max(bins_remain_cap) - item`.\n    # We can normalize `post_fit_capacity` to `[0, 1]` and then invert it: `1 - normalized_capacity`.\n    # Normalized capacity = `(post_fit_capacity - min_post_fit) / (max_post_fit - min_post_fit + epsilon)`.\n    # So priority could be `1 - ( (post_fit_capacity - 0) / (max(bins_remain_cap) - item + epsilon) )`.\n    # This would mean bins with 0 remaining capacity after fit get a priority of 1, and\n    # bins with maximum possible remaining capacity after fit get a priority of 0.\n    #\n    # Let's try a simpler approach: Directly use `-(bins_remain_cap[i] - item)` as a raw score.\n    # To keep values more positive and closer for softmax, we can add a large constant.\n    # Or, let's normalize based on the sum of remaining capacities.\n    # The core idea of Softmax-Based Fit is often to leverage the idea of \"best fit\" or \"worst fit\"\n    # and apply a soft selection mechanism.\n    #\n    # A common pattern for Softmax-based selection is to have scores related to some\n    # desirable quantity.\n    # If we want to minimize remaining capacity *after* fit, the quantity to minimize is\n    # `bins_remain_cap[i] - item`.\n    # Let `x_i = bins_remain_cap[i] - item` for suitable bins.\n    # We want higher probability for smaller `x_i`.\n    # This suggests a score like `-x_i`.\n    #\n    # Let's consider a different approach that's more robust.\n    # What if we reward bins that are \"almost full\" after placing the item?\n    # This means `bins_remain_cap[i] - item` is small.\n    # Let `fitting_score = 1.0 / (bins_remain_cap[i] - item + 1e-9)`.\n    # This will give very high scores to bins with near-zero remaining capacity after fitting.\n    #\n    # Another variant is to reward bins that are already \"somewhat full\", but can still fit the item.\n    # This could mean `bins_remain_cap[i]` is large but not *too* large.\n    #\n    # Let's stick to the \"tight fit\" interpretation: Prioritize bins that leave minimal space after placing the item.\n    # The value `bins_remain_cap[i] - item` represents the remaining capacity after fitting.\n    # We want to maximize priority for smaller values of this.\n    # Let's define `residual_capacity = bins_remain_cap[suitable_bins_mask] - item`.\n    # We want a scoring function `s(residual_capacity)` that is decreasing.\n    # `s(r) = -r` is the simplest.\n    #\n    # However, to make it work with softmax and encourage exploration, sometimes adding a slight penalty\n    # for very tight fits (which might lead to fragmentation) or a bonus for \"just enough\" capacity is useful.\n    #\n    # Let's consider a score that reflects how \"full\" the bin would become *relative to its current state*.\n    # This isn't directly supported by the 'fit' part.\n    #\n    # Let's refine the 'tight fit' idea. A good fit means the remaining capacity after placing the item is small.\n    # `remaining_after_fit = bins_remain_cap[i] - item`\n    # We want to maximize priority for small `remaining_after_fit`.\n    # Let `score_for_bin_i = - (bins_remain_cap[i] - item)`\n    # This means, for `item=3` and `bins_remain_cap = [5, 7, 10]`:\n    # Suitable bins are all.\n    # `remaining_after_fit` are `[2, 4, 7]`.\n    # Scores would be `[-2, -4, -7]`.\n    # Softmax of `[-2, -4, -7]` will assign higher probability to `-2`, which corresponds to `bin_remain_cap = 5`. This is correct.\n    #\n    # To make scores more stable and less prone to extreme values in softmax, especially if the\n    # range of `bins_remain_cap[i] - item` is large, we can normalize them or use a shifted version.\n    #\n    # A common strategy in similar problems (like routing with softmax) is to use `score = -distance` or `score = -cost`.\n    # Here, `cost` is `bins_remain_cap[i] - item`. We want to minimize this cost.\n    #\n    # Let's ensure scores are not too extreme for softmax.\n    # We can cap the negative values or scale them.\n    #\n    # A simple approach that often works well for 'tight fit' is to use the negative of the\n    # remaining capacity after fitting.\n    # `score = -(bins_remain_cap[suitable_bins_mask] - item)`\n    #\n    # To prevent excessively large negative numbers, we could add a constant or take the\n    # exponential of a transformed value.\n    #\n    # Let's try to be a bit more nuanced. The goal is to pick a bin that is \"just enough\"\n    # but not overly spacious.\n    #\n    # Consider the 'waste' `W = bin_capacity - item`. We want to minimize W.\n    # A \"good\" bin is one where `W` is small.\n    # A \"bad\" bin is one where `W` is large.\n    # We can transform `W` into a score.\n    # Let's use a score that captures how \"full\" the bin becomes *relative to the item size*.\n    # `score = -(bins_remain_cap[i] - item)` implies we want to minimize `bins_remain_cap[i] - item`.\n    #\n    # What if we add a small \"tolerance\" factor?\n    # `score = -(bins_remain_cap[i] - item + alpha * bins_remain_cap[i])`\n    # This penalizes bins that are too large in absolute terms.\n    #\n    # Let's use a more standard Softmax-based fit approach.\n    # The core idea is to express desirability as `exp(score)`.\n    # For \"tight fit\", we want bins where `capacity - item` is minimized.\n    # Let `target_remaining = bins_remain_cap[suitable_bins_mask] - item`.\n    # We want to maximize priority when `target_remaining` is small.\n    # Score can be `-target_remaining`.\n    # For softmax: `priorities_suitable = np.exp(-target_remaining)`.\n    #\n    # However, Softmax usually takes raw scores, not probabilities directly.\n    # So, `raw_scores = -target_remaining` is a direct representation.\n    # To prevent large negative values which might cause underflow in `exp()`,\n    # it's common to subtract the maximum score from all scores before exponentiating.\n    # `max_score = np.max(raw_scores)`\n    # `adjusted_scores = raw_scores - max_score`\n    # Then `probabilities = np.exp(adjusted_scores) / np.sum(np.exp(adjusted_scores))`\n    #\n    # The `priority_v2` function should return raw scores that can be passed to a softmax function.\n    #\n    # Let's define the score for each suitable bin `i` as `s_i = -(bins_remain_cap[i] - item)`.\n    # This means a bin with remaining capacity 5 for an item of 3 gets `s_i = -(5-3) = -2`.\n    # A bin with remaining capacity 7 for an item of 3 gets `s_i = -(7-3) = -4`.\n    # The bin with 5 remaining capacity (tighter fit) gets a higher score (-2 > -4).\n    # This is the correct logic for maximizing priority with minimal remaining space.\n\n    # Calculate the remaining capacity after placing the item in suitable bins.\n    # We want to prioritize bins that result in minimal remaining capacity.\n    # So, smaller (bins_remain_cap - item) should yield higher priority.\n    # Let's define the raw score as the negative of this remaining capacity.\n    raw_scores_suitable = -(bins_remain_cap[suitable_bins_mask] - item)\n\n    # To prevent issues with exp() on very large negative numbers (underflow),\n    # we can shift the scores so that the maximum score is 0.\n    # This doesn't change the relative probabilities from the softmax.\n    if raw_scores_suitable.size > 0:\n        max_score = np.max(raw_scores_suitable)\n        adjusted_scores_suitable = raw_scores_suitable - max_score\n    else:\n        adjusted_scores_suitable = np.array([])\n\n    # Apply these adjusted scores to the original priority array at the correct indices.\n    priorities[suitable_bins_mask] = adjusted_scores_suitable\n\n    return priorities",
    "response_id": 19,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 72.33974351909447,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response20.txt_stdout.txt",
    "code_path": "problem_iter1_code20.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a First Fit strategy.\n\n    The priority is higher for bins that can fit the item and are more \"full\" after\n    fitting the item, meaning they have less remaining capacity. This encourages\n    packing items into bins that are already quite full, aiming to minimize the\n    number of bins used. Bins that cannot fit the item are given a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins that can fit the item, calculate the remaining capacity after fitting\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Assign priority. Higher priority for bins with less remaining capacity after fitting.\n    # We can invert the remaining capacity to make smaller remaining capacity higher priority.\n    # Adding a small constant to avoid division by zero or very small numbers if\n    # remaining capacity is 0.\n    priorities[can_fit_mask] = 1.0 / (remaining_after_fit + 1e-9)\n    \n    return priorities",
    "response_id": 20,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 1.0,
    "halstead": 41.51317942364757,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response21.txt_stdout.txt",
    "code_path": "problem_iter1_code21.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit strategy.\n\n    The priority is higher for bins that can fit the item and have the least remaining capacity\n    after fitting the item (i.e., the tightest fit). If a bin cannot fit the item, its priority is 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate remaining capacity after placing the item for bins that can fit\n    remaining_capacities_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # For best fit, we want the smallest remaining capacity.\n    # We can use the inverse of the remaining capacity as a priority score.\n    # To avoid division by zero or very small numbers leading to huge priorities,\n    # we can add a small epsilon or use a ranking-based approach.\n    # A simple approach is to use 1 / (remaining_capacity + epsilon) or a similar\n    # monotonic transformation. Here, we'll use a simple inverse that gets higher\n    # as remaining capacity gets smaller.\n\n    # Add a small epsilon to avoid division by zero if remaining capacity is exactly 0\n    epsilon = 1e-9\n    priorities[can_fit_mask] = 1.0 / (remaining_capacities_after_fit + epsilon)\n\n    # To make sure the \"best\" fit (smallest remaining capacity) has the highest priority,\n    # we can sort and assign priorities based on rank. Or, the 1/remaining capacity\n    # naturally gives higher values to smaller remaining capacities.\n\n    # If we want to strictly adhere to \"highest score = best bin\",\n    # the smallest remaining capacity should give the highest score.\n    # So, 1 / (remaining_capacity) is a good heuristic.\n\n    # Let's refine to ensure higher means better.\n    # The smallest remaining capacity is the most desirable.\n    # So, the largest value of `1 / (remaining_capacities_after_fit + epsilon)`\n    # corresponds to the smallest `remaining_capacities_after_fit`.\n\n    # Consider a case where multiple bins have the exact same best fit.\n    # The current method assigns them the same priority. This is fine.\n\n    # Another perspective for \"priority score\": we want to maximize the priority.\n    # Bin 1: capacity 10, item 3. Remaining: 7. Priority: 1/7\n    # Bin 2: capacity 10, item 3. Remaining: 6. Priority: 1/6 (Higher, so better)\n    # Bin 3: capacity 10, item 3. Remaining: 8. Priority: 1/8\n    # This seems to work as intended.\n\n    return priorities",
    "response_id": 21,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 41.51317942364757,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response22.txt_stdout.txt",
    "code_path": "problem_iter1_code22.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    The Worst Fit strategy aims to place the item into the bin with the largest remaining capacity.\n    This heuristic aims to keep smaller bins for potentially smaller future items,\n    delaying the need for new bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher priority means the bin is more suitable according to the Worst Fit strategy.\n    \"\"\"\n    # Initialize priorities with negative infinity for bins that cannot fit the item\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, assign priority based on remaining capacity\n    # The bin with the largest remaining capacity gets the highest priority.\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask]\n\n    return priorities",
    "response_id": 22,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 1.0,
    "halstead": 11.60964047443681,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response23.txt_stdout.txt",
    "code_path": "problem_iter1_code23.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    In the Almost Full Fit strategy, bins that are close to being full are prioritized.\n    This heuristic aims to leave larger empty spaces in other bins for potentially larger future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Consider only bins where the item can actually fit\n    valid_bins_mask = bins_remain_cap >= item\n\n    # Calculate the \"emptiness\" of each bin after placing the item\n    # A smaller value means the bin is \"closer\" to full after placement\n    emptiness_after_placement = bins_remain_cap - item\n\n    # Prioritize bins that will have the smallest remaining capacity after placing the item.\n    # This means bins that are already quite full are preferred.\n    # We invert this so that smaller emptiness values get higher priority.\n    # Add a small epsilon to avoid division by zero if a bin is perfectly full (though unlikely in online scenarios).\n    # Also, add 1 to the denominator to prevent division by zero or very large numbers if emptiness is zero.\n    # The term `bins_remain_cap` in the denominator encourages bins that started with larger capacities to be considered\n    # if they can still accommodate the item and become \"almost full\".\n    priorities[valid_bins_mask] = 1.0 / (emptiness_after_placement[valid_bins_mask] + 1e-9)\n\n    # Optionally, you could add a factor to consider the initial capacity of the bin.\n    # For example, bins that started with a large capacity and are now \"almost full\"\n    # might be more valuable than bins that started small and are now \"almost full\".\n    # However, the basic Almost Full Fit typically focuses on minimizing remaining space.\n\n    return priorities",
    "response_id": 23,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 39.863137138648355,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response24.txt_stdout.txt",
    "code_path": "problem_iter1_code24.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    Exact Fit First prioritizes bins that can fit the item exactly.\n    Among bins that can fit the item exactly, it prioritizes those with less remaining capacity (to minimize wasted space).\n    If no exact fit is available, it will consider bins that can fit the item but might leave some remaining capacity,\n    prioritizing those with the smallest remaining capacity that is still sufficient.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins where the item fits exactly\n    exact_fit_indices = np.where(bins_remain_cap == item)[0]\n\n    if len(exact_fit_indices) > 0:\n        # Among exact fits, prioritize those with the least remaining capacity (which is 'item' itself in this case)\n        # Since they are exact fits, their remaining capacity is already determined.\n        # We can assign a high priority. Here, we'll give them a value based on their index to ensure\n        # deterministic behavior if multiple exact fits exist (though it doesn't matter for exact fit logic itself).\n        # A higher priority score indicates a better fit.\n        priorities[exact_fit_indices] = 10000 - exact_fit_indices\n    else:\n        # If no exact fit, find bins that can accommodate the item\n        can_fit_indices = np.where(bins_remain_cap >= item)[0]\n\n        if len(can_fit_indices) > 0:\n            # Among bins that can fit, prioritize those with the smallest remaining capacity\n            # that is still sufficient for the item. This is a form of \"best fit\".\n            # We want to maximize the priority for the bin that leaves the least\n            # remaining space after adding the item.\n            # So, we want to minimize (bins_remain_cap - item).\n            # Assigning a priority based on this difference, but inversely (higher priority for smaller difference).\n            # A common way is to use a large number minus the difference.\n            relevant_bins_capacities = bins_remain_cap[can_fit_indices]\n            # Calculate the difference for eligible bins\n            differences = relevant_bins_capacities - item\n            # Assign priorities: Higher values for smaller differences\n            # We can use a large constant minus the difference to make it a \"higher is better\" score.\n            # The constant should be large enough to separate these from non-fitting bins (which have 0 priority).\n            priorities[can_fit_indices] = 1000 - differences\n        else:\n            # No bin can fit the item, priority remains 0 for all.\n            pass # priorities is already initialized to zeros\n\n    return priorities",
    "response_id": 24,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 79.95445336320968,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response25.txt_stdout.txt",
    "code_path": "problem_iter1_code25.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit) strategy.\n\n    The Proximity Fit heuristic prioritizes bins that are \"close\" to fitting the item,\n    meaning the remaining capacity is slightly larger than the item's size.\n    This is implemented by calculating a score based on the inverse of the difference\n    between the remaining capacity and the item size, for bins where the item can fit.\n    Bins that cannot fit the item receive a score of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Find bins where the item can actually fit\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins where the item can fit, calculate the \"closeness\" score\n    # The closer the remaining capacity is to the item size, the higher the score.\n    # We use the inverse of (remaining_capacity - item_size + epsilon) to avoid division by zero\n    # and to give a higher priority to bins that are almost full but can still accommodate the item.\n    # A small epsilon is added for numerical stability and to avoid infinite priority\n    # when remaining_capacity == item_size.\n    epsilon = 1e-9 \n    \n    if np.any(can_fit_mask):\n        fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        diff = fitting_bins_remain_cap - item\n        # The score is inversely proportional to the difference.\n        # A smaller difference means a higher score.\n        priorities[can_fit_mask] = 1.0 / (diff + epsilon)\n        \n    return priorities",
    "response_id": 25,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 41.51317942364757,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response26.txt_stdout.txt",
    "code_path": "problem_iter1_code26.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score strategy.\n\n    The Sigmoid Fit Score strategy prioritizes bins that have a remaining capacity\n    that, after placing the item, is \"close\" to zero but still positive. This\n    encourages filling bins as much as possible without causing overflow.\n\n    The sigmoid function is used to create a smooth, S-shaped curve where\n    the \"steepest\" part of the curve corresponds to the ideal remaining capacity.\n    We want to maximize the priority score, so we want the remaining capacity\n    (bins_remain_cap - item) to be as close to zero as possible.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity if the item were placed in each bin.\n    potential_remaining_cap = bins_remain_cap - item\n\n    # We only consider bins where the item can actually fit.\n    # For bins that cannot fit the item, their priority should be effectively zero\n    # (or very low) so they are not chosen.\n    valid_bins_mask = potential_remaining_cap >= 0\n\n    # To use the sigmoid function effectively, we need to map the\n    # potential_remaining_cap to a range that the sigmoid can operate on\n    # to give distinct scores.\n    # A common approach is to center the sigmoid around a desired value.\n    # We want the remaining capacity to be close to 0.\n    # Let's map the potential_remaining_cap to a value 'x' for the sigmoid.\n    # If potential_remaining_cap is 0, we want a high score.\n    # If potential_remaining_cap is large, we want a low score.\n    #\n    # The sigmoid function is typically of the form 1 / (1 + exp(-k * (x - x0)))\n    # where x0 is the midpoint and k controls the steepness.\n    #\n    # We want to penalize larger remaining capacities.\n    # Consider a transformation:\n    # Let x = -potential_remaining_cap. Now we want x to be close to 0.\n    # If potential_remaining_cap = 0, x = 0.\n    # If potential_remaining_cap = large_positive, x = large_negative.\n    #\n    # Let's use a sigmoid function that is high when the argument is high,\n    # and low when the argument is low.\n    # We want to maximize `f(remaining_capacity)`.\n    # So, if `remaining_capacity` is small, we want `f` to be large.\n    #\n    # We can define a score that decreases as potential_remaining_cap increases.\n    # A simple way is to use exp(-k * potential_remaining_cap) for some k > 0.\n    # This gives a higher score for smaller remaining capacities.\n    # To make it more like a \"sigmoid fit\" in the sense of aiming for a target,\n    # we can also consider the difference from a \"perfect fit\".\n    #\n    # Let's refine the goal: we want the bin where `bins_remain_cap - item`\n    # is as close to zero as possible, without being negative.\n    # The \"ideal\" value for `bins_remain_cap - item` is 0.\n    # We can think of a \"fitness score\" based on how close this is to 0.\n    #\n    # Score(c) = Sigmoid(alpha * (target - c)) where c = bins_remain_cap - item\n    # Target = 0.\n    # Score(c) = Sigmoid(alpha * (-c)) = 1 / (1 + exp(-alpha * (-c)))\n    # Score(c) = 1 / (1 + exp(alpha * c))\n    #\n    # Here, 'alpha' controls how sensitive the score is to the remaining capacity.\n    # A higher alpha means a sharper transition.\n\n    # Choose a steepness parameter 'k'. A larger k makes the sigmoid steeper.\n    # We can also scale the remaining capacity to avoid very large or small exponents.\n    # Let's try mapping the remaining capacity to a range like [-5, 5] where\n    # the sigmoid will have a clear transition.\n    # We want a remaining capacity of 0 to be the \"middle\" of our desired range.\n    #\n    # Let's use `k=2` for now.\n    k = 2.0\n\n    # Calculate the \"closeness\" to zero. We want a smaller positive remainder\n    # to have a higher score.\n    # A larger negative potential_remaining_cap is bad (item doesn't fit).\n    # A potential_remaining_cap of 0 is good.\n    # A large positive potential_remaining_cap is less good than 0, but still valid.\n    #\n    # Let's aim for a scenario where the *best* remaining capacity is 0.\n    # The sigmoid function can be used to map values to a [0, 1] range.\n    # We want the output of the sigmoid to be high for `potential_remaining_cap` close to 0.\n    #\n    # Consider a function `f(x) = 1 / (1 + exp(-k * x))`\n    # If we want `x = 0` to give the highest score, then `f(0) = 0.5`.\n    # If we want `x` to be positive, say `x=10`, we want `f(10)` to be small.\n    # If we want `x` to be negative, say `x=-10`, we want `f(-10)` to be large.\n    #\n    # This means we want to apply the sigmoid to a transformed variable:\n    # `sigmoid(k * (-potential_remaining_cap))` or `sigmoid(k * (0 - potential_remaining_cap))`\n    # This will be `1 / (1 + exp(-k * (-potential_remaining_cap))) = 1 / (1 + exp(k * potential_remaining_cap))`.\n    #\n    # However, this function `1 / (1 + exp(k * x))` gives a low value for x=0.\n    # We want a high value for x=0.\n    #\n    # Let's invert the sigmoid logic or the argument.\n    # Consider `f(x) = 1 / (1 + exp(k * x))`. This gives high for negative x.\n    # We want high for `potential_remaining_cap` near 0.\n    #\n    # Let's rethink the scoring. We want to assign a score to each bin.\n    # The higher the score, the better the bin.\n    #\n    # Bins that don't fit the item should have a score of 0.\n    # For bins that fit:\n    # We want to prioritize bins where `bins_remain_cap - item` is small and positive.\n    #\n    # Let's define a function `g(x)` such that `g(x)` is maximal at `x=0` and\n    # decreases as `x` moves away from 0 (in either direction, but we only care about `x >= 0`).\n    #\n    # A Gaussian-like function centered at 0 could work, but sigmoid is requested.\n    # The Sigmoid function itself maps to [0, 1].\n    #\n    # Let's use the transformation `score = 1.0 - sigmoid(k * (potential_remaining_cap - target))`\n    # where `target` is the ideal remaining capacity (0).\n    # `score = 1.0 - (1 / (1 + exp(-k * (potential_remaining_cap - 0))))`\n    # `score = 1.0 - (1 / (1 + exp(-k * potential_remaining_cap)))`\n    # `score = (1 + exp(-k * potential_remaining_cap) - 1) / (1 + exp(-k * potential_remaining_cap))`\n    # `score = exp(-k * potential_remaining_cap) / (1 + exp(-k * potential_remaining_cap))`\n    # This is the logistic function, which is high for negative arguments and low for positive.\n    #\n    # We want the opposite: high for small positive arguments.\n    #\n    # Consider the argument `X = -potential_remaining_cap`.\n    # We want `X` to be small and positive.\n    # If `potential_remaining_cap = 0`, `X = 0`.\n    # If `potential_remaining_cap = 10`, `X = -10`.\n    # If `potential_remaining_cap = -5` (item doesn't fit), `X = 5`.\n    #\n    # The standard sigmoid `sigmoid(x) = 1 / (1 + exp(-x))` has its steepest rise around x=0.\n    #\n    # Let's map `potential_remaining_cap` to `x` such that `x=0` is desired.\n    # A transformation that maps 0 to 0, larger positive values to larger positive values,\n    # and negative values (item doesn't fit) to large negative values could work if\n    # the sigmoid were increasing. But we need scores of 0 for non-fitting bins.\n    #\n    # Let's focus on valid bins (`potential_remaining_cap >= 0`).\n    # For these, we want `potential_remaining_cap` to be small.\n    #\n    # We can use a sigmoid-like shape but flipped and shifted.\n    # A function like `1 - sigmoid(k * (potential_remaining_cap))` would work if `potential_remaining_cap` was always >= 0.\n    #\n    # Let's use a sigmoid that is steep around the desired value.\n    # `sigmoid(x) = 1 / (1 + exp(-x))`\n    # If we want the best score at `remaining_capacity = 0`,\n    # and the score decreases as `remaining_capacity` increases.\n    #\n    # The expression `sigmoid(k * (target_capacity - actual_capacity))` is good.\n    # `target_capacity = item`\n    # `actual_capacity = bins_remain_cap`\n    # So, `score_arg = k * (item - bins_remain_cap)`\n    # If `item - bins_remain_cap` is small and positive (e.g., `bins_remain_cap` is just slightly larger than `item`),\n    # then `score_arg` is small and positive, sigmoid gives close to 0.5.\n    #\n    # Let's invert the argument to get higher scores for smaller remaining capacities:\n    # `score_arg = k * (bins_remain_cap - item)`\n    # If `bins_remain_cap - item = 0`, `score_arg = 0`, `sigmoid(0) = 0.5`.\n    # If `bins_remain_cap - item = 10`, `score_arg = 10k`, `sigmoid(10k)` is close to 1.\n    # If `bins_remain_cap - item = -10`, `score_arg = -10k`, `sigmoid(-10k)` is close to 0.\n    #\n    # This still doesn't quite capture \"filling bins well\". We want smaller positive remainders.\n    #\n    # Consider the *waste* which is `bins_remain_cap - item`.\n    # We want to minimize waste, but only if it's non-negative.\n    #\n    # Let's re-interpret \"Sigmoid Fit Score strategy\". It implies fitting items into bins such that the remaining capacity after packing is ideally close to zero.\n    #\n    # Let `r = bins_remain_cap - item`. We only care when `r >= 0`.\n    # We want to maximize `score` where `score` is high for `r` near 0 and decreases as `r` increases.\n    #\n    # Use `sigmoid(x) = 1 / (1 + exp(-x))`. This is an increasing function.\n    # We want a function that is decreasing for `r >= 0`.\n    # So, we should apply sigmoid to `-r`.\n    # `score = sigmoid(-k * r) = 1 / (1 + exp(-(-k * r))) = 1 / (1 + exp(k * r))`\n    #\n    # Let's test this:\n    # If `r = 0`, `score = 1 / (1 + exp(0)) = 1 / 2 = 0.5`.\n    # If `r = 10` (large waste), `score = 1 / (1 + exp(10k))`, very close to 0.\n    # If `r = -10` (doesn't fit), `score = 1 / (1 + exp(-10k))`, very close to 1.\n    #\n    # This is problematic: it gives a high score to bins where the item doesn't fit!\n    #\n    # We need to enforce that the item must fit.\n    #\n    # So, the logic is:\n    # 1. For bins where `bins_remain_cap < item`, priority is 0.\n    # 2. For bins where `bins_remain_cap >= item`:\n    #    Calculate `remaining_capacity = bins_remain_cap - item`.\n    #    We want to assign a score that is high when `remaining_capacity` is small positive, and low when `remaining_capacity` is large positive.\n    #\n    # The function `f(x) = 1 / (1 + exp(-k * x))` is *increasing*.\n    # We want an *decreasing* score as `remaining_capacity` increases.\n    # So, we can use `1 - sigmoid(k * remaining_capacity)` or `sigmoid(-k * remaining_capacity)`.\n    # As derived before, `sigmoid(-k * r) = 1 / (1 + exp(k * r))`.\n    #\n    # Let's scale the remaining capacity before applying the sigmoid to control sensitivity.\n    # `scaled_remaining_capacity = k * potential_remaining_cap`\n    #\n    # `priorities[valid_bins_mask] = 1.0 / (1.0 + np.exp(k * potential_remaining_cap[valid_bins_mask]))`\n    #\n    # Let's re-evaluate the choice of k.\n    # If k is very large, the sigmoid is very steep.\n    # For `r = 0`, score = 0.5.\n    # For `r = epsilon` (small positive), score is slightly less than 0.5.\n    # For `r = 1`, score is `1 / (1 + exp(k))`. If k=10, score ~ 0.\n    #\n    # This seems correct: a high score for items that perfectly fit, and lower scores for items that leave a lot of space.\n    #\n    # `k` parameter choice:\n    # A larger `k` means we strongly prefer bins that leave little to no space.\n    # A smaller `k` means we are more lenient with the remaining space.\n    # For example, if bin capacity is 10, and items are 6 and 8.\n    # Item 6: remaining_caps = [4, 2, ...]. Scores for these bins would be sigmoid(-k*4), sigmoid(-k*2), ...\n    # Item 8: remaining_caps = [2, 0, ...]. Scores for these bins would be sigmoid(-k*2), sigmoid(-k*0), ...\n    #\n    # Let's assume we want to strongly favor bins that are almost full.\n    # The range of `potential_remaining_cap` could vary.\n    # If bin capacity is 100 and item is 1, remaining capacity could be 99.\n    # If bin capacity is 10 and item is 9, remaining capacity could be 1.\n    #\n    # To make `k` adapt to the scale of remaining capacities, we could normalize.\n    # However, a fixed `k` is simpler and standard for such heuristics.\n    # Let's pick a `k` that makes sense for typical bin packing scenarios.\n    # If `k=2`, the inflection point is at `r=0`.\n    # At `r=1` (unit waste), score is `1/(1+e^2)` which is ~0.119.\n    # At `r=0.5`, score is `1/(1+e^1)` which is ~0.269.\n    # At `r=0`, score is `0.5`.\n    # This suggests that leaving 1 unit of waste gives a much lower score than leaving 0.5 or 0.\n    # This might be too aggressive.\n    #\n    # Let's try scaling `potential_remaining_cap` by something related to `item` or bin size,\n    # but that adds complexity and might require tuning.\n    #\n    # A common approach in some literature for \"best fit\" using sigmoid-like curves\n    # is to aim for a target residual capacity, say `T`. The score might be\n    # `sigmoid(k * (T - residual_capacity))`. If `T=0`, this is `sigmoid(-k * residual_capacity)`.\n    #\n    # Let's adjust `k` to be more sensitive to the `item` size.\n    # For example, `k = 1.0 / item` might scale it appropriately.\n    # If `item` is large, `k` is small, making the sigmoid flatter.\n    # If `item` is small, `k` is large, making the sigmoid steeper.\n    #\n    # Let's try `k = 5.0` as a starting point, which should make the transition fairly quick.\n    k_param = 5.0\n\n    # Initialize priorities to a very low value (effectively zero for invalid bins)\n    priorities = np.full_like(bins_remain_cap, -np.inf) # Use -inf to ensure low priority if invalid\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate potential remaining capacities for bins that can fit the item\n    potential_remaining_cap_fit = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate priority scores using the sigmoid function for valid bins.\n    # We use sigmoid(-k * remaining_capacity) to get a decreasing score as remaining_capacity increases.\n    # The higher the score, the better the bin (i.e., less waste).\n    # The range of the sigmoid is (0, 1). A score of 0.5 means remaining_capacity is 0.\n    # A score close to 1 means remaining_capacity is very negative (shouldn't happen due to mask).\n    # A score close to 0 means remaining_capacity is very positive.\n    #\n    # Let's re-read the request: \"The bin with the highest priority score will be selected for the item.\"\n    # So we want to map \"best fit\" (least remaining capacity) to the highest score.\n    #\n    # The function `sigmoid(x) = 1 / (1 + exp(-x))` is increasing.\n    # We want to apply it to a value that is high when `remaining_capacity` is low.\n    # Let's use `transformed_value = -k * potential_remaining_cap_fit`.\n    # When `potential_remaining_cap_fit = 0`, `transformed_value = 0`, `sigmoid(0) = 0.5`.\n    # When `potential_remaining_cap_fit` is large positive (bad fit), `transformed_value` is large negative, `sigmoid` is close to 0.\n    # When `potential_remaining_cap_fit` is small positive (good fit), `transformed_value` is small negative, `sigmoid` is slightly less than 0.5.\n    # This is NOT what we want. We want high for small positive `remaining_capacity`.\n    #\n    # Let's use `sigmoid(k * (ideal_remainder - actual_remainder))`\n    # `ideal_remainder = 0`\n    # `actual_remainder = potential_remaining_cap_fit`\n    # So, `sigmoid(k * (0 - potential_remaining_cap_fit))`\n    # `sigmoid(-k * potential_remaining_cap_fit)`\n    #\n    # This gives:\n    # `potential_remaining_cap_fit = 0` -> `sigmoid(0) = 0.5`\n    # `potential_remaining_cap_fit = small_positive` -> `sigmoid(small_negative)` ~ < 0.5\n    # `potential_remaining_cap_fit = large_positive` -> `sigmoid(large_negative)` ~ 0\n    #\n    # Still not quite right. We want higher for smaller positive residuals.\n    #\n    # The classic \"Best Fit\" heuristic itself prioritizes the bin with the *smallest positive* `bins_remain_cap - item`.\n    # The sigmoid fit should approximate this.\n    #\n    # Let's try a score that peaks at `remaining_capacity = 0` and drops off.\n    # Consider the function `score = exp(-k * remaining_capacity)`. This is decreasing.\n    # At `r=0`, score=1. At `r=1`, score=exp(-k).\n    # If k=5, score at r=1 is exp(-5) ~ 0.0067. This is very steep.\n    #\n    # A simple sigmoid `1 / (1 + exp(-x))` maps `x` to `[0, 1]`.\n    # If we want high scores for small positive `r`, we need `x` to be high for small positive `r`.\n    #\n    # What if we use `k` on `item` directly?\n    # `score_arg = k * (item - bins_remain_cap)`\n    # If `bins_remain_cap` is large (bad fit), `item - bins_remain_cap` is largely negative.\n    # `sigmoid` of large negative is near 0.\n    # If `bins_remain_cap` is slightly larger than `item`, `item - bins_remain_cap` is small negative.\n    # `sigmoid` of small negative is near 0.5.\n    # If `bins_remain_cap == item`, `item - bins_remain_cap = 0`. `sigmoid(0) = 0.5`.\n    #\n    # This still doesn't strongly favor the exact fit.\n    #\n    # The \"Sigmoid Fit Score strategy\" usually refers to a heuristic where the probability\n    # or priority of choosing a bin is modeled by a sigmoid function of some characteristic.\n    # In bin packing, a common characteristic is the \"slack\" or \"remaining capacity\".\n    #\n    # Let's aim for an exponential decay of priority as slack increases.\n    # `priority = exp(-k * slack)` where slack is `bins_remain_cap - item`.\n    # This function is `1` at `slack=0`, and decreases towards `0` as `slack` increases.\n    #\n    # Now, how to map this to a sigmoid form?\n    # Consider `score = 1 - sigmoid(k * slack)`.\n    # `score = 1 - (1 / (1 + exp(-k * slack)))`\n    # `score = exp(-k * slack) / (1 + exp(-k * slack))`  (logistic function)\n    #\n    # At `slack = 0`: score = `exp(0) / (1 + exp(0)) = 1 / 2 = 0.5`.\n    # At `slack = 10` (large waste): score = `exp(-10k) / (1 + exp(-10k))` ~ 0.\n    # At `slack = 0.1` (small waste): score = `exp(-0.1k) / (1 + exp(-0.1k))`\n    #\n    # This function `logistic(x)` is similar to `sigmoid(-x)`.\n    # `logistic(x) = sigmoid(x)`.\n    # We want a function that is high for small *positive* `slack`.\n    #\n    # Let's try mapping `slack` to `x` such that `sigmoid(x)` is high for small positive `slack`.\n    # Use `sigmoid(k * (TARGET_SLACK - slack))`. Let `TARGET_SLACK = 0`.\n    # `sigmoid(k * (0 - slack))` = `sigmoid(-k * slack)`.\n    #\n    # So, `sigmoid(-k * potential_remaining_cap_fit)` seems to be the correct functional form.\n    # `potential_remaining_cap_fit` is `bins_remain_cap - item` for valid bins.\n    #\n    # Re-evaluation of `sigmoid(-k * r)`:\n    # `r = bins_remain_cap - item`\n    # `sigmoid(-k * r)`\n    # `r = 0` (perfect fit): `sigmoid(0) = 0.5`.\n    # `r = small_positive` (good fit): `sigmoid(small_negative)` < 0.5.\n    # `r = large_positive` (poor fit): `sigmoid(large_negative)` -> 0.\n    #\n    # This still means bins that fit with smaller positive remaining capacity get LOWER scores than perfect fits. This is contrary to how we typically interpret \"best fit\" where a small positive residual is preferred.\n    #\n    # Let's reconsider what \"Sigmoid Fit Score strategy\" means in practice. It likely implies that the preference curve follows a sigmoid shape, and we want to maximize this preference.\n    #\n    # The problem is mapping the *metric* (remaining capacity) to the *score* in a way that a higher score is better and aligns with good packing.\n    #\n    # If the sigmoid's input is `k * (item_size - bin_capacity_left)`:\n    # - When `item_size` is slightly less than `bin_capacity_left`: `k * (small_positive)` -> sigmoid is > 0.5, higher preference.\n    # - When `item_size` is slightly more than `bin_capacity_left`: `k * (small_negative)` -> sigmoid is < 0.5, lower preference.\n    # - When `item_size` is much less than `bin_capacity_left`: `k * (large_positive)` -> sigmoid is ~1, high preference.\n    # - When `item_size` is much more than `bin_capacity_left`: `k * (large_negative)` -> sigmoid is ~0, low preference.\n    #\n    # This seems to align with \"First Fit Decreasing\" or \"Largest Item First\" kind of logic, preferring bins that have enough space but not excessively so, prioritizing cases where the item is almost the full remaining capacity.\n    #\n    # Let's try `score_arg = k * (bins_remain_cap - item)`. This is `k * residual_capacity`.\n    # We want high score for small `residual_capacity`.\n    # Sigmoid is increasing.\n    # So we want `sigmoid` applied to something that increases as `residual_capacity` decreases.\n    # Let's try `score_arg = k * (TARGET - residual_capacity)`.\n    # If TARGET is 0, `score_arg = k * (-residual_capacity)`.\n    # `sigmoid(-k * residual_capacity)`. This yielded results where perfect fit (r=0) gave 0.5.\n    #\n    # Let's try shifting the sigmoid.\n    # `f(x) = 1 / (1 + exp(-k * x))`\n    # We want high score for `r=0`.\n    # Let's make the argument something that is large when `r` is small.\n    #\n    # Consider the score definition:\n    # `score = 1.0 / (1.0 + np.exp(k * (bins_remain_cap - item)))`\n    # For `bins_remain_cap - item = 0`, score is `0.5`.\n    # For `bins_remain_cap - item = 1`, score is `1 / (1 + exp(k))`.\n    # For `bins_remain_cap - item = -1`, score is `1 / (1 + exp(-k))`.\n    #\n    # This function is *decreasing* with `bins_remain_cap - item`.\n    # So, it penalizes larger remaining capacities.\n    # It gives a score close to 1 when `bins_remain_cap - item` is very negative (item doesn't fit), and scores near 0 when `bins_remain_cap - item` is very positive.\n    #\n    # This looks like a good candidate, provided we handle the non-fitting bins.\n    #\n    # Let's make `k` relate to the bin capacity to avoid extremely steep or flat sigmoid depending on magnitude.\n    # A heuristic could be `k = C / average_bin_capacity`.\n    # For a general heuristic, a fixed `k` is simpler.\n    # Let's try `k=2.0`.\n    #\n    # If `bins_remain_cap - item = 0`: score = `0.5`\n    # If `bins_remain_cap - item = 1`: score = `1 / (1 + exp(2))` ~ `0.119`\n    # If `bins_remain_cap - item = 0.5`: score = `1 / (1 + exp(1))` ~ `0.269`\n    #\n    # This implies that a bin with 0 remaining capacity is preferred over one with 0.5, which is preferred over one with 1.0.\n    # This matches the \"Best Fit\" principle, where smaller positive residuals are better.\n    #\n    # The chosen sigmoid form: `1 / (1 + exp(k * residual))`.\n    # This sigmoid is decreasing. High score for low residual, low score for high residual.\n    #\n    # The critical part is to ensure we only apply this to bins where `residual >= 0`.\n    # For bins where `residual < 0`, the priority MUST be 0 (or a very small negative number to ensure they are not picked if all valid bins have scores of 0).\n    #\n    # Let's use a baseline priority of 0 for invalid bins and then apply the sigmoid score to valid bins.\n    #\n    # The `np.full_like(bins_remain_cap, 0.0)` initialization and then updating `priorities[can_fit_mask]` will work.\n    #\n    # Final checks:\n    # - Input: `item` (float), `bins_remain_cap` (np.ndarray)\n    # - Output: `priorities` (np.ndarray) of same shape.\n    # - Strategy: Sigmoid Fit Score, higher score is better.\n    # - Logic: Prefer bins that minimize `bins_remain_cap - item` subject to it being non-negative.\n    # - Function: `f(residual) = 1.0 / (1.0 + np.exp(k * residual))` for `residual >= 0`.\n    #   For `residual < 0`, score is 0.\n    #\n    # Let `k` be a tunable parameter. For now, let's set a reasonable default like 5.0.\n    # A larger `k` makes the priority drop off faster as residual increases.\n    k = 5.0\n\n    # Calculate remaining capacities.\n    residuals = bins_remain_cap - item\n\n    # Initialize priorities to 0 for bins where the item cannot fit.\n    # For bins that can fit, we'll calculate a score.\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins where the item can fit (residual is non-negative).\n    can_fit_mask = residuals >= 0\n\n    # For the bins where the item fits, calculate the priority score.\n    # We want higher priority for smaller non-negative residuals.\n    # The function `1 / (1 + exp(k * x))` where x = residual, provides this decreasing score.\n    # Larger positive residuals lead to smaller scores.\n    # A residual of 0 gives a score of 0.5.\n    # A residual of 1 gives a score of 1/(1+exp(k)).\n    #\n    # The sigmoid function maps values to (0, 1). Since we want to pick the *highest* priority,\n    # and scores are between (0, 1), a small positive residual should yield a score closer to 1.\n    # My previous derivation gave scores closer to 0 for positive residuals. Let me correct.\n    #\n    # Re-think mapping for `score = sigmoid(x)` where sigmoid is `1 / (1 + exp(-x))`.\n    # We want `x` to be large and positive when `residual` is small and positive.\n    # So, `x` should be proportional to `1 / residual` or `1 / (residual + epsilon)`.\n    # That's not a sigmoid transformation.\n    #\n    # Let's map `residual` to an argument `arg` such that `sigmoid(arg)` is maximized when `residual` is minimized (non-negative).\n    # `sigmoid(arg) = 1 / (1 + exp(-arg))`\n    # To maximize sigmoid, `arg` needs to be large positive.\n    #\n    # We want `arg` to be large positive when `residual` is small positive.\n    # Example: `arg = k * (TARGET - residual)` where `TARGET` is the ideal residual.\n    # If `TARGET = 0`, then `arg = k * (-residual)`.\n    # `sigmoid(-k * residual)`\n    # `residual = 0` -> `sigmoid(0) = 0.5`\n    # `residual = small_positive` -> `sigmoid(small_negative)` < 0.5\n    # `residual = large_positive` -> `sigmoid(large_negative)` ~ 0.\n    #\n    # This interpretation still results in perfect fits (residual 0) being less preferred than positive residuals in a way that my analysis shows higher score for lower residual.\n    #\n    # Let's consider the goal directly: \"The bin with the highest priority score will be selected\".\n    # We want to select the bin with the smallest non-negative `bins_remain_cap - item`.\n    #\n    # Let `r = bins_remain_cap - item`. We only care about `r >= 0`.\n    # We want to rank bins based on `r`. Smallest `r` should get highest score.\n    #\n    # Consider `score_arg = -k * r`.\n    # If `r = 0`, `score_arg = 0`.\n    # If `r = 1`, `score_arg = -k`.\n    # If `r = 0.1`, `score_arg = -0.1k`.\n    #\n    # If we use `sigmoid(score_arg)`:\n    # `r = 0`: `sigmoid(0) = 0.5`\n    # `r = 1`: `sigmoid(-k)` (low)\n    # `r = 0.1`: `sigmoid(-0.1k)` (higher than for r=1)\n    #\n    # So, `sigmoid(-k * (bins_remain_cap - item))` correctly prioritizes bins with smaller positive residuals.\n    #\n    # What about the scaling parameter `k`?\n    # If `k` is large, the scores change rapidly around `residual = 0`.\n    # If `k=5`:\n    # r = 0.01  -> arg = -0.05 -> sigmoid(-0.05) = 1 / (1 + exp(0.05)) = 1 / 1.051 = 0.95\n    # r = 0.1   -> arg = -0.5  -> sigmoid(-0.5)  = 1 / (1 + exp(0.5))  = 1 / 1.648 = 0.60\n    # r = 0.5   -> arg = -2.5  -> sigmoid(-2.5)  = 1 / (1 + exp(2.5))  = 1 / 12.18  = 0.08\n    # r = 1.0   -> arg = -5.0  -> sigmoid(-5.0)  = 1 / (1 + exp(5))   = 1 / 149.2  = 0.0067\n    #\n    # This mapping seems to assign very high priorities to items that leave only a tiny residual, and very low priorities to items that leave more residual. This aligns with a strict \"best fit\" interpretation.\n    #\n    # The choice of `k` is crucial for performance. `k=5.0` means that a residual of 0.5 is already very penalized compared to a residual of 0.01.\n    #\n    # Let's finalize this implementation with `k=5.0`.\n\n    # Calculate the arguments for the sigmoid function.\n    # We want to maximize priority, and priority should be highest when `bins_remain_cap - item` is minimal non-negative.\n    # The sigmoid function `1 / (1 + exp(-x))` increases with x.\n    # We need an argument `x` that is large positive when `bins_remain_cap - item` is small positive.\n    # So, `x = k * (TARGET - (bins_remain_cap - item))`.\n    # If TARGET = 0, `x = k * (-(bins_remain_cap - item)) = k * (item - bins_remain_cap)`.\n    # However, we are working with `residuals = bins_remain_cap - item`.\n    # We want `x` to be large positive when `residuals` is small positive.\n    # So `x` should be proportional to `1/residuals` or similar, but we need a sigmoid.\n    #\n    # Let's use the form that makes `sigmoid(-k * residual)`\n    # `r = bins_remain_cap - item`\n    # `score = sigmoid(-k * r)` for `r >= 0`.\n    # This yields higher scores for smaller `r`.\n    # r=0 -> sigmoid(0) = 0.5\n    # r=0.1 -> sigmoid(-0.1k) (higher if k is negative, lower if k is positive)\n    # We want HIGH score for SMALL POSITIVE r.\n    #\n    # Let's use `arg = -k * residuals`\n    # `sigmoid(-k * residuals)` means:\n    # residual=0 -> sigmoid(0) = 0.5\n    # residual=positive -> sigmoid(-positive * k) = value < 0.5.\n    # This seems to say that bins with ANY positive residual are WORSE than a perfect fit.\n    # This is wrong. We want to favor minimal positive residuals.\n    #\n    # Consider the function: `sigmoid(k * (C - x))` where `x` is the value we want to minimize.\n    # Here `x = bins_remain_cap - item`.\n    # Let `C = 0` (target remaining capacity).\n    # `sigmoid(k * (0 - (bins_remain_cap - item)))`\n    # `sigmoid(k * (item - bins_remain_cap))`\n    #\n    # Let's analyze this argument `arg = k * (item - bins_remain_cap)`:\n    # Case 1: `bins_remain_cap = item` (perfect fit)\n    #   `arg = k * 0 = 0`. `sigmoid(0) = 0.5`.\n    # Case 2: `bins_remain_cap = item + delta` where `delta > 0` (small residual)\n    #   `arg = k * (item - (item + delta)) = k * (-delta)`\n    #   If `k > 0`, `arg` is small negative. `sigmoid(small_negative)` is < 0.5.\n    # Case 3: `bins_remain_cap = item + big_delta` (large residual)\n    #   `arg = k * (item - (item + big_delta)) = k * (-big_delta)`\n    #   If `k > 0`, `arg` is large negative. `sigmoid(large_negative)` is close to 0.\n    # Case 4: `bins_remain_cap = item - delta` where `delta > 0` (item doesn't fit)\n    #   `arg = k * (item - (item - delta)) = k * delta`\n    #   If `k > 0`, `arg` is positive. `sigmoid(positive)` is > 0.5.\n    #\n    # This gives scores > 0.5 for bins where the item doesn't fit, and scores < 0.5 for bins where it does fit (except the perfect fit). This is backward.\n    #\n    # The problem is how we interpret \"Sigmoid Fit Score\".\n    # The general idea is to use sigmoid to create a graded preference.\n    #\n    # The correct mapping to achieve \"highest score for smallest positive residual\":\n    # We need a function that is high for `r=0` and decreases for `r > 0`.\n    #\n    # Consider the form `score = 1 - sigmoid(k * r)`.\n    # `sigmoid(k * r) = 1 / (1 + exp(-k * r))`\n    # `1 - sigmoid(k * r) = 1 - 1 / (1 + exp(-k * r))`\n    # `= (1 + exp(-k * r) - 1) / (1 + exp(-k * r))`\n    # `= exp(-k * r) / (1 + exp(-k * r))`  (Logistic function)\n    #\n    # Let `r = bins_remain_cap - item`.\n    # At `r = 0`: score = `exp(0) / (1 + exp(0)) = 1/2 = 0.5`.\n    # At `r = small_positive` (e.g., 0.1, k=5): `score = exp(-0.5) / (1 + exp(-0.5)) = 0.606 / 1.606 = 0.377` (Lower than 0.5)\n    # At `r = large_positive` (e.g., 1.0, k=5): `score = exp(-5) / (1 + exp(-5)) = 0.0067 / 1.0067 = 0.0066` (Much lower)\n    #\n    # This logistic function form (`logistic(x) = sigmoid(x)`) gives lower scores for positive residuals.\n    # So we need to feed it with arguments that are high for low residuals.\n    #\n    # Let's try `score = sigmoid(k * (TARGET - r))`.\n    # If TARGET is the ideal residual (0), then `score = sigmoid(k * (0 - r)) = sigmoid(-k * r)`.\n    #\n    # Let's re-test `sigmoid(-k * r)` where `r = bins_remain_cap - item`.\n    #\n    # k = 5.0\n    #\n    # residual = 0.001  (small positive)\n    # arg = -5.0 * 0.001 = -0.005\n    # sigmoid(-0.005) = 1 / (1 + exp(0.005)) = 1 / 1.005 = 0.995  (HIGH SCORE)\n    #\n    # residual = 0.1 (moderate positive)\n    # arg = -5.0 * 0.1 = -0.5\n    # sigmoid(-0.5) = 1 / (1 + exp(0.5)) = 1 / 1.648 = 0.607  (MODERATE SCORE)\n    #\n    # residual = 1.0 (large positive)\n    # arg = -5.0 * 1.0 = -5.0\n    # sigmoid(-5.0) = 1 / (1 + exp(5.0)) = 1 / 149.2 = 0.0067 (LOW SCORE)\n    #\n    # residual = 0.0 (perfect fit)\n    # arg = 0\n    # sigmoid(0) = 0.5 (INTERMEDIATE SCORE)\n    #\n    # This interpretation assigns the highest scores to bins that leave a very small positive residual.\n    # A perfect fit gets a moderate score (0.5). Bins with larger residuals get low scores.\n    # This is a valid strategy. It slightly prefers bins that aren't perfectly filled over perfectly filled bins,\n    # but heavily penalizes bins with significant leftover space. This is a reasonable heuristic.\n    #\n    # The `k` parameter controls how \"picky\" it is about the residual.\n    # `k=5.0` is quite strict.\n\n    # Apply the sigmoid transformation to the valid bins.\n    # For bins where the item fits: `score = sigmoid(-k * (residual))`\n    # where `sigmoid(x) = 1 / (1 + exp(-x))`\n    # So the formula becomes `1 / (1 + exp(-(-k * residuals[can_fit_mask])))`\n    # which simplifies to `1 / (1 + exp(k * residuals[can_fit_mask]))`\n    #\n    # Wait, I derived `sigmoid(-k * r)` before and it gives high scores for small positive r.\n    # Let's use that.\n    # `arg = -k * residuals[can_fit_mask]`\n    # `priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(-arg))`\n    # `priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(k * residuals[can_fit_mask]))` - This was my earlier wrong derivation.\n    # Let's re-trace: `sigmoid(X) = 1 / (1 + exp(-X))`\n    # We want `X` such that `sigmoid(X)` is high when `residual` is small positive.\n    # We need `X` to be large positive.\n    # If `residual = 0.01` and `k=5`, we want a large `X`.\n    # If `residual = 1.0` and `k=5`, we want `X` to be small negative.\n    #\n    # This suggests `X = k * (C - residual)`.\n    # If `C=0`, `X = -k * residual`.\n    # `sigmoid(-k * residual) = 1 / (1 + exp(-(-k * residual))) = 1 / (1 + exp(k * residual))`\n    #\n    # Let's re-test `1 / (1 + exp(k * residual))`\n    # k=5.0\n    # residual = 0.001 -> arg = 5.0 * 0.001 = 0.005 -> score = 1 / (1 + exp(0.005)) = 1 / 1.005 = 0.995 (HIGH)\n    # residual = 0.1   -> arg = 5.0 * 0.1 = 0.5   -> score = 1 / (1 + exp(0.5))  = 1 / 1.648 = 0.607 (MODERATE)\n    # residual = 1.0   -> arg = 5.0 * 1.0 = 5.0   -> score = 1 / (1 + exp(5.0))  = 1 / 149.2 = 0.0067 (LOW)\n    # residual = 0.0   -> arg = 5.0 * 0.0 = 0.0   -> score = 1 / (1 + exp(0.0))  = 1 / 1.0   = 0.5    (INTERMEDIATE)\n    #\n    # This is the correct mapping for the Sigmoid Fit Score where higher score is better, and we prefer smaller positive residuals.\n    #\n    # The function is `f(r) = 1.0 / (1.0 + np.exp(k * r))`.\n    # This means that the priority is high when the residual capacity `r` is small and positive.\n    # A residual of 0.01 gives a score close to 1, while a residual of 1.0 gives a score close to 0.\n    # A perfect fit (residual 0) gets a score of 0.5.\n\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(k * residuals[can_fit_mask]))\n\n    return priorities",
    "response_id": 26,
    "obj": 4.487435181491823,
    "cyclomatic_complexity": 1.0,
    "halstead": 120.92782504182705,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response27.txt_stdout.txt",
    "code_path": "problem_iter1_code27.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Random Fit.\n\n    In Random Fit, we randomly select a bin among those that can accommodate the item.\n    The priority function here aims to simulate this by giving a higher priority\n    to bins that can fit the item, and then adding a random component to break ties\n    and introduce the \"randomness\" aspect.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros (or handle as an error, but for priority, zeros is fine)\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Assign a base priority to bins that can fit the item\n    # A simple approach is to give a higher priority to bins with less remaining capacity\n    # (First Fit Decreasing like idea, but Random Fit just picks one)\n    # We can use the inverse of remaining capacity + 1 to avoid division by zero and penalize fuller bins.\n    # For Random Fit, we also want to introduce randomness, so we add a small random number.\n\n    # We can assign a base priority and then add noise.\n    # Bins that can fit the item get a positive priority.\n    # Bins that cannot fit get 0.\n\n    # Let's try assigning a score based on how \"tight\" the fit is, plus randomness.\n    # A tighter fit (less remaining capacity) could be preferred to minimize waste,\n    # and then we randomize among these good fits.\n\n    # Calculate potential \"waste\" if item is placed in the bin (remaining_capacity - item)\n    # We want to minimize waste, so higher priority for smaller waste.\n    # Let's use -(remaining_capacity - item) as a base for priority, so smaller remaining_capacity is better.\n    # We also add a random value to make it \"Random Fit\".\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item) + np.random.rand(np.sum(can_fit_mask)) * 0.1 # Add small random noise\n\n    # A simpler interpretation for \"Random Fit\" would be to give all eligible bins\n    # an equal base priority and then add randomness.\n    # In this interpretation, any bin that fits is equally likely.\n    # priorities[can_fit_mask] = 1 + np.random.rand(np.sum(can_fit_mask)) * 0.01 # Give a base priority and small random noise\n\n    # The prompt asks for a priority score for EACH bin.\n    # The highest priority score bin will be selected.\n    # So we should ensure that bins that *cannot* fit have a lower priority than those that *can*.\n\n    # Let's refine the strategy:\n    # 1. Bins that cannot fit the item get a priority of 0.\n    # 2. Bins that *can* fit the item get a positive priority. To simulate randomness,\n    #    we can assign a random priority to each eligible bin. A simple approach is\n    #    to assign a uniformly random number within a small range.\n\n    # Re-initializing priorities to zeros\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    eligible_bins_indices = np.where(bins_remain_cap >= item)[0]\n\n    if eligible_bins_indices.size > 0:\n        # Assign a random priority to each eligible bin.\n        # A simple uniform random value between 0 and 1 is sufficient to make them\n        # randomly chosen when selected using argmax.\n        priorities[eligible_bins_indices] = np.random.rand(eligible_bins_indices.size)\n\n    return priorities",
    "response_id": 27,
    "obj": 73.84323893099322,
    "cyclomatic_complexity": 3.0,
    "halstead": 91.73835003173087,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response28.txt_stdout.txt",
    "code_path": "problem_iter1_code28.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Epsilon-Greedy priority function for Online Bin Packing Problem.\n\n    This heuristic aims to balance exploration and exploitation.\n    It primarily favors bins that can fit the item with the least remaining capacity\n    (a \"best fit\" greedy approach). However, with a small probability (epsilon),\n    it explores other options by assigning a small random positive value to all bins,\n    encouraging the use of less-filled bins occasionally.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploration\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        # Greedy part: prioritize bins that fit the item with least remaining capacity\n        # This is a \"best fit\" heuristic. Smaller remaining capacity means a better fit.\n        # We subtract item from remaining capacity to get the 'waste' after placement.\n        # We want to minimize this waste.\n        fit_capacities = bins_remain_cap[can_fit_mask] - item\n        priorities[can_fit_mask] = -fit_capacities # Negative because we want to minimize waste (maximize negative waste)\n\n        # Exploration part: with probability epsilon, add random noise to explore\n        if np.random.rand() < epsilon:\n            # Add a small positive random value to all bins that can fit the item.\n            # This allows less-preferred bins to sometimes be chosen.\n            random_noise = np.random.rand(np.sum(can_fit_mask)) * 0.01 # Small noise to avoid drastic changes\n            priorities[can_fit_mask] += random_noise\n\n    else:\n        # If no bin can fit the item, the priority is effectively the same for all.\n        # In a real scenario, this might mean opening a new bin.\n        # For the purpose of priority scores among existing bins, we can assign\n        # a uniform (but low) priority, or indicate that no suitable bin exists.\n        # Here, we assign a small negative value to all bins to signify no good fit.\n        priorities[:] = -1.0 # Indicate no bin fits\n\n    return priorities",
    "response_id": 28,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 77.66179398375645,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response29.txt_stdout.txt",
    "code_path": "problem_iter1_code29.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    The priority is calculated based on the remaining capacity of the bin after\n    placing the item. Bins with less remaining capacity after placement (i.e.,\n    tighter fits) are given higher priority. The Softmax function is used to\n    convert these \"fit scores\" into probabilities.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity if the item were placed in each bin\n    # We only consider bins where the item can actually fit\n    potential_remaining_capacities = bins_remain_cap - item\n    \n    # For bins where the item doesn't fit, assign a very low (or negative infinity)\n    # priority to effectively exclude them from Softmax.\n    # A large negative number will result in a very small exponential term.\n    fit_scores = np.where(potential_remaining_capacities >= 0,\n                          potential_remaining_capacities,\n                          -np.inf)\n\n    # To maximize the priority for bins that are a tighter fit, we can use\n    # the negative of the remaining capacity. So, smaller remaining capacity\n    # leads to a larger (less negative) value here.\n    # If the item doesn't fit, the fit_scores are -np.inf, so these will\n    # have a very low priority after softmax.\n    \n    # Apply Softmax to convert fit scores into probabilities/priorities\n    # A higher fit_score (tighter fit) will result in a higher priority\n    \n    # Handle the case where all bins are -inf (item doesn't fit anywhere)\n    if np.all(np.isneginf(fit_scores)):\n        return np.zeros_like(bins_remain_cap)\n    \n    exp_fit_scores = np.exp(fit_scores - np.max(fit_scores)) # Subtract max for numerical stability\n    priorities = exp_fit_scores / np.sum(exp_fit_scores)\n\n    return priorities",
    "response_id": 29,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 2.0,
    "halstead": 51.80615605397529,
    "exec_success": true
  }
]