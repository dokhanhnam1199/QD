```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a strategy
    that prioritizes bins with minimal *positive* residual capacity after packing.

    This version aims to find the tightest fit by favoring bins that, after
    packing the item, have the smallest remaining capacity. It also includes
    a mechanism to smooth the scoring for bins that are not perfect fits,
    using a non-linear function to give higher scores to bins that are
    significantly "fuller" after packing.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
        A higher score indicates a higher priority. Bins that cannot fit the
        item are given a score of -1. Bins that can fit are scored based on
        the *positive* remaining capacity after packing, favoring smaller
        remaining capacities with a non-linear boost.
    """
    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with low priority (-1)

    # Identify bins that can fit the item
    can_fit_mask = bins_remain_cap >= item

    # For bins that can fit the item, calculate the remaining capacity *after* packing
    fitting_bins_indices = np.where(can_fit_mask)[0]

    if fitting_bins_indices.size > 0:
        fitting_bins_capacities = bins_remain_cap[fitting_bins_indices]
        resulting_remain_cap = fitting_bins_capacities - item

        # Prioritize bins with minimal *positive* residual capacity.
        # A simple inverse relationship: 1 / (resulting_remain_cap + epsilon)
        # This gives higher scores to bins with smaller remaining capacity.
        # To make it more discriminating and emphasize "tightness" more strongly,
        # we can use a non-linear function. For instance, squaring the inverse
        # or using an exponential decay. Let's try a concave function like
        # log(1 + X) where X is related to tightness.
        # A good heuristic is to maximize (bin_capacity_before_packing - resulting_remain_cap)
        # which is `item` divided by the `resulting_remain_cap`. This can be unstable
        # if resulting_remain_cap is close to zero.

        # Let's prioritize bins that have the smallest `resulting_remain_cap`.
        # A straightforward way to do this with higher scores for better fits:
        # We want to maximize a score that is inversely related to `resulting_remain_cap`.
        # Consider `large_value - resulting_remain_cap`.
        # To make it more proportional to "tightness" and avoid issues with very small items
        # fitting into very large bins, we can use the *proportion* of capacity filled.
        # Proportion filled = `item / fitting_bins_capacities`.
        # This doesn't directly address the *residual* capacity minimization.

        # Let's focus on minimizing `resulting_remain_cap`.
        # A score like `1 / (resulting_remain_cap + epsilon)` is good.
        # To amplify the preference for tighter fits, we can use `1 / (resulting_remain_cap^p + epsilon)`
        # where p > 1, or a similar non-linear transformation.
        # Or, consider `max_possible_residual - resulting_remain_cap`, where `max_possible_residual`
        # is the maximum residual capacity across all bins that can fit the item.
        # This turns the problem into finding the bin with the minimum residual.

        # Let's use a score that's proportional to the "fullness" achieved by the item.
        # `item / fitting_bins_capacities` gives how much of the *current* space is used.
        # However, we want to prioritize *least remaining space*.

        # A strategy that prioritizes minimal *positive* residual capacity:
        # For bins that can fit, calculate `resulting_remain_cap`.
        # We want to assign higher scores to smaller `resulting_remain_cap`.
        # Using `1 / (resulting_remain_cap + epsilon)` achieves this.
        # To differentiate "good" fits from "okay" fits more strongly,
        # we can apply a non-linear transformation.
        # A simple approach is to square the inverse: `(1 / (resulting_remain_cap + epsilon))^2`.
        # This will significantly boost bins with very small `resulting_remain_cap`.

        epsilon = 1e-9
        # Scores bins based on the inverse of their remaining capacity after packing.
        # Higher scores for bins with less remaining capacity (tighter fits).
        # We use `resulting_remain_cap` directly, as smaller is better.
        # To map smaller to higher priority, we invert.
        # A non-linear increase for smaller `resulting_remain_cap` can be achieved
        # by using a power of the inverse. For instance, `1 / (resulting_remain_cap + epsilon)`.
        # To further emphasize tightness, we can use `1.0 / (resulting_remain_cap + epsilon)`
        # or a more aggressive function like `1.0 / np.sqrt(resulting_remain_cap + epsilon)`
        # or `1.0 / (resulting_remain_cap**2 + epsilon)`.

        # Let's consider `1.0 / (resulting_remain_cap + epsilon)`. This rewards smaller residuals.
        # To make it more "dynamic" and sensitive to how full the bin becomes relative to its
        # original state before the item, consider the proportion of space *used* by the item:
        # `item / fitting_bins_capacities`.
        # This gives a score between 0 and 1. Higher is better.
        # If `resulting_remain_cap` is the same for two bins, the one with larger
        # `fitting_bins_capacities` would have a smaller `item / fitting_bins_capacities` score.
        # This is not what we want for minimizing residual capacity.

        # Prioritize minimal *positive* residual capacity:
        # score = C - resulting_remain_cap
        # To make this non-negative and increase with "tightness":
        # score = (max_potential_residual - resulting_remain_cap)
        # If we consider max potential residual as some large constant, or the maximum
        # residual among all bins.

        # Let's use the inverse of the remaining capacity, but apply a transformation
        # that emphasizes smaller values more.
        # `score = 1 / (resulting_remain_cap + epsilon)`
        # To make it more sensitive to small `resulting_remain_cap`, we can use:
        # `score = 1 / np.sqrt(resulting_remain_cap + epsilon)`
        # Or, inversely related to the *proportion* of capacity *remaining*:
        # `proportion_remaining = resulting_remain_cap / fitting_bins_capacities`
        # `score = 1 / (proportion_remaining + epsilon)`

        # Let's go with a direct inverse of the remaining capacity, possibly with a non-linear boost.
        # A simple non-linear boost: using `log(1 + X)` where X is related to tightness.
        # Or, `1 / (residual + epsilon)` where we want smaller residual to give higher score.
        # To give a *smoother* graduation and avoid extremely high scores for very small residuals,
        # let's try `1.0 / (resulting_remain_cap + 1.0)`. This shifts the focus from very small
        # residuals to a more balanced approach.

        # Let's prioritize by the inverse of the remaining capacity, ensuring higher is better.
        # The score should be higher for bins with smaller `resulting_remain_cap`.
        # A simple way is `1 / (resulting_remain_cap + epsilon)`.
        # To make it more discriminating for small `resulting_remain_cap`, we can use a form
        # like `1.0 / np.sqrt(resulting_remain_cap + epsilon)`. This rewards tighter fits more
        # aggressively than a linear inverse.
        # This should provide the desired effect of prioritizing minimal *positive* residual capacity,
        # with a non-linear boost for very tight fits.

        priorities[fitting_bins_indices] = 1.0 / np.sqrt(resulting_remain_cap + epsilon)

    return priorities
```
