[
  {
    "stdout_filepath": "problem_iter6_response0.txt_stdout.txt",
    "code_path": "problem_iter6_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority using a refined First Fit strategy, favoring bins with minimal remaining capacity.\n\n    This heuristic prioritizes bins that can fit the item. Among the bins that can\n    accommodate the item, it assigns a higher priority to those with the least\n    remaining capacity. This is a direct implementation of the First Fit approach,\n    where the priority score is inversely related to the slack (remaining capacity minus item size).\n    Bins that cannot fit the item receive a score of -1.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority. Bins that can fit the item are\n        scored based on their remaining capacity, with smaller capacities being better.\n        Bins that cannot fit the item have a score of -1.\n    \"\"\"\n    # Initialize priorities to -1, indicating the item cannot fit or it's the lowest priority.\n    priorities = np.full_like(bins_remain_cap, -1.0)\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, assign a priority score.\n    # The goal is to prioritize bins with the smallest remaining capacity.\n    # A simple and direct way to achieve this is to use the negative of the\n    # remaining capacity. This means smaller remaining capacities will have\n    # less negative (i.e., higher) scores.\n    # For example, if `item = 0.3` and `bins_remain_cap` are `[0.5, 0.8, 0.2]`:\n    # - Bin 0: fits (0.5 >= 0.3), score = -0.5\n    # - Bin 1: fits (0.8 >= 0.3), score = -0.8\n    # - Bin 2: does not fit (0.2 < 0.3), score = -1.0\n    # The highest score is -0.5, corresponding to the bin with the least remaining capacity that fits.\n    fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n    if fitting_bins_capacities.size > 0:\n        priorities[can_fit_mask] = -fitting_bins_capacities\n\n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 31.699250014423125,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response1.txt_stdout.txt",
    "code_path": "problem_iter6_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a hybrid approach.\n\n    This heuristic combines aspects of Best Fit and First Fit. It prioritizes\n    bins that can fit the item, favoring those with less remaining capacity (First Fit\n    tendency) but also considering how tightly the item fits (Best Fit tendency).\n    Specifically, it assigns a priority score that is higher for bins with less\n    remaining capacity that can still accommodate the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher priority indicates a more suitable bin. Bins that cannot fit\n        the item are assigned a priority of -1.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with low priority\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, assign a priority.\n    # We want to prioritize bins that are \"fuller\" among those that can fit.\n    # This means prioritizing bins with smaller remaining capacity.\n    # A simple way to achieve this is to use a large value minus the remaining capacity.\n    # This way, smaller remaining capacities (closer to fitting the item) get higher scores.\n    fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_capacities.size > 0:\n        # To prioritize smaller remaining capacities, we can invert the remaining capacities\n        # or subtract them from a large number. Subtracting from a large number ensures\n        # that smaller remaining capacities result in larger priority scores.\n        # The maximum possible remaining capacity before fitting the item could be the\n        # maximum original capacity. A safe upper bound for the priority can be\n        # set by using a value larger than any possible remaining capacity after fitting.\n        # Let's use a value that is guaranteed to be larger than any remaining capacity.\n        # A simple approach is to use a large constant, or a value derived from the bins.\n        # For simplicity and to ensure a clear hierarchy for fitting bins,\n        # we use a large value minus the remaining capacity.\n        large_constant = np.max(bins_remain_cap) + item + 1 # A value larger than any capacity\n        priorities[can_fit_mask] = large_constant - fitting_bins_capacities\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 64.72503367497926,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response2.txt_stdout.txt",
    "code_path": "problem_iter6_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority using a sigmoid-based function that favors bins with small positive residuals.\n\n    This heuristic prioritizes bins where the remaining capacity (`r`) after placing the item\n    is as close to zero as possible, but still non-negative. The sigmoid function\n    `1 / (1 + exp(k * r))` is used, where `k` is a sensitivity parameter.\n    A higher `k` makes the priority drop faster as `r` increases.\n    Bins where the item does not fit (i.e., `r < 0`) receive a priority of 0.\n    The function is slightly modified to output a value closer to 1 for perfect fits (residual 0)\n    and values closer to 0 for larger residuals, but still positive.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Sensitivity parameter for the sigmoid function.\n    # A higher k means a sharper drop in priority as the remaining capacity increases.\n    # This value can be tuned based on experimental results.\n    k = 15.0\n\n    # Initialize priorities to 0. Bins where the item cannot fit will keep this score.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate the remaining capacity if the item were placed in each bin.\n    residuals = bins_remain_cap - item\n\n    # Identify bins where the item can fit (residual >= 0).\n    can_fit_mask = residuals >= 0\n\n    if np.any(can_fit_mask):\n        # We want to prioritize bins with the smallest positive residuals.\n        # The sigmoid function `1 / (1 + exp(k * residual))` maps `residual=0` to `0.5`.\n        # To better prioritize perfect fits (residual=0) as higher priority,\n        # we can transform the output or the input.\n        # Let's shift the input: `exp(k * (residual - epsilon))` where epsilon is a small positive value.\n        # Or, we can use `1 - sigmoid(-k * residual)` or `sigmoid(k * residual)`.\n        # Using `sigmoid(k * residual)`:\n        # - residual = 0  -> exp(0) = 1, score = 1 / (1+1) = 0.5\n        # - residual = 0.01, k=15 -> exp(0.15) approx 1.16, score = 1 / (1+1.16) approx 0.46\n        # This prioritizes smaller residuals, but 0.5 is not the peak.\n\n        # Let's consider a modified sigmoid or a different function.\n        # A simple approach for favoring smaller positive residuals is to use:\n        # `max_capacity_penalty - residual` for fitting bins, where `max_capacity_penalty`\n        # is a large value. This is similar to v0.\n\n        # Let's stick with the sigmoid idea but adjust the output to have a clear peak at residual=0.\n        # The function `1 / (1 + exp(-k * residual))` maps residual=0 to 0.5.\n        # For residual < 0, it's < 0.5. For residual > 0, it's > 0.5.\n        # This doesn't quite prioritize small POSITIVE residuals.\n\n        # A better approach for small positive residuals might be:\n        # `exp(-k * residual)` for residual >= 0.\n        # This function gives 1 for residual=0, a value close to 1 for small positive residuals,\n        # and a value close to 0 for large positive residuals.\n        # It also requires handling the case where `bins_remain_cap` is very large,\n        # which might lead to overflow in `exp`.\n\n        # Let's refine the `1 / (1 + exp(k * r))` approach.\n        # We want higher scores for smaller `r` (where `r >= 0`).\n        # The original `1 / (1 + exp(k * r))` makes scores *lower* as `r` increases.\n        # We need scores to be *higher* as `r` decreases (towards 0).\n        # So, we should use a function that is decreasing with `r`.\n\n        # Let's use `1 - sigmoid(k * residual)` which is `1 - 1 / (1 + exp(k * residual)) = exp(k * residual) / (1 + exp(k * residual))`.\n        # This function maps residual=0 to 0.5.\n        # For residual=0.01, k=15, exp(0.15) approx 1.16, score approx 1.16 / 2.16 approx 0.53.\n        # For residual=1.0, k=15, exp(15) is large, score approaches 1. This is not what we want.\n\n        # The initial `priority_v1` function's logic:\n        # \"When residual is 0 (perfect fit), score is 1 / (1 + exp(0)) = 0.5.\n        # When residual is small positive (e.g., 0.01, k=15), arg is 0.15, score is ~0.53 (slightly higher priority).\"\n        # This means it *does* slightly prioritize smaller positive residuals.\n        # The description in v1 seems correct for its implementation.\n\n        # Let's try to make the priority *higher* for smaller residuals.\n        # The function `exp(-k * residual)` for `residual >= 0` fits this.\n        # `exp(-k * 0)` = 1 (highest priority for perfect fit)\n        # `exp(-k * 0.01)` for k=15 is `exp(-0.15)` approx 0.86\n        # `exp(-k * 1.0)` for k=15 is `exp(-15)` very small.\n        # This maps well to the \"tightest fit\" idea.\n\n        # To avoid potential overflow with large `k * residual` for negative residuals\n        # (which would be `exp(large positive)`), we ensure `residual` is non-negative\n        # when applying this exponential. The `can_fit_mask` already handles this.\n\n        # Let's scale the output or adjust `k`.\n        # Using `exp(-k * residual)` directly gives values between 0 and 1.\n        # If we want the highest score to be clearly distinguishable, we can multiply by a factor or add an offset.\n\n        # Let's try to implement the `exp(-k * residual)` strategy.\n        # To ensure the score is higher for smaller `residual` (where `residual >= 0`).\n        # We want `f(0) > f(0.01) > f(1.0)`.\n        # `exp(-k * residual)` is a decreasing function for positive `k` and `residual`.\n\n        # Consider the residuals for fitting bins.\n        fitting_residuals = residuals[can_fit_mask]\n\n        # If we want to prioritize bins that are almost full, we want smaller residuals.\n        # The function `1 / (1 + exp(k * residual))` gives higher values for smaller residuals.\n        # The description of v1 is actually correct in that regard.\n        # \"When residual is 0 (perfect fit), score is 0.5.\n        # When residual is small positive (e.g., 0.01, k=15), arg is 0.15, score is ~0.53 (slightly higher priority).\"\n        # This means it *does* favor bins with smaller positive residuals.\n\n        # To make the priority *even more pronounced* for tight fits, we can increase `k`.\n        # Alternatively, we can rescale the output.\n\n        # Let's consider another approach: Rank the fitting bins by their residual.\n        # The bin with the smallest residual gets the highest priority.\n        # `priorities[can_fit_mask] = -fitting_residuals` would prioritize smallest residuals.\n        # This is similar to v0 but without the offset.\n\n        # The reflection specifically mentions sigmoid. Let's try to optimize the sigmoid usage.\n        # The function `1 / (1 + exp(k * r))` gives a higher value for smaller `r`.\n        # The description states that for `r=0.01` (positive small residual), the score is `0.53`, which is higher than `0.5` for `r=0`.\n        # This implies that `priority_v1` *underrates* perfect fits compared to very small positive residuals.\n        # This is counter-intuitive if \"tightest fit\" means residual closest to zero.\n\n        # If we want residual=0 to have the highest priority, we need a function where\n        # the maximum is at `residual = 0` and decreases as `residual` moves away from 0 (in positive direction).\n        # `exp(-k * abs(residual))` could work, but we only care about `residual >= 0`.\n        # So, `exp(-k * residual)` for `residual >= 0` is a good candidate.\n\n        # Let's modify `priority_v1` to use `exp(-k * residual)` for fitting bins.\n        # We want to give scores that reflect \"how good\" the fit is.\n        # The function `exp(-k * residual)` maps `residual=0` to 1, `residual` slightly positive to <1, and large `residual` to near 0.\n        # This seems to align better with prioritizing small positive residuals and perfect fits.\n\n        # We can scale this to have a wider range of scores if needed, or to ensure\n        # that \"non-fitting\" bins are clearly at the bottom.\n        # The current `priorities` are initialized to 0, so non-fitting bins have score 0.\n        # `exp(-k * residual)` will produce scores between (0, 1] for `residual >= 0`.\n        # This ensures fitting bins always have higher priority than non-fitting ones.\n\n        # Let's refine the `k` value. A higher `k` makes the priority drop faster.\n        # With `exp(-k * residual)`, a higher `k` means smaller residuals get much higher priority.\n\n        # Using the current `k = 15.0`:\n        # residual = 0.0: exp(0) = 1.0\n        # residual = 0.01: exp(-0.15) approx 0.86\n        # residual = 0.1: exp(-1.5) approx 0.22\n        # residual = 0.5: exp(-7.5) approx 0.00055\n        # This clearly prioritizes smaller residuals.\n\n        # The scores are already in a range that differentiates well.\n        # No further scaling needed unless specific range is required.\n\n        # Apply the exponential decay function for bins that can fit the item.\n        # We use `residuals[can_fit_mask]` to only consider positive residuals.\n        priorities[can_fit_mask] = np.exp(-k * residuals[can_fit_mask])\n\n        # Ensure no NaN values if `k * residuals` is extremely large negative (which shouldn't happen with residual >= 0).\n        # `np.clip` can be used for safety, but `exp` of negative numbers is always positive.\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 38.053747805010275,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response3.txt_stdout.txt",
    "code_path": "problem_iter6_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit strategy.\n\n    The Best Fit strategy prioritizes bins that can accommodate the item and\n    then selects the bin that results in the smallest remaining capacity after\n    packing. This version directly uses the negative of the *resulting* remaining\n    capacity as the priority score.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the\n        item are given a score of -1. Bins that can fit the item are scored\n        based on the negative of the remaining capacity after packing, favoring\n        smaller remaining capacities (which result in higher negative scores).\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with a low priority score\n\n    # Calculate potential remaining capacity if the item fits in each bin\n    potential_remaining_caps = bins_remain_cap - item\n\n    # Identify bins that can accommodate the current item\n    can_fit_mask = potential_remaining_caps >= 0\n\n    # For bins that can fit the item, assign a priority score.\n    # The \"best fit\" is achieved when the remaining capacity after placing the item is minimized.\n    # To translate minimizing remaining capacity into maximizing priority score,\n    # we use the negative of the potential remaining capacity.\n    # A smaller positive remaining capacity will result in a larger (less negative) or positive value,\n    # hence a higher score when we want to prioritize it.\n    fitting_bins_potential_remaining = potential_remaining_caps[can_fit_mask]\n\n    if fitting_bins_potential_remaining.size > 0:\n        # Assign priorities: -potential_remaining_caps.\n        # This directly implements the reflection: prioritize smaller remaining capacities,\n        # which means prioritizing larger (less negative) or positive values of -(remaining_capacity).\n        priorities[can_fit_mask] = -fitting_bins_potential_remaining\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 44.97261104228487,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response4.txt_stdout.txt",
    "code_path": "problem_iter6_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority using a strategy that favors bins with minimal positive residuals.\n\n    This heuristic prioritizes bins where the remaining capacity after placing the item\n    is as close to zero as possible, but still strictly positive. This aims to fill bins\n    more tightly. Bins where the item does not fit, or where the remaining capacity\n    after placement would be exactly zero, will have the lowest priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    # Initialize priorities to a very low value. Bins that cannot fit the item\n    # or result in zero remaining capacity will get this lowest priority.\n    priorities = np.full_like(bins_remain_cap, -float('inf'))\n\n    # Calculate the remaining capacity if the item were placed in each bin.\n    residuals = bins_remain_cap - item\n\n    # Identify bins where the item can fit and leaves a strictly positive residual.\n    # We use `residuals > epsilon` to avoid zero residual, encouraging a small gap.\n    epsilon = 1e-9\n    can_fit_positive_residual_mask = residuals > epsilon\n\n    # For bins that can fit and leave a positive residual, the priority is inversely\n    # proportional to the positive residual. A smaller positive residual means a tighter fit.\n    # Using `1.0 / residuals` for these bins.\n    priorities[can_fit_positive_residual_mask] = 1.0 / residuals[can_fit_positive_residual_mask]\n\n    # Consider bins where the item fits exactly (residual is zero).\n    # These are good fits, but less ideal than leaving a small positive gap.\n    # We can assign them a slightly lower priority than the best fits.\n    # If `residuals == 0`, `1.0 / residuals` would be infinity, which is not desired.\n    # Let's assign a high but finite priority.\n    fits_exactly_mask = np.isclose(residuals, 0)\n    # Assign a priority that is high but lower than the best possible positive residual case\n    # For instance, use a very large number, but smaller than `1.0 / epsilon`.\n    # Or, simply a slightly lower value than the maximum achievable from positive residuals.\n    # A simple approach is to use a fixed high value, or a value derived from the 'best' positive residual.\n    # For simplicity and to ensure they are considered but not preferred over tiny gaps,\n    # let's assign a value that's still high. A slightly smaller reciprocal could work if we knew the minimum positive residual.\n    # A common strategy is to use a large constant if the problem structure doesn't give a natural scaling.\n    # Let's make it a very high priority, but clearly distinguishable.\n    # The current `1.0 / residuals` will produce values that can be very large.\n    # If we want to prioritize exact fits slightly lower than tightest fits, we could do:\n    # max_possible_priority = 1.0 / epsilon # theoretical max if residuals are tiny positive\n    # priorities[fits_exactly_mask] = max_possible_priority * 0.99 # or some large constant.\n    # For this specific reflection, we are focusing on minimal *positive* residuals.\n    # Bins that fit exactly (residual=0) are still good fits and should have high priority,\n    # but the prompt implies prioritizing *positive* residuals.\n    # Let's stick to the reflection's emphasis: minimal *positive* residuals.\n    # Bins that fit exactly are less prioritized than those with minimal *positive* slack.\n\n    # Update: The reflection is about favoring minimal *positive* residuals.\n    # This means bins with `residual > 0` are preferred, and among those, the smallest `residual` is best.\n    # Bins with `residual == 0` are not explicitly prioritized by the reflection's wording.\n    # If we *must* prioritize minimal positive residuals, bins with residual=0 are not part of that.\n    # However, in practice, an exact fit is often very desirable.\n    # Let's adjust the priority to reflect the \"minimal positive residual\" focus precisely.\n    # The original `1.0 / residuals` for `residuals > epsilon` already does this.\n    # We can assign a moderate priority to exact fits to differentiate them.\n\n    # Let's re-evaluate the goal: \"prioritizes bins where the remaining capacity after placing the item\n    # is as close to zero as possible, but still strictly positive.\"\n    # This means `residual > 0` and `residual` is minimized.\n    # The `1.0 / residuals` where `residuals > epsilon` handles this.\n    # What about `residuals == 0`? The reflection implies they are less preferred than minimal *positive* ones.\n    # So, our current setup where they get `-inf` is consistent with *only* prioritizing strictly positive residuals.\n    # If an exact fit is desired, a different heuristic might be better.\n    # Given the reflection, let's keep the distinction clear.\n\n    # A common practical approach is to give exact fits a very high priority too.\n    # For clarity based on the reflection:\n    # - Bins with residual > epsilon: priority = 1.0 / residual\n    # - Bins with residual == 0: priority = moderate_high_value\n    # - Bins with residual < 0: priority = -inf\n\n    # Let's assign a slightly lower priority to exact fits than the best possible positive residual fit.\n    # The smallest positive residual would yield a priority close to 1/epsilon.\n    # We can assign a value like 1.0 / (epsilon * 2) or a large constant.\n    # For simplicity, let's use a value that's still high but bounded.\n    # The logic is that a tiny positive gap is slightly preferred over no gap.\n    # However, a pragmatic BPP solver would often consider exact fits excellent.\n\n    # For the purpose of implementing the reflection strictly:\n    # \"prioritizes bins where the remaining capacity ... is as close to zero as possible, but still strictly positive.\"\n    # This means bins with residual=0 are not candidates for the \"best\" priority according to this specific phrasing.\n    # Let's refine: If `residuals > epsilon`, score is `1.0 / residuals`.\n    # If `residuals == 0`, score is `some_value_less_than_min(1.0 / residuals[residuals > epsilon])`\n    # If `residuals < 0`, score is `-inf`.\n\n    # Let's refine the scoring for exact fits to be high, but less than ideal positive residuals.\n    # The best score from `1.0/residuals` will be for the smallest `residuals > epsilon`.\n    # Let's assign a score for exact fits that is high but bounded, and lower than any `1.0/positive_residual`.\n    # For example, if epsilon=1e-9, the max theoretical priority is 1e9.\n    # We can assign exact fits a priority like 1e8.\n    \n    # Resetting priorities for a cleaner approach focused on the reflection:\n    priorities = np.full_like(bins_remain_cap, -float('inf')) # All bins start with lowest priority\n\n    residuals = bins_remain_cap - item\n\n    # Mask for bins that can fit the item and leave a strictly positive residual\n    epsilon = 1e-9\n    mask_positive_residual = residuals > epsilon\n\n    # Calculate priorities for bins with strictly positive residuals\n    # Higher priority for smaller positive residuals\n    priorities[mask_positive_residual] = 1.0 / residuals[mask_positive_residual]\n\n    # Mask for bins where the item fits exactly\n    mask_exact_fit = np.isclose(residuals, 0)\n    \n    # Assign a high priority to exact fits, but lower than the best possible positive residual fits.\n    # The maximum priority from the positive residual case would be around 1/epsilon.\n    # So, assign a value significantly less than 1/epsilon but still high.\n    # A practical value could be related to the maximum possible number of bins or a large constant.\n    # For example, if there are N bins, a priority of N or N*2 is usually sufficient to rank well.\n    # Let's use a large constant that's clearly less than 1/epsilon.\n    # For instance, if epsilon is 1e-9, 1/epsilon is 1e9. Let's use 1e8.\n    # This ensures they are ranked above bins with larger positive residuals.\n    # If no bins have positive residuals, then exact fits will be the top priority.\n    if np.any(mask_exact_fit):\n        # If there are positive residuals, this value should be less than the minimum 1/positive_residual.\n        # If there are no positive residuals, this value should be the highest.\n        # A safe bet is a large number that's reliably less than 1/epsilon.\n        # Let's use a scaled value based on the number of bins as a heuristic for magnitude.\n        # Or simply a large constant.\n        \n        # Let's consider the distribution of 1/residuals. The minimum of `1.0 / residuals[mask_positive_residual]`\n        # gives the priority of the \"worst\" positive residual fit.\n        # Any priority for exact fit should ideally be related to this scale.\n        # If `mask_positive_residual` is empty, then exact fits are the best.\n        # If `mask_positive_residual` is not empty, then we want exact_fit_priority < min(1.0 / residuals[mask_positive_residual]).\n        \n        # A robust way: calculate all `1.0 / residuals` for `residuals > 0`.\n        # Then, assign exact fits a priority that is just below the minimum of these.\n        \n        if np.any(mask_positive_residual):\n            min_positive_residual_priority = np.min(priorities[mask_positive_residual])\n            priorities[mask_exact_fit] = min_positive_residual_priority * 0.9 # Slightly less priority than the best positive residual\n        else:\n            # If no positive residuals exist, exact fits are the best possible.\n            # Assign a very high priority.\n            priorities[mask_exact_fit] = 1e12 # A very large number\n\n    return priorities",
    "response_id": 4,
    "obj": 4.627044276027133,
    "cyclomatic_complexity": 3.0,
    "halstead": 100.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response5.txt_stdout.txt",
    "code_path": "problem_iter6_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, using a\n    modified Best Fit strategy.\n\n    This strategy prioritizes bins that can accommodate the item and leave the\n    least amount of remaining capacity. It aims to fill bins more efficiently.\n    Similar to Best Fit, it looks for bins where `bins_remain_cap - item` is minimized.\n    However, it also introduces a penalty for bins that have a very large remaining\n    capacity, to encourage filling up existing bins before opening new ones.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the\n        item are given a priority of negative infinity.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)  # Initialize with a very low priority\n\n    # Calculate the remaining capacity if the item is placed in each bin\n    potential_remaining_cap = bins_remain_cap - item\n\n    # Identify bins that can accommodate the item (remaining capacity is non-negative)\n    can_fit_mask = potential_remaining_cap >= 0\n\n    # For bins that can fit the item, assign a priority score.\n    # The primary goal is to minimize the residual capacity, so we use the negative of it.\n    # We add a small bonus to bins that have *some* capacity but not too much,\n    # encouraging the use of partially filled bins over very empty ones.\n    # A simple way to achieve this is to subtract the *log* of the remaining capacity,\n    # or a scaled version of it, which penalizes larger remaining capacities.\n    # Let's use a simple approach: penalize larger residuals more strongly.\n    # We can still use the negative residual as the base priority and then\n    # add a small adjustment based on the absolute remaining capacity.\n    # A common heuristic for online bin packing is to favor bins that are \"tightly\" filled.\n    # The negative residual captures the \"tightness\" directly.\n    # For an extra nudge towards slightly fuller bins, we could subtract a term\n    # that increases with the remaining capacity. For simplicity and staying close to BF,\n    # let's refine the negative residual idea:\n    # We want to maximize -(remaining_capacity).\n    # This means we want to minimize remaining_capacity.\n    # The current `priority_v1` already does this. Let's refine its scoring.\n    # Instead of a linear mapping, we can use an inverse relationship with residual.\n    # This would make smaller residuals much more preferable.\n    # E.g., 1 / (residual + epsilon).\n    # However, `priority_v1`'s simple negative residual is generally a strong Best Fit.\n    # Let's consider a slight modification to penalize *very* large remaining capacities\n    # more explicitly. This can be done by subtracting a term that grows with the residual.\n    # For example: -residual - alpha * residual^2, or -residual - alpha * log(residual) for residual > 0.\n    # Or, simply a linear penalty based on the *original* remaining capacity for bins\n    # that fit the item.\n\n    # Let's stick to the principle of minimizing residual capacity as the primary driver.\n    # The reflection suggests prioritizing minimal residual capacity.\n    # `priority_v1` does this with `-potential_remaining_cap`.\n    # Let's consider making the \"preference\" for smaller residuals stronger.\n    # Using an inverse relationship `1 / (residual + epsilon)` achieves this.\n    # This is similar to `priority_v0` but correctly handles the \"can fit\" logic\n    # and the negative infinity for non-fitting bins.\n\n    epsilon = 1e-9  # Small constant to avoid division by zero\n\n    if np.any(can_fit_mask):\n        # Calculate priorities for bins that can fit the item\n        # We want to maximize the priority, so smaller remaining capacity should yield higher priority.\n        # Using 1 / (remaining_capacity + epsilon) achieves this.\n        # Larger remaining_capacity -> smaller 1 / (remaining_capacity + epsilon)\n        # Smaller remaining_capacity -> larger 1 / (remaining_capacity + epsilon)\n        # This encourages tighter fits.\n        priorities[can_fit_mask] = 1.0 / (potential_remaining_cap[can_fit_mask] + epsilon)\n\n    return priorities",
    "response_id": 5,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 53.30296890880645,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response6.txt_stdout.txt",
    "code_path": "problem_iter6_code6.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, favoring tighter fits smoothly.\n\n    This heuristic prioritizes bins that can accommodate the item, and among those,\n    it favors bins that will have the smallest remaining capacity after packing.\n    This is a variation of the \"Best Fit\" strategy, using a smooth, graduated\n    priority based on the *remaining capacity* directly, not the gap.\n\n    Specifically, for bins where `bins_remain_cap >= item`:\n    - The priority is determined by the remaining capacity *before* packing (`bins_remain_cap`).\n    - Smaller remaining capacities (tighter fits) receive higher priority.\n    - Larger remaining capacities receive lower priority.\n    - Bins that cannot fit the item receive a priority of 0.\n\n    The priority score is calculated as:\n    `score = exp(-scale_factor * bins_remain_cap)`\n    for valid bins. The negative exponential function ensures that as the\n    remaining capacity decreases (i.e., the bin is fuller), the priority score\n    increases smoothly.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Scale factor to control the steepness of the exponential decay.\n    # A higher value makes the priority drop off more quickly as the remaining\n    # capacity increases. Tune this value based on empirical performance.\n    scale_factor = 0.1  # Adjusted scale_factor for smoother decay across typical remaining capacities\n\n    # Identify bins that have enough remaining capacity to fit the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Assign priorities using a negative exponential function on the remaining capacity.\n    # This provides a smooth, graduated priority:\n    # - As remaining capacity approaches 0 (bin is nearly full), exp(-scale_factor * 0) = 1 (highest priority).\n    # - As remaining capacity increases (bin is emptier), exp(-scale_factor * large_value) approaches 0 (low priority).\n    # This directly prioritizes bins that have less space available, encouraging tighter packing.\n    priorities[can_fit_mask] = np.exp(-scale_factor * bins_remain_cap[can_fit_mask])\n\n    return priorities",
    "response_id": 6,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 24.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response7.txt_stdout.txt",
    "code_path": "problem_iter6_code7.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority using a non-linear transformation favoring tighter fits.\n\n    This heuristic prioritizes bins with the smallest positive slack (remaining capacity - item).\n    It uses a transformation `1 / (slack^power + epsilon)` which gives higher priority\n    to bins with smaller slack values. The `power` parameter controls the graduation sensitivity.\n    A higher `power` makes the priority drop more steeply as slack increases, thus\n    more strongly favoring very tight fits. Bins that cannot fit the item are given a score of -1.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the\n        item are given a score of -1.\n    \"\"\"\n    # Power parameter for the slack transformation.\n    # power=1.0 is inverse proportionality.\n    # power > 1.0 makes the priority drop faster for larger slacks, thus more sensitive to tight fits.\n    # A value like 1.5 or 2.0 provides a good graduated response.\n    power = 1.5\n    epsilon = 1e-9  # For numerical stability and to avoid division by zero\n\n    # Initialize priorities to a low value (-1 indicates cannot fit).\n    priorities = np.full_like(bins_remain_cap, -1.0, dtype=float)\n\n    # Identify bins that can fit the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n\n        # Calculate the slack (positive residual) for fitting bins.\n        slack = fitting_bins_capacities - item\n\n        # Calculate priority: 1 / (slack^power + epsilon).\n        # This prioritizes bins with smaller slack values, providing a graduated priority.\n        # Higher power means priority decreases more rapidly as slack increases.\n        priorities[can_fit_mask] = 1.0 / ((slack + epsilon)**power)\n\n        # Optional: Further scaling or transformation could be applied here if needed for tuning.\n        # For example, normalizing priorities within the fitting bins, or applying a cap.\n        # However, the current form provides a meaningful graduated priority.\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 66.41714012534482,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response8.txt_stdout.txt",
    "code_path": "problem_iter6_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority using a strategy that balances First Fit with tighter packing.\n\n    This heuristic prioritizes bins where the remaining capacity after placing the item\n    is non-negative. Among bins that can fit the item, it favors those that result in\n    a smaller residual (tighter fit), but also considers the order of bins as in\n    First Fit by adding a small penalty to later bins. This aims to fill bins more\n    tightly while still adhering to a sequential-like preference.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the\n        item are given a score of -1.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with low priority (-1)\n\n    # Identify bins where the item can fit (residual >= 0).\n    residuals = bins_remain_cap - item\n    can_fit_mask = residuals >= 0\n\n    # For bins that can fit the item, calculate priorities.\n    # We want to favor tighter fits (smaller residuals).\n    # To incorporate a First Fit-like preference, we can add a small\n    # penalty based on the bin's index. This ensures that if two bins\n    # have the same residual, the one appearing earlier in the array\n    # (lower index) gets a slightly higher priority.\n\n    if np.any(can_fit_mask):\n        fitting_residuals = residuals[can_fit_mask]\n        # Calculate a score that prioritizes smaller residuals.\n        # A simple way is to use a large constant minus the residual.\n        # This makes smaller residuals result in larger scores.\n        # We add a small value based on the index to break ties and favor earlier bins.\n        # The index is normalized by adding 1 to avoid multiplying by zero for the first bin.\n        # The `1.0 / (index + 1)` creates a decreasing penalty for later bins.\n        # We invert this to add a smaller penalty for earlier bins.\n        indices = np.where(can_fit_mask)[0]\n        # Using a large base value to ensure these priorities are significantly higher than -1.\n        # Maximize the score for minimum residual, and use index as a tie-breaker favoring smaller indices.\n        # Score = (MaxPossibleResidual - residual) + (1.0 / (index + 1))\n        # Or to make it simpler and directly favor smaller residuals:\n        # Score = -(residual) + (1.0 / (index + 1))\n        # To make the higher score truly mean better priority for tighter fit,\n        # we can invert the residual and add a term for index.\n        # Let's create a score that is high for small residuals.\n        # Max residual + 1 - residual.\n        # The index priority can be represented by a small fraction, where earlier indices are larger fractions.\n        # e.g., N - residual + (N_bins - index) / N_bins.\n        # Let's try: (a large number - residual) + (a small number * (N_bins - index))\n        \n        # Let's simplify the logic: prioritize smaller residuals.\n        # For tie-breaking or adding a sequential preference, we can ensure that\n        # when residuals are similar, earlier bins get a slight edge.\n        # Score = -residual + (small_factor * (number_of_bins - index))\n        # Or, to keep it straightforward: smaller residual is better.\n        # We can use `1.0 / (residual + epsilon)` from v0, but add an index factor.\n        # A smaller index should get a higher score if residuals are equal.\n        \n        # Strategy: maximize (1 / (residual + epsilon)) and add a bonus for lower index.\n        epsilon = 1e-9\n        base_priorities = 1.0 / (fitting_residuals + epsilon)\n        \n        # Add a bonus for earlier bins. A simple way is to add `(num_bins - index)`.\n        # This adds a larger bonus to bins with smaller indices.\n        num_bins = len(bins_remain_cap)\n        index_bonus = (num_bins - indices) * 1e-6 # Small bonus, to not overpower residual preference\n        \n        priorities[can_fit_mask] = base_priorities + index_bonus\n        \n    return priorities",
    "response_id": 8,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 99.40434618240934,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response9.txt_stdout.txt",
    "code_path": "problem_iter6_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a strategy\n    that prioritizes bins with minimal *relative* remaining capacity after packing.\n\n    This version aims to differentiate \"good\" fits by considering the proportion\n    of remaining capacity relative to the bin's original capacity (approximated by\n    current remaining capacity + item size). It favors bins that are filled\n    more completely in proportion to their available space.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the\n        item are given a score of -1. Bins that can fit are scored based on\n        the *relative* remaining capacity after packing, favoring those that\n        are more \"tightly\" filled.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with low priority (-1)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity *after* packing\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n\n    if fitting_bins_indices.size > 0:\n        fitting_bins_capacities = bins_remain_cap[fitting_bins_indices]\n        resulting_remain_cap = fitting_bins_capacities - item\n\n        # To prioritize tighter fits, we want to favor bins where `resulting_remain_cap`\n        # is small relative to the bin's capacity before packing.\n        # The \"capacity before packing\" can be approximated by `fitting_bins_capacities`.\n        # A score that is inversely proportional to `resulting_remain_cap / fitting_bins_capacities`\n        # would favor smaller relative remaining space.\n        # To avoid division by zero or near-zero capacities, we can use:\n        # 1 / (resulting_remain_cap + epsilon) as a base score,\n        # and then potentially scale it by the proportion filled.\n\n        # A simpler approach to prioritize smaller *absolute* remaining capacity,\n        # while differentiating:\n        # Consider the \"tightness\" as (bin_capacity_before_item - resulting_remain_cap) / bin_capacity_before_item\n        # which simplifies to `item / fitting_bins_capacities`.\n        # This favors bins where the item takes up a larger proportion of the current space.\n        # To ensure higher score for better fit (larger proportion), we use this value directly.\n        # We still want to prefer bins that result in *less* remaining space overall if proportions are equal.\n        # Let's combine these: prioritize by (1 - proportion of remaining space).\n        # Proportion of remaining space = resulting_remain_cap / fitting_bins_capacities\n        # So, priority = 1 - (resulting_remain_cap / fitting_bins_capacities)\n        # = (fitting_bins_capacities - resulting_remain_cap) / fitting_bins_capacities\n        # = item / fitting_bins_capacities\n\n        # To ensure higher values are better, we can use a large constant minus this proportion,\n        # or directly use the proportion if we invert the logic and say smaller proportion is worse.\n        # Let's aim for higher score = better fit.\n        # Good fit means small `resulting_remain_cap`.\n        # So, a priority score like `1.0 / (resulting_remain_cap + epsilon)` is good.\n        # To incorporate the \"relative\" aspect, we can penalize bins that leave a lot of space relatively.\n        # Consider the \"waste\" as `resulting_remain_cap`. We want to minimize this waste.\n        # Prioritize by `large_value - resulting_remain_cap`.\n        # To make it relative, we can consider `large_value - resulting_remain_cap / fitting_bins_capacities`\n        # This still might not differentiate enough.\n\n        # Let's go back to prioritizing smaller *absolute* remaining capacity after packing,\n        # but also consider the original capacity available.\n        # A bin that is almost full is good. `fitting_bins_capacities - resulting_remain_cap` is `item`.\n        # We want to maximize `item / fitting_bins_capacities`.\n        # However, if two bins have the same `item / fitting_bins_capacities`, we should\n        # prefer the one with less `resulting_remain_cap`.\n\n        # A heuristic: priority = C - resulting_remain_cap.\n        # To make it relative, and ensure higher is better:\n        # Score = (fitting_bins_capacities - resulting_remain_cap) / fitting_bins_capacities\n        # = item / fitting_bins_capacities.\n        # This score is between 0 and 1. Higher means better fit.\n        # To make it more discriminating and always positive, and larger for better fits:\n        # Use `1 / (resulting_remain_cap + epsilon)` as in v0, but refine.\n        # Or use `fitting_bins_capacities - resulting_remain_cap`.\n\n        # Let's try to score based on how \"full\" the bin becomes relative to its initial state.\n        # A bin that is already very full is more likely to be a \"tight\" fit.\n        # Priority = `fitting_bins_capacities` - `resulting_remain_cap`\n        # This is `item`. So, this prioritizes larger items when fitting tightly.\n        # This doesn't account for the proportion.\n\n        # Consider the inverse of the *proportion* of remaining capacity.\n        # Proportion remaining = `resulting_remain_cap / fitting_bins_capacities`\n        # We want to minimize this proportion. So, priority is inversely related to it.\n        # Priority = `fitting_bins_capacities / (resulting_remain_cap + epsilon)`\n\n        epsilon = 1e-9\n        # This prioritizes bins where the remaining capacity is small *relative* to its current capacity.\n        # E.g., if bin has 10 capacity, item is 7, resulting cap is 3. Ratio = 3/10 = 0.3. Score = 1/0.3 = 3.33\n        # If bin has 100 capacity, item is 7, resulting cap is 93. Ratio = 93/100 = 0.93. Score = 1/0.93 = 1.07\n        # This clearly favors the first case (tighter fit).\n        priorities[fitting_bins_indices] = fitting_bins_capacities / (resulting_remain_cap + epsilon)\n\n    return priorities",
    "response_id": 9,
    "obj": 3.9589150378939015,
    "cyclomatic_complexity": 2.0,
    "halstead": 66.41714012534482,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response0.txt_stdout.txt",
    "code_path": "problem_iter7_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a strategy\n    that prioritizes bins with minimal *positive* residual capacity after packing.\n\n    This version aims to find the \"tightest\" possible fit. It prioritizes bins\n    that, after packing the item, will have the least remaining capacity,\n    but it still prioritizes bins that *can* fit the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the\n        item are given a score of -1. Bins that can fit are scored based on\n        the resulting remaining capacity, favoring smaller values.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with low priority (-1)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity *after* packing\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n\n    if fitting_bins_indices.size > 0:\n        fitting_bins_capacities = bins_remain_cap[fitting_bins_indices]\n        resulting_remain_cap = fitting_bins_capacities - item\n\n        # To prioritize minimal positive residual capacity, we want to find the bin\n        # where `resulting_remain_cap` is minimized.\n        # A simple way to assign higher priority to smaller values is to use\n        # a large constant minus the value, or an inverse relationship.\n        # Using an inverse relationship like `1 / (value + epsilon)` makes smaller\n        # values yield larger priorities.\n\n        epsilon = 1e-9  # To avoid division by zero or near-zero values\n        # Here, smaller `resulting_remain_cap` will lead to a higher score.\n        # For example, if resulting_remain_cap is 0.1, score is ~10.\n        # If resulting_remain_cap is 5, score is ~0.2.\n        priorities[fitting_bins_indices] = 1.0 / (resulting_remain_cap + epsilon)\n\n    return priorities",
    "response_id": 0,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 66.41714012534482,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response1.txt_stdout.txt",
    "code_path": "problem_iter7_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a strategy\n    that prioritizes bins with minimal *positive* residual capacity after packing.\n\n    This version aims to differentiate \"good\" fits by favoring bins that result in\n    a smaller remaining capacity. It uses a non-linear function to give higher\n    priority to bins that are more \"tightly\" filled, without discarding bins\n    that might be slightly larger but still offer a good fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the\n        item are given a score of -1. Bins that can fit are scored based on\n        the *inverse* of the remaining capacity after packing, favoring those\n        that leave less space.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with low priority (-1)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity *after* packing\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n\n    if fitting_bins_indices.size > 0:\n        fitting_bins_capacities = bins_remain_cap[fitting_bins_indices]\n        resulting_remain_cap = fitting_bins_capacities - item\n\n        # Prioritize bins with minimal *positive* residual capacity.\n        # A simple inverse: 1 / (resulting_remain_cap + epsilon)\n        # This gives higher scores to smaller remaining capacities.\n        # Adding a term related to the original capacity could also be considered,\n        # but for minimal positive residual capacity, the inverse is a good start.\n        # To ensure larger scores are better, we use the inverse.\n        # We can also add a small constant or scale it to make scores more spread out\n        # if needed, but the core idea is inverse of remaining space.\n\n        epsilon = 1e-9  # Small value to avoid division by zero\n        # A higher score means a better fit (smaller remaining capacity)\n        # This directly favors bins that are more tightly packed.\n        priorities[fitting_bins_indices] = 1.0 / (resulting_remain_cap + epsilon)\n\n    return priorities",
    "response_id": 1,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 66.41714012534482,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response2.txt_stdout.txt",
    "code_path": "problem_iter7_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a strategy\n    that prioritizes bins with minimal *positive* residual capacity, with\n    graduated non-linear scoring.\n\n    This version aims to differentiate \"good\" fits by considering the ratio of\n    the bin's capacity before packing to the remaining capacity after packing,\n    raised to a power. This emphasizes bins that are filled more completely.\n    The power function provides a smoother, graduated score and increases\n    sensitivity to very small positive remaining capacities.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the\n        item are given a score of -1. Bins that can fit are scored based on\n        the inverse of the cubic root of their remaining capacity after packing,\n        scaled by the capacity before packing.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with low priority (-1)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity *after* packing\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n\n    if fitting_bins_indices.size > 0:\n        fitting_bins_capacities = bins_remain_cap[fitting_bins_indices]\n        resulting_remain_cap = fitting_bins_capacities - item\n\n        # Use a power of 1.5 for graduated sensitivity to residual capacity.\n        # The score is `capacity_before_item / (residual_after_packing^1.5 + epsilon)`\n        # This prioritizes bins where residual is small, and differentiates more\n        # for smaller residual values.\n        # epsilon is added to avoid division by zero for perfect fits.\n        epsilon = 1e-9\n        power_term = np.power(resulting_remain_cap, 1.5) + epsilon\n        priorities[fitting_bins_indices] = fitting_bins_capacities / power_term\n\n    return priorities",
    "response_id": 2,
    "obj": 3.9589150378939015,
    "cyclomatic_complexity": 2.0,
    "halstead": 66.41714012534482,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response3.txt_stdout.txt",
    "code_path": "problem_iter7_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a strategy\n    that prioritizes bins with minimal *positive* residual capacity after packing.\n\n    This version aims to find the tightest fit by favoring bins that, after\n    packing the item, have the smallest remaining capacity. It also includes\n    a mechanism to smooth the scoring for bins that are not perfect fits,\n    using a non-linear function to give higher scores to bins that are\n    significantly \"fuller\" after packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the\n        item are given a score of -1. Bins that can fit are scored based on\n        the *positive* remaining capacity after packing, favoring smaller\n        remaining capacities with a non-linear boost.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with low priority (-1)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity *after* packing\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n\n    if fitting_bins_indices.size > 0:\n        fitting_bins_capacities = bins_remain_cap[fitting_bins_indices]\n        resulting_remain_cap = fitting_bins_capacities - item\n\n        # Prioritize bins with minimal *positive* residual capacity.\n        # A simple inverse relationship: 1 / (resulting_remain_cap + epsilon)\n        # This gives higher scores to bins with smaller remaining capacity.\n        # To make it more discriminating and emphasize \"tightness\" more strongly,\n        # we can use a non-linear function. For instance, squaring the inverse\n        # or using an exponential decay. Let's try a concave function like\n        # log(1 + X) where X is related to tightness.\n        # A good heuristic is to maximize (bin_capacity_before_packing - resulting_remain_cap)\n        # which is `item` divided by the `resulting_remain_cap`. This can be unstable\n        # if resulting_remain_cap is close to zero.\n\n        # Let's prioritize bins that have the smallest `resulting_remain_cap`.\n        # A straightforward way to do this with higher scores for better fits:\n        # We want to maximize a score that is inversely related to `resulting_remain_cap`.\n        # Consider `large_value - resulting_remain_cap`.\n        # To make it more proportional to \"tightness\" and avoid issues with very small items\n        # fitting into very large bins, we can use the *proportion* of capacity filled.\n        # Proportion filled = `item / fitting_bins_capacities`.\n        # This doesn't directly address the *residual* capacity minimization.\n\n        # Let's focus on minimizing `resulting_remain_cap`.\n        # A score like `1 / (resulting_remain_cap + epsilon)` is good.\n        # To amplify the preference for tighter fits, we can use `1 / (resulting_remain_cap^p + epsilon)`\n        # where p > 1, or a similar non-linear transformation.\n        # Or, consider `max_possible_residual - resulting_remain_cap`, where `max_possible_residual`\n        # is the maximum residual capacity across all bins that can fit the item.\n        # This turns the problem into finding the bin with the minimum residual.\n\n        # Let's use a score that's proportional to the \"fullness\" achieved by the item.\n        # `item / fitting_bins_capacities` gives how much of the *current* space is used.\n        # However, we want to prioritize *least remaining space*.\n\n        # A strategy that prioritizes minimal *positive* residual capacity:\n        # For bins that can fit, calculate `resulting_remain_cap`.\n        # We want to assign higher scores to smaller `resulting_remain_cap`.\n        # Using `1 / (resulting_remain_cap + epsilon)` achieves this.\n        # To differentiate \"good\" fits from \"okay\" fits more strongly,\n        # we can apply a non-linear transformation.\n        # A simple approach is to square the inverse: `(1 / (resulting_remain_cap + epsilon))^2`.\n        # This will significantly boost bins with very small `resulting_remain_cap`.\n\n        epsilon = 1e-9\n        # Scores bins based on the inverse of their remaining capacity after packing.\n        # Higher scores for bins with less remaining capacity (tighter fits).\n        # We use `resulting_remain_cap` directly, as smaller is better.\n        # To map smaller to higher priority, we invert.\n        # A non-linear increase for smaller `resulting_remain_cap` can be achieved\n        # by using a power of the inverse. For instance, `1 / (resulting_remain_cap + epsilon)`.\n        # To further emphasize tightness, we can use `1.0 / (resulting_remain_cap + epsilon)`\n        # or a more aggressive function like `1.0 / np.sqrt(resulting_remain_cap + epsilon)`\n        # or `1.0 / (resulting_remain_cap**2 + epsilon)`.\n\n        # Let's consider `1.0 / (resulting_remain_cap + epsilon)`. This rewards smaller residuals.\n        # To make it more \"dynamic\" and sensitive to how full the bin becomes relative to its\n        # original state before the item, consider the proportion of space *used* by the item:\n        # `item / fitting_bins_capacities`.\n        # This gives a score between 0 and 1. Higher is better.\n        # If `resulting_remain_cap` is the same for two bins, the one with larger\n        # `fitting_bins_capacities` would have a smaller `item / fitting_bins_capacities` score.\n        # This is not what we want for minimizing residual capacity.\n\n        # Prioritize minimal *positive* residual capacity:\n        # score = C - resulting_remain_cap\n        # To make this non-negative and increase with \"tightness\":\n        # score = (max_potential_residual - resulting_remain_cap)\n        # If we consider max potential residual as some large constant, or the maximum\n        # residual among all bins.\n\n        # Let's use the inverse of the remaining capacity, but apply a transformation\n        # that emphasizes smaller values more.\n        # `score = 1 / (resulting_remain_cap + epsilon)`\n        # To make it more sensitive to small `resulting_remain_cap`, we can use:\n        # `score = 1 / np.sqrt(resulting_remain_cap + epsilon)`\n        # Or, inversely related to the *proportion* of capacity *remaining*:\n        # `proportion_remaining = resulting_remain_cap / fitting_bins_capacities`\n        # `score = 1 / (proportion_remaining + epsilon)`\n\n        # Let's go with a direct inverse of the remaining capacity, possibly with a non-linear boost.\n        # A simple non-linear boost: using `log(1 + X)` where X is related to tightness.\n        # Or, `1 / (residual + epsilon)` where we want smaller residual to give higher score.\n        # To give a *smoother* graduation and avoid extremely high scores for very small residuals,\n        # let's try `1.0 / (resulting_remain_cap + 1.0)`. This shifts the focus from very small\n        # residuals to a more balanced approach.\n\n        # Let's prioritize by the inverse of the remaining capacity, ensuring higher is better.\n        # The score should be higher for bins with smaller `resulting_remain_cap`.\n        # A simple way is `1 / (resulting_remain_cap + epsilon)`.\n        # To make it more discriminating for small `resulting_remain_cap`, we can use a form\n        # like `1.0 / np.sqrt(resulting_remain_cap + epsilon)`. This rewards tighter fits more\n        # aggressively than a linear inverse.\n        # This should provide the desired effect of prioritizing minimal *positive* residual capacity,\n        # with a non-linear boost for very tight fits.\n\n        priorities[fitting_bins_indices] = 1.0 / np.sqrt(resulting_remain_cap + epsilon)\n\n    return priorities",
    "response_id": 3,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 66.41714012534482,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response4.txt_stdout.txt",
    "code_path": "problem_iter7_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a strategy\n    that prioritizes bins with minimal *positive* residual capacity after packing.\n\n    This version aims to find the \"tightest\" fit by favoring bins that, after\n    packing the item, have the least remaining capacity. To differentiate between\n    multiple tight fits, it also considers the initial remaining capacity of the bin;\n    bins that were already quite full (less initial remaining capacity) are\n    slightly preferred if they result in a similar final small residual capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the\n        item are given a score of -1. Bins that can fit are scored based on\n        minimizing residual capacity, with a tie-breaker favouring bins that\n        were initially less capacious (more full).\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with low priority (-1)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity *after* packing\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n\n    if fitting_bins_indices.size > 0:\n        fitting_bins_capacities = bins_remain_cap[fitting_bins_indices]\n        resulting_remain_cap = fitting_bins_capacities - item\n\n        # Strategy: Prioritize bins with minimal *positive* residual capacity.\n        # This means we want to minimize `resulting_remain_cap`.\n        # To convert minimization to maximization for priority score, we can use\n        # a large constant minus the value, or the inverse.\n        # Using the inverse `1 / (resulting_remain_cap + epsilon)` gives higher scores\n        # for smaller `resulting_remain_cap`.\n\n        # Tie-breaker: If multiple bins have very similar `resulting_remain_cap`,\n        # prefer the one that was initially less capacious (meaning it was more full).\n        # This can be achieved by adding a term that is inversely proportional to\n        # the initial `fitting_bins_capacities` or directly using it.\n        # If we want to prefer smaller initial capacity, we can subtract it.\n        # `priority = 1 / (resulting_remain_cap + epsilon) - fitting_bins_capacities`\n        # This would mean a smaller initial capacity gives a lower (more negative)\n        # subtraction, resulting in a higher overall priority.\n        # However, we want a positive score. Let's rephrase:\n        # Primary score component: inverse of resulting capacity (higher is better)\n        # Secondary score component: inverse of initial capacity (higher is better, if we want to favor nearly full bins, which is not the case here).\n        # If we want to favor bins that were *already fuller*, we want to prefer bins with *smaller* `fitting_bins_capacities`.\n        # So, a simple score could be:\n        # `score = C - resulting_remain_cap - alpha * fitting_bins_capacities`\n        # where C and alpha are constants.\n        # A more direct way to prioritize minimal positive residual capacity:\n        # use `1 / (resulting_remain_cap + epsilon)` as the primary component.\n        # For the tie-breaker, to prefer bins that were initially *less* full (i.e., had *more* capacity),\n        # we would add a term proportional to `fitting_bins_capacities`.\n        # `priority = 1 / (resulting_remain_cap + epsilon) + alpha * fitting_bins_capacities`\n        # This still feels a bit counter to \"minimal positive residual capacity\" for *tighter* fits.\n\n        # Let's refine the \"tightest fit\" idea:\n        # We want `resulting_remain_cap` to be as small as possible.\n        # Primary goal: Minimize `resulting_remain_cap`.\n        # Secondary goal: Among those with minimal `resulting_remain_cap`, pick the one that was initially \"most full\" relative to its capacity.\n        # Initial fullness relative to capacity is `1 - fitting_bins_capacities / BinCapacity`.\n        # But we don't have BinCapacity. We only have `fitting_bins_capacities`.\n        # So, let's assume all bins have the same implicit original capacity (e.g., the maximum initial capacity seen or a standard bin size).\n        # If we assume a standard bin size `B`, then initial fullness is `(B - fitting_bins_capacities) / B`. Maximizing this means minimizing `fitting_bins_capacities`.\n\n        # Revised Strategy:\n        # 1. Prioritize minimal `resulting_remain_cap`. (Higher score for smaller `resulting_remain_cap`)\n        # 2. As a tie-breaker, prioritize bins that were initially *less* full (i.e., larger `fitting_bins_capacities`). This seems more aligned with the \"minimal *positive* residual capacity\" which implies we want to use up space effectively.\n\n        # Let's try:\n        # Primary: `1 / (resulting_remain_cap + epsilon)` (high score for small remaining cap)\n        # Secondary: `fitting_bins_capacities` (if two bins have same `resulting_remain_cap`, pick the one that started with more space, implying the item took a larger chunk of its current state). This is *not* what we want.\n\n        # The reflection asks for \"minimal *positive* residual capacity\". This means we want `resulting_remain_cap` to be small.\n        # \"Prioritize minimal *positive* residual capacity for tighter fits.\"\n        # This means, if a bin has remaining capacity of 0.1, it's a better fit than a bin with remaining capacity of 0.5.\n        # So, `1 / (resulting_remain_cap + epsilon)` is a good basis.\n\n        # \"Consider relative fullness and smooth, non-linear functions for graduated scores.\"\n        # Relative fullness: Let's consider `item / fitting_bins_capacities`. This measures how much of the *current available space* the item occupies. Higher is better for a tight fit.\n        # `priority = item / fitting_bins_capacities`\n        # This prioritizes larger items relative to the current bin capacity.\n        # If `item=7`, `bin_cap=10`, `resulting=3`. Score = 7/10 = 0.7\n        # If `item=7`, `bin_cap=100`, `resulting=93`. Score = 7/100 = 0.07\n        # This prioritizes the first case, which is good.\n        # But if `item=7`, `bin_cap=8`, `resulting=1`. Score = 7/8 = 0.875. This is even better.\n        # This directly aligns with minimizing `resulting_remain_cap` as well.\n\n        # Let's combine minimizing `resulting_remain_cap` and maximizing `item / fitting_bins_capacities`.\n        # If `resulting_remain_cap` is minimal, `item / fitting_bins_capacities` is maximal. They are inversely related for a fixed item.\n        # So, `item / fitting_bins_capacities` seems like a good candidate.\n        # However, we want a graduated score that doesn't just depend on this ratio.\n        # Let's stick to prioritizing minimal residual capacity as the primary goal.\n        # `priority_component_1 = 1 / (resulting_remain_cap + epsilon)`\n\n        # For the \"graduated scores\" and \"relative fullness\", let's consider how much \"space is left unused\" relative to the item size.\n        # `unutilized_space = resulting_remain_cap`\n        # `priority_component_2 = 1 / (unutilized_space + epsilon)` - this is already covered by component 1.\n\n        # What about relative fullness of the bin *before* packing?\n        # `relative_fullness_before = 1 - fitting_bins_capacities / B` (assuming a Bin Capacity B)\n        # Or, relative to its current capacity: `item / fitting_bins_capacities`\n        # This measure `item / fitting_bins_capacities` is good for identifying items that fill a significant portion of their *current* space.\n\n        # Let's try to blend:\n        # Prioritize small `resulting_remain_cap` (good for tightest fit).\n        # Also, consider the \"efficiency\" of the fit: `item / fitting_bins_capacities`.\n        # A bin that is almost full (small `fitting_bins_capacities`) and can accommodate the item well (low `resulting_remain_cap`) is ideal.\n\n        # Consider the score as a combination of how small the remaining capacity is, and how much of the bin's current capacity was used.\n        # Score = `alpha * (1 / (resulting_remain_cap + epsilon)) + beta * (item / fitting_bins_capacities)`\n        # Where `alpha` and `beta` are weights.\n        # If we set `alpha=1` and `beta=1`, we are adding these two metrics.\n        # This would strongly favor bins with small `resulting_remain_cap`, and among those, favor bins where the item occupied a larger proportion of the current space.\n\n        epsilon = 1e-9\n        # Metric 1: Inverse of remaining capacity (prioritizes smallest residual space)\n        score_residual = 1.0 / (resulting_remain_cap + epsilon)\n\n        # Metric 2: Proportion of current capacity filled by the item (prioritizes items that take a larger chunk of the bin's current available space)\n        score_proportion = item / (fitting_bins_capacities + epsilon)\n\n        # Combine them. We want higher scores for better fits.\n        # Both metrics point towards better fits when they are higher.\n        # A simple sum or average can be used. Let's try sum for stronger emphasis on both.\n        priorities[fitting_bins_indices] = score_residual + score_proportion\n\n    return priorities",
    "response_id": 4,
    "obj": 4.038691663342641,
    "cyclomatic_complexity": 2.0,
    "halstead": 108.41805003750011,
    "exec_success": true
  }
]