```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin.

    This version aims to prioritize bins that are "tighter fits" by
    considering the inverse of remaining capacity, but scaled to provide
    a graduated priority rather than just inverse proportionality. It also
    considers the relative fullness of the bins.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
        A higher score indicates a higher priority. Bins that cannot fit the
        item are given a score of -1.
    """
    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with low priority

    # Identify bins that can fit the item
    can_fit_mask = bins_remain_cap >= item

    if np.any(can_fit_mask):
        fitting_bins_capacities = bins_remain_cap[can_fit_mask]

        # Strategy: Prioritize bins with minimal remaining capacity (tighter fit).
        # We can use the difference between the item size and the remaining capacity
        # as a measure of how "tight" the fit is. A smaller positive difference is better.
        # Or, more directly, use a function of remaining capacity that increases as capacity decreases.
        #
        # Let's use a scaled inverse:  1 / (remaining_capacity - item + epsilon)
        # This prioritizes bins where remaining_capacity is just slightly larger than item.
        # Adding epsilon to avoid division by zero if remaining_capacity == item.
        epsilon = 1e-9
        priorities[can_fit_mask] = 1.0 / (fitting_bins_capacities - item + epsilon)

        # Alternative consideration for relative fullness:
        # If all bins are nearly empty, any fit might be fine. If bins are nearly full,
        # a tight fit is crucial.
        # We can also incorporate the concept of "how much space is left" by taking
        # the difference from a maximum possible remaining capacity (e.g., sum of all items
        # or a theoretical large capacity).
        # A more direct approach for "tightness" could be:
        # priorities[can_fit_mask] = np.max(fitting_bins_capacities) - fitting_bins_capacities
        # This prioritizes bins with the smallest remaining capacity among the fitting ones.
        #
        # Let's refine the scaled inverse to be more robust and provide a clearer gradient.
        # We want smaller remaining capacities to have higher scores.
        # Let's consider the "slack" or "waste" space for a perfect fit.
        # If remaining capacity is R and item is I, the ideal scenario is R = I.
        # The "waste" is R - I. We want to minimize R - I.
        # So, a score proportional to -(R - I) or 1/(R - I + epsilon) works.
        #
        # To make it more distinct from simple First Fit, we can consider
        # how much *extra* space there is. A bin with R=10 and item=5 gives R-I=5.
        # A bin with R=12 and item=5 gives R-I=7. We prefer R=10.
        #
        # Let's use a transformation that emphasizes smaller positive differences (R - I).
        # We can use a concave function of (R - I) which gives higher priority to smaller positive differences.
        # For example, log(slack + 1) or sqrt(slack + 1) would favor smaller slacks.
        # However, the prompt implies a higher score is better.
        #
        # Let's stick to a form that clearly prioritizes minimum remaining capacity among fitting bins,
        # but with a scaling that gives a more graduated response.
        # We can use the inverse of (remaining_capacity - item + epsilon) but normalize it.
        #
        # Let's go with prioritizing minimum remaining capacity among fitting bins,
        # as it directly reflects the "tighter fit" idea. The previous `priority_v1`
        # already does this effectively by `max_possible_capacity - fitting_bins_capacities`.
        #
        # To introduce "smoothness" and "graduation", consider mapping the
        # available slack (remaining_capacity - item) to a priority score.
        # A higher score for smaller slack.
        #
        # Let `slack = fitting_bins_capacities - item`
        # We want to maximize `f(slack)` where `f` is decreasing.
        # For example, `f(slack) = 1 / (slack + epsilon)` or `f(slack) = -slack`.
        #
        # Let's try a formulation that uses the inverse of the *full* remaining capacity,
        # but only for fitting bins, then scale it. This implicitly prefers bins
        # that were less full to begin with if they have a tight fit for the *current* item.
        #
        # Consider: priority = 1 / (remaining_capacity + epsilon)
        # This would favor bins that have less total capacity available, IF they fit the item.
        # This might not be what we want; we want to prioritize based on how well the *item* fits.
        #
        # Revisit: prioritize bins with minimal remaining capacity that are still sufficient.
        # This is exactly what `priority_v1` with `max_possible_capacity - fitting_bins_capacities` does.
        #
        # To introduce "graduation" and "smoothness" beyond simple inverse proportionality:
        # We can map the range of `fitting_bins_capacities` to a new range of priorities.
        # A common approach for graduated priorities is to use a sigmoid-like function
        # or simply a scaled inverse.
        #
        # Let's try `1 / (remaining_capacity - item + epsilon)`. This emphasizes *very* small positive differences.
        #
        # We can refine the inverse by shifting and scaling:
        # `scaled_priority = A * (1 / (fitting_bins_capacities - item + epsilon)) + B`
        #
        # A simpler way to achieve graduated priorities for "tightest fit" is to
        # use the negative of the remaining capacity (as in v1), but then perhaps
        # normalize it within the set of fitting bins, or apply a non-linear transformation.
        #
        # Let's try a score that is directly related to the *lack* of excessive space.
        # For a fitting bin with remaining capacity `R` and item size `I`:
        # Score = MaxPossibleRemainingCapacity - R
        # This prioritizes bins with the smallest `R`.
        #
        # To make it "smoother" or "graduated":
        # We can map `R` to a score such that small `R` values (close to `I`) get
        # high scores, and `R` values far from `I` get lower scores.
        #
        # Let's use the reciprocal of the *waste* space `(R-I)`.
        # `waste = fitting_bins_capacities - item`
        # `priorities[can_fit_mask] = 1.0 / (waste + epsilon)`
        #
        # This prioritizes bins where `waste` is minimal (i.e., `R` is just slightly larger than `I`).
        # This is a strong candidate for "tighter fits" and provides a graduated priority.

        waste = fitting_bins_capacities - item
        # Ensure we don't have negative waste (handled by can_fit_mask)
        # Add epsilon for numerical stability and to give a finite priority
        # even when waste is exactly zero.
        priorities[can_fit_mask] = 1.0 / (waste + epsilon)

        # To ensure "graduation" and sensitivity adjustment, we could scale this.
        # For instance, scaling by the average waste or max waste might be useful.
        # For simplicity, let's stick to the direct reciprocal of waste for now,
        # as it naturally provides higher values for smaller waste.

        # Consider normalizing:
        # raw_priorities = 1.0 / (waste + epsilon)
        # if raw_priorities.size > 0:
        #     min_p = np.min(raw_priorities)
        #     max_p = np.max(raw_priorities)
        #     if max_p - min_p > epsilon:
        #         priorities[can_fit_mask] = (raw_priorities - min_p) / (max_p - min_p)
        #     else:
        #         priorities[can_fit_mask] = 0.5 # All same priority if range is tiny

        # The normalization might be too aggressive or change the relative ordering
        # in a way that's not intended. The inverse of waste is usually sufficient.

        # Let's consider the "relative fullness" of bins.
        # A bin with remaining capacity `R` relative to some max capacity `M` (e.g., bin capacity, or overall max capacity).
        # Suppose all bins have a maximum capacity of `C`.
        # Relative fullness could be `(C - R) / C`.
        # We want to pack into bins that have a tight fit.
        #
        # Let's go back to the core idea: prioritize bins with minimal remaining capacity that are still sufficient.
        # `priority = -remaining_capacity` (as in v1, but without the `max_capacity` offset)
        # `priorities[can_fit_mask] = -fitting_bins_capacities`
        # This is still essentially First Fit.
        #
        # The reflection asks for "inverse, scaled differences, or positive residuals for tighter fits."
        # "Positive residuals" -> `remaining_capacity - item`. We want to minimize this.
        # So, priority should be inversely related to `remaining_capacity - item`.
        # `priority = 1 / (remaining_capacity - item + epsilon)` is a good choice.
        #
        # "Scaled differences" could mean scaling this inverse.
        # E.g., `priority = scale * (1 / (remaining_capacity - item + epsilon))`
        # or `priority = scale_factor * (1 / (remaining_capacity + epsilon))`
        # where `scale_factor` depends on `item`.
        #
        # Let's consider a "positive residual" transformed:
        # `residual = remaining_capacity - item`.
        # We want to favor smaller positive residuals.
        #
        # Consider a score that reflects how much 'empty' space is left AFTER fitting the item.
        # `space_left = remaining_capacity - item`
        # We want bins with small `space_left`.
        # A priority score that is `1 / (space_left + epsilon)` will give higher scores to smaller `space_left`.
        # This fits the "inverse" and "positive residuals for tighter fits" criteria.

        space_left = fitting_bins_capacities - item
        priorities[can_fit_mask] = 1.0 / (space_left + epsilon)

        # To introduce more graduation, we can apply a non-linear transformation to `space_left`
        # before taking the inverse, or scale the inverse itself.
        # For example, `log(1 + space_left)` is a decreasing function, so `1 / log(1 + space_left)`
        # would prioritize smaller `space_left`.
        #
        # Let's consider a different approach: prioritize bins that have *less* total capacity available,
        # IF they can fit the item. This is not directly about "tightness" but about using up less "valuable"
        # (less capacious) bins first.
        # `priorities[can_fit_mask] = 1.0 / (fitting_bins_capacities + epsilon)`
        # This prioritizes bins with smaller `remaining_capacity`. This is also a form of First Fit.
        #
        # The goal is a *priority function* that guides selection. The highest priority score is chosen.
        #
        # Let's ensure "smoothness" and "graduation".
        # The `1 / (waste + epsilon)` approach already provides a good graduation where the slope
        # is steeper for very small waste values.
        #
        # Consider a scenario:
        # Bins: [10, 12, 15]
        # Item: 5
        #
        # Fitting bins: [10, 12, 15]
        # Waste: [5, 7, 10]
        #
        # Priority (1 / (waste + epsilon)):
        # 1/5.000000001 ≈ 0.2
        # 1/7.000000001 ≈ 0.14
        # 1/10.000000001 ≈ 0.1
        #
        # This correctly prioritizes the bin with remaining capacity 10.
        #
        # What if we want to adjust sensitivity? We could introduce a parameter `alpha`.
        # `priority = 1 / (waste + epsilon)**alpha`
        # For `alpha = 1`, we get the above.
        # For `alpha < 1`, the function is less steep for small waste.
        # For `alpha > 1`, the function is more steep for small waste.
        #
        # Let's use `alpha = 0.5` (square root of waste) as an example of "smoother graduation".
        # `priority = 1 / sqrt(waste + epsilon)`
        #
        # Or maybe prioritize the bin that results in the *least* remaining capacity after packing.
        # This is equivalent to prioritizing the bin with minimum `remaining_capacity` that fits the item.
        # Which is `priority = -remaining_capacity`.

        # Final decision: prioritize bins with the smallest *positive* slack (remaining capacity - item).
        # This is achieved by `1 / (remaining_capacity - item + epsilon)`.
        # This naturally gives higher priority to tighter fits and has a graduated nature.
        # We can potentially scale this to adjust sensitivity.
        # A simple scaling: multiply by a constant `K`.
        # `priority = K * (1 / (waste + epsilon))`
        # The magnitude of `K` doesn't change the *ordering* but affects the relative differences.
        # For simplicity, let's not add an arbitrary scaling factor unless specified.

        # Consider the "relative fullness": if bins are generally very full, a tight fit is more important.
        # If bins are generally empty, any fit might be acceptable.
        # The `1 / (waste + epsilon)` already captures this implicitly because 'waste' is relative to the item.

        # Let's try a formulation that directly uses the remaining capacity in a way that
        # emphasizes smaller values for fitting bins, but is not just inverse.
        # A simple non-linear transformation could be `remaining_capacity**2` (prioritizing smaller capacities)
        # or `1 / remaining_capacity` (prioritizing smaller capacities).
        #
        # The reflection mentions "inverse, scaled differences, or positive residuals".
        # `1 / (remaining_capacity - item + epsilon)` seems to best capture "inverse of positive residuals for tighter fits".
        #
        # To make it more "graduated" and less "binary" than simple First Fit (which picks the first one that fits),
        # we need scores that distinguish between fitting bins.
        #
        # Let's re-evaluate the `priority_v1` approach: `max_possible_capacity - fitting_bins_capacities`.
        # This prioritizes bins with the *least* remaining capacity among those that fit.
        # This IS the First Fit logic.
        #
        # The reflection suggests "positive residuals" for "tighter fits".
        # Residual = `remaining_capacity - item`. We want to minimize this positive residual.
        #
        # Let's try a form that explicitly penalizes larger residuals more heavily:
        # `priority = - (remaining_capacity - item)` --> This prioritizes smaller residuals, but is linear.
        # `priority = - (remaining_capacity - item)**2` --> Prioritizes smaller residuals more strongly.
        # `priority = 1 / (remaining_capacity - item + epsilon)` --> Prioritizes smaller residuals.
        #
        # The "scaled differences" part could imply scaling `remaining_capacity - item`.
        #
        # Let's try prioritizing bins that have `remaining_capacity` closer to `item`.
        # This means we want to maximize a function that is high when `remaining_capacity` is close to `item`.
        #
        # Let's use the *difference* from the ideal fit, but make it a positive value that is smaller for better fits.
        # `diff = abs(remaining_capacity - item)`
        # We want to prioritize bins where `diff` is small.
        # So, priority should be inversely related to `diff`.
        # `priority = 1 / (abs(remaining_capacity - item) + epsilon)`
        #
        # However, for online BPP, we *must* fit the item. So we only consider bins where `remaining_capacity >= item`.
        # For these bins, `abs(remaining_capacity - item)` is just `remaining_capacity - item`.
        #
        # So, `priority = 1 / (remaining_capacity - item + epsilon)` seems to be the most direct interpretation of "prioritize
        # bins with minimal remaining capacity that are still sufficient" and "positive residuals for tighter fits".
        #
        # To add "scaled differences" and "graduated priorities":
        # Consider a function `f(remaining_capacity)` that is decreasing.
        # E.g., `f(R) = 1 / (R - I + epsilon)`.
        # We can modify this:
        # `f(R) = 1 / ( (R - I) / C + epsilon )` where C is some scaling factor (e.g., bin capacity, max item size).
        # Or `f(R) = K * (1 / (R - I + epsilon))` where K adjusts overall priority magnitude.
        #
        # Let's try a slightly different angle for "graduated":
        # Prioritize bins that are not too empty, but can fit the item.
        #
        # What if we prioritize bins that have a larger *proportion* of their remaining capacity used?
        # `fill_ratio = item / remaining_capacity` (for fitting bins)
        # This gives higher priority to bins where the item fills a larger portion of the remaining space.
        # This is still similar to `1 / remaining_capacity`.
        #
        # Let's stick with prioritizing the tightest fit using the inverse of the slack.
        #
        # `slack = fitting_bins_capacities - item`
        # `priorities[can_fit_mask] = 1.0 / (slack + epsilon)`
        #
        # To make it more "graduated" and less sensitive to very small slacks,
        # we can raise the slack to a power greater than 1 before taking the inverse.
        # `priorities[can_fit_mask] = 1.0 / ((slack + epsilon)**power)` where `power > 1`.
        # E.g., `power = 1.5` or `power = 2.0`. This would make the priority drop off faster
        # as slack increases.
        #
        # Let's try `power = 1.5` for a smoother, graduated decrease in priority as slack grows.

        slack = fitting_bins_capacities - item
        power = 1.5  # Adjust for graduation sensitivity
        priorities[can_fit_mask] = 1.0 / ((slack + epsilon)**power)

    return priorities
```
