```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin. A Feynman-inspired heuristic.

    This heuristic attempts to balance exploration and exploitation. We want to find a good fit quickly,
    but also consider other bins to avoid getting stuck in local optima (a deep potential well, as I might say).

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # 1. Basic Feasibility: Rule out bins that are too small. This is like applying a hard energy cutoff.
    feasible_bins = bins_remain_cap >= item

    # If there are no feasible bins, signal that by returning -inf for all priorities
    if not np.any(feasible_bins):
        return np.full_like(bins_remain_cap, -np.inf, dtype=float)

    # 2. Feynman Path Integral (Sort Of): Consider all possible paths (bins) but weigh them differently.
    # The closer the item size is to the bin capacity, the better (exploitation). Think of it as minimizing the "action"
    # (wasted space). We also add a small exploration bonus to avoid purely greedy approaches.
    space_wasted = bins_remain_cap - item
    space_wasted[space_wasted < 0] = np.inf  # Penalize infeasible bins strongly after the hard cut.

    # This part is like the exponential of the negative action. smaller space wasted means higher "amplitude" or priority
    fit_priority = np.exp(-space_wasted)

    # 3. Encourage Balanced Bin Usage: Give a slight bonus to bins that are relatively empty.
    #   This is kind of like trying to minimize entanglement between the bins - we don't want a few bins hogging everything.
    #   We need to normalize it to scale the relative weights.

    normalized_capacities = bins_remain_cap / np.max(bins_remain_cap)  # Range [0, 1]
    exploration_bonus = 0.1 * normalized_capacities

    # 4. Combine all aspects
    priorities = feasible_bins * (fit_priority + exploration_bonus)

    # Re-scale, since numpy complains if some of them are 0 and it cannot normalize it
    # to have unit vector size
    if np.sum(priorities) == 0:
        return priorities
    return priorities
```
