```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Softmax-Based Fit strategy.

    This strategy prioritizes bins that can accommodate the item and have a remaining capacity
    that, after placing the item, leaves a "good fit". The "good fit" is approximated by
    considering the relative "tightness" or "looseness" of the remaining capacity.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # We only consider bins that can fit the item
    possible_fits = bins_remain_cap >= item
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Calculate the "fit score" for bins that can accommodate the item
    # A smaller remaining capacity after placing the item is generally better (tighter fit).
    # We can use the inverse of the remaining capacity after placement as a raw score.
    # To avoid division by zero or very small numbers, we add a small epsilon.
    # The softmax function will normalize these scores.
    epsilon = 1e-9
    remaining_capacities_after_fit = bins_remain_cap[possible_fits] - item
    
    # A simple approach: higher priority for bins that leave a smaller remainder
    # To avoid issues with zero remainder, we can transform it. 
    # A simple transformation that favors smaller remainders could be 1 / (remainder + epsilon)
    # Or more robustly, we can think of it as "how much capacity is left unused".
    # A common heuristic for "good fit" in bin packing is "best fit", which tries to minimize
    # the leftover space. So, we want to assign higher priority to bins with smaller `remaining_capacities_after_fit`.
    
    # Let's consider the remaining capacity *after* placing the item.
    # A smaller remaining capacity after placing the item implies a tighter fit.
    # We can map smaller remaining capacities to higher values for prioritization.
    # One way is to use a negative exponential of the remaining capacity.
    # Another simpler way is to use the negative of the remaining capacity.
    
    # Option 1: Simple negative remaining capacity. Larger negative means tighter fit.
    # However, softmax requires non-negative values typically.
    
    # Option 2: Map smaller remaining capacities to larger positive scores.
    # Let's use something like: max_remainder - current_remainder + epsilon
    # This ensures all values are positive and the tightest fit gets the highest score.
    
    if np.any(possible_fits):
        # Get the remaining capacities for bins that can fit the item
        current_remaining_after_fit = bins_remain_cap[possible_fits] - item
        
        # To get higher priority for smaller remainders, we can use a transformation
        # like: 1 / (remainder + epsilon) or by inverting it with a shift.
        # A simple score that increases as `current_remaining_after_fit` decreases:
        # Let's use a score proportional to the negative of the remaining capacity
        # after fitting, but offset to be positive.
        # A common practice is to use the log of the inverse of the capacity,
        # or simply the negative of the remaining capacity if we want a direct relationship.
        
        # Let's try a score that emphasizes a tighter fit.
        # If the remaining capacity after fitting is R, a smaller R is better.
        # We can assign a score proportional to some function of 1/R or -R.
        # Let's use `1.0 / (current_remaining_after_fit + epsilon)` as a raw score.
        # This favors bins with smaller leftovers.
        raw_scores = 1.0 / (current_remaining_after_fit + epsilon)

        # Softmax requires values that can be exponentiated.
        # Softmax ensures that the priorities sum to 1 (or are normalized such that their
        # exponential sum is used for probabilistic choice).
        # For a direct priority score (higher is better, not necessarily probabilistic),
        # we can still use the softmax function to "soften" the preference.

        # Let's make the scores directly proportional to how "good" the fit is.
        # A perfect fit (remainder 0) should be highest.
        # A very loose fit should be lowest.
        # The value `current_remaining_after_fit` is the space left.
        # Smaller means better fit.
        # Let's use a score that is high for small `current_remaining_after_fit`.
        # For example, `max_capacity_of_a_bin - current_remaining_after_fit` (if `max_capacity_of_a_bin` is known and constant).
        # Or, simply transform `current_remaining_after_fit` so smaller values yield larger scores.
        # Let's use `np.exp(-current_remaining_after_fit)` for a soft preference.
        # This assigns higher values to smaller remaining capacities.
        
        # Alternative: Using the difference between remaining capacity and item size.
        # A smaller difference means a better fit.
        # Let's assign priority based on the *inverse* of the remaining capacity after packing.
        # This means smaller remaining capacity gets a higher score.
        # score = 1 / (bins_remain_cap - item + epsilon) for fitting bins.

        # Let's use a score based on how close the remaining capacity *after* placing the item is to zero.
        # Specifically, we want to maximize the likelihood of choosing a bin that leaves
        # minimal remaining space.
        # A simple approach: `- (bins_remain_cap[possible_fits] - item)`. This will be negative.
        # We need positive values for softmax.
        # Let's use `bins_remain_cap[possible_fits] - item` as a measure of "waste". We want to minimize waste.
        # So, higher priority should go to smaller waste.
        
        # Let's define a "desirability" score. A higher score means more desirable.
        # Desirability is high when `bins_remain_cap - item` is small.
        # So, let's try a linear transformation: `max_possible_remainder - (bins_remain_cap[possible_fits] - item)`.
        # However, `max_possible_remainder` isn't readily available or fixed.
        # A simpler approach that works with softmax is to use values that have a clear ordering.
        
        # Let's consider the 'gap' after fitting the item.
        # `gap = bins_remain_cap[possible_fits] - item`
        # We want to prioritize bins with small `gap`.
        # Using `np.exp(-gap)` makes smaller gaps yield larger exponential values.
        
        # A more direct "priority" might be related to how much space is left *relative* to the bin's capacity,
        # but here we focus on fitting the *item* into the *remaining capacity*.
        
        # Let's use the raw remaining capacity *after* placing the item.
        # We want to select bins where this value is minimal.
        # We can transform `bins_remain_cap[possible_fits] - item` into a priority.
        # A common approach is `1 / (x + epsilon)` where `x` is the value to minimize.
        # So, `1.0 / (bins_remain_cap[possible_fits] - item + epsilon)`.
        
        # To implement "Softmax-Based Fit", we can consider how "full" the bin becomes.
        # The remaining capacity after fitting is `r_new = bins_remain_cap[possible_fits] - item`.
        # We want to maximize the "fit", which means minimizing `r_new`.
        # A good heuristic for Softmax-Based Fit would be to assign higher probabilities
        # to bins with smaller `r_new`.
        # So, we can use `exp(-r_new)` or `exp(max_possible_r_new - r_new)`.
        
        # Let's try a score that is inversely proportional to the remaining capacity.
        # `score = 1.0 / (bins_remain_cap[possible_fits] - item + epsilon)`
        # This gives higher scores to tighter fits (smaller remaining capacity).

        # Let's use a function that maps smaller remaining capacities to larger values.
        # Consider `score = exp(- (bins_remain_cap[possible_fits] - item))`.
        # This maps 0 remaining capacity to exp(0)=1, and larger remaining capacities to smaller values.
        # This is a reasonable preference for 'best fit'.
        
        fit_values = bins_remain_cap[possible_fits] - item
        
        # To ensure all values for softmax are positive and generally represent preference:
        # Let's map smaller `fit_values` to larger raw scores.
        # For example, we can use a "utility" function.
        # `utility = C - fit_value` where C is a large constant, or `1 / (fit_value + epsilon)`.
        
        # Let's use the reciprocal of the remaining capacity.
        # A smaller remaining capacity (tighter fit) will yield a larger reciprocal.
        raw_scores = 1.0 / (fit_values + epsilon)
        
        # Apply softmax to get the probabilities/priorities.
        # Softmax function: exp(x_i) / sum(exp(x_j))
        # For priority scores that we want to be directly comparable and higher means better,
        # we can use the exponential of the scores. Softmax is often used for
        # probabilistic selection, but here we can interpret the output of exp(score)
        # as the raw priority before normalization.
        
        # For this problem, the request is for "priority score", not necessarily probabilities.
        # So, we can return the values that would go into the exponent of softmax.
        # These are the scores where higher values mean higher priority.
        # We already computed `raw_scores` where higher is better.
        
        # Let's simply use the reciprocal of the remaining capacity as a direct priority score.
        # Larger score means better bin.
        priorities[possible_fits] = raw_scores
        
    return priorities
```
