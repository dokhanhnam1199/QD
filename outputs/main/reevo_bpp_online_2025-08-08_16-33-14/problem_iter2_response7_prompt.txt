{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    The Worst Fit strategy aims to place the item in the bin with the largest remaining capacity.\n    This heuristic is generally not good for minimizing the number of bins, but it's a valid strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to zero\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # For each bin, if the item can fit, assign a priority equal to the remaining capacity.\n    # Otherwise, assign a priority of 0, meaning it cannot be placed in that bin.\n    # The higher the remaining capacity, the higher the priority for Worst Fit.\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask]\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score prioritizes bins that are a \"good fit\" for the item.\n    A good fit is defined as a bin where the remaining capacity is slightly larger\n    than the item's size. This strategy aims to minimize wasted space in bins.\n\n    The sigmoid function is used to model this \"good fit\" concept. The function\n    will have a higher output when (bins_remain_cap - item) is close to zero,\n    and lower outputs as the difference increases (both positive and negative).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We want to maximize the score for bins where bins_remain_cap is just slightly\n    # larger than item. This means (bins_remain_cap - item) should be close to 0.\n    # The sigmoid function `1 / (1 + exp(-x))` has its steepest slope around x=0.\n    # To make the \"peak\" of our priority function align with the best fit,\n    # we can use `bins_remain_cap - item` as the input to the sigmoid.\n    # However, we need to ensure that we only consider bins where the item can actually fit.\n\n    # Initialize priorities to a very low value (negative infinity conceptually)\n    # for bins that cannot accommodate the item.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the difference between remaining capacity and item size for feasible bins\n    fit_diff = bins_remain_cap[can_fit_mask] - item\n\n    # Apply the sigmoid function. We want the peak at fit_diff = 0.\n    # A common choice for scaling and shifting the sigmoid is `a * x`.\n    # Let's use a scaling factor `k` to control the steepness of the sigmoid.\n    # A larger `k` means a sharper peak.\n    k = 2.0  # Tuning parameter for sigmoid steepness\n\n    # The sigmoid function: 1 / (1 + exp(-k * x))\n    # Here, `x` is our `fit_diff`.\n    # A positive difference (bin has more capacity than needed) is okay,\n    # but a slightly larger difference is less preferred than a perfect fit.\n    # A negative difference (item doesn't fit) is handled by the -inf initialization.\n\n    # We can use the `sigmoid` function directly.\n    # An alternative way to think about it is to map `fit_diff` to values\n    # where the sigmoid is \"interesting\".\n    # The sigmoid function itself is `1 / (1 + exp(-x))`.\n    # We want high scores when `fit_diff` is small.\n    # So, we can use `sigmoid(k * (-fit_diff))`. This makes the peak\n    # when `fit_diff` is 0.\n    # `sigmoid(-k * fit_diff)` will be high for small `fit_diff` (close to 0)\n    # and lower for larger `fit_diff`.\n\n    # Let's try to directly map `fit_diff` to a score.\n    # A simple sigmoid `1 / (1 + exp(-x))` ranges from 0 to 1.\n    # We can also consider `2 * sigmoid(x) - 1` to range from -1 to 1, or other transformations.\n    # For \"fit score\", we probably want higher values for better fits.\n    # Let's stick to the standard sigmoid and interpret higher values as better.\n    # We want to maximize `priority`.\n    # `fit_diff` represents the \"slack\" or wasted space. We want this slack to be minimal.\n    # So, for `fit_diff` close to 0, the priority should be high.\n    # For `fit_diff` much larger than 0, the priority should be lower.\n\n    # Using `np.exp(-k * fit_diff)` will be close to 1 when `fit_diff` is large,\n    # leading to a small sigmoid output. It will be close to 0 when `fit_diff` is small,\n    # leading to a sigmoid output close to 1. This is exactly what we want.\n    # So the sigmoid score will be `1 / (1 + np.exp(-k * fit_diff))`.\n\n    # Ensure we don't encounter overflow with `np.exp` for very large negative `fit_diff`\n    # (though this case is already handled by the -inf initialization).\n    # Also, for very large positive `fit_diff`, `exp(-k*fit_diff)` becomes very small,\n    # close to 0, making sigmoid close to 1. This might be counter-intuitive if we want\n    # \"best fit\" to mean least waste. A perfect fit (diff=0) should have highest score.\n    # A bin that is much larger might be less desirable than a bin that is just right.\n\n    # Let's consider a different sigmoid application:\n    # We want a peak when `fit_diff` is minimal (i.e., close to 0).\n    # The function `exp(-(fit_diff)^2)` has a peak at 0. We can normalize this.\n    # Or, we can use sigmoid applied to `-abs(fit_diff)`.\n\n    # Let's refine the sigmoid idea for \"good fit\".\n    # A \"good fit\" means `bins_remain_cap` is slightly larger than `item`.\n    # So, `bins_remain_cap - item` should be small and positive.\n    # If `bins_remain_cap - item` is negative, the item doesn't fit.\n    # If `bins_remain_cap - item` is very large, it's a bad fit (lots of waste).\n    # If `bins_remain_cap - item` is close to zero, it's a good fit.\n\n    # Strategy:\n    # 1. Bins where item doesn't fit get a very low score (-inf).\n    # 2. Bins where item fits:\n    #    - Score is high if `bins_remain_cap - item` is small (close to 0).\n    #    - Score decreases as `bins_remain_cap - item` increases.\n\n    # This suggests a sigmoid centered around 0, but we only care about positive differences.\n    # Let's consider `score = sigmoid(k * (item - bins_remain_cap[can_fit_mask]))`.\n    # If `item - bins_remain_cap` is close to 0 (meaning `bins_remain_cap` is close to `item`),\n    # the argument to sigmoid is close to 0, giving a high sigmoid value (around 0.5 or higher).\n    # If `item - bins_remain_cap` is large negative (meaning `bins_remain_cap` is much larger than `item`),\n    # the argument is large positive, sigmoid is close to 1. This is NOT what we want.\n\n    # Let's reconsider the `1 / (1 + exp(-x))` structure.\n    # If we use `bins_remain_cap - item` as `x`, then:\n    #   - `fit_diff` close to 0 -> `exp(0) = 1` -> `1 / (1 + 1) = 0.5`\n    #   - `fit_diff` large positive -> `exp(large_pos)` -> `1 / (1 + very_large)` -> close to 0\n    #   - `fit_diff` large negative -> `exp(large_neg)` -> `1 / (1 + very_small)` -> close to 1\n    # This means the highest score is for bins that are WAY too big. We need to invert this logic.\n\n    # How about `score = 1 / (1 + exp(k * fit_diff))`?\n    #   - `fit_diff` close to 0 -> `exp(0) = 1` -> `1 / (1 + 1) = 0.5` (Good)\n    #   - `fit_diff` large positive -> `exp(large_pos)` -> `1 / (1 + very_large)` -> close to 0 (Bad, too much waste)\n    #   - `fit_diff` large negative -> `exp(large_neg)` -> `1 / (1 + very_small)` -> close to 1 (Bad, doesn't fit, already handled by -inf)\n\n    # This looks like a reasonable approach for \"Sigmoid Fit Score\".\n    # The highest score is achieved when `fit_diff` is zero.\n    # As `fit_diff` increases (more waste), the score decreases.\n\n    # Calculate priorities for bins that can fit the item\n    scores_for_fitting_bins = 1 / (1 + np.exp(k * fit_diff))\n\n    # Assign these calculated scores back to the appropriate positions in the priorities array\n    priorities[can_fit_mask] = scores_for_fitting_bins\n\n    return priorities\n\n[Reflection]\nPrioritize bins with minimal slack. Tailor function shape to desired fit.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}