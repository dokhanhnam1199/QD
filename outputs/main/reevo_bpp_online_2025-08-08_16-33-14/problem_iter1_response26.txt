```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Sigmoid Fit Score.

    The Sigmoid Fit Score prioritizes bins that are a "good fit" for the item.
    A good fit is defined as a bin where the remaining capacity is slightly larger
    than the item's size. This strategy aims to minimize wasted space in bins.

    The sigmoid function is used to model this "good fit" concept. The function
    will have a higher output when (bins_remain_cap - item) is close to zero,
    and lower outputs as the difference increases (both positive and negative).

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # We want to maximize the score for bins where bins_remain_cap is just slightly
    # larger than item. This means (bins_remain_cap - item) should be close to 0.
    # The sigmoid function `1 / (1 + exp(-x))` has its steepest slope around x=0.
    # To make the "peak" of our priority function align with the best fit,
    # we can use `bins_remain_cap - item` as the input to the sigmoid.
    # However, we need to ensure that we only consider bins where the item can actually fit.

    # Initialize priorities to a very low value (negative infinity conceptually)
    # for bins that cannot accommodate the item.
    priorities = np.full_like(bins_remain_cap, -np.inf)

    # Identify bins where the item can fit
    can_fit_mask = bins_remain_cap >= item

    # Calculate the difference between remaining capacity and item size for feasible bins
    fit_diff = bins_remain_cap[can_fit_mask] - item

    # Apply the sigmoid function. We want the peak at fit_diff = 0.
    # A common choice for scaling and shifting the sigmoid is `a * x`.
    # Let's use a scaling factor `k` to control the steepness of the sigmoid.
    # A larger `k` means a sharper peak.
    k = 2.0  # Tuning parameter for sigmoid steepness

    # The sigmoid function: 1 / (1 + exp(-k * x))
    # Here, `x` is our `fit_diff`.
    # A positive difference (bin has more capacity than needed) is okay,
    # but a slightly larger difference is less preferred than a perfect fit.
    # A negative difference (item doesn't fit) is handled by the -inf initialization.

    # We can use the `sigmoid` function directly.
    # An alternative way to think about it is to map `fit_diff` to values
    # where the sigmoid is "interesting".
    # The sigmoid function itself is `1 / (1 + exp(-x))`.
    # We want high scores when `fit_diff` is small.
    # So, we can use `sigmoid(k * (-fit_diff))`. This makes the peak
    # when `fit_diff` is 0.
    # `sigmoid(-k * fit_diff)` will be high for small `fit_diff` (close to 0)
    # and lower for larger `fit_diff`.

    # Let's try to directly map `fit_diff` to a score.
    # A simple sigmoid `1 / (1 + exp(-x))` ranges from 0 to 1.
    # We can also consider `2 * sigmoid(x) - 1` to range from -1 to 1, or other transformations.
    # For "fit score", we probably want higher values for better fits.
    # Let's stick to the standard sigmoid and interpret higher values as better.
    # We want to maximize `priority`.
    # `fit_diff` represents the "slack" or wasted space. We want this slack to be minimal.
    # So, for `fit_diff` close to 0, the priority should be high.
    # For `fit_diff` much larger than 0, the priority should be lower.

    # Using `np.exp(-k * fit_diff)` will be close to 1 when `fit_diff` is large,
    # leading to a small sigmoid output. It will be close to 0 when `fit_diff` is small,
    # leading to a sigmoid output close to 1. This is exactly what we want.
    # So the sigmoid score will be `1 / (1 + np.exp(-k * fit_diff))`.

    # Ensure we don't encounter overflow with `np.exp` for very large negative `fit_diff`
    # (though this case is already handled by the -inf initialization).
    # Also, for very large positive `fit_diff`, `exp(-k*fit_diff)` becomes very small,
    # close to 0, making sigmoid close to 1. This might be counter-intuitive if we want
    # "best fit" to mean least waste. A perfect fit (diff=0) should have highest score.
    # A bin that is much larger might be less desirable than a bin that is just right.

    # Let's consider a different sigmoid application:
    # We want a peak when `fit_diff` is minimal (i.e., close to 0).
    # The function `exp(-(fit_diff)^2)` has a peak at 0. We can normalize this.
    # Or, we can use sigmoid applied to `-abs(fit_diff)`.

    # Let's refine the sigmoid idea for "good fit".
    # A "good fit" means `bins_remain_cap` is slightly larger than `item`.
    # So, `bins_remain_cap - item` should be small and positive.
    # If `bins_remain_cap - item` is negative, the item doesn't fit.
    # If `bins_remain_cap - item` is very large, it's a bad fit (lots of waste).
    # If `bins_remain_cap - item` is close to zero, it's a good fit.

    # Strategy:
    # 1. Bins where item doesn't fit get a very low score (-inf).
    # 2. Bins where item fits:
    #    - Score is high if `bins_remain_cap - item` is small (close to 0).
    #    - Score decreases as `bins_remain_cap - item` increases.

    # This suggests a sigmoid centered around 0, but we only care about positive differences.
    # Let's consider `score = sigmoid(k * (item - bins_remain_cap[can_fit_mask]))`.
    # If `item - bins_remain_cap` is close to 0 (meaning `bins_remain_cap` is close to `item`),
    # the argument to sigmoid is close to 0, giving a high sigmoid value (around 0.5 or higher).
    # If `item - bins_remain_cap` is large negative (meaning `bins_remain_cap` is much larger than `item`),
    # the argument is large positive, sigmoid is close to 1. This is NOT what we want.

    # Let's reconsider the `1 / (1 + exp(-x))` structure.
    # If we use `bins_remain_cap - item` as `x`, then:
    #   - `fit_diff` close to 0 -> `exp(0) = 1` -> `1 / (1 + 1) = 0.5`
    #   - `fit_diff` large positive -> `exp(large_pos)` -> `1 / (1 + very_large)` -> close to 0
    #   - `fit_diff` large negative -> `exp(large_neg)` -> `1 / (1 + very_small)` -> close to 1
    # This means the highest score is for bins that are WAY too big. We need to invert this logic.

    # How about `score = 1 / (1 + exp(k * fit_diff))`?
    #   - `fit_diff` close to 0 -> `exp(0) = 1` -> `1 / (1 + 1) = 0.5` (Good)
    #   - `fit_diff` large positive -> `exp(large_pos)` -> `1 / (1 + very_large)` -> close to 0 (Bad, too much waste)
    #   - `fit_diff` large negative -> `exp(large_neg)` -> `1 / (1 + very_small)` -> close to 1 (Bad, doesn't fit, already handled by -inf)

    # This looks like a reasonable approach for "Sigmoid Fit Score".
    # The highest score is achieved when `fit_diff` is zero.
    # As `fit_diff` increases (more waste), the score decreases.

    # Calculate priorities for bins that can fit the item
    scores_for_fitting_bins = 1 / (1 + np.exp(k * fit_diff))

    # Assign these calculated scores back to the appropriate positions in the priorities array
    priorities[can_fit_mask] = scores_for_fitting_bins

    return priorities
```
