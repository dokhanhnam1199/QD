[
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a First Fit Decreasing-like strategy.\n\n    This heuristic prioritizes bins that can fit the item and, among those,\n    prioritizes bins that will have the least remaining capacity *after*\n    placing the item. This aims to pack items tightly.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity after placement.\n    # We want to prioritize bins where placing the item leaves the LEAST remaining capacity.\n    # This is equivalent to prioritizing bins with smaller remaining capacity *after* placement.\n    # A higher score means higher priority. So, we can use the negative of the\n    # remaining capacity, or some transformation of it.\n    # Let's assign a score based on how \"tightly\" it fits. A tighter fit (less remaining capacity)\n    # should have a higher priority.\n    # So, we can use: -(remaining_capacity - item_size).\n    # This means if a bin has remaining capacity of 10 and item is 3, remaining is 7. Score is -7.\n    # If a bin has remaining capacity of 5 and item is 3, remaining is 2. Score is -2.\n    # -2 is greater than -7, so the bin with remaining capacity 5 is preferred.\n    # However, we only want to consider bins that *can* fit the item.\n    # For bins that cannot fit, the priority remains 0.\n\n    # Calculate the remaining capacity for bins that can fit the item\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # To prioritize tighter fits, we want smaller remaining_after_placement values to have higher scores.\n    # So, we can use negative of remaining_after_placement.\n    # However, simply assigning -(remaining_after_placement) might lead to issues if all values are negative or very large.\n    # A common approach for \"best fit\" (which this is similar to) is to prioritize by\n    # the size of the remaining capacity. The smaller the remaining capacity, the better.\n    # So, if we have `bins_remain_cap` and `item`, we are looking for the smallest\n    # `bins_remain_cap[i] - item` where `bins_remain_cap[i] >= item`.\n    # This smallest difference corresponds to the \"best fit\".\n    # We can assign priority as 1 / (remaining_after_placement + epsilon) for a higher value for smaller remaining.\n    # Or, a simpler way: we want to maximize (bin_remain_cap - item). This is not quite right.\n    # We want to minimize (bin_remain_cap - item).\n    # So, if we assign priority as `- (bin_remain_cap[i] - item)`, a smaller positive difference\n    # leads to a larger negative number, which is what we want to avoid.\n    #\n    # Let's reconsider: First Fit means iterate through bins and pick the first one that fits.\n    # This is not exactly First Fit, but more of a \"Best Fit\" like approach in terms of prioritization.\n    # The prompt implies a priority score for *each* bin, then picking the highest.\n    # For a First Fit strategy, the priority would be based on the *order* of the bins, not their capacity.\n    # The prompt asks for a priority function that returns scores for *each* bin.\n    #\n    # Let's reinterpret \"First Fit strategy\" in the context of assigning priorities.\n    # Perhaps it means: if a bin *can* fit the item, it gets a high priority.\n    # If multiple bins can fit, how do we prioritize? The \"First Fit\" aspect might imply\n    # prioritizing bins that have been open longer or are earlier in the list.\n    #\n    # Let's try to combine First Fit and capacity.\n    # First Fit often implies using the earliest available bin that fits.\n    # In an online scenario where we are given `bins_remain_cap`, it implies the order matters.\n    # So, a bin earlier in the array that fits is generally preferred.\n    # However, the question asks for a \"priority score\" for *each* bin.\n    #\n    # Let's consider a \"Best Fit\" approach disguised as a priority function for a \"First Fit strategy\":\n    # The idea is to make the \"best fitting\" bins have a higher priority.\n    # \"Best fitting\" means minimizing `bins_remain_cap[i] - item`.\n    #\n    # So, if `bins_remain_cap[i] >= item`:\n    # Priority_i = a value that is higher for smaller `bins_remain_cap[i] - item`.\n    # Let's assign priority as `1 / (bins_remain_cap[i] - item + 1e-9)` to avoid division by zero\n    # and give higher priority to smaller remaining capacities.\n    # Bins that cannot fit get 0 priority.\n\n    priorities[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)\n\n    return priorities",
    "response_id": 0,
    "obj": 4.198244914240141,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit strategy.\n\n    In Best Fit, we prioritize bins that have just enough remaining capacity\n    to fit the item. This aims to leave larger remaining capacities in other\n    bins for potentially larger future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a more preferred bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    # Calculate the remaining capacity after placing the item\n    remaining_after_fit = bins_remain_cap - item\n\n    # We only consider bins where the item can actually fit\n    can_fit_mask = remaining_after_fit >= 0\n\n    # For bins where the item fits, assign a priority.\n    # The ideal bin (Best Fit) is the one that results in the smallest non-negative remaining capacity.\n    # We want to maximize the \"closeness\" to zero remaining capacity without going negative.\n    # A simple way to achieve this is to use the negative of the remaining capacity.\n    # A larger (less negative) value means closer to zero remaining capacity.\n    # To avoid division by zero or issues with zero remaining capacity, we can add a small epsilon,\n    # or ensure that a perfect fit (0 remaining) gets a very high priority.\n    # Let's try a score that is high for perfect fits and decreases as the remaining capacity increases.\n    # A large positive score for perfect fits, and for others, a score based on how close they are to fitting perfectly.\n\n    # Let's define a score that prioritizes bins with the smallest positive remaining capacity after fitting.\n    # We want to maximize (bins_remain_cap - item) such that it's >= 0.\n    # So, if remaining_after_fit is 0, that's the best.\n    # If remaining_after_fit is 0.1, that's better than 0.5.\n\n    # We can invert the remaining_after_fit for fitting bins and normalize or scale it.\n    # A common heuristic approach is to use the negative of the remaining capacity if positive,\n    # as this rewards bins that have \"just enough\" space.\n\n    # Using a score that is negative of remaining_after_fit for fitting bins.\n    # The smaller the remaining_after_fit (closer to 0), the larger (less negative) the priority.\n    # We will flip this to get higher scores for better bins.\n\n    # Let's use the reciprocal of (remaining_after_fit + epsilon) for bins that fit.\n    # This makes smaller remaining capacities yield higher scores.\n    # Adding a small epsilon to avoid division by zero for perfect fits.\n    epsilon = 1e-9\n    priorities[can_fit_mask] = 1.0 / (remaining_after_fit[can_fit_mask] + epsilon)\n\n    # For bins where the item doesn't fit, their priority remains 0.\n    # If we want to explicitly disfavor them further, we could assign a negative value,\n    # but 0 is sufficient as it will be lower than any positive priority.\n\n    return priorities",
    "response_id": 1,
    "obj": 4.198244914240141,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response2.txt_stdout.txt",
    "code_path": "problem_iter1_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    The Worst Fit strategy aims to place the item into the bin with the largest remaining capacity,\n    thus leaving smaller capacities for potentially larger items that may arrive later.\n    The priority is higher for bins that have more remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # For bins where the item fits, the priority is its remaining capacity.\n    # For bins where the item does not fit, the priority is 0 (or a very low value).\n    # We want the bin with the LARGEST remaining capacity if it can fit the item.\n    \n    # Create a boolean mask indicating which bins have enough remaining capacity\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities with zeros\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # For bins that can fit the item, set their priority to their remaining capacity\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask]\n\n    return priorities",
    "response_id": 2,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response3.txt_stdout.txt",
    "code_path": "problem_iter1_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    The Almost Full Fit strategy prioritizes bins that are almost full,\n    meaning they have a small remaining capacity after accommodating the current item.\n    This aims to leave larger remaining capacities in other bins for potentially\n    larger future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Consider only bins that can fit the item\n    available_bins_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros (or handle as an error/new bin creation)\n    if not np.any(available_bins_mask):\n        return priorities\n\n    # Calculate the remaining capacity *after* placing the item\n    remaining_after_placement = bins_remain_cap[available_bins_mask] - item\n\n    # The priority is inversely proportional to the remaining capacity after placement.\n    # We want bins with smaller remaining capacity (i.e., bins that get *almost full*)\n    # to have higher priority.\n    # To avoid division by zero if remaining capacity is 0, we can add a small epsilon.\n    epsilon = 1e-9\n    priorities[available_bins_mask] = 1.0 / (remaining_after_placement + epsilon)\n\n    # Optionally, we can normalize the priorities to a range like [0, 1]\n    # This can sometimes help if the raw priority values are very large or very small.\n    # For a priority function, raw values are often fine, but normalization can be an option.\n    # if np.max(priorities[available_bins_mask]) > 0:\n    #     priorities[available_bins_mask] /= np.max(priorities[available_bins_mask])\n\n    return priorities",
    "response_id": 3,
    "obj": 4.198244914240141,
    "SLOC": 9.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response4.txt_stdout.txt",
    "code_path": "problem_iter1_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    The Exact Fit First strategy prioritizes bins that can accommodate the item\n    with the least amount of remaining space after the item is placed. This\n    aims to \"fill up\" bins as much as possible.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Calculate the remaining capacity if the item were placed in each bin\n    # Only consider bins where the item *can* fit\n    potential_remaining_caps = bins_remain_cap - item\n    \n    # A large negative value means the item doesn't fit in this bin.\n    # We want to prioritize bins that leave the *smallest* positive remainder.\n    # So, we can assign a very low priority to bins where the item doesn't fit.\n    # For bins where it fits, the priority will be related to the negative of\n    # the remaining space (or rather, the closeness to zero of the remainder).\n    \n    # Initialize priorities to a very low value (representing \"not a good fit\")\n    priorities = -np.inf\n    \n    # Find indices of bins where the item fits\n    can_fit_indices = np.where(bins_remain_cap >= item)[0]\n    \n    # For bins where the item fits, calculate the \"tightness\" of the fit.\n    # A smaller remaining capacity is a higher priority.\n    # We want the negative of the remaining capacity, so a smaller positive\n    # remaining capacity results in a higher (less negative) priority score.\n    if len(can_fit_indices) > 0:\n        remaining_after_fit = bins_remain_cap[can_fit_indices] - item\n        # We want the exact fit, meaning the smallest remaining capacity.\n        # So, we assign a priority based on the negative of this remaining capacity.\n        # The bin with the smallest remaining capacity will have the highest (least negative) score.\n        priorities[can_fit_indices] = -remaining_after_fit\n        \n    return priorities",
    "response_id": 4,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 44, in priority_v2\nTypeError: 'float' object does not support item assignment\n9\n2\n"
  },
  {
    "stdout_filepath": "problem_iter1_response5.txt_stdout.txt",
    "code_path": "problem_iter1_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit) strategy.\n\n    The priority is higher for bins that are a closer fit to the item's size.\n    Specifically, it calculates 1 / (1 + |remaining_capacity - item_size|) for bins\n    where remaining_capacity >= item_size. For bins that cannot fit the item,\n    the priority is 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Find bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the difference between remaining capacity and item size for fitting bins\n    diffs = bins_remain_cap[can_fit_mask] - item\n    \n    # Calculate priority using inverse distance: 1 / (1 + absolute difference)\n    # Adding 1 to the denominator to avoid division by zero if diff is 0.\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.abs(diffs))\n    \n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response6.txt_stdout.txt",
    "code_path": "problem_iter1_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score strategy prioritizes bins that can accommodate the item\n    and have a remaining capacity that is close to the item's size. This is achieved\n    by mapping the difference between the bin's remaining capacity and the item's size\n    through a sigmoid function. A higher score indicates a better fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Filter out bins that cannot accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_cap]\n\n    # Calculate the \"slack\" or remaining space after fitting the item\n    # A smaller slack means a tighter fit. We want to favor tighter fits.\n    slack = suitable_bins_cap - item\n\n    # Apply a sigmoid function to the negative slack.\n    # We use negative slack because we want smaller (tighter) slacks to have higher scores.\n    # The sigmoid function maps values to the range (0, 1).\n    # A commonly used sigmoid is 1 / (1 + exp(-x)).\n    # We can adjust the steepness and center of the sigmoid using parameters,\n    # but for a general strategy, a standard sigmoid can be used.\n    # A simple transformation to map slack to a more useful range for sigmoid:\n    # we want values closer to 0 (tight fit) to be high, and larger values to be low.\n    # So, let's consider a transformation that maps slack=0 to a high value and\n    # increasing slack to lower values.\n    # For instance, -slack. A tighter fit (slack=0) gives 0. A slack of 1 gives -1.\n    # Sigmoid(-0) = 0.5. Sigmoid(-1) approx 0.27. This isn't ideal as it starts at 0.5.\n\n    # Let's rethink the mapping. We want bins with remaining capacity *just* enough\n    # for the item to have the highest priority.\n    # The ideal scenario is when remaining_cap == item.\n    # The difference (remaining_cap - item) should be close to 0.\n    # We want to transform this difference into a priority score (0 to 1).\n    # A Gaussian-like or a scaled sigmoid centered around 0 difference could work.\n\n    # Let's use the difference directly, and map it such that difference=0 is peak.\n    # Consider the function: exp(-(x - mu)^2 / (2 * sigma^2)) where x is the difference.\n    # This gives a Gaussian peak at mu. We want mu = 0 (difference = 0).\n    # sigma controls the width. A smaller sigma means a sharper peak.\n\n    # Let's try a sigmoid approach but invert the thinking.\n    # If remaining_cap > item, we want remaining_cap - item to be small.\n    # Let's map `slack = remaining_cap - item`.\n    # We want a function f(slack) such that f(0) is high, f(large) is low.\n    # Consider `1 / (1 + exp(slack))`.\n    # If slack = 0, score = 1 / (1 + exp(0)) = 1 / (1 + 1) = 0.5\n    # If slack = 1, score = 1 / (1 + exp(1)) approx 0.27\n    # If slack = -1 (item larger than capacity, but we filtered those), score = 1 / (1 + exp(-1)) approx 0.73\n    # This gives higher scores for items that are *too small* for the bin, which is not what we want for fitting.\n\n    # Alternative: Use the *ratio* of item size to remaining capacity as a factor.\n    # Or, focus on the \"waste\" after packing. `waste = remaining_cap - item`.\n    # We want to minimize waste.\n    # Consider a logistic function on the *negative* waste to get higher scores for less waste.\n    # `sigmoid(k * -(waste))`\n    # `k` is a steepness parameter. Let's choose `k=1` for simplicity.\n    # `priority = 1 / (1 + exp(slack))`\n    # This function gives a score of 0.5 for a perfect fit (slack=0).\n    # For slack > 0 (excess capacity), score decreases.\n    # For slack < 0 (item too big), score increases.\n\n    # Let's try a strategy that prioritizes bins where `remaining_capacity` is\n    # as close as possible to `item`.\n    # We can define a score as `1 / (1 + abs(remaining_capacity - item))`.\n    # This gives a score of 1 for a perfect fit.\n\n    # For a Sigmoid Fit Score strategy, let's interpret it as prioritizing bins\n    # where the remaining capacity is \"just enough\" or slightly more.\n    # The sigmoid function maps to (0, 1). We want to map the condition\n    # `remaining_capacity >= item` to a score.\n\n    # Let's use a modified sigmoid:\n    # We want a high score when `bins_remain_cap` is close to `item`.\n    # Consider `score = sigmoid(k * (item - bins_remain_cap))`.\n    # If `bins_remain_cap = item`, `score = sigmoid(0) = 0.5`.\n    # If `bins_remain_cap = item + delta` (delta > 0), `score = sigmoid(-k * delta)`, which is < 0.5.\n    # If `bins_remain_cap = item - delta` (delta > 0), `score = sigmoid(k * delta)`, which is > 0.5.\n    # This again prioritizes bins that are *too small*.\n\n    # Let's consider the \"fitness\" of a bin for an item.\n    # A good fit means the bin has enough space, and the space left is minimized.\n    # Let `residual_capacity = bins_remain_cap - item`.\n    # We only care about `residual_capacity >= 0`.\n    # We want to maximize a score based on `residual_capacity`.\n    # A sigmoid function can transform a continuous variable into a probability-like score.\n    # Let's use a sigmoid that maps values close to 0 (for residual_capacity) to high scores.\n    # A common Sigmoid is `1 / (1 + exp(-x))`.\n    # If we input `-(residual_capacity)`, then:\n    # `residual_capacity = 0` -> `score = 1 / (1 + exp(0)) = 0.5`\n    # `residual_capacity = small_positive` -> `score = 1 / (1 + exp(-small_positive))` which is slightly > 0.5\n    # `residual_capacity = large_positive` -> `score = 1 / (1 + exp(-large_positive))` which approaches 1.\n    # This means it prioritizes bins with *more* remaining capacity, not a tight fit.\n\n    # Let's consider the objective of minimizing the number of bins.\n    # A good heuristic often tries to \"fill\" bins as much as possible.\n    # \"First Fit Decreasing\" sorts items by size. In online, we can't sort.\n    # \"Best Fit\" picks the bin that leaves the minimum residual capacity.\n    # Our sigmoid fit score should somehow approximate \"Best Fit\" using a sigmoid.\n\n    # Let's interpret \"Sigmoid Fit Score\" as a function that smoothly assigns higher\n    # priority to bins that are \"good fits\". A \"good fit\" could be interpreted as:\n    # 1. The bin can accommodate the item (`bins_remain_cap >= item`).\n    # 2. The remaining capacity after fitting the item (`bins_remain_cap - item`) is minimized.\n\n    # We can define a score based on the `residual_capacity = bins_remain_cap - item`.\n    # We want to penalize large residual capacities.\n    # Let's map `residual_capacity` to a score using a sigmoid.\n    # A suitable mapping would be one where `residual_capacity = 0` gives a high score,\n    # and `residual_capacity` increasing leads to a decreasing score.\n\n    # Consider the function: `sigmoid(k * (C - (bins_remain_cap - item)))`\n    # where C is a constant, and k is a steepness factor.\n    # Let C = 0 and k = 1. `sigmoid(-residual_capacity)`.\n    # `residual_capacity = 0` -> `sigmoid(0) = 0.5`\n    # `residual_capacity = 5` -> `sigmoid(-5)` is very small (close to 0)\n    # `residual_capacity = -2` (item too large, but filtered) -> `sigmoid(2)` is close to 1.\n\n    # This suggests we should use a transformation that gives high scores for `residual_capacity` near 0.\n    # Let's use `1 - sigmoid(k * residual_capacity)` where k > 0.\n    # `residual_capacity = 0` -> `1 - sigmoid(0) = 1 - 0.5 = 0.5`\n    # `residual_capacity = 5` -> `1 - sigmoid(5)` is very small (close to 0).\n    # `residual_capacity = -2` -> `1 - sigmoid(-2)` is close to 1. This is still problematic if not filtered.\n\n    # Let's enforce the constraint that only bins capable of holding the item get scores.\n    # For bins where `bins_remain_cap < item`, the priority should be 0 (or a very low value).\n\n    # We need a score `s` where:\n    # If `bins_remain_cap < item`, `s = 0`.\n    # If `bins_remain_cap >= item`, `s` should be high when `bins_remain_cap - item` is small.\n\n    # Let's use the `residual_capacity = bins_remain_cap - item`.\n    # For `residual_capacity >= 0`, we want to map this to a score using a sigmoid.\n    # A sigmoid that maps `[0, infinity)` to `(0, 1)` with decreasing values would be:\n    # `1 / (1 + exp(residual_capacity))` - This results in 0.5 for perfect fit, and lower for more slack.\n    # Let's try a different approach for the sigmoid shape.\n\n    # A \"Sigmoid Fit Score\" implies using a sigmoid function.\n    # A typical use case of sigmoid in scoring is when you want to reward\n    # values up to a certain point and then penalize further increases.\n    # Or, conversely, penalize values up to a point and reward after.\n\n    # Let's define the score for a bin `i` as `S_i`.\n    # `S_i = sigmoid(f(item, bins_remain_cap[i]))`\n    # where `f` is a function that captures the \"goodness\" of the fit.\n\n    # We want bins where `bins_remain_cap[i]` is slightly larger than `item` to be prioritized.\n    # Consider the difference `diff = bins_remain_cap[i] - item`.\n    # We are interested in `diff >= 0`.\n    # We want to maximize `S_i` when `diff` is small (close to 0).\n\n    # Let's try mapping `diff` to a sigmoid in a way that a small non-negative `diff` gives a high score.\n    # Use `sigmoid(-diff)`?\n    # If `diff = 0`, `sigmoid(0) = 0.5`.\n    # If `diff = 1`, `sigmoid(-1) approx 0.27`.\n    # If `diff = 10`, `sigmoid(-10)` is close to 0.\n    # If `diff = -5` (item too big), `sigmoid(5)` is close to 1.\n\n    # This function `sigmoid(-residual_capacity)` where `residual_capacity = bins_remain_cap - item`\n    # assigns a score of 0.5 to a perfect fit.\n    # It assigns scores closer to 1 for bins that are too small (but we will filter these out).\n    # It assigns scores closer to 0 for bins with a lot of slack.\n    # This seems like a reasonable \"Sigmoid Fit Score\" strategy, often called \"Soft Best Fit\".\n\n    # Let's adjust the sigmoid to favor tighter fits more strongly or shift the center.\n    # We can use `sigmoid(k * (-residual_capacity))` where `k` controls steepness.\n    # Or `sigmoid((M - residual_capacity) / alpha)` where `M` is some target slack and `alpha` scales it.\n\n    # Let's stick to a direct interpretation of a Sigmoid Fit Score:\n    # Prioritize bins that fit the item, and among those, prioritize those that leave minimal space.\n    # The sigmoid function transforms a value into a (0, 1) range.\n    # We want to transform `(bins_remain_cap[i] - item)` for valid bins.\n\n    # A commonly used approach for \"best fit\" like heuristic using sigmoid can be:\n    # `score = sigmoid(K - (bins_remain_cap - item))`\n    # where K is a bias or centering term.\n    # If we want perfect fit (`bins_remain_cap == item`) to have a high score,\n    # we need `K` to be large. Let's choose `K` to be, say, 10.\n    # Then `score = sigmoid(10 - residual_capacity)`.\n    # If `residual_capacity = 0`, `score = sigmoid(10)` approx 1.\n    # If `residual_capacity = 1`, `score = sigmoid(9)` approx 1.\n    # If `residual_capacity = 5`, `score = sigmoid(5)` approx 1.\n    # If `residual_capacity = 10`, `score = sigmoid(0) = 0.5`.\n    # If `residual_capacity = 15`, `score = sigmoid(-5)` approx 0.\n    # This prioritizes bins that have *just enough* space or a bit more,\n    # and penalizes bins with a lot of excess capacity.\n\n    # Let's refine this. The sigmoid is `1 / (1 + exp(-x))`.\n    # We want a score that is high when `bins_remain_cap - item` is small.\n    # Let `x = -(bins_remain_cap - item) = item - bins_remain_cap`.\n    # We are only interested when `bins_remain_cap >= item`, so `x <= 0`.\n    # So, `score = 1 / (1 + exp(-(item - bins_remain_cap))) = 1 / (1 + exp(bins_remain_cap - item))`\n    # If `bins_remain_cap = item`: score = 1 / (1 + exp(0)) = 0.5\n    # If `bins_remain_cap = item + 5`: score = 1 / (1 + exp(5)) approx 0.007\n    # If `bins_remain_cap = item - 5` (but this is filtered): score = 1 / (1 + exp(-5)) approx 0.993\n\n    # The problem statement mentions \"Sigmoid Fit Score strategy\".\n    # This implies using sigmoid to quantify \"fit\".\n    # A common interpretation of \"fit\" in bin packing is how well an item\n    # occupies the available space, ideally leaving minimal remainder.\n    # We want a higher score for bins where `bins_remain_cap - item` is small and non-negative.\n\n    # Let's define a function `f(residual_capacity)` that maps `[0, infinity)` to `(0, 1]`\n    # with `f(0)` being highest.\n    # Consider a \"shifted and scaled\" sigmoid or a Gaussian-like approach mapped by sigmoid.\n    #\n    # Example: Let `residual_capacity = bins_remain_cap - item`.\n    # We want to prioritize `residual_capacity = 0`.\n    # A possible function mapping `residual_capacity` to a score in (0,1):\n    # `score = exp(-residual_capacity / sigma)`\n    # This decreases as residual_capacity increases. Max score is 1 at `residual_capacity=0`.\n    # But this is not a sigmoid function.\n\n    # Let's try to force the sigmoid's \"S\" shape to capture this.\n    # We can map `bins_remain_cap` to the sigmoid argument.\n    # `bins_remain_cap`: the larger this is, the less \"tight\" the fit tends to be.\n    # We want to invert this.\n    # Let's use `score = sigmoid(k * (TargetCapacity - bins_remain_cap))`\n    # where `TargetCapacity` is what we consider a \"good\" remaining capacity,\n    # ideally around `item`'s size.\n\n    # A \"Sigmoid Fit Score\" typically prioritizes bins where the remaining capacity\n    # is close to the item's size.\n    # Let's consider the range of `bins_remain_cap`.\n    # If `bins_remain_cap` is very large, the score should be low.\n    # If `bins_remain_cap` is just slightly larger than `item`, the score should be high.\n    # If `bins_remain_cap < item`, the score should be 0.\n\n    # Let's model the preference for `bins_remain_cap` that is just above `item`.\n    # A function like `1 / (1 + exp(alpha * (bins_remain_cap - item - margin)))`\n    # where `margin` is a small buffer and `alpha` is steepness.\n    # If `bins_remain_cap - item - margin` is small and positive, score is ~0.5.\n    # If `bins_remain_cap - item - margin` is large and positive, score is near 0.\n    # If `bins_remain_cap - item - margin` is small and negative (meaning `bins_remain_cap` is closer to `item` than `margin`), score is near 1.\n\n    # This implies we want to transform the \"slack\" `residual_capacity = bins_remain_cap - item`\n    # using a sigmoid.\n    # Let's use a standard sigmoid `sigma(x) = 1 / (1 + exp(-x))`.\n    # We want to maximize this for `residual_capacity` close to 0.\n    # We can achieve this by inputting `k * (-residual_capacity)`.\n    # So, `score = sigma(k * -(bins_remain_cap - item)) = sigma(k * (item - bins_remain_cap))`\n    # with `k > 0`.\n\n    # Let `k = 1`.\n    # `score = 1 / (1 + exp(-(item - bins_remain_cap)))`\n    # `score = 1 / (1 + exp(bins_remain_cap - item))`\n\n    # For bins where `bins_remain_cap < item`, the term `bins_remain_cap - item` is negative.\n    # `exp(negative)` is small, so `1 + exp(negative)` is close to 1, and `score` is close to 1.\n    # This is problematic as we want 0 priority for bins that don't fit.\n\n    # Solution: Set priority to 0 for bins that cannot accommodate the item.\n    # For bins that can accommodate, apply the sigmoid score.\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find bins that can accommodate the item\n    can_accommodate_mask = bins_remain_cap >= item\n\n    # Calculate the 'residual capacity' for bins that can accommodate the item\n    # residual_capacity = bins_remain_cap - item\n    # We want to prioritize where residual_capacity is minimal.\n    # A higher score means better fit.\n    # Let's use the sigmoid on the negative residual capacity.\n    # score = 1 / (1 + exp(-(residual_capacity)))\n    # score = 1 / (1 + exp(-(bins_remain_cap[can_accommodate_mask] - item)))\n    # score = 1 / (1 + exp(item - bins_remain_cap[can_accommodate_mask]))\n\n    # A common practice for sigmoid fitting is to map the \"closeness\" to the target value.\n    # Let's consider the target remaining capacity to be `item`.\n    # We want `bins_remain_cap` to be close to `item`.\n    # `difference = bins_remain_cap - item`.\n    # We want to prioritize `difference` near 0 (for `difference >= 0`).\n    # Let's use `sigmoid(alpha * (item - difference))` which is `sigmoid(alpha * (2*item - bins_remain_cap))`.\n    # This still doesn't look right.\n\n    # Simpler approach: Soft Best Fit.\n    # Prioritize bins with minimum slack (`bins_remain_cap - item`).\n    # Use sigmoid to provide a smooth score.\n    # The argument to sigmoid should decrease as slack increases.\n    # So, `argument = -slack = -(bins_remain_cap - item) = item - bins_remain_cap`.\n    # Let's use `sigmoid(k * (item - bins_remain_cap))`.\n    # To make it specific for \"Sigmoid Fit Score\", let's ensure the values being transformed\n    # have a direct relation to the \"fit quality\".\n\n    # Let's use the ratio `item / bins_remain_cap`.\n    # If `bins_remain_cap` is just slightly larger than `item`, the ratio is close to 1.\n    # If `bins_remain_cap` is much larger than `item`, the ratio is small.\n    # We want to prioritize ratios close to 1.\n    # `score = sigmoid(k * (item / bins_remain_cap))` or `sigmoid(k * (bins_remain_cap / item))`\n    # Using `bins_remain_cap / item`:\n    # If `bins_remain_cap = item`, ratio is 1. Score = `sigmoid(k)`.\n    # If `bins_remain_cap = item + delta`, ratio > 1. Score > `sigmoid(k)`.\n    # If `bins_remain_cap = item - delta`, ratio < 1. Score < `sigmoid(k)`.\n    # This favors bins with *more* capacity.\n\n    # Let's go back to `residual_capacity = bins_remain_cap - item`.\n    # We want to score `residual_capacity` values that are close to 0 highly.\n    # Sigmoid: `1 / (1 + exp(-x))`.\n    # We want `x` to be large positive when `residual_capacity` is close to 0.\n    # This suggests mapping `residual_capacity` to a term that is large positive for small `residual_capacity`.\n    # Example: `k * (Target_Residual - residual_capacity)` where `Target_Residual = 0`.\n    # So, `k * (-residual_capacity)`.\n    # `score = 1 / (1 + exp(-k * residual_capacity))` for `residual_capacity >= 0`.\n    # If `residual_capacity = 0`, score = `1 / (1 + exp(0))` = 0.5.\n    # If `residual_capacity = 1`, score = `1 / (1 + exp(-k))`. If k is large, score is close to 1.\n    # If `residual_capacity = 10`, score = `1 / (1 + exp(-10k))`. If k is large, score is close to 1.\n\n    # This implies the opposite of what we want! We want scores to *decrease* with slack.\n\n    # Let's use `score = 1 / (1 + exp(k * residual_capacity))`.\n    # If `residual_capacity = 0`, score = `1 / (1 + exp(0))` = 0.5.\n    # If `residual_capacity = 1`, score = `1 / (1 + exp(k))`. If k is large, score is close to 0.\n    # If `residual_capacity = 10`, score = `1 / (1 + exp(10k))`. If k is large, score is close to 0.\n\n    # This seems correct for prioritizing minimal slack.\n    # Let's choose a `k` value. A moderate value like `k=1` or `k=2` can be used.\n    # A larger `k` makes the \"fit\" more critical.\n\n    # Parameter for sigmoid steepness. A higher value means a stronger preference for tighter fits.\n    STEEPNESS_FACTOR = 2.0\n\n    # Calculate residual capacity only for bins that can accommodate the item.\n    residual_capacity_for_suitable_bins = bins_remain_cap[can_accommodate_mask] - item\n\n    # Apply the sigmoid function. We want higher scores for smaller residual capacities.\n    # The sigmoid function `1 / (1 + exp(-x))` is increasing.\n    # We want a score that decreases as `residual_capacity` increases.\n    # Therefore, we need to pass a decreasing function of `residual_capacity` to the sigmoid.\n    # `x = -STEEPNESS_FACTOR * residual_capacity`\n    # This way, as `residual_capacity` increases, `x` decreases, and `sigmoid(x)` decreases.\n\n    # Sigmoid calculation: `1 / (1 + exp(-x))`\n    # `x = STEEPNESS_FACTOR * (item - residual_capacity)`\n    # `x = STEEPNESS_FACTOR * (item - (bins_remain_cap[can_accommodate_mask] - item))`\n    # `x = STEEPNESS_FACTOR * (2*item - bins_remain_cap[can_accommodate_mask])`\n\n    # Alternative: Use the \"slack\" directly as the input to an *inversely* behaving sigmoid-like transformation.\n    # A standard sigmoid `1 / (1 + exp(-x))` goes from 0 to 1 as x goes from -inf to +inf.\n    # We want a score that goes from near 1 to near 0 as `residual_capacity` goes from 0 to +inf.\n    # Let's use `1 - sigmoid(x)`. This goes from 1 to 0 as x goes from -inf to +inf.\n    # So, we need `x` to be a decreasing function of `residual_capacity`.\n    # Let `x = STEEPNESS_FACTOR * residual_capacity`.\n    # `score = 1 - (1 / (1 + exp(-STEEPNESS_FACTOR * residual_capacity)))`\n    # `score = exp(-STEEPNESS_FACTOR * residual_capacity) / (1 + exp(-STEEPNESS_FACTOR * residual_capacity))`\n    # This is equivalent to `1 / (1 + exp(STEEPNESS_FACTOR * residual_capacity))`.\n\n    # Let's use this form: `1 / (1 + exp(STEEPNESS_FACTOR * residual_capacity))`\n\n    # Ensure we are not using invalid values for exp\n    # The residual capacity will be >= 0 for suitable bins.\n    # `STEEPNESS_FACTOR * residual_capacity` will be >= 0.\n    # `exp(...)` will be >= 1.\n    # `1 + exp(...)` will be >= 2.\n    # The score will be in (0, 0.5].\n\n    # This is giving scores between 0 and 0.5, with 0.5 for perfect fit.\n    # If we want scores to be in (0, 1], perhaps we need to scale it or adjust the sigmoid form.\n\n    # Consider a sigmoid centered at a different point or with a different shape.\n    # What if we interpret \"Sigmoid Fit Score\" as applying sigmoid to a scaled\n    # measure of how *good* the fit is?\n\n    # Let's revisit the strategy for \"best fit\" using sigmoid.\n    # Often, it involves transforming the 'difference' to have a peak.\n    # A Gaussian-like score can be created using `exp(-(difference^2) / (2*sigma^2))`.\n    # This can be \"sigmoid-ified\" if needed.\n\n    # However, for a direct Sigmoid Fit Score:\n    # Prioritize bins that have capacity just enough for the item.\n    # `capacity = bins_remain_cap[i]`\n    # `item_size = item`\n    # We want to prioritize when `capacity` is close to `item_size`.\n    # The difference `d = capacity - item_size`.\n    # We are interested in `d >= 0`.\n    # We want score to be high for `d=0` and decrease as `d` increases.\n\n    # Let's use a sigmoid on a transformed value `y`.\n    # `score = sigmoid(y)`\n    # We want `y` to be large positive for small `d`, and small negative for large `d`.\n    # So, `y = -k * d`.\n    # `score = sigmoid(-k * d) = 1 / (1 + exp(k * d))`\n    # `score = 1 / (1 + exp(k * (bins_remain_cap - item)))` for `bins_remain_cap >= item`.\n\n    # Let's test this again:\n    # If `bins_remain_cap = item`: `score = 1 / (1 + exp(0)) = 0.5`.\n    # If `bins_remain_cap = item + 1`: `score = 1 / (1 + exp(k))`. If k=2, score ~0.119.\n    # If `bins_remain_cap = item + 0.1`: `score = 1 / (1 + exp(0.2))`. If k=2, score ~0.450.\n    # If `bins_remain_cap = item - 0.1` (filtered): score would be ~0.549.\n\n    # This gives scores between 0 and 0.5, with 0.5 for a perfect fit.\n    # This is a valid interpretation of a sigmoid scoring where 0.5 represents a neutral point,\n    # and values above/below indicate deviation.\n\n    # To get scores in (0, 1], we could add an offset or rescale, but the standard sigmoid\n    # definition is likely what's intended. If the goal is to rank, the relative values matter.\n\n    # We can make the preference for tighter fits stronger by increasing STEEPNESS_FACTOR.\n    # A higher STEEPNESS_FACTOR means the score drops more rapidly as slack increases.\n\n    # Calculate priorities for the suitable bins\n    # Use the logistic function `1 / (1 + exp(-x))`\n    # Let `x = STEEPNESS_FACTOR * (item - residual_capacity)`\n    # `x = STEEPNESS_FACTOR * (item - (bins_remain_cap[can_accommodate_mask] - item))`\n    # `x = STEEPNESS_FACTOR * (2 * item - bins_remain_cap[can_accommodate_mask])`\n    # This maps `2*item - bins_remain_cap` to sigmoid.\n    # When `bins_remain_cap` is slightly larger than `item`, `2*item - bins_remain_cap` is positive and smaller.\n    # e.g. `bins_remain_cap = item + delta`. `2*item - (item + delta) = item - delta`.\n    # This should be near zero.\n\n    # Let's use `x = STEEPNESS_FACTOR * (item - bins_remain_cap[can_accommodate_mask])`\n    # `item - bins_remain_cap` : if `bins_remain_cap` is slightly more than `item`, this is slightly negative.\n    # e.g. `bins_remain_cap = item + 0.1`, `item - bins_remain_cap = -0.1`.\n    # Sigmoid(-k * 0.1) will be < 0.5.\n\n    # This still does not capture the \"tightest fit first\" as the primary goal for sigmoid score.\n\n    # Re-interpreting Sigmoid Fit Score:\n    # A higher score indicates a *better* fit. A perfect fit is ideal.\n    # A good fit means `bins_remain_cap` is close to `item`.\n    # Let `residual_capacity = bins_remain_cap - item`.\n    # We want scores to be high when `residual_capacity` is near 0.\n\n    # Consider the term `1 / (1 + exp(STEEPNESS_FACTOR * residual_capacity))`.\n    # This yields values in `(0, 0.5]`. A perfect fit gets 0.5.\n    # This seems like a reasonable interpretation, prioritizing bins closer to exact fit.\n\n    # Alternative interpretation: Prioritize bins that are \"sufficiently full\" after packing.\n    # i.e., the remaining capacity is small.\n\n    # Let's try to map the \"goodness of fit\" into the argument of sigmoid such that\n    # a tight fit results in a high argument, and thus a score close to 1.\n    # The argument `x` to `sigmoid(x)` should be large positive for a tight fit.\n    # `residual_capacity = bins_remain_cap - item`.\n    # We want `x` to be large positive when `residual_capacity` is near 0.\n    # So, `x = some_function(residual_capacity)` where `some_function(0)` is large positive.\n    # Let's try `x = M - k * residual_capacity` where M and k are positive.\n    # `score = 1 / (1 + exp(-(M - k * residual_capacity)))`\n    # `score = 1 / (1 + exp(k * residual_capacity - M))`\n    # If `residual_capacity = 0`, `score = 1 / (1 + exp(-M))`. If M is large, this is near 1.\n    # If `residual_capacity = 1`, `score = 1 / (1 + exp(k - M))`.\n    # If `k < M`, then `k - M` is negative, score is > 0.5.\n    # If `k > M`, then `k - M` is positive, score is < 0.5.\n\n    # Let's pick M=0 and k=1 as a starting point. This yields `1 / (1 + exp(residual_capacity))`.\n    # This results in scores `(0, 0.5]` with 0.5 for perfect fit.\n\n    # The phrasing \"Sigmoid Fit Score\" implies a use of the sigmoid function's shape.\n    # The \"best fit\" heuristic minimizes `bins_remain_cap - item`.\n    # The sigmoid function can be used to smooth this preference.\n\n    # Let's make it simple and use a common transformation for \"closeness to a target\":\n    # The target for `bins_remain_cap` is `item`.\n    # We want to maximize a score when `bins_remain_cap` is close to `item`.\n    # Let `diff = bins_remain_cap - item`.\n    # Let's use `score = exp(-(diff / sigma)**2)` and then map this to sigmoid,\n    # or directly use a sigmoid that mimics this peak.\n\n    # A common method for \"best fit\" with a sigmoid flavour is to give higher scores to bins\n    # with `residual_capacity` closer to zero.\n    # Consider `f(residual_capacity) = 1 / (1 + exp(k * residual_capacity))`\n    # This maps `residual_capacity` in `[0, inf)` to `(0, 0.5]`.\n    # The highest score is 0.5 for a perfect fit.\n\n    # Let's use this form.\n    priorities[can_accommodate_mask] = 1 / (1 + np.exp(STEEPNESS_FACTOR * residual_capacity_for_suitable_bins))\n\n    # The resulting scores are in (0, 0.5]. A perfect fit gets 0.5.\n    # More slack reduces the score, approaching 0.\n    # Bins that cannot accommodate the item get a score of 0.\n\n    return priorities",
    "response_id": 6,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 20, in priority_v2\n    \nUnboundLocalError: local variable 'suitable_bins_cap' referenced before assignment\n10\n1\n"
  },
  {
    "stdout_filepath": "problem_iter1_response7.txt_stdout.txt",
    "code_path": "problem_iter1_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Random Fit.\n\n    Random Fit prioritizes bins randomly, with a slight bias towards bins that can fit the item.\n    A higher priority score means the bin is more preferred.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Assign a base priority to bins that can fit the item\n    # We'll add some randomness to make it \"Random Fit\"\n    base_priority = 1.0\n    priorities[can_fit_mask] = base_priority\n\n    # Introduce randomness: add a random value between 0 and 0.5\n    # This ensures that even if multiple bins can fit, their order might change.\n    # The randomness is only applied to bins that *can* fit the item.\n    random_noise = np.random.rand(len(bins_remain_cap)) * 0.5\n    priorities[can_fit_mask] += random_noise[can_fit_mask]\n\n    # Bins that cannot fit the item get a priority of 0 (or a very low value if needed)\n    # For Random Fit, we generally don't consider bins that can't fit, so 0 is appropriate.\n    # If we wanted to penalize them, we could assign negative values.\n\n    return priorities",
    "response_id": 7,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 30, in priority_v2\n    best_bin_index = -1\nnumpy._core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'\n8\n1\n"
  },
  {
    "stdout_filepath": "problem_iter1_response8.txt_stdout.txt",
    "code_path": "problem_iter1_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy strategy.\n\n    This heuristic aims to balance exploration (trying less full bins) and\n    exploitation (trying bins that are a good fit).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.2  # Probability of exploration (adjust as needed)\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate a \"goodness\" score for each bin: a larger value means a better fit\n    # We prioritize bins with just enough space for the item to minimize waste.\n    # If an item doesn't fit, its goodness score is 0.\n    goodness_scores = np.maximum(0, bins_remain_cap - item)\n\n    # Find the best (most exploited) bin: the one that leaves the least remaining capacity\n    # if we place the item in it, among bins that can fit the item.\n    # If no bin can fit the item, this will still be a very large number,\n    # which will be handled by the epsilon-greedy choice.\n    min_remaining_capacity_if_fit = np.inf\n    best_bin_index = -1\n\n    for i in range(num_bins):\n        if bins_remain_cap[i] >= item:\n            remaining = bins_remain_cap[i] - item\n            if remaining < min_remaining_capacity_if_fit:\n                min_remaining_capacity_if_fit = remaining\n                best_bin_index = i\n\n    # Epsilon-Greedy Strategy\n    for i in range(num_bins):\n        if np.random.rand() < epsilon:\n            # Exploration: Randomly choose a bin that can fit the item\n            # If no bin can fit, this might pick a bin that's too small,\n            # but the bin selection logic will handle it.\n            possible_bins = np.where(bins_remain_cap >= item)[0]\n            if len(possible_bins) > 0:\n                chosen_bin_index = np.random.choice(possible_bins)\n                priorities[chosen_bin_index] = 1.0  # Give equal high priority to explored bins\n            else:\n                # If no bin can fit the item, still assign some low priority\n                priorities[i] = 0.1\n        else:\n            # Exploitation: Choose the bin that provides the best fit (least remaining capacity)\n            if best_bin_index != -1:\n                priorities[best_bin_index] = 1.0  # Highest priority to the best fit bin\n            else:\n                # If no bin can fit the item, all priorities remain 0.\n                # The item won't be placed in any existing bin.\n                pass # All priorities are already 0\n\n    return priorities",
    "response_id": 8,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Command '['python3', '-u', '/home/dokhanhnam1199/QD/problems/bpp_online/eval.py', '5000', '/home/dokhanhnam1199/QD', 'train']' timed out after 49.999966515999404 seconds"
  },
  {
    "stdout_filepath": "problem_iter1_response9.txt_stdout.txt",
    "code_path": "problem_iter1_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit strategy.\n\n    The priority is higher for bins that have remaining capacity greater than or equal to the item size,\n    and among those, it's higher for bins with less remaining capacity (to promote filling bins).\n    Softmax is applied to these scores to get a probability distribution over the bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize scores to a very small negative number to ensure unfit bins get low softmax scores\n    scores = np.full_like(bins_remain_cap, -np.inf)\n\n    # Consider only bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, assign a score inversely proportional to remaining capacity\n    # This favors bins that are closer to being full, promoting tighter packing.\n    # We add a small epsilon to avoid division by zero if a bin has exactly the item's size.\n    scores[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)\n\n    # Apply softmax to convert scores into probabilities (priorities)\n    # Subtract the maximum score before exponentiation to prevent numerical overflow\n    exp_scores = np.exp(scores - np.max(scores))\n    priorities = exp_scores / np.sum(exp_scores)\n\n    # Handle cases where no bin can fit the item (e.g., if the item is too large for all bins)\n    # In such a scenario, exp_scores would be all zeros, leading to NaNs from division by zero sum.\n    # We can assign a uniform low priority or signal this condition, but for this function,\n    # returning zeros is a reasonable default indicating no good placement.\n    if np.sum(priorities) == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    return priorities",
    "response_id": 9,
    "obj": 4.198244914240141,
    "SLOC": 9.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response10.txt_stdout.txt",
    "code_path": "problem_iter1_code10.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This heuristic prioritizes bins that can perfectly fit the item,\n    giving them the highest priority. Among bins that can fit, it\n    then prioritizes those with less remaining capacity (tighter fit),\n    as this aligns with the First Fit strategy of filling bins as much\n    as possible. Bins that cannot fit the item are given a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, assign a priority based on how tightly they fit.\n    # We want to prioritize bins with less remaining capacity (tighter fit).\n    # A simple way to achieve this is to assign a priority that is the inverse\n    # of the remaining capacity (or a value derived from it).\n    # To give exact fits the highest priority, we can use a large base value\n    # and then subtract the remaining capacity.\n    # If a bin fits the item exactly, the remaining capacity after fitting will be 0.\n    # The value `bins_remain_cap[can_fit_mask] - item` represents the remaining capacity\n    # *after* placing the item. A smaller value is better.\n    # To make it a priority score where higher is better, we can negate this or\n    # use a ranking strategy.\n    # Let's use a strategy where perfect fits get the highest score, and among\n    # non-perfect fits, the ones that leave less space are prioritized.\n\n    # Calculate the remaining capacity *after* placing the item\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # Assign priorities:\n    # A high base score for bins that fit.\n    # A penalty for the amount of remaining space, so smaller remaining space is better.\n    # Using the negative of remaining_after_fit will naturally rank tighter fits higher.\n    # Let's assign a base priority to all fitting bins and then adjust based on remaining space.\n    # A simple approach: assign a score inversely proportional to the remaining capacity,\n    # but ensuring exact fits get the highest scores.\n    # One way is to use the remaining capacity directly as priority, so smaller is better.\n    # Then, we can invert it for our \"higher is better\" priority score.\n\n    # Let's try assigning priority based on the 'emptiness' introduced by the item.\n    # The less space left, the higher the priority.\n    # For a bin with capacity C and item size I, the remaining space is C-I.\n    # We want to prioritize smaller (C-I).\n    # So, a priority score could be - (C-I). A smaller (C-I) results in a higher priority.\n\n    # Create a score that is higher for bins with less remaining capacity after fitting.\n    # We can use `bins_remain_cap[can_fit_mask] - item`. A smaller value here means a better fit.\n    # To convert this to a \"higher priority is better\" score, we can take the negative.\n    # To differentiate exact fits from others, we can add a small bonus.\n\n    # Option 1: Simple inverse of remaining space (tighter fits get higher priority)\n    # `bins_remain_cap[can_fit_mask] - item` will be >= 0.\n    # We want smaller values here to have higher priority.\n    # So, `- (bins_remain_cap[can_fit_mask] - item)` works.\n    # For exact fits (remaining capacity is 0), priority is 0.\n    # For bins with capacity 10 and item 3, remaining is 7, priority is -7.\n    # For bins with capacity 10 and item 7, remaining is 3, priority is -3.\n    # This means the tighter fit (-3) gets a higher priority than the looser fit (-7).\n    # This seems reasonable.\n\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # Alternative strategy: rank bins by remaining capacity, with exact fits being special.\n    # Let's assign a large positive value to exact fits, and for others,\n    # use the negative remaining capacity.\n    # exact_fit_mask = (bins_remain_cap == item) & can_fit_mask\n    # priorities[exact_fit_mask] = 1e9  # Large value for exact fits\n    # non_exact_fit_mask = can_fit_mask & ~exact_fit_mask\n    # if np.any(non_exact_fit_mask):\n    #     priorities[non_exact_fit_mask] = -(bins_remain_cap[non_exact_fit_mask] - item)\n\n    # The first approach is simpler and still adheres to the spirit of First Fit\n    # by prioritizing tighter fits. Let's stick with that.\n\n    return priorities",
    "response_id": 10,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response11.txt_stdout.txt",
    "code_path": "problem_iter1_code11.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit.\n\n    The Best Fit strategy prioritizes bins that have just enough remaining capacity\n    to fit the item, aiming to minimize wasted space. This is achieved by\n    calculating a score that is high for bins with small remaining capacity\n    that can still accommodate the item, and lower for bins with much more\n    or insufficient capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, the priority is inversely proportional\n    # to the remaining capacity. We want the tightest fit.\n    # A common way to implement this is to use a large number minus the remaining capacity.\n    # To avoid issues with very large capacities or negative priorities,\n    # we can use a formula that rewards smaller positive differences.\n    # For instance, a small positive remaining capacity should get a high priority.\n    # A simple inverse could be problematic (division by zero or very large numbers).\n    # A better approach is to use a measure of \"how much space is left over\".\n    # The less space left over (but still enough to fit), the better.\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate \"slack\" for bins that can fit the item\n    slack = bins_remain_cap[can_fit_mask] - item\n\n    # Assign higher priorities to bins with less slack (tighter fit)\n    # A simple inverse of slack could be problematic (slack=0 leads to infinity).\n    # Let's use a transformation: a large constant minus slack, or consider\n    # the inverse of (slack + epsilon) to avoid division by zero.\n    # Another approach is to map slack to a priority score, where smaller slack gets a higher score.\n    # We can map slack to a value that decreases as slack increases.\n    # For example, 1 / (slack + 1) (adding 1 to avoid division by zero for slack=0).\n    # Or, more simply, assign a priority proportional to the 'emptiness' after fitting:\n    # Higher priority if remaining_capacity - item is small.\n    # Let's try: priority = max_possible_capacity - (remaining_capacity - item)\n    # If we assume a standard bin capacity `C`, then remaining_capacity - item is the wasted space.\n    # Maximizing `C - (remaining_capacity - item)` is equivalent to minimizing wasted space.\n    # If we don't have `C`, we can simply prioritize bins with smallest `remaining_capacity - item`.\n    # Using `1 / (slack + epsilon)` is a good way to achieve this.\n    # A large number minus slack is also effective if we have a bounded range.\n    # Let's use a scoring mechanism where smaller `slack` results in a larger score.\n    # We can use a penalty function: -slack or a transformation of slack.\n    # A large value for tight fits, smaller for looser fits.\n    # Let's aim for priorities in a range, e.g., [0, 1].\n    # For slack `s`, priority can be `1 / (s + 1)`. This gives higher priority to smaller slack.\n    # A slack of 0 gets priority 1. A slack of 1 gets priority 0.5. A slack of 10 gets priority 0.09.\n\n    # Calculate priorities for bins that can fit\n    # Add a small epsilon to slack to prevent division by zero if slack is exactly 0.\n    epsilon = 1e-9\n    priorities[can_fit_mask] = 1.0 / (slack + epsilon)\n\n    # We can normalize these priorities if needed, but for a greedy selection,\n    # the relative order is what matters.\n    # Alternatively, a large number minus the slack can be used, e.g., 100 - slack.\n    # Let's consider a penalty for bins that are *too* large and would leave\n    # a lot of wasted space even after fitting.\n    # A common Best Fit heuristic is to pick the bin that leaves the *minimum*\n    # remaining capacity. This means we want to minimize `bins_remain_cap - item`.\n\n    # Let's refine this: prioritize bins where `bins_remain_cap - item` is smallest.\n    # We can map `bins_remain_cap - item` to a priority score, where smaller values\n    # result in higher scores.\n    # A simple approach: a large number minus `bins_remain_cap - item`.\n    # If we assume a maximum possible bin capacity `MAX_CAP`, then the worst slack\n    # could be `MAX_CAP - item`. The best slack is 0.\n    # A priority like `MAX_CAP - (bins_remain_cap - item)` would work.\n    # Without `MAX_CAP`, we can simply use `bins_remain_cap - item` and take the minimum.\n    # To make it a priority function returning higher values for better fits:\n    # We can invert it: `1 / (bins_remain_cap - item + epsilon)`.\n    # Let's stick to the `1 / (slack + epsilon)` idea for now, as it directly captures\n    # minimizing the leftover space.\n\n    # Another common approach for Best Fit is to directly use the remaining capacity\n    # that *will* result after placing the item. The bin that results in the\n    # smallest remaining capacity after placement is preferred.\n    # So, for each bin `b` where `bins_remain_cap[b] >= item`, we are interested in\n    # `bins_remain_cap[b] - item`. We want to *minimize* this value.\n    # To turn this into a \"priority score\" where higher is better, we can do:\n    # `large_value - (bins_remain_cap[b] - item)`.\n    # A simple monotonic transformation: `- (bins_remain_cap[b] - item)` would result\n    # in higher scores for smaller `bins_remain_cap[b] - item`.\n    # However, scores should generally be non-negative or scaled appropriately.\n    # Using `1.0 / (bins_remain_cap[b] - item + epsilon)` achieves this.\n\n    # Let's create a simpler priority directly based on the goal:\n    # Find the bin `i` such that `bins_remain_cap[i] - item` is minimized, among valid bins.\n    # We can assign a priority score proportional to the negative of this difference.\n    # Or, to ensure positive priorities and larger for better fits,\n    # a large constant minus the difference.\n    # Consider the range of possible remaining capacities.\n    # If all bins are too small, priorities are 0.\n    # If a bin fits perfectly (slack=0), its priority should be maximal among valid bins.\n    # If a bin has a large slack, its priority should be low.\n\n    # Final attempt with a clear Best Fit interpretation:\n    # The goal is to find the bin that, after placing the item, has the *least*\n    # remaining capacity. This minimizes wasted space.\n    # So, for a bin `i` that can fit the item, the \"best fit\" is the one with the smallest\n    # `bins_remain_cap[i] - item`.\n    # Let `diff = bins_remain_cap[i] - item`. We want to prioritize smaller `diff`.\n    # We can assign priority as `1 / (diff + epsilon)`.\n\n    # Let's consider an alternative interpretation that's common in some priority schemes:\n    # How *likely* is this item to fit in this bin without much wasted space?\n    # A bin with very little remaining capacity is a good candidate if it can fit the item.\n    # A bin with *just* enough capacity is better than a bin with vastly more capacity.\n\n    # Let's try a different approach that might be more intuitive as a \"priority score\".\n    # For bins that can fit the item:\n    # Calculate the 'emptiness' after placing the item: `bins_remain_cap - item`.\n    # We want to prioritize bins with smaller 'emptiness'.\n    # Let's create a score where smaller `emptiness` results in a higher score.\n    # Consider `max_possible_emptiness - emptiness`.\n    # If we don't know `max_possible_emptiness`, we can use a large number.\n    # Or, simply, `-(emptiness)` would assign higher scores to smaller (more negative) values.\n    # But we want positive scores.\n    # A score of `1 / (emptiness + epsilon)` works well.\n\n    # Let's try to assign priorities reflecting the \"goodness of fit\" directly.\n    # A perfect fit (remaining_capacity == item) is ideal.\n    # A fit with remaining_capacity slightly larger than item is next best.\n    # A fit with remaining_capacity much larger is less desirable.\n    # A fit that cannot hold the item is undesirable (priority 0).\n\n    # The most direct Best Fit score would be the negative of the remaining capacity\n    # after placing the item, for bins that can accommodate it.\n    # This means minimizing `bins_remain_cap[i] - item`.\n    # So, `priority = -(bins_remain_cap[i] - item)`.\n    # To make priorities positive and avoid large negative numbers:\n    # Let's use `max_slack - (bins_remain_cap[i] - item)` where `max_slack` is the maximum\n    # possible slack for a fitting bin, or some sufficiently large number.\n    # A simpler way that often works is `1.0 / (bins_remain_cap[i] - item + epsilon)`.\n\n    # Let's simplify and ensure we are truly implementing the \"best fit\" concept:\n    # Best Fit aims to leave the smallest possible remaining capacity in the bin.\n    # For each bin `i`:\n    # If `bins_remain_cap[i] < item`, priority is 0.\n    # If `bins_remain_cap[i] >= item`, the \"goodness\" of this fit is measured by\n    # `bins_remain_cap[i] - item`. A smaller value is better.\n    # We want higher priority for smaller `bins_remain_cap[i] - item`.\n    # This can be achieved by `score = K - (bins_remain_cap[i] - item)` for some constant `K`.\n    # Or, `score = 1 / (bins_remain_cap[i] - item + epsilon)`.\n\n    # Let's implement `1 / (slack + epsilon)` more directly.\n    # `slack = bins_remain_cap[can_fit_mask] - item`\n    # For bins that can fit:\n    # Prioritize by `1.0 / (slack + epsilon)` to get highest priority for smallest slack.\n\n    # The code already calculated this. Let's add a comment explaining it clearly.\n    # The priorities are now derived from the `slack`. Bins with smaller `slack`\n    # (i.e., `bins_remain_cap - item` is smaller) get a higher priority score.\n    # This aligns with the Best Fit strategy.\n\n    # The logic:\n    # 1. Identify which bins can hold the `item`.\n    # 2. For those bins, calculate the `slack` (remaining capacity - item size).\n    # 3. The best fit is the bin with the *minimum* slack.\n    # 4. To convert \"minimum slack\" into a \"highest priority score\", we use an inverse relationship.\n    #    `priority = 1 / (slack + epsilon)` ensures that as slack decreases, priority increases.\n    #    A slack of 0 yields a priority of `1/epsilon` (very high).\n    #    A larger slack yields a priority closer to 0.\n    # 5. Bins that cannot fit the item get a priority of 0.\n\n    return priorities",
    "response_id": 11,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response12.txt_stdout.txt",
    "code_path": "problem_iter1_code12.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    The Worst Fit strategy aims to place the item in the bin with the largest remaining capacity.\n    This heuristic is generally not good for minimizing the number of bins, but it's a valid strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to zero\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # For each bin, if the item can fit, assign a priority equal to the remaining capacity.\n    # Otherwise, assign a priority of 0, meaning it cannot be placed in that bin.\n    # The higher the remaining capacity, the higher the priority for Worst Fit.\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask]\n\n    return priorities",
    "response_id": 12,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response13.txt_stdout.txt",
    "code_path": "problem_iter1_code13.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    The \"Almost Full Fit\" strategy prioritizes bins that can accommodate the item\n    and have a remaining capacity that is just slightly larger than the item's size.\n    This aims to leave larger gaps in other bins for potentially larger future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Consider only bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the \"tightness\" of the fit: how much space is left after adding the item\n    # We want this value to be small for the \"almost full\" bins.\n    # We add a small epsilon to avoid division by zero if a bin is exactly full after adding the item.\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # To prioritize bins that are \"almost full\" (small remaining capacity),\n    # we can invert the remaining_after_fit or use a function that assigns higher\n    # values to smaller remaining capacities. A simple inversion with a small\n    # offset can work.\n    # We want smaller positive values of (bins_remain_cap - item) to have higher priority.\n    # For example, if remaining_after_fit is 0.1, priority should be high.\n    # If remaining_after_fit is 1.0, priority should be lower.\n    # Using 1 / (remaining_after_fit + epsilon) is one way.\n    # Another way is to use a penalty for larger remaining capacities.\n    # Let's define \"almost full\" as having remaining capacity within a small margin of 0.\n    \n    # Option 1: Invert the remaining capacity after fit. A smaller value means a tighter fit.\n    # Add a small constant to avoid division by zero and to ensure positive priorities.\n    # Higher value means higher priority.\n    epsilon = 1e-9\n    # If remaining_after_fit is very close to 0, 1/epsilon becomes large, giving high priority.\n    priorities[can_fit_mask] = 1.0 / (remaining_after_fit + epsilon)\n\n    # Option 2 (Alternative): Use a penalty function.\n    # Penalize bins that have a lot of leftover space.\n    # Let's try to make bins that have remaining_after_fit = 0 the highest priority,\n    # and then decrease priority as remaining_after_fit increases.\n    # We could use something like `np.exp(-k * remaining_after_fit)` where k is a scaling factor.\n    # Or a simpler linear penalty: `max_remaining_after_fit - remaining_after_fit`.\n    # Let's stick to the inverse for now as it's a common heuristic approach for \"tight fit\".\n\n    # For bins that cannot fit the item, their priority remains 0 (or effectively negative if we consider strict ordering).\n    # The logic here is that if an item cannot fit, it shouldn't be considered for placement.\n    \n    return priorities",
    "response_id": 13,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response14.txt_stdout.txt",
    "code_path": "problem_iter1_code14.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    The Exact Fit First strategy prioritizes bins where the remaining capacity is\n    exactly equal to the item size. If multiple bins have this exact fit,\n    it might offer a secondary criterion. For simplicity, this implementation\n    gives a high priority (1.0) to exact fits and a lower priority (0.0) otherwise.\n    In a more sophisticated version, one could break ties by choosing the bin\n    with the smallest remaining capacity among the exact fits, or by considering\n    the number of items already in the bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find bins where the remaining capacity is exactly the item size\n    exact_fit_indices = np.where(bins_remain_cap == item)[0]\n\n    # Assign a high priority to bins that are an exact fit\n    priorities[exact_fit_indices] = 1.0\n\n    # Optional: If there are exact fits, you might want to further refine\n    # priorities. For this simple version, we'll just use 1.0 for exact fits.\n    # A more advanced strategy might also consider bins that have a very small\n    # remaining capacity after placing the item, but still can accommodate it.\n    # For pure Exact Fit First, we only care about the exact match.\n\n    return priorities",
    "response_id": 14,
    "obj": 4.198244914240141,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response15.txt_stdout.txt",
    "code_path": "problem_iter1_code15.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit) strategy.\n\n    The strategy prioritizes bins that have a remaining capacity closest to the item's size.\n    This aims to \"fit\" the item snugly into a bin, minimizing wasted space in that specific bin.\n    We calculate the absolute difference between the item size and the remaining capacity of each bin.\n    The inverse of this difference serves as the priority. A smaller difference (better fit)\n    results in a higher priority score. To avoid division by zero if an item perfectly fits,\n    we add a small epsilon.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Ensure item fits in the bin, otherwise, it's not a viable option.\n    # Bins with remaining capacity less than the item size get a priority of 0.\n    viable_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate the \"closeness\" of the remaining capacity to the item size.\n    # We want bins where remaining_capacity is close to item_size.\n    # A smaller difference is better.\n    differences = bins_remain_cap[viable_bins_mask] - item\n\n    # We want to prioritize smaller differences. So, we invert the differences.\n    # Add a small epsilon to avoid division by zero when the difference is exactly 0.\n    epsilon = 1e-9\n    priorities[viable_bins_mask] = 1.0 / (differences + epsilon)\n\n    # Handle cases where no bins are viable.\n    if not np.any(viable_bins_mask):\n        return np.zeros_like(bins_remain_cap)\n\n    return priorities",
    "response_id": 15,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response16.txt_stdout.txt",
    "code_path": "problem_iter1_code16.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score prioritizes bins that are a \"good fit\" for the item,\n    meaning they have a remaining capacity that is slightly larger than the item's size.\n    This helps to minimize wasted space.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Ensure item size is positive\n    item = max(0.0, item)\n\n    # Calculate the difference between bin remaining capacity and item size\n    diffs = bins_remain_cap - item\n\n    # We want to prioritize bins where diffs are small and positive (good fit)\n    # and penalize bins that are too small (negative diffs) or too large (large positive diffs).\n    # A sigmoid function can map these differences to a priority score between 0 and 1.\n    # We can shift and scale the differences to center the \"ideal fit\" around 0.\n    # A reasonable approach is to use the item size as a scaling factor for the difference.\n\n    # Avoid division by zero if item is 0, and handle cases where bins_remain_cap might be 0\n    # Use a small epsilon to prevent division by zero issues\n    epsilon = 1e-9\n    scaled_diffs = np.where(bins_remain_cap > epsilon, diffs / bins_remain_cap, -1.0)\n\n    # The sigmoid function is 1 / (1 + exp(-x)).\n    # We want a higher score for smaller positive differences (closer to 0).\n    # A negative scaled difference means the bin is too small.\n    # A large positive scaled difference means the bin has a lot of excess capacity.\n    # We can use a transformation of the scaled difference that is high around 0 and decreases\n    # as the difference becomes more positive or more negative.\n    # A simple approach is to use exp(-abs(scaled_diffs)).\n    # However, to leverage the \"good fit\" idea more directly, we can aim for a peak near diff=0.\n    # A modified sigmoid could work: sigmoid(k * (ideal_fit_capacity - remaining_capacity))\n    # Let's reframe: we want to maximize the value for bins where remaining_capacity is close to item size.\n\n    # Strategy: Prioritize bins that have *just enough* capacity.\n    # This means a positive difference that is as small as possible.\n    # Bins that are too small (negative difference) should have a low priority.\n    # Bins that have a lot of excess capacity (large positive difference) should also have lower priority than a \"just fit\".\n\n    # Let's try a sigmoid centered around an \"ideal\" fit where the remaining capacity is item + some small buffer.\n    # Or, more directly, penalize large positive differences.\n\n    # Consider diffs:\n    # diff < 0: Bin too small. Low priority.\n    # diff = 0: Perfect fit. High priority.\n    # diff > 0: Bin has excess capacity.\n    #   - Small diff > 0: Good fit. High priority.\n    #   - Large diff > 0: Wasteful. Lower priority than good fit.\n\n    # A function that peaks at diff = 0 and drops off as |diff| increases.\n    # exp(-abs(diffs)) might work, but let's try something more sigmoid-like.\n\n    # Sigmoid applied to negative differences to boost near-zero differences.\n    # We want to map diffs to a [0, 1] range where higher is better.\n    # For diffs < 0 (too small), priority should be near 0.\n    # For diffs >= 0 (sufficient capacity), priority should be higher, peaking near diff=0.\n\n    # A shifted and scaled sigmoid:\n    # Let's center the sigmoid around 0, so sigmoid(0) = 0.5.\n    # We can adjust the steepness (k) and the midpoint.\n    # The input to sigmoid could be related to diffs.\n    # We want higher priority for diffs closer to 0.\n\n    # Let's try mapping diffs to a range where the most desirable values are between 0 and a small positive number.\n    # We can transform diffs into something that reaches its maximum when diff is small and positive.\n\n    # Option 1: Focus on the \"tight fit\" aspect.\n    # Prioritize bins where capacity is slightly larger than the item.\n    # This means maximizing a function that is high for small positive diffs and lower for negative diffs or large positive diffs.\n    # We can use a sigmoid on the *negative* of the difference, scaled appropriately.\n    # `sigmoid(k * (item - bin_remain_cap))` might work, but it penalizes bins that are too small too heavily.\n\n    # Let's try a combination:\n    # 1. Filter out bins that are too small (capacity < item). Give them a very low priority.\n    # 2. For bins that are large enough (capacity >= item), use a sigmoid that peaks when capacity is just slightly larger than item.\n\n    # Using the remaining capacity directly, we want to be closer to `item`.\n    # Let's scale the differences by the item size to make it relative.\n    # `scaled_relative_diffs = diffs / (item + epsilon)`\n    # A sigmoid on negative scaled diffs: `1 / (1 + exp(-k * scaled_relative_diffs))`\n    # This will be high if `scaled_relative_diffs` is positive and small.\n\n    # Sigmoid function: sigmoid(x) = 1 / (1 + exp(-x))\n    # We want a higher score when `bins_remain_cap` is just above `item`.\n    # Let `x = -(bins_remain_cap - item) / item`  (this is `(item - bins_remain_cap) / item`)\n    # If `bins_remain_cap` is slightly larger than `item`, `item - bins_remain_cap` is small negative, so `x` is small positive. Sigmoid is near 0.5.\n    # If `bins_remain_cap` is much larger than `item`, `item - bins_remain_cap` is large negative, so `x` is large positive. Sigmoid is near 1. This is not what we want.\n\n    # Let's reconsider the goal: \"good fit\" implies the remaining capacity is just enough.\n    # This suggests we want `bins_remain_cap` to be close to `item`.\n    # If `bins_remain_cap < item`, it's unusable. Priority 0.\n    # If `bins_remain_cap == item`, it's a perfect fit. High priority.\n    # If `bins_remain_cap > item`, there's waste. Priority decreases as waste increases.\n\n    # Let's try mapping `bins_remain_cap` to a priority.\n    # Bins with `bins_remain_cap < item` get priority 0.\n    # Bins with `bins_remain_cap >= item` get a score based on how close `bins_remain_cap` is to `item`.\n    # A suitable function for this could be `exp(-k * (bins_remain_cap - item))` where k is a positive constant.\n    # This function is 1 when `bins_remain_cap == item` and decreases as `bins_remain_cap` increases.\n\n    # We can normalize `bins_remain_cap` and `item` relative to a hypothetical bin capacity or maximum possible item size.\n    # For simplicity, let's work with the absolute differences and a sigmoid transformation.\n\n    # Let's use a sigmoid on a transformation of `diffs`.\n    # We want higher priority for `diffs` near zero and positive.\n    # Consider `sigmoid(k * (max_reasonable_waste - diffs))`.\n    # If `diffs` is negative (too small), this will be `sigmoid(k * (max_reasonable_waste - neg_value))`, which is large. This is also not ideal.\n\n    # Let's use a sigmoid to map `diffs` to a range.\n    # We want `diffs` to be small and positive for high priority.\n    # `sigmoid(constant - k * diffs)`:\n    #   - If diffs is small positive, `k * diffs` is small positive, `constant - k*diffs` is large. Sigmoid is near 1.\n    #   - If diffs is large positive, `k * diffs` is large positive, `constant - k*diffs` is small negative. Sigmoid is near 0.\n    #   - If diffs is negative, `k * diffs` is small negative, `constant - k*diffs` is large positive. Sigmoid is near 1.\n\n    # This means we need to specifically handle the `diffs < 0` case.\n\n    # Let's filter out invalid bins first.\n    valid_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap)\n\n    if np.any(valid_bins_mask):\n        valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]\n        valid_diffs = valid_bins_remain_cap - item\n\n        # We want to prioritize smaller `valid_diffs`.\n        # Let's use a sigmoid that maps small positive numbers to high values and larger positive numbers to lower values.\n        # `sigmoid(k * (A - x))` where `x` is `valid_diffs`. We want `A` to represent an ideal surplus.\n        # A simple choice is to center it around 0 or a small positive value.\n\n        # Let's scale the differences relative to the item size to handle varying item scales.\n        # `scaled_valid_diffs = valid_diffs / (item + epsilon)`\n        # Now, we want `scaled_valid_diffs` to be close to 0.\n        # Let's use `sigmoid(k * (-scaled_valid_diffs))`.\n        # If `scaled_valid_diffs` is small positive (good fit), `-scaled_valid_diffs` is small negative. Sigmoid is < 0.5.\n        # If `scaled_valid_diffs` is near 0 (perfect fit), `-scaled_valid_diffs` is near 0. Sigmoid is 0.5.\n        # If `scaled_valid_diffs` is large positive (wasteful), `-scaled_valid_diffs` is large negative. Sigmoid is near 0.\n        # This is also not quite right.\n\n        # Let's re-frame for \"Sigmoid Fit Score\": prioritize bins that are a \"close fit\".\n        # A close fit means `bins_remain_cap` is close to `item`.\n        # Let `score = sigmoid(alpha - beta * abs(bins_remain_cap - item))`\n        # This score peaks when `abs(bins_remain_cap - item)` is minimal.\n        # However, we must also account for `bins_remain_cap < item`.\n\n        # A more common Sigmoid Fit strategy aims to find a bin whose remaining capacity is just enough.\n        # If `remaining_capacity < item`, priority is 0.\n        # If `remaining_capacity >= item`, the priority is `sigmoid((bin_capacity - item) / bin_capacity)` or similar.\n\n        # Let's try this: a higher priority for bins that have a smaller proportion of wasted space.\n        # Wasted space proportion: `(bins_remain_cap - item) / bins_remain_cap` for valid bins.\n        # We want to maximize this value, meaning minimize `(item / bins_remain_cap)`.\n        # Let `ratio = item / valid_bins_remain_cap`. We want to maximize `1 - ratio`.\n        # A higher value for `1-ratio` means `ratio` is smaller, i.e., `item` is smaller relative to `bins_remain_cap`.\n        # This means a lot of wasted space. This is counter-intuitive to \"fit score\".\n\n        # The \"fit\" part usually refers to the item filling up the bin as much as possible.\n        # This means prioritizing bins where `item / bins_remain_cap` is close to 1.\n        # This implies `bins_remain_cap` is close to `item`.\n\n        # Let's try this formulation for valid bins:\n        # Prioritize bins where `bins_remain_cap` is close to `item`.\n        # The input to the sigmoid should be high when `bins_remain_cap` is close to `item` (and >= item).\n        # Let `x = (bins_remain_cap - item)`.\n        # We want high scores for small positive `x`.\n        # `sigmoid(k * (some_value - x))`. If `some_value` is 0, we get `sigmoid(-k*x)`.\n        # If x is small positive, -kx is small negative, sigmoid < 0.5.\n        # If x is 0, sigmoid is 0.5.\n        # If x is large positive, -kx is large negative, sigmoid near 0.\n        # This prioritizes exact fits.\n\n        # We can shift the sigmoid: `sigmoid(k * (C - x))` where C is some desired surplus.\n        # Or, we can use `sigmoid(k * (-x))` and then `1 - sigmoid(-k * x)` which is `sigmoid(k*x)`.\n        # `sigmoid(k * x)`:\n        #   - x small positive (good fit): sigmoid near 0.5\n        #   - x zero (perfect fit): sigmoid is 0.5\n        #   - x large positive (wasteful): sigmoid near 1\n        # This seems to prioritize larger remaining capacities.\n\n        # Let's try a sigmoid on the inverse of the relative waste:\n        # `relative_waste = (bins_remain_cap - item) / bins_remain_cap`\n        # `inverse_relative_waste = bins_remain_cap / (bins_remain_cap - item)` if diff > 0.\n        # We want to maximize `item / bins_remain_cap`.\n\n        # Sigmoid Fit typically means making `bins_remain_cap` just slightly larger than `item`.\n        # Let `f(capacity) = sigmoid(k * (item - capacity))`. This maps values smaller than `item` to high priority, which is wrong.\n\n        # Let's map `bins_remain_cap` for `valid_bins_mask` to priorities.\n        # Consider the distance from a \"perfect\" fit.\n        # `distance_from_perfect = valid_bins_remain_cap - item`\n        # We want this distance to be small.\n        # A sigmoid centered around 0: `sigmoid(k * (-distance_from_perfect))`\n        # If distance is 0, input is 0, output is 0.5.\n        # If distance is small positive, input is small negative, output < 0.5.\n        # If distance is large positive, input is large negative, output ~ 0.\n        # This prioritizes bins that are too small.\n\n        # How about scaling the difference relative to the item?\n        # `scaled_diff = (valid_bins_remain_cap - item) / (item + epsilon)`\n        # We want this to be small and positive.\n        # Use `sigmoid(k * (C - scaled_diff))` where C is a small positive constant.\n        # Let C = 0.1 (allow up to 10% surplus).\n        # Let k = 10 (steepness).\n\n        # `sigmoid(10 * (0.1 - scaled_diff))`\n        # If `scaled_diff` = 0 (perfect fit), input is 10 * 0.1 = 1. Sigmoid = 0.73.\n        # If `scaled_diff` = 0.1 (10% surplus), input is 10 * (0.1 - 0.1) = 0. Sigmoid = 0.5.\n        # If `scaled_diff` = 0.2 (20% surplus), input is 10 * (0.1 - 0.2) = -1. Sigmoid = 0.27.\n        # If `scaled_diff` = -0.05 (item is 5% larger than capacity, effectively this is a small fit for item's perspective, but our `valid_bins_mask` prevents this)\n        # Let's re-evaluate `valid_bins_mask = bins_remain_cap >= item`.\n        # So `valid_diffs >= 0`, and `scaled_diff >= 0`.\n\n        # We want high priority for small `scaled_diff`.\n        # Use `sigmoid(k * (max_desired_scaled_diff - scaled_diff))`\n        # Let `max_desired_scaled_diff = 0.2` (allow up to 20% surplus)\n        # Let `k = 10`\n\n        # `sigmoid(10 * (0.2 - scaled_diff))`\n        # If `scaled_diff` = 0 (perfect fit), input is 10 * 0.2 = 2. Sigmoid = 0.88.\n        # If `scaled_diff` = 0.1 (10% surplus), input is 10 * (0.2 - 0.1) = 1. Sigmoid = 0.73.\n        # If `scaled_diff` = 0.2 (20% surplus), input is 10 * (0.2 - 0.2) = 0. Sigmoid = 0.5.\n        # If `scaled_diff` = 0.3 (30% surplus), input is 10 * (0.2 - 0.3) = -1. Sigmoid = 0.27.\n\n        # This looks reasonable. It prioritizes bins that have a smaller surplus relative to the item size.\n        # The `max_desired_scaled_diff` parameter controls how much surplus is tolerated before priority drops significantly.\n\n        # Let's refine this: the sigmoid argument should probably be based on the *ratio* of item to capacity for a direct \"filling\" measure.\n        # If `bins_remain_cap` is slightly larger than `item`, the ratio `item / bins_remain_cap` is close to 1.\n        # Let's try mapping this ratio to priority.\n        # `ratio = item / valid_bins_remain_cap`\n        # We want to maximize this ratio, but ensure `ratio <= 1`.\n        # `sigmoid(k * (ratio - 1))` would map ratios slightly less than 1 to values < 0.5, and ratios close to 0 to values near 0.\n        # `sigmoid(k * (1 - ratio))` would map ratios slightly less than 1 to values > 0.5, and ratios close to 0 to values near 1.\n\n        # The latter is closer to our goal: prioritizing bins that are \"filled\" more.\n        # `sigmoid(k * (1 - item / valid_bins_remain_cap))`\n        # If `valid_bins_remain_cap` is just above `item`, `item / valid_bins_remain_cap` is slightly less than 1.\n        # `1 - (slightly less than 1)` is small positive. Sigmoid is > 0.5.\n        # If `valid_bins_remain_cap` is much larger than `item`, `item / valid_bins_remain_cap` is small positive.\n        # `1 - (small positive)` is close to 1. Sigmoid is near 1. This means it prioritizes large empty bins!\n\n        # Okay, let's go back to `scaled_diff = (valid_bins_remain_cap - item) / (item + epsilon)`.\n        # We want small positive `scaled_diff`.\n        # `sigmoid(k * (MAX_SURPLUS_RATIO - scaled_diff))`\n        # `MAX_SURPLUS_RATIO` can be a tunable parameter, e.g., 0.1 for 10% surplus.\n\n        MAX_SURPLUS_RATIO = 0.2  # Allow up to 20% of item size as surplus\n        STEEPNESS = 10.0         # Controls how quickly priority drops after MAX_SURPLUS_RATIO\n\n        # Calculate scaled differences for valid bins\n        # Using item size for scaling helps normalize the surplus\n        scaled_valid_diffs = valid_diffs / (item + epsilon)\n\n        # Apply the sigmoid transformation\n        # The argument is `STEEPNESS * (MAX_SURPLUS_RATIO - scaled_valid_diffs)`\n        # This will yield high values for `scaled_valid_diffs` close to 0 and less than `MAX_SURPLUS_RATIO`.\n        # It will yield lower values for `scaled_valid_diffs` larger than `MAX_SURPLUS_RATIO`.\n        sigmoid_input = STEEPNESS * (MAX_SURPLUS_RATIO - scaled_valid_diffs)\n        \n        # Ensure that we don't get numerically unstable values for sigmoid input\n        sigmoid_input = np.clip(sigmoid_input, -20, 20) # To prevent overflow in exp\n\n        priorities[valid_bins_mask] = 1 / (1 + np.exp(-sigmoid_input))\n\n    return priorities",
    "response_id": 16,
    "obj": 4.487435181491823,
    "SLOC": 17.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response17.txt_stdout.txt",
    "code_path": "problem_iter1_code17.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Random Fit strategy.\n\n    In Random Fit, we randomly select a bin from the available bins that can\n    accommodate the item. The priority function here will assign a higher\n    priority to bins that have enough capacity for the item, and among those,\n    it will give a slight preference to bins that will leave less remaining\n    capacity after placing the item (i.e., better fit). The randomness is\n    introduced by adding a small random noise.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    sufficient_capacity_mask = bins_remain_cap >= item\n\n    # For bins with sufficient capacity, assign a priority based on how well they fit.\n    # A better fit means less remaining capacity after placing the item.\n    # We want higher priority for smaller remaining capacity.\n    # Therefore, priority is inversely proportional to (bins_remain_cap - item).\n    # We can achieve this by assigning a value like 1 / (bins_remain_cap - item + epsilon)\n    # or more simply, by assigning a value that is lower for larger remaining capacities.\n    # Let's use the negative of the remaining capacity after placing the item.\n    # The \"best\" fit (minimum remaining capacity) will have the highest negative value,\n    # which when adjusted will give higher priority.\n\n    # A simple approach for \"better fit\" could be to assign a score based on\n    # the remaining capacity *after* placing the item. Lower is better.\n    # To make it a priority (higher is better), we can invert this.\n    # For example, (max_possible_remaining - actual_remaining_after_fit).\n    # A simpler heuristic is to favor bins that will be *almost full* after fitting.\n    # So, prioritize bins where `bins_remain_cap - item` is small.\n\n    # Let's assign a base priority to bins that can fit the item.\n    # Then, we can add a bonus for a tighter fit.\n\n    # Assign a higher score to bins that can fit the item.\n    # For bins that can fit, calculate the \"tightness\" of the fit.\n    # A tighter fit means bins_remain_cap - item is smaller.\n    # Let's assign a score that's high for tight fits and low for loose fits.\n    # We can use the reciprocal of the remaining capacity after fitting.\n    # Ensure we don't divide by zero if a bin becomes exactly full.\n\n    fitting_bins_indices = np.where(sufficient_capacity_mask)[0]\n\n    if fitting_bins_indices.size > 0:\n        # Calculate the remaining capacity *after* placing the item for fitting bins\n        remaining_after_fit = bins_remain_cap[fitting_bins_indices] - item\n\n        # Assign a base priority to bins that can fit. A common strategy\n        # in bin packing is to try and fill bins as much as possible.\n        # We can use the inverse of the remaining capacity as a proxy for \"goodness of fit\".\n        # Smaller remaining capacity means better fit.\n        # We want higher priority for better fits, so we can use something like:\n        # priority = 1 / (remaining_capacity + epsilon) or a linear scale that\n        # increases with tightness.\n\n        # Let's use a simple score that rewards tighter fits.\n        # If remaining_after_fit is 0, it's the best fit. If it's large, it's a loose fit.\n        # We want higher scores for smaller remaining_after_fit.\n        # A simple transformation: max_possible_residual - actual_residual\n        # Where max_possible_residual is some upper bound, or simply that\n        # any positive remaining capacity contributes to a score.\n        # Let's try assigning a score proportional to the negative of remaining capacity,\n        # then adding a random component for the \"Random Fit\" aspect.\n\n        # For bins that can fit, their priority is based on how little capacity\n        # will be left. A smaller `bins_remain_cap - item` should get higher priority.\n        # Let's assign a score that is 100 - (remaining_capacity_after_fit)\n        # This way, 0 remaining capacity gets priority 100, and so on.\n        # To ensure it's \"Random Fit\", we add a small random noise to all valid bins.\n        # This noise should be structured to give *some* preference but not deterministic.\n\n        # Let's assign priority proportional to (bin_capacity - item).\n        # No, we want to penalize bins that leave a lot of space.\n        # So we want to reward bins that leave little space.\n        # Score = (Bin_Capacity - item) reversed, maybe?\n        # Or use a function that increases as (bin_capacity - item) decreases.\n\n        # For Random Fit, the key is random selection among feasible bins.\n        # We can assign a uniform high priority to all feasible bins, and then\n        # randomly pick among them. Or, we can add randomness to the priority scores.\n\n        # Let's make priority related to how full the bin will become.\n        # Prioritize bins that will be closer to full.\n        # Score = (Bin_Capacity - item). Higher score means more is used.\n        # If capacity is 10, item is 7. Remaining is 3. We want this to have higher priority than\n        # capacity 10, item 2. Remaining is 8.\n        # So, priority score could be related to `item / bins_remain_cap`. No, this is wrong.\n\n        # Let's use the complementary idea: priority is higher for bins that will have\n        # *less* remaining capacity after the item is placed.\n        # So, priority is higher for smaller values of `bins_remain_cap[i] - item`.\n        # Let's define a score: `score = 1.0 / (bins_remain_cap[i] - item + 1e-6)` for fitting bins.\n        # This gives higher scores to bins that are nearly full.\n\n        # Random Fit: give all fitting bins equal high priority and then randomly pick one.\n        # Or, add a random noise to the priorities of fitting bins.\n\n        # Let's assign a baseline priority to all bins that fit.\n        # Then, for a tighter fit, give a small bonus.\n        # And for \"Random Fit\", add random noise.\n\n        # Assign a base score for fitting bins. Let's say 1.\n        # Then add a bonus for tighter fit. Tighter fit means `bins_remain_cap - item` is small.\n        # Let's make the bonus proportional to the inverse of remaining capacity.\n        # bonus = 1 / (bins_remain_cap[i] - item + 1e-6)  -- but this can be very large.\n\n        # Simpler approach: Assign a score based on the remaining capacity *after* fitting.\n        # We want bins that leave less capacity to have higher priority.\n        # Let's assign `priority = (max_capacity_possible) - (bins_remain_cap[i] - item)`.\n        # This doesn't quite capture the \"random fit\" well.\n\n        # For Random Fit, we typically iterate through bins and pick the first one that fits.\n        # If we want a priority function, we should give some weight.\n        # A simple \"Random Fit\" heuristic: give a random priority to all bins that can fit.\n        # For example, assign a random number between 0 and 1 to bins that fit.\n        # Or, assign a random priority value, perhaps emphasizing the best fits slightly.\n\n        # Let's create a base priority for bins that fit.\n        # We want to prioritize bins that have less space remaining *after* the item is placed.\n        # So, if bin_rem_cap = 10, item = 7, remaining = 3.\n        # If bin_rem_cap = 10, item = 2, remaining = 8.\n        # We prefer the first case (remaining = 3) for a tighter fit.\n        # A higher priority should go to smaller remaining capacity.\n        # Let's use a priority inversely related to remaining capacity after fit.\n        # A simple way: `priority = some_large_value - (bins_remain_cap - item)` for fitting bins.\n\n        # A more direct \"Random Fit\" priority would be to simply assign a random number\n        # to all bins that fit, and a zero to those that don't.\n        # The highest random number among fitting bins wins.\n\n        # Option 1: Assign random scores to fitting bins.\n        random_scores = np.random.rand(bins_remain_cap.shape[0])\n        priorities[sufficient_capacity_mask] = random_scores[sufficient_capacity_mask]\n\n        # Option 2: Give a slight preference to better fits, then add randomness.\n        # For bins that can fit, calculate remaining capacity after placement.\n        # `remaining_after_fit = bins_remain_cap[sufficient_capacity_mask] - item`\n        # A good fit means `remaining_after_fit` is small.\n        # Let's assign a score that increases as `remaining_after_fit` decreases.\n        # `fit_score = 1.0 / (remaining_after_fit + 1e-6)`\n        # Now add randomness to this fit_score.\n        # `priorities[sufficient_capacity_mask] = fit_score + np.random.rand(fitting_bins_indices.size) * 0.1`\n\n        # For a simple \"Random Fit\", assigning random priorities to feasible bins is appropriate.\n        # The higher the random priority, the more likely that bin is chosen among feasible ones.\n        # This means the selection will be random among available bins.\n\n        # Let's assign a value that is based on the remaining capacity but also randomized.\n        # A simple \"Random Fit\" prioritizes any bin that fits, and the selection is random among them.\n        # So, assigning a random priority to all bins that fit effectively achieves this.\n\n        # The core of Random Fit is to randomly select an eligible bin.\n        # If we need a priority score for *each* bin, then for eligible bins,\n        # we should assign scores such that a random selection is likely.\n        # A uniform random score for all eligible bins achieves this.\n\n        # Let's refine this: assign a high random score to bins that fit.\n        # This makes them more likely to be chosen than bins that don't fit.\n        # The selection among those with high random scores will be random.\n        priorities = np.zeros_like(bins_remain_cap, dtype=float)\n        eligible_bins = np.where(bins_remain_cap >= item)[0]\n\n        if eligible_bins.size > 0:\n            # Assign a random priority to each eligible bin.\n            # Higher random numbers increase the chance of selection.\n            priorities[eligible_bins] = np.random.rand(eligible_bins.shape[0])\n        else:\n            # If no bin can fit the item, this function might not be called,\n            # or it implies an unsolveable state for this item.\n            # Return zeros or signal an error as appropriate.\n            pass\n\n    return priorities",
    "response_id": 17,
    "obj": 73.62385321100918,
    "SLOC": 15.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response18.txt_stdout.txt",
    "code_path": "problem_iter1_code18.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    This heuristic prioritizes bins that can fit the item, with a bias towards\n    bins that would leave less remaining capacity (i.e., a tighter fit).\n    A small epsilon probability is used to explore other bins that might not\n    be the best immediate fit but could be beneficial in the long run.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Exploration probability\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, assign a priority based on the tightness of the fit.\n    # A tighter fit (smaller remaining capacity after packing) gets a higher score.\n    # We use the negative of the remaining capacity to achieve this.\n    # Adding a small constant to avoid log(0) issues or negative infinity if capacity is exactly item size.\n    # We invert this to make smaller remaining capacities (better fits) have higher priority.\n    tight_fit_priority = - (bins_remain_cap[can_fit_mask] - item) + 1e-9\n    priorities[can_fit_mask] = tight_fit_priority\n\n    # Introduce exploration: with probability epsilon, all bins get an equal (low) priority.\n    # This encourages trying bins that might not be the immediate best choice.\n    num_bins = len(bins_remain_cap)\n    explore_indices = np.random.choice(num_bins, size=int(num_bins * epsilon), replace=False)\n    \n    # Assign a baseline priority to all bins for exploration,\n    # slightly lower than the \"best\" greedy choices to encourage exploration\n    # without completely disregarding good fits.\n    baseline_priority = np.min(tight_fit_priority) - 1 if np.any(can_fit_mask) else 0\n\n    priorities[explore_indices] = baseline_priority\n\n    return priorities",
    "response_id": 18,
    "obj": 4.078579976067022,
    "SLOC": 11.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit strategy.\n\n    This strategy prioritizes bins that can accommodate the item and have a remaining capacity\n    that, after placing the item, leaves a \"good fit\". The \"good fit\" is approximated by\n    considering the relative \"tightness\" or \"looseness\" of the remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We only consider bins that can fit the item\n    possible_fits = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate the \"fit score\" for bins that can accommodate the item\n    # A smaller remaining capacity after placing the item is generally better (tighter fit).\n    # We can use the inverse of the remaining capacity after placement as a raw score.\n    # To avoid division by zero or very small numbers, we add a small epsilon.\n    # The softmax function will normalize these scores.\n    epsilon = 1e-9\n    remaining_capacities_after_fit = bins_remain_cap[possible_fits] - item\n    \n    # A simple approach: higher priority for bins that leave a smaller remainder\n    # To avoid issues with zero remainder, we can transform it. \n    # A simple transformation that favors smaller remainders could be 1 / (remainder + epsilon)\n    # Or more robustly, we can think of it as \"how much capacity is left unused\".\n    # A common heuristic for \"good fit\" in bin packing is \"best fit\", which tries to minimize\n    # the leftover space. So, we want to assign higher priority to bins with smaller `remaining_capacities_after_fit`.\n    \n    # Let's consider the remaining capacity *after* placing the item.\n    # A smaller remaining capacity after placing the item implies a tighter fit.\n    # We can map smaller remaining capacities to higher values for prioritization.\n    # One way is to use a negative exponential of the remaining capacity.\n    # Another simpler way is to use the negative of the remaining capacity.\n    \n    # Option 1: Simple negative remaining capacity. Larger negative means tighter fit.\n    # However, softmax requires non-negative values typically.\n    \n    # Option 2: Map smaller remaining capacities to larger positive scores.\n    # Let's use something like: max_remainder - current_remainder + epsilon\n    # This ensures all values are positive and the tightest fit gets the highest score.\n    \n    if np.any(possible_fits):\n        # Get the remaining capacities for bins that can fit the item\n        current_remaining_after_fit = bins_remain_cap[possible_fits] - item\n        \n        # To get higher priority for smaller remainders, we can use a transformation\n        # like: 1 / (remainder + epsilon) or by inverting it with a shift.\n        # A simple score that increases as `current_remaining_after_fit` decreases:\n        # Let's use a score proportional to the negative of the remaining capacity\n        # after fitting, but offset to be positive.\n        # A common practice is to use the log of the inverse of the capacity,\n        # or simply the negative of the remaining capacity if we want a direct relationship.\n        \n        # Let's try a score that emphasizes a tighter fit.\n        # If the remaining capacity after fitting is R, a smaller R is better.\n        # We can assign a score proportional to some function of 1/R or -R.\n        # Let's use `1.0 / (current_remaining_after_fit + epsilon)` as a raw score.\n        # This favors bins with smaller leftovers.\n        raw_scores = 1.0 / (current_remaining_after_fit + epsilon)\n\n        # Softmax requires values that can be exponentiated.\n        # Softmax ensures that the priorities sum to 1 (or are normalized such that their\n        # exponential sum is used for probabilistic choice).\n        # For a direct priority score (higher is better, not necessarily probabilistic),\n        # we can still use the softmax function to \"soften\" the preference.\n\n        # Let's make the scores directly proportional to how \"good\" the fit is.\n        # A perfect fit (remainder 0) should be highest.\n        # A very loose fit should be lowest.\n        # The value `current_remaining_after_fit` is the space left.\n        # Smaller means better fit.\n        # Let's use a score that is high for small `current_remaining_after_fit`.\n        # For example, `max_capacity_of_a_bin - current_remaining_after_fit` (if `max_capacity_of_a_bin` is known and constant).\n        # Or, simply transform `current_remaining_after_fit` so smaller values yield larger scores.\n        # Let's use `np.exp(-current_remaining_after_fit)` for a soft preference.\n        # This assigns higher values to smaller remaining capacities.\n        \n        # Alternative: Using the difference between remaining capacity and item size.\n        # A smaller difference means a better fit.\n        # Let's assign priority based on the *inverse* of the remaining capacity after packing.\n        # This means smaller remaining capacity gets a higher score.\n        # score = 1 / (bins_remain_cap - item + epsilon) for fitting bins.\n\n        # Let's use a score based on how close the remaining capacity *after* placing the item is to zero.\n        # Specifically, we want to maximize the likelihood of choosing a bin that leaves\n        # minimal remaining space.\n        # A simple approach: `- (bins_remain_cap[possible_fits] - item)`. This will be negative.\n        # We need positive values for softmax.\n        # Let's use `bins_remain_cap[possible_fits] - item` as a measure of \"waste\". We want to minimize waste.\n        # So, higher priority should go to smaller waste.\n        \n        # Let's define a \"desirability\" score. A higher score means more desirable.\n        # Desirability is high when `bins_remain_cap - item` is small.\n        # So, let's try a linear transformation: `max_possible_remainder - (bins_remain_cap[possible_fits] - item)`.\n        # However, `max_possible_remainder` isn't readily available or fixed.\n        # A simpler approach that works with softmax is to use values that have a clear ordering.\n        \n        # Let's consider the 'gap' after fitting the item.\n        # `gap = bins_remain_cap[possible_fits] - item`\n        # We want to prioritize bins with small `gap`.\n        # Using `np.exp(-gap)` makes smaller gaps yield larger exponential values.\n        \n        # A more direct \"priority\" might be related to how much space is left *relative* to the bin's capacity,\n        # but here we focus on fitting the *item* into the *remaining capacity*.\n        \n        # Let's use the raw remaining capacity *after* placing the item.\n        # We want to select bins where this value is minimal.\n        # We can transform `bins_remain_cap[possible_fits] - item` into a priority.\n        # A common approach is `1 / (x + epsilon)` where `x` is the value to minimize.\n        # So, `1.0 / (bins_remain_cap[possible_fits] - item + epsilon)`.\n        \n        # To implement \"Softmax-Based Fit\", we can consider how \"full\" the bin becomes.\n        # The remaining capacity after fitting is `r_new = bins_remain_cap[possible_fits] - item`.\n        # We want to maximize the \"fit\", which means minimizing `r_new`.\n        # A good heuristic for Softmax-Based Fit would be to assign higher probabilities\n        # to bins with smaller `r_new`.\n        # So, we can use `exp(-r_new)` or `exp(max_possible_r_new - r_new)`.\n        \n        # Let's try a score that is inversely proportional to the remaining capacity.\n        # `score = 1.0 / (bins_remain_cap[possible_fits] - item + epsilon)`\n        # This gives higher scores to tighter fits (smaller remaining capacity).\n\n        # Let's use a function that maps smaller remaining capacities to larger values.\n        # Consider `score = exp(- (bins_remain_cap[possible_fits] - item))`.\n        # This maps 0 remaining capacity to exp(0)=1, and larger remaining capacities to smaller values.\n        # This is a reasonable preference for 'best fit'.\n        \n        fit_values = bins_remain_cap[possible_fits] - item\n        \n        # To ensure all values for softmax are positive and generally represent preference:\n        # Let's map smaller `fit_values` to larger raw scores.\n        # For example, we can use a \"utility\" function.\n        # `utility = C - fit_value` where C is a large constant, or `1 / (fit_value + epsilon)`.\n        \n        # Let's use the reciprocal of the remaining capacity.\n        # A smaller remaining capacity (tighter fit) will yield a larger reciprocal.\n        raw_scores = 1.0 / (fit_values + epsilon)\n        \n        # Apply softmax to get the probabilities/priorities.\n        # Softmax function: exp(x_i) / sum(exp(x_j))\n        # For priority scores that we want to be directly comparable and higher means better,\n        # we can use the exponential of the scores. Softmax is often used for\n        # probabilistic selection, but here we can interpret the output of exp(score)\n        # as the raw priority before normalization.\n        \n        # For this problem, the request is for \"priority score\", not necessarily probabilities.\n        # So, we can return the values that would go into the exponent of softmax.\n        # These are the scores where higher values mean higher priority.\n        # We already computed `raw_scores` where higher is better.\n        \n        # Let's simply use the reciprocal of the remaining capacity as a direct priority score.\n        # Larger score means better bin.\n        priorities[possible_fits] = raw_scores\n        \n    return priorities",
    "response_id": 19,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response20.txt_stdout.txt",
    "code_path": "problem_iter1_code20.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This heuristic prioritizes bins that can fit the item, and among those,\n    it prefers bins that will have the least remaining capacity after packing\n    the item (i.e., aiming for a tighter fit). If an item doesn't fit into any\n    bin, all priorities are 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # For bins that can fit, calculate a \"tightness\" score.\n    # A higher score means a tighter fit (less remaining capacity).\n    # We use the negative of remaining capacity after packing, so larger negative numbers (tighter fit) become higher priority.\n    # To ensure positive priorities for max-heap selection, we can invert the measure.\n    # A good approach is to assign a large positive value to bins that can fit and then penalize them based on remaining capacity.\n    # Let's assign a base priority if it fits, and then a bonus for a tighter fit.\n    # Option 1: Inverse remaining capacity (higher means better fit)\n    # To avoid division by zero, add a small epsilon or handle 0 remaining capacity specifically.\n    # If remaining capacity is 0, this is the best fit.\n\n    # Calculate remaining capacity after packing the item\n    remaining_after_packing = bins_remain_cap[can_fit_mask] - item\n\n    # The priority should be higher for bins that result in less remaining capacity.\n    # A simple inverse relationship could work: higher remaining capacity means lower priority.\n    # To make it work with typical maximization heuristics (higher score = better),\n    # we can use something like 1 / (1 + remaining_capacity).\n    # Or, a large constant minus remaining capacity. Let's use the latter for simplicity and to avoid division by small numbers.\n    # We want the smallest remaining capacity to have the highest priority.\n    # Let's assign a priority inversely proportional to the remaining capacity + a small epsilon to avoid division by zero.\n    # A simpler approach that favors tighter fits is to assign a high score to bins that leave less space.\n\n    # Assign a base priority if it fits, then subtract the remaining space to favor tighter fits.\n    # The larger the value, the higher the priority.\n    # Let's use a strategy where a tighter fit gets a higher priority.\n    # We can assign a value related to the 'goodness' of the fit.\n    # For example, the inverse of the remaining capacity, but that can lead to huge values.\n    # A common approach is to simply prioritize bins that result in the smallest leftover space.\n    # So, for bins that can fit: priority = - (remaining_capacity_after_packing)\n    # To ensure positive scores for maximization, we can add a large constant.\n    # Let's use the reciprocal of the remaining capacity after packing, plus a base value.\n\n    # Calculate remaining capacities *after* placing the item\n    remaining_after_packing = bins_remain_cap[can_fit_mask] - item\n\n    # Higher priority for smaller remaining capacity.\n    # Use the negative of remaining capacity, and add a constant to make it positive.\n    # The value `bins_remain_cap[can_fit_mask].max()` serves as a large constant.\n    # This means bins that become nearly full (small `remaining_after_packing`) get higher priority.\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask].max() - remaining_after_packing\n\n    # If there are bins with exactly 0 remaining capacity after packing, they should have the absolute highest priority.\n    # Let's boost their priority.\n    exact_fit_mask = (remaining_after_packing == 0)\n    if np.any(exact_fit_mask):\n        # Add an extra large value to exact fits\n        priorities[can_fit_mask][exact_fit_mask] += 1e9\n\n    return priorities",
    "response_id": 20,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response21.txt_stdout.txt",
    "code_path": "problem_iter1_code21.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit.\n\n    In Best Fit, we aim to place the item in the bin that leaves the least\n    remaining capacity after the item is placed. This minimizes wasted space\n    in that particular bin, ideally leading to fewer bins being used overall.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a more preferred bin. Bins that cannot fit the\n        item are given a very low priority (negative infinity).\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)  # Initialize with very low priority\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity after placing the item in eligible bins\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # For Best Fit, we want the bin that results in the *smallest* positive remaining capacity.\n    # This means the negative of the remaining capacity will give us the highest score\n    # for the tightest fits. Larger negative values mean a tighter fit.\n    # We can directly assign these values as priorities.\n    priorities[can_fit_mask] = -remaining_after_fit\n\n    return priorities",
    "response_id": 21,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response22.txt_stdout.txt",
    "code_path": "problem_iter1_code22.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Worst Fit: prioritize bins with the largest remaining capacity that can still fit the item.\n    # The intuition is to leave smaller bins for smaller items later.\n\n    # Create a mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities to a very low value (or negative infinity) to ensure that bins\n    # that cannot fit the item have the lowest priority.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # For bins that can fit the item, assign their remaining capacity as the priority.\n    # The higher the remaining capacity, the higher the priority for Worst Fit.\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask]\n\n    return priorities",
    "response_id": 22,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response23.txt_stdout.txt",
    "code_path": "problem_iter1_code23.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit.\n\n    Almost Full Fit prioritizes bins that will be \"almost full\" after placing the item.\n    This is achieved by favoring bins where the remaining capacity after placing the item\n    is minimized, but still non-negative (meaning the item fits).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher score indicates higher priority.\n    \"\"\"\n    # Initialize priorities to a very low value (or 0) for bins where the item doesn't fit.\n    # This ensures they won't be chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins where the item can fit\n    fits_mask = bins_remain_cap >= item\n\n    # For bins where the item fits, calculate the \"almost full\" score.\n    # A common way to define \"almost full\" is to minimize the remaining capacity\n    # after placing the item. We can achieve this by taking the negative of the\n    # remaining capacity after placing the item, or by minimizing the difference\n    # between the bin's capacity and the item's size.\n    # Here, we prioritize bins where bins_remain_cap - item is the smallest\n    # positive value. This is equivalent to maximizing - (bins_remain_cap - item).\n    if np.any(fits_mask):\n        # Calculate the remaining capacity if the item is placed in eligible bins\n        remaining_capacities_after_fit = bins_remain_cap[fits_mask] - item\n\n        # The priority is the inverse of the remaining capacity (or negative remaining capacity).\n        # Smaller remaining capacity = higher priority.\n        # We add a small epsilon to avoid division by zero if remaining capacity is 0.\n        epsilon = 1e-9\n        priorities[fits_mask] = 1.0 / (remaining_capacities_after_fit + epsilon)\n\n    return priorities",
    "response_id": 23,
    "obj": 4.198244914240141,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response24.txt_stdout.txt",
    "code_path": "problem_iter1_code24.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    In Exact Fit First, we prioritize bins that can exactly fit the item.\n    Among bins that can exactly fit, we further prioritize those with less remaining\n    capacity to leave larger gaps in other bins for potentially larger future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can exactly fit the item\n    exact_fit_mask = bins_remain_cap == item\n\n    # For bins that can exactly fit, assign a high priority based on *negative* remaining capacity\n    # This is because we want the bin with the *smallest* remaining capacity (closest to 0\n    # if it's an exact fit) to be prioritized among exact fits.\n    # We use negative to ensure that the largest remaining capacity (closest to 0) gets the highest score.\n    priorities[exact_fit_mask] = -bins_remain_cap[exact_fit_mask]\n\n    # For bins that cannot exactly fit, assign a lower priority.\n    # To further differentiate, we can assign a very small negative number if the item\n    # can fit, and an even smaller negative number if it cannot fit at all.\n    # This ensures that exact fits are always preferred.\n    # If the item *can* fit but not exactly, we assign a score that is lower than\n    # the exact fits but potentially higher than bins where it doesn't fit.\n    # Here, we'll assign a score that is guaranteed to be less than any exact fit priority.\n    can_fit_mask = bins_remain_cap > item\n    priorities[can_fit_mask] = -1e9 - bins_remain_cap[can_fit_mask] # Lower priority than exact fits\n    priorities[~can_fit_mask] = -1e12 # Very low priority for bins where item doesn't fit\n\n    # The highest priority score will be for an exact fit with the smallest capacity.\n    # If there are no exact fits, it will be for a bin that can fit with the smallest\n    # capacity among those. If no bin can fit, it will be the least negative score.\n\n    return priorities",
    "response_id": 24,
    "obj": 5.195452732349436,
    "SLOC": 8.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response25.txt_stdout.txt",
    "code_path": "problem_iter1_code25.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit) strategy.\n\n    The Inverse Distance (Proximity Fit) strategy prioritizes bins that are\n    \"close\" to fitting the current item. A bin is considered a good fit if its\n    remaining capacity is just slightly larger than the item's size. This\n    minimizes wasted space in the bin.\n\n    The priority is calculated as: 1 / (1 + |remaining_capacity - item_size|).\n    This formula gives higher priority to bins where the difference between\n    remaining capacity and item size is smaller (closer to zero).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the absolute difference between the item size and each bin's remaining capacity.\n    # We only consider bins where the item can actually fit.\n    fit_mask = bins_remain_cap >= item\n    diff = np.abs(bins_remain_cap - item)\n\n    # Calculate priorities. Add a small epsilon to the denominator to avoid division by zero\n    # if a bin has exactly the same remaining capacity as the item.\n    # The \"+ 1\" in the denominator ensures that the priority is always less than or equal to 1.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[fit_mask] = 1.0 / (1.0 + diff[fit_mask])\n\n    return priorities",
    "response_id": 25,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response26.txt_stdout.txt",
    "code_path": "problem_iter1_code26.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score prioritizes bins that are a \"good fit\" for the item.\n    A good fit is defined as a bin where the remaining capacity is slightly larger\n    than the item's size. This strategy aims to minimize wasted space in bins.\n\n    The sigmoid function is used to model this \"good fit\" concept. The function\n    will have a higher output when (bins_remain_cap - item) is close to zero,\n    and lower outputs as the difference increases (both positive and negative).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We want to maximize the score for bins where bins_remain_cap is just slightly\n    # larger than item. This means (bins_remain_cap - item) should be close to 0.\n    # The sigmoid function `1 / (1 + exp(-x))` has its steepest slope around x=0.\n    # To make the \"peak\" of our priority function align with the best fit,\n    # we can use `bins_remain_cap - item` as the input to the sigmoid.\n    # However, we need to ensure that we only consider bins where the item can actually fit.\n\n    # Initialize priorities to a very low value (negative infinity conceptually)\n    # for bins that cannot accommodate the item.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the difference between remaining capacity and item size for feasible bins\n    fit_diff = bins_remain_cap[can_fit_mask] - item\n\n    # Apply the sigmoid function. We want the peak at fit_diff = 0.\n    # A common choice for scaling and shifting the sigmoid is `a * x`.\n    # Let's use a scaling factor `k` to control the steepness of the sigmoid.\n    # A larger `k` means a sharper peak.\n    k = 2.0  # Tuning parameter for sigmoid steepness\n\n    # The sigmoid function: 1 / (1 + exp(-k * x))\n    # Here, `x` is our `fit_diff`.\n    # A positive difference (bin has more capacity than needed) is okay,\n    # but a slightly larger difference is less preferred than a perfect fit.\n    # A negative difference (item doesn't fit) is handled by the -inf initialization.\n\n    # We can use the `sigmoid` function directly.\n    # An alternative way to think about it is to map `fit_diff` to values\n    # where the sigmoid is \"interesting\".\n    # The sigmoid function itself is `1 / (1 + exp(-x))`.\n    # We want high scores when `fit_diff` is small.\n    # So, we can use `sigmoid(k * (-fit_diff))`. This makes the peak\n    # when `fit_diff` is 0.\n    # `sigmoid(-k * fit_diff)` will be high for small `fit_diff` (close to 0)\n    # and lower for larger `fit_diff`.\n\n    # Let's try to directly map `fit_diff` to a score.\n    # A simple sigmoid `1 / (1 + exp(-x))` ranges from 0 to 1.\n    # We can also consider `2 * sigmoid(x) - 1` to range from -1 to 1, or other transformations.\n    # For \"fit score\", we probably want higher values for better fits.\n    # Let's stick to the standard sigmoid and interpret higher values as better.\n    # We want to maximize `priority`.\n    # `fit_diff` represents the \"slack\" or wasted space. We want this slack to be minimal.\n    # So, for `fit_diff` close to 0, the priority should be high.\n    # For `fit_diff` much larger than 0, the priority should be lower.\n\n    # Using `np.exp(-k * fit_diff)` will be close to 1 when `fit_diff` is large,\n    # leading to a small sigmoid output. It will be close to 0 when `fit_diff` is small,\n    # leading to a sigmoid output close to 1. This is exactly what we want.\n    # So the sigmoid score will be `1 / (1 + np.exp(-k * fit_diff))`.\n\n    # Ensure we don't encounter overflow with `np.exp` for very large negative `fit_diff`\n    # (though this case is already handled by the -inf initialization).\n    # Also, for very large positive `fit_diff`, `exp(-k*fit_diff)` becomes very small,\n    # close to 0, making sigmoid close to 1. This might be counter-intuitive if we want\n    # \"best fit\" to mean least waste. A perfect fit (diff=0) should have highest score.\n    # A bin that is much larger might be less desirable than a bin that is just right.\n\n    # Let's consider a different sigmoid application:\n    # We want a peak when `fit_diff` is minimal (i.e., close to 0).\n    # The function `exp(-(fit_diff)^2)` has a peak at 0. We can normalize this.\n    # Or, we can use sigmoid applied to `-abs(fit_diff)`.\n\n    # Let's refine the sigmoid idea for \"good fit\".\n    # A \"good fit\" means `bins_remain_cap` is slightly larger than `item`.\n    # So, `bins_remain_cap - item` should be small and positive.\n    # If `bins_remain_cap - item` is negative, the item doesn't fit.\n    # If `bins_remain_cap - item` is very large, it's a bad fit (lots of waste).\n    # If `bins_remain_cap - item` is close to zero, it's a good fit.\n\n    # Strategy:\n    # 1. Bins where item doesn't fit get a very low score (-inf).\n    # 2. Bins where item fits:\n    #    - Score is high if `bins_remain_cap - item` is small (close to 0).\n    #    - Score decreases as `bins_remain_cap - item` increases.\n\n    # This suggests a sigmoid centered around 0, but we only care about positive differences.\n    # Let's consider `score = sigmoid(k * (item - bins_remain_cap[can_fit_mask]))`.\n    # If `item - bins_remain_cap` is close to 0 (meaning `bins_remain_cap` is close to `item`),\n    # the argument to sigmoid is close to 0, giving a high sigmoid value (around 0.5 or higher).\n    # If `item - bins_remain_cap` is large negative (meaning `bins_remain_cap` is much larger than `item`),\n    # the argument is large positive, sigmoid is close to 1. This is NOT what we want.\n\n    # Let's reconsider the `1 / (1 + exp(-x))` structure.\n    # If we use `bins_remain_cap - item` as `x`, then:\n    #   - `fit_diff` close to 0 -> `exp(0) = 1` -> `1 / (1 + 1) = 0.5`\n    #   - `fit_diff` large positive -> `exp(large_pos)` -> `1 / (1 + very_large)` -> close to 0\n    #   - `fit_diff` large negative -> `exp(large_neg)` -> `1 / (1 + very_small)` -> close to 1\n    # This means the highest score is for bins that are WAY too big. We need to invert this logic.\n\n    # How about `score = 1 / (1 + exp(k * fit_diff))`?\n    #   - `fit_diff` close to 0 -> `exp(0) = 1` -> `1 / (1 + 1) = 0.5` (Good)\n    #   - `fit_diff` large positive -> `exp(large_pos)` -> `1 / (1 + very_large)` -> close to 0 (Bad, too much waste)\n    #   - `fit_diff` large negative -> `exp(large_neg)` -> `1 / (1 + very_small)` -> close to 1 (Bad, doesn't fit, already handled by -inf)\n\n    # This looks like a reasonable approach for \"Sigmoid Fit Score\".\n    # The highest score is achieved when `fit_diff` is zero.\n    # As `fit_diff` increases (more waste), the score decreases.\n\n    # Calculate priorities for bins that can fit the item\n    scores_for_fitting_bins = 1 / (1 + np.exp(k * fit_diff))\n\n    # Assign these calculated scores back to the appropriate positions in the priorities array\n    priorities[can_fit_mask] = scores_for_fitting_bins\n\n    return priorities",
    "response_id": 26,
    "obj": 4.487435181491823,
    "SLOC": 8.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response27.txt_stdout.txt",
    "code_path": "problem_iter1_code27.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Random Fit.\n\n    In Random Fit, we consider all bins that can accommodate the item.\n    We then randomly select one of these bins. This heuristic prioritizes\n    bins that can fit the item and gives them a positive priority.\n    Bins that cannot fit the item receive a zero priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Assign a positive priority to bins that can fit the item.\n    # Since it's Random Fit, any bin that can fit gets an equal chance.\n    # We can represent this by giving them a uniform positive value,\n    # or in this case, simply a value of 1.0. The random selection\n    # logic will then pick among these positive-priority bins.\n    priorities[can_fit_mask] = 1.0\n\n    # For a true random fit, the selection mechanism (outside this function)\n    # would sample from the indices where priority > 0.\n    # This function's role is to identify potential candidates.\n    return priorities",
    "response_id": 27,
    "obj": 4.487435181491823,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response28.txt_stdout.txt",
    "code_path": "problem_iter1_code28.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.2  # Exploration rate\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Determine which bins can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit, return all zeros (though in a real scenario, a new bin would be opened)\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Greedy choice: favor bins with minimal remaining capacity after fitting the item\n    # This is the \"exploitation\" part\n    remaining_caps_after_fit = bins_remain_cap[can_fit_mask] - item\n    # Assign a high priority to bins that leave less remaining capacity\n    greedy_priorities = 1.0 / (1.0 + remaining_caps_after_fit)\n\n    # Epsilon-Greedy: explore by assigning random priorities to some bins\n    # The \"exploration\" part\n    num_to_explore = max(1, int(epsilon * np.sum(can_fit_mask))) # Ensure at least one is explored if possible\n\n    # Randomly select bins to explore\n    explore_indices_in_subset = np.random.choice(\n        np.arange(np.sum(can_fit_mask)),\n        size=num_to_explore,\n        replace=False\n    )\n    # Create random priorities for the explored bins\n    random_priorities = np.random.rand(num_to_explore)\n\n    # Combine greedy and random choices\n    # Initially, all priorities for fitting bins are zero.\n    # We'll assign greedy priorities first, then overwrite with random ones for exploration.\n    priorities[can_fit_mask] = greedy_priorities\n\n    # Replace greedy priorities with random priorities for the selected exploration bins\n    original_indices_to_explore = np.where(can_fit_mask)[0][explore_indices_in_subset]\n    priorities[original_indices_to_explore] = random_priorities\n\n    # Ensure that bins that cannot fit the item have a priority of 0\n    priorities[~can_fit_mask] = 0\n\n    return priorities",
    "response_id": 28,
    "obj": 36.408057439170335,
    "SLOC": 21.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response29.txt_stdout.txt",
    "code_path": "problem_iter1_code29.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit strategy.\n\n    The Softmax-Based Fit strategy assigns a higher priority to bins that have a\n    remaining capacity that, after placing the item, is closer to zero, effectively\n    trying to \"fill\" bins more effectively. The priority is calculated using the\n    softmax function, where a larger negative value of (remaining_capacity - item_size)\n    results in a higher probability of being chosen.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity if the item is placed in each bin\n    potential_remain_cap = bins_remain_cap - item\n\n    # We want to prioritize bins where the remaining capacity is small (close to zero)\n    # after placing the item. So, we use the negative of the potential remaining capacity.\n    # A smaller (less negative) value in potential_remain_cap means it's a better fit.\n    # To map this to a probability-like score where higher is better, we can use\n    # -potential_remain_cap, or more directly, the negative of the \"waste\".\n    # However, a simpler and effective approach for softmax is to consider the \"fit quality\" directly.\n    # We want to maximize the chance of placing the item without exceeding capacity,\n    # and among those that fit, we prefer those that leave minimal remaining space.\n    # Let's define a score that is higher when potential_remain_cap is small and non-negative.\n    # A common approach for \"fit\" is to maximize the remaining capacity up to the item size.\n    # Or, minimize the 'gap' after placing the item.\n\n    # Consider the 'fitness' as how well the item fits.\n    # A good fit is when `bins_remain_cap - item` is close to 0.\n    # We can create a value that is higher when `bins_remain_cap >= item` and\n    # `bins_remain_cap - item` is minimized.\n\n    # Let's transform the potential remaining capacities into scores.\n    # For bins where the item does not fit (potential_remain_cap < 0), we want to assign a very low score.\n    # For bins where the item fits, we want to assign a score based on how tightly it fits.\n    # A tighter fit means `potential_remain_cap` is smaller (but >= 0).\n\n    # We can use `np.exp` for softmax. The input to `exp` should be values\n    # where larger values correspond to higher probabilities.\n    # Let's create a score `s` for each bin such that:\n    # 1. If `bins_remain_cap[i] < item`, `s[i]` is very low.\n    # 2. If `bins_remain_cap[i] >= item`, `s[i]` is higher when `bins_remain_cap[i] - item` is smaller.\n\n    # We can model this by considering `-abs(potential_remain_cap)` for invalid fits\n    # and `-(potential_remain_cap)` for valid fits. Or, simpler, a large negative\n    # constant for invalid fits, and the negative of the remaining capacity for valid fits.\n\n    # Let's define a 'desirability' score:\n    # For bins where item fits: desirability = - (bins_remain_cap - item)\n    # For bins where item does not fit: desirability = -infinity (or a very small number)\n\n    # Create an array to store desirability scores\n    desirability_scores = np.full_like(bins_remain_cap, -np.inf)\n\n    # Find indices where the item can fit\n    fit_indices = bins_remain_cap >= item\n\n    # Calculate desirability for bins where the item fits\n    if np.any(fit_indices):\n        desirability_scores[fit_indices] = -(bins_remain_cap[fit_indices] - item)\n\n    # Apply softmax to convert desirability scores into probabilities (priorities)\n    # Ensure we don't have all -inf, which would cause issues with exp\n    if np.all(desirability_scores == -np.inf):\n        # If no bin can fit the item, return uniform zero priorities (or handle as error)\n        # In an online setting, this implies no solution is possible with current bins.\n        # For priority assignment, returning zeros is a reasonable fallback if no fit.\n        return np.zeros_like(bins_remain_cap)\n    else:\n        # The softmax function: exp(x) / sum(exp(x))\n        # The denominator ensures the priorities sum to 1.\n        # A small positive `temperature` parameter can be used to control the \"sharpness\"\n        # of the probability distribution. A temperature of 1 means standard softmax.\n        # Lower temperature means sharper distribution (more focus on best options).\n        # Higher temperature means flatter distribution (more exploration).\n        # For this problem, we aim to fill bins efficiently, so a moderate temperature is good.\n        # Let's use a temperature of 1 for now, which means directly using the exp of scores.\n        # The actual 'priority' is the normalized exponential of the desirability.\n        priorities = np.exp(desirability_scores)\n        sum_priorities = np.sum(priorities)\n\n        # Avoid division by zero if all priorities are effectively zero (e.g., due to -inf)\n        if sum_priorities == 0:\n            return np.zeros_like(bins_remain_cap)\n        else:\n            return priorities / sum_priorities",
    "response_id": 29,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  }
]