{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    The Worst Fit strategy aims to place the item in the bin with the largest remaining capacity.\n    This heuristic is generally not good for minimizing the number of bins, but it's a valid strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to zero\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # For each bin, if the item can fit, assign a priority equal to the remaining capacity.\n    # Otherwise, assign a priority of 0, meaning it cannot be placed in that bin.\n    # The higher the remaining capacity, the higher the priority for Worst Fit.\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask]\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit strategy.\n\n    This strategy prioritizes bins that can accommodate the item and have a remaining capacity\n    that, after placing the item, leaves a \"good fit\". The \"good fit\" is approximated by\n    considering the relative \"tightness\" or \"looseness\" of the remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We only consider bins that can fit the item\n    possible_fits = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate the \"fit score\" for bins that can accommodate the item\n    # A smaller remaining capacity after placing the item is generally better (tighter fit).\n    # We can use the inverse of the remaining capacity after placement as a raw score.\n    # To avoid division by zero or very small numbers, we add a small epsilon.\n    # The softmax function will normalize these scores.\n    epsilon = 1e-9\n    remaining_capacities_after_fit = bins_remain_cap[possible_fits] - item\n    \n    # A simple approach: higher priority for bins that leave a smaller remainder\n    # To avoid issues with zero remainder, we can transform it. \n    # A simple transformation that favors smaller remainders could be 1 / (remainder + epsilon)\n    # Or more robustly, we can think of it as \"how much capacity is left unused\".\n    # A common heuristic for \"good fit\" in bin packing is \"best fit\", which tries to minimize\n    # the leftover space. So, we want to assign higher priority to bins with smaller `remaining_capacities_after_fit`.\n    \n    # Let's consider the remaining capacity *after* placing the item.\n    # A smaller remaining capacity after placing the item implies a tighter fit.\n    # We can map smaller remaining capacities to higher values for prioritization.\n    # One way is to use a negative exponential of the remaining capacity.\n    # Another simpler way is to use the negative of the remaining capacity.\n    \n    # Option 1: Simple negative remaining capacity. Larger negative means tighter fit.\n    # However, softmax requires non-negative values typically.\n    \n    # Option 2: Map smaller remaining capacities to larger positive scores.\n    # Let's use something like: max_remainder - current_remainder + epsilon\n    # This ensures all values are positive and the tightest fit gets the highest score.\n    \n    if np.any(possible_fits):\n        # Get the remaining capacities for bins that can fit the item\n        current_remaining_after_fit = bins_remain_cap[possible_fits] - item\n        \n        # To get higher priority for smaller remainders, we can use a transformation\n        # like: 1 / (remainder + epsilon) or by inverting it with a shift.\n        # A simple score that increases as `current_remaining_after_fit` decreases:\n        # Let's use a score proportional to the negative of the remaining capacity\n        # after fitting, but offset to be positive.\n        # A common practice is to use the log of the inverse of the capacity,\n        # or simply the negative of the remaining capacity if we want a direct relationship.\n        \n        # Let's try a score that emphasizes a tighter fit.\n        # If the remaining capacity after fitting is R, a smaller R is better.\n        # We can assign a score proportional to some function of 1/R or -R.\n        # Let's use `1.0 / (current_remaining_after_fit + epsilon)` as a raw score.\n        # This favors bins with smaller leftovers.\n        raw_scores = 1.0 / (current_remaining_after_fit + epsilon)\n\n        # Softmax requires values that can be exponentiated.\n        # Softmax ensures that the priorities sum to 1 (or are normalized such that their\n        # exponential sum is used for probabilistic choice).\n        # For a direct priority score (higher is better, not necessarily probabilistic),\n        # we can still use the softmax function to \"soften\" the preference.\n\n        # Let's make the scores directly proportional to how \"good\" the fit is.\n        # A perfect fit (remainder 0) should be highest.\n        # A very loose fit should be lowest.\n        # The value `current_remaining_after_fit` is the space left.\n        # Smaller means better fit.\n        # Let's use a score that is high for small `current_remaining_after_fit`.\n        # For example, `max_capacity_of_a_bin - current_remaining_after_fit` (if `max_capacity_of_a_bin` is known and constant).\n        # Or, simply transform `current_remaining_after_fit` so smaller values yield larger scores.\n        # Let's use `np.exp(-current_remaining_after_fit)` for a soft preference.\n        # This assigns higher values to smaller remaining capacities.\n        \n        # Alternative: Using the difference between remaining capacity and item size.\n        # A smaller difference means a better fit.\n        # Let's assign priority based on the *inverse* of the remaining capacity after packing.\n        # This means smaller remaining capacity gets a higher score.\n        # score = 1 / (bins_remain_cap - item + epsilon) for fitting bins.\n\n        # Let's use a score based on how close the remaining capacity *after* placing the item is to zero.\n        # Specifically, we want to maximize the likelihood of choosing a bin that leaves\n        # minimal remaining space.\n        # A simple approach: `- (bins_remain_cap[possible_fits] - item)`. This will be negative.\n        # We need positive values for softmax.\n        # Let's use `bins_remain_cap[possible_fits] - item` as a measure of \"waste\". We want to minimize waste.\n        # So, higher priority should go to smaller waste.\n        \n        # Let's define a \"desirability\" score. A higher score means more desirable.\n        # Desirability is high when `bins_remain_cap - item` is small.\n        # So, let's try a linear transformation: `max_possible_remainder - (bins_remain_cap[possible_fits] - item)`.\n        # However, `max_possible_remainder` isn't readily available or fixed.\n        # A simpler approach that works with softmax is to use values that have a clear ordering.\n        \n        # Let's consider the 'gap' after fitting the item.\n        # `gap = bins_remain_cap[possible_fits] - item`\n        # We want to prioritize bins with small `gap`.\n        # Using `np.exp(-gap)` makes smaller gaps yield larger exponential values.\n        \n        # A more direct \"priority\" might be related to how much space is left *relative* to the bin's capacity,\n        # but here we focus on fitting the *item* into the *remaining capacity*.\n        \n        # Let's use the raw remaining capacity *after* placing the item.\n        # We want to select bins where this value is minimal.\n        # We can transform `bins_remain_cap[possible_fits] - item` into a priority.\n        # A common approach is `1 / (x + epsilon)` where `x` is the value to minimize.\n        # So, `1.0 / (bins_remain_cap[possible_fits] - item + epsilon)`.\n        \n        # To implement \"Softmax-Based Fit\", we can consider how \"full\" the bin becomes.\n        # The remaining capacity after fitting is `r_new = bins_remain_cap[possible_fits] - item`.\n        # We want to maximize the \"fit\", which means minimizing `r_new`.\n        # A good heuristic for Softmax-Based Fit would be to assign higher probabilities\n        # to bins with smaller `r_new`.\n        # So, we can use `exp(-r_new)` or `exp(max_possible_r_new - r_new)`.\n        \n        # Let's try a score that is inversely proportional to the remaining capacity.\n        # `score = 1.0 / (bins_remain_cap[possible_fits] - item + epsilon)`\n        # This gives higher scores to tighter fits (smaller remaining capacity).\n\n        # Let's use a function that maps smaller remaining capacities to larger values.\n        # Consider `score = exp(- (bins_remain_cap[possible_fits] - item))`.\n        # This maps 0 remaining capacity to exp(0)=1, and larger remaining capacities to smaller values.\n        # This is a reasonable preference for 'best fit'.\n        \n        fit_values = bins_remain_cap[possible_fits] - item\n        \n        # To ensure all values for softmax are positive and generally represent preference:\n        # Let's map smaller `fit_values` to larger raw scores.\n        # For example, we can use a \"utility\" function.\n        # `utility = C - fit_value` where C is a large constant, or `1 / (fit_value + epsilon)`.\n        \n        # Let's use the reciprocal of the remaining capacity.\n        # A smaller remaining capacity (tighter fit) will yield a larger reciprocal.\n        raw_scores = 1.0 / (fit_values + epsilon)\n        \n        # Apply softmax to get the probabilities/priorities.\n        # Softmax function: exp(x_i) / sum(exp(x_j))\n        # For priority scores that we want to be directly comparable and higher means better,\n        # we can use the exponential of the scores. Softmax is often used for\n        # probabilistic selection, but here we can interpret the output of exp(score)\n        # as the raw priority before normalization.\n        \n        # For this problem, the request is for \"priority score\", not necessarily probabilities.\n        # So, we can return the values that would go into the exponent of softmax.\n        # These are the scores where higher values mean higher priority.\n        # We already computed `raw_scores` where higher is better.\n        \n        # Let's simply use the reciprocal of the remaining capacity as a direct priority score.\n        # Larger score means better bin.\n        priorities[possible_fits] = raw_scores\n        \n    return priorities\n\n[Reflection]\nFavor tighter fits: prioritize bins with minimal remaining capacity after packing.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}