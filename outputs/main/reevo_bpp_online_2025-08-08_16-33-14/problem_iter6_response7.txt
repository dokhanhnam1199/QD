```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Implements a refined priority function for online Bin Packing Problem.
    This version focuses on prioritizing bins that offer a tight fit without
    over-packing, while also considering bins that leave a moderate amount of space.
    It introduces a penalty for bins that become excessively full after packing,
    making them prone to waste.

    Scoring logic:
    1. Perfect Fit: Highest priority.
    2. Tight Fit (leaves minimal positive remaining capacity): High priority,
       inversely proportional to remaining capacity.
    3. Good Fit (leaves moderate remaining capacity): Moderate priority,
       prioritizing bins that leave less space within this category.
    4. Loose Fit (leaves large remaining capacity): Lowest priority among fitting bins.

    A penalty is applied to bins where the remaining capacity after packing is
    very small (e.g., less than 10% of the item size or a small absolute value),
    as these are prone to over-packing.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Returns:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    num_bins = len(bins_remain_cap)
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins that can fit the item
    can_fit_mask = bins_remain_cap >= item
    fitting_bins_indices = np.where(can_fit_mask)[0]

    if not np.any(can_fit_mask):
        return priorities  # No bin can fit the item

    fitting_bins_caps = bins_remain_cap[can_fit_mask]
    remaining_after_fit = fitting_bins_caps - item

    # Define constants for scoring and penalty
    epsilon = 1e-9  # For considering near-zero remaining capacity
    
    # Penalty threshold: If remaining capacity is less than this, apply a penalty.
    # This is set to be a fraction of the item size, to avoid over-packing
    # relative to the item being placed. A small absolute fallback is also considered.
    penalty_threshold_relative = 0.1 * item
    penalty_threshold_absolute = 0.05  # Ensure a minimum penalty threshold
    penalty_threshold = max(penalty_threshold_relative, penalty_threshold_absolute)

    # Calculate base scores, favoring smaller remaining capacity.
    # Use a function like 1/(1+x) which gives higher scores for smaller x.
    # Add epsilon to denominator to prevent division by zero.
    base_scores = 1.0 / (1.0 + remaining_after_fit + epsilon)

    # Calculate penalty scores for bins that are too full.
    # The penalty is higher when remaining_after_fit is smaller (closer to zero).
    # Penalize bins where remaining_after_fit < penalty_threshold.
    penalty_scores = np.zeros_like(remaining_after_fit)
    too_full_mask = remaining_after_fit < penalty_threshold
    
    # The penalty magnitude should be inversely proportional to how far above
    # the threshold the remaining capacity is. For values below the threshold,
    # a larger "deficit" leads to a higher penalty.
    # Scale penalty to be significant but not overwhelming.
    penalty_magnitude_scale = 0.7 # Controls how strongly the penalty affects the score.
    if np.any(too_full_mask):
        penalty_scores[too_full_mask] = penalty_magnitude_scale * (
            1.0 - (remaining_after_fit[too_full_mask] / penalty_threshold)
        )

    # Combine base scores with penalties. A higher base score (good fit) is reduced
    # if the bin is deemed "too full".
    combined_scores = base_scores - penalty_scores

    # Ensure scores are non-negative.
    combined_scores = np.maximum(0, combined_scores)

    # Further refinement: Add a slight boost to bins that are "just right" -
    # not perfectly full, but with a small positive remaining capacity.
    # This encourages leaving a small, usable gap.
    # Define a target "ideal" remaining capacity, e.g., a small fraction of the item.
    ideal_remaining_target = 0.2 * item
    ideal_remaining_target = max(ideal_remaining_target, epsilon * 10) # Ensure it's positive and small

    # A Gaussian-like boost centered around the ideal target could work.
    # For simplicity, let's add a small boost if remaining_after_fit is close to ideal.
    boost_factor = 0.1
    # Boost for bins that are close to the ideal remaining capacity.
    # The closer, the higher the boost.
    boost_mask = np.abs(remaining_after_fit - ideal_remaining_target) < (ideal_remaining_target * 0.5)
    
    if np.any(boost_mask):
        boost_scores = boost_factor * (1.0 - np.abs(remaining_after_fit[boost_mask] - ideal_remaining_target) / (ideal_remaining_target * 0.5))
        combined_scores[boost_mask] += boost_scores


    # Add a small random perturbation for exploration and tie-breaking.
    exploration_noise_scale = 0.05
    exploration_noise = np.random.normal(0, exploration_noise_scale, len(fitting_bins_caps))
    combined_scores += exploration_noise
    
    # Ensure final priorities are non-negative after adding noise.
    combined_scores = np.maximum(0, combined_scores)

    # Assign the calculated scores to the corresponding bins in the original priority array
    priorities[can_fit_mask] = combined_scores

    return priorities
```
