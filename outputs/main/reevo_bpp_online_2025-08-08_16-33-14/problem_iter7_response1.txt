[Prior reflection]
The `priority_v1` function attempts to balance several desirable properties for bin packing: perfect fits, tight fits, and penalizing wasted space. It uses a tiered approach based on thresholds related to the item size. However, the thresholds (`tight_fit_threshold`, `wasteful_threshold`) are fixed relative to the item size, which might not adapt well to scenarios with very large or very small bins relative to the item. The scoring functions within each tier are also somewhat arbitrary and could be refined for better differentiation. The random jitter is a good addition for exploration.

The reflection points suggest:
1.  **Prioritize perfect fits**: This is already handled.
2.  **Then tight ones**: This is also handled, but the definition of "tight" could be more adaptive.
3.  **Penalize wasted space**: Handled, but could be more nuanced.
4.  **Tune thresholds, weights**: The current thresholds are fixed multiples of `item`. Making them adaptive to the *bin's* capacity, or the distribution of `bins_remain_cap`, could be beneficial.
5.  **Explore stochastically**: Already implemented with jitter, but could be made more systematic or tied to the quality of the "best" fit.

**Potential improvements for `priority_v2`**:

*   **Adaptive Thresholds**: Instead of `item` and `3*item`, consider thresholds relative to the bin's remaining capacity or the bin's original capacity (if known, which it isn't in the online version). A simpler adaptation could be based on the *distribution* of `bins_remain_cap`. For instance, "tight" could be less than the average remaining capacity, and "wasteful" could be significantly more.
*   **Smoother Scoring Functions**: The piecewise linear/power-law functions could be replaced with smoother, perhaps exponential or sigmoid-like, functions to avoid abrupt changes in priority.
*   **Focus on Space Utilization**: A score that directly reflects the utilization of the bin (item size / (original_capacity - remaining_capacity + item)) might be more robust. However, original capacity isn't available. We can use `bin_remain_cap` to infer how "full" a bin is.
*   **Better Waste Penalty**: The penalty for wasteful fits could be more aggressive or based on a clearer rationale, perhaps related to how many *more* items of a certain size could fit.
*   **Exploration**: The jitter could be proportional to the confidence in the current best fit. If many bins have very similar scores, more exploration is needed.

**Design for `priority_v2`**:

I will focus on making the scoring more continuous and adaptive to the current state of the bins.

1.  **Perfect Fit**: Maintain the highest priority.
2.  **Tight Fit**: Prioritize bins where `remaining_after_fit` is small *relative to the bin's current remaining capacity*. A bin that is almost full and has little space left is better than a large bin that happens to have a small gap after fitting the item. A score like `(bin_capacity - item) / bin_capacity` where `bin_capacity` is the *original* capacity (not available) or `(original_capacity - remaining_after_fit)` if we consider it relative to how "full" it gets. Without original capacity, let's use `1 - remaining_after_fit / bin_initial_available_space` where `bin_initial_available_space` is `bins_remain_cap` *before* adding the item. This is `bins_remain_cap[idx]` for a fitting bin `idx`. So, score is `1 - remaining_after_fit / original_bin_capacity_estimate`. Let's use `bins_remain_cap[idx]` as a proxy for how much capacity *was* available in that bin.
    A better proxy for "tightness" could be `(bins_remain_cap[fitting_bin_index] - item) / bins_remain_cap[fitting_bin_index]` if `bins_remain_cap[fitting_bin_index]` > 0.
3.  **Waste Penalty**: Penalize bins that leave a large *proportion* of their *current* remaining capacity unused. If a bin has 100 units left and we put an item of 10, leaving 90, that's worse than a bin with 50 units left and we put an item of 10, leaving 40. The penalty should be related to `remaining_after_fit / bins_remain_cap[fitting_bin_index]`.

Let's refine the scoring logic:

*   **Base score for fitting bins**: All bins that can fit the item get a baseline score, perhaps related to how much capacity they have left *after* packing. A higher remaining capacity might be good if we expect large future items, or bad if we want to fill bins tightly. For this heuristic, let's assume we want to fill bins tightly, so leaving *less* space is better.
*   **Perfect fit**: Max score.
*   **Tightness score**: `(bins_remain_cap[fitting_bin_index] - item) / bins_remain_cap[fitting_bin_index]` -- this measures the *reduction* in available space for that bin. Higher value means more of the bin's capacity is now utilized.
*   **Waste penalty score**: `remaining_after_fit / bins_remain_cap[fitting_bin_index]` -- this measures the *proportion* of the bin's *original* remaining capacity that is left unused. Lower value is better.

Combining these:
Priority = `w_perfect * is_perfect + w_tight * tightness_score - w_waste * waste_penalty`

Or, more simply:
1.  **Perfect fit**: highest priority.
2.  **Tight fit**: bins with small `remaining_after_fit`.
3.  **Wasteful**: bins with large `remaining_after_fit`.

Let's define "tightness" as minimizing `remaining_after_fit`.
Let's define "waste" as maximizing `remaining_after_fit / bins_remain_cap[fitting_bin_index]`.

A score could be:
`score = C1 * (1 / (remaining_after_fit + epsilon)) + C2 * (bins_remain_cap[fitting_bin_index] / (remaining_after_fit + epsilon))`
This tries to reward small `remaining_after_fit` and also reward bins that were already somewhat full (smaller `bins_remain_cap`).

Let's try a simpler, more direct approach for `v2`:

1.  **Perfect fit**: Assign a very high score.
2.  **Non-perfect fits**: Calculate a score based on the *efficiency* of the fit within that specific bin. Efficiency can be thought of as `item_size / (item_size + remaining_after_fit)`. This rewards bins where the item takes up a larger fraction of the space that is now effectively occupied.
3.  **Penalize large remaining space**: Apply a penalty if `remaining_after_fit` is significantly larger than `item` or some other threshold.

**Revised strategy for `priority_v2`**:

*   Use a base score for all fitting bins that is related to minimizing the remaining space after packing.
*   Boost score for perfect fits.
*   Add a penalty for "wasteful" bins, where the remaining space is large relative to the *bin's original remaining capacity*.

Let `R_i` be `bins_remain_cap[i]`.
Let `item` be the item size.
If `R_i < item`, priority is 0.
If `R_i >= item`, `remaining_after = R_i - item`.

Score for fitting bin `i`:
`score_i = f(item, R_i, remaining_after)`

1.  **Perfect Fit**: `remaining_after < epsilon`. High score.
2.  **Tight Fit**: `remaining_after` is small. E.g., `remaining_after < item`.
3.  **Moderate Fit**: `item <= remaining_after < some_threshold`.
4.  **Wasteful Fit**: `remaining_after >= some_threshold`.

Let's use a score that is generally `1 / (remaining_after_fit + epsilon)` and then adjust.

`priority_v2(item, bins_remain_cap)`:
*   Initialize `priorities` to 0.
*   Find `fitting_mask = bins_remain_cap >= item`.
*   Calculate `remaining_after_fit = bins_remain_cap[fitting_mask] - item`.
*   Calculate `original_remaining_cap_for_fitting = bins_remain_cap[fitting_mask]`.

*   **Perfect Fit (PF)**: `remaining_after_fit < epsilon`. High score (e.g., 1000).
*   **Tight Fit (TF)**: `epsilon <= remaining_after_fit <= item`. Score proportional to `1 / (remaining_after_fit + epsilon)`.
*   **Moderate Fit (MF)**: `item < remaining_after_fit <= C * item`. Score proportional to `1 / (remaining_after_fit + epsilon)`.
*   **Wasteful Fit (WF)**: `remaining_after_fit > C * item`. Score is penalized.

Let's try to use the "fill ratio" concept. For a fitting bin, the fill ratio is `item / original_bin_capacity`. We don't have original bin capacity. But we have `original_remaining_cap_for_fitting`.
Let's define a "utility" score for a fitting bin `i` as `item / original_remaining_cap_for_fitting[i]`. This is high if the item is a large fraction of the bin's available space.

Consider `priority = f(remaining_after_fit, original_remaining_cap_for_fitting)`.
A good heuristic is often to minimize remaining space. So `1 / (remaining_after_fit + epsilon)` is a starting point.

Let's combine the "tightness" (small `remaining_after_fit`) with "efficiency" (item is a good portion of the bin's available space).

Score for a fitting bin `i` with remaining capacity `R_i` and `remaining_after = R_i - item`:

1.  **Perfect Fit**: `remaining_after < epsilon` -> Score `1000.0`
2.  **Tightness-based score**: Prioritize smaller `remaining_after`. A score like `1.0 / (remaining_after + epsilon)`.
3.  **Efficiency bonus**: If the `original_remaining_cap_for_fitting[i]` was not too large, give a bonus. For example, if `original_remaining_cap_for_fitting[i] < K * item`, add a bonus. Or, perhaps more robustly, use the ratio `item / original_remaining_cap_for_fitting[i]`.

Let's combine these into a single score:
`score_i = (1.0 / (remaining_after + epsilon)) * (1.0 + item / original_remaining_cap_for_fitting[i])`
This rewards small `remaining_after` and also rewards fits where the item is a significant portion of the bin's initially available space.

We still need to penalize truly "wasteful" bins. A wasteful bin is one where `remaining_after` is large, *especially* if `original_remaining_cap_for_fitting[i]` was also large.

Let's try:
`score_i = (1.0 / (remaining_after + epsilon))`  (This part prioritizes minimal remaining space)
Then, add a "waste penalty":
If `remaining_after > threshold`, subtract a penalty.
Threshold can be `item` or `original_remaining_cap_for_fitting[i] / some_factor`.

Let's refine the idea of "wasteful". A bin is wasteful if `remaining_after` is large compared to the capacity that was *initially* available in that bin.
So, `waste_metric = remaining_after / original_remaining_cap_for_fitting[i]`.
We want to *penalize* high `waste_metric`.

Final proposed approach for `priority_v2`:

*   **Perfect Fit**: Highest priority (e.g., 1000).
*   **Other fits**:
    *   Calculate a base score proportional to `1.0 / (remaining_after_fit + epsilon)`. This favors smaller remaining spaces.
    *   Apply a bonus for "tightness" relative to the bin's original remaining capacity. If `original_remaining_cap_for_fitting[i]` is small, we want to favor it. This can be done by multiplying by `original_remaining_cap_for_fitting[i]`. So, `score_i = (1.0 / (remaining_after_fit + epsilon)) * original_remaining_cap_for_fitting[i]`. This might be too aggressive for large initial capacities.
    *   Let's use a different metric for "goodness of fit" and a separate "waste penalty".
    *   **Goodness of Fit (GOF)**: For fitting bins, `GOF_i = item / original_remaining_cap_for_fitting[i]`. This is high if the item fills a good portion of the bin.
    *   **Remaining Space Penalty (RSP)**: For fitting bins, `RSP_i = remaining_after_fit[i] / original_remaining_cap_for_fitting[i]`. This is the proportion of space left. We want to penalize high `RSP_i`.
    *   **Score Calculation**:
        *   Perfect Fit: `1000.0`
        *   Other Fits: `score = (1.0 / (remaining_after_fit + epsilon)) * (1.0 + GOF_i) - penalty_factor * RSP_i`
        *   The `1.0 / (remaining_after_fit + epsilon)` part favors minimizing final leftover space.
        *   The `(1.0 + GOF_i)` part favors fits where the item uses a good portion of the bin's initial available capacity.
        *   The `- penalty_factor * RSP_i` part penalizes leaving a large *proportion* of the bin's initial capacity unused.

Let's refine the penalty. Instead of subtracting, let's adjust the scoring structure to make it cleaner.

How about:
For fitting bins:
`score_i = (item / original_remaining_cap_for_fitting[i])`  -- represents how well item fills the available space.
`score_i += (1.0 / (remaining_after_fit[i] + epsilon))` -- adds preference for bins that end up with less space.
`score_i *= (1.0 / (remaining_after_fit[i] / original_remaining_cap_for_fitting[i] + epsilon))` -- penalizes high waste proportion.
This is getting complicated.

Let's stick to prioritizing tight fits and penalizing waste.
`priority_v2`:

1.  **Perfect fit**: `remaining_after_fit < epsilon`. Score: `1000.0`.
2.  **Tight fit**: `epsilon <= remaining_after_fit <= item`. Score: `1.0 / (remaining_after_fit + epsilon)`. This rewards minimal remaining space.
3.  **Moderate fit**: `item < remaining_after_fit <= 2 * item`. Score: `0.5 / (remaining_after_fit + epsilon)`. Lower priority than tight.
4.  **Wasteful fit**: `remaining_after_fit > 2 * item`. Score is heavily penalized.
    *   Penalty should increase with `remaining_after_fit`.
    *   Consider `remaining_after_fit / original_remaining_cap_for_fitting[i]` as a waste metric.
    *   Score: `0.1 / (remaining_after_fit + epsilon) - penalty_factor * (remaining_after_fit / original_remaining_cap_for_fitting[i])`.

Let's try to make the thresholds adaptive based on bin capacities.
The "tight" threshold could be a fraction of `original_remaining_cap_for_fitting[i]`.
The "wasteful" threshold could also be a fraction.

Consider `priority_v2`:

*   **Perfect fit**: `remaining_after_fit < epsilon`. Score: `1000.0`.
*   **General score for fitting bins**:
    *   Primary goal: Minimize `remaining_after_fit`. So, `1.0 / (remaining_after_fit + epsilon)` is a good base.
    *   Secondary goal: Consider the "fill ratio" of the bin. If `original_remaining_cap_for_fitting[i]` is small, and `item` is put in it, it's a good use of space. If `original_remaining_cap_for_fitting[i]` is large, putting `item` in it might still leave a lot of space.
    *   Let's try a score that combines the inverse of remaining space with a penalty for relative waste.
    *   `score_i = (1.0 / (remaining_after_fit[i] + epsilon)) * (1.0 - remaining_after_fit[i] / original_remaining_cap_for_fitting[i])`
        *   This is essentially `(item / original_remaining_cap_for_fitting[i])`. This rewards fitting a larger item into a bin's available space.
    *   Let's call this the "efficiency score".
    *   Perfect fits are a special case of high efficiency.

`priority_v2` refined structure:

1.  **Perfect Fit**: `remaining_after_fit < epsilon`. Score: `1000.0`.
2.  **Tight Fits**: `epsilon <= remaining_after_fit <= item`.
    *   Score based on `remaining_after_fit`. High score for small `remaining_after_fit`.
    *   Let's use `1.0 / (remaining_after_fit + epsilon)`.
3.  **Moderate Fits**: `item < remaining_after_fit <= K * item` (where K is like 2 or 3).
    *   Score should be lower than tight fits.
    *   Let's use `0.5 / (remaining_after_fit + epsilon)`.
4.  **Wasteful Fits**: `remaining_after_fit > K * item`.
    *   Heavily penalized. The penalty should grow with `remaining_after_fit` and perhaps `original_remaining_cap_for_fitting[i]`.
    *   Let's use a penalty proportional to `remaining_after_fit / original_remaining_cap_for_fitting[i]`.
    *   Score: `0.1 / (remaining_after_fit + epsilon) - penalty_factor * (remaining_after_fit / original_remaining_cap_for_fitting[i])`.

We need to ensure continuity and relative scaling.
Let's define functions based on normalized remaining space `x = remaining_after_fit`.
And `y = original_remaining_cap_for_fitting[i]`.
We want to maximize `item / y` and minimize `x`.

Let's try a score that directly models "good use of space".
For a fitting bin `i`:
`utilization_score = item / original_remaining_cap_for_fitting[i]`
This is high when the item takes a large chunk of the bin's available space.

Combine this with the preference for leaving less space overall:
`score_i = utilization_score + 1.0 / (remaining_after_fit[i] + epsilon)`

This would mean:
*   Perfect fit: `remaining_after_fit = 0`. Score = `item/original_cap + infinity`. This needs handling.
*   Let's cap the infinity. If `remaining_after_fit < epsilon`, score is a large constant, e.g. `1000`.
*   Otherwise, `score_i = (item / original_remaining_cap_for_fitting[i]) + (1.0 / (remaining_after_fit[i] + epsilon))`.

This still doesn't explicitly penalize "wasteful" bins in a way that differentiates them strongly. A bin with `original_remaining_cap = 100`, `item = 10`, `remaining_after = 90` would have `util = 0.1`, `inv_rem = 0.01`. Total `0.11`.
A bin with `original_remaining_cap = 20`, `item = 10`, `remaining_after = 10` would have `util = 0.5`, `inv_rem = 0.1`. Total `0.6`. This is better, as expected.

What if we have `original_remaining_cap = 100`, `item = 10`, `remaining_after = 10`?
`util = 0.1`, `inv_rem = 0.1`. Total `0.2`.
This approach seems to reward bins that had more initial capacity and now have little left.

Let's go back to explicit tiers but make their scoring more refined.

`priority_v2(item, bins_remain_cap)`:
*   `epsilon = 1e-9`
*   `priorities = np.zeros_like(bins_remain_cap, dtype=float)`
*   `can_fit_mask = bins_remain_cap >= item`
*   `fitting_bins_indices = np.where(can_fit_mask)[0]`

*   If no fitting bins, return `priorities`.

*   `fitting_bins_caps = bins_remain_cap[can_fit_mask]`
*   `remaining_after_fit = fitting_bins_caps - item`

*   **Tier 1: Perfect Fits**
    *   `perfect_mask = remaining_after_fit < epsilon`
    *   `priorities[can_fit_mask][perfect_mask] = 1000.0`

*   **Tier 2: Tight Fits** (remaining space is small, relative to item size and original capacity)
    *   Define "tightness" threshold based on item size and original capacity.
    *   Let `tight_threshold_abs = item`
    *   Let `tight_threshold_rel = 0.25` (meaning remaining space is < 25% of original capacity)
    *   `tight_mask = (remaining_after_fit >= epsilon) & (remaining_after_fit <= tight_threshold_abs)`
    *   For these, score inversely proportional to `remaining_after_fit`.
    *   `scores_tight = 1.0 / (remaining_after_fit[tight_mask] + epsilon)`
    *   Add a bonus if the fit was "relatively tight" too:
        *   `relative_remaining = remaining_after_fit[tight_mask] / fitting_bins_caps[tight_mask]`
        *   `scores_tight += 10.0 * (1.0 - relative_remaining)` (bonus if relative remaining is small)

*   **Tier 3: Moderate Fits** (remaining space is moderate, not perfect, not clearly wasteful)
    *   Let `moderate_threshold = 3 * item`
    *   `moderate_mask = (remaining_after_fit > tight_threshold_abs) & (remaining_after_fit <= moderate_threshold)`
    *   Score inversely proportional to `remaining_after_fit`, but with lower weight.
    *   `scores_moderate = 0.5 / (remaining_after_fit[moderate_mask] + epsilon)`
    *   Add a smaller bonus for good relative fit:
        *   `relative_remaining = remaining_after_fit[moderate_mask] / fitting_bins_caps[moderate_mask]`
        *   `scores_moderate += 5.0 * (1.0 - relative_remaining)`

*   **Tier 4: Wasteful Fits** (remaining space is large)
    *   `wasteful_mask = remaining_after_fit > moderate_threshold`
    *   Heavily penalize. Penalty should increase with `remaining_after_fit` and `relative_remaining`.
    *   Base score: `0.1 / (remaining_after_fit[wasteful_mask] + epsilon)` (give a small base value)
    *   Penalty: `penalty_factor = 10.0`
    *   `relative_remaining = remaining_after_fit[wasteful_mask] / fitting_bins_caps[wasteful_mask]`
    *   `scores_wasteful = (0.1 / (remaining_after_fit[wasteful_mask] + epsilon)) - penalty_factor * relative_remaining`
    *   Ensure scores are non-negative.

*   Combine scores into `priorities` array.

Let's use a more unified scoring function and then apply a penalty.

Score for fitting bin `i`:
Let `rem_i = remaining_after_fit[i]`
Let `orig_rem_i = fitting_bins_caps[i]`

1.  **Perfect Fit**: If `rem_i < epsilon`, score = `1000.0`.
2.  **General Score**: `score_i = (1.0 / (rem_i + epsilon)) * (item / orig_rem_i)`
    *   This rewards small `rem_i` and high `item / orig_rem_i`.
    *   The `item / orig_rem_i` term is high when the item is a large portion of the bin's original capacity.
3.  **Waste Penalty**: If `rem_i / orig_rem_i > 0.5` (i.e., more than half the bin's original capacity is left), apply a penalty.
    *   Penalty = `P * (rem_i / orig_rem_i - 0.5)`.
    *   The penalty should be significant.

Let's try a simpler function that captures the reflection: "Prioritize perfect fits, then tight ones. Penalize wasted space."

`priority_v2`:
*   For fitting bins:
    *   **Perfect Fit**: `remaining_after < epsilon`. Score `1000`.
    *   **Tightness**: Prefer smaller `remaining_after`. Use `1.0 / (remaining_after + epsilon)`.
    *   **Waste Penalty**: Penalize if `remaining_after` is large relative to `original_remaining_capacity`.
        *   Consider `waste_ratio = remaining_after / original_remaining_capacity`.
        *   If `waste_ratio > threshold`, apply penalty.

Let's try this:
Score for fitting bin `i` with `rem_i`, `orig_rem_i`:

*   If `rem_i < epsilon`: `score = 1000.0`
*   Else:
    *   `base_score = 1.0 / (rem_i + epsilon)`  (favors small `rem_i`)
    *   `efficiency_factor = item / orig_rem_i` (favors item taking large portion of capacity)
    *   `waste_ratio = rem_i / orig_rem_i` (penalize large leftover proportion)

    *   `score = base_score * (1 + efficiency_factor) - 10.0 * max(0, waste_ratio - 0.3)`
        *   `base_score` prioritizes minimal leftover.
        *   `(1 + efficiency_factor)` boosts score for efficient use.
        *   `- 10.0 * max(0, waste_ratio - 0.3)` penalizes if more than 30% of bin capacity is left unused.

This seems like a reasonable combination. The weights `1.0`, `1.0`, `10.0` and threshold `0.3` are tunable parameters.

```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Implements a refined priority function for the online Bin Packing Problem.
    This version prioritizes perfect fits, then tight fits, and penalizes
    bins that leave a disproportionately large amount of space unused relative
    to their original remaining capacity.

    The priority for a fitting bin is calculated based on:
    1. Minimizing the absolute remaining space.
    2. Maximizing the efficiency of the fit (item size relative to bin's initial remaining capacity).
    3. Penalizing bins where the final remaining space is a large proportion of the initial remaining capacity.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Returns:
        Array of same size as bins_remain_cap with priority score of each bin.
        Higher score indicates a higher priority.
    """
    num_bins = len(bins_remain_cap)
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    epsilon = 1e-9  # For numerical stability

    # Identify bins that can fit the item
    can_fit_mask = bins_remain_cap >= item
    
    if not np.any(can_fit_mask):
        return priorities  # No bin can fit the item

    # Extract capacities and calculate remaining space for fitting bins
    fitting_bins_caps = bins_remain_cap[can_fit_mask]
    remaining_after_fit = fitting_bins_caps - item

    # --- Scoring Calculation ---
    scores_for_fitting_bins = np.zeros_like(fitting_bins_caps, dtype=float)

    # 1. Perfect Fits: Highest priority
    perfect_mask = remaining_after_fit < epsilon
    scores_for_fitting_bins[perfect_mask] = 1000.0

    # 2. Other Fits: Combine minimizing remaining space, efficiency, and waste penalty
    non_perfect_mask = ~perfect_mask
    
    if np.any(non_perfect_mask):
        # Get data for non-perfect fits
        rem_non_perfect = remaining_after_fit[non_perfect_mask]
        orig_rem_non_perfect = fitting_bins_caps[non_perfect_mask]

        # Base score: Inversely proportional to absolute remaining space (favors tighter fits)
        # E.g., 1 / (rem_i + epsilon)
        base_score_tightness = 1.0 / (rem_non_perfect + epsilon)

        # Efficiency factor: Item size relative to bin's initial remaining capacity.
        # High value means item uses a good portion of the available space in that bin.
        # E.g., item / orig_rem_i
        efficiency_factor = item / orig_rem_non_perfect

        # Waste ratio: Proportion of bin's initial remaining capacity left unused.
        # E.g., rem_i / orig_rem_i. We want to penalize high values of this.
        waste_ratio = rem_non_perfect / orig_rem_non_perfect

        # Combine scores:
        # Start with tightness score, boost with efficiency, then penalize waste.
        # Weights and thresholds are tunable parameters.
        
        # Tunable parameters:
        efficiency_weight = 1.0
        waste_penalty_weight = 10.0
        waste_threshold_ratio = 0.3 # Penalize if more than 30% of bin's initial capacity is left.

        # Calculate scores for non-perfect fits
        scores_non_perfect = (base_score_tightness * (1.0 + efficiency_factor * efficiency_weight) - 
                              waste_penalty_weight * np.maximum(0, waste_ratio - waste_threshold_ratio))
        
        # Ensure scores are non-negative
        scores_non_perfect[scores_non_perfect < 0] = 0
        
        # Assign these scores to the correct positions in scores_for_fitting_bins
        scores_for_fitting_bins[non_perfect_mask] = scores_non_perfect

    # Assign calculated scores to the original priorities array
    priorities[can_fit_mask] = scores_for_fitting_bins

    # Add a small random jitter for exploration to break ties and encourage diversity
    # Jitter is proportional to the average score magnitude to be effective across scales.
    # Ensure jitter is not too aggressive.
    average_score = np.mean(priorities[priorities > 0] + epsilon) if np.any(priorities > 0) else 1.0
    exploration_factor = 0.05 # 5% of average score magnitude
    jitter = np.random.rand(num_bins) * exploration_factor * average_score
    
    # Apply jitter only to bins that can fit the item and have a non-zero priority already.
    # This prevents introducing spurious priorities for bins that cannot fit.
    jitter_mask = can_fit_mask # + (priorities > 0) # Consider jittering non-fitting bins too if we want to break ties for them for some reason
    priorities[jitter_mask] += jitter[jitter_mask]
    
    # Ensure no negative priorities after jitter (should not happen with current logic, but good practice)
    priorities[priorities < 0] = 0

    return priorities
```
