```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a hybrid strategy.

    This strategy prioritizes bins that offer a tight fit (minimizing leftover capacity)
    while also considering bins that might accommodate future items better by leaving
    a moderate amount of space. It uses a soft-max like approach to create relative
    priorities and a small penalty for bins that would be *too* full after packing.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    num_bins = len(bins_remain_cap)
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Determine which bins can fit the item
    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        return priorities  # No bin can fit, return all zeros

    fitting_bins_caps = bins_remain_cap[can_fit_mask]

    # Calculate potential remaining capacities after fitting the item
    remaining_after_fit = fitting_bins_caps - item

    # Strategy: Prioritize bins that result in a remaining capacity that is "just right".
    # This means not too much space left (avoiding "First Fit" tendencies for large remaining capacity)
    # and not too little space left (avoiding "Worst Fit" where there's barely any space).
    # We can model this using a Gaussian-like function centered around a "good" remaining capacity.
    # A good remaining capacity could be a small fraction of the bin's original capacity,
    # or a small fixed amount. Let's consider a target remaining capacity.
    # For simplicity, let's aim for remaining capacity that is small but not zero.
    # A simple approach: maximize priority when remaining_after_fit is small, but
    # penalize very small remaining capacities.

    # Option 1: Favor small remaining capacity, but penalize zero or very small remaining.
    # A log-like function can achieve this: log(1 + x) increases with x, but at a decreasing rate.
    # We want to minimize remaining_after_fit, so we take the inverse.
    # To avoid division by zero for remaining_after_fit == 0, we can use 1 + remaining_after_fit.
    # A small positive remaining capacity is ideal (tightest fit without being *too* tight).
    # We can penalize bins where remaining_after_fit is very small to avoid "Worst Fit" issues.
    # Let's define a score that is high for small remaining_after_fit, but drops off as it approaches zero.
    # We can use 1 / (1 + remaining_after_fit) for the "best fit" aspect.
    # To penalize being *too* full (e.g., remaining_after_fit < 1e-6), we can introduce a penalty.

    # Score that favors small remaining capacity. Add a small epsilon to avoid division by zero.
    base_scores = 1.0 / (1.0 + remaining_after_fit + 1e-6)

    # Penalty for being too full (very small remaining capacity)
    # This encourages leaving a small but non-zero gap.
    too_full_penalty = np.zeros_like(fitting_bins_caps)
    too_full_threshold = 0.05 # e.g., 5% of item size or a small absolute value
    too_full_mask = remaining_after_fit < too_full_threshold
    too_full_penalty[too_full_mask] = (too_full_threshold - remaining_after_fit[too_full_mask]) / too_full_threshold

    # Combine scores: high priority for tight fits, with a penalty for being *too* tight.
    # We want to subtract the penalty from the base score.
    combined_scores = base_scores - (0.5 * too_full_penalty)

    # A small diversification factor inspired by softmax to create relative differences
    # Softmax ensures that scores are positive and sum to 1 (or can be scaled to do so).
    # Using it here can help break ties and introduce some probabilistic selection.
    # Adding a small constant before softmax to avoid issues with all scores being equal.
    diversification_scores = np.exp(combined_scores + np.random.normal(0, 0.1, len(combined_scores)))
    # Normalize to create relative probabilities, could be used directly as priorities
    normalized_diversification = diversification_scores / np.sum(diversification_scores)
    # Or, use combined_scores directly and normalize if needed, but raw scores are often fine for priority.

    # Let's assign the combined_scores directly as priorities, possibly scaled.
    # Scale scores to a reasonable range, e.g., 0-1.
    # Ensure no negative priorities if penalty was too strong.
    final_priorities = np.maximum(0, combined_scores)

    # Assign the calculated scores to the corresponding bins
    priorities[can_fit_mask] = final_priorities

    # Ensure bins that cannot fit have a priority of 0
    priorities[~can_fit_mask] = 0

    return priorities
```
