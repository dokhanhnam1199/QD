[
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Inverse Distance strategy.\n\n    This strategy prioritizes bins that are a close fit to the item's size.\n    It calculates a priority score based on the inverse of (1 + difference) for bins\n    where remaining_capacity is greater than or equal to the item size.\n    The score is designed to be less sensitive to extreme differences compared to v0,\n    and avoids potential division by zero issues more robustly than v1 by adding 1 to the denominator\n    and ensuring the difference is non-negative.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Find bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins that can fit, calculate the difference between remaining capacity and item size.\n    # We are interested in how \"tight\" the fit is.\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    differences = fitting_bins_remain_cap - item\n    \n    # Calculate priority using inverse of (1 + difference).\n    # This ensures that bins with smaller differences (better fits) get higher priorities.\n    # Adding 1 to the denominator prevents division by zero when the item perfectly fills a bin.\n    # The addition of 1 also acts as a small baseline priority for all fitting bins.\n    priorities[can_fit_mask] = 1.0 / (1.0 + differences)\n    \n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response1.txt_stdout.txt",
    "code_path": "problem_iter2_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit-like strategy.\n\n    This strategy prioritizes bins that can accommodate the item and, after placing the item,\n    leave the smallest possible remaining capacity. This is a direct implementation of\n    favoring tighter fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher score indicates a higher priority.\n    \"\"\"\n    # Initialize priorities to a very low value (negative infinity) for bins that cannot fit.\n    # This ensures they are never selected if any bin can fit the item.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can fit the item\n    possible_fits_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity *after* placing the item.\n    # We want to prioritize bins where this value is smallest (tightest fit).\n    # A simple way to assign higher priority to smaller values is to use their reciprocal.\n    # To avoid division by zero or extremely large numbers when the remainder is close to zero,\n    # we add a small epsilon.\n    epsilon = 1e-9\n    remaining_capacities_after_fit = bins_remain_cap[possible_fits_mask] - item\n\n    # The priority score is inversely proportional to the remaining capacity.\n    # A smaller remaining capacity results in a larger priority score.\n    priorities[possible_fits_mask] = 1.0 / (remaining_capacities_after_fit + epsilon)\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response2.txt_stdout.txt",
    "code_path": "problem_iter2_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit heuristic approach.\n\n    This strategy prioritizes bins that can accommodate the item and, after placing the item,\n    leave the minimal possible remaining capacity in the bin. This aims to achieve a \"tight fit\".\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate a more preferred bin.\n    \"\"\"\n    # Initialize priorities to zero.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that have enough remaining capacity to fit the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros. The caller will handle creating a new bin.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the remaining capacity *after* placing the item in the bins that can fit it.\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # To prioritize bins with minimal leftover capacity (best fit), we want to assign\n    # higher scores to bins where `remaining_after_placement` is smaller.\n    # A common way to achieve this is to use the inverse of the remaining capacity.\n    # A small epsilon is added to avoid division by zero if an item perfectly fills a bin.\n    epsilon = 1e-9\n    priorities[can_fit_mask] = 1.0 / (remaining_after_placement + epsilon)\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit-like strategy.\n\n    This heuristic prioritizes bins that can fit the item and, among those,\n    prioritizes bins that will have the least remaining capacity *after*\n    placing the item. This aims to pack items tightly, minimizing wasted space.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity after placement.\n    # We want to prioritize bins where placing the item leaves the LEAST remaining capacity.\n    # This means we want to MINIMIZE `bins_remain_cap[i] - item`.\n    # To translate minimization into a maximization of priority, we can use the reciprocal\n    # of the remaining capacity after placement (plus a small epsilon to avoid division by zero).\n    # Smaller remaining capacity after placement -> larger reciprocal -> higher priority.\n    \n    # Calculate remaining capacity for bins that can fit\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # Assign priority: higher priority for smaller remaining capacity after placement.\n    # Add a small epsilon to avoid division by zero and ensure positive values for the reciprocal.\n    priorities[can_fit_mask] = 1.0 / (remaining_after_placement + 1e-9)\n\n    return priorities",
    "response_id": 3,
    "obj": 4.198244914240141,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response4.txt_stdout.txt",
    "code_path": "problem_iter2_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a hybrid strategy.\n\n    This strategy combines the \"Best Fit\" approach (minimizing remaining capacity)\n    with a stochastic element to encourage exploration of less obviously optimal bins.\n    It also prioritizes bins that are \"almost full\" to potentially group smaller items\n    more efficiently.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Determine which bins can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit, return all zeros\n\n    fitting_bins_caps = bins_remain_cap[can_fit_mask]\n\n    # Strategy 1: Best Fit - prioritize bins that will have the least remaining capacity\n    # This is a common greedy strategy for BPP.\n    remaining_after_fit = fitting_bins_caps - item\n    # We want to minimize remaining_after_fit, so higher priority for smaller remaining capacity.\n    # Add a small constant to avoid division by zero if remaining_after_fit is 0.\n    best_fit_scores = 1.0 / (1.0 + remaining_after_fit)\n\n    # Strategy 2: \"Almost Full\" bins - prioritize bins that are very close to being full\n    # This can help consolidate smaller items and leave larger capacities open.\n    # We define \"almost full\" as having a remaining capacity between 0 and a small threshold.\n    almost_full_threshold = 0.1 * np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 0.1\n    almost_full_mask_subset = (fitting_bins_caps > 0) & (fitting_bins_caps <= almost_full_threshold)\n    almost_full_scores = np.zeros_like(fitting_bins_caps)\n    almost_full_scores[almost_full_mask_subset] = 0.5 # Assign a moderate priority\n\n    # Strategy 3: Exploration - add a small random component to encourage trying different bins\n    # This is inspired by exploration in reinforcement learning.\n    exploration_factor = 0.1\n    random_scores = np.random.rand(len(fitting_bins_caps)) * exploration_factor\n\n    # Combine scores. A bin is good if it's a good best-fit OR it's almost full.\n    # We use a weighted sum, prioritizing best fit more.\n    combined_scores = (0.7 * best_fit_scores) + (0.3 * almost_full_scores) + random_scores\n\n    # Assign the calculated scores to the corresponding bins\n    priorities[can_fit_mask] = combined_scores\n\n    # Ensure bins that cannot fit have a priority of 0\n    priorities[~can_fit_mask] = 0\n\n    return priorities",
    "response_id": 4,
    "obj": 3.9289988033506273,
    "SLOC": 19.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response5.txt_stdout.txt",
    "code_path": "problem_iter2_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit Decreasing-like strategy.\n\n    This heuristic prioritizes bins that can fit the item and, among those,\n    prioritizes bins that will have the least remaining capacity *after*\n    placing the item. This aims to pack items tightly and minimize wasted space.\n\n    The priority is calculated as: 1 / (1 + remaining_capacity_after_placement).\n    This formula gives higher priority to bins where the remaining capacity\n    after placing the item is smaller.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity for bins that can fit the item\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # To prioritize bins with minimal remaining capacity after placement,\n    # we use the inverse of (1 + remaining_capacity_after_placement).\n    # Adding 1 ensures the denominator is always at least 1, and the inverse\n    # means smaller remaining capacities get higher priorities.\n    # Using 1e-9 as a small epsilon to avoid potential issues with very small remaining capacities,\n    # though with the '+ 1.0' this might be less critical but still good practice for numerical stability.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[can_fit_mask] = 1.0 / (1.0 + remaining_after_placement + 1e-9)\n\n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response6.txt_stdout.txt",
    "code_path": "problem_iter2_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a hybrid strategy.\n\n    This strategy prioritizes bins that can fit the item. Among those that can fit,\n    it favors bins that are closer to being full (i.e., have less remaining capacity\n    after fitting the item). This is a greedy approach that tries to leave larger\n    gaps in other bins for potentially larger future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize scores. Bins that cannot fit will have a very low score.\n    # We use a large negative number to ensure they are ranked last.\n    scores = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate a score.\n    # We want to prioritize bins that have less remaining capacity after the item is placed.\n    # The \"remaining capacity after fit\" is (bins_remain_cap - item).\n    # To prioritize smaller remaining capacities, we can use the negative of this value.\n    # Alternatively, to use a \"goodness\" score where higher is better, we can use\n    # the inverse of the remaining capacity *after* the item is placed, plus a small\n    # epsilon to avoid division by zero and to ensure that bins that become exactly full\n    # get a high score.\n    # A simple approach is to use `bins_remain_cap - item`, and we want to minimize this value.\n    # To convert this into a \"priority\" where higher is better, we can use its negative.\n    # A common heuristic for \"best fit\" is to find the bin with the minimum `bins_remain_cap - item`.\n    # So, a higher priority should be given to smaller values of `bins_remain_cap - item`.\n    # Let's assign a score that is the negative of the remaining capacity *after* fitting.\n    # This means bins that have `bins_remain_cap - item == 0` will have a score of 0.\n    # Bins that have `bins_remain_cap - item == X` will have a score of -X.\n    # This naturally prioritizes exact fits (score 0) over bins with remaining space.\n    scores[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # To make these scores more \"softmax-friendly\" or generally interpretable as positive\n    # priorities, we can shift them so that the minimum possible score (if we consider\n    # all possible remaining capacities) maps to a small positive value.\n    # The smallest `bins_remain_cap - item` could be 0. The largest would depend on\n    # the initial bin capacities and item sizes.\n    # A simpler approach that directly reflects the \"best fit\" idea is to assign a\n    # score that is higher for bins closer to fitting the item.\n    # Let's re-evaluate: we want to prioritize bins where `bins_remain_cap - item` is small.\n    # The smaller `bins_remain_cap - item` is, the higher the priority.\n    # So, a simple monotonic transformation that maps smaller `bins_remain_cap - item`\n    # to larger priority values is desired.\n    # Using `1.0 / (bins_remain_cap[can_fit_mask] - item + epsilon)` achieves this,\n    # as seen in `priority_v1`. This also provides a \"soft\" prioritization.\n\n    # Let's refine the reflection: \"Prioritize fits, then closeness to full.\"\n    # \"Closeness to full\" means minimizing the *remaining* capacity.\n    # For a bin `b` with remaining capacity `c_b`, and an item `i` of size `s_i`:\n    # If `c_b >= s_i`, the bin can fit.\n    # The \"closeness to full\" after fitting is `c_b - s_i`. We want to minimize this.\n    # So, priority should be higher when `c_b - s_i` is smaller.\n\n    # Let's try a simple assignment:\n    # For bins that can fit, score = -(bins_remain_cap - item)\n    # This means:\n    # - Exact fit (remaining capacity after fit = 0) gets score 0.\n    # - Bin with remaining capacity after fit = 1 gets score -1.\n    # - Bin with remaining capacity after fit = 10 gets score -10.\n    # This correctly prioritizes exact fits (0 > -1 > -10).\n\n    scores[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # If we want to ensure that even non-fitting bins have a chance to be selected\n    # (though with very low priority), we could assign a small negative value to them.\n    # However, for typical online bin packing, if an item cannot fit anywhere,\n    # a new bin must be opened. So, prioritizing only fitting bins is correct.\n    # The current `scores` array has `-np.inf` for non-fitting bins.\n\n    # The problem asks for a priority *score* for each bin.\n    # The previous \"Better code\" used softmax to convert scores into probabilities.\n    # The reflection suggests prioritizing fits, then closeness to full.\n    # A simple way to achieve this without softmax (if softmax is not strictly required by the prompt)\n    # is to directly return scores that reflect this preference.\n\n    # Let's consider a scenario:\n    # item = 5\n    # bins_remain_cap = [10, 6, 5, 12]\n    #\n    # Bin 0: can fit, remaining after fit = 10 - 5 = 5. Score = -5\n    # Bin 1: can fit, remaining after fit = 6 - 5 = 1. Score = -1\n    # Bin 2: can fit, remaining after fit = 5 - 5 = 0. Score = 0\n    # Bin 3: can fit, remaining after fit = 12 - 5 = 7. Score = -7\n    #\n    # Scores: [-5, -1, 0, -7]\n    # The highest score (0) is for bin 2 (exact fit).\n    # The next highest (-1) is for bin 1 (closest to full after fit).\n    # This aligns with the reflection.\n\n    # The question is what the output *should* be. If it's a direct priority score\n    # that will be used in a `max()` function to select the bin, then these negative\n    # scores work. If it's for a probabilistic selection (like softmax), then\n    # `priority_v1` is more appropriate. The prompt says \"the bin with the highest priority score will be selected\".\n    # This implies a deterministic selection based on the highest score.\n\n    # Therefore, the `scores` computed above directly serve as priority scores.\n    # No need for softmax if the selection is deterministic.\n\n    # Let's ensure that the priority scores are easily distinguishable and\n    # that non-fitting bins are clearly lower.\n    # Using `-np.inf` for non-fitting bins is good.\n    # For fitting bins, `-(bins_remain_cap - item)` works.\n\n    # Consider numerical stability and range of values.\n    # If `bins_remain_cap - item` can be very large, the negative scores will be very small (large negative).\n    # This is fine for direct comparison.\n\n    # Let's make it slightly more robust by ensuring non-fitting bins get a clearly worse score\n    # than any possible fitting bin score.\n    # The minimum possible value for `bins_remain_cap - item` is 0. So the maximum score is 0.\n    # Any score less than 0 is worse. So `-np.inf` for non-fitting bins is appropriate.\n\n    # The current implementation `scores[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)`\n    # directly implements the \"prioritize fits, then closeness to full\" by assigning\n    # higher scores to bins with smaller remaining capacity after fitting the item.\n\n    # Let's consider the case where the item is larger than any bin's capacity.\n    # `can_fit_mask` would be all `False`. `scores` would remain all `-np.inf`.\n    # This correctly indicates no valid placement.\n\n    # Final check of the strategy:\n    # 1. Prioritize bins that *can* fit the item. (Handled by `can_fit_mask` and initial `-np.inf`)\n    # 2. Among those that can fit, prioritize bins that are \"closer to full\".\n    #    \"Closer to full\" after placing the item means `bins_remain_cap - item` is minimized.\n    #    So, we want to assign higher priority when `bins_remain_cap - item` is small.\n    #    Our score `-(bins_remain_cap - item)` does exactly this:\n    #    - If `bins_remain_cap - item = 0` (exact fit), score is 0 (highest).\n    #    - If `bins_remain_cap - item = 1`, score is -1.\n    #    - If `bins_remain_cap - item = 5`, score is -5.\n    #    This order `0 > -1 > -5` matches the desired priority.\n\n    # This looks like a solid implementation of the described strategy.\n\n    return scores",
    "response_id": 6,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response7.txt_stdout.txt",
    "code_path": "problem_iter2_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Sigmoid Fit Score.\n\n    This version prioritizes bins where the remaining capacity is just enough to fit the item,\n    aiming to minimize wasted space. It uses a sigmoid function to create a peak preference\n    when the difference between bin capacity and item size is close to zero.\n\n    The function is designed to return higher scores for bins with less remaining capacity\n    that can still accommodate the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate a more preferred bin.\n    \"\"\"\n    # Initialize priorities to a very low value for bins that cannot fit the item.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities # No bin can fit the item\n\n    # Calculate the \"slack\" or wasted space for bins that can fit the item.\n    # slack = remaining_capacity - item_size\n    slack = bins_remain_cap[can_fit_mask] - item\n\n    # We want to maximize the score when slack is minimized (ideally 0).\n    # A common way to achieve a peak around 0 is using a Gaussian-like function\n    # or a transformed sigmoid.\n    #\n    # Let's use a sigmoid on the negative slack, scaled by a factor `k` to control steepness.\n    # The sigmoid function is `1 / (1 + exp(-x))`.\n    # We want high values when `slack` is small.\n    # So, consider `score = sigmoid(k * (-slack))`.\n    #   - If `slack` is 0: `sigmoid(0)` is 0.5.\n    #   - If `slack` is small positive: `sigmoid(small_neg)` is < 0.5.\n    #   - If `slack` is large positive: `sigmoid(large_neg)` is close to 0.\n    # This means we prioritize bins with *less* slack, but a slack of 0 isn't the highest score.\n    #\n    # Let's consider `score = sigmoid(k * (slack_limit - slack))`, where `slack_limit`\n    # is the ideal slack we're aiming for (e.g., 0).\n    # Or, more directly, let's create a score that peaks at `slack = 0`.\n    #\n    # A function like `exp(-k * slack**2)` peaks at slack=0. We can normalize this.\n    # However, sticking to the sigmoid family for continuity:\n    # The function `1 / (1 + exp(-k * x))` peaks at `x = 0`.\n    # We want slack to be close to 0.\n    # Consider `score = 1 / (1 + exp(k * slack))`.\n    #   - slack = 0: `1 / (1 + exp(0)) = 1 / (1 + 1) = 0.5`.\n    #   - slack > 0 (positive waste): `exp(k * slack)` > 1, so `1 / (1 + exp(k*slack))` < 0.5.\n    #   - slack < 0 (item doesn't fit, handled by -inf initialization).\n    # This function gives the highest score for a perfect fit (slack=0) and decreases as waste increases.\n    #\n    # A small modification to increase the peak value and steepness could be useful.\n    # We can scale the output or shift the input.\n    # Let's try to make the peak value closer to 1 and have it drop off faster.\n    #\n    # Let's use a parameter `sensitivity` to control how quickly the score drops off\n    # as slack increases. A higher sensitivity means a sharper drop.\n    #\n    # Proposed function: `score = exp(-sensitivity * slack)`\n    #   - slack = 0: `exp(0) = 1` (Highest score)\n    #   - slack > 0: `exp(-sensitivity * slack)` < 1, decreases as slack increases.\n    # This is simpler and directly targets minimizing slack. It avoids the [-inf, 1] range issue.\n    # The problem statement implies a higher score is better.\n\n    # Let's re-evaluate the sigmoid's role for \"good fit\".\n    # We want bins where `bins_remain_cap` is slightly larger than `item`.\n    # `fit_diff = bins_remain_cap - item`. We want `fit_diff` to be small and positive.\n    # The function `1 / (1 + exp(-k * (item - bins_remain_cap)))` or `1 / (1 + exp(k * (bins_remain_cap - item)))`\n    # has its peak where `bins_remain_cap - item` is minimal.\n\n    # Let's use a function that peaks at `slack = 0` and decreases for `slack > 0`.\n    # A simple exponential decay `exp(-k * slack)` works well.\n    # A slightly modified sigmoid might also work.\n    # Let's try `score = 1 / (1 + exp(k * slack))` as derived before, but we might want to\n    # scale it to be more pronounced.\n\n    # To make the \"best fit\" (slack=0) more distinct, we can:\n    # 1. Scale the slack: `k * slack`\n    # 2. Apply a function that peaks at 0. `exp(-k * slack)` is good.\n    # 3. Or use sigmoid with a shift: `sigmoid(k * (ideal_slack - slack))` where ideal_slack is 0.\n    #    `sigmoid(-k * slack)`:\n    #       - slack=0: sigmoid(0) = 0.5\n    #       - slack>0: sigmoid(-k * slack) < 0.5\n    #       - slack<0: sigmoid(-k * slack) > 0.5 (but these are already -inf)\n    #    This also prioritizes smaller slack.\n\n    # Let's try a combination: use sigmoid on the negative difference, scaled by a factor `k`\n    # for steepness, and maybe add an offset or scale to make the best fit have a higher score.\n    # Or, more simply, transform the slack into a score directly.\n\n    # Let's use the `exp(-k * slack)` approach for clarity and effectiveness.\n    # Higher `k` means the score drops faster as slack increases.\n    k_sensitivity = 2.0 # Controls how quickly preference drops with increasing slack\n\n    # Calculate scores: higher for smaller slack (closer to 0)\n    scores_for_fitting_bins = np.exp(-k_sensitivity * slack)\n\n    # Assign these calculated scores back to the appropriate positions in the priorities array\n    priorities[can_fit_mask] = scores_for_fitting_bins\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "SLOC": 10.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response8.txt_stdout.txt",
    "code_path": "problem_iter2_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit strategy.\n\n    The Best Fit strategy prioritizes bins that leave the minimum remaining capacity\n    after placing the item. This strategy aims to fill bins as much as possible,\n    potentially reducing the total number of bins used.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros. The calling function\n    # would typically handle creating a new bin in this scenario.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the remaining capacity after placing the item in suitable bins\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # The priority is the negative of the remaining capacity after placement.\n    # We use the negative because we want the *minimum* remaining capacity\n    # to have the highest priority (i.e., the most negative value).\n    # Higher priority values mean the bin is a better fit.\n    priorities[can_fit_mask] = -remaining_after_placement\n\n    return priorities",
    "response_id": 8,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority for packing an item into bins, prioritizing tight fits.\n\n    This heuristic prioritizes bins that can accommodate the item. Among those,\n    it assigns a higher priority to bins that will have the least remaining\n    capacity after packing (i.e., a tighter fit). Bins that are too small\n    for the item receive a priority of 0.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n        Higher scores indicate a more desirable bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that have enough remaining capacity for the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit the item, return all zeros.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # For bins that can fit the item, calculate the remaining capacity *after* packing.\n    # The goal is to minimize this remaining capacity for a tighter fit.\n    remaining_after_packing = bins_remain_cap[can_fit_mask] - item\n\n    # To prioritize tighter fits (smaller remaining_after_packing), we can use\n    # the negative of the remaining capacity. Since priority functions typically\n    # aim to maximize the score, we can transform this. A common strategy is to\n    # use a large value minus the remaining capacity, ensuring that smaller\n    # remaining capacities yield higher scores. Using the maximum available\n    # capacity as the large constant helps normalize scores relative to bin sizes.\n    # The `bins_remain_cap[can_fit_mask].max()` is used to ensure that even\n    # with the subtraction, the priority remains positive and is scaled by\n    # the general magnitude of bin capacities.\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask].max() - remaining_after_packing\n\n    # Add a strong preference for bins that result in exactly zero remaining capacity\n    # (perfect fits). This can be achieved by adding a very large constant to these.\n    exact_fit_mask = (remaining_after_packing == 0)\n    if np.any(exact_fit_mask):\n        # A large epsilon to ensure exact fits are always preferred over near-fits.\n        priorities[can_fit_mask][exact_fit_mask] += 1e9\n\n    return priorities",
    "response_id": 9,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  }
]