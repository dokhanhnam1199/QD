```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a refined Sigmoid Fit Score.

    This version prioritizes bins where the remaining capacity is just enough to fit the item,
    aiming to minimize wasted space. It uses a sigmoid function to create a peak preference
    when the difference between bin capacity and item size is close to zero.

    The function is designed to return higher scores for bins with less remaining capacity
    that can still accommodate the item.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Returns:
        Array of same size as bins_remain_cap with priority score of each bin.
        Higher scores indicate a more preferred bin.
    """
    # Initialize priorities to a very low value for bins that cannot fit the item.
    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)

    # Identify bins where the item can fit.
    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        return priorities # No bin can fit the item

    # Calculate the "slack" or wasted space for bins that can fit the item.
    # slack = remaining_capacity - item_size
    slack = bins_remain_cap[can_fit_mask] - item

    # We want to maximize the score when slack is minimized (ideally 0).
    # A common way to achieve a peak around 0 is using a Gaussian-like function
    # or a transformed sigmoid.
    #
    # Let's use a sigmoid on the negative slack, scaled by a factor `k` to control steepness.
    # The sigmoid function is `1 / (1 + exp(-x))`.
    # We want high values when `slack` is small.
    # So, consider `score = sigmoid(k * (-slack))`.
    #   - If `slack` is 0: `sigmoid(0)` is 0.5.
    #   - If `slack` is small positive: `sigmoid(small_neg)` is < 0.5.
    #   - If `slack` is large positive: `sigmoid(large_neg)` is close to 0.
    # This means we prioritize bins with *less* slack, but a slack of 0 isn't the highest score.
    #
    # Let's consider `score = sigmoid(k * (slack_limit - slack))`, where `slack_limit`
    # is the ideal slack we're aiming for (e.g., 0).
    # Or, more directly, let's create a score that peaks at `slack = 0`.
    #
    # A function like `exp(-k * slack**2)` peaks at slack=0. We can normalize this.
    # However, sticking to the sigmoid family for continuity:
    # The function `1 / (1 + exp(-k * x))` peaks at `x = 0`.
    # We want slack to be close to 0.
    # Consider `score = 1 / (1 + exp(k * slack))`.
    #   - slack = 0: `1 / (1 + exp(0)) = 1 / (1 + 1) = 0.5`.
    #   - slack > 0 (positive waste): `exp(k * slack)` > 1, so `1 / (1 + exp(k*slack))` < 0.5.
    #   - slack < 0 (item doesn't fit, handled by -inf initialization).
    # This function gives the highest score for a perfect fit (slack=0) and decreases as waste increases.
    #
    # A small modification to increase the peak value and steepness could be useful.
    # We can scale the output or shift the input.
    # Let's try to make the peak value closer to 1 and have it drop off faster.
    #
    # Let's use a parameter `sensitivity` to control how quickly the score drops off
    # as slack increases. A higher sensitivity means a sharper drop.
    #
    # Proposed function: `score = exp(-sensitivity * slack)`
    #   - slack = 0: `exp(0) = 1` (Highest score)
    #   - slack > 0: `exp(-sensitivity * slack)` < 1, decreases as slack increases.
    # This is simpler and directly targets minimizing slack. It avoids the [-inf, 1] range issue.
    # The problem statement implies a higher score is better.

    # Let's re-evaluate the sigmoid's role for "good fit".
    # We want bins where `bins_remain_cap` is slightly larger than `item`.
    # `fit_diff = bins_remain_cap - item`. We want `fit_diff` to be small and positive.
    # The function `1 / (1 + exp(-k * (item - bins_remain_cap)))` or `1 / (1 + exp(k * (bins_remain_cap - item)))`
    # has its peak where `bins_remain_cap - item` is minimal.

    # Let's use a function that peaks at `slack = 0` and decreases for `slack > 0`.
    # A simple exponential decay `exp(-k * slack)` works well.
    # A slightly modified sigmoid might also work.
    # Let's try `score = 1 / (1 + exp(k * slack))` as derived before, but we might want to
    # scale it to be more pronounced.

    # To make the "best fit" (slack=0) more distinct, we can:
    # 1. Scale the slack: `k * slack`
    # 2. Apply a function that peaks at 0. `exp(-k * slack)` is good.
    # 3. Or use sigmoid with a shift: `sigmoid(k * (ideal_slack - slack))` where ideal_slack is 0.
    #    `sigmoid(-k * slack)`:
    #       - slack=0: sigmoid(0) = 0.5
    #       - slack>0: sigmoid(-k * slack) < 0.5
    #       - slack<0: sigmoid(-k * slack) > 0.5 (but these are already -inf)
    #    This also prioritizes smaller slack.

    # Let's try a combination: use sigmoid on the negative difference, scaled by a factor `k`
    # for steepness, and maybe add an offset or scale to make the best fit have a higher score.
    # Or, more simply, transform the slack into a score directly.

    # Let's use the `exp(-k * slack)` approach for clarity and effectiveness.
    # Higher `k` means the score drops faster as slack increases.
    k_sensitivity = 2.0 # Controls how quickly preference drops with increasing slack

    # Calculate scores: higher for smaller slack (closer to 0)
    scores_for_fitting_bins = np.exp(-k_sensitivity * slack)

    # Assign these calculated scores back to the appropriate positions in the priorities array
    priorities[can_fit_mask] = scores_for_fitting_bins

    return priorities
```
