```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a hybrid strategy.

    This strategy prioritizes bins that offer a tight fit (minimizing leftover capacity)
    while also considering bins that leave sufficient space for future, potentially larger items.
    It aims to balance exploiting near-perfect fits with exploring options that maintain
    flexibility. A small bias is introduced to favor bins that are less utilized,
    thereby encouraging a more even distribution of items and avoiding premature
    filling of a few bins.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    num_bins = len(bins_remain_cap)
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins that can fit the current item
    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        return priorities  # No bin can fit the item

    fitting_bins_caps = bins_remain_cap[can_fit_mask]

    # Strategy 1: Prioritize tight fits (Best Fit)
    # Calculate remaining capacity after placing the item. We want to minimize this.
    remaining_after_fit = fitting_bins_caps - item
    # Invert the remaining capacity and add 1 to avoid division by zero and to
    # ensure higher scores for smaller remaining capacities.
    best_fit_scores = 1.0 / (1.0 + remaining_after_fit)

    # Strategy 2: Favor bins that leave more space (Worst Fit tendency for future)
    # This aims to keep bins with larger remaining capacities open for larger items.
    # We want to prioritize bins with higher remaining capacity among those that can fit the item.
    # Adding a small epsilon to avoid division by zero if fitting_bins_caps is 0 (though unlikely here due to can_fit_mask).
    # We scale this by a factor to balance it with best_fit_scores.
    # Using a log scale can help compress large differences and prevent extreme values.
    # Using max capacity as a reference point for scaling.
    max_cap_overall = np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 1.0
    space_for_future_scores = np.log1p(fitting_bins_caps / max_cap_overall) * 0.5 # Log scale, scaled down

    # Strategy 3: Introduce a slight bias towards less utilized bins to encourage diversity
    # Calculate the proportion of capacity used in bins that can fit the item.
    # Higher proportion means more utilized. We want to slightly penalize these.
    # Or, equivalently, reward bins with more remaining capacity (less utilized).
    # Let's rethink: We want to favor bins that are NOT nearly full, to leave space.
    # So, we want to prioritize bins that have *more* remaining capacity *among those that can fit*.
    # This is somewhat captured by space_for_future_scores.
    # A simpler approach: penalize bins that are *very* full.
    # Let's use a term that increases as remaining_after_fit gets closer to 0.
    # Instead, let's favor bins that are less full to encourage distribution.
    # So, we want to boost priority for bins with larger remaining capacity.
    # This is already handled by space_for_future_scores.

    # Let's combine Best Fit with a preference for bins that leave *reasonable* space,
    # but not excessively large gaps that might be wasted.
    # We can use a Gaussian-like function centered slightly away from zero remaining capacity.
    # Or simply, combine Best Fit with a factor that prefers bins with more room, but not excessively so.

    # Let's refine the combination:
    # Primary: Best Fit (minimizing remaining space)
    # Secondary: Favor bins that leave a moderate amount of space.
    # Calculate a score that peaks when remaining_after_fit is moderate (e.g., item_size).
    # This means the bin capacity was ~2*item_size.
    # We want to encourage fits where remaining_after_fit is not too small (handled by BF)
    # and not too large.

    # A simpler approach for diversity/exploration without explicit randomness:
    # Prioritize bins that are not *too* close to full, but also not *too* empty.
    # Let's try a score that favors bins with remaining capacity between X and Y.
    # This can be complex.

    # Let's stick to a combination of tight fits and a slight preference for
    # bins that still have a good amount of capacity left for future items.
    # Combine Best Fit with a scaled version of remaining capacity.
    # Higher remaining capacity = higher score (but this can contradict best fit).

    # Let's try a weighted sum favoring best fit, but giving a boost to bins
    # that are not too full and not too empty.
    # Consider the 'slack' or 'unfilled' capacity in a fitting bin.
    # We want to minimize (fitting_bins_caps - item) but also not
    # pick bins that are almost empty if a tighter fit exists.

    # Revised combination:
    # - Prioritize tightest fits (minimize `remaining_after_fit`).
    # - Add a slight bonus for bins that are not *nearly* full, to leave more room.
    # This means for `remaining_after_fit`, smaller is better, but we might
    # slightly penalize the absolute minimum if it's extremely close to zero.

    # Let's use a score that is high for small `remaining_after_fit`,
    # but perhaps has a slight decay if `remaining_after_fit` is very, very small.
    # Or, a score that rewards `remaining_after_fit` up to a certain point, then decays.
    # A Gaussian-like function centered at a moderate `remaining_after_fit` could work.
    # e.g., exp(-(remaining_after_fit - target_slack)^2 / sigma^2)

    # Let's try a simpler hybrid:
    # Score = (Best Fit Score) + (Bonus for leaving reasonable space)
    # Best Fit Score: 1 / (1 + remaining_after_fit)
    # Bonus for leaving space: A function that increases with `fitting_bins_caps` up to a point.
    # Let's try `np.tanh(fitting_bins_caps / max_cap_overall)` as a bonus.
    # This would boost bins that are less full.

    # Combine Best Fit with a preference for bins with more remaining capacity
    # among those that can fit the item. This is to preserve larger bins for larger items.
    # A simple linear scaling of remaining capacity (normalized).
    # Higher remaining capacity should get a higher score here.
    # Let's use remaining_after_fit scaled.

    # Combined score:
    # We want to maximize tight fits AND maximize remaining capacity among fitting bins.
    # This is a multi-objective problem. A common approach is a weighted sum.
    # We can define two components and sum them.

    # Component 1: Tightness (Higher is better for smaller remaining_after_fit)
    tightness_score = 1.0 / (1.0 + remaining_after_fit + 1e-9) # Add epsilon for stability

    # Component 2: Space Preservation (Higher is better for larger remaining_after_fit)
    # This is counter to tightness. Let's reframe.
    # We want to *minimize* remaining_after_fit.
    # Let's consider the 'quality' of the fit. A perfect fit (remaining_after_fit = 0) is good.
    # A fit that leaves a lot of space might be good for future items.

    # Let's prioritize bins that are almost full, as this is a common greedy heuristic.
    # If a bin is almost full, placing an item there might be efficient.
    # However, the reflection asks to balance this with leaving room.

    # Let's try a score that is high when remaining_after_fit is small (best fit),
    # but also adds a term that prefers bins that are not completely empty after the fit.
    # This encourages packing items into bins that already have some content.

    # Consider `remaining_after_fit`. We want to be small.
    # Let's create a score that rewards small values of `remaining_after_fit`,
    # but perhaps with a slight bonus for the *second* best fit if the best fit is *too* tight
    # (i.e., leaves almost no room). This encourages diversity.

    # Revised Strategy:
    # 1. Primary goal: Minimize `remaining_after_fit` (Best Fit).
    # 2. Secondary goal: If there are multiple bins with very small `remaining_after_fit`,
    #    prefer the one that leaves a bit more space to avoid "over-packing".
    #    This can be achieved by slightly penalizing `remaining_after_fit` values close to zero.

    # Let's create a score that peaks when `remaining_after_fit` is close to zero but not exactly zero.
    # This is tricky. A simple inverse is fine for best fit.

    # Let's combine Best Fit with a penalty for leaving too much room.
    # `best_fit_scores` already prioritizes minimal remaining capacity.
    # If `fitting_bins_caps` are large, `remaining_after_fit` will also be large.
    # `best_fit_scores` will be small for these. So it naturally penalizes leaving too much room.

    # How to balance "tight fits" with "leaving room for future items"?
    # If we have item sizes {0.6, 0.6, 0.2, 0.2, 0.2} and bin capacity 1.0.
    # Item 0.6: Fit into bin 1. Remaining cap: [0.4].
    # Item 0.6: Fit into bin 2. Remaining cap: [0.4, 0.4].
    # Item 0.2:
    #   Bin 1: remaining_after_fit = 0.4 - 0.2 = 0.2. Best fit score = 1/(1+0.2) = 0.833
    #   Bin 2: remaining_after_fit = 0.4 - 0.2 = 0.2. Best fit score = 1/(1+0.2) = 0.833
    #   If there's another bin with capacity 1.0 (Bin 3): remaining_after_fit = 1.0 - 0.2 = 0.8. Best fit score = 1/(1+0.8) = 0.555
    # The current `best_fit_scores` would favor Bin 1 or 2.

    # Let's try to add a component that favors bins that are not nearly empty *after* fitting.
    # This means we want `remaining_after_fit` to be not too large.
    # So, we want to minimize `remaining_after_fit` and also minimize `remaining_after_fit` again?
    # This implies a preference for the absolute best fit.

    # The reflection suggests:
    # - Prioritize tight fits by minimizing leftover capacity. (This is Best Fit).
    # - Explore diverse bin selection strategies.
    # - Balance exploiting near-perfect fits with exploring options that leave room for future items.

    # To balance, we can't just do pure Best Fit.
    # Let's create a score that is sensitive to the *magnitude* of the remaining capacity.
    # A function that is high for `remaining_after_fit` near 0, but also high for
    # `remaining_after_fit` that is still significant (but not excessive).

    # Let's try a score based on the ratio of remaining capacity to item size.
    # `fitting_bins_caps / item`.
    # A ratio close to 1 means `fitting_bins_caps` is just slightly larger than `item`.
    # We want `fitting_bins_caps` to be just slightly larger than `item`.
    # `fitting_bins_caps - item` should be small.

    # Consider the 'gap' relative to the bin capacity.
    # `remaining_after_fit / fitting_bins_caps`. We want this to be small.
    # So, `1.0 / (1.0 + remaining_after_fit / fitting_bins_caps)` as a score.
    # This is essentially `fitting_bins_caps / (fitting_bins_caps + fitting_bins_caps - item)`
    # = `fitting_bins_caps / (2 * fitting_bins_caps - item)`.

    # Let's combine Best Fit with a factor that favors bins with moderate remaining space.
    # `best_fit_scores = 1.0 / (1.0 + remaining_after_fit)`
    # We want to add a bonus if `remaining_after_fit` is not too small AND not too large.
    # A Gaussian-like function could work.
    # Let target_slack be `item * 0.5` (a moderate slack).
    # Slack score = exp(-(remaining_after_fit - target_slack)**2 / sigma**2)
    # This might be too complex.

    # Simpler approach:
    # Combine Best Fit with a term that boosts bins with larger remaining capacity.
    # This encourages leaving bins less full.
    # But "leaving room for future items" can also mean packing smaller items into
    # bins that already have some capacity, rather than opening a new bin.

    # Let's try a weighted sum:
    # `priority = w1 * best_fit_score + w2 * preference_for_more_space`
    # `best_fit_score`: `1.0 / (1.0 + remaining_after_fit)` (higher is better)
    # `preference_for_more_space`: `fitting_bins_caps` (higher is better)
    # We need to normalize `fitting_bins_caps`.
    # Let's use `fitting_bins_caps / max_cap_overall`.

    # Let's weight Best Fit more heavily, and add a scaled remaining capacity component.
    # The goal is to be greedy (best fit) but also to keep options open.
    # A bin that is 70% full and can fit the item, leaving it 85% full, might be better
    # than a bin that is 10% full and can fit the item, leaving it 30% full.
    # This depends on the item size.

    # Let's focus on minimizing the *waste* created by the current item placement.
    # Waste = `remaining_after_fit`. We want to minimize this.
    # But "leaving room for future items" suggests that sometimes a slightly larger `remaining_after_fit`
    # might be preferable if it's a more balanced use of space.

    # Consider the state of the bin itself: `fitting_bins_caps`.
    # A bin that is `0.8` capacity and fits `0.2` item (leaves `0.6`) is different from
    # a bin that is `0.3` capacity and fits `0.2` item (leaves `0.1`).
    # The first case leaves more absolute space.

    # Let's try a score that combines the inverse of remaining capacity (for tight fits)
    # with the remaining capacity itself (to favor bins with more space).
    # We can use a quadratic function to capture this preference for moderate slack.
    # Score = -(remaining_after_fit - slack_target)^2
    # This would peak at `slack_target`.

    # Let's try a simpler weighted sum:
    # Prioritize bins that result in minimum remaining capacity.
    # Also, give a slight boost to bins that have more capacity to begin with (among fitting bins).
    # This encourages spreading items.

    # `best_fit_metric = -remaining_after_fit` (we want to maximize this, i.e., minimize remaining_after_fit)
    # `space_metric = fitting_bins_caps` (we want to maximize this)

    # Combine these: `score = weight_bf * (-remaining_after_fit) + weight_space * fitting_bins_caps`
    # Need to normalize `fitting_bins_caps` and ensure the scale of `remaining_after_fit` is handled.
    # Using `1.0 / (1.0 + remaining_after_fit)` handles the scale for BF.
    # For space, `fitting_bins_caps / max_cap_overall` is a good normalized metric.

    # Let's try:
    # Score = (1 - alpha) * (1.0 / (1.0 + remaining_after_fit)) + alpha * (fitting_bins_caps / max_cap_overall)
    # This would prioritize best fit, but with a tendency to favor bins that are less full.
    # If `alpha` is small, it's mostly best fit. If `alpha` is large, it favors less full bins.

    # Let's use `alpha = 0.3` to give a significant but secondary preference for leaving space.
    alpha = 0.3
    max_cap_overall = np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 1.0

    # Best Fit component: Higher score for smaller `remaining_after_fit`.
    best_fit_component = 1.0 / (1.0 + remaining_after_fit + 1e-9)

    # Space preservation component: Higher score for larger `fitting_bins_caps`.
    # Normalize by the maximum capacity to get a relative measure.
    space_component = fitting_bins_caps / max_cap_overall

    # Combine components with weights.
    # The reflection asks to balance exploiting near-perfect fits with exploring options that leave room.
    # This suggests a preference for moderate slack, not just minimum or maximum.
    # The current combination prioritizes tight fits, and then favors bins that are less full overall.
    # This might push items to less utilized bins even if a tighter fit exists elsewhere.

    # Let's refine the 'leaving room' aspect.
    # We want to avoid bins that become *too* empty after fitting.
    # So, we want `remaining_after_fit` not to be too large.
    # This means `fitting_bins_caps` should not be excessively larger than `item`.

    # Let's try a score that is high when `remaining_after_fit` is small, but also
    # penalizes cases where `fitting_bins_caps` is much larger than `item`.
    # This means we want `fitting_bins_caps` to be slightly larger than `item`.

    # Consider the ratio `fitting_bins_caps / item`.
    # We want this ratio to be close to 1.
    # A score like `1.0 / (1.0 + abs(fitting_bins_caps / item - 1.0))` might work.
    # This favors fits where the bin capacity is just slightly larger than the item.
    # This implicitly means `remaining_after_fit` is small.

    # Let's go back to a simpler, effective heuristic:
    # Prioritize bins that minimize remaining capacity.
    # If there are ties, or near-ties, consider which bin has more residual capacity.
    # This leads to a weighted combination of minimizing remaining capacity and maximizing remaining capacity.

    # Let's try a metric that favors bins that are "almost full" but not completely.
    # This is a form of "first fit decreasing" or "best fit" strategy.
    # For online, we adapt.

    # Let's try a score that is high for tight fits, and a secondary measure that
    # favors bins that are not too empty.
    # `best_fit_score`: `1 / (1 + remaining_after_fit)`
    # `not_too_empty_score`: `tanh(fitting_bins_caps / max_cap_overall)` - this favors bins that were initially more full.

    # How about: Maximize the utility of the bin.
    # Utility can be related to how "full" the bin becomes.
    # We want to reach a state where bins are filled efficiently.

    # Let's try a compromise:
    # Priority is mainly determined by Best Fit.
    # A secondary factor that slightly boosts bins that are not nearly empty *after* the fit.
    # This means `remaining_after_fit` shouldn't be extremely small, but also not too large.

    # Consider `remaining_after_fit`.
    # Best Fit: High score for small values.
    # Leaving room: High score for large values.
    # Balance: High score for moderate values.

    # Let's try a simple scoring function that rewards tightness but doesn't completely ignore
    # bins that leave more room.

    # Score = `best_fit_score` + `slack_bonus`
    # `best_fit_score`: `1.0 / (1.0 + remaining_after_fit + 1e-9)`
    # `slack_bonus`: This should be higher for `remaining_after_fit` that is not too small,
    #                but also not excessively large.
    # Let's use `np.tanh(remaining_after_fit / max_cap_overall)` as a bonus.
    # This bonus increases with remaining capacity.
    # This combination would favor tight fits that also happen to have more initial capacity.

    # The reflection emphasizes "balancing exploiting near-perfect fits with exploring options that leave room".
    # This suggests that a slightly less perfect fit that leaves more room might be preferred over an extremely tight fit.

    # Let's try a metric that is high when `remaining_after_fit` is small, but not extremely small.
    # And also not extremely large.
    # Example: item=0.5, bin_caps=[0.6, 0.8, 1.0]
    #   Bin 1: rem_after_fit = 0.1. BF = 1/1.1 = 0.909
    #   Bin 2: rem_after_fit = 0.3. BF = 1/1.3 = 0.769
    #   Bin 3: rem_after_fit = 0.5. BF = 1/1.5 = 0.667
    # Best fit picks Bin 1.
    # If we want to leave more room, maybe Bin 2 is good.

    # Let's try a score that is a weighted sum of the negative remaining capacity (to maximize)
    # and the negative squared remaining capacity (to penalize larger remaining capacity).
    # Score = `w1 * (-remaining_after_fit)` + `w2 * (-remaining_after_fit**2)`
    # This would strongly favor small `remaining_after_fit`.

    # Alternative: A penalty for both small and large remaining capacity.
    # Minimize `(remaining_after_fit - target_slack)**2`.
    # This means maximizing `-(remaining_after_fit - target_slack)**2`.
    # This function peaks at `target_slack`.

    # Let `target_slack` be `item * 0.2` (a small slack).
    # score = np.exp(-(remaining_after_fit - item * 0.2)**2 / (item * 0.5)**2)
    # This would favor fits leaving ~0.2 item size as remainder.

    # Let's try a simpler approach based on the reflection:
    # Prioritize tight fits (minimize `remaining_after_fit`).
    # Also, consider bins that leave "room for future items".
    # This could mean bins that still have a significant amount of capacity left.

    # Let's use a score that is primarily best-fit, but with a boost for bins
    # that have a moderate amount of remaining capacity *after* the item is placed.
    # A bin that has 0.1 remaining is very tight. A bin with 0.5 remaining might be too much.
    # A bin with 0.2-0.3 remaining might be a good compromise.

    # Score = `best_fit_score` + `slack_preference`
    # `best_fit_score = 1.0 / (1.0 + remaining_after_fit + 1e-9)`
    # `slack_preference`: a function that is high for `remaining_after_fit` in a certain range.
    # Let's use a simple linear increase for `slack_preference` but capped.
    # Prefer bins with more remaining capacity (among fitting bins) but with diminishing returns.

    # Let's combine `best_fit_score` with `fitting_bins_caps`.
    # `combined_score = best_fit_score * (1 + 0.2 * (fitting_bins_caps / max_cap_overall))`
    # This gives a boost to bins that were initially more full.

    # Let's re-read the reflection: "Balance exploiting near-perfect fits with exploring options that leave room for future items."
    # This means we don't always pick the absolute best fit.
    # We want a score that is high for `remaining_after_fit` close to 0, but also for `remaining_after_fit` that is moderately larger.

    # Consider a score function `f(r)` where `r` is `remaining_after_fit`.
    # `f(r)` should be high for small `r`, but also for moderate `r`.
    # A possible function: `r` itself (favors large remaining) AND `1/(1+r)` (favors small remaining).
    # Weighted sum: `w1 * (1/(1+r)) + w2 * r`.
    # `w1` should be larger for tight fits. `w2` for leaving room.

    # Let's try:
    # Score = (1-beta) * (1 / (1 + remaining_after_fit)) + beta * (remaining_after_fit / max_cap_overall)
    # Here, `remaining_after_fit` is scaled to be comparable.
    # If `beta` is small, it's mostly best fit.
    # If `beta` is large, it favors bins that leave more space.
    # This still doesn't capture "moderate slack".

    # Let's use a score that rewards tightness, and also rewards bins that were initially less full.
    # The logic is: a tight fit into a bin that is already somewhat full is good.
    # A tight fit into a very empty bin is also okay.
    # The problem is picking between a tight fit and a looser fit that leaves more room.

    # Let's try a weighted combination of two preferences:
    # 1. Minimizing remaining capacity: `score1 = 1.0 / (1.0 + remaining_after_fit + 1e-9)`
    # 2. Maximizing the remaining capacity *after* the fit, normalized: `score2 = fitting_bins_caps / max_cap_overall`
    # The reflection suggests balancing these.
    # `final_score = (1 - weight) * score1 + weight * score2`
    # If `weight` is small, we lean towards best fit.
    # If `weight` is large, we lean towards leaving more space.

    # Let's consider `weight = 0.4`. This gives a significant preference for leaving more space.
    weight = 0.4
    score1 = 1.0 / (1.0 + remaining_after_fit + 1e-9)
    score2 = fitting_bins_caps / max_cap_overall

    # A potential issue: if `fitting_bins_caps` are very close to each other, `score2` differences will be small.
    # If `remaining_after_fit` values are very close, `score1` differences will be small.

    # Let's try to introduce a "quality" measure of the fit.
    # Quality = `1.0 - remaining_after_fit / fitting_bins_caps` (proportion of bin used)
    # We want this to be high.
    # `quality_score = fitting_bins_caps - remaining_after_fit` -> which is `item`? No.
    # `quality_score = item / fitting_bins_caps`

    # Let's try a combination that favors tight fits, but penalizes fits that leave *very little* or *excessive* space.
    # Score = `-(remaining_after_fit - slack_target)^2`
    # Let `slack_target = item * 0.2` (a small slack).
    # This function peaks when remaining_after_fit is `item * 0.2`.
    # This favors fits where the bin was about `item + item*0.2 = 1.2 * item` capacity.

    # Let's try this:
    # Score = `1.0 / (1.0 + remaining_after_fit)`  (Best Fit)
    # Add a term that slightly favors bins that are not "too full" after the fit.
    # This means `remaining_after_fit` shouldn't be too small.
    # But it also shouldn't be too large.

    # Final attempt: Combine Best Fit with a preference for bins that leave a moderate amount of space.
    # The overall goal is to reduce the number of bins.
    # Tight fits are good for this.
    # Leaving room is good if it means future items can be packed more efficiently.

    # Let's simplify the "leaving room" aspect: prioritize bins that are not almost full.
    # Among the fitting bins, we want to pick one that minimizes `remaining_after_fit`,
    # but if there are multiple bins with similar `remaining_after_fit`, pick the one
    # that was initially less full (i.e., had more `fitting_bins_caps`).

    # `best_fit_score = 1.0 / (1.0 + remaining_after_fit + 1e-9)`
    # `less_full_bonus = fitting_bins_caps / max_cap_overall`
    # `combined_score = best_fit_score + 0.2 * less_full_bonus`
    # This seems reasonable: prioritize tight fits, with a secondary preference for less full bins.
    # The weight 0.2 can be tuned.

    # Let's try this combination.
    combined_scores_subset = (1.0 / (1.0 + remaining_after_fit + 1e-9)) + \
                             0.2 * (fitting_bins_caps / max_cap_overall)

    # Assign the calculated scores to the corresponding bins
    priorities[can_fit_mask] = combined_scores_subset

    # Ensure bins that cannot fit have a priority of 0
    priorities[~can_fit_mask] = 0

    return priorities
```
