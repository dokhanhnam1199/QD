```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Softmax-Based Fit strategy.

    The Softmax-Based Fit strategy assigns a higher priority to bins that have a
    remaining capacity that, after placing the item, is closer to zero, effectively
    trying to "fill" bins more effectively. The priority is calculated using the
    softmax function, where a larger negative value of (remaining_capacity - item_size)
    results in a higher probability of being chosen.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Calculate the remaining capacity if the item is placed in each bin
    potential_remain_cap = bins_remain_cap - item

    # We want to prioritize bins where the remaining capacity is small (close to zero)
    # after placing the item. So, we use the negative of the potential remaining capacity.
    # A smaller (less negative) value in potential_remain_cap means it's a better fit.
    # To map this to a probability-like score where higher is better, we can use
    # -potential_remain_cap, or more directly, the negative of the "waste".
    # However, a simpler and effective approach for softmax is to consider the "fit quality" directly.
    # We want to maximize the chance of placing the item without exceeding capacity,
    # and among those that fit, we prefer those that leave minimal remaining space.
    # Let's define a score that is higher when potential_remain_cap is small and non-negative.
    # A common approach for "fit" is to maximize the remaining capacity up to the item size.
    # Or, minimize the 'gap' after placing the item.

    # Consider the 'fitness' as how well the item fits.
    # A good fit is when `bins_remain_cap - item` is close to 0.
    # We can create a value that is higher when `bins_remain_cap >= item` and
    # `bins_remain_cap - item` is minimized.

    # Let's transform the potential remaining capacities into scores.
    # For bins where the item does not fit (potential_remain_cap < 0), we want to assign a very low score.
    # For bins where the item fits, we want to assign a score based on how tightly it fits.
    # A tighter fit means `potential_remain_cap` is smaller (but >= 0).

    # We can use `np.exp` for softmax. The input to `exp` should be values
    # where larger values correspond to higher probabilities.
    # Let's create a score `s` for each bin such that:
    # 1. If `bins_remain_cap[i] < item`, `s[i]` is very low.
    # 2. If `bins_remain_cap[i] >= item`, `s[i]` is higher when `bins_remain_cap[i] - item` is smaller.

    # We can model this by considering `-abs(potential_remain_cap)` for invalid fits
    # and `-(potential_remain_cap)` for valid fits. Or, simpler, a large negative
    # constant for invalid fits, and the negative of the remaining capacity for valid fits.

    # Let's define a 'desirability' score:
    # For bins where item fits: desirability = - (bins_remain_cap - item)
    # For bins where item does not fit: desirability = -infinity (or a very small number)

    # Create an array to store desirability scores
    desirability_scores = np.full_like(bins_remain_cap, -np.inf)

    # Find indices where the item can fit
    fit_indices = bins_remain_cap >= item

    # Calculate desirability for bins where the item fits
    if np.any(fit_indices):
        desirability_scores[fit_indices] = -(bins_remain_cap[fit_indices] - item)

    # Apply softmax to convert desirability scores into probabilities (priorities)
    # Ensure we don't have all -inf, which would cause issues with exp
    if np.all(desirability_scores == -np.inf):
        # If no bin can fit the item, return uniform zero priorities (or handle as error)
        # In an online setting, this implies no solution is possible with current bins.
        # For priority assignment, returning zeros is a reasonable fallback if no fit.
        return np.zeros_like(bins_remain_cap)
    else:
        # The softmax function: exp(x) / sum(exp(x))
        # The denominator ensures the priorities sum to 1.
        # A small positive `temperature` parameter can be used to control the "sharpness"
        # of the probability distribution. A temperature of 1 means standard softmax.
        # Lower temperature means sharper distribution (more focus on best options).
        # Higher temperature means flatter distribution (more exploration).
        # For this problem, we aim to fill bins efficiently, so a moderate temperature is good.
        # Let's use a temperature of 1 for now, which means directly using the exp of scores.
        # The actual 'priority' is the normalized exponential of the desirability.
        priorities = np.exp(desirability_scores)
        sum_priorities = np.sum(priorities)

        # Avoid division by zero if all priorities are effectively zero (e.g., due to -inf)
        if sum_priorities == 0:
            return np.zeros_like(bins_remain_cap)
        else:
            return priorities / sum_priorities

```
