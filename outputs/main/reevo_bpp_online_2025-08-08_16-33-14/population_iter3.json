[
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Inverse Distance strategy.\n\n    This strategy prioritizes bins that are a close fit to the item's size.\n    It calculates a priority score based on the inverse of (1 + difference) for bins\n    where remaining_capacity is greater than or equal to the item size.\n    The score is designed to be less sensitive to extreme differences compared to v0,\n    and avoids potential division by zero issues more robustly than v1 by adding 1 to the denominator\n    and ensuring the difference is non-negative.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Find bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins that can fit, calculate the difference between remaining capacity and item size.\n    # We are interested in how \"tight\" the fit is.\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    differences = fitting_bins_remain_cap - item\n    \n    # Calculate priority using inverse of (1 + difference).\n    # This ensures that bins with smaller differences (better fits) get higher priorities.\n    # Adding 1 to the denominator prevents division by zero when the item perfectly fills a bin.\n    # The addition of 1 also acts as a small baseline priority for all fitting bins.\n    priorities[can_fit_mask] = 1.0 / (1.0 + differences)\n    \n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response1.txt_stdout.txt",
    "code_path": "problem_iter2_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit-like strategy.\n\n    This strategy prioritizes bins that can accommodate the item and, after placing the item,\n    leave the smallest possible remaining capacity. This is a direct implementation of\n    favoring tighter fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher score indicates a higher priority.\n    \"\"\"\n    # Initialize priorities to a very low value (negative infinity) for bins that cannot fit.\n    # This ensures they are never selected if any bin can fit the item.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can fit the item\n    possible_fits_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity *after* placing the item.\n    # We want to prioritize bins where this value is smallest (tightest fit).\n    # A simple way to assign higher priority to smaller values is to use their reciprocal.\n    # To avoid division by zero or extremely large numbers when the remainder is close to zero,\n    # we add a small epsilon.\n    epsilon = 1e-9\n    remaining_capacities_after_fit = bins_remain_cap[possible_fits_mask] - item\n\n    # The priority score is inversely proportional to the remaining capacity.\n    # A smaller remaining capacity results in a larger priority score.\n    priorities[possible_fits_mask] = 1.0 / (remaining_capacities_after_fit + epsilon)\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response2.txt_stdout.txt",
    "code_path": "problem_iter2_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit heuristic approach.\n\n    This strategy prioritizes bins that can accommodate the item and, after placing the item,\n    leave the minimal possible remaining capacity in the bin. This aims to achieve a \"tight fit\".\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate a more preferred bin.\n    \"\"\"\n    # Initialize priorities to zero.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that have enough remaining capacity to fit the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros. The caller will handle creating a new bin.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the remaining capacity *after* placing the item in the bins that can fit it.\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # To prioritize bins with minimal leftover capacity (best fit), we want to assign\n    # higher scores to bins where `remaining_after_placement` is smaller.\n    # A common way to achieve this is to use the inverse of the remaining capacity.\n    # A small epsilon is added to avoid division by zero if an item perfectly fills a bin.\n    epsilon = 1e-9\n    priorities[can_fit_mask] = 1.0 / (remaining_after_placement + epsilon)\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit-like strategy.\n\n    This heuristic prioritizes bins that can fit the item and, among those,\n    prioritizes bins that will have the least remaining capacity *after*\n    placing the item. This aims to pack items tightly, minimizing wasted space.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity after placement.\n    # We want to prioritize bins where placing the item leaves the LEAST remaining capacity.\n    # This means we want to MINIMIZE `bins_remain_cap[i] - item`.\n    # To translate minimization into a maximization of priority, we can use the reciprocal\n    # of the remaining capacity after placement (plus a small epsilon to avoid division by zero).\n    # Smaller remaining capacity after placement -> larger reciprocal -> higher priority.\n    \n    # Calculate remaining capacity for bins that can fit\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # Assign priority: higher priority for smaller remaining capacity after placement.\n    # Add a small epsilon to avoid division by zero and ensure positive values for the reciprocal.\n    priorities[can_fit_mask] = 1.0 / (remaining_after_placement + 1e-9)\n\n    return priorities",
    "response_id": 3,
    "obj": 4.198244914240141,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response4.txt_stdout.txt",
    "code_path": "problem_iter2_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a hybrid strategy.\n\n    This strategy combines the \"Best Fit\" approach (minimizing remaining capacity)\n    with a stochastic element to encourage exploration of less obviously optimal bins.\n    It also prioritizes bins that are \"almost full\" to potentially group smaller items\n    more efficiently.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Determine which bins can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit, return all zeros\n\n    fitting_bins_caps = bins_remain_cap[can_fit_mask]\n\n    # Strategy 1: Best Fit - prioritize bins that will have the least remaining capacity\n    # This is a common greedy strategy for BPP.\n    remaining_after_fit = fitting_bins_caps - item\n    # We want to minimize remaining_after_fit, so higher priority for smaller remaining capacity.\n    # Add a small constant to avoid division by zero if remaining_after_fit is 0.\n    best_fit_scores = 1.0 / (1.0 + remaining_after_fit)\n\n    # Strategy 2: \"Almost Full\" bins - prioritize bins that are very close to being full\n    # This can help consolidate smaller items and leave larger capacities open.\n    # We define \"almost full\" as having a remaining capacity between 0 and a small threshold.\n    almost_full_threshold = 0.1 * np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 0.1\n    almost_full_mask_subset = (fitting_bins_caps > 0) & (fitting_bins_caps <= almost_full_threshold)\n    almost_full_scores = np.zeros_like(fitting_bins_caps)\n    almost_full_scores[almost_full_mask_subset] = 0.5 # Assign a moderate priority\n\n    # Strategy 3: Exploration - add a small random component to encourage trying different bins\n    # This is inspired by exploration in reinforcement learning.\n    exploration_factor = 0.1\n    random_scores = np.random.rand(len(fitting_bins_caps)) * exploration_factor\n\n    # Combine scores. A bin is good if it's a good best-fit OR it's almost full.\n    # We use a weighted sum, prioritizing best fit more.\n    combined_scores = (0.7 * best_fit_scores) + (0.3 * almost_full_scores) + random_scores\n\n    # Assign the calculated scores to the corresponding bins\n    priorities[can_fit_mask] = combined_scores\n\n    # Ensure bins that cannot fit have a priority of 0\n    priorities[~can_fit_mask] = 0\n\n    return priorities",
    "response_id": 4,
    "obj": 3.9289988033506273,
    "SLOC": 19.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response5.txt_stdout.txt",
    "code_path": "problem_iter2_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit Decreasing-like strategy.\n\n    This heuristic prioritizes bins that can fit the item and, among those,\n    prioritizes bins that will have the least remaining capacity *after*\n    placing the item. This aims to pack items tightly and minimize wasted space.\n\n    The priority is calculated as: 1 / (1 + remaining_capacity_after_placement).\n    This formula gives higher priority to bins where the remaining capacity\n    after placing the item is smaller.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity for bins that can fit the item\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # To prioritize bins with minimal remaining capacity after placement,\n    # we use the inverse of (1 + remaining_capacity_after_placement).\n    # Adding 1 ensures the denominator is always at least 1, and the inverse\n    # means smaller remaining capacities get higher priorities.\n    # Using 1e-9 as a small epsilon to avoid potential issues with very small remaining capacities,\n    # though with the '+ 1.0' this might be less critical but still good practice for numerical stability.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[can_fit_mask] = 1.0 / (1.0 + remaining_after_placement + 1e-9)\n\n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response6.txt_stdout.txt",
    "code_path": "problem_iter2_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a hybrid strategy.\n\n    This strategy prioritizes bins that can fit the item. Among those that can fit,\n    it favors bins that are closer to being full (i.e., have less remaining capacity\n    after fitting the item). This is a greedy approach that tries to leave larger\n    gaps in other bins for potentially larger future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize scores. Bins that cannot fit will have a very low score.\n    # We use a large negative number to ensure they are ranked last.\n    scores = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate a score.\n    # We want to prioritize bins that have less remaining capacity after the item is placed.\n    # The \"remaining capacity after fit\" is (bins_remain_cap - item).\n    # To prioritize smaller remaining capacities, we can use the negative of this value.\n    # Alternatively, to use a \"goodness\" score where higher is better, we can use\n    # the inverse of the remaining capacity *after* the item is placed, plus a small\n    # epsilon to avoid division by zero and to ensure that bins that become exactly full\n    # get a high score.\n    # A simple approach is to use `bins_remain_cap - item`, and we want to minimize this value.\n    # To convert this into a \"priority\" where higher is better, we can use its negative.\n    # A common heuristic for \"best fit\" is to find the bin with the minimum `bins_remain_cap - item`.\n    # So, a higher priority should be given to smaller values of `bins_remain_cap - item`.\n    # Let's assign a score that is the negative of the remaining capacity *after* fitting.\n    # This means bins that have `bins_remain_cap - item == 0` will have a score of 0.\n    # Bins that have `bins_remain_cap - item == X` will have a score of -X.\n    # This naturally prioritizes exact fits (score 0) over bins with remaining space.\n    scores[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # To make these scores more \"softmax-friendly\" or generally interpretable as positive\n    # priorities, we can shift them so that the minimum possible score (if we consider\n    # all possible remaining capacities) maps to a small positive value.\n    # The smallest `bins_remain_cap - item` could be 0. The largest would depend on\n    # the initial bin capacities and item sizes.\n    # A simpler approach that directly reflects the \"best fit\" idea is to assign a\n    # score that is higher for bins closer to fitting the item.\n    # Let's re-evaluate: we want to prioritize bins where `bins_remain_cap - item` is small.\n    # The smaller `bins_remain_cap - item` is, the higher the priority.\n    # So, a simple monotonic transformation that maps smaller `bins_remain_cap - item`\n    # to larger priority values is desired.\n    # Using `1.0 / (bins_remain_cap[can_fit_mask] - item + epsilon)` achieves this,\n    # as seen in `priority_v1`. This also provides a \"soft\" prioritization.\n\n    # Let's refine the reflection: \"Prioritize fits, then closeness to full.\"\n    # \"Closeness to full\" means minimizing the *remaining* capacity.\n    # For a bin `b` with remaining capacity `c_b`, and an item `i` of size `s_i`:\n    # If `c_b >= s_i`, the bin can fit.\n    # The \"closeness to full\" after fitting is `c_b - s_i`. We want to minimize this.\n    # So, priority should be higher when `c_b - s_i` is smaller.\n\n    # Let's try a simple assignment:\n    # For bins that can fit, score = -(bins_remain_cap - item)\n    # This means:\n    # - Exact fit (remaining capacity after fit = 0) gets score 0.\n    # - Bin with remaining capacity after fit = 1 gets score -1.\n    # - Bin with remaining capacity after fit = 10 gets score -10.\n    # This correctly prioritizes exact fits (0 > -1 > -10).\n\n    scores[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # If we want to ensure that even non-fitting bins have a chance to be selected\n    # (though with very low priority), we could assign a small negative value to them.\n    # However, for typical online bin packing, if an item cannot fit anywhere,\n    # a new bin must be opened. So, prioritizing only fitting bins is correct.\n    # The current `scores` array has `-np.inf` for non-fitting bins.\n\n    # The problem asks for a priority *score* for each bin.\n    # The previous \"Better code\" used softmax to convert scores into probabilities.\n    # The reflection suggests prioritizing fits, then closeness to full.\n    # A simple way to achieve this without softmax (if softmax is not strictly required by the prompt)\n    # is to directly return scores that reflect this preference.\n\n    # Let's consider a scenario:\n    # item = 5\n    # bins_remain_cap = [10, 6, 5, 12]\n    #\n    # Bin 0: can fit, remaining after fit = 10 - 5 = 5. Score = -5\n    # Bin 1: can fit, remaining after fit = 6 - 5 = 1. Score = -1\n    # Bin 2: can fit, remaining after fit = 5 - 5 = 0. Score = 0\n    # Bin 3: can fit, remaining after fit = 12 - 5 = 7. Score = -7\n    #\n    # Scores: [-5, -1, 0, -7]\n    # The highest score (0) is for bin 2 (exact fit).\n    # The next highest (-1) is for bin 1 (closest to full after fit).\n    # This aligns with the reflection.\n\n    # The question is what the output *should* be. If it's a direct priority score\n    # that will be used in a `max()` function to select the bin, then these negative\n    # scores work. If it's for a probabilistic selection (like softmax), then\n    # `priority_v1` is more appropriate. The prompt says \"the bin with the highest priority score will be selected\".\n    # This implies a deterministic selection based on the highest score.\n\n    # Therefore, the `scores` computed above directly serve as priority scores.\n    # No need for softmax if the selection is deterministic.\n\n    # Let's ensure that the priority scores are easily distinguishable and\n    # that non-fitting bins are clearly lower.\n    # Using `-np.inf` for non-fitting bins is good.\n    # For fitting bins, `-(bins_remain_cap - item)` works.\n\n    # Consider numerical stability and range of values.\n    # If `bins_remain_cap - item` can be very large, the negative scores will be very small (large negative).\n    # This is fine for direct comparison.\n\n    # Let's make it slightly more robust by ensuring non-fitting bins get a clearly worse score\n    # than any possible fitting bin score.\n    # The minimum possible value for `bins_remain_cap - item` is 0. So the maximum score is 0.\n    # Any score less than 0 is worse. So `-np.inf` for non-fitting bins is appropriate.\n\n    # The current implementation `scores[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)`\n    # directly implements the \"prioritize fits, then closeness to full\" by assigning\n    # higher scores to bins with smaller remaining capacity after fitting the item.\n\n    # Let's consider the case where the item is larger than any bin's capacity.\n    # `can_fit_mask` would be all `False`. `scores` would remain all `-np.inf`.\n    # This correctly indicates no valid placement.\n\n    # Final check of the strategy:\n    # 1. Prioritize bins that *can* fit the item. (Handled by `can_fit_mask` and initial `-np.inf`)\n    # 2. Among those that can fit, prioritize bins that are \"closer to full\".\n    #    \"Closer to full\" after placing the item means `bins_remain_cap - item` is minimized.\n    #    So, we want to assign higher priority when `bins_remain_cap - item` is small.\n    #    Our score `-(bins_remain_cap - item)` does exactly this:\n    #    - If `bins_remain_cap - item = 0` (exact fit), score is 0 (highest).\n    #    - If `bins_remain_cap - item = 1`, score is -1.\n    #    - If `bins_remain_cap - item = 5`, score is -5.\n    #    This order `0 > -1 > -5` matches the desired priority.\n\n    # This looks like a solid implementation of the described strategy.\n\n    return scores",
    "response_id": 6,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response7.txt_stdout.txt",
    "code_path": "problem_iter2_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Sigmoid Fit Score.\n\n    This version prioritizes bins where the remaining capacity is just enough to fit the item,\n    aiming to minimize wasted space. It uses a sigmoid function to create a peak preference\n    when the difference between bin capacity and item size is close to zero.\n\n    The function is designed to return higher scores for bins with less remaining capacity\n    that can still accommodate the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate a more preferred bin.\n    \"\"\"\n    # Initialize priorities to a very low value for bins that cannot fit the item.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities # No bin can fit the item\n\n    # Calculate the \"slack\" or wasted space for bins that can fit the item.\n    # slack = remaining_capacity - item_size\n    slack = bins_remain_cap[can_fit_mask] - item\n\n    # We want to maximize the score when slack is minimized (ideally 0).\n    # A common way to achieve a peak around 0 is using a Gaussian-like function\n    # or a transformed sigmoid.\n    #\n    # Let's use a sigmoid on the negative slack, scaled by a factor `k` to control steepness.\n    # The sigmoid function is `1 / (1 + exp(-x))`.\n    # We want high values when `slack` is small.\n    # So, consider `score = sigmoid(k * (-slack))`.\n    #   - If `slack` is 0: `sigmoid(0)` is 0.5.\n    #   - If `slack` is small positive: `sigmoid(small_neg)` is < 0.5.\n    #   - If `slack` is large positive: `sigmoid(large_neg)` is close to 0.\n    # This means we prioritize bins with *less* slack, but a slack of 0 isn't the highest score.\n    #\n    # Let's consider `score = sigmoid(k * (slack_limit - slack))`, where `slack_limit`\n    # is the ideal slack we're aiming for (e.g., 0).\n    # Or, more directly, let's create a score that peaks at `slack = 0`.\n    #\n    # A function like `exp(-k * slack**2)` peaks at slack=0. We can normalize this.\n    # However, sticking to the sigmoid family for continuity:\n    # The function `1 / (1 + exp(-k * x))` peaks at `x = 0`.\n    # We want slack to be close to 0.\n    # Consider `score = 1 / (1 + exp(k * slack))`.\n    #   - slack = 0: `1 / (1 + exp(0)) = 1 / (1 + 1) = 0.5`.\n    #   - slack > 0 (positive waste): `exp(k * slack)` > 1, so `1 / (1 + exp(k*slack))` < 0.5.\n    #   - slack < 0 (item doesn't fit, handled by -inf initialization).\n    # This function gives the highest score for a perfect fit (slack=0) and decreases as waste increases.\n    #\n    # A small modification to increase the peak value and steepness could be useful.\n    # We can scale the output or shift the input.\n    # Let's try to make the peak value closer to 1 and have it drop off faster.\n    #\n    # Let's use a parameter `sensitivity` to control how quickly the score drops off\n    # as slack increases. A higher sensitivity means a sharper drop.\n    #\n    # Proposed function: `score = exp(-sensitivity * slack)`\n    #   - slack = 0: `exp(0) = 1` (Highest score)\n    #   - slack > 0: `exp(-sensitivity * slack)` < 1, decreases as slack increases.\n    # This is simpler and directly targets minimizing slack. It avoids the [-inf, 1] range issue.\n    # The problem statement implies a higher score is better.\n\n    # Let's re-evaluate the sigmoid's role for \"good fit\".\n    # We want bins where `bins_remain_cap` is slightly larger than `item`.\n    # `fit_diff = bins_remain_cap - item`. We want `fit_diff` to be small and positive.\n    # The function `1 / (1 + exp(-k * (item - bins_remain_cap)))` or `1 / (1 + exp(k * (bins_remain_cap - item)))`\n    # has its peak where `bins_remain_cap - item` is minimal.\n\n    # Let's use a function that peaks at `slack = 0` and decreases for `slack > 0`.\n    # A simple exponential decay `exp(-k * slack)` works well.\n    # A slightly modified sigmoid might also work.\n    # Let's try `score = 1 / (1 + exp(k * slack))` as derived before, but we might want to\n    # scale it to be more pronounced.\n\n    # To make the \"best fit\" (slack=0) more distinct, we can:\n    # 1. Scale the slack: `k * slack`\n    # 2. Apply a function that peaks at 0. `exp(-k * slack)` is good.\n    # 3. Or use sigmoid with a shift: `sigmoid(k * (ideal_slack - slack))` where ideal_slack is 0.\n    #    `sigmoid(-k * slack)`:\n    #       - slack=0: sigmoid(0) = 0.5\n    #       - slack>0: sigmoid(-k * slack) < 0.5\n    #       - slack<0: sigmoid(-k * slack) > 0.5 (but these are already -inf)\n    #    This also prioritizes smaller slack.\n\n    # Let's try a combination: use sigmoid on the negative difference, scaled by a factor `k`\n    # for steepness, and maybe add an offset or scale to make the best fit have a higher score.\n    # Or, more simply, transform the slack into a score directly.\n\n    # Let's use the `exp(-k * slack)` approach for clarity and effectiveness.\n    # Higher `k` means the score drops faster as slack increases.\n    k_sensitivity = 2.0 # Controls how quickly preference drops with increasing slack\n\n    # Calculate scores: higher for smaller slack (closer to 0)\n    scores_for_fitting_bins = np.exp(-k_sensitivity * slack)\n\n    # Assign these calculated scores back to the appropriate positions in the priorities array\n    priorities[can_fit_mask] = scores_for_fitting_bins\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "SLOC": 10.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response8.txt_stdout.txt",
    "code_path": "problem_iter2_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit strategy.\n\n    The Best Fit strategy prioritizes bins that leave the minimum remaining capacity\n    after placing the item. This strategy aims to fill bins as much as possible,\n    potentially reducing the total number of bins used.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros. The calling function\n    # would typically handle creating a new bin in this scenario.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the remaining capacity after placing the item in suitable bins\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # The priority is the negative of the remaining capacity after placement.\n    # We use the negative because we want the *minimum* remaining capacity\n    # to have the highest priority (i.e., the most negative value).\n    # Higher priority values mean the bin is a better fit.\n    priorities[can_fit_mask] = -remaining_after_placement\n\n    return priorities",
    "response_id": 8,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority for packing an item into bins, prioritizing tight fits.\n\n    This heuristic prioritizes bins that can accommodate the item. Among those,\n    it assigns a higher priority to bins that will have the least remaining\n    capacity after packing (i.e., a tighter fit). Bins that are too small\n    for the item receive a priority of 0.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n        Higher scores indicate a more desirable bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that have enough remaining capacity for the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit the item, return all zeros.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # For bins that can fit the item, calculate the remaining capacity *after* packing.\n    # The goal is to minimize this remaining capacity for a tighter fit.\n    remaining_after_packing = bins_remain_cap[can_fit_mask] - item\n\n    # To prioritize tighter fits (smaller remaining_after_packing), we can use\n    # the negative of the remaining capacity. Since priority functions typically\n    # aim to maximize the score, we can transform this. A common strategy is to\n    # use a large value minus the remaining capacity, ensuring that smaller\n    # remaining capacities yield higher scores. Using the maximum available\n    # capacity as the large constant helps normalize scores relative to bin sizes.\n    # The `bins_remain_cap[can_fit_mask].max()` is used to ensure that even\n    # with the subtraction, the priority remains positive and is scaled by\n    # the general magnitude of bin capacities.\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask].max() - remaining_after_packing\n\n    # Add a strong preference for bins that result in exactly zero remaining capacity\n    # (perfect fits). This can be achieved by adding a very large constant to these.\n    exact_fit_mask = (remaining_after_packing == 0)\n    if np.any(exact_fit_mask):\n        # A large epsilon to ensure exact fits are always preferred over near-fits.\n        priorities[can_fit_mask][exact_fit_mask] += 1e9\n\n    return priorities",
    "response_id": 9,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response0.txt_stdout.txt",
    "code_path": "problem_iter3_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a hybrid strategy.\n\n    This strategy prioritizes bins that offer a tight fit (minimizing leftover capacity)\n    while also considering bins that might accommodate future items better by leaving\n    a moderate amount of space. It uses a soft-max like approach to create relative\n    priorities and a small penalty for bins that would be *too* full after packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Determine which bins can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit, return all zeros\n\n    fitting_bins_caps = bins_remain_cap[can_fit_mask]\n\n    # Calculate potential remaining capacities after fitting the item\n    remaining_after_fit = fitting_bins_caps - item\n\n    # Strategy: Prioritize bins that result in a remaining capacity that is \"just right\".\n    # This means not too much space left (avoiding \"First Fit\" tendencies for large remaining capacity)\n    # and not too little space left (avoiding \"Worst Fit\" where there's barely any space).\n    # We can model this using a Gaussian-like function centered around a \"good\" remaining capacity.\n    # A good remaining capacity could be a small fraction of the bin's original capacity,\n    # or a small fixed amount. Let's consider a target remaining capacity.\n    # For simplicity, let's aim for remaining capacity that is small but not zero.\n    # A simple approach: maximize priority when remaining_after_fit is small, but\n    # penalize very small remaining capacities.\n\n    # Option 1: Favor small remaining capacity, but penalize zero or very small remaining.\n    # A log-like function can achieve this: log(1 + x) increases with x, but at a decreasing rate.\n    # We want to minimize remaining_after_fit, so we take the inverse.\n    # To avoid division by zero for remaining_after_fit == 0, we can use 1 + remaining_after_fit.\n    # A small positive remaining capacity is ideal (tightest fit without being *too* tight).\n    # We can penalize bins where remaining_after_fit is very small to avoid \"Worst Fit\" issues.\n    # Let's define a score that is high for small remaining_after_fit, but drops off as it approaches zero.\n    # We can use 1 / (1 + remaining_after_fit) for the \"best fit\" aspect.\n    # To penalize being *too* full (e.g., remaining_after_fit < 1e-6), we can introduce a penalty.\n\n    # Score that favors small remaining capacity. Add a small epsilon to avoid division by zero.\n    base_scores = 1.0 / (1.0 + remaining_after_fit + 1e-6)\n\n    # Penalty for being too full (very small remaining capacity)\n    # This encourages leaving a small but non-zero gap.\n    too_full_penalty = np.zeros_like(fitting_bins_caps)\n    too_full_threshold = 0.05 # e.g., 5% of item size or a small absolute value\n    too_full_mask = remaining_after_fit < too_full_threshold\n    too_full_penalty[too_full_mask] = (too_full_threshold - remaining_after_fit[too_full_mask]) / too_full_threshold\n\n    # Combine scores: high priority for tight fits, with a penalty for being *too* tight.\n    # We want to subtract the penalty from the base score.\n    combined_scores = base_scores - (0.5 * too_full_penalty)\n\n    # A small diversification factor inspired by softmax to create relative differences\n    # Softmax ensures that scores are positive and sum to 1 (or can be scaled to do so).\n    # Using it here can help break ties and introduce some probabilistic selection.\n    # Adding a small constant before softmax to avoid issues with all scores being equal.\n    diversification_scores = np.exp(combined_scores + np.random.normal(0, 0.1, len(combined_scores)))\n    # Normalize to create relative probabilities, could be used directly as priorities\n    normalized_diversification = diversification_scores / np.sum(diversification_scores)\n    # Or, use combined_scores directly and normalize if needed, but raw scores are often fine for priority.\n\n    # Let's assign the combined_scores directly as priorities, possibly scaled.\n    # Scale scores to a reasonable range, e.g., 0-1.\n    # Ensure no negative priorities if penalty was too strong.\n    final_priorities = np.maximum(0, combined_scores)\n\n    # Assign the calculated scores to the corresponding bins\n    priorities[can_fit_mask] = final_priorities\n\n    # Ensure bins that cannot fit have a priority of 0\n    priorities[~can_fit_mask] = 0\n\n    return priorities",
    "response_id": 0,
    "obj": 4.427602712405275,
    "SLOC": 20.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response1.txt_stdout.txt",
    "code_path": "problem_iter3_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a hybrid strategy.\n\n    This strategy prioritizes bins that offer a tight fit (minimizing leftover capacity)\n    while also considering bins that leave sufficient space for future, potentially larger items.\n    It aims to balance exploiting near-perfect fits with exploring options that maintain\n    flexibility. A small bias is introduced to favor bins that are less utilized,\n    thereby encouraging a more even distribution of items and avoiding premature\n    filling of a few bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the current item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    fitting_bins_caps = bins_remain_cap[can_fit_mask]\n\n    # Strategy 1: Prioritize tight fits (Best Fit)\n    # Calculate remaining capacity after placing the item. We want to minimize this.\n    remaining_after_fit = fitting_bins_caps - item\n    # Invert the remaining capacity and add 1 to avoid division by zero and to\n    # ensure higher scores for smaller remaining capacities.\n    best_fit_scores = 1.0 / (1.0 + remaining_after_fit)\n\n    # Strategy 2: Favor bins that leave more space (Worst Fit tendency for future)\n    # This aims to keep bins with larger remaining capacities open for larger items.\n    # We want to prioritize bins with higher remaining capacity among those that can fit the item.\n    # Adding a small epsilon to avoid division by zero if fitting_bins_caps is 0 (though unlikely here due to can_fit_mask).\n    # We scale this by a factor to balance it with best_fit_scores.\n    # Using a log scale can help compress large differences and prevent extreme values.\n    # Using max capacity as a reference point for scaling.\n    max_cap_overall = np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 1.0\n    space_for_future_scores = np.log1p(fitting_bins_caps / max_cap_overall) * 0.5 # Log scale, scaled down\n\n    # Strategy 3: Introduce a slight bias towards less utilized bins to encourage diversity\n    # Calculate the proportion of capacity used in bins that can fit the item.\n    # Higher proportion means more utilized. We want to slightly penalize these.\n    # Or, equivalently, reward bins with more remaining capacity (less utilized).\n    # Let's rethink: We want to favor bins that are NOT nearly full, to leave space.\n    # So, we want to prioritize bins that have *more* remaining capacity *among those that can fit*.\n    # This is somewhat captured by space_for_future_scores.\n    # A simpler approach: penalize bins that are *very* full.\n    # Let's use a term that increases as remaining_after_fit gets closer to 0.\n    # Instead, let's favor bins that are less full to encourage distribution.\n    # So, we want to boost priority for bins with larger remaining capacity.\n    # This is already handled by space_for_future_scores.\n\n    # Let's combine Best Fit with a preference for bins that leave *reasonable* space,\n    # but not excessively large gaps that might be wasted.\n    # We can use a Gaussian-like function centered slightly away from zero remaining capacity.\n    # Or simply, combine Best Fit with a factor that prefers bins with more room, but not excessively so.\n\n    # Let's refine the combination:\n    # Primary: Best Fit (minimizing remaining space)\n    # Secondary: Favor bins that leave a moderate amount of space.\n    # Calculate a score that peaks when remaining_after_fit is moderate (e.g., item_size).\n    # This means the bin capacity was ~2*item_size.\n    # We want to encourage fits where remaining_after_fit is not too small (handled by BF)\n    # and not too large.\n\n    # A simpler approach for diversity/exploration without explicit randomness:\n    # Prioritize bins that are not *too* close to full, but also not *too* empty.\n    # Let's try a score that favors bins with remaining capacity between X and Y.\n    # This can be complex.\n\n    # Let's stick to a combination of tight fits and a slight preference for\n    # bins that still have a good amount of capacity left for future items.\n    # Combine Best Fit with a scaled version of remaining capacity.\n    # Higher remaining capacity = higher score (but this can contradict best fit).\n\n    # Let's try a weighted sum favoring best fit, but giving a boost to bins\n    # that are not too full and not too empty.\n    # Consider the 'slack' or 'unfilled' capacity in a fitting bin.\n    # We want to minimize (fitting_bins_caps - item) but also not\n    # pick bins that are almost empty if a tighter fit exists.\n\n    # Revised combination:\n    # - Prioritize tightest fits (minimize `remaining_after_fit`).\n    # - Add a slight bonus for bins that are not *nearly* full, to leave more room.\n    # This means for `remaining_after_fit`, smaller is better, but we might\n    # slightly penalize the absolute minimum if it's extremely close to zero.\n\n    # Let's use a score that is high for small `remaining_after_fit`,\n    # but perhaps has a slight decay if `remaining_after_fit` is very, very small.\n    # Or, a score that rewards `remaining_after_fit` up to a certain point, then decays.\n    # A Gaussian-like function centered at a moderate `remaining_after_fit` could work.\n    # e.g., exp(-(remaining_after_fit - target_slack)^2 / sigma^2)\n\n    # Let's try a simpler hybrid:\n    # Score = (Best Fit Score) + (Bonus for leaving reasonable space)\n    # Best Fit Score: 1 / (1 + remaining_after_fit)\n    # Bonus for leaving space: A function that increases with `fitting_bins_caps` up to a point.\n    # Let's try `np.tanh(fitting_bins_caps / max_cap_overall)` as a bonus.\n    # This would boost bins that are less full.\n\n    # Combine Best Fit with a preference for bins with more remaining capacity\n    # among those that can fit the item. This is to preserve larger bins for larger items.\n    # A simple linear scaling of remaining capacity (normalized).\n    # Higher remaining capacity should get a higher score here.\n    # Let's use remaining_after_fit scaled.\n\n    # Combined score:\n    # We want to maximize tight fits AND maximize remaining capacity among fitting bins.\n    # This is a multi-objective problem. A common approach is a weighted sum.\n    # We can define two components and sum them.\n\n    # Component 1: Tightness (Higher is better for smaller remaining_after_fit)\n    tightness_score = 1.0 / (1.0 + remaining_after_fit + 1e-9) # Add epsilon for stability\n\n    # Component 2: Space Preservation (Higher is better for larger remaining_after_fit)\n    # This is counter to tightness. Let's reframe.\n    # We want to *minimize* remaining_after_fit.\n    # Let's consider the 'quality' of the fit. A perfect fit (remaining_after_fit = 0) is good.\n    # A fit that leaves a lot of space might be good for future items.\n\n    # Let's prioritize bins that are almost full, as this is a common greedy heuristic.\n    # If a bin is almost full, placing an item there might be efficient.\n    # However, the reflection asks to balance this with leaving room.\n\n    # Let's try a score that is high when remaining_after_fit is small (best fit),\n    # but also adds a term that prefers bins that are not completely empty after the fit.\n    # This encourages packing items into bins that already have some content.\n\n    # Consider `remaining_after_fit`. We want to be small.\n    # Let's create a score that rewards small values of `remaining_after_fit`,\n    # but perhaps with a slight bonus for the *second* best fit if the best fit is *too* tight\n    # (i.e., leaves almost no room). This encourages diversity.\n\n    # Revised Strategy:\n    # 1. Primary goal: Minimize `remaining_after_fit` (Best Fit).\n    # 2. Secondary goal: If there are multiple bins with very small `remaining_after_fit`,\n    #    prefer the one that leaves a bit more space to avoid \"over-packing\".\n    #    This can be achieved by slightly penalizing `remaining_after_fit` values close to zero.\n\n    # Let's create a score that peaks when `remaining_after_fit` is close to zero but not exactly zero.\n    # This is tricky. A simple inverse is fine for best fit.\n\n    # Let's combine Best Fit with a penalty for leaving too much room.\n    # `best_fit_scores` already prioritizes minimal remaining capacity.\n    # If `fitting_bins_caps` are large, `remaining_after_fit` will also be large.\n    # `best_fit_scores` will be small for these. So it naturally penalizes leaving too much room.\n\n    # How to balance \"tight fits\" with \"leaving room for future items\"?\n    # If we have item sizes {0.6, 0.6, 0.2, 0.2, 0.2} and bin capacity 1.0.\n    # Item 0.6: Fit into bin 1. Remaining cap: [0.4].\n    # Item 0.6: Fit into bin 2. Remaining cap: [0.4, 0.4].\n    # Item 0.2:\n    #   Bin 1: remaining_after_fit = 0.4 - 0.2 = 0.2. Best fit score = 1/(1+0.2) = 0.833\n    #   Bin 2: remaining_after_fit = 0.4 - 0.2 = 0.2. Best fit score = 1/(1+0.2) = 0.833\n    #   If there's another bin with capacity 1.0 (Bin 3): remaining_after_fit = 1.0 - 0.2 = 0.8. Best fit score = 1/(1+0.8) = 0.555\n    # The current `best_fit_scores` would favor Bin 1 or 2.\n\n    # Let's try to add a component that favors bins that are not nearly empty *after* fitting.\n    # This means we want `remaining_after_fit` to be not too large.\n    # So, we want to minimize `remaining_after_fit` and also minimize `remaining_after_fit` again?\n    # This implies a preference for the absolute best fit.\n\n    # The reflection suggests:\n    # - Prioritize tight fits by minimizing leftover capacity. (This is Best Fit).\n    # - Explore diverse bin selection strategies.\n    # - Balance exploiting near-perfect fits with exploring options that leave room for future items.\n\n    # To balance, we can't just do pure Best Fit.\n    # Let's create a score that is sensitive to the *magnitude* of the remaining capacity.\n    # A function that is high for `remaining_after_fit` near 0, but also high for\n    # `remaining_after_fit` that is still significant (but not excessive).\n\n    # Let's try a score based on the ratio of remaining capacity to item size.\n    # `fitting_bins_caps / item`.\n    # A ratio close to 1 means `fitting_bins_caps` is just slightly larger than `item`.\n    # We want `fitting_bins_caps` to be just slightly larger than `item`.\n    # `fitting_bins_caps - item` should be small.\n\n    # Consider the 'gap' relative to the bin capacity.\n    # `remaining_after_fit / fitting_bins_caps`. We want this to be small.\n    # So, `1.0 / (1.0 + remaining_after_fit / fitting_bins_caps)` as a score.\n    # This is essentially `fitting_bins_caps / (fitting_bins_caps + fitting_bins_caps - item)`\n    # = `fitting_bins_caps / (2 * fitting_bins_caps - item)`.\n\n    # Let's combine Best Fit with a factor that favors bins with moderate remaining space.\n    # `best_fit_scores = 1.0 / (1.0 + remaining_after_fit)`\n    # We want to add a bonus if `remaining_after_fit` is not too small AND not too large.\n    # A Gaussian-like function could work.\n    # Let target_slack be `item * 0.5` (a moderate slack).\n    # Slack score = exp(-(remaining_after_fit - target_slack)**2 / sigma**2)\n    # This might be too complex.\n\n    # Simpler approach:\n    # Combine Best Fit with a term that boosts bins with larger remaining capacity.\n    # This encourages leaving bins less full.\n    # But \"leaving room for future items\" can also mean packing smaller items into\n    # bins that already have some capacity, rather than opening a new bin.\n\n    # Let's try a weighted sum:\n    # `priority = w1 * best_fit_score + w2 * preference_for_more_space`\n    # `best_fit_score`: `1.0 / (1.0 + remaining_after_fit)` (higher is better)\n    # `preference_for_more_space`: `fitting_bins_caps` (higher is better)\n    # We need to normalize `fitting_bins_caps`.\n    # Let's use `fitting_bins_caps / max_cap_overall`.\n\n    # Let's weight Best Fit more heavily, and add a scaled remaining capacity component.\n    # The goal is to be greedy (best fit) but also to keep options open.\n    # A bin that is 70% full and can fit the item, leaving it 85% full, might be better\n    # than a bin that is 10% full and can fit the item, leaving it 30% full.\n    # This depends on the item size.\n\n    # Let's focus on minimizing the *waste* created by the current item placement.\n    # Waste = `remaining_after_fit`. We want to minimize this.\n    # But \"leaving room for future items\" suggests that sometimes a slightly larger `remaining_after_fit`\n    # might be preferable if it's a more balanced use of space.\n\n    # Consider the state of the bin itself: `fitting_bins_caps`.\n    # A bin that is `0.8` capacity and fits `0.2` item (leaves `0.6`) is different from\n    # a bin that is `0.3` capacity and fits `0.2` item (leaves `0.1`).\n    # The first case leaves more absolute space.\n\n    # Let's try a score that combines the inverse of remaining capacity (for tight fits)\n    # with the remaining capacity itself (to favor bins with more space).\n    # We can use a quadratic function to capture this preference for moderate slack.\n    # Score = -(remaining_after_fit - slack_target)^2\n    # This would peak at `slack_target`.\n\n    # Let's try a simpler weighted sum:\n    # Prioritize bins that result in minimum remaining capacity.\n    # Also, give a slight boost to bins that have more capacity to begin with (among fitting bins).\n    # This encourages spreading items.\n\n    # `best_fit_metric = -remaining_after_fit` (we want to maximize this, i.e., minimize remaining_after_fit)\n    # `space_metric = fitting_bins_caps` (we want to maximize this)\n\n    # Combine these: `score = weight_bf * (-remaining_after_fit) + weight_space * fitting_bins_caps`\n    # Need to normalize `fitting_bins_caps` and ensure the scale of `remaining_after_fit` is handled.\n    # Using `1.0 / (1.0 + remaining_after_fit)` handles the scale for BF.\n    # For space, `fitting_bins_caps / max_cap_overall` is a good normalized metric.\n\n    # Let's try:\n    # Score = (1 - alpha) * (1.0 / (1.0 + remaining_after_fit)) + alpha * (fitting_bins_caps / max_cap_overall)\n    # This would prioritize best fit, but with a tendency to favor bins that are less full.\n    # If `alpha` is small, it's mostly best fit. If `alpha` is large, it favors less full bins.\n\n    # Let's use `alpha = 0.3` to give a significant but secondary preference for leaving space.\n    alpha = 0.3\n    max_cap_overall = np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 1.0\n\n    # Best Fit component: Higher score for smaller `remaining_after_fit`.\n    best_fit_component = 1.0 / (1.0 + remaining_after_fit + 1e-9)\n\n    # Space preservation component: Higher score for larger `fitting_bins_caps`.\n    # Normalize by the maximum capacity to get a relative measure.\n    space_component = fitting_bins_caps / max_cap_overall\n\n    # Combine components with weights.\n    # The reflection asks to balance exploiting near-perfect fits with exploring options that leave room.\n    # This suggests a preference for moderate slack, not just minimum or maximum.\n    # The current combination prioritizes tight fits, and then favors bins that are less full overall.\n    # This might push items to less utilized bins even if a tighter fit exists elsewhere.\n\n    # Let's refine the 'leaving room' aspect.\n    # We want to avoid bins that become *too* empty after fitting.\n    # So, we want `remaining_after_fit` not to be too large.\n    # This means `fitting_bins_caps` should not be excessively larger than `item`.\n\n    # Let's try a score that is high when `remaining_after_fit` is small, but also\n    # penalizes cases where `fitting_bins_caps` is much larger than `item`.\n    # This means we want `fitting_bins_caps` to be slightly larger than `item`.\n\n    # Consider the ratio `fitting_bins_caps / item`.\n    # We want this ratio to be close to 1.\n    # A score like `1.0 / (1.0 + abs(fitting_bins_caps / item - 1.0))` might work.\n    # This favors fits where the bin capacity is just slightly larger than the item.\n    # This implicitly means `remaining_after_fit` is small.\n\n    # Let's go back to a simpler, effective heuristic:\n    # Prioritize bins that minimize remaining capacity.\n    # If there are ties, or near-ties, consider which bin has more residual capacity.\n    # This leads to a weighted combination of minimizing remaining capacity and maximizing remaining capacity.\n\n    # Let's try a metric that favors bins that are \"almost full\" but not completely.\n    # This is a form of \"first fit decreasing\" or \"best fit\" strategy.\n    # For online, we adapt.\n\n    # Let's try a score that is high for tight fits, and a secondary measure that\n    # favors bins that are not too empty.\n    # `best_fit_score`: `1 / (1 + remaining_after_fit)`\n    # `not_too_empty_score`: `tanh(fitting_bins_caps / max_cap_overall)` - this favors bins that were initially more full.\n\n    # How about: Maximize the utility of the bin.\n    # Utility can be related to how \"full\" the bin becomes.\n    # We want to reach a state where bins are filled efficiently.\n\n    # Let's try a compromise:\n    # Priority is mainly determined by Best Fit.\n    # A secondary factor that slightly boosts bins that are not nearly empty *after* the fit.\n    # This means `remaining_after_fit` shouldn't be extremely small, but also not too large.\n\n    # Consider `remaining_after_fit`.\n    # Best Fit: High score for small values.\n    # Leaving room: High score for large values.\n    # Balance: High score for moderate values.\n\n    # Let's try a simple scoring function that rewards tightness but doesn't completely ignore\n    # bins that leave more room.\n\n    # Score = `best_fit_score` + `slack_bonus`\n    # `best_fit_score`: `1.0 / (1.0 + remaining_after_fit + 1e-9)`\n    # `slack_bonus`: This should be higher for `remaining_after_fit` that is not too small,\n    #                but also not excessively large.\n    # Let's use `np.tanh(remaining_after_fit / max_cap_overall)` as a bonus.\n    # This bonus increases with remaining capacity.\n    # This combination would favor tight fits that also happen to have more initial capacity.\n\n    # The reflection emphasizes \"balancing exploiting near-perfect fits with exploring options that leave room\".\n    # This suggests that a slightly less perfect fit that leaves more room might be preferred over an extremely tight fit.\n\n    # Let's try a metric that is high when `remaining_after_fit` is small, but not extremely small.\n    # And also not extremely large.\n    # Example: item=0.5, bin_caps=[0.6, 0.8, 1.0]\n    #   Bin 1: rem_after_fit = 0.1. BF = 1/1.1 = 0.909\n    #   Bin 2: rem_after_fit = 0.3. BF = 1/1.3 = 0.769\n    #   Bin 3: rem_after_fit = 0.5. BF = 1/1.5 = 0.667\n    # Best fit picks Bin 1.\n    # If we want to leave more room, maybe Bin 2 is good.\n\n    # Let's try a score that is a weighted sum of the negative remaining capacity (to maximize)\n    # and the negative squared remaining capacity (to penalize larger remaining capacity).\n    # Score = `w1 * (-remaining_after_fit)` + `w2 * (-remaining_after_fit**2)`\n    # This would strongly favor small `remaining_after_fit`.\n\n    # Alternative: A penalty for both small and large remaining capacity.\n    # Minimize `(remaining_after_fit - target_slack)**2`.\n    # This means maximizing `-(remaining_after_fit - target_slack)**2`.\n    # This function peaks at `target_slack`.\n\n    # Let `target_slack` be `item * 0.2` (a small slack).\n    # score = np.exp(-(remaining_after_fit - item * 0.2)**2 / (item * 0.5)**2)\n    # This would favor fits leaving ~0.2 item size as remainder.\n\n    # Let's try a simpler approach based on the reflection:\n    # Prioritize tight fits (minimize `remaining_after_fit`).\n    # Also, consider bins that leave \"room for future items\".\n    # This could mean bins that still have a significant amount of capacity left.\n\n    # Let's use a score that is primarily best-fit, but with a boost for bins\n    # that have a moderate amount of remaining capacity *after* the item is placed.\n    # A bin that has 0.1 remaining is very tight. A bin with 0.5 remaining might be too much.\n    # A bin with 0.2-0.3 remaining might be a good compromise.\n\n    # Score = `best_fit_score` + `slack_preference`\n    # `best_fit_score = 1.0 / (1.0 + remaining_after_fit + 1e-9)`\n    # `slack_preference`: a function that is high for `remaining_after_fit` in a certain range.\n    # Let's use a simple linear increase for `slack_preference` but capped.\n    # Prefer bins with more remaining capacity (among fitting bins) but with diminishing returns.\n\n    # Let's combine `best_fit_score` with `fitting_bins_caps`.\n    # `combined_score = best_fit_score * (1 + 0.2 * (fitting_bins_caps / max_cap_overall))`\n    # This gives a boost to bins that were initially more full.\n\n    # Let's re-read the reflection: \"Balance exploiting near-perfect fits with exploring options that leave room for future items.\"\n    # This means we don't always pick the absolute best fit.\n    # We want a score that is high for `remaining_after_fit` close to 0, but also for `remaining_after_fit` that is moderately larger.\n\n    # Consider a score function `f(r)` where `r` is `remaining_after_fit`.\n    # `f(r)` should be high for small `r`, but also for moderate `r`.\n    # A possible function: `r` itself (favors large remaining) AND `1/(1+r)` (favors small remaining).\n    # Weighted sum: `w1 * (1/(1+r)) + w2 * r`.\n    # `w1` should be larger for tight fits. `w2` for leaving room.\n\n    # Let's try:\n    # Score = (1-beta) * (1 / (1 + remaining_after_fit)) + beta * (remaining_after_fit / max_cap_overall)\n    # Here, `remaining_after_fit` is scaled to be comparable.\n    # If `beta` is small, it's mostly best fit.\n    # If `beta` is large, it favors bins that leave more space.\n    # This still doesn't capture \"moderate slack\".\n\n    # Let's use a score that rewards tightness, and also rewards bins that were initially less full.\n    # The logic is: a tight fit into a bin that is already somewhat full is good.\n    # A tight fit into a very empty bin is also okay.\n    # The problem is picking between a tight fit and a looser fit that leaves more room.\n\n    # Let's try a weighted combination of two preferences:\n    # 1. Minimizing remaining capacity: `score1 = 1.0 / (1.0 + remaining_after_fit + 1e-9)`\n    # 2. Maximizing the remaining capacity *after* the fit, normalized: `score2 = fitting_bins_caps / max_cap_overall`\n    # The reflection suggests balancing these.\n    # `final_score = (1 - weight) * score1 + weight * score2`\n    # If `weight` is small, we lean towards best fit.\n    # If `weight` is large, we lean towards leaving more space.\n\n    # Let's consider `weight = 0.4`. This gives a significant preference for leaving more space.\n    weight = 0.4\n    score1 = 1.0 / (1.0 + remaining_after_fit + 1e-9)\n    score2 = fitting_bins_caps / max_cap_overall\n\n    # A potential issue: if `fitting_bins_caps` are very close to each other, `score2` differences will be small.\n    # If `remaining_after_fit` values are very close, `score1` differences will be small.\n\n    # Let's try to introduce a \"quality\" measure of the fit.\n    # Quality = `1.0 - remaining_after_fit / fitting_bins_caps` (proportion of bin used)\n    # We want this to be high.\n    # `quality_score = fitting_bins_caps - remaining_after_fit` -> which is `item`? No.\n    # `quality_score = item / fitting_bins_caps`\n\n    # Let's try a combination that favors tight fits, but penalizes fits that leave *very little* or *excessive* space.\n    # Score = `-(remaining_after_fit - slack_target)^2`\n    # Let `slack_target = item * 0.2` (a small slack).\n    # This function peaks when remaining_after_fit is `item * 0.2`.\n    # This favors fits where the bin was about `item + item*0.2 = 1.2 * item` capacity.\n\n    # Let's try this:\n    # Score = `1.0 / (1.0 + remaining_after_fit)`  (Best Fit)\n    # Add a term that slightly favors bins that are not \"too full\" after the fit.\n    # This means `remaining_after_fit` shouldn't be too small.\n    # But it also shouldn't be too large.\n\n    # Final attempt: Combine Best Fit with a preference for bins that leave a moderate amount of space.\n    # The overall goal is to reduce the number of bins.\n    # Tight fits are good for this.\n    # Leaving room is good if it means future items can be packed more efficiently.\n\n    # Let's simplify the \"leaving room\" aspect: prioritize bins that are not almost full.\n    # Among the fitting bins, we want to pick one that minimizes `remaining_after_fit`,\n    # but if there are multiple bins with similar `remaining_after_fit`, pick the one\n    # that was initially less full (i.e., had more `fitting_bins_caps`).\n\n    # `best_fit_score = 1.0 / (1.0 + remaining_after_fit + 1e-9)`\n    # `less_full_bonus = fitting_bins_caps / max_cap_overall`\n    # `combined_score = best_fit_score + 0.2 * less_full_bonus`\n    # This seems reasonable: prioritize tight fits, with a secondary preference for less full bins.\n    # The weight 0.2 can be tuned.\n\n    # Let's try this combination.\n    combined_scores_subset = (1.0 / (1.0 + remaining_after_fit + 1e-9)) + \\\n                             0.2 * (fitting_bins_caps / max_cap_overall)\n\n    # Assign the calculated scores to the corresponding bins\n    priorities[can_fit_mask] = combined_scores_subset\n\n    # Ensure bins that cannot fit have a priority of 0\n    priorities[~can_fit_mask] = 0\n\n    return priorities",
    "response_id": 1,
    "obj": 71.3402473075389,
    "SLOC": 24.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response2.txt_stdout.txt",
    "code_path": "problem_iter3_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements a hybrid priority function for online Bin Packing Problem.\n    This version refines the scoring by categorizing fits:\n    1. Perfect Fit (zero remaining capacity).\n    2. Tight Fit (remaining capacity <= item size).\n    3. Good Fit (remaining capacity > item size but not excessively large).\n    4. Loose Fit (remaining capacity is very large).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    fitting_bins_caps = bins_remain_cap[can_fit_mask]\n    remaining_after_fit = fitting_bins_caps - item\n\n    # Define score components based on remaining capacity after fit\n    # We want to prioritize smaller remaining_after_fit.\n    # Add a small epsilon to avoid division by zero for perfect fits.\n    epsilon = 1e-9\n\n    # Scores for different fit categories\n    perfect_fit_scores = np.zeros_like(remaining_after_fit)\n    tight_fit_scores = np.zeros_like(remaining_after_fit)\n    good_fit_scores = np.zeros_like(remaining_after_fit)\n    loose_fit_scores = np.zeros_like(remaining_after_fit)\n\n    # Thresholds for 'good fit' and 'loose fit'\n    # Threshold 1: Maximum remaining capacity for a 'tight' fit (relative to item size)\n    tight_threshold = item\n    # Threshold 2: Maximum remaining capacity for a 'good' fit (relative to item size)\n    # This aims to leave a decent amount of space, e.g., up to 3 times the item size.\n    good_threshold = 3.0 * item\n\n    # Assign scores based on categories\n    # Perfect Fit: highest priority\n    perfect_mask = (remaining_after_fit < epsilon)\n    perfect_fit_scores[perfect_mask] = 10.0\n\n    # Tight Fit: prioritize bins that leave little room but not zero\n    tight_mask = (remaining_after_fit > epsilon) & (remaining_after_fit <= tight_threshold)\n    tight_fit_scores[tight_mask] = 5.0 / (1.0 + remaining_after_fit[tight_mask]) # Scaled best-fit\n\n    # Good Fit: prioritize bins that leave a moderate amount of room\n    # This encourages leaving space for future items.\n    good_mask = (remaining_after_fit > tight_threshold) & (remaining_after_fit <= good_threshold)\n    # For good fits, we want remaining_after_fit to be as small as possible within this range.\n    # We can use a inverse relationship, but scaled differently.\n    good_fit_scores[good_mask] = 2.0 / (1.0 + remaining_after_fit[good_mask])\n\n    # Loose Fit: lowest priority among fitting bins, provide minimal boost for exploration\n    loose_mask = (remaining_after_fit > good_threshold)\n    loose_fit_scores[loose_mask] = 0.5 / (1.0 + remaining_after_fit[loose_mask])\n\n    # Combine scores. Weights are heuristic and can be tuned.\n    # Prioritize perfect > tight > good > loose.\n    combined_scores = (\n        perfect_fit_scores * 1.0 +\n        tight_fit_scores * 0.8 +\n        good_fit_scores * 0.5 +\n        loose_fit_scores * 0.2\n    )\n\n    # Add a small random component for exploration to all fitting bins\n    exploration_factor = 0.1\n    random_scores = np.random.rand(len(fitting_bins_caps)) * exploration_factor\n    combined_scores += random_scores\n\n    # Assign the calculated scores to the corresponding bins in the original priority array\n    priorities[can_fit_mask] = combined_scores\n\n    return priorities",
    "response_id": 2,
    "obj": 3.8891104906262464,
    "SLOC": 35.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response3.txt_stdout.txt",
    "code_path": "problem_iter3_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using an improved hybrid strategy.\n\n    This strategy enhances the \"Best Fit\" by considering the \"waste\" more directly.\n    It also introduces a \"Diversity\" factor to prefer bins that have more space\n    remaining after the item is placed, aiming to leave larger contiguous spaces\n    for potentially larger future items. A small stochastic element is kept for\n    exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Determine which bins can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit, return all zeros\n\n    fitting_bins_caps = bins_remain_cap[can_fit_mask]\n\n    # Strategy 1: Best Fit (Minimized Waste)\n    # Calculate the remaining capacity after fitting the item.\n    # Higher priority for bins with smaller remaining capacity (less waste).\n    waste = fitting_bins_caps - item\n    # Use a value that increases as waste decreases. Adding 1 to avoid division by zero.\n    best_fit_scores = 1.0 / (1.0 + waste)\n\n    # Strategy 2: Diversity (Maximized Remaining Space after fit)\n    # Prioritize bins that leave more space after packing the item.\n    # This is beneficial for consolidating smaller items and leaving larger\n    # contiguous spaces for potentially larger future items.\n    # We want to maximize (fitting_bins_caps - item). Higher score for larger values.\n    # Normalize to prevent extreme values and ensure it plays well with best_fit_scores.\n    # Add a small epsilon to avoid log(0) or division by zero if fitting_bins_caps - item is 0.\n    max_possible_remaining = np.max(bins_remain_cap) - item if np.max(bins_remain_cap) >= item else 1\n    diversity_scores = np.log1p(waste) / np.log1p(max_possible_remaining) if max_possible_remaining > 0 else np.zeros_like(waste)\n\n\n    # Strategy 3: Exploration (Stochastic Element)\n    # Add a small random component to encourage trying different bins.\n    exploration_factor = 0.05\n    random_scores = np.random.rand(len(fitting_bins_caps)) * exploration_factor\n\n    # Combine scores. Prioritize bins that are a good fit AND leave good remaining space.\n    # The weights are tuned to balance the two primary strategies.\n    # We normalize the scores before combining to ensure they are on a similar scale.\n    # Normalizing best_fit_scores: higher is better (less waste)\n    # Normalizing diversity_scores: higher is better (more remaining space)\n\n    # Normalize best_fit_scores (higher is better, closer to 1)\n    if np.max(best_fit_scores) > 0:\n        normalized_best_fit = best_fit_scores / np.max(best_fit_scores)\n    else:\n        normalized_best_fit = best_fit_scores\n\n    # Normalize diversity_scores (higher is better)\n    if np.max(diversity_scores) > 0:\n        normalized_diversity = diversity_scores / np.max(diversity_scores)\n    else:\n        normalized_diversity = diversity_scores\n\n    # Weighted sum of normalized scores\n    combined_scores = (0.6 * normalized_best_fit) + (0.4 * normalized_diversity) + random_scores\n\n    # Assign the calculated scores to the corresponding bins\n    priorities[can_fit_mask] = combined_scores\n\n    # Ensure bins that cannot fit have a priority of 0\n    priorities[~can_fit_mask] = 0\n\n    return priorities",
    "response_id": 3,
    "obj": 4.038691663342641,
    "SLOC": 25.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response4.txt_stdout.txt",
    "code_path": "problem_iter3_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a hybrid strategy.\n\n    This strategy prioritizes bins that offer a \"tight fit\" (minimizing leftover capacity)\n    while also encouraging exploration by giving a slight boost to bins that are not too full,\n    leaving more room for potentially larger future items. It also introduces a mechanism\n    to avoid bins that are *too* close to being full, which might prevent fitting\n    future items of moderate size.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Determine which bins can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit, return all zeros\n\n    fitting_bins_caps = bins_remain_cap[can_fit_mask]\n\n    # Strategy 1: Tight Fit - prioritize bins that will have the least remaining capacity.\n    # This aims to minimize waste. We want smaller remaining capacity to get higher scores.\n    # Adding 1.0 to the denominator avoids division by zero and scales scores to be positive.\n    remaining_after_fit = fitting_bins_caps - item\n    tight_fit_scores = 1.0 / (1.0 + remaining_after_fit)\n\n    # Strategy 2: Room for Future Items - prioritize bins that have substantial remaining capacity,\n    # but not so much that they are inefficiently used by this item.\n    # This prevents over-prioritizing bins that are almost empty.\n    # We define \"good room\" as having remaining capacity that is significantly larger than the item,\n    # but not excessively large. A simple heuristic could be remaining capacity in the middle range.\n    # Let's consider bins with remaining capacity greater than item * 2 but less than bin_capacity / 2.\n    # This is a tunable parameter. For simplicity here, let's boost bins that have\n    # a moderate amount of remaining space after fitting.\n    # A bin with remaining capacity = item is the \"tightest\" fit, score = 1.\n    # A bin with remaining capacity = item * 2 is a \"good fit\", score is lower.\n    # We can inversely relate to remaining_after_fit, but ensure it's not too large.\n    # Let's create a score that is higher for intermediate remaining capacities.\n    # For example, a quadratic function peaking at some intermediate remaining capacity.\n    # A simpler approach: slightly boost bins that aren't *just* barely fitting.\n    # Let's give a slight positive score for bins that have remaining_after_fit > 0.\n    # This is to encourage using bins that have some \"slack\".\n    exploration_boost = 0.1  # Small boost for having some remaining space\n    exploration_scores = np.zeros_like(fitting_bins_caps)\n    exploration_scores[remaining_after_fit > 0] = exploration_boost\n\n    # Strategy 3: Avoid Over-filling \"Almost Full\" Bins -\n    # Slightly penalize bins that would become *extremely* full if the item is added,\n    # if they were already very close to capacity.\n    # This is to prevent situations where a bin is filled to 99% with a small item,\n    # making it impossible to fit even a moderately sized future item.\n    # Let's define \"almost full\" as remaining_after_fit < a small epsilon.\n    # The penalty should be proportional to how \"full\" it gets.\n    epsilon = 0.05 # A small threshold for \"almost full\"\n    penalty_factor = 0.5\n    penalty_scores = np.zeros_like(fitting_bins_caps)\n    almost_full_condition = (remaining_after_fit < epsilon) & (fitting_bins_caps > epsilon) # Ensure it's not an empty bin\n    # Penalize more if the remaining capacity after fit is very small\n    penalty_scores[almost_full_condition] = -penalty_factor * (epsilon - remaining_after_fit[almost_full_condition]) / epsilon\n\n\n    # Combine scores. Prioritize tight fits, give a small boost for having room, and penalize very tight fits.\n    # Weights are chosen heuristically.\n    combined_scores = (0.8 * tight_fit_scores) + (0.2 * exploration_scores) + penalty_scores\n\n    # Assign the calculated scores to the corresponding bins\n    priorities[can_fit_mask] = combined_scores\n\n    # Ensure bins that cannot fit have a priority of 0 (already handled by initialization and mask)\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "SLOC": 20.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  }
]