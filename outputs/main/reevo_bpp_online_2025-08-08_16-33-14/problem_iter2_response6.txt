```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a hybrid strategy.

    This strategy prioritizes bins that can fit the item. Among those that can fit,
    it favors bins that are closer to being full (i.e., have less remaining capacity
    after fitting the item). This is a greedy approach that tries to leave larger
    gaps in other bins for potentially larger future items.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Initialize scores. Bins that cannot fit will have a very low score.
    # We use a large negative number to ensure they are ranked last.
    scores = np.full_like(bins_remain_cap, -np.inf)

    # Identify bins that can fit the item
    can_fit_mask = bins_remain_cap >= item

    # For bins that can fit, calculate a score.
    # We want to prioritize bins that have less remaining capacity after the item is placed.
    # The "remaining capacity after fit" is (bins_remain_cap - item).
    # To prioritize smaller remaining capacities, we can use the negative of this value.
    # Alternatively, to use a "goodness" score where higher is better, we can use
    # the inverse of the remaining capacity *after* the item is placed, plus a small
    # epsilon to avoid division by zero and to ensure that bins that become exactly full
    # get a high score.
    # A simple approach is to use `bins_remain_cap - item`, and we want to minimize this value.
    # To convert this into a "priority" where higher is better, we can use its negative.
    # A common heuristic for "best fit" is to find the bin with the minimum `bins_remain_cap - item`.
    # So, a higher priority should be given to smaller values of `bins_remain_cap - item`.
    # Let's assign a score that is the negative of the remaining capacity *after* fitting.
    # This means bins that have `bins_remain_cap - item == 0` will have a score of 0.
    # Bins that have `bins_remain_cap - item == X` will have a score of -X.
    # This naturally prioritizes exact fits (score 0) over bins with remaining space.
    scores[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)

    # To make these scores more "softmax-friendly" or generally interpretable as positive
    # priorities, we can shift them so that the minimum possible score (if we consider
    # all possible remaining capacities) maps to a small positive value.
    # The smallest `bins_remain_cap - item` could be 0. The largest would depend on
    # the initial bin capacities and item sizes.
    # A simpler approach that directly reflects the "best fit" idea is to assign a
    # score that is higher for bins closer to fitting the item.
    # Let's re-evaluate: we want to prioritize bins where `bins_remain_cap - item` is small.
    # The smaller `bins_remain_cap - item` is, the higher the priority.
    # So, a simple monotonic transformation that maps smaller `bins_remain_cap - item`
    # to larger priority values is desired.
    # Using `1.0 / (bins_remain_cap[can_fit_mask] - item + epsilon)` achieves this,
    # as seen in `priority_v1`. This also provides a "soft" prioritization.

    # Let's refine the reflection: "Prioritize fits, then closeness to full."
    # "Closeness to full" means minimizing the *remaining* capacity.
    # For a bin `b` with remaining capacity `c_b`, and an item `i` of size `s_i`:
    # If `c_b >= s_i`, the bin can fit.
    # The "closeness to full" after fitting is `c_b - s_i`. We want to minimize this.
    # So, priority should be higher when `c_b - s_i` is smaller.

    # Let's try a simple assignment:
    # For bins that can fit, score = -(bins_remain_cap - item)
    # This means:
    # - Exact fit (remaining capacity after fit = 0) gets score 0.
    # - Bin with remaining capacity after fit = 1 gets score -1.
    # - Bin with remaining capacity after fit = 10 gets score -10.
    # This correctly prioritizes exact fits (0 > -1 > -10).

    scores[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)

    # If we want to ensure that even non-fitting bins have a chance to be selected
    # (though with very low priority), we could assign a small negative value to them.
    # However, for typical online bin packing, if an item cannot fit anywhere,
    # a new bin must be opened. So, prioritizing only fitting bins is correct.
    # The current `scores` array has `-np.inf` for non-fitting bins.

    # The problem asks for a priority *score* for each bin.
    # The previous "Better code" used softmax to convert scores into probabilities.
    # The reflection suggests prioritizing fits, then closeness to full.
    # A simple way to achieve this without softmax (if softmax is not strictly required by the prompt)
    # is to directly return scores that reflect this preference.

    # Let's consider a scenario:
    # item = 5
    # bins_remain_cap = [10, 6, 5, 12]
    #
    # Bin 0: can fit, remaining after fit = 10 - 5 = 5. Score = -5
    # Bin 1: can fit, remaining after fit = 6 - 5 = 1. Score = -1
    # Bin 2: can fit, remaining after fit = 5 - 5 = 0. Score = 0
    # Bin 3: can fit, remaining after fit = 12 - 5 = 7. Score = -7
    #
    # Scores: [-5, -1, 0, -7]
    # The highest score (0) is for bin 2 (exact fit).
    # The next highest (-1) is for bin 1 (closest to full after fit).
    # This aligns with the reflection.

    # The question is what the output *should* be. If it's a direct priority score
    # that will be used in a `max()` function to select the bin, then these negative
    # scores work. If it's for a probabilistic selection (like softmax), then
    # `priority_v1` is more appropriate. The prompt says "the bin with the highest priority score will be selected".
    # This implies a deterministic selection based on the highest score.

    # Therefore, the `scores` computed above directly serve as priority scores.
    # No need for softmax if the selection is deterministic.

    # Let's ensure that the priority scores are easily distinguishable and
    # that non-fitting bins are clearly lower.
    # Using `-np.inf` for non-fitting bins is good.
    # For fitting bins, `-(bins_remain_cap - item)` works.

    # Consider numerical stability and range of values.
    # If `bins_remain_cap - item` can be very large, the negative scores will be very small (large negative).
    # This is fine for direct comparison.

    # Let's make it slightly more robust by ensuring non-fitting bins get a clearly worse score
    # than any possible fitting bin score.
    # The minimum possible value for `bins_remain_cap - item` is 0. So the maximum score is 0.
    # Any score less than 0 is worse. So `-np.inf` for non-fitting bins is appropriate.

    # The current implementation `scores[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)`
    # directly implements the "prioritize fits, then closeness to full" by assigning
    # higher scores to bins with smaller remaining capacity after fitting the item.

    # Let's consider the case where the item is larger than any bin's capacity.
    # `can_fit_mask` would be all `False`. `scores` would remain all `-np.inf`.
    # This correctly indicates no valid placement.

    # Final check of the strategy:
    # 1. Prioritize bins that *can* fit the item. (Handled by `can_fit_mask` and initial `-np.inf`)
    # 2. Among those that can fit, prioritize bins that are "closer to full".
    #    "Closer to full" after placing the item means `bins_remain_cap - item` is minimized.
    #    So, we want to assign higher priority when `bins_remain_cap - item` is small.
    #    Our score `-(bins_remain_cap - item)` does exactly this:
    #    - If `bins_remain_cap - item = 0` (exact fit), score is 0 (highest).
    #    - If `bins_remain_cap - item = 1`, score is -1.
    #    - If `bins_remain_cap - item = 5`, score is -5.
    #    This order `0 > -1 > -5` matches the desired priority.

    # This looks like a solid implementation of the described strategy.

    return scores
```
