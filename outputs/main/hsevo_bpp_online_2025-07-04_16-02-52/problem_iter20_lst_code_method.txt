{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive heuristic combining waste minimization, fill ratio, and bin landscape awareness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    # Waste minimization with scaled penalty\n    wasted_space = bins_remain_cap - item\n    wasted_space[~feasible_bins] = np.inf\n    priorities[feasible_bins] -= (wasted_space[feasible_bins] / bins_remain_cap.max())**2 * (item / bins_remain_cap.max())\n\n    # Adaptive fill ratio bonus, scaled by item size and bin fullness\n    fill_ratio_after = item / bins_remain_cap[feasible_bins]\n    item_scale = item / bins_remain_cap.max()\n    bin_fullness = 1 - bins_remain_cap[feasible_bins] / bins_remain_cap.max()\n    priorities[feasible_bins] += fill_ratio_after * 0.3 * item_scale * (1 + bin_fullness)\n\n    # Controlled randomization, inversely proportional to bin fill\n    randomization_strength = 0.05 * item_scale * (bins_remain_cap[feasible_bins] / bins_remain_cap.max())\n    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * randomization_strength\n\n    # Adaptive bin-emptiness penalty, scaling with item and average occupancy\n    average_occupancy = 1 - np.mean(bins_remain_cap / bins_remain_cap.max())\n    empty_bin_penalty = (bins_remain_cap / bins_remain_cap.max()) * (1 - item / bins_remain_cap.max())\n    empty_bin_penalty_weight = 0.02 + 0.08 * average_occupancy\n    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * empty_bin_penalty_weight\n\n    # Bonus for almost full bins after insertion\n    wasted_space_after = bins_remain_cap[feasible_bins] - item\n    almost_full = wasted_space_after / bins_remain_cap.max() < 0.05\n    priorities[feasible_bins][almost_full] += 0.1\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive heuristic combining waste minimization, fill ratio, and bin landscape awareness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    # Waste minimization with scaled penalty\n    wasted_space = bins_remain_cap - item\n    wasted_space[~feasible_bins] = np.inf\n    priorities[feasible_bins] -= (wasted_space[feasible_bins] / bins_remain_cap.max())**2 * (item / bins_remain_cap.max())\n\n    # Adaptive fill ratio bonus, scaled by item size and bin fullness\n    fill_ratio_after = item / bins_remain_cap[feasible_bins]\n    item_scale = item / bins_remain_cap.max()\n    bin_fullness = 1 - bins_remain_cap[feasible_bins] / bins_remain_cap.max()\n    priorities[feasible_bins] += fill_ratio_after * 0.3 * item_scale * (1 + bin_fullness)\n\n    # Controlled randomization, inversely proportional to bin fill\n    randomization_strength = 0.05 * item_scale * (bins_remain_cap[feasible_bins] / bins_remain_cap.max())\n    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * randomization_strength\n\n    # Adaptive bin-emptiness penalty, scaling with item and average occupancy\n    average_occupancy = 1 - np.mean(bins_remain_cap / bins_remain_cap.max())\n    empty_bin_penalty = (bins_remain_cap / bins_remain_cap.max()) * (1 - item / bins_remain_cap.max())\n    empty_bin_penalty_weight = 0.02 + 0.08 * average_occupancy\n    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * empty_bin_penalty_weight\n\n    # Bonus for almost full bins after insertion\n    wasted_space_after = bins_remain_cap[feasible_bins] - item\n    almost_full = wasted_space_after / bins_remain_cap.max() < 0.05\n    priorities[feasible_bins][almost_full] += 0.1\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive heuristic combining waste minimization, fill ratio, and bin landscape awareness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    # Waste minimization with scaled penalty\n    wasted_space = bins_remain_cap - item\n    wasted_space[~feasible_bins] = np.inf\n    priorities[feasible_bins] -= (wasted_space[feasible_bins] / bins_remain_cap.max())**2 * (item / bins_remain_cap.max())\n\n    # Adaptive fill ratio bonus, scaled by item size and bin fullness\n    fill_ratio_after = item / bins_remain_cap[feasible_bins]\n    item_scale = item / bins_remain_cap.max()\n    bin_fullness = 1 - bins_remain_cap[feasible_bins] / bins_remain_cap.max()\n    priorities[feasible_bins] += fill_ratio_after * 0.3 * item_scale * (1 + bin_fullness)\n\n    # Controlled randomization, inversely proportional to bin fill\n    randomization_strength = 0.05 * item_scale * (bins_remain_cap[feasible_bins] / bins_remain_cap.max())\n    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * randomization_strength\n\n    # Adaptive bin-emptiness penalty, scaling with item and average occupancy\n    average_occupancy = 1 - np.mean(bins_remain_cap / bins_remain_cap.max())\n    empty_bin_penalty = (bins_remain_cap / bins_remain_cap.max()) * (1 - item / bins_remain_cap.max())\n    empty_bin_penalty_weight = 0.02 + 0.08 * average_occupancy\n    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * empty_bin_penalty_weight\n\n    # Bonus for almost full bins after insertion\n    wasted_space_after = bins_remain_cap[feasible_bins] - item\n    almost_full = wasted_space_after / bins_remain_cap.max() < 0.05\n    priorities[feasible_bins][almost_full] += 0.1\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Enhanced adaptive heuristic with dynamic exploration/exploitation.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    # Waste minimization with dynamic non-linear penalty. Larger waste penalized more\n    wasted_space = bins_remain_cap - item\n    wasted_space[~feasible_bins] = np.inf\n    waste_penalty = (wasted_space[feasible_bins] / bins_remain_cap.max())**3 * (item / bins_remain_cap.max())\n    priorities[feasible_bins] -= waste_penalty\n\n    # Fill ratio bonus, scaled by item size and bin fullness.  Emphasis on nearly-full bins.\n    fill_ratio_after = item / bins_remain_cap[feasible_bins]\n    item_scale = item / bins_remain_cap.max()\n    bin_fullness = 1 - bins_remain_cap[feasible_bins] / bins_remain_cap.max()\n    priorities[feasible_bins] += fill_ratio_after * 0.4 * item_scale * (1 + bin_fullness**2) # Non-linear fullness boost\n\n    # Dynamic randomization, proportional to item size and remaining capacity variance\n    capacity_std = np.std(bins_remain_cap[feasible_bins] / bins_remain_cap.max()) if np.sum(feasible_bins) > 1 else 0.0\n    randomization_strength = 0.03 * item_scale * (1 + capacity_std) * (bins_remain_cap[feasible_bins] / bins_remain_cap.max())\n    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * randomization_strength\n\n    # Penalty for leaving bins mostly empty, adjusted by average occupancy\n    average_occupancy = 1 - np.mean(bins_remain_cap / bins_remain_cap.max())\n    empty_bin_penalty = (bins_remain_cap / bins_remain_cap.max())**2 * (1 - item / bins_remain_cap.max())\n    empty_bin_penalty_weight = 0.01 + 0.05 * average_occupancy\n    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * empty_bin_penalty_weight\n\n    # Significant bonus for bins becoming almost full, scaled by how close they are\n    wasted_space_after = bins_remain_cap[feasible_bins] - item\n    almost_full = wasted_space_after / bins_remain_cap.max() < 0.05\n    almost_full_bonus = 0.2 * (1 - wasted_space_after[almost_full] / (0.05 * bins_remain_cap.max())) if np.any(almost_full) else 0.0\n    priorities[feasible_bins][almost_full] += almost_full_bonus\n\n    # Smaller items have a higher chance to explore (fit in emptier bins)\n    if item < 0.2 * bins_remain_cap.max():\n        exploration_bonus = 0.05 * (bins_remain_cap[feasible_bins] / bins_remain_cap.max())\n        priorities[feasible_bins] += exploration_bonus\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive heuristic combining waste minimization, fill ratio, bin landscape awareness, and dynamic exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    # 1. Waste Minimization with Non-linear Penalty and Item Size Awareness\n    wasted_space = bins_remain_cap - item\n    wasted_space[~feasible_bins] = np.inf\n    waste_penalty = (wasted_space[feasible_bins] / bins_remain_cap.max())**3 * (item / bins_remain_cap.max()) # Cubed penalty for larger waste\n    priorities[feasible_bins] -= waste_penalty\n\n    # 2. Fill Ratio Bonus with Dynamic Scaling based on Item Size and Bin Fullness\n    fill_ratio_after = item / bins_remain_cap[feasible_bins]\n    item_scale = item / bins_remain_cap.max()\n    bin_fullness = 1 - bins_remain_cap[feasible_bins] / bins_remain_cap.max()\n    fill_bonus = fill_ratio_after * 0.4 * item_scale * (1 + bin_fullness**2)  # Higher bonus for nearly full bins\n    priorities[feasible_bins] += fill_bonus\n\n    # 3. Dynamic Exploration with Item-Size-Dependent Randomization\n    randomization_strength = 0.01 * (1 - item_scale) * (bins_remain_cap[feasible_bins] / bins_remain_cap.max()) # Smaller items get more randomization\n    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * randomization_strength\n\n    # 4. Adaptive Bin-Emptiness Penalty with Average Occupancy and Item Size Consideration\n    average_occupancy = 1 - np.mean(bins_remain_cap / bins_remain_cap.max())\n    empty_bin_penalty = (bins_remain_cap / bins_remain_cap.max()) * (1 - item / bins_remain_cap.max())**2 # Larger items cause higher penalty for near-empty bins\n    empty_bin_penalty_weight = 0.01 + 0.09 * average_occupancy\n    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * empty_bin_penalty_weight\n\n    # 5. Strong Bonus for Almost Full Bins After Insertion\n    wasted_space_after = bins_remain_cap[feasible_bins] - item\n    almost_full = wasted_space_after / bins_remain_cap.max() < 0.03 # more strict condition for almost full\n    priorities[feasible_bins][almost_full] += 0.15  #Increased bonus\n\n    # 6. Moderate penalty for filling bins above a threshold.\n    threshold = 0.7\n    overfilled = (1 - (bins_remain_cap[feasible_bins] - item) / bins_remain_cap.max()) > threshold\n    priorities[feasible_bins][overfilled] -= 0.05 * (1 - average_occupancy) # Scale penalty\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, adaptive fill ratio, and empty bin penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    # Waste minimization with non-linear penalty\n    wasted_space = bins_remain_cap - item\n    wasted_space[~feasible_bins] = np.inf\n    priorities[feasible_bins] -= (wasted_space[feasible_bins] / bins_remain_cap.max())**2\n\n    # Adaptive fill ratio bonus\n    fill_ratio_after = item / bins_remain_cap[feasible_bins]\n    priorities[feasible_bins] += fill_ratio_after * 0.3\n\n    # Adaptive bin-emptiness penalty\n    average_occupancy = 1 - np.mean(bins_remain_cap / bins_remain_cap.max())\n    empty_bin_penalty = (bins_remain_cap / bins_remain_cap.max())\n    empty_bin_penalty_weight = 0.02 + 0.08 * average_occupancy\n    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * empty_bin_penalty_weight\n\n    # Bonus for almost full bins\n    almost_full_bins = bins_remain_cap < 1.1 * item\n    almost_full_bins_feasible = feasible_bins & almost_full_bins\n    if np.any(almost_full_bins_feasible):\n      priorities[almost_full_bins_feasible] += 0.1 * (1 - (bins_remain_cap[almost_full_bins_feasible] / (1.1*item)))\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Enhanced adaptive heuristic with dynamic exploration/exploitation.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    # Waste minimization with dynamic non-linear penalty. Larger waste penalized more\n    wasted_space = bins_remain_cap - item\n    wasted_space[~feasible_bins] = np.inf\n    waste_penalty = (wasted_space[feasible_bins] / bins_remain_cap.max())**3 * (item / bins_remain_cap.max())\n    priorities[feasible_bins] -= waste_penalty\n\n    # Fill ratio bonus, scaled by item size and bin fullness.  Emphasis on nearly-full bins.\n    fill_ratio_after = item / bins_remain_cap[feasible_bins]\n    item_scale = item / bins_remain_cap.max()\n    bin_fullness = 1 - bins_remain_cap[feasible_bins] / bins_remain_cap.max()\n    priorities[feasible_bins] += fill_ratio_after * 0.4 * item_scale * (1 + bin_fullness**2) # Non-linear fullness boost\n\n    # Dynamic randomization, proportional to item size and remaining capacity variance\n    capacity_std = np.std(bins_remain_cap[feasible_bins] / bins_remain_cap.max()) if np.sum(feasible_bins) > 1 else 0.0\n    randomization_strength = 0.03 * item_scale * (1 + capacity_std) * (bins_remain_cap[feasible_bins] / bins_remain_cap.max())\n    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * randomization_strength\n\n    # Penalty for leaving bins mostly empty, adjusted by average occupancy\n    average_occupancy = 1 - np.mean(bins_remain_cap / bins_remain_cap.max())\n    empty_bin_penalty = (bins_remain_cap / bins_remain_cap.max())**2 * (1 - item / bins_remain_cap.max())\n    empty_bin_penalty_weight = 0.01 + 0.05 * average_occupancy\n    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * empty_bin_penalty_weight\n\n    # Significant bonus for bins becoming almost full, scaled by how close they are\n    wasted_space_after = bins_remain_cap[feasible_bins] - item\n    almost_full = wasted_space_after / bins_remain_cap.max() < 0.05\n    almost_full_bonus = 0.2 * (1 - wasted_space_after[almost_full] / (0.05 * bins_remain_cap.max())) if np.any(almost_full) else 0.0\n    priorities[feasible_bins][almost_full] += almost_full_bonus\n\n    # Smaller items have a higher chance to explore (fit in emptier bins)\n    if item < 0.2 * bins_remain_cap.max():\n        exploration_bonus = 0.05 * (bins_remain_cap[feasible_bins] / bins_remain_cap.max())\n        priorities[feasible_bins] += exploration_bonus\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Advanced adaptive heuristic with dynamic exploration/exploitation and non-linear waste penalties.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    max_bin_cap = np.max(bins_remain_cap)\n    avg_bin_cap = np.mean(bins_remain_cap)\n\n    # 1. Dynamic Waste Minimization (Non-linear penalty)\n    wasted_space = bins_remain_cap - item\n    wasted_space[~feasible_bins] = np.inf\n    waste_penalty = (wasted_space / max_bin_cap)**3  # Cubic penalty for larger waste\n    priorities[feasible_bins] -= waste_penalty[feasible_bins] * (item / max_bin_cap)\n\n    # 2. Adaptive Fill Ratio Bonus\n    fill_ratio_after = item / bins_remain_cap[feasible_bins]\n    item_scale = item / max_bin_cap\n\n    # Adjust fill ratio bonus based on item size and bin fullness\n    bin_fullness = 1 - bins_remain_cap[feasible_bins] / max_bin_cap\n    fill_bonus = fill_ratio_after * 0.4 * item_scale * (1 + bin_fullness**2) # Non-linear fullness boost\n    priorities[feasible_bins] += fill_bonus\n\n    # 3. Dynamic Exploration (Randomization)\n    # More exploration for smaller items and fuller bins\n    exploration_strength = 0.03 * (item / max_bin_cap) * (1 - avg_bin_cap / max_bin_cap)\n    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_strength\n\n    # 4. Empty Bin Penalty (Adaptive)\n    # Scales with both item size and average remaining capacity\n    empty_bin_penalty = (bins_remain_cap / max_bin_cap) * (item / max_bin_cap)\n    empty_bin_weight = 0.01 + 0.04 * (1 - avg_bin_cap / max_bin_cap) # Scales with occupancy\n    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * empty_bin_weight\n\n    # 5. \"Almost Full\" Bonus (Enhanced)\n    wasted_space_after = bins_remain_cap[feasible_bins] - item\n    almost_full_threshold = 0.04 * max_bin_cap\n    almost_full = wasted_space_after <= almost_full_threshold\n    priorities[feasible_bins][almost_full] += 0.15  # Increased bonus\n\n    # 6. Large Item Consideration (New)\n    # Incentivize placing large items in emptier bins\n    if item > 0.5 * max_bin_cap:\n        empty_bin_bonus = (bins_remain_cap[feasible_bins] / max_bin_cap) * 0.05\n        priorities[feasible_bins] += empty_bin_bonus\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Hybrid approach: Best-fit, fill ratio, emptiness, near-full rewards, adaptive randomization.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[bins_remain_cap < item] = -np.inf\n\n    wasted_space = bins_remain_cap - item\n    feasible_bins = bins_remain_cap >= item\n\n    # Best-fit with non-linear penalty\n    priorities[feasible_bins] = -wasted_space[feasible_bins]**2 / bins_remain_cap.max()\n\n    # Adaptive fill ratio bonus, scaled by item size\n    fill_ratio_after = (1 - wasted_space / bins_remain_cap.max())\n    item_scale = item / bins_remain_cap.max()\n    fill_bonus_weight = 0.2 + 0.3 * item_scale\n    priorities[feasible_bins] += fill_ratio_after[feasible_bins] * fill_bonus_weight\n\n    # Adaptive Randomization: More randomization for fuller bins\n    randomization_strength = 0.05 * item_scale * (bins_remain_cap.max() - bins_remain_cap) / bins_remain_cap.max()\n    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * randomization_strength[feasible_bins]\n\n    # Adaptive bin-emptiness penalty, scaling with item and average occupancy\n    average_occupancy = 1 - np.mean(bins_remain_cap / bins_remain_cap.max())\n    empty_bin_penalty = (bins_remain_cap / bins_remain_cap.max()) * (1 - item / bins_remain_cap.max())\n    empty_bin_penalty_weight = 0.02 + 0.08 * average_occupancy\n    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * empty_bin_penalty_weight\n\n    # Bonus for almost full bins\n    almost_full = (wasted_space / bins_remain_cap.max() < 0.05) & feasible_bins\n    priorities[almost_full] += 0.1\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Advanced adaptive heuristic with dynamic exploration/exploitation and non-linear waste penalties.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    max_bin_cap = np.max(bins_remain_cap)\n    avg_bin_cap = np.mean(bins_remain_cap)\n\n    # 1. Dynamic Waste Minimization (Non-linear penalty)\n    wasted_space = bins_remain_cap - item\n    wasted_space[~feasible_bins] = np.inf\n    waste_penalty = (wasted_space / max_bin_cap)**3  # Cubic penalty for larger waste\n    priorities[feasible_bins] -= waste_penalty[feasible_bins] * (item / max_bin_cap)\n\n    # 2. Adaptive Fill Ratio Bonus\n    fill_ratio_after = item / bins_remain_cap[feasible_bins]\n    item_scale = item / max_bin_cap\n\n    # Adjust fill ratio bonus based on item size and bin fullness\n    bin_fullness = 1 - bins_remain_cap[feasible_bins] / max_bin_cap\n    fill_bonus = fill_ratio_after * 0.4 * item_scale * (1 + bin_fullness**2) # Non-linear fullness boost\n    priorities[feasible_bins] += fill_bonus\n\n    # 3. Dynamic Exploration (Randomization)\n    # More exploration for smaller items and fuller bins\n    exploration_strength = 0.03 * (item / max_bin_cap) * (1 - avg_bin_cap / max_bin_cap)\n    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * exploration_strength\n\n    # 4. Empty Bin Penalty (Adaptive)\n    # Scales with both item size and average remaining capacity\n    empty_bin_penalty = (bins_remain_cap / max_bin_cap) * (item / max_bin_cap)\n    empty_bin_weight = 0.01 + 0.04 * (1 - avg_bin_cap / max_bin_cap) # Scales with occupancy\n    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * empty_bin_weight\n\n    # 5. \"Almost Full\" Bonus (Enhanced)\n    wasted_space_after = bins_remain_cap[feasible_bins] - item\n    almost_full_threshold = 0.04 * max_bin_cap\n    almost_full = wasted_space_after <= almost_full_threshold\n    priorities[feasible_bins][almost_full] += 0.15  # Increased bonus\n\n    # 6. Large Item Consideration (New)\n    # Incentivize placing large items in emptier bins\n    if item > 0.5 * max_bin_cap:\n        empty_bin_bonus = (bins_remain_cap[feasible_bins] / max_bin_cap) * 0.05\n        priorities[feasible_bins] += empty_bin_bonus\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive fill ratio, randomization, emptiness, and near-full reward.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    # Waste minimization with non-linear penalty, scaled by max bin cap.\n    wasted_space = bins_remain_cap - item\n    priorities[feasible_bins] -= (wasted_space[feasible_bins]**2) / bins_remain_cap.max()\n\n    # Adaptive fill ratio bonus, scaled by item size and bin capacity\n    item_scale = item / bins_remain_cap.max()\n    fill_ratio_after = (1 - wasted_space / bins_remain_cap.max())\n    fill_bonus_weight = 0.2 + 0.3 * item_scale\n    priorities[feasible_bins] += fill_ratio_after[feasible_bins] * fill_bonus_weight\n\n    # Adaptive Randomization scaled by remaining capacity, less for bigger items\n    randomization_strength = 0.05 * (1 - item_scale) * (bins_remain_cap / bins_remain_cap.max())\n    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * randomization_strength[feasible_bins]\n\n    # Adaptive bin-emptiness penalty, avoid filling almost empty bins early\n    empty_bin_penalty = (bins_remain_cap / bins_remain_cap.max()) * (1 - item_scale)\n    empty_bin_penalty_weight = 0.03\n    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * empty_bin_penalty_weight\n\n    # Near full bonus: Reward bins close to full before insertion\n    almost_full = (wasted_space / bins_remain_cap.max() < 0.05) & feasible_bins\n    priorities[almost_full] += 0.1\n    \n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive randomization, and dynamic empty bin penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Mark infeasible bins with the lowest priority\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    # Focus only on feasible bins\n    feasible_mask = bins_remain_cap >= item\n    feasible_bins_cap = bins_remain_cap[feasible_mask]\n\n    if feasible_bins_cap.size == 0:\n        return priorities\n\n    # Waste-based prioritization (smaller waste is better)\n    wasted_space = feasible_bins_cap - item\n    priorities[feasible_mask] = -wasted_space\n\n    # Adaptive randomization, scaling with item size and bin capacity\n    randomization_strength = 0.05 * item * (1 - wasted_space / bins_remain_cap.max())\n    priorities[feasible_mask] += np.random.rand(feasible_bins_cap.size) * randomization_strength\n\n    # Dynamic empty bin penalty, adjusting to the overall bin landscape\n    average_bin_usage = np.mean(bins_remain_cap) / bins_remain_cap.max()\n    empty_bin_penalty = (bins_remain_cap / bins_remain_cap.max()) * (1 - average_bin_usage)\n    priorities[feasible_mask] -= empty_bin_penalty[feasible_mask] * 0.1\n\n    # Encourage filling bins, with bonus based on fill ratio AFTER insertion\n    fill_ratio_after = (1 - wasted_space / bins_remain_cap.max())\n    priorities[feasible_mask] += fill_ratio_after * 0.2\n\n    # Non-linear wasted space penalty\n    priorities[feasible_mask] -= (wasted_space / bins_remain_cap.max())**2 * 0.1\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive randomization, and dynamic empty bin penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Mark infeasible bins with the lowest priority\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    # Focus only on feasible bins\n    feasible_mask = bins_remain_cap >= item\n    feasible_bins_cap = bins_remain_cap[feasible_mask]\n\n    if feasible_bins_cap.size == 0:\n        return priorities\n\n    # Waste-based prioritization (smaller waste is better)\n    wasted_space = feasible_bins_cap - item\n    priorities[feasible_mask] = -wasted_space\n\n    # Adaptive randomization, scaling with item size and bin capacity\n    randomization_strength = 0.05 * item * (1 - wasted_space / bins_remain_cap.max())\n    priorities[feasible_mask] += np.random.rand(feasible_bins_cap.size) * randomization_strength\n\n    # Dynamic empty bin penalty, adjusting to the overall bin landscape\n    average_bin_usage = np.mean(bins_remain_cap) / bins_remain_cap.max()\n    empty_bin_penalty = (bins_remain_cap / bins_remain_cap.max()) * (1 - average_bin_usage)\n    priorities[feasible_mask] -= empty_bin_penalty[feasible_mask] * 0.1\n\n    # Encourage filling bins, with bonus based on fill ratio AFTER insertion\n    fill_ratio_after = (1 - wasted_space / bins_remain_cap.max())\n    priorities[feasible_mask] += fill_ratio_after * 0.2\n\n    # Non-linear wasted space penalty\n    priorities[feasible_mask] -= (wasted_space / bins_remain_cap.max())**2 * 0.1\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive randomization, and dynamic empty bin penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Mark infeasible bins with the lowest priority\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    # Focus only on feasible bins\n    feasible_mask = bins_remain_cap >= item\n    feasible_bins_cap = bins_remain_cap[feasible_mask]\n\n    if feasible_bins_cap.size == 0:\n        return priorities\n\n    # Waste-based prioritization (smaller waste is better)\n    wasted_space = feasible_bins_cap - item\n    priorities[feasible_mask] = -wasted_space\n\n    # Adaptive randomization, scaling with item size and bin capacity\n    randomization_strength = 0.05 * item * (1 - wasted_space / bins_remain_cap.max())\n    priorities[feasible_mask] += np.random.rand(feasible_bins_cap.size) * randomization_strength\n\n    # Dynamic empty bin penalty, adjusting to the overall bin landscape\n    average_bin_usage = np.mean(bins_remain_cap) / bins_remain_cap.max()\n    empty_bin_penalty = (bins_remain_cap / bins_remain_cap.max()) * (1 - average_bin_usage)\n    priorities[feasible_mask] -= empty_bin_penalty[feasible_mask] * 0.1\n\n    # Encourage filling bins, with bonus based on fill ratio AFTER insertion\n    fill_ratio_after = (1 - wasted_space / bins_remain_cap.max())\n    priorities[feasible_mask] += fill_ratio_after * 0.2\n\n    # Non-linear wasted space penalty\n    priorities[feasible_mask] -= (wasted_space / bins_remain_cap.max())**2 * 0.1\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit, adaptive randomization, and bin fullness reward.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    # Waste minimization with non-linear penalty.\n    wasted_space = bins_remain_cap - item\n    wasted_space[~feasible_bins] = np.inf\n    priorities[feasible_bins] -= (wasted_space[feasible_bins] / bins_remain_cap.max())**2\n\n    # Adaptive fill ratio bonus scaled by item size.\n    fill_ratio_after = item / bins_remain_cap[feasible_bins]\n    item_scale = item / bins_remain_cap.max()\n    priorities[feasible_bins] += fill_ratio_after * 0.2 * item_scale\n    \n    # Adaptive randomization. Higher randomness for smaller occupancy bins.\n    randomization_strength = 0.05 * item_scale * (1 - bins_remain_cap[feasible_bins] / bins_remain_cap.max())\n    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * randomization_strength\n\n    # Bonus for bins nearing full capacity, stronger when the item is large.\n    almost_perfect_fit = np.exp(-wasted_space[feasible_bins] * 5 / item)\n    priorities[feasible_bins] += almost_perfect_fit * 0.05\n\n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive bin prioritization with waste, fill ratio, randomization, and empty bin penalty.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    # Waste minimization with non-linear penalty, scaled by item size\n    wasted_space = bins_remain_cap - item\n    wasted_space[~feasible_bins] = np.inf\n    priorities[feasible_bins] -= (wasted_space[feasible_bins] / bins_remain_cap.max())**2 * (item / bins_remain_cap.max())\n\n    # Adaptive fill ratio bonus, more significant for larger items\n    fill_ratio_after = item / bins_remain_cap[feasible_bins]\n    item_scale = item / bins_remain_cap.max()\n    priorities[feasible_bins] += fill_ratio_after * 0.3 * item_scale\n\n    # Controlled randomization, inversely proportional to bin fill and scaled by item size\n    randomization_strength = 0.05 * item_scale * (1 - bins_remain_cap[feasible_bins] / bins_remain_cap.max())\n    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * randomization_strength\n\n    # Adaptive bin-emptiness penalty, scaling with item and average occupancy\n    average_occupancy = 1 - np.mean(bins_remain_cap / bins_remain_cap.max())\n    empty_bin_penalty = (bins_remain_cap / bins_remain_cap.max()) * (1 - item / bins_remain_cap.max())\n    empty_bin_penalty_weight = 0.02 + 0.08 * average_occupancy\n    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * empty_bin_penalty_weight\n    \n    # Encourage filling bins, bonus based on fill ratio AFTER placement. Scale with occupancy\n    fill_ratio_after_simple = (1 - wasted_space / bins_remain_cap.max())\n    priorities[feasible_bins] += fill_ratio_after_simple[feasible_bins] * 0.2 * average_occupancy\n    \n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive heuristic combining waste minimization, fill ratio, and bin landscape.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    # Waste minimization with non-linear penalty, scaled by item size\n    wasted_space = bins_remain_cap - item\n    wasted_space[~feasible_bins] = np.inf\n    priorities[feasible_bins] -= (wasted_space[feasible_bins] / bins_remain_cap.max())**2 * (item / bins_remain_cap.max())\n\n    # Adaptive fill ratio bonus\n    fill_ratio_after = item / bins_remain_cap[feasible_bins]\n    item_scale = item / bins_remain_cap.max()\n    priorities[feasible_bins] += fill_ratio_after * 0.3 * item_scale\n\n    # Adaptive Randomization: Smaller items get more randomization.\n    randomization_strength = 0.05 * (1 - item_scale)\n    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * randomization_strength\n\n    # Encourage filling bins, bonus based on fill ratio AFTER placement\n    fill_ratio_after_all = (bins_remain_cap - wasted_space) / bins_remain_cap.max()\n    priorities[feasible_bins] += fill_ratio_after_all[feasible_bins] * 0.2\n\n    # Bonus for bins that would become nearly full after placing the item\n    nearly_full_threshold = 0.9\n    nearly_full_bonus = np.where((bins_remain_cap >= item) & (fill_ratio_after_all >= nearly_full_threshold), 0.2, 0)\n    priorities[bins_remain_cap >= item] += nearly_full_bonus[bins_remain_cap >= item]\n\n    # Adaptive bin-emptiness penalty, scaling with item and average occupancy\n    average_occupancy = 1 - np.mean(bins_remain_cap / bins_remain_cap.max())\n    empty_bin_penalty = (bins_remain_cap / bins_remain_cap.max()) * (1 - item / bins_remain_cap.max())\n    empty_bin_penalty_weight = 0.02 + 0.08 * average_occupancy\n    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * empty_bin_penalty_weight\n\n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, fill_ratio_weight: float = 0.4870062417351315, randomization_strength_base: float = 0.09225855045409503,\n                empty_bin_penalty_weight_base: float = 0.026994073078877856, empty_bin_penalty_weight_scale: float = 0.13162472370352277,\n                almost_full_threshold: float = 0.0017910109562941812, almost_full_bonus: float = 0.19759070503147336) -> np.ndarray:\n    \"\"\"Adaptive heuristic combining waste minimization, fill ratio, and bin landscape awareness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, adaptive fill ratio, controlled randomization,\n    empty bin penalty, and almost full reward. Adapts to bin landscape.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    wasted_space = bins_remain_cap - item\n    wasted_space[wasted_space < 0] = 0\n\n    # Prioritize based on wasted space, normalized\n    priorities[feasible_bins] = -wasted_space[feasible_bins] / np.max(bins_remain_cap)\n\n    # Adaptive fill ratio bonus, scaled by item size\n    fill_ratio_after = (bins_remain_cap[feasible_bins] - item) / np.max(bins_remain_cap)\n    priorities[feasible_bins] += (1 - fill_ratio_after) * 0.3 * (1 - item / np.max(bins_remain_cap))\n\n    # Adaptive Randomization: Reduce randomization as bins become fuller.\n    randomization_strength = 0.1 * item * (np.max(bins_remain_cap) - bins_remain_cap[feasible_bins]) / np.max(bins_remain_cap)\n    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * randomization_strength\n\n    # Bonus for almost full bins\n    almost_full_threshold = 0.1\n    almost_full_bins = (bins_remain_cap[feasible_bins] <= almost_full_threshold * np.max(bins_remain_cap))\n    priorities[feasible_bins][almost_full_bins] += 0.2\n\n    # Empty Bin Penalty: Penalize bins that are too empty, adjust based on average fill.\n    average_fill = np.mean(1 - bins_remain_cap / np.max(bins_remain_cap))\n    empty_bin_penalty = (bins_remain_cap / np.max(bins_remain_cap))\n    penalty_scale = 0.05 * (1 - average_fill)\n    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * penalty_scale\n\n    # Reward bins close to full before insertion\n    current_fill_ratio = 1 - bins_remain_cap / np.max(bins_remain_cap)\n    priorities[feasible_bins] += current_fill_ratio[feasible_bins] * 0.05\n\n\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, adaptive fill ratio, controlled randomization,\n    empty bin penalty, and almost full reward. Adapts to bin landscape.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    wasted_space = bins_remain_cap - item\n    wasted_space[wasted_space < 0] = 0\n\n    # Prioritize based on wasted space, normalized\n    priorities[feasible_bins] = -wasted_space[feasible_bins] / np.max(bins_remain_cap)\n\n    # Adaptive fill ratio bonus, scaled by item size\n    fill_ratio_after = (bins_remain_cap[feasible_bins] - item) / np.max(bins_remain_cap)\n    priorities[feasible_bins] += (1 - fill_ratio_after) * 0.3 * (1 - item / np.max(bins_remain_cap))\n\n    # Adaptive Randomization: Reduce randomization as bins become fuller.\n    randomization_strength = 0.1 * item * (np.max(bins_remain_cap) - bins_remain_cap[feasible_bins]) / np.max(bins_remain_cap)\n    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * randomization_strength\n\n    # Bonus for almost full bins\n    almost_full_threshold = 0.1\n    almost_full_bins = (bins_remain_cap[feasible_bins] <= almost_full_threshold * np.max(bins_remain_cap))\n    priorities[feasible_bins][almost_full_bins] += 0.2\n\n    # Empty Bin Penalty: Penalize bins that are too empty, adjust based on average fill.\n    average_fill = np.mean(1 - bins_remain_cap / np.max(bins_remain_cap))\n    empty_bin_penalty = (bins_remain_cap / np.max(bins_remain_cap))\n    penalty_scale = 0.05 * (1 - average_fill)\n    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * penalty_scale\n\n    # Reward bins close to full before insertion\n    current_fill_ratio = 1 - bins_remain_cap / np.max(bins_remain_cap)\n    priorities[feasible_bins] += current_fill_ratio[feasible_bins] * 0.05\n\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}