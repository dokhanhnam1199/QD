{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Advanced heuristic combining waste minimization, fill ratio, bin landscape awareness, and dynamic exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    # Waste minimization with adaptive scaling\n    wasted_space = bins_remain_cap - item\n    wasted_space[~feasible_bins] = np.inf\n    waste_penalty_scale = np.clip(item / bins_remain_cap.max(), 0.1, 0.5)  # Scale based on item size\n    priorities[feasible_bins] -= (wasted_space[feasible_bins] / bins_remain_cap.max())**2 * waste_penalty_scale\n\n    # Fill ratio bonus, scaled by item size and bin fullness (non-linear reward)\n    fill_ratio_after = item / bins_remain_cap[feasible_bins]\n    item_scale = item / bins_remain_cap.max()\n    bin_fullness = 1 - bins_remain_cap[feasible_bins] / bins_remain_cap.max()\n    priorities[feasible_bins] += fill_ratio_after**1.5 * 0.4 * item_scale * (1 + bin_fullness**2) # non-linear fill ratio\n\n    # Controlled randomization, inversely proportional to bin fullness (adaptive exploration)\n    randomization_strength = 0.1 * item_scale * (bins_remain_cap[feasible_bins] / bins_remain_cap.max())**2 # less full means more random\n    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * randomization_strength\n\n    # Adaptive bin-emptiness penalty, scaling with item and average occupancy (dynamic)\n    average_occupancy = 1 - np.mean(bins_remain_cap / bins_remain_cap.max())\n    empty_bin_penalty = (bins_remain_cap / bins_remain_cap.max()) * (1 - item / bins_remain_cap.max())\n    empty_bin_penalty_weight = 0.02 + 0.1 * average_occupancy # dynamic update based on average\n    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * empty_bin_penalty_weight\n\n    # Strong bonus for almost full bins after insertion (exploitation)\n    wasted_space_after = bins_remain_cap[feasible_bins] - item\n    almost_full = wasted_space_after / bins_remain_cap.max() < 0.05\n    priorities[feasible_bins][almost_full] += 0.2  # stronger bonus\n\n    # Penalty for creating nearly empty bins (bin landscape awareness)\n    nearly_empty = (bins_remain_cap[feasible_bins] - item) / bins_remain_cap.max() > 0.9\n    priorities[feasible_bins][nearly_empty] -= 0.15 * item_scale # small penalty\n\n    # Encourage utilizing bins around the median fullness to promote even distribution.\n    median_fullness = np.median(1 - bins_remain_cap / bins_remain_cap.max())\n    fullness = 1 - bins_remain_cap[feasible_bins] / bins_remain_cap.max()\n    distance_from_median = np.abs(fullness - median_fullness)\n    priorities[feasible_bins] -= 0.05 * item_scale * distance_from_median\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines waste minimization, adaptive fill ratio, controlled randomization,\n    empty bin penalty, and almost full reward. Adapts to bin landscape.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    feasible_bins = bins_remain_cap >= item\n\n    if not np.any(feasible_bins):\n        return priorities - np.inf\n\n    wasted_space = bins_remain_cap - item\n    wasted_space[wasted_space < 0] = 0\n\n    # Prioritize based on wasted space, normalized\n    priorities[feasible_bins] = -wasted_space[feasible_bins] / np.max(bins_remain_cap)\n\n    # Adaptive fill ratio bonus, scaled by item size\n    fill_ratio_after = (bins_remain_cap[feasible_bins] - item) / np.max(bins_remain_cap)\n    priorities[feasible_bins] += (1 - fill_ratio_after) * 0.3 * (1 - item / np.max(bins_remain_cap))\n\n    # Adaptive Randomization: Reduce randomization as bins become fuller.\n    randomization_strength = 0.05 * item * (np.max(bins_remain_cap) - bins_remain_cap[feasible_bins]) / np.max(bins_remain_cap)\n    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * randomization_strength\n\n    # Bonus for almost full bins\n    almost_full_threshold = 0.1\n    almost_full_bins = (bins_remain_cap[feasible_bins] <= almost_full_threshold * np.max(bins_remain_cap))\n    priorities[feasible_bins][almost_full_bins] += 0.2\n\n    # Empty Bin Penalty: Penalize bins that are too empty, adjust based on average fill.\n    average_fill = np.mean(1 - bins_remain_cap / np.max(bins_remain_cap))\n    empty_bin_penalty = (bins_remain_cap / np.max(bins_remain_cap))\n    penalty_scale = 0.05 * (1 - average_fill)\n    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * penalty_scale\n\n    # Reward bins close to full before insertion, non-linear bonus\n    current_fill_ratio = 1 - bins_remain_cap / np.max(bins_remain_cap)\n    priorities[feasible_bins] += current_fill_ratio[feasible_bins]**2 * 0.1 # Non-linear bonus\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic includes a bonus for almost full bins and a penalty for placing small items in almost empty bins, which are absent in the worst. (2nd best) vs (19th) reveals that (2nd) does not penalize placing small items in almost empty bins. Comparing (1st) vs (2nd), they are identical, suggesting other factors influence ranking. (3rd) vs (4th) are identical. Comparing (second worst) vs (worst), they are identical. Overall: Top heuristics have nuanced adjustments based on item size and bin fullness, promoting efficient space use and preventing premature commitment to empty bins with small items. They demonstrate better landscape awareness.\n- \nOkay, I understand the task. Let's redefine \"Current self-reflection\" to design better heuristics, avoiding the pitfalls of \"Ineffective self-reflection\" and focusing on actionable advice.\n\nHere's a breakdown to guide the process:\n\n*   **Keywords:** Adaptive parameters, non-linear scaling, controlled randomness, bin landscape, item size, bin capacity, state-dependent adjustments, balanced exploration/exploitation.\n\n*   **Advice:** Design heuristics with parameters that dynamically adapt to the current state of the bins and items. Use non-linear scaling for rewards and penalties to fine-tune behavior. Balance exploration (trying different placements) with exploitation (placing items where they fit best based on current knowledge).\n\n*   **Avoid:** Overly simplistic, direct heuristics based solely on remaining capacity. Relying on complex analogies without empirical validation. Undifferentiated, global randomness.\n\n*   **Explanation:** Effective heuristics should \"sense\" the overall packing situation and adjust their behavior accordingly. Adaptive parameters, state-dependent adjustments, non-linear scaling and balanced exploration/exploitation are key.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}