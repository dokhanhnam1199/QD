```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Advanced heuristic combining waste minimization, fill ratio, bin landscape awareness, and dynamic exploration."""
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    feasible_bins = bins_remain_cap >= item

    if not np.any(feasible_bins):
        return priorities - np.inf

    # Waste minimization with adaptive scaling
    wasted_space = bins_remain_cap - item
    wasted_space[~feasible_bins] = np.inf
    waste_penalty_scale = np.clip(item / bins_remain_cap.max(), 0.1, 0.5)  # Scale based on item size
    priorities[feasible_bins] -= (wasted_space[feasible_bins] / bins_remain_cap.max())**2 * waste_penalty_scale

    # Fill ratio bonus, scaled by item size and bin fullness (non-linear reward)
    fill_ratio_after = item / bins_remain_cap[feasible_bins]
    item_scale = item / bins_remain_cap.max()
    bin_fullness = 1 - bins_remain_cap[feasible_bins] / bins_remain_cap.max()
    priorities[feasible_bins] += fill_ratio_after**1.5 * 0.4 * item_scale * (1 + bin_fullness**2) # non-linear fill ratio

    # Controlled randomization, inversely proportional to bin fullness (adaptive exploration)
    randomization_strength = 0.1 * item_scale * (bins_remain_cap[feasible_bins] / bins_remain_cap.max())**2 # less full means more random
    priorities[feasible_bins] += np.random.rand(np.sum(feasible_bins)) * randomization_strength

    # Adaptive bin-emptiness penalty, scaling with item and average occupancy (dynamic)
    average_occupancy = 1 - np.mean(bins_remain_cap / bins_remain_cap.max())
    empty_bin_penalty = (bins_remain_cap / bins_remain_cap.max()) * (1 - item / bins_remain_cap.max())
    empty_bin_penalty_weight = 0.02 + 0.1 * average_occupancy # dynamic update based on average
    priorities[feasible_bins] -= empty_bin_penalty[feasible_bins] * empty_bin_penalty_weight

    # Strong bonus for almost full bins after insertion (exploitation)
    wasted_space_after = bins_remain_cap[feasible_bins] - item
    almost_full = wasted_space_after / bins_remain_cap.max() < 0.05
    priorities[feasible_bins][almost_full] += 0.2  # stronger bonus

    # Penalty for creating nearly empty bins (bin landscape awareness)
    nearly_empty = (bins_remain_cap[feasible_bins] - item) / bins_remain_cap.max() > 0.9
    priorities[feasible_bins][nearly_empty] -= 0.15 * item_scale # small penalty

    # Encourage utilizing bins around the median fullness to promote even distribution.
    median_fullness = np.median(1 - bins_remain_cap / bins_remain_cap.max())
    fullness = 1 - bins_remain_cap[feasible_bins] / bins_remain_cap.max()
    distance_from_median = np.abs(fullness - median_fullness)
    priorities[feasible_bins] -= 0.05 * item_scale * distance_from_median

    return priorities
```
