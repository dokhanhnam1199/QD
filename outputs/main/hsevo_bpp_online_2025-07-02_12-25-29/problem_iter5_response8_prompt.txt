{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version prioritizes bins based on a combination of factors:\n    1. Closeness of fit: Bins where the item fits nearly perfectly get a high priority.\n    2. Resulting fragmentation: Bins that would leave minimal wasted space after adding the item are favored.\n    3. Avoidance of near-empty bins: Penalizes bins that, after adding the item, are still far from full.\n    4. Considers a minimum threshold for bin utilization to avoid extreme fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # 1. Closeness of fit: Exponential decay around the perfect fit.\n    close_fit = np.abs(bins_remain_cap - item)\n    priorities = np.exp(-2 * close_fit)  # Increased sensitivity to close fit\n\n    # Penalize bins where the item doesn't fit (assign zero priority)\n    priorities[bins_remain_cap < item] = 0\n\n    # 2. Reward bins that become nearly full after adding the item.\n    remaining_after_fit = bins_remain_cap - item\n    nearly_full = np.exp(-2 * np.abs(remaining_after_fit)) # Increased sensitivity\n\n    priorities += nearly_full\n\n    # 3. Penalize bins that remain far from full. Encourage utilization.\n    # Avoid adding items to bins that will still be mostly empty\n    bin_size = np.max(bins_remain_cap)  # Assuming all bins have the same capacity\n    utilization = (bin_size - remaining_after_fit) / bin_size\n    empty_penalty = np.exp(-5 * (1 - utilization)) # Stronger penalty for low utilization\n    priorities *= empty_penalty # Reduced impact if poorly utilized\n\n\n    # 4. Minimum utilization threshold to reduce extreme fragmentation\n    min_utilization_threshold = 0.2  # e.g., require at least 20% utilization.\n    mask = utilization < min_utilization_threshold\n    priorities[mask] = 0  # Zero out priorities for bins below utilization threshold\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins based on wasted space, fullness, and exploration.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Infeasible bins get -inf priority\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    # Wasted space calculation (exclude infeasible)\n    wasted_space = bins_remain_cap - item\n    wasted_space[infeasible_mask] = np.inf\n\n    # Reward close fits exponentially\n    close_fit_reward = np.exp(-wasted_space)\n\n    # Penalize nearly full bins exponentially\n    nearly_full_penalty = np.exp(-1 / (wasted_space + 1e-9))\n\n    # Combine rewards and penalties\n    priorities = close_fit_reward * nearly_full_penalty\n\n    # Encourage filling bins already somewhat full + randomness\n    priorities += bins_remain_cap * 0.01 + np.random.normal(0, 0.01, size=bins_remain_cap.shape)\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see that the first one prioritizes both close fit and resulting fullness, while the last one prioritizes wasted space, fullness, and exploration (adding noise). The first one uses a simple addition of exponential decays, while the last uses a product of exponential decays and adds a small exploration term.\n\n(2nd best) vs (second worst), we see that the second prioritizes wasted space and fullness, adding a boost for almost full bins. The second-worst also considers wasted space and fullness, but it combines close fit and fullness through multiplication and adds exploration through noise.\n\nComparing (1st) vs (2nd), we see that the first one uses a simple addition of `close_fit` and `nearly_full` priorities, while the second focuses on `waste` and adds a priority boost when the bin is almost full. The first uses `-np.inf` for infeasible bins while the second immediately returns when all bins are infeasible.\n\n(3rd) vs (4th), we see that the third calculates a closeness of fit, rewards nearly full bins, penalizes bins that remain far from full, and considers a minimum utilization threshold.  The fourth simply combines closeness of fit and near fullness without considering utilization or thresholds.\n\nComparing (second worst) vs (worst), we see the only difference is the comment.\n\nOverall: The better heuristics seem to focus on directly combining closeness of fit and fullness, often using exponential decay to prioritize bins. They also incorporate mechanisms to avoid fragmentation, either through a minimum utilization threshold or by penalizing bins that will remain mostly empty after an item is added. The worse heuristics tend to multiply the priority scores (reward and penalty) together and adds noise to encourage exploration, which appears to be less effective in this context. Also the best heuristics directly assign -inf to infeasible bins.\n- \nHere's a redefinition of \"Current self-reflection,\" geared towards effective heuristic design:\n\n*   **Keywords:** Direct factors, additive combinations, exponential decay, infeasibility handling, fragmentation exploration (additive).\n*   **Advice:** Focus on additive combinations of clear, relevant factors. Use exponential decay for parameter tuning. Explicitly penalize infeasibility.\n*   **Avoid:** Multiplicative combinations, excessive randomness that overshadows core heuristic logic.\n*   **Explanation:** Prioritize simple, interpretable rules, focusing on additive combinations of relevant factors. Control randomness to ensure heuristic reliability.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}