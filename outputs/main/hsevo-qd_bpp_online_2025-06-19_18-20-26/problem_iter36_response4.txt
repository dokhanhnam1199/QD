```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    A dynamic priority function for online bin packing that uses a reinforcement learning inspired approach.
    It prioritizes bins based on a weighted combination of factors,
    including space utilization, waste minimization, bin balancing,
    and a adaptive learning rate. The weights are adjusted
    dynamically based on a decaying exploration rate and state.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    valid_bins = bins_remain_cap >= item
    priorities[~valid_bins] = -np.inf

    # Best-Fit component
    size_diff = bins_remain_cap - item
    priorities[valid_bins] += 1.0 / (size_diff[valid_bins] + 0.0001)

    # Waste Minimization: Discourage very small waste
    remaining_after_fit = bins_remain_cap[valid_bins] - item
    small_waste_penalty = np.exp(-10 * remaining_after_fit * item)
    priorities[valid_bins] -= small_waste_penalty

    # Bin Balancing
    utilization = 1 - (bins_remain_cap / np.max(bins_remain_cap))
    overall_utilization = np.mean(utilization)

    # Exploration Rate Decay: Start high, gradually reduce exploration.
    exploration_rate = np.exp(-5 * overall_utilization)

    # Adaptive Weights based on Exploration Rate.
    weight_best_fit = 0.4 + 0.6 * (1 - exploration_rate)
    weight_waste = 0.3 + 0.2 * (1 - exploration_rate)
    weight_balance = 0.3 + 0.2 * (exploration_rate)

    # Combination of factors with adaptive weights
    priorities[valid_bins] = (
        weight_best_fit * (1.0 / (size_diff[valid_bins] + 0.0001))
        - weight_waste * small_waste_penalty
        - weight_balance * np.abs(utilization[valid_bins] - overall_utilization)
    )

    # Adaptive component: bonus to bins nearing fullness
    fullness_bonus = np.exp(-5 * np.abs(bins_remain_cap / np.max(bins_remain_cap) - 0))

    priorities[valid_bins] += exploration_rate * fullness_bonus[valid_bins]

    return priorities
```
