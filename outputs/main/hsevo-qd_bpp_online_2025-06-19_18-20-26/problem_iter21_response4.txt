```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    An adaptive priority function for online bin packing that incorporates
    dynamic weight adjustment based on bin fill distribution, item size,
    and historical packing success.

    It prioritizes bins based on a weighted combination of best-fit,
    waste minimization, bin balancing, and a success-based reinforcement
    mechanism.
    """

    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    valid_bins = bins_remain_cap >= item
    priorities[~valid_bins] = -np.inf

    # 1. Best-Fit with Item Size Consideration:
    size_diff = bins_remain_cap - item
    priorities[valid_bins] += (1.0 / (size_diff[valid_bins] + 0.0001)**2) * (item) # Weight by item size.  Larger items favor tighter fits

    # 2. Waste Minimization with Dynamic Threshold:
    remaining_after_fit = bins_remain_cap[valid_bins] - item
    #Dynamically set acceptable wastage based on the item size.
    waste_threshold = 0.1 * item #Acceptable wastage is 10% of item size.
    small_waste_penalty = np.where(remaining_after_fit <= waste_threshold, np.exp(-10 * remaining_after_fit / (waste_threshold+0.0001)), 0) #Penalize only if less than wastage
    priorities[valid_bins] -= small_waste_penalty

    # 3. Bin Balancing using Gini Coefficient of Remaining Capacities:
    # Calculate Gini coefficient as a measure of capacity distribution.
    bins_remain_cap_normalized = bins_remain_cap / np.max(bins_remain_cap)
    n = len(bins_remain_cap_normalized)
    index = np.arange(1, n + 1)
    gini_coefficient = np.sum((2 * index - n - 1) * bins_remain_cap_normalized) / (n * np.sum(bins_remain_cap_normalized)) if np.sum(bins_remain_cap_normalized) > 0 else 0

    # Adjust bin balancing based on Gini coefficient.  Higher Gini means more imbalance.
    if gini_coefficient > 0.4:
        # Penalize bins that are much larger than average, encourage filling.
        capacity_deviation = bins_remain_cap_normalized - np.mean(bins_remain_cap_normalized)
        priorities[valid_bins] -= 5 * np.abs(capacity_deviation[valid_bins])
    else:
        # Encourage even distribution by penalizing extreme utilizations.
        utilization = 1 - bins_remain_cap_normalized
        priorities[valid_bins] -= 2 * np.abs(utilization[valid_bins] - np.mean(utilization))

    # 4. Success-Based Reinforcement (Simplified - Requires Persistent Storage for Real Implementation):
    #   - In a real online setting, maintain a history of bin choices and their impact
    #   - For now, simulate this with a bias towards bins with higher initial capacity
    capacity_ratio = bins_remain_cap / np.max(bins_remain_cap)
    priorities[valid_bins] += 0.5 * capacity_ratio[valid_bins]

    # 5. Adaptive Adjustment based on Item Size and Average Remaining Capacity:
    avg_remaining_capacity = np.mean(bins_remain_cap)
    if item > avg_remaining_capacity:
        # For large items, prioritize best-fit more aggressively to avoid fragmentation.
        priorities[valid_bins] += (1.0 / (size_diff[valid_bins] + 0.0001)**2) * 2
    else:
        # For smaller items, slightly favor bins with higher remaining capacity for better balance.
        priorities[valid_bins] += 0.2 * bins_remain_cap[valid_bins] / np.max(bins_remain_cap)

    return priorities
```
