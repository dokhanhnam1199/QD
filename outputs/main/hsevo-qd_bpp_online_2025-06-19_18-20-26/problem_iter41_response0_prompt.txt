{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    A priority function for online bin packing that dynamically adjusts\n    its behavior based on the item size relative to the average remaining\n    bin capacity, incorporating best-fit, waste minimization, bin balancing,\n    and dynamic adjustment of weights to promote robustness.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap >= item\n    priorities[~valid_bins] = -np.inf\n\n    # Adaptive weighting based on item size relative to average bin capacity\n    avg_bin_capacity = np.mean(bins_remain_cap)\n    item_ratio = item / avg_bin_capacity\n\n    # Best-Fit component, scaled dynamically.\n    size_diff = bins_remain_cap - item\n    priorities[valid_bins] += (1.0 / (size_diff[valid_bins] + 0.0001)**2) * (1 + item_ratio)\n\n    # Waste Minimization: Discourage very small waste, dynamically penalized.\n    remaining_after_fit = bins_remain_cap[valid_bins] - item\n    waste_penalty_factor = 10 + 5 * item_ratio  # Higher penalty for larger items\n    small_waste_penalty = np.exp(-waste_penalty_factor * remaining_after_fit * item)\n    priorities[valid_bins] -= small_waste_penalty\n\n    # Bin Balancing using utilization and overall utilization: Dynamically adjusted\n    utilization = 1 - (bins_remain_cap / np.max(bins_remain_cap))\n    overall_utilization = np.mean(utilization)\n\n    # Dynamic Bin Balancing with sharper penalties\n    if overall_utilization < 0.3:\n        extreme_utilization_penalty = 8 * np.abs(utilization - 0.2) * (1 + item_ratio)  # Encourage filling more aggressively\n    elif overall_utilization > 0.7:\n        extreme_utilization_penalty = 20 * np.abs(utilization - 0.8) * (1 + item_ratio) #Discourage further filling more aggressively\n    else:\n        extreme_utilization_penalty = 12 * np.abs(utilization - 0.5) * (1 + item_ratio)  #Balance, more sensitive\n\n    priorities[valid_bins] -= extreme_utilization_penalty[valid_bins]\n\n    # Adaptive component: Bonus to bins close to full, dynamically adjusted.\n    fullness_bonus = np.exp(-5 * np.abs(bins_remain_cap / np.max(bins_remain_cap) - 0))\n    bonus_scaling = 2 * (1 - overall_utilization) * (1 + item_ratio)\n\n    if overall_utilization < 0.5:\n        priorities[valid_bins] += bonus_scaling * fullness_bonus[valid_bins]\n    elif overall_utilization < 0.8 :\n         priorities[valid_bins] +=  0.75 * bonus_scaling * fullness_bonus[valid_bins]  #Reduced bonus compared to v1\n    else:\n        priorities[valid_bins] += 0.25 * bonus_scaling * fullness_bonus[valid_bins] #further reduced bonus\n\n    # Edge Case Handling: Favor bins close to item size, but not too close to full capacity\n    close_to_item = np.abs(bins_remain_cap - item) / item\n    close_to_full = bins_remain_cap / np.max(bins_remain_cap)\n\n    edge_case_bonus = np.exp(-5 * close_to_item)  * (close_to_full > 0.1) # boost bins with remain capacity close to item and not close to full\n    priorities[valid_bins] += 0.5 * edge_case_bonus[valid_bins]\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n                bins_remain_cap: np.ndarray,\n                size_diff_epsilon: float = 0.0004976631192414565,\n                small_waste_penalty_factor: float = 13.759620087105052,\n                extreme_utilization_penalty_factor: float = 10.96255884116071,\n                target_utilization: float = 0.2781292500038949) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This priority function incorporates several heuristics:\n\n    1.  **First Fit Decreasing (FFD) Approximation:** Bins closer in size to the item\n        (but still large enough) get higher priority.  This approximates the FFD\n        algorithm which is known to be reasonably effective.\n\n    2.  **Waste Minimization:**  Penalizes bins where the item leaves a very small\n        remaining capacity (high waste). This is crucial in online scenarios where\n        future items might fit perfectly into larger waste spaces.\n\n    3.  **Bin Utilization Balance:** Bins that are very empty or almost full are\n        slightly penalized to encourage mid-range utilization, promoting a more\n        balanced distribution of items across bins. This aims to avoid creating\n        bins that are almost full with one item.\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        size_diff_epsilon: A small constant added to size_diff to avoid division by zero.\n        small_waste_penalty_factor: Factor to control the strength of the small waste penalty.\n        extreme_utilization_penalty_factor: Factor to control the strength of the extreme utilization penalty.\n        target_utilization: The target utilization level (fraction full) for balancing.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # 1. First Fit Decreasing Approximation (size proximity)\n    size_diff = np.abs(bins_remain_cap - item)\n    priorities = 1.0 / (size_diff + size_diff_epsilon)  # Avoid division by zero and give preference to bins closest to item size. Small constant added to size_diff\n\n    # Consider only bins with sufficient capacity\n    valid_bins = bins_remain_cap >= item\n    priorities[~valid_bins] = -np.inf\n\n    # 2. Waste Minimization Penalty\n    remaining_after_fit = bins_remain_cap - item\n    small_waste_penalty = np.exp(-small_waste_penalty_factor * remaining_after_fit)\n    priorities[valid_bins] -= small_waste_penalty[valid_bins]\n\n    # 3. Bin Utilization Balance Penalty (U-shaped utilization preference)\n    utilization = (1 - (bins_remain_cap / np.max(bins_remain_cap)))  # Fraction full, range [0, 1] where 1 is empty and 0 is full\n    extreme_utilization_penalty = extreme_utilization_penalty_factor * np.abs(utilization - target_utilization) # Penalize bins that have utilization far from 0.5 (half full)\n\n    priorities[valid_bins] -= extreme_utilization_penalty[valid_bins]\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the 1st incorporates best-fit, waste minimization, bin balancing, and adaptive fullness, while the 20th only uses a ratio of item size to bin capacity. (2nd best) vs (19th), the second best uses best-fit, waste minimization, dynamic bin balancing, and adaptive fullness bonus, while the 19th only uses FFD approximation and waste minimization. Comparing (1st) vs (2nd), we see similar functionality but the comments are more descriptive in the second heuristic. Comparing (3rd) vs (4th), both combine best-fit considerations with dynamic waste management, bin balancing, and learning, however, the third makes small adjustments to the best-fit to encourage more full bins. Comparing (second worst) vs (worst), the second worst implements waste minimization, first fit approximation, and bin balancing, while the worst one just implements a ratio. Overall: the better heuristics include more components such as best-fit, waste minimization, bin balancing and fullness, while the worse heuristics only consider a few components. Better heuristics use adaptive methods to dynamically adjust their weighting and parameters. Better heuristics include comprehensive edge case handling.\n- \nOkay, I understand. To redefine \"Current self-reflection\" for designing better bin packing heuristics, focusing on *avoiding* the pitfalls illustrated in the \"Ineffective self-reflection\" examples, we need a more targeted and insightful approach. Here's a breakdown:\n\n*   **Keywords:** *Systematic Exploration*, *Quantitative Evaluation*, *Iterative Refinement*, *Problem-Specific Adaptation*, *Performance Bottleneck Analysis*.\n\n*   **Advice:** Focus on designing experiments to *quantitatively* measure the impact of each component of your heuristic. Then prioritize adapting components that most improve performance. Design explicit tests to identify weakness and improve that part.\n\n*   **Avoid:** Vague statements about \"combining factors,\" \"adaptive parameters,\" or \"handling edge cases\" without specific plans for implementation, evaluation, or iteration. Also, avoid focusing on \"multiple relevant factors\". Also, *avoid* intuitive judgements over empirical data.\n\n*   **Explanation:** Instead of generally saying \"use adaptive parameters,\" define *how* the parameters will be adapted (e.g., a specific formula based on bin fill ratios), *why* that specific adaptation is expected to improve performance (a hypothesis), and *how* you will measure whether the adaptation actually achieves its goal (a controlled experiment).\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}