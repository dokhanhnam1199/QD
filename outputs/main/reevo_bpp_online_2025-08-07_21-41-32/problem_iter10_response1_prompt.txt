{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n\n    item: float,\n    bins_remain_cap: np.ndarray,\n    low_percentile: float = 0.10,\n    boost_factor: float = 5.0,\n    random_state: Optional[int] = None,\n) -> np.ndarray:\n    \"\"\"\n    Priority function for the online Bin Packing Problem (BPP).\n\n    For each existing bin a score is computed; the bin with the highest score\n    is selected for the incoming ``item``.  The heuristic combines several\n    ideas:\n\n    1. **Inverse\u2011slack bias** \u2013 bins that would have less remaining capacity\n       after placement receive a larger base score.\n    2. **Low\u2011percentile slack boost** \u2013 bins whose post\u2011placement slack falls\n       below ``low_percentile`` (default 10\u202f%) are boosted, encouraging the\n       algorithm to fill \u201calmost full\u2019\u2019 bins.\n    3. **Adaptive weighting** \u2013 the relative weight of the bias and the boost\n       adapts to the current packing situation: when few bins can accommodate\n       the item, the inverse\u2011slack bias dominates; when many bins are feasible,\n       the boost gets a larger share.\n    4. **Normalization** \u2013 deterministic components are normalised to the\n       interval ``[0, 1]`` before blending, preserving ordering while keeping\n       scores comparable across instances.\n    5. **Tiny random tie\u2011breaker** \u2013 a minute random perturbation (\u22481e\u20118) breaks\n       ties without affecting the deterministic ordering.\n\n    Infeasible bins (those that cannot accommodate the item) receive\n    ``-np.inf`` guaranteeing they are never chosen.\n\n    Parameters\n    ----------\n    item : float\n        Size of the incoming item.\n    bins_remain_cap : np.ndarray\n        1\u2011D array of remaining capacities for each bin.\n    low_percentile : float, optional\n        Percentile (in [0, 1]) used to define \u201calmost full\u2019\u2019 bins.\n        Default is ``0.10`` (10\u202f%).\n    boost_factor : float, optional\n        Multiplier controlling the magnitude of the low\u2011percentile boost.\n        Default is ``5.0``.\n    random_state : int or None, optional\n        Seed for reproducible random tie\u2011breaking.\n\n    Returns\n    -------\n    np.ndarray\n        1\u2011D array of priority scores (higher is better). Infeasible bins are\n        marked with ``-np.inf``.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # 0) Pre\u2011processing & feasibility mask\n    # ------------------------------------------------------------------\n    caps = np.asarray(bins_remain_cap, dtype=float).ravel()\n    n_bins = caps.size\n\n    # Edge case: no bins at all\n    if n_bins == 0:\n        return np.array([], dtype=float)\n\n    # Slack after hypothetically placing the item\n    slack = caps - item\n    feasible = slack >= 0\n\n    # Initialise all scores as -inf (infeasible)\n    priorities = np.full(n_bins, -np.inf, dtype=float)\n\n    # Early exit if nothing fits\n    if not feasible.any():\n        return priorities\n\n    # ------------------------------------------------------------------\n    # 1) Inverse\u2011slack bias (tight\u2011fit component)\n    # ------------------------------------------------------------------\n    slack_feas = slack[feasible]\n\n    # Scale epsilon with typical slack magnitude to avoid division\u2011by\u2011zero\n    mean_slack = np.mean(slack_feas)\n    eps = max(1e-12, 1e-6 * mean_slack)\n\n    inv_slack = 1.0 / (slack_feas + eps)          # larger when slack is smaller\n\n    # Normalise to [0, 1]\n    inv_slack_norm = inv_slack / (inv_slack.max() + eps)\n\n    # ------------------------------------------------------------------\n    # 2) Low\u2011percentile slack boost\n    # ------------------------------------------------------------------\n    low_percentile = np.clip(low_percentile, 0.0, 1.0)\n\n    if slack_feas.size > 0:\n        threshold = np.quantile(slack_feas, low_percentile)\n    else:\n        threshold = 0.0\n\n    boost = np.zeros_like(slack_feas)\n    low_mask = slack_feas <= threshold\n    boost[low_mask] = boost_factor * (threshold - slack_feas[low_mask])\n\n    # Normalise boost (if any boost is non\u2011zero)\n    if boost.max() > eps:\n        boost_norm = boost / (boost.max() + eps)\n    else:\n        boost_norm = boost  # all zeros\n\n    # ------------------------------------------------------------------\n    # 3) Adaptive weighting between bias and boost\n    # ------------------------------------------------------------------\n    feasible_frac = feasible.sum() / n_bins  # \u2208 (0, 1]\n    # Inverse\u2011slack gets more weight when few bins are feasible,\n    # boost gets more weight when many bins are feasible.\n    w_inv = 1.0 - feasible_frac\n    w_boost = feasible_frac\n\n    # Clamp weights to avoid domination of a single term\n    w_inv = np.clip(w_inv, 0.1, 0.9)\n    w_boost = 1.0 - w_inv\n\n    # Weighted combination (still in [0, 1] because components are normalised)\n    combined = w_inv * inv_slack_norm + w_boost * boost_norm\n\n    # ------------------------------------------------------------------\n    # 4) Normalisation of the combined score\n    # ------------------------------------------------------------------\n    combined_norm = combined / (combined.max() + eps)\n\n    # ------------------------------------------------------------------\n    # 5) Tiny random tie\u2011breaker\n    # ------------------------------------------------------------------\n    rng = np.random.default_rng(random_state)\n    tie_eps = 1e-8\n    jitter = tie_eps * rng.random(combined_norm.shape[0])\n\n    final_score = combined_norm + jitter\n\n    # ------------------------------------------------------------------\n    # 6) Write back to full priorities array\n    # ------------------------------------------------------------------\n    priorities[feasible] = final_score\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Random Fit priority for online bin packing.\n\n    Each feasible bin (with enough remaining capacity) receives a random score.\n    Infeasible bins receive -inf so they are never selected. This implements a\n    pure random fit strategy, optionally biased toward tighter fits.\n\n    Args:\n        item: Size of the incoming item.\n        bins_remain_cap: 1\u2011D array of remaining capacities for each bin.\n\n    Returns:\n        A 1\u2011D array of priority scores, one per bin.\n    \"\"\"\n    # Ensure capacities are a float array.\n    caps = np.asarray(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item.\n    feasible = caps >= item\n\n    # Initialise all priorities to -inf (so infeasible bins are never chosen).\n    priorities = np.full_like(caps, -np.inf, dtype=float)\n\n    # Generate independent random numbers for each bin.\n    rand_scores = np.random.rand(caps.size)\n\n    # Bias towards tighter fits: less slack -> larger bias.\n    # Small epsilon avoids division by zero for exact fits.\n    epsilon = 1e-12\n    slack = caps - item\n    bias = np.zeros_like(caps)\n    bias[feasible] = 1.0 / (slack[feasible] + epsilon)  # higher when slack is small\n\n    # Combine random component with bias.\n    # Multiplying emphasizes bins with higher bias while preserving randomness.\n    priorities[feasible] = bias[feasible] * rand_scores[feasible]\n\n    return priorities\n\n[Reflection]\nKeep priority calculation simple, add mild bias toward tight fit, avoid heavy normalization, use tie\u2011breaking, test on diverse instances.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}