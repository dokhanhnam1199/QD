{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins: exact fits highest, then closest fits based on normalized inverse excess.\n\n    Combines Exact Fit preference with a scaled Best Fit strategy.\n    Exact fits receive the highest priority. Other fitting bins are prioritized\n    based on the inverse of their normalized excess capacity after placement.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        fitting_bins_cap = bins_remain_cap[can_fit_mask]\n        fitting_bins_indices = np.where(can_fit_mask)[0]\n\n        # Highest priority for exact fits\n        exact_fit_mask = fitting_bins_cap == item\n        priorities[fitting_bins_indices[exact_fit_mask]] = 2.0\n\n        # Medium priority for non-exact fits, based on how close they are\n        non_exact_fitting_bins_cap = fitting_bins_cap[~exact_fit_mask]\n        non_exact_fitting_bins_indices = fitting_bins_indices[~exact_fit_mask]\n\n        if len(non_exact_fitting_bins_cap) > 0:\n            space_after_placement = non_exact_fitting_bins_cap - item\n\n            # Normalize differences to [0, 1] range for scoring\n            min_diff = np.min(space_after_placement)\n            max_diff = np.max(space_after_placement)\n\n            if max_diff - min_diff > 1e-9:\n                normalized_diff = (space_after_placement - min_diff) / (max_diff - min_diff)\n            else:\n                normalized_diff = np.zeros_like(space_after_placement) # All are same diff\n\n            # Assign priorities: inverse relationship with normalized difference, scaled\n            # Give higher priority to smaller differences (closer fits)\n            # Scale to be less than exact fit priority (e.g., 0.1 to 1.1)\n            priorities[non_exact_fitting_bins_indices] = 1.1 - normalized_diff\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    if np.any(can_fit_mask):\n        available_caps = bins_remain_cap[can_fit_mask]\n        \n        # Sigmoid-like function: higher priority for bins that are \"closer\" to fitting the item\n        # but not too close to waste significant space.\n        # We want to favor bins where item fills a significant portion of remaining capacity\n        # but leaves a reasonable amount.\n        \n        # Normalized remaining capacity relative to bin size if we were to fit the item\n        # Higher value means more wasted space if item is put in this bin\n        wasted_space_ratio = (available_caps - item) / available_caps\n        \n        # Apply sigmoid to map the ratio to a [0, 1] range, then invert it.\n        # We want to penalize large wasted_space_ratio, so we use 1 - sigmoid(x).\n        # A small positive constant 'k' is used to control the steepness of the sigmoid.\n        k = 5.0 \n        sigmoid_values = 1 / (1 + np.exp(-k * (wasted_space_ratio - 0.5)))\n        \n        # Invert the sigmoid: prioritize bins with lower wasted space ratio (closer fit)\n        # but avoid extremely tight fits by not making the priority too close to 1.\n        # A simple inversion (1 - sigmoid) might over-prioritize near-perfect fits.\n        # Let's try to emphasize the middle ground.\n        \n        # A different approach: penalize bins that are too large and too small for the item\n        # We want to find a bin that is \"just right\"\n        \n        # Let's define \"goodness\" of fit as how close the remaining capacity is to the item size.\n        # Normalize this difference.\n        \n        # Score: inversely proportional to the difference between remaining capacity and item size.\n        # But also, if remaining capacity is much larger than item size, it's not good.\n        \n        # Let's use the negative of the wasted space ratio as a base, which favors tighter fits.\n        # Then apply a sigmoid-like transform that peaks when wasted_space_ratio is around 0.\n        \n        # For each bin that can fit the item, calculate a score.\n        # Score = sigmoid( k * ( (available_caps - item) / AVAILABLE_CAP_SUM - 0.5) )\n        # Where AVAILABLE_CAP_SUM is the sum of capacities of bins that can fit the item.\n        # This might still be tricky.\n        \n        # Let's stick to a simple approach:\n        # Prioritize bins where the remaining capacity is just enough or slightly more than the item.\n        # A good heuristic: the inverse of the squared difference between remaining capacity and item size.\n        # However, this doesn't account for the \"too much space\" problem.\n        \n        # Let's try a sigmoid centered around the ideal fit (remaining_cap = item).\n        # The closer remaining_cap is to item, the higher the score.\n        \n        # Calculate a 'fit_quality' score for each bin that can fit the item.\n        # We want to penalize bins where `available_caps` is much larger than `item`.\n        # A possible metric: `item / available_caps`. This favors bins that are closer to being full if the item is added.\n        # Values are between 0 and 1. Higher means better fit (less wasted space).\n        fit_quality = item / available_caps\n        \n        # Apply a sigmoid to the fit_quality.\n        # Higher fit_quality means the bin is \"more full\" if the item is added.\n        # We want to map high fit_quality to high priority.\n        # Sigmoid(k * (fit_quality - threshold))\n        # Threshold can be around 0.5 for example, meaning we prefer bins where item takes up ~50% of remaining cap.\n        # A steeper sigmoid will favor very close fits.\n        \n        k_sigmoid = 10.0\n        threshold = 0.7 # Favor bins where item takes up more than 70% of remaining capacity\n        \n        scores_for_fitting_bins = 1 / (1 + np.exp(-k_sigmoid * (fit_quality - threshold)))\n        \n        priorities[can_fit_mask] = scores_for_fitting_bins\n        \n    return priorities\n\n### Analyze & experience\n- *   **Heuristics 1 vs 2:** Heuristic 1 (`priority_v2`) directly assigns a very high score (`item * 1e10`) to exact fits and uses `bins_remain_cap / (diff + epsilon)` for other fits, providing a clear hierarchy and prioritizing tight fits. Heuristic 2 uses a normalized inverse of remaining capacity after placement but scales it by `0.1`, which might dilute the \"tight fit\" preference and its combination with the base priority of `1.0` is less structured than Heuristic 1's explicit high score for exact fits. Heuristic 1 is better because its scoring is more direct and impactful for exact fits.\n*   **Heuristics 2 vs 3:** Heuristic 2 prioritizes bins with just enough capacity and normalizes these scores, creating a relative ranking of \"tightness.\" Heuristic 3 uses `1.0 / (available_bins_remain_cap - item + 1e-9)` and then ranks these scores using `argsort(argsort(...))`. While both aim for tight fits, Heuristic 2's normalization and explicit \"can fit\" priority feels more robust than the double `argsort` which can be sensitive to the distribution of differences. Heuristic 2 is better due to its more interpretable normalization.\n*   **Heuristics 3 vs 4:** Heuristic 3 uses `1 / (diff + epsilon)` and then ranks the scores. Heuristic 4 explicitly separates exact fits (given `1e9`) and then scales `bins_remain_cap / (diff + epsilon)` for close fits. Heuristic 4 provides a clearer separation of priorities (exact > close > none) and uses a scoring that is more directly related to the remaining capacity. Heuristic 4 is better for its explicit prioritization tiers.\n*   **Heuristics 4 vs 5:** Heuristic 4 prioritizes exact fits with `1e9` and scales `bins_remain_cap / (diff + epsilon)` for close fits. Heuristic 5 is identical to Heuristic 3, using `1 / (diff + epsilon)` and `argsort(argsort(...))`. Heuristic 4 is superior due to its explicit handling of exact fits and more principled scoring for close fits.\n*   **Heuristics 5 vs 6:** Heuristic 5 is essentially the same as Heuristic 3. Heuristic 6 uses `1.0 / (available_bins_cap + 1e-9)` and normalizes it. This approach prioritizes bins that will be *more full* after placement, which is similar to tight fitting but expressed differently. Heuristic 5/3's `1 / (diff + epsilon)` is a more direct measure of tightness. Heuristic 5/3 is slightly better for its directness in measuring excess space.\n*   **Heuristics 6 vs 7:** Heuristic 6 normalizes `1.0 / (available_bins_cap + 1e-9)`. Heuristic 7 assigns `2.0` to exact fits and `1.1 - normalized_diff` to close fits. Heuristic 7's explicit separation of exact fits (priority `2.0`) and then a structured approach for close fits (higher for smaller `diff`) is more robust and hierarchical than Heuristic 6's single normalized score. Heuristic 7 is better.\n*   **Heuristics 7 vs 8:** Heuristic 7 prioritizes exact fits (`2.0`) and then uses `1.1 - normalized_diff` for close fits. Heuristic 8 uses `-remaining_after_packing` and then shifts/adds epsilon. This effectively prioritizes bins with the smallest non-negative `remaining_after_packing`. While similar in goal to Heuristic 7's close fit strategy, Heuristic 7's explicit handling of exact fits with a higher score is more defined. Heuristic 7 is better for its clear tiers.\n*   **Heuristics 8 vs 9:** Heuristics 8 and 9 are identical. They use negative remaining capacity after packing and then shift to make it positive, effectively prioritizing the tightest fits.\n*   **Heuristics 9 vs 10:** Heuristic 9 uses `-remaining_after_packing` and shifts. Heuristic 10 assigns `1.0` to exact fits and scales `1.0 / (space_after_placement + epsilon)` to `[0.5, 0.99]` for close fits. Heuristic 10 provides a clearer hierarchy (exact > close > none) and scales the secondary priority to avoid overlapping with exact fits. Heuristic 10 is better.\n*   **Heuristics 10 vs 11:** Heuristic 10 uses explicit tiers: exact fits (`1.0`), scaled inverse distance (`[0.5, 0.99]`). Heuristic 11 iterates through bins, assigning `1.0 / (bins_remain_cap[i] - item + 1e-9)`. Heuristic 10's structured approach with clear priority levels is superior to Heuristic 11's simple, unscaled inverse difference which doesn't explicitly handle exact fits separately.\n*   **Heuristics 11 vs 12:** Heuristic 11 uses `1.0 / (diff + epsilon)` per bin. Heuristic 12 assigns `1.0` to exact fits and `0.5 + 0.45 * (1.0 / (normalized_excess + 1e-9))` to close fits, capped at `0.99`. Heuristic 12's explicit prioritization of exact fits and structured scaling for close fits makes it better.\n*   **Heuristics 12 vs 13:** Heuristic 12 assigns `1.0` to exact fits and scaled `1 / (normalized_excess + 1e-9)` to close fits. Heuristic 13 assigns `1.0` to exact fits and then `normalized_priorities * 0.9` to close fits. Both aim for similar goals. Heuristic 12's `0.5 + 0.45 * ...` scaling provides a more controlled range for close fits. Heuristic 12 is slightly better for its more refined scaling.\n*   **Heuristics 13 vs 14:** Heuristics 13 and 14 are identical.\n*   **Heuristics 14 vs 15:** Heuristic 14 assigns `1.0` to exact fits and scales close fits to `[0.5, 0.99]`. Heuristic 15 assigns `2.0` to exact fits and scales close fits to `[0.1, 1.0]`. Heuristic 15 provides a higher explicit score for exact fits (`2.0` vs `1.0`) and a reasonable range for close fits, making its hierarchy clearer. Heuristic 15 is better.\n*   **Heuristics 15 vs 16:** Heuristics 15 and 16 are identical.\n*   **Heuristics 16 vs 17:** Heuristic 16 assigns `1.0` to exact fits and scales close fits to `[0.1, 1.0]`. Heuristic 17 uses `1 / (1 + exp(-k * (wasted_space_ratio - 0.5)))` for bins that fit. Heuristic 17's sigmoid approach is less direct and potentially more complex to tune than Heuristic 16's explicit scoring and normalization. Heuristic 16 is better for its clarity and robustness.\n*   **Heuristics 17 vs 18:** Heuristic 17 uses a sigmoid on `item / available_caps`. Heuristic 18 is identical to Heuristic 16. Heuristic 18 is better due to clearer priority separation.\n*   **Heuristics 18 vs 19:** Heuristics 18 and 19 are identical.\n*   **Heuristics 19 vs 20:** Heuristic 19 assigns `2.0` to exact fits and scales close fits to `[0.1, 1.0]`. Heuristic 20 assigns `1.0` to exact fits and ranks non-exact fits into `[0.1, 0.9]`. Heuristic 19's higher explicit score for exact fits (`2.0`) makes its hierarchical preference stronger. Heuristic 19 is better.\n*   **Overall:** The best heuristics clearly prioritize exact fits with a distinctively high score, then use a well-defined strategy (often normalized inverse excess capacity) for \"close\" fits, ensuring these scores are lower than exact fits. Heuristics that directly implement a hierarchy and use robust scoring for tiers are better.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Exact fit, scaled closeness, interpretability, robustness, vectorization.\n*   **Advice:** Design a multi-tiered scoring system. Assign the highest scores to exact fits. For near-fits, use a scaled inverse of *normalized* excess capacity, ensuring scores decrease monotonically with increasing excess capacity. Keep scoring functions simple and directly tied to fit quality.\n*   **Avoid:** Complex, non-monotonic, or highly sensitive non-linear scoring functions. Avoid manual iteration over array elements; leverage vectorization. Do not normalize without a clear rationale, as over-normalization can obscure differences.\n*   **Explanation:** This approach balances the strong preference for exact fits with a robust, interpretable method for ranking near-fits, while promoting efficient, maintainable code.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}