{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            priorities[i] = 1.0 / (bins_remain_cap[i] - item + 1e-9)\n        else:\n            priorities[i] = 0.0\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins by favoring exact fits, then closest fits, using a smooth scoring.\n\n    Combines Exact Fit First and Inverse Distance strategies:\n    1. High priority for bins where remaining_capacity == item.\n    2. Medium priority for bins where remaining_capacity > item, inversely\n       proportional to the difference (remaining_capacity - item).\n    3. Zero priority for bins where remaining_capacity < item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n\n    if len(fitting_bins_cap) > 0:\n        # Prioritize exact fits with a high score\n        exact_fit_mask = fitting_bins_cap == item\n        priorities[fitting_bins_indices[exact_fit_mask]] = 2.0  # Highest priority\n\n        # For bins that are not exact fits, calculate priority based on how close they are\n        non_exact_fitting_bins_cap = fitting_bins_cap[~exact_fit_mask]\n        non_exact_fitting_bins_indices = fitting_bins_indices[~exact_fit_mask]\n\n        if len(non_exact_fitting_bins_cap) > 0:\n            # Calculate the difference (space after placing the item)\n            space_after_placement = non_exact_fitting_bins_cap - item\n\n            # Normalize these differences to a [0, 1] range for scoring.\n            # Smaller difference means higher priority.\n            min_diff = np.min(space_after_placement)\n            max_diff = np.max(space_after_placement)\n\n            if max_diff - min_diff > 1e-9: # Avoid division by zero if all differences are the same\n                normalized_diff = (space_after_placement - min_diff) / (max_diff - min_diff)\n            else:\n                normalized_diff = np.zeros_like(space_after_placement)\n            \n            # Assign priorities: inverse relationship with normalized difference, scaled down.\n            # This gives medium priority, lower than exact fits.\n            # Add a small offset to ensure it's greater than 0 for fitting bins.\n            priorities[non_exact_fitting_bins_indices] = 1.0 - normalized_diff + 0.1\n\n    return priorities\n\n### Analyze & experience\n- *   **Comparing Heuristic 1 (Priority v2) vs. Heuristic 2 (Priority v2):**\n    *   Heuristic 1 attempts to normalize the \"tightness\" of the fit by calculating `(bins_remain_cap - item)`, finding min/max differences, and then normalizing. It then inverts this normalized score to give higher priority to tighter fits. It also adds a small boost (+ 0.1) to these scores to differentiate them from bins that cannot fit.\n    *   Heuristic 2 calculates `diffs = eligible_capacities - item`, finds the `min_diff`, and then assigns priority as `1.0 / (diffs - min_diff + 1e-9)`. This also aims to prioritize tighter fits by giving higher scores to smaller differences relative to the minimum difference.\n    *   **Observation:** Both heuristics aim to prioritize bins with less excess capacity. Heuristic 1's normalization is more complex, potentially more stable across different scales of differences, but might be overkill. Heuristic 2 is simpler and directly emphasizes the bins closest to the minimum excess.\n\n*   **Comparing Heuristic 3 (Exact Fit First) vs. Heuristic 4 (Priority v2):**\n    *   Heuristic 3 explicitly prioritizes bins that *exactly* fit (`bins_remain_cap == item`) with a score of `1.0`. If no exact fit exists, it falls back to prioritizing bins with the least remaining capacity that *can* fit, using an inverse of the excess capacity, normalized.\n    *   Heuristic 4 is identical to Heuristic 2, focusing solely on inverse difference from minimum excess capacity for all fitting bins, without an explicit \"exact fit\" priority level.\n    *   **Observation:** Heuristic 3 provides a distinct, higher priority for exact fits, which is a common and effective strategy in bin packing. Heuristic 4 treats exact fits the same as other close fits, potentially overlooking the benefit of perfect utilization.\n\n*   **Comparing Heuristic 5 (Inverse Distance) vs. Heuristic 8 (Loop-based Inverse Distance):**\n    *   Heuristic 5 calculates `space_after_placement = bins_remain_cap - item`, assigns `1 / (space_after_placement + epsilon)` for fitting bins, and `-np.inf` for non-fitting bins. This directly favors bins with minimal positive `space_after_placement`.\n    *   Heuristic 8 uses a loop to iterate through bins, achieving the same logic: `1.0 / (bins_remain_cap[i] - item + 1e-9)` for fitting bins, `0.0` otherwise.\n    *   **Observation:** Heuristic 5 is more concise and leverages NumPy's vectorized operations, making it more efficient and Pythonic than the explicit loop in Heuristic 8. Both implement the same core \"inverse distance\" or \"best fit\" logic.\n\n*   **Comparing Heuristic 9 (Exact Fit + Scaled Inverse Distance) vs. Heuristic 14 (Exact Fit + Scaled Inverse Difference):**\n    *   Heuristic 9 assigns `1.0` for exact fits. For non-exact fits, it calculates `inverse_distance_scores = 1.0 / (space_after_placement + epsilon)`, normalizes these scores to a range of `[0.5, 0.99]`, ensuring they are lower than exact fits.\n    *   Heuristic 14 assigns `2.0` for exact fits (even higher priority). For non-exact fits, it calculates `space_after_placement`, normalizes it (`1.0 - normalized_diff`), and adds `0.1`, resulting in scores between `0.1` and `1.1` (before considering the `2.0` for exact fits). The scaling is different: Heuristic 9 scales the inverse of the difference, while Heuristic 14 scales the normalized difference itself (then inverts).\n    *   **Observation:** Both combine exact fits with a \"best fit\" approach. Heuristic 14's use of `2.0` for exact fits creates a clearer hierarchy. Its normalization `1.0 - normalized_diff + 0.1` aims to prioritize smaller differences. Heuristic 9's approach of scaling inverse distance to `[0.5, 0.99]` is also a reasonable way to create a distinct priority band for close fits.\n\n*   **Comparing Heuristic 16/17/18 (Sigmoid Fit Score) vs. Heuristic 19 (Exact Fit + Best Fit):**\n    *   Heuristics 16-18 use a sigmoid function applied to `item / available_caps` with a threshold (`0.7`) and steepness (`10.0`). This aims to favor bins where the item takes up a significant portion of the remaining capacity, but not necessarily the absolute tightest fit. It penalizes bins that are too large.\n    *   Heuristic 19 prioritizes exact fits (`1.0`), then uses normalized inverse of space after placement for non-exact fits, scaled to `[0, 0.9]`. This focuses on minimal excess space for non-exact fits.\n    *   **Observation:** The sigmoid approach (16-18) is more complex and might be trying to find a balance point, not just the absolute closest fit. The prioritization based on `item / available_caps` is an interesting way to avoid bins that are *too* large. Heuristic 19 is a more direct combination of exact and best fit.\n\n*   **Comparing Heuristic 2 (Inverse Distance) vs. Heuristic 8 (Loop-based Inverse Distance):**\n    *   Heuristic 2: `priorities[eligible_bins] = 1.0 / (diffs - min_diff + 1e-9)`\n    *   Heuristic 8: `priorities[i] = 1.0 / (bins_remain_cap[i] - item + 1e-9)`\n    *   **Observation:** Heuristic 2 is superior because it normalizes the inverse distance relative to the minimum difference. This prevents bins with extremely large capacities (but still fitting) from getting disproportionately high scores simply because their raw difference `(capacity - item)` is large, even if it's closer to the minimum difference than others. Heuristic 8's scores can be highly skewed by large capacities.\n\n*   **Comparing Heuristic 1 (Normalized Tight Fit) vs. Heuristic 2 (Inverse Difference from Min Diff):**\n    *   Heuristic 1 attempts to normalize the `(capacity - item)` difference to a `[0, 1]` range and then applies `1.0 - normalized_tight_fit`. It also adds `0.1` and a base `1.0` for fitting bins.\n    *   Heuristic 2 uses `1.0 / (diffs - min_diff + 1e-9)`.\n    *   **Observation:** Heuristic 2 is simpler and more direct in prioritizing the *smallest* differences relative to the minimum difference. Heuristic 1's normalization is more involved and the additive `0.1` seems less principled than a multiplicative scaling or direct inverse.\n\n*   **Overall:** Heuristics that explicitly prioritize exact fits (like 3, 9, 14, 19, 20) generally perform better because perfect utilization is a strong objective. Among those that focus on \"best fit\" (minimal excess capacity), prioritizing the inverse of the *difference from the minimum difference* (like Heuristic 2, 4) is more robust than simply taking the inverse of the difference (like Heuristic 8). Vectorized operations (like 1, 2, 5) are preferred over loops (like 8). Complex sigmoid functions (16-18) are less interpretable and may not offer clear advantages over simpler \"best fit\" or \"exact fit\" strategies without empirical validation.\n- \nHere's a redefined \"Current self-reflection\" for designing better heuristics, focusing on actionable improvements:\n\n*   **Keywords:** Robustness, Simplicity, Efficiency, Interpretability.\n*   **Advice:** Develop scoring functions that are robust to capacity variations (e.g., by normalizing differences) and prioritize simple, interpretable metrics. Leverage vectorization for computational efficiency.\n*   **Avoid:** Overly complex, non-linear scoring functions requiring extensive tuning and obscure array manipulations.\n*   **Explanation:** Focusing on robust, simple metrics and efficient implementation leads to more reliable, maintainable, and adaptable heuristics, crucial for dynamic optimization problems.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}