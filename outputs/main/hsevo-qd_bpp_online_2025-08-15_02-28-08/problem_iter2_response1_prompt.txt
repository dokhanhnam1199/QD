{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit strategy.\n\n    In First Fit, the item is placed in the first bin that has enough remaining capacity.\n    This heuristic prioritizes bins that can accommodate the item and gives higher\n    priority to bins that have just enough capacity to avoid fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    # Assign a high priority to bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = 1.0\n\n    # Among those that can fit, prioritize bins that have just enough capacity.\n    # This is a greedy approach to minimize wasted space in the selected bin.\n    # We can use the inverse of the remaining capacity minus the item size as a measure\n    # of how \"tight\" the fit is. Smaller difference means higher priority.\n    tight_fit_scores = np.where(can_fit_mask, bins_remain_cap - item, np.inf)\n    \n    # Normalize the tight fit scores to avoid overly large or small values.\n    # Add a small epsilon to avoid division by zero if all differences are the same.\n    min_tight_fit = np.min(tight_fit_scores[tight_fit_scores != np.inf]) if np.any(tight_fit_scores != np.inf) else 0\n    max_tight_fit = np.max(tight_fit_scores[tight_fit_scores != np.inf]) if np.any(tight_fit_scores != np.inf) else 1\n    \n    if max_tight_fit - min_tight_fit > 1e-9: # Avoid division by zero if all are the same\n        normalized_tight_fit = (tight_fit_scores - min_tight_fit) / (max_tight_fit - min_tight_fit)\n    else:\n        normalized_tight_fit = np.zeros_like(tight_fit_scores)\n    \n    # Invert to give higher priority to smaller differences (tighter fits)\n    inverted_normalized_tight_fit = 1.0 - normalized_tight_fit\n    \n    # Combine the \"can fit\" priority with the \"tight fit\" priority.\n    # We want to boost bins that fit and then order them by tightness.\n    # The \"+ 0.1\" ensures that bins that can fit are always prioritized over those that cannot,\n    # even if their tight fit score is very high (which shouldn't happen if they can't fit).\n    priorities = np.where(can_fit_mask, 1.0 + inverted_normalized_tight_fit * 0.1, 0)\n    \n    # Ensure that bins that cannot fit have a priority of 0\n    priorities[~can_fit_mask] = 0\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    if np.any(can_fit_mask):\n        available_caps = bins_remain_cap[can_fit_mask]\n        \n        # Sigmoid-like function: higher priority for bins that are \"closer\" to fitting the item\n        # but not too close to waste significant space.\n        # We want to favor bins where item fills a significant portion of remaining capacity\n        # but leaves a reasonable amount.\n        \n        # Normalized remaining capacity relative to bin size if we were to fit the item\n        # Higher value means more wasted space if item is put in this bin\n        wasted_space_ratio = (available_caps - item) / available_caps\n        \n        # Apply sigmoid to map the ratio to a [0, 1] range, then invert it.\n        # We want to penalize large wasted_space_ratio, so we use 1 - sigmoid(x).\n        # A small positive constant 'k' is used to control the steepness of the sigmoid.\n        k = 5.0 \n        sigmoid_values = 1 / (1 + np.exp(-k * (wasted_space_ratio - 0.5)))\n        \n        # Invert the sigmoid: prioritize bins with lower wasted space ratio (closer fit)\n        # but avoid extremely tight fits by not making the priority too close to 1.\n        # A simple inversion (1 - sigmoid) might over-prioritize near-perfect fits.\n        # Let's try to emphasize the middle ground.\n        \n        # A different approach: penalize bins that are too large and too small for the item\n        # We want to find a bin that is \"just right\"\n        \n        # Let's define \"goodness\" of fit as how close the remaining capacity is to the item size.\n        # Normalize this difference.\n        \n        # Score: inversely proportional to the difference between remaining capacity and item size.\n        # But also, if remaining capacity is much larger than item size, it's not good.\n        \n        # Let's use the negative of the wasted space ratio as a base, which favors tighter fits.\n        # Then apply a sigmoid-like transform that peaks when wasted_space_ratio is around 0.\n        \n        # For each bin that can fit the item, calculate a score.\n        # Score = sigmoid( k * ( (available_caps - item) / AVAILABLE_CAP_SUM - 0.5) )\n        # Where AVAILABLE_CAP_SUM is the sum of capacities of bins that can fit the item.\n        # This might still be tricky.\n        \n        # Let's stick to a simple approach:\n        # Prioritize bins where the remaining capacity is just enough or slightly more than the item.\n        # A good heuristic: the inverse of the squared difference between remaining capacity and item size.\n        # However, this doesn't account for the \"too much space\" problem.\n        \n        # Let's try a sigmoid centered around the ideal fit (remaining_cap = item).\n        # The closer remaining_cap is to item, the higher the score.\n        \n        # Calculate a 'fit_quality' score for each bin that can fit the item.\n        # We want to penalize bins where `available_caps` is much larger than `item`.\n        # A possible metric: `item / available_caps`. This favors bins that are closer to being full if the item is added.\n        # Values are between 0 and 1. Higher means better fit (less wasted space).\n        fit_quality = item / available_caps\n        \n        # Apply a sigmoid to the fit_quality.\n        # Higher fit_quality means the bin is \"more full\" if the item is added.\n        # We want to map high fit_quality to high priority.\n        # Sigmoid(k * (fit_quality - threshold))\n        # Threshold can be around 0.5 for example, meaning we prefer bins where item takes up ~50% of remaining cap.\n        # A steeper sigmoid will favor very close fits.\n        \n        k_sigmoid = 10.0\n        threshold = 0.7 # Favor bins where item takes up more than 70% of remaining capacity\n        \n        scores_for_fitting_bins = 1 / (1 + np.exp(-k_sigmoid * (fit_quality - threshold)))\n        \n        priorities[can_fit_mask] = scores_for_fitting_bins\n        \n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1 and 2: These are identical. They implement a \"First Fit\" strategy that prioritizes bins that can fit the item, with a secondary priority given to bins that offer a \"tight fit\" to minimize fragmentation. The normalization of \"tight fit scores\" is a good attempt to make the priorities relative.\n\nComparing Heuristics 3 and 4: These are identical. They use an \"Inverse Distance\" (Proximity Fit) approach, prioritizing bins where the remaining capacity is just enough or slightly more than the item. The use of `1 / (space_after_placement + epsilon)` directly rewards smaller positive differences. Setting priority to `-np.inf` for bins that don't fit is a strong negative signal.\n\nComparing Heuristics 5 and 8: These are identical. They seem to aim for an \"Almost Full Fit\" by sorting available bins and assigning priorities based on their inverse remaining capacity after placing the item, then re-ranking these priorities. The re-ranking with `np.argsort(np.argsort(...))` is unusual and might be intended to assign ranks based on the sorted inverse capacities.\n\nComparing Heuristics 6 and 9: These are identical. They implement an \"Exact Fit First\" heuristic. They give the highest priority to bins where the remaining capacity *exactly* matches the item size. If no exact fit exists, they fall back to prioritizing bins with the least excess capacity (closest to an exact fit) among those that can fit the item. Normalization of scores to [0, 1] is applied in the fallback case.\n\nComparing Heuristics 7 and the others: Heuristic 7 uses a \"Random Fit\" description but its implementation is closer to a \"Best Fit\" variant. It prioritizes bins that can fit the item and assigns priorities based on the inverse of the remaining capacity (after fitting), normalizing these values. This favors bins that will be more full after the item is placed.\n\nComparing Heuristics 10, 11, and 12: These are identical. They represent a basic \"Inverse Fit\" strategy. They iterate through bins, assigning a priority of `1.0 / (bins_remain_cap - item + 1e-9)` if the item fits, and 0 otherwise. This is a straightforward approach to favor tighter fits.\n\nComparing Heuristics 13 through 20: These are all identical and represent a \"Sigmoid Fit Score\" heuristic. They aim to use a sigmoid function to assign priorities. The current implementation calculates `item / available_caps` as a `fit_quality`, intending to favor bins where the item fills a larger portion of the remaining capacity. A sigmoid is then applied to this `fit_quality` with specific `k` and `threshold` parameters. Bins that cannot fit the item receive a priority of 0.\n\nOverall Comparison:\n- **Best:** Heuristics 1 & 2 (\"First Fit\" with tight fit consideration) and Heuristics 3 & 4 (\"Inverse Distance\" / Proximity Fit) seem to be well-reasoned strategies for minimizing waste. Heuristics 6 & 9 (\"Exact Fit First\") are also strong, directly targeting ideal fits.\n- **Middle:** Heuristics 10, 11, 12 (basic \"Inverse Fit\") are simple and effective. Heuristic 7's implementation (normalized inverse capacity) is also reasonable.\n- **Lower:** Heuristics 5 & 8's use of `argsort(argsort())` is unclear in its intent and likely suboptimal.\n- **Worst:** Heuristics 13-20 (\"Sigmoid Fit Score\") have a complex, potentially over-engineered approach. The choice of `item / available_caps` and the specific sigmoid parameters (`k=10`, `threshold=0.7`) might not generalize well and could be sensitive to the distribution of item and bin sizes. The description also seems contradictory, mentioning penalizing large wasted space and then favoring bins where the item fills a \"significant portion\" (which implies less wasted space but also potentially very tight fits).\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Simplicity, Intuitiveness, Measurability, Robustness.\n*   **Advice:** Focus on simple, linear or monotonic relationships that directly address core problem objectives. Ensure your heuristic's behavior is easily understood and its impact on key metrics can be directly measured. Prioritize solutions that are less sensitive to parameter tuning and edge cases.\n*   **Avoid:** Overly complex, non-monotonic, or \"black box\" logic. Unnecessary algorithmic overhead. Heuristics that require extensive data fitting or are brittle to slight input variations.\n*   **Explanation:** Effective heuristics are transparent and predictable. By favoring straightforward logic and clear objectives, you create more robust and maintainable solutions that are easier to debug and improve over time, ultimately leading to better optimization outcomes.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}