{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    eligible_bins = bins_remain_cap >= item\n    eligible_capacities = bins_remain_cap[eligible_bins]\n\n    if eligible_capacities.size > 0:\n        diffs = eligible_capacities - item\n        min_diff = np.min(diffs)\n        priorities[eligible_bins] = 1.0 / (diffs - min_diff + 1e-9)\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins that offer an exact fit or the smallest remaining capacity after placement.\n\n    Combines 'Exact Fit First' and 'Inverse Distance' strategies for a robust approach.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n        # Strategy 1: Exact Fit (Highest Priority)\n        exact_fit_mask = available_bins_remain_cap == item\n        if np.any(exact_fit_mask):\n            priorities[can_fit_mask][exact_fit_mask] = 1.0\n            # If there are exact fits, we only consider them for ranking (effectively)\n            # but to allow other bins to have non-zero priority if needed, we proceed.\n            # However, for simplicity and clear hierarchy, we could return here if only exact fits are desired as the sole choice.\n            # For a more nuanced approach, we allow other bins to compete if they are \"close\".\n\n        # Strategy 2: Proximity Fit (Inverse of remaining capacity after placement)\n        # Assigns higher priority to bins that leave less space after placing the item.\n        # Avoids division by zero by adding a small epsilon.\n        space_after_placement = available_bins_remain_cap - item\n        \n        # We want to prioritize bins with smaller space_after_placement.\n        # Using 1 / (space_after_placement + epsilon) favors smaller positive differences.\n        # For bins where space_after_placement is 0 (exact fit), this will be 1/epsilon (very high).\n        # To avoid extremely high values for exact fits that might dominate too much,\n        # and to ensure proximity fits are ranked meaningfully, we can cap or scale.\n        # A simple approach is to ensure exact fits get priority 1.0 and then rank others.\n\n        # Let's refine: assign 1.0 to exact fits, and then rank the rest based on inverse remaining capacity.\n        \n        non_exact_fit_mask = ~exact_fit_mask\n        if np.any(non_exact_fit_mask):\n            non_exact_available_caps = available_bins_remain_cap[non_exact_fit_mask]\n            non_exact_space_after_placement = non_exact_available_caps - item\n\n            # Calculate inverse of remaining capacity (higher score for smaller remaining capacity)\n            # Add epsilon to avoid division by zero. The smaller the remaining capacity, the higher the score.\n            proximity_scores = 1.0 / (non_exact_space_after_placement + 1e-9)\n\n            # Normalize these scores to be between 0 and (1 - epsilon) to not overlap with exact fits\n            # if we wanted a strict hierarchy.\n            # For a combined heuristic, we can normalize them relative to each other.\n            if len(proximity_scores) > 0:\n                min_prox_score = np.min(proximity_scores)\n                max_prox_score = np.max(proximity_scores)\n                \n                # Normalize scores to a range that doesn't conflict with exact fit priority (e.g., 0 to 0.99)\n                # or simply use their relative ranking.\n                # Using ranks is often more robust than raw values.\n                \n                # Rank the non-exact fits based on their proximity score (higher score = better rank)\n                # argsort returns indices that would sort the array.\n                # We want higher proximity_scores to have higher ranks.\n                sorted_indices_for_non_exact = np.argsort(proximity_scores)\n                \n                # Assign ranks: the bin with the highest proximity_score gets the highest rank (close to 1).\n                # The indices obtained from argsort are ascending for smaller values.\n                # So, if proximity_scores are [10, 5, 20], argsort gives [1, 0, 2].\n                # We want ranks [0.33, 0.66, 1.0] or similar.\n                # Let's assign ranks such that smaller space_after_placement gets higher priority.\n                \n                # Sort the actual non-exact remaining capacities to get a clear order for prioritization.\n                sorted_non_exact_space_after = non_exact_space_after_placement[sorted_indices_for_non_exact]\n                \n                # Now assign priorities based on these sorted capacities.\n                # The bin with the smallest space_after_placement gets the highest priority (excluding exact fits).\n                # We can assign priorities from 0.0 up to something less than 1.0 (e.g., 0.99).\n                # Let's normalize the ranks to a range like [0.1, 0.9].\n                \n                if len(sorted_non_exact_space_after) > 1:\n                    rank_values = np.linspace(0.1, 0.9, len(sorted_non_exact_space_after))\n                    # The smallest remaining space should get the highest rank (0.9).\n                    # The current sorted_non_exact_space_after is ascending, so the last element is the largest.\n                    # We want to assign highest priority to the smallest values.\n                    assigned_priorities_for_non_exact = rank_values[::-1] # Reverse to give highest to smallest\n                else:\n                    assigned_priorities_for_non_exact = np.array([0.5]) # Default priority for single non-exact fit\n\n                # Map these assigned priorities back to the original indices of the available_bins_array.\n                # `sorted_indices_for_non_exact` are indices within `non_exact_available_caps`\n                # We need to map these back to `can_fit_mask` indices.\n                \n                # Get the original indices within `can_fit_mask` that correspond to non_exact_fit_mask\n                original_indices_of_non_exact_fits = np.where(can_fit_mask)[0][non_exact_fit_mask]\n                \n                # Update priorities for these bins\n                priorities[original_indices_of_non_exact_fits] = assigned_priorities_for_non_exact\n\n    return priorities\n\n### Analyze & experience\n- *   **Comparing Heuristic 1 (Priority v2) vs. Heuristic 2 (Priority v2):**\n    *   Heuristic 1 attempts to normalize the \"tightness\" of the fit by calculating `(bins_remain_cap - item)`, finding min/max differences, and then normalizing. It then inverts this normalized score to give higher priority to tighter fits. It also adds a small boost (+ 0.1) to these scores to differentiate them from bins that cannot fit.\n    *   Heuristic 2 calculates `diffs = eligible_capacities - item`, finds the `min_diff`, and then assigns priority as `1.0 / (diffs - min_diff + 1e-9)`. This also aims to prioritize tighter fits by giving higher scores to smaller differences relative to the minimum difference.\n    *   **Observation:** Both heuristics aim to prioritize bins with less excess capacity. Heuristic 1's normalization is more complex, potentially more stable across different scales of differences, but might be overkill. Heuristic 2 is simpler and directly emphasizes the bins closest to the minimum excess.\n\n*   **Comparing Heuristic 3 (Exact Fit First) vs. Heuristic 4 (Priority v2):**\n    *   Heuristic 3 explicitly prioritizes bins that *exactly* fit (`bins_remain_cap == item`) with a score of `1.0`. If no exact fit exists, it falls back to prioritizing bins with the least remaining capacity that *can* fit, using an inverse of the excess capacity, normalized.\n    *   Heuristic 4 is identical to Heuristic 2, focusing solely on inverse difference from minimum excess capacity for all fitting bins, without an explicit \"exact fit\" priority level.\n    *   **Observation:** Heuristic 3 provides a distinct, higher priority for exact fits, which is a common and effective strategy in bin packing. Heuristic 4 treats exact fits the same as other close fits, potentially overlooking the benefit of perfect utilization.\n\n*   **Comparing Heuristic 5 (Inverse Distance) vs. Heuristic 8 (Loop-based Inverse Distance):**\n    *   Heuristic 5 calculates `space_after_placement = bins_remain_cap - item`, assigns `1 / (space_after_placement + epsilon)` for fitting bins, and `-np.inf` for non-fitting bins. This directly favors bins with minimal positive `space_after_placement`.\n    *   Heuristic 8 uses a loop to iterate through bins, achieving the same logic: `1.0 / (bins_remain_cap[i] - item + 1e-9)` for fitting bins, `0.0` otherwise.\n    *   **Observation:** Heuristic 5 is more concise and leverages NumPy's vectorized operations, making it more efficient and Pythonic than the explicit loop in Heuristic 8. Both implement the same core \"inverse distance\" or \"best fit\" logic.\n\n*   **Comparing Heuristic 9 (Exact Fit + Scaled Inverse Distance) vs. Heuristic 14 (Exact Fit + Scaled Inverse Difference):**\n    *   Heuristic 9 assigns `1.0` for exact fits. For non-exact fits, it calculates `inverse_distance_scores = 1.0 / (space_after_placement + epsilon)`, normalizes these scores to a range of `[0.5, 0.99]`, ensuring they are lower than exact fits.\n    *   Heuristic 14 assigns `2.0` for exact fits (even higher priority). For non-exact fits, it calculates `space_after_placement`, normalizes it (`1.0 - normalized_diff`), and adds `0.1`, resulting in scores between `0.1` and `1.1` (before considering the `2.0` for exact fits). The scaling is different: Heuristic 9 scales the inverse of the difference, while Heuristic 14 scales the normalized difference itself (then inverts).\n    *   **Observation:** Both combine exact fits with a \"best fit\" approach. Heuristic 14's use of `2.0` for exact fits creates a clearer hierarchy. Its normalization `1.0 - normalized_diff + 0.1` aims to prioritize smaller differences. Heuristic 9's approach of scaling inverse distance to `[0.5, 0.99]` is also a reasonable way to create a distinct priority band for close fits.\n\n*   **Comparing Heuristic 16/17/18 (Sigmoid Fit Score) vs. Heuristic 19 (Exact Fit + Best Fit):**\n    *   Heuristics 16-18 use a sigmoid function applied to `item / available_caps` with a threshold (`0.7`) and steepness (`10.0`). This aims to favor bins where the item takes up a significant portion of the remaining capacity, but not necessarily the absolute tightest fit. It penalizes bins that are too large.\n    *   Heuristic 19 prioritizes exact fits (`1.0`), then uses normalized inverse of space after placement for non-exact fits, scaled to `[0, 0.9]`. This focuses on minimal excess space for non-exact fits.\n    *   **Observation:** The sigmoid approach (16-18) is more complex and might be trying to find a balance point, not just the absolute closest fit. The prioritization based on `item / available_caps` is an interesting way to avoid bins that are *too* large. Heuristic 19 is a more direct combination of exact and best fit.\n\n*   **Comparing Heuristic 2 (Inverse Distance) vs. Heuristic 8 (Loop-based Inverse Distance):**\n    *   Heuristic 2: `priorities[eligible_bins] = 1.0 / (diffs - min_diff + 1e-9)`\n    *   Heuristic 8: `priorities[i] = 1.0 / (bins_remain_cap[i] - item + 1e-9)`\n    *   **Observation:** Heuristic 2 is superior because it normalizes the inverse distance relative to the minimum difference. This prevents bins with extremely large capacities (but still fitting) from getting disproportionately high scores simply because their raw difference `(capacity - item)` is large, even if it's closer to the minimum difference than others. Heuristic 8's scores can be highly skewed by large capacities.\n\n*   **Comparing Heuristic 1 (Normalized Tight Fit) vs. Heuristic 2 (Inverse Difference from Min Diff):**\n    *   Heuristic 1 attempts to normalize the `(capacity - item)` difference to a `[0, 1]` range and then applies `1.0 - normalized_tight_fit`. It also adds `0.1` and a base `1.0` for fitting bins.\n    *   Heuristic 2 uses `1.0 / (diffs - min_diff + 1e-9)`.\n    *   **Observation:** Heuristic 2 is simpler and more direct in prioritizing the *smallest* differences relative to the minimum difference. Heuristic 1's normalization is more involved and the additive `0.1` seems less principled than a multiplicative scaling or direct inverse.\n\n*   **Overall:** Heuristics that explicitly prioritize exact fits (like 3, 9, 14, 19, 20) generally perform better because perfect utilization is a strong objective. Among those that focus on \"best fit\" (minimal excess capacity), prioritizing the inverse of the *difference from the minimum difference* (like Heuristic 2, 4) is more robust than simply taking the inverse of the difference (like Heuristic 8). Vectorized operations (like 1, 2, 5) are preferred over loops (like 8). Complex sigmoid functions (16-18) are less interpretable and may not offer clear advantages over simpler \"best fit\" or \"exact fit\" strategies without empirical validation.\n- \nHere's a redefined \"Current self-reflection\" for designing better heuristics, focusing on actionable improvements:\n\n*   **Keywords:** Robustness, Simplicity, Efficiency, Interpretability.\n*   **Advice:** Develop scoring functions that are robust to capacity variations (e.g., by normalizing differences) and prioritize simple, interpretable metrics. Leverage vectorization for computational efficiency.\n*   **Avoid:** Overly complex, non-linear scoring functions requiring extensive tuning and obscure array manipulations.\n*   **Explanation:** Focusing on robust, simple metrics and efficient implementation leads to more reliable, maintainable, and adaptable heuristics, crucial for dynamic optimization problems.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}