{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a more sophisticated strategy.\n\n    This heuristic prioritizes bins that have just enough remaining capacity to fit the item,\n    while also considering bins that have significantly more capacity as a secondary factor.\n    It aims to reduce fragmentation by favoring tighter fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 1e-9\n    \n    # Calculate the difference between remaining capacity and item size\n    diff = bins_remain_cap - item\n    \n    # Initialize priorities to zero (for bins where the item doesn't fit)\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Identify bins where the item can fit\n    can_fit_mask = diff >= 0\n    \n    # For bins where the item fits, calculate a priority score.\n    # The primary goal is to find bins with a small positive difference (tightest fit).\n    # We use an inverse of (difference + epsilon) for tighter fits to give them higher scores.\n    # We also add a small bonus for bins with much larger remaining capacity to not\n    # completely discard them if no tight fit is available. This bonus is smaller.\n    \n    # Calculate inverse difference for tight fits: higher score for smaller positive difference\n    tight_fit_scores = 1 / (diff[can_fit_mask] + epsilon)\n    \n    # Calculate a secondary score for bins with more remaining capacity than the item size.\n    # This is a simpler inverse of remaining capacity, scaled down.\n    # We want to avoid division by zero for bins with zero remaining capacity if they exist (though item wouldn't fit)\n    # and to give some priority to larger bins if no tight fit is available.\n    # Using the original bins_remain_cap for this secondary scoring.\n    \n    # We can normalize the remaining capacities to get a sense of \"how much space\" is left relative to bin capacity.\n    # However, since we don't have the original bin capacity, we can use a heuristic.\n    # Let's simply use the remaining capacity itself, but scaled.\n    # A simpler approach without original bin capacity is to just use the inverse of difference for those that fit.\n    \n    # Let's refine the logic:\n    # Priority 1: Bins with smallest positive `diff` (tightest fit). This is `1 / (diff + epsilon)`.\n    # Priority 2: Bins with larger `diff`. These are less preferred than tight fits.\n    # A simple way to combine is to give a high score to tight fits and a moderate score to larger fits.\n    \n    # Let's use a piecewise approach for scoring:\n    # For bins that fit:\n    # If diff is very small (e.g., diff < threshold), assign a high priority (e.g., 100 + 1/diff).\n    # If diff is larger, assign a lower priority (e.g., 10 + 1/diff).\n    # This requires tuning `threshold`.\n    \n    # A more robust approach without arbitrary thresholds:\n    # We want to maximize `1/(diff + epsilon)` for tight fits and still assign some score to larger fits.\n    # Consider the ratio of remaining capacity to item size.\n    \n    # Let's try scoring based on inverse difference, and then add a penalty for \"too much\" space.\n    # Or, simply, prioritize bins where `bins_remain_cap` is \"close\" to `item`.\n    \n    # Let's use a score that is high for small positive differences and decreases as the difference grows.\n    # A Gaussian-like function centered around 0 (for `diff`) could work, but it's complex.\n    \n    # Simpler idea: prioritize bins that have *just enough* space.\n    # We can define \"just enough\" as being within a certain percentage of the item size.\n    # For example, if diff is between 0 and `item * tolerance`.\n    \n    # Let's revisit the inverse distance, but modify it to be more sensitive to small differences.\n    # A score that is high for small `diff` and then drops off.\n    # Consider `score = 1 / (diff^2 + epsilon)` or `score = exp(-k * diff)`\n    \n    # Let's try a score that emphasizes the \"tightness\" by squaring the inverse of the difference.\n    # This will amplify the priority for very tight fits.\n    \n    # Calculate the inverse of difference for bins that can fit the item\n    # For bins where it can fit, the priority is proportional to 1 / (difference + epsilon)\n    # We want to boost the priority for smaller differences more significantly.\n    # Let's use (1 / (diff + epsilon))^2 for a stronger emphasis on tightness.\n    \n    # This still might give a very small positive difference a disproportionately high score.\n    \n    # Alternative: Focus on the ratio of remaining capacity to item size.\n    # Bins with `bins_remain_cap / item` close to 1 are good.\n    # Ratio = bins_remain_cap / item. We want ratio ~ 1.\n    # Score could be proportional to `1 / abs(ratio - 1)`.\n    # However, this doesn't account for the absolute amount of space. A bin with 10 capacity\n    # and item 9 (ratio 1.11) is better than bin with 100 capacity and item 9 (ratio 1.01)\n    # if we only consider this ratio. We need to combine it.\n    \n    # Let's stick to the difference but prioritize small positive differences more strongly.\n    # A function like `f(x) = 1/(x+epsilon)` is already good.\n    # What if we add a small bonus for bins that have \"plenty\" of space, but significantly less than tight fits?\n    \n    # Let's try to make it more robust to scale by normalizing.\n    # If we knew the maximum bin capacity, we could normalize. Without it, it's hard.\n    \n    # Back to basics, `priority_v1` favors bins with largest remaining capacity among those that fit.\n    # `1 / (diff + epsilon)` favors bins with smallest `diff`. This is generally good.\n    \n    # How to improve:\n    # 1. Give stronger weight to *very* tight fits.\n    # 2. Ensure that bins that are *almost* full but still fit are prioritized over bins that are nearly empty but fit.\n    \n    # Let's try a compound score:\n    # Score1: Inverse difference (prioritizes tight fits)\n    # Score2: A small bonus for bins that are not too empty, scaled by how much they can fit.\n    \n    # For bins that can fit:\n    # `tight_fit_score = 1 / (diff + epsilon)`\n    \n    # Now, consider the \"emptiness\" of the bin if it fits.\n    # A bin that has `bins_remain_cap` close to `item` is good.\n    # A bin that has `bins_remain_cap` much larger than `item` is less ideal in terms of fragmentation.\n    \n    # Let's define a score that peaks at `diff = 0` (or slightly negative) and decreases.\n    # However, we only consider `diff >= 0`. So we want it to peak at `diff = 0`.\n    \n    # Consider a function that is `1/(diff + epsilon)` for tight fits, and maybe a constant or decaying function for larger fits.\n    \n    # Let's try a score that emphasizes \"just enough\" space more, and \"plenty\" of space less.\n    # If `diff` is small (e.g., < `item / 2`), give it a higher score.\n    # If `diff` is large (e.g., > `item / 2`), give it a lower score.\n    \n    # Let's normalize `diff` relative to the item size.\n    # `normalized_diff = diff / item` (if item > 0)\n    # We want `normalized_diff` close to 0.\n    \n    # Score = `1 / (normalized_diff + epsilon)` for `normalized_diff >= 0`\n    # This is equivalent to `item / (bins_remain_cap - item + epsilon)` which is similar to `priority_v1` but normalized.\n    \n    # Let's try to blend the inverse difference with a penalty for being too \"empty\".\n    # A bin that fits has `bins_remain_cap >= item`.\n    # If `bins_remain_cap` is much larger than `item`, it's \"too empty\".\n    \n    # Let's use a score based on how much of the remaining capacity is *used* by the item.\n    # If `bins_remain_cap` is very close to `item`, then `item / bins_remain_cap` is close to 1.\n    # If `bins_remain_cap` is much larger than `item`, then `item / bins_remain_cap` is close to 0.\n    \n    # So, we want to prioritize bins where `item / bins_remain_cap` is close to 1.\n    # Score = `1 / abs((item / bins_remain_cap) - 1 + epsilon)`\n    # This can be written as `bins_remain_cap / abs(bins_remain_cap - item + epsilon)`.\n    # This is effectively `bins_remain_cap / (diff + epsilon)` for bins that fit.\n    \n    # Let's test this:\n    # item = 5\n    # bins_remain_cap = [10, 6, 20, 5.1]\n    # diff = [5, 1, 15, 0.1]\n    #\n    # priority_v1:\n    # 1/(5+eps) = 0.2\n    # 1/(1+eps) = 1.0\n    # 1/(15+eps) = 0.066\n    # 1/(0.1+eps) = 9.09\n    # Max priority for 5.1 remaining.\n    #\n    # New approach: `bins_remain_cap / (diff + epsilon)`\n    # 10 / (5+eps) = 2.0\n    # 6 / (1+eps) = 6.0\n    # 20 / (15+eps) = 1.33\n    # 5.1 / (0.1+eps) = 51.0\n    # Max priority for 5.1 remaining. This seems to amplify the preference for tight fits.\n    \n    # Let's implement this: `bins_remain_cap / (diff + epsilon)` for bins that fit.\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins where the item fits, calculate the priority score\n    # Prioritize bins where the remaining capacity is closest to the item size.\n    # Score is `bins_remain_cap / (bins_remain_cap - item + epsilon)`\n    # This is equivalent to `bins_remain_cap / (diff + epsilon)`\n    \n    # Add a small epsilon to bins_remain_cap to avoid division by zero if item == bins_remain_cap == 0,\n    # although can_fit_mask should prevent this for item > 0.\n    \n    # Ensure item is not zero to avoid division by zero in potential alternative calculations.\n    # In this case, we are using bins_remain_cap which is always non-negative.\n    \n    # The division `bins_remain_cap / (bins_remain_cap - item + epsilon)` can be large if `bins_remain_cap - item` is small.\n    # This correctly prioritizes tight fits.\n    \n    # Let's make sure the score is well-behaved.\n    # If `bins_remain_cap` is large, and `item` is small, `diff` is large.\n    # `bins_remain_cap / (diff + epsilon)` will be `large / large` -> moderate score.\n    # If `bins_remain_cap` is just slightly larger than `item`, `diff` is small.\n    # `bins_remain_cap / (diff + epsilon)` will be `~item / small` -> high score.\n    \n    # This seems like a good candidate for `priority_v2`.\n    \n    # Avoid division by zero if item is 0, though problem statement implies item > 0.\n    # If item is 0, any bin can fit it with infinite priority if `diff=0`.\n    # For item > 0, `bins_remain_cap` must be >= `item`.\n    \n    # Let's consider `bins_remain_cap - item`. If this is 0, the item perfectly fills the bin.\n    # In that case, `bins_remain_cap / epsilon` would be very large. This is desired.\n    \n    # Calculate the score for bins where the item fits\n    # Score = remaining_capacity / (remaining_capacity - item + epsilon)\n    # This is equivalent to: (item + diff) / (diff + epsilon)\n    # This ratio is maximized when `diff` is minimized (closest to 0).\n    \n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask] / (diff[can_fit_mask] + epsilon)\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes exact fits, then bins with minimal excess capacity using a scaled inverse difference.\n\n    This heuristic gives the highest priority to bins that exactly fit the item.\n    For bins that don't fit exactly but can accommodate the item, it assigns\n    priority based on the inverse of the excess capacity, scaled to emphasize\n    tighter fits.\n    \"\"\"\n    epsilon = 1e-9\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate excess capacity for bins that can fit the item\n    excess_capacity = bins_remain_cap[can_fit_mask] - item\n\n    # Assign highest priority (1.0) to exact fits\n    exact_fit_mask_local = (excess_capacity < epsilon)\n    priorities[can_fit_mask][exact_fit_mask_local] = 1.0\n\n    # For bins that don't fit exactly, calculate priority based on inverse excess capacity\n    close_fit_mask_local = ~exact_fit_mask_local\n    \n    if np.any(close_fit_mask_local):\n        eligible_excess_capacity = excess_capacity[close_fit_mask_local]\n        \n        # Calculate a score proportional to 1 / (excess_capacity + epsilon).\n        # This favors smaller excess capacities (tighter fits).\n        # We normalize this score to be between 0 and 1 to avoid overwhelming exact fits.\n        # A simple normalization: 1 / (1 + scaled_excess_capacity)\n        # Where scaled_excess_capacity is normalized difference.\n        \n        # To ensure scores are distinct from exact fits and rank close fits,\n        # let's use a score that starts below 1.0 and increases as excess capacity decreases.\n        # A simple way is to map the inverse excess capacity to a range like [0.1, 0.9].\n        # The inverse of excess capacity is `1 / (eligible_excess_capacity + epsilon)`.\n        # To bound this, we can consider the maximum inverse excess capacity.\n        \n        # A robust way: Use the inverse of (normalized excess + 1) to get scores in (0, 1].\n        # Let's normalize excess capacity relative to the item size itself,\n        # but ensuring we handle cases where item is very small or zero gracefully if needed.\n        \n        # A simpler score: `1 / (excess_capacity + epsilon)`\n        # Let's scale this to be less than 1.0.\n        # A common approach is `1 / (1 + scaled_value)`.\n        # Let's use `1 / (1 + normalized_excess_capacity)` where normalized is between 0 and some value.\n        \n        # Alternative: score = 0.5 + 0.45 * (1 / (normalized_excess_capacity + epsilon))\n        # This maps normalized excess from 0 to some value into [0.5, ~0.95].\n        \n        # Let's use the remaining capacity itself, but inverted and scaled.\n        # Score = remaining_cap / (remaining_cap - item + epsilon)\n        # This is `(item + excess) / (excess + epsilon)`.\n        # To keep it below 1, we can divide by a factor, e.g., `(item + excess) / (excess + epsilon) / (item/item + epsilon)`\n        \n        # A better approach is to normalize the excess capacity and then use its inverse.\n        # Normalize excess capacity: consider a reasonable upper bound for excess_capacity.\n        # If we assume item size is at most B (max bin capacity), then excess capacity is at most B.\n        # Let's consider `excess_capacity / item` (if item > 0).\n        \n        # A simple and effective score for close fits: `1 / (excess_capacity + epsilon)`\n        # We need to scale this to be less than 1.0.\n        # Find the maximum inverse excess capacity among close fits.\n        max_inv_excess = np.max(1.0 / (eligible_excess_capacity + epsilon))\n        \n        # Scale scores for close fits to be in the range [0.1, 0.9]\n        # Score = 0.1 + 0.8 * (1.0 / (eligible_excess_capacity + epsilon)) / max_inv_excess\n        # This ensures that the tightest fit gets ~0.9 and others scale down.\n        scores_for_close_fits = 0.1 + 0.8 * (1.0 / (eligible_excess_capacity + epsilon)) / (max_inv_excess + epsilon)\n        \n        # Ensure scores are not exactly 1.0 (reserved for exact fits)\n        scores_for_close_fits = np.minimum(scores_for_close_fits, 0.999)\n        \n        priorities[can_fit_mask][close_fit_mask_local] = scores_for_close_fits\n        \n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Best) and Heuristic 5 (also the same as 1): Both use `bins_remain_cap / (diff + epsilon)` to prioritize tight fits, favoring bins where `bins_remain_cap` is close to `item`.\n\nComparing Heuristic 2 and Heuristic 4: Both are identical to Heuristic 1, essentially repeating the same logic. The extensive commented-out exploration in Heuristic 2 doesn't translate to a distinct improvement or change in the final implemented logic compared to Heuristic 1.\n\nComparing Heuristic 3 and Heuristic 12/13/14: Heuristic 3 uses `bins_remain_cap / (diff + epsilon)`, similar to Heuristic 1. Heuristics 12, 13, and 14 introduce a tiered approach: highest priority for exact fits (score 2.0), and medium priority for non-exact fits based on normalized differences (scaled to be less than 2.0, e.g., `1.0 - normalized_diff + 0.1`). This tiered approach is more sophisticated than a single scoring function.\n\nComparing Heuristic 6 and Heuristic 7: Heuristic 6 uses a two-tier system: 1.0 for exact fits, and then normalized scores (0.0-0.9) for non-exact fits based on inverse of excess capacity relative to the minimum excess. Heuristic 7 is similar but assigns 2.0 to exact fits and scales non-exact fits to [0.1, 1.0]. The normalization in Heuristic 7 might be more robust if the range of excess capacities is large.\n\nComparing Heuristic 10/11 and Heuristic 16/17/19: Heuristics 10, 11 use a fixed priority of 1.0 for exact fits and scale non-exact fits to a range [0.5, 0.99] based on inverse normalized excess. Heuristics 16, 17, 19 also give 1.0 to exact fits but scale non-exact fits to a range like [0.1, 0.9] using inverse excess capacity relative to the maximum inverse excess capacity. The scaling in 16/17/19 seems more nuanced for ranking close fits.\n\nComparing Heuristic 8 and Heuristic 9: Heuristic 8 sorts available bins by remaining capacity and then assigns ranks based on sorted priorities (which appears to be a form of inverse remaining capacity). Heuristic 9 directly uses the inverse of available capacity (not excess capacity) and normalizes it. Heuristic 8's approach of ranking based on sorted values might be more stable.\n\nComparing Heuristic 15/18 and Heuristic 20: Heuristic 15/18 prioritizes exact fits with 1.0 and scales non-exact fits to [0, 0.9] using normalized inverse of space after placement. Heuristic 20 also prioritizes exact fits with 1.0 but scales non-exact fits to [0.5, 0.99] using inverse of normalized excess capacity. The latter scaling and explicit re-assertion of exact fit priority seem slightly more robust.\n\nOverall: More sophisticated heuristics implement a multi-tiered strategy (exact fit, then best fit based on normalized excess/inverse excess) with carefully chosen scaling factors to differentiate priorities. Simple inverse or ratio-based scoring is less effective than tiered approaches.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Exact Fit, Minimal Excess Capacity, Scaled Difference, Vectorization.\n*   **Advice:** Explicitly separate exact fits with maximum priority. For close fits, score based on a scaled measure of excess capacity, ensuring these scores are always lower than exact fit scores. Leverage vectorized operations for efficiency.\n*   **Avoid:** Complex non-linear transformations on capacity differences, convoluted array manipulations, and overly aggressive normalization that can obscure relative performance.\n*   **Explanation:** This approach clearly prioritizes perfect solutions while providing a predictable and tunable mechanism for selecting the \"best\" imperfect fit, all within an efficient computational framework.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}