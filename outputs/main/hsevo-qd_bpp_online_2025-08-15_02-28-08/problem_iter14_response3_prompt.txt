{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    A more sophisticated priority function for online Bin Packing.\n\n    This heuristic prioritizes bins that offer an \"exact fit\" (remaining capacity\n    is very close to the item size) with the highest possible score. For bins\n    that do not offer an exact fit but can accommodate the item, it prioritizes\n    those with minimal excess capacity. It uses a scaled difference to rank these\n    \"close fits\", ensuring they receive a score lower than exact fits.\n\n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: A numpy array containing the remaining capacity of each bin.\n\n    Returns:\n        A numpy array of priority scores for each bin, with the same shape as bins_remain_cap.\n    \"\"\"\n    epsilon = 1e-9  # Small value to avoid division by zero\n    high_priority_score = 1e9  # Score for exact fits\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Separate bins into exact fits and close fits\n    exact_fit_mask = np.abs(bins_remain_cap - item) < epsilon\n    close_fit_mask = can_fit_mask & ~exact_fit_mask\n\n    # Assign the highest priority to exact fits\n    priorities[exact_fit_mask] = high_priority_score\n\n    # For close fits, calculate a score based on the inverse of the excess capacity.\n    # We want to prioritize bins with less excess capacity (smaller positive difference).\n    # A simple approach is to use `1 / (difference + epsilon)`.\n    # To ensure these scores are always lower than exact fits, we can scale them down,\n    # or ensure the `high_priority_score` is sufficiently large.\n    # Using `bins_remain_cap / (bins_remain_cap - item + epsilon)` as suggested in thought process\n    # is equivalent to `(item + diff) / (diff + epsilon)` which is a good score for close fits.\n    # We will scale this score down to be less than `high_priority_score`.\n\n    if np.any(close_fit_mask):\n        diff_close_fits = bins_remain_cap[close_fit_mask] - item\n        # The score `bins_remain_cap / (diff + epsilon)` is good for ranking close fits,\n        # but we need to ensure it's less than the exact fit score.\n        # A simple scaling or a different functional form can achieve this.\n        # Let's use a score that decays with increasing difference.\n        # A score like `1 / (diff^2 + epsilon)` could amplify small differences.\n        # Let's stick to the robust `bins_remain_cap / (diff + epsilon)` and scale it.\n        \n        # Calculate the raw score for close fits: prioritize smaller positive difference\n        raw_scores = bins_remain_cap[close_fit_mask] / (diff_close_fits + epsilon)\n        \n        # Scale these scores down to be less than high_priority_score.\n        # We can normalize them relative to the maximum possible raw score or\n        # simply divide by a large constant.\n        # A robust way is to map the range of `diff_close_fits` to a range below `high_priority_score`.\n        # Let's use `1 / (diff + epsilon)` and then scale it.\n        # The maximum value of `1/(diff+eps)` occurs at the minimum positive diff.\n        # Let's ensure scores are less than `high_priority_score`.\n        \n        # A simpler approach: assign priority based on `1 / (diff + epsilon)` and then ensure\n        # exact fits are strictly higher.\n        \n        # Let's use the previous v1 logic for close fits, but ensure it's lower than exact fits.\n        # `1 / (diff + epsilon)`\n        \n        # Instead of `1/(diff+eps)`, let's use the `bins_remain_cap / (diff + epsilon)` idea but scale it.\n        # This score `bins_remain_cap / (diff + epsilon)` is `(item + diff) / (diff + epsilon)`.\n        # If diff is small, this is large. If diff is large, this approaches 1.\n        # We want to prioritize small diff.\n        \n        # Let's use a transformed score: `k / (diff + c)` where k and c are tuned.\n        # The goal is to rank them appropriately among themselves, and keep them below exact fits.\n        \n        # Let's refine the scoring for close fits:\n        # We want minimal `diff` for close fits to have higher priority.\n        # A score like `1 / (diff + epsilon)` already does this.\n        # To ensure these scores are lower than `high_priority_score`, we can map the scores\n        # derived from `1 / (diff + epsilon)` into a range below `high_priority_score`.\n        \n        # Calculate the difference for close fits\n        diff_close = bins_remain_cap[close_fit_mask] - item\n        \n        # Calculate a score that prioritizes smaller differences.\n        # Use `1 / (diff_close + epsilon)`. This gives higher scores for smaller positive differences.\n        # To ensure these scores are distinguishable from exact fits, we can scale them,\n        # or use a secondary factor.\n        \n        # Let's consider a different approach:\n        # Prioritize bins that are *most full* among those that fit, but not exact fits.\n        # This means prioritizing bins with the smallest `bins_remain_cap` such that `bins_remain_cap >= item`.\n        # This is equivalent to prioritizing smallest `diff = bins_remain_cap - item`.\n        \n        # Score for close fits: `1 / (diff_close + epsilon)`\n        # This is already what `priority_v1` does for bins that fit.\n        # The improvement can come from how we combine exact fits with close fits.\n        \n        # Let's give a very high score for exact fits, and a decreasing score for close fits.\n        # The scaling of `bins_remain_cap / (diff + epsilon)` is good for ranking close fits.\n        # Let's normalize this to be below `high_priority_score`.\n        \n        # For close fits, calculate `bins_remain_cap / (diff + epsilon)`\n        # Let's cap these values to be less than `high_priority_score`.\n        \n        # A simple heuristic: Use the `bins_remain_cap / (diff + epsilon)` score.\n        # Then, scale all these scores down so they are consistently lower than `high_priority_score`.\n        # For example, find the max `bins_remain_cap / (diff + epsilon)` among close fits,\n        # and then scale all to be within `[0, high_priority_score * 0.9]`.\n        \n        # Let's implement the idea of \"minimal excess capacity\" more directly.\n        # For close fits, the priority should be inversely proportional to the excess capacity (`diff`).\n        # `priority = 1.0 / (diff + epsilon)` gives higher scores to smaller `diff`.\n        \n        # To make it distinct from exact fits, we can add a small constant to the difference\n        # before taking the inverse, or use a different function.\n        \n        # Let's use a score that is inversely related to `diff`, but scaled to be lower than exact fits.\n        # Consider `score = max_possible_close_fit_score - diff` where `max_possible_close_fit_score`\n        # is slightly less than `high_priority_score`.\n        # This is essentially `C - diff`, which would prioritize smaller `diff`.\n        \n        # Let's combine the two ideas:\n        # 1. Exact fits get `high_priority_score`.\n        # 2. Close fits get a score based on `bins_remain_cap / (diff + epsilon)`, scaled.\n        \n        # Calculate the base score for close fits.\n        close_fit_scores_base = bins_remain_cap[close_fit_mask] / (diff_close + epsilon)\n        \n        # Normalize these scores to a range [0, 1] and then scale them.\n        # To avoid issues with division by zero if all close fits have same diff,\n        # or if max_score is very small.\n        \n        # A simpler approach is to add a penalty to `diff` for non-exact fits.\n        # Let's use `1 / (diff + some_constant + epsilon)` where `some_constant` ensures\n        # these scores are lower than exact fits.\n        \n        # `priority = 1 / (bins_remain_cap - item + some_constant + epsilon)`\n        # If `some_constant` is chosen to be large enough, the scores will be smaller.\n        # For example, `some_constant = 1.0` might be enough.\n        \n        # Let's use the idea of `bins_remain_cap / (diff + epsilon)` but add a base penalty for not being an exact fit.\n        # `score = bins_remain_cap[close_fit_mask] / (diff_close + epsilon) - penalty`\n        \n        # The most direct way to implement \"minimal excess capacity\" is to directly use `1/diff`.\n        # Let's try `1 / (diff_close + epsilon)` and ensure it's lower than exact fits.\n        \n        priorities[close_fit_mask] = 1.0 / (diff_close + epsilon)\n        \n        # Now, we need to ensure these are lower than `high_priority_score`.\n        # If there are exact fits, their scores will be `high_priority_score`.\n        # If `1.0 / (diff_close + epsilon)` can become larger than `high_priority_score`\n        # (e.g., if `diff_close` is very close to zero, but not within epsilon of it),\n        # we need to cap it.\n        \n        # Let's adjust the range of scores for close fits to be clearly below exact fits.\n        # We can scale the `1 / (diff + epsilon)` scores.\n        # Find the maximum score among close fits and scale them.\n        \n        if np.any(priorities[close_fit_mask] > 0): # Check if there are any scores for close fits\n            max_close_fit_score = np.max(priorities[close_fit_mask])\n            # Scale these scores to be in a range like [0, `high_priority_score` * 0.9]\n            # If max_close_fit_score is 0 (e.g., all diffs are very large), this scaling might be tricky.\n            # Let's ensure a minimum score of 0 and a maximum score below `high_priority_score`.\n            \n            # A more robust scaling:\n            # If max_close_fit_score is positive, scale to `high_priority_score * 0.9`\n            if max_close_fit_score > epsilon:\n                # Scale the scores for close fits to be in [0, `high_priority_score` * 0.9]\n                # The relative order within close fits is maintained.\n                scaled_close_fit_scores = (priorities[close_fit_mask] / max_close_fit_score) * (high_priority_score * 0.9)\n                priorities[close_fit_mask] = scaled_close_fit_scores\n            else:\n                # If all close fit scores are near zero, assign a small constant score\n                priorities[close_fit_mask] = 1.0 # Or some small value\n\n    # Bins where the item does not fit have a priority of 0, which is already set.\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines Exact Fit First with Inverse Distance for robust bin packing.\n\n    Prioritizes exact fits, then falls back to the closest fit to minimize waste.\n    This hybrid approach balances ideal packing with practical close fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    epsilon = 1e-9\n\n    # Calculate exact fit priority: highest for bins where remaining capacity equals item size.\n    exact_fit_mask = np.abs(bins_remain_cap - item) < epsilon\n    priorities[exact_fit_mask] = 1.0\n\n    # Calculate inverse distance priority for bins that can fit the item and are not exact fits.\n    can_fit_mask = bins_remain_cap >= item\n    non_exact_fit_mask = can_fit_mask & ~exact_fit_mask\n\n    if np.any(non_exact_fit_mask):\n        space_after_placement = bins_remain_cap[non_exact_fit_mask] - item\n        # Prioritize bins with smaller positive difference (less wasted space).\n        # Use a scaled inverse to ensure these priorities are lower than exact fits.\n        inverse_distance_scores = 1.0 / (space_after_placement + epsilon)\n        \n        # Normalize these scores to a range lower than 1.0, e.g., [0.5, 0.99]\n        # This ensures exact fits are still preferred.\n        min_norm = 0.5\n        max_norm = 0.99\n        \n        # Scale scores to [0, 1] based on their distribution, then shift to [0.5, 0.99]\n        if inverse_distance_scores.size > 1:\n            normalized_scores = (inverse_distance_scores - np.min(inverse_distance_scores)) / (np.max(inverse_distance_scores) - np.min(inverse_distance_scores) + epsilon)\n            scaled_scores = min_norm + normalized_scores * (max_norm - min_norm)\n        else:\n            # If only one bin, assign the midpoint of the secondary range\n            scaled_scores = (min_norm + max_norm) / 2.0\n\n        priorities[non_exact_fit_mask] = scaled_scores\n        \n    # Bins that cannot fit the item retain their default priority of 0.\n\n    return priorities\n\n### Analyze & experience\n- *   **Heuristics 1 vs 2:** Heuristic 1 (`priority_v2`) directly assigns a very high score (`item * 1e10`) to exact fits and uses `bins_remain_cap / (diff + epsilon)` for other fits, providing a clear hierarchy and prioritizing tight fits. Heuristic 2 uses a normalized inverse of remaining capacity after placement but scales it by `0.1`, which might dilute the \"tight fit\" preference and its combination with the base priority of `1.0` is less structured than Heuristic 1's explicit high score for exact fits. Heuristic 1 is better because its scoring is more direct and impactful for exact fits.\n*   **Heuristics 2 vs 3:** Heuristic 2 prioritizes bins with just enough capacity and normalizes these scores, creating a relative ranking of \"tightness.\" Heuristic 3 uses `1.0 / (available_bins_remain_cap - item + 1e-9)` and then ranks these scores using `argsort(argsort(...))`. While both aim for tight fits, Heuristic 2's normalization and explicit \"can fit\" priority feels more robust than the double `argsort` which can be sensitive to the distribution of differences. Heuristic 2 is better due to its more interpretable normalization.\n*   **Heuristics 3 vs 4:** Heuristic 3 uses `1 / (diff + epsilon)` and then ranks the scores. Heuristic 4 explicitly separates exact fits (given `1e9`) and then scales `bins_remain_cap / (diff + epsilon)` for close fits. Heuristic 4 provides a clearer separation of priorities (exact > close > none) and uses a scoring that is more directly related to the remaining capacity. Heuristic 4 is better for its explicit prioritization tiers.\n*   **Heuristics 4 vs 5:** Heuristic 4 prioritizes exact fits with `1e9` and scales `bins_remain_cap / (diff + epsilon)` for close fits. Heuristic 5 is identical to Heuristic 3, using `1 / (diff + epsilon)` and `argsort(argsort(...))`. Heuristic 4 is superior due to its explicit handling of exact fits and more principled scoring for close fits.\n*   **Heuristics 5 vs 6:** Heuristic 5 is essentially the same as Heuristic 3. Heuristic 6 uses `1.0 / (available_bins_cap + 1e-9)` and normalizes it. This approach prioritizes bins that will be *more full* after placement, which is similar to tight fitting but expressed differently. Heuristic 5/3's `1 / (diff + epsilon)` is a more direct measure of tightness. Heuristic 5/3 is slightly better for its directness in measuring excess space.\n*   **Heuristics 6 vs 7:** Heuristic 6 normalizes `1.0 / (available_bins_cap + 1e-9)`. Heuristic 7 assigns `2.0` to exact fits and `1.1 - normalized_diff` to close fits. Heuristic 7's explicit separation of exact fits (priority `2.0`) and then a structured approach for close fits (higher for smaller `diff`) is more robust and hierarchical than Heuristic 6's single normalized score. Heuristic 7 is better.\n*   **Heuristics 7 vs 8:** Heuristic 7 prioritizes exact fits (`2.0`) and then uses `1.1 - normalized_diff` for close fits. Heuristic 8 uses `-remaining_after_packing` and then shifts/adds epsilon. This effectively prioritizes bins with the smallest non-negative `remaining_after_packing`. While similar in goal to Heuristic 7's close fit strategy, Heuristic 7's explicit handling of exact fits with a higher score is more defined. Heuristic 7 is better for its clear tiers.\n*   **Heuristics 8 vs 9:** Heuristics 8 and 9 are identical. They use negative remaining capacity after packing and then shift to make it positive, effectively prioritizing the tightest fits.\n*   **Heuristics 9 vs 10:** Heuristic 9 uses `-remaining_after_packing` and shifts. Heuristic 10 assigns `1.0` to exact fits and scales `1.0 / (space_after_placement + epsilon)` to `[0.5, 0.99]` for close fits. Heuristic 10 provides a clearer hierarchy (exact > close > none) and scales the secondary priority to avoid overlapping with exact fits. Heuristic 10 is better.\n*   **Heuristics 10 vs 11:** Heuristic 10 uses explicit tiers: exact fits (`1.0`), scaled inverse distance (`[0.5, 0.99]`). Heuristic 11 iterates through bins, assigning `1.0 / (bins_remain_cap[i] - item + 1e-9)`. Heuristic 10's structured approach with clear priority levels is superior to Heuristic 11's simple, unscaled inverse difference which doesn't explicitly handle exact fits separately.\n*   **Heuristics 11 vs 12:** Heuristic 11 uses `1.0 / (diff + epsilon)` per bin. Heuristic 12 assigns `1.0` to exact fits and `0.5 + 0.45 * (1.0 / (normalized_excess + 1e-9))` to close fits, capped at `0.99`. Heuristic 12's explicit prioritization of exact fits and structured scaling for close fits makes it better.\n*   **Heuristics 12 vs 13:** Heuristic 12 assigns `1.0` to exact fits and scaled `1 / (normalized_excess + 1e-9)` to close fits. Heuristic 13 assigns `1.0` to exact fits and then `normalized_priorities * 0.9` to close fits. Both aim for similar goals. Heuristic 12's `0.5 + 0.45 * ...` scaling provides a more controlled range for close fits. Heuristic 12 is slightly better for its more refined scaling.\n*   **Heuristics 13 vs 14:** Heuristics 13 and 14 are identical.\n*   **Heuristics 14 vs 15:** Heuristic 14 assigns `1.0` to exact fits and scales close fits to `[0.5, 0.99]`. Heuristic 15 assigns `2.0` to exact fits and scales close fits to `[0.1, 1.0]`. Heuristic 15 provides a higher explicit score for exact fits (`2.0` vs `1.0`) and a reasonable range for close fits, making its hierarchy clearer. Heuristic 15 is better.\n*   **Heuristics 15 vs 16:** Heuristics 15 and 16 are identical.\n*   **Heuristics 16 vs 17:** Heuristic 16 assigns `1.0` to exact fits and scales close fits to `[0.1, 1.0]`. Heuristic 17 uses `1 / (1 + exp(-k * (wasted_space_ratio - 0.5)))` for bins that fit. Heuristic 17's sigmoid approach is less direct and potentially more complex to tune than Heuristic 16's explicit scoring and normalization. Heuristic 16 is better for its clarity and robustness.\n*   **Heuristics 17 vs 18:** Heuristic 17 uses a sigmoid on `item / available_caps`. Heuristic 18 is identical to Heuristic 16. Heuristic 18 is better due to clearer priority separation.\n*   **Heuristics 18 vs 19:** Heuristics 18 and 19 are identical.\n*   **Heuristics 19 vs 20:** Heuristic 19 assigns `2.0` to exact fits and scales close fits to `[0.1, 1.0]`. Heuristic 20 assigns `1.0` to exact fits and ranks non-exact fits into `[0.1, 0.9]`. Heuristic 19's higher explicit score for exact fits (`2.0`) makes its hierarchical preference stronger. Heuristic 19 is better.\n*   **Overall:** The best heuristics clearly prioritize exact fits with a distinctively high score, then use a well-defined strategy (often normalized inverse excess capacity) for \"close\" fits, ensuring these scores are lower than exact fits. Heuristics that directly implement a hierarchy and use robust scoring for tiers are better.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Exact fit, scaled closeness, interpretability, robustness, vectorization.\n*   **Advice:** Design a multi-tiered scoring system. Assign the highest scores to exact fits. For near-fits, use a scaled inverse of *normalized* excess capacity, ensuring scores decrease monotonically with increasing excess capacity. Keep scoring functions simple and directly tied to fit quality.\n*   **Avoid:** Complex, non-monotonic, or highly sensitive non-linear scoring functions. Avoid manual iteration over array elements; leverage vectorization. Do not normalize without a clear rationale, as over-normalization can obscure differences.\n*   **Explanation:** This approach balances the strong preference for exact fits with a robust, interpretable method for ranking near-fits, while promoting efficient, maintainable code.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}