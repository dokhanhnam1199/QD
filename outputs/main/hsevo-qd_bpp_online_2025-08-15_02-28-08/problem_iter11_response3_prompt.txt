{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    epsilon = 1e-9\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        diff = bins_remain_cap[can_fit_mask] - item\n        \n        # Score: Prioritize bins where the remaining capacity is closest to the item size.\n        # The score is calculated as: remaining_capacity / (remaining_capacity - item + epsilon)\n        # This can be rewritten as: (item + diff) / (diff + epsilon)\n        # This function is maximized when `diff` is minimized (i.e., a tight fit).\n        # For very tight fits (diff close to 0), the score becomes very high.\n        # For larger diffs, the score approaches 1.\n        # This prioritizes bins that are \"just enough\" to fit the item.\n        \n        scores = bins_remain_cap[can_fit_mask] / (diff + epsilon)\n        \n        # A small adjustment to slightly penalize bins that are excessively large if a tight fit exists.\n        # This could be a multiplicative factor or an additive one, but to keep it simple and effective,\n        # let's rely on the primary score's behavior. The current score already implicitly favors smaller available space\n        # when it's just enough.\n        \n        # For bins where the item fits, we want to maximize the ratio `bins_remain_cap / (bins_remain_cap - item)`.\n        # This is equivalent to `(item + diff) / (diff)`.\n        # Let's normalize `diff` by `item` to make the score less dependent on absolute scales.\n        # `normalized_diff = diff / item`\n        # Score = `(item + normalized_diff * item) / (normalized_diff * item + epsilon)`\n        # Score = `(1 + normalized_diff) / (normalized_diff + epsilon / item)`\n        # This makes the score proportional to `1 / normalized_diff` for small `normalized_diff`.\n        \n        # Let's refine the score to focus on the 'goodness' of the fit relative to the item size.\n        # We want bins where `bins_remain_cap` is just slightly larger than `item`.\n        # Consider `bins_remain_cap / item`. We want this ratio to be close to 1.\n        # A score like `1 / abs(bins_remain_cap / item - 1 + epsilon)` is good.\n        # This simplifies to `item / abs(bins_remain_cap - item + epsilon)`.\n        # For bins that fit (`bins_remain_cap >= item`), this is `item / (bins_remain_cap - item + epsilon)`.\n        # This is very similar to `bins_remain_cap / (bins_remain_cap - item + epsilon)` but normalized by item size.\n        \n        # Let's try to create a score that peaks at `diff = 0` and decreases.\n        # A simple approach is to use `1 / (diff + epsilon)`.\n        # To amplify the preference for *very* tight fits, we can use `1 / (diff^2 + epsilon)` or `exp(-k * diff)`.\n        \n        # Let's try a score that is high for tight fits and moderately high for larger fits,\n        # without completely favoring large bins unless no tight fits exist.\n        \n        # A more robust approach:\n        # Prioritize bins with minimal positive difference (`diff`).\n        # Then, among bins with similar small differences, pick the one with less remaining capacity.\n        \n        # Let's score bins based on `1 / (diff + epsilon)`. This gives higher scores to smaller `diff`.\n        # To differentiate further, consider the actual remaining capacity.\n        # If `diff` is small, `bins_remain_cap` is close to `item`.\n        # If `diff` is large, `bins_remain_cap` is much larger than `item`.\n        \n        # Let's try to combine \"tightness\" and \"utilization\".\n        # A good bin is one that fits tightly.\n        # Score = `1 / (diff + epsilon)` is a good start.\n        \n        # To make it more sensitive to *tight* fits and less sensitive to *very large* capacities,\n        # let's use `item / (diff + epsilon)`. This scales the priority by the item size.\n        # This is equivalent to `item / (bins_remain_cap - item + epsilon)`.\n        \n        # If item = 5, bins = [6, 10, 20]\n        # diff = [1, 5, 15]\n        #\n        # Current (v1) `1/(diff+eps)`: [1, 0.2, 0.06] -> Bin with 6 remaining cap wins.\n        # Proposed `item/(diff+eps)`: [5, 1, 0.33] -> Bin with 6 remaining cap wins.\n        \n        # What if we want to penalize bins that are too large?\n        # For example, if `bins_remain_cap` is more than `2 * item`.\n        \n        # Let's try a score that rewards bins that are \"just enough\" and penalizes bins that are \"too much\".\n        # Score = `1 / (diff + epsilon)` gives highest score for smallest `diff`.\n        # We can add a factor that decreases as `bins_remain_cap` increases beyond `item`.\n        \n        # Let's try `score = (1 / (diff + epsilon)) * (item / (bins_remain_cap + epsilon))`\n        # This combines the inverse difference with a factor that reduces score for larger remaining capacities.\n        \n        # If item = 5, bins = [6, 10, 20]\n        # diff = [1, 5, 15]\n        # v1: [1, 0.2, 0.06]\n        #\n        # New score:\n        # Bin 1 (rem=6, diff=1): (1/1) * (5/6) = 0.83\n        # Bin 2 (rem=10, diff=5): (1/5) * (5/10) = 0.1 * 0.5 = 0.05\n        # Bin 3 (rem=20, diff=15): (1/15) * (5/20) = 0.066 * 0.25 = 0.0165\n        \n        # This seems to penalize larger bins too aggressively, making it similar to First Fit Decreasing if items are sorted.\n        \n        # Let's go back to the idea of prioritizing tight fits, but ensure that if there are multiple tight fits,\n        # we prefer the one that leaves less residual space.\n        \n        # Consider a score that is a function of `diff` and `bins_remain_cap`.\n        # A common approach is to use `(bins_remain_cap - item)` as the primary criterion for tightness.\n        # We want to minimize `bins_remain_cap - item`.\n        \n        # Let's try a score that is higher for smaller `diff`, and among equal `diff`s, it doesn't matter much.\n        # The key is to make the score sensitive to small positive `diff`.\n        \n        # A score like `1 / (diff + epsilon)` is already good.\n        # To make it \"better\", we need to ensure it generalizes well.\n        \n        # Let's try to use the reciprocal of the \"waste\" if the item fits perfectly, or a scaled waste.\n        # For a tight fit, `bins_remain_cap` is close to `item`.\n        # Consider `score = bins_remain_cap / (bins_remain_cap - item + epsilon)`.\n        # This was explored before and seemed to amplify tight fits.\n        \n        # Let's try to make the score more focused on the *proportion* of remaining capacity used.\n        # For a bin to be ideal, `item / bins_remain_cap` should be close to 1.\n        # Score = `1 / abs(item / bins_remain_cap - 1 + epsilon)`\n        # Score = `bins_remain_cap / abs(bins_remain_cap - item + epsilon)`\n        # This is `bins_remain_cap / (diff + epsilon)` for bins that fit.\n        \n        # Let's consider a two-stage approach:\n        # 1. Favor bins with `diff` within a small range (e.g., `0 <= diff < threshold`).\n        # 2. Within that range, pick the bin with minimum `diff`.\n        # 3. If no bin is in that range, pick the bin with the minimum `diff` overall.\n        \n        # This can be encoded into a single score.\n        # Let's use a score that is very high for tight fits, and moderately high for larger fits.\n        \n        # A score that is proportional to `1 / (diff + epsilon)` is good for prioritizing tight fits.\n        # To make it \"better\", we can scale it by a factor that is less sensitive to extremely large capacities.\n        \n        # Let's try `score = (1 + item) / (diff + epsilon)`\n        # If item = 5, bins = [6, 10, 20]\n        # diff = [1, 5, 15]\n        #\n        # v1 `1/(diff+eps)`: [1, 0.2, 0.06]\n        # Proposed `(1+item)/(diff+eps)`:\n        # Bin 1 (rem=6, diff=1): (1+5) / 1 = 6\n        # Bin 2 (rem=10, diff=5): (1+5) / 5 = 1.2\n        # Bin 3 (rem=20, diff=15): (1+5) / 15 = 0.4\n        \n        # This seems to amplify the preference for tight fits even more strongly than `item / (diff + epsilon)`.\n        # It's essentially prioritizing bins with smaller `diff` and giving them a boost proportional to `item + 1`.\n        # This can be interpreted as: \"how much value does this bin's remaining space offer for fitting this item tightly?\"\n        \n        # Let's use `(bins_remain_cap + 1) / (diff + epsilon)`. This is slightly different.\n        # If item = 5, bins = [6, 10, 20]\n        # diff = [1, 5, 15]\n        #\n        # Proposed `(bins_remain_cap + 1) / (diff + epsilon)`:\n        # Bin 1 (rem=6, diff=1): (6+1) / 1 = 7\n        # Bin 2 (rem=10, diff=5): (10+1) / 5 = 11 / 5 = 2.2\n        # Bin 3 (rem=20, diff=15): (20+1) / 15 = 21 / 15 = 1.4\n        \n        # This score favors bins that are *closer* to the item size AND have *more* remaining capacity if the difference is the same.\n        # This is not ideal. We want to prioritize tight fits.\n        \n        # Let's stick to prioritizing tight fits. The score `bins_remain_cap / (diff + epsilon)` is a good candidate.\n        # It means we prefer bins where `bins_remain_cap / item` is close to 1.\n        # `bins_remain_cap / (bins_remain_cap - item + epsilon)`\n        # This is `(item + diff) / (diff + epsilon)`\n        \n        # Consider the goal: minimize the number of bins.\n        # This is achieved by packing items as tightly as possible.\n        # So, prioritize bins where `bins_remain_cap - item` is minimized and non-negative.\n        \n        # Let's consider a score that combines the 'tightness' `1/(diff + epsilon)` with a penalty for 'over-filling' if the bin is much larger.\n        # The score `bins_remain_cap / (diff + epsilon)` already tends to favor smaller remaining capacities if the difference is the same.\n        \n        # Final approach: Prioritize bins with the smallest positive difference.\n        # To make this robust, normalize the difference by the item size.\n        # Score = `1 / (diff / item + epsilon)` if `item > 0`.\n        # Score = `item / (diff + epsilon)` for `item > 0`.\n        # This emphasizes tight fits relative to the item's size.\n        \n        scores = item / (diff + epsilon)\n        \n        priorities[can_fit_mask] = scores\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes exact fits, then bins with minimal excess capacity using normalized inverse differences.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    eligible_bins_indices = np.where(can_fit_mask)[0]\n    \n    if len(eligible_bins_indices) == 0:\n        return priorities\n\n    eligible_capacities = bins_remain_cap[eligible_bins_indices]\n    \n    # Calculate excess capacity for eligible bins\n    excess_capacity = eligible_capacities - item\n    \n    # --- Strategy: Exact Fit First ---\n    # Assign highest priority to bins that are an exact fit.\n    exact_fit_mask = (excess_capacity == 0)\n    exact_fit_indices = eligible_bins_indices[exact_fit_mask]\n    if len(exact_fit_indices) > 0:\n        priorities[exact_fit_indices] = 2.0 # Highest priority score\n        \n    # --- Strategy: Best Fit (Minimal Excess Capacity) ---\n    # For bins that are not an exact fit, prioritize those with the least excess capacity.\n    # Use a normalized inverse of the excess capacity to give higher scores to tighter fits.\n    non_exact_fit_indices = eligible_bins_indices[~exact_fit_mask]\n    \n    if len(non_exact_fit_indices) > 0:\n        non_exact_excess_capacity = excess_capacity[~exact_fit_mask]\n        \n        # Normalize differences: smaller difference means higher priority.\n        # We use (1.0 - normalized_difference) to map smaller excess to higher score.\n        min_excess = np.min(non_exact_excess_capacity)\n        max_excess = np.max(non_exact_excess_capacity)\n        \n        if max_excess - min_excess > 1e-9: # Avoid division by zero if all are the same\n            normalized_excess = (non_exact_excess_capacity - min_excess) / (max_excess - min_excess)\n            # Higher score for smaller normalized excess\n            best_fit_scores = 1.0 - normalized_excess \n        else:\n            best_fit_scores = np.ones_like(non_exact_excess_capacity) # All are equally good if range is zero\n\n        # Scale scores to a range lower than exact fit (e.g., [0.1, 1.0])\n        # Add a base score and scale to ensure they are distinct from exact fit but ordered.\n        scaled_best_fit_scores = 0.1 + best_fit_scores * 0.9\n        \n        priorities[non_exact_fit_indices] = scaled_best_fit_scores\n    \n    # Ensure bins that don't fit at all have zero priority\n    priorities[~can_fit_mask] = 0.0\n    \n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Best) and Heuristic 5 (also the same as 1): Both use `bins_remain_cap / (diff + epsilon)` to prioritize tight fits, favoring bins where `bins_remain_cap` is close to `item`.\n\nComparing Heuristic 2 and Heuristic 4: Both are identical to Heuristic 1, essentially repeating the same logic. The extensive commented-out exploration in Heuristic 2 doesn't translate to a distinct improvement or change in the final implemented logic compared to Heuristic 1.\n\nComparing Heuristic 3 and Heuristic 12/13/14: Heuristic 3 uses `bins_remain_cap / (diff + epsilon)`, similar to Heuristic 1. Heuristics 12, 13, and 14 introduce a tiered approach: highest priority for exact fits (score 2.0), and medium priority for non-exact fits based on normalized differences (scaled to be less than 2.0, e.g., `1.0 - normalized_diff + 0.1`). This tiered approach is more sophisticated than a single scoring function.\n\nComparing Heuristic 6 and Heuristic 7: Heuristic 6 uses a two-tier system: 1.0 for exact fits, and then normalized scores (0.0-0.9) for non-exact fits based on inverse of excess capacity relative to the minimum excess. Heuristic 7 is similar but assigns 2.0 to exact fits and scales non-exact fits to [0.1, 1.0]. The normalization in Heuristic 7 might be more robust if the range of excess capacities is large.\n\nComparing Heuristic 10/11 and Heuristic 16/17/19: Heuristics 10, 11 use a fixed priority of 1.0 for exact fits and scale non-exact fits to a range [0.5, 0.99] based on inverse normalized excess. Heuristics 16, 17, 19 also give 1.0 to exact fits but scale non-exact fits to a range like [0.1, 0.9] using inverse excess capacity relative to the maximum inverse excess capacity. The scaling in 16/17/19 seems more nuanced for ranking close fits.\n\nComparing Heuristic 8 and Heuristic 9: Heuristic 8 sorts available bins by remaining capacity and then assigns ranks based on sorted priorities (which appears to be a form of inverse remaining capacity). Heuristic 9 directly uses the inverse of available capacity (not excess capacity) and normalizes it. Heuristic 8's approach of ranking based on sorted values might be more stable.\n\nComparing Heuristic 15/18 and Heuristic 20: Heuristic 15/18 prioritizes exact fits with 1.0 and scales non-exact fits to [0, 0.9] using normalized inverse of space after placement. Heuristic 20 also prioritizes exact fits with 1.0 but scales non-exact fits to [0.5, 0.99] using inverse of normalized excess capacity. The latter scaling and explicit re-assertion of exact fit priority seem slightly more robust.\n\nOverall: More sophisticated heuristics implement a multi-tiered strategy (exact fit, then best fit based on normalized excess/inverse excess) with carefully chosen scaling factors to differentiate priorities. Simple inverse or ratio-based scoring is less effective than tiered approaches.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Exact Fit, Minimal Excess Capacity, Scaled Difference, Vectorization.\n*   **Advice:** Explicitly separate exact fits with maximum priority. For close fits, score based on a scaled measure of excess capacity, ensuring these scores are always lower than exact fit scores. Leverage vectorized operations for efficiency.\n*   **Avoid:** Complex non-linear transformations on capacity differences, convoluted array manipulations, and overly aggressive normalization that can obscure relative performance.\n*   **Explanation:** This approach clearly prioritizes perfect solutions while providing a predictable and tunable mechanism for selecting the \"best\" imperfect fit, all within an efficient computational framework.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}