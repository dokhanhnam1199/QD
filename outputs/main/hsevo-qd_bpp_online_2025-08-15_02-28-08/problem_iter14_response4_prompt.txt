{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            priorities[i] = 1.0 / (bins_remain_cap[i] - item + 1e-9)\n        else:\n            priorities[i] = 0.0\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Exact Fit priority with a normalized Best Fit strategy.\n    Gives highest priority to exact fits, then prioritizes bins with\n    the smallest remaining capacity that can fit the item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    eligible_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Exact Fit Strategy ---\n    # Prioritize bins that fit the item exactly with a high score\n    exact_fit_mask_local = (eligible_bins_remain_cap == item)\n    \n    # --- Normalized Best Fit Strategy ---\n    # For bins that do not fit exactly, calculate a score based on how close they are\n    # to fitting the item. Prioritize those with minimal excess capacity.\n    close_fit_mask_local = ~exact_fit_mask_local\n    \n    # Calculate excess capacity for close fits\n    excess_capacities = eligible_bins_remain_cap[close_fit_mask_local] - item\n    \n    # Assign high priority to exact fits\n    if np.any(exact_fit_mask_local):\n        priorities[can_fit_mask][exact_fit_mask_local] = 1.0\n        \n    # If there are close fits, calculate their priorities\n    if np.any(close_fit_mask_local):\n        # Normalize excess capacities to be between 0 and 1\n        # A smaller excess capacity should get a higher score.\n        # We use 1 / (normalized_excess + epsilon) to give higher scores to smaller excesses.\n        min_excess = 0.0\n        max_excess = np.max(excess_capacities) # Max excess capacity among close fits\n\n        # Avoid division by zero if all close fits have the same excess capacity\n        if max_excess > min_excess:\n            normalized_excess = (excess_capacities - min_excess) / (max_excess - min_excess)\n        else:\n            normalized_excess = np.zeros_like(excess_capacities) # All close fits have same excess\n        \n        # Calculate scores: higher score for smaller normalized excess capacity\n        # Add a base score to differentiate from bins that don't fit (score 0)\n        # and ensure scores are distinct from exact fits.\n        close_fit_scores = 0.5 + 0.45 * (1.0 / (normalized_excess + 1e-9)) # Scale to [0.5, ~0.95]\n        \n        # Ensure scores are not greater than exact fit priority\n        close_fit_scores = np.minimum(close_fit_scores, 0.99) \n        \n        # Assign scores to the corresponding bins\n        priorities[can_fit_mask][close_fit_mask_local] = close_fit_scores\n\n    # Ensure exact fits have priority over close fits if they overlap in scoring range\n    if np.any(exact_fit_mask_local) and np.any(close_fit_mask_local):\n         priorities[can_fit_mask][exact_fit_mask_local] = 1.0 # Re-assert max priority for exact fits\n\n\n    return priorities\n\n### Analyze & experience\n- *   **Heuristics 1 vs 2:** Heuristic 1 (`priority_v2`) directly assigns a very high score (`item * 1e10`) to exact fits and uses `bins_remain_cap / (diff + epsilon)` for other fits, providing a clear hierarchy and prioritizing tight fits. Heuristic 2 uses a normalized inverse of remaining capacity after placement but scales it by `0.1`, which might dilute the \"tight fit\" preference and its combination with the base priority of `1.0` is less structured than Heuristic 1's explicit high score for exact fits. Heuristic 1 is better because its scoring is more direct and impactful for exact fits.\n*   **Heuristics 2 vs 3:** Heuristic 2 prioritizes bins with just enough capacity and normalizes these scores, creating a relative ranking of \"tightness.\" Heuristic 3 uses `1.0 / (available_bins_remain_cap - item + 1e-9)` and then ranks these scores using `argsort(argsort(...))`. While both aim for tight fits, Heuristic 2's normalization and explicit \"can fit\" priority feels more robust than the double `argsort` which can be sensitive to the distribution of differences. Heuristic 2 is better due to its more interpretable normalization.\n*   **Heuristics 3 vs 4:** Heuristic 3 uses `1 / (diff + epsilon)` and then ranks the scores. Heuristic 4 explicitly separates exact fits (given `1e9`) and then scales `bins_remain_cap / (diff + epsilon)` for close fits. Heuristic 4 provides a clearer separation of priorities (exact > close > none) and uses a scoring that is more directly related to the remaining capacity. Heuristic 4 is better for its explicit prioritization tiers.\n*   **Heuristics 4 vs 5:** Heuristic 4 prioritizes exact fits with `1e9` and scales `bins_remain_cap / (diff + epsilon)` for close fits. Heuristic 5 is identical to Heuristic 3, using `1 / (diff + epsilon)` and `argsort(argsort(...))`. Heuristic 4 is superior due to its explicit handling of exact fits and more principled scoring for close fits.\n*   **Heuristics 5 vs 6:** Heuristic 5 is essentially the same as Heuristic 3. Heuristic 6 uses `1.0 / (available_bins_cap + 1e-9)` and normalizes it. This approach prioritizes bins that will be *more full* after placement, which is similar to tight fitting but expressed differently. Heuristic 5/3's `1 / (diff + epsilon)` is a more direct measure of tightness. Heuristic 5/3 is slightly better for its directness in measuring excess space.\n*   **Heuristics 6 vs 7:** Heuristic 6 normalizes `1.0 / (available_bins_cap + 1e-9)`. Heuristic 7 assigns `2.0` to exact fits and `1.1 - normalized_diff` to close fits. Heuristic 7's explicit separation of exact fits (priority `2.0`) and then a structured approach for close fits (higher for smaller `diff`) is more robust and hierarchical than Heuristic 6's single normalized score. Heuristic 7 is better.\n*   **Heuristics 7 vs 8:** Heuristic 7 prioritizes exact fits (`2.0`) and then uses `1.1 - normalized_diff` for close fits. Heuristic 8 uses `-remaining_after_packing` and then shifts/adds epsilon. This effectively prioritizes bins with the smallest non-negative `remaining_after_packing`. While similar in goal to Heuristic 7's close fit strategy, Heuristic 7's explicit handling of exact fits with a higher score is more defined. Heuristic 7 is better for its clear tiers.\n*   **Heuristics 8 vs 9:** Heuristics 8 and 9 are identical. They use negative remaining capacity after packing and then shift to make it positive, effectively prioritizing the tightest fits.\n*   **Heuristics 9 vs 10:** Heuristic 9 uses `-remaining_after_packing` and shifts. Heuristic 10 assigns `1.0` to exact fits and scales `1.0 / (space_after_placement + epsilon)` to `[0.5, 0.99]` for close fits. Heuristic 10 provides a clearer hierarchy (exact > close > none) and scales the secondary priority to avoid overlapping with exact fits. Heuristic 10 is better.\n*   **Heuristics 10 vs 11:** Heuristic 10 uses explicit tiers: exact fits (`1.0`), scaled inverse distance (`[0.5, 0.99]`). Heuristic 11 iterates through bins, assigning `1.0 / (bins_remain_cap[i] - item + 1e-9)`. Heuristic 10's structured approach with clear priority levels is superior to Heuristic 11's simple, unscaled inverse difference which doesn't explicitly handle exact fits separately.\n*   **Heuristics 11 vs 12:** Heuristic 11 uses `1.0 / (diff + epsilon)` per bin. Heuristic 12 assigns `1.0` to exact fits and `0.5 + 0.45 * (1.0 / (normalized_excess + 1e-9))` to close fits, capped at `0.99`. Heuristic 12's explicit prioritization of exact fits and structured scaling for close fits makes it better.\n*   **Heuristics 12 vs 13:** Heuristic 12 assigns `1.0` to exact fits and scaled `1 / (normalized_excess + 1e-9)` to close fits. Heuristic 13 assigns `1.0` to exact fits and then `normalized_priorities * 0.9` to close fits. Both aim for similar goals. Heuristic 12's `0.5 + 0.45 * ...` scaling provides a more controlled range for close fits. Heuristic 12 is slightly better for its more refined scaling.\n*   **Heuristics 13 vs 14:** Heuristics 13 and 14 are identical.\n*   **Heuristics 14 vs 15:** Heuristic 14 assigns `1.0` to exact fits and scales close fits to `[0.5, 0.99]`. Heuristic 15 assigns `2.0` to exact fits and scales close fits to `[0.1, 1.0]`. Heuristic 15 provides a higher explicit score for exact fits (`2.0` vs `1.0`) and a reasonable range for close fits, making its hierarchy clearer. Heuristic 15 is better.\n*   **Heuristics 15 vs 16:** Heuristics 15 and 16 are identical.\n*   **Heuristics 16 vs 17:** Heuristic 16 assigns `1.0` to exact fits and scales close fits to `[0.1, 1.0]`. Heuristic 17 uses `1 / (1 + exp(-k * (wasted_space_ratio - 0.5)))` for bins that fit. Heuristic 17's sigmoid approach is less direct and potentially more complex to tune than Heuristic 16's explicit scoring and normalization. Heuristic 16 is better for its clarity and robustness.\n*   **Heuristics 17 vs 18:** Heuristic 17 uses a sigmoid on `item / available_caps`. Heuristic 18 is identical to Heuristic 16. Heuristic 18 is better due to clearer priority separation.\n*   **Heuristics 18 vs 19:** Heuristics 18 and 19 are identical.\n*   **Heuristics 19 vs 20:** Heuristic 19 assigns `2.0` to exact fits and scales close fits to `[0.1, 1.0]`. Heuristic 20 assigns `1.0` to exact fits and ranks non-exact fits into `[0.1, 0.9]`. Heuristic 19's higher explicit score for exact fits (`2.0`) makes its hierarchical preference stronger. Heuristic 19 is better.\n*   **Overall:** The best heuristics clearly prioritize exact fits with a distinctively high score, then use a well-defined strategy (often normalized inverse excess capacity) for \"close\" fits, ensuring these scores are lower than exact fits. Heuristics that directly implement a hierarchy and use robust scoring for tiers are better.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Exact fit, scaled closeness, interpretability, robustness, vectorization.\n*   **Advice:** Design a multi-tiered scoring system. Assign the highest scores to exact fits. For near-fits, use a scaled inverse of *normalized* excess capacity, ensuring scores decrease monotonically with increasing excess capacity. Keep scoring functions simple and directly tied to fit quality.\n*   **Avoid:** Complex, non-monotonic, or highly sensitive non-linear scoring functions. Avoid manual iteration over array elements; leverage vectorization. Do not normalize without a clear rationale, as over-normalization can obscure differences.\n*   **Explanation:** This approach balances the strong preference for exact fits with a robust, interpretable method for ranking near-fits, while promoting efficient, maintainable code.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}