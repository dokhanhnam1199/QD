{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins with exact or near-exact fits using a ratio-based score.\n\n    This heuristic favors bins where the remaining capacity is as close as possible to\n    the item size, assigning higher scores to tighter fits.\n    \"\"\"\n    epsilon = 1e-9\n    \n    # Initialize priorities to zero for all bins\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the difference between remaining capacity and item size for fitting bins\n    diff = bins_remain_cap[can_fit_mask] - item\n    \n    # Calculate scores for bins where the item fits.\n    # The score is `bins_remain_cap / (diff + epsilon)`.\n    # This ratio emphasizes bins where `diff` is small (tight fits).\n    # For an exact fit (diff=0), the score becomes `bins_remain_cap / epsilon`, which is very high.\n    # For larger differences, the ratio tends to be smaller.\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask] / (diff + epsilon)\n    \n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    epsilon = 1e-9\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        diff = bins_remain_cap[can_fit_mask] - item\n        \n        # Score: Prioritize bins where the remaining capacity is closest to the item size.\n        # The score is calculated as: remaining_capacity / (remaining_capacity - item + epsilon)\n        # This can be rewritten as: (item + diff) / (diff + epsilon)\n        # This function is maximized when `diff` is minimized (i.e., a tight fit).\n        # For very tight fits (diff close to 0), the score becomes very high.\n        # For larger diffs, the score approaches 1.\n        # This prioritizes bins that are \"just enough\" to fit the item.\n        \n        scores = bins_remain_cap[can_fit_mask] / (diff + epsilon)\n        \n        # A small adjustment to slightly penalize bins that are excessively large if a tight fit exists.\n        # This could be a multiplicative factor or an additive one, but to keep it simple and effective,\n        # let's rely on the primary score's behavior. The current score already implicitly favors smaller available space\n        # when it's just enough.\n        \n        # For bins where the item fits, we want to maximize the ratio `bins_remain_cap / (bins_remain_cap - item)`.\n        # This is equivalent to `(item + diff) / (diff)`.\n        # Let's normalize `diff` by `item` to make the score less dependent on absolute scales.\n        # `normalized_diff = diff / item`\n        # Score = `(item + normalized_diff * item) / (normalized_diff * item + epsilon)`\n        # Score = `(1 + normalized_diff) / (normalized_diff + epsilon / item)`\n        # This makes the score proportional to `1 / normalized_diff` for small `normalized_diff`.\n        \n        # Let's refine the score to focus on the 'goodness' of the fit relative to the item size.\n        # We want bins where `bins_remain_cap` is just slightly larger than `item`.\n        # Consider `bins_remain_cap / item`. We want this ratio to be close to 1.\n        # A score like `1 / abs(bins_remain_cap / item - 1 + epsilon)` is good.\n        # This simplifies to `item / abs(bins_remain_cap - item + epsilon)`.\n        # For bins that fit (`bins_remain_cap >= item`), this is `item / (bins_remain_cap - item + epsilon)`.\n        # This is very similar to `bins_remain_cap / (bins_remain_cap - item + epsilon)` but normalized by item size.\n        \n        # Let's try to create a score that peaks at `diff = 0` and decreases.\n        # A simple approach is to use `1 / (diff + epsilon)`.\n        # To amplify the preference for *very* tight fits, we can use `1 / (diff^2 + epsilon)` or `exp(-k * diff)`.\n        \n        # Let's try a score that is high for tight fits and moderately high for larger fits,\n        # without completely favoring large bins unless no tight fits exist.\n        \n        # A more robust approach:\n        # Prioritize bins with minimal positive difference (`diff`).\n        # Then, among bins with similar small differences, pick the one with less remaining capacity.\n        \n        # Let's score bins based on `1 / (diff + epsilon)`. This gives higher scores to smaller `diff`.\n        # To differentiate further, consider the actual remaining capacity.\n        # If `diff` is small, `bins_remain_cap` is close to `item`.\n        # If `diff` is large, `bins_remain_cap` is much larger than `item`.\n        \n        # Let's try to combine \"tightness\" and \"utilization\".\n        # A good bin is one that fits tightly.\n        # Score = `1 / (diff + epsilon)` is a good start.\n        \n        # To make it more sensitive to *tight* fits and less sensitive to *very large* capacities,\n        # let's use `item / (diff + epsilon)`. This scales the priority by the item size.\n        # This is equivalent to `item / (bins_remain_cap - item + epsilon)`.\n        \n        # If item = 5, bins = [6, 10, 20]\n        # diff = [1, 5, 15]\n        #\n        # Current (v1) `1/(diff+eps)`: [1, 0.2, 0.06] -> Bin with 6 remaining cap wins.\n        # Proposed `item/(diff+eps)`: [5, 1, 0.33] -> Bin with 6 remaining cap wins.\n        \n        # What if we want to penalize bins that are too large?\n        # For example, if `bins_remain_cap` is more than `2 * item`.\n        \n        # Let's try a score that rewards bins that are \"just enough\" and penalizes bins that are \"too much\".\n        # Score = `1 / (diff + epsilon)` gives highest score for smallest `diff`.\n        # We can add a factor that decreases as `bins_remain_cap` increases beyond `item`.\n        \n        # Let's try `score = (1 / (diff + epsilon)) * (item / (bins_remain_cap + epsilon))`\n        # This combines the inverse difference with a factor that reduces score for larger remaining capacities.\n        \n        # If item = 5, bins = [6, 10, 20]\n        # diff = [1, 5, 15]\n        # v1: [1, 0.2, 0.06]\n        #\n        # New score:\n        # Bin 1 (rem=6, diff=1): (1/1) * (5/6) = 0.83\n        # Bin 2 (rem=10, diff=5): (1/5) * (5/10) = 0.1 * 0.5 = 0.05\n        # Bin 3 (rem=20, diff=15): (1/15) * (5/20) = 0.066 * 0.25 = 0.0165\n        \n        # This seems to penalize larger bins too aggressively, making it similar to First Fit Decreasing if items are sorted.\n        \n        # Let's go back to the idea of prioritizing tight fits, but ensure that if there are multiple tight fits,\n        # we prefer the one that leaves less residual space.\n        \n        # Consider a score that is a function of `diff` and `bins_remain_cap`.\n        # A common approach is to use `(bins_remain_cap - item)` as the primary criterion for tightness.\n        # We want to minimize `bins_remain_cap - item`.\n        \n        # Let's try a score that is higher for smaller `diff`, and among equal `diff`s, it doesn't matter much.\n        # The key is to make the score sensitive to small positive `diff`.\n        \n        # A score like `1 / (diff + epsilon)` is already good.\n        # To make it \"better\", we need to ensure it generalizes well.\n        \n        # Let's try to use the reciprocal of the \"waste\" if the item fits perfectly, or a scaled waste.\n        # For a tight fit, `bins_remain_cap` is close to `item`.\n        # Consider `score = bins_remain_cap / (bins_remain_cap - item + epsilon)`.\n        # This was explored before and seemed to amplify tight fits.\n        \n        # Let's try to make the score more focused on the *proportion* of remaining capacity used.\n        # For a bin to be ideal, `item / bins_remain_cap` should be close to 1.\n        # Score = `1 / abs(item / bins_remain_cap - 1 + epsilon)`\n        # Score = `bins_remain_cap / abs(bins_remain_cap - item + epsilon)`\n        # This is `bins_remain_cap / (diff + epsilon)` for bins that fit.\n        \n        # Let's consider a two-stage approach:\n        # 1. Favor bins with `diff` within a small range (e.g., `0 <= diff < threshold`).\n        # 2. Within that range, pick the bin with minimum `diff`.\n        # 3. If no bin is in that range, pick the bin with the minimum `diff` overall.\n        \n        # This can be encoded into a single score.\n        # Let's use a score that is very high for tight fits, and moderately high for larger fits.\n        \n        # A score that is proportional to `1 / (diff + epsilon)` is good for prioritizing tight fits.\n        # To make it \"better\", we can scale it by a factor that is less sensitive to extremely large capacities.\n        \n        # Let's try `score = (1 + item) / (diff + epsilon)`\n        # If item = 5, bins = [6, 10, 20]\n        # diff = [1, 5, 15]\n        #\n        # v1 `1/(diff+eps)`: [1, 0.2, 0.06]\n        # Proposed `(1+item)/(diff+eps)`:\n        # Bin 1 (rem=6, diff=1): (1+5) / 1 = 6\n        # Bin 2 (rem=10, diff=5): (1+5) / 5 = 1.2\n        # Bin 3 (rem=20, diff=15): (1+5) / 15 = 0.4\n        \n        # This seems to amplify the preference for tight fits even more strongly than `item / (diff + epsilon)`.\n        # It's essentially prioritizing bins with smaller `diff` and giving them a boost proportional to `item + 1`.\n        # This can be interpreted as: \"how much value does this bin's remaining space offer for fitting this item tightly?\"\n        \n        # Let's use `(bins_remain_cap + 1) / (diff + epsilon)`. This is slightly different.\n        # If item = 5, bins = [6, 10, 20]\n        # diff = [1, 5, 15]\n        #\n        # Proposed `(bins_remain_cap + 1) / (diff + epsilon)`:\n        # Bin 1 (rem=6, diff=1): (6+1) / 1 = 7\n        # Bin 2 (rem=10, diff=5): (10+1) / 5 = 11 / 5 = 2.2\n        # Bin 3 (rem=20, diff=15): (20+1) / 15 = 21 / 15 = 1.4\n        \n        # This score favors bins that are *closer* to the item size AND have *more* remaining capacity if the difference is the same.\n        # This is not ideal. We want to prioritize tight fits.\n        \n        # Let's stick to prioritizing tight fits. The score `bins_remain_cap / (diff + epsilon)` is a good candidate.\n        # It means we prefer bins where `bins_remain_cap / item` is close to 1.\n        # `bins_remain_cap / (bins_remain_cap - item + epsilon)`\n        # This is `(item + diff) / (diff + epsilon)`\n        \n        # Consider the goal: minimize the number of bins.\n        # This is achieved by packing items as tightly as possible.\n        # So, prioritize bins where `bins_remain_cap - item` is minimized and non-negative.\n        \n        # Let's consider a score that combines the 'tightness' `1/(diff + epsilon)` with a penalty for 'over-filling' if the bin is much larger.\n        # The score `bins_remain_cap / (diff + epsilon)` already tends to favor smaller remaining capacities if the difference is the same.\n        \n        # Final approach: Prioritize bins with the smallest positive difference.\n        # To make this robust, normalize the difference by the item size.\n        # Score = `1 / (diff / item + epsilon)` if `item > 0`.\n        # Score = `item / (diff + epsilon)` for `item > 0`.\n        # This emphasizes tight fits relative to the item's size.\n        \n        scores = item / (diff + epsilon)\n        \n        priorities[can_fit_mask] = scores\n    \n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a more sophisticated strategy.\n\n    This heuristic prioritizes bins that have just enough remaining capacity to fit the item,\n    while also considering bins that have significantly more capacity as a secondary factor.\n    It aims to reduce fragmentation by favoring tighter fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 1e-9\n    \n    # Calculate the difference between remaining capacity and item size\n    diff = bins_remain_cap - item\n    \n    # Initialize priorities to zero (for bins where the item doesn't fit)\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Identify bins where the item can fit\n    can_fit_mask = diff >= 0\n    \n    # For bins where the item fits, calculate a priority score.\n    # The primary goal is to find bins with a small positive difference (tightest fit).\n    # We use an inverse of (difference + epsilon) for tighter fits to give them higher scores.\n    # We also add a small bonus for bins with much larger remaining capacity to not\n    # completely discard them if no tight fit is available. This bonus is smaller.\n    \n    # Calculate inverse difference for tight fits: higher score for smaller positive difference\n    tight_fit_scores = 1 / (diff[can_fit_mask] + epsilon)\n    \n    # Calculate a secondary score for bins with more remaining capacity than the item size.\n    # This is a simpler inverse of remaining capacity, scaled down.\n    # We want to avoid division by zero for bins with zero remaining capacity if they exist (though item wouldn't fit)\n    # and to give some priority to larger bins if no tight fit is available.\n    # Using the original bins_remain_cap for this secondary scoring.\n    \n    # We can normalize the remaining capacities to get a sense of \"how much space\" is left relative to bin capacity.\n    # However, since we don't have the original bin capacity, we can use a heuristic.\n    # Let's simply use the remaining capacity itself, but scaled.\n    # A simpler approach without original bin capacity is to just use the inverse of difference for those that fit.\n    \n    # Let's refine the logic:\n    # Priority 1: Bins with smallest positive `diff` (tightest fit). This is `1 / (diff + epsilon)`.\n    # Priority 2: Bins with larger `diff`. These are less preferred than tight fits.\n    # A simple way to combine is to give a high score to tight fits and a moderate score to larger fits.\n    \n    # Let's use a piecewise approach for scoring:\n    # For bins that fit:\n    # If diff is very small (e.g., diff < threshold), assign a high priority (e.g., 100 + 1/diff).\n    # If diff is larger, assign a lower priority (e.g., 10 + 1/diff).\n    # This requires tuning `threshold`.\n    \n    # A more robust approach without arbitrary thresholds:\n    # We want to maximize `1/(diff + epsilon)` for tight fits and still assign some score to larger fits.\n    # Consider the ratio of remaining capacity to item size.\n    \n    # Let's try scoring based on inverse difference, and then add a penalty for \"too much\" space.\n    # Or, simply, prioritize bins where `bins_remain_cap` is \"close\" to `item`.\n    \n    # Let's use a score that is high for small positive differences and decreases as the difference grows.\n    # A Gaussian-like function centered around 0 (for `diff`) could work, but it's complex.\n    \n    # Simpler idea: prioritize bins that have *just enough* space.\n    # We can define \"just enough\" as being within a certain percentage of the item size.\n    # For example, if diff is between 0 and `item * tolerance`.\n    \n    # Let's revisit the inverse distance, but modify it to be more sensitive to small differences.\n    # A score that is high for small `diff` and then drops off.\n    # Consider `score = 1 / (diff^2 + epsilon)` or `score = exp(-k * diff)`\n    \n    # Let's try a score that emphasizes the \"tightness\" by squaring the inverse of the difference.\n    # This will amplify the priority for very tight fits.\n    \n    # Calculate the inverse of difference for bins that can fit the item\n    # For bins where it can fit, the priority is proportional to 1 / (difference + epsilon)\n    # We want to boost the priority for smaller differences more significantly.\n    # Let's use (1 / (diff + epsilon))^2 for a stronger emphasis on tightness.\n    \n    # This still might give a very small positive difference a disproportionately high score.\n    \n    # Alternative: Focus on the ratio of remaining capacity to item size.\n    # Bins with `bins_remain_cap / item` close to 1 are good.\n    # Ratio = bins_remain_cap / item. We want ratio ~ 1.\n    # Score could be proportional to `1 / abs(ratio - 1)`.\n    # However, this doesn't account for the absolute amount of space. A bin with 10 capacity\n    # and item 9 (ratio 1.11) is better than bin with 100 capacity and item 9 (ratio 1.01)\n    # if we only consider this ratio. We need to combine it.\n    \n    # Let's stick to the difference but prioritize small positive differences more strongly.\n    # A function like `f(x) = 1/(x+epsilon)` is already good.\n    # What if we add a small bonus for bins that have \"plenty\" of space, but significantly less than tight fits?\n    \n    # Let's try to make it more robust to scale by normalizing.\n    # If we knew the maximum bin capacity, we could normalize. Without it, it's hard.\n    \n    # Back to basics, `priority_v1` favors bins with largest remaining capacity among those that fit.\n    # `1 / (diff + epsilon)` favors bins with smallest `diff`. This is generally good.\n    \n    # How to improve:\n    # 1. Give stronger weight to *very* tight fits.\n    # 2. Ensure that bins that are *almost* full but still fit are prioritized over bins that are nearly empty but fit.\n    \n    # Let's try a compound score:\n    # Score1: Inverse difference (prioritizes tight fits)\n    # Score2: A small bonus for bins that are not too empty, scaled by how much they can fit.\n    \n    # For bins that can fit:\n    # `tight_fit_score = 1 / (diff + epsilon)`\n    \n    # Now, consider the \"emptiness\" of the bin if it fits.\n    # A bin that has `bins_remain_cap` close to `item` is good.\n    # A bin that has `bins_remain_cap` much larger than `item` is less ideal in terms of fragmentation.\n    \n    # Let's define a score that peaks at `diff = 0` (or slightly negative) and decreases.\n    # However, we only consider `diff >= 0`. So we want it to peak at `diff = 0`.\n    \n    # Consider a function that is `1/(diff + epsilon)` for tight fits, and maybe a constant or decaying function for larger fits.\n    \n    # Let's try a score that emphasizes \"just enough\" space more, and \"plenty\" of space less.\n    # If `diff` is small (e.g., < `item / 2`), give it a higher score.\n    # If `diff` is large (e.g., > `item / 2`), give it a lower score.\n    \n    # Let's normalize `diff` relative to the item size.\n    # `normalized_diff = diff / item` (if item > 0)\n    # We want `normalized_diff` close to 0.\n    \n    # Score = `1 / (normalized_diff + epsilon)` for `normalized_diff >= 0`\n    # This is equivalent to `item / (bins_remain_cap - item + epsilon)` which is similar to `priority_v1` but normalized.\n    \n    # Let's try to blend the inverse difference with a penalty for being too \"empty\".\n    # A bin that fits has `bins_remain_cap >= item`.\n    # If `bins_remain_cap` is much larger than `item`, it's \"too empty\".\n    \n    # Let's use a score based on how much of the remaining capacity is *used* by the item.\n    # If `bins_remain_cap` is very close to `item`, then `item / bins_remain_cap` is close to 1.\n    # If `bins_remain_cap` is much larger than `item`, then `item / bins_remain_cap` is close to 0.\n    \n    # So, we want to prioritize bins where `item / bins_remain_cap` is close to 1.\n    # Score = `1 / abs((item / bins_remain_cap) - 1 + epsilon)`\n    # This can be written as `bins_remain_cap / abs(bins_remain_cap - item + epsilon)`.\n    # This is effectively `bins_remain_cap / (diff + epsilon)` for bins that fit.\n    \n    # Let's test this:\n    # item = 5\n    # bins_remain_cap = [10, 6, 20, 5.1]\n    # diff = [5, 1, 15, 0.1]\n    #\n    # priority_v1:\n    # 1/(5+eps) = 0.2\n    # 1/(1+eps) = 1.0\n    # 1/(15+eps) = 0.066\n    # 1/(0.1+eps) = 9.09\n    # Max priority for 5.1 remaining.\n    #\n    # New approach: `bins_remain_cap / (diff + epsilon)`\n    # 10 / (5+eps) = 2.0\n    # 6 / (1+eps) = 6.0\n    # 20 / (15+eps) = 1.33\n    # 5.1 / (0.1+eps) = 51.0\n    # Max priority for 5.1 remaining. This seems to amplify the preference for tight fits.\n    \n    # Let's implement this: `bins_remain_cap / (diff + epsilon)` for bins that fit.\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins where the item fits, calculate the priority score\n    # Prioritize bins where the remaining capacity is closest to the item size.\n    # Score is `bins_remain_cap / (bins_remain_cap - item + epsilon)`\n    # This is equivalent to `bins_remain_cap / (diff + epsilon)`\n    \n    # Add a small epsilon to bins_remain_cap to avoid division by zero if item == bins_remain_cap == 0,\n    # although can_fit_mask should prevent this for item > 0.\n    \n    # Ensure item is not zero to avoid division by zero in potential alternative calculations.\n    # In this case, we are using bins_remain_cap which is always non-negative.\n    \n    # The division `bins_remain_cap / (bins_remain_cap - item + epsilon)` can be large if `bins_remain_cap - item` is small.\n    # This correctly prioritizes tight fits.\n    \n    # Let's make sure the score is well-behaved.\n    # If `bins_remain_cap` is large, and `item` is small, `diff` is large.\n    # `bins_remain_cap / (diff + epsilon)` will be `large / large` -> moderate score.\n    # If `bins_remain_cap` is just slightly larger than `item`, `diff` is small.\n    # `bins_remain_cap / (diff + epsilon)` will be `~item / small` -> high score.\n    \n    # This seems like a good candidate for `priority_v2`.\n    \n    # Avoid division by zero if item is 0, though problem statement implies item > 0.\n    # If item is 0, any bin can fit it with infinite priority if `diff=0`.\n    # For item > 0, `bins_remain_cap` must be >= `item`.\n    \n    # Let's consider `bins_remain_cap - item`. If this is 0, the item perfectly fills the bin.\n    # In that case, `bins_remain_cap / epsilon` would be very large. This is desired.\n    \n    # Calculate the score for bins where the item fits\n    # Score = remaining_capacity / (remaining_capacity - item + epsilon)\n    # This is equivalent to: (item + diff) / (diff + epsilon)\n    # This ratio is maximized when `diff` is minimized (closest to 0).\n    \n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask] / (diff[can_fit_mask] + epsilon)\n    \n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a more sophisticated strategy.\n\n    This heuristic prioritizes bins that have just enough remaining capacity to fit the item,\n    while also considering bins that have significantly more capacity as a secondary factor.\n    It aims to reduce fragmentation by favoring tighter fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 1e-9\n    \n    # Calculate the difference between remaining capacity and item size\n    diff = bins_remain_cap - item\n    \n    # Initialize priorities to zero (for bins where the item doesn't fit)\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Identify bins where the item can fit\n    can_fit_mask = diff >= 0\n    \n    # For bins where the item fits, calculate a priority score.\n    # The primary goal is to find bins with a small positive difference (tightest fit).\n    # We use an inverse of (difference + epsilon) for tighter fits to give them higher scores.\n    # We also add a small bonus for bins with much larger remaining capacity to not\n    # completely discard them if no tight fit is available. This bonus is smaller.\n    \n    # Calculate inverse difference for tight fits: higher score for smaller positive difference\n    tight_fit_scores = 1 / (diff[can_fit_mask] + epsilon)\n    \n    # Calculate a secondary score for bins with more remaining capacity than the item size.\n    # This is a simpler inverse of remaining capacity, scaled down.\n    # We want to avoid division by zero for bins with zero remaining capacity if they exist (though item wouldn't fit)\n    # and to give some priority to larger bins if no tight fit is available.\n    # Using the original bins_remain_cap for this secondary scoring.\n    \n    # We can normalize the remaining capacities to get a sense of \"how much space\" is left relative to bin capacity.\n    # However, since we don't have the original bin capacity, we can use a heuristic.\n    # Let's simply use the remaining capacity itself, but scaled.\n    # A simpler approach without original bin capacity is to just use the inverse of difference for those that fit.\n    \n    # Let's refine the logic:\n    # Priority 1: Bins with smallest positive `diff` (tightest fit). This is `1 / (diff + epsilon)`.\n    # Priority 2: Bins with larger `diff`. These are less preferred than tight fits.\n    # A simple way to combine is to give a high score to tight fits and a moderate score to larger fits.\n    \n    # Let's use a piecewise approach for scoring:\n    # For bins that fit:\n    # If diff is very small (e.g., diff < threshold), assign a high priority (e.g., 100 + 1/diff).\n    # If diff is larger, assign a lower priority (e.g., 10 + 1/diff).\n    # This requires tuning `threshold`.\n    \n    # A more robust approach without arbitrary thresholds:\n    # We want to maximize `1/(diff + epsilon)` for tight fits and still assign some score to larger fits.\n    # Consider the ratio of remaining capacity to item size.\n    \n    # Let's try scoring based on inverse difference, and then add a penalty for \"too much\" space.\n    # Or, simply, prioritize bins where `bins_remain_cap` is \"close\" to `item`.\n    \n    # Let's use a score that is high for small positive differences and decreases as the difference grows.\n    # A Gaussian-like function centered around 0 (for `diff`) could work, but it's complex.\n    \n    # Simpler idea: prioritize bins that have *just enough* space.\n    # We can define \"just enough\" as being within a certain percentage of the item size.\n    # For example, if diff is between 0 and `item * tolerance`.\n    \n    # Let's revisit the inverse distance, but modify it to be more sensitive to small differences.\n    # A score that is high for small `diff` and then drops off.\n    # Consider `score = 1 / (diff^2 + epsilon)` or `score = exp(-k * diff)`\n    \n    # Let's try a score that emphasizes the \"tightness\" by squaring the inverse of the difference.\n    # This will amplify the priority for very tight fits.\n    \n    # Calculate the inverse of difference for bins that can fit the item\n    # For bins where it can fit, the priority is proportional to 1 / (difference + epsilon)\n    # We want to boost the priority for smaller differences more significantly.\n    # Let's use (1 / (diff + epsilon))^2 for a stronger emphasis on tightness.\n    \n    # This still might give a very small positive difference a disproportionately high score.\n    \n    # Alternative: Focus on the ratio of remaining capacity to item size.\n    # Bins with `bins_remain_cap / item` close to 1 are good.\n    # Ratio = bins_remain_cap / item. We want ratio ~ 1.\n    # Score could be proportional to `1 / abs(ratio - 1)`.\n    # However, this doesn't account for the absolute amount of space. A bin with 10 capacity\n    # and item 9 (ratio 1.11) is better than bin with 100 capacity and item 9 (ratio 1.01)\n    # if we only consider this ratio. We need to combine it.\n    \n    # Let's stick to the difference but prioritize small positive differences more strongly.\n    # A function like `f(x) = 1/(x+epsilon)` is already good.\n    # What if we add a small bonus for bins that have \"plenty\" of space, but significantly less than tight fits?\n    \n    # Let's try to make it more robust to scale by normalizing.\n    # If we knew the maximum bin capacity, we could normalize. Without it, it's hard.\n    \n    # Back to basics, `priority_v1` favors bins with largest remaining capacity among those that fit.\n    # `1 / (diff + epsilon)` favors bins with smallest `diff`. This is generally good.\n    \n    # How to improve:\n    # 1. Give stronger weight to *very* tight fits.\n    # 2. Ensure that bins that are *almost* full but still fit are prioritized over bins that are nearly empty but fit.\n    \n    # Let's try a compound score:\n    # Score1: Inverse difference (prioritizes tight fits)\n    # Score2: A small bonus for bins that are not too empty, scaled by how much they can fit.\n    \n    # For bins that can fit:\n    # `tight_fit_score = 1 / (diff + epsilon)`\n    \n    # Now, consider the \"emptiness\" of the bin if it fits.\n    # A bin that has `bins_remain_cap` close to `item` is good.\n    # A bin that has `bins_remain_cap` much larger than `item` is less ideal in terms of fragmentation.\n    \n    # Let's define a score that peaks at `diff = 0` (or slightly negative) and decreases.\n    # However, we only consider `diff >= 0`. So we want it to peak at `diff = 0`.\n    \n    # Consider a function that is `1/(diff + epsilon)` for tight fits, and maybe a constant or decaying function for larger fits.\n    \n    # Let's try a score that emphasizes \"just enough\" space more, and \"plenty\" of space less.\n    # If `diff` is small (e.g., < `item / 2`), give it a higher score.\n    # If `diff` is large (e.g., > `item / 2`), give it a lower score.\n    \n    # Let's normalize `diff` relative to the item size.\n    # `normalized_diff = diff / item` (if item > 0)\n    # We want `normalized_diff` close to 0.\n    \n    # Score = `1 / (normalized_diff + epsilon)` for `normalized_diff >= 0`\n    # This is equivalent to `item / (bins_remain_cap - item + epsilon)` which is similar to `priority_v1` but normalized.\n    \n    # Let's try to blend the inverse difference with a penalty for being too \"empty\".\n    # A bin that fits has `bins_remain_cap >= item`.\n    # If `bins_remain_cap` is much larger than `item`, it's \"too empty\".\n    \n    # Let's use a score based on how much of the remaining capacity is *used* by the item.\n    # If `bins_remain_cap` is very close to `item`, then `item / bins_remain_cap` is close to 1.\n    # If `bins_remain_cap` is much larger than `item`, then `item / bins_remain_cap` is close to 0.\n    \n    # So, we want to prioritize bins where `item / bins_remain_cap` is close to 1.\n    # Score = `1 / abs((item / bins_remain_cap) - 1 + epsilon)`\n    # This can be written as `bins_remain_cap / abs(bins_remain_cap - item + epsilon)`.\n    # This is effectively `bins_remain_cap / (diff + epsilon)` for bins that fit.\n    \n    # Let's test this:\n    # item = 5\n    # bins_remain_cap = [10, 6, 20, 5.1]\n    # diff = [5, 1, 15, 0.1]\n    #\n    # priority_v1:\n    # 1/(5+eps) = 0.2\n    # 1/(1+eps) = 1.0\n    # 1/(15+eps) = 0.066\n    # 1/(0.1+eps) = 9.09\n    # Max priority for 5.1 remaining.\n    #\n    # New approach: `bins_remain_cap / (diff + epsilon)`\n    # 10 / (5+eps) = 2.0\n    # 6 / (1+eps) = 6.0\n    # 20 / (15+eps) = 1.33\n    # 5.1 / (0.1+eps) = 51.0\n    # Max priority for 5.1 remaining. This seems to amplify the preference for tight fits.\n    \n    # Let's implement this: `bins_remain_cap / (diff + epsilon)` for bins that fit.\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins where the item fits, calculate the priority score\n    # Prioritize bins where the remaining capacity is closest to the item size.\n    # Score is `bins_remain_cap / (bins_remain_cap - item + epsilon)`\n    # This is equivalent to `bins_remain_cap / (diff + epsilon)`\n    \n    # Add a small epsilon to bins_remain_cap to avoid division by zero if item == bins_remain_cap == 0,\n    # although can_fit_mask should prevent this for item > 0.\n    \n    # Ensure item is not zero to avoid division by zero in potential alternative calculations.\n    # In this case, we are using bins_remain_cap which is always non-negative.\n    \n    # The division `bins_remain_cap / (bins_remain_cap - item + epsilon)` can be large if `bins_remain_cap - item` is small.\n    # This correctly prioritizes tight fits.\n    \n    # Let's make sure the score is well-behaved.\n    # If `bins_remain_cap` is large, and `item` is small, `diff` is large.\n    # `bins_remain_cap / (diff + epsilon)` will be `large / large` -> moderate score.\n    # If `bins_remain_cap` is just slightly larger than `item`, `diff` is small.\n    # `bins_remain_cap / (diff + epsilon)` will be `~item / small` -> high score.\n    \n    # This seems like a good candidate for `priority_v2`.\n    \n    # Avoid division by zero if item is 0, though problem statement implies item > 0.\n    # If item is 0, any bin can fit it with infinite priority if `diff=0`.\n    # For item > 0, `bins_remain_cap` must be >= `item`.\n    \n    # Let's consider `bins_remain_cap - item`. If this is 0, the item perfectly fills the bin.\n    # In that case, `bins_remain_cap / epsilon` would be very large. This is desired.\n    \n    # Calculate the score for bins where the item fits\n    # Score = remaining_capacity / (remaining_capacity - item + epsilon)\n    # This is equivalent to: (item + diff) / (diff + epsilon)\n    # This ratio is maximized when `diff` is minimized (closest to 0).\n    \n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask] / (diff[can_fit_mask] + epsilon)\n    \n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins with exact or near-exact fits using a ratio-based score.\n\n    This heuristic favors bins where the remaining capacity is as close as possible to\n    the item size, assigning higher scores to tighter fits.\n    \"\"\"\n    epsilon = 1e-9\n    \n    # Initialize priorities to zero for all bins\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the difference between remaining capacity and item size for fitting bins\n    diff = bins_remain_cap[can_fit_mask] - item\n    \n    # Calculate scores for bins where the item fits.\n    # The score is `bins_remain_cap / (diff + epsilon)`.\n    # This ratio emphasizes bins where `diff` is small (tight fits).\n    # For an exact fit (diff=0), the score becomes `bins_remain_cap / epsilon`, which is very high.\n    # For larger differences, the ratio tends to be smaller.\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask] / (diff + epsilon)\n    \n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Exact Fit First with a normalized Best Fit strategy.\n    Prioritizes exact fits, then bins with the least excess capacity\n    relative to the minimum possible excess.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[can_fit_mask]\n    \n    if eligible_bins_cap.size == 0:\n        return priorities\n\n    # Mask for exact fits\n    exact_fit_mask = (eligible_bins_cap == item)\n    \n    if np.any(exact_fit_mask):\n        # Assign highest priority (1.0) to exact fits\n        priorities[can_fit_mask][exact_fit_mask] = 1.0\n        \n        # For non-exact fits, assign a lower priority band (0.0 to 0.9)\n        non_exact_fit_mask = can_fit_mask.copy()\n        non_exact_fit_mask[can_fit_mask][exact_fit_mask] = False\n        \n        if np.any(non_exact_fit_mask):\n            non_exact_bins_cap = bins_remain_cap[non_exact_fit_mask]\n            # Calculate excess capacity for non-exact fits\n            excess_capacities = non_exact_bins_cap - item\n            \n            # Find the minimum excess capacity among non-exact fits\n            min_excess_capacity = np.min(excess_capacities)\n            \n            # Calculate normalized scores: higher score for smaller excess capacity relative to min\n            # Add epsilon to avoid division by zero and to ensure scores are not infinite\n            normalized_scores = 1.0 / (excess_capacities - min_excess_capacity + 1e-9)\n            \n            # Scale these scores to a lower range (e.g., 0.0 to 0.9)\n            max_normalized_score = np.max(normalized_scores)\n            if max_normalized_score > 0:\n                scaled_scores = 0.9 * (normalized_scores / max_normalized_score)\n            else:\n                scaled_scores = np.zeros_like(normalized_scores) # Should not happen if there are non-exact fits\n\n            priorities[non_exact_fit_mask] = scaled_scores\n            \n    else:\n        # If no exact fit, all fitting bins are non-exact fits\n        # Calculate excess capacity for all eligible bins\n        excess_capacities = eligible_bins_cap - item\n        \n        # Find the minimum excess capacity\n        min_excess_capacity = np.min(excess_capacities)\n        \n        # Calculate normalized scores: higher score for smaller excess capacity relative to min\n        normalized_scores = 1.0 / (excess_capacities - min_excess_capacity + 1e-9)\n        \n        # Scale these scores to a range (e.g., 0.0 to 1.0)\n        max_normalized_score = np.max(normalized_scores)\n        if max_normalized_score > 0:\n            scaled_scores = normalized_scores / max_normalized_score\n        else:\n            scaled_scores = np.zeros_like(normalized_scores) # Should not happen\n\n        priorities[can_fit_mask] = scaled_scores\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes exact fits, then bins with minimal excess capacity using normalized inverse differences.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    eligible_bins_indices = np.where(can_fit_mask)[0]\n    \n    if len(eligible_bins_indices) == 0:\n        return priorities\n\n    eligible_capacities = bins_remain_cap[eligible_bins_indices]\n    \n    # Calculate excess capacity for eligible bins\n    excess_capacity = eligible_capacities - item\n    \n    # --- Strategy: Exact Fit First ---\n    # Assign highest priority to bins that are an exact fit.\n    exact_fit_mask = (excess_capacity == 0)\n    exact_fit_indices = eligible_bins_indices[exact_fit_mask]\n    if len(exact_fit_indices) > 0:\n        priorities[exact_fit_indices] = 2.0 # Highest priority score\n        \n    # --- Strategy: Best Fit (Minimal Excess Capacity) ---\n    # For bins that are not an exact fit, prioritize those with the least excess capacity.\n    # Use a normalized inverse of the excess capacity to give higher scores to tighter fits.\n    non_exact_fit_indices = eligible_bins_indices[~exact_fit_mask]\n    \n    if len(non_exact_fit_indices) > 0:\n        non_exact_excess_capacity = excess_capacity[~exact_fit_mask]\n        \n        # Normalize differences: smaller difference means higher priority.\n        # We use (1.0 - normalized_difference) to map smaller excess to higher score.\n        min_excess = np.min(non_exact_excess_capacity)\n        max_excess = np.max(non_exact_excess_capacity)\n        \n        if max_excess - min_excess > 1e-9: # Avoid division by zero if all are the same\n            normalized_excess = (non_exact_excess_capacity - min_excess) / (max_excess - min_excess)\n            # Higher score for smaller normalized excess\n            best_fit_scores = 1.0 - normalized_excess \n        else:\n            best_fit_scores = np.ones_like(non_exact_excess_capacity) # All are equally good if range is zero\n\n        # Scale scores to a range lower than exact fit (e.g., [0.1, 1.0])\n        # Add a base score and scale to ensure they are distinct from exact fit but ordered.\n        scaled_best_fit_scores = 0.1 + best_fit_scores * 0.9\n        \n        priorities[non_exact_fit_indices] = scaled_best_fit_scores\n    \n    # Ensure bins that don't fit at all have zero priority\n    priorities[~can_fit_mask] = 0.0\n    \n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    available_bins_mask = bins_remain_cap >= item\n    \n    if np.any(available_bins_mask):\n        available_bins_remain_cap = bins_remain_cap[available_bins_mask]\n        \n        sorted_indices = np.argsort(available_bins_remain_cap)\n        \n        priorities[available_bins_mask] = 1.0 / (available_bins_remain_cap - item + 1e-9)\n        \n        priorities[available_bins_mask] = np.argsort(np.argsort(priorities[available_bins_mask]))\n    \n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements a Random Fit strategy for the online Bin Packing Problem.\n    Prioritizes bins that can fit the item. A higher priority is given\n    to bins with less remaining capacity that can still fit the item,\n    encouraging tighter packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        available_bins_cap = bins_remain_cap[can_fit_mask]\n        \n        # Assign priority based on how full the bin would become\n        # Smaller remaining capacity (i.e., larger fraction filled) gets higher priority\n        # We invert the remaining capacity to make smaller values larger priorities\n        inverted_cap = 1.0 / (available_bins_cap + 1e-9) # Add epsilon to avoid division by zero\n        \n        # Normalize priorities to be between 0 and 1\n        min_p = np.min(inverted_cap)\n        max_p = np.max(inverted_cap)\n        \n        if max_p - min_p > 1e-9:\n            normalized_priorities = (inverted_cap - min_p) / (max_p - min_p)\n        else:\n            normalized_priorities = np.ones_like(inverted_cap) * 0.5 # Uniform priority if all capacities are the same\n        \n        priorities[can_fit_mask] = normalized_priorities\n    \n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Exact Fit First with a scaled inverse of remaining space for non-exact fits.\n\n    Prioritizes exact fits, then favors bins with the least space after placement,\n    scaled to ensure they are lower than exact fit scores.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    epsilon = 1e-9\n\n    # Exact fit has the highest priority\n    exact_fit_mask = np.abs(bins_remain_cap - item) < epsilon\n    priorities[exact_fit_mask] = 1.0\n\n    # For bins that can fit but are not exact fits\n    can_fit_mask = bins_remain_cap >= item\n    non_exact_fit_mask = can_fit_mask & ~exact_fit_mask\n\n    if np.any(non_exact_fit_mask):\n        space_after_placement = bins_remain_cap[non_exact_fit_mask] - item\n        \n        # Use inverse of space_after_placement to favor smaller remaining space\n        # Scale these scores to be less than 1.0 to ensure exact fits are always preferred.\n        # A simple inverse without scaling might result in scores higher than 1.0 if the difference is small.\n        # We map the inverse scores to a range [0.5, 0.99]\n        min_secondary_priority = 0.5\n        max_secondary_priority = 0.99\n\n        # Calculate inverse scores for non-exact fits\n        inverse_scores = 1.0 / (space_after_placement + epsilon)\n\n        # Normalize these inverse scores to [0, 1]\n        if inverse_scores.size > 1:\n            normalized_inverse_scores = (inverse_scores - np.min(inverse_scores)) / (np.max(inverse_scores) - np.min(inverse_scores) + epsilon)\n        else:\n            normalized_inverse_scores = np.ones_like(inverse_scores) # If only one non-exact bin, it gets max secondary score\n\n        # Scale normalized scores to the desired secondary priority range\n        scaled_secondary_scores = min_secondary_priority + normalized_inverse_scores * (max_secondary_priority - min_secondary_priority)\n        \n        priorities[non_exact_fit_mask] = scaled_secondary_scores\n        \n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Exact Fit First with a scaled inverse of remaining space for non-exact fits.\n\n    Prioritizes exact fits, then favors bins with the least space after placement,\n    scaled to ensure they are lower than exact fit scores.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    epsilon = 1e-9\n\n    # Exact fit has the highest priority\n    exact_fit_mask = np.abs(bins_remain_cap - item) < epsilon\n    priorities[exact_fit_mask] = 1.0\n\n    # For bins that can fit but are not exact fits\n    can_fit_mask = bins_remain_cap >= item\n    non_exact_fit_mask = can_fit_mask & ~exact_fit_mask\n\n    if np.any(non_exact_fit_mask):\n        space_after_placement = bins_remain_cap[non_exact_fit_mask] - item\n        \n        # Use inverse of space_after_placement to favor smaller remaining space\n        # Scale these scores to be less than 1.0 to ensure exact fits are always preferred.\n        # A simple inverse without scaling might result in scores higher than 1.0 if the difference is small.\n        # We map the inverse scores to a range [0.5, 0.99]\n        min_secondary_priority = 0.5\n        max_secondary_priority = 0.99\n\n        # Calculate inverse scores for non-exact fits\n        inverse_scores = 1.0 / (space_after_placement + epsilon)\n\n        # Normalize these inverse scores to [0, 1]\n        if inverse_scores.size > 1:\n            normalized_inverse_scores = (inverse_scores - np.min(inverse_scores)) / (np.max(inverse_scores) - np.min(inverse_scores) + epsilon)\n        else:\n            normalized_inverse_scores = np.ones_like(inverse_scores) # If only one non-exact bin, it gets max secondary score\n\n        # Scale normalized scores to the desired secondary priority range\n        scaled_secondary_scores = min_secondary_priority + normalized_inverse_scores * (max_secondary_priority - min_secondary_priority)\n        \n        priorities[non_exact_fit_mask] = scaled_secondary_scores\n        \n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins by favoring exact fits, then closest fits, using a smooth scoring.\n\n    Combines Exact Fit First and Inverse Distance strategies:\n    1. High priority for bins where remaining_capacity == item.\n    2. Medium priority for bins where remaining_capacity > item, inversely\n       proportional to the difference (remaining_capacity - item).\n    3. Zero priority for bins where remaining_capacity < item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n\n    if len(fitting_bins_cap) > 0:\n        # Prioritize exact fits with a high score\n        exact_fit_mask = fitting_bins_cap == item\n        priorities[fitting_bins_indices[exact_fit_mask]] = 2.0  # Highest priority\n\n        # For bins that are not exact fits, calculate priority based on how close they are\n        non_exact_fitting_bins_cap = fitting_bins_cap[~exact_fit_mask]\n        non_exact_fitting_bins_indices = fitting_bins_indices[~exact_fit_mask]\n\n        if len(non_exact_fitting_bins_cap) > 0:\n            # Calculate the difference (space after placing the item)\n            space_after_placement = non_exact_fitting_bins_cap - item\n\n            # Normalize these differences to a [0, 1] range for scoring.\n            # Smaller difference means higher priority.\n            min_diff = np.min(space_after_placement)\n            max_diff = np.max(space_after_placement)\n\n            if max_diff - min_diff > 1e-9: # Avoid division by zero if all differences are the same\n                normalized_diff = (space_after_placement - min_diff) / (max_diff - min_diff)\n            else:\n                normalized_diff = np.zeros_like(space_after_placement)\n            \n            # Assign priorities: inverse relationship with normalized difference, scaled down.\n            # This gives medium priority, lower than exact fits.\n            # Add a small offset to ensure it's greater than 0 for fitting bins.\n            priorities[non_exact_fitting_bins_indices] = 1.0 - normalized_diff + 0.1\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins by favoring exact fits, then closest fits, using a smooth scoring.\n\n    Combines Exact Fit First and Inverse Distance strategies:\n    1. High priority for bins where remaining_capacity == item.\n    2. Medium priority for bins where remaining_capacity > item, inversely\n       proportional to the difference (remaining_capacity - item).\n    3. Zero priority for bins where remaining_capacity < item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n\n    if len(fitting_bins_cap) > 0:\n        # Prioritize exact fits with a high score\n        exact_fit_mask = fitting_bins_cap == item\n        priorities[fitting_bins_indices[exact_fit_mask]] = 2.0  # Highest priority\n\n        # For bins that are not exact fits, calculate priority based on how close they are\n        non_exact_fitting_bins_cap = fitting_bins_cap[~exact_fit_mask]\n        non_exact_fitting_bins_indices = fitting_bins_indices[~exact_fit_mask]\n\n        if len(non_exact_fitting_bins_cap) > 0:\n            # Calculate the difference (space after placing the item)\n            space_after_placement = non_exact_fitting_bins_cap - item\n\n            # Normalize these differences to a [0, 1] range for scoring.\n            # Smaller difference means higher priority.\n            min_diff = np.min(space_after_placement)\n            max_diff = np.max(space_after_placement)\n\n            if max_diff - min_diff > 1e-9: # Avoid division by zero if all differences are the same\n                normalized_diff = (space_after_placement - min_diff) / (max_diff - min_diff)\n            else:\n                normalized_diff = np.zeros_like(space_after_placement)\n            \n            # Assign priorities: inverse relationship with normalized difference, scaled down.\n            # This gives medium priority, lower than exact fits.\n            # Add a small offset to ensure it's greater than 0 for fitting bins.\n            priorities[non_exact_fitting_bins_indices] = 1.0 - normalized_diff + 0.1\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins by favoring exact fits, then closest fits, using a smooth scoring.\n\n    Combines Exact Fit First and Inverse Distance strategies:\n    1. High priority for bins where remaining_capacity == item.\n    2. Medium priority for bins where remaining_capacity > item, inversely\n       proportional to the difference (remaining_capacity - item).\n    3. Zero priority for bins where remaining_capacity < item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n\n    if len(fitting_bins_cap) > 0:\n        # Prioritize exact fits with a high score\n        exact_fit_mask = fitting_bins_cap == item\n        priorities[fitting_bins_indices[exact_fit_mask]] = 2.0  # Highest priority\n\n        # For bins that are not exact fits, calculate priority based on how close they are\n        non_exact_fitting_bins_cap = fitting_bins_cap[~exact_fit_mask]\n        non_exact_fitting_bins_indices = fitting_bins_indices[~exact_fit_mask]\n\n        if len(non_exact_fitting_bins_cap) > 0:\n            # Calculate the difference (space after placing the item)\n            space_after_placement = non_exact_fitting_bins_cap - item\n\n            # Normalize these differences to a [0, 1] range for scoring.\n            # Smaller difference means higher priority.\n            min_diff = np.min(space_after_placement)\n            max_diff = np.max(space_after_placement)\n\n            if max_diff - min_diff > 1e-9: # Avoid division by zero if all differences are the same\n                normalized_diff = (space_after_placement - min_diff) / (max_diff - min_diff)\n            else:\n                normalized_diff = np.zeros_like(space_after_placement)\n            \n            # Assign priorities: inverse relationship with normalized difference, scaled down.\n            # This gives medium priority, lower than exact fits.\n            # Add a small offset to ensure it's greater than 0 for fitting bins.\n            priorities[non_exact_fitting_bins_indices] = 1.0 - normalized_diff + 0.1\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Exact Fit preference with a Best Fit fallback.\n    Prioritizes bins that exactly fit the item, then bins with minimal excess capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        available_bins_cap = bins_remain_cap[can_fit_mask]\n        \n        # Calculate the exact fit score (highest priority)\n        exact_fit_mask = available_bins_cap == item\n        if np.any(exact_fit_mask):\n            priorities[can_fit_mask][exact_fit_mask] = 1.0\n            \n        # Calculate the best fit score for remaining bins (secondary priority)\n        non_exact_fit_mask = ~exact_fit_mask\n        if np.any(non_exact_fit_mask):\n            remaining_available_bins_cap = available_bins_cap[non_exact_fit_mask]\n            \n            # Prioritize bins with the least remaining capacity after placing the item\n            # Higher priority for smaller remaining space (closer fit)\n            # We invert (remaining_capacity - item) to make smaller values higher priorities\n            space_after_placement = remaining_available_bins_cap - item\n            \n            # Avoid division by zero and normalize to avoid extreme values\n            inverted_space = 1.0 / (space_after_placement + 1e-9) \n            \n            min_inv_space = np.min(inverted_space)\n            max_inv_space = np.max(inverted_space)\n            \n            if max_inv_space - min_inv_space > 1e-9:\n                normalized_priorities = (inverted_space - min_inv_space) / (max_inv_space - min_inv_space)\n            else:\n                normalized_priorities = np.ones_like(inverted_space) * 0.5 # Uniform if all same\n\n            # Add a small base priority to non-exact fits so they are considered after exact fits\n            # but only if they have higher priority than non-fitting bins (which is always 0)\n            priorities[can_fit_mask][non_exact_fit_mask] = normalized_priorities * 0.9\n            \n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes exact fits, then bins with minimal excess capacity using a scaled inverse difference.\n\n    This heuristic gives the highest priority to bins that exactly fit the item.\n    For bins that don't fit exactly but can accommodate the item, it assigns\n    priority based on the inverse of the excess capacity, scaled to emphasize\n    tighter fits.\n    \"\"\"\n    epsilon = 1e-9\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate excess capacity for bins that can fit the item\n    excess_capacity = bins_remain_cap[can_fit_mask] - item\n\n    # Assign highest priority (1.0) to exact fits\n    exact_fit_mask_local = (excess_capacity < epsilon)\n    priorities[can_fit_mask][exact_fit_mask_local] = 1.0\n\n    # For bins that don't fit exactly, calculate priority based on inverse excess capacity\n    close_fit_mask_local = ~exact_fit_mask_local\n    \n    if np.any(close_fit_mask_local):\n        eligible_excess_capacity = excess_capacity[close_fit_mask_local]\n        \n        # Calculate a score proportional to 1 / (excess_capacity + epsilon).\n        # This favors smaller excess capacities (tighter fits).\n        # We normalize this score to be between 0 and 1 to avoid overwhelming exact fits.\n        # A simple normalization: 1 / (1 + scaled_excess_capacity)\n        # Where scaled_excess_capacity is normalized difference.\n        \n        # To ensure scores are distinct from exact fits and rank close fits,\n        # let's use a score that starts below 1.0 and increases as excess capacity decreases.\n        # A simple way is to map the inverse excess capacity to a range like [0.1, 0.9].\n        # The inverse of excess capacity is `1 / (eligible_excess_capacity + epsilon)`.\n        # To bound this, we can consider the maximum inverse excess capacity.\n        \n        # A robust way: Use the inverse of (normalized excess + 1) to get scores in (0, 1].\n        # Let's normalize excess capacity relative to the item size itself,\n        # but ensuring we handle cases where item is very small or zero gracefully if needed.\n        \n        # A simpler score: `1 / (excess_capacity + epsilon)`\n        # Let's scale this to be less than 1.0.\n        # A common approach is `1 / (1 + scaled_value)`.\n        # Let's use `1 / (1 + normalized_excess_capacity)` where normalized is between 0 and some value.\n        \n        # Alternative: score = 0.5 + 0.45 * (1 / (normalized_excess_capacity + epsilon))\n        # This maps normalized excess from 0 to some value into [0.5, ~0.95].\n        \n        # Let's use the remaining capacity itself, but inverted and scaled.\n        # Score = remaining_cap / (remaining_cap - item + epsilon)\n        # This is `(item + excess) / (excess + epsilon)`.\n        # To keep it below 1, we can divide by a factor, e.g., `(item + excess) / (excess + epsilon) / (item/item + epsilon)`\n        \n        # A better approach is to normalize the excess capacity and then use its inverse.\n        # Normalize excess capacity: consider a reasonable upper bound for excess_capacity.\n        # If we assume item size is at most B (max bin capacity), then excess capacity is at most B.\n        # Let's consider `excess_capacity / item` (if item > 0).\n        \n        # A simple and effective score for close fits: `1 / (excess_capacity + epsilon)`\n        # We need to scale this to be less than 1.0.\n        # Find the maximum inverse excess capacity among close fits.\n        max_inv_excess = np.max(1.0 / (eligible_excess_capacity + epsilon))\n        \n        # Scale scores for close fits to be in the range [0.1, 0.9]\n        # Score = 0.1 + 0.8 * (1.0 / (eligible_excess_capacity + epsilon)) / max_inv_excess\n        # This ensures that the tightest fit gets ~0.9 and others scale down.\n        scores_for_close_fits = 0.1 + 0.8 * (1.0 / (eligible_excess_capacity + epsilon)) / (max_inv_excess + epsilon)\n        \n        # Ensure scores are not exactly 1.0 (reserved for exact fits)\n        scores_for_close_fits = np.minimum(scores_for_close_fits, 0.999)\n        \n        priorities[can_fit_mask][close_fit_mask_local] = scores_for_close_fits\n        \n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes exact fits, then bins with minimal excess capacity using a scaled inverse difference.\n\n    This heuristic gives the highest priority to bins that exactly fit the item.\n    For bins that don't fit exactly but can accommodate the item, it assigns\n    priority based on the inverse of the excess capacity, scaled to emphasize\n    tighter fits.\n    \"\"\"\n    epsilon = 1e-9\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate excess capacity for bins that can fit the item\n    excess_capacity = bins_remain_cap[can_fit_mask] - item\n\n    # Assign highest priority (1.0) to exact fits\n    exact_fit_mask_local = (excess_capacity < epsilon)\n    priorities[can_fit_mask][exact_fit_mask_local] = 1.0\n\n    # For bins that don't fit exactly, calculate priority based on inverse excess capacity\n    close_fit_mask_local = ~exact_fit_mask_local\n    \n    if np.any(close_fit_mask_local):\n        eligible_excess_capacity = excess_capacity[close_fit_mask_local]\n        \n        # Calculate a score proportional to 1 / (excess_capacity + epsilon).\n        # This favors smaller excess capacities (tighter fits).\n        # We normalize this score to be between 0 and 1 to avoid overwhelming exact fits.\n        # A simple normalization: 1 / (1 + scaled_excess_capacity)\n        # Where scaled_excess_capacity is normalized difference.\n        \n        # To ensure scores are distinct from exact fits and rank close fits,\n        # let's use a score that starts below 1.0 and increases as excess capacity decreases.\n        # A simple way is to map the inverse excess capacity to a range like [0.1, 0.9].\n        # The inverse of excess capacity is `1 / (eligible_excess_capacity + epsilon)`.\n        # To bound this, we can consider the maximum inverse excess capacity.\n        \n        # A robust way: Use the inverse of (normalized excess + 1) to get scores in (0, 1].\n        # Let's normalize excess capacity relative to the item size itself,\n        # but ensuring we handle cases where item is very small or zero gracefully if needed.\n        \n        # A simpler score: `1 / (excess_capacity + epsilon)`\n        # Let's scale this to be less than 1.0.\n        # A common approach is `1 / (1 + scaled_value)`.\n        # Let's use `1 / (1 + normalized_excess_capacity)` where normalized is between 0 and some value.\n        \n        # Alternative: score = 0.5 + 0.45 * (1 / (normalized_excess_capacity + epsilon))\n        # This maps normalized excess from 0 to some value into [0.5, ~0.95].\n        \n        # Let's use the remaining capacity itself, but inverted and scaled.\n        # Score = remaining_cap / (remaining_cap - item + epsilon)\n        # This is `(item + excess) / (excess + epsilon)`.\n        # To keep it below 1, we can divide by a factor, e.g., `(item + excess) / (excess + epsilon) / (item/item + epsilon)`\n        \n        # A better approach is to normalize the excess capacity and then use its inverse.\n        # Normalize excess capacity: consider a reasonable upper bound for excess_capacity.\n        # If we assume item size is at most B (max bin capacity), then excess capacity is at most B.\n        # Let's consider `excess_capacity / item` (if item > 0).\n        \n        # A simple and effective score for close fits: `1 / (excess_capacity + epsilon)`\n        # We need to scale this to be less than 1.0.\n        # Find the maximum inverse excess capacity among close fits.\n        max_inv_excess = np.max(1.0 / (eligible_excess_capacity + epsilon))\n        \n        # Scale scores for close fits to be in the range [0.1, 0.9]\n        # Score = 0.1 + 0.8 * (1.0 / (eligible_excess_capacity + epsilon)) / max_inv_excess\n        # This ensures that the tightest fit gets ~0.9 and others scale down.\n        scores_for_close_fits = 0.1 + 0.8 * (1.0 / (eligible_excess_capacity + epsilon)) / (max_inv_excess + epsilon)\n        \n        # Ensure scores are not exactly 1.0 (reserved for exact fits)\n        scores_for_close_fits = np.minimum(scores_for_close_fits, 0.999)\n        \n        priorities[can_fit_mask][close_fit_mask_local] = scores_for_close_fits\n        \n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Exact Fit preference with a Best Fit fallback.\n    Prioritizes bins that exactly fit the item, then bins with minimal excess capacity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        available_bins_cap = bins_remain_cap[can_fit_mask]\n        \n        # Calculate the exact fit score (highest priority)\n        exact_fit_mask = available_bins_cap == item\n        if np.any(exact_fit_mask):\n            priorities[can_fit_mask][exact_fit_mask] = 1.0\n            \n        # Calculate the best fit score for remaining bins (secondary priority)\n        non_exact_fit_mask = ~exact_fit_mask\n        if np.any(non_exact_fit_mask):\n            remaining_available_bins_cap = available_bins_cap[non_exact_fit_mask]\n            \n            # Prioritize bins with the least remaining capacity after placing the item\n            # Higher priority for smaller remaining space (closer fit)\n            # We invert (remaining_capacity - item) to make smaller values higher priorities\n            space_after_placement = remaining_available_bins_cap - item\n            \n            # Avoid division by zero and normalize to avoid extreme values\n            inverted_space = 1.0 / (space_after_placement + 1e-9) \n            \n            min_inv_space = np.min(inverted_space)\n            max_inv_space = np.max(inverted_space)\n            \n            if max_inv_space - min_inv_space > 1e-9:\n                normalized_priorities = (inverted_space - min_inv_space) / (max_inv_space - min_inv_space)\n            else:\n                normalized_priorities = np.ones_like(inverted_space) * 0.5 # Uniform if all same\n\n            # Add a small base priority to non-exact fits so they are considered after exact fits\n            # but only if they have higher priority than non-fitting bins (which is always 0)\n            priorities[can_fit_mask][non_exact_fit_mask] = normalized_priorities * 0.9\n            \n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes exact fits, then bins with minimal excess capacity using a scaled inverse difference.\n\n    This heuristic gives the highest priority to bins that exactly fit the item.\n    For bins that don't fit exactly but can accommodate the item, it assigns\n    priority based on the inverse of the excess capacity, scaled to emphasize\n    tighter fits.\n    \"\"\"\n    epsilon = 1e-9\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate excess capacity for bins that can fit the item\n    excess_capacity = bins_remain_cap[can_fit_mask] - item\n\n    # Assign highest priority (1.0) to exact fits\n    exact_fit_mask_local = (excess_capacity < epsilon)\n    priorities[can_fit_mask][exact_fit_mask_local] = 1.0\n\n    # For bins that don't fit exactly, calculate priority based on inverse excess capacity\n    close_fit_mask_local = ~exact_fit_mask_local\n    \n    if np.any(close_fit_mask_local):\n        eligible_excess_capacity = excess_capacity[close_fit_mask_local]\n        \n        # Calculate a score proportional to 1 / (excess_capacity + epsilon).\n        # This favors smaller excess capacities (tighter fits).\n        # We normalize this score to be between 0 and 1 to avoid overwhelming exact fits.\n        # A simple normalization: 1 / (1 + scaled_excess_capacity)\n        # Where scaled_excess_capacity is normalized difference.\n        \n        # To ensure scores are distinct from exact fits and rank close fits,\n        # let's use a score that starts below 1.0 and increases as excess capacity decreases.\n        # A simple way is to map the inverse excess capacity to a range like [0.1, 0.9].\n        # The inverse of excess capacity is `1 / (eligible_excess_capacity + epsilon)`.\n        # To bound this, we can consider the maximum inverse excess capacity.\n        \n        # A robust way: Use the inverse of (normalized excess + 1) to get scores in (0, 1].\n        # Let's normalize excess capacity relative to the item size itself,\n        # but ensuring we handle cases where item is very small or zero gracefully if needed.\n        \n        # A simpler score: `1 / (excess_capacity + epsilon)`\n        # Let's scale this to be less than 1.0.\n        # A common approach is `1 / (1 + scaled_value)`.\n        # Let's use `1 / (1 + normalized_excess_capacity)` where normalized is between 0 and some value.\n        \n        # Alternative: score = 0.5 + 0.45 * (1 / (normalized_excess_capacity + epsilon))\n        # This maps normalized excess from 0 to some value into [0.5, ~0.95].\n        \n        # Let's use the remaining capacity itself, but inverted and scaled.\n        # Score = remaining_cap / (remaining_cap - item + epsilon)\n        # This is `(item + excess) / (excess + epsilon)`.\n        # To keep it below 1, we can divide by a factor, e.g., `(item + excess) / (excess + epsilon) / (item/item + epsilon)`\n        \n        # A better approach is to normalize the excess capacity and then use its inverse.\n        # Normalize excess capacity: consider a reasonable upper bound for excess_capacity.\n        # If we assume item size is at most B (max bin capacity), then excess capacity is at most B.\n        # Let's consider `excess_capacity / item` (if item > 0).\n        \n        # A simple and effective score for close fits: `1 / (excess_capacity + epsilon)`\n        # We need to scale this to be less than 1.0.\n        # Find the maximum inverse excess capacity among close fits.\n        max_inv_excess = np.max(1.0 / (eligible_excess_capacity + epsilon))\n        \n        # Scale scores for close fits to be in the range [0.1, 0.9]\n        # Score = 0.1 + 0.8 * (1.0 / (eligible_excess_capacity + epsilon)) / max_inv_excess\n        # This ensures that the tightest fit gets ~0.9 and others scale down.\n        scores_for_close_fits = 0.1 + 0.8 * (1.0 / (eligible_excess_capacity + epsilon)) / (max_inv_excess + epsilon)\n        \n        # Ensure scores are not exactly 1.0 (reserved for exact fits)\n        scores_for_close_fits = np.minimum(scores_for_close_fits, 0.999)\n        \n        priorities[can_fit_mask][close_fit_mask_local] = scores_for_close_fits\n        \n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Exact Fit priority with a normalized Best Fit strategy.\n    Gives highest priority to exact fits, then prioritizes bins with\n    the smallest remaining capacity that can fit the item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    eligible_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Exact Fit Strategy ---\n    # Prioritize bins that fit the item exactly with a high score\n    exact_fit_mask_local = (eligible_bins_remain_cap == item)\n    \n    # --- Normalized Best Fit Strategy ---\n    # For bins that do not fit exactly, calculate a score based on how close they are\n    # to fitting the item. Prioritize those with minimal excess capacity.\n    close_fit_mask_local = ~exact_fit_mask_local\n    \n    # Calculate excess capacity for close fits\n    excess_capacities = eligible_bins_remain_cap[close_fit_mask_local] - item\n    \n    # Assign high priority to exact fits\n    if np.any(exact_fit_mask_local):\n        priorities[can_fit_mask][exact_fit_mask_local] = 1.0\n        \n    # If there are close fits, calculate their priorities\n    if np.any(close_fit_mask_local):\n        # Normalize excess capacities to be between 0 and 1\n        # A smaller excess capacity should get a higher score.\n        # We use 1 / (normalized_excess + epsilon) to give higher scores to smaller excesses.\n        min_excess = 0.0\n        max_excess = np.max(excess_capacities) # Max excess capacity among close fits\n\n        # Avoid division by zero if all close fits have the same excess capacity\n        if max_excess > min_excess:\n            normalized_excess = (excess_capacities - min_excess) / (max_excess - min_excess)\n        else:\n            normalized_excess = np.zeros_like(excess_capacities) # All close fits have same excess\n        \n        # Calculate scores: higher score for smaller normalized excess capacity\n        # Add a base score to differentiate from bins that don't fit (score 0)\n        # and ensure scores are distinct from exact fits.\n        close_fit_scores = 0.5 + 0.45 * (1.0 / (normalized_excess + 1e-9)) # Scale to [0.5, ~0.95]\n        \n        # Ensure scores are not greater than exact fit priority\n        close_fit_scores = np.minimum(close_fit_scores, 0.99) \n        \n        # Assign scores to the corresponding bins\n        priorities[can_fit_mask][close_fit_mask_local] = close_fit_scores\n\n    # Ensure exact fits have priority over close fits if they overlap in scoring range\n    if np.any(exact_fit_mask_local) and np.any(close_fit_mask_local):\n         priorities[can_fit_mask][exact_fit_mask_local] = 1.0 # Re-assert max priority for exact fits\n\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}