{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    A mutated version of priority_v1 for the online Bin Packing Problem.\n\n    This heuristic prioritizes bins that can exactly fit the item (zero remaining capacity after packing).\n    Among bins that fit, it favors those with the minimal excess capacity (tightest fit).\n    Bins with larger excess capacity are given lower scores, but still positive if they fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 1e-9\n    \n    # Initialize priorities to zero (for bins where the item doesn't fit)\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the difference between remaining capacity and item size for bins that can fit\n    diff = bins_remain_cap[can_fit_mask] - item\n    \n    # Assign a very high priority to bins that provide an exact fit\n    # These are bins where diff is zero or very close to zero.\n    exact_fit_mask = diff < epsilon\n    \n    # For exact fits, assign the highest possible priority value.\n    # We use a value significantly larger than any possible score from other bins.\n    # The score for non-exact fits will be related to item / diff, which is bounded.\n    # A very large constant or a value based on item size is appropriate.\n    # Let's use a score that ensures it's greater than any other score.\n    # The maximum finite score from the next step would be roughly (item + large_diff) / large_diff ~ 1,\n    # or if diff is tiny, item / epsilon.\n    # So, let's use something like item / epsilon * 2.\n    # A simpler way is to assign a large number, but tying it to item size is better.\n    # Let's assign a score proportional to item, effectively prioritizing exact fits for larger items.\n    \n    # The highest priority will be for bins with `diff` very close to 0.\n    # We can assign a score of `item / epsilon` for these.\n    # For other bins that fit, the score is `bins_remain_cap / diff`.\n    # So, `(item + diff) / diff`\n    \n    # To ensure exact fits are always best, let's give them a score that's guaranteed to be higher.\n    # For bins that fit, the score is item / (bins_remain_cap - item + epsilon) = item / (diff + epsilon)\n    # For exact fits (diff approx 0), this score becomes very large.\n    # Let's ensure a distinct separation.\n    \n    # For bins that fit:\n    # If diff is effectively zero, assign a very high score.\n    # Otherwise, assign a score inversely proportional to the difference.\n    # A good score for non-exact fits is `item / (diff + epsilon)` which is equivalent to `bins_remain_cap / (diff + epsilon)`.\n    \n    # Let's combine these:\n    # If diff < epsilon (exact fit): high score.\n    # If diff >= epsilon (close or loose fit): score based on inverse difference.\n    \n    # Create a temporary array for the scores of bins that can fit\n    fitting_bins_scores = np.zeros_like(diff)\n    \n    # Identify exact fits within the fitting bins\n    exact_fit_in_fitting_mask = diff < epsilon\n    \n    # Assign a very high priority to exact fits. This ensures they are chosen first.\n    # The exact score should be higher than any calculated score for non-exact fits.\n    # The non-exact fit score is `bins_remain_cap / diff`. If diff is small but positive, this can be large.\n    # To make exact fits superior, assign a score that's at least `max(bins_remain_cap / diff)` + a margin.\n    # A simpler, robust approach: assign a score that depends on the item size, making exact fits for larger items\n    # more \"valuable\".\n    \n    # Let's try this:\n    # For exact fits: `item * (1 / epsilon)`\n    # For other fits: `item / (diff + epsilon)` (This is the same as `bins_remain_cap / (diff + epsilon)`)\n    # This makes exact fits have a very high score, proportional to the item size.\n    \n    # For bins that fit:\n    # If `diff` is close to 0: `high_priority = item * 1e10` (or some large multiplier)\n    # If `diff` > 0: `priority = item / (diff + epsilon)`\n    \n    # Let's assign the highest priority to exact fits.\n    # The score for other bins is `bins_remain_cap / (diff + epsilon)`.\n    # If `diff` is small, this value is large. If `diff` is large, this value is closer to 1.\n    # We want exact fits to be strictly better.\n    \n    # Assign a very large number to exact fits to ensure they are always picked.\n    # A value like 1e10 or even larger would work, but making it relative is better.\n    # Let's use a value that's guaranteed to be larger than `bins_remain_cap / diff` for any `diff > epsilon`.\n    # The maximum value of `bins_remain_cap` is not known.\n    # If `diff` is very small (e.g., `epsilon`), score is `item / epsilon`.\n    # So, `item / epsilon * 2` should be safe.\n    \n    # Calculate scores for all bins that can fit.\n    # Score = `bins_remain_cap / (diff + epsilon)`\n    # This inherently gives higher scores to tighter fits.\n    \n    # We want to explicitly boost exact fits.\n    # If `diff < epsilon`: this is an exact fit. Give it a bonus.\n    \n    # Let's use a score that is proportional to `bins_remain_cap / diff` but with a large bonus for exact fits.\n    # `score = bins_remain_cap / (diff + epsilon)`\n    # If `diff < epsilon`, add a large bonus: `score += LargeBonus`\n    \n    # This can be implemented by modifying the `diff` for exact fits.\n    # For exact fits, we want `diff` to be effectively 0.\n    # For non-exact fits, `diff` is positive.\n    \n    # A common strategy is:\n    # 1. Assign a very high fixed priority to exact fits.\n    # 2. For other fits, assign a priority that decreases as `diff` increases.\n    #    The `bins_remain_cap / (diff + epsilon)` is a good candidate.\n    \n    # Let's assign a base score for all fitting bins:\n    # `base_scores = bins_remain_cap[can_fit_mask] / (diff + epsilon)`\n    \n    # Now, identify exact fits among those that can fit.\n    exact_fits_indices = np.where(diff < epsilon)[0]\n    \n    # For these exact fits, assign a significantly higher score.\n    # Let's make their score `item / epsilon * 2` to ensure it's higher than other potential scores.\n    # The scores for non-exact fits are `bins_remain_cap / diff`.\n    # If `diff` is `epsilon`, score is `bins_remain_cap / epsilon`.\n    # So `item / epsilon * 2` is indeed a good choice for exact fits.\n    \n    # Calculate scores for all bins that can fit using the `bins_remain_cap / diff` logic\n    fitting_bins_scores = bins_remain_cap[can_fit_mask] / (diff + epsilon)\n    \n    # Identify exact fits among these:\n    exact_fit_indices_in_fitting = np.where(diff < epsilon)[0]\n    \n    # Boost the priority for exact fits.\n    # Assign a score that is guaranteed to be higher than any calculated score.\n    # A simple approach is to add a large value.\n    # Let's assign a score of `item * 1e10` for exact fits.\n    # For other fits, the score is `bins_remain_cap / diff`.\n    # If `diff` is very small (e.g. `epsilon`), the score is `bins_remain_cap / epsilon`.\n    # Since `bins_remain_cap >= item`, this is at least `item / epsilon`.\n    # So `item * 1e10` is a safe high value.\n    \n    # For indices in `fitting_bins_scores` corresponding to exact fits:\n    # The original `diff` for these was < epsilon.\n    # `fitting_bins_scores[exact_fit_indices_in_fitting]` would be `bins_remain_cap[exact_fit_indices_in_fitting] / diff_values_for_exact_fits`.\n    # If `diff` is tiny, this is large.\n    \n    # Let's re-calculate scores to ensure clarity and separation:\n    scores_for_fitting = np.zeros_like(diff)\n    \n    # Bins with exact fit (diff is effectively 0)\n    exact_fit_mask_for_fitting = diff < epsilon\n    \n    # Assign a very high priority to exact fits.\n    # The score is `item * (1 / epsilon)` ensures it's prioritized.\n    # Using `item * 1e10` for a large multiplier.\n    scores_for_fitting[exact_fit_mask_for_fitting] = item * 1e10\n    \n    # Bins with a positive difference (item fits, but not exactly)\n    non_exact_fit_mask_for_fitting = ~exact_fit_mask_for_fitting\n    \n    # For these, use a score inversely proportional to the difference.\n    # `bins_remain_cap / diff` gives higher scores to smaller differences.\n    # This is `(item + diff) / diff`.\n    scores_for_fitting[non_exact_fit_mask_for_fitting] = bins_remain_cap[non_exact_fit_mask_for_fitting] / diff[non_exact_fit_mask_for_fitting]\n    \n    # Place these scores back into the main priorities array\n    priorities[can_fit_mask] = scores_for_fitting\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins that offer an exact fit or the smallest remaining capacity after placement.\n\n    Combines 'Exact Fit First' and 'Inverse Distance' strategies for a robust approach.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n        # Strategy 1: Exact Fit (Highest Priority)\n        exact_fit_mask = available_bins_remain_cap == item\n        if np.any(exact_fit_mask):\n            priorities[can_fit_mask][exact_fit_mask] = 1.0\n            # If there are exact fits, we only consider them for ranking (effectively)\n            # but to allow other bins to have non-zero priority if needed, we proceed.\n            # However, for simplicity and clear hierarchy, we could return here if only exact fits are desired as the sole choice.\n            # For a more nuanced approach, we allow other bins to compete if they are \"close\".\n\n        # Strategy 2: Proximity Fit (Inverse of remaining capacity after placement)\n        # Assigns higher priority to bins that leave less space after placing the item.\n        # Avoids division by zero by adding a small epsilon.\n        space_after_placement = available_bins_remain_cap - item\n        \n        # We want to prioritize bins with smaller space_after_placement.\n        # Using 1 / (space_after_placement + epsilon) favors smaller positive differences.\n        # For bins where space_after_placement is 0 (exact fit), this will be 1/epsilon (very high).\n        # To avoid extremely high values for exact fits that might dominate too much,\n        # and to ensure proximity fits are ranked meaningfully, we can cap or scale.\n        # A simple approach is to ensure exact fits get priority 1.0 and then rank others.\n\n        # Let's refine: assign 1.0 to exact fits, and then rank the rest based on inverse remaining capacity.\n        \n        non_exact_fit_mask = ~exact_fit_mask\n        if np.any(non_exact_fit_mask):\n            non_exact_available_caps = available_bins_remain_cap[non_exact_fit_mask]\n            non_exact_space_after_placement = non_exact_available_caps - item\n\n            # Calculate inverse of remaining capacity (higher score for smaller remaining capacity)\n            # Add epsilon to avoid division by zero. The smaller the remaining capacity, the higher the score.\n            proximity_scores = 1.0 / (non_exact_space_after_placement + 1e-9)\n\n            # Normalize these scores to be between 0 and (1 - epsilon) to not overlap with exact fits\n            # if we wanted a strict hierarchy.\n            # For a combined heuristic, we can normalize them relative to each other.\n            if len(proximity_scores) > 0:\n                min_prox_score = np.min(proximity_scores)\n                max_prox_score = np.max(proximity_scores)\n                \n                # Normalize scores to a range that doesn't conflict with exact fit priority (e.g., 0 to 0.99)\n                # or simply use their relative ranking.\n                # Using ranks is often more robust than raw values.\n                \n                # Rank the non-exact fits based on their proximity score (higher score = better rank)\n                # argsort returns indices that would sort the array.\n                # We want higher proximity_scores to have higher ranks.\n                sorted_indices_for_non_exact = np.argsort(proximity_scores)\n                \n                # Assign ranks: the bin with the highest proximity_score gets the highest rank (close to 1).\n                # The indices obtained from argsort are ascending for smaller values.\n                # So, if proximity_scores are [10, 5, 20], argsort gives [1, 0, 2].\n                # We want ranks [0.33, 0.66, 1.0] or similar.\n                # Let's assign ranks such that smaller space_after_placement gets higher priority.\n                \n                # Sort the actual non-exact remaining capacities to get a clear order for prioritization.\n                sorted_non_exact_space_after = non_exact_space_after_placement[sorted_indices_for_non_exact]\n                \n                # Now assign priorities based on these sorted capacities.\n                # The bin with the smallest space_after_placement gets the highest priority (excluding exact fits).\n                # We can assign priorities from 0.0 up to something less than 1.0 (e.g., 0.99).\n                # Let's normalize the ranks to a range like [0.1, 0.9].\n                \n                if len(sorted_non_exact_space_after) > 1:\n                    rank_values = np.linspace(0.1, 0.9, len(sorted_non_exact_space_after))\n                    # The smallest remaining space should get the highest rank (0.9).\n                    # The current sorted_non_exact_space_after is ascending, so the last element is the largest.\n                    # We want to assign highest priority to the smallest values.\n                    assigned_priorities_for_non_exact = rank_values[::-1] # Reverse to give highest to smallest\n                else:\n                    assigned_priorities_for_non_exact = np.array([0.5]) # Default priority for single non-exact fit\n\n                # Map these assigned priorities back to the original indices of the available_bins_array.\n                # `sorted_indices_for_non_exact` are indices within `non_exact_available_caps`\n                # We need to map these back to `can_fit_mask` indices.\n                \n                # Get the original indices within `can_fit_mask` that correspond to non_exact_fit_mask\n                original_indices_of_non_exact_fits = np.where(can_fit_mask)[0][non_exact_fit_mask]\n                \n                # Update priorities for these bins\n                priorities[original_indices_of_non_exact_fits] = assigned_priorities_for_non_exact\n\n    return priorities\n\n### Analyze & experience\n- *   **Heuristics 1 vs 2:** Heuristic 1 (`priority_v2`) directly assigns a very high score (`item * 1e10`) to exact fits and uses `bins_remain_cap / (diff + epsilon)` for other fits, providing a clear hierarchy and prioritizing tight fits. Heuristic 2 uses a normalized inverse of remaining capacity after placement but scales it by `0.1`, which might dilute the \"tight fit\" preference and its combination with the base priority of `1.0` is less structured than Heuristic 1's explicit high score for exact fits. Heuristic 1 is better because its scoring is more direct and impactful for exact fits.\n*   **Heuristics 2 vs 3:** Heuristic 2 prioritizes bins with just enough capacity and normalizes these scores, creating a relative ranking of \"tightness.\" Heuristic 3 uses `1.0 / (available_bins_remain_cap - item + 1e-9)` and then ranks these scores using `argsort(argsort(...))`. While both aim for tight fits, Heuristic 2's normalization and explicit \"can fit\" priority feels more robust than the double `argsort` which can be sensitive to the distribution of differences. Heuristic 2 is better due to its more interpretable normalization.\n*   **Heuristics 3 vs 4:** Heuristic 3 uses `1 / (diff + epsilon)` and then ranks the scores. Heuristic 4 explicitly separates exact fits (given `1e9`) and then scales `bins_remain_cap / (diff + epsilon)` for close fits. Heuristic 4 provides a clearer separation of priorities (exact > close > none) and uses a scoring that is more directly related to the remaining capacity. Heuristic 4 is better for its explicit prioritization tiers.\n*   **Heuristics 4 vs 5:** Heuristic 4 prioritizes exact fits with `1e9` and scales `bins_remain_cap / (diff + epsilon)` for close fits. Heuristic 5 is identical to Heuristic 3, using `1 / (diff + epsilon)` and `argsort(argsort(...))`. Heuristic 4 is superior due to its explicit handling of exact fits and more principled scoring for close fits.\n*   **Heuristics 5 vs 6:** Heuristic 5 is essentially the same as Heuristic 3. Heuristic 6 uses `1.0 / (available_bins_cap + 1e-9)` and normalizes it. This approach prioritizes bins that will be *more full* after placement, which is similar to tight fitting but expressed differently. Heuristic 5/3's `1 / (diff + epsilon)` is a more direct measure of tightness. Heuristic 5/3 is slightly better for its directness in measuring excess space.\n*   **Heuristics 6 vs 7:** Heuristic 6 normalizes `1.0 / (available_bins_cap + 1e-9)`. Heuristic 7 assigns `2.0` to exact fits and `1.1 - normalized_diff` to close fits. Heuristic 7's explicit separation of exact fits (priority `2.0`) and then a structured approach for close fits (higher for smaller `diff`) is more robust and hierarchical than Heuristic 6's single normalized score. Heuristic 7 is better.\n*   **Heuristics 7 vs 8:** Heuristic 7 prioritizes exact fits (`2.0`) and then uses `1.1 - normalized_diff` for close fits. Heuristic 8 uses `-remaining_after_packing` and then shifts/adds epsilon. This effectively prioritizes bins with the smallest non-negative `remaining_after_packing`. While similar in goal to Heuristic 7's close fit strategy, Heuristic 7's explicit handling of exact fits with a higher score is more defined. Heuristic 7 is better for its clear tiers.\n*   **Heuristics 8 vs 9:** Heuristics 8 and 9 are identical. They use negative remaining capacity after packing and then shift to make it positive, effectively prioritizing the tightest fits.\n*   **Heuristics 9 vs 10:** Heuristic 9 uses `-remaining_after_packing` and shifts. Heuristic 10 assigns `1.0` to exact fits and scales `1.0 / (space_after_placement + epsilon)` to `[0.5, 0.99]` for close fits. Heuristic 10 provides a clearer hierarchy (exact > close > none) and scales the secondary priority to avoid overlapping with exact fits. Heuristic 10 is better.\n*   **Heuristics 10 vs 11:** Heuristic 10 uses explicit tiers: exact fits (`1.0`), scaled inverse distance (`[0.5, 0.99]`). Heuristic 11 iterates through bins, assigning `1.0 / (bins_remain_cap[i] - item + 1e-9)`. Heuristic 10's structured approach with clear priority levels is superior to Heuristic 11's simple, unscaled inverse difference which doesn't explicitly handle exact fits separately.\n*   **Heuristics 11 vs 12:** Heuristic 11 uses `1.0 / (diff + epsilon)` per bin. Heuristic 12 assigns `1.0` to exact fits and `0.5 + 0.45 * (1.0 / (normalized_excess + 1e-9))` to close fits, capped at `0.99`. Heuristic 12's explicit prioritization of exact fits and structured scaling for close fits makes it better.\n*   **Heuristics 12 vs 13:** Heuristic 12 assigns `1.0` to exact fits and scaled `1 / (normalized_excess + 1e-9)` to close fits. Heuristic 13 assigns `1.0` to exact fits and then `normalized_priorities * 0.9` to close fits. Both aim for similar goals. Heuristic 12's `0.5 + 0.45 * ...` scaling provides a more controlled range for close fits. Heuristic 12 is slightly better for its more refined scaling.\n*   **Heuristics 13 vs 14:** Heuristics 13 and 14 are identical.\n*   **Heuristics 14 vs 15:** Heuristic 14 assigns `1.0` to exact fits and scales close fits to `[0.5, 0.99]`. Heuristic 15 assigns `2.0` to exact fits and scales close fits to `[0.1, 1.0]`. Heuristic 15 provides a higher explicit score for exact fits (`2.0` vs `1.0`) and a reasonable range for close fits, making its hierarchy clearer. Heuristic 15 is better.\n*   **Heuristics 15 vs 16:** Heuristics 15 and 16 are identical.\n*   **Heuristics 16 vs 17:** Heuristic 16 assigns `1.0` to exact fits and scales close fits to `[0.1, 1.0]`. Heuristic 17 uses `1 / (1 + exp(-k * (wasted_space_ratio - 0.5)))` for bins that fit. Heuristic 17's sigmoid approach is less direct and potentially more complex to tune than Heuristic 16's explicit scoring and normalization. Heuristic 16 is better for its clarity and robustness.\n*   **Heuristics 17 vs 18:** Heuristic 17 uses a sigmoid on `item / available_caps`. Heuristic 18 is identical to Heuristic 16. Heuristic 18 is better due to clearer priority separation.\n*   **Heuristics 18 vs 19:** Heuristics 18 and 19 are identical.\n*   **Heuristics 19 vs 20:** Heuristic 19 assigns `2.0` to exact fits and scales close fits to `[0.1, 1.0]`. Heuristic 20 assigns `1.0` to exact fits and ranks non-exact fits into `[0.1, 0.9]`. Heuristic 19's higher explicit score for exact fits (`2.0`) makes its hierarchical preference stronger. Heuristic 19 is better.\n*   **Overall:** The best heuristics clearly prioritize exact fits with a distinctively high score, then use a well-defined strategy (often normalized inverse excess capacity) for \"close\" fits, ensuring these scores are lower than exact fits. Heuristics that directly implement a hierarchy and use robust scoring for tiers are better.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Exact fit, scaled closeness, interpretability, robustness, vectorization.\n*   **Advice:** Design a multi-tiered scoring system. Assign the highest scores to exact fits. For near-fits, use a scaled inverse of *normalized* excess capacity, ensuring scores decrease monotonically with increasing excess capacity. Keep scoring functions simple and directly tied to fit quality.\n*   **Avoid:** Complex, non-monotonic, or highly sensitive non-linear scoring functions. Avoid manual iteration over array elements; leverage vectorization. Do not normalize without a clear rationale, as over-normalization can obscure differences.\n*   **Explanation:** This approach balances the strong preference for exact fits with a robust, interpretable method for ranking near-fits, while promoting efficient, maintainable code.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}