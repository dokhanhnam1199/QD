{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a refined 'Best Fit' like strategy.\n\n    This heuristic prioritizes bins that leave the least remaining capacity after packing the item,\n    aiming to minimize wasted space. It also favors bins that have sufficient capacity,\n    assigning a lower priority to bins that are too small.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity if the item is placed in each bin\n    remaining_after_packing = bins_remain_cap - item\n\n    # Assign a priority score.\n    # For bins where the item fits (remaining_after_packing >= 0):\n    # Higher priority is given to bins with smaller remaining capacity after packing.\n    # This is achieved by using the negative of the remaining capacity (so smaller positive numbers are prioritized).\n    # We add a small epsilon to ensure that bins with zero remaining capacity after packing\n    # (i.e., perfect fit) have a very high priority (close to 0 when negated and scaled).\n    # For bins where the item does not fit, assign a very low priority (negative infinity conceptually).\n    epsilon = 1e-9\n    priorities = np.where(\n        remaining_after_packing >= 0,\n        -remaining_after_packing,\n        -np.inf\n    )\n\n    # Normalize priorities to be non-negative and to give a higher score to bins that are a better fit.\n    # Shifting by adding the maximum negative value (or minimum positive value) ensures all priorities are >= 0.\n    # The 'best fit' bins (closest to 0 remaining capacity) will have the highest scores.\n    min_priority = np.min(priorities[priorities != -np.inf]) if np.any(priorities != -np.inf) else 0\n    priorities = np.where(\n        priorities == -np.inf,\n        0,  # Bins where item doesn't fit get 0 priority\n        priorities - min_priority + epsilon # Shift to make all priorities non-negative and prioritize smallest remaining capacity\n    )\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritizes exact fits with highest score, then bins with minimal excess\n    capacity, scaled to differentiate from exact fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    eligible_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # Highest priority for exact fits\n    exact_fit_mask_local = (eligible_bins_remain_cap == item)\n    priorities[can_fit_mask][exact_fit_mask_local] = 2.0\n\n    # For non-exact fits, prioritize based on minimal excess capacity\n    non_exact_fit_mask_local = ~exact_fit_mask_local\n    if np.any(non_exact_fit_mask_local):\n        excess_capacities = eligible_bins_remain_cap[non_exact_fit_mask_local] - item\n        \n        # Normalize excess capacities to [0, 1]\n        min_excess = 0.0\n        max_excess = np.max(excess_capacities)\n\n        if max_excess > min_excess:\n            normalized_excess = (excess_capacities - min_excess) / (max_excess - min_excess)\n        else:\n            normalized_excess = np.zeros_like(excess_capacities) # All excess capacities are the same\n\n        # Assign priority: smaller normalized excess means higher priority, scaled to be less than exact fit score\n        # We use (1 - normalized_excess) to give higher score to smaller excess.\n        # Adding a small epsilon to the denominator for robustness, though normalization should handle it.\n        non_exact_priorities = 1.0 - normalized_excess\n        \n        # Scale these priorities to be between 0.1 and 1.0, ensuring they are always less than exact fit priority (2.0)\n        # and higher than the default 0.0\n        min_target_p = 0.1\n        max_target_p = 1.0\n        \n        if np.max(non_exact_priorities) - np.min(non_exact_priorities) > 1e-9:\n             scaled_non_exact_priorities = min_target_p + (non_exact_priorities - np.min(non_exact_priorities)) * (max_target_p - min_target_p) / (np.max(non_exact_priorities) - np.min(non_exact_priorities))\n        else:\n             scaled_non_exact_priorities = np.full_like(non_exact_priorities, (min_target_p + max_target_p) / 2.0)\n\n        priorities[can_fit_mask][non_exact_fit_mask_local] = scaled_non_exact_priorities\n\n    return priorities\n\n### Analyze & experience\n- *   **Heuristics 1 vs 2:** Heuristic 1 (`priority_v2`) directly assigns a very high score (`item * 1e10`) to exact fits and uses `bins_remain_cap / (diff + epsilon)` for other fits, providing a clear hierarchy and prioritizing tight fits. Heuristic 2 uses a normalized inverse of remaining capacity after placement but scales it by `0.1`, which might dilute the \"tight fit\" preference and its combination with the base priority of `1.0` is less structured than Heuristic 1's explicit high score for exact fits. Heuristic 1 is better because its scoring is more direct and impactful for exact fits.\n*   **Heuristics 2 vs 3:** Heuristic 2 prioritizes bins with just enough capacity and normalizes these scores, creating a relative ranking of \"tightness.\" Heuristic 3 uses `1.0 / (available_bins_remain_cap - item + 1e-9)` and then ranks these scores using `argsort(argsort(...))`. While both aim for tight fits, Heuristic 2's normalization and explicit \"can fit\" priority feels more robust than the double `argsort` which can be sensitive to the distribution of differences. Heuristic 2 is better due to its more interpretable normalization.\n*   **Heuristics 3 vs 4:** Heuristic 3 uses `1 / (diff + epsilon)` and then ranks the scores. Heuristic 4 explicitly separates exact fits (given `1e9`) and then scales `bins_remain_cap / (diff + epsilon)` for close fits. Heuristic 4 provides a clearer separation of priorities (exact > close > none) and uses a scoring that is more directly related to the remaining capacity. Heuristic 4 is better for its explicit prioritization tiers.\n*   **Heuristics 4 vs 5:** Heuristic 4 prioritizes exact fits with `1e9` and scales `bins_remain_cap / (diff + epsilon)` for close fits. Heuristic 5 is identical to Heuristic 3, using `1 / (diff + epsilon)` and `argsort(argsort(...))`. Heuristic 4 is superior due to its explicit handling of exact fits and more principled scoring for close fits.\n*   **Heuristics 5 vs 6:** Heuristic 5 is essentially the same as Heuristic 3. Heuristic 6 uses `1.0 / (available_bins_cap + 1e-9)` and normalizes it. This approach prioritizes bins that will be *more full* after placement, which is similar to tight fitting but expressed differently. Heuristic 5/3's `1 / (diff + epsilon)` is a more direct measure of tightness. Heuristic 5/3 is slightly better for its directness in measuring excess space.\n*   **Heuristics 6 vs 7:** Heuristic 6 normalizes `1.0 / (available_bins_cap + 1e-9)`. Heuristic 7 assigns `2.0` to exact fits and `1.1 - normalized_diff` to close fits. Heuristic 7's explicit separation of exact fits (priority `2.0`) and then a structured approach for close fits (higher for smaller `diff`) is more robust and hierarchical than Heuristic 6's single normalized score. Heuristic 7 is better.\n*   **Heuristics 7 vs 8:** Heuristic 7 prioritizes exact fits (`2.0`) and then uses `1.1 - normalized_diff` for close fits. Heuristic 8 uses `-remaining_after_packing` and then shifts/adds epsilon. This effectively prioritizes bins with the smallest non-negative `remaining_after_packing`. While similar in goal to Heuristic 7's close fit strategy, Heuristic 7's explicit handling of exact fits with a higher score is more defined. Heuristic 7 is better for its clear tiers.\n*   **Heuristics 8 vs 9:** Heuristics 8 and 9 are identical. They use negative remaining capacity after packing and then shift to make it positive, effectively prioritizing the tightest fits.\n*   **Heuristics 9 vs 10:** Heuristic 9 uses `-remaining_after_packing` and shifts. Heuristic 10 assigns `1.0` to exact fits and scales `1.0 / (space_after_placement + epsilon)` to `[0.5, 0.99]` for close fits. Heuristic 10 provides a clearer hierarchy (exact > close > none) and scales the secondary priority to avoid overlapping with exact fits. Heuristic 10 is better.\n*   **Heuristics 10 vs 11:** Heuristic 10 uses explicit tiers: exact fits (`1.0`), scaled inverse distance (`[0.5, 0.99]`). Heuristic 11 iterates through bins, assigning `1.0 / (bins_remain_cap[i] - item + 1e-9)`. Heuristic 10's structured approach with clear priority levels is superior to Heuristic 11's simple, unscaled inverse difference which doesn't explicitly handle exact fits separately.\n*   **Heuristics 11 vs 12:** Heuristic 11 uses `1.0 / (diff + epsilon)` per bin. Heuristic 12 assigns `1.0` to exact fits and `0.5 + 0.45 * (1.0 / (normalized_excess + 1e-9))` to close fits, capped at `0.99`. Heuristic 12's explicit prioritization of exact fits and structured scaling for close fits makes it better.\n*   **Heuristics 12 vs 13:** Heuristic 12 assigns `1.0` to exact fits and scaled `1 / (normalized_excess + 1e-9)` to close fits. Heuristic 13 assigns `1.0` to exact fits and then `normalized_priorities * 0.9` to close fits. Both aim for similar goals. Heuristic 12's `0.5 + 0.45 * ...` scaling provides a more controlled range for close fits. Heuristic 12 is slightly better for its more refined scaling.\n*   **Heuristics 13 vs 14:** Heuristics 13 and 14 are identical.\n*   **Heuristics 14 vs 15:** Heuristic 14 assigns `1.0` to exact fits and scales close fits to `[0.5, 0.99]`. Heuristic 15 assigns `2.0` to exact fits and scales close fits to `[0.1, 1.0]`. Heuristic 15 provides a higher explicit score for exact fits (`2.0` vs `1.0`) and a reasonable range for close fits, making its hierarchy clearer. Heuristic 15 is better.\n*   **Heuristics 15 vs 16:** Heuristics 15 and 16 are identical.\n*   **Heuristics 16 vs 17:** Heuristic 16 assigns `1.0` to exact fits and scales close fits to `[0.1, 1.0]`. Heuristic 17 uses `1 / (1 + exp(-k * (wasted_space_ratio - 0.5)))` for bins that fit. Heuristic 17's sigmoid approach is less direct and potentially more complex to tune than Heuristic 16's explicit scoring and normalization. Heuristic 16 is better for its clarity and robustness.\n*   **Heuristics 17 vs 18:** Heuristic 17 uses a sigmoid on `item / available_caps`. Heuristic 18 is identical to Heuristic 16. Heuristic 18 is better due to clearer priority separation.\n*   **Heuristics 18 vs 19:** Heuristics 18 and 19 are identical.\n*   **Heuristics 19 vs 20:** Heuristic 19 assigns `2.0` to exact fits and scales close fits to `[0.1, 1.0]`. Heuristic 20 assigns `1.0` to exact fits and ranks non-exact fits into `[0.1, 0.9]`. Heuristic 19's higher explicit score for exact fits (`2.0`) makes its hierarchical preference stronger. Heuristic 19 is better.\n*   **Overall:** The best heuristics clearly prioritize exact fits with a distinctively high score, then use a well-defined strategy (often normalized inverse excess capacity) for \"close\" fits, ensuring these scores are lower than exact fits. Heuristics that directly implement a hierarchy and use robust scoring for tiers are better.\n- \nHere's a redefined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Exact fit, scaled closeness, interpretability, robustness, vectorization.\n*   **Advice:** Design a multi-tiered scoring system. Assign the highest scores to exact fits. For near-fits, use a scaled inverse of *normalized* excess capacity, ensuring scores decrease monotonically with increasing excess capacity. Keep scoring functions simple and directly tied to fit quality.\n*   **Avoid:** Complex, non-monotonic, or highly sensitive non-linear scoring functions. Avoid manual iteration over array elements; leverage vectorization. Do not normalize without a clear rationale, as over-normalization can obscure differences.\n*   **Explanation:** This approach balances the strong preference for exact fits with a robust, interpretable method for ranking near-fits, while promoting efficient, maintainable code.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}