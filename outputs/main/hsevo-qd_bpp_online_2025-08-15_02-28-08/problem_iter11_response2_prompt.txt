{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines Exact Fit First with a scaled inverse of remaining space for non-exact fits.\n\n    Prioritizes exact fits, then favors bins with the least space after placement,\n    scaled to ensure they are lower than exact fit scores.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    epsilon = 1e-9\n\n    # Exact fit has the highest priority\n    exact_fit_mask = np.abs(bins_remain_cap - item) < epsilon\n    priorities[exact_fit_mask] = 1.0\n\n    # For bins that can fit but are not exact fits\n    can_fit_mask = bins_remain_cap >= item\n    non_exact_fit_mask = can_fit_mask & ~exact_fit_mask\n\n    if np.any(non_exact_fit_mask):\n        space_after_placement = bins_remain_cap[non_exact_fit_mask] - item\n        \n        # Use inverse of space_after_placement to favor smaller remaining space\n        # Scale these scores to be less than 1.0 to ensure exact fits are always preferred.\n        # A simple inverse without scaling might result in scores higher than 1.0 if the difference is small.\n        # We map the inverse scores to a range [0.5, 0.99]\n        min_secondary_priority = 0.5\n        max_secondary_priority = 0.99\n\n        # Calculate inverse scores for non-exact fits\n        inverse_scores = 1.0 / (space_after_placement + epsilon)\n\n        # Normalize these inverse scores to [0, 1]\n        if inverse_scores.size > 1:\n            normalized_inverse_scores = (inverse_scores - np.min(inverse_scores)) / (np.max(inverse_scores) - np.min(inverse_scores) + epsilon)\n        else:\n            normalized_inverse_scores = np.ones_like(inverse_scores) # If only one non-exact bin, it gets max secondary score\n\n        # Scale normalized scores to the desired secondary priority range\n        scaled_secondary_scores = min_secondary_priority + normalized_inverse_scores * (max_secondary_priority - min_secondary_priority)\n        \n        priorities[non_exact_fit_mask] = scaled_secondary_scores\n        \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins by favoring exact fits, then closest fits, using a smooth scoring.\n\n    Combines Exact Fit First and Inverse Distance strategies:\n    1. High priority for bins where remaining_capacity == item.\n    2. Medium priority for bins where remaining_capacity > item, inversely\n       proportional to the difference (remaining_capacity - item).\n    3. Zero priority for bins where remaining_capacity < item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    can_fit_mask = bins_remain_cap >= item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n\n    if len(fitting_bins_cap) > 0:\n        # Prioritize exact fits with a high score\n        exact_fit_mask = fitting_bins_cap == item\n        priorities[fitting_bins_indices[exact_fit_mask]] = 2.0  # Highest priority\n\n        # For bins that are not exact fits, calculate priority based on how close they are\n        non_exact_fitting_bins_cap = fitting_bins_cap[~exact_fit_mask]\n        non_exact_fitting_bins_indices = fitting_bins_indices[~exact_fit_mask]\n\n        if len(non_exact_fitting_bins_cap) > 0:\n            # Calculate the difference (space after placing the item)\n            space_after_placement = non_exact_fitting_bins_cap - item\n\n            # Normalize these differences to a [0, 1] range for scoring.\n            # Smaller difference means higher priority.\n            min_diff = np.min(space_after_placement)\n            max_diff = np.max(space_after_placement)\n\n            if max_diff - min_diff > 1e-9: # Avoid division by zero if all differences are the same\n                normalized_diff = (space_after_placement - min_diff) / (max_diff - min_diff)\n            else:\n                normalized_diff = np.zeros_like(space_after_placement)\n            \n            # Assign priorities: inverse relationship with normalized difference, scaled down.\n            # This gives medium priority, lower than exact fits.\n            # Add a small offset to ensure it's greater than 0 for fitting bins.\n            priorities[non_exact_fitting_bins_indices] = 1.0 - normalized_diff + 0.1\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Best) and Heuristic 5 (also the same as 1): Both use `bins_remain_cap / (diff + epsilon)` to prioritize tight fits, favoring bins where `bins_remain_cap` is close to `item`.\n\nComparing Heuristic 2 and Heuristic 4: Both are identical to Heuristic 1, essentially repeating the same logic. The extensive commented-out exploration in Heuristic 2 doesn't translate to a distinct improvement or change in the final implemented logic compared to Heuristic 1.\n\nComparing Heuristic 3 and Heuristic 12/13/14: Heuristic 3 uses `bins_remain_cap / (diff + epsilon)`, similar to Heuristic 1. Heuristics 12, 13, and 14 introduce a tiered approach: highest priority for exact fits (score 2.0), and medium priority for non-exact fits based on normalized differences (scaled to be less than 2.0, e.g., `1.0 - normalized_diff + 0.1`). This tiered approach is more sophisticated than a single scoring function.\n\nComparing Heuristic 6 and Heuristic 7: Heuristic 6 uses a two-tier system: 1.0 for exact fits, and then normalized scores (0.0-0.9) for non-exact fits based on inverse of excess capacity relative to the minimum excess. Heuristic 7 is similar but assigns 2.0 to exact fits and scales non-exact fits to [0.1, 1.0]. The normalization in Heuristic 7 might be more robust if the range of excess capacities is large.\n\nComparing Heuristic 10/11 and Heuristic 16/17/19: Heuristics 10, 11 use a fixed priority of 1.0 for exact fits and scale non-exact fits to a range [0.5, 0.99] based on inverse normalized excess. Heuristics 16, 17, 19 also give 1.0 to exact fits but scale non-exact fits to a range like [0.1, 0.9] using inverse excess capacity relative to the maximum inverse excess capacity. The scaling in 16/17/19 seems more nuanced for ranking close fits.\n\nComparing Heuristic 8 and Heuristic 9: Heuristic 8 sorts available bins by remaining capacity and then assigns ranks based on sorted priorities (which appears to be a form of inverse remaining capacity). Heuristic 9 directly uses the inverse of available capacity (not excess capacity) and normalizes it. Heuristic 8's approach of ranking based on sorted values might be more stable.\n\nComparing Heuristic 15/18 and Heuristic 20: Heuristic 15/18 prioritizes exact fits with 1.0 and scales non-exact fits to [0, 0.9] using normalized inverse of space after placement. Heuristic 20 also prioritizes exact fits with 1.0 but scales non-exact fits to [0.5, 0.99] using inverse of normalized excess capacity. The latter scaling and explicit re-assertion of exact fit priority seem slightly more robust.\n\nOverall: More sophisticated heuristics implement a multi-tiered strategy (exact fit, then best fit based on normalized excess/inverse excess) with carefully chosen scaling factors to differentiate priorities. Simple inverse or ratio-based scoring is less effective than tiered approaches.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Exact Fit, Minimal Excess Capacity, Scaled Difference, Vectorization.\n*   **Advice:** Explicitly separate exact fits with maximum priority. For close fits, score based on a scaled measure of excess capacity, ensuring these scores are always lower than exact fit scores. Leverage vectorized operations for efficiency.\n*   **Avoid:** Complex non-linear transformations on capacity differences, convoluted array manipulations, and overly aggressive normalization that can obscure relative performance.\n*   **Explanation:** This approach clearly prioritizes perfect solutions while providing a predictable and tunable mechanism for selecting the \"best\" imperfect fit, all within an efficient computational framework.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}