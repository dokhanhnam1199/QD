```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    epsilon = 1e-9
    
    # Initialize priorities to zero
    priorities = np.zeros_like(bins_remain_cap)
    
    # Identify bins where the item can fit exactly
    exact_fit_mask = np.abs(bins_remain_cap - item) < epsilon
    
    # Identify bins where the item can fit but not exactly
    can_fit_mask = (bins_remain_cap >= item) & ~exact_fit_mask
    
    # Assign the highest priority to exact fits
    priorities[exact_fit_mask] = 1.0 / epsilon  # A very high score for exact fits
    
    # For bins that can fit but not exactly, calculate a priority score.
    # We want to prioritize bins with minimal excess capacity (smallest `diff`).
    # The score should be lower than exact fits but higher for tighter fits.
    # A score inversely proportional to the excess capacity `diff` is suitable.
    # To make it distinct from exact fits and tunable, we can use `1 / (diff + small_constant)`.
    # Let's use `1 / (diff + item * 0.1)` to scale the "closeness" relative to item size.
    # This prioritizes bins where `bins_remain_cap` is just slightly larger than `item`.
    
    diff = bins_remain_cap[can_fit_mask] - item
    
    # Scale the excess capacity. A small percentage of item size ensures that
    # small absolute differences are prioritized over large absolute differences.
    # Example: item=10. diff=1 (score 1/1.1). diff=2 (score 1/2.1). item=100. diff=1 (score 1/10.1). diff=2 (score 1/12.1).
    # This ensures small absolute differences get relatively higher priority when item is smaller.
    # Let's try `diff / item` as a measure of "how much extra space".
    # We want to prioritize smaller values of `diff / item`.
    # So score can be `1 / (diff / item + epsilon)` which is `item / (diff + epsilon * item)`.
    # This scales the "tightness" by the item size itself.
    
    # Let's simplify: prioritize based on how much of the remaining capacity is used.
    # If remaining capacity is `R` and item is `I`, we want `R` to be close to `I`.
    # The ratio `I / R` is close to 1 for tight fits.
    # Score = `1 / abs(I / R - 1 + epsilon)`
    # Score = `R / abs(R - I + epsilon)`
    # For bins that fit, `R >= I`, so `R - I >= 0`.
    # Score = `R / (R - I + epsilon)`
    # This is `bins_remain_cap / (diff + epsilon)`.
    
    # Let's refine the scaling for non-exact fits. We want to give a high score for minimal excess,
    # and a decreasing score for larger excess.
    # A function that penalizes larger `diff` more.
    # Consider `1 / (diff + small_constant)` where `small_constant` is relative to `item`.
    # Using `diff + item * 0.05` as the denominator for non-exact fits.
    # This way, if item is small, even a small absolute `diff` is treated as significant.
    # If item is large, a small absolute `diff` is still prioritized.
    
    # Let's make the priority for non-exact fits a fraction of the exact fit priority.
    # If `diff` is small, the score is high. If `diff` is large, the score is low.
    # Let's use `1 / (diff + epsilon)` but scale it down so it doesn't compete with exact fits.
    
    # Alternative: Prioritize by the *ratio* of remaining capacity to item size, aiming for a ratio close to 1.
    # `ratio = bins_remain_cap[can_fit_mask] / item`
    # Score for these bins: `1 / (abs(ratio - 1) + epsilon)`
    # This is `item / abs(bins_remain_cap[can_fit_mask] - item + epsilon)`.
    # This is essentially `item / (diff + epsilon)`.
    
    # To ensure non-exact fits are strictly lower than exact fits:
    # The exact fits have score `1/epsilon`.
    # We need scores for non-exact fits to be significantly less.
    
    # Let's use `1 / (diff + item * alpha)` for non-exact fits, where `alpha` is a small factor.
    # `alpha = 0.01` for example.
    
    # Final strategy:
    # 1. Exact fits get highest priority (`1 / epsilon`).
    # 2. Non-exact fits get priority based on minimal excess capacity (`diff`).
    #    Score = `1 / (diff + small_factor * item + epsilon)`.
    #    The `small_factor * item` ensures that the denominator is never too small for large items,
    #    and it provides a baseline penalty for excess capacity that scales with item size.
    
    # Let's set a factor for the small excess. `0.05` seems reasonable.
    scaling_factor = 0.05
    priorities[can_fit_mask] = 1.0 / (diff + scaling_factor * item + epsilon)
    
    return priorities
```
