```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a more sophisticated strategy.

    This heuristic prioritizes bins that have remaining capacity exactly equal to the item size (perfect fit).
    As a secondary criterion, it prioritizes bins with the smallest positive difference between remaining capacity
    and item size (tightest fit). It aims to reduce fragmentation by favoring bins that are almost full.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Returns:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    epsilon = 1e-9
    
    # Initialize priorities to zero for all bins.
    priorities = np.zeros_like(bins_remain_cap)
    
    # Identify bins where the item can fit.
    can_fit_mask = bins_remain_cap >= item
    
    # Calculate the difference between remaining capacity and item size for bins that can fit the item.
    diff = bins_remain_cap[can_fit_mask] - item
    
    # Calculate the scores for bins where the item fits.
    # We want to give the highest priority to bins where the item fits perfectly (diff == 0).
    # For bins where diff > 0, we want to prioritize smaller diff values.
    
    # A scoring mechanism that strongly favors exact fits and then tight fits.
    # We can use a piecewise function or a function that has a sharp peak at diff = 0.
    
    # Let's try a score that is a combination of a large bonus for perfect fits
    # and a scaled inverse difference for near-perfect fits.
    
    # Consider a score that is `large_value` if `diff` is very close to zero,
    # and `1 / diff` (or a scaled version) otherwise.
    
    # We can use a Gaussian-like function centered at 0 for the difference,
    # or a ratio that amplifies small positive differences.
    
    # Alternative: A score based on the ratio of remaining capacity to item size.
    # `bins_remain_cap / item` ratio. We want this ratio to be close to 1.
    # If `bins_remain_cap == item`, ratio is 1.
    # If `bins_remain_cap` is slightly larger than `item`, ratio is slightly > 1.
    # If `bins_remain_cap` is much larger than `item`, ratio is much > 1.
    
    # Score = `1 / abs(ratio - 1)` is problematic if `ratio` is very close to 1.
    # Consider `bins_remain_cap / (bins_remain_cap - item + epsilon)`.
    # This score amplifies smaller differences:
    # - If diff = 0 (perfect fit): bins_remain_cap / epsilon -> very large score.
    # - If diff is small (e.g., 0.1): bins_remain_cap / 0.1 -> large score.
    # - If diff is large (e.g., 10): bins_remain_cap / 10 -> smaller score.
    
    # Let's try to give an extra boost to perfect fits.
    # Score = (1000 + 1 / (diff + epsilon)) for diff > 0, and a much larger value for diff == 0.
    
    # A simpler, robust approach: Prioritize bins where `bins_remain_cap` is as close as possible to `item`.
    # We can use the inverse of the difference, but add a term that significantly boosts perfect fits.
    
    # Let's try `(bins_remain_cap / item) / (diff + epsilon)`.
    # If diff=0, score is `bins_remain_cap / epsilon` (very large).
    # If diff > 0, score is `(item + diff) / diff` = `1 + item/diff`.
    # This still favors bins where `diff` is small.
    
    # Consider a score that is a large constant for perfect fits, and then a diminishing score for near fits.
    # For instance, score = 1000 if `diff < epsilon`, else `1 / (diff + epsilon)`.
    
    # Let's refine the previous approach: `bins_remain_cap / (bins_remain_cap - item + epsilon)`.
    # This can be written as `(item + diff) / (diff + epsilon)`.
    # This is equivalent to `item / (diff + epsilon) + diff / (diff + epsilon)`.
    # For small `diff`, `diff / (diff + epsilon)` is close to 1.
    # So the score is roughly `item / diff + 1`.
    # This means larger `item` for the same `diff` leads to higher priority.
    # And smaller `diff` for the same `item` leads to higher priority.
    
    # Let's normalize the difference by the item size to make it scale-invariant.
    # `normalized_diff = diff / item` (assuming item > 0)
    # We want `normalized_diff` to be as close to 0 as possible.
    
    # Score = `1 / (normalized_diff + epsilon)`
    # This is `item / (diff + epsilon)`. This is already a good heuristic.
    
    # How to boost perfect fits?
    # We can add a large constant if `diff < epsilon`.
    
    # Let's create a score that uses a power of the inverse difference to emphasize tightness.
    # Score = `(1 / (diff + epsilon))^p` where `p > 1`.
    # For example, `p=2`. `(1 / (diff + epsilon))^2`.
    # This will amplify the priority of very tight fits significantly.
    
    # Let's combine a strong preference for perfect fits with a smooth decay for near fits.
    # We can use `np.exp(-k * diff)` which peaks at `diff=0`.
    # Let's scale it so that `exp(-k * diff)` for small `diff` is still significant.
    # A common approach is `1 / (diff + epsilon)` for its simplicity and effectiveness.
    
    # Let's reconsider the interpretation:
    # "Prioritize bins that have just enough remaining capacity to fit the item."
    # This means `bins_remain_cap` should be close to `item`.
    # The metric `bins_remain_cap / (bins_remain_cap - item + epsilon)` from `priority_v1` already does this well.
    
    # How to improve? Consider the *ratio* of remaining capacity to item size.
    # `ratio = bins_remain_cap / item`. We want `ratio` close to 1.
    # A score that is high when `ratio` is close to 1 and low otherwise.
    # Example: `1 / abs(ratio - 1 + epsilon)` -> `bins_remain_cap / abs(bins_remain_cap - item + epsilon)`.
    # This is precisely what `priority_v1` is doing (up to scaling).
    
    # Let's add a mechanism to distinguish between perfect fits and just-fitting bins.
    # If `diff < epsilon` (perfect fit), give it a very high score.
    # If `diff >= epsilon`, use a score that prioritizes smaller `diff`.
    
    # Let's try:
    # If `diff < epsilon`, score = `large_number` (e.g., 1e9)
    # Else, score = `1 / (diff + epsilon)`
    
    # This is a common approach for "Best Fit" type heuristics.
    # `priority_v1` as implemented actually does `1 / (diff + epsilon)`.
    
    # Let's try a different angle: consider the "waste" if the item is placed.
    # Waste = `bins_remain_cap - item`. We want to minimize waste.
    # So, priority should be inversely proportional to waste.
    # `1 / (bins_remain_cap - item + epsilon)`. This is `priority_v1`.
    
    # What if we want to favor bins that are *almost full* but still fit?
    # This means `bins_remain_cap` is large, but `bins_remain_cap - item` is small.
    
    # Let's try a score that is high for small differences and also considers the absolute remaining capacity.
    # Score = `f(bins_remain_cap) * g(diff)` where `g` is decreasing in `diff` and `f` is increasing in `bins_remain_cap`.
    # Example: `bins_remain_cap * (1 / (diff + epsilon))`
    # This is `bins_remain_cap^2 / (bins_remain_cap - item + epsilon)`.
    # This amplifies tightness AND prefers bins with more remaining capacity (which might be counter-intuitive if we want to save large bins).
    
    # Let's stick to prioritizing tightness.
    # How about amplifying the difference for tight fits?
    # Score = `1 / (diff^2 + epsilon)` or `1 / (diff^3 + epsilon)`
    # This makes very tight fits much more favorable.
    
    # Let's try a combination: high score for perfect fits, and a decaying score for near fits.
    # We can use a large constant if the item fits perfectly, and then a scaled inverse difference.
    
    # For the values that can fit:
    # `valid_bins_remain_cap = bins_remain_cap[can_fit_mask]`
    # `valid_diff = valid_bins_remain_cap - item`
    
    # Calculate priorities for the valid bins
    # Option 1: Pure "Best Fit" - prioritize smallest non-negative difference.
    # This is `1 / (valid_diff + epsilon)`. This is `priority_v1`.
    
    # Option 2: "Best Fit" with amplified tightness.
    # `(1 / (valid_diff + epsilon)) ** 2` or `exp(-k * valid_diff)` with k chosen to give a good decay.
    
    # Let's try a score that explicitly favors `diff == 0` heavily, and then `1/diff`.
    # Score = `1e9` if `valid_diff < epsilon`
    # Score = `1 / (valid_diff + epsilon)` if `valid_diff >= epsilon`
    
    # This can be implemented efficiently.
    
    # We can use `np.where` to achieve this conditional scoring.
    
    # Calculate the score for bins where the item fits.
    # We want to prioritize bins where `bins_remain_cap` is as close as possible to `item`.
    # Specifically, prioritize bins where `bins_remain_cap == item` (perfect fit).
    # Then, prioritize bins where `bins_remain_cap` is slightly greater than `item`.
    
    # A robust score could be:
    # If `diff == 0`, priority is a very high constant.
    # If `diff > 0`, priority is inversely proportional to `diff`.
    
    # Let's define a high score for perfect fits.
    perfect_fit_bonus = 1e9  # A large bonus for exact matches.
    
    # Calculate the scores for bins where the item fits.
    # We assign a high bonus to perfect fits and a standard "best fit" score to near fits.
    
    # We can compute `1 / (diff + epsilon)` for all fitting bins first.
    # Then, override the score for bins where `diff` is effectively zero.
    
    scores_for_fitting_bins = 1.0 / (diff + epsilon)
    
    # Identify perfect fits (where difference is very small)
    perfect_fit_mask = diff < epsilon
    
    # Apply the bonus for perfect fits
    scores_for_fitting_bins[perfect_fit_mask] = perfect_fit_bonus
    
    # Assign these calculated scores back to the original priorities array.
    priorities[can_fit_mask] = scores_for_fitting_bins
    
    return priorities
```
