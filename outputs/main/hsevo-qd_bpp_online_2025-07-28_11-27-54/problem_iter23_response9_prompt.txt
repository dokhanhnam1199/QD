{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority scores for bins using a distribution-aware heuristic with smooth exponential scoring.\n    \n    Scores are computed by blending residual minimization and fragmentation penalties via adaptive weights,\n    dynamically adjusted to the current bin capacity distribution. Uses feasibility masks to exclude invalid bins.\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    if not np.any(can_fit):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    mu = np.mean(bins_remain_cap)\n    sigma = np.std(bins_remain_cap)\n    epsilon = 1e-9\n    \n    # Dynamic threshold for fragmentation awareness\n    threshold = mu - sigma\n    \n    # Residual computation for feasible bins\n    r = bins_remain_cap - item\n    \n    # Smooth exponential residual score (smaller r \u2192 higher score)\n    residual_score = np.exp(-r / (mu + sigma + epsilon))\n    \n    # Smooth threshold-crossing score using logistic transition\n    threshold_diff = r - threshold\n    fragment_score = 1.0 / (1.0 + np.exp(threshold_diff / (sigma + epsilon)))\n    \n    # Adaptive weights based on coefficient of variation\n    cv = sigma / (mu + epsilon)\n    weight_residual = 1.0 - np.tanh(cv)  # Dominates when distribution is tight\n    weight_fragment = np.tanh(cv)        # Dominates when distribution is spread out\n    \n    # Combine components with adaptive weights\n    combined_score = weight_residual * residual_score + weight_fragment * fragment_score\n    \n    # Apply feasibility mask\n    return np.where(can_fit, combined_score, -np.inf)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores for bins using a thresholded categorical heuristic.\n    \n    Bins that can fit the item are scored in discrete tiers based on normalized\n    remaining space after placement. Thresholds are anchored to item size for\n    contextual adaptability without instability from continuous scoring.\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Contextual thresholds based on item size\n    tight_threshold = 0.15 * item\n    moderate_threshold = 0.4 * item\n\n    # Identify viable bins and compute post-placement space\n    can_fit = bins_remain_cap >= item\n    remaining_after = bins_remain_cap - item\n\n    # Initialize scores with -inf for invalid bins\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    # Tier 1: Tight fit (minimal leftover space)\n    tight_mask = can_fit & (remaining_after <= tight_threshold)\n    # Tier 2: Moderate fit (small leftover space)\n    mod_mask = can_fit & (remaining_after > tight_threshold) & (remaining_after <= moderate_threshold)\n    # Tier 3: Loose fit (significant leftover space)\n    loose_mask = can_fit & (remaining_after > moderate_threshold)\n\n    # Assign discrete priority scores\n    scores[tight_mask] = 3\n    scores[mod_mask] = 2\n    scores[loose_mask] = 1\n\n    return scores\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see that top heuristics use dynamic statistical adaptation (mean, std, median) and multi-objective blending (residual minimization + fragmentation penalties), while the worst ones apply rigid soft penalties without contextual awareness. (2nd) vs (19th) show that adaptive thresholds (mean+std) and feasibility masks outperform static thresholds and uniform scoring. (3rd) vs (18th) reveal that explicit feasibility enforcement (-\u221e masking) and exponential decay penalties dominate over naive zero-scoring. (6th) vs (17th) highlight that item-relative rewards (e.g., r/item scaling) and multi-component blending (residual + penalty + reward) outperform single-objective approaches. (11th) vs (16th) demonstrate that categorical tiering (tight/moderate/loose fits) with discrete scores underperforms smooth exponential scoring. Overall: Superior heuristics dynamically adapt weights to bin/item statistics, combine residual minimization with fragmentation avoidance via smooth functions, and rigorously enforce feasibility. Inferior ones rely on fixed thresholds, lack multi-objective balance, or fail to penalize infeasibility effectively.\n- \nKeywords: Dynamic statistical adaptation, multi-objective blending, smooth scoring, feasibility masking  \nAdvice: Integrate context-aware weights combining residual minimization and fragmentation penalties, using logistic/exponential scoring calibrated to real-time bin/item distributions. Prioritize adaptivity via median/\u03c3-driven thresholds and -\u221e infeasibility masking.  \nAvoid: Fixed/static thresholds, single-objective prioritization, stepwise scoring, soft constraint handling.  \nExplanation: Dynamic thresholds (e.g., \u03bc\u00b1\u03c3) and continuous scoring ensure mathematical robustness, while contextual multi-objective blending prevents brittleness in varying distributions. Rigorous masking maintains feasibility without heuristic fallbacks.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}