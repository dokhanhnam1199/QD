{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority scores using adaptive item size classification and smooth scoring.\"\"\"\n    can_fit = bins_remain_cap >= item\n    if len(bins_remain_cap) == 0:\n        return np.array([])\n    \n    # Dynamic item size classification based on harmonic mean of remaining capacities\n    eps = 1e-9\n    harmonic_mean = len(bins_remain_cap) / (np.sum(1.0 / (bins_remain_cap + eps)))\n    is_large = item > (harmonic_mean * 0.8)  # Adaptive threshold with margin\n    \n    # Smooth scoring with secondary tie-breaker\n    with np.errstate(divide='ignore', invalid='ignore'):\n        fit_ratio = item / (bins_remain_cap + eps)  # Primary term: fill ratio\n        underfill_penalty = bins_remain_cap - item  # Secondary term: leftover space\n        underfill_penalty = np.clip(underfill_penalty, 0, None)\n        \n        # Smooth exponential weighting of terms\n        fit_component = np.exp(fit_ratio * 2) * can_fit\n        penalty_component = np.exp(-underfill_penalty * 0.5) * can_fit\n        \n        # Adaptive strategy blending\n        if is_large:\n            score = fit_component * penalty_component\n        else:\n            # Prefer underutilized bins with smooth capacity scaling\n            capacity_scaling = bins_remain_cap / (bins_remain_cap.mean() + eps)\n            score = (1.0 + capacity_scaling) * penalty_component\n    \n    return np.where(can_fit, score, -np.inf)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (1st) vs (20th): The best uses adaptive thresholds (harmonic mean) and smooth exponential scoring combining fit ratio/penalty, while the worst returns zero. Key difference: adaptability and multi-factorial smooth evaluation.  \n(2nd) vs (19th): Best Fit scoring (prioritizes minimal leftover) outperforms null scoring, showing explicit placement logic beats no logic.  \n(4th) vs (7th): Adaptive exponential scoring (4th) with context-driven thresholds outperforms fixed-step Best/Worst Fit (7th), highlighting smoothness and dynamic classification advantages.  \n(10th) vs (15th): Thresholded tiers (10th) using item-relative thresholds (0.15x/0.4x) still outperform random/null baselines (15th), but lack adaptability of harmonic mean-based methods.  \n(1st) vs (2nd): The top heuristic adds capacity scaling for underutilized bins and harmonic mean adaptation, while 2nd uses simpler Best Fit. This shows dynamic context modeling improves over static strategies.  \n(6th) vs (9th): Duplicate code in 6th adds harmonic mean adaptation vs 9th's fixed threshold, proving dynamic thresholds > static rules.  \n(7th) vs (10th): Step functions (7th) underperform tiered scoring (10th), suggesting discrete tiers > abrupt decisions but still inferior to smooth exponential weighting.\n- \n- **Keywords**: Residual capacity thresholds, blended scoring, feasibility masks, mathematical continuity  \n- **Advice**: Dynamically adjust thresholds using residual capacities, blend smoothed exponential scoring with secondary penalties (e.g., underfill/fragmentation), enforce feasibility via -\u221e masks for invalid bins, prioritize continuous functions over discrete steps.  \n- **Avoid**: Fixed/static thresholds, generic adaptive methods (e.g., vague \"item size classification\"), step functions, fragmented tie-breakers without contextual grounding.  \n- **Explanation**: Contextual thresholds (residual capacity) and mathematically smooth blending enhance adaptability, while feasibility masks ensure valid decisions. Continuity reduces instability vs. step functions, and targeted penalties minimize suboptimal tradeoffs.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}