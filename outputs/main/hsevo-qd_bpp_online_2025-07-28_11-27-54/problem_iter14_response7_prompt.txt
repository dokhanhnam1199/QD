{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority scores using adaptive item size classification and smooth scoring.\"\"\"\n    can_fit = bins_remain_cap >= item\n    if len(bins_remain_cap) == 0:\n        return np.array([])\n    \n    # Dynamic item size classification based on harmonic mean of remaining capacities\n    eps = 1e-9\n    harmonic_mean = len(bins_remain_cap) / (np.sum(1.0 / (bins_remain_cap + eps)))\n    is_large = item > (harmonic_mean * 0.8)  # Adaptive threshold with margin\n    \n    # Smooth scoring with secondary tie-breaker\n    with np.errstate(divide='ignore', invalid='ignore'):\n        fit_ratio = item / (bins_remain_cap + eps)  # Primary term: fill ratio\n        underfill_penalty = bins_remain_cap - item  # Secondary term: leftover space\n        underfill_penalty = np.clip(underfill_penalty, 0, None)\n        \n        # Smooth exponential weighting of terms\n        fit_component = np.exp(fit_ratio * 2) * can_fit\n        penalty_component = np.exp(-underfill_penalty * 0.5) * can_fit\n        \n        # Adaptive strategy blending\n        if is_large:\n            score = fit_component * penalty_component\n        else:\n            # Prefer underutilized bins with smooth capacity scaling\n            capacity_scaling = bins_remain_cap / (bins_remain_cap.mean() + eps)\n            score = (1.0 + capacity_scaling) * penalty_component\n    \n    return np.where(can_fit, score, -np.inf)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see adaptive multi-objective scoring (residual + fragmentation) vs static zero scores; (2nd) uses harmonic mean for item classification vs (19th)'s random returns; (3rd) leverages dynamic tau scaling vs (18th)'s flat cannot_fit_score. Top heuristics (1-2) integrate statistical measures (\u03bc, \u03c3, harmonic mean) and smooth exponential penalties to balance space utilization and fragmentation, while lower ones (11-17) use rigid thresholds. (7th) combines Worst Fit/Best Fit via item size classification, outperforming (18-20)'s non-adaptive strategies. Worst heuristics (18-20) fail to differentiate bins entirely.\n- \nKeywords: Contextual dynamic thresholds (bin statistics), core objective blending, smooth exponential continuity, feasibility-aware scoring  \nAdvice: Use thresholds derived from real-time bin statistics (mean/std), integrate objectives like residual minimization + fragmentation avoidance as primary terms, ensure scoring differentiability via exponential functions, and replace invalid-bin masks with soft feasibility penalties.  \nAvoid: Item-classification thresholds, tie-breaker penalties (e.g., underfill terms), step functions/discrete tiers, and -\u221e feasibility masks.  \nExplanation: Bin-centric adaptivity and unified objectives reduce heuristic rigidity, while mathematical continuity improves gradient-guided optimization. Soft feasibility constraints maintain search stability versus hard thresholds.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}