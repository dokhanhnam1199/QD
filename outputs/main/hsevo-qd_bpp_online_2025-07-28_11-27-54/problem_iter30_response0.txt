```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority scores for bins using dynamic statistical adaptation with exponential blending.
    
    Combines residual minimization relative to item size and distribution statistics with 
    secondary density penalties via Gaussian kernel weighting. Uses smooth exponential scaling
    for asymptotic robustness and adaptive normalization.
    
    Args:
        item: Size of the item to be packed.
        bins_remain_cap: Array of remaining capacities for each bin.
    
    Returns:
        Array of priority scores for each bin.
    """
    can_fit = bins_remain_cap >= item
    feasible_residuals = bins_remain_cap[can_fit] - item
    
    # Return -inf for all bins if no feasible placement
    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    if not np.any(can_fit):
        return scores
    
    # Compute statistical parameters of feasible residuals
    mean_res = feasible_residuals.mean()
    std_res = feasible_residuals.std()
    std_res += 1e-6  # Avoid division by zero
    
    # Vectorized calculations for all bins
    residuals = bins_remain_cap - item
    
    # Z-score relative to current distribution of residuals
    z = (residuals - mean_res) / std_res
    
    # Contextual scaling relative to item size and distribution width
    # Residual scaled by item size and standard deviation
    scaled_res = residuals / (item + std_res + 1e-6)
    
    # Smooth exponential residual minimization term with sqrt asymptotic scaling
    residual_term = np.exp(-np.sqrt(np.abs(scaled_res) + 1e-6))
    
    # Secondary density penalty via Gaussian kernel weighting (peaks at z=0)
    density_penalty = np.exp(-0.5 * z**2)
    
    # Combined priority with adaptive normalization
    priority = residual_term * density_penalty
    
    # Apply feasibility mask and return
    scores = np.where(can_fit, priority, -np.inf)
    return scores
```
