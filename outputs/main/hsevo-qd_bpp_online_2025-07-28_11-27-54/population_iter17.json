[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)",
    "response_id": 0,
    "obj": 4.487435181491823,
    "SLOC": 2.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores for bins using a Best Fit heuristic.\n    \n    Bins that can fit the item are scored by how little space remains after placement.\n    Bins that cannot fit the item receive a very low priority score.\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    fit_scores = item - bins_remain_cap  # Higher score for bins with less remaining space after placement\n    return np.where(can_fit, fit_scores, -np.inf)",
    "response_id": 1,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive exponential scoring combining Worst Fit for small items and Best Fit for large items.\n    \n    Uses smooth exponential prioritization for both cases: exp(remaining capacity) for small items,\n    exp(-leftover) for large items to minimize fragmentation. Combines dynamic item size classification\n    with mathematically smooth scoring for improved bin utilization.\n    \"\"\"\n    THRESHOLD = 0.5  # Dynamic context-driven threshold for item classification\n    valid_mask = bins_remain_cap >= item\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if item <= THRESHOLD:\n        # Prioritize bins with largest remaining capacity using exponential scoring\n        priorities[valid_mask] = np.exp(bins_remain_cap[valid_mask])  # Smooth amplification of Worst Fit\n    else:\n        # Prioritize bins with smallest leftover using exponential decay\n        leftover = bins_remain_cap[valid_mask] - item\n        priorities[valid_mask] = np.exp(-leftover)  # Smooth Best Fit variant\n    \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response3.txt_stdout.txt",
    "code_path": "problem_iter3_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores using adaptive item size classification and smooth scoring.\"\"\"\n    can_fit = bins_remain_cap >= item\n    if len(bins_remain_cap) == 0:\n        return np.array([])\n    \n    # Dynamic item size classification based on harmonic mean of remaining capacities\n    eps = 1e-9\n    harmonic_mean = len(bins_remain_cap) / (np.sum(1.0 / (bins_remain_cap + eps)))\n    is_large = item > (harmonic_mean * 0.8)  # Adaptive threshold with margin\n    \n    # Smooth scoring with secondary tie-breaker\n    with np.errstate(divide='ignore', invalid='ignore'):\n        fit_ratio = item / (bins_remain_cap + eps)  # Primary term: fill ratio\n        underfill_penalty = bins_remain_cap - item  # Secondary term: leftover space\n        underfill_penalty = np.clip(underfill_penalty, 0, None)\n        \n        # Smooth exponential weighting of terms\n        fit_component = np.exp(fit_ratio * 2) * can_fit\n        penalty_component = np.exp(-underfill_penalty * 0.5) * can_fit\n        \n        # Adaptive strategy blending\n        if is_large:\n            score = fit_component * penalty_component\n        else:\n            # Prefer underutilized bins with smooth capacity scaling\n            capacity_scaling = bins_remain_cap / (bins_remain_cap.mean() + eps)\n            score = (1.0 + capacity_scaling) * penalty_component\n    \n    return np.where(can_fit, score, -np.inf)",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response1.txt_stdout.txt",
    "code_path": "problem_iter5_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritize bins using fixed thresholds and non-smooth Best/Worst Fit for large/small items.\n    \n    Fixed 0.5 threshold classifies items. Best Fit (step) for large, Worst Fit (step) for small.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([])\n    \n    can_fit = bins_remain_cap >= item\n    is_large = item > 0.5  # Fixed threshold for item classification\n    \n    # Initialize priority scores\n    priority = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if not np.any(can_fit):\n        return priority\n    \n    if is_large:\n        # Best Fit: prioritize bins with minimal leftover space (step function)\n        leftover = np.where(can_fit, bins_remain_cap - item, np.inf)\n        min_leftover = leftover.min()\n        best_fit = (leftover == min_leftover) & can_fit\n        priority[best_fit] = 1.0\n    else:\n        # Worst Fit: prioritize bins with maximum remaining capacity (step function)\n        remaining = np.where(can_fit, bins_remain_cap, -np.inf)\n        max_remaining = remaining.max()\n        best_worst = (remaining == max_remaining) & can_fit\n        priority[best_worst] = 1.0\n    \n    return priority",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response4.txt_stdout.txt",
    "code_path": "problem_iter6_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores for bins using a thresholded categorical heuristic.\n    \n    Bins that can fit the item are scored in discrete tiers based on normalized\n    remaining space after placement. Thresholds are anchored to item size for\n    contextual adaptability without instability from continuous scoring.\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Contextual thresholds based on item size\n    tight_threshold = 0.15 * item\n    moderate_threshold = 0.4 * item\n\n    # Identify viable bins and compute post-placement space\n    can_fit = bins_remain_cap >= item\n    remaining_after = bins_remain_cap - item\n\n    # Initialize scores with -inf for invalid bins\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    # Tier 1: Tight fit (minimal leftover space)\n    tight_mask = can_fit & (remaining_after <= tight_threshold)\n    # Tier 2: Moderate fit (small leftover space)\n    mod_mask = can_fit & (remaining_after > tight_threshold) & (remaining_after <= moderate_threshold)\n    # Tier 3: Loose fit (significant leftover space)\n    loose_mask = can_fit & (remaining_after > moderate_threshold)\n\n    # Assign discrete priority scores\n    scores[tight_mask] = 3\n    scores[mod_mask] = 2\n    scores[loose_mask] = 1\n\n    return scores",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.15835660151576,
    "SLOC": 13.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response0.txt_stdout.txt",
    "code_path": "problem_iter7_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, cannot_fit_score: float = -686154.5177323952) -> np.ndarray:\n    \"\"\"Returns priority scores for bins using a Best Fit heuristic.\n    \n    Bins that can fit the item are scored by how little space remains after placement.\n    Bins that cannot fit the item receive a very low priority score.\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n        cannot_fit_score: The score assigned to bins that cannot fit the item.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    fit_scores = item - bins_remain_cap\n    return np.where(can_fit, fit_scores, cannot_fit_score)",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 4.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response3.txt_stdout.txt",
    "code_path": "problem_iter9_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores using dynamic threshold blending with exponential penalties.\n    \n    Blends residual minimization with fragmentation avoidance via:\n    1. Feasibility masking (same as v1)\n    2. Smooth exponential penalty for residuals below dynamic median threshold\n    3. Continuous scoring combining both objectives\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    r = bins_remain_cap - item  # Residual capacity after placement\n    \n    # Dynamic threshold based on current distribution of remaining capacities\n    T = np.clip(np.median(bins_remain_cap), 1e-8, None)  # Avoid division by zero\n    \n    # Smooth penalty term: exponential decay penalizes small residuals relative to T\n    feasible_r = np.where(can_fit, r, np.inf)  # For safe statistics\n    penalty = np.exp(-feasible_r / T)\n    \n    # Continuous blended objective: combine residual minimization with fragmentation avoidance\n    blended_score = -r - penalty  # Primary term: residual minimization; Secondary: penalty shaping\n    \n    # Apply feasibility mask with negative infinity for invalid bins\n    return np.where(can_fit, blended_score, -np.inf)",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response0.txt_stdout.txt",
    "code_path": "problem_iter11_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive priority combining residual minimization and fragmentation avoidance.\n    \n    Uses dynamic alpha blending based on item-to-threshold ratio to balance objectives:\n    large items prioritize fit quality (-r), small items prioritize space cohesion (-exp(-r/T)).\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    r = bins_remain_cap - item  # Residual capacity after placement\n    \n    # Dynamic threshold using median remaining capacity\n    T = np.clip(np.median(bins_remain_cap), 1e-8, None)\n    \n    # Adaptive blending factor based on item size relative to T (sigmoidal response)\n    alpha = 1.0 - np.exp(-item / T)  # Approaches 1 for large items, 0 for small\n    \n    # Smooth penalty term for fragmentation avoidance\n    feasible_r = np.where(can_fit, r, np.inf)  # Mask invalid bins for statistics\n    penalty_term = np.exp(-feasible_r / T)  # Exponential decay penalizes small residuals\n    \n    # Convex combination of residual minimization and penalty shaping\n    blended_score = alpha * (-r) + (1 - alpha) * (-penalty_term)\n    \n    # Enforce feasibility with negative infinity mask\n    return np.where(can_fit, blended_score, -np.inf)",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response2.txt_stdout.txt",
    "code_path": "problem_iter12_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    feasible = bins_remain_cap >= item\n    if not feasible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    r = bins_remain_cap - item\n    feasible_r = r[feasible]\n    \n    mu = feasible_r.mean()\n    std = feasible_r.std()\n    eps = 1e-6\n    tau = mu + std + eps  # Dynamic scale combining central tendency and spread\n    \n    # Smooth penalty term to avoid extreme fragmentation\n    penalty_term = np.exp(-r / tau)\n    \n    # Adaptive weight balancing residual minimization and fragmentation avoidance\n    beta = item / tau\n    \n    # Multi-objective score blending\n    residual_objective = -r\n    fragmentation_objective = -beta * penalty_term\n    score = residual_objective + fragmentation_objective\n    \n    return np.where(feasible, score, -np.inf).astype(np.float64)",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 17.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response2.txt_stdout.txt",
    "code_path": "problem_iter15_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    # Compute dynamic bin statistics\n    mu = np.mean(bins_remain_cap)\n    sigma = np.std(bins_remain_cap)\n    epsilon = 1e-8  # Small epsilon to prevent division by zero\n\n    # Adaptive penalty factor based on average remaining capacity\n    penalty_factor = 1.0 / (mu + epsilon)\n    \n    # Threshold for fragmentation avoidance (1\u03c3 below mean)\n    threshold = mu - sigma\n    \n    # Residual in remaining capacity after placing the item\n    residual = bins_remain_cap - item\n    feasible = residual >= 0\n    \n    # Compute feasible bin scores\n    # Term1: Exponential decay for residual minimization\n    term1 = np.exp(- residual * penalty_factor)\n    \n    # Term2: Fragmentation penalty (exponential decay on threshold deficit)\n    delta = np.clip(threshold - residual, a_min=0.0, a_max=None)\n    term2 = np.exp(- delta * penalty_factor)\n    \n    feasible_scores = term1 * term2\n    \n    # Compute infeasible bin scores (soft penalty)\n    deficit = item - bins_remain_cap\n    infeasible_scores = np.exp(- deficit * penalty_factor * 5.0)  # Stronger penalty for overflow\n    \n    # Combine using convex-like combination (soft masking)\n    scores = np.where(feasible, feasible_scores, infeasible_scores)\n    \n    return scores",
    "response_id": 2,
    "tryHS": false,
    "obj": 5.494615077782224,
    "SLOC": 13.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response0.txt_stdout.txt",
    "code_path": "problem_iter17_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive priority using mean+std thresholding to blend residual minimization and fragmentation penalties.\"\"\"\n    can_fit = bins_remain_cap >= item\n    r = bins_remain_cap - item\n    \n    # Dynamic threshold using mean+std of feasible bins (or all bins if none feasible)\n    feasible_caps = bins_remain_cap[can_fit]\n    if feasible_caps.size == 0:\n        mu, sigma = np.mean(bins_remain_cap), np.std(bins_remain_cap)\n    else:\n        mu, sigma = np.mean(feasible_caps), np.std(feasible_caps)\n    T = np.clip(mu + sigma, 1e-8, None)\n    \n    # Adaptive alpha blending based on item-to-threshold ratio\n    alpha = 1.0 - np.exp(-item / T)\n    \n    # Fragmentation penalty using exponential decay\n    feasible_r = np.where(can_fit, r, np.inf)\n    penalty_term = np.exp(-feasible_r / T)\n    \n    # Multi-objective blend with feasibility enforcement\n    blended_score = alpha * (-r) + (1 - alpha) * (-penalty_term)\n    return np.where(can_fit, blended_score, -np.inf)",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 22.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  }
]