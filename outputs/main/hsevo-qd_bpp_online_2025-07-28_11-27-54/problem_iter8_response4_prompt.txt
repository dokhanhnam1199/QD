{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Adaptive exponential scoring combining Worst Fit for small items and Best Fit for large items.\n    \n    Uses smooth exponential prioritization for both cases: exp(remaining capacity) for small items,\n    exp(-leftover) for large items to minimize fragmentation. Combines dynamic item size classification\n    with mathematically smooth scoring for improved bin utilization.\n    \"\"\"\n    THRESHOLD = 0.5  # Dynamic context-driven threshold for item classification\n    valid_mask = bins_remain_cap >= item\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if item <= THRESHOLD:\n        # Prioritize bins with largest remaining capacity using exponential scoring\n        priorities[valid_mask] = np.exp(bins_remain_cap[valid_mask])  # Smooth amplification of Worst Fit\n    else:\n        # Prioritize bins with smallest leftover using exponential decay\n        leftover = bins_remain_cap[valid_mask] - item\n        priorities[valid_mask] = np.exp(-leftover)  # Smooth Best Fit variant\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores for bins using a thresholded categorical heuristic.\n    \n    Bins that can fit the item are scored in discrete tiers based on normalized\n    remaining space after placement. Thresholds are anchored to item size for\n    contextual adaptability without instability from continuous scoring.\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Contextual thresholds based on item size\n    tight_threshold = 0.15 * item\n    moderate_threshold = 0.4 * item\n\n    # Identify viable bins and compute post-placement space\n    can_fit = bins_remain_cap >= item\n    remaining_after = bins_remain_cap - item\n\n    # Initialize scores with -inf for invalid bins\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    # Tier 1: Tight fit (minimal leftover space)\n    tight_mask = can_fit & (remaining_after <= tight_threshold)\n    # Tier 2: Moderate fit (small leftover space)\n    mod_mask = can_fit & (remaining_after > tight_threshold) & (remaining_after <= moderate_threshold)\n    # Tier 3: Loose fit (significant leftover space)\n    loose_mask = can_fit & (remaining_after > moderate_threshold)\n\n    # Assign discrete priority scores\n    scores[tight_mask] = 3\n    scores[mod_mask] = 2\n    scores[loose_mask] = 1\n\n    return scores\n\n### Analyze & experience\n- Comparing (1st) vs (20th): The best uses adaptive thresholds (harmonic mean) and smooth exponential scoring combining fit ratio/penalty, while the worst returns zero. Key difference: adaptability and multi-factorial smooth evaluation.  \n(2nd) vs (19th): Best Fit scoring (prioritizes minimal leftover) outperforms null scoring, showing explicit placement logic beats no logic.  \n(4th) vs (7th): Adaptive exponential scoring (4th) with context-driven thresholds outperforms fixed-step Best/Worst Fit (7th), highlighting smoothness and dynamic classification advantages.  \n(10th) vs (15th): Thresholded tiers (10th) using item-relative thresholds (0.15x/0.4x) still outperform random/null baselines (15th), but lack adaptability of harmonic mean-based methods.  \n(1st) vs (2nd): The top heuristic adds capacity scaling for underutilized bins and harmonic mean adaptation, while 2nd uses simpler Best Fit. This shows dynamic context modeling improves over static strategies.  \n(6th) vs (9th): Duplicate code in 6th adds harmonic mean adaptation vs 9th's fixed threshold, proving dynamic thresholds > static rules.  \n(7th) vs (10th): Step functions (7th) underperform tiered scoring (10th), suggesting discrete tiers > abrupt decisions but still inferior to smooth exponential weighting.\n- \n- **Keywords**: Residual capacity thresholds, blended scoring, feasibility masks, mathematical continuity  \n- **Advice**: Dynamically adjust thresholds using residual capacities, blend smoothed exponential scoring with secondary penalties (e.g., underfill/fragmentation), enforce feasibility via -\u221e masks for invalid bins, prioritize continuous functions over discrete steps.  \n- **Avoid**: Fixed/static thresholds, generic adaptive methods (e.g., vague \"item size classification\"), step functions, fragmented tie-breakers without contextual grounding.  \n- **Explanation**: Contextual thresholds (residual capacity) and mathematically smooth blending enhance adaptability, while feasibility masks ensure valid decisions. Continuity reduces instability vs. step functions, and targeted penalties minimize suboptimal tradeoffs.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}