{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Adaptive priority using mean+std thresholding to blend residual minimization and fragmentation penalties.\"\"\"\n    can_fit = bins_remain_cap >= item\n    r = bins_remain_cap - item\n    \n    # Dynamic threshold using mean+std of feasible bins (or all bins if none feasible)\n    feasible_caps = bins_remain_cap[can_fit]\n    if feasible_caps.size == 0:\n        mu, sigma = np.mean(bins_remain_cap), np.std(bins_remain_cap)\n    else:\n        mu, sigma = np.mean(feasible_caps), np.std(feasible_caps)\n    T = np.clip(mu + sigma, 1e-8, None)\n    \n    # Adaptive alpha blending based on item-to-threshold ratio\n    alpha = 1.0 - np.exp(-item / T)\n    \n    # Fragmentation penalty using exponential decay\n    feasible_r = np.where(can_fit, r, np.inf)\n    penalty_term = np.exp(-feasible_r / T)\n    \n    # Multi-objective blend with feasibility enforcement\n    blended_score = alpha * (-r) + (1 - alpha) * (-penalty_term)\n    return np.where(can_fit, blended_score, -np.inf)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    # Compute dynamic bin statistics\n    mu = np.mean(bins_remain_cap)\n    sigma = np.std(bins_remain_cap)\n    epsilon = 1e-8  # Small epsilon to prevent division by zero\n\n    # Adaptive penalty factor based on average remaining capacity\n    penalty_factor = 1.0 / (mu + epsilon)\n    \n    # Threshold for fragmentation avoidance (1\u03c3 below mean)\n    threshold = mu - sigma\n    \n    # Residual in remaining capacity after placing the item\n    residual = bins_remain_cap - item\n    feasible = residual >= 0\n    \n    # Compute feasible bin scores\n    # Term1: Exponential decay for residual minimization\n    term1 = np.exp(- residual * penalty_factor)\n    \n    # Term2: Fragmentation penalty (exponential decay on threshold deficit)\n    delta = np.clip(threshold - residual, a_min=0.0, a_max=None)\n    term2 = np.exp(- delta * penalty_factor)\n    \n    feasible_scores = term1 * term2\n    \n    # Compute infeasible bin scores (soft penalty)\n    deficit = item - bins_remain_cap\n    infeasible_scores = np.exp(- deficit * penalty_factor * 5.0)  # Stronger penalty for overflow\n    \n    # Combine using convex-like combination (soft masking)\n    scores = np.where(feasible, feasible_scores, infeasible_scores)\n    \n    return scores\n\n### Analyze & experience\n- Comparing (1st) vs (15th-16th), we see dynamic statistical thresholds (mean+std) and smooth exponential penalties outperform static zero-scores; (2nd) vs (5th) shows adaptive item classification with exponential scoring surpasses fixed thresholds and step functions; (3rd-4th) use linear Best Fit, inferior to distribution-aware blending in (6th-8th). Comparing (1st) vs (2nd), both leverage smooth adaptivity but (1st) integrates spread (std) for fragmentation control. (3rd) vs (4th) are duplicates, highlighting linear scoring limitations. (5th) vs (15th) reveals even basic heuristics outperform no strategy. Overall: superior heuristics balance residual minimization, fragmentation avoidance, and contextual adaptivity via smooth, dynamic scoring.\n- \nKeywords: Adaptive bin statistics, smooth penalty gradients, multi-objective context blending, feasibility masking  \nAdvice: Use bin/item-aware dynamic thresholds (e.g., std dev from current load mean), penalize fragmentation with continuous functions (logistic decay), balance fit and future capacity in scoring, enforce hard constraints with -\u221e masks.  \nAvoid: Fixed thresholds, stepwise penalties, static weight ratios, context-ignorant scoring  \nExplanation: Dynamic thresholds adapt to evolving distributions, smooth gradients enable stable convergence, multi-objective tradeoffs prevent myopic decisions, and feasibility masks guarantee valid allocations.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}