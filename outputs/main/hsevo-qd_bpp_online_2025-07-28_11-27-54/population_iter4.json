[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)",
    "response_id": 0,
    "obj": 4.487435181491823,
    "SLOC": 2.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores for bins using a Best Fit heuristic.\n    \n    Bins that can fit the item are scored by how little space remains after placement.\n    Bins that cannot fit the item receive a very low priority score.\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    fit_scores = item - bins_remain_cap  # Higher score for bins with less remaining space after placement\n    return np.where(can_fit, fit_scores, -np.inf)",
    "response_id": 1,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive exponential scoring combining Worst Fit for small items and Best Fit for large items.\n    \n    Uses smooth exponential prioritization for both cases: exp(remaining capacity) for small items,\n    exp(-leftover) for large items to minimize fragmentation. Combines dynamic item size classification\n    with mathematically smooth scoring for improved bin utilization.\n    \"\"\"\n    THRESHOLD = 0.5  # Dynamic context-driven threshold for item classification\n    valid_mask = bins_remain_cap >= item\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if item <= THRESHOLD:\n        # Prioritize bins with largest remaining capacity using exponential scoring\n        priorities[valid_mask] = np.exp(bins_remain_cap[valid_mask])  # Smooth amplification of Worst Fit\n    else:\n        # Prioritize bins with smallest leftover using exponential decay\n        leftover = bins_remain_cap[valid_mask] - item\n        priorities[valid_mask] = np.exp(-leftover)  # Smooth Best Fit variant\n    \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response3.txt_stdout.txt",
    "code_path": "problem_iter3_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores using adaptive item size classification and smooth scoring.\"\"\"\n    can_fit = bins_remain_cap >= item\n    if len(bins_remain_cap) == 0:\n        return np.array([])\n    \n    # Dynamic item size classification based on harmonic mean of remaining capacities\n    eps = 1e-9\n    harmonic_mean = len(bins_remain_cap) / (np.sum(1.0 / (bins_remain_cap + eps)))\n    is_large = item > (harmonic_mean * 0.8)  # Adaptive threshold with margin\n    \n    # Smooth scoring with secondary tie-breaker\n    with np.errstate(divide='ignore', invalid='ignore'):\n        fit_ratio = item / (bins_remain_cap + eps)  # Primary term: fill ratio\n        underfill_penalty = bins_remain_cap - item  # Secondary term: leftover space\n        underfill_penalty = np.clip(underfill_penalty, 0, None)\n        \n        # Smooth exponential weighting of terms\n        fit_component = np.exp(fit_ratio * 2) * can_fit\n        penalty_component = np.exp(-underfill_penalty * 0.5) * can_fit\n        \n        # Adaptive strategy blending\n        if is_large:\n            score = fit_component * penalty_component\n        else:\n            # Prefer underutilized bins with smooth capacity scaling\n            capacity_scaling = bins_remain_cap / (bins_remain_cap.mean() + eps)\n            score = (1.0 + capacity_scaling) * penalty_component\n    \n    return np.where(can_fit, score, -np.inf)",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  }
]