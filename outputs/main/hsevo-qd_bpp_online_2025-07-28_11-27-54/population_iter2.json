[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)",
    "response_id": 0,
    "obj": 4.487435181491823,
    "SLOC": 2.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores for bins using a Best Fit heuristic.\n    \n    Bins that can fit the item are scored by how little space remains after placement.\n    Bins that cannot fit the item receive a very low priority score.\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    fit_scores = item - bins_remain_cap  # Higher score for bins with less remaining space after placement\n    return np.where(can_fit, fit_scores, -np.inf)",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive exponential scoring combining Worst Fit for small items and Best Fit for large items.\n    \n    Uses smooth exponential prioritization for both cases: exp(remaining capacity) for small items,\n    exp(-leftover) for large items to minimize fragmentation. Combines dynamic item size classification\n    with mathematically smooth scoring for improved bin utilization.\n    \"\"\"\n    THRESHOLD = 0.5  # Dynamic context-driven threshold for item classification\n    valid_mask = bins_remain_cap >= item\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if item <= THRESHOLD:\n        # Prioritize bins with largest remaining capacity using exponential scoring\n        priorities[valid_mask] = np.exp(bins_remain_cap[valid_mask])  # Smooth amplification of Worst Fit\n    else:\n        # Prioritize bins with smallest leftover using exponential decay\n        leftover = bins_remain_cap[valid_mask] - item\n        priorities[valid_mask] = np.exp(-leftover)  # Smooth Best Fit variant\n    \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  }
]