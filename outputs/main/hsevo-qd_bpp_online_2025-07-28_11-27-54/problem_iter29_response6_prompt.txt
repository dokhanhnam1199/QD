{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority scores using adaptive statistical blending of residual and ratio objectives.\n    \n    Smooth exponential components weighted by coefficient of variation prioritize:\n    - Bins with high item/capacity ratio (density-aware packing)\n    - Residual minimization scaled by capacity distribution statistics\n    - Contextual normalization via mean/std of bin capacities\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    \n    # Statistical context of current bin capacities\n    mean_cap = np.mean(bins_remain_cap)\n    std_cap = np.std(bins_remain_cap)\n    epsilon = 1e-6\n    \n    # Adaptive weight via logistic coefficient of variation scaling\n    cv = std_cap / (mean_cap + epsilon)\n    alpha = 1.0 / (1.0 + np.exp(-cv * 10))  # Dynamic ratio/residual weighting\n    \n    # Component calculations\n    residual = bins_remain_cap - item\n    ratio = item / (bins_remain_cap + epsilon)\n    \n    # Smooth exponential residual component scaled by capacity statistics\n    tau_residual = mean_cap + std_cap + epsilon\n    component_residual = np.exp(-residual / tau_residual)\n    \n    # Density-aware ratio component\n    component_ratio = ratio\n    \n    # Multi-objective blending with adaptive weights\n    composite = alpha * component_ratio + (1 - alpha) * component_residual\n    \n    # Enforce feasibility constraint with -inf masking\n    return np.where(can_fit, composite, -np.inf)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (1st) vs (worst): Top heuristics use dynamic thresholds (median/std) and smooth exponential penalties to balance residual minimization with fragmentation avoidance, while the worst (zeros-returning) ignore item/bin context entirely.  \n(2nd) vs (second worst): Adaptive weights (beta=cv-based) in 2nd improve contextual balance vs. 16th-20th's fixed penalty factors.  \n(3rd) vs (4th): Basic Best Fit (3rd) lacks statistical adaptation vs. 4th's z-score density rewards.  \n(5th) vs (6th): 5th's cv-driven weight blending outperforms 6th's static exponential components.  \n(10th) vs (11th): Thresholded tiers (10th) introduce instability vs. 11th's complete non-decision.  \nOverall: Superior heuristics combine **dynamic statistical adaptation** (cv, median, z-scores), **multi-objective smooth blending** (exponential/logistic penalties), and **hard feasibility masking**, while inferior ones use rigid thresholds, ignore context, or fail to penalize fragmentation.\n- \n- Keywords: Dynamic statistical adaptation (mean, std, quantiles), Landau notation asymptoic weighting, smooth exponential/transcendental blending, asymptotic penalty scaling  \n- Advice: Use contextually scaled residual minimization with smooth penalty exponentiation, secondary density penalties via inverse proportionality to bin sparsity, adaptive standardization of weights towards item size harmonics, and rigorous feasibility masking (-\u221e on invalids).  \n- Avoid: Fixed thresholds, dynamic thresholds tied purely to bin stats (e.g., mean/\u03bc+\u03c3), single objective dominance, and any step functions.  \n- Explanation: Dynamic statistical adaptation with smooth exponential blending avoids rigid thresholds while ensuring mathematical continuity. Secondary density penalties balance exploration and packing efficiency. Landau notation asymptotic weighting improves robustness to scale variations, and strict feasibility masking ensures only valid actions are considered.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}