{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority scores using dynamic threshold blending with exponential penalties.\n    \n    Blends residual minimization with fragmentation avoidance via:\n    1. Feasibility masking (same as v1)\n    2. Smooth exponential penalty for residuals below dynamic median threshold\n    3. Continuous scoring combining both objectives\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    r = bins_remain_cap - item  # Residual capacity after placement\n    \n    # Dynamic threshold based on current distribution of remaining capacities\n    T = np.clip(np.median(bins_remain_cap), 1e-8, None)  # Avoid division by zero\n    \n    # Smooth penalty term: exponential decay penalizes small residuals relative to T\n    feasible_r = np.where(can_fit, r, np.inf)  # For safe statistics\n    penalty = np.exp(-feasible_r / T)\n    \n    # Continuous blended objective: combine residual minimization with fragmentation avoidance\n    blended_score = -r - penalty  # Primary term: residual minimization; Secondary: penalty shaping\n    \n    # Apply feasibility mask with negative infinity for invalid bins\n    return np.where(can_fit, blended_score, -np.inf)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores for bins using a thresholded categorical heuristic.\n    \n    Bins that can fit the item are scored in discrete tiers based on normalized\n    remaining space after placement. Thresholds are anchored to item size for\n    contextual adaptability without instability from continuous scoring.\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Contextual thresholds based on item size\n    tight_threshold = 0.15 * item\n    moderate_threshold = 0.4 * item\n\n    # Identify viable bins and compute post-placement space\n    can_fit = bins_remain_cap >= item\n    remaining_after = bins_remain_cap - item\n\n    # Initialize scores with -inf for invalid bins\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    # Tier 1: Tight fit (minimal leftover space)\n    tight_mask = can_fit & (remaining_after <= tight_threshold)\n    # Tier 2: Moderate fit (small leftover space)\n    mod_mask = can_fit & (remaining_after > tight_threshold) & (remaining_after <= moderate_threshold)\n    # Tier 3: Loose fit (significant leftover space)\n    loose_mask = can_fit & (remaining_after > moderate_threshold)\n\n    # Assign discrete priority scores\n    scores[tight_mask] = 3\n    scores[mod_mask] = 2\n    scores[loose_mask] = 1\n\n    return scores\n\n### Analyze & experience\n- Comparing (1st) vs (17th-18th): Top heuristics use adaptive statistical weights (mean, std, logistic balance) for multi-objective blending, while worst ones return flat scores (no prioritization).  \n(2nd) vs (19th-20th): Better heuristics use dynamic thresholds (e.g., item-to-capacity ratios) and exponential penalties, while lower-ranked ones rely on static \u03c3-based thresholds with less contextual adaptivity.  \n(3rd) vs (11th-16th): Smooth exponential scoring (3rd) outperforms discrete tiered scoring (11th-16th) by avoiding abrupt thresholds and enabling gradient-based bin selection.  \n(4th) vs (5th): Dynamic median-based blending (4th) balances residual minimization and fragmentation avoidance more robustly than fixed thresholds.  \n(6th-7th) vs (8th-10th): Heuristics combining residual objectives with item-relative rewards (e.g., r/item normalization) adapt better to varying item sizes than\u5355\u4e00 penalty terms.\n- \n**Keywords**: Dynamic thresholds, statistical context, smooth exponential blending, feasibility masking  \n**Advice**: Use bin/item statistics (mean, std) to adaptively weight residual minimization + density-aware penalties; apply smooth exponential/logistic scoring; enforce feasibility via -\u221e masking; prioritize contextual scaling (e.g., size ratios).  \n**Avoid**: Static thresholds, discrete tiers, non-adaptive weights, zero-based fallbacks, or step functions.  \n**Explanation**: Dynamic statistical adaptation ensures context-aware prioritization, while smooth scoring and feasibility masking prevent fragmentation and invalid allocations. Multi-objective blending with adaptive weights balances residual reduction and density optimization, enhancing robustness across varying distributions.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}