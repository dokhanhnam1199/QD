{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Adaptive priority combining residual minimization and fragmentation avoidance.\n    \n    Uses dynamic alpha blending based on item-to-threshold ratio to balance objectives:\n    large items prioritize fit quality (-r), small items prioritize space cohesion (-exp(-r/T)).\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    r = bins_remain_cap - item  # Residual capacity after placement\n    \n    # Dynamic threshold using median remaining capacity\n    T = np.clip(np.median(bins_remain_cap), 1e-8, None)\n    \n    # Adaptive blending factor based on item size relative to T (sigmoidal response)\n    alpha = 1.0 - np.exp(-item / T)  # Approaches 1 for large items, 0 for small\n    \n    # Smooth penalty term for fragmentation avoidance\n    feasible_r = np.where(can_fit, r, np.inf)  # Mask invalid bins for statistics\n    penalty_term = np.exp(-feasible_r / T)  # Exponential decay penalizes small residuals\n    \n    # Convex combination of residual minimization and penalty shaping\n    blended_score = alpha * (-r) + (1 - alpha) * (-penalty_term)\n    \n    # Enforce feasibility with negative infinity mask\n    return np.where(can_fit, blended_score, -np.inf)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see that top heuristics use dynamic statistical adaptation (mean, std, median) and multi-objective blending (residual minimization + fragmentation penalties), while the worst ones apply rigid soft penalties without contextual awareness. (2nd) vs (19th) show that adaptive thresholds (mean+std) and feasibility masks outperform static thresholds and uniform scoring. (3rd) vs (18th) reveal that explicit feasibility enforcement (-\u221e masking) and exponential decay penalties dominate over naive zero-scoring. (6th) vs (17th) highlight that item-relative rewards (e.g., r/item scaling) and multi-component blending (residual + penalty + reward) outperform single-objective approaches. (11th) vs (16th) demonstrate that categorical tiering (tight/moderate/loose fits) with discrete scores underperforms smooth exponential scoring. Overall: Superior heuristics dynamically adapt weights to bin/item statistics, combine residual minimization with fragmentation avoidance via smooth functions, and rigorously enforce feasibility. Inferior ones rely on fixed thresholds, lack multi-objective balance, or fail to penalize infeasibility effectively.\n- \nKeywords: Dynamic statistical adaptation, multi-objective blending, smooth scoring, feasibility masking  \nAdvice: Integrate context-aware weights combining residual minimization and fragmentation penalties, using logistic/exponential scoring calibrated to real-time bin/item distributions. Prioritize adaptivity via median/\u03c3-driven thresholds and -\u221e infeasibility masking.  \nAvoid: Fixed/static thresholds, single-objective prioritization, stepwise scoring, soft constraint handling.  \nExplanation: Dynamic thresholds (e.g., \u03bc\u00b1\u03c3) and continuous scoring ensure mathematical robustness, while contextual multi-objective blending prevents brittleness in varying distributions. Rigorous masking maintains feasibility without heuristic fallbacks.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}