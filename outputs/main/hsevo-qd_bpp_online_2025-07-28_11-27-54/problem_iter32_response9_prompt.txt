{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines item-relative exponential fit rewards with z-score density rewards, adaptively weighted by coefficient of variation.\n    \n    Key design: Smooth exponential scoring for item-normalized residuals (tighter fits get higher scores) blended with \n    statistical density rewards (preferring bins near current residual mean), dynamically balanced via logistic-weighted \n    coefficient of variation to handle varying bin distributions.\n    \"\"\"\n    EPS = 1e-8\n    feasible = bins_remain_cap >= item\n    \n    if not np.any(feasible):\n        return -np.inf * np.ones_like(bins_remain_cap)\n    \n    # Item-relative fit reward: exponential decay penalty for larger leftover/item ratios\n    r = bins_remain_cap - item\n    fit_component = np.exp(-r / (item + EPS))  # Tighter fits (small r/item) yield higher scores\n    \n    # Statistical density reward: Gaussian-like scoring for residuals near current mean\n    mu, sigma = np.mean(bins_remain_cap), np.std(bins_remain_cap)\n    z = (r - mu) / (sigma + EPS)\n    density_component = np.exp(-0.5 * z**2)  # Peaks when residual matches current mean\n    \n    # Adaptive weight via logistic coefficient of variation\n    cv = sigma / (mu + EPS)\n    weight = 1 / (1 + np.exp(-cv))  # Higher weight on fit_component when bin capacities vary more\n    \n    # Combine objectives and enforce feasibility\n    combined = weight * fit_component + (1 - weight) * density_component\n    return np.where(feasible, combined, -np.inf)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores for bins using a thresholded categorical heuristic.\n    \n    Bins that can fit the item are scored in discrete tiers based on normalized\n    remaining space after placement. Thresholds are anchored to item size for\n    contextual adaptability without instability from continuous scoring.\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Contextual thresholds based on item size\n    tight_threshold = 0.15 * item\n    moderate_threshold = 0.4 * item\n\n    # Identify viable bins and compute post-placement space\n    can_fit = bins_remain_cap >= item\n    remaining_after = bins_remain_cap - item\n\n    # Initialize scores with -inf for invalid bins\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    # Tier 1: Tight fit (minimal leftover space)\n    tight_mask = can_fit & (remaining_after <= tight_threshold)\n    # Tier 2: Moderate fit (small leftover space)\n    mod_mask = can_fit & (remaining_after > tight_threshold) & (remaining_after <= moderate_threshold)\n    # Tier 3: Loose fit (significant leftover space)\n    loose_mask = can_fit & (remaining_after > moderate_threshold)\n\n    # Assign discrete priority scores\n    scores[tight_mask] = 3\n    scores[mod_mask] = 2\n    scores[loose_mask] = 1\n\n    return scores\n\n### Analyze & experience\n- Comparing **1st vs 7th**, the top heuristic uses adaptive logistic-weighted coefficient of variation to balance fit and density rewards, while the 7th relies on fixed thresholds and non-smooth step functions. Smooth exponential scoring and dynamic statistical blending dominate in better heuristics.  \n\n**2nd vs 13th**: The 2nd introduces adaptive density rewards via z-scores and median-based thresholds, whereas the 13th-20th (identical) use static penalty factors and threshold clipping without contextual adaptability.  \n\n**3rd vs 4th**: The 3rd balances residual minimization and fragmentation penalties via tanh-weighted coefficient of variation, while the 4th combines item-relative exponential rewards with less dynamic statistical normalization.  \n\n**5th vs 10th**: Mid-ranked heuristics like the 5th/6th use item-size classification (small/large) with exponential scoring, while the 10th/11th apply fixed categorical tiers (tight/moderate/loose), which are less responsive to distribution shifts.  \n\n**12th (worst)** simply returns zeros, ignoring feasibility and context entirely.  \n\nOverall: Top heuristics excel through **adaptive statistical blending**, **smooth exponential scoring**, and **multi-objective prioritization** (e.g., residual minimization + density rewards), while lower-ranked ones suffer from fixed thresholds, non-smooth penalties, or lack of contextual adaptivity.\n- \nKeywords: Adaptive weights (CV-based), smooth exponential/logistic scoring, multi-objective blending, feasibility masking  \nAdvice: Prioritize context-aware adaptive weights over thresholds, blend residual/density objectives with smooth scoring, use logistic transitions for continuity, enforce feasibility strictly via -\u221e masks.  \nAvoid: Fixed thresholds, step functions, secondary tie-breakers, soft infeasibility handling  \nExplanation: Adaptive weights (vs. thresholds) improve context sensitivity without abrupt transitions. Smooth scoring ensures mathematical continuity for stable gradients. Multi-objective blending balances residual minimization and density efficiency. Strict feasibility masking eliminates invalid solutions, enhancing robustness.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}