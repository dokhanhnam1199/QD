```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority scores using adaptive statistical blending of residual minimization and density-aware penalties.
    
    Key Innovations:
    1. Dynamic length-scale normalization using σ_c(1+size_ratio) for residual scaling
    2. Smooth exponential combination of residual fitness and z-score proximity
    3. Logistic-weighted blending via contextual size_ratio = s/μ_c
    4. Density-aware penalties through residual z-score proximity
    
    Args:
        item: Size of the item to be packed
        bins_remain_cap: Array of remaining capacities for each bin
    
    Returns:
        Array of priority scores for each bin (higher = better)
    """
    # Feasibility masking
    can_fit = bins_remain_cap >= item
    feasible_caps = bins_remain_cap[can_fit]
    
    if feasible_caps.size == 0:
        return np.full_like(bins_remain_cap, -np.inf)
    
    # Core statistics
    residual = feasible_caps - item
    mu_c, sigma_c = feasible_caps.mean(), feasible_caps.std()
    size_ratio = item / (mu_c + 1e-8)
    
    # Adaptive length scale with smooth exponential response
    length_scale = sigma_c * (1 + size_ratio)
    term1 = np.exp(-residual / (length_scale + 1e-8))  # Residual minimization
    
    # Density-aware penalty using z-score proximity
    mu_r, sigma_r = residual.mean(), residual.std()
    z_r = (residual - mu_r) / (sigma_r + 1e-8)
    term2 = np.exp(-np.abs(z_r))  # L1 density proximity
    
    # Logistic blending weight based on contextual size ratio
    alpha = 1.0 / (1.0 + np.exp(-size_ratio))
    score = alpha * term1 + (1 - alpha) * term2
    
    # Final score construction
    full_score = np.full_like(bins_remain_cap, -np.inf)
    full_score[can_fit] = score
    return full_score
```
