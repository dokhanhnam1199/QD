{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines item-relative exponential fit rewards with z-score density rewards, adaptively weighted by coefficient of variation.\n    \n    Key design: Smooth exponential scoring for item-normalized residuals (tighter fits get higher scores) blended with \n    statistical density rewards (preferring bins near current residual mean), dynamically balanced via logistic-weighted \n    coefficient of variation to handle varying bin distributions.\n    \"\"\"\n    EPS = 1e-8\n    feasible = bins_remain_cap >= item\n    \n    if not np.any(feasible):\n        return -np.inf * np.ones_like(bins_remain_cap)\n    \n    # Item-relative fit reward: exponential decay penalty for larger leftover/item ratios\n    r = bins_remain_cap - item\n    fit_component = np.exp(-r / (item + EPS))  # Tighter fits (small r/item) yield higher scores\n    \n    # Statistical density reward: Gaussian-like scoring for residuals near current mean\n    mu, sigma = np.mean(bins_remain_cap), np.std(bins_remain_cap)\n    z = (r - mu) / (sigma + EPS)\n    density_component = np.exp(-0.5 * z**2)  # Peaks when residual matches current mean\n    \n    # Adaptive weight via logistic coefficient of variation\n    cv = sigma / (mu + EPS)\n    weight = 1 / (1 + np.exp(-cv))  # Higher weight on fit_component when bin capacities vary more\n    \n    # Combine objectives and enforce feasibility\n    combined = weight * fit_component + (1 - weight) * density_component\n    return np.where(feasible, combined, -np.inf)\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Dynamic median-based blending with z-score density rewards for residual clustering.\n    \n    Combines residual minimization & fragmentation avoidance (v0) with secondary\n    density rewards using z-score proximity to mean residual. Adaptive weight scaling\n    amplifies clustering incentives when bin capacity variance is high.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    r = bins_remain_cap - item  # Residual capacity after placement\n    \n    # Feasible residual mask for numerical stability\n    feasible_r = np.where(can_fit, r, np.inf)\n    \n    # Dynamic threshold based on median remaining capacity\n    T = np.clip(np.median(bins_remain_cap), 1e-8, None)\n    \n    # Primary objective: residual minimization with exponential fragmentation penalty\n    penalty_primary = np.exp(-feasible_r / T)  # Higher penalty for small residuals\n    blended_primary = -r - penalty_primary     # Balance residual & penalty\n    \n    # Secondary objective: density rewards via z-score proximity to mean residual\n    feasible_mask = can_fit & (feasible_r != np.inf)\n    feasible_count = np.sum(feasible_mask)\n    \n    if feasible_count > 0:\n        mu_r = np.mean(feasible_r[feasible_mask])\n        sigma_r = np.std(feasible_r[feasible_mask]) + 1e-8\n        \n        # Z-score normalized by residual distribution statistics\n        z_scores = (r - mu_r) / sigma_r\n        secondary_reward = np.exp(-np.abs(z_scores))  # Reward proximity to mean\n    else:\n        secondary_reward = np.zeros_like(r)\n    \n    # Adaptive scaling: amplify secondary reward when bin capacity variance is high\n    bin_cv = np.std(bins_remain_cap) / (T + 1e-8)  # Coefficient of variation\n    adaptive_weight = 1.0 + bin_cv  # Amplify clustering incentives\n    \n    # Final score: primary objective + scaled density reward\n    blended_total = blended_primary + adaptive_weight * secondary_reward\n    \n    # Enforce feasibility constraints with -inf mask\n    return np.where(can_fit, blended_total, -np.inf)\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores for bins using a distribution-aware heuristic with smooth exponential scoring.\n    \n    Scores are computed by blending residual minimization and fragmentation penalties via adaptive weights,\n    dynamically adjusted to the current bin capacity distribution. Uses feasibility masks to exclude invalid bins.\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    if not np.any(can_fit):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    mu = np.mean(bins_remain_cap)\n    sigma = np.std(bins_remain_cap)\n    epsilon = 1e-9\n    \n    # Dynamic threshold for fragmentation awareness\n    threshold = mu - sigma\n    \n    # Residual computation for feasible bins\n    r = bins_remain_cap - item\n    \n    # Smooth exponential residual score (smaller r \u2192 higher score)\n    residual_score = np.exp(-r / (mu + sigma + epsilon))\n    \n    # Smooth threshold-crossing score using logistic transition\n    threshold_diff = r - threshold\n    fragment_score = 1.0 / (1.0 + np.exp(threshold_diff / (sigma + epsilon)))\n    \n    # Adaptive weights based on coefficient of variation\n    cv = sigma / (mu + epsilon)\n    weight_residual = 1.0 - np.tanh(cv)  # Dominates when distribution is tight\n    weight_fragment = np.tanh(cv)        # Dominates when distribution is spread out\n    \n    # Combine components with adaptive weights\n    combined_score = weight_residual * residual_score + weight_fragment * fragment_score\n    \n    # Apply feasibility mask\n    return np.where(can_fit, combined_score, -np.inf)\n\n[Heuristics 4th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Priority heuristic combining dynamic bin statistics with item-relative rewards.\n    \n    Uses adaptive thresholds (mean+std) and smooth penalties from v0, augmented with\n    item-aware exponential rewards (v1-inspired) to prioritize fits where remaining space\n    is small relative to item size. Balances residual minimization, fragmentation\n    avoidance, and contextual adaptivity through multi-objective blending.\n    \"\"\"\n    feasible = bins_remain_cap >= item\n    if not feasible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    r = bins_remain_cap - item\n    feasible_r = r[feasible]\n    \n    mu = feasible_r.mean()\n    std = feasible_r.std()\n    eps = 1e-9\n    tau = mu + std + eps\n    \n    # Core components from v0\n    beta = item / tau\n    residual_objective = -r\n    penalty_term = np.exp(-r / tau)\n    fragmentation_objective = -beta * penalty_term\n    \n    # Item-relative reward (smooth v1-inspired component)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        item_rel_scale = r / (item + eps)\n    item_relative_reward = np.exp(-item_rel_scale)  # High reward for small r/item ratios\n    \n    # Multi-objective combination\n    score = residual_objective + fragmentation_objective + item_relative_reward\n    \n    return np.where(feasible, score, -np.inf).astype(np.float64)\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive exponential scoring combining Worst Fit for small items and Best Fit for large items.\n    \n    Uses smooth exponential prioritization for both cases: exp(remaining capacity) for small items,\n    exp(-leftover) for large items to minimize fragmentation. Combines dynamic item size classification\n    with mathematically smooth scoring for improved bin utilization.\n    \"\"\"\n    THRESHOLD = 0.5  # Dynamic context-driven threshold for item classification\n    valid_mask = bins_remain_cap >= item\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if item <= THRESHOLD:\n        # Prioritize bins with largest remaining capacity using exponential scoring\n        priorities[valid_mask] = np.exp(bins_remain_cap[valid_mask])  # Smooth amplification of Worst Fit\n    else:\n        # Prioritize bins with smallest leftover using exponential decay\n        leftover = bins_remain_cap[valid_mask] - item\n        priorities[valid_mask] = np.exp(-leftover)  # Smooth Best Fit variant\n    \n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive exponential scoring combining Worst Fit for small items and Best Fit for large items.\n    \n    Uses smooth exponential prioritization for both cases: exp(remaining capacity) for small items,\n    exp(-leftover) for large items to minimize fragmentation. Combines dynamic item size classification\n    with mathematically smooth scoring for improved bin utilization.\n    \"\"\"\n    THRESHOLD = 0.5  # Dynamic context-driven threshold for item classification\n    valid_mask = bins_remain_cap >= item\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if item <= THRESHOLD:\n        # Prioritize bins with largest remaining capacity using exponential scoring\n        priorities[valid_mask] = np.exp(bins_remain_cap[valid_mask])  # Smooth amplification of Worst Fit\n    else:\n        # Prioritize bins with smallest leftover using exponential decay\n        leftover = bins_remain_cap[valid_mask] - item\n        priorities[valid_mask] = np.exp(-leftover)  # Smooth Best Fit variant\n    \n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritize bins using fixed thresholds and non-smooth Best/Worst Fit for large/small items.\n    \n    Fixed 0.5 threshold classifies items. Best Fit (step) for large, Worst Fit (step) for small.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([])\n    \n    can_fit = bins_remain_cap >= item\n    is_large = item > 0.5  # Fixed threshold for item classification\n    \n    # Initialize priority scores\n    priority = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if not np.any(can_fit):\n        return priority\n    \n    if is_large:\n        # Best Fit: prioritize bins with minimal leftover space (step function)\n        leftover = np.where(can_fit, bins_remain_cap - item, np.inf)\n        min_leftover = leftover.min()\n        best_fit = (leftover == min_leftover) & can_fit\n        priority[best_fit] = 1.0\n    else:\n        # Worst Fit: prioritize bins with maximum remaining capacity (step function)\n        remaining = np.where(can_fit, bins_remain_cap, -np.inf)\n        max_remaining = remaining.max()\n        best_worst = (remaining == max_remaining) & can_fit\n        priority[best_worst] = 1.0\n    \n    return priority\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores using adaptive statistical blending of residual and ratio objectives.\n    \n    Smooth exponential components weighted by coefficient of variation prioritize:\n    - Bins with high item/capacity ratio (density-aware packing)\n    - Residual minimization scaled by capacity distribution statistics\n    - Contextual normalization via mean/std of bin capacities\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    \n    # Statistical context of current bin capacities\n    mean_cap = np.mean(bins_remain_cap)\n    std_cap = np.std(bins_remain_cap)\n    epsilon = 1e-6\n    \n    # Adaptive weight via logistic coefficient of variation scaling\n    cv = std_cap / (mean_cap + epsilon)\n    alpha = 1.0 / (1.0 + np.exp(-cv * 10))  # Dynamic ratio/residual weighting\n    \n    # Component calculations\n    residual = bins_remain_cap - item\n    ratio = item / (bins_remain_cap + epsilon)\n    \n    # Smooth exponential residual component scaled by capacity statistics\n    tau_residual = mean_cap + std_cap + epsilon\n    component_residual = np.exp(-residual / tau_residual)\n    \n    # Density-aware ratio component\n    component_ratio = ratio\n    \n    # Multi-objective blending with adaptive weights\n    composite = alpha * component_ratio + (1 - alpha) * component_residual\n    \n    # Enforce feasibility constraint with -inf masking\n    return np.where(can_fit, composite, -np.inf)\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines item-relative exponential fit rewards with z-score density rewards, adaptively weighted by coefficient of variation.\n    \n    Key design: Smooth exponential scoring for item-normalized residuals (tighter fits get higher scores) blended with \n    statistical density rewards (preferring bins near current residual mean), dynamically balanced via logistic-weighted \n    coefficient of variation to handle varying bin distributions.\n    \"\"\"\n    EPS = 1e-8\n    feasible = bins_remain_cap >= item\n    \n    if not np.any(feasible):\n        return -np.inf * np.ones_like(bins_remain_cap)\n    \n    # Item-relative fit reward: exponential decay penalty for larger leftover/item ratios\n    r = bins_remain_cap - item\n    fit_component = np.exp(-r / (item + EPS))  # Tighter fits (small r/item) yield higher scores\n    \n    # Statistical density reward: Gaussian-like scoring for residuals near current mean\n    mu, sigma = np.mean(bins_remain_cap), np.std(bins_remain_cap)\n    z = (r - mu) / (sigma + EPS)\n    density_component = np.exp(-0.5 * z**2)  # Peaks when residual matches current mean\n    \n    # Adaptive weight via logistic coefficient of variation\n    cv = sigma / (mu + EPS)\n    weight = 1 / (1 + np.exp(-cv))  # Higher weight on fit_component when bin capacities vary more\n    \n    # Combine objectives and enforce feasibility\n    combined = weight * fit_component + (1 - weight) * density_component\n    return np.where(feasible, combined, -np.inf)\n\n[Heuristics 10th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores for bins using a thresholded categorical heuristic.\n    \n    Bins that can fit the item are scored in discrete tiers based on normalized\n    remaining space after placement. Thresholds are anchored to item size for\n    contextual adaptability without instability from continuous scoring.\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Contextual thresholds based on item size\n    tight_threshold = 0.15 * item\n    moderate_threshold = 0.4 * item\n\n    # Identify viable bins and compute post-placement space\n    can_fit = bins_remain_cap >= item\n    remaining_after = bins_remain_cap - item\n\n    # Initialize scores with -inf for invalid bins\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    # Tier 1: Tight fit (minimal leftover space)\n    tight_mask = can_fit & (remaining_after <= tight_threshold)\n    # Tier 2: Moderate fit (small leftover space)\n    mod_mask = can_fit & (remaining_after > tight_threshold) & (remaining_after <= moderate_threshold)\n    # Tier 3: Loose fit (significant leftover space)\n    loose_mask = can_fit & (remaining_after > moderate_threshold)\n\n    # Assign discrete priority scores\n    scores[tight_mask] = 3\n    scores[mod_mask] = 2\n    scores[loose_mask] = 1\n\n    return scores\n\n[Heuristics 11th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores for bins using a thresholded categorical heuristic.\n    \n    Bins that can fit the item are scored in discrete tiers based on normalized\n    remaining space after placement. Thresholds are anchored to item size for\n    contextual adaptability without instability from continuous scoring.\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Contextual thresholds based on item size\n    tight_threshold = 0.15 * item\n    moderate_threshold = 0.4 * item\n\n    # Identify viable bins and compute post-placement space\n    can_fit = bins_remain_cap >= item\n    remaining_after = bins_remain_cap - item\n\n    # Initialize scores with -inf for invalid bins\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    # Tier 1: Tight fit (minimal leftover space)\n    tight_mask = can_fit & (remaining_after <= tight_threshold)\n    # Tier 2: Moderate fit (small leftover space)\n    mod_mask = can_fit & (remaining_after > tight_threshold) & (remaining_after <= moderate_threshold)\n    # Tier 3: Loose fit (significant leftover space)\n    loose_mask = can_fit & (remaining_after > moderate_threshold)\n\n    # Assign discrete priority scores\n    scores[tight_mask] = 3\n    scores[mod_mask] = 2\n    scores[loose_mask] = 1\n\n    return scores\n\n[Heuristics 12th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    # Compute dynamic bin statistics\n    mu = np.mean(bins_remain_cap)\n    sigma = np.std(bins_remain_cap)\n    epsilon = 1e-8  # Small epsilon to prevent division by zero\n\n    # Adaptive penalty factor based on average remaining capacity\n    penalty_factor = 1.0 / (mu + epsilon)\n    \n    # Threshold for fragmentation avoidance (1\u03c3 below mean)\n    threshold = mu - sigma\n    \n    # Residual in remaining capacity after placing the item\n    residual = bins_remain_cap - item\n    feasible = residual >= 0\n    \n    # Compute feasible bin scores\n    # Term1: Exponential decay for residual minimization\n    term1 = np.exp(- residual * penalty_factor)\n    \n    # Term2: Fragmentation penalty (exponential decay on threshold deficit)\n    delta = np.clip(threshold - residual, a_min=0.0, a_max=None)\n    term2 = np.exp(- delta * penalty_factor)\n    \n    feasible_scores = term1 * term2\n    \n    # Compute infeasible bin scores (soft penalty)\n    deficit = item - bins_remain_cap\n    infeasible_scores = np.exp(- deficit * penalty_factor * 5.0)  # Stronger penalty for overflow\n    \n    # Combine using convex-like combination (soft masking)\n    scores = np.where(feasible, feasible_scores, infeasible_scores)\n    \n    return scores\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    # Compute dynamic bin statistics\n    mu = np.mean(bins_remain_cap)\n    sigma = np.std(bins_remain_cap)\n    epsilon = 1e-8  # Small epsilon to prevent division by zero\n\n    # Adaptive penalty factor based on average remaining capacity\n    penalty_factor = 1.0 / (mu + epsilon)\n    \n    # Threshold for fragmentation avoidance (1\u03c3 below mean)\n    threshold = mu - sigma\n    \n    # Residual in remaining capacity after placing the item\n    residual = bins_remain_cap - item\n    feasible = residual >= 0\n    \n    # Compute feasible bin scores\n    # Term1: Exponential decay for residual minimization\n    term1 = np.exp(- residual * penalty_factor)\n    \n    # Term2: Fragmentation penalty (exponential decay on threshold deficit)\n    delta = np.clip(threshold - residual, a_min=0.0, a_max=None)\n    term2 = np.exp(- delta * penalty_factor)\n    \n    feasible_scores = term1 * term2\n    \n    # Compute infeasible bin scores (soft penalty)\n    deficit = item - bins_remain_cap\n    infeasible_scores = np.exp(- deficit * penalty_factor * 5.0)  # Stronger penalty for overflow\n    \n    # Combine using convex-like combination (soft masking)\n    scores = np.where(feasible, feasible_scores, infeasible_scores)\n    \n    return scores\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    # Compute dynamic bin statistics\n    mu = np.mean(bins_remain_cap)\n    sigma = np.std(bins_remain_cap)\n    epsilon = 1e-8  # Small epsilon to prevent division by zero\n\n    # Adaptive penalty factor based on average remaining capacity\n    penalty_factor = 1.0 / (mu + epsilon)\n    \n    # Threshold for fragmentation avoidance (1\u03c3 below mean)\n    threshold = mu - sigma\n    \n    # Residual in remaining capacity after placing the item\n    residual = bins_remain_cap - item\n    feasible = residual >= 0\n    \n    # Compute feasible bin scores\n    # Term1: Exponential decay for residual minimization\n    term1 = np.exp(- residual * penalty_factor)\n    \n    # Term2: Fragmentation penalty (exponential decay on threshold deficit)\n    delta = np.clip(threshold - residual, a_min=0.0, a_max=None)\n    term2 = np.exp(- delta * penalty_factor)\n    \n    feasible_scores = term1 * term2\n    \n    # Compute infeasible bin scores (soft penalty)\n    deficit = item - bins_remain_cap\n    infeasible_scores = np.exp(- deficit * penalty_factor * 5.0)  # Stronger penalty for overflow\n    \n    # Combine using convex-like combination (soft masking)\n    scores = np.where(feasible, feasible_scores, infeasible_scores)\n    \n    return scores\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    # Compute dynamic bin statistics\n    mu = np.mean(bins_remain_cap)\n    sigma = np.std(bins_remain_cap)\n    epsilon = 1e-8  # Small epsilon to prevent division by zero\n\n    # Adaptive penalty factor based on average remaining capacity\n    penalty_factor = 1.0 / (mu + epsilon)\n    \n    # Threshold for fragmentation avoidance (1\u03c3 below mean)\n    threshold = mu - sigma\n    \n    # Residual in remaining capacity after placing the item\n    residual = bins_remain_cap - item\n    feasible = residual >= 0\n    \n    # Compute feasible bin scores\n    # Term1: Exponential decay for residual minimization\n    term1 = np.exp(- residual * penalty_factor)\n    \n    # Term2: Fragmentation penalty (exponential decay on threshold deficit)\n    delta = np.clip(threshold - residual, a_min=0.0, a_max=None)\n    term2 = np.exp(- delta * penalty_factor)\n    \n    feasible_scores = term1 * term2\n    \n    # Compute infeasible bin scores (soft penalty)\n    deficit = item - bins_remain_cap\n    infeasible_scores = np.exp(- deficit * penalty_factor * 5.0)  # Stronger penalty for overflow\n    \n    # Combine using convex-like combination (soft masking)\n    scores = np.where(feasible, feasible_scores, infeasible_scores)\n    \n    return scores\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    # Compute dynamic bin statistics\n    mu = np.mean(bins_remain_cap)\n    sigma = np.std(bins_remain_cap)\n    epsilon = 1e-8  # Small epsilon to prevent division by zero\n\n    # Adaptive penalty factor based on average remaining capacity\n    penalty_factor = 1.0 / (mu + epsilon)\n    \n    # Threshold for fragmentation avoidance (1\u03c3 below mean)\n    threshold = mu - sigma\n    \n    # Residual in remaining capacity after placing the item\n    residual = bins_remain_cap - item\n    feasible = residual >= 0\n    \n    # Compute feasible bin scores\n    # Term1: Exponential decay for residual minimization\n    term1 = np.exp(- residual * penalty_factor)\n    \n    # Term2: Fragmentation penalty (exponential decay on threshold deficit)\n    delta = np.clip(threshold - residual, a_min=0.0, a_max=None)\n    term2 = np.exp(- delta * penalty_factor)\n    \n    feasible_scores = term1 * term2\n    \n    # Compute infeasible bin scores (soft penalty)\n    deficit = item - bins_remain_cap\n    infeasible_scores = np.exp(- deficit * penalty_factor * 5.0)  # Stronger penalty for overflow\n    \n    # Combine using convex-like combination (soft masking)\n    scores = np.where(feasible, feasible_scores, infeasible_scores)\n    \n    return scores\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    # Compute dynamic bin statistics\n    mu = np.mean(bins_remain_cap)\n    sigma = np.std(bins_remain_cap)\n    epsilon = 1e-8  # Small epsilon to prevent division by zero\n\n    # Adaptive penalty factor based on average remaining capacity\n    penalty_factor = 1.0 / (mu + epsilon)\n    \n    # Threshold for fragmentation avoidance (1\u03c3 below mean)\n    threshold = mu - sigma\n    \n    # Residual in remaining capacity after placing the item\n    residual = bins_remain_cap - item\n    feasible = residual >= 0\n    \n    # Compute feasible bin scores\n    # Term1: Exponential decay for residual minimization\n    term1 = np.exp(- residual * penalty_factor)\n    \n    # Term2: Fragmentation penalty (exponential decay on threshold deficit)\n    delta = np.clip(threshold - residual, a_min=0.0, a_max=None)\n    term2 = np.exp(- delta * penalty_factor)\n    \n    feasible_scores = term1 * term2\n    \n    # Compute infeasible bin scores (soft penalty)\n    deficit = item - bins_remain_cap\n    infeasible_scores = np.exp(- deficit * penalty_factor * 5.0)  # Stronger penalty for overflow\n    \n    # Combine using convex-like combination (soft masking)\n    scores = np.where(feasible, feasible_scores, infeasible_scores)\n    \n    return scores\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    # Compute dynamic bin statistics\n    mu = np.mean(bins_remain_cap)\n    sigma = np.std(bins_remain_cap)\n    epsilon = 1e-8  # Small epsilon to prevent division by zero\n\n    # Adaptive penalty factor based on average remaining capacity\n    penalty_factor = 1.0 / (mu + epsilon)\n    \n    # Threshold for fragmentation avoidance (1\u03c3 below mean)\n    threshold = mu - sigma\n    \n    # Residual in remaining capacity after placing the item\n    residual = bins_remain_cap - item\n    feasible = residual >= 0\n    \n    # Compute feasible bin scores\n    # Term1: Exponential decay for residual minimization\n    term1 = np.exp(- residual * penalty_factor)\n    \n    # Term2: Fragmentation penalty (exponential decay on threshold deficit)\n    delta = np.clip(threshold - residual, a_min=0.0, a_max=None)\n    term2 = np.exp(- delta * penalty_factor)\n    \n    feasible_scores = term1 * term2\n    \n    # Compute infeasible bin scores (soft penalty)\n    deficit = item - bins_remain_cap\n    infeasible_scores = np.exp(- deficit * penalty_factor * 5.0)  # Stronger penalty for overflow\n    \n    # Combine using convex-like combination (soft masking)\n    scores = np.where(feasible, feasible_scores, infeasible_scores)\n    \n    return scores\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    # Compute dynamic bin statistics\n    mu = np.mean(bins_remain_cap)\n    sigma = np.std(bins_remain_cap)\n    epsilon = 1e-8  # Small epsilon to prevent division by zero\n\n    # Adaptive penalty factor based on average remaining capacity\n    penalty_factor = 1.0 / (mu + epsilon)\n    \n    # Threshold for fragmentation avoidance (1\u03c3 below mean)\n    threshold = mu - sigma\n    \n    # Residual in remaining capacity after placing the item\n    residual = bins_remain_cap - item\n    feasible = residual >= 0\n    \n    # Compute feasible bin scores\n    # Term1: Exponential decay for residual minimization\n    term1 = np.exp(- residual * penalty_factor)\n    \n    # Term2: Fragmentation penalty (exponential decay on threshold deficit)\n    delta = np.clip(threshold - residual, a_min=0.0, a_max=None)\n    term2 = np.exp(- delta * penalty_factor)\n    \n    feasible_scores = term1 * term2\n    \n    # Compute infeasible bin scores (soft penalty)\n    deficit = item - bins_remain_cap\n    infeasible_scores = np.exp(- deficit * penalty_factor * 5.0)  # Stronger penalty for overflow\n    \n    # Combine using convex-like combination (soft masking)\n    scores = np.where(feasible, feasible_scores, infeasible_scores)\n    \n    return scores\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}