```python
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Adaptive scoring combining Best/Worst Fit with normalized fragmentation penalties.
    
    Uses harmonic mean for item classification and smooth exponential blending 
    of fit ratio + normalized leftover penalties. Preserves bin viability through 
    continuous scoring.
    """
    if len(bins_remain_cap) == 0:
        return np.array([])
    eps = 1e-9
    can_fit = bins_remain_cap >= item
    
    # Dynamic item size classification using harmonic mean
    harmonic_mean = len(bins_remain_cap) / (np.sum(1.0 / (bins_remain_cap + eps)) + eps)
    is_large = item > (harmonic_mean * 0.7)  # Adaptive threshold
    
    # Key metrics
    fit_ratio = item / (bins_remain_cap + eps)
    leftover = bins_remain_cap - item
    
    # Contextual leftover normalization (relative to item size and stats)
    normalized_leftover = leftover / (item + harmonic_mean + eps)
    
    # Smooth component construction
    with np.errstate(divide='ignore', invalid='ignore'):
        fit_component = np.exp(fit_ratio * 2)  # Primary fit quality
        penalty_component = np.exp(-normalized_leftover * 0.75)  # Fragmentation penalty
        
        if is_large:
            # Large items prioritize tight fits exponentially
            score = fit_component * penalty_component
        else:
            # Small items use capacity scaling + soft fragmentation penalty
            capacity_scaling = (bins_remain_cap / (harmonic_mean + eps)) ** 0.5
            score = (1 + capacity_scaling) * penalty_component ** 0.5
    
    return np.where(can_fit, score, -np.inf)
```
