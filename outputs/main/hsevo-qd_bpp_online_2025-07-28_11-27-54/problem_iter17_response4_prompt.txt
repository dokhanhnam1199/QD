{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    feasible = bins_remain_cap >= item\n    if not feasible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    r = bins_remain_cap - item\n    feasible_r = r[feasible]\n    \n    mu = feasible_r.mean()\n    std = feasible_r.std()\n    eps = 1e-6\n    tau = mu + std + eps  # Dynamic scale combining central tendency and spread\n    \n    # Smooth penalty term to avoid extreme fragmentation\n    penalty_term = np.exp(-r / tau)\n    \n    # Adaptive weight balancing residual minimization and fragmentation avoidance\n    beta = item / tau\n    \n    # Multi-objective score blending\n    residual_objective = -r\n    fragmentation_objective = -beta * penalty_term\n    score = residual_objective + fragmentation_objective\n    \n    return np.where(feasible, score, -np.inf).astype(np.float64)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    # Compute dynamic bin statistics\n    mu = np.mean(bins_remain_cap)\n    sigma = np.std(bins_remain_cap)\n    epsilon = 1e-8  # Small epsilon to prevent division by zero\n\n    # Adaptive penalty factor based on average remaining capacity\n    penalty_factor = 1.0 / (mu + epsilon)\n    \n    # Threshold for fragmentation avoidance (1\u03c3 below mean)\n    threshold = mu - sigma\n    \n    # Residual in remaining capacity after placing the item\n    residual = bins_remain_cap - item\n    feasible = residual >= 0\n    \n    # Compute feasible bin scores\n    # Term1: Exponential decay for residual minimization\n    term1 = np.exp(- residual * penalty_factor)\n    \n    # Term2: Fragmentation penalty (exponential decay on threshold deficit)\n    delta = np.clip(threshold - residual, a_min=0.0, a_max=None)\n    term2 = np.exp(- delta * penalty_factor)\n    \n    feasible_scores = term1 * term2\n    \n    # Compute infeasible bin scores (soft penalty)\n    deficit = item - bins_remain_cap\n    infeasible_scores = np.exp(- deficit * penalty_factor * 5.0)  # Stronger penalty for overflow\n    \n    # Combine using convex-like combination (soft masking)\n    scores = np.where(feasible, feasible_scores, infeasible_scores)\n    \n    return scores\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see adaptive alpha blending with dynamic thresholds (median-based) in 1st outperforms static penalty factors in 20th. (3rd) vs (12th): 3rd\u2019s context-aware tau (\u03bc+\u03c3) balances residual minimization and fragmentation avoidance, while 12th\u2019s zero-scores fail to prioritize. (6th) vs (4th): Smooth exponential scoring in 6th improves bin utilization over 4th\u2019s rigid step-function thresholds. (11th) vs (15th): Item-relative tiered thresholds in 11th reduce fragmentation better than 15th\u2019s generic penalty terms. Overall: Top heuristics use dynamic thresholds, multi-objective blending, and smooth scoring; worst rely on static rules or trivial outputs.\n- \n- **Keywords**: Dynamic thresholds (\u03bc/\u03c3), adaptive weights (\u03b1/\u03b2 scaling), smooth exponential scoring, feasibility masks  \n- **Advice**: Prioritize distribution-aware thresholds (median, mean+stddev), blend residual minimization with fragmentation penalties via scaled adaptive weights, use smooth exponential scoring for continuity, enforce strict feasibility via -\u221e masks.  \n- **Avoid**: Static thresholds, ratio-based scoring, zero-based fallbacks, simplistic priority schemes, contextual size classification.  \n- **Explanation**: Dynamic thresholds adapt to real-time data, smooth scoring ensures gradient stability, adaptive weights balance competing objectives, and feasibility masks eliminate invalid choices, collectively enhancing robustness and efficiency.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}