[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)",
    "response_id": 0,
    "obj": 4.487435181491823,
    "SLOC": 2.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores for bins using a Best Fit heuristic.\n    \n    Bins that can fit the item are scored by how little space remains after placement.\n    Bins that cannot fit the item receive a very low priority score.\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    fit_scores = item - bins_remain_cap  # Higher score for bins with less remaining space after placement\n    return np.where(can_fit, fit_scores, -np.inf)",
    "response_id": 1,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive exponential scoring combining Worst Fit for small items and Best Fit for large items.\n    \n    Uses smooth exponential prioritization for both cases: exp(remaining capacity) for small items,\n    exp(-leftover) for large items to minimize fragmentation. Combines dynamic item size classification\n    with mathematically smooth scoring for improved bin utilization.\n    \"\"\"\n    THRESHOLD = 0.5  # Dynamic context-driven threshold for item classification\n    valid_mask = bins_remain_cap >= item\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if item <= THRESHOLD:\n        # Prioritize bins with largest remaining capacity using exponential scoring\n        priorities[valid_mask] = np.exp(bins_remain_cap[valid_mask])  # Smooth amplification of Worst Fit\n    else:\n        # Prioritize bins with smallest leftover using exponential decay\n        leftover = bins_remain_cap[valid_mask] - item\n        priorities[valid_mask] = np.exp(-leftover)  # Smooth Best Fit variant\n    \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response3.txt_stdout.txt",
    "code_path": "problem_iter3_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores using adaptive item size classification and smooth scoring.\"\"\"\n    can_fit = bins_remain_cap >= item\n    if len(bins_remain_cap) == 0:\n        return np.array([])\n    \n    # Dynamic item size classification based on harmonic mean of remaining capacities\n    eps = 1e-9\n    harmonic_mean = len(bins_remain_cap) / (np.sum(1.0 / (bins_remain_cap + eps)))\n    is_large = item > (harmonic_mean * 0.8)  # Adaptive threshold with margin\n    \n    # Smooth scoring with secondary tie-breaker\n    with np.errstate(divide='ignore', invalid='ignore'):\n        fit_ratio = item / (bins_remain_cap + eps)  # Primary term: fill ratio\n        underfill_penalty = bins_remain_cap - item  # Secondary term: leftover space\n        underfill_penalty = np.clip(underfill_penalty, 0, None)\n        \n        # Smooth exponential weighting of terms\n        fit_component = np.exp(fit_ratio * 2) * can_fit\n        penalty_component = np.exp(-underfill_penalty * 0.5) * can_fit\n        \n        # Adaptive strategy blending\n        if is_large:\n            score = fit_component * penalty_component\n        else:\n            # Prefer underutilized bins with smooth capacity scaling\n            capacity_scaling = bins_remain_cap / (bins_remain_cap.mean() + eps)\n            score = (1.0 + capacity_scaling) * penalty_component\n    \n    return np.where(can_fit, score, -np.inf)",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response1.txt_stdout.txt",
    "code_path": "problem_iter5_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritize bins using fixed thresholds and non-smooth Best/Worst Fit for large/small items.\n    \n    Fixed 0.5 threshold classifies items. Best Fit (step) for large, Worst Fit (step) for small.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([])\n    \n    can_fit = bins_remain_cap >= item\n    is_large = item > 0.5  # Fixed threshold for item classification\n    \n    # Initialize priority scores\n    priority = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if not np.any(can_fit):\n        return priority\n    \n    if is_large:\n        # Best Fit: prioritize bins with minimal leftover space (step function)\n        leftover = np.where(can_fit, bins_remain_cap - item, np.inf)\n        min_leftover = leftover.min()\n        best_fit = (leftover == min_leftover) & can_fit\n        priority[best_fit] = 1.0\n    else:\n        # Worst Fit: prioritize bins with maximum remaining capacity (step function)\n        remaining = np.where(can_fit, bins_remain_cap, -np.inf)\n        max_remaining = remaining.max()\n        best_worst = (remaining == max_remaining) & can_fit\n        priority[best_worst] = 1.0\n    \n    return priority",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response4.txt_stdout.txt",
    "code_path": "problem_iter6_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores for bins using a thresholded categorical heuristic.\n    \n    Bins that can fit the item are scored in discrete tiers based on normalized\n    remaining space after placement. Thresholds are anchored to item size for\n    contextual adaptability without instability from continuous scoring.\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    # Contextual thresholds based on item size\n    tight_threshold = 0.15 * item\n    moderate_threshold = 0.4 * item\n\n    # Identify viable bins and compute post-placement space\n    can_fit = bins_remain_cap >= item\n    remaining_after = bins_remain_cap - item\n\n    # Initialize scores with -inf for invalid bins\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    # Tier 1: Tight fit (minimal leftover space)\n    tight_mask = can_fit & (remaining_after <= tight_threshold)\n    # Tier 2: Moderate fit (small leftover space)\n    mod_mask = can_fit & (remaining_after > tight_threshold) & (remaining_after <= moderate_threshold)\n    # Tier 3: Loose fit (significant leftover space)\n    loose_mask = can_fit & (remaining_after > moderate_threshold)\n\n    # Assign discrete priority scores\n    scores[tight_mask] = 3\n    scores[mod_mask] = 2\n    scores[loose_mask] = 1\n\n    return scores",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.15835660151576,
    "SLOC": 13.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response0.txt_stdout.txt",
    "code_path": "problem_iter7_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, cannot_fit_score: float = -686154.5177323952) -> np.ndarray:\n    \"\"\"Returns priority scores for bins using a Best Fit heuristic.\n    \n    Bins that can fit the item are scored by how little space remains after placement.\n    Bins that cannot fit the item receive a very low priority score.\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n        cannot_fit_score: The score assigned to bins that cannot fit the item.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    fit_scores = item - bins_remain_cap\n    return np.where(can_fit, fit_scores, cannot_fit_score)",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 4.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response3.txt_stdout.txt",
    "code_path": "problem_iter9_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores using dynamic threshold blending with exponential penalties.\n    \n    Blends residual minimization with fragmentation avoidance via:\n    1. Feasibility masking (same as v1)\n    2. Smooth exponential penalty for residuals below dynamic median threshold\n    3. Continuous scoring combining both objectives\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    r = bins_remain_cap - item  # Residual capacity after placement\n    \n    # Dynamic threshold based on current distribution of remaining capacities\n    T = np.clip(np.median(bins_remain_cap), 1e-8, None)  # Avoid division by zero\n    \n    # Smooth penalty term: exponential decay penalizes small residuals relative to T\n    feasible_r = np.where(can_fit, r, np.inf)  # For safe statistics\n    penalty = np.exp(-feasible_r / T)\n    \n    # Continuous blended objective: combine residual minimization with fragmentation avoidance\n    blended_score = -r - penalty  # Primary term: residual minimization; Secondary: penalty shaping\n    \n    # Apply feasibility mask with negative infinity for invalid bins\n    return np.where(can_fit, blended_score, -np.inf)",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response0.txt_stdout.txt",
    "code_path": "problem_iter11_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive priority combining residual minimization and fragmentation avoidance.\n    \n    Uses dynamic alpha blending based on item-to-threshold ratio to balance objectives:\n    large items prioritize fit quality (-r), small items prioritize space cohesion (-exp(-r/T)).\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    r = bins_remain_cap - item  # Residual capacity after placement\n    \n    # Dynamic threshold using median remaining capacity\n    T = np.clip(np.median(bins_remain_cap), 1e-8, None)\n    \n    # Adaptive blending factor based on item size relative to T (sigmoidal response)\n    alpha = 1.0 - np.exp(-item / T)  # Approaches 1 for large items, 0 for small\n    \n    # Smooth penalty term for fragmentation avoidance\n    feasible_r = np.where(can_fit, r, np.inf)  # Mask invalid bins for statistics\n    penalty_term = np.exp(-feasible_r / T)  # Exponential decay penalizes small residuals\n    \n    # Convex combination of residual minimization and penalty shaping\n    blended_score = alpha * (-r) + (1 - alpha) * (-penalty_term)\n    \n    # Enforce feasibility with negative infinity mask\n    return np.where(can_fit, blended_score, -np.inf)",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response2.txt_stdout.txt",
    "code_path": "problem_iter12_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    feasible = bins_remain_cap >= item\n    if not feasible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    r = bins_remain_cap - item\n    feasible_r = r[feasible]\n    \n    mu = feasible_r.mean()\n    std = feasible_r.std()\n    eps = 1e-6\n    tau = mu + std + eps  # Dynamic scale combining central tendency and spread\n    \n    # Smooth penalty term to avoid extreme fragmentation\n    penalty_term = np.exp(-r / tau)\n    \n    # Adaptive weight balancing residual minimization and fragmentation avoidance\n    beta = item / tau\n    \n    # Multi-objective score blending\n    residual_objective = -r\n    fragmentation_objective = -beta * penalty_term\n    score = residual_objective + fragmentation_objective\n    \n    return np.where(feasible, score, -np.inf).astype(np.float64)",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 17.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response2.txt_stdout.txt",
    "code_path": "problem_iter15_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    # Compute dynamic bin statistics\n    mu = np.mean(bins_remain_cap)\n    sigma = np.std(bins_remain_cap)\n    epsilon = 1e-8  # Small epsilon to prevent division by zero\n\n    # Adaptive penalty factor based on average remaining capacity\n    penalty_factor = 1.0 / (mu + epsilon)\n    \n    # Threshold for fragmentation avoidance (1\u03c3 below mean)\n    threshold = mu - sigma\n    \n    # Residual in remaining capacity after placing the item\n    residual = bins_remain_cap - item\n    feasible = residual >= 0\n    \n    # Compute feasible bin scores\n    # Term1: Exponential decay for residual minimization\n    term1 = np.exp(- residual * penalty_factor)\n    \n    # Term2: Fragmentation penalty (exponential decay on threshold deficit)\n    delta = np.clip(threshold - residual, a_min=0.0, a_max=None)\n    term2 = np.exp(- delta * penalty_factor)\n    \n    feasible_scores = term1 * term2\n    \n    # Compute infeasible bin scores (soft penalty)\n    deficit = item - bins_remain_cap\n    infeasible_scores = np.exp(- deficit * penalty_factor * 5.0)  # Stronger penalty for overflow\n    \n    # Combine using convex-like combination (soft masking)\n    scores = np.where(feasible, feasible_scores, infeasible_scores)\n    \n    return scores",
    "response_id": 2,
    "tryHS": false,
    "obj": 5.494615077782224,
    "SLOC": 13.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response0.txt_stdout.txt",
    "code_path": "problem_iter17_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive priority using mean+std thresholding to blend residual minimization and fragmentation penalties.\"\"\"\n    can_fit = bins_remain_cap >= item\n    r = bins_remain_cap - item\n    \n    # Dynamic threshold using mean+std of feasible bins (or all bins if none feasible)\n    feasible_caps = bins_remain_cap[can_fit]\n    if feasible_caps.size == 0:\n        mu, sigma = np.mean(bins_remain_cap), np.std(bins_remain_cap)\n    else:\n        mu, sigma = np.mean(feasible_caps), np.std(feasible_caps)\n    T = np.clip(mu + sigma, 1e-8, None)\n    \n    # Adaptive alpha blending based on item-to-threshold ratio\n    alpha = 1.0 - np.exp(-item / T)\n    \n    # Fragmentation penalty using exponential decay\n    feasible_r = np.where(can_fit, r, np.inf)\n    penalty_term = np.exp(-feasible_r / T)\n    \n    # Multi-objective blend with feasibility enforcement\n    blended_score = alpha * (-r) + (1 - alpha) * (-penalty_term)\n    return np.where(can_fit, blended_score, -np.inf)",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 22.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter18_response0.txt_stdout.txt",
    "code_path": "problem_iter18_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores for bins using a distribution-aware heuristic with smooth exponential scoring.\n    \n    Scores are computed by blending residual minimization and fragmentation penalties via adaptive weights,\n    dynamically adjusted to the current bin capacity distribution. Uses feasibility masks to exclude invalid bins.\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    if not np.any(can_fit):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    mu = np.mean(bins_remain_cap)\n    sigma = np.std(bins_remain_cap)\n    epsilon = 1e-9\n    \n    # Dynamic threshold for fragmentation awareness\n    threshold = mu - sigma\n    \n    # Residual computation for feasible bins\n    r = bins_remain_cap - item\n    \n    # Smooth exponential residual score (smaller r \u2192 higher score)\n    residual_score = np.exp(-r / (mu + sigma + epsilon))\n    \n    # Smooth threshold-crossing score using logistic transition\n    threshold_diff = r - threshold\n    fragment_score = 1.0 / (1.0 + np.exp(threshold_diff / (sigma + epsilon)))\n    \n    # Adaptive weights based on coefficient of variation\n    cv = sigma / (mu + epsilon)\n    weight_residual = 1.0 - np.tanh(cv)  # Dominates when distribution is tight\n    weight_fragment = np.tanh(cv)        # Dominates when distribution is spread out\n    \n    # Combine components with adaptive weights\n    combined_score = weight_residual * residual_score + weight_fragment * fragment_score\n    \n    # Apply feasibility mask\n    return np.where(can_fit, combined_score, -np.inf)",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 19.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response0.txt_stdout.txt",
    "code_path": "problem_iter20_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Priority heuristic combining dynamic bin statistics with item-relative rewards.\n    \n    Uses adaptive thresholds (mean+std) and smooth penalties from v0, augmented with\n    item-aware exponential rewards (v1-inspired) to prioritize fits where remaining space\n    is small relative to item size. Balances residual minimization, fragmentation\n    avoidance, and contextual adaptivity through multi-objective blending.\n    \"\"\"\n    feasible = bins_remain_cap >= item\n    if not feasible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    r = bins_remain_cap - item\n    feasible_r = r[feasible]\n    \n    mu = feasible_r.mean()\n    std = feasible_r.std()\n    eps = 1e-9\n    tau = mu + std + eps\n    \n    # Core components from v0\n    beta = item / tau\n    residual_objective = -r\n    penalty_term = np.exp(-r / tau)\n    fragmentation_objective = -beta * penalty_term\n    \n    # Item-relative reward (smooth v1-inspired component)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        item_rel_scale = r / (item + eps)\n    item_relative_reward = np.exp(-item_rel_scale)  # High reward for small r/item ratios\n    \n    # Multi-objective combination\n    score = residual_objective + fragmentation_objective + item_relative_reward\n    \n    return np.where(feasible, score, -np.inf).astype(np.float64)",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response0.txt_stdout.txt",
    "code_path": "problem_iter23_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines residual minimization and density-based rewards with adaptive weights.\n    \n    Uses z-score density of residuals and normalized fit scores, blending objectives\n    via logistic-weighted sum calibrated to bin capacity distribution statistics.\n    \"\"\"\n    EPS = 1e-8\n    feasible = bins_remain_cap >= item\n    \n    if not np.any(feasible):\n        return -np.inf * np.ones_like(bins_remain_cap)\n    \n    # Residual and basic fit score\n    r = bins_remain_cap - item\n    fit_score = (-r) / (item + EPS)  # Item-relative normalization\n    \n    # Statistical context\n    mu, sigma = np.mean(bins_remain_cap), np.std(bins_remain_cap)\n    z = (r - mu) / (sigma + EPS)\n    \n    # Density-based reward (smooth exponential scoring)\n    density_reward = np.exp(-0.5 * z**2)\n    \n    # Adaptive weight via logistic coefficient of variation\n    cv = sigma / (mu + EPS)\n    weight = 1 / (1 + np.exp(-cv))  # Dynamic balance factor\n    \n    # Multi-objective blend and feasibility mask\n    combined = weight * fit_score + (1 - weight) * density_reward\n    return np.where(feasible, combined, -np.inf)",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 20.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter26_response0.txt_stdout.txt",
    "code_path": "problem_iter26_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines item-relative exponential fit rewards with z-score density rewards, adaptively weighted by coefficient of variation.\n    \n    Key design: Smooth exponential scoring for item-normalized residuals (tighter fits get higher scores) blended with \n    statistical density rewards (preferring bins near current residual mean), dynamically balanced via logistic-weighted \n    coefficient of variation to handle varying bin distributions.\n    \"\"\"\n    EPS = 1e-8\n    feasible = bins_remain_cap >= item\n    \n    if not np.any(feasible):\n        return -np.inf * np.ones_like(bins_remain_cap)\n    \n    # Item-relative fit reward: exponential decay penalty for larger leftover/item ratios\n    r = bins_remain_cap - item\n    fit_component = np.exp(-r / (item + EPS))  # Tighter fits (small r/item) yield higher scores\n    \n    # Statistical density reward: Gaussian-like scoring for residuals near current mean\n    mu, sigma = np.mean(bins_remain_cap), np.std(bins_remain_cap)\n    z = (r - mu) / (sigma + EPS)\n    density_component = np.exp(-0.5 * z**2)  # Peaks when residual matches current mean\n    \n    # Adaptive weight via logistic coefficient of variation\n    cv = sigma / (mu + EPS)\n    weight = 1 / (1 + np.exp(-cv))  # Higher weight on fit_component when bin capacities vary more\n    \n    # Combine objectives and enforce feasibility\n    combined = weight * fit_component + (1 - weight) * density_component\n    return np.where(feasible, combined, -np.inf)",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 20.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter27_response4.txt_stdout.txt",
    "code_path": "problem_iter27_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority scores using adaptive statistical blending of residual and ratio objectives.\n    \n    Smooth exponential components weighted by coefficient of variation prioritize:\n    - Bins with high item/capacity ratio (density-aware packing)\n    - Residual minimization scaled by capacity distribution statistics\n    - Contextual normalization via mean/std of bin capacities\n    \n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: Array of remaining capacities for each bin.\n    \n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    \n    # Statistical context of current bin capacities\n    mean_cap = np.mean(bins_remain_cap)\n    std_cap = np.std(bins_remain_cap)\n    epsilon = 1e-6\n    \n    # Adaptive weight via logistic coefficient of variation scaling\n    cv = std_cap / (mean_cap + epsilon)\n    alpha = 1.0 / (1.0 + np.exp(-cv * 10))  # Dynamic ratio/residual weighting\n    \n    # Component calculations\n    residual = bins_remain_cap - item\n    ratio = item / (bins_remain_cap + epsilon)\n    \n    # Smooth exponential residual component scaled by capacity statistics\n    tau_residual = mean_cap + std_cap + epsilon\n    component_residual = np.exp(-residual / tau_residual)\n    \n    # Density-aware ratio component\n    component_ratio = ratio\n    \n    # Multi-objective blending with adaptive weights\n    composite = alpha * component_ratio + (1 - alpha) * component_residual\n    \n    # Enforce feasibility constraint with -inf masking\n    return np.where(can_fit, composite, -np.inf)",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter29_response0.txt_stdout.txt",
    "code_path": "problem_iter29_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Dynamic median-based blending with z-score density rewards for residual clustering.\n    \n    Combines residual minimization & fragmentation avoidance (v0) with secondary\n    density rewards using z-score proximity to mean residual. Adaptive weight scaling\n    amplifies clustering incentives when bin capacity variance is high.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    r = bins_remain_cap - item  # Residual capacity after placement\n    \n    # Feasible residual mask for numerical stability\n    feasible_r = np.where(can_fit, r, np.inf)\n    \n    # Dynamic threshold based on median remaining capacity\n    T = np.clip(np.median(bins_remain_cap), 1e-8, None)\n    \n    # Primary objective: residual minimization with exponential fragmentation penalty\n    penalty_primary = np.exp(-feasible_r / T)  # Higher penalty for small residuals\n    blended_primary = -r - penalty_primary     # Balance residual & penalty\n    \n    # Secondary objective: density rewards via z-score proximity to mean residual\n    feasible_mask = can_fit & (feasible_r != np.inf)\n    feasible_count = np.sum(feasible_mask)\n    \n    if feasible_count > 0:\n        mu_r = np.mean(feasible_r[feasible_mask])\n        sigma_r = np.std(feasible_r[feasible_mask]) + 1e-8\n        \n        # Z-score normalized by residual distribution statistics\n        z_scores = (r - mu_r) / sigma_r\n        secondary_reward = np.exp(-np.abs(z_scores))  # Reward proximity to mean\n    else:\n        secondary_reward = np.zeros_like(r)\n    \n    # Adaptive scaling: amplify secondary reward when bin capacity variance is high\n    bin_cv = np.std(bins_remain_cap) / (T + 1e-8)  # Coefficient of variation\n    adaptive_weight = 1.0 + bin_cv  # Amplify clustering incentives\n    \n    # Final score: primary objective + scaled density reward\n    blended_total = blended_primary + adaptive_weight * secondary_reward\n    \n    # Enforce feasibility constraints with -inf mask\n    return np.where(can_fit, blended_total, -np.inf)",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 21.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  }
]