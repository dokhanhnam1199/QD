{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Dynamic median-based blending with z-score density rewards for residual clustering.\n    \n    Combines residual minimization & fragmentation avoidance (v0) with secondary\n    density rewards using z-score proximity to mean residual. Adaptive weight scaling\n    amplifies clustering incentives when bin capacity variance is high.\n    \"\"\"\n    can_fit = bins_remain_cap >= item\n    r = bins_remain_cap - item  # Residual capacity after placement\n    \n    # Feasible residual mask for numerical stability\n    feasible_r = np.where(can_fit, r, np.inf)\n    \n    # Dynamic threshold based on median remaining capacity\n    T = np.clip(np.median(bins_remain_cap), 1e-8, None)\n    \n    # Primary objective: residual minimization with exponential fragmentation penalty\n    penalty_primary = np.exp(-feasible_r / T)  # Higher penalty for small residuals\n    blended_primary = -r - penalty_primary     # Balance residual & penalty\n    \n    # Secondary objective: density rewards via z-score proximity to mean residual\n    feasible_mask = can_fit & (feasible_r != np.inf)\n    feasible_count = np.sum(feasible_mask)\n    \n    if feasible_count > 0:\n        mu_r = np.mean(feasible_r[feasible_mask])\n        sigma_r = np.std(feasible_r[feasible_mask]) + 1e-8\n        \n        # Z-score normalized by residual distribution statistics\n        z_scores = (r - mu_r) / sigma_r\n        secondary_reward = np.exp(-np.abs(z_scores))  # Reward proximity to mean\n    else:\n        secondary_reward = np.zeros_like(r)\n    \n    # Adaptive scaling: amplify secondary reward when bin capacity variance is high\n    bin_cv = np.std(bins_remain_cap) / (T + 1e-8)  # Coefficient of variation\n    adaptive_weight = 1.0 + bin_cv  # Amplify clustering incentives\n    \n    # Final score: primary objective + scaled density reward\n    blended_total = blended_primary + adaptive_weight * secondary_reward\n    \n    # Enforce feasibility constraints with -inf mask\n    return np.where(can_fit, blended_total, -np.inf)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing **1st vs 7th**, the top heuristic uses adaptive logistic-weighted coefficient of variation to balance fit and density rewards, while the 7th relies on fixed thresholds and non-smooth step functions. Smooth exponential scoring and dynamic statistical blending dominate in better heuristics.  \n\n**2nd vs 13th**: The 2nd introduces adaptive density rewards via z-scores and median-based thresholds, whereas the 13th-20th (identical) use static penalty factors and threshold clipping without contextual adaptability.  \n\n**3rd vs 4th**: The 3rd balances residual minimization and fragmentation penalties via tanh-weighted coefficient of variation, while the 4th combines item-relative exponential rewards with less dynamic statistical normalization.  \n\n**5th vs 10th**: Mid-ranked heuristics like the 5th/6th use item-size classification (small/large) with exponential scoring, while the 10th/11th apply fixed categorical tiers (tight/moderate/loose), which are less responsive to distribution shifts.  \n\n**12th (worst)** simply returns zeros, ignoring feasibility and context entirely.  \n\nOverall: Top heuristics excel through **adaptive statistical blending**, **smooth exponential scoring**, and **multi-objective prioritization** (e.g., residual minimization + density rewards), while lower-ranked ones suffer from fixed thresholds, non-smooth penalties, or lack of contextual adaptivity.\n- \nKeywords: Adaptive weights (CV-based), smooth exponential/logistic scoring, multi-objective blending, feasibility masking  \nAdvice: Prioritize context-aware adaptive weights over thresholds, blend residual/density objectives with smooth scoring, use logistic transitions for continuity, enforce feasibility strictly via -\u221e masks.  \nAvoid: Fixed thresholds, step functions, secondary tie-breakers, soft infeasibility handling  \nExplanation: Adaptive weights (vs. thresholds) improve context sensitivity without abrupt transitions. Smooth scoring ensures mathematical continuity for stable gradients. Multi-objective blending balances residual minimization and density efficiency. Strict feasibility masking eliminates invalid solutions, enhancing robustness.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}