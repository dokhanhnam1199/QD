{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Priority heuristic combining dynamic bin statistics with item-relative rewards.\n    \n    Uses adaptive thresholds (mean+std) and smooth penalties from v0, augmented with\n    item-aware exponential rewards (v1-inspired) to prioritize fits where remaining space\n    is small relative to item size. Balances residual minimization, fragmentation\n    avoidance, and contextual adaptivity through multi-objective blending.\n    \"\"\"\n    feasible = bins_remain_cap >= item\n    if not feasible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    r = bins_remain_cap - item\n    feasible_r = r[feasible]\n    \n    mu = feasible_r.mean()\n    std = feasible_r.std()\n    eps = 1e-9\n    tau = mu + std + eps\n    \n    # Core components from v0\n    beta = item / tau\n    residual_objective = -r\n    penalty_term = np.exp(-r / tau)\n    fragmentation_objective = -beta * penalty_term\n    \n    # Item-relative reward (smooth v1-inspired component)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        item_rel_scale = r / (item + eps)\n    item_relative_reward = np.exp(-item_rel_scale)  # High reward for small r/item ratios\n    \n    # Multi-objective combination\n    score = residual_objective + fragmentation_objective + item_relative_reward\n    \n    return np.where(feasible, score, -np.inf).astype(np.float64)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    # Compute dynamic bin statistics\n    mu = np.mean(bins_remain_cap)\n    sigma = np.std(bins_remain_cap)\n    epsilon = 1e-8  # Small epsilon to prevent division by zero\n\n    # Adaptive penalty factor based on average remaining capacity\n    penalty_factor = 1.0 / (mu + epsilon)\n    \n    # Threshold for fragmentation avoidance (1\u03c3 below mean)\n    threshold = mu - sigma\n    \n    # Residual in remaining capacity after placing the item\n    residual = bins_remain_cap - item\n    feasible = residual >= 0\n    \n    # Compute feasible bin scores\n    # Term1: Exponential decay for residual minimization\n    term1 = np.exp(- residual * penalty_factor)\n    \n    # Term2: Fragmentation penalty (exponential decay on threshold deficit)\n    delta = np.clip(threshold - residual, a_min=0.0, a_max=None)\n    term2 = np.exp(- delta * penalty_factor)\n    \n    feasible_scores = term1 * term2\n    \n    # Compute infeasible bin scores (soft penalty)\n    deficit = item - bins_remain_cap\n    infeasible_scores = np.exp(- deficit * penalty_factor * 5.0)  # Stronger penalty for overflow\n    \n    # Combine using convex-like combination (soft masking)\n    scores = np.where(feasible, feasible_scores, infeasible_scores)\n    \n    return scores\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see that top heuristics use dynamic statistical adaptation (mean, std, median) and multi-objective blending (residual minimization + fragmentation penalties), while the worst ones apply rigid soft penalties without contextual awareness. (2nd) vs (19th) show that adaptive thresholds (mean+std) and feasibility masks outperform static thresholds and uniform scoring. (3rd) vs (18th) reveal that explicit feasibility enforcement (-\u221e masking) and exponential decay penalties dominate over naive zero-scoring. (6th) vs (17th) highlight that item-relative rewards (e.g., r/item scaling) and multi-component blending (residual + penalty + reward) outperform single-objective approaches. (11th) vs (16th) demonstrate that categorical tiering (tight/moderate/loose fits) with discrete scores underperforms smooth exponential scoring. Overall: Superior heuristics dynamically adapt weights to bin/item statistics, combine residual minimization with fragmentation avoidance via smooth functions, and rigorously enforce feasibility. Inferior ones rely on fixed thresholds, lack multi-objective balance, or fail to penalize infeasibility effectively.\n- \nKeywords: Dynamic statistical adaptation, multi-objective blending, smooth scoring, feasibility masking  \nAdvice: Integrate context-aware weights combining residual minimization and fragmentation penalties, using logistic/exponential scoring calibrated to real-time bin/item distributions. Prioritize adaptivity via median/\u03c3-driven thresholds and -\u221e infeasibility masking.  \nAvoid: Fixed/static thresholds, single-objective prioritization, stepwise scoring, soft constraint handling.  \nExplanation: Dynamic thresholds (e.g., \u03bc\u00b1\u03c3) and continuous scoring ensure mathematical robustness, while contextual multi-objective blending prevents brittleness in varying distributions. Rigorous masking maintains feasibility without heuristic fallbacks.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}