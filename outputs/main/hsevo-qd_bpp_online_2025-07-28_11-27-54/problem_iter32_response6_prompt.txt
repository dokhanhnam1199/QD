{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritize bins using fixed thresholds and non-smooth Best/Worst Fit for large/small items.\n    \n    Fixed 0.5 threshold classifies items. Best Fit (step) for large, Worst Fit (step) for small.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([])\n    \n    can_fit = bins_remain_cap >= item\n    is_large = item > 0.5  # Fixed threshold for item classification\n    \n    # Initialize priority scores\n    priority = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if not np.any(can_fit):\n        return priority\n    \n    if is_large:\n        # Best Fit: prioritize bins with minimal leftover space (step function)\n        leftover = np.where(can_fit, bins_remain_cap - item, np.inf)\n        min_leftover = leftover.min()\n        best_fit = (leftover == min_leftover) & can_fit\n        priority[best_fit] = 1.0\n    else:\n        # Worst Fit: prioritize bins with maximum remaining capacity (step function)\n        remaining = np.where(can_fit, bins_remain_cap, -np.inf)\n        max_remaining = remaining.max()\n        best_worst = (remaining == max_remaining) & can_fit\n        priority[best_worst] = 1.0\n    \n    return priority\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    # Compute dynamic bin statistics\n    mu = np.mean(bins_remain_cap)\n    sigma = np.std(bins_remain_cap)\n    epsilon = 1e-8  # Small epsilon to prevent division by zero\n\n    # Adaptive penalty factor based on average remaining capacity\n    penalty_factor = 1.0 / (mu + epsilon)\n    \n    # Threshold for fragmentation avoidance (1\u03c3 below mean)\n    threshold = mu - sigma\n    \n    # Residual in remaining capacity after placing the item\n    residual = bins_remain_cap - item\n    feasible = residual >= 0\n    \n    # Compute feasible bin scores\n    # Term1: Exponential decay for residual minimization\n    term1 = np.exp(- residual * penalty_factor)\n    \n    # Term2: Fragmentation penalty (exponential decay on threshold deficit)\n    delta = np.clip(threshold - residual, a_min=0.0, a_max=None)\n    term2 = np.exp(- delta * penalty_factor)\n    \n    feasible_scores = term1 * term2\n    \n    # Compute infeasible bin scores (soft penalty)\n    deficit = item - bins_remain_cap\n    infeasible_scores = np.exp(- deficit * penalty_factor * 5.0)  # Stronger penalty for overflow\n    \n    # Combine using convex-like combination (soft masking)\n    scores = np.where(feasible, feasible_scores, infeasible_scores)\n    \n    return scores\n\n### Analyze & experience\n- Comparing **1st vs 7th**, the top heuristic uses adaptive logistic-weighted coefficient of variation to balance fit and density rewards, while the 7th relies on fixed thresholds and non-smooth step functions. Smooth exponential scoring and dynamic statistical blending dominate in better heuristics.  \n\n**2nd vs 13th**: The 2nd introduces adaptive density rewards via z-scores and median-based thresholds, whereas the 13th-20th (identical) use static penalty factors and threshold clipping without contextual adaptability.  \n\n**3rd vs 4th**: The 3rd balances residual minimization and fragmentation penalties via tanh-weighted coefficient of variation, while the 4th combines item-relative exponential rewards with less dynamic statistical normalization.  \n\n**5th vs 10th**: Mid-ranked heuristics like the 5th/6th use item-size classification (small/large) with exponential scoring, while the 10th/11th apply fixed categorical tiers (tight/moderate/loose), which are less responsive to distribution shifts.  \n\n**12th (worst)** simply returns zeros, ignoring feasibility and context entirely.  \n\nOverall: Top heuristics excel through **adaptive statistical blending**, **smooth exponential scoring**, and **multi-objective prioritization** (e.g., residual minimization + density rewards), while lower-ranked ones suffer from fixed thresholds, non-smooth penalties, or lack of contextual adaptivity.\n- \nKeywords: Adaptive weights (CV-based), smooth exponential/logistic scoring, multi-objective blending, feasibility masking  \nAdvice: Prioritize context-aware adaptive weights over thresholds, blend residual/density objectives with smooth scoring, use logistic transitions for continuity, enforce feasibility strictly via -\u221e masks.  \nAvoid: Fixed thresholds, step functions, secondary tie-breakers, soft infeasibility handling  \nExplanation: Adaptive weights (vs. thresholds) improve context sensitivity without abrupt transitions. Smooth scoring ensures mathematical continuity for stable gradients. Multi-objective blending balances residual minimization and density efficiency. Strict feasibility masking eliminates invalid solutions, enhancing robustness.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}