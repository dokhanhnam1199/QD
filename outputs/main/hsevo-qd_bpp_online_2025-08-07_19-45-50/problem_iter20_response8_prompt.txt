{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n# Priority function combining deterministic waste and optional epsilon\u2011greedy exploration\n                random_state: Optional[int] = None) -> np.ndarray:\n    \"\"\"Priority = -waste for feasible bins; optional epsilon\u2011greedy random scores.\"\"\"\n    feasible = bins_remain_cap >= item\n    if epsilon > 0.0:\n        rng = np.random.default_rng(random_state)\n        if rng.random() < epsilon:\n            rand_scores = rng.random(bins_remain_cap.shape[0])\n            return np.where(feasible, rand_scores, -np.inf)\n    waste = bins_remain_cap - item\n    return np.where(feasible, -waste, -np.inf)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n                epsilon: float = 0.0, rng: np.random.Generator = None) -> np.ndarray:\n    \"\"\"Softmax fit (temp\u202ftau) with epsilon\u2011greedy random exploration.\"\"\"\n    # Combine stable softmax scoring of waste with occasional random scoring.\n    if rng is None:\n        rng = np.random.default_rng()\n    residual = bins_remain_cap - item\n    mask = residual >= 0\n    if not np.any(mask):\n        return np.zeros_like(bins_remain_cap, dtype=float)\n    if epsilon > 0 and rng.random() < epsilon:\n        rand_scores = rng.random(bins_remain_cap.shape[0])\n        rand_scores[~mask] = -np.inf\n        max_rand = rand_scores[mask].max()\n        exp_rand = np.exp(rand_scores - max_rand)\n        exp_rand[~mask] = 0.0\n        total_rand = exp_rand.sum()\n        return exp_rand / total_rand if total_rand > 0 else np.zeros_like(bins_remain_cap, dtype=float)\n\n### Analyze & experience\n- - **(best) #1 vs (worst) #20:** #1 uses a single deterministic score (\u2011waste) with an optional \u03b5\u2011greedy random fallback, returning raw priorities and \u2011\u221e for infeasible bins. The docstring is concise. #20 layers a logistic best\u2011fit transform, mixes random scores, applies a stable softmax, and returns probabilities. Its complexity, extra hyper\u2011parameters (k, \u03c4), and heavier numerical work make it slower and harder to reason about.  \n- **(second\u2011best) #2 vs (second\u2011worst) #19:** #2 offers temperature\u2011scaled softmax over (\u2011waste), a clear \u03b5\u2011weighted mixture of deterministic and random components, and robust handling of empty feasible sets. #19 uses \u03b5 only as a hard switch between random logits and deterministic softmax, lacking the smooth blending of #2 and providing less control over exploration\u2011exploitation balance.  \n- **#1 vs #2 (1st vs 2nd):** #1 returns raw scores, ideal for greedy selection; #2 returns a probability distribution, enabling stochastic sampling. #2 adds temperature for tunable \u201csoftness\u201d and mixes \u03b5 in a weighted fashion, but incurs extra computation. Both mask infeasible bins correctly; #2 includes a stable softmax (max\u2011shift) for numeric safety.  \n- **#3 vs #4 (3rd vs 4th):** #3 mirrors #1 with a seed\u2011based RNG and simple \u03b5\u2011greedy random scores. #4 introduces a logistic slack\u2011ratio score and blends it with random scores before a softmax, aiming for a more nuanced fit. The logistic transformation adds non\u2011linearity that can help in tight packing but also raises complexity and parameter sensitivity (k).  \n- **#19 vs #20 (second\u2011worst vs worst):** Both return probabilities via softmax, but #19\u2019s random branch is a pure softmax of random logits, whereas #20 mixes deterministic logistic scores with random scores before softmax. #20\u2019s extra logistic layer and higher\u2011k sigmoid can cause overly aggressive bias toward near\u2011full bins, reducing robustness. #19 remains marginally simpler.  \n\n**Overall:** Simpler deterministic waste\u2011based scoring with optional \u03b5\u2011greedy randomness (as in #1) provides clarity, speed, and easy reproducibility. When probabilistic selection is needed, a temperature\u2011scaled softmax with weighted \u03b5\u2011mixing (#2) offers controlled exploration without unnecessary logistic transforms.\n- \n- **Keywords:** multi\u2011objective scoring, adaptive perturbation, penalty\u2011based feasibility, rank (sic)\u2011based weighting.  \n- **Advice:** blend waste, cost, risk into a composite score; rank scores and assign probabilities decreasing with rank (e.g., power\u2011law); inject decaying random perturbations; replace binary masks with penalty terms for infeasibility; let scaling adapt from outcomes; modularize components to swap scoring or perturbation.  \n- **Avoid:** pure deterministic pipelines, fixed random seeds, hard infeasibility masking, single\u2011objective focus, static probability scaling, excessive reliance on terse code, heavy documentation or type annotation.  \n- **Explanation:** these principles create a flexible, stable heuristic that explores broadly and adapts to problem specifics, without rigid coding or deterministic limits.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}