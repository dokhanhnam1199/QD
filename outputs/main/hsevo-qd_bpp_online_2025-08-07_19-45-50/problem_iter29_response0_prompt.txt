{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns softmax-based priority for placing an item into each bin.\"\"\"\n    feasible = bins_remain_cap >= item\n    if not np.any(feasible):\n        return np.zeros_like(bins_remain_cap, dtype=float)\n    caps = bins_remain_cap.astype(float)\n    residual = caps - item\n    raw_scores = np.where(feasible, -residual, -np.inf)\n    max_score = raw_scores.max()\n    exp_scores = np.exp(raw_scores - max_score)\n    sum_exp = exp_scores.sum()\n    priorities = exp_scores / sum_exp if sum_exp > 0 else np.zeros_like(exp_scores)\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Exact-fit-first priority with optional epsilon-greedy exploration.\"\"\"\n    # Feasibility mask and deterministic priority (negative waste)\n    feasible = bins_remain_cap >= item\n    deterministic = np.where(feasible, -(bins_remain_cap - item), -np.inf)\n    # With probability epsilon, use random scores for feasible bins\n    if np.random.rand() < epsilon:\n        random_scores = np.random.rand(bins := bins_remain_cap.shape[0])\n        return np.where(feasible, random_scores, -np.inf)\n    return deterministic\n\n### Analyze & experience\n- - **Comparing (best) vs (worst), we see** that the best heuristic (Heuristics\u202f1) follows a clean design: clear docstring, proper feasibility mask, lazy RNG initialization, \u03b5\u2011greedy fallback, temperature\u2011scaled stable softmax, and a guaranteed probability vector. The worst heuristic (Heuristics\u202f20) overloads the function with rank weighting, penalty, risk, and Gaussian noise, yet it *fails to exclude infeasible bins* (they receive positive scores) and mixes many hyper\u2011parameters that make the method brittle and prone to selecting impossible placements.\n\n- **(Second best) vs (second worst), we see** that the second\u2011best (Heuristics\u202f2) is a minimalist, deterministic softmax on negative waste with numeric stability and correct handling of infeasibility. The second\u2011worst (Heuristics\u202f19) repeats the same flawed pattern as the worst: it assigns high scores to infeasible bins, adds unnecessary rank and noise terms, and provides no guarantee that the returned vector respects feasibility.\n\n- **Comparing (1st) vs (2nd), we see** that the top heuristic adds optional \u03b5\u2011greedy exploration and a temperature parameter, giving a controllable trade\u2011off between exploitation and exploration. The second heuristic is deterministic and lacks these knobs, making it less flexible but still correct and stable.\n\n- **(3rd) vs (4th), we see** both aim to combine \u03b5\u2011greedy and temperature\u2011scaled softmax. The third implementation (Heuristics\u202f3) correctly lazily creates a `Generator`, uses stable softmax, and normalizes probabilities. The fourth (Heuristics\u202f4) contains a critical bug: it sets `rng = 0` as a placeholder and never re\u2011initializes it, causing a runtime error when `rng.random()` is called.\n\n- **Comparing (second worst\u2011case) vs (worst), we see** that Heuristics\u202f19 and\u202f20 are practically identical in logic and suffer the same feasibility\u2011mask omission, excessive parameterisation, and noisy scoring. The only difference is a slightly different signature; both produce unreliable priorities.\n\n- **Overall:** Simplicity, correct feasibility handling, numerical stability (max\u2011subtraction before exponentiation), and optional but well\u2011implemented exploration (\u03b5\u2011greedy, temperature) consistently separate the higher\u2011ranked heuristics from the lower ones. Over\u2011engineering with ranks, penalties, or noise without rigorous masking introduces bugs and degrades performance.\n- \n- **Keywords:** numerical stability, infeasibility masking, \u03b5\u2011greedy, temperature scaling, softmax, RNG control, clear docs, vectorized.  \n- **Advice:** design with optional stochasticity, stable softmax, early infeasibility mask, normalize probabilities, provide type hints, enable RNG injection, document defaults.  \n- **Avoid:** deterministic hardcoding, unnecessary softmax when not sampling, duplicate logic, undefined names, uninitialized RNG, over\u2011complex transformations, magic constants.  \n- **Explanation:** These practices ensure robust, exploration\u2011friendly heuristics that remain numerically stable and easy to maintain.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}