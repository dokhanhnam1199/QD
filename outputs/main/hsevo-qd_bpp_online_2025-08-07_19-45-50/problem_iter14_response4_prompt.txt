{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Temperature-scaled softmax with optional epsilon-greedy exploration; returns normalized priorities for feasible bins.\"\"\"\n    feasible = bins_remain_cap >= item\n    if not np.any(feasible):\n        return np.zeros_like(bins_remain_cap, dtype=float)\n    if rng is None:\n        rng = np.random.default_rng()\n    if rng.random() < epsilon:\n        probs = np.zeros_like(bins_remain_cap, dtype=float)\n        probs[feasible] = 1.0 / np.count_nonzero(feasible)\n        return probs\n    residual = bins_remain_cap.astype(float) - item\n    raw_scores = -residual / tau\n    raw_scores[~feasible] = -np.inf\n    max_score = np.max(raw_scores)\n    exp_scores = np.exp(raw_scores - max_score)\n    sum_exp = np.sum(exp_scores)\n    return exp_scores / sum_exp if sum_exp > 0 else np.zeros_like(exp_scores)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n                epsilon: float = 0.0, rng: np.random.Generator = None) -> np.ndarray:\n    \"\"\"Softmax fit (temp\u202ftau) with epsilon\u2011greedy random exploration.\"\"\"\n    # Combine stable softmax scoring of waste with occasional random scoring.\n    if rng is None:\n        rng = np.random.default_rng()\n    residual = bins_remain_cap - item\n    mask = residual >= 0\n    if not np.any(mask):\n        return np.zeros_like(bins_remain_cap, dtype=float)\n    if epsilon > 0 and rng.random() < epsilon:\n        rand_scores = rng.random(bins_remain_cap.shape[0])\n        rand_scores[~mask] = -np.inf\n        max_rand = rand_scores[mask].max()\n        exp_rand = np.exp(rand_scores - max_rand)\n        exp_rand[~mask] = 0.0\n        total_rand = exp_rand.sum()\n        return exp_rand / total_rand if total_rand > 0 else np.zeros_like(bins_remain_cap, dtype=float)\n\n### Analyze & experience\n- - **Comparing Heuristics\u202f1st vs\u202f20th, we see** that the 1st implements a full epsilon\u2011greedy softmax: it initializes a RNG only when needed, uses a stable exp shift, handles infeasible bins by returning zeros, and documents the behavior clearly. The 20th omits the core scoring branch when \u03b5=0, returns an all\u2011zero distribution, ignores the \u03c4 parameter, and has a terse docstring that fails to warn about the missing logic.\n\n- **Comparing Heuristics\u202f2nd vs\u202f19th, we see** the same pattern: the 2nd duplicates the robust design of the 1st, whereas the 19th repeats the same incomplete logic as the 20th, including the missing normal\u2011score branch and lack of proper RNG handling.\n\n- **Comparing Heuristics\u202f1st vs\u202f2nd, we see** no difference at all; both are identical copies of the best implementation.\n\n- **Comparing Heuristics\u202f3rd vs\u202f4th, we see** the 3rd is a copy of the 1st, while the 4th substitutes `np.random` for a passed RNG, slightly reduces flexibility, and uses a `temperature` parameter but still performs a stable softmax. The docstring is shorter, and the code mixes parameter names (`tau` vs `temperature`) inconsistently.\n\n- **Comparing Heuristics\u202f18th vs\u202f20th, we see** both share the same flaw: the normal scoring branch is missing; only the random\u2011exploration path is executed when \u03b5>0. For \u03b5=0 they return a zero vector, leaving the heuristic useless in the common case. The \u03c4 parameter is unused, and the implementation can silently produce NaNs if no feasible bins exist.\n\n**Overall:** Robustness hinges on consistent RNG usage, clear epsilon handling, a complete scoring branch, and stable numerical computation. The top heuristics satisfy these; the lowest ones either omit critical logic or expose bugs.\n- \n- **Keywords:** type safety, RNG flexibility, stable softmax, explicit\u202f\u03b5, infeasibility mask, full distribution, thorough docs  \n- **Advice:** inject a seeded RNG, compute softmax via log\u2011sum\u2011exp, add\u202f\u03b5 to scores, mask infeasible bins before normalizing, guarantee every branch returns a normalized probability vector, annotate types and docstrings  \n- **Avoid:** deterministic\u2011only heuristics, skipping softmax, optional\u202f\u03b5, missing docs, absent type hints, raw\u202f\u2011inf scores, duplicated logic, branches that drop probability mass, ambiguous RNG  \n- **Explanation:** these practices ensure numerical stability, reproducible stochastic exploration, safe handling of infeasible choices, and maintainable code that always yields a valid probability distribution.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}