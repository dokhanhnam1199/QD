{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n                epsilon: float = 0.0, rng: np.random.Generator = None) -> np.ndarray:\n    \"\"\"\n    Softmax fit with temperature and optional epsilon\u2011greedy randomization.\n    Tighter fits get higher probability; random selection occurs with probability epsilon.\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n    residual = bins_remain_cap - item\n    feasible = residual >= 0\n    if not np.any(feasible):\n        return np.zeros_like(bins_remain_cap, dtype=float)\n    if epsilon > 0 and rng.random() < epsilon:\n        rand_vals = rng.random(bins_remain_cap.shape[0])\n        scores = np.where(feasible, rand_vals, 0.0)\n        total = scores.sum()\n        if total == 0:\n            return np.zeros_like(bins_remain_cap, dtype=float)\n        return scores / total\n    raw = -residual\n    raw[~feasible] = -np.inf\n    scaled = raw / max(temperature, 1e-12)\n    max_score = scaled[feasible].max()\n    exp_scores = np.exp(scaled - max_score)\n    exp_scores[~feasible] = 0.0\n    total = exp_scores.sum()\n    if total == 0:\n        return np.zeros_like(bins_remain_cap, dtype=float)\n    return exp_scores / total\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n                epsilon: float = 0.0, rng: np.random.Generator = None) -> np.ndarray:\n    \"\"\"Softmax fit (temp\u202ftau) with epsilon\u2011greedy random exploration.\"\"\"\n    # Combine stable softmax scoring of waste with occasional random scoring.\n    if rng is None:\n        rng = np.random.default_rng()\n    residual = bins_remain_cap - item\n    mask = residual >= 0\n    if not np.any(mask):\n        return np.zeros_like(bins_remain_cap, dtype=float)\n    if epsilon > 0 and rng.random() < epsilon:\n        rand_scores = rng.random(bins_remain_cap.shape[0])\n        rand_scores[~mask] = -np.inf\n        max_rand = rand_scores[mask].max()\n        exp_rand = np.exp(rand_scores - max_rand)\n        exp_rand[~mask] = 0.0\n        total_rand = exp_rand.sum()\n        return exp_rand / total_rand if total_rand > 0 else np.zeros_like(bins_remain_cap, dtype=float)\n\n### Analyze & experience\n- Comparing (best) vs (worst), we see that heuristic\u202f1 provides a complete, numerically\u2011stable softmax with epsilon\u2011greedy exploration and explicit handling of infeasible bins, while heuristic\u202f20 is incomplete \u2013 it only defines a random branch, lacks a deterministic return, misuses `-inf`, and can return `None`.  \nComparing (second best) vs (second worst), we see heuristic\u202f2 implements a robust softmax over feasible bins and correctly returns zeros for infeasibility, whereas heuristic\u202f19 suffers the same omission as 20: no deterministic branch and a missing return, making it non\u2011functional.  \nComparing (1st) vs (2nd), both use softmax of negative waste; heuristic\u202f1 includes a default `\u03b5=0.1` and computes scores for all bins with `-inf` for infeasible ones, while heuristic\u202f2 isolates the feasible subset, defaults `\u03b5=0.\u202f0` and uses a sharper `\u03c4=0.5`, yielding a leaner implementation but fewer features.  \nComparing (3rd) vs (4th), heuristic\u202f3 simply returns linear negative waste (no normalization), whereas heuristic\u202f4 applies a sigmoid to the fill\u2011ratio (`k=12`), providing a non\u2011linear ranking but still no probability distribution; both ignore exploration and temperature.  \nComparing (second worst) vs (worst), heuristics\u202f19 and\u202f20 are identical copies with the same bug: they only handle the \u03b5\u2011branch and omit a return for the normal softmax case, so they are equally broken.  \nOverall, the top heuristics prioritize numerical stability, clear infeasibility handling, optional exploration, and return a proper probability distribution; mid\u2011tier heuristics vary in sophistication but often lack normalization; the bottom heuristics contain critical logical gaps and are unusable.\n- \n- **Keywords**: softmax, numerical stability, infeasibility masking, \u03b5\u2011exploration, defined distribution, concise code, documentation  \n- **Advice**: Use a stable softmax, explicitly mask infeasible options, add optional \u03b5\u2011exploration, always return a full probability vector, keep code concise, avoid duplicate logic, and document default parameters.  \n- **Avoid**: deterministic behavior, excessive brevity, full vectorization, needless softmax or stochasticity unless explicitly required.  \n- **Explanation**: Non\u2011determinism preserves exploration; controlled stochasticity improves robustness, while a stable softmax guarantees numerical safety and a usable probability vector.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}