{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Temperature-scaled softmax with optional epsilon-greedy exploration; returns normalized priorities for feasible bins.\"\"\"\n    feasible = bins_remain_cap >= item\n    if not np.any(feasible):\n        return np.zeros_like(bins_remain_cap, dtype=float)\n    if rng is None:\n        rng = np.random.default_rng()\n    if rng.random() < epsilon:\n        probs = np.zeros_like(bins_remain_cap, dtype=float)\n        probs[feasible] = 1.0 / np.count_nonzero(feasible)\n        return probs\n    residual = bins_remain_cap.astype(float) - item\n    raw_scores = -residual / tau\n    raw_scores[~feasible] = -np.inf\n    max_score = np.max(raw_scores)\n    exp_scores = np.exp(raw_scores - max_score)\n    sum_exp = np.sum(exp_scores)\n    return exp_scores / sum_exp if sum_exp > 0 else np.zeros_like(exp_scores)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Hybrid waste\u2011based residual scoring with temperature\u2011scaled softmax and \u03b5\u2011weighted random\u2011logit exploration.\"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n    bins = np.asarray(bins_remain_cap, dtype=float)\n    feasible = bins >= item\n    if not np.any(feasible):\n        return np.full_like(bins, -np.inf, dtype=float)\n    residual = bins - item\n    det_logits = np.where(feasible, -residual, -np.inf)\n    max_det = np.max(det_logits[feasible])\n    det_shifted = (det_logits - max_det) / max(tau, 1e-12)\n    det_exp = np.exp(det_shifted) * feasible\n    det_sum = det_exp.sum()\n    det_softmax = np.where(det_sum > 0, det_exp / det_sum, 0.0)\n    rand_logits = rng.random(bins.shape)\n    rand_logits = np.where(feasible, rand_logits, -np.inf)\n    max_rand = np.max(rand_logits[feasible])\n    rand_shifted = rand_logits - max_rand\n    rand_exp = np.exp(rand_shifted) * feasible\n    rand_sum = rand_exp.sum()\n    rand_softmax = np.where(rand_sum > 0, rand_exp / rand_sum, 0.0)\n    probs = (1.0 - epsilon) * det_softmax + epsilon * rand_softmax\n    return np.where(feasible, probs, -np.inf)\n\n### Analyze & experience\n- Comparing the best (heuristic\u202f1) vs the worst (heuristic\u202f20) we see a clean, temperature\u2011scaled softmax with \u03b5\u2011greedy uniform exploration, stable exponent handling and concise documentation versus an over\u2011engineered pipeline that mixes penalties, risk\u2010adjusted scores, rank weighting, adaptive noise, and many magic constants \u2013 far less interpretable and prone to instability.  \nComparing the second best (heuristic\u202f2) vs the second worst (heuristic\u202f19) the former retains a simple deterministic waste\u2011based softmax and optional random\u2011softmax mixing, while the latter mirrors the complexity of heuristic\u202f20 (rank\u2011based adaptive scoring with noise).  \nComparing (1st) vs (2nd) we note that heuristic\u202f1 uses a single \u03b5\u2011greedy branch (uniform random or pure softmax), whereas heuristic\u202f2 always blends deterministic and random softmaxes, adding unnecessary computation and diluting the deterministic signal.  \nComparing (3rd) vs (4th) the third returns \u2013\u221e for infeasible bins and mixes deterministic/random softmaxes; the fourth enriches exploration with Gumbel perturbation and \u03b5\u2011uniform mixing, offering more principled stochasticity at modest cost.  \nComparing the second worst (heuristic\u202f19) vs the worst (heuristic\u202f20) reveals they are identical copies \u2013 no design difference.  \n**Overall:** Simpler softmax\u2011based designs with clear \u03b5\u2011greedy control and stable numeric handling consistently outperform heavily parameterised, noisy, or ad\u2011hoc constructions; clarity, minimalism, and well\u2011defined exploration are key to robust heuristic performance.\n- \n- **Keywords:** temperature scaling, max\u2011value subtraction, prune invalid choices, calibrated randomness, minimal hyper\u2011parameters, reproducibility.  \n- **Advice:** use temperature\u2011scaled scores after subtracting the max; ignore invalid candidates at the start; add calibrated Gumbel perturbations for exploration; expose only knobs, leave other parts unchanged.  \n- **Avoid:** hard\u2011coded magic numbers, layered penalty transforms, redundant branches, hidden randomness, over\u2011engineered feature combos.  \n- **Explanation:** subtracting the max score before exponentiation prevents overflow while preserving relative ordering; ignoring invalid candidates early ensures no probability goes to them; calibrated perturbations give controlled exploration without destabilizing the search; a hyper\u2011parameter set stays transparent, easy to tune, and reproducible with seeded RNG.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}