{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    feasible = bins_remain_cap >= item\n    residual = bins_remain_cap - item\n    priorities = np.full(bins_remain_cap.shape, -np.inf, dtype=float)\n    priorities[feasible] = -residual[feasible]\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    item: float,\n    bins_remain_cap: np.ndarray,\n    *,\n    epsilon: float = 0.1,\n    temperature: float = 0.1,\n    rng: Optional[np.random.Generator] = None,\n) -> np.ndarray:\n    \"\"\"Epsilon\u2011greedy softmax: random logits with prob \u03b5, else residual\u2011based softmax.\"\"\"\n    bins = np.asarray(bins_remain_cap, dtype=float)\n    feasible = bins >= item\n    if not np.any(feasible):\n        return np.zeros_like(bins, dtype=float)\n    if rng is None:\n        rng = np.random.default_rng()\n    # Exploration: random logits; Exploitation: residual\u2011based logits.\n    if rng.random() < epsilon:\n        logits = np.where(feasible, rng.random(bins.shape), -np.inf)\n    else:\n        residual = bins - item\n        temp = max(temperature, 1e-12)\n        logits = np.where(feasible, -residual / temp, -np.inf)\n    # Stable softmax over feasible bins.\n    max_logit = np.max(logits[feasible])\n    exp_shifted = np.exp(logits - max_logit)\n    probs = exp_shifted / exp_shifted.sum()\n    return probs\n\n### Analyze & experience\n- - Comparing (best) vs (worst), we see **H1** uses a stable, temperature\u2011scaled softmax with clean \u03b5\u2011exploration and returns a proper probability vector, whereas **H20** mixes logistic transformation and \u03b5\u2011blending before softmax, adding unnecessary complexity and potential numerical instability.  \n- (second best) vs (second worst), **H2** implements a tidy softmax with optional \u03b5\u2011exploration and returns normalized probabilities, while **H19** (duplicate of H15) lacks a fallback for exploitation and yields zeros when \u03b5=0, making it unusable.  \n- Comparing (1st) vs (2nd), **H1** directly masks infeasible bins with \u2011inf and computes raw scores in a single expression, whereas **H2** builds a separate scores array and duplicates logic, resulting in slightly more verbose code and a minor performance hit.  \n- (3rd) vs (4th), **H3** applies a sigmoid on the slack ratio, producing bounded but non\u2011probability priorities and no exploration; **H4** returns negative waste priorities with optional \u03b5\u2011exploration but uses \u2011inf for infeasible bins and does not normalize to a probability distribution, complicating downstream decisions.  \n- Comparing (second worst) vs (worst), **H19** returns zeros or improper scores due to a missing exploitation branch, whereas **H20** produces a valid probability distribution but still contains unnecessary logistic blending and may suffer from exponentiation of large combined values.  \n- Overall: The top heuristics share consistent infeasible\u2011bin handling, stable temperature\u2011scaled softmax, optional \u03b5\u2011exploration, and clear probability outputs. The bottom ones suffer from incomplete logic, non\u2011probability outputs, or redundant complexity.\n- \n**Keywords:** Adaptive ranking, Contextual scoring, Learning\u2011based priority, Dynamic thresholds, Clustered selection, Feedback loops, Batch optimization, Hybrid heuristics, Resilience.  \n\n**Advice:** Build a predictive model to rank items from features; use dynamic thresholds that shift with recent outcomes; cluster similar items to reduce complexity; employ feedback loops to tune scores; weight options with contextual signals; favor rank\u2011based adjustments over chance\u2011based weighting.  \n\n**Avoid:** Fixed ranking rules; static thresholds; ignoring context; duplicate evaluations; opaque heuristics; simplistic score tweaks; disregarding noisy inputs.  \n\n**Explanation:** Adaptive, data\u2011driven, feedback\u2011rich methods deliver robust, responsive heuristics without unstable random weighting.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}