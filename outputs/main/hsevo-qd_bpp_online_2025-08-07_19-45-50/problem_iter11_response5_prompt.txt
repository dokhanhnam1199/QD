{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    item: float,\n    bins_remain_cap: np.ndarray,\n    tau: float = 1.0,\n    epsilon: float = 0.0,\n    rng: np.random.Generator = None,\n) -> np.ndarray:\n    \"\"\"Priority via temperature\u2011scaled softmax of negative waste mixed with epsilon\u2011greedy random scores; infeasible bins get -inf.\"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n    feasible = bins_remain_cap >= item\n    if not np.any(feasible):\n        return np.full(bins_remain_cap.shape, -np.inf, dtype=float)\n    # Deterministic score: negative waste (higher is better)\n    det_score = np.where(feasible, -(bins_remain_cap - item), -np.inf)\n    # Stable softmax with temperature\n    max_score = np.max(det_score[feasible])\n    shifted = (det_score - max_score) / max(tau, 1e-12)\n    exp_shifted = np.exp(shifted) * feasible\n    sum_exp = exp_shifted.sum_exp if hasattr(exp_shifted, \"sum_exp\") else exp_shifted.sum()\n    if sum_exp > 0:\n        softmax = exp_shifted / sum_exp\n    else:\n        # Fallback: uniform among max\u2011score bins\n        max_mask = (det_score == max_score) & feasible\n        count_max = max_mask.sum()\n        softmax = np.where(max_mask, 1.0 / count_max, 0.0)\n    # Random uniform distribution over feasible bins\n    rand_vals = rng.random(bins_remain_cap.shape) * feasible\n    sum_rand = rand_vals.sum()\n    if sum_rand > 0:\n        rand_dist = rand_vals / sum_rand\n    else:\n        rand_dist = np.zeros_like(bins_remain_cap, dtype=float)\n    # Mix deterministic and random scores\n    combined = (1.0 - epsilon) * softmax + epsilon * rand_dist\n    return np.where(feasible, combined, -np.inf)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n_EPSILON = 0.05\n_TEMPERATURE = 0.1\n_RNG = np.random.default_rng()\n\n    \"\"\"Softmax priority with epsilon\u2011greedy exploration, stable and masked.\"\"\"\n    bins_remain_cap = np.asarray(bins_remain_cap, dtype=float)\n    feasible = bins_remain_cap >= item\n    if not np.any(feasible):\n        return np.zeros_like(bins_remain_cap)\n    if _RNG.random() < _EPSILON:\n        probs = np.zeros_like(bins_remain_cap)\n        probs[feasible] = 1.0 / feasible.sum()\n        return probs\n    residual = bins_remain_cap[feasible] - item\n    temperature = max(_TEMPERATURE, 1e-12)\n    logits = -residual / temperature\n    max_logit = np.max(logits)\n    exp_shifted = np.exp(logits - max_logit)\n    softmax = exp_shifted / np.sum(exp_shifted)\n    scores = np.zeros_like(bins_remain_cap)\n    scores[feasible] = softmax\n    return scores\n\n### Analyze & experience\n- - **Comparing (best) vs (worst):** The first function has a full docstring, type hints, safe defaults, and robust handling of infeasible bins and epsilon\u2011greedy exploration, returning a well\u2011normalized probability distribution. The twentieth version contains numerous syntactic and semantic errors (undefined names, missing imports, no docstring) and would raise runtime exceptions.  \n- **Comparing (second best) vs (second worst):** The second function cleanly masks exact fits and produces deterministic priorities, suitable for arg\u2011max selection, but lacks normalization and explicit error handling. The nineteenth implementation fails to import required constants, uses an undefined `-inf`, and omits RNG initialization, rendering it non\u2011functional.  \n- **Comparing (1st) vs (2nd):** The first offers flexibility through temperature scaling and epsilon exploration and outputs a probability vector, while the second gives raw priorities with an arbitrary large constant for exact matches and no normalization or exploration.  \n- **Comparing (3rd) vs (4th):** The third uses a logistic transform with a hard\u2011coded slope, no exploration, and returns raw negative values without clear probabilistic meaning. The fourth follows a softmax scheme, supports temperature and epsilon, normalizes outputs, and is more robust\u2014despite a missing import.  \n- **Comparing (second worst) vs (worst):** Both the nineteenth and twentieth heuristics suffer from identical fundamental issues: undefined identifiers, missing imports, and incomplete logic, making them non\u2011operational.  \n\n**Overall:** The top heuristics balance clear documentation, safe defaults, vectorized computation, and explicit handling of infeasible cases. The lower\u2011ranked ones lack robust error handling, proper normalization, or even syntactic correctness, underscoring the importance of comprehensive testing and code review.\n- \n- **Keywords**: Softmax, epsilon\u2011greedy; **Advice**: Use stable softmax with epsilon\u2011greedy; **Avoid**: raw scores or deterministic only; **Explanation**: Gives smooth, exploratory probabilities.  \n- **Keywords**: Infeasibility masking; **Advice**: Mask infeasible bins before normalization; **Avoid**: implicit -inf or NaN; **Explanation**: Guarantees valid selections and stable sums.  \n- **Keywords**: RNG seed control; **Advice**: Pass a seed to a local RNG and use it for all randomness; **Avoid**: global unseeded RNG; **Explanation**: Enables reproducibility and debugging.  \n- **Keywords**: Docs & tests; **Advice**: Write full docstrings, type hints, and unit tests for edge cases; **Avoid**: minimal comments or unchecked inputs; **Explanation**: Improves maintainability and catches hidden bugs.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}