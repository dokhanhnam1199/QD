{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n# Priority function combining deterministic waste and optional epsilon\u2011greedy exploration\n                random_state: Optional[int] = None) -> np.ndarray:\n    \"\"\"Priority = -waste for feasible bins; optional epsilon\u2011greedy random scores.\"\"\"\n    feasible = bins_remain_cap >= item\n    if epsilon > 0.0:\n        rng = np.random.default_rng(random_state)\n        if rng.random() < epsilon:\n            rand_scores = rng.random(bins_remain_cap.shape[0])\n            return np.where(feasible, rand_scores, -np.inf)\n    waste = bins_remain_cap - item\n    return np.where(feasible, -waste, -np.inf)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Exact-fit-first priority with optional epsilon-greedy exploration.\"\"\"\n    # Feasibility mask and deterministic priority (negative waste)\n    feasible = bins_remain_cap >= item\n    deterministic = np.where(feasible, -(bins_remain_cap - item), -np.inf)\n    # With probability epsilon, use random scores for feasible bins\n    if np.random.rand() < epsilon:\n        random_scores = np.random.rand(bins := bins_remain_cap.shape[0])\n        return np.where(feasible, random_scores, -np.inf)\n    return deterministic\n\n### Analyze & experience\n- - **Comparing (best) vs (worst):** The best heuristic (1st) uses a numerically stable softmax, normalizes across all feasible bins, and returns zeros for infeasible cases. Its docstring accurately describes this behavior. The worst heuristic (20th) simply returns raw negative waste or random scores with `-inf` for infeasible bins; it lacks normalization, offers no probability distribution, and its docstring misleads by claiming epsilon\u2011greedy exploration that is inconsistently applied. This omission leads to biased and unstable bin choices.  \n- **(second best) vs (second worst):** The second best (2nd) introduces a temperature parameter (`tau`) but otherwise follows the softmax pattern, providing a tunable exploration factor and a robust, normalized priority vector. The second worst (19th) reproduces the epsilon\u2011greedy design without any softmax, returning `-inf` for infeasible bins and random scores for exploration. The temperature\u2011scaled softmax is more principled and offers better control over exploration than raw or random scoring.  \n- **Comparing (1st) vs (2nd):** Both implement softmax; the 2nd adds explicit temperature scaling, whereas the 1st\u2019s implementation is slightly simpler and zeroes infeasible bins early. Functionally they are similar, but the 2nd\u2019s `tau` parameter allows fine\u2011tuning of exploration.  \n- **(3rd) vs (4th):** These are identical implementations: they use an explicit mask, set infeasible scores to `-inf`, compute a stable softmax, and return a proper probability distribution. No difference in quality.  \n- **Comparing (second worst) vs (worst):** 18th and 20th are the same epsilon\u2011greedy design, neither normalizes nor applies softmax. They treat infeasible bins with `-inf` and occasionally return random scores, leading to equivalent but inferior performance compared to the softmax\u2011based heuristics.  \n- **Overall:** The top heuristics excel by ensuring numerical stability, proper softmax normalization, and early handling of infeasible bins. Lower\u2011ranked heuristics fail to do so, either by omitting softmax, returning `-inf` scores directly, or relying on unnormalised random exploration, which results in inconsistent or biased placement decisions.\n- \n- **Keywords:** Softmax, numerical stability, explicit masking, early pruning, controlled randomness, temperature scaling, reproducible RNG, clear documentation.  \n- **Advice:** Apply log\u2011sum\u2011exp softmax after masking infeasible entries; prune impossible bins early; tune temperature to balance exploration/exploitation; seed random generators for repeatable runs; document each scoring step.  \n- **Avoid:** Raw `-inf`/NaN scores, unchecked overflow/underflow, fixed tie\u2011breakers that bias results, uncontrolled randomness that breaks reproducibility.  \n- **Explanation:** Stabilized softmax keeps rankings reliable under extreme values; masking + early exit saves compute; temperature + seeded randomness yields diverse yet repeatable search; thorough docs prevent misuse and simplify debugging.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}