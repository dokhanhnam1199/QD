[
  {
    "stdout_filepath": "problem_iter20_response0.txt_stdout.txt",
    "code_path": "problem_iter20_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, adaptive penalty/bonus, and controlled randomness.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return priorities\n\n    valid_bins = np.where(can_fit)[0]\n    bin_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization (Best-Fit)\n    waste = bins_remain_cap[can_fit] - item\n    priorities[can_fit] = 1.0 / (1e-6 + waste)\n\n    # Adaptive Near-Full Management: Item Size-Dependent\n    near_full_threshold = 0.1 * bin_capacity\n    is_near_full = bins_remain_cap[can_fit] < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.8 * (item / bin_capacity), 0.0)\n    priorities[valid_bins] += near_full_penalty\n\n    # Small item bonus\n    small_item_threshold = 0.2 * bin_capacity\n    if item < small_item_threshold:\n      almost_full_threshold = 0.1 * bin_capacity\n      almost_full_valid_bins = bins_remain_cap[can_fit] < almost_full_threshold + item\n      almost_full_indices = np.where(can_fit)[0][almost_full_valid_bins]\n      priorities[almost_full_indices] += 0.2\n\n    # Controlled Randomness (decaying with item size and remaining capacity)\n    randomness_scale = 0.01 * (1 - item/bin_capacity) * (bins_remain_cap[can_fit] / bin_capacity)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n    priorities[can_fit] += randomness\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.5073793378540135,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response1.txt_stdout.txt",
    "code_path": "problem_iter20_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines adaptive weighting, near-full management, learning rate, & bin diversity.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level (Dynamic)\n    current_utilization = np.mean((bin_capacity - bins_remain_cap) / bin_capacity)\n    target_fill_level = 0.7 + 0.1 * current_utilization\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level * bin_capacity)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.12 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.8 * (item/bin_capacity), 0.0)\n\n    # Adaptive Weighting: item size & bin utilization\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n    remaining_capacity_factor = np.mean(bins_remain_cap[can_fit] / bin_capacity)\n\n\n    tightness_weight = 0.3 * (1 - item_size_factor) * (1 + utilization_factor) * (1 + remaining_capacity_factor)\n    fill_weight = 0.4 * (1 + item_size_factor) * (1 - utilization_factor) * (1 - remaining_capacity_factor)\n    near_full_weight = 0.15\n\n\n    # Stochasticity scaled by item size and bin utilization\n    randomness_scale = 0.02 * (1 + item_size_factor) * (1 - utilization_factor) * (1 + remaining_capacity_factor)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    # Learning Rate and Bin Diversity Reward\n    learning_rate = 0.01\n    bin_diversity_reward = (1 / (np.std(bins_remain_cap[can_fit]) + 0.0001)) * learning_rate\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               randomness + bin_diversity_reward)\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 17.471080973274837,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response2.txt_stdout.txt",
    "code_path": "problem_iter20_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines adaptive weights, dynamic penalties, and strategic randomness.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n    bin_capacity = bins_remain_cap.max()\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level\n    target_fill_level = 0.8 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.15 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9 * (item/bin_capacity), 0.0)\n\n    # Smaller Item Bonus\n    small_item_threshold = bin_capacity * 0.3\n    if item < small_item_threshold:\n        almost_full_threshold = bin_capacity * 0.08\n        almost_full_bonus = np.exp(-remaining_after / (almost_full_threshold + 0.0001))\n    else:\n        almost_full_bonus = 0.0\n\n    # Larger Item Penalty\n    large_item_threshold = bin_capacity * 0.7\n    if item > large_item_threshold:\n        small_space_penalty = np.where(bins_remain_cap[can_fit] < (item + 0.15 * bin_capacity), -0.8 * (item/bin_capacity), 0.0)\n    else:\n        small_space_penalty = 0.0\n\n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n    remaining_capacity_factor = np.mean(bins_remain_cap[can_fit] / bin_capacity)\n\n    tightness_weight = 0.3 * (1 - item_size_factor) * (1 + utilization_factor) * (1 + remaining_capacity_factor)\n    fill_weight = 0.4 * (1 + item_size_factor) * (1 - utilization_factor) * (1 - remaining_capacity_factor)\n    near_full_weight = 0.15\n    small_item_weight = 0.075\n    large_item_weight = 0.075\n\n    # Strategic Randomness (Tie-breaking)\n    num_valid_bins = len(valid_bins)\n    if num_valid_bins > 1:\n        best_priority = np.max((tightness_weight * tightness +\n                                   fill_weight * fill_score +\n                                   near_full_weight * near_full_penalty +\n                                   small_item_weight * almost_full_bonus +\n                                   large_item_weight * small_space_penalty))\n\n        potential_bins = valid_bins[np.isclose((tightness_weight * tightness +\n                                   fill_weight * fill_score +\n                                   near_full_weight * near_full_penalty +\n                                   small_item_weight * almost_full_bonus +\n                                   large_item_weight * small_space_penalty), best_priority)]\n        if len(potential_bins) > 1:\n             chosen_bin = np.random.choice(potential_bins)\n             #Assign high probability to selected bin, so that it will be packed\n             randomness = np.zeros(len(valid_bins))\n             chosen_idx = np.where(valid_bins == chosen_bin)[0]\n             randomness[chosen_idx] = 0.01\n        else:\n            randomness = np.zeros(len(valid_bins))\n    else:\n         randomness = np.zeros(len(valid_bins))\n\n    # Learning Rate (Bin Diversity Reward)\n    learning_rate = 0.01\n    bin_diversity_reward = (1 / (np.std(bins_remain_cap[can_fit]) + 0.0001)) * learning_rate\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus +\n                               large_item_weight * small_space_penalty +\n                               randomness + bin_diversity_reward)\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 2.991623454327882,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response3.txt_stdout.txt",
    "code_path": "problem_iter20_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive weights, strategic randomness, problem-specific objectives by adaptive\n    exploration vs exploitation.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level\n    target_fill_level = 0.8 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.15 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9 * (item/bin_capacity), 0.0)\n\n    # Smaller Item Bonus\n    small_item_threshold = bin_capacity * 0.3\n    if item < small_item_threshold:\n        almost_full_threshold = bin_capacity * 0.08\n        almost_full_bonus = np.exp(-remaining_after / (almost_full_threshold + 0.0001))\n    else:\n        almost_full_bonus = 0.0\n\n    # Larger Item Penalty\n    large_item_threshold = bin_capacity * 0.7\n    if item > large_item_threshold:\n        small_space_penalty = np.where(bins_remain_cap[can_fit] < (item + 0.15 * bin_capacity), -0.8 * (item/bin_capacity), 0.0)\n    else:\n        small_space_penalty = 0.0\n\n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n    remaining_capacity_factor = np.mean(bins_remain_cap[can_fit] / bin_capacity)\n\n    tightness_weight = 0.3 * (1 - item_size_factor) * (1 + utilization_factor) * (1 + remaining_capacity_factor)\n    fill_weight = 0.4 * (1 + item_size_factor) * (1 - utilization_factor) * (1 - remaining_capacity_factor)\n    near_full_weight = 0.15\n    small_item_weight = 0.075\n    large_item_weight = 0.075\n\n    # Strategic Randomness\n    randomness_scale = 0.02 * (1 + item_size_factor) * (1 - utilization_factor) * (1 + remaining_capacity_factor)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    # Learning Rate & bin diversity reward\n    learning_rate = 0.01\n    bin_diversity_reward = (1 / (np.std(bins_remain_cap[can_fit]) + 0.0001)) * learning_rate\n\n    #Adaptive exploration vs exploitation\n    exploration_prob = 0.1  # Probability of exploration\n\n    if np.random.rand() < exploration_prob: #Explore by reducing the weight\n        tightness_weight *= (1 - learning_rate)\n        fill_weight *= (1 + learning_rate)\n    else: # Exploit by increasing the weight\n       tightness_weight *= (1 + learning_rate)\n       fill_weight *= (1 - learning_rate)\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus +\n                               large_item_weight * small_space_penalty +\n                               randomness + bin_diversity_reward)\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 21.81890706023135,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response4.txt_stdout.txt",
    "code_path": "problem_iter20_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, target fill, dynamic penalties, adaptive weighting, and decaying randomness based on item size and bin utilization.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level\n    target_fill_level = 0.75 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.1 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9 * (item/bin_capacity), 0.0)\n\n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n    learning_rate = 0.1\n\n    tightness_weight = 0.35\n    fill_weight = 0.35\n    near_full_weight = 0.15\n    random_weight = 0.15\n\n\n    tightness_weight += learning_rate * (1 - item_size_factor) * (1 + utilization_factor) - tightness_weight\n    fill_weight += learning_rate * (1 + item_size_factor) * (1 - utilization_factor) - fill_weight\n\n    # Decaying Randomness based on item size and bin utilization\n    randomness_scale = 0.02 * (1 + item_size_factor) * (1 - utilization_factor)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               random_weight * randomness)\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 0.8775428799361856,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response5.txt_stdout.txt",
    "code_path": "problem_iter20_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines adaptive weights, waste minimization, and strategic randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n    bin_capacity = bins_remain_cap.max()\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization (Best-Fit)\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target fill level\n    target_fill = 0.8 * bin_capacity\n    fill_diff = np.abs(bins_remain_cap[can_fit] - target_fill)\n    fill_priority = np.exp(-fill_diff / bin_capacity)\n\n    # Near-full penalty, scaled by item size\n    nearly_full_threshold = 0.1 * bin_capacity\n    nearly_full_valid_bins = remaining_after < nearly_full_threshold\n    near_full_penalty = np.where(nearly_full_valid_bins, -0.7 * (item / bin_capacity), 0.0)\n\n    # Adaptive Weighting: item size & bin utilization\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n\n    tightness_weight = 0.4 * (1 - item_size_factor) * (1 + utilization_factor)\n    fill_weight = 0.4 * (1 + item_size_factor) * (1 - utilization_factor)\n    near_full_weight = 0.2\n\n    # Decaying Stochasticity:\n    randomness_scale = 0.015 * (1 - item/bin_capacity) * (np.mean(bins_remain_cap[can_fit]) / bin_capacity)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_priority +\n                               near_full_weight * near_full_penalty +\n                               randomness)\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 1.4459513362584764,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response6.txt_stdout.txt",
    "code_path": "problem_iter20_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive weights, near-empty/full, and strategic randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization (Tightness)\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level (Adaptive)\n    current_utilization = np.mean((bin_capacity - bins_remain_cap) / bin_capacity)\n    target_fill_level = 0.7 + 0.1 * current_utilization\n    fill_diff = np.abs(bins_remain_cap[can_fit] - target_fill_level * bin_capacity)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Near-Empty Bonus\n    near_empty_threshold = 0.9 * bin_capacity\n    is_near_empty = bins_remain_cap[can_fit] > near_empty_threshold\n    near_empty_bonus = np.where(is_near_empty, 0.2 * (1 - item / bin_capacity), 0.0)\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.12 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.8 * (item / bin_capacity), 0.0)\n\n    # Adaptive Weighting: Item size and Bin Utilization\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n\n    tightness_weight = 0.4 * (1 - item_size_factor) * (1 + utilization_factor)\n    fill_weight = 0.4 * (1 + item_size_factor) * (1 - utilization_factor)\n    near_empty_weight = 0.1\n    near_full_weight = 0.1\n\n    # Strategic Randomness (Decaying) - Tie Breaking + Perturbation\n    if not hasattr(priority_v2, \"time_step\"):\n        priority_v2.time_step = 1\n    else:\n        priority_v2.time_step += 1\n    exploration_factor = np.exp(-priority_v2.time_step / 1000.0)\n    randomness_scale = 0.01 * (1 + item_size_factor) * exploration_factor\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_empty_weight * near_empty_bonus +\n                               near_full_weight * near_full_penalty +\n                               randomness)\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 2.7822098125249393,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response7.txt_stdout.txt",
    "code_path": "problem_iter20_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive fill, waste minimization, decaying randomness.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return priorities\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n\n    # Waste Minimization - Prefer bins where the item fits tightly\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n    priorities[valid_bins] += tightness * 0.5\n\n    # Adaptive Target fill level\n    target_fill = 0.75 * bin_capacity\n    fill_diff = np.abs(bins_remain_cap[can_fit] - target_fill)\n    fill_priority = np.exp(-fill_diff / bin_capacity)\n    priorities[valid_bins] += fill_priority * 0.4\n\n    # Near-full penalty\n    nearly_full_threshold = 0.1 * bin_capacity\n    near_full_penalty = np.where(remaining_after < nearly_full_threshold, -0.5, 0.0)\n    priorities[valid_bins] += near_full_penalty\n\n    # Decaying randomness\n    item_size_weight = item / bin_capacity\n    randomness = np.random.rand(len(valid_bins)) * 0.02 * (1 - item_size_weight)\n    priorities[valid_bins] += randomness\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 2.961707219784608,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response8.txt_stdout.txt",
    "code_path": "problem_iter20_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines adaptive weighting, waste minimization, and targeted fill.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    max_bin_cap = bins_remain_cap.max()\n    bins_utilization = (max_bin_cap - bins_remain_cap[can_fit]) / max_bin_cap\n\n    # Waste minimization\n    waste = remaining_after\n    tightness = 1.0 / (1e-6 + waste)\n    item_size_factor = item / max_bin_cap\n    utilization_factor = np.mean(bins_utilization)\n    tightness_weight = 0.4 * (1 - item_size_factor) * (1 + utilization_factor)\n\n\n    # Target fill level\n    target_fill = 0.75 * max_bin_cap\n    fill_diff = np.abs(bins_remain_cap[can_fit] - target_fill)\n    fill_priority = np.exp(-fill_diff / max_bin_cap)\n    fill_weight = 0.3 * (1 + item_size_factor) * (1 - utilization_factor)\n\n    # Near-full penalty\n    nearly_full_threshold = 0.1 * max_bin_cap\n    near_full_penalty = np.where(remaining_after < nearly_full_threshold, -0.5, 0.0)\n    penalty_weight = 0.2\n\n    # Item size consideration (adaptive randomness)\n    randomness_scale = 0.01 * (1 - item_size_weight)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n    random_weight = 0.1\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                              fill_weight * fill_priority +\n                              penalty_weight * near_full_penalty +\n                              random_weight * randomness)\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 37, in priority_v2\n    tightness_weight = 0.4\nNameError: name 'item_size_weight' is not defined\n"
  },
  {
    "stdout_filepath": "problem_iter20_response9.txt_stdout.txt",
    "code_path": "problem_iter20_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines adaptive weighting, strategic randomness, and anticipatory penalties.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization (Focus on tightness)\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level (Emphasis on achieving target)\n    target_fill_level = 0.75 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Anticipatory Near-Full Penalty (Stronger penalty, scaled by item size)\n    near_full_threshold = 0.1 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.95 * (item / bin_capacity), 0.0)\n\n    # Adaptive Weighting (Item size & utilization)\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n    learning_rate = 0.1\n\n    tightness_weight = 0.4\n    fill_weight = 0.35\n    near_full_weight = 0.25\n\n    tightness_weight += learning_rate * (1 - item_size_factor) * (1 + utilization_factor) - tightness_weight\n    fill_weight += learning_rate * (1 + item_size_factor) * (1 - utilization_factor) - fill_weight\n\n    # Strategic Randomness (Controlled, decays slower for smaller items)\n    randomness_scale = 0.015 * (1 + item_size_factor) * (1 - utilization_factor)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               randomness)\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 1.126844834463509,
    "exec_success": true
  }
]