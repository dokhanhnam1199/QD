{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, target fill, dynamic penalties, adaptive weighting, and decaying randomness based on item size and bin utilization.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level\n    target_fill_level = 0.75 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.1 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9 * (item/bin_capacity), 0.0)\n\n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n    learning_rate = 0.1\n\n    tightness_weight = 0.35\n    fill_weight = 0.35\n    near_full_weight = 0.15\n    random_weight = 0.15\n\n\n    tightness_weight += learning_rate * (1 - item_size_factor) * (1 + utilization_factor) - tightness_weight\n    fill_weight += learning_rate * (1 + item_size_factor) * (1 - utilization_factor) - fill_weight\n\n    # Decaying Randomness based on item size and bin utilization\n    randomness_scale = 0.02 * (1 + item_size_factor) * (1 - utilization_factor)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               random_weight * randomness)\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, target fill, dynamic penalties, adaptive weighting, and decaying randomness based on item size and bin utilization.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level\n    target_fill_level = 0.75 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.1 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9 * (item/bin_capacity), 0.0)\n\n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n    learning_rate = 0.1\n\n    tightness_weight = 0.35\n    fill_weight = 0.35\n    near_full_weight = 0.15\n    random_weight = 0.15\n\n\n    tightness_weight += learning_rate * (1 - item_size_factor) * (1 + utilization_factor) - tightness_weight\n    fill_weight += learning_rate * (1 + item_size_factor) * (1 - utilization_factor) - fill_weight\n\n    # Decaying Randomness based on item size and bin utilization\n    randomness_scale = 0.02 * (1 + item_size_factor) * (1 - utilization_factor)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               random_weight * randomness)\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines adaptive weighting, strategic randomness, and anticipatory penalties.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization (Focus on tightness)\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level (Emphasis on achieving target)\n    target_fill_level = 0.75 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Anticipatory Near-Full Penalty (Stronger penalty, scaled by item size)\n    near_full_threshold = 0.1 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.95 * (item / bin_capacity), 0.0)\n\n    # Adaptive Weighting (Item size & utilization)\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n    learning_rate = 0.1\n\n    tightness_weight = 0.4\n    fill_weight = 0.35\n    near_full_weight = 0.25\n\n    tightness_weight += learning_rate * (1 - item_size_factor) * (1 + utilization_factor) - tightness_weight\n    fill_weight += learning_rate * (1 + item_size_factor) * (1 - utilization_factor) - fill_weight\n\n    # Strategic Randomness (Controlled, decays slower for smaller items)\n    randomness_scale = 0.015 * (1 + item_size_factor) * (1 - utilization_factor)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               randomness)\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines adaptive weights, waste minimization, and strategic randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n    bin_capacity = bins_remain_cap.max()\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization (Best-Fit)\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target fill level\n    target_fill = 0.8 * bin_capacity\n    fill_diff = np.abs(bins_remain_cap[can_fit] - target_fill)\n    fill_priority = np.exp(-fill_diff / bin_capacity)\n\n    # Near-full penalty, scaled by item size\n    nearly_full_threshold = 0.1 * bin_capacity\n    nearly_full_valid_bins = remaining_after < nearly_full_threshold\n    near_full_penalty = np.where(nearly_full_valid_bins, -0.7 * (item / bin_capacity), 0.0)\n\n    # Adaptive Weighting: item size & bin utilization\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n\n    tightness_weight = 0.4 * (1 - item_size_factor) * (1 + utilization_factor)\n    fill_weight = 0.4 * (1 + item_size_factor) * (1 - utilization_factor)\n    near_full_weight = 0.2\n\n    # Decaying Stochasticity:\n    randomness_scale = 0.015 * (1 - item/bin_capacity) * (np.mean(bins_remain_cap[can_fit]) / bin_capacity)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_priority +\n                               near_full_weight * near_full_penalty +\n                               randomness)\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive weights, near-empty/full, and strategic randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization (Tightness)\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level (Adaptive)\n    current_utilization = np.mean((bin_capacity - bins_remain_cap) / bin_capacity)\n    target_fill_level = 0.7 + 0.1 * current_utilization\n    fill_diff = np.abs(bins_remain_cap[can_fit] - target_fill_level * bin_capacity)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Near-Empty Bonus\n    near_empty_threshold = 0.9 * bin_capacity\n    is_near_empty = bins_remain_cap[can_fit] > near_empty_threshold\n    near_empty_bonus = np.where(is_near_empty, 0.2 * (1 - item / bin_capacity), 0.0)\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.12 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.8 * (item / bin_capacity), 0.0)\n\n    # Adaptive Weighting: Item size and Bin Utilization\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n\n    tightness_weight = 0.4 * (1 - item_size_factor) * (1 + utilization_factor)\n    fill_weight = 0.4 * (1 + item_size_factor) * (1 - utilization_factor)\n    near_empty_weight = 0.1\n    near_full_weight = 0.1\n\n    # Strategic Randomness (Decaying) - Tie Breaking + Perturbation\n    if not hasattr(priority_v2, \"time_step\"):\n        priority_v2.time_step = 1\n    else:\n        priority_v2.time_step += 1\n    exploration_factor = np.exp(-priority_v2.time_step / 1000.0)\n    randomness_scale = 0.01 * (1 + item_size_factor) * exploration_factor\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_empty_weight * near_empty_bonus +\n                               near_full_weight * near_full_penalty +\n                               randomness)\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive fill, waste minimization, decaying randomness.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return priorities\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n\n    # Waste Minimization - Prefer bins where the item fits tightly\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n    priorities[valid_bins] += tightness * 0.5\n\n    # Adaptive Target fill level\n    target_fill = 0.75 * bin_capacity\n    fill_diff = np.abs(bins_remain_cap[can_fit] - target_fill)\n    fill_priority = np.exp(-fill_diff / bin_capacity)\n    priorities[valid_bins] += fill_priority * 0.4\n\n    # Near-full penalty\n    nearly_full_threshold = 0.1 * bin_capacity\n    near_full_penalty = np.where(remaining_after < nearly_full_threshold, -0.5, 0.0)\n    priorities[valid_bins] += near_full_penalty\n\n    # Decaying randomness\n    item_size_weight = item / bin_capacity\n    randomness = np.random.rand(len(valid_bins)) * 0.02 * (1 - item_size_weight)\n    priorities[valid_bins] += randomness\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive fill, waste minimization, decaying randomness.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return priorities\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n\n    # Waste Minimization - Prefer bins where the item fits tightly\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n    priorities[valid_bins] += tightness * 0.5\n\n    # Adaptive Target fill level\n    target_fill = 0.75 * bin_capacity\n    fill_diff = np.abs(bins_remain_cap[can_fit] - target_fill)\n    fill_priority = np.exp(-fill_diff / bin_capacity)\n    priorities[valid_bins] += fill_priority * 0.4\n\n    # Near-full penalty\n    nearly_full_threshold = 0.1 * bin_capacity\n    near_full_penalty = np.where(remaining_after < nearly_full_threshold, -0.5, 0.0)\n    priorities[valid_bins] += near_full_penalty\n\n    # Decaying randomness\n    item_size_weight = item / bin_capacity\n    randomness = np.random.rand(len(valid_bins)) * 0.02 * (1 - item_size_weight)\n    priorities[valid_bins] += randomness\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines adaptive weights, dynamic penalties, and strategic randomness.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n    bin_capacity = bins_remain_cap.max()\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level\n    target_fill_level = 0.8 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.15 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9 * (item/bin_capacity), 0.0)\n\n    # Smaller Item Bonus\n    small_item_threshold = bin_capacity * 0.3\n    if item < small_item_threshold:\n        almost_full_threshold = bin_capacity * 0.08\n        almost_full_bonus = np.exp(-remaining_after / (almost_full_threshold + 0.0001))\n    else:\n        almost_full_bonus = 0.0\n\n    # Larger Item Penalty\n    large_item_threshold = bin_capacity * 0.7\n    if item > large_item_threshold:\n        small_space_penalty = np.where(bins_remain_cap[can_fit] < (item + 0.15 * bin_capacity), -0.8 * (item/bin_capacity), 0.0)\n    else:\n        small_space_penalty = 0.0\n\n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n    remaining_capacity_factor = np.mean(bins_remain_cap[can_fit] / bin_capacity)\n\n    tightness_weight = 0.3 * (1 - item_size_factor) * (1 + utilization_factor) * (1 + remaining_capacity_factor)\n    fill_weight = 0.4 * (1 + item_size_factor) * (1 - utilization_factor) * (1 - remaining_capacity_factor)\n    near_full_weight = 0.15\n    small_item_weight = 0.075\n    large_item_weight = 0.075\n\n    # Strategic Randomness (Tie-breaking)\n    num_valid_bins = len(valid_bins)\n    if num_valid_bins > 1:\n        best_priority = np.max((tightness_weight * tightness +\n                                   fill_weight * fill_score +\n                                   near_full_weight * near_full_penalty +\n                                   small_item_weight * almost_full_bonus +\n                                   large_item_weight * small_space_penalty))\n\n        potential_bins = valid_bins[np.isclose((tightness_weight * tightness +\n                                   fill_weight * fill_score +\n                                   near_full_weight * near_full_penalty +\n                                   small_item_weight * almost_full_bonus +\n                                   large_item_weight * small_space_penalty), best_priority)]\n        if len(potential_bins) > 1:\n             chosen_bin = np.random.choice(potential_bins)\n             #Assign high probability to selected bin, so that it will be packed\n             randomness = np.zeros(len(valid_bins))\n             chosen_idx = np.where(valid_bins == chosen_bin)[0]\n             randomness[chosen_idx] = 0.01\n        else:\n            randomness = np.zeros(len(valid_bins))\n    else:\n         randomness = np.zeros(len(valid_bins))\n\n    # Learning Rate (Bin Diversity Reward)\n    learning_rate = 0.01\n    bin_diversity_reward = (1 / (np.std(bins_remain_cap[can_fit]) + 0.0001)) * learning_rate\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus +\n                               large_item_weight * small_space_penalty +\n                               randomness + bin_diversity_reward)\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines adaptive weights, dynamic penalties, and strategic randomness.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n    bin_capacity = bins_remain_cap.max()\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level\n    target_fill_level = 0.8 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.15 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9 * (item/bin_capacity), 0.0)\n\n    # Smaller Item Bonus\n    small_item_threshold = bin_capacity * 0.3\n    if item < small_item_threshold:\n        almost_full_threshold = bin_capacity * 0.08\n        almost_full_bonus = np.exp(-remaining_after / (almost_full_threshold + 0.0001))\n    else:\n        almost_full_bonus = 0.0\n\n    # Larger Item Penalty\n    large_item_threshold = bin_capacity * 0.7\n    if item > large_item_threshold:\n        small_space_penalty = np.where(bins_remain_cap[can_fit] < (item + 0.15 * bin_capacity), -0.8 * (item/bin_capacity), 0.0)\n    else:\n        small_space_penalty = 0.0\n\n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n    remaining_capacity_factor = np.mean(bins_remain_cap[can_fit] / bin_capacity)\n\n    tightness_weight = 0.3 * (1 - item_size_factor) * (1 + utilization_factor) * (1 + remaining_capacity_factor)\n    fill_weight = 0.4 * (1 + item_size_factor) * (1 - utilization_factor) * (1 - remaining_capacity_factor)\n    near_full_weight = 0.15\n    small_item_weight = 0.075\n    large_item_weight = 0.075\n\n    # Strategic Randomness (Tie-breaking)\n    num_valid_bins = len(valid_bins)\n    if num_valid_bins > 1:\n        best_priority = np.max((tightness_weight * tightness +\n                                   fill_weight * fill_score +\n                                   near_full_weight * near_full_penalty +\n                                   small_item_weight * almost_full_bonus +\n                                   large_item_weight * small_space_penalty))\n\n        potential_bins = valid_bins[np.isclose((tightness_weight * tightness +\n                                   fill_weight * fill_score +\n                                   near_full_weight * near_full_penalty +\n                                   small_item_weight * almost_full_bonus +\n                                   large_item_weight * small_space_penalty), best_priority)]\n        if len(potential_bins) > 1:\n             chosen_bin = np.random.choice(potential_bins)\n             #Assign high probability to selected bin, so that it will be packed\n             randomness = np.zeros(len(valid_bins))\n             chosen_idx = np.where(valid_bins == chosen_bin)[0]\n             randomness[chosen_idx] = 0.01\n        else:\n            randomness = np.zeros(len(valid_bins))\n    else:\n         randomness = np.zeros(len(valid_bins))\n\n    # Learning Rate (Bin Diversity Reward)\n    learning_rate = 0.01\n    bin_diversity_reward = (1 / (np.std(bins_remain_cap[can_fit]) + 0.0001)) * learning_rate\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus +\n                               large_item_weight * small_space_penalty +\n                               randomness + bin_diversity_reward)\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines adaptive weights, dynamic penalties, and strategic randomness.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n    bin_capacity = bins_remain_cap.max()\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level\n    target_fill_level = 0.8 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.15 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9 * (item/bin_capacity), 0.0)\n\n    # Smaller Item Bonus\n    small_item_threshold = bin_capacity * 0.3\n    if item < small_item_threshold:\n        almost_full_threshold = bin_capacity * 0.08\n        almost_full_bonus = np.exp(-remaining_after / (almost_full_threshold + 0.0001))\n    else:\n        almost_full_bonus = 0.0\n\n    # Larger Item Penalty\n    large_item_threshold = bin_capacity * 0.7\n    if item > large_item_threshold:\n        small_space_penalty = np.where(bins_remain_cap[can_fit] < (item + 0.15 * bin_capacity), -0.8 * (item/bin_capacity), 0.0)\n    else:\n        small_space_penalty = 0.0\n\n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n    remaining_capacity_factor = np.mean(bins_remain_cap[can_fit] / bin_capacity)\n\n    tightness_weight = 0.3 * (1 - item_size_factor) * (1 + utilization_factor) * (1 + remaining_capacity_factor)\n    fill_weight = 0.4 * (1 + item_size_factor) * (1 - utilization_factor) * (1 - remaining_capacity_factor)\n    near_full_weight = 0.15\n    small_item_weight = 0.075\n    large_item_weight = 0.075\n\n    # Strategic Randomness (Tie-breaking)\n    num_valid_bins = len(valid_bins)\n    if num_valid_bins > 1:\n        best_priority = np.max((tightness_weight * tightness +\n                                   fill_weight * fill_score +\n                                   near_full_weight * near_full_penalty +\n                                   small_item_weight * almost_full_bonus +\n                                   large_item_weight * small_space_penalty))\n\n        potential_bins = valid_bins[np.isclose((tightness_weight * tightness +\n                                   fill_weight * fill_score +\n                                   near_full_weight * near_full_penalty +\n                                   small_item_weight * almost_full_bonus +\n                                   large_item_weight * small_space_penalty), best_priority)]\n        if len(potential_bins) > 1:\n             chosen_bin = np.random.choice(potential_bins)\n             #Assign high probability to selected bin, so that it will be packed\n             randomness = np.zeros(len(valid_bins))\n             chosen_idx = np.where(valid_bins == chosen_bin)[0]\n             randomness[chosen_idx] = 0.01\n        else:\n            randomness = np.zeros(len(valid_bins))\n    else:\n         randomness = np.zeros(len(valid_bins))\n\n    # Learning Rate (Bin Diversity Reward)\n    learning_rate = 0.01\n    bin_diversity_reward = (1 / (np.std(bins_remain_cap[can_fit]) + 0.0001)) * learning_rate\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus +\n                               large_item_weight * small_space_penalty +\n                               randomness + bin_diversity_reward)\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                target_fill_level_ratio: float = 0.6347546940353668,\n                fill_score_decay_rate_ratio: float = 0.29574412249719567,\n                near_full_threshold_ratio: float = 0.06661477682762248,\n                near_full_penalty_ratio: float = 0.5894908144262263,\n                learning_rate: float = 0.07925042422378327,\n                tightness_weight: float = 0.4103903243957867,\n                fill_weight: float = 0.3972697484039701,\n                near_full_weight: float = 0.22245729973446654,\n                random_weight: float = 0.12589877986640546,\n                randomness_scale_factor: float = 0.020267146647381322,\n                waste_minimization_epsilon: float = 0.00047444542944351806) -> np.ndarray:\n    \"\"\"Combines waste minimization, target fill, dynamic penalties, adaptive weighting, and decaying randomness based on item size and bin utilization.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n[Heuristics 12th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                target_fill_level_ratio: float = 0.6347546940353668,\n                fill_score_decay_rate_ratio: float = 0.29574412249719567,\n                near_full_threshold_ratio: float = 0.06661477682762248,\n                near_full_penalty_ratio: float = 0.5894908144262263,\n                learning_rate: float = 0.07925042422378327,\n                tightness_weight: float = 0.4103903243957867,\n                fill_weight: float = 0.3972697484039701,\n                near_full_weight: float = 0.22245729973446654,\n                random_weight: float = 0.12589877986640546,\n                randomness_scale_factor: float = 0.020267146647381322,\n                waste_minimization_epsilon: float = 0.00047444542944351806) -> np.ndarray:\n    \"\"\"Combines waste minimization, target fill, dynamic penalties, adaptive weighting, and decaying randomness based on item size and bin utilization.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n[Heuristics 13th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                target_fill_level_ratio: float = 0.6347546940353668,\n                fill_score_decay_rate_ratio: float = 0.29574412249719567,\n                near_full_threshold_ratio: float = 0.06661477682762248,\n                near_full_penalty_ratio: float = 0.5894908144262263,\n                learning_rate: float = 0.07925042422378327,\n                tightness_weight: float = 0.4103903243957867,\n                fill_weight: float = 0.3972697484039701,\n                near_full_weight: float = 0.22245729973446654,\n                random_weight: float = 0.12589877986640546,\n                randomness_scale_factor: float = 0.020267146647381322,\n                waste_minimization_epsilon: float = 0.00047444542944351806) -> np.ndarray:\n    \"\"\"Combines waste minimization, target fill, dynamic penalties, adaptive weighting, and decaying randomness based on item size and bin utilization.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, adaptive penalty/bonus, and controlled randomness.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return priorities\n\n    valid_bins = np.where(can_fit)[0]\n    bin_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization (Best-Fit)\n    waste = bins_remain_cap[can_fit] - item\n    priorities[can_fit] = 1.0 / (1e-6 + waste)\n\n    # Adaptive Near-Full Management: Item Size-Dependent\n    near_full_threshold = 0.1 * bin_capacity\n    is_near_full = bins_remain_cap[can_fit] < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.8 * (item / bin_capacity), 0.0)\n    priorities[valid_bins] += near_full_penalty\n\n    # Small item bonus\n    small_item_threshold = 0.2 * bin_capacity\n    if item < small_item_threshold:\n      almost_full_threshold = 0.1 * bin_capacity\n      almost_full_valid_bins = bins_remain_cap[can_fit] < almost_full_threshold + item\n      almost_full_indices = np.where(can_fit)[0][almost_full_valid_bins]\n      priorities[almost_full_indices] += 0.2\n\n    # Controlled Randomness (decaying with item size and remaining capacity)\n    randomness_scale = 0.01 * (1 - item/bin_capacity) * (bins_remain_cap[can_fit] / bin_capacity)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n    priorities[can_fit] += randomness\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Enhanced heuristic: adaptive weights, dynamic near-full, item size considerations, and randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization: Prioritize bins that minimize remaining space relative to item size\n    waste_ratio = remaining_after / item\n    waste_priority = np.exp(-5 * waste_ratio)  # Exponentially decreasing priority with increasing waste\n\n    # Target Fill Level: Reward bins close to a target fill level, but adapt the target\n    # based on item size.  Smaller items shift the target higher.\n    target_fill_level = 0.8 * bin_capacity - 0.1 * item\n    fill_level_diff = np.abs(bins_remain_cap[can_fit] - target_fill_level)\n    fill_level_priority = np.exp(-fill_level_diff / (bin_capacity * 0.2))\n\n    # Near-Full Management: Penalize bins that become *very* near full, but only if the\n    # item isn't tiny.  Tiny items can top off bins.\n    near_full_threshold = 0.08 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full & (item > 0.1 * bin_capacity), -0.9, 0.0)\n\n    # Small Item Bonus: Significantly reward bins that can be nearly filled by small items.\n    small_item_threshold = 0.2 * bin_capacity\n    if item < small_item_threshold:\n        almost_full_threshold = 0.05 * bin_capacity\n        almost_full_bonus = np.exp(-remaining_after / (almost_full_threshold + 0.0001))\n    else:\n        almost_full_bonus = 0.0\n\n    # Large Item Penalty: Penalize bins where a large item will leave very little space,\n    # especially if that space is smaller than the average item size seen so far.\n    large_item_threshold = 0.7 * bin_capacity\n    if item > large_item_threshold:\n        small_space_threshold = 0.15 * bin_capacity #was 0.1\n        small_space_penalty = np.where(remaining_after < small_space_threshold, -0.8, 0.0)\n    else:\n        small_space_penalty = 0.0\n\n    # Anticipatory Penalty: Penalize bins that will lead to highly fragmented space\n    # (multiple small spaces).  This requires a simplified simulation.\n    fragmentation_penalty = np.zeros_like(remaining_after)\n\n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization) if bins_utilization.size > 0 else 0.0\n\n    waste_weight = 0.3 * (1 - item_size_factor) * (1 + utilization_factor)\n    fill_weight = 0.3 * (1 + item_size_factor) * (1 - utilization_factor)\n    near_full_weight = 0.1\n    small_item_weight = 0.15\n    large_item_weight = 0.15\n\n    # Strategic Randomness:  Introduce randomness proportional to the *uncertainty*\n    # in the current state (high uncertainty when bins are similar, low when\n    # one bin is clearly better).\n    priority_range = np.max([waste_priority.max(), fill_level_priority.max(), 0.1]) - np.min([waste_priority.min(), fill_level_priority.min(), -0.1]) #Avoid empty sequence\n    randomness_scale = 0.02 * (1 + item_size_factor) * (1 - utilization_factor) * priority_range\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (waste_weight * waste_priority +\n                               fill_weight * fill_level_priority +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus +\n                               large_item_weight * small_space_penalty +\n                               randomness +\n                               fragmentation_penalty)\n\n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Enhanced heuristic: adaptive weights, dynamic near-full, item size considerations, and randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization: Prioritize bins that minimize remaining space relative to item size\n    waste_ratio = remaining_after / item\n    waste_priority = np.exp(-5 * waste_ratio)  # Exponentially decreasing priority with increasing waste\n\n    # Target Fill Level: Reward bins close to a target fill level, but adapt the target\n    # based on item size.  Smaller items shift the target higher.\n    target_fill_level = 0.8 * bin_capacity - 0.1 * item\n    fill_level_diff = np.abs(bins_remain_cap[can_fit] - target_fill_level)\n    fill_level_priority = np.exp(-fill_level_diff / (bin_capacity * 0.2))\n\n    # Near-Full Management: Penalize bins that become *very* near full, but only if the\n    # item isn't tiny.  Tiny items can top off bins.\n    near_full_threshold = 0.08 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full & (item > 0.1 * bin_capacity), -0.9, 0.0)\n\n    # Small Item Bonus: Significantly reward bins that can be nearly filled by small items.\n    small_item_threshold = 0.2 * bin_capacity\n    if item < small_item_threshold:\n        almost_full_threshold = 0.05 * bin_capacity\n        almost_full_bonus = np.exp(-remaining_after / (almost_full_threshold + 0.0001))\n    else:\n        almost_full_bonus = 0.0\n\n    # Large Item Penalty: Penalize bins where a large item will leave very little space,\n    # especially if that space is smaller than the average item size seen so far.\n    large_item_threshold = 0.7 * bin_capacity\n    if item > large_item_threshold:\n        small_space_threshold = 0.15 * bin_capacity #was 0.1\n        small_space_penalty = np.where(remaining_after < small_space_threshold, -0.8, 0.0)\n    else:\n        small_space_penalty = 0.0\n\n    # Anticipatory Penalty: Penalize bins that will lead to highly fragmented space\n    # (multiple small spaces).  This requires a simplified simulation.\n    fragmentation_penalty = np.zeros_like(remaining_after)\n\n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization) if bins_utilization.size > 0 else 0.0\n\n    waste_weight = 0.3 * (1 - item_size_factor) * (1 + utilization_factor)\n    fill_weight = 0.3 * (1 + item_size_factor) * (1 - utilization_factor)\n    near_full_weight = 0.1\n    small_item_weight = 0.15\n    large_item_weight = 0.15\n\n    # Strategic Randomness:  Introduce randomness proportional to the *uncertainty*\n    # in the current state (high uncertainty when bins are similar, low when\n    # one bin is clearly better).\n    priority_range = np.max([waste_priority.max(), fill_level_priority.max(), 0.1]) - np.min([waste_priority.min(), fill_level_priority.min(), -0.1]) #Avoid empty sequence\n    randomness_scale = 0.02 * (1 + item_size_factor) * (1 - utilization_factor) * priority_range\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (waste_weight * waste_priority +\n                               fill_weight * fill_level_priority +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus +\n                               large_item_weight * small_space_penalty +\n                               randomness +\n                               fragmentation_penalty)\n\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Enhanced heuristic: adaptive weights, dynamic near-full, item size considerations, and randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization: Prioritize bins that minimize remaining space relative to item size\n    waste_ratio = remaining_after / item\n    waste_priority = np.exp(-5 * waste_ratio)  # Exponentially decreasing priority with increasing waste\n\n    # Target Fill Level: Reward bins close to a target fill level, but adapt the target\n    # based on item size.  Smaller items shift the target higher.\n    target_fill_level = 0.8 * bin_capacity - 0.1 * item\n    fill_level_diff = np.abs(bins_remain_cap[can_fit] - target_fill_level)\n    fill_level_priority = np.exp(-fill_level_diff / (bin_capacity * 0.2))\n\n    # Near-Full Management: Penalize bins that become *very* near full, but only if the\n    # item isn't tiny.  Tiny items can top off bins.\n    near_full_threshold = 0.08 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full & (item > 0.1 * bin_capacity), -0.9, 0.0)\n\n    # Small Item Bonus: Significantly reward bins that can be nearly filled by small items.\n    small_item_threshold = 0.2 * bin_capacity\n    if item < small_item_threshold:\n        almost_full_threshold = 0.05 * bin_capacity\n        almost_full_bonus = np.exp(-remaining_after / (almost_full_threshold + 0.0001))\n    else:\n        almost_full_bonus = 0.0\n\n    # Large Item Penalty: Penalize bins where a large item will leave very little space,\n    # especially if that space is smaller than the average item size seen so far.\n    large_item_threshold = 0.7 * bin_capacity\n    if item > large_item_threshold:\n        small_space_threshold = 0.15 * bin_capacity #was 0.1\n        small_space_penalty = np.where(remaining_after < small_space_threshold, -0.8, 0.0)\n    else:\n        small_space_penalty = 0.0\n\n    # Anticipatory Penalty: Penalize bins that will lead to highly fragmented space\n    # (multiple small spaces).  This requires a simplified simulation.\n    fragmentation_penalty = np.zeros_like(remaining_after)\n\n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization) if bins_utilization.size > 0 else 0.0\n\n    waste_weight = 0.3 * (1 - item_size_factor) * (1 + utilization_factor)\n    fill_weight = 0.3 * (1 + item_size_factor) * (1 - utilization_factor)\n    near_full_weight = 0.1\n    small_item_weight = 0.15\n    large_item_weight = 0.15\n\n    # Strategic Randomness:  Introduce randomness proportional to the *uncertainty*\n    # in the current state (high uncertainty when bins are similar, low when\n    # one bin is clearly better).\n    priority_range = np.max([waste_priority.max(), fill_level_priority.max(), 0.1]) - np.min([waste_priority.min(), fill_level_priority.min(), -0.1]) #Avoid empty sequence\n    randomness_scale = 0.02 * (1 + item_size_factor) * (1 - utilization_factor) * priority_range\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (waste_weight * waste_priority +\n                               fill_weight * fill_level_priority +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus +\n                               large_item_weight * small_space_penalty +\n                               randomness +\n                               fragmentation_penalty)\n\n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive weights, strategic randomness, problem-specific objectives by adaptive\n    exploration vs exploitation.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level\n    target_fill_level = 0.8 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.15 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9 * (item/bin_capacity), 0.0)\n\n    # Smaller Item Bonus\n    small_item_threshold = bin_capacity * 0.3\n    if item < small_item_threshold:\n        almost_full_threshold = bin_capacity * 0.08\n        almost_full_bonus = np.exp(-remaining_after / (almost_full_threshold + 0.0001))\n    else:\n        almost_full_bonus = 0.0\n\n    # Larger Item Penalty\n    large_item_threshold = bin_capacity * 0.7\n    if item > large_item_threshold:\n        small_space_penalty = np.where(bins_remain_cap[can_fit] < (item + 0.15 * bin_capacity), -0.8 * (item/bin_capacity), 0.0)\n    else:\n        small_space_penalty = 0.0\n\n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n    remaining_capacity_factor = np.mean(bins_remain_cap[can_fit] / bin_capacity)\n\n    tightness_weight = 0.3 * (1 - item_size_factor) * (1 + utilization_factor) * (1 + remaining_capacity_factor)\n    fill_weight = 0.4 * (1 + item_size_factor) * (1 - utilization_factor) * (1 - remaining_capacity_factor)\n    near_full_weight = 0.15\n    small_item_weight = 0.075\n    large_item_weight = 0.075\n\n    # Strategic Randomness\n    randomness_scale = 0.02 * (1 + item_size_factor) * (1 - utilization_factor) * (1 + remaining_capacity_factor)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    # Learning Rate & bin diversity reward\n    learning_rate = 0.01\n    bin_diversity_reward = (1 / (np.std(bins_remain_cap[can_fit]) + 0.0001)) * learning_rate\n\n    #Adaptive exploration vs exploitation\n    exploration_prob = 0.1  # Probability of exploration\n\n    if np.random.rand() < exploration_prob: #Explore by reducing the weight\n        tightness_weight *= (1 - learning_rate)\n        fill_weight *= (1 + learning_rate)\n    else: # Exploit by increasing the weight\n       tightness_weight *= (1 + learning_rate)\n       fill_weight *= (1 - learning_rate)\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus +\n                               large_item_weight * small_space_penalty +\n                               randomness + bin_diversity_reward)\n\n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive weights, strategic randomness, problem-specific objectives by adaptive\n    exploration vs exploitation.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level\n    target_fill_level = 0.8 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.15 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9 * (item/bin_capacity), 0.0)\n\n    # Smaller Item Bonus\n    small_item_threshold = bin_capacity * 0.3\n    if item < small_item_threshold:\n        almost_full_threshold = bin_capacity * 0.08\n        almost_full_bonus = np.exp(-remaining_after / (almost_full_threshold + 0.0001))\n    else:\n        almost_full_bonus = 0.0\n\n    # Larger Item Penalty\n    large_item_threshold = bin_capacity * 0.7\n    if item > large_item_threshold:\n        small_space_penalty = np.where(bins_remain_cap[can_fit] < (item + 0.15 * bin_capacity), -0.8 * (item/bin_capacity), 0.0)\n    else:\n        small_space_penalty = 0.0\n\n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n    remaining_capacity_factor = np.mean(bins_remain_cap[can_fit] / bin_capacity)\n\n    tightness_weight = 0.3 * (1 - item_size_factor) * (1 + utilization_factor) * (1 + remaining_capacity_factor)\n    fill_weight = 0.4 * (1 + item_size_factor) * (1 - utilization_factor) * (1 - remaining_capacity_factor)\n    near_full_weight = 0.15\n    small_item_weight = 0.075\n    large_item_weight = 0.075\n\n    # Strategic Randomness\n    randomness_scale = 0.02 * (1 + item_size_factor) * (1 - utilization_factor) * (1 + remaining_capacity_factor)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    # Learning Rate & bin diversity reward\n    learning_rate = 0.01\n    bin_diversity_reward = (1 / (np.std(bins_remain_cap[can_fit]) + 0.0001)) * learning_rate\n\n    #Adaptive exploration vs exploitation\n    exploration_prob = 0.1  # Probability of exploration\n\n    if np.random.rand() < exploration_prob: #Explore by reducing the weight\n        tightness_weight *= (1 - learning_rate)\n        fill_weight *= (1 + learning_rate)\n    else: # Exploit by increasing the weight\n       tightness_weight *= (1 + learning_rate)\n       fill_weight *= (1 - learning_rate)\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus +\n                               large_item_weight * small_space_penalty +\n                               randomness + bin_diversity_reward)\n\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Enhanced heuristic for online bin packing, focusing on waste minimization,\n    target fill levels, dynamic adjustments, and strategic randomness.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization: Emphasize smaller waste\n    waste = remaining_after\n    tightness = np.exp(-5 * waste / bin_capacity)  # Exponential to favor smaller waste\n\n    # Target Fill Level: Aim for a range, penalize both under and over\n    target_fill_level_low = 0.70 * bin_capacity\n    target_fill_level_high = 0.90 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n\n    fill_score = np.where(\n        fill_level < target_fill_level_low,\n        np.exp(-((fill_level - target_fill_level_low)**2) / (2 * (bin_capacity * 0.1)**2)),  # Gaussian penalty for underfill\n        np.where(\n            fill_level > target_fill_level_high,\n            np.exp(-((fill_level - target_fill_level_high)**2) / (2 * (bin_capacity * 0.1)**2)),  # Gaussian penalty for overfill\n            1.0  # Ideal: within target range\n        )\n    )\n\n    # Dynamic Near-Full Management: Stronger penalty, adaptive threshold\n    near_full_threshold = 0.15 * bin_capacity  # Slightly larger threshold\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9 * (item / bin_capacity), 0.0)  # Stronger penalty\n\n    # Anticipatory Penalty: If remaining space is too small for common items\n    avg_item_size = 0.2 * bin_capacity  # Estimate average item size\n    anticipatory_penalty = np.where(remaining_after < (0.5 * avg_item_size), -0.5, 0.0)  # Penalize if too small\n\n    # Item Size Consideration: Reward fitting larger items well\n    item_fit_score = np.exp(-np.abs(remaining_after - (0.1 * bin_capacity)) / (bin_capacity * 0.2)) if item > (0.5 * bin_capacity) else 0.0\n\n    # Adaptive Weighting: Based on item size and average bin utilization\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n\n    tightness_weight = 0.4 * (1 - item_size_factor) * (1 + utilization_factor)\n    fill_weight = 0.3 * (1 + item_size_factor) * (1 - utilization_factor)\n    near_full_weight = 0.15\n    anticipatory_weight = 0.1\n    item_fit_weight = 0.05\n\n    # Strategic Randomness: Scaled by item size and decreased over time\n    randomness_scale = 0.02 * (1 + item_size_factor) * (1 - utilization_factor)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    # Introduce a decay factor over time (simulated by a global variable or function)\n    # decay_factor = get_decay_factor()  # Retrieve a decay factor that decreases over time\n    # randomness *= decay_factor\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               anticipatory_weight * anticipatory_penalty +\n                               item_fit_weight * item_fit_score +\n                               randomness)\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}