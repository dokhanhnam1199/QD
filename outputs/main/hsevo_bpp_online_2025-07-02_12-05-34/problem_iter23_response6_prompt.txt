{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines adaptive weighting, strategic randomness, and anticipatory penalties.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization (Focus on tightness)\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level (Emphasis on achieving target)\n    target_fill_level = 0.75 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Anticipatory Near-Full Penalty (Stronger penalty, scaled by item size)\n    near_full_threshold = 0.1 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.95 * (item / bin_capacity), 0.0)\n\n    # Adaptive Weighting (Item size & utilization)\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n    learning_rate = 0.1\n\n    tightness_weight = 0.4\n    fill_weight = 0.35\n    near_full_weight = 0.25\n\n    tightness_weight += learning_rate * (1 - item_size_factor) * (1 + utilization_factor) - tightness_weight\n    fill_weight += learning_rate * (1 + item_size_factor) * (1 - utilization_factor) - fill_weight\n\n    # Strategic Randomness (Controlled, decays slower for smaller items)\n    randomness_scale = 0.015 * (1 + item_size_factor) * (1 - utilization_factor)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               randomness)\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines adaptive weights, waste minimization, and strategic randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n    bin_capacity = bins_remain_cap.max()\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization (Best-Fit)\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target fill level\n    target_fill = 0.8 * bin_capacity\n    fill_diff = np.abs(bins_remain_cap[can_fit] - target_fill)\n    fill_priority = np.exp(-fill_diff / bin_capacity)\n\n    # Near-full penalty, scaled by item size\n    nearly_full_threshold = 0.1 * bin_capacity\n    nearly_full_valid_bins = remaining_after < nearly_full_threshold\n    near_full_penalty = np.where(nearly_full_valid_bins, -0.7 * (item / bin_capacity), 0.0)\n\n    # Adaptive Weighting: item size & bin utilization\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n\n    tightness_weight = 0.4 * (1 - item_size_factor) * (1 + utilization_factor)\n    fill_weight = 0.4 * (1 + item_size_factor) * (1 - utilization_factor)\n    near_full_weight = 0.2\n\n    # Decaying Stochasticity:\n    randomness_scale = 0.015 * (1 - item/bin_capacity) * (np.mean(bins_remain_cap[can_fit]) / bin_capacity)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_priority +\n                               near_full_weight * near_full_penalty +\n                               randomness)\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), we see that they are identical.\nComparing (3rd) vs (4th), we see the 3rd has \"Anticipatory Near-Full Penalty (Stronger penalty, scaled by item size)\" and \"Strategic Randomness (Controlled, decays slower for smaller items)\", while the 4th does not. The 4th uses item size and bin utilization in adaptive weighting, while the 3rd only uses item size.\nComparing (2nd worst) vs (worst), we see the worst one uses much more hyperparameter and `scipy`, `random`, `math`, `torch`, which may not be necessary and cause overhead.\nComparing (1st) vs (11th), we see the 1st has many considerations like waste minimization, target fill level, and near-full management, while the 11th is just the start of the function.\nComparing (19th) vs (20th), the 20th enhanced heuristic focuses on a range of target fill levels, using Gaussian penalties for both underfill and overfill, and includes an anticipatory penalty based on average item size and an item fit score. The 19th does not.\nOverall: Top heuristics combine waste minimization, target fill level, dynamic penalties (near full, smaller item bonus, larger item penalty), adaptive weighting (item size, utilization), and strategic randomness. The weights are often made adaptive with learning rate to further enhance the performance. High-performing heuristics often include mechanisms to reduce fragmentation, like an anticipatory penalty or an item fit score. Less effective heuristics either lack key components, use simpler calculations, or fail to adapt to item sizes. Over-parameterization also lead to low performance.\n- \nOkay, let's redefine \"Current self-reflection\" to design better bin packing heuristics, while avoiding the pitfalls of \"Ineffective self-reflection.\" Here's a breakdown:\n\n*   **Keywords:** Adaptive weighting, dynamic penalties/bonuses, target fill, stochasticity, item characteristics, bin state.\n\n*   **Advice:** Focus on *how* to make these elements adaptive and dynamic. Develop clear, measurable metrics for bin utilization and item characteristics to drive adaptation. Implement decaying randomness.\n\n*   **Avoid:** Overly complex rules, focusing solely on individual factors in isolation, and premature optimization (code duplication before concept validation).\n\n*   **Explanation:** Shift from *what* factors to consider (waste minimization, etc.) to *how* they interact dynamically. The key is creating a feedback loop where item properties and bin states influence weighting, penalties, and randomness. Simpler core logic with sophisticated adaptation is preferred.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}