```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Advanced heuristic: adaptive learning rate, dynamic target fill, item fragmentation consideration, bin diversity, and intelligent stochasticity."""
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    can_fit = bins_remain_cap >= item

    if not np.any(can_fit):
        return np.full_like(priorities, -1.0)

    valid_bins = np.where(can_fit)[0]
    remaining_after = bins_remain_cap[can_fit] - item
    bin_capacity = bins_remain_cap.max()
    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity

    # 1. Waste Minimization with Adaptive Learning:
    waste = remaining_after
    inverse_waste = 1 / (waste + 0.0001)
    # Adaptive learning rate based on how full the bins are on average
    learning_rate = np.mean(bins_utilization)
    tightness = inverse_waste * (1 + learning_rate)

    # 2. Dynamic Target Fill Level: Adaptive to Item Size Distribution
    # Dynamically adjust target based on item size; shift it lower if item is big
    item_size_ratio = item / bin_capacity
    dynamic_target_fill = 0.75 * bin_capacity * (1 - 0.2 * item_size_ratio)
    fill_level = bins_remain_cap[can_fit]
    fill_diff = np.abs(fill_level - dynamic_target_fill)
    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))

    # 3. Near-Full Management with Dynamic Penalty:
    near_full_threshold = 0.12 * bin_capacity
    is_near_full = remaining_after < near_full_threshold
    # Increase the penalty if the item is large to discourage creating tiny splits
    near_full_penalty = np.where(is_near_full, -0.9 * (item/bin_capacity) * (1 + item_size_ratio), 0.0)

    # 4. Item Fragmentation Consideration: Encourage larger items in less full bins.
    large_item_threshold = bin_capacity * 0.6
    if item > large_item_threshold:
        fragmentation_penalty = np.where(bins_utilization < 0.5, 0.1, 0.0) * (item / bin_capacity)
    else:
        fragmentation_penalty = 0.0

    # 5. Bin Diversity: Balance usage between bins.
    bin_diversity_bonus = 0.1 * (1 - bins_utilization)

    # 6. Intelligent Stochasticity:
    # Introduce more randomness when bins are similarly filled
    bin_similarity = np.std(bins_utilization)
    randomness_scale = 0.01 * (1 + item_size_ratio) * (1 - bin_similarity)
    randomness = np.random.normal(0, randomness_scale, len(valid_bins))

    # 7. Adaptive Weighting (tuned)
    tightness_weight = 0.40
    fill_weight = 0.30
    near_full_weight = 0.15
    fragmentation_weight = 0.075
    diversity_weight = 0.075

    priorities[valid_bins] = (tightness_weight * tightness +
                               fill_weight * fill_score +
                               near_full_weight * near_full_penalty +
                               fragmentation_weight * fragmentation_penalty +
                               diversity_weight * bin_diversity_bonus +
                               randomness)
    return priorities
```
