{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\nCurrent heuristics:\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines waste minimization, target fill, adaptive weighting,\n    edge case handling, and stochasticity for improved bin packing.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level\n    target_fill_level = 0.75 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Near-Full Penalty\n    near_full_threshold = 0.1 * bin_capacity\n    near_full_penalty = np.where(remaining_after < near_full_threshold, -0.7, 0.0)\n\n    # Small Item Bonus\n    small_item_threshold = bin_capacity * 0.2\n    if item < small_item_threshold:\n        almost_full_threshold = bin_capacity * 0.1\n        almost_full_bonus = np.exp(-remaining_after / (almost_full_threshold + 0.0001))\n    else:\n        almost_full_bonus = 0\n\n    # Large Item Penalty\n    large_item_threshold = bin_capacity * 0.8\n    if item > large_item_threshold:\n        small_space_penalty = np.where(bins_remain_cap[can_fit] < (item + 0.1 * bin_capacity), -0.6, 0.0)\n    else:\n        small_space_penalty = 0.0\n\n    # Adaptive Weighting (Dynamically adjust based on item size and bin states)\n    item_size_factor = item / bin_capacity  # Normalize item size\n    tightness_weight = 0.4 * (1 - item_size_factor) # Smaller items, prioritize tightness\n    fill_weight = 0.3 * (1 + item_size_factor)   # Larger items, prioritize target fill\n    near_full_weight = 0.2\n    small_item_weight = 0.1\n    large_item_weight = 0.1\n\n    # Stochasticity\n    randomness = np.random.normal(0, 0.01, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus +\n                               large_item_weight * small_space_penalty +\n                               randomness)\n\n    return priorities\n\nNow, think outside the box write a mutated function `priority_v2` better than current version.\nYou can use some hints below:\n- \nOkay, I'm ready to refine \"Current self-reflection\" to design better heuristics, focusing on actionable insights and avoiding common pitfalls. Let's aim for that $999K heuristic! Here's my take:\n\n*   **Keywords:** Objective function landscape, adaptive learning, multi-objective optimization, exploration-exploitation balance.\n\n*   **Advice:** Analyze the objective function landscape to inform exploration strategies. Use adaptive learning to dynamically adjust weights and parameters based on feedback during the search. Explicitly define and manage the exploration-exploitation trade-off.\n\n*   **Avoid:** Vague statements about \"fine-tuning\" or \"balancing.\" Don't rely solely on trial-and-error; base adjustments on observed performance and understanding of the problem structure.\n\n*   **Explanation:** Instead of just saying \"tune weights,\" focus on *how* to tune them. Track performance metrics (e.g., waste, fill level, time) and use these to guide weight adjustments. Adaptive learning algorithms (e.g., reinforcement learning) can automate this process. Understanding the objective function landscape helps design more effective exploration strategies (e.g., biased sampling towards promising regions).\n\n\nOutput code only and enclose your code with Python code block: ```python ... ```.\nI'm going to tip $999K for a better solution!"}