[
  {
    "stdout_filepath": "problem_iter23_response0.txt_stdout.txt",
    "code_path": "problem_iter23_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins using waste, target fill, near-full penalty, & adaptive randomness.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return priorities\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n    priorities[valid_bins] += tightness * 0.5\n\n    # Adaptive Target Fill\n    target_fill = 0.8 * bin_capacity - 0.1 * item\n    fill_diff = np.abs(bins_remain_cap[can_fit] - target_fill)\n    fill_priority = np.exp(-fill_diff / bin_capacity)\n    priorities[valid_bins] += fill_priority * 0.4\n\n    # Near-Full Penalty (only for larger items)\n    near_full_threshold = 0.08 * bin_capacity\n    near_full_penalty = np.where((remaining_after < near_full_threshold) & (item > 0.1 * bin_capacity), -0.5, 0.0)\n    priorities[valid_bins] += near_full_penalty\n    \n    #Decaying Randomness\n    item_size_weight = item / bin_capacity\n    randomness = np.random.rand(len(valid_bins)) * 0.02 * (1 - item_size_weight)\n    priorities[valid_bins] += randomness\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 2.8819305943358686,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response1.txt_stdout.txt",
    "code_path": "problem_iter23_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Hybrid heuristic: Adaptive fill target, near-full/small-item bonuses, and decaying randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization (Tightness)\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level (Adaptive)\n    current_utilization = np.mean((bin_capacity - bins_remain_cap) / bin_capacity)\n    target_fill_level = 0.7 + 0.1 * current_utilization\n    fill_diff = np.abs(bins_remain_cap[can_fit] - target_fill_level * bin_capacity)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.12 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.8 * (item / bin_capacity), 0.0)\n\n    # Smaller Item Bonus\n    small_item_threshold = bin_capacity * 0.3\n    if item < small_item_threshold:\n        almost_full_threshold = bin_capacity * 0.08\n        almost_full_bonus = np.exp(-remaining_after / (almost_full_threshold + 0.0001))\n    else:\n        almost_full_bonus = 0.0\n\n    # Adaptive Weighting: Item size and Bin Utilization\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n\n    tightness_weight = 0.3 * (1 - item_size_factor) * (1 + utilization_factor)\n    fill_weight = 0.4 * (1 + item_size_factor) * (1 - utilization_factor)\n    near_full_weight = 0.15\n    small_item_weight = 0.075\n\n    # Strategic Randomness (Decaying)\n    if not hasattr(priority_v2, \"time_step\"):\n        priority_v2.time_step = 1\n    else:\n        priority_v2.time_step += 1\n    exploration_factor = np.exp(-priority_v2.time_step / 1000.0)\n    randomness_scale = 0.01 * (1 + item_size_factor) * exploration_factor\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus+\n                               randomness)\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 3.0215396888711563,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response2.txt_stdout.txt",
    "code_path": "problem_iter23_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines adaptive weights, dynamic penalties, and strategic randomness based on item size.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n    bin_capacity = bins_remain_cap.max()\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level\n    target_fill_level = 0.75 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.1 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9 * (item/bin_capacity), 0.0)\n\n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n    learning_rate = 0.1\n\n    tightness_weight = 0.35\n    fill_weight = 0.35\n    near_full_weight = 0.15\n    random_weight = 0.15\n\n\n    tightness_weight += learning_rate * (1 - item_size_factor) * (1 + utilization_factor) - tightness_weight\n    fill_weight += learning_rate * (1 + item_size_factor) * (1 - utilization_factor) - fill_weight\n\n    # Decaying Randomness based on item size and bin utilization\n    randomness_scale = 0.02 * (1 + item_size_factor) * (1 - utilization_factor)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               random_weight * randomness)\n\n    return priorities",
    "response_id": 2,
    "tryHS": true,
    "obj": 0.8875149581172807,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response3.txt_stdout.txt",
    "code_path": "problem_iter23_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines adaptive fill, waste minimization, and decaying randomness.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return priorities\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n    priorities[valid_bins] += tightness * 0.5\n\n    # Adaptive Target fill level\n    target_fill = 0.75 * bin_capacity\n    fill_diff = np.abs(bins_remain_cap[can_fit] - target_fill)\n    fill_priority = np.exp(-fill_diff / bin_capacity)\n    priorities[valid_bins] += fill_priority * 0.4\n\n    # Near-full penalty\n    nearly_full_threshold = 0.1 * bin_capacity\n    near_full_penalty = np.where(remaining_after < nearly_full_threshold, -0.5 * (item/bin_capacity), 0.0)  #scaled penalty by item size\n    priorities[valid_bins] += near_full_penalty\n\n    # Decaying randomness\n    item_size_weight = item / bin_capacity\n    randomness = np.random.rand(len(valid_bins)) * 0.02 * (1 - item_size_weight) # decay slower for smaller items\n    priorities[valid_bins] += randomness\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 2.602712405265271,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response4.txt_stdout.txt",
    "code_path": "problem_iter23_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, adaptive penalty, and randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return priorities\n\n    valid_bins = np.where(can_fit)[0]\n    bin_capacity = np.max(bins_remain_cap)\n\n    # Waste Minimization\n    waste = bins_remain_cap[can_fit] - item\n    priorities[can_fit] = 1.0 / (1e-6 + waste)\n\n    # Adaptive Near-Full Penalty\n    near_full_threshold = 0.1 * bin_capacity\n    is_near_full = bins_remain_cap[can_fit] < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.8 * (item / bin_capacity), 0.0)\n    priorities[valid_bins] += near_full_penalty\n\n    # Small item bonus\n    small_item_threshold = 0.2 * bin_capacity\n    if item < small_item_threshold:\n      almost_full_threshold = 0.1 * bin_capacity\n      almost_full_valid_bins = bins_remain_cap[can_fit] < almost_full_threshold + item\n      almost_full_indices = np.where(can_fit)[0][almost_full_valid_bins]\n      priorities[almost_full_indices] += 0.2\n\n    # Adaptive Weighting and Randomness (dependent on item size and remaining capacity)\n    item_size_factor = item / bin_capacity\n    randomness_scale = 0.01 * (1 - item_size_factor) * (bins_remain_cap[can_fit] / bin_capacity)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n    priorities[can_fit] += randomness\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.5073793378540135,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response5.txt_stdout.txt",
    "code_path": "problem_iter23_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, target fill, dynamic near-full, adaptive weighting based on item and bin.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization: Prioritize bins that minimize remaining space relative to item size\n    waste_ratio = remaining_after / item\n    waste_priority = np.exp(-5 * waste_ratio)\n\n    # Target Fill Level: Reward bins close to a target fill level\n    target_fill_level = 0.8 * bin_capacity\n    fill_level_diff = np.abs(bins_remain_cap[can_fit] - target_fill_level)\n    fill_level_priority = np.exp(-fill_level_diff / (bin_capacity * 0.2))\n\n    # Near-Full Management: Penalize bins that become very near full\n    near_full_threshold = 0.08 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9, 0.0)\n\n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization) if bins_utilization.size > 0 else 0.0\n\n    waste_weight = 0.3 * (1 - item_size_factor) * (1 + utilization_factor)\n    fill_weight = 0.3 * (1 + item_size_factor) * (1 - utilization_factor)\n    near_full_weight = 0.1\n\n    # Strategic Randomness\n    priority_range = np.max([waste_priority.max(), fill_level_priority.max(), 0.1]) - np.min([waste_priority.min(), fill_level_priority.min(), -0.1]) #Avoid empty sequence\n    randomness_scale = 0.02 * (1 + item_size_factor) * (1 - utilization_factor) * priority_range\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (waste_weight * waste_priority +\n                               fill_weight * fill_level_priority +\n                               near_full_weight * near_full_penalty +\n                               randomness)\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 24.54128440366974,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response6.txt_stdout.txt",
    "code_path": "problem_iter23_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive heuristic combining waste minimization, target fill, and stochasticity.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n    bin_capacity = bins_remain_cap.max()\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization (Best-Fit)\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target fill level\n    target_fill = 0.75 * bin_capacity\n    fill_diff = np.abs(bins_remain_cap[can_fit] - target_fill)\n    fill_priority = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Near-full penalty, scaled by item size\n    nearly_full_threshold = 0.1 * bin_capacity\n    nearly_full_valid_bins = remaining_after < nearly_full_threshold\n    near_full_penalty = np.where(nearly_full_valid_bins, -0.95 * (item / bin_capacity), 0.0)\n\n    # Adaptive Weighting: item size & bin utilization, with learning\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n    learning_rate = 0.1\n\n    tightness_weight = 0.4\n    fill_weight = 0.35\n    near_full_weight = 0.25\n\n    tightness_weight += learning_rate * (1 - item_size_factor) * (1 + utilization_factor) - tightness_weight\n    fill_weight += learning_rate * (1 + item_size_factor) * (1 - utilization_factor) - fill_weight\n\n\n    # Strategic Randomness: Decays slower for smaller items, adjusts to utilization\n    randomness_scale = 0.015 * (1 + item_size_factor) * (1 - utilization_factor)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_priority +\n                               near_full_weight * near_full_penalty +\n                               randomness)\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 1.1368169126446042,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response7.txt_stdout.txt",
    "code_path": "problem_iter23_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines target fill, dynamic penalties, and adaptive weighting.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n    bin_capacity = bins_remain_cap.max()\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    \n    # Target Fill Level\n    target_fill_level = 0.8 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n    \n    # Dynamic Near-Full Management\n    near_full_threshold = 0.15 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9 * (item/bin_capacity), 0.0)\n    \n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean((bin_capacity - bins_remain_cap[can_fit]) / bin_capacity) # utilization factor\n    tightness = 1 / (remaining_after + 0.0001) #waste minization\n    \n    tightness_weight = 0.3 * (1 - item_size_factor) * (1 + utilization_factor)\n    fill_weight = 0.4 * (1 + item_size_factor) * (1 - utilization_factor)\n    near_full_weight = 0.15\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty)\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 2.8220981252492976,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response8.txt_stdout.txt",
    "code_path": "problem_iter23_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines target fill, waste minimization, and adaptive penalty.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n    bin_capacity = bins_remain_cap.max()\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n\n    # Waste Minimization: Prioritize bins with smaller remaining space.\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level: Encourage bins to reach a target fill level.\n    target_fill_level = 0.8 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Adaptive Penalty for Near-Full Bins: Discourage filling bins that are already near full.\n    near_full_threshold = 0.15 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9 * (item / bin_capacity), 0.0)\n\n    # Combine Scores with Weights\n    tightness_weight = 0.4\n    fill_weight = 0.4\n    near_full_weight = 0.2\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty)\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 1.0869565217391397,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response9.txt_stdout.txt",
    "code_path": "problem_iter23_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines waste minimization, target fill, and dynamic adjustments with strategic randomness.\n    Adaptive weights respond to item size and bin utilization.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = np.exp(-5 * waste / bin_capacity)\n\n    # Target Fill Level\n    target_fill_level_low = 0.70 * bin_capacity\n    target_fill_level_high = 0.90 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n\n    fill_score = np.where(\n        fill_level < target_fill_level_low,\n        np.exp(-((fill_level - target_fill_level_low)**2) / (2 * (bin_capacity * 0.1)**2)),\n        np.where(\n            fill_level > target_fill_level_high,\n            np.exp(-((fill_level - target_fill_level_high)**2) / (2 * (bin_capacity * 0.1)**2)),\n            1.0\n        )\n    )\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.15 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9 * (item / bin_capacity), 0.0)\n\n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n\n    tightness_weight = 0.4 * (1 - item_size_factor) * (1 + utilization_factor)\n    fill_weight = 0.3 * (1 + item_size_factor) * (1 - utilization_factor)\n    near_full_weight = 0.15\n\n    # Strategic Randomness\n    randomness_scale = 0.02 * (1 + item_size_factor) * (1 - utilization_factor)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               randomness)\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 36.37814120462704,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter24_response0.txt_stdout.txt",
    "code_path": "problem_iter24_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive heuristic with dynamic weighting, decaying randomness, and bin/item characteristics.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization with exponential scaling\n    waste = remaining_after\n    tightness = np.exp(-waste / (bin_capacity * 0.1))\n\n    # Target Fill Level with adaptive target\n    current_avg_fill = np.mean((bin_capacity - bins_remain_cap) / bin_capacity)\n    target_fill_level = 0.75 * bin_capacity  # Base target\n    target_fill_level += 0.1 * bin_capacity * (current_avg_fill - 0.5)  # Adjust based on average fill\n    target_fill_level = np.clip(target_fill_level, 0.5 * bin_capacity, 0.9 * bin_capacity) # Keep target within bounds\n\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Dynamic Near-Full Management with Item-Size aware penalty\n    near_full_threshold = 0.12 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.8 * (item / bin_capacity) * (1 - bins_utilization), 0.0)\n\n    # Smaller Item Bonus adjusted by remaining capacity\n    small_item_threshold = bin_capacity * 0.25\n    if item < small_item_threshold:\n        almost_full_threshold = bin_capacity * 0.1\n        almost_full_bonus = np.exp(-remaining_after / (almost_full_threshold + 0.0001)) * (1 - (remaining_after/bin_capacity)) # Scale bonus by remaining capacity\n    else:\n        almost_full_bonus = 0.0\n\n    # Larger Item Penalty adjusted by bin utilization\n    large_item_threshold = bin_capacity * 0.75\n    if item > large_item_threshold:\n        small_space_penalty = np.where(bins_remain_cap[can_fit] < (item + 0.1 * bin_capacity), -0.7 * (item / bin_capacity) * (1 + bins_utilization), 0.0)\n    else:\n        small_space_penalty = 0.0\n\n    # Adaptive Weighting: item size & bin utilization\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n\n    tightness_weight = 0.35 * (1 - item_size_factor) * (1 + utilization_factor)\n    fill_weight = 0.35 * (1 + item_size_factor) * (1 - utilization_factor)\n    near_full_weight = 0.15\n    small_item_weight = 0.075\n    large_item_weight = 0.075\n\n    # Decaying Stochasticity\n    decay_rate = 0.995  # Adjust decay rate as needed\n    randomness_scale = 0.015 * (1 + item_size_factor) * (1 - utilization_factor)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n    randomness *= (decay_rate ** len(bins_remain_cap)) # Reduce randomness over time or number of items\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus +\n                               large_item_weight * small_space_penalty +\n                               randomness)\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 3.5500598324691004,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter24_response1.txt_stdout.txt",
    "code_path": "problem_iter24_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Enhanced heuristic with adaptive weighting, dynamic penalties/bonuses,\n    target fill, decaying stochasticity, and item/bin state awareness.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # 1. Waste Minimization with Adaptive Tightness\n    waste = remaining_after\n    base_tightness = 1 / (waste + 0.0001)\n    # Adapt tightness based on item size: smaller items benefit more from tight fits.\n    item_size_factor = item / bin_capacity\n    tightness = base_tightness * (1 + (1 - item_size_factor))  # Boost tightness for small items\n\n\n    # 2. Target Fill Level with Dynamic Target\n    # Adjust target fill based on average bin utilization: if bins are filling up, aim higher.\n    avg_utilization = np.mean(bins_utilization)\n    target_fill_level = (0.65 + 0.2 * avg_utilization) * bin_capacity #was 0.75\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # 3. Dynamic Near-Full Management\n    near_full_threshold = 0.12 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    # Adaptive penalty: larger penalty if the item is also large relative to bin capacity\n    near_full_penalty = np.where(is_near_full, -0.7 * (item/bin_capacity) * (1+bins_utilization), 0.0)\n\n    # 4. Smaller Item Bonus (if item fits nearly perfectly into a near-full bin)\n    small_item_threshold = bin_capacity * 0.25\n    if item < small_item_threshold:\n        almost_full_threshold = bin_capacity * 0.1\n        almost_full_bonus = np.exp(-remaining_after / (almost_full_threshold + 0.0001))\n    else:\n        almost_full_bonus = 0.0\n\n    # 5. Larger Item Penalty (if placing it leaves very little space)\n    large_item_threshold = bin_capacity * 0.75\n    if item > large_item_threshold:\n        small_space_penalty = np.where(bins_remain_cap[can_fit] < (item + 0.1 * bin_capacity), -0.6 * (item/bin_capacity), 0.0) #was 0.7\n    else:\n        small_space_penalty = 0.0\n\n    # 6. Adaptive Weighting: item size & bin utilization\n    #   - Emphasis on tightness when item is small and bins aren't fully utilized\n    #   - Emphasis on fill when item is large or bins are highly utilized\n    tightness_weight = 0.35 * (1.2 - item_size_factor) * (1.1 - avg_utilization) #was (1 - item_size_factor) * (1 + utilization_factor)\n    fill_weight = 0.35 * (0.8 + item_size_factor) * (0.9 + avg_utilization) #was (1 + item_size_factor) * (1 - utilization_factor)\n    near_full_weight = 0.15\n    small_item_weight = 0.075\n    large_item_weight = 0.075\n\n    # 7. Decaying Stochasticity\n    #   - Reduce randomness as bins fill up to stabilize the packing\n    randomness_scale = 0.015 * (1 + item_size_factor) * (1 - avg_utilization) #was 0.015 * (1 + item_size_factor) * (1 - utilization_factor)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus +\n                               large_item_weight * small_space_penalty +\n                               randomness)\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 2.961707219784608,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter24_response2.txt_stdout.txt",
    "code_path": "problem_iter24_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Advanced heuristic with adaptive weighting, dynamic penalties, and controlled randomness.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # 1. Waste Minimization with Adaptive Tightness\n    waste = remaining_after\n    # Use a more aggressive scaling for tightness.  Smaller waste is *much* better.\n    tightness = 1 / (waste**0.75 + 0.0001)\n\n    # 2. Target Fill Level with Dynamic Adjustment\n    target_fill_level = 0.75 * bin_capacity  #Initial target\n    # Adapt target fill based on average bin utilization\n    target_fill_level += 0.1 * bin_capacity * (np.mean(bins_utilization) - 0.5)  #Adjust target\n    target_fill_level = np.clip(target_fill_level, 0.5 * bin_capacity, 0.9 * bin_capacity) # Clamp target\n\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # 3. Dynamic Near-Full Management\n    near_full_threshold = 0.12 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9 * (item/bin_capacity), 0.0)  #Stronger Penalty\n\n    # 4. Item Size Considerations\n    small_item_threshold = bin_capacity * 0.25\n    large_item_threshold = bin_capacity * 0.75\n    almost_full_threshold = bin_capacity * 0.1\n\n    almost_full_bonus = np.where((item < small_item_threshold) & (remaining_after < almost_full_threshold),\n                                  np.exp(-remaining_after / (almost_full_threshold / 2 + 0.0001)), 0.0)  #More aggressive bonus\n\n    small_space_penalty = np.where((item > large_item_threshold) & (bins_remain_cap[can_fit] < (item + 0.05 * bin_capacity)),\n                                    -0.8 * (item/bin_capacity), 0.0) #Stricter penalty if item almost doesnt fit\n\n    # 5. Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n\n    # Weights that adapt based on item size and bin utilization\n    tightness_weight = 0.4 * (1 - item_size_factor) * (1 + utilization_factor)\n    fill_weight = 0.4 * (1 + item_size_factor) * (1 - utilization_factor)\n    near_full_weight = 0.12\n    small_item_weight = 0.04\n    large_item_weight = 0.04\n\n    # 6. Controlled Randomness\n    # Decaying randomness: more randomness early on, less later\n    randomness_scale = 0.02 * (1 + item_size_factor) * (1 - utilization_factor)\n    # Simulate \"cooling\" - reduce randomness as bins fill\n    cooling_factor = np.exp(-utilization_factor * 5)  # Adjust exponent as needed\n    randomness_scale *= cooling_factor\n\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus +\n                               large_item_weight * small_space_penalty +\n                               randomness)\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 1.5456721180694057,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter24_response3.txt_stdout.txt",
    "code_path": "problem_iter24_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Enhanced heuristic with adaptive elements, dynamic adjustments based on item size\n    and bin state, and refined stochasticity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization (Tightness)\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level\n    target_fill_level = 0.8 * bin_capacity  # Slightly higher target\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.15 * bin_capacity  # Adjusted threshold\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.7 * (item/bin_capacity), 0.0)  # Reduced penalty\n\n    # Item Size Considerations\n    item_size_ratio = item / bin_capacity\n\n    # Smaller Item Bonus (if item fits nearly perfectly into a near-full bin)\n    small_item_threshold = bin_capacity * 0.2\n    almost_full_threshold = bin_capacity * 0.08\n    almost_full_bonus = np.where((item < small_item_threshold) & is_near_full,\n                                  np.exp(-remaining_after / (almost_full_threshold + 0.0001)),\n                                  0.0)\n\n    # Larger Item Penalty (if placing it leaves very little space)\n    large_item_threshold = bin_capacity * 0.7\n    small_space_penalty = np.where((item > large_item_threshold) & (bins_remain_cap[can_fit] < (item + 0.1 * bin_capacity)),\n                                     -0.6 * (item/bin_capacity),  # Reduced penalty\n                                     0.0)\n\n    # Adaptive Weighting: Focus on Item Size and Bin Utilization\n    utilization_level = np.mean(bins_utilization)  # Overall utilization\n    tightness_weight = 0.4 * (1 - item_size_ratio) * (1 + utilization_level)\n    fill_weight = 0.4 * (1 + item_size_ratio) * (1 - utilization_level)\n    near_full_weight = 0.10\n    small_item_weight = 0.05\n    large_item_weight = 0.05\n\n    # Adaptive Stochasticity (Decaying)\n    randomness_scale = 0.02 * (1 + item_size_ratio) * (1 - utilization_level)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus +\n                               large_item_weight * small_space_penalty +\n                               randomness)\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 3.181092939768657,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter24_response4.txt_stdout.txt",
    "code_path": "problem_iter24_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Enhanced heuristic: adaptive weights, dynamic near-full, item size considerations, and decaying randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level (Adaptive)\n    target_fill_level_base = 0.75\n    item_size_ratio = item / bin_capacity\n    # Adjust target fill based on item size: smaller items -> higher target\n    target_fill_level = (target_fill_level_base + 0.15 * (1 - item_size_ratio)) * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Dynamic Near-Full Management (Adaptive Threshold)\n    near_full_threshold_base = 0.12\n    # Adjust threshold based on bin utilization: more utilized bins -> smaller threshold\n    near_full_threshold = (near_full_threshold_base - 0.05 * np.mean(bins_utilization)) * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.8 * (item/bin_capacity), 0.0)\n\n    # Smaller Item Bonus (if item fits nearly perfectly into a near-full bin)\n    small_item_threshold = bin_capacity * 0.25\n    if item < small_item_threshold:\n        almost_full_threshold = bin_capacity * 0.1\n        almost_full_bonus = np.exp(-remaining_after / (almost_full_threshold + 0.0001))\n    else:\n        almost_full_bonus = 0.0\n\n    # Larger Item Penalty (if placing it leaves very little space)\n    large_item_threshold = bin_capacity * 0.75\n    if item > large_item_threshold:\n        small_space_penalty = np.where(bins_remain_cap[can_fit] < (item + 0.1 * bin_capacity), -0.7 * (item/bin_capacity), 0.0)\n    else:\n        small_space_penalty = 0.0\n\n    # Adaptive Weighting: item size & bin utilization\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n\n    tightness_weight = 0.35 * (1 - item_size_factor) * (1 + utilization_factor)\n    fill_weight = 0.35 * (1 + item_size_factor) * (1 - utilization_factor)\n    near_full_weight = 0.15\n    small_item_weight = 0.075\n    large_item_weight = 0.075\n\n    # Decaying Stochasticity (reduces over time, encouraging convergence)\n    #Simulate time by using the average bin utilization\n    time_decay = max(0.0, 1.0 - utilization_factor) #Clamped between 0 and 1\n    randomness_scale = 0.015 * (1 + item_size_factor) * (1 - utilization_factor) * time_decay\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus +\n                               large_item_weight * small_space_penalty +\n                               randomness)\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 15.047865975269255,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter25_response0.txt_stdout.txt",
    "code_path": "problem_iter25_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                target_fill_level_ratio: float = 0.9221462247756516,\n                fill_level_deviation_ratio: float = 0.1713448674075636,\n                near_full_threshold_ratio: float = 0.16803777971854475,\n                near_full_penalty_scale: float = 0.8607925481753331,\n                learning_rate: float = 0.18698329902302885,\n                tightness_weight: float = 0.3502576865020792,\n                fill_weight: float = 0.34546294713703984,\n                near_full_weight: float = 0.18439088964679867,\n                random_weight: float = 0.19313490230268088,\n                randomness_scale_factor: float = 0.018529630773417054,\n                waste_epsilon: float = 0.0008545898773918969) -> np.ndarray:\n    \"\"\"Combines adaptive weights, dynamic penalties, and strategic randomness based on item size.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n    bin_capacity = bins_remain_cap.max()\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.487435181491823,
    "exec_success": true
  }
]