```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Enhanced heuristic: adaptive weights, dynamic near-full, item size considerations, and randomness."""
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    can_fit = bins_remain_cap >= item

    if not np.any(can_fit):
        return np.full_like(priorities, -1.0)

    valid_bins = np.where(can_fit)[0]
    remaining_after = bins_remain_cap[can_fit] - item
    bin_capacity = bins_remain_cap.max()
    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity

    # Waste Minimization: Prioritize bins that minimize remaining space relative to item size
    waste_ratio = remaining_after / item
    waste_priority = np.exp(-5 * waste_ratio)  # Exponentially decreasing priority with increasing waste

    # Target Fill Level: Reward bins close to a target fill level, but adapt the target
    # based on item size.  Smaller items shift the target higher.
    target_fill_level = 0.8 * bin_capacity - 0.1 * item
    fill_level_diff = np.abs(bins_remain_cap[can_fit] - target_fill_level)
    fill_level_priority = np.exp(-fill_level_diff / (bin_capacity * 0.2))

    # Near-Full Management: Penalize bins that become *very* near full, but only if the
    # item isn't tiny.  Tiny items can top off bins.
    near_full_threshold = 0.08 * bin_capacity
    is_near_full = remaining_after < near_full_threshold
    near_full_penalty = np.where(is_near_full & (item > 0.1 * bin_capacity), -0.9, 0.0)

    # Small Item Bonus: Significantly reward bins that can be nearly filled by small items.
    small_item_threshold = 0.2 * bin_capacity
    if item < small_item_threshold:
        almost_full_threshold = 0.05 * bin_capacity
        almost_full_bonus = np.exp(-remaining_after / (almost_full_threshold + 0.0001))
    else:
        almost_full_bonus = 0.0

    # Large Item Penalty: Penalize bins where a large item will leave very little space,
    # especially if that space is smaller than the average item size seen so far.
    large_item_threshold = 0.7 * bin_capacity
    if item > large_item_threshold:
        small_space_threshold = 0.15 * bin_capacity #was 0.1
        small_space_penalty = np.where(remaining_after < small_space_threshold, -0.8, 0.0)
    else:
        small_space_penalty = 0.0

    # Anticipatory Penalty: Penalize bins that will lead to highly fragmented space
    # (multiple small spaces).  This requires a simplified simulation.
    fragmentation_penalty = np.zeros_like(remaining_after)

    # Adaptive Weighting
    item_size_factor = item / bin_capacity
    utilization_factor = np.mean(bins_utilization) if bins_utilization.size > 0 else 0.0

    waste_weight = 0.3 * (1 - item_size_factor) * (1 + utilization_factor)
    fill_weight = 0.3 * (1 + item_size_factor) * (1 - utilization_factor)
    near_full_weight = 0.1
    small_item_weight = 0.15
    large_item_weight = 0.15

    # Strategic Randomness:  Introduce randomness proportional to the *uncertainty*
    # in the current state (high uncertainty when bins are similar, low when
    # one bin is clearly better).
    priority_range = np.max([waste_priority.max(), fill_level_priority.max(), 0.1]) - np.min([waste_priority.min(), fill_level_priority.min(), -0.1]) #Avoid empty sequence
    randomness_scale = 0.02 * (1 + item_size_factor) * (1 - utilization_factor) * priority_range
    randomness = np.random.normal(0, randomness_scale, len(valid_bins))

    priorities[valid_bins] = (waste_weight * waste_priority +
                               fill_weight * fill_level_priority +
                               near_full_weight * near_full_penalty +
                               small_item_weight * almost_full_bonus +
                               large_item_weight * small_space_penalty +
                               randomness +
                               fragmentation_penalty)

    return priorities
```
