{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines waste minimization, target fill, dynamic penalties, and adaptive randomness.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return priorities\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    max_bin_cap = bins_remain_cap.max()\n    bins_utilization = (max_bin_cap - bins_remain_cap[can_fit]) / max_bin_cap\n\n    # Waste minimization\n    waste = remaining_after\n    tightness = 1.0 / (1e-6 + waste)\n    tightness_weight = 0.4\n\n    # Target fill level\n    target_fill = 0.75 * max_bin_cap\n    fill_diff = np.abs(bins_remain_cap[can_fit] - target_fill)\n    fill_priority = np.exp(-fill_diff / max_bin_cap)\n    fill_weight = 0.3\n\n    # Near-full penalty and near-empty bonus\n    nearly_full_threshold = 0.1 * max_bin_cap\n    nearly_empty_threshold = 0.9 * max_bin_cap\n    near_full_penalty = np.where(remaining_after < nearly_full_threshold, -0.5, 0.0)\n    near_empty_bonus = np.where(bins_remain_cap[can_fit] > nearly_empty_threshold, 0.1, 0.0)\n    penalty_weight = 0.2\n\n    # Item size consideration (adaptive randomness)\n    item_size_weight = item / max_bin_cap\n    randomness_scale = 0.01 * (1 - item_size_weight)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n    random_weight = 0.1\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                              fill_weight * fill_priority +\n                              penalty_weight * (near_full_penalty + near_empty_bonus) +\n                              random_weight * randomness)\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines adaptive weighting and dynamic elements with decaying randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit = bins_remain_cap >= item\n\n    if not np.any(can_fit):\n        return np.full_like(priorities, -1.0)\n\n    valid_bins = np.where(can_fit)[0]\n    remaining_after = bins_remain_cap[can_fit] - item\n    bin_capacity = bins_remain_cap.max()\n    bins_utilization = (bin_capacity - bins_remain_cap[can_fit]) / bin_capacity\n\n    # Waste Minimization\n    waste = remaining_after\n    tightness = 1 / (waste + 0.0001)\n\n    # Target Fill Level\n    target_fill_level = 0.8 * bin_capacity\n    fill_level = bins_remain_cap[can_fit]\n    fill_diff = np.abs(fill_level - target_fill_level)\n    fill_score = np.exp(-fill_diff / (bin_capacity * 0.2))\n\n    # Dynamic Near-Full Management\n    near_full_threshold = 0.1 * bin_capacity\n    is_near_full = remaining_after < near_full_threshold\n    near_full_penalty = np.where(is_near_full, -0.9 * (item/bin_capacity), 0.0)\n\n    # Smaller Item Bonus\n    small_item_threshold = bin_capacity * 0.2\n    if item < small_item_threshold:\n        almost_full_threshold = bin_capacity * 0.08\n        almost_full_bonus = np.exp(-remaining_after / (almost_full_threshold + 0.0001))\n    else:\n        almost_full_bonus = 0.0\n\n    # Larger Item Penalty\n    large_item_threshold = bin_capacity * 0.8\n    if item > large_item_threshold:\n        small_space_penalty = np.where(bins_remain_cap[can_fit] < (item + 0.05 * bin_capacity), -0.8 * (item/bin_capacity), 0.0)\n    else:\n        small_space_penalty = 0.0\n\n    # Adaptive Weighting\n    item_size_factor = item / bin_capacity\n    utilization_factor = np.mean(bins_utilization)\n    learning_rate = 0.1\n\n    tightness_weight = 0.35\n    fill_weight = 0.35\n    near_full_weight = 0.15\n    small_item_weight = 0.075\n    large_item_weight = 0.075\n\n    tightness_weight += learning_rate * (1 - item_size_factor) * (1 + utilization_factor) - tightness_weight\n    fill_weight += learning_rate * (1 + item_size_factor) * (1 - utilization_factor) - fill_weight\n\n    # Decaying Randomness based on item size and bin utilization\n    randomness_scale = 0.02 * (1 + item_size_factor) * (1 - utilization_factor)\n    randomness = np.random.normal(0, randomness_scale, len(valid_bins))\n\n    priorities[valid_bins] = (tightness_weight * tightness +\n                               fill_weight * fill_score +\n                               near_full_weight * near_full_penalty +\n                               small_item_weight * almost_full_bonus +\n                               large_item_weight * small_space_penalty +\n                               randomness)\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the 1st includes a waste minimization component (`tightness = 1 / (waste + 0.0001)`) and adaptive weighting based on item size and bin utilization, whereas the 20th focuses on target fill, near-empty bonus, and decaying randomness.\n(2nd best) vs (19th) is almost no different.\nComparing (1st) vs (2nd), there is almost no difference.\n(3rd) vs (4th), there is almost no difference.\nComparing (second worst) vs (worst), there is almost no difference.\nComparing (1st) vs (10th), 1st considers bin utilization in adaptive weighting, 10th does not.\nComparing (14th) vs (15th), there is almost no difference.\nComparing (16th) vs (17th), there is almost no difference.\nOverall: The better heuristics incorporate waste minimization, adaptive weighting that considers both item size and bin utilization, and dynamic management of near-full bins, while the less effective heuristics rely more on target fill levels and near-empty bonuses without considering bin utilization. Some good heuristics also have bin diversity rewards and a learning rate.\n- \nOkay, let's refine \"Current Self-Reflection\" into actionable steps for heuristic design, avoiding the pitfalls of \"Ineffective Self-Reflection\". Here's a step-by-step approach, followed by the requested summary:\n\n**Step 1: Problem-Specific Objective Definition:**\n\n*   Instead of generally minimizing waste, define specific waste metrics relevant to the problem. Is it about minimizing the *number* of bins? Or minimizing *volume* of unused space? Or minimizing *cost* of unused space (where different regions of a bin might have different costs associated with them)?\n*   Target Fill Level: What *specifically* are we targeting? Are we trying to achieve a specific average fill percentage across all bins? Are we aiming for each bin to reach at least X% full *before* opening a new bin?\n\n**Step 2: Adaptive Weighting Mechanisms:**\n\n*   **Beyond Item Size/Bin State:** What other contextual factors matter? Item shape, item type, item priority, relationships between items (e.g., some items *must* be packed together).\n*   **Weight Adjustment Rules:** How do we *dynamically adjust* the weights?  Base it on the *success* (or failure) of previous packing attempts. If a particular item-size grouping consistently leads to waste, *increase* the weight of the waste minimization objective for that grouping.  Use reinforcement learning concepts to dynamically tune weights based on reward signals.\n*   **Exploration vs. Exploitation Bias:** Make weight adjustments explicitly consider the exploration/exploitation trade-off.  Early in the search, favor weights that encourage exploring less-utilized bin-selection strategies. As the search progresses, shift towards weights that exploit currently successful strategies.\n\n**Step 3: Strategic Randomness Injection:**\n\n*   **Controlled Randomness:**  Don't just blindly inject randomness.  Introduce randomness *strategically*.  For example:\n    *   **Randomized Tie-Breaking:**  When multiple bins are equally good candidates based on current weighting, randomly select one.\n    *   **Perturbation of Scores:**  Add small random perturbations to the scores of candidate bins to slightly alter the ranking.\n    *   **Randomized Initial Placement:** The very first item could be placed randomly to jumpstart the optimization with different seeds.\n*   **Adaptive Randomness Decay:**  The *rate* of decay should be problem-dependent and, ideally, *adaptive*.  If the heuristic consistently gets stuck in local optima, *slow down* the decay rate. Use simulated annealing concepts.\n\n**Step 4: Penalty/Bonus Design Refinement:**\n\n*   **Granularity:**  Penalties/bonuses should be applied with appropriate granularity.  Penalizing \"near-full\" bins might be too coarse. Instead, penalize based on the *specific amount* of remaining space and the *suitability* of that space for future items.\n*   **Anticipatory Penalties:** Don't just react to the current state. Anticipate *future* consequences. Penalize a bin selection if it will leave a small, unusable space that is unsuitable for future items, *even if* it seems like a good fit for the current item.\n\n**Step 5: Benchmarking and Evaluation:**\n\n*   **Diverse Datasets:** Use a wide variety of bin packing instances to evaluate the heuristic. These instances should vary in item sizes, item distributions, constraints, etc.\n*   **Performance Metrics:** Beyond just waste minimization, measure runtime, solution consistency, and adaptability to different instance types.\n*   **Ablation Studies:** Systematically remove components of the heuristic (e.g., adaptive weighting, randomness injection) to understand their individual contributions.\n*   **Visualizations:** Visualize the bin packing process. This will expose patterns of bin utilization and waste that may not be apparent in numerical data.\n\n**Step 6: Implementation Details**\n*   **Vectorization:** Vectorized operations are good, but profile your code before optimizing. Sometimes, simpler loops are actually faster, especially if vectorized operations lead to many conditional branches within the vectorized code.\n*   **Data Structures:** Choose the right data structures. For example, use a priority queue for selecting which bin to put the next item into.\n*   **Code Structure:** Even if the solution looks simple, structure the code to easily test new combinations of the ideas.\n*   **Parallelization:** Explore simple ways to execute the algorithm in parallel, like running with different seeds and adaptive weighting.\n\nHere's the breakdown in bullet points:\n\n*   **Keywords:** Adaptive weighting, strategic randomness, problem-specific objectives, anticipatory penalties, dynamic adjustment, solution consistency.\n*   **Advice:** Define specific objectives, use adaptive weighting based on problem success, inject randomness strategically, anticipate future consequences, and benchmark extensively across diverse datasets. Fine-tune the rate of exploration vs exploitation depending on solution consistency.\n*   **Avoid:** Overly general rules, blind randomness, solely focusing on current state, ignoring runtime considerations, unnecessary code duplication.\n*   **Explanation:** Move beyond generic \"waste minimization\" by specifying what kind of waste matters. Use adaptive weights that adjust based on the success of previous packing. Introduce randomness in a controlled manner, not randomly. Penalize decisions that create future problems, and rigorously test all aspects of your design across a variety of cases.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}