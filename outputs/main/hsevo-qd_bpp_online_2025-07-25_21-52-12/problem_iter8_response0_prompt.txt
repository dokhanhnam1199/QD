{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function employs a dynamic adaptation strategy that considers both the remaining capacity and the penalizes \n    large leftover space logarithmically to avoid overly simplistic linear penalties. It also introduces a nuanced \n    strategy that rewards bins with a balance between remaining capacity and potential for efficient future packings.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Ensure that the item can actually fit into the bin\n    can_fit = bins_remain_cap >= item\n    space_left = bins_remain_cap - item\n    \n    # Logistic balance to penalize overly large empty spaces, but gracefully\n    # This avoids the rigidity of linear penalties and allows for a smoother decision-making process\n    penalty_factor = 1 - np.log2(1 + space_left / (bins_remain_cap + 1e-9)) / np.log2(2)\n    \n    # Reward for bins that have efficiently packed, introducing a balance between current fit and future potential\n    efficiency_reward = np.exp(-((bins_remain_cap - item) / bins_remain_cap) ** 2)\n    \n    # Combining penalties and rewards dynamically\n    priority = can_fit * (penalty_factor + efficiency_reward)\n    \n    return priority\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Balanced priority combining remaining capacity, leftover space, and closeness to item size.\"\"\"\n    can_fit = bins_remain_cap >= item\n    space_left = bins_remain_cap - item\n    priority_fit = can_fit * (1 - (space_left / bins_remain_cap))\n    close_fit = np.clip(bins_remain_cap - item, 0, 1)\n    priority = priority_fit + 0.3 * close_fit\n    return priority\n\n### Analyze & experience\n- Comparing (best) Heuristics 1st vs (worst) Heuristics 20th, we see that the best uses a nuanced combination of penalties for leftover space (penalized logarithmically) and rewards for balanced packing, whereas the worst uses a simple combination of leftover space and a sinusoidal function that isn't as adaptive. (Second best) Heuristics 2nd vs (second worst) Heuristics 19th, we see that the second best dynamically weighs penalties and rewards without clipping, whereas the second worst redundantly calculates similar metrics across multiple heuristics with clipping and normalization. (1st) vs (2nd), we see the importance of non-linear penalties over simple linear ones. (3rd) vs (4th), we see that random imports and hardcoded parameters in (3rd) do not add value. (Second worst) vs (worst), we see that maintaining simplicity and avoiding unnecessary computations can improve performance.\n\nOverall: The best heuristics incorporate dynamic, non-linear penalties and rewards, maintaining efficiency and effectiveness.\n- \n- **Keywords**: Dynamic adaptive penalties, non-linear functions, incremental learning, context-aware adjustments\n- **Advice**: Leverage machine learning for incremental updates to penalties and rewards. Employ context-aware adjustments to consider varying conditions in the decision-making process.\n- **Avoid**: Rigid penalty/reward setups, static weighting, excessive dependency on historical data, ignoring real-time changes in the environment.\n- **Explanation**: Incorporating machine learning allows for continuous improvement and adaptation based on new data. Context-aware adjustments ensure that the heuristic remains effective under different conditions and avoids relying on outdated assumptions or overly simplistic models.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}