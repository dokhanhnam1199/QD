```python
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a hybrid strategy.

    This strategy aims to balance the preference for tight fits (minimizing wasted space
    in the immediate bin) with a slight preference for bins that offer a bit more
    room, promoting better global packing. It uses a combination of a "Best Fit"
    (minimal slack) component and a "First Fit"-like component (favoring bins
    that are not too empty).

    The priority is calculated based on the negative slack (prioritizing bins
    where `bins_remain_cap - item` is small and positive). To encourage exploration
    and prevent premature filling of slightly-too-small gaps that might be better
    suited for smaller items, a small penalty is applied to bins with extremely
    tight fits, and a bonus is given to bins with a moderate amount of slack.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Returns:
        Array of same size as bins_remain_cap with priority score of each bin.
        Bins that cannot fit the item will have a priority of 0.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins that can fit the item
    fittable_bins_mask = bins_remain_cap >= item

    if np.any(fittable_bins_mask):
        fittable_bins_capacities = bins_remain_cap[fittable_bins_mask]

        # Calculate slack: remaining capacity after fitting the item.
        # We want slack to be as close to 0 as possible.
        slack = fittable_bins_capacities - item

        # Heuristic:
        # 1. Prioritize bins with small positive slack (tight fits).
        # 2. Avoid extremely tight fits (slack very close to 0) by slightly penalizing them.
        # 3. Give a small bonus to bins with moderate slack to encourage using "less full" bins sometimes.
        # This can be achieved by mapping slack to a score where a small positive value is optimal.

        # Using a quadratic function: -(slack - ideal_slack)^2
        # Let's aim for an "ideal" slack of, say, 10% of the item size,
        # but with a floor to avoid negative ideal slack for very small items.
        ideal_slack = max(0.1 * item, 0.1) # Aim for ~10% of item size, minimum 0.1
        
        # Penalize deviations from the ideal slack.
        # A higher value means closer to ideal_slack is better.
        # We want to maximize this value.
        # The function `-(slack - ideal_slack)**2` will be maximized when slack is close to ideal_slack.
        # However, we also want to prioritize *tight* fits, meaning slack near 0 should be good.
        
        # Let's try a different approach: Reward tightness, but with diminishing returns.
        # Prioritize bins where (bin_capacity - item) is small.
        # A simple reward for tightness: 1 / (1 + slack)
        # To balance with exploration: maybe add a small bonus if slack is small but not zero.
        
        # Hybrid approach:
        # Prioritize bins with minimal slack (close to 0). This is the "Best Fit" aspect.
        # But also give a slight preference to bins with a moderate amount of slack
        # to avoid leaving many bins in a state that's hard to fill.

        # A scoring function that peaks at slack = 0 and then decreases,
        # but perhaps less sharply for very small slacks.
        
        # Let's use a scaled inverse relationship with slack, and add a small bonus for moderate slack.
        # For very small slack (tight fit): prioritize.
        # For moderate slack: also consider it favorable.
        # For large slack: disfavor.
        
        # Strategy: Base priority on inverse slack (favoring small slack),
        # but add a small bonus proportional to slack up to a certain point,
        # and then penalize larger slacks.

        # Let's try a function that is high for slack near 0, then decreases.
        # Consider a penalty for slack that is too small (e.g., < 0.1) and too large.
        
        # A common heuristic for balancing is to use a function that is high for "just right".
        # For bin packing, "just right" is usually a tight fit.
        # To encourage exploration, we can slightly boost bins that aren't *exactly* the tightest.
        
        # Let's use a function that rewards closeness to 0 slack, but less aggressively for very small slacks.
        # And perhaps a small bonus for slack that is not excessively large.

        # Option 1: Gamma distribution-like scoring based on slack
        # Prioritize small slack, with a peak somewhere slightly above zero.
        # Example: (slack^a) * exp(-b*slack)
        # This can be tricky to tune.

        # Option 2: Combining a "Best Fit" component with a "Least Full" component.
        # Best Fit: Maximize (1 / (1 + slack))
        # Least Full: Maximize (bins_remain_cap) -- but we want to avoid *large* bins, so this isn't direct.
        
        # Let's try to directly score bins based on how "good" their remaining space is.
        # A good remaining space is small, but not so small that it becomes unusable.
        
        # Score: Maximize (1 / (1 + slack)) - prioritize tight fits.
        # Bonus for moderate slack: add a small value if slack is between X and Y.
        
        # Let's consider slack values.
        # slack = 0: Best.
        # slack = 0.1: Good.
        # slack = 1.0: Okay.
        # slack = 5.0: Not great.
        
        # A function like: `exp(-slack / constant)` will heavily favor small slacks.
        # To add exploration: `exp(-slack / constant) + bonus_if_slack_is_moderate`
        
        # Let's try a concave function that peaks at slack=0, but with a flattened initial slope.
        # This can be achieved by `1 - exp(-k*slack)`. This maps slack=0 to 0, slack=inf to 1.
        # We want the opposite: slack=0 should be high.
        
        # Revisit the "Worse code" logic: `1 / (1 + slack)` is good for favoring tight fits.
        # The reflection asks to favor tight fits but explore slightly larger gaps.
        # This means we want the function to be high for small slack, but not drop off *too* quickly.
        
        # Let's use a sigmoidal function that starts high for small slacks and gradually decreases.
        # A sigmoid shifted and scaled.
        
        # Consider slack values:
        # If slack is very small (e.g., 0.01), we want a high score.
        # If slack is moderate (e.g., 0.5), we want a reasonably high score, maybe slightly lower than 0.01.
        # If slack is large (e.g., 2.0), we want a low score.
        
        # A function like `1 / (1 + slack**p)` where p is between 0 and 1 might work.
        # If p=1, it's `1/(1+slack)` (original logic).
        # If p=0.5, it's `1/(1 + sqrt(slack))`. This penalizes large slacks less.
        
        # Let's use a scaling factor for slack to control the steepness of the decay.
        # And perhaps add a small constant to slack to avoid division by zero and
        # give a base priority to bins with zero slack.
        
        # Let's try a heuristic that penalizes large remaining capacities more heavily.
        # We want to prefer bins that, after packing, have *some* remaining capacity, but not too much.
        
        # Consider the remaining capacity *after* packing: `bins_remain_cap - item`.
        # We want this to be small and positive.
        
        # Score = f(bins_remain_cap - item)
        # f(x) should be high for small positive x.
        
        # Let's use `1 / (1 + slack)` as a base, but then add a small bonus for slacks within a certain range.
        # Or, modify the `slack` value itself before applying `1 / (1 + slack)`.
        
        # Idea: Apply a transformation to slack that emphasizes values close to 0.
        # Let `transformed_slack = slack / (slack + constant)`
        # As slack -> 0, transformed_slack -> 0.
        # As slack -> inf, transformed_slack -> 1.
        # So, priority could be `1 - transformed_slack = constant / (slack + constant)`
        # This is similar to `1 / (1 + slack/constant)`.
        
        # Let's try to incorporate the "just right" idea.
        # If slack is too small, it's bad (maybe the item almost doesn't fit).
        # If slack is too large, it's bad (wasted space).
        # If slack is moderate, it's good.
        
        # This suggests a function with a peak.
        # However, for Bin Packing, the primary goal is still to fit items efficiently.
        # The "exploration" part should probably not override the fundamental "fit tightly" heuristic too much.
        
        # Let's use a softened Best Fit approach.
        # Instead of `1 / (1 + slack)`, use a function that is less steep for small slacks.
        # Consider `log(1 + 1/slack)` for slack > 0. This also favors small slack.
        
        # Let's go back to the idea of preferring bins with small positive remaining capacity.
        # Priority is inversely related to `bins_remain_cap - item`.
        
        # Consider a function `g(remaining_capacity)` where `remaining_capacity = bins_remain_cap - item`.
        # We want `g(x)` to be high for small positive `x`.
        
        # `g(x) = 1 / (1 + x)`: Peaks at 1 for x=0, decreases.
        # `g(x) = exp(-k*x)`: Peaks at 1 for x=0, decreases.
        
        # To encourage exploration of slightly larger gaps:
        # We want the function to decrease slower for small `x`.
        
        # Let's try a hyperbolic tangent (tanh) based approach.
        # `tanh(a - b*slack)`:
        # If `a` is large and `b` is positive, it will be close to 1 for small slack, and decrease.
        # Example: `tanh(5 - 2*slack)`
        # slack=0 -> tanh(5) ~ 1
        # slack=1 -> tanh(3) ~ 1
        # slack=2 -> tanh(1) ~ 0.76
        # slack=3 -> tanh(-1) ~ -0.76
        
        # This favors small slack, but also gives reasonable scores for moderate slack.
        # It penalizes large slack significantly.
        
        # Let's refine this: `tanh(a * (1 - slack/ideal_slack))`
        # This will peak when slack = ideal_slack. We want peak at slack = 0.
        
        # Let's try a function that combines "tightness" and "not-too-empty".
        # Priority = (1 / (1 + slack)) * (slack / (slack + C))  -- this will go to 0.
        
        # How about favoring bins that are not too close to full, nor too empty?
        # This is more like "Second Fit".
        
        # The reflection: "Favor tight fits, but explore slightly larger gaps for flexibility."
        # This implies a function that is high for small slack, but the decay is gentle initially.
        
        # Let's use a modified inverse relationship.
        # Instead of `1 / (1 + slack)`, consider `1 / (1 + slack^p)` with `p < 1`.
        # Or `1 / (1 + sqrt(slack))`. This gives a higher score for small slacks compared to `1/slack`.
        # Let's try `1 / (1 + slack**0.5)`
        
        # slack = 0.01: 1 / (1 + 0.1) = 0.909
        # slack = 0.1:  1 / (1 + 0.316) = 0.76
        # slack = 0.5:  1 / (1 + 0.707) = 0.58
        # slack = 1.0:  1 / (1 + 1.0)   = 0.5
        
        # Compared to `1 / (1 + slack)`:
        # slack = 0.01: 1 / (1 + 0.01) = 0.99
        # slack = 0.1:  1 / (1 + 0.1)  = 0.909
        # slack = 0.5:  1 / (1 + 0.5)  = 0.667
        # slack = 1.0:  1 / (1 + 1.0)  = 0.5
        
        # The `slack**0.5` version penalizes larger slacks less harshly. This seems to align with "explore slightly larger gaps".
        
        # Let's add a small constant to the denominator to ensure non-zero priority even for zero slack.
        # And maybe scale the slack to control the sensitivity.
        
        # Let `scaled_slack = slack / item` (fractional slack)
        # Priority ~ `1 / (1 + scaled_slack**p)`
        
        # Alternative: Focus on the remaining capacity `rc = bins_remain_cap - item`.
        # We want `rc` to be small and positive.
        # Let's define a penalty function for `rc`.
        # Penalty is 0 if `rc` is ideal (e.g., 0), increases as `rc` deviates.
        # But we want to favor small `rc`.
        
        # How about: `exp(-k * rc)`? This is similar to sigmoid.
        
        # Let's try a combination: a base score for tightness, plus a bonus for moderate non-zero slack.
        
        # Base score: `1 / (1 + slack)`
        # Bonus: Apply a Gaussian-like function centered at a small positive slack, e.g., 0.5.
        # `bonus = exp(-(slack - 0.5)**2 / (2 * sigma**2))`
        
        # This could get complex. Let's simplify.
        
        # The reflection is key: "Favor tight fits, but explore slightly larger gaps for flexibility."
        # This means the priority function should decrease as slack increases, but not too rapidly.
        
        # A function like `1 / (1 + slack^p)` with `0 < p < 1` is a good candidate.
        # Let's choose `p = 0.5` (square root) for a start.
        
        # `priorities[fittable_bins_mask] = 1.0 / (1.0 + np.sqrt(slack))`
        
        # To prevent division by zero if slack could be negative (which it can't here since we filter `bins_remain_cap >= item`),
        # and to avoid extremely high priorities for zero slack, let's add a small epsilon.
        
        # Let's also consider scaling slack by item size to make it more relative.
        # `relative_slack = slack / item` (handle item=0 case)
        # `priority = 1.0 / (1.0 + np.sqrt(relative_slack))`
        
        # Let's use the absolute slack but scale its influence.
        # `score = 1.0 / (1.0 + (slack / scale_factor)**p)`
        
        # A simple approach that balances tight fits with some room:
        # Favor bins where `bins_remain_cap - item` is small.
        # Also, consider the absolute `bins_remain_cap`. Larger bins might be useful for larger items later.
        
        # Let's consider the "quality" of the remaining space.
        # A very small remaining space (tight fit) is good for immediate packing.
        # A slightly larger remaining space is also good because it leaves room.
        # A very large remaining space is less good, as it might indicate a poor fit for the current item.
        
        # Let's try a function that peaks at a small positive slack value.
        # A log-normal or gamma-like shape can achieve this.
        # Example: `slack**alpha * exp(-beta * slack)`
        # To maximize this, take derivative and set to zero.
        # `alpha*slack**(alpha-1)*exp(-beta*slack) - beta*slack**alpha*exp(-beta*slack) = 0`
        # `alpha*slack**(-1) - beta = 0`
        # `alpha/slack = beta` => `slack = alpha / beta`
        # So, we can tune `alpha` and `beta` to have the peak at a desired slack.
        
        # Let `alpha = 2`, `beta = 4`. Peak at `slack = 2/4 = 0.5`.
        # `priorities[fittable_bins_mask] = slack**2 * np.exp(-4*slack)`
        # This function is 0 at slack=0, peaks at 0.5, and goes to 0 as slack increases.
        # This might be *too* much exploration and not enough tight fit preference.
        
        # Let's stick to favoring small slack primarily, but with a less aggressive decay.
        # `1 / (1 + slack^p)` with `p` around 0.5 seems a good balance.
        
        # Consider `p = 0.7`.
        # slack = 0.01: 1 / (1 + 0.0046) = 0.995
        # slack = 0.1:  1 / (1 + 0.1778) = 0.845
        # slack = 0.5:  1 / (1 + 0.421)  = 0.703
        # slack = 1.0:  1 / (1 + 1.0)    = 0.5
        
        # This function is higher for small slacks than `1/(1+slack^0.5)`, and decays slower.
        # This means it favors tight fits, but also gives good scores to moderately tight fits,
        # promoting exploration of slightly larger gaps.
        
        # Let's use `p = 0.7` as a parameter.
        p_value = 0.7
        
        # Add a small epsilon to slack to avoid potential issues with `slack=0` and `p<1` leading to undefined behavior
        # or excessively high values if `slack` could be extremely close to zero.
        # However, `0**p` is 0 for `p>0`. So, `1/(1+0)` is 1. This is fine.
        
        # Let's use a scaling factor for slack to control the "tightness" preference.
        # `scaled_slack = slack / item_size_scale`
        # A scale factor related to average item size might be good.
        # For now, let's use absolute slack with `p=0.7`.
        
        priorities[fittable_bins_mask] = 1.0 / (1.0 + slack**p_value)
        
        # Ensure priorities are not NaN or Inf (though unlikely with this formula and non-negative slack)
        priorities[fittable_bins_mask] = np.nan_to_num(priorities[fittable_bins_mask], nan=0.0, posinf=1.0, neginf=0.0)
        
    return priorities
```
