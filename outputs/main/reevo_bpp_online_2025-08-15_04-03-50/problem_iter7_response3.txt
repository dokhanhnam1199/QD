```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a refined Almost Full Fit strategy.

    This strategy prioritizes bins that have a "tight fit" for the item,
    meaning the remaining capacity after insertion is minimized. It also gently penalizes
    bins that have a very large remaining capacity, preferring bins that are already
    moderately full but can still accommodate the item.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins where the item can fit
    fit_mask = bins_remain_cap >= item

    # Calculate the remaining capacity if the item is placed in each fitting bin
    potential_remaining_cap = bins_remain_cap[fit_mask] - item

    # Strategy:
    # 1. Prioritize bins that leave the least remaining capacity (tightest fit).
    #    This can be achieved by taking the negative of the remaining capacity,
    #    or by using an inverse relationship like 1 / (1 + remaining_capacity).
    #    We use 1 / (1 + slack) where slack is `bins_remain_cap - item`. This gives
    #    higher scores to smaller non-negative slacks.
    # 2. Penalize bins that are already very full, *if* they leave a large gap after insertion
    #    (i.e., have a large slack). The 1/(1+slack) naturally does this.
    # 3. Gently penalize bins with very large remaining capacity *even if* they are a tight fit for the current item.
    #    This is implicitly handled by the 1/(1+slack) form; a bin with 100 remaining capacity
    #    and fitting a 1-unit item will have slack 99, leading to a low priority.
    # 4. We want to favor bins that are "almost full", which means small `potential_remaining_cap`.
    #    So, a higher priority should be given to smaller non-negative `potential_remaining_cap`.

    # A score based on the inverse of the "slack" (remaining_capacity - item).
    # Adding 1 to the slack in the denominator prevents division by zero and ensures
    # that a perfect fit (slack=0) gets the highest priority (1.0).
    # Bins with larger slack values get exponentially smaller priorities.
    priorities[fit_mask] = 1.0 / (1.0 + potential_remaining_cap)

    # Refinement: To further emphasize "almost full", we can slightly boost
    # bins that have a negative remaining capacity after potential insertion
    # *if* that negative value is small (meaning the item almost fit, or slightly exceeded).
    # However, for online BPP, we must select a bin where the item *fits*.
    # So, `fit_mask` is essential.

    # An alternative perspective for "almost full":
    # We want to maximize the "fullness" after placing the item.
    # Fullness can be seen as `(BinCapacity - RemainingCapacity) / BinCapacity`.
    # Or, after placing item: `(BinCapacity - (BinCapacity - item)) / BinCapacity = item / BinCapacity`.
    # This is First Fit Decreasing logic. For online, we don't know future items.

    # Reverting to the prior thought: prioritize minimal slack.
    # The current `1.0 / (1.0 + slack)` is a good heuristic for prioritizing tight fits.
    # It ensures that bins that become "most full" (smallest positive or zero slack) are preferred.

    # To gently penalize large gaps *while* prioritizing tight fits:
    # Consider a score like `priority = exp(-slack / some_scale)`.
    # Or a simpler linear approach: `priority = max_slack - slack`. This requires knowing max_slack.
    # The current `1/(1+slack)` is a good balance.

    # Let's slightly adjust the priority to favor bins that are NOT too empty.
    # If `potential_remaining_cap` is large, the priority should be lower.
    # A simple modification: `priority = (1.0 / (1.0 + slack)) * (1.0 / (1.0 + slack / average_bin_capacity))`
    # This adds a penalty for large slacks relative to average bin capacity.
    # For simplicity and robustness, let's stick to prioritizing minimal slack.
    # The existing `1.0 / (1.0 + slack)` metric already does this effectively.
    # It prioritizes bins where `bins_remain_cap - item` is smallest and non-negative.

    # If we want to ensure that bins that become *very* full (small positive slack)
    # are strongly preferred over bins that are moderately full (larger positive slack),
    # the inverse relationship works well.

    # To align with "gently penalize large gaps":
    # Consider a score that is high for small slack and decreases as slack increases.
    # The function `1.0 / (1.0 + slack)` does this.
    # For instance:
    # slack = 0 (perfect fit) -> priority = 1.0
    # slack = 1 -> priority = 0.5
    # slack = 10 -> priority = 0.09
    # slack = 100 -> priority = 0.01

    # If we want to distinguish more between tight fits and looser fits,
    # we could use a steeper decay, e.g., `1.0 / (1.0 + slack^2)`.
    # Or even `exp(-slack / avg_bin_capacity)`.

    # Let's consider a score that is sensitive to how *much* remaining capacity there is
    # relative to the item size.
    # `score = 1.0 / (1.0 + (bins_remain_cap[fit_mask] - item))` is good.

    # To add a slight bias towards less empty bins overall, we can consider the original `bins_remain_cap`.
    # `score = (1.0 / (1.0 + slack)) * (bins_remain_cap[fit_mask] / MAX_BIN_CAPACITY)`
    # This would give a slight edge to bins that are already more full, assuming they provide a tight fit.
    # But this might conflict with "tight fit" if the more full bin has a larger slack.

    # Sticking to the core "Almost Full Fit" intent: prioritize minimal slack.
    # The `1.0 / (1.0 + slack)` is a robust interpretation.

    # Let's try to incorporate the "penalize large gaps gently" aspect more directly.
    # A bin that fits the item but leaves a large remaining capacity (large slack) should have lower priority
    # than a bin that fits and leaves a small remaining capacity.
    # The `1.0 / (1.0 + slack)` already handles this.

    # What if we prioritize bins where `potential_remaining_cap` is closest to zero (but non-negative)?
    # This is precisely what `1.0 / (1.0 + slack)` does.

    # Let's consider the "fullness" of the bin *after* placing the item.
    # This is `(BinCapacity - potential_remaining_cap) / BinCapacity`.
    # For example, if bin capacity is 10:
    # Item 3, bin_remain_cap 5 -> slack 2, potential_remaining_cap 2. Fullness = (10-2)/10 = 0.8
    # Item 3, bin_remain_cap 4 -> slack 1, potential_remaining_cap 1. Fullness = (10-1)/10 = 0.9
    # Item 3, bin_remain_cap 3 -> slack 0, potential_remaining_cap 0. Fullness = (10-0)/10 = 1.0

    # This "fullness" metric would prioritize the most full bins.
    # The score would be `(bins_remain_cap[fit_mask] - item) / MAX_BIN_CAPACITY`? No, this is inverse.
    # It should be `(BIN_CAPACITY - (BIN_CAPACITY - item)) / BIN_CAPACITY = item / BIN_CAPACITY`.
    # This strategy is essentially First Fit Decreasing if we could sort by item size.
    # For online, this means we prefer bins that are already fuller (less capacity remaining).

    # Let's combine:
    # 1. Tight fit (small slack) is primary.
    # 2. Among tight fits, perhaps prefer bins that were already more full.

    # A scoring function could be:
    # `priority = (1.0 / (1.0 + slack)) * (bins_remain_cap[fit_mask] / MAX_BIN_CAPACITY)`
    # where `MAX_BIN_CAPACITY` is some global value or an average.

    # Let's try a simpler modification to `1.0 / (1.0 + slack)`
    # The reflection says: "Prioritize tight fits and fuller bins."
    # `1.0 / (1.0 + slack)` prioritizes tight fits.
    # To prioritize fuller bins, we can multiply by a term that increases with `bins_remain_cap[fit_mask]`.
    # However, this might push us away from tight fits if a very full bin has a slightly larger slack.

    # A robust way to prioritize fuller bins among tight fits is to prioritize bins
    # that have `bins_remain_cap[fit_mask] - item` closer to 0.
    # If we have two bins with slack=0, we should prefer the one that was already fuller.
    # However, the current strategy might pick the one that was less full if its absolute remaining capacity is smaller.

    # Let's stick to the interpretation that "almost full" means minimal positive slack.
    # The `1.0 / (1.0 + slack)` metric is a good heuristic for this.
    # The "fuller bins" part is implicitly handled by favoring smaller slacks.

    # Consider a score that rewards smaller `bins_remain_cap[fit_mask]` overall,
    # while still favoring smaller `slack`.

    # A common heuristic for online BPP is "Best Fit", which picks the bin with the minimum slack.
    # This is captured by maximizing `1.0 / (1.0 + slack)`.

    # The current implementation `priorities[fit_mask] = 1.0 / (1.0 + potential_remaining_cap)`
    # directly implements a form of "Best Fit" or "Almost Full Fit" by prioritizing minimal slack.
    # The "gently penalize large gaps" is inherent in the decay of `1/(1+x)`.

    # Let's ensure robustness for edge cases and clarify the "fuller bins" aspect.
    # If we have two bins that are perfect fits (slack=0), the current method gives them equal priority.
    # If we want to prefer the *fuller* bin in this case, we can add a secondary sorting criterion.
    # But the priority function should return a single score.

    # A potential scoring could be:
    # `score = -slack + C * bins_remain_cap[fit_mask]`
    # where C is a small positive constant to break ties for slack.
    # This would favor smaller slack, and then larger `bins_remain_cap`.

    # Let's try implementing `-slack` first for direct "minimal slack" preference.
    # `priorities[fit_mask] = -potential_remaining_cap`
    # Then, to penalize larger gaps, we add a term.
    # A score like `-slack / MAX_SLACK` or `-slack / AVERAGE_SLACK`
    # Or we can use `bins_remain_cap` as a positive factor to break ties for slack.

    # Let's refine: we want to maximize `f(slack)`.
    # `f(slack) = 1 / (1 + slack)` is good.
    # To incorporate "fuller bins", we can add a term that increases with `bins_remain_cap[fit_mask]`.
    # `score = (1.0 / (1.0 + slack)) + alpha * bins_remain_cap[fit_mask]`
    # The alpha needs tuning.

    # A simpler way to favor fuller bins among tight fits:
    # What if we want to prioritize bins that are "almost full" in an absolute sense?
    # E.g., bins with remaining capacity between 0 and 10% of total capacity are good.
    # This suggests penalizing bins with remaining capacity > 10% of total capacity.

    # Let's try a score that is high for small slack and slightly higher for bins
    # that were already more full.

    # Score = (1 / (1 + slack))  # Prioritizes tightest fits
    # Let's try multiplying by original capacity, scaled.
    # It's tricky to combine them into a single score without making one dominate too much.

    # Revisiting the reflection: "Prioritize tight fits and fuller bins. Penalize large gaps gently."
    # The `1.0 / (1.0 + slack)` approach handles "tight fits" and "penalize large gaps gently".
    # For "fuller bins", it's about choosing between bins with similar slack.
    # If slack is equal, it doesn't distinguish.

    # Let's try a score that combines slack and original remaining capacity:
    # `score = -slack + C * bins_remain_cap[fit_mask]`
    # This prioritizes minimum slack. For same slack, it prioritizes larger `bins_remain_cap`.
    # This might be counter-intuitive to "fuller bins" meaning "less empty".
    # It would mean if bin A has capacity 10, item 5 (slack 5), and bin B has capacity 20, item 15 (slack 5),
    # it would prefer bin B.
    # If we want to prefer bins that are already more full (less original capacity),
    # it would be `score = -slack - C * bins_remain_cap[fit_mask]` (minimizing this score).
    # Or maximizing `slack + C * bins_remain_cap[fit_mask]` if slack is the penalty.

    # Let's try a simpler approach based on the original logic but with a slight boost for fuller bins.
    # The original logic: `1.0 / (1.0 + slack)`
    # We want to boost bins that are more full.
    # `priority = (1.0 / (1.0 + slack)) * (bins_remain_cap[fit_mask] / MAX_POSSIBLE_REMAINING_CAP)`
    # This could normalize the effect of original capacity.

    # A simpler approach that combines minimal slack and fuller bins:
    # If we consider the goal of minimizing total bins, then filling bins tightly is key.
    # The `1.0 / (1.0 + slack)` metric directly addresses minimizing slack.

    # What if we use `potential_remaining_cap` and penalize larger values?
    # `score = -potential_remaining_cap` maximizes fullness directly.
    # But this doesn't distinguish between bins that are `potential_remaining_cap = -1` and `potential_remaining_cap = -10`
    # if the item size is large.
    # The `1.0 / (1.0 + slack)` metric is better because slack is always >= 0 for fitting bins.

    # Let's consider a metric that penalizes bins that are TOO empty after insertion.
    # If `potential_remaining_cap` is large, the priority should be low.
    # `score = 1.0 / (1.0 + potential_remaining_cap)` is good.

    # A slight variation: use the negative of the remaining capacity as a base,
    # then apply a penalty for larger absolute remaining capacity.
    # `score = -potential_remaining_cap - penalty_factor * max(0, potential_remaining_cap)`
    # This gives higher score to more negative `potential_remaining_cap` (more full),
    # and then adds a penalty if `potential_remaining_cap` is positive (less full).

    # Let's go with a robust approach that prioritizes minimal slack, and as a secondary preference,
    # favors bins that were already more full.
    # We can achieve this by a weighted sum or by modifying the score.

    # Score = (1.0 / (1.0 + slack)) * (1.0 + bins_remain_cap[fit_mask] / SOME_SCALE)
    # This boosts the priority for fuller bins.
    # Let's try a simpler version:
    # `score = -slack + C * bins_remain_cap[fit_mask]`
    # This score would be maximized.
    # This favors smaller slack. Among equal slacks, it favors larger `bins_remain_cap`.
    # This may not be ideal for "fuller bins" meaning "less capacity".

    # Final attempt at a combined heuristic:
    # Prioritize minimal slack `s = bin_cap - item`.
    # Score = f(s). We want f(s) to be decreasing for s >= 0.
    # Then, boost for fuller bins (smaller `bin_cap`).
    # Score = g(s) * h(bin_cap) where g is decreasing and h is increasing (for less capacity).

    # Let's use the `-slack` approach directly, which favors minimal slack.
    # `priorities[fit_mask] = -potential_remaining_cap`
    # Now, how to add "fuller bins"?
    # If we have two bins with slack 0, we prefer the one with smaller `bins_remain_cap`.
    # So, we want to *minimize* `bins_remain_cap` for tie-breaking.
    # In a maximization score, this means adding a penalty for larger `bins_remain_cap`.
    # `score = -potential_remaining_cap - C * bins_remain_cap[fit_mask]`

    # Let's consider the "Almost Full Fit" aspect more directly:
    # Prioritize bins where `bins_remain_cap - item` is small and non-negative.
    # `score = 1.0 / (1.0 + slack)` does this.
    # For "fuller bins", this means if slack is the same, choose the bin that was already fuller (had less remaining capacity).
    # The `1.0 / (1.0 + slack)` doesn't distinguish if slack is the same.

    # A good heuristic is to prioritize minimal slack. If there's a tie in slack,
    # pick the bin that was originally fuller.
    # This can be achieved by:
    # 1. Score = -slack
    # 2. For ties in score, use -original_remaining_capacity as secondary score.
    # Since the function must return a single score:
    # `score = -slack - C * original_remaining_capacity` where C is small.
    # This implies we are minimizing the score. But we need to maximize for the highest priority.
    # So, maximize: `slack + C * original_remaining_capacity`

    # Let's use the `1.0 / (1.0 + slack)` for minimal slack, and try to add a factor for fuller bins.
    # `priority = (1.0 / (1.0 + slack)) * (1.0 + (MAX_CAPACITY - bins_remain_cap[fit_mask]) / MAX_CAPACITY)`
    # This second term increases as `bins_remain_cap` decreases (bin is fuller).

    # Let's test this combined approach:
    # `MAX_CAPACITY` is not available. Let's assume it or use average.
    # A simpler way to penalize large gaps gently and prioritize fuller bins:
    # Use the inverse of remaining capacity after placement, scaled.
    # `score = (MAX_CAPACITY - potential_remaining_cap)` for fitting bins.
    # This maximizes the final occupied space, which means minimal remaining capacity.
    # And it prioritizes bins that have more space *filled* (thus are "fuller").

    # Let's use a metric that rewards small `potential_remaining_cap`.
    # `score = -potential_remaining_cap` is a good candidate.
    # To add the "fuller bins" preference, if `potential_remaining_cap` is same, prefer smaller `bins_remain_cap`.
    # `score = -potential_remaining_cap - C * bins_remain_cap[fit_mask]`

    # Considering the reflection: "Prioritize tight fits and fuller bins. Penalize large gaps gently."
    # The `1.0 / (1.0 + slack)` strategy is excellent for tight fits and gentle penalization of gaps.
    # For "fuller bins", if `slack` is equal, we prefer the bin that was originally more full.
    # We can add a term that reflects this.
    # Let's try `score = (1.0 / (1.0 + slack)) * (1.0 / (1.0 + bins_remain_cap[fit_mask] / SOME_SCALE))`
    # This penalizes larger `bins_remain_cap`. This is the opposite of what we want for "fuller bins".

    # We want to maximize `slack_score * fullness_score`.
    # `slack_score = 1.0 / (1.0 + slack)` (higher for smaller slack)
    # `fullness_score` should be higher for smaller `bins_remain_cap`.
    # `fullness_score = 1.0 / (1.0 + bins_remain_cap[fit_mask])` is one way.

    # So, `priority = (1.0 / (1.0 + slack)) * (1.0 / (1.0 + bins_remain_cap[fit_mask]))`
    # This prioritizes bins that are both tight fits AND were already more full.

    slack = bins_remain_cap[fit_mask] - item
    original_caps = bins_remain_cap[fit_mask]

    # Prioritize tight fits (small slack) using inverse relationship.
    tight_fit_score = 1.0 / (1.0 + slack)

    # Prioritize fuller bins (smaller original remaining capacity).
    # Adding a small epsilon to original_caps to avoid potential division by zero if original_cap is 0,
    # though this is unlikely in BPP. More importantly, to ensure the denominator is > 1,
    # giving a score < 1 for non-zero capacities.
    fullness_score = 1.0 / (1.0 + original_caps)

    # Combine scores. Multiplication tends to favor bins that are good on both criteria.
    # This approach prioritizes bins that are tight fits AND were already more full.
    priorities[fit_mask] = tight_fit_score * fullness_score

    # Alternative combined score (additive, might give different relative weights):
    # priorities[fit_mask] = tight_fit_score + fullness_score * 0.1 # Small weight for fullness

    # Let's test `tight_fit_score * fullness_score`:
    # Bin A: item=3, bin_cap=5 => slack=2, original_cap=5.
    #        tight_fit_score = 1/(1+2) = 0.333. fullness_score = 1/(1+5) = 0.167. Total = 0.055
    # Bin B: item=3, bin_cap=4 => slack=1, original_cap=4.
    #        tight_fit_score = 1/(1+1) = 0.5. fullness_score = 1/(1+4) = 0.2. Total = 0.1
    # Bin C: item=3, bin_cap=3 => slack=0, original_cap=3.
    #        tight_fit_score = 1/(1+0) = 1.0. fullness_score = 1/(1+3) = 0.25. Total = 0.25

    # This seems to align well with prioritizing tight fits and then fuller bins.
    # The penalization of large gaps is handled by the steep decay of 1/(1+slack).
    # The "fuller bins" preference is handled by the 1/(1+original_cap) factor.

    return priorities
```
