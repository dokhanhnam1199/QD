[
  {
    "stdout_filepath": "problem_iter4_response0.txt_stdout.txt",
    "code_path": "problem_iter4_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined strategy.\n\n    This strategy prioritizes bins that offer a \"tight fit\" for the item,\n    meaning the remaining capacity after insertion is minimized. It achieves this\n    by assigning a priority score based on the inverse of (1 + slack), where slack\n    is the difference between the bin's remaining capacity and the item's size.\n\n    To further balance this with the \"fullness\" of bins (prioritizing bins that\n    are already closer to full, i.e., have less remaining capacity overall), a small\n    secondary score is added. This secondary score is proportional to the negative\n    of the bin's remaining capacity, effectively giving a slight preference to\n    smaller bins when tight fits are comparable.\n\n    A small amount of noise is added to the priority scores to introduce a stochastic\n    element, promoting exploration and helping to escape local optima, as suggested\n    by the \"balance with random exploration for robustness\" aspect of the reflection.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities\n\n    # Calculate slack for bins that can fit the item\n    # Slack = remaining_capacity - item_size\n    slack = bins_remain_cap[fit_mask] - item\n\n    # Primary scoring: Favor tight fits by using 1 / (1 + slack).\n    # This assigns higher scores to bins with smaller positive slack.\n    tight_fit_scores = 1.0 / (1.0 + slack)\n\n    # Secondary scoring: Favor \"fuller\" bins (those with less initial remaining capacity).\n    # We use a small negative term proportional to the bin's remaining capacity.\n    # Maximizing this term means minimizing bins_remain_cap for fitting bins.\n    # The small multiplier ensures tight fit is primary.\n    fullness_scores = -0.1 * bins_remain_cap[fit_mask]\n\n    # Combine scores\n    combined_scores = tight_fit_scores + fullness_scores\n\n    # Add a small amount of random noise to encourage exploration.\n    # This helps to break ties and explore different packing configurations.\n    noise = np.random.normal(0, 0.01, size=combined_scores.shape)\n    priorities[fit_mask] = combined_scores + noise\n\n    # Normalize priorities so they sum to 1 over the fittable bins.\n    # This converts scores into probabilities if using a probabilistic selection method.\n    # If deterministic selection (e.g., argmax) is used, normalization isn't strictly necessary\n    # but can help in interpreting relative preferences.\n    current_priorities = priorities[fit_mask]\n    if np.any(current_priorities):\n        # Ensure no division by zero if all priorities are zero (though unlikely with noise)\n        sum_priorities = np.sum(current_priorities)\n        if sum_priorities > 0:\n            priorities[fit_mask] = current_priorities / sum_priorities\n        else:\n            # If for some reason all priorities are non-positive, assign equal probability\n            # to all fittable bins.\n            priorities[fit_mask] = 1.0 / len(current_priorities) if len(current_priorities) > 0 else 0\n\n    return priorities",
    "response_id": 0,
    "obj": 4.487435181491823,
    "cyclomatic_complexity": 5.0,
    "halstead": 175.93083758004835,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response1.txt_stdout.txt",
    "code_path": "problem_iter4_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a strategy that prioritizes bins with minimal remaining space after packing, and among those, favors bins that are fuller.\n\n    This strategy is a refinement of Best Fit.\n    1. Primary objective: Minimize the \"slack\" (remaining capacity - item size).\n       Bins with slack closer to zero are preferred.\n    2. Secondary objective: Among bins with the same slack, prefer bins that are fuller.\n       A fuller bin has less remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate slack for fittable bins\n    # Slack = remaining_capacity - item_size\n    slack = bins_remain_cap[can_fit_mask] - item\n\n    # Primary scoring: Minimize slack. We want to maximize a function that is\n    # large for small slack. Using 1.0 / (slack + epsilon) achieves this.\n    # A slack of 0 gives the highest score (1.0).\n    best_fit_score = 1.0 / (slack + 1e-9)\n\n    # Secondary scoring: Favor fuller bins. This means preferring bins with\n    # smaller remaining capacity among those with the same slack.\n    # We can achieve this by maximizing 1.0 / bins_remain_cap.\n    fuller_bin_score = 1.0 / (bins_remain_cap[can_fit_mask] + 1e-9)\n\n    # Combine scores. We want slack to be the primary driver.\n    # A simple way to combine is to add them, but to ensure slack is dominant,\n    # we can multiply by a factor that makes the slack score much larger.\n    # Alternatively, we can use a weighted sum where the slack component\n    # has a larger weight.\n    # Let's try a composite score where we consider slack first, and then remaining capacity.\n    # A common way to achieve lexicographical ordering is to use a base and scaled values.\n    # For example: score = PrimaryScore * Scale + SecondaryScore\n    # Here, we want to maximize `best_fit_score` and `fuller_bin_score`.\n    # If `best_fit_score` is the primary, we want it to have a higher impact.\n\n    # Let's use a weighting approach.\n    # Weight for best fit: Give it a higher weight to ensure it's considered first.\n    # Weight for fuller bin: Give it a smaller weight as it's a secondary preference.\n    # However, directly adding might lead to issues if the scales are very different.\n\n    # A robust way for \"minimize A, then minimize B\" is to maximize (-A - alpha*B).\n    # So, we want to maximize `-(slack) - alpha * (bins_remain_cap)`.\n    # This translates to maximizing `-(slack + alpha * bins_remain_cap)`.\n    # Let's use alpha = 1, and see how it performs.\n    # The priority will be `- (slack + bins_remain_cap[can_fit_mask])`.\n    # This means smaller values of `slack + bins_remain_cap` get higher priority.\n    # Example:\n    # Bin A: slack=0, rem_cap=5  => sum=5, priority = -5\n    # Bin B: slack=0, rem_cap=10 => sum=10, priority = -10\n    # Bin C: slack=1, rem_cap=5  => sum=6, priority = -6\n    # Bin D: slack=1, rem_cap=10 => sum=11, priority = -11\n    # This correctly prioritizes Bin A (best fit, fullest), then Bin C (good fit, fullest), then Bin B (best fit, less full), then Bin D.\n\n    # However, the reflection asked to \"prioritize fuller bins by inverting remaining capacity for better fit.\"\n    # This implies that `1.0 / bins_remain_cap` should contribute positively to the priority.\n    # So, we want to maximize `1.0 / (slack + epsilon)` AND maximize `1.0 / (bins_remain_cap + epsilon)`.\n    # Let's use a simple additive approach, ensuring the best-fit component is primary.\n    # We can scale the \"fuller bin\" score.\n    # `priorities[can_fit_mask] = best_fit_score + 0.1 * fuller_bin_score`\n    # The `0.1` is a heuristic weight. It makes the \"fuller bin\" score less influential than \"best fit\".\n    # For example, if slack is 0, BF score is 1. If rem_cap is 5, FB score is 0.2. Total 1.2.\n    # If slack is 0, rem_cap is 10, BF score is 1. FB score is 0.1. Total 1.1.\n    # This correctly prioritizes the fuller bin among those with the same slack.\n\n    # Let's consider the case where slack is small but not zero.\n    # slack=0.1, rem_cap=5. BF score = 1 / 1.1 = 0.909. FB score = 0.2. Total = 1.109.\n    # slack=0, rem_cap=10. BF score = 1.0. FB score = 0.1. Total = 1.1.\n    # In this case, the slack=0.1 bin is prioritized over the slack=0, fuller bin. This is not ideal.\n\n    # The reflection implies a stronger preference for fuller bins when slack is the same.\n    # Let's combine them in a way that ensures the primary objective (minimal slack) is paramount,\n    # and the secondary objective (fuller bins) breaks ties effectively.\n\n    # A common way is to sort by (slack, bins_remain_cap). We want the smallest such pair.\n    # For priority, we want to maximize a score reflecting this.\n    # So, we want to maximize `(-slack, -bins_remain_cap)`.\n    # We can achieve this by converting to a single score: `-slack - alpha * bins_remain_cap`.\n    # If alpha is chosen large enough, `-slack` dominates.\n    # Let's use `alpha = 1`. The score is `-(slack + bins_remain_cap[can_fit_mask])`.\n    # This means the highest priority (least negative) is for the smallest `slack + bins_remain_cap`.\n\n    # Example:\n    # Bin A: slack=0, rem_cap=5  => sum=5, priority = -5\n    # Bin B: slack=0, rem_cap=10 => sum=10, priority = -10\n    # Bin C: slack=1, rem_cap=5  => sum=6, priority = -6\n    # Bin D: slack=1, rem_cap=10 => sum=11, priority = -11\n    # This seems to match the desired behavior: A is best, then C, then B, then D.\n    # This priority function directly implements minimizing slack first, then minimizing remaining capacity.\n\n    priorities[can_fit_mask] = -(slack + bins_remain_cap[can_fit_mask])\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 104.2481250360578,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response2.txt_stdout.txt",
    "code_path": "problem_iter4_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a hybrid strategy.\n\n    This strategy aims to balance the preference for tight fits (minimizing wasted space\n    in the immediate bin) with a slight preference for bins that offer a bit more\n    room, promoting better global packing. It uses a combination of a \"Best Fit\"\n    (minimal slack) component and a \"First Fit\"-like component (favoring bins\n    that are not too empty).\n\n    The priority is calculated based on the negative slack (prioritizing bins\n    where `bins_remain_cap - item` is small and positive). To encourage exploration\n    and prevent premature filling of slightly-too-small gaps that might be better\n    suited for smaller items, a small penalty is applied to bins with extremely\n    tight fits, and a bonus is given to bins with a moderate amount of slack.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if np.any(fittable_bins_mask):\n        fittable_bins_capacities = bins_remain_cap[fittable_bins_mask]\n\n        # Calculate slack: remaining capacity after fitting the item.\n        # We want slack to be as close to 0 as possible.\n        slack = fittable_bins_capacities - item\n\n        # Heuristic:\n        # 1. Prioritize bins with small positive slack (tight fits).\n        # 2. Avoid extremely tight fits (slack very close to 0) by slightly penalizing them.\n        # 3. Give a small bonus to bins with moderate slack to encourage using \"less full\" bins sometimes.\n        # This can be achieved by mapping slack to a score where a small positive value is optimal.\n\n        # Using a quadratic function: -(slack - ideal_slack)^2\n        # Let's aim for an \"ideal\" slack of, say, 10% of the item size,\n        # but with a floor to avoid negative ideal slack for very small items.\n        ideal_slack = max(0.1 * item, 0.1) # Aim for ~10% of item size, minimum 0.1\n        \n        # Penalize deviations from the ideal slack.\n        # A higher value means closer to ideal_slack is better.\n        # We want to maximize this value.\n        # The function `-(slack - ideal_slack)**2` will be maximized when slack is close to ideal_slack.\n        # However, we also want to prioritize *tight* fits, meaning slack near 0 should be good.\n        \n        # Let's try a different approach: Reward tightness, but with diminishing returns.\n        # Prioritize bins where (bin_capacity - item) is small.\n        # A simple reward for tightness: 1 / (1 + slack)\n        # To balance with exploration: maybe add a small bonus if slack is small but not zero.\n        \n        # Hybrid approach:\n        # Prioritize bins with minimal slack (close to 0). This is the \"Best Fit\" aspect.\n        # But also give a slight preference to bins with a moderate amount of slack\n        # to avoid leaving many bins in a state that's hard to fill.\n\n        # A scoring function that peaks at slack = 0 and then decreases,\n        # but perhaps less sharply for very small slacks.\n        \n        # Let's use a scaled inverse relationship with slack, and add a small bonus for moderate slack.\n        # For very small slack (tight fit): prioritize.\n        # For moderate slack: also consider it favorable.\n        # For large slack: disfavor.\n        \n        # Strategy: Base priority on inverse slack (favoring small slack),\n        # but add a small bonus proportional to slack up to a certain point,\n        # and then penalize larger slacks.\n\n        # Let's try a function that is high for slack near 0, then decreases.\n        # Consider a penalty for slack that is too small (e.g., < 0.1) and too large.\n        \n        # A common heuristic for balancing is to use a function that is high for \"just right\".\n        # For bin packing, \"just right\" is usually a tight fit.\n        # To encourage exploration, we can slightly boost bins that aren't *exactly* the tightest.\n        \n        # Let's use a function that rewards closeness to 0 slack, but less aggressively for very small slacks.\n        # And perhaps a small bonus for slack that is not excessively large.\n\n        # Option 1: Gamma distribution-like scoring based on slack\n        # Prioritize small slack, with a peak somewhere slightly above zero.\n        # Example: (slack^a) * exp(-b*slack)\n        # This can be tricky to tune.\n\n        # Option 2: Combining a \"Best Fit\" component with a \"Least Full\" component.\n        # Best Fit: Maximize (1 / (1 + slack))\n        # Least Full: Maximize (bins_remain_cap) -- but we want to avoid *large* bins, so this isn't direct.\n        \n        # Let's try to directly score bins based on how \"good\" their remaining space is.\n        # A good remaining space is small, but not so small that it becomes unusable.\n        \n        # Score: Maximize (1 / (1 + slack)) - prioritize tight fits.\n        # Bonus for moderate slack: add a small value if slack is between X and Y.\n        \n        # Let's consider slack values.\n        # slack = 0: Best.\n        # slack = 0.1: Good.\n        # slack = 1.0: Okay.\n        # slack = 5.0: Not great.\n        \n        # A function like: `exp(-slack / constant)` will heavily favor small slacks.\n        # To add exploration: `exp(-slack / constant) + bonus_if_slack_is_moderate`\n        \n        # Let's try a concave function that peaks at slack=0, but with a flattened initial slope.\n        # This can be achieved by `1 - exp(-k*slack)`. This maps slack=0 to 0, slack=inf to 1.\n        # We want the opposite: slack=0 should be high.\n        \n        # Revisit the \"Worse code\" logic: `1 / (1 + slack)` is good for favoring tight fits.\n        # The reflection asks to favor tight fits but explore slightly larger gaps.\n        # This means we want the function to be high for small slack, but not drop off *too* quickly.\n        \n        # Let's use a sigmoidal function that starts high for small slacks and gradually decreases.\n        # A sigmoid shifted and scaled.\n        \n        # Consider slack values:\n        # If slack is very small (e.g., 0.01), we want a high score.\n        # If slack is moderate (e.g., 0.5), we want a reasonably high score, maybe slightly lower than 0.01.\n        # If slack is large (e.g., 2.0), we want a low score.\n        \n        # A function like `1 / (1 + slack**p)` where p is between 0 and 1 might work.\n        # If p=1, it's `1/(1+slack)` (original logic).\n        # If p=0.5, it's `1/(1 + sqrt(slack))`. This penalizes large slacks less.\n        \n        # Let's use a scaling factor for slack to control the steepness of the decay.\n        # And perhaps add a small constant to slack to avoid division by zero and\n        # give a base priority to bins with zero slack.\n        \n        # Let's try a heuristic that penalizes large remaining capacities more heavily.\n        # We want to prefer bins that, after packing, have *some* remaining capacity, but not too much.\n        \n        # Consider the remaining capacity *after* packing: `bins_remain_cap - item`.\n        # We want this to be small and positive.\n        \n        # Score = f(bins_remain_cap - item)\n        # f(x) should be high for small positive x.\n        \n        # Let's use `1 / (1 + slack)` as a base, but then add a small bonus for slacks within a certain range.\n        # Or, modify the `slack` value itself before applying `1 / (1 + slack)`.\n        \n        # Idea: Apply a transformation to slack that emphasizes values close to 0.\n        # Let `transformed_slack = slack / (slack + constant)`\n        # As slack -> 0, transformed_slack -> 0.\n        # As slack -> inf, transformed_slack -> 1.\n        # So, priority could be `1 - transformed_slack = constant / (slack + constant)`\n        # This is similar to `1 / (1 + slack/constant)`.\n        \n        # Let's try to incorporate the \"just right\" idea.\n        # If slack is too small, it's bad (maybe the item almost doesn't fit).\n        # If slack is too large, it's bad (wasted space).\n        # If slack is moderate, it's good.\n        \n        # This suggests a function with a peak.\n        # However, for Bin Packing, the primary goal is still to fit items efficiently.\n        # The \"exploration\" part should probably not override the fundamental \"fit tightly\" heuristic too much.\n        \n        # Let's use a softened Best Fit approach.\n        # Instead of `1 / (1 + slack)`, use a function that is less steep for small slacks.\n        # Consider `log(1 + 1/slack)` for slack > 0. This also favors small slack.\n        \n        # Let's go back to the idea of preferring bins with small positive remaining capacity.\n        # Priority is inversely related to `bins_remain_cap - item`.\n        \n        # Consider a function `g(remaining_capacity)` where `remaining_capacity = bins_remain_cap - item`.\n        # We want `g(x)` to be high for small positive `x`.\n        \n        # `g(x) = 1 / (1 + x)`: Peaks at 1 for x=0, decreases.\n        # `g(x) = exp(-k*x)`: Peaks at 1 for x=0, decreases.\n        \n        # To encourage exploration of slightly larger gaps:\n        # We want the function to decrease slower for small `x`.\n        \n        # Let's try a hyperbolic tangent (tanh) based approach.\n        # `tanh(a - b*slack)`:\n        # If `a` is large and `b` is positive, it will be close to 1 for small slack, and decrease.\n        # Example: `tanh(5 - 2*slack)`\n        # slack=0 -> tanh(5) ~ 1\n        # slack=1 -> tanh(3) ~ 1\n        # slack=2 -> tanh(1) ~ 0.76\n        # slack=3 -> tanh(-1) ~ -0.76\n        \n        # This favors small slack, but also gives reasonable scores for moderate slack.\n        # It penalizes large slack significantly.\n        \n        # Let's refine this: `tanh(a * (1 - slack/ideal_slack))`\n        # This will peak when slack = ideal_slack. We want peak at slack = 0.\n        \n        # Let's try a function that combines \"tightness\" and \"not-too-empty\".\n        # Priority = (1 / (1 + slack)) * (slack / (slack + C))  -- this will go to 0.\n        \n        # How about favoring bins that are not too close to full, nor too empty?\n        # This is more like \"Second Fit\".\n        \n        # The reflection: \"Favor tight fits, but explore slightly larger gaps for flexibility.\"\n        # This implies a function that is high for small slack, but the decay is gentle initially.\n        \n        # Let's use a modified inverse relationship.\n        # Instead of `1 / (1 + slack)`, consider `1 / (1 + slack^p)` with `p < 1`.\n        # Or `1 / (1 + sqrt(slack))`. This gives a higher score for small slacks compared to `1/slack`.\n        # Let's try `1 / (1 + slack**0.5)`\n        \n        # slack = 0.01: 1 / (1 + 0.1) = 0.909\n        # slack = 0.1:  1 / (1 + 0.316) = 0.76\n        # slack = 0.5:  1 / (1 + 0.707) = 0.58\n        # slack = 1.0:  1 / (1 + 1.0)   = 0.5\n        \n        # Compared to `1 / (1 + slack)`:\n        # slack = 0.01: 1 / (1 + 0.01) = 0.99\n        # slack = 0.1:  1 / (1 + 0.1)  = 0.909\n        # slack = 0.5:  1 / (1 + 0.5)  = 0.667\n        # slack = 1.0:  1 / (1 + 1.0)  = 0.5\n        \n        # The `slack**0.5` version penalizes larger slacks less harshly. This seems to align with \"explore slightly larger gaps\".\n        \n        # Let's add a small constant to the denominator to ensure non-zero priority even for zero slack.\n        # And maybe scale the slack to control the sensitivity.\n        \n        # Let `scaled_slack = slack / item` (fractional slack)\n        # Priority ~ `1 / (1 + scaled_slack**p)`\n        \n        # Alternative: Focus on the remaining capacity `rc = bins_remain_cap - item`.\n        # We want `rc` to be small and positive.\n        # Let's define a penalty function for `rc`.\n        # Penalty is 0 if `rc` is ideal (e.g., 0), increases as `rc` deviates.\n        # But we want to favor small `rc`.\n        \n        # How about: `exp(-k * rc)`? This is similar to sigmoid.\n        \n        # Let's try a combination: a base score for tightness, plus a bonus for moderate non-zero slack.\n        \n        # Base score: `1 / (1 + slack)`\n        # Bonus: Apply a Gaussian-like function centered at a small positive slack, e.g., 0.5.\n        # `bonus = exp(-(slack - 0.5)**2 / (2 * sigma**2))`\n        \n        # This could get complex. Let's simplify.\n        \n        # The reflection is key: \"Favor tight fits, but explore slightly larger gaps for flexibility.\"\n        # This means the priority function should decrease as slack increases, but not too rapidly.\n        \n        # A function like `1 / (1 + slack^p)` with `0 < p < 1` is a good candidate.\n        # Let's choose `p = 0.5` (square root) for a start.\n        \n        # `priorities[fittable_bins_mask] = 1.0 / (1.0 + np.sqrt(slack))`\n        \n        # To prevent division by zero if slack could be negative (which it can't here since we filter `bins_remain_cap >= item`),\n        # and to avoid extremely high priorities for zero slack, let's add a small epsilon.\n        \n        # Let's also consider scaling slack by item size to make it more relative.\n        # `relative_slack = slack / item` (handle item=0 case)\n        # `priority = 1.0 / (1.0 + np.sqrt(relative_slack))`\n        \n        # Let's use the absolute slack but scale its influence.\n        # `score = 1.0 / (1.0 + (slack / scale_factor)**p)`\n        \n        # A simple approach that balances tight fits with some room:\n        # Favor bins where `bins_remain_cap - item` is small.\n        # Also, consider the absolute `bins_remain_cap`. Larger bins might be useful for larger items later.\n        \n        # Let's consider the \"quality\" of the remaining space.\n        # A very small remaining space (tight fit) is good for immediate packing.\n        # A slightly larger remaining space is also good because it leaves room.\n        # A very large remaining space is less good, as it might indicate a poor fit for the current item.\n        \n        # Let's try a function that peaks at a small positive slack value.\n        # A log-normal or gamma-like shape can achieve this.\n        # Example: `slack**alpha * exp(-beta * slack)`\n        # To maximize this, take derivative and set to zero.\n        # `alpha*slack**(alpha-1)*exp(-beta*slack) - beta*slack**alpha*exp(-beta*slack) = 0`\n        # `alpha*slack**(-1) - beta = 0`\n        # `alpha/slack = beta` => `slack = alpha / beta`\n        # So, we can tune `alpha` and `beta` to have the peak at a desired slack.\n        \n        # Let `alpha = 2`, `beta = 4`. Peak at `slack = 2/4 = 0.5`.\n        # `priorities[fittable_bins_mask] = slack**2 * np.exp(-4*slack)`\n        # This function is 0 at slack=0, peaks at 0.5, and goes to 0 as slack increases.\n        # This might be *too* much exploration and not enough tight fit preference.\n        \n        # Let's stick to favoring small slack primarily, but with a less aggressive decay.\n        # `1 / (1 + slack^p)` with `p` around 0.5 seems a good balance.\n        \n        # Consider `p = 0.7`.\n        # slack = 0.01: 1 / (1 + 0.0046) = 0.995\n        # slack = 0.1:  1 / (1 + 0.1778) = 0.845\n        # slack = 0.5:  1 / (1 + 0.421)  = 0.703\n        # slack = 1.0:  1 / (1 + 1.0)    = 0.5\n        \n        # This function is higher for small slacks than `1/(1+slack^0.5)`, and decays slower.\n        # This means it favors tight fits, but also gives good scores to moderately tight fits,\n        # promoting exploration of slightly larger gaps.\n        \n        # Let's use `p = 0.7` as a parameter.\n        p_value = 0.7\n        \n        # Add a small epsilon to slack to avoid potential issues with `slack=0` and `p<1` leading to undefined behavior\n        # or excessively high values if `slack` could be extremely close to zero.\n        # However, `0**p` is 0 for `p>0`. So, `1/(1+0)` is 1. This is fine.\n        \n        # Let's use a scaling factor for slack to control the \"tightness\" preference.\n        # `scaled_slack = slack / item_size_scale`\n        # A scale factor related to average item size might be good.\n        # For now, let's use absolute slack with `p=0.7`.\n        \n        priorities[fittable_bins_mask] = 1.0 / (1.0 + slack**p_value)\n        \n        # Ensure priorities are not NaN or Inf (though unlikely with this formula and non-negative slack)\n        priorities[fittable_bins_mask] = np.nan_to_num(priorities[fittable_bins_mask], nan=0.0, posinf=1.0, neginf=0.0)\n        \n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 70.32403072095333,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response3.txt_stdout.txt",
    "code_path": "problem_iter4_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Best Fit strategy.\n\n    This strategy prioritizes bins that leave minimal remaining capacity after packing\n    the current item, similar to Best Fit. However, it also incorporates a penalty for\n    bins that have a very large remaining capacity, as these might be better saved\n    for larger future items, thus encouraging better overall packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities to 0\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # For bins that can fit the item, calculate the remaining capacity after placing the item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_placement = fitting_bins_remain_cap - item\n\n    # Best Fit component: Prioritize bins with minimal remaining capacity.\n    # We use the inverse of (1 + remaining_capacity) to give higher scores to smaller remaining capacities.\n    best_fit_scores = 1.0 / (1.0 + remaining_after_placement)\n\n    # Penalty for bins with very large remaining capacity:\n    # If a bin has significantly more capacity than needed (e.g., more than twice the item size),\n    # it might be better to save it for a larger item. We can penalize these bins.\n    # Let's define a threshold, e.g., capacity > 2 * item\n    large_capacity_penalty_factor = 0.5  # Adjust this factor to control the penalty\n    large_capacity_bins_mask = fitting_bins_remain_cap > 2 * item\n    penalty = np.ones_like(remaining_after_placement)\n    penalty[large_capacity_bins_mask] = large_capacity_penalty_factor\n\n    # Combine Best Fit score with the penalty\n    # We multiply the best_fit_scores by the penalty. Bins with large capacity will have their scores reduced.\n    combined_scores = best_fit_scores * penalty\n\n    # Assign the combined scores to the priorities array\n    priorities[can_fit_mask] = combined_scores\n\n    # Optional: Normalize priorities to sum to 1 for probabilistic selection.\n    # This is useful if the priority scores are to be used directly in a sampling mechanism.\n    # If the highest score should always be chosen, normalization is not strictly necessary.\n    fittable_scores_sum = np.sum(priorities[can_fit_mask])\n    if fittable_scores_sum > 0:\n        priorities[can_fit_mask] /= fittable_scores_sum\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 127.37720526058406,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response4.txt_stdout.txt",
    "code_path": "problem_iter4_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Sigmoid Fit Score with a penalty for very large capacities and exploration.\n\n    This version prioritizes bins that have a remaining capacity that is just\n    slightly larger than the item size. This aims to minimize wasted space in\n    the selected bin, leaving larger contiguous free spaces in other bins for\n    potentially larger future items.\n\n    The priority is calculated using a sigmoid function applied to the difference\n    between the bin's remaining capacity and the item's size. A smaller\n    positive difference (a tighter fit) results in a higher priority score.\n    Additionally, bins with very large remaining capacities (relative to the item)\n    are penalized to encourage spreading. A small probability of exploration is also included.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    exploration_prob = 0.05\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_capacities = bins_remain_cap[fittable_bins_mask]\n    slack = fittable_bins_capacities - item\n\n    # Sigmoid for tight fits (prioritize near-zero slack)\n    k_fit = 5.0\n    tight_fit_score = 1.0 / (1.0 + np.exp(k_fit * slack))\n\n    # Penalty for very large remaining capacities.\n    # We want to discourage putting a small item into a very large bin if a tighter fit exists.\n    # This can be modeled with a logistic function that decreases as capacity grows.\n    # Let's use a threshold and a decay factor.\n    # We can normalize capacities relative to the item size or bin capacity.\n    # A simple approach: Penalize bins whose remaining capacity is much larger than the item.\n    # Let's create a score that is high for capacities close to the item size and decreases.\n    # We can use a similar sigmoid but inverted or a different function.\n    # Alternative: use 1 / (1 + exp(-k_large * (capacity - threshold)))\n    # A simpler penalty: if capacity is > C * item, reduce score.\n\n    # Let's try a penalty based on normalized slack.\n    # We want the penalty to be low for slack close to 0 and high for large slack.\n    # This is the opposite of the tight fit score.\n    # We can use a decaying function of slack.\n    # e.g., 1 / (1 + slack) or exp(-slack / scale)\n    # Let's try a sigmoid on the negative slack to penalize larger slack.\n    k_penalty = 0.5 # Controls how quickly the penalty increases with slack\n    large_capacity_penalty = 1.0 / (1.0 + np.exp(-k_penalty * slack))\n\n    # Combine scores. The tight_fit_score is high for small slack.\n    # The large_capacity_penalty is high for small slack, and low for large slack.\n    # We want to combine them such that:\n    # 1. Small slack (tight fit) is good -> high tight_fit_score\n    # 2. Large slack (too much space) is bad -> needs a penalty\n    # Let's try to combine them additively.\n    # A higher score for tight fits, and a lower score for large slack.\n    # The large_capacity_penalty goes from ~0.5 to ~1 as slack increases. This is not a penalty.\n    # Let's re-think the penalty. We want to penalize large slack.\n    # The sigmoid `1 / (1 + exp(k * slack))` already penalizes large slack.\n    # So maybe just use a weighted combination of the tight fit score and the slack itself.\n    # Or, use the slack directly, but inverted and scaled.\n\n    # Let's re-evaluate the reflection: \"penalize large remaining capacities\"\n    # The `tight_fit_score` already does this by giving low scores to large slack.\n    # Maybe the reflection implies we should *also* consider the absolute capacity.\n    # For example, putting an item of size 1 into a bin with remaining capacity 100\n    # is worse than putting it into a bin with remaining capacity 1.\n    # The slack approach (capacity - item) naturally handles this:\n    # slack for (100, 1) is 99, slack for (1, 1) is 0.\n\n    # Let's reconsider the \"Worst Fit\" element from v0 and combine it with tight fit.\n    # Best Fit component (tight_fit_score) -> prioritizes small slack\n    # Worst Fit component (prioritizes bins with more remaining capacity) -> prioritizes large capacity\n    # This seems contradictory. The reflection \"Focus on tighter fits, penalize large remaining capacities\"\n    # suggests we should prioritize small slack and penalize large slack. The `tight_fit_score` does this.\n\n    # Let's refine the \"penalty for very large remaining capacities\".\n    # A simple approach: normalize slack by some reference, or use a threshold.\n    # Let's say if remaining_capacity > 2 * item, we start penalizing.\n    # Max slack we want to consider for high priority: say, up to `max_slack_ideal`.\n    # Any slack beyond that should be heavily penalized.\n    max_slack_ideal = 2.0 * item # An item of size 'item' should ideally fit into a bin with 2*item remaining capacity at most for a good fit.\n    # Penalize slack if it's much larger than max_slack_ideal.\n    # We can use a linear penalty or a sigmoid that drops sharply.\n    # Let's use a sigmoid that is high for slack <= max_slack_ideal and low for slack > max_slack_ideal.\n    # Use `1 / (1 + exp(k_penalty * (slack - max_slack_ideal)))`\n    # This gives 0.5 at slack = max_slack_ideal, decreases for slack > max_slack_ideal.\n    k_penalty = 1.0 # Controls the steepness of the penalty\n    penalty_score = 1.0 / (1.0 + np.exp(k_penalty * (slack - max_slack_ideal)))\n\n    # Combine tight_fit_score and penalty_score.\n    # We want both to contribute positively.\n    # tight_fit_score is high for small slack.\n    # penalty_score is high for small slack (and slack <= max_slack_ideal).\n    # So, a linear combination might work.\n    # Let's weight them. Give more weight to the tight fit.\n    combined_scores = 0.7 * tight_fit_score + 0.3 * penalty_score\n\n    priorities[fittable_bins_mask] = combined_scores\n\n    # Apply exploration\n    if np.random.rand() < exploration_prob:\n        fittable_indices = np.where(fittable_bins_mask)[0]\n        chosen_bin_global_index = np.random.choice(fittable_indices)\n        priorities.fill(0)\n        priorities[chosen_bin_global_index] = 1.0\n    else:\n        # Normalize priorities for fittable bins to sum to 1\n        fittable_priorities = priorities[fittable_bins_mask]\n        if np.sum(fittable_priorities) > 0:\n            priorities[fittable_bins_mask] /= np.sum(fittable_priorities)\n\n    return priorities",
    "response_id": 4,
    "obj": 4.098524132429212,
    "cyclomatic_complexity": 4.0,
    "halstead": 322.4095353505972,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response5.txt_stdout.txt",
    "code_path": "problem_iter4_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Almost Full Fit strategy.\n\n    This strategy prioritizes bins that provide the \"tightest fit\" for the item,\n    meaning the remaining capacity after insertion is minimized. This is achieved\n    by maximizing `1.0 / (1.0 + slack)`, where slack is `remaining_capacity - item_size`.\n    Additionally, it introduces a secondary preference for bins that are already\n    \"fuller\" (i.e., have less initial remaining capacity) among those offering similar\n    tight fits, to promote more balanced packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate the slack for bins that can fit the item\n    # Slack = remaining_capacity - item_size\n    slack = bins_remain_cap[fit_mask] - item\n\n    # Primary scoring: Prioritize minimal slack (tightest fits).\n    # Use `1.0 / (1.0 + slack)`: higher score for smaller slack.\n    # A slack of 0 gives priority 1.0.\n    primary_score = 1.0 / (1.0 + slack)\n\n    # Secondary scoring: Among bins with similar slacks, prioritize those that are\n    # \"fuller\" initially. This means preferring bins with smaller original `bins_remain_cap`.\n    # We can add a term proportional to the negative of the initial remaining capacity.\n    # A small coefficient `alpha` ensures the primary criterion (tightness) dominates.\n    alpha = 0.01\n    secondary_score = -alpha * bins_remain_cap[fit_mask]\n\n    # Combine scores: maximize primary score, then secondary score\n    priorities[fit_mask] = primary_score + secondary_score\n\n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 81.7492568250068,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response6.txt_stdout.txt",
    "code_path": "problem_iter4_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit strategy with a slight random perturbation.\n\n    The Best Fit strategy aims to minimize wasted space by selecting the bin that\n    has the least remaining capacity after the item is placed. To encourage\n    exploration and avoid getting stuck in local optima, a small random value is\n    added to the priority of eligible bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Identify bins that can accommodate the item\n    eligible_bins_mask = bins_remain_cap >= item\n\n    # Initialize priorities to a very low value for bins that cannot accommodate the item.\n    # This ensures they are never chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    if np.any(eligible_bins_mask):\n        # For eligible bins, calculate a priority based on the remaining capacity.\n        # The Best Fit strategy prioritizes bins with the smallest remaining capacity\n        # after placing the item. Therefore, we use -(bins_remain_cap - item).\n        # A small random perturbation is added to break ties and introduce some\n        # stochasticity, which can help in escaping local optima.\n        # The smaller the remaining capacity after placement (bins_remain_cap - item),\n        # the higher the priority (less negative value).\n        priorities[eligible_bins_mask] = -(bins_remain_cap[eligible_bins_mask] - item) + np.random.rand(np.sum(eligible_bins_mask)) * 1e-6\n\n    return priorities",
    "response_id": 6,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 60.91767875292166,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response7.txt_stdout.txt",
    "code_path": "problem_iter4_code7.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, prioritizing fuller bins first, then best fit.\n\n    This heuristic prioritizes bins that are already fuller (have less remaining capacity).\n    If multiple bins have the same minimal remaining capacity, it then applies the Best Fit\n    principle to choose the one that results in the least waste. This aims to pack items\n    more densely by preferring bins that are closer to being full.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher score indicates higher priority. Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Create a boolean mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    fittable_bins_capacities = bins_remain_cap[can_fit_mask]\n\n    # Primary goal: Prioritize fuller bins. This means prioritizing bins with smaller remaining capacity.\n    # We can use the inverse of remaining capacity as a score. Higher score for smaller capacity.\n    # `fullness_score = 1.0 / (fittable_bins_capacities + epsilon)`\n\n    # Secondary goal: Among equally full bins, prioritize the best fit (minimize waste).\n    # Waste = `fittable_bins_capacities - item`.\n    # Best fit score = `1.0 / (waste + epsilon)`\n\n    # Combine the two: We want to prioritize bins with smaller `bins_remain_cap` first,\n    # and then smaller `fittable_bins_capacities - item` as a tie-breaker.\n    # A single score that achieves this is to create a composite key:\n    # `composite_key = (fittable_bins_capacities, fittable_bins_capacities - item)`\n    # We want to minimize this composite key lexicographically.\n    # For priority (maximize), we want to maximize the inverse of this key.\n    # A simple way to combine them into a single score for maximization is:\n    # `priority = (1.0 / (fittable_bins_capacities + epsilon)) + (1.0 / (fittable_bins_capacities - item + epsilon))`\n    # This gives a higher score to bins that are both fuller and have less waste.\n    # Let's test this combination:\n    # Bin A: remain_cap=0.6, item=0.5 -> waste=0.1. Score = 1/0.6 + 1/0.1 = 1.667 + 10 = 11.667\n    # Bin B: remain_cap=0.7, item=0.5 -> waste=0.2. Score = 1/0.7 + 1/0.2 = 1.428 + 5 = 6.428\n    # Bin C: remain_cap=0.6, item=0.4 -> waste=0.2. Score = 1/0.6 + 1/0.2 = 1.667 + 5 = 6.667\n    # Bin A (fullest, best fit) gets highest score. Bin C (equally full as A, worse fit) gets second highest. Bin B (less full, worse fit) gets lowest.\n    # This combination seems to correctly prioritize fullness first, then best fit.\n\n    combined_score = (1.0 / (fittable_bins_capacities + epsilon)) + \\\n                     (1.0 / (fittable_bins_capacities - item + epsilon))\n\n    priorities[can_fit_mask] = combined_score\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 92.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response8.txt_stdout.txt",
    "code_path": "problem_iter4_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, prioritizing minimal waste and then fuller bins.\n\n    This strategy aims to find bins where the remaining capacity is just enough to fit the item (minimizing slack).\n    Among bins that offer a similar \"tight fit\" (i.e., similar slack), it prioritizes those that are already fuller\n    (have less remaining capacity).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate the \"slack\" (waste after insertion)\n    slack = bins_remain_cap[can_fit_mask] - item\n\n    # To prioritize minimal waste (smallest slack), we want a score that is\n    # higher for smaller slack values. A common way is `1.0 / (slack + epsilon)`.\n    # This maps slack [0, infinity) to priority (1, 0].\n\n    # To prioritize fuller bins among those with similar slack, we want a score\n    # that is higher for smaller `bins_remain_cap`. A common way is `1.0 / bins_remain_cap`.\n\n    # Combine these two objectives additively.\n    # The primary objective is minimal slack, so its term should ideally dominate.\n    # However, a simple sum often works well, assuming the scales are somewhat managed.\n    # Using `1.0 / (slack + epsilon)` for minimal waste.\n    # Using `1.0 / (bins_remain_cap[can_fit_mask] + epsilon)` for fuller bins preference.\n    # Adding a small epsilon to denominators to prevent division by zero and to avoid\n    # issues with exact zero remaining capacities (though unlikely in BPP context, good practice).\n\n    # If slack is 0, the first term is ~1. If slack is large, first term approaches 0.\n    # If bin_remain_cap is small (fuller bin), second term is large. If bin_remain_cap is large, second term is small.\n    # This combination correctly prioritizes bins with small slack and small remaining capacity.\n    \n    epsilon = 1e-9  # Small value to prevent division by zero\n\n    # Calculate the score for fitting bins\n    # Score = (Inverse of slack) + (Inverse of remaining capacity)\n    # Higher score means better priority\n    priorities[can_fit_mask] = (1.0 / (slack + epsilon)) + (1.0 / (bins_remain_cap[can_fit_mask] + epsilon))\n\n    return priorities",
    "response_id": 8,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 82.0447025077789,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response9.txt_stdout.txt",
    "code_path": "problem_iter4_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined \"Almost Full Fit\" strategy.\n\n    This strategy prioritizes bins that offer the tightest fit for the item, meaning\n    the remaining capacity after insertion is minimized. This is achieved by maximizing\n    the negative of the slack (remaining_capacity - item_size).\n    To also incorporate the \"favoring fuller bins\" aspect (i.e., preferring bins that\n    were already closer to full before insertion), a secondary prioritization is\n    applied: preferring bins with smaller initial remaining capacities.\n\n    The combined priority is a weighted sum designed to maximize:\n    -minimize(slack) - alpha * minimize(initial_remaining_capacity)\n    This translates to maximizing:\n    -slack - alpha * initial_remaining_capacity\n    which is equivalent to:\n    -(bins_remain_cap - item) - alpha * bins_remain_cap\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        # If no bins can fit the item, return all zeros (no priority)\n        return priorities\n\n    # Calculate the slack for bins that can fit the item\n    # slack = remaining_capacity - item_size\n    slack = bins_remain_cap[fit_mask] - item\n\n    # Primary criterion: Minimize slack (tight fit). Maximize -(slack).\n    # Secondary criterion: Minimize initial remaining capacity (favor fuller bins). Maximize -(bins_remain_cap).\n    # We combine them, giving slightly more weight to the tight fit.\n    # The expression is: -slack - alpha * bins_remain_cap\n    # This means we are maximizing: (item - bins_remain_cap) - alpha * bins_remain_cap\n    # Or more directly, maximizing the negative of (slack + alpha * bins_remain_cap)\n\n    alpha = 0.1  # Weight for the secondary criterion (smaller bins)\n\n    # Calculate the combined priority for fitting bins\n    # We want to maximize -(slack + alpha * bins_remain_cap[fit_mask])\n    # This is equivalent to maximizing (item - bins_remain_cap[fit_mask]) - alpha * bins_remain_cap[fit_mask]\n    # Let's rewrite to avoid confusion with maximization: we assign a score where higher is better.\n    # High score = small slack AND small initial capacity.\n    # Score = -slack - alpha * initial_capacity\n    #       = -(bins_remain_cap[fit_mask] - item) - alpha * bins_remain_cap[fit_mask]\n    #       = item - bins_remain_cap[fit_mask] - alpha * bins_remain_cap[fit_mask]\n\n    # Let's try the simpler approach of maximizing the negative slack, and then using\n    # the negative initial capacity as a tie-breaker.\n    # Priority = -slack - alpha * bins_remain_cap[fit_mask]\n    priorities[fit_mask] = -slack - alpha * bins_remain_cap[fit_mask]\n\n    # Example: item = 3\n    # bins_remain_cap = [5, 4, 7]\n    # Bin 0: cap=5, slack=2. priority = -2 - 0.1*5 = -2.5\n    # Bin 1: cap=4, slack=1. priority = -1 - 0.1*4 = -1.4\n    # Bin 2: cap=7, slack=4. priority = -4 - 0.1*7 = -4.7\n\n    # Bin 1 has the highest priority (-1.4), which is correct (tightest fit).\n    # Bin 0 has the second highest priority (-2.5), which is also correct (smaller initial capacity than Bin 2, though slack is larger).\n    # Bin 2 has the lowest priority (-4.7).\n\n    # Example: item = 3\n    # bins_remain_cap = [5, 5]\n    # Bin 0: cap=5, slack=2. priority = -2 - 0.1*5 = -2.5\n    # Bin 1: cap=5, slack=2. priority = -2 - 0.1*5 = -2.5\n    # Tie. This is expected as both criteria are equal. The selection would be arbitrary between them.\n\n    return priorities",
    "response_id": 9,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 60.91767875292166,
    "exec_success": true
  }
]