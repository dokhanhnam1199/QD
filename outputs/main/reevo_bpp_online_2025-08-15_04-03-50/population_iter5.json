[
  {
    "stdout_filepath": "problem_iter4_response0.txt_stdout.txt",
    "code_path": "problem_iter4_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined strategy.\n\n    This strategy prioritizes bins that offer a \"tight fit\" for the item,\n    meaning the remaining capacity after insertion is minimized. It achieves this\n    by assigning a priority score based on the inverse of (1 + slack), where slack\n    is the difference between the bin's remaining capacity and the item's size.\n\n    To further balance this with the \"fullness\" of bins (prioritizing bins that\n    are already closer to full, i.e., have less remaining capacity overall), a small\n    secondary score is added. This secondary score is proportional to the negative\n    of the bin's remaining capacity, effectively giving a slight preference to\n    smaller bins when tight fits are comparable.\n\n    A small amount of noise is added to the priority scores to introduce a stochastic\n    element, promoting exploration and helping to escape local optima, as suggested\n    by the \"balance with random exploration for robustness\" aspect of the reflection.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities\n\n    # Calculate slack for bins that can fit the item\n    # Slack = remaining_capacity - item_size\n    slack = bins_remain_cap[fit_mask] - item\n\n    # Primary scoring: Favor tight fits by using 1 / (1 + slack).\n    # This assigns higher scores to bins with smaller positive slack.\n    tight_fit_scores = 1.0 / (1.0 + slack)\n\n    # Secondary scoring: Favor \"fuller\" bins (those with less initial remaining capacity).\n    # We use a small negative term proportional to the bin's remaining capacity.\n    # Maximizing this term means minimizing bins_remain_cap for fitting bins.\n    # The small multiplier ensures tight fit is primary.\n    fullness_scores = -0.1 * bins_remain_cap[fit_mask]\n\n    # Combine scores\n    combined_scores = tight_fit_scores + fullness_scores\n\n    # Add a small amount of random noise to encourage exploration.\n    # This helps to break ties and explore different packing configurations.\n    noise = np.random.normal(0, 0.01, size=combined_scores.shape)\n    priorities[fit_mask] = combined_scores + noise\n\n    # Normalize priorities so they sum to 1 over the fittable bins.\n    # This converts scores into probabilities if using a probabilistic selection method.\n    # If deterministic selection (e.g., argmax) is used, normalization isn't strictly necessary\n    # but can help in interpreting relative preferences.\n    current_priorities = priorities[fit_mask]\n    if np.any(current_priorities):\n        # Ensure no division by zero if all priorities are zero (though unlikely with noise)\n        sum_priorities = np.sum(current_priorities)\n        if sum_priorities > 0:\n            priorities[fit_mask] = current_priorities / sum_priorities\n        else:\n            # If for some reason all priorities are non-positive, assign equal probability\n            # to all fittable bins.\n            priorities[fit_mask] = 1.0 / len(current_priorities) if len(current_priorities) > 0 else 0\n\n    return priorities",
    "response_id": 0,
    "obj": 4.487435181491823,
    "cyclomatic_complexity": 5.0,
    "halstead": 175.93083758004835,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response1.txt_stdout.txt",
    "code_path": "problem_iter4_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a strategy that prioritizes bins with minimal remaining space after packing, and among those, favors bins that are fuller.\n\n    This strategy is a refinement of Best Fit.\n    1. Primary objective: Minimize the \"slack\" (remaining capacity - item size).\n       Bins with slack closer to zero are preferred.\n    2. Secondary objective: Among bins with the same slack, prefer bins that are fuller.\n       A fuller bin has less remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate slack for fittable bins\n    # Slack = remaining_capacity - item_size\n    slack = bins_remain_cap[can_fit_mask] - item\n\n    # Primary scoring: Minimize slack. We want to maximize a function that is\n    # large for small slack. Using 1.0 / (slack + epsilon) achieves this.\n    # A slack of 0 gives the highest score (1.0).\n    best_fit_score = 1.0 / (slack + 1e-9)\n\n    # Secondary scoring: Favor fuller bins. This means preferring bins with\n    # smaller remaining capacity among those with the same slack.\n    # We can achieve this by maximizing 1.0 / bins_remain_cap.\n    fuller_bin_score = 1.0 / (bins_remain_cap[can_fit_mask] + 1e-9)\n\n    # Combine scores. We want slack to be the primary driver.\n    # A simple way to combine is to add them, but to ensure slack is dominant,\n    # we can multiply by a factor that makes the slack score much larger.\n    # Alternatively, we can use a weighted sum where the slack component\n    # has a larger weight.\n    # Let's try a composite score where we consider slack first, and then remaining capacity.\n    # A common way to achieve lexicographical ordering is to use a base and scaled values.\n    # For example: score = PrimaryScore * Scale + SecondaryScore\n    # Here, we want to maximize `best_fit_score` and `fuller_bin_score`.\n    # If `best_fit_score` is the primary, we want it to have a higher impact.\n\n    # Let's use a weighting approach.\n    # Weight for best fit: Give it a higher weight to ensure it's considered first.\n    # Weight for fuller bin: Give it a smaller weight as it's a secondary preference.\n    # However, directly adding might lead to issues if the scales are very different.\n\n    # A robust way for \"minimize A, then minimize B\" is to maximize (-A - alpha*B).\n    # So, we want to maximize `-(slack) - alpha * (bins_remain_cap)`.\n    # This translates to maximizing `-(slack + alpha * bins_remain_cap)`.\n    # Let's use alpha = 1, and see how it performs.\n    # The priority will be `- (slack + bins_remain_cap[can_fit_mask])`.\n    # This means smaller values of `slack + bins_remain_cap` get higher priority.\n    # Example:\n    # Bin A: slack=0, rem_cap=5  => sum=5, priority = -5\n    # Bin B: slack=0, rem_cap=10 => sum=10, priority = -10\n    # Bin C: slack=1, rem_cap=5  => sum=6, priority = -6\n    # Bin D: slack=1, rem_cap=10 => sum=11, priority = -11\n    # This correctly prioritizes Bin A (best fit, fullest), then Bin C (good fit, fullest), then Bin B (best fit, less full), then Bin D.\n\n    # However, the reflection asked to \"prioritize fuller bins by inverting remaining capacity for better fit.\"\n    # This implies that `1.0 / bins_remain_cap` should contribute positively to the priority.\n    # So, we want to maximize `1.0 / (slack + epsilon)` AND maximize `1.0 / (bins_remain_cap + epsilon)`.\n    # Let's use a simple additive approach, ensuring the best-fit component is primary.\n    # We can scale the \"fuller bin\" score.\n    # `priorities[can_fit_mask] = best_fit_score + 0.1 * fuller_bin_score`\n    # The `0.1` is a heuristic weight. It makes the \"fuller bin\" score less influential than \"best fit\".\n    # For example, if slack is 0, BF score is 1. If rem_cap is 5, FB score is 0.2. Total 1.2.\n    # If slack is 0, rem_cap is 10, BF score is 1. FB score is 0.1. Total 1.1.\n    # This correctly prioritizes the fuller bin among those with the same slack.\n\n    # Let's consider the case where slack is small but not zero.\n    # slack=0.1, rem_cap=5. BF score = 1 / 1.1 = 0.909. FB score = 0.2. Total = 1.109.\n    # slack=0, rem_cap=10. BF score = 1.0. FB score = 0.1. Total = 1.1.\n    # In this case, the slack=0.1 bin is prioritized over the slack=0, fuller bin. This is not ideal.\n\n    # The reflection implies a stronger preference for fuller bins when slack is the same.\n    # Let's combine them in a way that ensures the primary objective (minimal slack) is paramount,\n    # and the secondary objective (fuller bins) breaks ties effectively.\n\n    # A common way is to sort by (slack, bins_remain_cap). We want the smallest such pair.\n    # For priority, we want to maximize a score reflecting this.\n    # So, we want to maximize `(-slack, -bins_remain_cap)`.\n    # We can achieve this by converting to a single score: `-slack - alpha * bins_remain_cap`.\n    # If alpha is chosen large enough, `-slack` dominates.\n    # Let's use `alpha = 1`. The score is `-(slack + bins_remain_cap[can_fit_mask])`.\n    # This means the highest priority (least negative) is for the smallest `slack + bins_remain_cap`.\n\n    # Example:\n    # Bin A: slack=0, rem_cap=5  => sum=5, priority = -5\n    # Bin B: slack=0, rem_cap=10 => sum=10, priority = -10\n    # Bin C: slack=1, rem_cap=5  => sum=6, priority = -6\n    # Bin D: slack=1, rem_cap=10 => sum=11, priority = -11\n    # This seems to match the desired behavior: A is best, then C, then B, then D.\n    # This priority function directly implements minimizing slack first, then minimizing remaining capacity.\n\n    priorities[can_fit_mask] = -(slack + bins_remain_cap[can_fit_mask])\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 104.2481250360578,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response2.txt_stdout.txt",
    "code_path": "problem_iter4_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a hybrid strategy.\n\n    This strategy aims to balance the preference for tight fits (minimizing wasted space\n    in the immediate bin) with a slight preference for bins that offer a bit more\n    room, promoting better global packing. It uses a combination of a \"Best Fit\"\n    (minimal slack) component and a \"First Fit\"-like component (favoring bins\n    that are not too empty).\n\n    The priority is calculated based on the negative slack (prioritizing bins\n    where `bins_remain_cap - item` is small and positive). To encourage exploration\n    and prevent premature filling of slightly-too-small gaps that might be better\n    suited for smaller items, a small penalty is applied to bins with extremely\n    tight fits, and a bonus is given to bins with a moderate amount of slack.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if np.any(fittable_bins_mask):\n        fittable_bins_capacities = bins_remain_cap[fittable_bins_mask]\n\n        # Calculate slack: remaining capacity after fitting the item.\n        # We want slack to be as close to 0 as possible.\n        slack = fittable_bins_capacities - item\n\n        # Heuristic:\n        # 1. Prioritize bins with small positive slack (tight fits).\n        # 2. Avoid extremely tight fits (slack very close to 0) by slightly penalizing them.\n        # 3. Give a small bonus to bins with moderate slack to encourage using \"less full\" bins sometimes.\n        # This can be achieved by mapping slack to a score where a small positive value is optimal.\n\n        # Using a quadratic function: -(slack - ideal_slack)^2\n        # Let's aim for an \"ideal\" slack of, say, 10% of the item size,\n        # but with a floor to avoid negative ideal slack for very small items.\n        ideal_slack = max(0.1 * item, 0.1) # Aim for ~10% of item size, minimum 0.1\n        \n        # Penalize deviations from the ideal slack.\n        # A higher value means closer to ideal_slack is better.\n        # We want to maximize this value.\n        # The function `-(slack - ideal_slack)**2` will be maximized when slack is close to ideal_slack.\n        # However, we also want to prioritize *tight* fits, meaning slack near 0 should be good.\n        \n        # Let's try a different approach: Reward tightness, but with diminishing returns.\n        # Prioritize bins where (bin_capacity - item) is small.\n        # A simple reward for tightness: 1 / (1 + slack)\n        # To balance with exploration: maybe add a small bonus if slack is small but not zero.\n        \n        # Hybrid approach:\n        # Prioritize bins with minimal slack (close to 0). This is the \"Best Fit\" aspect.\n        # But also give a slight preference to bins with a moderate amount of slack\n        # to avoid leaving many bins in a state that's hard to fill.\n\n        # A scoring function that peaks at slack = 0 and then decreases,\n        # but perhaps less sharply for very small slacks.\n        \n        # Let's use a scaled inverse relationship with slack, and add a small bonus for moderate slack.\n        # For very small slack (tight fit): prioritize.\n        # For moderate slack: also consider it favorable.\n        # For large slack: disfavor.\n        \n        # Strategy: Base priority on inverse slack (favoring small slack),\n        # but add a small bonus proportional to slack up to a certain point,\n        # and then penalize larger slacks.\n\n        # Let's try a function that is high for slack near 0, then decreases.\n        # Consider a penalty for slack that is too small (e.g., < 0.1) and too large.\n        \n        # A common heuristic for balancing is to use a function that is high for \"just right\".\n        # For bin packing, \"just right\" is usually a tight fit.\n        # To encourage exploration, we can slightly boost bins that aren't *exactly* the tightest.\n        \n        # Let's use a function that rewards closeness to 0 slack, but less aggressively for very small slacks.\n        # And perhaps a small bonus for slack that is not excessively large.\n\n        # Option 1: Gamma distribution-like scoring based on slack\n        # Prioritize small slack, with a peak somewhere slightly above zero.\n        # Example: (slack^a) * exp(-b*slack)\n        # This can be tricky to tune.\n\n        # Option 2: Combining a \"Best Fit\" component with a \"Least Full\" component.\n        # Best Fit: Maximize (1 / (1 + slack))\n        # Least Full: Maximize (bins_remain_cap) -- but we want to avoid *large* bins, so this isn't direct.\n        \n        # Let's try to directly score bins based on how \"good\" their remaining space is.\n        # A good remaining space is small, but not so small that it becomes unusable.\n        \n        # Score: Maximize (1 / (1 + slack)) - prioritize tight fits.\n        # Bonus for moderate slack: add a small value if slack is between X and Y.\n        \n        # Let's consider slack values.\n        # slack = 0: Best.\n        # slack = 0.1: Good.\n        # slack = 1.0: Okay.\n        # slack = 5.0: Not great.\n        \n        # A function like: `exp(-slack / constant)` will heavily favor small slacks.\n        # To add exploration: `exp(-slack / constant) + bonus_if_slack_is_moderate`\n        \n        # Let's try a concave function that peaks at slack=0, but with a flattened initial slope.\n        # This can be achieved by `1 - exp(-k*slack)`. This maps slack=0 to 0, slack=inf to 1.\n        # We want the opposite: slack=0 should be high.\n        \n        # Revisit the \"Worse code\" logic: `1 / (1 + slack)` is good for favoring tight fits.\n        # The reflection asks to favor tight fits but explore slightly larger gaps.\n        # This means we want the function to be high for small slack, but not drop off *too* quickly.\n        \n        # Let's use a sigmoidal function that starts high for small slacks and gradually decreases.\n        # A sigmoid shifted and scaled.\n        \n        # Consider slack values:\n        # If slack is very small (e.g., 0.01), we want a high score.\n        # If slack is moderate (e.g., 0.5), we want a reasonably high score, maybe slightly lower than 0.01.\n        # If slack is large (e.g., 2.0), we want a low score.\n        \n        # A function like `1 / (1 + slack**p)` where p is between 0 and 1 might work.\n        # If p=1, it's `1/(1+slack)` (original logic).\n        # If p=0.5, it's `1/(1 + sqrt(slack))`. This penalizes large slacks less.\n        \n        # Let's use a scaling factor for slack to control the steepness of the decay.\n        # And perhaps add a small constant to slack to avoid division by zero and\n        # give a base priority to bins with zero slack.\n        \n        # Let's try a heuristic that penalizes large remaining capacities more heavily.\n        # We want to prefer bins that, after packing, have *some* remaining capacity, but not too much.\n        \n        # Consider the remaining capacity *after* packing: `bins_remain_cap - item`.\n        # We want this to be small and positive.\n        \n        # Score = f(bins_remain_cap - item)\n        # f(x) should be high for small positive x.\n        \n        # Let's use `1 / (1 + slack)` as a base, but then add a small bonus for slacks within a certain range.\n        # Or, modify the `slack` value itself before applying `1 / (1 + slack)`.\n        \n        # Idea: Apply a transformation to slack that emphasizes values close to 0.\n        # Let `transformed_slack = slack / (slack + constant)`\n        # As slack -> 0, transformed_slack -> 0.\n        # As slack -> inf, transformed_slack -> 1.\n        # So, priority could be `1 - transformed_slack = constant / (slack + constant)`\n        # This is similar to `1 / (1 + slack/constant)`.\n        \n        # Let's try to incorporate the \"just right\" idea.\n        # If slack is too small, it's bad (maybe the item almost doesn't fit).\n        # If slack is too large, it's bad (wasted space).\n        # If slack is moderate, it's good.\n        \n        # This suggests a function with a peak.\n        # However, for Bin Packing, the primary goal is still to fit items efficiently.\n        # The \"exploration\" part should probably not override the fundamental \"fit tightly\" heuristic too much.\n        \n        # Let's use a softened Best Fit approach.\n        # Instead of `1 / (1 + slack)`, use a function that is less steep for small slacks.\n        # Consider `log(1 + 1/slack)` for slack > 0. This also favors small slack.\n        \n        # Let's go back to the idea of preferring bins with small positive remaining capacity.\n        # Priority is inversely related to `bins_remain_cap - item`.\n        \n        # Consider a function `g(remaining_capacity)` where `remaining_capacity = bins_remain_cap - item`.\n        # We want `g(x)` to be high for small positive `x`.\n        \n        # `g(x) = 1 / (1 + x)`: Peaks at 1 for x=0, decreases.\n        # `g(x) = exp(-k*x)`: Peaks at 1 for x=0, decreases.\n        \n        # To encourage exploration of slightly larger gaps:\n        # We want the function to decrease slower for small `x`.\n        \n        # Let's try a hyperbolic tangent (tanh) based approach.\n        # `tanh(a - b*slack)`:\n        # If `a` is large and `b` is positive, it will be close to 1 for small slack, and decrease.\n        # Example: `tanh(5 - 2*slack)`\n        # slack=0 -> tanh(5) ~ 1\n        # slack=1 -> tanh(3) ~ 1\n        # slack=2 -> tanh(1) ~ 0.76\n        # slack=3 -> tanh(-1) ~ -0.76\n        \n        # This favors small slack, but also gives reasonable scores for moderate slack.\n        # It penalizes large slack significantly.\n        \n        # Let's refine this: `tanh(a * (1 - slack/ideal_slack))`\n        # This will peak when slack = ideal_slack. We want peak at slack = 0.\n        \n        # Let's try a function that combines \"tightness\" and \"not-too-empty\".\n        # Priority = (1 / (1 + slack)) * (slack / (slack + C))  -- this will go to 0.\n        \n        # How about favoring bins that are not too close to full, nor too empty?\n        # This is more like \"Second Fit\".\n        \n        # The reflection: \"Favor tight fits, but explore slightly larger gaps for flexibility.\"\n        # This implies a function that is high for small slack, but the decay is gentle initially.\n        \n        # Let's use a modified inverse relationship.\n        # Instead of `1 / (1 + slack)`, consider `1 / (1 + slack^p)` with `p < 1`.\n        # Or `1 / (1 + sqrt(slack))`. This gives a higher score for small slacks compared to `1/slack`.\n        # Let's try `1 / (1 + slack**0.5)`\n        \n        # slack = 0.01: 1 / (1 + 0.1) = 0.909\n        # slack = 0.1:  1 / (1 + 0.316) = 0.76\n        # slack = 0.5:  1 / (1 + 0.707) = 0.58\n        # slack = 1.0:  1 / (1 + 1.0)   = 0.5\n        \n        # Compared to `1 / (1 + slack)`:\n        # slack = 0.01: 1 / (1 + 0.01) = 0.99\n        # slack = 0.1:  1 / (1 + 0.1)  = 0.909\n        # slack = 0.5:  1 / (1 + 0.5)  = 0.667\n        # slack = 1.0:  1 / (1 + 1.0)  = 0.5\n        \n        # The `slack**0.5` version penalizes larger slacks less harshly. This seems to align with \"explore slightly larger gaps\".\n        \n        # Let's add a small constant to the denominator to ensure non-zero priority even for zero slack.\n        # And maybe scale the slack to control the sensitivity.\n        \n        # Let `scaled_slack = slack / item` (fractional slack)\n        # Priority ~ `1 / (1 + scaled_slack**p)`\n        \n        # Alternative: Focus on the remaining capacity `rc = bins_remain_cap - item`.\n        # We want `rc` to be small and positive.\n        # Let's define a penalty function for `rc`.\n        # Penalty is 0 if `rc` is ideal (e.g., 0), increases as `rc` deviates.\n        # But we want to favor small `rc`.\n        \n        # How about: `exp(-k * rc)`? This is similar to sigmoid.\n        \n        # Let's try a combination: a base score for tightness, plus a bonus for moderate non-zero slack.\n        \n        # Base score: `1 / (1 + slack)`\n        # Bonus: Apply a Gaussian-like function centered at a small positive slack, e.g., 0.5.\n        # `bonus = exp(-(slack - 0.5)**2 / (2 * sigma**2))`\n        \n        # This could get complex. Let's simplify.\n        \n        # The reflection is key: \"Favor tight fits, but explore slightly larger gaps for flexibility.\"\n        # This means the priority function should decrease as slack increases, but not too rapidly.\n        \n        # A function like `1 / (1 + slack^p)` with `0 < p < 1` is a good candidate.\n        # Let's choose `p = 0.5` (square root) for a start.\n        \n        # `priorities[fittable_bins_mask] = 1.0 / (1.0 + np.sqrt(slack))`\n        \n        # To prevent division by zero if slack could be negative (which it can't here since we filter `bins_remain_cap >= item`),\n        # and to avoid extremely high priorities for zero slack, let's add a small epsilon.\n        \n        # Let's also consider scaling slack by item size to make it more relative.\n        # `relative_slack = slack / item` (handle item=0 case)\n        # `priority = 1.0 / (1.0 + np.sqrt(relative_slack))`\n        \n        # Let's use the absolute slack but scale its influence.\n        # `score = 1.0 / (1.0 + (slack / scale_factor)**p)`\n        \n        # A simple approach that balances tight fits with some room:\n        # Favor bins where `bins_remain_cap - item` is small.\n        # Also, consider the absolute `bins_remain_cap`. Larger bins might be useful for larger items later.\n        \n        # Let's consider the \"quality\" of the remaining space.\n        # A very small remaining space (tight fit) is good for immediate packing.\n        # A slightly larger remaining space is also good because it leaves room.\n        # A very large remaining space is less good, as it might indicate a poor fit for the current item.\n        \n        # Let's try a function that peaks at a small positive slack value.\n        # A log-normal or gamma-like shape can achieve this.\n        # Example: `slack**alpha * exp(-beta * slack)`\n        # To maximize this, take derivative and set to zero.\n        # `alpha*slack**(alpha-1)*exp(-beta*slack) - beta*slack**alpha*exp(-beta*slack) = 0`\n        # `alpha*slack**(-1) - beta = 0`\n        # `alpha/slack = beta` => `slack = alpha / beta`\n        # So, we can tune `alpha` and `beta` to have the peak at a desired slack.\n        \n        # Let `alpha = 2`, `beta = 4`. Peak at `slack = 2/4 = 0.5`.\n        # `priorities[fittable_bins_mask] = slack**2 * np.exp(-4*slack)`\n        # This function is 0 at slack=0, peaks at 0.5, and goes to 0 as slack increases.\n        # This might be *too* much exploration and not enough tight fit preference.\n        \n        # Let's stick to favoring small slack primarily, but with a less aggressive decay.\n        # `1 / (1 + slack^p)` with `p` around 0.5 seems a good balance.\n        \n        # Consider `p = 0.7`.\n        # slack = 0.01: 1 / (1 + 0.0046) = 0.995\n        # slack = 0.1:  1 / (1 + 0.1778) = 0.845\n        # slack = 0.5:  1 / (1 + 0.421)  = 0.703\n        # slack = 1.0:  1 / (1 + 1.0)    = 0.5\n        \n        # This function is higher for small slacks than `1/(1+slack^0.5)`, and decays slower.\n        # This means it favors tight fits, but also gives good scores to moderately tight fits,\n        # promoting exploration of slightly larger gaps.\n        \n        # Let's use `p = 0.7` as a parameter.\n        p_value = 0.7\n        \n        # Add a small epsilon to slack to avoid potential issues with `slack=0` and `p<1` leading to undefined behavior\n        # or excessively high values if `slack` could be extremely close to zero.\n        # However, `0**p` is 0 for `p>0`. So, `1/(1+0)` is 1. This is fine.\n        \n        # Let's use a scaling factor for slack to control the \"tightness\" preference.\n        # `scaled_slack = slack / item_size_scale`\n        # A scale factor related to average item size might be good.\n        # For now, let's use absolute slack with `p=0.7`.\n        \n        priorities[fittable_bins_mask] = 1.0 / (1.0 + slack**p_value)\n        \n        # Ensure priorities are not NaN or Inf (though unlikely with this formula and non-negative slack)\n        priorities[fittable_bins_mask] = np.nan_to_num(priorities[fittable_bins_mask], nan=0.0, posinf=1.0, neginf=0.0)\n        \n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 70.32403072095333,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response3.txt_stdout.txt",
    "code_path": "problem_iter4_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Best Fit strategy.\n\n    This strategy prioritizes bins that leave minimal remaining capacity after packing\n    the current item, similar to Best Fit. However, it also incorporates a penalty for\n    bins that have a very large remaining capacity, as these might be better saved\n    for larger future items, thus encouraging better overall packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities to 0\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # For bins that can fit the item, calculate the remaining capacity after placing the item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_placement = fitting_bins_remain_cap - item\n\n    # Best Fit component: Prioritize bins with minimal remaining capacity.\n    # We use the inverse of (1 + remaining_capacity) to give higher scores to smaller remaining capacities.\n    best_fit_scores = 1.0 / (1.0 + remaining_after_placement)\n\n    # Penalty for bins with very large remaining capacity:\n    # If a bin has significantly more capacity than needed (e.g., more than twice the item size),\n    # it might be better to save it for a larger item. We can penalize these bins.\n    # Let's define a threshold, e.g., capacity > 2 * item\n    large_capacity_penalty_factor = 0.5  # Adjust this factor to control the penalty\n    large_capacity_bins_mask = fitting_bins_remain_cap > 2 * item\n    penalty = np.ones_like(remaining_after_placement)\n    penalty[large_capacity_bins_mask] = large_capacity_penalty_factor\n\n    # Combine Best Fit score with the penalty\n    # We multiply the best_fit_scores by the penalty. Bins with large capacity will have their scores reduced.\n    combined_scores = best_fit_scores * penalty\n\n    # Assign the combined scores to the priorities array\n    priorities[can_fit_mask] = combined_scores\n\n    # Optional: Normalize priorities to sum to 1 for probabilistic selection.\n    # This is useful if the priority scores are to be used directly in a sampling mechanism.\n    # If the highest score should always be chosen, normalization is not strictly necessary.\n    fittable_scores_sum = np.sum(priorities[can_fit_mask])\n    if fittable_scores_sum > 0:\n        priorities[can_fit_mask] /= fittable_scores_sum\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 127.37720526058406,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response4.txt_stdout.txt",
    "code_path": "problem_iter4_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Sigmoid Fit Score with a penalty for very large capacities and exploration.\n\n    This version prioritizes bins that have a remaining capacity that is just\n    slightly larger than the item size. This aims to minimize wasted space in\n    the selected bin, leaving larger contiguous free spaces in other bins for\n    potentially larger future items.\n\n    The priority is calculated using a sigmoid function applied to the difference\n    between the bin's remaining capacity and the item's size. A smaller\n    positive difference (a tighter fit) results in a higher priority score.\n    Additionally, bins with very large remaining capacities (relative to the item)\n    are penalized to encourage spreading. A small probability of exploration is also included.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    exploration_prob = 0.05\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_capacities = bins_remain_cap[fittable_bins_mask]\n    slack = fittable_bins_capacities - item\n\n    # Sigmoid for tight fits (prioritize near-zero slack)\n    k_fit = 5.0\n    tight_fit_score = 1.0 / (1.0 + np.exp(k_fit * slack))\n\n    # Penalty for very large remaining capacities.\n    # We want to discourage putting a small item into a very large bin if a tighter fit exists.\n    # This can be modeled with a logistic function that decreases as capacity grows.\n    # Let's use a threshold and a decay factor.\n    # We can normalize capacities relative to the item size or bin capacity.\n    # A simple approach: Penalize bins whose remaining capacity is much larger than the item.\n    # Let's create a score that is high for capacities close to the item size and decreases.\n    # We can use a similar sigmoid but inverted or a different function.\n    # Alternative: use 1 / (1 + exp(-k_large * (capacity - threshold)))\n    # A simpler penalty: if capacity is > C * item, reduce score.\n\n    # Let's try a penalty based on normalized slack.\n    # We want the penalty to be low for slack close to 0 and high for large slack.\n    # This is the opposite of the tight fit score.\n    # We can use a decaying function of slack.\n    # e.g., 1 / (1 + slack) or exp(-slack / scale)\n    # Let's try a sigmoid on the negative slack to penalize larger slack.\n    k_penalty = 0.5 # Controls how quickly the penalty increases with slack\n    large_capacity_penalty = 1.0 / (1.0 + np.exp(-k_penalty * slack))\n\n    # Combine scores. The tight_fit_score is high for small slack.\n    # The large_capacity_penalty is high for small slack, and low for large slack.\n    # We want to combine them such that:\n    # 1. Small slack (tight fit) is good -> high tight_fit_score\n    # 2. Large slack (too much space) is bad -> needs a penalty\n    # Let's try to combine them additively.\n    # A higher score for tight fits, and a lower score for large slack.\n    # The large_capacity_penalty goes from ~0.5 to ~1 as slack increases. This is not a penalty.\n    # Let's re-think the penalty. We want to penalize large slack.\n    # The sigmoid `1 / (1 + exp(k * slack))` already penalizes large slack.\n    # So maybe just use a weighted combination of the tight fit score and the slack itself.\n    # Or, use the slack directly, but inverted and scaled.\n\n    # Let's re-evaluate the reflection: \"penalize large remaining capacities\"\n    # The `tight_fit_score` already does this by giving low scores to large slack.\n    # Maybe the reflection implies we should *also* consider the absolute capacity.\n    # For example, putting an item of size 1 into a bin with remaining capacity 100\n    # is worse than putting it into a bin with remaining capacity 1.\n    # The slack approach (capacity - item) naturally handles this:\n    # slack for (100, 1) is 99, slack for (1, 1) is 0.\n\n    # Let's reconsider the \"Worst Fit\" element from v0 and combine it with tight fit.\n    # Best Fit component (tight_fit_score) -> prioritizes small slack\n    # Worst Fit component (prioritizes bins with more remaining capacity) -> prioritizes large capacity\n    # This seems contradictory. The reflection \"Focus on tighter fits, penalize large remaining capacities\"\n    # suggests we should prioritize small slack and penalize large slack. The `tight_fit_score` does this.\n\n    # Let's refine the \"penalty for very large remaining capacities\".\n    # A simple approach: normalize slack by some reference, or use a threshold.\n    # Let's say if remaining_capacity > 2 * item, we start penalizing.\n    # Max slack we want to consider for high priority: say, up to `max_slack_ideal`.\n    # Any slack beyond that should be heavily penalized.\n    max_slack_ideal = 2.0 * item # An item of size 'item' should ideally fit into a bin with 2*item remaining capacity at most for a good fit.\n    # Penalize slack if it's much larger than max_slack_ideal.\n    # We can use a linear penalty or a sigmoid that drops sharply.\n    # Let's use a sigmoid that is high for slack <= max_slack_ideal and low for slack > max_slack_ideal.\n    # Use `1 / (1 + exp(k_penalty * (slack - max_slack_ideal)))`\n    # This gives 0.5 at slack = max_slack_ideal, decreases for slack > max_slack_ideal.\n    k_penalty = 1.0 # Controls the steepness of the penalty\n    penalty_score = 1.0 / (1.0 + np.exp(k_penalty * (slack - max_slack_ideal)))\n\n    # Combine tight_fit_score and penalty_score.\n    # We want both to contribute positively.\n    # tight_fit_score is high for small slack.\n    # penalty_score is high for small slack (and slack <= max_slack_ideal).\n    # So, a linear combination might work.\n    # Let's weight them. Give more weight to the tight fit.\n    combined_scores = 0.7 * tight_fit_score + 0.3 * penalty_score\n\n    priorities[fittable_bins_mask] = combined_scores\n\n    # Apply exploration\n    if np.random.rand() < exploration_prob:\n        fittable_indices = np.where(fittable_bins_mask)[0]\n        chosen_bin_global_index = np.random.choice(fittable_indices)\n        priorities.fill(0)\n        priorities[chosen_bin_global_index] = 1.0\n    else:\n        # Normalize priorities for fittable bins to sum to 1\n        fittable_priorities = priorities[fittable_bins_mask]\n        if np.sum(fittable_priorities) > 0:\n            priorities[fittable_bins_mask] /= np.sum(fittable_priorities)\n\n    return priorities",
    "response_id": 4,
    "obj": 4.098524132429212,
    "cyclomatic_complexity": 4.0,
    "halstead": 322.4095353505972,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response5.txt_stdout.txt",
    "code_path": "problem_iter4_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Almost Full Fit strategy.\n\n    This strategy prioritizes bins that provide the \"tightest fit\" for the item,\n    meaning the remaining capacity after insertion is minimized. This is achieved\n    by maximizing `1.0 / (1.0 + slack)`, where slack is `remaining_capacity - item_size`.\n    Additionally, it introduces a secondary preference for bins that are already\n    \"fuller\" (i.e., have less initial remaining capacity) among those offering similar\n    tight fits, to promote more balanced packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate the slack for bins that can fit the item\n    # Slack = remaining_capacity - item_size\n    slack = bins_remain_cap[fit_mask] - item\n\n    # Primary scoring: Prioritize minimal slack (tightest fits).\n    # Use `1.0 / (1.0 + slack)`: higher score for smaller slack.\n    # A slack of 0 gives priority 1.0.\n    primary_score = 1.0 / (1.0 + slack)\n\n    # Secondary scoring: Among bins with similar slacks, prioritize those that are\n    # \"fuller\" initially. This means preferring bins with smaller original `bins_remain_cap`.\n    # We can add a term proportional to the negative of the initial remaining capacity.\n    # A small coefficient `alpha` ensures the primary criterion (tightness) dominates.\n    alpha = 0.01\n    secondary_score = -alpha * bins_remain_cap[fit_mask]\n\n    # Combine scores: maximize primary score, then secondary score\n    priorities[fit_mask] = primary_score + secondary_score\n\n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 81.7492568250068,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response6.txt_stdout.txt",
    "code_path": "problem_iter4_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit strategy with a slight random perturbation.\n\n    The Best Fit strategy aims to minimize wasted space by selecting the bin that\n    has the least remaining capacity after the item is placed. To encourage\n    exploration and avoid getting stuck in local optima, a small random value is\n    added to the priority of eligible bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Identify bins that can accommodate the item\n    eligible_bins_mask = bins_remain_cap >= item\n\n    # Initialize priorities to a very low value for bins that cannot accommodate the item.\n    # This ensures they are never chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    if np.any(eligible_bins_mask):\n        # For eligible bins, calculate a priority based on the remaining capacity.\n        # The Best Fit strategy prioritizes bins with the smallest remaining capacity\n        # after placing the item. Therefore, we use -(bins_remain_cap - item).\n        # A small random perturbation is added to break ties and introduce some\n        # stochasticity, which can help in escaping local optima.\n        # The smaller the remaining capacity after placement (bins_remain_cap - item),\n        # the higher the priority (less negative value).\n        priorities[eligible_bins_mask] = -(bins_remain_cap[eligible_bins_mask] - item) + np.random.rand(np.sum(eligible_bins_mask)) * 1e-6\n\n    return priorities",
    "response_id": 6,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 60.91767875292166,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response7.txt_stdout.txt",
    "code_path": "problem_iter4_code7.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, prioritizing fuller bins first, then best fit.\n\n    This heuristic prioritizes bins that are already fuller (have less remaining capacity).\n    If multiple bins have the same minimal remaining capacity, it then applies the Best Fit\n    principle to choose the one that results in the least waste. This aims to pack items\n    more densely by preferring bins that are closer to being full.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher score indicates higher priority. Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Create a boolean mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    fittable_bins_capacities = bins_remain_cap[can_fit_mask]\n\n    # Primary goal: Prioritize fuller bins. This means prioritizing bins with smaller remaining capacity.\n    # We can use the inverse of remaining capacity as a score. Higher score for smaller capacity.\n    # `fullness_score = 1.0 / (fittable_bins_capacities + epsilon)`\n\n    # Secondary goal: Among equally full bins, prioritize the best fit (minimize waste).\n    # Waste = `fittable_bins_capacities - item`.\n    # Best fit score = `1.0 / (waste + epsilon)`\n\n    # Combine the two: We want to prioritize bins with smaller `bins_remain_cap` first,\n    # and then smaller `fittable_bins_capacities - item` as a tie-breaker.\n    # A single score that achieves this is to create a composite key:\n    # `composite_key = (fittable_bins_capacities, fittable_bins_capacities - item)`\n    # We want to minimize this composite key lexicographically.\n    # For priority (maximize), we want to maximize the inverse of this key.\n    # A simple way to combine them into a single score for maximization is:\n    # `priority = (1.0 / (fittable_bins_capacities + epsilon)) + (1.0 / (fittable_bins_capacities - item + epsilon))`\n    # This gives a higher score to bins that are both fuller and have less waste.\n    # Let's test this combination:\n    # Bin A: remain_cap=0.6, item=0.5 -> waste=0.1. Score = 1/0.6 + 1/0.1 = 1.667 + 10 = 11.667\n    # Bin B: remain_cap=0.7, item=0.5 -> waste=0.2. Score = 1/0.7 + 1/0.2 = 1.428 + 5 = 6.428\n    # Bin C: remain_cap=0.6, item=0.4 -> waste=0.2. Score = 1/0.6 + 1/0.2 = 1.667 + 5 = 6.667\n    # Bin A (fullest, best fit) gets highest score. Bin C (equally full as A, worse fit) gets second highest. Bin B (less full, worse fit) gets lowest.\n    # This combination seems to correctly prioritize fullness first, then best fit.\n\n    combined_score = (1.0 / (fittable_bins_capacities + epsilon)) + \\\n                     (1.0 / (fittable_bins_capacities - item + epsilon))\n\n    priorities[can_fit_mask] = combined_score\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 92.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response8.txt_stdout.txt",
    "code_path": "problem_iter4_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, prioritizing minimal waste and then fuller bins.\n\n    This strategy aims to find bins where the remaining capacity is just enough to fit the item (minimizing slack).\n    Among bins that offer a similar \"tight fit\" (i.e., similar slack), it prioritizes those that are already fuller\n    (have less remaining capacity).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate the \"slack\" (waste after insertion)\n    slack = bins_remain_cap[can_fit_mask] - item\n\n    # To prioritize minimal waste (smallest slack), we want a score that is\n    # higher for smaller slack values. A common way is `1.0 / (slack + epsilon)`.\n    # This maps slack [0, infinity) to priority (1, 0].\n\n    # To prioritize fuller bins among those with similar slack, we want a score\n    # that is higher for smaller `bins_remain_cap`. A common way is `1.0 / bins_remain_cap`.\n\n    # Combine these two objectives additively.\n    # The primary objective is minimal slack, so its term should ideally dominate.\n    # However, a simple sum often works well, assuming the scales are somewhat managed.\n    # Using `1.0 / (slack + epsilon)` for minimal waste.\n    # Using `1.0 / (bins_remain_cap[can_fit_mask] + epsilon)` for fuller bins preference.\n    # Adding a small epsilon to denominators to prevent division by zero and to avoid\n    # issues with exact zero remaining capacities (though unlikely in BPP context, good practice).\n\n    # If slack is 0, the first term is ~1. If slack is large, first term approaches 0.\n    # If bin_remain_cap is small (fuller bin), second term is large. If bin_remain_cap is large, second term is small.\n    # This combination correctly prioritizes bins with small slack and small remaining capacity.\n    \n    epsilon = 1e-9  # Small value to prevent division by zero\n\n    # Calculate the score for fitting bins\n    # Score = (Inverse of slack) + (Inverse of remaining capacity)\n    # Higher score means better priority\n    priorities[can_fit_mask] = (1.0 / (slack + epsilon)) + (1.0 / (bins_remain_cap[can_fit_mask] + epsilon))\n\n    return priorities",
    "response_id": 8,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 82.0447025077789,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response9.txt_stdout.txt",
    "code_path": "problem_iter4_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined \"Almost Full Fit\" strategy.\n\n    This strategy prioritizes bins that offer the tightest fit for the item, meaning\n    the remaining capacity after insertion is minimized. This is achieved by maximizing\n    the negative of the slack (remaining_capacity - item_size).\n    To also incorporate the \"favoring fuller bins\" aspect (i.e., preferring bins that\n    were already closer to full before insertion), a secondary prioritization is\n    applied: preferring bins with smaller initial remaining capacities.\n\n    The combined priority is a weighted sum designed to maximize:\n    -minimize(slack) - alpha * minimize(initial_remaining_capacity)\n    This translates to maximizing:\n    -slack - alpha * initial_remaining_capacity\n    which is equivalent to:\n    -(bins_remain_cap - item) - alpha * bins_remain_cap\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        # If no bins can fit the item, return all zeros (no priority)\n        return priorities\n\n    # Calculate the slack for bins that can fit the item\n    # slack = remaining_capacity - item_size\n    slack = bins_remain_cap[fit_mask] - item\n\n    # Primary criterion: Minimize slack (tight fit). Maximize -(slack).\n    # Secondary criterion: Minimize initial remaining capacity (favor fuller bins). Maximize -(bins_remain_cap).\n    # We combine them, giving slightly more weight to the tight fit.\n    # The expression is: -slack - alpha * bins_remain_cap\n    # This means we are maximizing: (item - bins_remain_cap) - alpha * bins_remain_cap\n    # Or more directly, maximizing the negative of (slack + alpha * bins_remain_cap)\n\n    alpha = 0.1  # Weight for the secondary criterion (smaller bins)\n\n    # Calculate the combined priority for fitting bins\n    # We want to maximize -(slack + alpha * bins_remain_cap[fit_mask])\n    # This is equivalent to maximizing (item - bins_remain_cap[fit_mask]) - alpha * bins_remain_cap[fit_mask]\n    # Let's rewrite to avoid confusion with maximization: we assign a score where higher is better.\n    # High score = small slack AND small initial capacity.\n    # Score = -slack - alpha * initial_capacity\n    #       = -(bins_remain_cap[fit_mask] - item) - alpha * bins_remain_cap[fit_mask]\n    #       = item - bins_remain_cap[fit_mask] - alpha * bins_remain_cap[fit_mask]\n\n    # Let's try the simpler approach of maximizing the negative slack, and then using\n    # the negative initial capacity as a tie-breaker.\n    # Priority = -slack - alpha * bins_remain_cap[fit_mask]\n    priorities[fit_mask] = -slack - alpha * bins_remain_cap[fit_mask]\n\n    # Example: item = 3\n    # bins_remain_cap = [5, 4, 7]\n    # Bin 0: cap=5, slack=2. priority = -2 - 0.1*5 = -2.5\n    # Bin 1: cap=4, slack=1. priority = -1 - 0.1*4 = -1.4\n    # Bin 2: cap=7, slack=4. priority = -4 - 0.1*7 = -4.7\n\n    # Bin 1 has the highest priority (-1.4), which is correct (tightest fit).\n    # Bin 0 has the second highest priority (-2.5), which is also correct (smaller initial capacity than Bin 2, though slack is larger).\n    # Bin 2 has the lowest priority (-4.7).\n\n    # Example: item = 3\n    # bins_remain_cap = [5, 5]\n    # Bin 0: cap=5, slack=2. priority = -2 - 0.1*5 = -2.5\n    # Bin 1: cap=5, slack=2. priority = -2 - 0.1*5 = -2.5\n    # Tie. This is expected as both criteria are equal. The selection would be arbitrary between them.\n\n    return priorities",
    "response_id": 9,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 60.91767875292166,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response0.txt_stdout.txt",
    "code_path": "problem_iter5_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Almost Full Fit strategy.\n\n    This strategy prioritizes bins that have just enough capacity to fit the item,\n    with a preference for bins that will have less remaining capacity after the item is added.\n    It balances the tightness of the fit with a slight preference for bins that aren't\n    already excessively full, to avoid wasting potential in very tight spots if slightly\n    looser but still efficient fits exist.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Mask for bins that can fit the item\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity if the item is placed in each bin\n    potential_remaining_cap = bins_remain_cap - item\n\n    # For bins that fit, we want to prioritize those with smaller remaining capacity.\n    # A common heuristic for \"almost full\" is to use the negative of the remaining capacity,\n    # as smaller remaining capacity means a higher (less negative) score.\n    # However, to incorporate balancing and avoid extremely tight fits if a slightly\n    # looser but still good fit exists, we can modify this.\n\n    # Let's focus on the slack (bins_remain_cap - item). Smaller slack is better.\n    # We want to rank bins with smaller non-negative slack higher.\n    # A score that is inversely proportional to slack + 1 (to avoid division by zero and keep it positive)\n    # works well: 1/(1 + slack). This gives priority 1 for slack 0, and decreasing priority for larger slack.\n\n    # To add a balancing element: If multiple bins have very small slacks, we might\n    # prefer the one with slightly more capacity to keep more \"tight fit\" opportunities.\n    # However, the prompt emphasizes \"almost full fit\", so minimizing slack is primary.\n\n    # Let's refine the priority based on `potential_remaining_cap`.\n    # We want to prioritize smaller `potential_remaining_cap` values (i.e., bins that become more full).\n    # Using `1.0 / (1.0 + potential_remaining_cap)` for bins where `potential_remaining_cap >= 0`.\n    # This gives priority 1 for perfect fits (potential_remaining_cap = 0) and less for larger remaining capacities.\n\n    # To incorporate the idea of \"slightly larger gaps for robustness\" or balancing,\n    # we can add a penalty for bins that are *too* full, such that they might not\n    # fit future items well, or a slight bonus for bins that are not excessively empty.\n    # However, the most direct interpretation of \"Almost Full Fit\" is prioritizing minimum remaining capacity.\n\n    # Let's use the `1.0 / (1.0 + slack)` approach as it directly prioritizes minimal non-negative slack.\n    # This encourages using up space efficiently.\n    slack = bins_remain_cap[fit_mask] - item\n    priorities[fit_mask] = 1.0 / (1.0 + slack)\n\n    # To incorporate the \"balance with bin fullness\" and \"slightly larger gaps for robustness\":\n    # If we have two bins that fit an item, Bin A (remain=4, item=3, slack=1) and Bin B (remain=5, item=3, slack=2).\n    # Current priority for A = 1/(1+1) = 0.5. Current priority for B = 1/(1+2) = 0.33.\n    # Bin A is preferred. This is good.\n    # What if Bin C (remain=10, item=3, slack=7) also fits? Priority C = 1/(1+7) = 0.125.\n    # Bin A is still preferred.\n\n    # The core idea is to select a bin that is \"almost full\" with the current item.\n    # The `1.0 / (1.0 + slack)` metric achieves this by favoring smaller non-negative slacks.\n    # A small positive slack means the bin is almost full. A zero slack means it's perfectly full.\n    # Larger slacks mean the bin is less \"almost full\".\n\n    # Alternative: Prioritize bins that become most full: maximize `-potential_remaining_cap`.\n    # If Bin A: remain=4, item=3 -> potential_remain=1. Score = -1.\n    # If Bin B: remain=5, item=3 -> potential_remain=2. Score = -2.\n    # Bin A (-1) is better than Bin B (-2). This also favors tighter fits.\n    # Let's test this against the prompt's \"slight preference for bins that aren't already excessively full\".\n    # If Bin A (remain=4, item=3) and Bin D (remain=100, item=3).\n    # A: slack=1, score=0.5. B: slack=97, score=1/98 ~ 0.01.\n    # A is preferred.\n    # Using negative potential remaining:\n    # A: potential_remain=1, score = -1. B: potential_remain=97, score = -97.\n    # A is preferred.\n\n    # The `1.0 / (1.0 + slack)` heuristic seems robust and directly targets the \"almost full\" aspect by minimizing positive slack.\n    # Let's ensure there's a slight preference for less empty bins as a tie-breaker or a secondary factor.\n    # A simple way to boost bins that are not excessively empty might be to add a small term\n    # related to their original remaining capacity, but scaled down.\n    # For instance, `priority = (1.0 / (1.0 + slack)) + alpha * (bins_remain_cap[fit_mask] / MAX_CAPACITY)`\n    # where `alpha` is small. This could slightly favor bins that are fuller if slacks are similar.\n\n    # For now, sticking to the strong \"almost full\" heuristic is best. The `1.0 / (1.0 + slack)` method is simple and effective.\n    # The \"balance with bin fullness\" might be implicitly handled if smaller bins tend to have smaller slacks.\n\n    # Let's reconsider the prompt's refinement: \"Balance this with bin fullness and explore slightly larger gaps for robustness.\"\n    # This suggests that simply minimizing slack might be too aggressive.\n    # A bin with remaining capacity 5 and item 3 (slack 2, priority 0.5) might be preferred over capacity 3 and item 3 (slack 0, priority 1).\n    # This sounds like a compromise.\n\n    # Let's try a score that penalizes very small slacks (perfect fits) slightly,\n    # and also penalizes large slacks.\n    # Consider a quadratic function or a Gaussian-like function centered around a small slack.\n    # E.g., `score = exp(-(slack - target_slack)^2 / sigma^2)`.\n    # Where `target_slack` is a small value, say 1 or 2.\n\n    # For robustness, let's prioritize bins that are not excessively empty.\n    # We can add a small bonus proportional to how much capacity is already used.\n    # `(bins_remain_cap[fit_mask] - item) / bins_remain_cap[fit_mask]` is the proportion of used space after fitting.\n    # Or, simpler, `1 - (bins_remain_cap[fit_mask] / BIN_CAPACITY)`.\n\n    # Let's try a combination: minimize slack, but add a slight bonus for bins that are more full.\n    # `priority = (1.0 / (1.0 + slack)) + alpha * (bins_remain_cap[fit_mask] - item)`\n    # This rewards smaller slack and also rewards bins that have less remaining capacity *after* fitting.\n    # The `bins_remain_cap[fit_mask] - item` term is exactly the `potential_remaining_cap`.\n    # We want to maximize this negative value for tightness.\n    # So, `priority = (1.0 / (1.0 + slack)) - beta * potential_remaining_cap[fit_mask]`\n\n    # Let's keep it simpler to match the spirit of heuristic development:\n    # Prioritize small slack, but if slacks are close, pick the one with higher original capacity.\n    # This can be achieved by sorting by slack, then by original capacity descending.\n    # Or in scoring: use slack primarily, and original capacity as a tie-breaker.\n    # If scores are based on `1.0 / (1.0 + slack)`, we can add a term related to original capacity.\n    # `score = (1.0 / (1.0 + slack)) + alpha * bins_remain_cap[fit_mask]`\n    # This prioritizes tightness, then higher capacity among equally tight bins.\n\n    # Let's try a simplified approach that favors minimal non-negative slack,\n    # and then favors bins that are \"moderately\" full, not too empty.\n    # Score: `-slack + some_factor * (1 - slack/max_capacity_overall)`\n    # This is getting complicated.\n\n    # The most straightforward implementation of \"Almost Full Fit\" is to pick the bin\n    # that, after placing the item, has the minimal remaining capacity.\n    # This means maximizing `-(bins_remain_cap - item)`.\n    # Let's try that, and if it needs balancing, we can adjust.\n    # `potential_remaining_cap = bins_remain_cap[fit_mask] - item`\n    # Maximize `-potential_remaining_cap`.\n    # This means we want to pick bins where `potential_remaining_cap` is smallest (closest to 0).\n    # This is equivalent to minimizing `slack` for valid bins.\n\n    # Revisit `1.0 / (1.0 + slack)`:\n    # Slack 0 -> 1.0\n    # Slack 1 -> 0.5\n    # Slack 2 -> 0.33\n    # Slack 5 -> 0.16\n    # Slack 10 -> 0.09\n\n    # This seems to prioritize very tight fits strongly.\n    # To \"balance with bin fullness\" and \"explore slightly larger gaps for robustness\",\n    # we might want to reduce the steepness of this decay, or add a secondary factor.\n\n    # Consider prioritizing based on the *ratio* of remaining capacity to item size.\n    # `ratio = (bins_remain_cap[fit_mask] - item) / item`\n    # We want this ratio to be small.\n    # Score could be `1.0 / (1.0 + ratio)`.\n    # If item=3:\n    # Bin A: remain=4, slack=1, ratio=1/3. Score = 1/(1+1/3) = 1/(4/3) = 3/4 = 0.75\n    # Bin B: remain=5, slack=2, ratio=2/3. Score = 1/(1+2/3) = 1/(5/3) = 3/5 = 0.6\n    # Bin C: remain=3, slack=0, ratio=0.   Score = 1/(1+0) = 1.0\n    # Bin D: remain=6, slack=3, ratio=1.   Score = 1/(1+1) = 0.5\n\n    # This ratio-based approach also strongly favors tight fits (low slack relative to item size).\n    # It's similar in spirit to `1/(1+slack)` but normalized by item size.\n\n    # Let's try a simple modification to `1.0 / (1.0 + slack)` to add some robustness.\n    # We can make the priority decay slower for larger slacks.\n    # Instead of `1/(1+slack)`, maybe `1/(1+slack^p)` with `p < 1`. Or `sqrt(slack)` in denominator.\n    # `1 / (1 + np.sqrt(slack))`\n    # Slack 0 -> 1.0\n    # Slack 1 -> 1/(1+1) = 0.5\n    # Slack 4 -> 1/(1+2) = 0.33\n    # Slack 9 -> 1/(1+3) = 0.25\n    # Slack 16 -> 1/(1+4) = 0.2\n\n    # Comparing `1/(1+slack)` vs `1/(1+sqrt(slack))`:\n    # Slack=1: 0.5 vs 0.5 (same)\n    # Slack=4: 0.2 vs 0.33 (sqrt version gives higher priority)\n    # Slack=9: 0.09 vs 0.25 (sqrt version gives much higher priority)\n    # This `1/(1+sqrt(slack))` seems to better balance tightness and allow slightly larger gaps.\n    # It prioritizes tight fits, but doesn't penalize moderately larger gaps as harshly.\n\n    priorities[fit_mask] = 1.0 / (1.0 + np.sqrt(slack))\n\n    # Optional: Add a small bonus for bins that are not excessively empty.\n    # This could be `alpha * (1 - slack / max_possible_slack)` but `max_possible_slack` is tricky.\n    # Or `alpha * (bins_remain_cap[fit_mask] / MAX_BIN_SIZE)`\n\n    # Let's stick to `1.0 / (1.0 + np.sqrt(slack))` as it addresses the prompt's needs reasonably well.\n    # It favors minimal slack (almost full), and the square root dampens the penalty for slightly larger gaps.\n\n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 75.28421251514429,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response1.txt_stdout.txt",
    "code_path": "problem_iter5_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a balanced 'Almost Full Fit' strategy.\n\n    This strategy prioritizes bins that will have minimal remaining capacity after the item is added.\n    It balances the preference for tight fits with a less aggressive penalty for slightly larger gaps,\n    aiming for robustness.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority. Bins that cannot fit the item have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity if the item is placed in each fitting bin\n    # Use only fitting bins for calculation\n    potential_remaining_cap_fitting = bins_remain_cap[fit_mask] - item\n\n    # If no bins can fit the item, return all zeros\n    if potential_remaining_cap_fitting.size == 0:\n        return priorities\n\n    # Strategy: Maximize -(potential_remaining_cap) for fitting bins.\n    # This means minimizing potential_remaining_cap (tightest fit).\n    # To make the priority scores positive and to create a relative ranking,\n    # we can subtract each potential_remaining_cap from the maximum potential_remaining_cap found among fitting bins.\n    # This ensures the tightest fit gets the highest score, and larger remaining capacities get lower scores linearly.\n    max_potential_rem_cap = np.max(potential_remaining_cap_fitting)\n\n    # Assign priorities to the fitting bins\n    # The priority is higher for bins with smaller potential_remaining_cap\n    priorities[fit_mask] = max_potential_rem_cap - potential_remaining_cap_fitting\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 39.863137138648355,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response2.txt_stdout.txt",
    "code_path": "problem_iter5_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Almost Full Fit strategy.\n\n    This strategy prioritizes bins that have just enough capacity to fit the item,\n    aiming to minimize the remaining capacity after placing the item.\n    It balances the \"tight fit\" aspect with a consideration for overall bin fullness,\n    slightly favoring bins that are already somewhat full but can still accommodate the item,\n    while penalizing bins that are excessively large.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit.\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate the slack (unused capacity) for bins that can fit the item.\n    slack = bins_remain_cap[fit_mask] - item\n\n    # The core idea is to prioritize bins with smaller slack (tighter fits).\n    # We use `1 / (1 + slack)` to give higher scores to smaller slack values.\n    # A slack of 0 (perfect fit) gets a priority of 1.0.\n    # A slack of 1 gets 0.5, etc.\n    # This part captures the \"almost full fit\".\n    tight_fit_score = 1.0 / (1.0 + slack)\n\n    # To balance this with robustness and avoid picking excessively large bins\n    # just because they might eventually fit something, we can also consider\n    # the absolute remaining capacity or its inverse.\n    # Let's introduce a factor that slightly favors bins that are already more full,\n    # but not to an extreme degree. A simple way is to use the inverse of the\n    # *original* remaining capacity for bins that fit.\n    # However, we want to combine this with the tight fit.\n    # Let's try a weighted sum or a multiplicative approach.\n\n    # A common heuristic is to prioritize bins that leave the least remaining space.\n    # This means we want to minimize `potential_remaining_cap = bins_remain_cap[fit_mask] - item`.\n    # So, a higher priority is associated with smaller `potential_remaining_cap`.\n    # Using `-potential_remaining_cap` as the base score would work.\n    # For example, if bins have remaining capacities [5, 4, 3, 2] and item is 3:\n    # Fits: [5, 4, 3]. Potential remaining: [2, 1, 0].\n    # Scores (-potential_remaining): [-2, -1, 0]. Max score is 0 for bin with capacity 3.\n\n    # Let's refine the \"Almost Full Fit\" to prioritize minimal slack.\n    # The previous version `1 / (1 + slack)` is good.\n    # To add robustness and avoid large empty bins, we can also penalize\n    # bins that have *very large* remaining capacity, even if the slack is small.\n    # A simple penalty could be `1 / (1 + bins_remain_cap[fit_mask])`.\n    # This gives higher scores to bins that are already less full.\n    # This seems counter to \"almost full fit\".\n\n    # The reflection suggests \"slightly larger gaps for robustness\".\n    # This might mean if there are multiple \"tight fits\" (small slack),\n    # pick the one that has a bit more capacity overall?\n    # Or, if an item doesn't fit tightly, consider bins that have a bit more room but are still reasonable.\n\n    # Let's stick to prioritizing the smallest non-negative slack.\n    # The `1 / (1 + slack)` approach already does this.\n    # To introduce \"slightly larger gaps for robustness\", perhaps we can add a small\n    # bonus for bins that are not *exactly* at slack=0, but close to it.\n    # Or, to avoid very large bins, we can cap the priority based on original capacity.\n\n    # Let's try a score that combines minimizing slack with not being excessively large.\n    # We can use `1 / (1 + slack)` as the primary score.\n    # To penalize very large bins, we could multiply by a factor related to `1 / bins_remain_cap[fit_mask]`.\n    # This would reduce the score for bins with larger initial capacity.\n    # For example: `priority = (1 / (1 + slack)) * (1 / (1 + bins_remain_cap[fit_mask]))`\n    # Let's test this:\n    # Bin A: remain=5, item=3 => slack=2. Score = (1/3) * (1/6) = 1/18\n    # Bin B: remain=4, item=3 => slack=1. Score = (1/2) * (1/5) = 1/10\n    # Bin C: remain=3, item=3 => slack=0. Score = (1/1) * (1/4) = 1/4\n    # This prioritizes perfect fits, then smaller slack, and then smaller original capacity.\n    # This seems like a reasonable heuristic reflecting the reflection.\n\n    # Avoid division by zero if original remaining capacity is 0 (though this shouldn't happen if item fits)\n    # and also for slack if it's very large. The `1 +` handles these.\n    original_capacity = bins_remain_cap[fit_mask]\n    robust_score = (1.0 / (1.0 + slack)) * (1.0 / (1.0 + original_capacity))\n\n    # Assign the calculated priorities to the bins that can fit the item.\n    priorities[fit_mask] = robust_score\n\n    # The reflection also mentions \"avoid overly complex combinations\".\n    # The current `robust_score` is a multiplicative combination, which is not overly complex.\n\n    # Let's consider the case where multiple bins offer the *same* minimal slack.\n    # For example, if item=2 and capacities are [5, 5, 6]. Slacks are [3, 3, 4].\n    # With `1/(1+slack)`, both 5s get 1/4.\n    # With `robust_score`, both 5s get (1/4)*(1/6) = 1/24. The 6 gets (1/5)*(1/7) = 1/35.\n    # In this scenario, the heuristic doesn't provide a strong tie-breaker between the two '5' bins.\n    # For online BPP, tie-breaking is often arbitrary or based on bin index.\n    # If we wanted to break ties by choosing the bin that becomes *more* full (smallest potential remaining capacity),\n    # we would add a penalty based on `potential_remaining_cap`.\n    # E.g., `robust_score - 0.01 * potential_remaining_cap`.\n    # For example, if remaining capacities were [5, 5, 6] and item = 2.\n    # Bins with capacity 5: slack=3, potential_remaining=3. `1/(1+3)` = 0.25.\n    # Bin with capacity 6: slack=4, potential_remaining=4. `1/(1+4)` = 0.20.\n    # If we want to prefer smaller potential remaining capacity among those with same slack,\n    # we could add a term.\n    # Let's try prioritizing smaller *original* capacity for bins with same slack, as it might preserve larger bins for larger items.\n    # The `robust_score` already does this implicitly via `1 / (1 + original_capacity)`.\n\n    # Let's refine the \"slightly larger gaps for robustness\". This might imply that\n    # if there are bins with *very* small slack (e.g., slack < epsilon), we might\n    # want to prefer slightly larger slack values that are still small, if they come from bins\n    # that are not too full. This sounds complex.\n\n    # A simpler interpretation: the \"Almost Full Fit\" should prioritize bins that result in the smallest possible positive remaining capacity.\n    # Our `1 / (1 + slack)` does exactly this. The added `* (1 / (1 + original_capacity))` term\n    # serves as the \"balance\" and robustness against overly large bins.\n\n    # Let's consider another possibility for robustness: if multiple bins have the minimum slack,\n    # perhaps we should pick the one that is currently \"most full\" to encourage denser packing.\n    # This would mean prioritizing bins with smaller `bins_remain_cap` among those with minimal slack.\n    # Our `robust_score` already implicitly favors smaller `bins_remain_cap`.\n\n    # Final decision: The `robust_score` implementation seems to capture the essence of prioritizing tight fits\n    # while adding a preference for less voluminous bins, thus providing a form of robustness.\n    # It's a balance between minimizing slack and not picking extremely large bins.\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 108.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response3.txt_stdout.txt",
    "code_path": "problem_iter5_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This strategy prioritizes bins that will have minimal remaining capacity\n    after the item is added (\"tight fits\"). It also balances this by\n    giving a slight preference to smaller bins (less original capacity),\n    to avoid committing large bins to small items unnecessarily.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher score means higher priority.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate slack for each bin: how much space is left after adding the item.\n    # For bins that cannot fit the item, this will be negative.\n    slack = bins_remain_cap - item\n    \n    # Identify bins where the item can actually fit (slack >= 0)\n    fit_mask = slack >= 0\n    \n    # For bins that can fit the item:\n    # The primary goal is to minimize slack (tight fit).\n    # A score of 1.0 / (1.0 + slack) achieves this:\n    # - Slack 0 gives priority 1.0\n    # - Slack 1 gives priority 0.5\n    # - Slack 10 gives priority ~0.09\n    # This prioritizes smaller slacks.\n    \n    # Secondary goal: balance with bin fullness by preferring smaller bins.\n    # We can penalize larger original bins by subtracting a term proportional to bins_remain_cap.\n    # This means for bins with similar slack, the one with less original capacity gets a higher priority.\n    \n    # Let's use a small penalty factor (e.g., 0.05) to ensure slack remains the dominant factor.\n    penalty_factor = 0.05\n    \n    # Calculate priority for fitting bins\n    # score = (primary_tight_fit_score) - (secondary_bin_size_penalty)\n    priorities[fit_mask] = (1.0 / (1.0 + slack[fit_mask])) - (penalty_factor * bins_remain_cap[fit_mask])\n    \n    # Ensure that bins which fit but result in a very low calculated priority (e.g., due to large original capacity)\n    # don't get selected over an obviously better bin. The current calculation already ensures\n    # positive values for reasonably tight fits. If a bin is barely fitting and very large,\n    # its score might be low, which is intended.\n\n    # Optionally, we could ensure that all fitting bins have a non-negative priority,\n    # but the current formula might produce small negative values if `bins_remain_cap` is very large\n    # and slack is also relatively large (though still >= 0).\n    # For instance, if bins_remain_cap=100, item=10, slack=90. Score = 1/(1+90) - 0.05*100 = 0.01 - 0.5 = -0.49.\n    # This might lead to selection of a bin with zero priority over one with a slightly negative score.\n    # To avoid this, we can set any negative calculated priority to 0.\n    # However, the goal is to select the *highest* priority. If all are negative, the least negative is chosen.\n    # A cleaner approach might be to ensure the scores are scaled to be positive and comparable.\n    \n    # Let's ensure priorities are non-negative to avoid issues if a bin barely fits and is very large.\n    # If a bin barely fits and is large, it might get a negative score by the formula,\n    # but a bin that fits slightly worse but is smaller might have a higher score.\n    # If all fitting bins get negative scores, the least negative is picked.\n    # Let's re-think: We want the MAX priority. If all calculated scores are negative,\n    # the least negative one is chosen. This is fine.\n    \n    # To further \"explore slightly larger gaps for robustness\", the `1.0 / (1.0 + slack)` function\n    # naturally gives decreasing priorities for increasing slack. It doesn't strongly penalize\n    # slightly larger gaps compared to the absolute tightest fit. The penalty_factor\n    # influences the preference among similar slack values.\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 72.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response4.txt_stdout.txt",
    "code_path": "problem_iter5_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Almost Full Fit strategy.\n\n    This strategy prioritizes bins that have just enough capacity to fit the item,\n    with a preference for bins that will have less remaining capacity after the item is added.\n    It balances tight fits with a slight preference for bins that are not excessively large,\n    even if they could accommodate the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate the remaining capacity if the item is placed in each bin\n    potential_remaining_cap = bins_remain_cap - item\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate their priority.\n    # The core idea of \"Almost Full Fit\" is to minimize the remaining capacity *after* placing the item.\n    # This means we want to prioritize bins where `potential_remaining_cap` is minimal and non-negative.\n    # A common heuristic for this is to use the inverse of `potential_remaining_cap` + epsilon.\n    # Using `1.0 / (1.0 + slack)` where slack = `potential_remaining_cap` is a good approach.\n    # It gives higher priority to smaller non-negative slacks (closer to 0).\n\n    # Calculate slack for bins that can fit the item\n    slack = potential_remaining_cap[fit_mask]\n\n    # Assign priority: Higher priority for smaller slack.\n    # The term `1.0 + slack` ensures that:\n    # 1. For a perfect fit (slack=0), priority is 1.0.\n    # 2. For small positive slacks, priority is high but less than 1.0.\n    # 3. For larger slacks, priority decreases.\n    # This prioritizes bins that become \"most full\" after adding the item.\n    priorities[fit_mask] = 1.0 / (1.0 + slack)\n\n    # To add robustness and avoid bins that are excessively large being chosen just because\n    # they might have a small negative `potential_remaining_cap` (if we were considering them),\n    # or even a small positive `slack` but still have a lot of original capacity,\n    # we can introduce a secondary factor that slightly penalizes bins with very large initial remaining capacity.\n    # This helps in ensuring that we don't \"waste\" large bins on items that could fit in smaller ones,\n    # while still prioritizing tight fits.\n    # A simple way to do this is to slightly decrease the priority for bins that have a large initial capacity.\n    # However, the primary driver should be the \"almost full\" characteristic after insertion.\n\n    # A common refinement is to use a function that is steep for small slacks and flattens out.\n    # The current `1.0 / (1.0 + slack)` achieves this.\n\n    # Let's consider the \"balance this with bin fullness\" part of the reflection.\n    # This might mean we don't want to pick a bin that's *too* empty, unless it's the only option.\n    # However, the primary goal is \"almost full fit\".\n    # The current heuristic `1.0 / (1.0 + slack)` strongly favors minimum slack.\n\n    # To balance minimal remaining capacity with bin fullness and explore slightly larger gaps:\n    # We want to prioritize small non-negative `potential_remaining_cap`.\n    # Let's consider a score that is higher for smaller `potential_remaining_cap`.\n    # And, for bins that have very large `potential_remaining_cap`, we can give them a slightly\n    # lower priority relative to their slack, to favor moderately sized bins.\n\n    # A simple approach for \"slightly larger gaps\":\n    # Consider the ratio of the item size to the bin's original capacity.\n    # `item / bins_remain_cap[fit_mask]`\n    # This ratio is higher for bins that are smaller relative to the item.\n    # We want to combine this with minimal slack.\n\n    # Let's refine the priority calculation.\n    # We want to maximize `1 / (1 + slack)`.\n    # To \"balance with bin fullness\" (or consider slightly larger gaps as a secondary preference if slack is similar),\n    # we can modify the score.\n\n    # Option 1: Prioritize minimal slack, then largest capacity among those with minimal slack.\n    # This is hard to do directly in a single priority score.\n\n    # Option 2: A score that favors minimal slack, but not too aggressively if the bin is very large.\n    # Consider `(1.0 / (1.0 + slack)) * (item / bins_remain_cap[fit_mask])` - this would favor\n    # smaller ratios, which means smaller bins for the item, but might conflict with minimal slack.\n\n    # Let's stick to the \"almost full\" and \"tight fit\" principle:\n    # The current `1.0 / (1.0 + slack)` is a strong candidate for \"tight fits\".\n    # The \"balance this with bin fullness\" might suggest avoiding picking a bin\n    # that, after packing, is still *very* empty, even if its slack is minimal compared to other bins.\n    # Example: Bin A (cap 100), item 98 -> slack 2, priority ~ 1/3. Bin B (cap 5), item 3 -> slack 2, priority ~ 1/3.\n    # The current heuristic would give them equal priority. If we want to favor Bin B (more filled relative to its capacity),\n    # we might introduce a term related to `item / original_capacity`.\n\n    # Let's consider a hybrid approach:\n    # Primary factor: Minimal slack (for \"almost full\").\n    # Secondary factor: Larger ratio of `item / (original_capacity_before_item_added)`\n    # to favor bins that are more utilized by this item.\n\n    # `original_capacity_before_item_added` = `bins_remain_cap[fit_mask]`\n    # So, we want to maximize `1.0 / (1.0 + slack)` AND maximize `item / bins_remain_cap[fit_mask]`.\n    # We can combine these multiplicatively.\n\n    # Let's try: `priority = (1.0 / (1.0 + slack)) * (item / bins_remain_cap[fit_mask])`\n    # For slack=0 (perfect fit): priority = `1.0 * (item / bins_remain_cap[fit_mask])`\n    # This would favor a perfect fit in a smaller bin.\n\n    # Example with this new approach:\n    # Item = 3\n    # Bin A: bins_remain_cap=5. Slack = 2. Ratio = 3/5 = 0.6. Priority = (1/3) * 0.6 = 0.2\n    # Bin B: bins_remain_cap=4. Slack = 1. Ratio = 3/4 = 0.75. Priority = (1/2) * 0.75 = 0.375\n    # Bin C: bins_remain_cap=3. Slack = 0. Ratio = 3/3 = 1.0. Priority = (1/1) * 1.0 = 1.0\n    # Bin D: bins_remain_cap=2. No fit. Priority = 0.\n    # Bin E: bins_remain_cap=6. Slack = 3. Ratio = 3/6 = 0.5. Priority = (1/4) * 0.5 = 0.125\n\n    # This seems to better capture \"tight fits\" (from slack) and also \"balancing with bin fullness\"\n    # by favoring bins that are more utilized by the current item (from the ratio).\n    # \"Explore slightly larger gaps for robustness\" is subjective, but this might pick\n    # a bin that's not *perfectly* the tightest, if it offers a better overall utilization ratio.\n\n    # Let's implement this refined heuristic.\n    # We need to be careful if `bins_remain_cap[fit_mask]` is zero or very small, but\n    # `fit_mask` already ensures `bins_remain_cap[fit_mask] >= item`, and we assume item > 0.\n    # So, `bins_remain_cap[fit_mask]` will be at least `item`, which is positive.\n\n    original_caps_fitting = bins_remain_cap[fit_mask]\n    utilization_ratio = item / original_caps_fitting\n\n    # Combine slack-based priority and utilization ratio.\n    # Higher priority for smaller slack AND higher utilization ratio.\n    # Using `1.0 / (1.0 + slack)` for minimal slack.\n    # Using `utilization_ratio` for better utilization.\n    # Multiplying them together:\n    priorities[fit_mask] = (1.0 / (1.0 + slack)) * utilization_ratio\n\n    # Consider edge case: if `item` is very close to `original_caps_fitting`,\n    # slack is close to 0, ratio is close to 1. This gives high priority.\n    # If `item` is much smaller than `original_caps_fitting`, slack is large, ratio is small.\n    # This seems to align with the desired properties.\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 91.37651812938249,
    "exec_success": true
  }
]