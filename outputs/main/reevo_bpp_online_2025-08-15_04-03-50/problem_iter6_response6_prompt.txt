{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Sigmoid Fit Score with a penalty for very large capacities and exploration.\n\n    This version prioritizes bins that have a remaining capacity that is just\n    slightly larger than the item size. This aims to minimize wasted space in\n    the selected bin, leaving larger contiguous free spaces in other bins for\n    potentially larger future items.\n\n    The priority is calculated using a sigmoid function applied to the difference\n    between the bin's remaining capacity and the item's size. A smaller\n    positive difference (a tighter fit) results in a higher priority score.\n    Additionally, bins with very large remaining capacities (relative to the item)\n    are penalized to encourage spreading. A small probability of exploration is also included.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    exploration_prob = 0.05\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_capacities = bins_remain_cap[fittable_bins_mask]\n    slack = fittable_bins_capacities - item\n\n    # Sigmoid for tight fits (prioritize near-zero slack)\n    k_fit = 5.0\n    tight_fit_score = 1.0 / (1.0 + np.exp(k_fit * slack))\n\n    # Penalty for very large remaining capacities.\n    # We want to discourage putting a small item into a very large bin if a tighter fit exists.\n    # This can be modeled with a logistic function that decreases as capacity grows.\n    # Let's use a threshold and a decay factor.\n    # We can normalize capacities relative to the item size or bin capacity.\n    # A simple approach: Penalize bins whose remaining capacity is much larger than the item.\n    # Let's create a score that is high for capacities close to the item size and decreases.\n    # We can use a similar sigmoid but inverted or a different function.\n    # Alternative: use 1 / (1 + exp(-k_large * (capacity - threshold)))\n    # A simpler penalty: if capacity is > C * item, reduce score.\n\n    # Let's try a penalty based on normalized slack.\n    # We want the penalty to be low for slack close to 0 and high for large slack.\n    # This is the opposite of the tight fit score.\n    # We can use a decaying function of slack.\n    # e.g., 1 / (1 + slack) or exp(-slack / scale)\n    # Let's try a sigmoid on the negative slack to penalize larger slack.\n    k_penalty = 0.5 # Controls how quickly the penalty increases with slack\n    large_capacity_penalty = 1.0 / (1.0 + np.exp(-k_penalty * slack))\n\n    # Combine scores. The tight_fit_score is high for small slack.\n    # The large_capacity_penalty is high for small slack, and low for large slack.\n    # We want to combine them such that:\n    # 1. Small slack (tight fit) is good -> high tight_fit_score\n    # 2. Large slack (too much space) is bad -> needs a penalty\n    # Let's try to combine them additively.\n    # A higher score for tight fits, and a lower score for large slack.\n    # The large_capacity_penalty goes from ~0.5 to ~1 as slack increases. This is not a penalty.\n    # Let's re-think the penalty. We want to penalize large slack.\n    # The sigmoid `1 / (1 + exp(k * slack))` already penalizes large slack.\n    # So maybe just use a weighted combination of the tight fit score and the slack itself.\n    # Or, use the slack directly, but inverted and scaled.\n\n    # Let's re-evaluate the reflection: \"penalize large remaining capacities\"\n    # The `tight_fit_score` already does this by giving low scores to large slack.\n    # Maybe the reflection implies we should *also* consider the absolute capacity.\n    # For example, putting an item of size 1 into a bin with remaining capacity 100\n    # is worse than putting it into a bin with remaining capacity 1.\n    # The slack approach (capacity - item) naturally handles this:\n    # slack for (100, 1) is 99, slack for (1, 1) is 0.\n\n    # Let's reconsider the \"Worst Fit\" element from v0 and combine it with tight fit.\n    # Best Fit component (tight_fit_score) -> prioritizes small slack\n    # Worst Fit component (prioritizes bins with more remaining capacity) -> prioritizes large capacity\n    # This seems contradictory. The reflection \"Focus on tighter fits, penalize large remaining capacities\"\n    # suggests we should prioritize small slack and penalize large slack. The `tight_fit_score` does this.\n\n    # Let's refine the \"penalty for very large remaining capacities\".\n    # A simple approach: normalize slack by some reference, or use a threshold.\n    # Let's say if remaining_capacity > 2 * item, we start penalizing.\n    # Max slack we want to consider for high priority: say, up to `max_slack_ideal`.\n    # Any slack beyond that should be heavily penalized.\n    max_slack_ideal = 2.0 * item # An item of size 'item' should ideally fit into a bin with 2*item remaining capacity at most for a good fit.\n    # Penalize slack if it's much larger than max_slack_ideal.\n    # We can use a linear penalty or a sigmoid that drops sharply.\n    # Let's use a sigmoid that is high for slack <= max_slack_ideal and low for slack > max_slack_ideal.\n    # Use `1 / (1 + exp(k_penalty * (slack - max_slack_ideal)))`\n    # This gives 0.5 at slack = max_slack_ideal, decreases for slack > max_slack_ideal.\n    k_penalty = 1.0 # Controls the steepness of the penalty\n    penalty_score = 1.0 / (1.0 + np.exp(k_penalty * (slack - max_slack_ideal)))\n\n    # Combine tight_fit_score and penalty_score.\n    # We want both to contribute positively.\n    # tight_fit_score is high for small slack.\n    # penalty_score is high for small slack (and slack <= max_slack_ideal).\n    # So, a linear combination might work.\n    # Let's weight them. Give more weight to the tight fit.\n    combined_scores = 0.7 * tight_fit_score + 0.3 * penalty_score\n\n    priorities[fittable_bins_mask] = combined_scores\n\n    # Apply exploration\n    if np.random.rand() < exploration_prob:\n        fittable_indices = np.where(fittable_bins_mask)[0]\n        chosen_bin_global_index = np.random.choice(fittable_indices)\n        priorities.fill(0)\n        priorities[chosen_bin_global_index] = 1.0\n    else:\n        # Normalize priorities for fittable bins to sum to 1\n        fittable_priorities = priorities[fittable_bins_mask]\n        if np.sum(fittable_priorities) > 0:\n            priorities[fittable_bins_mask] /= np.sum(fittable_priorities)\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Almost Full Fit strategy.\n\n    This strategy prioritizes bins that have just enough capacity to fit the item,\n    aiming to minimize the remaining capacity after placing the item.\n    It balances the \"tight fit\" aspect with a consideration for overall bin fullness,\n    slightly favoring bins that are already somewhat full but can still accommodate the item,\n    while penalizing bins that are excessively large.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit.\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate the slack (unused capacity) for bins that can fit the item.\n    slack = bins_remain_cap[fit_mask] - item\n\n    # The core idea is to prioritize bins with smaller slack (tighter fits).\n    # We use `1 / (1 + slack)` to give higher scores to smaller slack values.\n    # A slack of 0 (perfect fit) gets a priority of 1.0.\n    # A slack of 1 gets 0.5, etc.\n    # This part captures the \"almost full fit\".\n    tight_fit_score = 1.0 / (1.0 + slack)\n\n    # To balance this with robustness and avoid picking excessively large bins\n    # just because they might eventually fit something, we can also consider\n    # the absolute remaining capacity or its inverse.\n    # Let's introduce a factor that slightly favors bins that are already more full,\n    # but not to an extreme degree. A simple way is to use the inverse of the\n    # *original* remaining capacity for bins that fit.\n    # However, we want to combine this with the tight fit.\n    # Let's try a weighted sum or a multiplicative approach.\n\n    # A common heuristic is to prioritize bins that leave the least remaining space.\n    # This means we want to minimize `potential_remaining_cap = bins_remain_cap[fit_mask] - item`.\n    # So, a higher priority is associated with smaller `potential_remaining_cap`.\n    # Using `-potential_remaining_cap` as the base score would work.\n    # For example, if bins have remaining capacities [5, 4, 3, 2] and item is 3:\n    # Fits: [5, 4, 3]. Potential remaining: [2, 1, 0].\n    # Scores (-potential_remaining): [-2, -1, 0]. Max score is 0 for bin with capacity 3.\n\n    # Let's refine the \"Almost Full Fit\" to prioritize minimal slack.\n    # The previous version `1 / (1 + slack)` is good.\n    # To add robustness and avoid large empty bins, we can also penalize\n    # bins that have *very large* remaining capacity, even if the slack is small.\n    # A simple penalty could be `1 / (1 + bins_remain_cap[fit_mask])`.\n    # This gives higher scores to bins that are already less full.\n    # This seems counter to \"almost full fit\".\n\n    # The reflection suggests \"slightly larger gaps for robustness\".\n    # This might mean if there are multiple \"tight fits\" (small slack),\n    # pick the one that has a bit more capacity overall?\n    # Or, if an item doesn't fit tightly, consider bins that have a bit more room but are still reasonable.\n\n    # Let's stick to prioritizing the smallest non-negative slack.\n    # The `1 / (1 + slack)` approach already does this.\n    # To introduce \"slightly larger gaps for robustness\", perhaps we can add a small\n    # bonus for bins that are not *exactly* at slack=0, but close to it.\n    # Or, to avoid very large bins, we can cap the priority based on original capacity.\n\n    # Let's try a score that combines minimizing slack with not being excessively large.\n    # We can use `1 / (1 + slack)` as the primary score.\n    # To penalize very large bins, we could multiply by a factor related to `1 / bins_remain_cap[fit_mask]`.\n    # This would reduce the score for bins with larger initial capacity.\n    # For example: `priority = (1 / (1 + slack)) * (1 / (1 + bins_remain_cap[fit_mask]))`\n    # Let's test this:\n    # Bin A: remain=5, item=3 => slack=2. Score = (1/3) * (1/6) = 1/18\n    # Bin B: remain=4, item=3 => slack=1. Score = (1/2) * (1/5) = 1/10\n    # Bin C: remain=3, item=3 => slack=0. Score = (1/1) * (1/4) = 1/4\n    # This prioritizes perfect fits, then smaller slack, and then smaller original capacity.\n    # This seems like a reasonable heuristic reflecting the reflection.\n\n    # Avoid division by zero if original remaining capacity is 0 (though this shouldn't happen if item fits)\n    # and also for slack if it's very large. The `1 +` handles these.\n    original_capacity = bins_remain_cap[fit_mask]\n    robust_score = (1.0 / (1.0 + slack)) * (1.0 / (1.0 + original_capacity))\n\n    # Assign the calculated priorities to the bins that can fit the item.\n    priorities[fit_mask] = robust_score\n\n    # The reflection also mentions \"avoid overly complex combinations\".\n    # The current `robust_score` is a multiplicative combination, which is not overly complex.\n\n    # Let's consider the case where multiple bins offer the *same* minimal slack.\n    # For example, if item=2 and capacities are [5, 5, 6]. Slacks are [3, 3, 4].\n    # With `1/(1+slack)`, both 5s get 1/4.\n    # With `robust_score`, both 5s get (1/4)*(1/6) = 1/24. The 6 gets (1/5)*(1/7) = 1/35.\n    # In this scenario, the heuristic doesn't provide a strong tie-breaker between the two '5' bins.\n    # For online BPP, tie-breaking is often arbitrary or based on bin index.\n    # If we wanted to break ties by choosing the bin that becomes *more* full (smallest potential remaining capacity),\n    # we would add a penalty based on `potential_remaining_cap`.\n    # E.g., `robust_score - 0.01 * potential_remaining_cap`.\n    # For example, if remaining capacities were [5, 5, 6] and item = 2.\n    # Bins with capacity 5: slack=3, potential_remaining=3. `1/(1+3)` = 0.25.\n    # Bin with capacity 6: slack=4, potential_remaining=4. `1/(1+4)` = 0.20.\n    # If we want to prefer smaller potential remaining capacity among those with same slack,\n    # we could add a term.\n    # Let's try prioritizing smaller *original* capacity for bins with same slack, as it might preserve larger bins for larger items.\n    # The `robust_score` already does this implicitly via `1 / (1 + original_capacity)`.\n\n    # Let's refine the \"slightly larger gaps for robustness\". This might imply that\n    # if there are bins with *very* small slack (e.g., slack < epsilon), we might\n    # want to prefer slightly larger slack values that are still small, if they come from bins\n    # that are not too full. This sounds complex.\n\n    # A simpler interpretation: the \"Almost Full Fit\" should prioritize bins that result in the smallest possible positive remaining capacity.\n    # Our `1 / (1 + slack)` does exactly this. The added `* (1 / (1 + original_capacity))` term\n    # serves as the \"balance\" and robustness against overly large bins.\n\n    # Let's consider another possibility for robustness: if multiple bins have the minimum slack,\n    # perhaps we should pick the one that is currently \"most full\" to encourage denser packing.\n    # This would mean prioritizing bins with smaller `bins_remain_cap` among those with minimal slack.\n    # Our `robust_score` already implicitly favors smaller `bins_remain_cap`.\n\n    # Final decision: The `robust_score` implementation seems to capture the essence of prioritizing tight fits\n    # while adding a preference for less voluminous bins, thus providing a form of robustness.\n    # It's a balance between minimizing slack and not picking extremely large bins.\n\n    return priorities\n\n[Reflection]\nPrioritize tight fits. Penalize excessively large bins. Balance packing efficiency with robustness.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}