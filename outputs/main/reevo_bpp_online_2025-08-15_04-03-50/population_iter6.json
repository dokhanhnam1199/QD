[
  {
    "stdout_filepath": "problem_iter6_response0.txt_stdout.txt",
    "code_path": "problem_iter6_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Best Fit strategy with slack penalty.\n\n    This strategy prioritizes bins that leave minimal remaining capacity after packing\n    the current item (Best Fit). Additionally, it penalizes bins that have excessive\n    slack (remaining capacity significantly larger than the item size), aiming to\n    reserve large bins for potentially larger future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities to 0\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Get capacities of bins that can fit the item\n    fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n\n    # Calculate the slack for each fittable bin\n    slack = fitting_bins_capacities - item\n\n    # --- Score Calculation ---\n    # 1. Best Fit Component: Prioritize bins with minimum slack.\n    #    We use 1 / (1 + slack) so that smaller slack yields a higher score.\n    #    Adding 1 avoids division by zero and ensures positive scores.\n    best_fit_score = 1.0 / (1.0 + slack)\n\n    # 2. Slack Penalty Component: Penalize bins with excessive slack.\n    #    We define \"excessive\" as slack larger than a certain multiple of the item size.\n    #    Let's use a threshold like 2 * item.\n    #    A sigmoid function that decreases as slack increases can model this penalty.\n    #    `1 / (1 + exp(k * (slack - threshold)))` will be close to 1 for slack <= threshold\n    #    and decrease for slack > threshold.\n    slack_threshold = 2.0 * item  # Threshold for excessive slack\n    k_penalty = 0.5  # Steepness of the penalty function\n    slack_penalty = 1.0 / (1.0 + np.exp(k_penalty * (slack - slack_threshold)))\n\n    # Combine scores:\n    # We want to favor small slack (high best_fit_score) and penalize large slack\n    # (low slack_penalty). A simple multiplication is often effective here.\n    # The best_fit_score is high for small slack. The slack_penalty is high for small slack too.\n    # Multiplying them means both conditions (tight fit and not excessively large) contribute positively.\n    # A tighter fit (small slack) will naturally result in a higher score in both components.\n    # The penalty is more about preventing the use of overly large bins.\n    combined_scores = best_fit_score * slack_penalty\n\n    # Assign the calculated scores to the priorities for the fittable bins\n    priorities[can_fit_mask] = combined_scores\n\n    # Normalize priorities for fittable bins so they sum to 1 (optional, for probabilistic selection)\n    fittable_scores_sum = np.sum(priorities[can_fit_mask])\n    if fittable_scores_sum > 0:\n        priorities[can_fit_mask] /= fittable_scores_sum\n\n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 176.46653521143952,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response1.txt_stdout.txt",
    "code_path": "problem_iter6_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a strategy that prioritizes bins with minimal remaining space after packing (tightest fit), and among those, favors bins that are fuller (less remaining capacity).\n\n    This strategy is a refinement of Best Fit, aiming to pack items tightly while\n    also considering the overall fullness of the bins for tie-breaking.\n\n    1. Primary objective: Minimize the \"slack\" (remaining capacity - item size).\n       Bins with slack closer to zero are preferred. This is achieved by maximizing\n       a term inversely proportional to slack, or directly using negative slack\n       where smaller negative slack (closer to zero) is better.\n    2. Secondary objective: Among bins with the same slack, prefer bins that are fuller.\n       A fuller bin has less remaining capacity. This is achieved by preferring\n       bins with smaller `bins_remain_cap`.\n\n    The combined priority aims to achieve a lexicographical ordering: minimize slack first,\n    then minimize remaining capacity. This can be modeled by maximizing a score like\n    `-(slack + alpha * bins_remain_cap)`, where `alpha` is a scaling factor. For simplicity\n    and to directly reflect the two objectives, we can use `-(slack + bins_remain_cap)`.\n    The highest priority (least negative value) will correspond to the bin with the smallest\n    sum of slack and remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score (less negative) indicates a higher priority. Bins that cannot fit\n        the item will have a priority of negative infinity (or a very large negative number)\n        to ensure they are never chosen if fittable bins exist.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        # No bin can fit the item, return all -inf priorities\n        return priorities\n\n    # Calculate slack for fittable bins\n    # Slack = remaining_capacity - item_size\n    fittable_bins_capacities = bins_remain_cap[can_fit_mask]\n    slack = fittable_bins_capacities - item\n\n    # Combine objectives: Minimize slack, then minimize remaining capacity.\n    # We want to maximize a score that reflects this.\n    # Using `-(slack + bins_remain_cap)` achieves this.\n    # A smaller (slack + bins_remain_cap) value results in a larger (less negative) priority.\n    # Example:\n    # Bin A: slack=0, rem_cap=5  => sum=5, priority = -5\n    # Bin B: slack=0, rem_cap=10 => sum=10, priority = -10\n    # Bin C: slack=1, rem_cap=5  => sum=6, priority = -6\n    # Bin D: slack=1, rem_cap=10 => sum=11, priority = -11\n    # This prioritizes A > C > B > D, which is the desired order.\n    combined_score = -(slack + fittable_bins_capacities)\n\n    priorities[can_fit_mask] = combined_score\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 53.77443751081735,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response2.txt_stdout.txt",
    "code_path": "problem_iter6_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a smooth decay favoring tight fits.\n\n    This heuristic aims to favor bins with tight fits (small remaining capacity after packing)\n    while allowing for exploration of slightly larger gaps. It uses a function that\n    rewards small slack values but decreases smoothly, giving reasonable scores\n    to bins with moderate slack. This prevents prematurely filling bins that\n    might be only slightly larger than needed, thus keeping flexibility for future items.\n\n    The priority is calculated using a power-law inverse relationship with the slack\n    (remaining capacity minus item size). Specifically, it uses `1 / (1 + slack^p)`,\n    where `p` is a parameter controlling the steepness of the decay. A `p` value\n    less than 1 (e.g., 0.7) ensures that larger slacks are penalized less severely\n    than a simple inverse `1 / (1 + slack)`, promoting exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if np.any(fittable_bins_mask):\n        fittable_bins_capacities = bins_remain_cap[fittable_bins_mask]\n\n        # Calculate slack: remaining capacity after fitting the item.\n        # We want slack to be as close to 0 as possible for tight fits.\n        slack = fittable_bins_capacities - item\n\n        # Parameter to control the decay rate of priority as slack increases.\n        # A value < 1.0 makes the function decay slower than 1/(1+slack),\n        # favoring slightly larger gaps more than a strict \"Best Fit\".\n        # p = 0.7 is chosen as a balance: favors tight fits (slack near 0) but\n        # gives good scores to moderately tight fits (slack up to ~1-2),\n        # promoting exploration of slightly larger gaps.\n        p_value = 0.7\n\n        # Calculate priority: 1 / (1 + slack^p)\n        # This function maps slack=0 to priority=1, and as slack increases,\n        # the priority decreases towards 0. The power `p` controls how quickly\n        # this decrease happens. A smaller `p` results in a slower decay,\n        # thus exploring larger gaps more effectively.\n        # We add a small epsilon to slack to avoid potential numerical issues\n        # if slack is extremely close to zero or zero itself, although `0**p` is 0 for p>0.\n        # A small additive term like `epsilon` (e.g., 1e-9) can ensure the denominator is never exactly 0\n        # and also slightly reduces the priority for the absolute tightest fit, promoting exploration.\n        epsilon = 1e-9\n        priorities[fittable_bins_mask] = 1.0 / (1.0 + (slack + epsilon)**p_value)\n\n        # Ensure priorities are within a valid range and handle potential NaNs (unlikely here)\n        priorities[fittable_bins_mask] = np.nan_to_num(priorities[fittable_bins_mask], nan=0.0, posinf=1.0, neginf=0.0)\n        \n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 70.32403072095333,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response3.txt_stdout.txt",
    "code_path": "problem_iter6_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined strategy.\n\n    This strategy prioritizes bins that offer the \"tightest fit\" for the item,\n    meaning the remaining capacity after insertion is minimized. This is achieved\n    by maximizing `1.0 / (1.0 + slack)`, where slack is `remaining_capacity - item_size`.\n    Additionally, it introduces a secondary preference for bins that are already\n    \"fuller\" (i.e., have less initial remaining capacity) among those offering similar\n    tight fits, to promote more balanced packing. This favors bins that are closer to\n    being full, making it easier to close them sooner.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros\n    if not np.any(fit_mask):\n        return priorities\n\n    # Calculate the slack for bins that can fit the item\n    # Slack = remaining_capacity - item_size\n    slack = bins_remain_cap[fit_mask] - item\n\n    # Primary scoring: Prioritize minimal slack (tightest fits).\n    # Use `1.0 / (1.0 + slack)`: higher score for smaller slack.\n    # A slack of 0 gives priority 1.0.\n    primary_score = 1.0 / (1.0 + slack)\n\n    # Secondary scoring: Among bins with similar slacks, prioritize those that are\n    # \"fuller\" initially. This means preferring bins with smaller original `bins_remain_cap`.\n    # A small negative term `bins_remain_cap` will be added to the priority.\n    # We add a small epsilon to `slack` before inverting to avoid division by zero\n    # if an item perfectly fits into a bin (slack=0). However, since we are\n    # using `1.0 + slack`, this is not strictly necessary as `1.0 + 0 = 1.0`.\n    # The secondary score is simply the negative of the bin's remaining capacity.\n    # This implicitly favors bins that are closer to being full (smaller remaining capacity).\n    secondary_score = -bins_remain_cap[fit_mask]\n\n    # Combine scores: Maximize primary score (tightest fit), then secondary score (fuller bin).\n    # A simple addition works here, as the primary score is generally more significant.\n    # Normalization is not strictly required by the prompt but can be useful for\n    # comparing different scoring functions or if probabilities are needed.\n    # For this function, we are returning raw scores, and the selection mechanism\n    # (e.g., argmax) will handle picking the highest score.\n    priorities[fit_mask] = primary_score + secondary_score\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 76.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response4.txt_stdout.txt",
    "code_path": "problem_iter6_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Almost Full Fit strategy.\n\n    This strategy prioritizes bins that provide the \"tightest fit\" for the item,\n    meaning the remaining capacity after insertion is minimized. This is achieved\n    by maximizing `1.0 / (1.0 + slack)`, where slack is `remaining_capacity - item_size`.\n    Additionally, it introduces a secondary preference for bins that are already\n    \"fuller\" (i.e., have less initial remaining capacity) among those offering similar\n    tight fits, to promote more balanced packing. The weight of the secondary\n    criterion is tuned to balance the two objectives.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities\n\n    # Calculate the slack for bins that can fit the item\n    # Slack = remaining_capacity - item_size\n    slack = bins_remain_cap[fit_mask] - item\n\n    # Primary scoring: Prioritize minimal slack (tightest fits).\n    # Use `1.0 / (1.0 + slack)`: higher score for smaller slack.\n    # A slack of 0 gives priority 1.0.\n    primary_score = 1.0 / (1.0 + slack)\n\n    # Secondary scoring: Among bins with similar slacks, prioritize those that are\n    # \"fuller\" initially. This means preferring bins with smaller original `bins_remain_cap`.\n    # We add a term proportional to the negative of the initial remaining capacity.\n    # The coefficient `alpha` is tuned to balance the primary and secondary criteria.\n    # A smaller alpha means the \"fuller bin\" preference has less impact.\n    alpha = 0.05  # Tuned parameter\n    secondary_score = -alpha * bins_remain_cap[fit_mask]\n\n    # Combine scores: maximize primary score, then secondary score\n    priorities[fit_mask] = primary_score + secondary_score\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 93.45440529575887,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response5.txt_stdout.txt",
    "code_path": "problem_iter6_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, prioritizing tight fits and penalizing large gaps.\n\n    This version prioritizes bins that have a remaining capacity that is just\n    slightly larger than the item size, aiming to minimize wasted space.\n    It also penalizes bins with very large remaining capacities to encourage\n    spreading and avoid creating excessively large empty spaces that might\n    not be efficiently utilized by future items. A small amount of random noise\n    is added to the scores to break ties and encourage exploration.\n\n    The priority is calculated as follows:\n    1. For bins that can fit the item, the base priority is related to the\n       'slack' (remaining capacity - item size). A smaller slack is better.\n    2. A penalty is applied for large slack. This penalty increases as the\n       slack grows beyond a certain threshold (e.g., twice the item size).\n    3. A small random value is added to introduce stochasticity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of -inf.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    eligible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(eligible_bins_mask):\n        return priorities\n\n    fittable_bins_capacities = bins_remain_cap[eligible_bins_mask]\n    slack = fittable_bins_capacities - item\n\n    # --- Prioritize tight fits ---\n    # We want smaller slack to have higher priority.\n    # A good score would be inversely related to slack, e.g., 1 / (1 + slack).\n    # Using sigmoid of negative slack: 1 / (1 + exp(-k * slack))\n    # This gives higher values for smaller slack.\n    k_fit = 5.0  # Steepness for tight fits\n    tight_fit_score = 1.0 / (1.0 + np.exp(-k_fit * slack))\n\n    # --- Penalize large gaps ---\n    # We want to penalize slack values that are significantly larger than ideal.\n    # Define an \"ideal maximum slack\" (e.g., twice the item size).\n    # Beyond this, we want the score to decrease.\n    ideal_max_slack = 2.0 * item\n    # Use a sigmoid that penalizes slack > ideal_max_slack.\n    # 1 / (1 + exp(k * (slack - ideal_max_slack)))\n    # This score is ~0.5 at ideal_max_slack and decreases for larger slack.\n    k_penalty = 1.0 # Steepness of the penalty\n    large_gap_penalty = 1.0 / (1.0 + np.exp(k_penalty * (slack - ideal_max_slack)))\n\n    # Combine scores. We want both tight fits and reasonable gaps.\n    # The tight_fit_score is high for small slack.\n    # The large_gap_penalty is high for small slack (and below ideal_max_slack).\n    # A simple linear combination seems appropriate.\n    # Let's weight the tight fit more, as minimizing waste is primary.\n    combined_score_values = 0.7 * tight_fit_score + 0.3 * large_gap_penalty\n\n    # Add a small random noise to break ties and encourage exploration.\n    random_noise = np.random.rand(len(slack)) * 1e-6\n    priorities[eligible_bins_mask] = combined_score_values + random_noise\n\n    return priorities",
    "response_id": 5,
    "obj": 24.780614280015957,
    "cyclomatic_complexity": 2.0,
    "halstead": 259.4606049037673,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response6.txt_stdout.txt",
    "code_path": "problem_iter6_code6.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Almost Full Fit with a preference for less empty space.\n\n    This strategy prioritizes bins that have just enough capacity to fit the item,\n    aiming to minimize the remaining capacity after placing the item (tight fit).\n    To promote robustness and prevent packing small items into extremely large,\n    mostly empty bins, it also penalizes bins with very large remaining capacities.\n    The priority score is a combination that rewards tighter fits and discourages\n    placing items into bins that will remain significantly empty.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit.\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities\n\n    fittable_capacities = bins_remain_cap[fit_mask]\n\n    # Calculate the slack (unused capacity) for bins that can fit the item.\n    # This is the primary factor for \"almost full fit\". Smaller slack is better.\n    slack = fittable_capacities - item\n\n    # Score based on minimizing slack: 1 / (1 + slack)\n    # A perfect fit (slack=0) gets a score of 1.0.\n    # As slack increases, the score decreases.\n    tight_fit_score = 1.0 / (1.0 + slack)\n\n    # Penalize excessively large bins. We want to avoid putting a small item\n    # into a bin that will still be mostly empty. This promotes more even\n    # distribution and leaves larger contiguous spaces in other bins.\n    # A simple penalty can be based on the inverse of the remaining capacity.\n    # Bins with larger remaining capacity should get a lower penalty factor.\n    # Using 1 / (1 + capacity) ensures we don't divide by zero and also\n    # that larger capacities yield smaller (less favorable) scores here.\n    # This factor is for the *original* remaining capacity of the fittable bins.\n    large_bin_penalty_factor = 1.0 / (1.0 + fittable_capacities)\n\n    # Combine the tight fit score with the penalty factor.\n    # We multiply them: prioritize tight fits AND penalize large bins.\n    # This means bins that are a tight fit AND have smaller remaining capacity\n    # will get the highest scores.\n    combined_priority = tight_fit_score * large_bin_penalty_factor\n\n    # Assign the calculated priorities to the bins that can fit the item.\n    priorities[fit_mask] = combined_priority\n\n    # Normalize priorities among fittable bins so they sum to 1.\n    # This ensures that the selection is probabilistic and relative.\n    fittable_priorities = priorities[fit_mask]\n    if np.sum(fittable_priorities) > 0:\n        priorities[fit_mask] /= np.sum(fittable_priorities)\n    else:\n        # If all fittable priorities are zero (e.g., due to extreme large_bin_penalty_factor),\n        # fall back to uniform probability among fittable bins.\n        # This ensures we always select a bin if one is available.\n        priorities[fit_mask] = 1.0 / len(fittable_priorities)\n\n    return priorities",
    "response_id": 6,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 142.7018117963935,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response7.txt_stdout.txt",
    "code_path": "problem_iter6_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, prioritizing fuller bins first, then best fit.\n\n    This heuristic prioritizes bins that are already fuller (have less remaining capacity).\n    If multiple bins have the same minimal remaining capacity, it then applies the Best Fit\n    principle to choose the one that results in the least waste. This aims to pack items\n    more densely by preferring bins that are closer to being full. The strategy aims for\n    deterministic behavior, prioritizing fuller bins, and then best fit among equally full bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher score indicates higher priority. Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9  # Small value to prevent division by zero\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    fittable_bins_capacities = bins_remain_cap[can_fit_mask]\n\n    # Primary objective: Prioritize fuller bins (smaller remaining capacity).\n    # Score component 1: Inverse of remaining capacity. Higher score for smaller capacity.\n    fullness_score_component = 1.0 / (fittable_bins_capacities + epsilon)\n\n    # Secondary objective: Among equally full bins, prioritize best fit (minimize waste).\n    # Waste = remaining_capacity - item_size.\n    # Score component 2: Inverse of waste. Higher score for smaller waste.\n    waste = fittable_bins_capacities - item\n    best_fit_score_component = 1.0 / (waste + epsilon)\n\n    # Combine scores: Summing the two components gives a higher priority to bins that are\n    # both fuller and offer a better fit. This combination prioritizes fullness first\n    # due to the nature of inverse functions, and then best fit as a tie-breaker or\n    # secondary factor.\n    # Example:\n    # Bin A: remain_cap=0.6, item=0.5 -> waste=0.1. Score = 1/0.6 + 1/0.1 = 1.667 + 10 = 11.667\n    # Bin B: remain_cap=0.7, item=0.5 -> waste=0.2. Score = 1/0.7 + 1/0.2 = 1.428 + 5 = 6.428\n    # Bin C: remain_cap=0.6, item=0.4 -> waste=0.2. Score = 1/0.6 + 1/0.2 = 1.667 + 5 = 6.667\n    # Bin A (fullest, best fit) gets highest score. Bin C (equally full as A, worse fit) gets second highest. Bin B (less full, worse fit) gets lowest.\n    # This combination correctly prioritizes fullness, then best fit.\n    combined_scores = fullness_score_component + best_fit_score_component\n\n    priorities[can_fit_mask] = combined_scores\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 92.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response8.txt_stdout.txt",
    "code_path": "problem_iter6_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined strategy.\n\n    This strategy favors tight fits by prioritizing bins with minimal remaining capacity\n    after packing the item. To encourage exploration and prevent premature selection\n    of bins that are only slightly better fits, the priority function exhibits a\n    gentler decay for small remaining capacities compared to a simple inverse relationship.\n    This means bins with a small positive remaining capacity (slack) are preferred,\n    but the preference does not drop off extremely sharply as slack increases, allowing\n    for exploration of slightly less tight fits.\n\n    The priority score is calculated using a function of the form `1 / (1 + slack^p)`,\n    where `slack = bins_remain_cap - item` and `p` is a parameter less than 1.\n    A smaller `p` leads to a gentler decay and thus more exploration of larger gaps.\n    A value of `p = 0.7` is used here to balance tight fitting with exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if np.any(fittable_bins_mask):\n        fittable_bins_capacities = bins_remain_cap[fittable_bins_mask]\n\n        # Calculate slack: remaining capacity after fitting the item.\n        # We want slack to be as close to 0 as possible.\n        slack = fittable_bins_capacities - item\n\n        # Use a power function `slack^p` with `p < 1` (e.g., 0.7) to achieve a gentler decay.\n        # This favors tight fits (small slack) but gives relatively higher scores\n        # to bins with moderately small slack compared to a `1/(1+slack)` approach.\n        # This encourages exploration of slightly larger gaps.\n        p_value = 0.7\n        \n        # Calculate priority: 1 / (1 + slack^p).\n        # For slack=0, priority is 1. For larger slack, it decreases.\n        # The power `p` controls the steepness of the decay.\n        # A value of `p=0.7` makes the decay less steep than `p=1`.\n        \n        # Add a small epsilon to slack before exponentiation to ensure numerical stability\n        # and prevent potential issues if slack is extremely close to zero, though\n        # `0**p` is well-defined for p > 0.\n        # For practical purposes, `np.power` handles this gracefully.\n        \n        # Ensure slack is non-negative, though `fittable_bins_mask` already guarantees this.\n        # If `item` can be 0, `slack` can be `bins_remain_cap`.\n        \n        # Handle the case where slack might be extremely small, leading to `slack**p_value` being\n        # very close to zero, making the priority close to 1. This is desired for tight fits.\n        \n        # Calculate `slack**p_value`. `np.power` handles `0**p` correctly (results in 0 for p>0).\n        # Using `np.power` is generally preferred over `**` for array operations for clarity and potential optimizations.\n        priorities[fittable_bins_mask] = 1.0 / (1.0 + np.power(slack, p_value))\n\n        # Clean up any potential NaN or Inf values, although unlikely with this formula and valid inputs.\n        priorities[fittable_bins_mask] = np.nan_to_num(priorities[fittable_bins_mask], nan=0.0, posinf=1.0, neginf=0.0)\n\n    return priorities",
    "response_id": 8,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 39.863137138648355,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response9.txt_stdout.txt",
    "code_path": "problem_iter6_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined strategy that prioritizes tight fits, penalizes excessive capacity, and balances with robustness.\n\n    This heuristic aims to:\n    1. Prioritize bins with minimal remaining capacity after fitting the item (tightest fit).\n    2. Penalize bins that have a very large remaining capacity, even if they can fit the item,\n       to avoid wasting large free spaces on small items if better alternatives exist.\n    3. Introduce robustness by slightly favoring bins that are not extremely empty if multiple\n       tight fits are available, or by not overly penalizing slightly larger slack if the bin\n       capacity itself is moderate.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Mask for bins that can accommodate the item.\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities\n\n    fittable_capacities = bins_remain_cap[fit_mask]\n    slack = fittable_capacities - item\n\n    # Score component 1: Prioritize tightest fits.\n    # A simple way is to use the inverse of slack + 1.\n    # Score is 1 for slack=0, decreases as slack increases.\n    tight_fit_score = 1.0 / (1.0 + slack)\n\n    # Score component 2: Penalize excessive capacity.\n    # We want to reduce priority for bins with very large remaining capacity.\n    # A common approach is to use a function that decays with capacity.\n    # Using `1 / (1 + capacity)` can achieve this. Larger capacity -> smaller score.\n    # This encourages using bins that are already more utilized.\n    capacity_consideration_score = 1.0 / (1.0 + fittable_capacities)\n\n    # Combine scores multiplicatively. This way, if either score is low, the combined score is low.\n    # We want a high score if the fit is tight AND the capacity is not excessive.\n    combined_score = tight_fit_score * capacity_consideration_score\n\n    # Add a small boost to ensure that even if capacity_consideration_score is small,\n    # a perfect tight fit can still have a high overall priority.\n    # E.g., if capacity=100, item=1, slack=99. tight_fit=1/100. capacity_consideration=1/101.\n    # Combined = 1/10100. This is very low.\n    # Maybe a weighted sum is better for balancing.\n\n    # Let's reconsider the reflection: \"prioritize tight fits, penalize excessive capacity, and balance with robustness.\"\n    # A tight fit means small slack. A penalty for excessive capacity means a low score for large remaining_capacity.\n    # Robustness might mean not being too sensitive to tiny variations, or favoring bins that are not *too* empty.\n\n    # Let's try a different combination:\n    # Prioritize small slack: `slack` itself, or `1/(1+slack)`.\n    # Penalize large capacity: `-fittable_capacities` or `1/(1+fittable_capacities)`.\n    # Let's use a weighted approach for clarity and tunability.\n    # Weight for tight fit: emphasize minimizing waste.\n    # Weight for capacity: deemphasize using huge bins for small items.\n\n    # Let's create a score for tightness and a score for \"emptiness\" (inverse capacity).\n    # Score_tightness: 1 if slack is 0, decreases for larger slack. `np.exp(-slack / scale_slack)` where scale_slack is small.\n    # Score_emptiness: 1 if capacity is very small, decreases for large capacity. `np.exp(-fittable_capacities / scale_capacity)` where scale_capacity is moderate.\n\n    # A simpler, common approach is to directly optimize for minimal slack, and break ties with maximal original capacity (Worst Fit heuristic) or minimal original capacity (Best Fit heuristic).\n    # Our reflection asks for tight fits (Best Fit-like) and penalty for excessive capacity (which means favoring smaller capacities), suggesting Best Fit with a cap on large bins.\n\n    # Let's try the following composite score:\n    # Priority = (1 - slack / BinCapacity) * (1 / (1 + slack))\n    # This is trying to normalize slack by capacity, which is tricky.\n\n    # Let's stick to the idea of minimizing slack primarily, and using capacity as a secondary factor.\n    # For robustness, perhaps we can slightly prefer slack that is small but not zero,\n    # if it comes from a moderate capacity bin, over a zero slack from a very large bin.\n\n    # Let's try a score that is high for small slack, and for bins that are not excessively large.\n    # Penalty for large slack: `np.exp(-slack / slack_scale)`\n    # Penalty for large capacity: `np.exp(-fittable_capacities / capacity_scale)`\n    # Combine them additively or multiplicatively.\n\n    # Alternative reflection interpretation:\n    # Tight fit: prefer bins where `fittable_capacities - item` is small.\n    # Penalize excessive capacity: if `fittable_capacities` is very large, reduce priority.\n    # Robustness: if there are multiple \"good\" options (e.g., similar small slack),\n    # maybe pick the one that leaves more room, or the one that is less full.\n\n    # Let's go with a score that is high for small slack and also for moderate original capacity.\n    # Score = (1 / (1 + slack)) * (1 / (1 + fittable_capacities))\n    # This is what `priority_v1` used. Let's refine the 'robustness' aspect.\n\n    # Robustness: If multiple bins offer the same minimal slack, which one to choose?\n    # The `(1 / (1 + fittable_capacities))` term already favors smaller capacities, which is a form of robustness.\n\n    # Let's introduce a slight penalty for *too tight* fits if the capacity is very large.\n    # For example, if slack is very small but capacity is huge.\n    # This means we want to avoid `slack` being very small *and* `fittable_capacities` being very large.\n    # This is implicitly handled by the multiplication: `(1/(1+slack)) * (1/(1+capacity))`.\n\n    # Consider the contribution:\n    # small slack: High `1/(1+slack)`\n    # small capacity: High `1/(1+capacity)`\n    # large slack: Low `1/(1+slack)`\n    # large capacity: Low `1/(1+capacity)`\n\n    # The product `tight_fit_score * capacity_consideration_score` naturally prioritizes bins that are\n    # both good fits and not excessively large.\n\n    # Let's add a component that slightly prefers bins that aren't nearly empty,\n    # for robustness (to avoid fragmenting space too much).\n    # This would mean favoring bins with higher `fittable_capacities` among those with similar slack.\n    # This contradicts the \"penalize excessive capacity\" slightly.\n\n    # Let's focus on the main reflection: \"prioritize tight fits, penalize excessive capacity, and balance with robustness.\"\n    # `tight_fit_score` addresses the first point.\n    # `capacity_consideration_score` addresses the second point.\n    # The multiplication `tight_fit_score * capacity_consideration_score` balances these.\n\n    # For robustness, we might want to ensure that the priority doesn't drop too sharply.\n    # A simple scaling might be helpful.\n    # Let's normalize the scores before combining.\n    # `norm_tight_fit = (tight_fit_score - min(tight_fit_score)) / (max(tight_fit_score) - min(tight_fit_score))`\n\n    # Let's try a simplified approach that is robust and still effective.\n    # Focus on minimizing slack. If there are ties in slack, pick the one with less total capacity.\n    # This is essentially a Best Fit strategy with tie-breaking towards smaller bins.\n\n    # The existing `tight_fit_score` (1 / (1 + slack)) is excellent for prioritizing minimal slack.\n    # The `capacity_consideration_score` (1 / (1 + fittable_capacities)) penalizes large bins.\n    # The product is a good compromise.\n\n    # Let's consider a case:\n    # item = 1\n    # bins = [10, 11, 2, 3]\n    # fittable_capacities = [10, 11, 2, 3]\n    # slack = [9, 10, 1, 2]\n    # tight_fit_score = [1/10, 1/11, 1/2, 1/3] = [0.1, 0.0909, 0.5, 0.333]\n    # capacity_consideration_score = [1/11, 1/12, 1/3, 1/4] = [0.0909, 0.0833, 0.333, 0.25]\n    # Combined = [0.00909, 0.00757, 0.1665, 0.08325]\n    # This correctly prioritizes the bin with capacity 2 (slack 1), then capacity 3 (slack 2).\n    # The bins with capacity 10 and 11 (large slack) are penalized.\n\n    # What if capacities are [3, 4] and item is 3?\n    # fittable_capacities = [3, 4]\n    # slack = [0, 1]\n    # tight_fit_score = [1/1, 1/2] = [1, 0.5]\n    # capacity_consideration_score = [1/3, 1/4] = [0.333, 0.25]\n    # Combined = [0.333, 0.125]\n    # This prioritizes the bin with capacity 3 (perfect fit, smaller capacity) over bin with capacity 4 (tight fit, larger capacity).\n\n    # What if capacities are [100, 101] and item is 1?\n    # fittable_capacities = [100, 101]\n    # slack = [99, 100]\n    # tight_fit_score = [1/100, 1/101] = [0.01, 0.0099]\n    # capacity_consideration_score = [1/101, 1/102] = [0.0099, 0.0098]\n    # Combined = [0.000099, 0.000097]\n    # This prioritizes the bin with capacity 100, which is the \"tighter\" fit and also smaller capacity.\n\n    # Let's try to introduce a slight preference for slightly larger slack if the capacity is not excessive.\n    # This is a form of \"robustness\" by not making a bin *too* full.\n    # Example: item=2, bins=[4, 5].\n    # Cap=[4, 5], Slack=[2, 3]\n    # TFs=[1/3, 1/4] = [0.333, 0.25]\n    # CS=[1/5, 1/6] = [0.2, 0.166]\n    # Comb=[0.0666, 0.0416] -> Prefers bin 4.\n\n    # If we wanted to prefer bin 5 in this case (slightly more room), we'd need a different heuristic.\n    # Maybe add a small bonus to slack?\n    # `score = (1/(1+slack) + bonus_slack) * (1/(1+capacity))`\n    # `bonus_slack = slack * slack_bonus_factor`\n    # Let's try `slack_bonus_factor = 0.01`\n    # For bin 4: `(0.333 + 0.02) * 0.2 = 0.353 * 0.2 = 0.0706`\n    # For bin 5: `(0.25 + 0.03) * 0.166 = 0.28 * 0.166 = 0.0465`\n    # This now favors bin 4 even more.\n\n    # What if we use `slack` directly in the second term?\n    # `score = (1/(1+slack)) * (1/(1 + slack + capacity))` This doesn't seem right.\n\n    # Let's modify the capacity term to be less punishing for moderate capacities.\n    # Maybe the `capacity_consideration_score` should not decay as rapidly.\n    # Or, we can add a slight bonus for the \"tighter\" fit among those that are already \"somewhat full\".\n\n    # The prompt requests: \"prioritize tight fits, penalize excessive capacity, and balance with robustness.\"\n    # The current combined score `tight_fit_score * capacity_consideration_score` does prioritize tight fits\n    # and penalizes excessive capacity. For robustness, it's somewhat balanced.\n\n    # Let's consider the \"robustness\" as not creating extremely full bins.\n    # This would mean slightly penalizing zero slack if the capacity is very small.\n    # E.g., item=2, bins=[2, 5].\n    # Cap=[2, 5], Slack=[0, 3]\n    # TFs=[1, 1/4]=[1, 0.25]\n    # CS=[1/3, 1/6]=[0.333, 0.166]\n    # Comb=[0.333, 0.0416] -> Prefers bin 2.\n\n    # If we want to prefer bin 5 for robustness, we'd need to slightly boost slack.\n    # Let's try `score = (1/(1+slack + epsilon)) * (1/(1+capacity))` with epsilon being small.\n    # Or, `score = (1/(1+slack)) * (1/(1+capacity) + epsilon_cap)`\n    # Or, `score = (1/(1+slack)) * exp(-capacity/scale)`\n\n    # Let's try a sigmoid for the slack that emphasizes small slack, and a separate term for capacity.\n    # Score = `sigmoid(k_tight * (slack_max - slack))` where slack_max is some upper bound for good fits.\n    # And `penalty_large_cap = sigmoid(k_large * (capacity_max - capacity))`\n\n    # A common practical heuristic for online BPP is First Fit Decreasing (FFD) or Best Fit Decreasing (BFD)\n    # for offline, but for online, First Fit (FF) or Best Fit (BF) are common.\n    # Best Fit aims for minimal slack.\n    # Our `1 / (1 + slack)` is a good approximation of BF's objective.\n    # The reflection adds \"penalize excessive capacity\".\n\n    # Let's consider a different formulation:\n    # Prioritize bins that leave minimum remaining capacity: `fittable_capacities - item`.\n    # Maximize `-(fittable_capacities - item)` = `item - fittable_capacities`.\n    # For robustness, perhaps add a term related to the original capacity, but not too heavily.\n    # Maximize `(item - fittable_capacities) + w * fittable_capacities` where w is small and positive.\n    # `(item - fittable_capacities) + w * fittable_capacities = item + (w-1) * fittable_capacities`.\n    # This means we want to minimize `fittable_capacities`.\n\n    # Let's go back to the original multiplicative idea but ensure it reflects the priorities clearly.\n    # Priority = `(fittable_capacities - item)` is what we want to minimize.\n    # So higher priority for smaller `slack`.\n    # `priority = 1 / (1 + slack)` is good.\n\n    # Penalize excessive capacity: if `fittable_capacities` is large, reduce priority.\n    # Combine: `priority = (1 / (1 + slack)) * (1 / (1 + fittable_capacities))`\n\n    # Robustness: If we have two bins with slack=0 and capacities=2 and 100.\n    # TF_scores = [1, 1]. CS_scores = [1/3, 1/101]. Comb = [1/3, 1/101]. Prefers bin 2.\n    # This seems reasonable.\n\n    # Let's consider \"robustness\" as: among bins that are good fits, pick one that is not too empty.\n    # This means if slack is similar, prefer larger capacity.\n    # If current score is `S = (1/(1+slack)) * (1/(1+capacity))`.\n    # For two bins with same slack, `s`:\n    # Bin A: Cap `c1`. Score `S_A = (1/(1+s)) * (1/(1+c1))`.\n    # Bin B: Cap `c2 > c1`. Score `S_B = (1/(1+s)) * (1/(1+c2))`.\n    # `S_A > S_B`. So it currently prefers smaller capacity.\n\n    # To prefer larger capacity for robustness (among similar slack fits), we need to modify the `capacity_consideration_score`.\n    # Instead of `1/(1+capacity)`, maybe use something that increases with capacity up to a point.\n    # Or, we can add a term based on capacity to the existing score.\n\n    # Let's reconsider the reflection: \"prioritize tight fits, penalize excessive capacity, and balance with robustness.\"\n    # Tight fit: minimize `slack`.\n    # Penalize excessive capacity: if `capacity` is large, reduce priority.\n    # Balance with robustness: This could mean:\n    # 1. If many tight fits exist, choose the one that is not too full. (Favors larger capacity among tight fits)\n    # 2. If capacities are similar, slightly prefer larger slack to leave more room. (Favors larger slack)\n\n    # Let's try to incorporate preference for larger capacity among good fits.\n    # We can add a term to the score that is proportional to capacity.\n    # `score = (1 / (1 + slack)) * (1 / (1 + fittable_capacities)) + weight * fittable_capacities`\n    # Let's use a specific transformation for slack and capacity.\n    # Slack score: `exp(-slack / scale_slack)`\n    # Capacity score: `exp(capacity / scale_capacity)` - this is not good as it encourages very large bins.\n    # Capacity score: `log(1 + capacity)` - this increases with capacity, but slowly.\n\n    # Let's combine the inverse slack and the inverse capacity, and see if we can tune it for robustness.\n    # `score = (1.0 / (1.0 + slack)) * (1.0 / (1.0 + fittable_capacities))`\n    # This prioritizes tight fits and penalizes large bins. This is already a good balance.\n    # For further robustness, consider how to break ties between similar scores.\n\n    # Let's modify the `capacity_consideration_score` slightly. Instead of penalizing large capacities,\n    # let's just use their magnitude as a tie-breaker.\n    # The primary driver is still tight fit.\n    # If `slack` is small, `tight_fit_score` is high.\n    # If we have similar `tight_fit_score` (meaning similar small slack),\n    # we want to select the one with larger `fittable_capacities` for robustness (leaves more space).\n    # So, we want to *add* a term proportional to `fittable_capacities`.\n    # `score = (1.0 / (1.0 + slack)) + w * fittable_capacities`  (This might over-emphasize capacity)\n\n    # Let's try to amplify the effect of small slack and moderate capacities.\n    # Use a scaled inverse for slack: `1 / (1 + k_slack * slack)`\n    # Use a scaled inverse for capacity: `1 / (1 + k_capacity * fittable_capacities)`\n    # And combine them multiplicatively.\n\n    # Let's re-evaluate the prompt and reflection.\n    # \"Prioritize tight fits\": high score for small slack.\n    # \"Penalize excessive capacity\": low score for large capacity.\n    # \"Balance with robustness\": This is the tricky part. It could mean:\n    #   a) Among tight fits, prefer bins that aren't too full (favors larger capacity).\n    #   b) Avoid filling a bin too much if a slightly larger gap exists in a moderate bin.\n\n    # Consider a score: `1 / (1 + slack) + factor * (fittable_capacities / BinCapacity_Max)`\n    # This might be too complex.\n\n    # Let's try a simple modification of the `v1` score.\n    # `v1_score = (1.0 / (1.0 + slack)) * (1.0 / (1.0 + fittable_capacities))`\n    # This prioritizes small slack and small capacity.\n    # To favor robustness (larger capacity among tight fits), we want to increase the score for larger capacities.\n    # Let's consider `score = (1.0 / (1.0 + slack)) + w * fittable_capacities`\n    # Let `w` be small, e.g., `1e-3`.\n\n    # Example: item=2, bins=[3, 100]\n    # Cap=[3, 100], Slack=[1, 98]\n    # TF=[0.5, 0.0102]\n    # If `w=0.001`:\n    # Score for bin 3: `0.5 + 0.001 * 3 = 0.503`\n    # Score for bin 100: `0.0102 + 0.001 * 100 = 0.0102 + 0.1 = 0.1102`\n    # This still prefers bin 3, which is expected because the slack is much smaller.\n\n    # Example: item=50, bins=[55, 60]\n    # Cap=[55, 60], Slack=[5, 10]\n    # TF=[1/6, 1/11] = [0.166, 0.0909]\n    # If `w=0.001`:\n    # Score for bin 55: `0.166 + 0.001 * 55 = 0.166 + 0.055 = 0.221`\n    # Score for bin 60: `0.0909 + 0.001 * 60 = 0.0909 + 0.060 = 0.1509`\n    # This correctly prioritizes the bin with less slack (55).\n\n    # The additive approach seems to give more weight to the tight fit, and capacity is a tie-breaker.\n    # This aligns with prioritizing tight fits, penalizing excessive capacity (by giving lower base scores to larger capacities),\n    # and then balancing with robustness (favoring larger capacities among similarly fitting bins).\n\n    # Let's re-implement this additive approach.\n    # The \"penalty for excessive capacity\" is still handled by the base `1/(1+slack)`.\n    # The additive term `w * fittable_capacities` is for robustness (favoring less full bins among good fits).\n\n    # Consider the range of values. Slack can be large. Capacity can be large.\n    # `1/(1+slack)` can be small. `w * capacity` can be large if `w` is not chosen carefully.\n    # Example: item=1, bins=[1000, 1001]\n    # Cap=[1000, 1001], Slack=[999, 1000]\n    # TF=[1/1000, 1/1001] = [0.001, 0.000999]\n    # If `w=0.001`:\n    # Score bin 1000: `0.001 + 0.001 * 1000 = 0.001 + 1 = 1.001`\n    # Score bin 1001: `0.000999 + 0.001 * 1001 = 0.000999 + 1.001 = 1.001999`\n    # This favors bin 1001, which has larger capacity. This is consistent with robustness.\n\n    # Let's try to integrate the \"penalize excessive capacity\" more directly into the score,\n    # not just as a tie-breaker.\n    # Reflection: \"prioritize tight fits, penalize excessive capacity, and balance with robustness.\"\n\n    # Let's try a weighted sum of a measure of tightness and a measure of capacity.\n    # Tightness measure: `1 - slack / max_possible_slack` or `exp(-slack / scale_s)`\n    # Capacity measure: `1 / (1 + capacity)` or `exp(-capacity / scale_c)`\n\n    # Consider score = `w1 * (1 / (1 + slack)) + w2 * (1 / (1 + fittable_capacities))`\n    # This is a linear combination. If `w1 > w2`, it prioritizes tight fits.\n    # If `w2` is also significant, it penalizes large capacities.\n\n    # For robustness, the interpretation could be to avoid bins that are *too* empty.\n    # This means we might slightly prefer larger slack if it comes from a moderate capacity.\n    # Or, if multiple tight fits exist, prefer the one with larger capacity.\n\n    # Let's use the additive approach but ensure the `w` is chosen well.\n    # The `1/(1+slack)` term is the primary driver for tight fits.\n    # The `w * fittable_capacities` term provides robustness by favoring larger bins among good fits.\n    # The \"penalty for excessive capacity\" is implicitly handled by the fact that if slack is large, `1/(1+slack)` is small.\n    # If capacity is also large, the additive term `w * capacity` might still make it competitive, which is not ideal for \"penalizing excessive capacity\".\n\n    # Let's go back to the multiplicative approach from v1 and adjust it for robustness.\n    # `score = (1.0 / (1.0 + slack)) * (1.0 / (1.0 + fittable_capacities))`\n    # This prioritizes (small slack AND small capacity).\n\n    # If robustness means preferring larger capacity among tight fits:\n    # We need the score to be higher for larger capacity if slack is similar and small.\n    # The current multiplicative score does the opposite.\n\n    # Let's try modifying the capacity term.\n    # `score = (1.0 / (1.0 + slack)) * (1.0 + k_capacity * fittable_capacities)`\n    # This would mean larger capacities are preferred.\n    # Example: item=50, bins=[55, 60]\n    # Cap=[55, 60], Slack=[5, 10]\n    # TF=[0.166, 0.0909]\n    # Let `k_capacity = 0.01`\n    # Score bin 55: `0.166 * (1 + 0.01*55) = 0.166 * (1.55) = 0.2573`\n    # Score bin 60: `0.0909 * (1 + 0.01*60) = 0.0909 * (1.60) = 0.14544`\n    # This now prioritizes bin 55.\n\n    # This seems to be the most promising approach for \"prioritize tight fits, penalize excessive capacity, and balance with robustness (by favoring larger bins among good fits)\".\n    # The `1/(1+slack)` term ensures tight fits get higher priority.\n    # The `(1 + k_capacity * fittable_capacities)` term boosts priority for larger bins, acting as a robustness factor and somewhat countering the \"penalty for excessive capacity\" if the fit is very tight.\n    # This formulation implicitly penalizes excessive capacity because a larger capacity with the *same* slack will have a lower `1/(1+slack)` term, and the additive `(1 + k_cap * cap)` term might not fully compensate if the slack difference is significant.\n\n    # Let's refine the capacity term. We want to penalize *excessive* capacity.\n    # So the term should increase with capacity but not too quickly.\n    # `log(1 + capacity)` or `capacity / scale` or `1 - exp(-capacity/scale)`\n\n    # A common heuristic is to consider slack relative to capacity: `slack / capacity`.\n    # Minimize this ratio. `1 / (1 + slack/capacity)`\n\n    # Let's stick to the additive combination for simplicity and clear interpretation of contributions.\n    # Score = `BaseScore_TightFit + Bonus_Robustness`\n    # BaseScore_TightFit: prioritize small slack. `1.0 / (1.0 + slack)`\n    # Bonus_Robustness: If slack is small, larger capacity is good. `w * fittable_capacities`\n\n    # Let's consider the 'penalty for excessive capacity' part.\n    # This means if capacity is large, the score should be low, *unless* the slack is very small.\n    # The additive score `(1/(1+slack)) + w*capacity` might not penalize large capacities enough.\n\n    # Let's try a normalized approach:\n    # Normalize slack: `norm_slack = slack / max(slack_values)`\n    # Normalize capacity: `norm_capacity = fittable_capacities / max(capacity_values)`\n    # Score = `w1 * (1 - norm_slack) + w2 * (1 - norm_capacity)`\n    # This prioritizes small slack and small capacity. Not ideal for robustness.\n\n    # Final strategy:\n    # 1. Primary goal: Minimize slack. Use `1.0 / (1.0 + slack)` as a base score.\n    # 2. Secondary goal for robustness: If slack is similar and small, prefer bins that are not too full (larger capacity).\n    # This means if slack is small, we want to *boost* the score for larger capacities.\n    # So, an additive boost seems appropriate: `score = (1.0 / (1.0 + slack)) + w * fittable_capacities`.\n    # The \"penalty for excessive capacity\" is handled by the primary goal: large slack means small `1/(1+slack)`,\n    # and if capacity is also large, the additive term might not compensate enough if the slack is truly excessive.\n\n    # Let's use `w = 0.005` as a starting point. This is a small constant.\n    # The score will primarily be driven by tight fits.\n    # If two bins have very similar small slacks, the one with larger capacity will get a slightly higher score.\n\n    # Let's ensure the score is non-negative. `1/(1+slack)` is always positive. `w*capacity` is positive.\n    # The priority scores are then positive. Normalization might be needed at the end, but the problem asks for raw scores.\n\n    # Parameters:\n    # `slack_scale`: Controls how quickly priority drops with slack.\n    # `capacity_boost_weight`: Controls how much larger capacities are favored among good fits.\n\n    slack_scale = 1.0  # Scale for slack (effectively influences sensitivity to slack)\n    capacity_boost_weight = 0.005  # Weight for boosting based on capacity\n\n    # Calculate priority based on tight fit (inverse of slack)\n    # Using `np.exp(-slack / slack_scale)` makes it more S-shaped,\n    # similar to sigmoid but directly from 1 to near 0.\n    # `1.0 / (1.0 + slack)` is also good and simpler. Let's use that.\n    tight_fit_priority = 1.0 / (1.0 + slack)\n\n    # Calculate robustness boost based on capacity\n    # This term favors larger capacities among bins with similar (small) slack.\n    robustness_boost = capacity_boost_weight * fittable_capacities\n\n    # Combine the scores. The boost is added to the tight fit priority.\n    # This means tight fits are primary, and capacity is a secondary factor for robustness.\n    # The \"penalty for excessive capacity\" is implicitly handled: if slack is large,\n    # `tight_fit_priority` is small, and the boost might not be enough to compensate for a very large capacity if it also implies large slack.\n    final_scores = tight_fit_priority + robustness_boost\n\n    priorities[fit_mask] = final_scores\n\n    return priorities",
    "response_id": 9,
    "obj": 83.26685281212605,
    "cyclomatic_complexity": 2.0,
    "halstead": 151.26748332105768,
    "exec_success": true
  }
]