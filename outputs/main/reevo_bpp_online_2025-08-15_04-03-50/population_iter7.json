[
  {
    "stdout_filepath": "problem_iter6_response0.txt_stdout.txt",
    "code_path": "problem_iter6_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Best Fit strategy with slack penalty.\n\n    This strategy prioritizes bins that leave minimal remaining capacity after packing\n    the current item (Best Fit). Additionally, it penalizes bins that have excessive\n    slack (remaining capacity significantly larger than the item size), aiming to\n    reserve large bins for potentially larger future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities to 0\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Get capacities of bins that can fit the item\n    fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n\n    # Calculate the slack for each fittable bin\n    slack = fitting_bins_capacities - item\n\n    # --- Score Calculation ---\n    # 1. Best Fit Component: Prioritize bins with minimum slack.\n    #    We use 1 / (1 + slack) so that smaller slack yields a higher score.\n    #    Adding 1 avoids division by zero and ensures positive scores.\n    best_fit_score = 1.0 / (1.0 + slack)\n\n    # 2. Slack Penalty Component: Penalize bins with excessive slack.\n    #    We define \"excessive\" as slack larger than a certain multiple of the item size.\n    #    Let's use a threshold like 2 * item.\n    #    A sigmoid function that decreases as slack increases can model this penalty.\n    #    `1 / (1 + exp(k * (slack - threshold)))` will be close to 1 for slack <= threshold\n    #    and decrease for slack > threshold.\n    slack_threshold = 2.0 * item  # Threshold for excessive slack\n    k_penalty = 0.5  # Steepness of the penalty function\n    slack_penalty = 1.0 / (1.0 + np.exp(k_penalty * (slack - slack_threshold)))\n\n    # Combine scores:\n    # We want to favor small slack (high best_fit_score) and penalize large slack\n    # (low slack_penalty). A simple multiplication is often effective here.\n    # The best_fit_score is high for small slack. The slack_penalty is high for small slack too.\n    # Multiplying them means both conditions (tight fit and not excessively large) contribute positively.\n    # A tighter fit (small slack) will naturally result in a higher score in both components.\n    # The penalty is more about preventing the use of overly large bins.\n    combined_scores = best_fit_score * slack_penalty\n\n    # Assign the calculated scores to the priorities for the fittable bins\n    priorities[can_fit_mask] = combined_scores\n\n    # Normalize priorities for fittable bins so they sum to 1 (optional, for probabilistic selection)\n    fittable_scores_sum = np.sum(priorities[can_fit_mask])\n    if fittable_scores_sum > 0:\n        priorities[can_fit_mask] /= fittable_scores_sum\n\n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 176.46653521143952,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response1.txt_stdout.txt",
    "code_path": "problem_iter6_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a strategy that prioritizes bins with minimal remaining space after packing (tightest fit), and among those, favors bins that are fuller (less remaining capacity).\n\n    This strategy is a refinement of Best Fit, aiming to pack items tightly while\n    also considering the overall fullness of the bins for tie-breaking.\n\n    1. Primary objective: Minimize the \"slack\" (remaining capacity - item size).\n       Bins with slack closer to zero are preferred. This is achieved by maximizing\n       a term inversely proportional to slack, or directly using negative slack\n       where smaller negative slack (closer to zero) is better.\n    2. Secondary objective: Among bins with the same slack, prefer bins that are fuller.\n       A fuller bin has less remaining capacity. This is achieved by preferring\n       bins with smaller `bins_remain_cap`.\n\n    The combined priority aims to achieve a lexicographical ordering: minimize slack first,\n    then minimize remaining capacity. This can be modeled by maximizing a score like\n    `-(slack + alpha * bins_remain_cap)`, where `alpha` is a scaling factor. For simplicity\n    and to directly reflect the two objectives, we can use `-(slack + bins_remain_cap)`.\n    The highest priority (least negative value) will correspond to the bin with the smallest\n    sum of slack and remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score (less negative) indicates a higher priority. Bins that cannot fit\n        the item will have a priority of negative infinity (or a very large negative number)\n        to ensure they are never chosen if fittable bins exist.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        # No bin can fit the item, return all -inf priorities\n        return priorities\n\n    # Calculate slack for fittable bins\n    # Slack = remaining_capacity - item_size\n    fittable_bins_capacities = bins_remain_cap[can_fit_mask]\n    slack = fittable_bins_capacities - item\n\n    # Combine objectives: Minimize slack, then minimize remaining capacity.\n    # We want to maximize a score that reflects this.\n    # Using `-(slack + bins_remain_cap)` achieves this.\n    # A smaller (slack + bins_remain_cap) value results in a larger (less negative) priority.\n    # Example:\n    # Bin A: slack=0, rem_cap=5  => sum=5, priority = -5\n    # Bin B: slack=0, rem_cap=10 => sum=10, priority = -10\n    # Bin C: slack=1, rem_cap=5  => sum=6, priority = -6\n    # Bin D: slack=1, rem_cap=10 => sum=11, priority = -11\n    # This prioritizes A > C > B > D, which is the desired order.\n    combined_score = -(slack + fittable_bins_capacities)\n\n    priorities[can_fit_mask] = combined_score\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 53.77443751081735,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response2.txt_stdout.txt",
    "code_path": "problem_iter6_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a smooth decay favoring tight fits.\n\n    This heuristic aims to favor bins with tight fits (small remaining capacity after packing)\n    while allowing for exploration of slightly larger gaps. It uses a function that\n    rewards small slack values but decreases smoothly, giving reasonable scores\n    to bins with moderate slack. This prevents prematurely filling bins that\n    might be only slightly larger than needed, thus keeping flexibility for future items.\n\n    The priority is calculated using a power-law inverse relationship with the slack\n    (remaining capacity minus item size). Specifically, it uses `1 / (1 + slack^p)`,\n    where `p` is a parameter controlling the steepness of the decay. A `p` value\n    less than 1 (e.g., 0.7) ensures that larger slacks are penalized less severely\n    than a simple inverse `1 / (1 + slack)`, promoting exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if np.any(fittable_bins_mask):\n        fittable_bins_capacities = bins_remain_cap[fittable_bins_mask]\n\n        # Calculate slack: remaining capacity after fitting the item.\n        # We want slack to be as close to 0 as possible for tight fits.\n        slack = fittable_bins_capacities - item\n\n        # Parameter to control the decay rate of priority as slack increases.\n        # A value < 1.0 makes the function decay slower than 1/(1+slack),\n        # favoring slightly larger gaps more than a strict \"Best Fit\".\n        # p = 0.7 is chosen as a balance: favors tight fits (slack near 0) but\n        # gives good scores to moderately tight fits (slack up to ~1-2),\n        # promoting exploration of slightly larger gaps.\n        p_value = 0.7\n\n        # Calculate priority: 1 / (1 + slack^p)\n        # This function maps slack=0 to priority=1, and as slack increases,\n        # the priority decreases towards 0. The power `p` controls how quickly\n        # this decrease happens. A smaller `p` results in a slower decay,\n        # thus exploring larger gaps more effectively.\n        # We add a small epsilon to slack to avoid potential numerical issues\n        # if slack is extremely close to zero or zero itself, although `0**p` is 0 for p>0.\n        # A small additive term like `epsilon` (e.g., 1e-9) can ensure the denominator is never exactly 0\n        # and also slightly reduces the priority for the absolute tightest fit, promoting exploration.\n        epsilon = 1e-9\n        priorities[fittable_bins_mask] = 1.0 / (1.0 + (slack + epsilon)**p_value)\n\n        # Ensure priorities are within a valid range and handle potential NaNs (unlikely here)\n        priorities[fittable_bins_mask] = np.nan_to_num(priorities[fittable_bins_mask], nan=0.0, posinf=1.0, neginf=0.0)\n        \n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 70.32403072095333,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response3.txt_stdout.txt",
    "code_path": "problem_iter6_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined strategy.\n\n    This strategy prioritizes bins that offer the \"tightest fit\" for the item,\n    meaning the remaining capacity after insertion is minimized. This is achieved\n    by maximizing `1.0 / (1.0 + slack)`, where slack is `remaining_capacity - item_size`.\n    Additionally, it introduces a secondary preference for bins that are already\n    \"fuller\" (i.e., have less initial remaining capacity) among those offering similar\n    tight fits, to promote more balanced packing. This favors bins that are closer to\n    being full, making it easier to close them sooner.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros\n    if not np.any(fit_mask):\n        return priorities\n\n    # Calculate the slack for bins that can fit the item\n    # Slack = remaining_capacity - item_size\n    slack = bins_remain_cap[fit_mask] - item\n\n    # Primary scoring: Prioritize minimal slack (tightest fits).\n    # Use `1.0 / (1.0 + slack)`: higher score for smaller slack.\n    # A slack of 0 gives priority 1.0.\n    primary_score = 1.0 / (1.0 + slack)\n\n    # Secondary scoring: Among bins with similar slacks, prioritize those that are\n    # \"fuller\" initially. This means preferring bins with smaller original `bins_remain_cap`.\n    # A small negative term `bins_remain_cap` will be added to the priority.\n    # We add a small epsilon to `slack` before inverting to avoid division by zero\n    # if an item perfectly fits into a bin (slack=0). However, since we are\n    # using `1.0 + slack`, this is not strictly necessary as `1.0 + 0 = 1.0`.\n    # The secondary score is simply the negative of the bin's remaining capacity.\n    # This implicitly favors bins that are closer to being full (smaller remaining capacity).\n    secondary_score = -bins_remain_cap[fit_mask]\n\n    # Combine scores: Maximize primary score (tightest fit), then secondary score (fuller bin).\n    # A simple addition works here, as the primary score is generally more significant.\n    # Normalization is not strictly required by the prompt but can be useful for\n    # comparing different scoring functions or if probabilities are needed.\n    # For this function, we are returning raw scores, and the selection mechanism\n    # (e.g., argmax) will handle picking the highest score.\n    priorities[fit_mask] = primary_score + secondary_score\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 76.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response4.txt_stdout.txt",
    "code_path": "problem_iter6_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Almost Full Fit strategy.\n\n    This strategy prioritizes bins that provide the \"tightest fit\" for the item,\n    meaning the remaining capacity after insertion is minimized. This is achieved\n    by maximizing `1.0 / (1.0 + slack)`, where slack is `remaining_capacity - item_size`.\n    Additionally, it introduces a secondary preference for bins that are already\n    \"fuller\" (i.e., have less initial remaining capacity) among those offering similar\n    tight fits, to promote more balanced packing. The weight of the secondary\n    criterion is tuned to balance the two objectives.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities\n\n    # Calculate the slack for bins that can fit the item\n    # Slack = remaining_capacity - item_size\n    slack = bins_remain_cap[fit_mask] - item\n\n    # Primary scoring: Prioritize minimal slack (tightest fits).\n    # Use `1.0 / (1.0 + slack)`: higher score for smaller slack.\n    # A slack of 0 gives priority 1.0.\n    primary_score = 1.0 / (1.0 + slack)\n\n    # Secondary scoring: Among bins with similar slacks, prioritize those that are\n    # \"fuller\" initially. This means preferring bins with smaller original `bins_remain_cap`.\n    # We add a term proportional to the negative of the initial remaining capacity.\n    # The coefficient `alpha` is tuned to balance the primary and secondary criteria.\n    # A smaller alpha means the \"fuller bin\" preference has less impact.\n    alpha = 0.05  # Tuned parameter\n    secondary_score = -alpha * bins_remain_cap[fit_mask]\n\n    # Combine scores: maximize primary score, then secondary score\n    priorities[fit_mask] = primary_score + secondary_score\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 93.45440529575887,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response5.txt_stdout.txt",
    "code_path": "problem_iter6_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, prioritizing tight fits and penalizing large gaps.\n\n    This version prioritizes bins that have a remaining capacity that is just\n    slightly larger than the item size, aiming to minimize wasted space.\n    It also penalizes bins with very large remaining capacities to encourage\n    spreading and avoid creating excessively large empty spaces that might\n    not be efficiently utilized by future items. A small amount of random noise\n    is added to the scores to break ties and encourage exploration.\n\n    The priority is calculated as follows:\n    1. For bins that can fit the item, the base priority is related to the\n       'slack' (remaining capacity - item size). A smaller slack is better.\n    2. A penalty is applied for large slack. This penalty increases as the\n       slack grows beyond a certain threshold (e.g., twice the item size).\n    3. A small random value is added to introduce stochasticity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of -inf.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    eligible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(eligible_bins_mask):\n        return priorities\n\n    fittable_bins_capacities = bins_remain_cap[eligible_bins_mask]\n    slack = fittable_bins_capacities - item\n\n    # --- Prioritize tight fits ---\n    # We want smaller slack to have higher priority.\n    # A good score would be inversely related to slack, e.g., 1 / (1 + slack).\n    # Using sigmoid of negative slack: 1 / (1 + exp(-k * slack))\n    # This gives higher values for smaller slack.\n    k_fit = 5.0  # Steepness for tight fits\n    tight_fit_score = 1.0 / (1.0 + np.exp(-k_fit * slack))\n\n    # --- Penalize large gaps ---\n    # We want to penalize slack values that are significantly larger than ideal.\n    # Define an \"ideal maximum slack\" (e.g., twice the item size).\n    # Beyond this, we want the score to decrease.\n    ideal_max_slack = 2.0 * item\n    # Use a sigmoid that penalizes slack > ideal_max_slack.\n    # 1 / (1 + exp(k * (slack - ideal_max_slack)))\n    # This score is ~0.5 at ideal_max_slack and decreases for larger slack.\n    k_penalty = 1.0 # Steepness of the penalty\n    large_gap_penalty = 1.0 / (1.0 + np.exp(k_penalty * (slack - ideal_max_slack)))\n\n    # Combine scores. We want both tight fits and reasonable gaps.\n    # The tight_fit_score is high for small slack.\n    # The large_gap_penalty is high for small slack (and below ideal_max_slack).\n    # A simple linear combination seems appropriate.\n    # Let's weight the tight fit more, as minimizing waste is primary.\n    combined_score_values = 0.7 * tight_fit_score + 0.3 * large_gap_penalty\n\n    # Add a small random noise to break ties and encourage exploration.\n    random_noise = np.random.rand(len(slack)) * 1e-6\n    priorities[eligible_bins_mask] = combined_score_values + random_noise\n\n    return priorities",
    "response_id": 5,
    "obj": 24.780614280015957,
    "cyclomatic_complexity": 2.0,
    "halstead": 259.4606049037673,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response6.txt_stdout.txt",
    "code_path": "problem_iter6_code6.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Almost Full Fit with a preference for less empty space.\n\n    This strategy prioritizes bins that have just enough capacity to fit the item,\n    aiming to minimize the remaining capacity after placing the item (tight fit).\n    To promote robustness and prevent packing small items into extremely large,\n    mostly empty bins, it also penalizes bins with very large remaining capacities.\n    The priority score is a combination that rewards tighter fits and discourages\n    placing items into bins that will remain significantly empty.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit.\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities\n\n    fittable_capacities = bins_remain_cap[fit_mask]\n\n    # Calculate the slack (unused capacity) for bins that can fit the item.\n    # This is the primary factor for \"almost full fit\". Smaller slack is better.\n    slack = fittable_capacities - item\n\n    # Score based on minimizing slack: 1 / (1 + slack)\n    # A perfect fit (slack=0) gets a score of 1.0.\n    # As slack increases, the score decreases.\n    tight_fit_score = 1.0 / (1.0 + slack)\n\n    # Penalize excessively large bins. We want to avoid putting a small item\n    # into a bin that will still be mostly empty. This promotes more even\n    # distribution and leaves larger contiguous spaces in other bins.\n    # A simple penalty can be based on the inverse of the remaining capacity.\n    # Bins with larger remaining capacity should get a lower penalty factor.\n    # Using 1 / (1 + capacity) ensures we don't divide by zero and also\n    # that larger capacities yield smaller (less favorable) scores here.\n    # This factor is for the *original* remaining capacity of the fittable bins.\n    large_bin_penalty_factor = 1.0 / (1.0 + fittable_capacities)\n\n    # Combine the tight fit score with the penalty factor.\n    # We multiply them: prioritize tight fits AND penalize large bins.\n    # This means bins that are a tight fit AND have smaller remaining capacity\n    # will get the highest scores.\n    combined_priority = tight_fit_score * large_bin_penalty_factor\n\n    # Assign the calculated priorities to the bins that can fit the item.\n    priorities[fit_mask] = combined_priority\n\n    # Normalize priorities among fittable bins so they sum to 1.\n    # This ensures that the selection is probabilistic and relative.\n    fittable_priorities = priorities[fit_mask]\n    if np.sum(fittable_priorities) > 0:\n        priorities[fit_mask] /= np.sum(fittable_priorities)\n    else:\n        # If all fittable priorities are zero (e.g., due to extreme large_bin_penalty_factor),\n        # fall back to uniform probability among fittable bins.\n        # This ensures we always select a bin if one is available.\n        priorities[fit_mask] = 1.0 / len(fittable_priorities)\n\n    return priorities",
    "response_id": 6,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 142.7018117963935,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response7.txt_stdout.txt",
    "code_path": "problem_iter6_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, prioritizing fuller bins first, then best fit.\n\n    This heuristic prioritizes bins that are already fuller (have less remaining capacity).\n    If multiple bins have the same minimal remaining capacity, it then applies the Best Fit\n    principle to choose the one that results in the least waste. This aims to pack items\n    more densely by preferring bins that are closer to being full. The strategy aims for\n    deterministic behavior, prioritizing fuller bins, and then best fit among equally full bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher score indicates higher priority. Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9  # Small value to prevent division by zero\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    fittable_bins_capacities = bins_remain_cap[can_fit_mask]\n\n    # Primary objective: Prioritize fuller bins (smaller remaining capacity).\n    # Score component 1: Inverse of remaining capacity. Higher score for smaller capacity.\n    fullness_score_component = 1.0 / (fittable_bins_capacities + epsilon)\n\n    # Secondary objective: Among equally full bins, prioritize best fit (minimize waste).\n    # Waste = remaining_capacity - item_size.\n    # Score component 2: Inverse of waste. Higher score for smaller waste.\n    waste = fittable_bins_capacities - item\n    best_fit_score_component = 1.0 / (waste + epsilon)\n\n    # Combine scores: Summing the two components gives a higher priority to bins that are\n    # both fuller and offer a better fit. This combination prioritizes fullness first\n    # due to the nature of inverse functions, and then best fit as a tie-breaker or\n    # secondary factor.\n    # Example:\n    # Bin A: remain_cap=0.6, item=0.5 -> waste=0.1. Score = 1/0.6 + 1/0.1 = 1.667 + 10 = 11.667\n    # Bin B: remain_cap=0.7, item=0.5 -> waste=0.2. Score = 1/0.7 + 1/0.2 = 1.428 + 5 = 6.428\n    # Bin C: remain_cap=0.6, item=0.4 -> waste=0.2. Score = 1/0.6 + 1/0.2 = 1.667 + 5 = 6.667\n    # Bin A (fullest, best fit) gets highest score. Bin C (equally full as A, worse fit) gets second highest. Bin B (less full, worse fit) gets lowest.\n    # This combination correctly prioritizes fullness, then best fit.\n    combined_scores = fullness_score_component + best_fit_score_component\n\n    priorities[can_fit_mask] = combined_scores\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 92.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response8.txt_stdout.txt",
    "code_path": "problem_iter6_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined strategy.\n\n    This strategy favors tight fits by prioritizing bins with minimal remaining capacity\n    after packing the item. To encourage exploration and prevent premature selection\n    of bins that are only slightly better fits, the priority function exhibits a\n    gentler decay for small remaining capacities compared to a simple inverse relationship.\n    This means bins with a small positive remaining capacity (slack) are preferred,\n    but the preference does not drop off extremely sharply as slack increases, allowing\n    for exploration of slightly less tight fits.\n\n    The priority score is calculated using a function of the form `1 / (1 + slack^p)`,\n    where `slack = bins_remain_cap - item` and `p` is a parameter less than 1.\n    A smaller `p` leads to a gentler decay and thus more exploration of larger gaps.\n    A value of `p = 0.7` is used here to balance tight fitting with exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if np.any(fittable_bins_mask):\n        fittable_bins_capacities = bins_remain_cap[fittable_bins_mask]\n\n        # Calculate slack: remaining capacity after fitting the item.\n        # We want slack to be as close to 0 as possible.\n        slack = fittable_bins_capacities - item\n\n        # Use a power function `slack^p` with `p < 1` (e.g., 0.7) to achieve a gentler decay.\n        # This favors tight fits (small slack) but gives relatively higher scores\n        # to bins with moderately small slack compared to a `1/(1+slack)` approach.\n        # This encourages exploration of slightly larger gaps.\n        p_value = 0.7\n        \n        # Calculate priority: 1 / (1 + slack^p).\n        # For slack=0, priority is 1. For larger slack, it decreases.\n        # The power `p` controls the steepness of the decay.\n        # A value of `p=0.7` makes the decay less steep than `p=1`.\n        \n        # Add a small epsilon to slack before exponentiation to ensure numerical stability\n        # and prevent potential issues if slack is extremely close to zero, though\n        # `0**p` is well-defined for p > 0.\n        # For practical purposes, `np.power` handles this gracefully.\n        \n        # Ensure slack is non-negative, though `fittable_bins_mask` already guarantees this.\n        # If `item` can be 0, `slack` can be `bins_remain_cap`.\n        \n        # Handle the case where slack might be extremely small, leading to `slack**p_value` being\n        # very close to zero, making the priority close to 1. This is desired for tight fits.\n        \n        # Calculate `slack**p_value`. `np.power` handles `0**p` correctly (results in 0 for p>0).\n        # Using `np.power` is generally preferred over `**` for array operations for clarity and potential optimizations.\n        priorities[fittable_bins_mask] = 1.0 / (1.0 + np.power(slack, p_value))\n\n        # Clean up any potential NaN or Inf values, although unlikely with this formula and valid inputs.\n        priorities[fittable_bins_mask] = np.nan_to_num(priorities[fittable_bins_mask], nan=0.0, posinf=1.0, neginf=0.0)\n\n    return priorities",
    "response_id": 8,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 39.863137138648355,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response9.txt_stdout.txt",
    "code_path": "problem_iter6_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined strategy that prioritizes tight fits, penalizes excessive capacity, and balances with robustness.\n\n    This heuristic aims to:\n    1. Prioritize bins with minimal remaining capacity after fitting the item (tightest fit).\n    2. Penalize bins that have a very large remaining capacity, even if they can fit the item,\n       to avoid wasting large free spaces on small items if better alternatives exist.\n    3. Introduce robustness by slightly favoring bins that are not extremely empty if multiple\n       tight fits are available, or by not overly penalizing slightly larger slack if the bin\n       capacity itself is moderate.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Mask for bins that can accommodate the item.\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities\n\n    fittable_capacities = bins_remain_cap[fit_mask]\n    slack = fittable_capacities - item\n\n    # Score component 1: Prioritize tightest fits.\n    # A simple way is to use the inverse of slack + 1.\n    # Score is 1 for slack=0, decreases as slack increases.\n    tight_fit_score = 1.0 / (1.0 + slack)\n\n    # Score component 2: Penalize excessive capacity.\n    # We want to reduce priority for bins with very large remaining capacity.\n    # A common approach is to use a function that decays with capacity.\n    # Using `1 / (1 + capacity)` can achieve this. Larger capacity -> smaller score.\n    # This encourages using bins that are already more utilized.\n    capacity_consideration_score = 1.0 / (1.0 + fittable_capacities)\n\n    # Combine scores multiplicatively. This way, if either score is low, the combined score is low.\n    # We want a high score if the fit is tight AND the capacity is not excessive.\n    combined_score = tight_fit_score * capacity_consideration_score\n\n    # Add a small boost to ensure that even if capacity_consideration_score is small,\n    # a perfect tight fit can still have a high overall priority.\n    # E.g., if capacity=100, item=1, slack=99. tight_fit=1/100. capacity_consideration=1/101.\n    # Combined = 1/10100. This is very low.\n    # Maybe a weighted sum is better for balancing.\n\n    # Let's reconsider the reflection: \"prioritize tight fits, penalize excessive capacity, and balance with robustness.\"\n    # A tight fit means small slack. A penalty for excessive capacity means a low score for large remaining_capacity.\n    # Robustness might mean not being too sensitive to tiny variations, or favoring bins that are not *too* empty.\n\n    # Let's try a different combination:\n    # Prioritize small slack: `slack` itself, or `1/(1+slack)`.\n    # Penalize large capacity: `-fittable_capacities` or `1/(1+fittable_capacities)`.\n    # Let's use a weighted approach for clarity and tunability.\n    # Weight for tight fit: emphasize minimizing waste.\n    # Weight for capacity: deemphasize using huge bins for small items.\n\n    # Let's create a score for tightness and a score for \"emptiness\" (inverse capacity).\n    # Score_tightness: 1 if slack is 0, decreases for larger slack. `np.exp(-slack / scale_slack)` where scale_slack is small.\n    # Score_emptiness: 1 if capacity is very small, decreases for large capacity. `np.exp(-fittable_capacities / scale_capacity)` where scale_capacity is moderate.\n\n    # A simpler, common approach is to directly optimize for minimal slack, and break ties with maximal original capacity (Worst Fit heuristic) or minimal original capacity (Best Fit heuristic).\n    # Our reflection asks for tight fits (Best Fit-like) and penalty for excessive capacity (which means favoring smaller capacities), suggesting Best Fit with a cap on large bins.\n\n    # Let's try the following composite score:\n    # Priority = (1 - slack / BinCapacity) * (1 / (1 + slack))\n    # This is trying to normalize slack by capacity, which is tricky.\n\n    # Let's stick to the idea of minimizing slack primarily, and using capacity as a secondary factor.\n    # For robustness, perhaps we can slightly prefer slack that is small but not zero,\n    # if it comes from a moderate capacity bin, over a zero slack from a very large bin.\n\n    # Let's try a score that is high for small slack, and for bins that are not excessively large.\n    # Penalty for large slack: `np.exp(-slack / slack_scale)`\n    # Penalty for large capacity: `np.exp(-fittable_capacities / capacity_scale)`\n    # Combine them additively or multiplicatively.\n\n    # Alternative reflection interpretation:\n    # Tight fit: prefer bins where `fittable_capacities - item` is small.\n    # Penalize excessive capacity: if `fittable_capacities` is very large, reduce priority.\n    # Robustness: if there are multiple \"good\" options (e.g., similar small slack),\n    # maybe pick the one that leaves more room, or the one that is less full.\n\n    # Let's go with a score that is high for small slack and also for moderate original capacity.\n    # Score = (1 / (1 + slack)) * (1 / (1 + fittable_capacities))\n    # This is what `priority_v1` used. Let's refine the 'robustness' aspect.\n\n    # Robustness: If multiple bins offer the same minimal slack, which one to choose?\n    # The `(1 / (1 + fittable_capacities))` term already favors smaller capacities, which is a form of robustness.\n\n    # Let's introduce a slight penalty for *too tight* fits if the capacity is very large.\n    # For example, if slack is very small but capacity is huge.\n    # This means we want to avoid `slack` being very small *and* `fittable_capacities` being very large.\n    # This is implicitly handled by the multiplication: `(1/(1+slack)) * (1/(1+capacity))`.\n\n    # Consider the contribution:\n    # small slack: High `1/(1+slack)`\n    # small capacity: High `1/(1+capacity)`\n    # large slack: Low `1/(1+slack)`\n    # large capacity: Low `1/(1+capacity)`\n\n    # The product `tight_fit_score * capacity_consideration_score` naturally prioritizes bins that are\n    # both good fits and not excessively large.\n\n    # Let's add a component that slightly prefers bins that aren't nearly empty,\n    # for robustness (to avoid fragmenting space too much).\n    # This would mean favoring bins with higher `fittable_capacities` among those with similar slack.\n    # This contradicts the \"penalize excessive capacity\" slightly.\n\n    # Let's focus on the main reflection: \"prioritize tight fits, penalize excessive capacity, and balance with robustness.\"\n    # `tight_fit_score` addresses the first point.\n    # `capacity_consideration_score` addresses the second point.\n    # The multiplication `tight_fit_score * capacity_consideration_score` balances these.\n\n    # For robustness, we might want to ensure that the priority doesn't drop too sharply.\n    # A simple scaling might be helpful.\n    # Let's normalize the scores before combining.\n    # `norm_tight_fit = (tight_fit_score - min(tight_fit_score)) / (max(tight_fit_score) - min(tight_fit_score))`\n\n    # Let's try a simplified approach that is robust and still effective.\n    # Focus on minimizing slack. If there are ties in slack, pick the one with less total capacity.\n    # This is essentially a Best Fit strategy with tie-breaking towards smaller bins.\n\n    # The existing `tight_fit_score` (1 / (1 + slack)) is excellent for prioritizing minimal slack.\n    # The `capacity_consideration_score` (1 / (1 + fittable_capacities)) penalizes large bins.\n    # The product is a good compromise.\n\n    # Let's consider a case:\n    # item = 1\n    # bins = [10, 11, 2, 3]\n    # fittable_capacities = [10, 11, 2, 3]\n    # slack = [9, 10, 1, 2]\n    # tight_fit_score = [1/10, 1/11, 1/2, 1/3] = [0.1, 0.0909, 0.5, 0.333]\n    # capacity_consideration_score = [1/11, 1/12, 1/3, 1/4] = [0.0909, 0.0833, 0.333, 0.25]\n    # Combined = [0.00909, 0.00757, 0.1665, 0.08325]\n    # This correctly prioritizes the bin with capacity 2 (slack 1), then capacity 3 (slack 2).\n    # The bins with capacity 10 and 11 (large slack) are penalized.\n\n    # What if capacities are [3, 4] and item is 3?\n    # fittable_capacities = [3, 4]\n    # slack = [0, 1]\n    # tight_fit_score = [1/1, 1/2] = [1, 0.5]\n    # capacity_consideration_score = [1/3, 1/4] = [0.333, 0.25]\n    # Combined = [0.333, 0.125]\n    # This prioritizes the bin with capacity 3 (perfect fit, smaller capacity) over bin with capacity 4 (tight fit, larger capacity).\n\n    # What if capacities are [100, 101] and item is 1?\n    # fittable_capacities = [100, 101]\n    # slack = [99, 100]\n    # tight_fit_score = [1/100, 1/101] = [0.01, 0.0099]\n    # capacity_consideration_score = [1/101, 1/102] = [0.0099, 0.0098]\n    # Combined = [0.000099, 0.000097]\n    # This prioritizes the bin with capacity 100, which is the \"tighter\" fit and also smaller capacity.\n\n    # Let's try to introduce a slight preference for slightly larger slack if the capacity is not excessive.\n    # This is a form of \"robustness\" by not making a bin *too* full.\n    # Example: item=2, bins=[4, 5].\n    # Cap=[4, 5], Slack=[2, 3]\n    # TFs=[1/3, 1/4] = [0.333, 0.25]\n    # CS=[1/5, 1/6] = [0.2, 0.166]\n    # Comb=[0.0666, 0.0416] -> Prefers bin 4.\n\n    # If we wanted to prefer bin 5 in this case (slightly more room), we'd need a different heuristic.\n    # Maybe add a small bonus to slack?\n    # `score = (1/(1+slack) + bonus_slack) * (1/(1+capacity))`\n    # `bonus_slack = slack * slack_bonus_factor`\n    # Let's try `slack_bonus_factor = 0.01`\n    # For bin 4: `(0.333 + 0.02) * 0.2 = 0.353 * 0.2 = 0.0706`\n    # For bin 5: `(0.25 + 0.03) * 0.166 = 0.28 * 0.166 = 0.0465`\n    # This now favors bin 4 even more.\n\n    # What if we use `slack` directly in the second term?\n    # `score = (1/(1+slack)) * (1/(1 + slack + capacity))` This doesn't seem right.\n\n    # Let's modify the capacity term to be less punishing for moderate capacities.\n    # Maybe the `capacity_consideration_score` should not decay as rapidly.\n    # Or, we can add a slight bonus for the \"tighter\" fit among those that are already \"somewhat full\".\n\n    # The prompt requests: \"prioritize tight fits, penalize excessive capacity, and balance with robustness.\"\n    # The current combined score `tight_fit_score * capacity_consideration_score` does prioritize tight fits\n    # and penalizes excessive capacity. For robustness, it's somewhat balanced.\n\n    # Let's consider the \"robustness\" as not creating extremely full bins.\n    # This would mean slightly penalizing zero slack if the capacity is very small.\n    # E.g., item=2, bins=[2, 5].\n    # Cap=[2, 5], Slack=[0, 3]\n    # TFs=[1, 1/4]=[1, 0.25]\n    # CS=[1/3, 1/6]=[0.333, 0.166]\n    # Comb=[0.333, 0.0416] -> Prefers bin 2.\n\n    # If we want to prefer bin 5 for robustness, we'd need to slightly boost slack.\n    # Let's try `score = (1/(1+slack + epsilon)) * (1/(1+capacity))` with epsilon being small.\n    # Or, `score = (1/(1+slack)) * (1/(1+capacity) + epsilon_cap)`\n    # Or, `score = (1/(1+slack)) * exp(-capacity/scale)`\n\n    # Let's try a sigmoid for the slack that emphasizes small slack, and a separate term for capacity.\n    # Score = `sigmoid(k_tight * (slack_max - slack))` where slack_max is some upper bound for good fits.\n    # And `penalty_large_cap = sigmoid(k_large * (capacity_max - capacity))`\n\n    # A common practical heuristic for online BPP is First Fit Decreasing (FFD) or Best Fit Decreasing (BFD)\n    # for offline, but for online, First Fit (FF) or Best Fit (BF) are common.\n    # Best Fit aims for minimal slack.\n    # Our `1 / (1 + slack)` is a good approximation of BF's objective.\n    # The reflection adds \"penalize excessive capacity\".\n\n    # Let's consider a different formulation:\n    # Prioritize bins that leave minimum remaining capacity: `fittable_capacities - item`.\n    # Maximize `-(fittable_capacities - item)` = `item - fittable_capacities`.\n    # For robustness, perhaps add a term related to the original capacity, but not too heavily.\n    # Maximize `(item - fittable_capacities) + w * fittable_capacities` where w is small and positive.\n    # `(item - fittable_capacities) + w * fittable_capacities = item + (w-1) * fittable_capacities`.\n    # This means we want to minimize `fittable_capacities`.\n\n    # Let's go back to the original multiplicative idea but ensure it reflects the priorities clearly.\n    # Priority = `(fittable_capacities - item)` is what we want to minimize.\n    # So higher priority for smaller `slack`.\n    # `priority = 1 / (1 + slack)` is good.\n\n    # Penalize excessive capacity: if `fittable_capacities` is large, reduce priority.\n    # Combine: `priority = (1 / (1 + slack)) * (1 / (1 + fittable_capacities))`\n\n    # Robustness: If we have two bins with slack=0 and capacities=2 and 100.\n    # TF_scores = [1, 1]. CS_scores = [1/3, 1/101]. Comb = [1/3, 1/101]. Prefers bin 2.\n    # This seems reasonable.\n\n    # Let's consider \"robustness\" as: among bins that are good fits, pick one that is not too empty.\n    # This means if slack is similar, prefer larger capacity.\n    # If current score is `S = (1/(1+slack)) * (1/(1+capacity))`.\n    # For two bins with same slack, `s`:\n    # Bin A: Cap `c1`. Score `S_A = (1/(1+s)) * (1/(1+c1))`.\n    # Bin B: Cap `c2 > c1`. Score `S_B = (1/(1+s)) * (1/(1+c2))`.\n    # `S_A > S_B`. So it currently prefers smaller capacity.\n\n    # To prefer larger capacity for robustness (among similar slack fits), we need to modify the `capacity_consideration_score`.\n    # Instead of `1/(1+capacity)`, maybe use something that increases with capacity up to a point.\n    # Or, we can add a term based on capacity to the existing score.\n\n    # Let's reconsider the reflection: \"prioritize tight fits, penalize excessive capacity, and balance with robustness.\"\n    # Tight fit: minimize `slack`.\n    # Penalize excessive capacity: if `capacity` is large, reduce priority.\n    # Balance with robustness: This could mean:\n    # 1. If many tight fits exist, choose the one that is not too full. (Favors larger capacity among tight fits)\n    # 2. If capacities are similar, slightly prefer larger slack to leave more room. (Favors larger slack)\n\n    # Let's try to incorporate preference for larger capacity among good fits.\n    # We can add a term to the score that is proportional to capacity.\n    # `score = (1 / (1 + slack)) * (1 / (1 + fittable_capacities)) + weight * fittable_capacities`\n    # Let's use a specific transformation for slack and capacity.\n    # Slack score: `exp(-slack / scale_slack)`\n    # Capacity score: `exp(capacity / scale_capacity)` - this is not good as it encourages very large bins.\n    # Capacity score: `log(1 + capacity)` - this increases with capacity, but slowly.\n\n    # Let's combine the inverse slack and the inverse capacity, and see if we can tune it for robustness.\n    # `score = (1.0 / (1.0 + slack)) * (1.0 / (1.0 + fittable_capacities))`\n    # This prioritizes tight fits and penalizes large bins. This is already a good balance.\n    # For further robustness, consider how to break ties between similar scores.\n\n    # Let's modify the `capacity_consideration_score` slightly. Instead of penalizing large capacities,\n    # let's just use their magnitude as a tie-breaker.\n    # The primary driver is still tight fit.\n    # If `slack` is small, `tight_fit_score` is high.\n    # If we have similar `tight_fit_score` (meaning similar small slack),\n    # we want to select the one with larger `fittable_capacities` for robustness (leaves more space).\n    # So, we want to *add* a term proportional to `fittable_capacities`.\n    # `score = (1.0 / (1.0 + slack)) + w * fittable_capacities`  (This might over-emphasize capacity)\n\n    # Let's try to amplify the effect of small slack and moderate capacities.\n    # Use a scaled inverse for slack: `1 / (1 + k_slack * slack)`\n    # Use a scaled inverse for capacity: `1 / (1 + k_capacity * fittable_capacities)`\n    # And combine them multiplicatively.\n\n    # Let's re-evaluate the prompt and reflection.\n    # \"Prioritize tight fits\": high score for small slack.\n    # \"Penalize excessive capacity\": low score for large capacity.\n    # \"Balance with robustness\": This is the tricky part. It could mean:\n    #   a) Among tight fits, prefer bins that aren't too full (favors larger capacity).\n    #   b) Avoid filling a bin too much if a slightly larger gap exists in a moderate bin.\n\n    # Consider a score: `1 / (1 + slack) + factor * (fittable_capacities / BinCapacity_Max)`\n    # This might be too complex.\n\n    # Let's try a simple modification of the `v1` score.\n    # `v1_score = (1.0 / (1.0 + slack)) * (1.0 / (1.0 + fittable_capacities))`\n    # This prioritizes small slack and small capacity.\n    # To favor robustness (larger capacity among tight fits), we want to increase the score for larger capacities.\n    # Let's consider `score = (1.0 / (1.0 + slack)) + w * fittable_capacities`\n    # Let `w` be small, e.g., `1e-3`.\n\n    # Example: item=2, bins=[3, 100]\n    # Cap=[3, 100], Slack=[1, 98]\n    # TF=[0.5, 0.0102]\n    # If `w=0.001`:\n    # Score for bin 3: `0.5 + 0.001 * 3 = 0.503`\n    # Score for bin 100: `0.0102 + 0.001 * 100 = 0.0102 + 0.1 = 0.1102`\n    # This still prefers bin 3, which is expected because the slack is much smaller.\n\n    # Example: item=50, bins=[55, 60]\n    # Cap=[55, 60], Slack=[5, 10]\n    # TF=[1/6, 1/11] = [0.166, 0.0909]\n    # If `w=0.001`:\n    # Score for bin 55: `0.166 + 0.001 * 55 = 0.166 + 0.055 = 0.221`\n    # Score for bin 60: `0.0909 + 0.001 * 60 = 0.0909 + 0.060 = 0.1509`\n    # This correctly prioritizes the bin with less slack (55).\n\n    # The additive approach seems to give more weight to the tight fit, and capacity is a tie-breaker.\n    # This aligns with prioritizing tight fits, penalizing excessive capacity (by giving lower base scores to larger capacities),\n    # and then balancing with robustness (favoring larger capacities among similarly fitting bins).\n\n    # Let's re-implement this additive approach.\n    # The \"penalty for excessive capacity\" is still handled by the base `1/(1+slack)`.\n    # The additive term `w * fittable_capacities` is for robustness (favoring less full bins among good fits).\n\n    # Consider the range of values. Slack can be large. Capacity can be large.\n    # `1/(1+slack)` can be small. `w * capacity` can be large if `w` is not chosen carefully.\n    # Example: item=1, bins=[1000, 1001]\n    # Cap=[1000, 1001], Slack=[999, 1000]\n    # TF=[1/1000, 1/1001] = [0.001, 0.000999]\n    # If `w=0.001`:\n    # Score bin 1000: `0.001 + 0.001 * 1000 = 0.001 + 1 = 1.001`\n    # Score bin 1001: `0.000999 + 0.001 * 1001 = 0.000999 + 1.001 = 1.001999`\n    # This favors bin 1001, which has larger capacity. This is consistent with robustness.\n\n    # Let's try to integrate the \"penalize excessive capacity\" more directly into the score,\n    # not just as a tie-breaker.\n    # Reflection: \"prioritize tight fits, penalize excessive capacity, and balance with robustness.\"\n\n    # Let's try a weighted sum of a measure of tightness and a measure of capacity.\n    # Tightness measure: `1 - slack / max_possible_slack` or `exp(-slack / scale_s)`\n    # Capacity measure: `1 / (1 + capacity)` or `exp(-capacity / scale_c)`\n\n    # Consider score = `w1 * (1 / (1 + slack)) + w2 * (1 / (1 + fittable_capacities))`\n    # This is a linear combination. If `w1 > w2`, it prioritizes tight fits.\n    # If `w2` is also significant, it penalizes large capacities.\n\n    # For robustness, the interpretation could be to avoid bins that are *too* empty.\n    # This means we might slightly prefer larger slack if it comes from a moderate capacity.\n    # Or, if multiple tight fits exist, prefer the one with larger capacity.\n\n    # Let's use the additive approach but ensure the `w` is chosen well.\n    # The `1/(1+slack)` term is the primary driver for tight fits.\n    # The `w * fittable_capacities` term provides robustness by favoring larger bins among good fits.\n    # The \"penalty for excessive capacity\" is implicitly handled by the fact that if slack is large, `1/(1+slack)` is small.\n    # If capacity is also large, the additive term `w * capacity` might still make it competitive, which is not ideal for \"penalizing excessive capacity\".\n\n    # Let's go back to the multiplicative approach from v1 and adjust it for robustness.\n    # `score = (1.0 / (1.0 + slack)) * (1.0 / (1.0 + fittable_capacities))`\n    # This prioritizes (small slack AND small capacity).\n\n    # If robustness means preferring larger capacity among tight fits:\n    # We need the score to be higher for larger capacity if slack is similar and small.\n    # The current multiplicative score does the opposite.\n\n    # Let's try modifying the capacity term.\n    # `score = (1.0 / (1.0 + slack)) * (1.0 + k_capacity * fittable_capacities)`\n    # This would mean larger capacities are preferred.\n    # Example: item=50, bins=[55, 60]\n    # Cap=[55, 60], Slack=[5, 10]\n    # TF=[0.166, 0.0909]\n    # Let `k_capacity = 0.01`\n    # Score bin 55: `0.166 * (1 + 0.01*55) = 0.166 * (1.55) = 0.2573`\n    # Score bin 60: `0.0909 * (1 + 0.01*60) = 0.0909 * (1.60) = 0.14544`\n    # This now prioritizes bin 55.\n\n    # This seems to be the most promising approach for \"prioritize tight fits, penalize excessive capacity, and balance with robustness (by favoring larger bins among good fits)\".\n    # The `1/(1+slack)` term ensures tight fits get higher priority.\n    # The `(1 + k_capacity * fittable_capacities)` term boosts priority for larger bins, acting as a robustness factor and somewhat countering the \"penalty for excessive capacity\" if the fit is very tight.\n    # This formulation implicitly penalizes excessive capacity because a larger capacity with the *same* slack will have a lower `1/(1+slack)` term, and the additive `(1 + k_cap * cap)` term might not fully compensate if the slack difference is significant.\n\n    # Let's refine the capacity term. We want to penalize *excessive* capacity.\n    # So the term should increase with capacity but not too quickly.\n    # `log(1 + capacity)` or `capacity / scale` or `1 - exp(-capacity/scale)`\n\n    # A common heuristic is to consider slack relative to capacity: `slack / capacity`.\n    # Minimize this ratio. `1 / (1 + slack/capacity)`\n\n    # Let's stick to the additive combination for simplicity and clear interpretation of contributions.\n    # Score = `BaseScore_TightFit + Bonus_Robustness`\n    # BaseScore_TightFit: prioritize small slack. `1.0 / (1.0 + slack)`\n    # Bonus_Robustness: If slack is small, larger capacity is good. `w * fittable_capacities`\n\n    # Let's consider the 'penalty for excessive capacity' part.\n    # This means if capacity is large, the score should be low, *unless* the slack is very small.\n    # The additive score `(1/(1+slack)) + w*capacity` might not penalize large capacities enough.\n\n    # Let's try a normalized approach:\n    # Normalize slack: `norm_slack = slack / max(slack_values)`\n    # Normalize capacity: `norm_capacity = fittable_capacities / max(capacity_values)`\n    # Score = `w1 * (1 - norm_slack) + w2 * (1 - norm_capacity)`\n    # This prioritizes small slack and small capacity. Not ideal for robustness.\n\n    # Final strategy:\n    # 1. Primary goal: Minimize slack. Use `1.0 / (1.0 + slack)` as a base score.\n    # 2. Secondary goal for robustness: If slack is similar and small, prefer bins that are not too full (larger capacity).\n    # This means if slack is small, we want to *boost* the score for larger capacities.\n    # So, an additive boost seems appropriate: `score = (1.0 / (1.0 + slack)) + w * fittable_capacities`.\n    # The \"penalty for excessive capacity\" is handled by the primary goal: large slack means small `1/(1+slack)`,\n    # and if capacity is also large, the additive term might not compensate enough if the slack is truly excessive.\n\n    # Let's use `w = 0.005` as a starting point. This is a small constant.\n    # The score will primarily be driven by tight fits.\n    # If two bins have very similar small slacks, the one with larger capacity will get a slightly higher score.\n\n    # Let's ensure the score is non-negative. `1/(1+slack)` is always positive. `w*capacity` is positive.\n    # The priority scores are then positive. Normalization might be needed at the end, but the problem asks for raw scores.\n\n    # Parameters:\n    # `slack_scale`: Controls how quickly priority drops with slack.\n    # `capacity_boost_weight`: Controls how much larger capacities are favored among good fits.\n\n    slack_scale = 1.0  # Scale for slack (effectively influences sensitivity to slack)\n    capacity_boost_weight = 0.005  # Weight for boosting based on capacity\n\n    # Calculate priority based on tight fit (inverse of slack)\n    # Using `np.exp(-slack / slack_scale)` makes it more S-shaped,\n    # similar to sigmoid but directly from 1 to near 0.\n    # `1.0 / (1.0 + slack)` is also good and simpler. Let's use that.\n    tight_fit_priority = 1.0 / (1.0 + slack)\n\n    # Calculate robustness boost based on capacity\n    # This term favors larger capacities among bins with similar (small) slack.\n    robustness_boost = capacity_boost_weight * fittable_capacities\n\n    # Combine the scores. The boost is added to the tight fit priority.\n    # This means tight fits are primary, and capacity is a secondary factor for robustness.\n    # The \"penalty for excessive capacity\" is implicitly handled: if slack is large,\n    # `tight_fit_priority` is small, and the boost might not be enough to compensate for a very large capacity if it also implies large slack.\n    final_scores = tight_fit_priority + robustness_boost\n\n    priorities[fit_mask] = final_scores\n\n    return priorities",
    "response_id": 9,
    "obj": 83.26685281212605,
    "cyclomatic_complexity": 2.0,
    "halstead": 151.26748332105768,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response0.txt_stdout.txt",
    "code_path": "problem_iter7_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin,\n    prioritizing tight fits and fuller bins.\n\n    This strategy aims to achieve \"Almost Full Fit\" by prioritizing bins\n    that leave minimal remaining capacity after the item is packed (tight fit).\n    Additionally, it favors bins that were already fuller (higher initial\n    remaining capacity), as a secondary criterion. This combination helps\n    utilize space efficiently.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate the slack for bins that can fit the item\n    # Slack is the remaining capacity after placing the item\n    slack = bins_remain_cap[fit_mask] - item\n\n    # --- Scoring Strategy ---\n    # The primary goal is to prioritize tight fits, meaning bins with small slack.\n    # The secondary goal is to prioritize fuller bins (higher bins_remain_cap).\n    # We combine these by giving a base score based on slack, and amplifying it\n    # by the bin's initial remaining capacity.\n\n    # Base score for tight fit: higher for smaller slack.\n    # Using 1.0 / (1.0 + slack) ensures that:\n    # - Perfect fits (slack=0) get a score of 1.0.\n    # - Tighter fits (small positive slack) get scores closer to 1.0.\n    # - Looser fits (larger slack) get scores closer to 0.0.\n    base_tight_fit_score = 1.0 / (1.0 + slack)\n\n    # Amplification factor for fuller bins:\n    # We multiply the base score by the bin's initial remaining capacity.\n    # This means that among bins with similar slack, those that started with\n    # more capacity (i.e., were 'fuller' in a sense of having more space to begin with)\n    # will receive a higher overall priority.\n    # We use `bins_remain_cap[fit_mask]` directly as the multiplier.\n    # This gives higher priority to bins that are fuller, conditional on fitting tightly.\n\n    # Handle cases where bins_remain_cap might be very large, which could\n    # lead to extremely high priorities. A mild normalization or capping\n    # might be considered, but for simplicity, we'll use it directly.\n    # A small constant could be added to `bins_remain_cap` if we want to ensure\n    # that even less full bins get some base priority, but the `base_tight_fit_score`\n    # already provides this.\n\n    # Combining: priority = base_tight_fit_score * bins_remain_cap\n    # This prioritizes bins where:\n    # 1. The item fits (`fit_mask`).\n    # 2. The slack `(bins_remain_cap - item)` is minimized (via `base_tight_fit_score`).\n    # 3. Among those with similar slack, the initial `bins_remain_cap` is maximized.\n    # Penalizing large gaps: If slack is large, `base_tight_fit_score` becomes small,\n    # reducing the priority regardless of `bins_remain_cap`.\n\n    priorities[fit_mask] = base_tight_fit_score * bins_remain_cap[fit_mask]\n\n    # Ensure no NaNs or Infs in case of edge cases (though slack should be non-negative here)\n    priorities = np.nan_to_num(priorities, posinf=np.finfo(float).max, neginf=np.finfo(float).min)\n\n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 55.506595772116384,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response1.txt_stdout.txt",
    "code_path": "problem_iter7_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Almost Full Fit strategy.\n\n    This strategy prioritizes bins that have just enough capacity to fit the item,\n    with a preference for bins that will have less remaining capacity after the item is added.\n    It gently penalizes bins with very large remaining capacity. The goal is to fill bins\n    more tightly and efficiently, potentially reducing the total number of bins used.\n\n    The priority is calculated based on the \"slack\" (remaining capacity - item size).\n    Bins with smaller non-negative slack get higher priority. A term is added to\n    slightly favor bins that are not excessively large even if they fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate slack for bins that can fit the item\n    # Slack is the remaining capacity after placing the item.\n    # We want to minimize this slack for bins that fit.\n    slack = bins_remain_cap[fit_mask] - item\n\n    # --- Prioritization Strategy ---\n    # We want to prioritize bins with smaller slack.\n    # A common way to achieve this is using an inverse relationship.\n    # Using 1 / (1 + slack) gives higher scores for smaller slack.\n    # For example:\n    # slack = 0 (perfect fit): priority = 1 / (1 + 0) = 1.0\n    # slack = 1 (tight fit): priority = 1 / (1 + 1) = 0.5\n    # slack = 5 (moderate fit): priority = 1 / (1 + 5) = 0.167\n    # slack = 10 (loose fit): priority = 1 / (1 + 10) = 0.083\n\n    # To prevent very large capacity bins from getting high priority just because\n    # they fit, we can also consider a penalty based on the original remaining capacity\n    # or the inverse of the remaining capacity. However, a simple \"almost full fit\"\n    # focuses on the slack.\n\n    # Let's refine the slack-based priority. We want to favor smaller slack.\n    # The inverse function `1 / (1 + slack)` already does this.\n    # To \"gently penalize large gaps\", we could adjust the slope of this function,\n    # or perhaps add a term that reduces priority for bins that were initially very large.\n\n    # A simple refinement to prioritize \"almost full\" fits is to use a scoring\n    # where smaller slack gets higher priority.\n    # Let's use a term that is high when slack is small and decreases as slack increases.\n    # `1.0 / (1.0 + slack)` is a good start.\n    # To slightly penalize very large initial capacities that might not be \"tight fits\"\n    # even if they have a small slack, we can subtract a small penalty proportional\n    # to the original capacity (but only for bins that fit).\n\n    # Let's try a weighted sum or a more direct inverse:\n    # Priority ~ 1 / (slack + epsilon) for bins that fit.\n    # A slightly different approach: prioritize bins that will have minimal remaining capacity.\n    # This means maximizing `-(bins_remain_cap[fit_mask] - item)`.\n    # So, `item - bins_remain_cap[fit_mask]` is what we want to maximize (become more negative).\n    # This is equivalent to minimizing `bins_remain_cap[fit_mask] - item`.\n\n    # Let's stick to the `1 / (1 + slack)` as it directly favors small non-negative slack.\n    # To incorporate \"gently penalize large gaps\", we can consider the original capacity.\n    # A bin with `bins_remain_cap = 100` and `item = 3` (slack=97) shouldn't have as high priority\n    # as a bin with `bins_remain_cap = 4` and `item = 3` (slack=1), even if we use a formula\n    # that tries to invert slack.\n\n    # A different approach for \"almost full fit\":\n    # Prioritize bins where `bins_remain_cap - item` is minimized, subject to being non-negative.\n    # This means we want to find the minimum of `bins_remain_cap[fit_mask] - item`.\n    # The priority should be inversely related to this minimum.\n\n    # Consider the range of slack. If slack is very small (e.g., 0 to 5), we want high priority.\n    # If slack is large (e.g., 50+), we want low priority.\n    # The function `1.0 / (1.0 + slack)` achieves this.\n\n    # To refine \"gently penalize large gaps\", we could make the priority\n    # decrease more steeply for larger slacks.\n    # For instance, `exp(-slack / scale)` where scale is a tunable parameter.\n    # Or, directly use the negative of slack, but clamp it or scale it.\n\n    # Let's try to directly prioritize by the *negative* remaining capacity after placement,\n    # but ensure it's relative and favors tighter fits.\n    # The core idea of \"Almost Full Fit\" is to minimize `bins_remain_cap - item`.\n    # So, we want to maximize `-(bins_remain_cap - item)`.\n    # This is `item - bins_remain_cap`.\n\n    # Let's try prioritizing based on how close `bins_remain_cap` is to `item`.\n    # We want `bins_remain_cap` to be just slightly larger than `item`.\n    # So, `bins_remain_cap - item` should be small and non-negative.\n    # The priority score should be high for small `bins_remain_cap - item`.\n\n    # Using `1.0 / (1.0 + slack)` is a robust way. Let's try to add a small penalty\n    # for initially large bins.\n\n    # Penalize bins that are excessively large, even if they fit.\n    # We can achieve this by subtracting a small fraction of the original capacity.\n    # For example, `priority = (1.0 / (1.0 + slack)) - alpha * bins_remain_cap[fit_mask]`\n    # where alpha is a small positive constant (e.g., 0.01).\n    # This would reduce the priority of very large bins.\n\n    # Let's use `1.0 / (1.0 + slack)` for the primary \"almost full\" metric.\n    # To incorporate the \"gently penalize large gaps\" aspect:\n    # Perhaps the priority should be related to the *ratio* of item size to remaining capacity?\n    # No, that favors filling smaller bins.\n\n    # The core of \"Almost Full Fit\" is indeed minimizing `remaining_capacity - item`.\n    # So, maximizing `item - remaining_capacity` for valid bins.\n\n    # Let's use a score that emphasizes small positive slack.\n    # The `1.0 / (1.0 + slack)` works well for this.\n    # To \"gently penalize large gaps\", we can add a term that reduces priority for\n    # bins with very large *original* capacities.\n\n    # Option 1: Stick with `1 / (1 + slack)`. This already penalizes large gaps implicitly.\n    # Option 2: Introduce a penalty for large original capacity.\n    # Example: `priority = (1.0 / (1.0 + slack)) * (1.0 / (1.0 + C * bins_remain_cap[fit_mask]))`\n    # where C is a small constant. This would decrease priority for larger original capacities.\n\n    # Let's use a slightly modified version of the inverse slack, but perhaps make it\n    # more sensitive to smaller slacks.\n    # Consider `1.0 / (slack**p + epsilon)` for small p, or `exp(-slack/scale)`.\n\n    # A common heuristic for \"Best Fit\" is to find the bin with `min(bins_remain_cap - item)`.\n    # For \"Almost Full Fit\", it's similar but might include a slight preference for\n    # bins that aren't *too* full if that leaves no room for future items.\n\n    # Let's refine `1.0 / (1.0 + slack)`.\n    # If `bins_remain_cap` is very large, `slack` will be large, and `1/(1+slack)` will be small.\n    # This inherently penalizes large gaps.\n    # The \"gently penalize large gaps\" might mean that if two bins have similar small slacks,\n    # but one started much larger, the one that started smaller is preferred.\n\n    # A simple, effective way to get \"almost full\" is to maximize `item - (bins_remain_cap - item)`.\n    # This is `2*item - bins_remain_cap`. We want to maximize this, subject to `bins_remain_cap >= item`.\n    # So, bins with `bins_remain_cap` closer to `item` (but >= item) get higher priority.\n\n    # Let's use the negative of the slack for prioritization. Higher value means smaller slack.\n    # To ensure we penalize larger gaps gently, we can add a term that is less sensitive\n    # to the absolute size of the bin, or perhaps favors bins that were not excessively large.\n    # A common approach is to normalize or scale slack values.\n\n    # Consider `priority = -slack`. Maximizing this means minimizing slack.\n    # To avoid very large bins dominating, maybe scale slack by original capacity or vice versa.\n    # Or, use `(item - bins_remain_cap)`.\n    # For bins that fit:\n    # Priority = `item - bins_remain_cap[fit_mask]`\n    # This means a bin with `bins_remain_cap = 4, item = 3` gives `-1`.\n    # A bin with `bins_remain_cap = 5, item = 3` gives `-2`.\n    # We want to maximize this value, so `-1` is better than `-2`. This favors smaller slacks.\n\n    # Let's introduce a slight bonus for bins that have just enough capacity.\n    # A simple approach could be to make the priority inversely proportional to `1 + slack`.\n\n    # Let's refine the `1.0 / (1.0 + slack)` approach.\n    # This favors smaller slack values.\n    # To \"gently penalize large gaps\", we could modify the denominator.\n    # For example, `1.0 / (1.0 + slack * scaling_factor)` where `scaling_factor`\n    # could be adjusted to make the penalty steeper.\n    # Or, simply ensure that bins with *very* large slack get significantly lower priority.\n\n    # Let's use a strategy that aims to find bins where `bins_remain_cap` is just slightly larger than `item`.\n    # Priority for fitting bins: `1.0 / (1.0 + (bins_remain_cap[fit_mask] - item))`\n    # This gives priority 1.0 for perfect fits, and decreasing priority for larger slacks.\n\n    # To address \"gently penalize large gaps\", we can consider the ratio of slack to total bin capacity\n    # or simply the magnitude of the original capacity.\n    # A common \"Almost Full Fit\" heuristic is indeed to minimize slack `bins_remain_cap - item`.\n\n    # Let's try prioritizing based on `bins_remain_cap` relative to `item`.\n    # We want `bins_remain_cap` to be as close to `item` as possible, from above.\n    # Consider `bins_remain_cap / item`. We want this ratio to be close to 1.\n    # If `bins_remain_cap = 5, item = 3`, ratio = 1.67\n    # If `bins_remain_cap = 4, item = 3`, ratio = 1.33\n    # If `bins_remain_cap = 3, item = 3`, ratio = 1.0\n    # We want to prioritize smaller ratios.\n    # So, priority can be `1.0 / (bins_remain_cap[fit_mask] / item)`.\n\n    # Let's refine the `1 / (1 + slack)` idea.\n    # It directly targets minimizing slack.\n    # To incorporate \"gently penalize large gaps\": The inverse function already does this.\n    # Perhaps the \"gently\" part means not having an extreme penalty.\n    # The `1 / (1 + slack)` is a gentle exponential decay.\n\n    # Let's add a small component that favors bins that are not excessively large *initially*.\n    # This is tricky because \"almost full\" inherently means we don't want bins that are too empty.\n    # The strategy should prioritize tight fits.\n\n    # A commonly cited \"Almost Full Fit\" heuristic prioritizes bins with the minimum remaining capacity after packing.\n    # This means maximizing `-potential_remaining_cap`.\n    # So, `item - bins_remain_cap[fit_mask]` should be maximized.\n\n    # Let's use the negative of the slack as a base priority, and then\n    # introduce a term that slightly reduces priority for bins that were initially very large.\n\n    # Base priority: higher for smaller slack (closer to 0)\n    # `priority_base = -(bins_remain_cap[fit_mask] - item)` which is `item - bins_remain_cap[fit_mask]`\n    # This is maximized when `bins_remain_cap[fit_mask]` is minimized.\n\n    # To gently penalize large original capacities, we can subtract a small fraction of `bins_remain_cap[fit_mask]`.\n    # `penalty = alpha * bins_remain_cap[fit_mask]`\n    # Final priority = `priority_base - penalty`\n    # Final priority = `item - bins_remain_cap[fit_mask] - alpha * bins_remain_cap[fit_mask]`\n    # Final priority = `item - (1 + alpha) * bins_remain_cap[fit_mask]`\n    # Maximizing this means minimizing `(1 + alpha) * bins_remain_cap[fit_mask]`.\n    # This is still primarily minimizing `bins_remain_cap[fit_mask]`, but with a slight bias away from large bins.\n\n    # Let's try a simpler formulation that captures the spirit:\n    # Prioritize bins where `bins_remain_cap` is closest to `item`, but >= `item`.\n    # This means `bins_remain_cap - item` should be minimized.\n    # The `1.0 / (1.0 + slack)` strategy is good for this.\n\n    # A different interpretation of \"gently penalize large gaps\":\n    # Consider the percentage of capacity filled. We want to maximize `item / bins_remain_cap_original`.\n    # No, that's for filling smaller bins.\n\n    # Back to the most direct interpretation of \"Almost Full Fit\":\n    # Prioritize bins that, after packing the item, will have the least remaining capacity.\n    # This means minimizing `bins_remain_cap[fit_mask] - item`.\n    # So, we can set priority as `- (bins_remain_cap[fit_mask] - item)`.\n    # Let's add a small constant to avoid negative zero and ensure all valid bins have positive priority.\n\n    # A common heuristic for \"Almost Full Fit\" is to rank bins by `bins_remain_cap - item`.\n    # We select the bin with the minimum non-negative `bins_remain_cap - item`.\n    # So, the priority should be higher for smaller non-negative `bins_remain_cap - item`.\n\n    # Let's use a slight modification of `1.0 / (1.0 + slack)` to make it\n    # more sensitive to the difference between `bins_remain_cap` and `item`.\n    # Consider `priority = (bins_remain_cap[fit_mask] - item)`. We want to MINIMIZE this.\n    # So, priority should be inversely proportional to this.\n\n    # Let's consider the original `priority_v1` score: `1.0 / (1.0 + slack)`.\n    # This favors bins with small slack.\n    # \"Gently penalize large gaps\": This implies if a very large bin fits the item,\n    # its priority should be lower than a smaller bin that fits the item with the same slack.\n    # For example, if item=3:\n    # Bin A: cap=4, slack=1. priority = 1/(1+1) = 0.5\n    # Bin B: cap=103, slack=1. priority = 1/(1+1) = 0.5\n    # We want Bin A to have higher priority than Bin B.\n\n    # We can achieve this by subtracting a small fraction of the original capacity.\n    # The target is to minimize `bins_remain_cap - item`.\n    # Let's prioritize based on `-(bins_remain_cap - item)`, which is `item - bins_remain_cap`.\n    # To penalize large original capacities, we subtract a portion of it.\n    # Score = `item - bins_remain_cap[fit_mask] - alpha * bins_remain_cap[fit_mask]`\n    # Score = `item - (1 + alpha) * bins_remain_cap[fit_mask]`\n    # We want to maximize this score.\n\n    # Let's pick a small alpha, e.g., 0.1.\n    alpha = 0.1\n    priorities[fit_mask] = item - (1 + alpha) * bins_remain_cap[fit_mask]\n\n    # Ensure that bins that are \"too full\" (i.e., remaining capacity is very small,\n    # potentially negative if we didn't have fit_mask) are not highly prioritized.\n    # The `fit_mask` already handles non-fitting bins.\n    # The goal is to find the tightest fit.\n\n    # A more direct approach to penalizing large gaps:\n    # Prioritize bins where `bins_remain_cap` is between `item` and `item + tolerance`.\n    # And for those, pick the one with smallest `bins_remain_cap`.\n\n    # Let's use a simple formula that prioritizes small slack but de-prioritizes very large original capacities.\n    # The score `item - (1 + alpha) * bins_remain_cap[fit_mask]` achieves this.\n    # For example:\n    # item = 3\n    # Bin A: cap=4. slack=1. score = 3 - (1.1)*4 = 3 - 4.4 = -1.4\n    # Bin B: cap=5. slack=2. score = 3 - (1.1)*5 = 3 - 5.5 = -2.5\n    # Bin C: cap=103. slack=100. score = 3 - (1.1)*103 = 3 - 113.3 = -110.3\n    # Bin D: cap=104. slack=101. score = 3 - (1.1)*104 = 3 - 114.4 = -111.4\n\n    # This prioritizes Bin A (-1.4) over Bin B (-2.5), which is correct (smaller slack).\n    # It also drastically de-prioritizes Bin C (-110.3) and Bin D (-111.4), which is the \"gently penalize large gaps\" part.\n    # The \"gently\" aspect comes from `alpha`. A smaller alpha makes the penalty less severe.\n\n    # Let's try to make priorities positive for better interpretation and potential use in selection.\n    # We can shift the scores or use a transform.\n    # The relative ordering is what matters.\n    # If we want higher priority to mean \"more desirable\", we need to invert or shift.\n\n    # A common way to handle priorities where you want to minimize a value is to use `1 / (value + epsilon)`.\n    # Here, we want to minimize `(1 + alpha) * bins_remain_cap[fit_mask] - item`.\n    # Let `effective_remaining_capacity = (1 + alpha) * bins_remain_cap[fit_mask]`.\n    # We want to minimize `effective_remaining_capacity - item`.\n    # So, priority = `1.0 / (1.0 + (effective_remaining_capacity - item))`\n    # priority = `1.0 / (1.0 + (1 + alpha) * bins_remain_cap[fit_mask] - item)`\n\n    # Let's test this revised priority function:\n    # item = 3, alpha = 0.1\n    # Bin A: cap=4. effective_rem_cap = 1.1 * 4 = 4.4. value = 4.4 - 3 = 1.4. priority = 1 / (1 + 1.4) = 1 / 2.4 = 0.417\n    # Bin B: cap=5. slack=2. effective_rem_cap = 1.1 * 5 = 5.5. value = 5.5 - 3 = 2.5. priority = 1 / (1 + 2.5) = 1 / 3.5 = 0.286\n    # Bin C: cap=103. slack=100. effective_rem_cap = 1.1 * 103 = 113.3. value = 113.3 - 3 = 110.3. priority = 1 / (1 + 110.3) = 1 / 111.3 = 0.009\n    # Bin D: cap=104. slack=101. effective_rem_cap = 1.1 * 104 = 114.4. value = 114.4 - 3 = 111.4. priority = 1 / (1 + 111.4) = 1 / 112.4 = 0.0089\n\n    # This seems to correctly prioritize Bin A (tightest fit, moderate original capacity)\n    # over Bin B (looser fit, moderate original capacity), and both over very large bins.\n    # The \"gently penalize\" is achieved by the `alpha` factor which scales the original capacity's influence.\n\n    # The denominator `1.0 + (1 + alpha) * bins_remain_cap[fit_mask] - item` is essentially\n    # `1.0 + slack + alpha * bins_remain_cap[fit_mask]`.\n    # This penalizes larger original capacities by increasing the denominator, thus decreasing priority.\n\n    effective_slack = (1 + alpha) * bins_remain_cap[fit_mask] - item\n    priorities[fit_mask] = 1.0 / (1.0 + effective_slack)\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 125.09775004326937,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response2.txt_stdout.txt",
    "code_path": "problem_iter7_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Almost Full Fit strategy.\n\n    This strategy prioritizes bins that have just enough capacity to fit the item,\n    with a preference for bins that will have less remaining capacity (smaller slack)\n    after the item is added. It aims to fill bins more tightly.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit.\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate the slack for bins that can fit the item.\n    # Slack is the remaining capacity after placing the item.\n    slack = bins_remain_cap[fit_mask] - item\n\n    # Assign priority. We want to prioritize bins with smaller slack.\n    # A common way to do this is to use an inverse function of slack.\n    # Using 1.0 / (1.0 + slack) gives higher priority to smaller slack values.\n    # A slack of 0 (perfect fit) gets priority 1.0.\n    # A slack of 1 gets priority 0.5.\n    # A slack of 10 gets priority 1/11 (approx 0.09).\n    # This ensures that bins that become \"more full\" (less slack) are preferred.\n    # We add a small constant in the denominator to prevent division by zero\n    # and to ensure the priority is always positive and bounded.\n    priorities[fit_mask] = 1.0 / (1.0 + slack)\n\n    # Optional: A slight adjustment to further emphasize \"almost full\" by\n    # slightly boosting bins that are already quite full, even if the slack\n    # isn't the absolute smallest. This could be done by adding a term\n    # related to the original `bins_remain_cap` for fitting bins, but\n    # let's keep it simple and focused on slack for this version.\n    # For example, to slightly favor already fuller bins among those with similar small slacks:\n    # priorities[fit_mask] += (bins_remain_cap[fit_mask] / BIN_CAPACITY) * 0.1\n    # (where BIN_CAPACITY is known or estimated). But for now, let's stick to slack.\n\n    # To ensure that very large gaps are penalized gently, the inverse relationship\n    # with slack already achieves this: larger slacks result in exponentially smaller\n    # priorities.\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 39.863137138648355,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response3.txt_stdout.txt",
    "code_path": "problem_iter7_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Almost Full Fit strategy.\n\n    This strategy prioritizes bins that have a \"tight fit\" for the item,\n    meaning the remaining capacity after insertion is minimized. It also gently penalizes\n    bins that have a very large remaining capacity, preferring bins that are already\n    moderately full but can still accommodate the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity if the item is placed in each fitting bin\n    potential_remaining_cap = bins_remain_cap[fit_mask] - item\n\n    # Strategy:\n    # 1. Prioritize bins that leave the least remaining capacity (tightest fit).\n    #    This can be achieved by taking the negative of the remaining capacity,\n    #    or by using an inverse relationship like 1 / (1 + remaining_capacity).\n    #    We use 1 / (1 + slack) where slack is `bins_remain_cap - item`. This gives\n    #    higher scores to smaller non-negative slacks.\n    # 2. Penalize bins that are already very full, *if* they leave a large gap after insertion\n    #    (i.e., have a large slack). The 1/(1+slack) naturally does this.\n    # 3. Gently penalize bins with very large remaining capacity *even if* they are a tight fit for the current item.\n    #    This is implicitly handled by the 1/(1+slack) form; a bin with 100 remaining capacity\n    #    and fitting a 1-unit item will have slack 99, leading to a low priority.\n    # 4. We want to favor bins that are \"almost full\", which means small `potential_remaining_cap`.\n    #    So, a higher priority should be given to smaller non-negative `potential_remaining_cap`.\n\n    # A score based on the inverse of the \"slack\" (remaining_capacity - item).\n    # Adding 1 to the slack in the denominator prevents division by zero and ensures\n    # that a perfect fit (slack=0) gets the highest priority (1.0).\n    # Bins with larger slack values get exponentially smaller priorities.\n    priorities[fit_mask] = 1.0 / (1.0 + potential_remaining_cap)\n\n    # Refinement: To further emphasize \"almost full\", we can slightly boost\n    # bins that have a negative remaining capacity after potential insertion\n    # *if* that negative value is small (meaning the item almost fit, or slightly exceeded).\n    # However, for online BPP, we must select a bin where the item *fits*.\n    # So, `fit_mask` is essential.\n\n    # An alternative perspective for \"almost full\":\n    # We want to maximize the \"fullness\" after placing the item.\n    # Fullness can be seen as `(BinCapacity - RemainingCapacity) / BinCapacity`.\n    # Or, after placing item: `(BinCapacity - (BinCapacity - item)) / BinCapacity = item / BinCapacity`.\n    # This is First Fit Decreasing logic. For online, we don't know future items.\n\n    # Reverting to the prior thought: prioritize minimal slack.\n    # The current `1.0 / (1.0 + slack)` is a good heuristic for prioritizing tight fits.\n    # It ensures that bins that become \"most full\" (smallest positive or zero slack) are preferred.\n\n    # To gently penalize large gaps *while* prioritizing tight fits:\n    # Consider a score like `priority = exp(-slack / some_scale)`.\n    # Or a simpler linear approach: `priority = max_slack - slack`. This requires knowing max_slack.\n    # The current `1/(1+slack)` is a good balance.\n\n    # Let's slightly adjust the priority to favor bins that are NOT too empty.\n    # If `potential_remaining_cap` is large, the priority should be lower.\n    # A simple modification: `priority = (1.0 / (1.0 + slack)) * (1.0 / (1.0 + slack / average_bin_capacity))`\n    # This adds a penalty for large slacks relative to average bin capacity.\n    # For simplicity and robustness, let's stick to prioritizing minimal slack.\n    # The existing `1.0 / (1.0 + slack)` metric already does this effectively.\n    # It prioritizes bins where `bins_remain_cap - item` is smallest and non-negative.\n\n    # If we want to ensure that bins that become *very* full (small positive slack)\n    # are strongly preferred over bins that are moderately full (larger positive slack),\n    # the inverse relationship works well.\n\n    # To align with \"gently penalize large gaps\":\n    # Consider a score that is high for small slack and decreases as slack increases.\n    # The function `1.0 / (1.0 + slack)` does this.\n    # For instance:\n    # slack = 0 (perfect fit) -> priority = 1.0\n    # slack = 1 -> priority = 0.5\n    # slack = 10 -> priority = 0.09\n    # slack = 100 -> priority = 0.01\n\n    # If we want to distinguish more between tight fits and looser fits,\n    # we could use a steeper decay, e.g., `1.0 / (1.0 + slack^2)`.\n    # Or even `exp(-slack / avg_bin_capacity)`.\n\n    # Let's consider a score that is sensitive to how *much* remaining capacity there is\n    # relative to the item size.\n    # `score = 1.0 / (1.0 + (bins_remain_cap[fit_mask] - item))` is good.\n\n    # To add a slight bias towards less empty bins overall, we can consider the original `bins_remain_cap`.\n    # `score = (1.0 / (1.0 + slack)) * (bins_remain_cap[fit_mask] / MAX_BIN_CAPACITY)`\n    # This would give a slight edge to bins that are already more full, assuming they provide a tight fit.\n    # But this might conflict with \"tight fit\" if the more full bin has a larger slack.\n\n    # Sticking to the core \"Almost Full Fit\" intent: prioritize minimal slack.\n    # The `1.0 / (1.0 + slack)` is a robust interpretation.\n\n    # Let's try to incorporate the \"penalize large gaps gently\" aspect more directly.\n    # A bin that fits the item but leaves a large remaining capacity (large slack) should have lower priority\n    # than a bin that fits and leaves a small remaining capacity.\n    # The `1.0 / (1.0 + slack)` already handles this.\n\n    # What if we prioritize bins where `potential_remaining_cap` is closest to zero (but non-negative)?\n    # This is precisely what `1.0 / (1.0 + slack)` does.\n\n    # Let's consider the \"fullness\" of the bin *after* placing the item.\n    # This is `(BinCapacity - potential_remaining_cap) / BinCapacity`.\n    # For example, if bin capacity is 10:\n    # Item 3, bin_remain_cap 5 -> slack 2, potential_remaining_cap 2. Fullness = (10-2)/10 = 0.8\n    # Item 3, bin_remain_cap 4 -> slack 1, potential_remaining_cap 1. Fullness = (10-1)/10 = 0.9\n    # Item 3, bin_remain_cap 3 -> slack 0, potential_remaining_cap 0. Fullness = (10-0)/10 = 1.0\n\n    # This \"fullness\" metric would prioritize the most full bins.\n    # The score would be `(bins_remain_cap[fit_mask] - item) / MAX_BIN_CAPACITY`? No, this is inverse.\n    # It should be `(BIN_CAPACITY - (BIN_CAPACITY - item)) / BIN_CAPACITY = item / BIN_CAPACITY`.\n    # This strategy is essentially First Fit Decreasing if we could sort by item size.\n    # For online, this means we prefer bins that are already fuller (less capacity remaining).\n\n    # Let's combine:\n    # 1. Tight fit (small slack) is primary.\n    # 2. Among tight fits, perhaps prefer bins that were already more full.\n\n    # A scoring function could be:\n    # `priority = (1.0 / (1.0 + slack)) * (bins_remain_cap[fit_mask] / MAX_BIN_CAPACITY)`\n    # where `MAX_BIN_CAPACITY` is some global value or an average.\n\n    # Let's try a simpler modification to `1.0 / (1.0 + slack)`\n    # The reflection says: \"Prioritize tight fits and fuller bins.\"\n    # `1.0 / (1.0 + slack)` prioritizes tight fits.\n    # To prioritize fuller bins, we can multiply by a term that increases with `bins_remain_cap[fit_mask]`.\n    # However, this might push us away from tight fits if a very full bin has a slightly larger slack.\n\n    # A robust way to prioritize fuller bins among tight fits is to prioritize bins\n    # that have `bins_remain_cap[fit_mask] - item` closer to 0.\n    # If we have two bins with slack=0, we should prefer the one that was already fuller.\n    # However, the current strategy might pick the one that was less full if its absolute remaining capacity is smaller.\n\n    # Let's stick to the interpretation that \"almost full\" means minimal positive slack.\n    # The `1.0 / (1.0 + slack)` metric is a good heuristic for this.\n    # The \"fuller bins\" part is implicitly handled by favoring smaller slacks.\n\n    # Consider a score that rewards smaller `bins_remain_cap[fit_mask]` overall,\n    # while still favoring smaller `slack`.\n\n    # A common heuristic for online BPP is \"Best Fit\", which picks the bin with the minimum slack.\n    # This is captured by maximizing `1.0 / (1.0 + slack)`.\n\n    # The current implementation `priorities[fit_mask] = 1.0 / (1.0 + potential_remaining_cap)`\n    # directly implements a form of \"Best Fit\" or \"Almost Full Fit\" by prioritizing minimal slack.\n    # The \"gently penalize large gaps\" is inherent in the decay of `1/(1+x)`.\n\n    # Let's ensure robustness for edge cases and clarify the \"fuller bins\" aspect.\n    # If we have two bins that are perfect fits (slack=0), the current method gives them equal priority.\n    # If we want to prefer the *fuller* bin in this case, we can add a secondary sorting criterion.\n    # But the priority function should return a single score.\n\n    # A potential scoring could be:\n    # `score = -slack + C * bins_remain_cap[fit_mask]`\n    # where C is a small positive constant to break ties for slack.\n    # This would favor smaller slack, and then larger `bins_remain_cap`.\n\n    # Let's try implementing `-slack` first for direct \"minimal slack\" preference.\n    # `priorities[fit_mask] = -potential_remaining_cap`\n    # Then, to penalize larger gaps, we add a term.\n    # A score like `-slack / MAX_SLACK` or `-slack / AVERAGE_SLACK`\n    # Or we can use `bins_remain_cap` as a positive factor to break ties for slack.\n\n    # Let's refine: we want to maximize `f(slack)`.\n    # `f(slack) = 1 / (1 + slack)` is good.\n    # To incorporate \"fuller bins\", we can add a term that increases with `bins_remain_cap[fit_mask]`.\n    # `score = (1.0 / (1.0 + slack)) + alpha * bins_remain_cap[fit_mask]`\n    # The alpha needs tuning.\n\n    # A simpler way to favor fuller bins among tight fits:\n    # What if we want to prioritize bins that are \"almost full\" in an absolute sense?\n    # E.g., bins with remaining capacity between 0 and 10% of total capacity are good.\n    # This suggests penalizing bins with remaining capacity > 10% of total capacity.\n\n    # Let's try a score that is high for small slack and slightly higher for bins\n    # that were already more full.\n\n    # Score = (1 / (1 + slack))  # Prioritizes tightest fits\n    # Let's try multiplying by original capacity, scaled.\n    # It's tricky to combine them into a single score without making one dominate too much.\n\n    # Revisiting the reflection: \"Prioritize tight fits and fuller bins. Penalize large gaps gently.\"\n    # The `1.0 / (1.0 + slack)` approach handles \"tight fits\" and \"penalize large gaps gently\".\n    # For \"fuller bins\", it's about choosing between bins with similar slack.\n    # If slack is equal, it doesn't distinguish.\n\n    # Let's try a score that combines slack and original remaining capacity:\n    # `score = -slack + C * bins_remain_cap[fit_mask]`\n    # This prioritizes minimum slack. For same slack, it prioritizes larger `bins_remain_cap`.\n    # This might be counter-intuitive to \"fuller bins\" meaning \"less empty\".\n    # It would mean if bin A has capacity 10, item 5 (slack 5), and bin B has capacity 20, item 15 (slack 5),\n    # it would prefer bin B.\n    # If we want to prefer bins that are already more full (less original capacity),\n    # it would be `score = -slack - C * bins_remain_cap[fit_mask]` (minimizing this score).\n    # Or maximizing `slack + C * bins_remain_cap[fit_mask]` if slack is the penalty.\n\n    # Let's try a simpler approach based on the original logic but with a slight boost for fuller bins.\n    # The original logic: `1.0 / (1.0 + slack)`\n    # We want to boost bins that are more full.\n    # `priority = (1.0 / (1.0 + slack)) * (bins_remain_cap[fit_mask] / MAX_POSSIBLE_REMAINING_CAP)`\n    # This could normalize the effect of original capacity.\n\n    # A simpler approach that combines minimal slack and fuller bins:\n    # If we consider the goal of minimizing total bins, then filling bins tightly is key.\n    # The `1.0 / (1.0 + slack)` metric directly addresses minimizing slack.\n\n    # What if we use `potential_remaining_cap` and penalize larger values?\n    # `score = -potential_remaining_cap` maximizes fullness directly.\n    # But this doesn't distinguish between bins that are `potential_remaining_cap = -1` and `potential_remaining_cap = -10`\n    # if the item size is large.\n    # The `1.0 / (1.0 + slack)` metric is better because slack is always >= 0 for fitting bins.\n\n    # Let's consider a metric that penalizes bins that are TOO empty after insertion.\n    # If `potential_remaining_cap` is large, the priority should be low.\n    # `score = 1.0 / (1.0 + potential_remaining_cap)` is good.\n\n    # A slight variation: use the negative of the remaining capacity as a base,\n    # then apply a penalty for larger absolute remaining capacity.\n    # `score = -potential_remaining_cap - penalty_factor * max(0, potential_remaining_cap)`\n    # This gives higher score to more negative `potential_remaining_cap` (more full),\n    # and then adds a penalty if `potential_remaining_cap` is positive (less full).\n\n    # Let's go with a robust approach that prioritizes minimal slack, and as a secondary preference,\n    # favors bins that were already more full.\n    # We can achieve this by a weighted sum or by modifying the score.\n\n    # Score = (1.0 / (1.0 + slack)) * (1.0 + bins_remain_cap[fit_mask] / SOME_SCALE)\n    # This boosts the priority for fuller bins.\n    # Let's try a simpler version:\n    # `score = -slack + C * bins_remain_cap[fit_mask]`\n    # This score would be maximized.\n    # This favors smaller slack. Among equal slacks, it favors larger `bins_remain_cap`.\n    # This may not be ideal for \"fuller bins\" meaning \"less capacity\".\n\n    # Final attempt at a combined heuristic:\n    # Prioritize minimal slack `s = bin_cap - item`.\n    # Score = f(s). We want f(s) to be decreasing for s >= 0.\n    # Then, boost for fuller bins (smaller `bin_cap`).\n    # Score = g(s) * h(bin_cap) where g is decreasing and h is increasing (for less capacity).\n\n    # Let's use the `-slack` approach directly, which favors minimal slack.\n    # `priorities[fit_mask] = -potential_remaining_cap`\n    # Now, how to add \"fuller bins\"?\n    # If we have two bins with slack 0, we prefer the one with smaller `bins_remain_cap`.\n    # So, we want to *minimize* `bins_remain_cap` for tie-breaking.\n    # In a maximization score, this means adding a penalty for larger `bins_remain_cap`.\n    # `score = -potential_remaining_cap - C * bins_remain_cap[fit_mask]`\n\n    # Let's consider the \"Almost Full Fit\" aspect more directly:\n    # Prioritize bins where `bins_remain_cap - item` is small and non-negative.\n    # `score = 1.0 / (1.0 + slack)` does this.\n    # For \"fuller bins\", this means if slack is the same, choose the bin that was already fuller (had less remaining capacity).\n    # The `1.0 / (1.0 + slack)` doesn't distinguish if slack is the same.\n\n    # A good heuristic is to prioritize minimal slack. If there's a tie in slack,\n    # pick the bin that was originally fuller.\n    # This can be achieved by:\n    # 1. Score = -slack\n    # 2. For ties in score, use -original_remaining_capacity as secondary score.\n    # Since the function must return a single score:\n    # `score = -slack - C * original_remaining_capacity` where C is small.\n    # This implies we are minimizing the score. But we need to maximize for the highest priority.\n    # So, maximize: `slack + C * original_remaining_capacity`\n\n    # Let's use the `1.0 / (1.0 + slack)` for minimal slack, and try to add a factor for fuller bins.\n    # `priority = (1.0 / (1.0 + slack)) * (1.0 + (MAX_CAPACITY - bins_remain_cap[fit_mask]) / MAX_CAPACITY)`\n    # This second term increases as `bins_remain_cap` decreases (bin is fuller).\n\n    # Let's test this combined approach:\n    # `MAX_CAPACITY` is not available. Let's assume it or use average.\n    # A simpler way to penalize large gaps gently and prioritize fuller bins:\n    # Use the inverse of remaining capacity after placement, scaled.\n    # `score = (MAX_CAPACITY - potential_remaining_cap)` for fitting bins.\n    # This maximizes the final occupied space, which means minimal remaining capacity.\n    # And it prioritizes bins that have more space *filled* (thus are \"fuller\").\n\n    # Let's use a metric that rewards small `potential_remaining_cap`.\n    # `score = -potential_remaining_cap` is a good candidate.\n    # To add the \"fuller bins\" preference, if `potential_remaining_cap` is same, prefer smaller `bins_remain_cap`.\n    # `score = -potential_remaining_cap - C * bins_remain_cap[fit_mask]`\n\n    # Considering the reflection: \"Prioritize tight fits and fuller bins. Penalize large gaps gently.\"\n    # The `1.0 / (1.0 + slack)` strategy is excellent for tight fits and gentle penalization of gaps.\n    # For \"fuller bins\", if `slack` is equal, we prefer the bin that was originally more full.\n    # We can add a term that reflects this.\n    # Let's try `score = (1.0 / (1.0 + slack)) * (1.0 / (1.0 + bins_remain_cap[fit_mask] / SOME_SCALE))`\n    # This penalizes larger `bins_remain_cap`. This is the opposite of what we want for \"fuller bins\".\n\n    # We want to maximize `slack_score * fullness_score`.\n    # `slack_score = 1.0 / (1.0 + slack)` (higher for smaller slack)\n    # `fullness_score` should be higher for smaller `bins_remain_cap`.\n    # `fullness_score = 1.0 / (1.0 + bins_remain_cap[fit_mask])` is one way.\n\n    # So, `priority = (1.0 / (1.0 + slack)) * (1.0 / (1.0 + bins_remain_cap[fit_mask]))`\n    # This prioritizes bins that are both tight fits AND were already more full.\n\n    slack = bins_remain_cap[fit_mask] - item\n    original_caps = bins_remain_cap[fit_mask]\n\n    # Prioritize tight fits (small slack) using inverse relationship.\n    tight_fit_score = 1.0 / (1.0 + slack)\n\n    # Prioritize fuller bins (smaller original remaining capacity).\n    # Adding a small epsilon to original_caps to avoid potential division by zero if original_cap is 0,\n    # though this is unlikely in BPP. More importantly, to ensure the denominator is > 1,\n    # giving a score < 1 for non-zero capacities.\n    fullness_score = 1.0 / (1.0 + original_caps)\n\n    # Combine scores. Multiplication tends to favor bins that are good on both criteria.\n    # This approach prioritizes bins that are tight fits AND were already more full.\n    priorities[fit_mask] = tight_fit_score * fullness_score\n\n    # Alternative combined score (additive, might give different relative weights):\n    # priorities[fit_mask] = tight_fit_score + fullness_score * 0.1 # Small weight for fullness\n\n    # Let's test `tight_fit_score * fullness_score`:\n    # Bin A: item=3, bin_cap=5 => slack=2, original_cap=5.\n    #        tight_fit_score = 1/(1+2) = 0.333. fullness_score = 1/(1+5) = 0.167. Total = 0.055\n    # Bin B: item=3, bin_cap=4 => slack=1, original_cap=4.\n    #        tight_fit_score = 1/(1+1) = 0.5. fullness_score = 1/(1+4) = 0.2. Total = 0.1\n    # Bin C: item=3, bin_cap=3 => slack=0, original_cap=3.\n    #        tight_fit_score = 1/(1+0) = 1.0. fullness_score = 1/(1+3) = 0.25. Total = 0.25\n\n    # This seems to align well with prioritizing tight fits and then fuller bins.\n    # The penalization of large gaps is handled by the steep decay of 1/(1+slack).\n    # The \"fuller bins\" preference is handled by the 1/(1+original_cap) factor.\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 125.09775004326937,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response4.txt_stdout.txt",
    "code_path": "problem_iter7_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using an enhanced 'Almost Full Fit' strategy.\n\n    This strategy prioritizes bins that fit the item tightly, aiming to minimize\n    remaining capacity after packing. It slightly penalizes bins with very large\n    remaining capacities even if they can fit the item, and gently penalizes\n    bins that would become extremely full (very close to zero remaining capacity)\n    to encourage slightly more open bins if tightness is otherwise equal.\n    The core idea is to favor bins that are neither too empty nor too full,\n    but rather \"just right\" to fit the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity if the item is placed in each fitting bin\n    potential_remaining_cap = bins_remain_cap[fit_mask] - item\n    fitting_bins_indices = np.where(fit_mask)[0]\n\n    # Strategy: Prioritize bins with small non-negative remaining capacity (tight fits).\n    # We want to assign higher scores to smaller values of `potential_remaining_cap`.\n    # A common approach is to use the inverse.\n    # To avoid division by zero and to provide a smooth curve: 1 / (1 + slack)\n    # This assigns 1.0 to perfect fits (slack=0), lower values for larger slacks.\n\n    # However, the reflection suggests penalizing large gaps gently and balancing efficiency with robustness.\n    # Let's consider the \"slack\": slack = bins_remain_cap - item.\n    # Smaller slack is generally better for \"almost full\".\n\n    # To prioritize tight fits and fuller bins, we want to maximize the \"fullness\"\n    # after packing. This means minimizing `potential_remaining_cap`.\n    # Let's use `-(potential_remaining_cap)` as a base score.\n    # A bin that ends up with -0.1 remaining capacity (overfilled) is worse than one that ends up with 0.0.\n    # But we only consider bins where `item <= bins_remain_cap`.\n\n    # Refined strategy:\n    # 1. Prioritize bins that fit the item.\n    # 2. Among fitting bins, prioritize those with smaller slack (`bins_remain_cap - item`).\n    #    This means `potential_remaining_cap` should be as close to zero as possible.\n    # 3. To avoid very large gaps being penalized too much, and to avoid very tight bins\n    #    being overly favored if they are extremely small, we can introduce a gentle penalty\n    #    for very large remaining capacities (large slack) and a slight bias for moderate fullness.\n\n    # Let's use a score that peaks for a \"just right\" slack, but favors smaller slacks overall.\n    # Consider a function that is high for slack near 0, and decreases as slack increases.\n    # A Gaussian-like or logistic function could work, but for simplicity, let's stick to\n    # inverse relationships with adjustments.\n\n    # Primary scoring: Minimize `potential_remaining_cap`.\n    # So, maximize `-potential_remaining_cap`.\n    # Let's use `-potential_remaining_cap` directly, which favors bins that will be most full.\n    # This is a direct \"Best Fit\" interpretation, which is related to \"Almost Full Fit\".\n\n    # To \"gently penalize large gaps\":\n    # If slack is very large, the priority should be lower.\n    # `1.0 / (1.0 + slack)` does this. A slack of 10 gives ~0.09, slack of 100 gives ~0.01.\n\n    # To ensure the \"almost full\" aspect, we want smaller slack values to be more preferred.\n    # The `1.0 / (1.0 + slack)` function already does this.\n    # Slack = 0  => Priority = 1.0\n    # Slack = 1  => Priority = 0.5\n    # Slack = 2  => Priority = 0.33\n\n    # Reflection point: \"Penalize large gaps gently.\" and \"Prioritize tight fits and fuller bins.\"\n    # The `1.0 / (1.0 + slack)` is a good balance. It strongly favors tight fits (slack near 0)\n    # and gently reduces priority as slack increases.\n\n    # Let's try a slightly modified score that emphasizes bins that become *very* full,\n    # by slightly amplifying the negative remaining capacity.\n    # The existing `1.0 / (1.0 + slack)` is a good baseline for favoring smaller slacks.\n\n    # To add a nuance: what if a bin has capacity 100 and item is 10 (slack 90) vs.\n    # capacity 12 and item is 10 (slack 2)? Both fit. `1/(1+90)` vs `1/(1+2)`.\n    # The second bin is strongly preferred. This seems reasonable.\n\n    # Let's ensure that bins with zero remaining capacity after fitting (perfect fit)\n    # get the highest priority.\n    # The current `1.0 / (1.0 + slack)` handles slack=0 by giving priority 1.0.\n\n    # To ensure \"fuller bins\" are prioritized if slacks are comparable,\n    # we can add a small bonus for bins that started with more capacity,\n    # but this might contradict \"tight fits\".\n\n    # Let's refine the score slightly: use `1 / (epsilon + slack)` where epsilon is small.\n    # This still favors small slack.\n    # Perhaps a score that is sensitive to the *absolute* remaining capacity after placement?\n    # Maximize `-potential_remaining_cap`.\n    # If potential_remaining_cap = 0, score = 0.\n    # If potential_remaining_cap = 1, score = -1.\n    # If potential_remaining_cap = 5, score = -5.\n    # This means we prefer bins that become most full (closest to 0 remaining).\n\n    # Consider `1 - (potential_remaining_cap / BIN_CAPACITY)` or similar normalized approaches.\n    # This is getting complex. Let's keep it simple and effective.\n\n    # The `1.0 / (1.0 + slack)` approach seems robust and directly addresses\n    # prioritizing tight fits (small slack).\n\n    # Let's consider the reflection again: \"Prioritize tight fits and fuller bins.\"\n    # Tight fit = small slack. Fuller bins = smaller `potential_remaining_cap`.\n    # These are mostly aligned.\n    # \"Penalize large gaps gently.\" `1 / (1 + slack)` does this.\n    # \"Balance efficiency with robustness through simple, tunable scoring.\"\n\n    # Let's slightly modify `1.0 / (1.0 + slack)` to give a bit more emphasis to\n    # *very* small slacks.\n    # We can use a non-linear transformation.\n    # For example, `exp(-k * slack)` where k is a tuning parameter.\n    # Or, `1.0 / (1.0 + slack**p)` where p > 1.\n    # Let's use `p=1.5` to give more weight to smaller slacks.\n\n    slack_values = bins_remain_cap[fit_mask] - item\n    priorities[fitting_bins_indices] = 1.0 / (1.0 + slack_values**1.5)\n\n    # This gives:\n    # Slack = 0   => Priority = 1.0 / (1.0 + 0^1.5) = 1.0\n    # Slack = 1   => Priority = 1.0 / (1.0 + 1^1.5) = 0.5\n    # Slack = 2   => Priority = 1.0 / (1.0 + 2^1.5) = 1.0 / (1.0 + 2.828) approx 0.26\n    # Slack = 0.5 => Priority = 1.0 / (1.0 + 0.5^1.5) = 1.0 / (1.0 + 0.707) approx 0.58\n\n    # This emphasizes smaller slacks more than `1/(1+slack)`.\n    # It still penalizes large gaps gently and prioritizes tight fits.\n    # The power `1.5` is tunable.\n\n    # Consider an alternative: maximize `-(potential_remaining_cap)`.\n    # This would favor bins that end up with capacity closest to zero.\n    # Example:\n    # Bin A: capacity 5, item 3 -> potential_remaining_cap = 2, score = -2\n    # Bin B: capacity 4, item 3 -> potential_remaining_cap = 1, score = -1\n    # Bin C: capacity 3, item 3 -> potential_remaining_cap = 0, score = 0\n    # This favors Bin C (perfect fit), then B, then A. This is \"Best Fit\".\n\n    # The prompt for \"Almost Full Fit\" usually implies minimizing slack.\n    # The `1 / (1 + slack**p)` approach is a good way to do this while being robust.\n\n    # To further align with \"fuller bins\", let's consider the actual remaining capacity after item placement.\n    # Let's try a combined score:\n    # Priority = (1 - slack_norm) + (1 - original_capacity_norm)\n    # where slack_norm = slack / max_slack, etc. This gets complicated.\n\n    # Let's stick to the `1 / (1 + slack**p)` as it directly addresses the prioritized properties.\n    # It's simple, tunable (via `p`), prioritizes small slacks, and penalizes large slacks gently.\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 68.53238859703687,
    "exec_success": true
  }
]