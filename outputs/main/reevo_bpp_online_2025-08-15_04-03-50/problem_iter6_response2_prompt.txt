{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Sigmoid Fit Score with a penalty for very large capacities and exploration.\n\n    This version prioritizes bins that have a remaining capacity that is just\n    slightly larger than the item size. This aims to minimize wasted space in\n    the selected bin, leaving larger contiguous free spaces in other bins for\n    potentially larger future items.\n\n    The priority is calculated using a sigmoid function applied to the difference\n    between the bin's remaining capacity and the item's size. A smaller\n    positive difference (a tighter fit) results in a higher priority score.\n    Additionally, bins with very large remaining capacities (relative to the item)\n    are penalized to encourage spreading. A small probability of exploration is also included.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    exploration_prob = 0.05\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_capacities = bins_remain_cap[fittable_bins_mask]\n    slack = fittable_bins_capacities - item\n\n    # Sigmoid for tight fits (prioritize near-zero slack)\n    k_fit = 5.0\n    tight_fit_score = 1.0 / (1.0 + np.exp(k_fit * slack))\n\n    # Penalty for very large remaining capacities.\n    # We want to discourage putting a small item into a very large bin if a tighter fit exists.\n    # This can be modeled with a logistic function that decreases as capacity grows.\n    # Let's use a threshold and a decay factor.\n    # We can normalize capacities relative to the item size or bin capacity.\n    # A simple approach: Penalize bins whose remaining capacity is much larger than the item.\n    # Let's create a score that is high for capacities close to the item size and decreases.\n    # We can use a similar sigmoid but inverted or a different function.\n    # Alternative: use 1 / (1 + exp(-k_large * (capacity - threshold)))\n    # A simpler penalty: if capacity is > C * item, reduce score.\n\n    # Let's try a penalty based on normalized slack.\n    # We want the penalty to be low for slack close to 0 and high for large slack.\n    # This is the opposite of the tight fit score.\n    # We can use a decaying function of slack.\n    # e.g., 1 / (1 + slack) or exp(-slack / scale)\n    # Let's try a sigmoid on the negative slack to penalize larger slack.\n    k_penalty = 0.5 # Controls how quickly the penalty increases with slack\n    large_capacity_penalty = 1.0 / (1.0 + np.exp(-k_penalty * slack))\n\n    # Combine scores. The tight_fit_score is high for small slack.\n    # The large_capacity_penalty is high for small slack, and low for large slack.\n    # We want to combine them such that:\n    # 1. Small slack (tight fit) is good -> high tight_fit_score\n    # 2. Large slack (too much space) is bad -> needs a penalty\n    # Let's try to combine them additively.\n    # A higher score for tight fits, and a lower score for large slack.\n    # The large_capacity_penalty goes from ~0.5 to ~1 as slack increases. This is not a penalty.\n    # Let's re-think the penalty. We want to penalize large slack.\n    # The sigmoid `1 / (1 + exp(k * slack))` already penalizes large slack.\n    # So maybe just use a weighted combination of the tight fit score and the slack itself.\n    # Or, use the slack directly, but inverted and scaled.\n\n    # Let's re-evaluate the reflection: \"penalize large remaining capacities\"\n    # The `tight_fit_score` already does this by giving low scores to large slack.\n    # Maybe the reflection implies we should *also* consider the absolute capacity.\n    # For example, putting an item of size 1 into a bin with remaining capacity 100\n    # is worse than putting it into a bin with remaining capacity 1.\n    # The slack approach (capacity - item) naturally handles this:\n    # slack for (100, 1) is 99, slack for (1, 1) is 0.\n\n    # Let's reconsider the \"Worst Fit\" element from v0 and combine it with tight fit.\n    # Best Fit component (tight_fit_score) -> prioritizes small slack\n    # Worst Fit component (prioritizes bins with more remaining capacity) -> prioritizes large capacity\n    # This seems contradictory. The reflection \"Focus on tighter fits, penalize large remaining capacities\"\n    # suggests we should prioritize small slack and penalize large slack. The `tight_fit_score` does this.\n\n    # Let's refine the \"penalty for very large remaining capacities\".\n    # A simple approach: normalize slack by some reference, or use a threshold.\n    # Let's say if remaining_capacity > 2 * item, we start penalizing.\n    # Max slack we want to consider for high priority: say, up to `max_slack_ideal`.\n    # Any slack beyond that should be heavily penalized.\n    max_slack_ideal = 2.0 * item # An item of size 'item' should ideally fit into a bin with 2*item remaining capacity at most for a good fit.\n    # Penalize slack if it's much larger than max_slack_ideal.\n    # We can use a linear penalty or a sigmoid that drops sharply.\n    # Let's use a sigmoid that is high for slack <= max_slack_ideal and low for slack > max_slack_ideal.\n    # Use `1 / (1 + exp(k_penalty * (slack - max_slack_ideal)))`\n    # This gives 0.5 at slack = max_slack_ideal, decreases for slack > max_slack_ideal.\n    k_penalty = 1.0 # Controls the steepness of the penalty\n    penalty_score = 1.0 / (1.0 + np.exp(k_penalty * (slack - max_slack_ideal)))\n\n    # Combine tight_fit_score and penalty_score.\n    # We want both to contribute positively.\n    # tight_fit_score is high for small slack.\n    # penalty_score is high for small slack (and slack <= max_slack_ideal).\n    # So, a linear combination might work.\n    # Let's weight them. Give more weight to the tight fit.\n    combined_scores = 0.7 * tight_fit_score + 0.3 * penalty_score\n\n    priorities[fittable_bins_mask] = combined_scores\n\n    # Apply exploration\n    if np.random.rand() < exploration_prob:\n        fittable_indices = np.where(fittable_bins_mask)[0]\n        chosen_bin_global_index = np.random.choice(fittable_indices)\n        priorities.fill(0)\n        priorities[chosen_bin_global_index] = 1.0\n    else:\n        # Normalize priorities for fittable bins to sum to 1\n        fittable_priorities = priorities[fittable_bins_mask]\n        if np.sum(fittable_priorities) > 0:\n            priorities[fittable_bins_mask] /= np.sum(fittable_priorities)\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a hybrid strategy.\n\n    This strategy aims to balance the preference for tight fits (minimizing wasted space\n    in the immediate bin) with a slight preference for bins that offer a bit more\n    room, promoting better global packing. It uses a combination of a \"Best Fit\"\n    (minimal slack) component and a \"First Fit\"-like component (favoring bins\n    that are not too empty).\n\n    The priority is calculated based on the negative slack (prioritizing bins\n    where `bins_remain_cap - item` is small and positive). To encourage exploration\n    and prevent premature filling of slightly-too-small gaps that might be better\n    suited for smaller items, a small penalty is applied to bins with extremely\n    tight fits, and a bonus is given to bins with a moderate amount of slack.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if np.any(fittable_bins_mask):\n        fittable_bins_capacities = bins_remain_cap[fittable_bins_mask]\n\n        # Calculate slack: remaining capacity after fitting the item.\n        # We want slack to be as close to 0 as possible.\n        slack = fittable_bins_capacities - item\n\n        # Heuristic:\n        # 1. Prioritize bins with small positive slack (tight fits).\n        # 2. Avoid extremely tight fits (slack very close to 0) by slightly penalizing them.\n        # 3. Give a small bonus to bins with moderate slack to encourage using \"less full\" bins sometimes.\n        # This can be achieved by mapping slack to a score where a small positive value is optimal.\n\n        # Using a quadratic function: -(slack - ideal_slack)^2\n        # Let's aim for an \"ideal\" slack of, say, 10% of the item size,\n        # but with a floor to avoid negative ideal slack for very small items.\n        ideal_slack = max(0.1 * item, 0.1) # Aim for ~10% of item size, minimum 0.1\n        \n        # Penalize deviations from the ideal slack.\n        # A higher value means closer to ideal_slack is better.\n        # We want to maximize this value.\n        # The function `-(slack - ideal_slack)**2` will be maximized when slack is close to ideal_slack.\n        # However, we also want to prioritize *tight* fits, meaning slack near 0 should be good.\n        \n        # Let's try a different approach: Reward tightness, but with diminishing returns.\n        # Prioritize bins where (bin_capacity - item) is small.\n        # A simple reward for tightness: 1 / (1 + slack)\n        # To balance with exploration: maybe add a small bonus if slack is small but not zero.\n        \n        # Hybrid approach:\n        # Prioritize bins with minimal slack (close to 0). This is the \"Best Fit\" aspect.\n        # But also give a slight preference to bins with a moderate amount of slack\n        # to avoid leaving many bins in a state that's hard to fill.\n\n        # A scoring function that peaks at slack = 0 and then decreases,\n        # but perhaps less sharply for very small slacks.\n        \n        # Let's use a scaled inverse relationship with slack, and add a small bonus for moderate slack.\n        # For very small slack (tight fit): prioritize.\n        # For moderate slack: also consider it favorable.\n        # For large slack: disfavor.\n        \n        # Strategy: Base priority on inverse slack (favoring small slack),\n        # but add a small bonus proportional to slack up to a certain point,\n        # and then penalize larger slacks.\n\n        # Let's try a function that is high for slack near 0, then decreases.\n        # Consider a penalty for slack that is too small (e.g., < 0.1) and too large.\n        \n        # A common heuristic for balancing is to use a function that is high for \"just right\".\n        # For bin packing, \"just right\" is usually a tight fit.\n        # To encourage exploration, we can slightly boost bins that aren't *exactly* the tightest.\n        \n        # Let's use a function that rewards closeness to 0 slack, but less aggressively for very small slacks.\n        # And perhaps a small bonus for slack that is not excessively large.\n\n        # Option 1: Gamma distribution-like scoring based on slack\n        # Prioritize small slack, with a peak somewhere slightly above zero.\n        # Example: (slack^a) * exp(-b*slack)\n        # This can be tricky to tune.\n\n        # Option 2: Combining a \"Best Fit\" component with a \"Least Full\" component.\n        # Best Fit: Maximize (1 / (1 + slack))\n        # Least Full: Maximize (bins_remain_cap) -- but we want to avoid *large* bins, so this isn't direct.\n        \n        # Let's try to directly score bins based on how \"good\" their remaining space is.\n        # A good remaining space is small, but not so small that it becomes unusable.\n        \n        # Score: Maximize (1 / (1 + slack)) - prioritize tight fits.\n        # Bonus for moderate slack: add a small value if slack is between X and Y.\n        \n        # Let's consider slack values.\n        # slack = 0: Best.\n        # slack = 0.1: Good.\n        # slack = 1.0: Okay.\n        # slack = 5.0: Not great.\n        \n        # A function like: `exp(-slack / constant)` will heavily favor small slacks.\n        # To add exploration: `exp(-slack / constant) + bonus_if_slack_is_moderate`\n        \n        # Let's try a concave function that peaks at slack=0, but with a flattened initial slope.\n        # This can be achieved by `1 - exp(-k*slack)`. This maps slack=0 to 0, slack=inf to 1.\n        # We want the opposite: slack=0 should be high.\n        \n        # Revisit the \"Worse code\" logic: `1 / (1 + slack)` is good for favoring tight fits.\n        # The reflection asks to favor tight fits but explore slightly larger gaps.\n        # This means we want the function to be high for small slack, but not drop off *too* quickly.\n        \n        # Let's use a sigmoidal function that starts high for small slacks and gradually decreases.\n        # A sigmoid shifted and scaled.\n        \n        # Consider slack values:\n        # If slack is very small (e.g., 0.01), we want a high score.\n        # If slack is moderate (e.g., 0.5), we want a reasonably high score, maybe slightly lower than 0.01.\n        # If slack is large (e.g., 2.0), we want a low score.\n        \n        # A function like `1 / (1 + slack**p)` where p is between 0 and 1 might work.\n        # If p=1, it's `1/(1+slack)` (original logic).\n        # If p=0.5, it's `1/(1 + sqrt(slack))`. This penalizes large slacks less.\n        \n        # Let's use a scaling factor for slack to control the steepness of the decay.\n        # And perhaps add a small constant to slack to avoid division by zero and\n        # give a base priority to bins with zero slack.\n        \n        # Let's try a heuristic that penalizes large remaining capacities more heavily.\n        # We want to prefer bins that, after packing, have *some* remaining capacity, but not too much.\n        \n        # Consider the remaining capacity *after* packing: `bins_remain_cap - item`.\n        # We want this to be small and positive.\n        \n        # Score = f(bins_remain_cap - item)\n        # f(x) should be high for small positive x.\n        \n        # Let's use `1 / (1 + slack)` as a base, but then add a small bonus for slacks within a certain range.\n        # Or, modify the `slack` value itself before applying `1 / (1 + slack)`.\n        \n        # Idea: Apply a transformation to slack that emphasizes values close to 0.\n        # Let `transformed_slack = slack / (slack + constant)`\n        # As slack -> 0, transformed_slack -> 0.\n        # As slack -> inf, transformed_slack -> 1.\n        # So, priority could be `1 - transformed_slack = constant / (slack + constant)`\n        # This is similar to `1 / (1 + slack/constant)`.\n        \n        # Let's try to incorporate the \"just right\" idea.\n        # If slack is too small, it's bad (maybe the item almost doesn't fit).\n        # If slack is too large, it's bad (wasted space).\n        # If slack is moderate, it's good.\n        \n        # This suggests a function with a peak.\n        # However, for Bin Packing, the primary goal is still to fit items efficiently.\n        # The \"exploration\" part should probably not override the fundamental \"fit tightly\" heuristic too much.\n        \n        # Let's use a softened Best Fit approach.\n        # Instead of `1 / (1 + slack)`, use a function that is less steep for small slacks.\n        # Consider `log(1 + 1/slack)` for slack > 0. This also favors small slack.\n        \n        # Let's go back to the idea of preferring bins with small positive remaining capacity.\n        # Priority is inversely related to `bins_remain_cap - item`.\n        \n        # Consider a function `g(remaining_capacity)` where `remaining_capacity = bins_remain_cap - item`.\n        # We want `g(x)` to be high for small positive `x`.\n        \n        # `g(x) = 1 / (1 + x)`: Peaks at 1 for x=0, decreases.\n        # `g(x) = exp(-k*x)`: Peaks at 1 for x=0, decreases.\n        \n        # To encourage exploration of slightly larger gaps:\n        # We want the function to decrease slower for small `x`.\n        \n        # Let's try a hyperbolic tangent (tanh) based approach.\n        # `tanh(a - b*slack)`:\n        # If `a` is large and `b` is positive, it will be close to 1 for small slack, and decrease.\n        # Example: `tanh(5 - 2*slack)`\n        # slack=0 -> tanh(5) ~ 1\n        # slack=1 -> tanh(3) ~ 1\n        # slack=2 -> tanh(1) ~ 0.76\n        # slack=3 -> tanh(-1) ~ -0.76\n        \n        # This favors small slack, but also gives reasonable scores for moderate slack.\n        # It penalizes large slack significantly.\n        \n        # Let's refine this: `tanh(a * (1 - slack/ideal_slack))`\n        # This will peak when slack = ideal_slack. We want peak at slack = 0.\n        \n        # Let's try a function that combines \"tightness\" and \"not-too-empty\".\n        # Priority = (1 / (1 + slack)) * (slack / (slack + C))  -- this will go to 0.\n        \n        # How about favoring bins that are not too close to full, nor too empty?\n        # This is more like \"Second Fit\".\n        \n        # The reflection: \"Favor tight fits, but explore slightly larger gaps for flexibility.\"\n        # This implies a function that is high for small slack, but the decay is gentle initially.\n        \n        # Let's use a modified inverse relationship.\n        # Instead of `1 / (1 + slack)`, consider `1 / (1 + slack^p)` with `p < 1`.\n        # Or `1 / (1 + sqrt(slack))`. This gives a higher score for small slacks compared to `1/slack`.\n        # Let's try `1 / (1 + slack**0.5)`\n        \n        # slack = 0.01: 1 / (1 + 0.1) = 0.909\n        # slack = 0.1:  1 / (1 + 0.316) = 0.76\n        # slack = 0.5:  1 / (1 + 0.707) = 0.58\n        # slack = 1.0:  1 / (1 + 1.0)   = 0.5\n        \n        # Compared to `1 / (1 + slack)`:\n        # slack = 0.01: 1 / (1 + 0.01) = 0.99\n        # slack = 0.1:  1 / (1 + 0.1)  = 0.909\n        # slack = 0.5:  1 / (1 + 0.5)  = 0.667\n        # slack = 1.0:  1 / (1 + 1.0)  = 0.5\n        \n        # The `slack**0.5` version penalizes larger slacks less harshly. This seems to align with \"explore slightly larger gaps\".\n        \n        # Let's add a small constant to the denominator to ensure non-zero priority even for zero slack.\n        # And maybe scale the slack to control the sensitivity.\n        \n        # Let `scaled_slack = slack / item` (fractional slack)\n        # Priority ~ `1 / (1 + scaled_slack**p)`\n        \n        # Alternative: Focus on the remaining capacity `rc = bins_remain_cap - item`.\n        # We want `rc` to be small and positive.\n        # Let's define a penalty function for `rc`.\n        # Penalty is 0 if `rc` is ideal (e.g., 0), increases as `rc` deviates.\n        # But we want to favor small `rc`.\n        \n        # How about: `exp(-k * rc)`? This is similar to sigmoid.\n        \n        # Let's try a combination: a base score for tightness, plus a bonus for moderate non-zero slack.\n        \n        # Base score: `1 / (1 + slack)`\n        # Bonus: Apply a Gaussian-like function centered at a small positive slack, e.g., 0.5.\n        # `bonus = exp(-(slack - 0.5)**2 / (2 * sigma**2))`\n        \n        # This could get complex. Let's simplify.\n        \n        # The reflection is key: \"Favor tight fits, but explore slightly larger gaps for flexibility.\"\n        # This means the priority function should decrease as slack increases, but not too rapidly.\n        \n        # A function like `1 / (1 + slack^p)` with `0 < p < 1` is a good candidate.\n        # Let's choose `p = 0.5` (square root) for a start.\n        \n        # `priorities[fittable_bins_mask] = 1.0 / (1.0 + np.sqrt(slack))`\n        \n        # To prevent division by zero if slack could be negative (which it can't here since we filter `bins_remain_cap >= item`),\n        # and to avoid extremely high priorities for zero slack, let's add a small epsilon.\n        \n        # Let's also consider scaling slack by item size to make it more relative.\n        # `relative_slack = slack / item` (handle item=0 case)\n        # `priority = 1.0 / (1.0 + np.sqrt(relative_slack))`\n        \n        # Let's use the absolute slack but scale its influence.\n        # `score = 1.0 / (1.0 + (slack / scale_factor)**p)`\n        \n        # A simple approach that balances tight fits with some room:\n        # Favor bins where `bins_remain_cap - item` is small.\n        # Also, consider the absolute `bins_remain_cap`. Larger bins might be useful for larger items later.\n        \n        # Let's consider the \"quality\" of the remaining space.\n        # A very small remaining space (tight fit) is good for immediate packing.\n        # A slightly larger remaining space is also good because it leaves room.\n        # A very large remaining space is less good, as it might indicate a poor fit for the current item.\n        \n        # Let's try a function that peaks at a small positive slack value.\n        # A log-normal or gamma-like shape can achieve this.\n        # Example: `slack**alpha * exp(-beta * slack)`\n        # To maximize this, take derivative and set to zero.\n        # `alpha*slack**(alpha-1)*exp(-beta*slack) - beta*slack**alpha*exp(-beta*slack) = 0`\n        # `alpha*slack**(-1) - beta = 0`\n        # `alpha/slack = beta` => `slack = alpha / beta`\n        # So, we can tune `alpha` and `beta` to have the peak at a desired slack.\n        \n        # Let `alpha = 2`, `beta = 4`. Peak at `slack = 2/4 = 0.5`.\n        # `priorities[fittable_bins_mask] = slack**2 * np.exp(-4*slack)`\n        # This function is 0 at slack=0, peaks at 0.5, and goes to 0 as slack increases.\n        # This might be *too* much exploration and not enough tight fit preference.\n        \n        # Let's stick to favoring small slack primarily, but with a less aggressive decay.\n        # `1 / (1 + slack^p)` with `p` around 0.5 seems a good balance.\n        \n        # Consider `p = 0.7`.\n        # slack = 0.01: 1 / (1 + 0.0046) = 0.995\n        # slack = 0.1:  1 / (1 + 0.1778) = 0.845\n        # slack = 0.5:  1 / (1 + 0.421)  = 0.703\n        # slack = 1.0:  1 / (1 + 1.0)    = 0.5\n        \n        # This function is higher for small slacks than `1/(1+slack^0.5)`, and decays slower.\n        # This means it favors tight fits, but also gives good scores to moderately tight fits,\n        # promoting exploration of slightly larger gaps.\n        \n        # Let's use `p = 0.7` as a parameter.\n        p_value = 0.7\n        \n        # Add a small epsilon to slack to avoid potential issues with `slack=0` and `p<1` leading to undefined behavior\n        # or excessively high values if `slack` could be extremely close to zero.\n        # However, `0**p` is 0 for `p>0`. So, `1/(1+0)` is 1. This is fine.\n        \n        # Let's use a scaling factor for slack to control the \"tightness\" preference.\n        # `scaled_slack = slack / item_size_scale`\n        # A scale factor related to average item size might be good.\n        # For now, let's use absolute slack with `p=0.7`.\n        \n        priorities[fittable_bins_mask] = 1.0 / (1.0 + slack**p_value)\n        \n        # Ensure priorities are not NaN or Inf (though unlikely with this formula and non-negative slack)\n        priorities[fittable_bins_mask] = np.nan_to_num(priorities[fittable_bins_mask], nan=0.0, posinf=1.0, neginf=0.0)\n        \n    return priorities\n\n[Reflection]\nFavor tight fits but smoothly explore slightly larger gaps.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}