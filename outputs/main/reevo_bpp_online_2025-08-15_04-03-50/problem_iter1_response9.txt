```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.

    The priority is calculated based on how well an item fits into a bin.
    A good fit means a smaller remaining capacity after packing, encouraging
    using bins efficiently. The Softmax function is used to convert these
    "fit scores" into probabilities (priorities).

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Calculate the remaining capacity if the item is placed in each bin
    # We are only interested in bins where the item *can* fit.
    possible_fits = bins_remain_cap - item

    # For bins where the item doesn't fit, assign a very low (negative) score
    # so they have virtually zero probability after softmax.
    # Using a large negative number like -1e9 to ensure it's smaller than any potential good fit.
    fit_scores = np.where(possible_fits >= 0, possible_fits, -1e9)

    # Apply the Softmax function to the fit scores.
    # A smaller remaining capacity (better fit) will result in a higher priority.
    # Softmax formula: exp(x_i) / sum(exp(x_j))
    # We want smaller remaining capacity to have higher priority.
    # Thus, we can invert the scores or use a transformation like `-fit_scores`.
    # Let's use a transformation that penalizes larger remaining capacities more heavily.
    # A common approach for "best fit" type of heuristic with softmax is to
    # use the negative of the remaining capacity as the input to softmax.
    # The smaller the remaining capacity (closer to 0), the larger (less negative)
    # the `-fit_scores` will be, leading to a higher priority.

    # We also want to ensure that bins that cannot fit the item get a score close to zero.
    # The `-1e9` for non-fitting bins will result in `exp(-1e9)` which is effectively zero,
    # so the softmax naturally handles this.

    # Let's adjust the fit scores to be more conducive to softmax for "best fit".
    # We want smaller `possible_fits` to have higher probability.
    # So we use `-possible_fits` as the input to softmax.
    # Smaller `possible_fits` -> larger `-possible_fits` -> higher softmax value.

    # Consider the inverse of the remaining capacity if the item fits.
    # For bins that cannot fit the item, the remaining capacity would be negative.
    # We want to assign a very low score to these.
    # Let's transform the `possible_fits` such that a good fit (small `possible_fits`)
    # results in a large positive number for softmax.
    # A simple transformation is `-(possible_fits + epsilon)` where epsilon is small,
    # or even just `-possible_fits`. However, directly using `possible_fits` in softmax
    # would favor bins with *larger* remaining capacity after packing.

    # To achieve a "best fit" (smallest remaining capacity), we should use scores
    # where smaller remaining capacity corresponds to larger exponent values.
    # Let's consider the value `bins_remain_cap - item`. A smaller value of this
    # is better. So, let's use `-(bins_remain_cap - item)` or `item - bins_remain_cap`.
    # However, we must handle cases where `bins_remain_cap < item`.
    #
    # Let's reframe: we want to maximize `bins_remain_cap - item`.
    # So, for bins that can fit: `bins_remain_cap - item`.
    # For bins that cannot fit: a very small value.
    # Softmax works best with positive exponent values.
    # If we want smaller `bins_remain_cap - item` to be higher priority, we can
    # use `K - (bins_remain_cap - item)` where K is a large constant.
    # Let's use a simple and common approach:
    # If item fits (bins_remain_cap >= item), score = bins_remain_cap - item (smaller is better)
    # If item does not fit, score = -infinity

    # To make smaller remaining capacity map to higher priority with softmax,
    # we can transform the scores. A common transformation is to use a large constant minus
    # the remaining capacity or similar, or directly use negative values if we want
    # the smallest (least negative) values to have higher probabilities.

    # Let's define "fitness" as how much capacity is left. We want to minimize this.
    # So, for fitting bins: `fitness = bins_remain_cap - item`.
    # For non-fitting bins: `fitness = infinity`.
    # For softmax, we want larger values to be higher priority.
    # So, we can use `priorities = -fitness`.
    # `priorities = -(bins_remain_cap - item)` for fitting bins.
    # `priorities = -infinity` for non-fitting bins.
    # This will map `bins_remain_cap - item = 0` (perfect fit) to `priority = 0`.
    # And `bins_remain_cap - item = very large` to `priority = very small negative`.
    # This seems counter-intuitive for softmax where larger positive exponents are better.

    # Alternative approach for Softmax-based Best Fit:
    # A common strategy for "best fit" with softmax is to assign a score that is
    # inversely related to the remaining capacity.
    # For example, if item fits, score = 1 / (bins_remain_cap - item + epsilon)
    # If item does not fit, score = 0

    # Let's try a simpler transformation suitable for softmax:
    # We want the *smaller* the `bins_remain_cap - item`, the *higher* the priority.
    # So, we can use `-(bins_remain_cap - item)` as input to the softmax exponent.
    # For bins where `bins_remain_cap - item < 0`, we set the exponent to a very low value.
    # The `np.exp` function is sensitive to large negative inputs, mapping them to ~0.

    exponent_values = np.zeros_like(bins_remain_cap)
    can_fit_mask = bins_remain_cap >= item
    # For bins that can fit, we want smaller `remaining_capacity` to have higher priority.
    # So, we use the negative of the remaining capacity.
    # Smaller `bins_remain_cap - item` means a larger (less negative) value of `-(bins_remain_cap - item)`.
    exponent_values[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)

    # To avoid numerical instability with exp(very large negative numbers) for non-fitting bins,
    # we ensure they are sufficiently small. `np.exp(-1e9)` is close to 0.
    # Let's ensure exponent values for non-fitting bins are significantly smaller
    # than any possible value for fitting bins.
    # If the minimum remaining capacity for a fitting bin is 0, the min exponent is 0.
    # If we consider item size 1 and capacities [0.5, 0.5], they don't fit.
    # If we consider item size 0.1 and capacities [0.5, 0.5], remaining capacities are 0.4.
    # Exponent values would be -0.4. exp(-0.4) is ~0.67.
    # If capacities were [0.15, 0.15], remaining are 0.05. Exponents are -0.05. exp(-0.05) is ~0.95.
    # This seems to work for "best fit" where smaller remaining capacity is better.

    # Avoid exponentiating potentially very large positive numbers if item size is negative (which shouldn't happen)
    # or if bins_remain_cap is extremely large. We can cap the exponent values or use a robust softmax.
    # For typical BPP, item sizes and capacities are positive and reasonable.

    # Calculate the exponent terms for the softmax
    # A smaller `bins_remain_cap - item` should lead to a higher probability.
    # So, we use `-(bins_remain_cap - item)`.
    # For items that don't fit, `bins_remain_cap - item` is negative.
    # We want these to have a very low probability.
    # Setting exponent to a large negative number handles this.

    # Ensure that the maximum exponent is not excessively large, which might cause overflow in exp.
    # Also, ensure that the minimum exponent for fitting bins is not too small (very negative).
    # Let's cap the exponent values to a reasonable range, e.g., [-5, 5].
    # The difference in priorities should be discernible.

    # If `bins_remain_cap - item` is 0, exponent is 0, exp(0)=1. This is good.
    # If `bins_remain_cap - item` is large positive (poor fit), exponent is large negative, exp(large negative) ~ 0.
    # If `bins_remain_cap - item` is small positive (good fit), exponent is small negative.
    # This is still mapping good fits to small negative exponents.

    # Let's try the inverse logic: Higher priority for bins where `bins_remain_cap - item` is *small*.
    # This means `-(bins_remain_cap - item)` should be large.
    #
    # Consider the "closeness" to zero remaining capacity.
    # If `bins_remain_cap - item` is the difference.
    # We want the smallest non-negative difference.
    #
    # A direct mapping for Softmax "Best Fit" could be:
    # For bins where `bins_remain_cap >= item`: score = `C - (bins_remain_cap - item)` for a large C.
    # For bins where `bins_remain_cap < item`: score = `-infinity`.
    # Let's choose `C` such that `C - (bins_remain_cap - item)` remains positive and reasonably spread.
    # If `bins_remain_cap` can be large, `bins_remain_cap - item` can be large.
    #
    # A more numerically stable and conceptually cleaner approach for "best fit" is
    # often to consider how much capacity *would be wasted* or how *tight* the fit is.
    #
    # Let's use the negative of the remaining capacity as the input to exp.
    # `score = - (bins_remain_cap - item)` for valid fits.
    # `score = -infinity` for invalid fits.

    # Calculate the raw score, making invalid bins very undesirable.
    # The goal is to have the smallest `bins_remain_cap - item`.
    # This means `-(bins_remain_cap - item)` should be the largest (least negative or most positive).
    scores_for_softmax = np.full_like(bins_remain_cap, -np.inf)
    valid_indices = bins_remain_cap >= item
    if np.any(valid_indices):
        # We want to maximize `bins_remain_cap - item` to be close to zero (but not negative).
        # To use softmax, we want the argument to exp to be larger for better bins.
        # So, we can use `-(bins_remain_cap - item)`. A smaller (bins_remain_cap - item) gives a larger negative score.
        # This is still leading to mapping better fits to smaller exponent values.

        # Correct approach for Softmax Best Fit:
        # The value that we want to maximize is `- (bins_remain_cap - item)`, for bins where `bins_remain_cap - item >= 0`.
        # For bins where `bins_remain_cap - item < 0`, the value should be very small.
        # So, we want to maximize the value `bins_remain_cap - item` by turning it into
        # a preference for smaller values.
        # Let's use `-(bins_remain_cap - item)` for bins that fit.
        #
        # Example: item = 0.3, bins_remain_cap = [0.5, 0.8, 0.2]
        # Bin 0: can fit, remaining = 0.5 - 0.3 = 0.2. Score for softmax = -0.2
        # Bin 1: can fit, remaining = 0.8 - 0.3 = 0.5. Score for softmax = -0.5
        # Bin 2: cannot fit. Score for softmax = -inf
        #
        # Softmax: exp(-0.2) / (exp(-0.2) + exp(-0.5) + exp(-inf))
        #         exp(-0.2) / (exp(-0.2) + exp(-0.5) + 0)
        # This gives higher probability to the bin with remaining capacity 0.2 (Bin 0), which is the "best fit".
        # This seems correct.

        scores_for_softmax[valid_indices] = -(bins_remain_cap[valid_indices] - item)

    # Apply softmax. We need to handle the case where all scores are -inf (item too large for all bins).
    # In such a case, `np.exp` of all `-np.inf` is `0`. The sum would be `0`. Division by zero.
    # A common way to handle this is to add a small epsilon to the denominator or to check for all -inf.
    # However, `np.exp` of `-np.inf` results in `0.0`.
    # The sum of zeros would be zero. Let's ensure at least one non-infinite value if possible.
    # If `scores_for_softmax` contains `-np.inf`, `np.exp` turns them to `0.0`.
    # If all are `-np.inf`, the sum is `0.0`.
    # If at least one is finite, the sum will be non-zero.

    # Softmax implementation
    exp_scores = np.exp(scores_for_softmax)
    sum_exp_scores = np.sum(exp_scores)

    # Handle the case where the item cannot fit into any bin.
    # In this scenario, `scores_for_softmax` will be all `-np.inf`, `exp_scores` all `0.0`,
    # and `sum_exp_scores` will be `0.0`.
    if sum_exp_scores == 0:
        # If no bin can fit the item, return a uniform distribution of zero probability
        # or a default behavior (e.g., priority to the bin with most capacity, though
        # the problem states we select *a* bin for the item).
        # For this heuristic, if it cannot fit, it shouldn't be prioritized for fitting.
        # Returning zeros indicates no valid priority can be assigned by this heuristic.
        # A robust implementation might also try to create a new bin if this were the case.
        # For this function, we return zeros if no bin is a candidate.
        return np.zeros_like(bins_remain_cap)
    else:
        priorities = exp_scores / sum_exp_scores
        return priorities

```
