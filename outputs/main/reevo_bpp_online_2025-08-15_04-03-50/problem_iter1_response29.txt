```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.

    This strategy prioritizes bins that have a remaining capacity closely matching the item's size,
    aiming to minimize wasted space. The Softmax function is used to convert these "fits" into
    probabilities (priorities), where a better fit results in a higher priority.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Calculate the "fitness" for each bin: how well the item fits.
    # A good fit means the remaining capacity is just enough for the item.
    # We want to minimize (remaining_capacity - item_size).
    # However, we only consider bins where the item can actually fit (remaining_capacity >= item_size).
    # For bins where the item doesn't fit, we assign a very low fitness (or effectively zero priority).

    # Calculate potential remaining capacity after placing the item
    potential_remaining_caps = bins_remain_cap - item

    # For bins where the item fits (potential_remaining_caps >= 0)
    # we want to minimize the potential_remaining_caps (i.e., get closer to 0).
    # The "closeness" can be represented by the absolute value of potential_remaining_caps.
    # For bins where the item does not fit, we assign a large penalty or ensure they have zero priority.

    # Option 1: Simple "goodness of fit" where smaller leftover is better
    # For bins that can fit the item
    fits = bins_remain_cap - item
    # Only consider bins where the item fits
    valid_fits = fits[fits >= 0]
    invalid_indices = np.where(fits < 0)[0]

    # Transform fits into priorities using softmax.
    # We want higher priority for smaller 'fits' (less leftover space).
    # So, we can use negative of the fit, or simply the fit and let softmax handle it.
    # Using a "temperature" parameter can control the sharpness of the distribution.
    # A smaller temperature makes the distribution sharper (more emphasis on the best fit).
    # A larger temperature makes it flatter (more exploration).
    temperature = 1.0 # Can be tuned

    # For valid bins, the "value" for softmax can be the negative of the remaining capacity after fit.
    # This way, a smaller remaining capacity (closer fit) gets a larger positive value.
    # Alternatively, we can use the probability of fit directly, but that's less about priority for *that specific* bin.
    # Let's consider the remaining capacity itself as a factor, and penalize large remaining capacities.
    # A good fit is when bins_remain_cap is close to item.
    # So, we can use something like exp(-(bins_remain_cap - item) / temperature) for valid bins.
    # This gives higher values when bins_remain_cap - item is small.

    # Let's focus on the negative difference: if bins_remain_cap[i] - item is small and positive, that's good.
    # If it's large and positive, that's bad. If it's negative, it's impossible.
    # We can define a score where a smaller remaining capacity is better.
    # Score = - (bins_remain_cap - item) if item fits, else a very small number (or NaN for softmax handling).

    scores = np.full_like(bins_remain_cap, -np.inf) # Initialize with a very low score

    for i in range(len(bins_remain_cap)):
        if bins_remain_cap[i] >= item:
            # The "closeness" metric: how much space is left. We want to minimize this.
            # So, a smaller remaining capacity is better.
            # To use softmax, we want higher values for better "fits".
            # Let's use the negative of the remaining capacity *after* packing.
            # Smaller remaining capacity (good fit) leads to a less negative value, thus higher priority.
            scores[i] = -(bins_remain_cap[i] - item)

    # Apply Softmax to get probabilities (priorities)
    # Handle cases where all scores might be -inf (if item doesn't fit anywhere)
    if np.all(np.isinf(scores)):
        return np.zeros_like(bins_remain_cap) # Or handle as error/no placement

    # Avoid numerical instability with very large negative numbers
    # We can shift the scores by subtracting the maximum score before exp
    max_score = np.max(scores)
    shifted_scores = scores - max_score

    # Calculate the exponential of the shifted scores
    exp_scores = np.exp(shifted_scores / temperature)

    # Calculate the sum of exponentials for normalization
    sum_exp_scores = np.sum(exp_scores)

    # Calculate the final priorities (probabilities)
    if sum_exp_scores > 0:
        priorities = exp_scores / sum_exp_scores
    else:
        # This case should ideally not happen if scores are handled correctly
        # but as a fallback, assign equal probability if sum is zero.
        priorities = np.ones_like(bins_remain_cap) / len(bins_remain_cap)

    return priorities
```
