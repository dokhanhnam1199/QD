{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Sigmoid Fit Score with a penalty for very large capacities and exploration.\n\n    This version prioritizes bins that have a remaining capacity that is just\n    slightly larger than the item size. This aims to minimize wasted space in\n    the selected bin, leaving larger contiguous free spaces in other bins for\n    potentially larger future items.\n\n    The priority is calculated using a sigmoid function applied to the difference\n    between the bin's remaining capacity and the item's size. A smaller\n    positive difference (a tighter fit) results in a higher priority score.\n    Additionally, bins with very large remaining capacities (relative to the item)\n    are penalized to encourage spreading. A small probability of exploration is also included.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    exploration_prob = 0.05\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(fittable_bins_mask):\n        return priorities\n\n    fittable_bins_capacities = bins_remain_cap[fittable_bins_mask]\n    slack = fittable_bins_capacities - item\n\n    # Sigmoid for tight fits (prioritize near-zero slack)\n    k_fit = 5.0\n    tight_fit_score = 1.0 / (1.0 + np.exp(k_fit * slack))\n\n    # Penalty for very large remaining capacities.\n    # We want to discourage putting a small item into a very large bin if a tighter fit exists.\n    # This can be modeled with a logistic function that decreases as capacity grows.\n    # Let's use a threshold and a decay factor.\n    # We can normalize capacities relative to the item size or bin capacity.\n    # A simple approach: Penalize bins whose remaining capacity is much larger than the item.\n    # Let's create a score that is high for capacities close to the item size and decreases.\n    # We can use a similar sigmoid but inverted or a different function.\n    # Alternative: use 1 / (1 + exp(-k_large * (capacity - threshold)))\n    # A simpler penalty: if capacity is > C * item, reduce score.\n\n    # Let's try a penalty based on normalized slack.\n    # We want the penalty to be low for slack close to 0 and high for large slack.\n    # This is the opposite of the tight fit score.\n    # We can use a decaying function of slack.\n    # e.g., 1 / (1 + slack) or exp(-slack / scale)\n    # Let's try a sigmoid on the negative slack to penalize larger slack.\n    k_penalty = 0.5 # Controls how quickly the penalty increases with slack\n    large_capacity_penalty = 1.0 / (1.0 + np.exp(-k_penalty * slack))\n\n    # Combine scores. The tight_fit_score is high for small slack.\n    # The large_capacity_penalty is high for small slack, and low for large slack.\n    # We want to combine them such that:\n    # 1. Small slack (tight fit) is good -> high tight_fit_score\n    # 2. Large slack (too much space) is bad -> needs a penalty\n    # Let's try to combine them additively.\n    # A higher score for tight fits, and a lower score for large slack.\n    # The large_capacity_penalty goes from ~0.5 to ~1 as slack increases. This is not a penalty.\n    # Let's re-think the penalty. We want to penalize large slack.\n    # The sigmoid `1 / (1 + exp(k * slack))` already penalizes large slack.\n    # So maybe just use a weighted combination of the tight fit score and the slack itself.\n    # Or, use the slack directly, but inverted and scaled.\n\n    # Let's re-evaluate the reflection: \"penalize large remaining capacities\"\n    # The `tight_fit_score` already does this by giving low scores to large slack.\n    # Maybe the reflection implies we should *also* consider the absolute capacity.\n    # For example, putting an item of size 1 into a bin with remaining capacity 100\n    # is worse than putting it into a bin with remaining capacity 1.\n    # The slack approach (capacity - item) naturally handles this:\n    # slack for (100, 1) is 99, slack for (1, 1) is 0.\n\n    # Let's reconsider the \"Worst Fit\" element from v0 and combine it with tight fit.\n    # Best Fit component (tight_fit_score) -> prioritizes small slack\n    # Worst Fit component (prioritizes bins with more remaining capacity) -> prioritizes large capacity\n    # This seems contradictory. The reflection \"Focus on tighter fits, penalize large remaining capacities\"\n    # suggests we should prioritize small slack and penalize large slack. The `tight_fit_score` does this.\n\n    # Let's refine the \"penalty for very large remaining capacities\".\n    # A simple approach: normalize slack by some reference, or use a threshold.\n    # Let's say if remaining_capacity > 2 * item, we start penalizing.\n    # Max slack we want to consider for high priority: say, up to `max_slack_ideal`.\n    # Any slack beyond that should be heavily penalized.\n    max_slack_ideal = 2.0 * item # An item of size 'item' should ideally fit into a bin with 2*item remaining capacity at most for a good fit.\n    # Penalize slack if it's much larger than max_slack_ideal.\n    # We can use a linear penalty or a sigmoid that drops sharply.\n    # Let's use a sigmoid that is high for slack <= max_slack_ideal and low for slack > max_slack_ideal.\n    # Use `1 / (1 + exp(k_penalty * (slack - max_slack_ideal)))`\n    # This gives 0.5 at slack = max_slack_ideal, decreases for slack > max_slack_ideal.\n    k_penalty = 1.0 # Controls the steepness of the penalty\n    penalty_score = 1.0 / (1.0 + np.exp(k_penalty * (slack - max_slack_ideal)))\n\n    # Combine tight_fit_score and penalty_score.\n    # We want both to contribute positively.\n    # tight_fit_score is high for small slack.\n    # penalty_score is high for small slack (and slack <= max_slack_ideal).\n    # So, a linear combination might work.\n    # Let's weight them. Give more weight to the tight fit.\n    combined_scores = 0.7 * tight_fit_score + 0.3 * penalty_score\n\n    priorities[fittable_bins_mask] = combined_scores\n\n    # Apply exploration\n    if np.random.rand() < exploration_prob:\n        fittable_indices = np.where(fittable_bins_mask)[0]\n        chosen_bin_global_index = np.random.choice(fittable_indices)\n        priorities.fill(0)\n        priorities[chosen_bin_global_index] = 1.0\n    else:\n        # Normalize priorities for fittable bins to sum to 1\n        fittable_priorities = priorities[fittable_bins_mask]\n        if np.sum(fittable_priorities) > 0:\n            priorities[fittable_bins_mask] /= np.sum(fittable_priorities)\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a strategy that prioritizes bins with minimal remaining space after packing, and among those, favors bins that are fuller.\n\n    This strategy is a refinement of Best Fit.\n    1. Primary objective: Minimize the \"slack\" (remaining capacity - item size).\n       Bins with slack closer to zero are preferred.\n    2. Secondary objective: Among bins with the same slack, prefer bins that are fuller.\n       A fuller bin has less remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate slack for fittable bins\n    # Slack = remaining_capacity - item_size\n    slack = bins_remain_cap[can_fit_mask] - item\n\n    # Primary scoring: Minimize slack. We want to maximize a function that is\n    # large for small slack. Using 1.0 / (slack + epsilon) achieves this.\n    # A slack of 0 gives the highest score (1.0).\n    best_fit_score = 1.0 / (slack + 1e-9)\n\n    # Secondary scoring: Favor fuller bins. This means preferring bins with\n    # smaller remaining capacity among those with the same slack.\n    # We can achieve this by maximizing 1.0 / bins_remain_cap.\n    fuller_bin_score = 1.0 / (bins_remain_cap[can_fit_mask] + 1e-9)\n\n    # Combine scores. We want slack to be the primary driver.\n    # A simple way to combine is to add them, but to ensure slack is dominant,\n    # we can multiply by a factor that makes the slack score much larger.\n    # Alternatively, we can use a weighted sum where the slack component\n    # has a larger weight.\n    # Let's try a composite score where we consider slack first, and then remaining capacity.\n    # A common way to achieve lexicographical ordering is to use a base and scaled values.\n    # For example: score = PrimaryScore * Scale + SecondaryScore\n    # Here, we want to maximize `best_fit_score` and `fuller_bin_score`.\n    # If `best_fit_score` is the primary, we want it to have a higher impact.\n\n    # Let's use a weighting approach.\n    # Weight for best fit: Give it a higher weight to ensure it's considered first.\n    # Weight for fuller bin: Give it a smaller weight as it's a secondary preference.\n    # However, directly adding might lead to issues if the scales are very different.\n\n    # A robust way for \"minimize A, then minimize B\" is to maximize (-A - alpha*B).\n    # So, we want to maximize `-(slack) - alpha * (bins_remain_cap)`.\n    # This translates to maximizing `-(slack + alpha * bins_remain_cap)`.\n    # Let's use alpha = 1, and see how it performs.\n    # The priority will be `- (slack + bins_remain_cap[can_fit_mask])`.\n    # This means smaller values of `slack + bins_remain_cap` get higher priority.\n    # Example:\n    # Bin A: slack=0, rem_cap=5  => sum=5, priority = -5\n    # Bin B: slack=0, rem_cap=10 => sum=10, priority = -10\n    # Bin C: slack=1, rem_cap=5  => sum=6, priority = -6\n    # Bin D: slack=1, rem_cap=10 => sum=11, priority = -11\n    # This correctly prioritizes Bin A (best fit, fullest), then Bin C (good fit, fullest), then Bin B (best fit, less full), then Bin D.\n\n    # However, the reflection asked to \"prioritize fuller bins by inverting remaining capacity for better fit.\"\n    # This implies that `1.0 / bins_remain_cap` should contribute positively to the priority.\n    # So, we want to maximize `1.0 / (slack + epsilon)` AND maximize `1.0 / (bins_remain_cap + epsilon)`.\n    # Let's use a simple additive approach, ensuring the best-fit component is primary.\n    # We can scale the \"fuller bin\" score.\n    # `priorities[can_fit_mask] = best_fit_score + 0.1 * fuller_bin_score`\n    # The `0.1` is a heuristic weight. It makes the \"fuller bin\" score less influential than \"best fit\".\n    # For example, if slack is 0, BF score is 1. If rem_cap is 5, FB score is 0.2. Total 1.2.\n    # If slack is 0, rem_cap is 10, BF score is 1. FB score is 0.1. Total 1.1.\n    # This correctly prioritizes the fuller bin among those with the same slack.\n\n    # Let's consider the case where slack is small but not zero.\n    # slack=0.1, rem_cap=5. BF score = 1 / 1.1 = 0.909. FB score = 0.2. Total = 1.109.\n    # slack=0, rem_cap=10. BF score = 1.0. FB score = 0.1. Total = 1.1.\n    # In this case, the slack=0.1 bin is prioritized over the slack=0, fuller bin. This is not ideal.\n\n    # The reflection implies a stronger preference for fuller bins when slack is the same.\n    # Let's combine them in a way that ensures the primary objective (minimal slack) is paramount,\n    # and the secondary objective (fuller bins) breaks ties effectively.\n\n    # A common way is to sort by (slack, bins_remain_cap). We want the smallest such pair.\n    # For priority, we want to maximize a score reflecting this.\n    # So, we want to maximize `(-slack, -bins_remain_cap)`.\n    # We can achieve this by converting to a single score: `-slack - alpha * bins_remain_cap`.\n    # If alpha is chosen large enough, `-slack` dominates.\n    # Let's use `alpha = 1`. The score is `-(slack + bins_remain_cap[can_fit_mask])`.\n    # This means the highest priority (least negative) is for the smallest `slack + bins_remain_cap`.\n\n    # Example:\n    # Bin A: slack=0, rem_cap=5  => sum=5, priority = -5\n    # Bin B: slack=0, rem_cap=10 => sum=10, priority = -10\n    # Bin C: slack=1, rem_cap=5  => sum=6, priority = -6\n    # Bin D: slack=1, rem_cap=10 => sum=11, priority = -11\n    # This seems to match the desired behavior: A is best, then C, then B, then D.\n    # This priority function directly implements minimizing slack first, then minimizing remaining capacity.\n\n    priorities[can_fit_mask] = -(slack + bins_remain_cap[can_fit_mask])\n\n    return priorities\n\n[Reflection]\nPrioritize tight fits and fuller bins. Combine objectives carefully for desired impact.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}