```python
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a refined Sigmoid Fit Score with a penalty for very large capacities and exploration.

    This version prioritizes bins that have a remaining capacity that is just
    slightly larger than the item size. This aims to minimize wasted space in
    the selected bin, leaving larger contiguous free spaces in other bins for
    potentially larger future items.

    The priority is calculated using a sigmoid function applied to the difference
    between the bin's remaining capacity and the item's size. A smaller
    positive difference (a tighter fit) results in a higher priority score.
    Additionally, bins with very large remaining capacities (relative to the item)
    are penalized to encourage spreading. A small probability of exploration is also included.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Returns:
        Array of same size as bins_remain_cap with priority score of each bin.
        Bins that cannot fit the item will have a priority of 0.
    """
    exploration_prob = 0.05
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    fittable_bins_mask = bins_remain_cap >= item

    if not np.any(fittable_bins_mask):
        return priorities

    fittable_bins_capacities = bins_remain_cap[fittable_bins_mask]
    slack = fittable_bins_capacities - item

    # Sigmoid for tight fits (prioritize near-zero slack)
    k_fit = 5.0
    tight_fit_score = 1.0 / (1.0 + np.exp(k_fit * slack))

    # Penalty for very large remaining capacities.
    # We want to discourage putting a small item into a very large bin if a tighter fit exists.
    # This can be modeled with a logistic function that decreases as capacity grows.
    # Let's use a threshold and a decay factor.
    # We can normalize capacities relative to the item size or bin capacity.
    # A simple approach: Penalize bins whose remaining capacity is much larger than the item.
    # Let's create a score that is high for capacities close to the item size and decreases.
    # We can use a similar sigmoid but inverted or a different function.
    # Alternative: use 1 / (1 + exp(-k_large * (capacity - threshold)))
    # A simpler penalty: if capacity is > C * item, reduce score.

    # Let's try a penalty based on normalized slack.
    # We want the penalty to be low for slack close to 0 and high for large slack.
    # This is the opposite of the tight fit score.
    # We can use a decaying function of slack.
    # e.g., 1 / (1 + slack) or exp(-slack / scale)
    # Let's try a sigmoid on the negative slack to penalize larger slack.
    k_penalty = 0.5 # Controls how quickly the penalty increases with slack
    large_capacity_penalty = 1.0 / (1.0 + np.exp(-k_penalty * slack))

    # Combine scores. The tight_fit_score is high for small slack.
    # The large_capacity_penalty is high for small slack, and low for large slack.
    # We want to combine them such that:
    # 1. Small slack (tight fit) is good -> high tight_fit_score
    # 2. Large slack (too much space) is bad -> needs a penalty
    # Let's try to combine them additively.
    # A higher score for tight fits, and a lower score for large slack.
    # The large_capacity_penalty goes from ~0.5 to ~1 as slack increases. This is not a penalty.
    # Let's re-think the penalty. We want to penalize large slack.
    # The sigmoid `1 / (1 + exp(k * slack))` already penalizes large slack.
    # So maybe just use a weighted combination of the tight fit score and the slack itself.
    # Or, use the slack directly, but inverted and scaled.

    # Let's re-evaluate the reflection: "penalize large remaining capacities"
    # The `tight_fit_score` already does this by giving low scores to large slack.
    # Maybe the reflection implies we should *also* consider the absolute capacity.
    # For example, putting an item of size 1 into a bin with remaining capacity 100
    # is worse than putting it into a bin with remaining capacity 1.
    # The slack approach (capacity - item) naturally handles this:
    # slack for (100, 1) is 99, slack for (1, 1) is 0.

    # Let's reconsider the "Worst Fit" element from v0 and combine it with tight fit.
    # Best Fit component (tight_fit_score) -> prioritizes small slack
    # Worst Fit component (prioritizes bins with more remaining capacity) -> prioritizes large capacity
    # This seems contradictory. The reflection "Focus on tighter fits, penalize large remaining capacities"
    # suggests we should prioritize small slack and penalize large slack. The `tight_fit_score` does this.

    # Let's refine the "penalty for very large remaining capacities".
    # A simple approach: normalize slack by some reference, or use a threshold.
    # Let's say if remaining_capacity > 2 * item, we start penalizing.
    # Max slack we want to consider for high priority: say, up to `max_slack_ideal`.
    # Any slack beyond that should be heavily penalized.
    max_slack_ideal = 2.0 * item # An item of size 'item' should ideally fit into a bin with 2*item remaining capacity at most for a good fit.
    # Penalize slack if it's much larger than max_slack_ideal.
    # We can use a linear penalty or a sigmoid that drops sharply.
    # Let's use a sigmoid that is high for slack <= max_slack_ideal and low for slack > max_slack_ideal.
    # Use `1 / (1 + exp(k_penalty * (slack - max_slack_ideal)))`
    # This gives 0.5 at slack = max_slack_ideal, decreases for slack > max_slack_ideal.
    k_penalty = 1.0 # Controls the steepness of the penalty
    penalty_score = 1.0 / (1.0 + np.exp(k_penalty * (slack - max_slack_ideal)))

    # Combine tight_fit_score and penalty_score.
    # We want both to contribute positively.
    # tight_fit_score is high for small slack.
    # penalty_score is high for small slack (and slack <= max_slack_ideal).
    # So, a linear combination might work.
    # Let's weight them. Give more weight to the tight fit.
    combined_scores = 0.7 * tight_fit_score + 0.3 * penalty_score

    priorities[fittable_bins_mask] = combined_scores

    # Apply exploration
    if np.random.rand() < exploration_prob:
        fittable_indices = np.where(fittable_bins_mask)[0]
        chosen_bin_global_index = np.random.choice(fittable_indices)
        priorities.fill(0)
        priorities[chosen_bin_global_index] = 1.0
    else:
        # Normalize priorities for fittable bins to sum to 1
        fittable_priorities = priorities[fittable_bins_mask]
        if np.sum(fittable_priorities) > 0:
            priorities[fittable_bins_mask] /= np.sum(fittable_priorities)

    return priorities
```
