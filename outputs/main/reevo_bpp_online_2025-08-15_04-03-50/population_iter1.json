[
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined First Fit strategy.\n\n    This heuristic prioritizes bins that can accommodate the item and have a remaining capacity\n    that is \"just enough\" or slightly larger than the item. This aims to leave larger gaps in\n    other bins for potentially larger future items, a common strategy in First Fit variants.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Only consider bins that can actually fit the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    # For suitable bins, calculate priority.\n    # A bin with remaining capacity slightly larger than the item gets a higher priority.\n    # We can achieve this by looking at the difference (remaining_capacity - item).\n    # A smaller positive difference is preferred.\n    # We invert this difference to make smaller positive differences have higher priority.\n    # To handle bins that are exactly the right fit (difference = 0), we can add a small epsilon\n    # or simply handle the zero case.\n    # Let's use a score that rewards fitting the item as snugly as possible.\n    # A bin with remaining_capacity = item + delta, where delta is small and positive, is good.\n    # We can calculate (remaining_capacity - item). A smaller non-negative value is good.\n    # To turn this into a higher priority score, we can do something like 1 / (1 + diff).\n    # Or, we can use the negative of the difference, and add a large constant to ensure\n    # positive scores for fitting bins.\n\n    suitable_bin_capacities = bins_remain_cap[suitable_bins_mask]\n    differences = suitable_bin_capacities - item\n\n    # Priority: higher for smaller non-negative differences\n    # We can map differences to priorities. For example, differences of 0 are most preferred.\n    # Differences slightly larger than 0 are next most preferred.\n    # Using the negative difference, and a large offset to keep priorities positive.\n    # A large offset ensures that even if differences are large negative (which won't happen due to mask),\n    # the priority is still somewhat meaningful. The key is the relative ordering for suitable bins.\n\n    # Example mapping:\n    # diff = 0   -> priority ~ MAX_PRIORITY\n    # diff = 0.1 -> priority ~ MAX_PRIORITY - 0.1\n    # diff = 1.0 -> priority ~ MAX_PRIORITY - 1.0\n\n    # Let's set a base priority for fitting bins and reduce it based on the \"waste\" if the item fits.\n    # If item fits exactly, remaining_cap - item = 0.\n    # If item fits with some room, remaining_cap - item > 0.\n    # The goal is to minimize remaining_cap - item for suitable bins.\n\n    # Priority = 1 / (1 + difference) might work, but it gives very low priority to bins with\n    # larger remaining capacities, even if they fit. We want to favor those that fit \"best\".\n\n    # Let's assign a high base priority to any bin that can fit the item, and then\n    # give a bonus based on how tightly it fits.\n    base_priority_for_fitting_bins = 1000.0\n    # A smaller \"slack\" (remaining_cap - item) should result in a higher bonus.\n    # We can use the negative of the slack as a bonus, so smaller slack is a larger negative slack,\n    # which when negated becomes a larger positive bonus.\n    bonus_for_tight_fit = -differences\n\n    # Assign priorities to suitable bins\n    priorities[suitable_bins_mask] = base_priority_for_fitting_bins + bonus_for_tight_fit\n\n    # Note: This heuristic might be overly aggressive in trying to find the \"perfect\" fit\n    # for every item, potentially leading to fragmentation if not carefully tuned or if\n    # the definition of \"tight fit\" is too strict. However, it aims to be more intelligent\n    # than a simple \"can fit\" by considering the degree of fit.\n    # An alternative could be: priority = 1 / (item + epsilon) if bin_remain_cap[i] >= item else 0\n    # to prioritize bins that have more capacity left *after* placing the item.\n\n    # Let's refine it: prioritize bins where the remaining capacity is closest to the item size.\n    # We want `bins_remain_cap[i] - item` to be small and non-negative.\n    # The smaller this value, the higher the priority.\n    # For bins where `bins_remain_cap[i] < item`, the priority is 0.\n\n    # Calculate the absolute difference from being a perfect fit, for suitable bins.\n    # We want to minimize this difference.\n    # abs_diff_from_perfect_fit = np.abs(bins_remain_cap[suitable_bins_mask] - item)\n    # For bins that can fit, the priority should be inversely proportional to this difference.\n    # A small difference means high priority.\n\n    # Let's re-think: First Fit's strength is simplicity and quick decision.\n    # A priority function should still reflect that.\n    # What if we give a higher priority to bins that are \"just large enough\" or \"slightly too large\"?\n    # This is what v2 tried to do.\n\n    # Let's try another approach that emphasizes \"best fit\" within the First Fit paradigm.\n    # Prioritize bins that have *just enough* capacity.\n    # This means bins where `bins_remain_cap[i]` is between `item` and `item + margin` for some small `margin`.\n    # Bins that are much larger might be better saved for larger items.\n\n    # For bins that can fit the item:\n    # Priority = 1 if bin_remain_cap[i] == item\n    # Priority = 0.9 if item < bin_remain_cap[i] <= item + epsilon\n    # Priority = 0.8 if item + epsilon < bin_remain_cap[i] <= item + 2*epsilon\n    # ... and so on, decreasing priority as remaining capacity increases.\n\n    # Let's simplify: give highest priority to bins that fit the item exactly.\n    # Then, progressively lower priority for bins that fit with more \"slack\".\n    # Let's define \"slack\" as `bins_remain_cap[i] - item`.\n    # We want to minimize slack.\n\n    # Assign a base priority to all bins that can fit.\n    # For those that fit, add a penalty based on the slack. The smaller the slack, the less penalty (higher priority).\n    base_priority = 1.0  # A default for any bin that *could* fit if we only cared about fitting\n    penalty_scale = 0.1 # Controls how quickly priority drops with slack\n\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            slack = bins_remain_cap[i] - item\n            # We want smaller slack to mean higher priority.\n            # So, priority is inversely related to slack.\n            # A simple way is: base_priority - slack * penalty_scale\n            # This gives higher priority to bins with smaller remaining capacity *after* placing the item.\n            priorities[i] = base_priority - slack * penalty_scale\n            # Ensure priorities are not negative if slack is very large (though unlikely to be chosen anyway)\n            priorities[i] = max(priorities[i], 0.0)\n        else:\n            priorities[i] = 0.0 # Cannot fit, so zero priority\n\n    # This strategy prioritizes bins that leave the least remaining capacity after placing the item.\n    # This is a common \"best fit\" inspired idea adapted to a priority queue context.\n    # The \"First Fit\" aspect is implicit in that we consider *all* bins currently open.\n\n    # Consider the \"first fit\" intuition more directly: place it in the first bin it fits.\n    # The priority here is about *which* of the available bins that fit should be preferred.\n    # This v2 attempts to \"smartly\" pick among the fitting bins.\n\n    # Let's consider a scenario:\n    # bins_remain_cap = [10, 5, 8, 2]\n    # item = 3\n    #\n    # Bin 0: fits (10 >= 3). Slack = 10 - 3 = 7. Priority = 1.0 - 7*0.1 = 0.3\n    # Bin 1: fits (5 >= 3). Slack = 5 - 3 = 2. Priority = 1.0 - 2*0.1 = 0.8\n    # Bin 2: fits (8 >= 3). Slack = 8 - 3 = 5. Priority = 1.0 - 5*0.1 = 0.5\n    # Bin 3: cannot fit (2 < 3). Priority = 0.0\n    #\n    # Priorities: [0.3, 0.8, 0.5, 0.0]\n    # Max priority is for Bin 1, which has the smallest remaining capacity that still fits the item.\n\n    # This heuristic implements a \"best fit\" type of selection among available bins.\n    # If the goal is strictly First Fit (pick the first one encountered), then a priority function\n    # doesn't make sense unless we're re-ordering the bins *before* processing.\n    # Given the prompt asks for a priority function to select a bin, it implies we are choosing\n    # among valid bins.\n\n    # Let's add a small epsilon to avoid zero slack getting the same priority as slack close to zero.\n    # For better discrimination.\n    epsilon_for_slack = 0.01\n    priorities_refined = np.zeros_like(bins_remain_cap, dtype=float)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            slack = bins_remain_cap[i] - item\n            # Higher priority for smaller slack.\n            # Add 1 to slack to ensure a non-zero denominator and that perfect fit has high priority.\n            # The smaller slack, the larger this priority value will be.\n            # Add a small value to distinguish between identical slacks if needed, but np.maximum helps.\n            priorities_refined[i] = 1.0 / (slack + epsilon_for_slack)\n        else:\n            priorities_refined[i] = 0.0\n\n    # Example with refined:\n    # bins_remain_cap = [10, 5, 8, 2]\n    # item = 3\n    #\n    # Bin 0: slack = 7. Priority = 1.0 / (7 + 0.01) = 1.0 / 7.01 ~ 0.142\n    # Bin 1: slack = 2. Priority = 1.0 / (2 + 0.01) = 1.0 / 2.01 ~ 0.497\n    # Bin 2: slack = 5. Priority = 1.0 / (5 + 0.01) = 1.0 / 5.01 ~ 0.199\n    # Bin 3: Priority = 0.0\n    #\n    # Priorities: [0.142, 0.497, 0.199, 0.0]\n    # This also prioritizes the bin that fits most snugly.\n\n    # This version is a good candidate for `priority_v2` as it refines the strategy.\n    return priorities_refined",
    "response_id": 0,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Command '['python3', '-u', '/home/dokhanhnam1199/QD/problems/bpp_online/eval.py', '5000', '/home/dokhanhnam1199/QD', 'train']' timed out after 49.999949913006276 seconds"
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit strategy.\n\n    The Best Fit strategy aims to place the item into the bin where it leaves the\n    least amount of remaining capacity. This is achieved by prioritizing bins\n    that have just enough space for the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    # Calculate the remaining capacity if the item is placed in each bin.\n    # Only consider bins that have enough capacity to hold the item.\n    potential_remaining_cap = bins_remain_cap - item\n    \n    # Bins that cannot fit the item should have a priority of 0.\n    # We can achieve this by setting their priority to a very low negative value\n    # or by filtering them out. Here, we assign 0 priority to bins that cannot fit.\n    can_fit_mask = bins_remain_cap >= item\n    \n    # The Best Fit strategy prioritizes bins that result in the *smallest*\n    # positive remaining capacity. This means we want to minimize (bins_remain_cap - item).\n    # A higher priority should be given to smaller values of (bins_remain_cap - item).\n    # To map smaller (bins_remain_cap - item) to higher priorities, we can use\n    # the negative of these values, or a function that is inversely related.\n    #\n    # A common way to implement \"best fit\" is to calculate the \"waste\"\n    # (bins_remain_cap - item) and then select the bin with the minimum waste.\n    # For a priority function, we want the highest priority for the minimum waste.\n    #\n    # Consider the following:\n    # If we want to minimize `waste = bins_remain_cap - item`, then\n    # we want the largest `priority = -waste`.\n    #\n    # So, for bins where `can_fit_mask` is True, the priority is `-(bins_remain_cap - item)`.\n    # For bins where `can_fit_mask` is False, the priority is 0.\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # For bins that can fit the item, calculate the priority.\n    # The priority is the negative of the remaining capacity after fitting the item.\n    # This ensures that bins with less remaining space (better fit) get higher priority.\n    # We invert the concept: smaller positive differences are better.\n    # `priorities = -(bins_remain_cap - item)` would work, but a slightly adjusted\n    # approach can be more robust if we want to differentiate more clearly between\n    # \"perfect fits\" and \"almost perfect fits\".\n    #\n    # A simple way to achieve \"best fit\" priority is to assign higher priority\n    # to the smallest positive difference.\n    # Let's assign priority as 1 / (1 + remaining_capacity_after_fit) for fitting bins.\n    # This maps smaller remaining capacities to larger priority values.\n    # Adding 1 in the denominator prevents division by zero and scales priorities.\n    \n    # We want to maximize `bins_remain_cap - item` to be as close to 0 as possible,\n    # for the bins that can fit the item.\n    # So, if `bins_remain_cap[i] >= item`, we want a high priority for small `bins_remain_cap[i] - item`.\n    # A good heuristic for this is to use `1.0 / (1.0 + (bins_remain_cap[i] - item))`\n    # This maps the smallest non-negative difference to the highest priority (close to 1).\n    # For example, if remaining cap is 10 and item is 7, difference is 3. Priority = 1/(1+3) = 0.25\n    # If remaining cap is 10 and item is 9, difference is 1. Priority = 1/(1+1) = 0.5\n    # If remaining cap is 10 and item is 10, difference is 0. Priority = 1/(1+0) = 1.0\n    \n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Calculate the \"cost\" or \"waste\" for fitting bins\n    waste = fitting_bins_remain_cap - item\n    \n    # Assign priorities: higher priority for smaller waste.\n    # We use 1 / (1 + waste) to ensure priorities are between 0 and 1,\n    # and smaller waste gets a higher score.\n    priorities[can_fit_mask] = 1.0 / (1.0 + waste)\n    \n    # For bins that cannot fit the item, their priority remains 0,\n    # which means they will not be chosen if any bin can fit the item.\n    \n    return priorities",
    "response_id": 1,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 1.0,
    "halstead": 49.82892142331044,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response2.txt_stdout.txt",
    "code_path": "problem_iter1_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit.\n\n    Worst Fit strategy prioritizes bins that have the most remaining capacity,\n    aiming to leave smaller gaps that might be harder to fill later.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # In Worst Fit, we want to choose the bin with the most remaining capacity.\n    # However, we can only place the item if it fits.\n    # A simple way to represent \"highest priority\" is to assign a score equal\n    # to the remaining capacity, but only for bins that can accommodate the item.\n    # For bins that cannot accommodate the item, the priority should be zero\n    # or negative infinity to ensure they are not chosen.\n    \n    # Create a copy to avoid modifying the original array\n    priorities = np.copy(bins_remain_cap)\n    \n    # Set priority to 0 for bins that cannot fit the item\n    priorities[bins_remain_cap < item] = 0.0\n    \n    # The higher the remaining capacity, the higher the priority\n    # We can optionally add a small constant to ensure positive priorities if needed,\n    # or scale them if a specific range is desired, but for direct comparison,\n    # remaining capacity works.\n    \n    # To make it more robust and less sensitive to the exact capacity values,\n    # we can introduce a slight penalty for very large remaining capacities\n    # which might encourage filling up bins more evenly if that's a secondary goal,\n    # but for pure Worst Fit, the remaining capacity itself is the priority.\n    # For this implementation, we stick to the core Worst Fit principle.\n    \n    return priorities",
    "response_id": 2,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 1.0,
    "halstead": 4.754887502163469,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response3.txt_stdout.txt",
    "code_path": "problem_iter1_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    The strategy prioritizes bins that have just enough capacity to fit the item,\n    with a preference for bins that will have less remaining capacity after the item is added.\n    This aims to fill bins more tightly, potentially reducing the total number of bins used.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate the remaining capacity if the item is placed in each bin\n    potential_remaining_cap = bins_remain_cap - item\n\n    # We only consider bins where the item can actually fit\n    # For bins that cannot fit the item, their priority will remain 0 (or be set to a very low value if we wanted to explicitly exclude them)\n    fit_mask = bins_remain_cap >= item\n\n    # The \"Almost Full Fit\" strategy prioritizes bins that will be \"almost full\" after packing.\n    # This means bins with the smallest remaining capacity after placing the item.\n    # We can approximate this by penalizing bins with larger remaining capacity.\n    # A simple approach is to use the negative of the remaining capacity as a priority score.\n    # We want higher scores for bins that become \"more full\", meaning smaller remaining capacity.\n\n    # Let's also introduce a small bias towards bins that have *just* enough space,\n    # but not so much that they are \"too empty\".\n    # The idea is to pick bins that are \"tight fits\" for the current item.\n    # We can achieve this by looking at the 'slack' (bins_remain_cap - item).\n    # Smaller slack is better for 'almost full fit'.\n\n    # We can combine these ideas. A bin that fits the item should have a positive priority.\n    # The primary driver is minimizing the remaining capacity after adding the item.\n    # Therefore, bins with smaller `potential_remaining_cap` should have higher priority.\n    # So, we can use `-potential_remaining_cap` for bins that fit.\n\n    # To ensure that bins that *barely* fit (small slack) are preferred over bins\n    # that have a lot of extra space but could still fit the item, we can also consider\n    # the slack. A bin that has exactly `item` remaining capacity (slack = 0) is ideal.\n    # Bins with very small positive slack are also good candidates.\n\n    # Let's try a scoring mechanism:\n    # Score = (MaxPossibleSlack - Slack)\n    # where MaxPossibleSlack is the maximum slack among all bins that can fit the item.\n    # This would give higher scores to bins with less slack.\n    # If multiple bins have the same minimum slack, we can break ties by\n    # choosing the bin that has more remaining capacity overall (a slightly less full bin)\n    # to potentially save some very tight fits for later. This latter part is a bit counter-intuitive\n    # to pure \"almost full fit\" but can be a useful heuristic.\n\n    # A more direct interpretation of \"Almost Full Fit\" prioritizes bins that, after placing the item,\n    # have the smallest remaining capacity. So, we directly use the negative of the remaining capacity\n    # as a priority.\n\n    # If a bin can fit the item, its priority is the inverse of the remaining capacity after insertion.\n    # Lower remaining capacity (closer to zero) should yield a higher priority.\n    # So, we use `-potential_remaining_cap`.\n    # We only assign priorities to bins that can fit the item.\n    priorities[fit_mask] = -potential_remaining_cap[fit_mask]\n\n    # Optionally, to further refine for \"almost full\" and avoid very large bins being chosen\n    # if they happen to have a similar negative remaining capacity, we could also factor in\n    # the original remaining capacity or the \"slack\" (bins_remain_cap - item).\n    # For a pure \"almost full\" feeling, bins that will have capacity 0 or very close to 0 are best.\n\n    # A common way to implement \"Almost Full Fit\" is to prioritize bins\n    # that have the minimum remaining capacity *after* the item is placed,\n    # provided the item fits.\n    # So, we want to maximize `-(bins_remain_cap - item)` for valid bins.\n\n    # To make it robust, we can ensure very large capacities that would fit don't get a high priority\n    # unless they become \"almost full\" relative to other bins.\n    # A simple approach that captures \"almost full\" is to use the remaining capacity as the priority value.\n    # However, this would prioritize bins that are *already* almost full and might not fit the item well.\n\n    # The prompt suggests \"the bin with the highest priority score will be selected\".\n    # So, if we want bins that become *most* full, we want the smallest `potential_remaining_cap`.\n    # This translates to maximizing `-potential_remaining_cap`.\n\n    # Let's reconsider the \"Almost Full Fit\" strategy's intent:\n    # \"Prioritize bins that will be 'almost full' after packing.\"\n    # This implies we want `potential_remaining_cap` to be small.\n    # If we use `potential_remaining_cap` directly, we'd pick the bin that ends up *most* empty (highest remaining capacity).\n    # If we use `-potential_remaining_cap`, we'd pick the bin that ends up *most* full (lowest remaining capacity).\n    # This seems to align with the goal.\n\n    # However, let's also consider what might happen if `potential_remaining_cap` is negative\n    # (i.e., item doesn't fit). These bins should have very low priority.\n    # Our `fit_mask` handles this by not assigning positive priorities.\n    # But what if multiple bins *can* fit the item, but one results in `-0.1` remaining\n    # and another results in `-5` remaining? The `-5` bin is more \"almost full\".\n\n    # A common way to interpret \"Almost Full Fit\" is to look at bins where\n    # `bins_remain_cap - item` is small and non-negative.\n    # If `bins_remain_cap - item` is negative, the item doesn't fit, so priority is 0.\n    # If `bins_remain_cap - item` is 0, it's a perfect fit.\n    # If `bins_remain_cap - item` is small and positive, it's an \"almost full\" fit.\n    # We want to prioritize smaller non-negative `bins_remain_cap - item` values.\n    # So, we can assign priority as `1.0 / (1 + (bins_remain_cap - item))` for bins that fit,\n    # where `bins_remain_cap - item` is non-negative.\n    # This way, a slack of 0 gives a priority of 1.0, a slack of 1 gives 0.5, a slack of 10 gives ~0.09.\n    # This favors smaller slacks.\n\n    # Let's use a strategy that directly reflects \"almost full\".\n    # We want to minimize `bins_remain_cap - item` for valid bins.\n    # The priority should be inversely related to this value.\n    # Higher priority for smaller non-negative `bins_remain_cap - item`.\n\n    # Calculate slack for bins where the item fits\n    slack = bins_remain_cap[fit_mask] - item\n\n    # Assign priority: Higher priority for smaller slack (closer to 0).\n    # We add a small epsilon to avoid division by zero if slack is exactly 0,\n    # though `1 / (1 + slack)` naturally handles slack=0 as 1.\n    # The `1 +` in the denominator ensures we don't get infinite priority for perfect fits\n    # and also that priorities are positive and bounded.\n    # A bin with slack 0 gets priority 1.0.\n    # A bin with slack 1 gets priority 1/2 = 0.5.\n    # A bin with slack 10 gets priority 1/11 ~ 0.09.\n    priorities[fit_mask] = 1.0 / (1.0 + slack)\n\n    # To make it more \"almost full\" focused, we could slightly boost\n    # bins that have *very little* remaining capacity after insertion,\n    # even if that remaining capacity is negative (i.e., the item doesn't fit perfectly).\n    # However, the prompt is about selecting a bin for an item, so items that don't fit\n    # should naturally have no priority.\n    # The current implementation focuses on minimizing slack for fitting bins.\n\n    # An alternative interpretation of \"Almost Full Fit\":\n    # Prioritize bins that, after fitting the item, leave the *least* amount of remaining capacity.\n    # This means we want to maximize `-potential_remaining_cap`.\n    # Let's stick to the idea of prioritizing smaller non-negative slacks.\n\n    # Final Check:\n    # If item fits in bins A and B:\n    # Bin A: bins_remain_cap=5, item=3  => slack=2, priority = 1/(1+2) = 0.33\n    # Bin B: bins_remain_cap=4, item=3  => slack=1, priority = 1/(1+1) = 0.5\n    # Bin C: bins_remain_cap=3, item=3  => slack=0, priority = 1/(1+0) = 1.0\n    # Bin D: bins_remain_cap=2, item=3  => does not fit, priority = 0\n    # Bin E: bins_remain_cap=6, item=3  => slack=3, priority = 1/(1+3) = 0.25\n\n    # This heuristic favors bins that have minimal unused space after packing the item.\n    # This is a reasonable implementation of \"Almost Full Fit\".\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 60.94436251225966,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response4.txt_stdout.txt",
    "code_path": "problem_iter1_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    The Exact Fit First strategy prioritizes bins that have just enough remaining\n    capacity to fit the current item. This aims to minimize wasted space by\n    filling bins as precisely as possible.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"exact fit\" value. This is the remaining capacity\n    # that would perfectly fit the item. A value of 0 indicates an exact fit.\n    # We add a small epsilon to avoid division by zero and to slightly penalize\n    # bins that are exactly full before considering other factors.\n    exact_fit_diff = bins_remain_cap - item\n\n    # The priority is inversely proportional to the remaining capacity after\n    # placing the item, but only for bins where the item fits.\n    # A smaller remaining capacity after placement indicates a better fit.\n    # We use a large number (1000) as a base priority for fitting bins,\n    # and subtract the remaining capacity to prioritize smaller remainders.\n    # We also add a penalty for bins that are too small to fit the item.\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            # Prioritize bins that result in less remaining capacity\n            # This means the bin is filled more \"exactly\"\n            priorities[i] = 1000 - exact_fit_diff[i]\n        else:\n            # Heavily penalize bins that cannot fit the item\n            priorities[i] = -1\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 34.86917501586544,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response5.txt_stdout.txt",
    "code_path": "problem_iter1_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance strategy.\n\n    The Inverse Distance (Proximity Fit) strategy prioritizes bins that have\n    a remaining capacity closest to the item's size. This aims to minimize\n    wasted space in bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate a higher priority. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Find bins that can accommodate the item\n    available_bins_mask = bins_remain_cap >= item\n    \n    # For available bins, calculate priority based on inverse distance\n    # Inverse distance = 1 / (remaining_capacity - item_size + epsilon)\n    # Adding epsilon to avoid division by zero if remaining_capacity == item_size\n    epsilon = 1e-9\n    available_bins_remain_cap = bins_remain_cap[available_bins_mask]\n    \n    # Calculate the difference between remaining capacity and item size\n    differences = available_bins_remain_cap - item\n    \n    # Calculate inverse distance. Higher difference (more wasted space) means lower priority.\n    # We want bins closer to fitting the item, so smaller differences are better.\n    # A simple inverse relationship might be 1 / (difference + epsilon).\n    # However, to prioritize bins *closest* to fitting, we want the smallest positive difference.\n    # A larger (remaining_cap - item) means more wasted space.\n    # Thus, the \"distance\" to a perfect fit is (remaining_cap - item).\n    # We want to minimize this distance. The inverse distance heuristic means\n    # a smaller positive difference should yield a higher score.\n    \n    # For proximity fit, we want the bin where remaining_capacity is closest to item size.\n    # This means (remaining_capacity - item_size) should be minimized and non-negative.\n    # The \"distance\" to the perfect fit is (remaining_capacity - item_size).\n    # We want to prioritize bins with small positive distances.\n    # A simple inverse distance would be 1 / (distance + epsilon).\n    # If distance is 0, priority is 1/epsilon (very high).\n    # If distance is large, priority is small.\n    \n    # Let's use a simple inverse: 1 / (distance + small_value)\n    priorities[available_bins_mask] = 1.0 / (differences + epsilon)\n    \n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 41.51317942364757,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response6.txt_stdout.txt",
    "code_path": "problem_iter1_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements the Sigmoid Fit Score strategy for online Bin Packing Problem.\n\n    This heuristic prioritizes bins based on how well an item fits, using a sigmoid\n    function to smooth the preference. Bins that can accommodate the item with\n    a remaining capacity close to the item's size (but not too small) are\n    given higher priority.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as bins_remain_cap, where each element\n        represents the priority score for placing the item into the corresponding bin.\n        Higher scores indicate a higher priority.\n    \"\"\"\n    # We want to prioritize bins where the remaining capacity is just enough for the item.\n    # A good candidate for 'just enough' is when remaining_capacity is close to item_size.\n    #\n    # Let's define a 'target' remaining capacity. If remaining_capacity is much larger\n    # than item_size, it's not ideal (waste of space). If remaining_capacity is smaller\n    # than item_size, it's not a valid fit.\n    #\n    # We can model this with a sigmoid function. The sigmoid function is good at\n    # mapping values to a range between 0 and 1, and it has an \"S\" shape.\n    #\n    # We'll shift and scale the remaining capacities so that the \"sweet spot\"\n    # (where remaining_capacity is closest to item_size) maps to the steepest\n    # part of the sigmoid.\n\n    # Ensure we only consider bins that can actually fit the item\n    valid_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Calculate a \"closeness\" score for valid bins.\n    # We want remaining_cap - item to be close to 0.\n    # To use sigmoid effectively, we'll transform this difference.\n    # Let's consider the relative difference: (remaining_cap - item) / remaining_cap\n    # This score will be close to 0 if remaining_cap >> item, and close to 1 if remaining_cap is just slightly larger than item.\n    # Or, even simpler, let's just look at remaining_cap directly.\n\n    # We want remaining_cap to be roughly item + a small gap.\n    # Let's try mapping remaining_cap.\n    # The center of the sigmoid should be where remaining_cap is \"ideal\".\n    # A good ideal might be slightly larger than 'item', perhaps item + some epsilon.\n    # Let's use a characteristic size for scaling. The average remaining capacity\n    # could be a good reference point, or maybe just a constant, or even the bin capacity.\n    # For simplicity, let's make the sigmoid sensitive around the item size.\n\n    # A common sigmoid form is 1 / (1 + exp(-k * (x - x0)))\n    # where x is the input, x0 is the center, and k controls steepness.\n\n    # We want high priority when bins_remain_cap is close to 'item'.\n    # Let's set the center (x0) of our sigmoid to be slightly above 'item'.\n    # A small offset like 0.1 or a percentage of item could work.\n    # Let's try to center it around `item + item * 0.1` (10% buffer).\n\n    center = item * 1.1  # Ideal remaining capacity is 10% larger than item\n\n    # The steepness (k) will determine how sensitive the priority is to deviations\n    # from the ideal center. A larger k means a sharper transition.\n    # Let's make the steepness dependent on the item size. Larger items might\n    # benefit from less steep sigmoid, smaller items from steeper. Or just a constant.\n    steepness = 5.0  # A tunable parameter\n\n    # Calculate the sigmoid score for valid bins\n    # The input to the sigmoid will be `bins_remain_cap`.\n    # The sigmoid function will output a value between 0 and 1.\n    # We want higher values when bins_remain_cap is close to `center`.\n    # However, a standard sigmoid increases as the input increases.\n    # We want priority to be high when `bins_remain_cap` is *around* `center`.\n    #\n    # A better approach: Use a bell-shaped curve (like Gaussian or a\n    # derivative of sigmoid). Or, we can invert the input to the sigmoid.\n    # If we use sigmoid(-k * (x - x0)), it peaks at x0.\n\n    # Let's re-think: we want high priority when `remaining_cap` is close to `item`.\n    # Let `score = remaining_cap - item`.\n    # We want `score` to be close to 0.\n    # We want a function that peaks at `score = 0`.\n    # Consider a Gaussian: `exp(-a * score^2)`. This is like a squared distance.\n    #\n    # Let's stick with sigmoid but modify the input.\n    # We want to map `bins_remain_cap` to a priority score.\n    #\n    # Let's aim for higher priority when `bins_remain_cap` is NOT too large and NOT too small.\n    # Bins that are very full (small remaining capacity) should have low priority.\n    # Bins that are very empty (large remaining capacity) should also have low priority.\n    # Bins that are \"just right\" should have high priority.\n    #\n    # Consider `(bins_remain_cap - item)`.\n    # If this is positive and small, good. If it's negative, invalid. If it's large, bad.\n    #\n    # Let's try to model a score that is high when `bins_remain_cap` is within a certain range of `item`.\n    # We can achieve this by combining two sigmoid functions or by using a transformation.\n    #\n    # Alternative: Let's focus on the \"waste\". Waste = `bins_remain_cap - item`.\n    # We want minimal waste, but `bins_remain_cap >= item`.\n    #\n    # Sigmoid Fit Score strategy often refers to prioritizing bins that leave the LEAST remaining capacity\n    # AFTER packing the item (i.e., Best Fit). Our current `bins_remain_cap` is BEFORE packing.\n    # So, we want to select a bin such that `bins_remain_cap - item` is minimized, but still non-negative.\n    # This is Best Fit.\n    #\n    # The request specifically mentions \"Sigmoid Fit Score\". This implies using sigmoid for scoring.\n    # Let's interpret \"Sigmoid Fit Score\" as a variation of \"First Fit\", \"Best Fit\" or \"Worst Fit\"\n    # that uses a sigmoid to assign probabilities or priorities.\n    #\n    # If we use the standard \"Best Fit\" approach (minimize `bins_remain_cap - item`),\n    # a simple way to use sigmoid is to transform this difference.\n    #\n    # Let `diff = bins_remain_cap - item`. We want to prioritize small non-negative `diff`.\n    #\n    # If `bins_remain_cap < item`, priority is 0.\n    # If `bins_remain_cap >= item`, we want to prioritize smaller `bins_remain_cap`.\n    #\n    # Let's scale `bins_remain_cap` values.\n    # A common approach for Sigmoid Fit is to relate it to how full the bin is *after* packing.\n    # The \"fit\" is how well the item fills the bin.\n    #\n    # Let's consider the *percentage of space utilized* for the item in the bin.\n    # This is `item / original_bin_capacity`. This isn't directly available here.\n    # We only have `bins_remain_cap`.\n    #\n    # Let's use the sigmoid to encourage bins that leave a small but positive remaining capacity.\n    # The range of `bins_remain_cap` can vary greatly. We need to normalize or scale appropriately.\n    #\n    # A simple sigmoid mapping of `bins_remain_cap` would give higher scores to larger `bins_remain_cap`\n    # (if it's increasing sigmoid) or smaller `bins_remain_cap` (if it's decreasing).\n    #\n    # Let's consider `x = bins_remain_cap`. We want high priority when `x` is close to `item`.\n    # We can define a score function `f(x)` that peaks at `x = item`.\n    # A Gaussian `exp(-k * (x - item)**2)` would do this.\n    # How to use sigmoid?\n    #\n    # If we want to maximize the sigmoid output, we need the argument to be large positive.\n    # If we want to minimize sigmoid output, we need the argument to be large negative.\n    #\n    # Let's map `bins_remain_cap` to a score that is high when `bins_remain_cap` is `item`.\n    #\n    # Consider the transformation: `-(bins_remain_cap - item)**2`. This peaks at `item`.\n    # Then apply sigmoid to this: `sigmoid(k * -(bins_remain_cap - item)**2)`\n    # `1 / (1 + exp(k * (bins_remain_cap - item)**2))`\n    # This function is `1` when `bins_remain_cap = item` and approaches `0` as `bins_remain_cap` moves away from `item`.\n    # This is a bell curve shape, often associated with \"best fit\" properties when smoothed by sigmoid.\n\n    # Apply the valid bins mask\n    valid_bins_remain_cap = bins_remain_cap[valid_bins_remain_cap]\n\n    if valid_bins_remain_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"squared difference from ideal fit\"\n    # Ideal remaining capacity is `item` to minimize waste.\n    # We are penalizing bins that are too large or too small (if less than item, which we filtered)\n    squared_diff = (valid_bins_remain_cap - item)**2\n\n    # Apply sigmoid transformation.\n    # We want the highest priority when squared_diff is 0.\n    # So, we want the input to sigmoid to be as large and positive as possible.\n    # This means `sigmoid(k * (something that is high when squared_diff is low))`.\n    # Let's use `sigmoid(-k * squared_diff)`. This peaks at 0 difference.\n    # `1 / (1 + exp(k * squared_diff))`\n    # This gives a value close to 1 for small `squared_diff` and close to 0 for large `squared_diff`.\n\n    # To make it more like a score that can be used in selection, higher is better.\n    # Let's scale `squared_diff` to control the steepness.\n    # A common practice is to normalize based on the range of possible differences.\n    # The maximum possible difference could be related to the bin capacity.\n    # However, we don't have original bin capacity.\n    # Let's try scaling `squared_diff` by a factor.\n\n    # Scale the squared differences.\n    # If the differences are very large, the sigmoid might quickly become zero.\n    # We can scale the squared differences by a factor `scale` such that\n    # `scale * squared_diff` maps to the sigmoid's sensitive region.\n    # If `squared_diff` is around `item^2`, we want that to be significant.\n    # Let's try scaling by `1 / item**2` to make the relative difference matter.\n    # Or, let's choose a `scale` parameter that's tunable.\n    # A simpler approach is to use a constant scaling factor for `k`.\n\n    k_scaled = 5.0 / (item + 1e-6) # Make steepness somewhat dependent on item size (avoid division by zero)\n\n    # Calculate the transformed scores using the inverted sigmoid logic\n    # higher_score_for_closer_fit = 1 / (1 + np.exp(k_scaled * squared_diff))\n    # This still peaks at 1 for the best fit.\n\n    # Let's try a different sigmoid interpretation:\n    # Prioritize bins that are \"not too full\" and \"not too empty\".\n    # This implies a peak around the middle of available capacities.\n    # The term \"Sigmoid Fit Score\" is a bit ambiguous without a precise definition provided.\n    # A common interpretation for similar heuristics is to use sigmoid to translate\n    # the \"goodness of fit\" into a probability or priority.\n\n    # Let's simplify and try to make it \"Best Fit with Sigmoid Shaping\".\n    # Best Fit aims to minimize `bins_remain_cap - item`.\n    # We can transform `bins_remain_cap - item` to a score.\n    #\n    # Consider the ratio `(bins_remain_cap - item) / bins_remain_cap` for valid bins.\n    # This ratio is 0 when `bins_remain_cap = item`.\n    # This ratio approaches 1 when `bins_remain_cap` is very large.\n    # This ratio is negative if `bins_remain_cap < item` (but we filter these).\n    #\n    # We want to prioritize smaller positive values of this ratio.\n    # `ratio = (valid_bins_remain_cap - item) / valid_bins_remain_cap`\n    # A ratio close to 0 is good.\n    #\n    # To use sigmoid for this, we want a score that increases as the ratio decreases.\n    # Let `score_input = -ratio`.\n    #\n    # Then `priorities_valid = 1 / (1 + np.exp(-k_sigmoid * score_input))`\n    # `priorities_valid = 1 / (1 + np.exp(k_sigmoid * ratio))`\n    # This will give higher priority when `ratio` is smaller (closer to 0).\n    # `k_sigmoid` controls the steepness. Let's set it to a reasonable value.\n\n    k_sigmoid = 10.0 # Tunable parameter for steepness\n\n    # Calculate ratio (percentage of leftover space relative to current remaining capacity)\n    # Avoid division by zero for bins that might have zero remaining capacity (though filtered by valid_bins_mask)\n    ratios = (valid_bins_remain_cap - item) / (valid_bins_remain_cap + 1e-9)\n\n    # Apply sigmoid: high score for small ratios (good fit)\n    # The sigmoid function `1 / (1 + exp(-x))` is monotonically increasing.\n    # We want high scores for small `ratios`. So we need an argument `x` that is large negative when `ratios` is small.\n    # Let `x = -k_sigmoid * ratios`.\n    # Score = `1 / (1 + exp(k_sigmoid * ratios))`\n    priorities_valid = 1.0 / (1.0 + np.exp(k_sigmoid * ratios))\n\n    # Distribute the calculated priorities back to the original array shape\n    priorities[valid_bins_remain_cap > 0] = priorities_valid # Ensure we only assign to valid bins\n\n    return priorities",
    "response_id": 6,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 152, in priority_v2\nUnboundLocalError: local variable 'valid_bins_remain_cap' referenced before assignment\n2\n199.7052750908657\n"
  },
  {
    "stdout_filepath": "problem_iter1_response7.txt_stdout.txt",
    "code_path": "problem_iter1_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Random Fit strategy.\n\n    In the Random Fit strategy for online bin packing, an item is placed into a\n    randomly chosen bin that has sufficient capacity. The \"priority\" here reflects\n    the desirability of a bin, and for Random Fit, any bin with enough capacity\n    is equally desirable (or undesirable). We can simulate this randomness by\n    assigning a random priority score to bins that can fit the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of -1.\n        Bins that can fit the item will have a random priority score.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with -1 (cannot fit)\n\n    # Find bins that have enough remaining capacity for the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Assign a random priority to the bins that can fit the item\n    # The range of random numbers doesn't strictly matter for Random Fit as\n    # selection is purely random among viable bins. We use [0, 1) for simplicity.\n    priorities[can_fit_mask] = np.random.rand(np.sum(can_fit_mask))\n\n    return priorities",
    "response_id": 7,
    "obj": 4.487435181491823,
    "cyclomatic_complexity": 1.0,
    "halstead": 11.60964047443681,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response8.txt_stdout.txt",
    "code_path": "problem_iter1_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploring (choosing a random bin)\n    n_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if np.any(fittable_bins_mask):\n        # Greedy part: Calculate priority for fittable bins\n        # Prioritize bins with less remaining capacity that can still fit the item.\n        # This is a \"best fit\" approach for the greedy choice.\n        fittable_bins_remain_cap = bins_remain_cap[fittable_bins_mask]\n        \n        # To prioritize bins with less remaining capacity, we want higher scores\n        # for smaller remaining capacities. We can use the inverse of remaining capacity.\n        # To avoid division by zero or very small numbers, we add a small constant.\n        # A higher score means higher priority.\n        priorities[fittable_bins_mask] = 1.0 / (fittable_bins_remain_cap - item + 1e-6)\n        \n        # Normalize priorities so they sum to 1 for the fittable bins\n        if np.sum(priorities[fittable_bins_mask]) > 0:\n            priorities[fittable_bins_mask] /= np.sum(priorities[fittable_bins_mask])\n\n        # Epsilon-Greedy: With probability epsilon, choose a random fittable bin\n        if np.random.rand() < epsilon:\n            random_bin_index = np.random.choice(np.where(fittable_bins_mask)[0])\n            # Set the priority of the random bin to 1 and others to 0 for selection\n            priorities = np.zeros_like(bins_remain_cap, dtype=float)\n            priorities[random_bin_index] = 1.0\n    else:\n        # If no bin can fit the item, all priorities remain 0 (or you might want\n        # to signal that a new bin is needed, but for priority scoring, 0 is fine).\n        pass\n\n    return priorities",
    "response_id": 8,
    "obj": 4.148384523334677,
    "cyclomatic_complexity": 4.0,
    "halstead": 89.20647778231529,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response9.txt_stdout.txt",
    "code_path": "problem_iter1_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.\n\n    The priority is calculated based on how well an item fits into a bin.\n    A good fit means a smaller remaining capacity after packing, encouraging\n    using bins efficiently. The Softmax function is used to convert these\n    \"fit scores\" into probabilities (priorities).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity if the item is placed in each bin\n    # We are only interested in bins where the item *can* fit.\n    possible_fits = bins_remain_cap - item\n\n    # For bins where the item doesn't fit, assign a very low (negative) score\n    # so they have virtually zero probability after softmax.\n    # Using a large negative number like -1e9 to ensure it's smaller than any potential good fit.\n    fit_scores = np.where(possible_fits >= 0, possible_fits, -1e9)\n\n    # Apply the Softmax function to the fit scores.\n    # A smaller remaining capacity (better fit) will result in a higher priority.\n    # Softmax formula: exp(x_i) / sum(exp(x_j))\n    # We want smaller remaining capacity to have higher priority.\n    # Thus, we can invert the scores or use a transformation like `-fit_scores`.\n    # Let's use a transformation that penalizes larger remaining capacities more heavily.\n    # A common approach for \"best fit\" type of heuristic with softmax is to\n    # use the negative of the remaining capacity as the input to softmax.\n    # The smaller the remaining capacity (closer to 0), the larger (less negative)\n    # the `-fit_scores` will be, leading to a higher priority.\n\n    # We also want to ensure that bins that cannot fit the item get a score close to zero.\n    # The `-1e9` for non-fitting bins will result in `exp(-1e9)` which is effectively zero,\n    # so the softmax naturally handles this.\n\n    # Let's adjust the fit scores to be more conducive to softmax for \"best fit\".\n    # We want smaller `possible_fits` to have higher probability.\n    # So we use `-possible_fits` as the input to softmax.\n    # Smaller `possible_fits` -> larger `-possible_fits` -> higher softmax value.\n\n    # Consider the inverse of the remaining capacity if the item fits.\n    # For bins that cannot fit the item, the remaining capacity would be negative.\n    # We want to assign a very low score to these.\n    # Let's transform the `possible_fits` such that a good fit (small `possible_fits`)\n    # results in a large positive number for softmax.\n    # A simple transformation is `-(possible_fits + epsilon)` where epsilon is small,\n    # or even just `-possible_fits`. However, directly using `possible_fits` in softmax\n    # would favor bins with *larger* remaining capacity after packing.\n\n    # To achieve a \"best fit\" (smallest remaining capacity), we should use scores\n    # where smaller remaining capacity corresponds to larger exponent values.\n    # Let's consider the value `bins_remain_cap - item`. A smaller value of this\n    # is better. So, let's use `-(bins_remain_cap - item)` or `item - bins_remain_cap`.\n    # However, we must handle cases where `bins_remain_cap < item`.\n    #\n    # Let's reframe: we want to maximize `bins_remain_cap - item`.\n    # So, for bins that can fit: `bins_remain_cap - item`.\n    # For bins that cannot fit: a very small value.\n    # Softmax works best with positive exponent values.\n    # If we want smaller `bins_remain_cap - item` to be higher priority, we can\n    # use `K - (bins_remain_cap - item)` where K is a large constant.\n    # Let's use a simple and common approach:\n    # If item fits (bins_remain_cap >= item), score = bins_remain_cap - item (smaller is better)\n    # If item does not fit, score = -infinity\n\n    # To make smaller remaining capacity map to higher priority with softmax,\n    # we can transform the scores. A common transformation is to use a large constant minus\n    # the remaining capacity or similar, or directly use negative values if we want\n    # the smallest (least negative) values to have higher probabilities.\n\n    # Let's define \"fitness\" as how much capacity is left. We want to minimize this.\n    # So, for fitting bins: `fitness = bins_remain_cap - item`.\n    # For non-fitting bins: `fitness = infinity`.\n    # For softmax, we want larger values to be higher priority.\n    # So, we can use `priorities = -fitness`.\n    # `priorities = -(bins_remain_cap - item)` for fitting bins.\n    # `priorities = -infinity` for non-fitting bins.\n    # This will map `bins_remain_cap - item = 0` (perfect fit) to `priority = 0`.\n    # And `bins_remain_cap - item = very large` to `priority = very small negative`.\n    # This seems counter-intuitive for softmax where larger positive exponents are better.\n\n    # Alternative approach for Softmax-based Best Fit:\n    # A common strategy for \"best fit\" with softmax is to assign a score that is\n    # inversely related to the remaining capacity.\n    # For example, if item fits, score = 1 / (bins_remain_cap - item + epsilon)\n    # If item does not fit, score = 0\n\n    # Let's try a simpler transformation suitable for softmax:\n    # We want the *smaller* the `bins_remain_cap - item`, the *higher* the priority.\n    # So, we can use `-(bins_remain_cap - item)` as input to the softmax exponent.\n    # For bins where `bins_remain_cap - item < 0`, we set the exponent to a very low value.\n    # The `np.exp` function is sensitive to large negative inputs, mapping them to ~0.\n\n    exponent_values = np.zeros_like(bins_remain_cap)\n    can_fit_mask = bins_remain_cap >= item\n    # For bins that can fit, we want smaller `remaining_capacity` to have higher priority.\n    # So, we use the negative of the remaining capacity.\n    # Smaller `bins_remain_cap - item` means a larger (less negative) value of `-(bins_remain_cap - item)`.\n    exponent_values[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # To avoid numerical instability with exp(very large negative numbers) for non-fitting bins,\n    # we ensure they are sufficiently small. `np.exp(-1e9)` is close to 0.\n    # Let's ensure exponent values for non-fitting bins are significantly smaller\n    # than any possible value for fitting bins.\n    # If the minimum remaining capacity for a fitting bin is 0, the min exponent is 0.\n    # If we consider item size 1 and capacities [0.5, 0.5], they don't fit.\n    # If we consider item size 0.1 and capacities [0.5, 0.5], remaining capacities are 0.4.\n    # Exponent values would be -0.4. exp(-0.4) is ~0.67.\n    # If capacities were [0.15, 0.15], remaining are 0.05. Exponents are -0.05. exp(-0.05) is ~0.95.\n    # This seems to work for \"best fit\" where smaller remaining capacity is better.\n\n    # Avoid exponentiating potentially very large positive numbers if item size is negative (which shouldn't happen)\n    # or if bins_remain_cap is extremely large. We can cap the exponent values or use a robust softmax.\n    # For typical BPP, item sizes and capacities are positive and reasonable.\n\n    # Calculate the exponent terms for the softmax\n    # A smaller `bins_remain_cap - item` should lead to a higher probability.\n    # So, we use `-(bins_remain_cap - item)`.\n    # For items that don't fit, `bins_remain_cap - item` is negative.\n    # We want these to have a very low probability.\n    # Setting exponent to a large negative number handles this.\n\n    # Ensure that the maximum exponent is not excessively large, which might cause overflow in exp.\n    # Also, ensure that the minimum exponent for fitting bins is not too small (very negative).\n    # Let's cap the exponent values to a reasonable range, e.g., [-5, 5].\n    # The difference in priorities should be discernible.\n\n    # If `bins_remain_cap - item` is 0, exponent is 0, exp(0)=1. This is good.\n    # If `bins_remain_cap - item` is large positive (poor fit), exponent is large negative, exp(large negative) ~ 0.\n    # If `bins_remain_cap - item` is small positive (good fit), exponent is small negative.\n    # This is still mapping good fits to small negative exponents.\n\n    # Let's try the inverse logic: Higher priority for bins where `bins_remain_cap - item` is *small*.\n    # This means `-(bins_remain_cap - item)` should be large.\n    #\n    # Consider the \"closeness\" to zero remaining capacity.\n    # If `bins_remain_cap - item` is the difference.\n    # We want the smallest non-negative difference.\n    #\n    # A direct mapping for Softmax \"Best Fit\" could be:\n    # For bins where `bins_remain_cap >= item`: score = `C - (bins_remain_cap - item)` for a large C.\n    # For bins where `bins_remain_cap < item`: score = `-infinity`.\n    # Let's choose `C` such that `C - (bins_remain_cap - item)` remains positive and reasonably spread.\n    # If `bins_remain_cap` can be large, `bins_remain_cap - item` can be large.\n    #\n    # A more numerically stable and conceptually cleaner approach for \"best fit\" is\n    # often to consider how much capacity *would be wasted* or how *tight* the fit is.\n    #\n    # Let's use the negative of the remaining capacity as the input to exp.\n    # `score = - (bins_remain_cap - item)` for valid fits.\n    # `score = -infinity` for invalid fits.\n\n    # Calculate the raw score, making invalid bins very undesirable.\n    # The goal is to have the smallest `bins_remain_cap - item`.\n    # This means `-(bins_remain_cap - item)` should be the largest (least negative or most positive).\n    scores_for_softmax = np.full_like(bins_remain_cap, -np.inf)\n    valid_indices = bins_remain_cap >= item\n    if np.any(valid_indices):\n        # We want to maximize `bins_remain_cap - item` to be close to zero (but not negative).\n        # To use softmax, we want the argument to exp to be larger for better bins.\n        # So, we can use `-(bins_remain_cap - item)`. A smaller (bins_remain_cap - item) gives a larger negative score.\n        # This is still leading to mapping better fits to smaller exponent values.\n\n        # Correct approach for Softmax Best Fit:\n        # The value that we want to maximize is `- (bins_remain_cap - item)`, for bins where `bins_remain_cap - item >= 0`.\n        # For bins where `bins_remain_cap - item < 0`, the value should be very small.\n        # So, we want to maximize the value `bins_remain_cap - item` by turning it into\n        # a preference for smaller values.\n        # Let's use `-(bins_remain_cap - item)` for bins that fit.\n        #\n        # Example: item = 0.3, bins_remain_cap = [0.5, 0.8, 0.2]\n        # Bin 0: can fit, remaining = 0.5 - 0.3 = 0.2. Score for softmax = -0.2\n        # Bin 1: can fit, remaining = 0.8 - 0.3 = 0.5. Score for softmax = -0.5\n        # Bin 2: cannot fit. Score for softmax = -inf\n        #\n        # Softmax: exp(-0.2) / (exp(-0.2) + exp(-0.5) + exp(-inf))\n        #         exp(-0.2) / (exp(-0.2) + exp(-0.5) + 0)\n        # This gives higher probability to the bin with remaining capacity 0.2 (Bin 0), which is the \"best fit\".\n        # This seems correct.\n\n        scores_for_softmax[valid_indices] = -(bins_remain_cap[valid_indices] - item)\n\n    # Apply softmax. We need to handle the case where all scores are -inf (item too large for all bins).\n    # In such a case, `np.exp` of all `-np.inf` is `0`. The sum would be `0`. Division by zero.\n    # A common way to handle this is to add a small epsilon to the denominator or to check for all -inf.\n    # However, `np.exp` of `-np.inf` results in `0.0`.\n    # The sum of zeros would be zero. Let's ensure at least one non-infinite value if possible.\n    # If `scores_for_softmax` contains `-np.inf`, `np.exp` turns them to `0.0`.\n    # If all are `-np.inf`, the sum is `0.0`.\n    # If at least one is finite, the sum will be non-zero.\n\n    # Softmax implementation\n    exp_scores = np.exp(scores_for_softmax)\n    sum_exp_scores = np.sum(exp_scores)\n\n    # Handle the case where the item cannot fit into any bin.\n    # In this scenario, `scores_for_softmax` will be all `-np.inf`, `exp_scores` all `0.0`,\n    # and `sum_exp_scores` will be `0.0`.\n    if sum_exp_scores == 0:\n        # If no bin can fit the item, return a uniform distribution of zero probability\n        # or a default behavior (e.g., priority to the bin with most capacity, though\n        # the problem states we select *a* bin for the item).\n        # For this heuristic, if it cannot fit, it shouldn't be prioritized for fitting.\n        # Returning zeros indicates no valid priority can be assigned by this heuristic.\n        # A robust implementation might also try to create a new bin if this were the case.\n        # For this function, we return zeros if no bin is a candidate.\n        return np.zeros_like(bins_remain_cap)\n    else:\n        priorities = exp_scores / sum_exp_scores\n        return priorities",
    "response_id": 9,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 130.79881092001088,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response10.txt_stdout.txt",
    "code_path": "problem_iter1_code10.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a First Fit strategy with a preference for fuller bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Create a boolean mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins that can fit the item, calculate priority\n    # The priority is inversely related to the remaining capacity.\n    # Bins with less remaining capacity (i.e., are fuller) get higher priority.\n    # Add a small epsilon to avoid division by zero if a bin has exactly 'item' remaining capacity.\n    priorities[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)\n    \n    # Bins that cannot fit the item get a priority of 0 (or a very low negative number if desired for clear distinction)\n    # For this implementation, 0 is sufficient as it will not be chosen over any bin that can fit.\n    \n    return priorities",
    "response_id": 10,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 41.51317942364757,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response11.txt_stdout.txt",
    "code_path": "problem_iter1_code11.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Best Fit strategy aims to place the item in the bin that leaves the minimum\n    # remaining capacity. This often leads to better packing.\n    # We can achieve this by prioritizing bins where (remaining_capacity - item_size)\n    # is minimized, but only for bins that can actually fit the item.\n\n    # Initialize priorities to a very low value (or negative infinity) for bins\n    # that cannot fit the item.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that have enough capacity to fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity after placing the item.\n    # The \"priority\" is the negative of this remaining capacity, so smaller remaining\n    # capacities result in higher (less negative) priority scores.\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    return priorities",
    "response_id": 11,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 30.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response12.txt_stdout.txt",
    "code_path": "problem_iter1_code12.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    The Worst Fit strategy prioritizes bins that have the largest remaining capacity,\n    as this aims to leave smaller remaining capacities in more bins, potentially\n    accommodating future smaller items. If an item fits into a bin, its priority\n    is its remaining capacity. If it doesn't fit, its priority is 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    # A bin is only considered if the item fits into it.\n    # For bins where the item fits, the priority is the remaining capacity.\n    # Worst Fit prefers the bin with the largest remaining capacity.\n    fits_mask = bins_remain_cap >= item\n    priorities[fits_mask] = bins_remain_cap[fits_mask]\n    return priorities",
    "response_id": 12,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 1.0,
    "halstead": 4.754887502163469,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response13.txt_stdout.txt",
    "code_path": "problem_iter1_code13.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit.\n\n    Almost Full Fit (AFF) is a heuristic for the bin packing problem.\n    The idea behind AFF is to pack the current item into the bin that has\n    just enough remaining capacity to hold the item, or the bin that has the\n    smallest remaining capacity among those that can hold the item. This\n    tends to leave larger gaps in other bins for future, potentially larger items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher priority is given to bins that are almost full (i.e., have a small\n        remaining capacity that is still sufficient for the item).\n    \"\"\"\n    # Initialize priorities to a very low value for bins that cannot accommodate the item\n    # and a base priority for bins that can.\n    # We want to prioritize bins that leave the smallest gap after packing.\n    # The gap is bins_remain_cap - item.\n    # So, we want to minimize this gap.\n    # A simple way to prioritize is to assign a higher score to smaller positive gaps.\n\n    # Identify bins that can accommodate the item\n    can_accommodate_mask = bins_remain_cap >= item\n\n    # For bins that can accommodate the item, calculate the \"tightness\" or remaining capacity.\n    # The tighter the fit (smaller remaining capacity after packing), the higher the priority.\n    # We want to sort bins by remaining_capacity - item.\n    # However, directly using remaining_capacity - item might lead to issues if we just\n    # want to assign high values to \"almost full\".\n    # Let's define priority as the inverse of the remaining capacity after packing,\n    # but only for bins that can fit the item.\n\n    # A good strategy for AFF would be to give the highest priority to the bin\n    # where (bin_capacity - item) is minimized.\n    # If multiple bins have the same minimum difference, we can break ties by\n    # selecting the bin with the smallest index or some other arbitrary but consistent method.\n\n    # Let's create a priority score where a smaller remaining capacity (after packing)\n    # results in a higher priority.\n    # The difference is bins_remain_cap[i] - item.\n    # We want to prioritize smaller positive differences.\n    # We can achieve this by using the negative of the difference.\n    # However, this still needs to be non-positive for bins that *cannot* fit the item.\n\n    priorities = np.full_like(bins_remain_cap, -np.inf)  # Initialize with a very low priority\n\n    # Calculate the remaining capacity after placing the item in eligible bins\n    remaining_after_packing = bins_remain_cap[can_accommodate_mask] - item\n\n    # For AFF, we want the smallest positive difference.\n    # So, we can assign a priority score based on this difference.\n    # A common way to ensure higher values are preferred is to use negative values\n    # and then take the negative, or simply sort in reverse.\n    # Let's assign priorities such that the smallest positive (bins_remain_cap - item)\n    # gets the highest priority.\n    # We can achieve this by using the negative of the remaining capacity after packing.\n    # Or more simply, the remaining capacity itself, and then we'd look for the minimum among positive values.\n\n    # Let's try a score where a bin that *exactly* fits gets the highest priority,\n    # and bins that have a slightly larger remaining capacity get slightly lower priority.\n    # This suggests a function that is high when (bins_remain_cap - item) is small and positive.\n\n    # Calculate the difference (gap) for eligible bins.\n    gaps = bins_remain_cap[can_accommodate_mask] - item\n\n    # To prioritize the smallest positive gap, we can assign priorities like:\n    # If gap == 0, priority = infinity (or a very large number).\n    # If gap > 0, priority = 1 / gap.\n    # This might be problematic with floating point precision.\n\n    # A simpler approach: assign priority based on the negative of the gap.\n    # The smallest gap will have the largest negative value (closest to zero).\n    # This still requires careful handling of negative gaps (bins that can't fit).\n\n    # Let's try a more direct interpretation of AFF: find the bin with the smallest\n    # `bins_remain_cap` that is still greater than or equal to `item`.\n    # We can achieve this by assigning a \"priority\" score where the minimum `bins_remain_cap`\n    # (among those >= item) gets the highest score.\n    # A score of `-bins_remain_cap` would naturally prioritize smaller remaining capacities.\n    # For bins that *cannot* fit, we assign a very low score.\n\n    priorities[can_accommodate_mask] = -bins_remain_cap[can_accommodate_mask]\n\n    # Another common way to implement \"smallest remaining capacity\" priority is to\n    # give a high priority to bins that are \"closest\" to fitting the item without going over.\n    # This means bins with `bins_remain_cap - item` closest to 0 (and positive).\n    # A simple way to get this behavior: if a bin can fit, assign a priority inversely proportional\n    # to its remaining capacity *after* the item is placed.\n\n    # Let's use the remaining capacity after placement as the basis.\n    # We want to MINIMIZE this remaining capacity.\n    # So, a smaller value should correspond to a higher priority.\n    # Using `-bins_remain_cap + item` directly gives the negative of the gap.\n    # The bin with the smallest positive gap will have the least negative value.\n\n    # For bins that can accommodate the item:\n    # The priority should be higher for bins with smaller remaining capacity.\n    # So, we want to maximize `bins_remain_cap - item` when `bins_remain_cap - item` is small.\n    # Or, we want to minimize `bins_remain_cap` among eligible bins.\n\n    # Let's assign a score based on the negative of the bin's remaining capacity.\n    # The largest negative value (closest to 0) will correspond to the smallest capacity.\n    # For bins that can accommodate the item, we assign a score of `-bins_remain_cap`.\n    # This implicitly prioritizes bins with smaller remaining capacities.\n\n    # Consider the case where multiple bins have the same minimal remaining capacity.\n    # AFF typically means selecting *one* such bin. Our scoring mechanism would\n    # give them equal scores. A tie-breaking rule would be needed in the selection logic,\n    # but the priority function just needs to assign comparable scores.\n\n    # A simple and effective priority for AFF could be:\n    # For bins that can fit the item, assign a priority based on their current remaining capacity.\n    # The lower the remaining capacity, the higher the priority.\n    # So, priority = -bins_remain_cap.\n    # For bins that cannot fit, assign a very low priority (-infinity).\n\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    eligible_bins_indices = np.where(bins_remain_cap >= item)[0]\n\n    if eligible_bins_indices.size > 0:\n        # Assign a priority score such that smaller remaining capacities get higher scores.\n        # Using the negative of the remaining capacity works well:\n        # e.g., if remaining caps are [5, 3, 8] and item is 2,\n        # eligible caps are [5, 3, 8]. Scores would be [-5, -3, -8].\n        # The max score is -3, which corresponds to the bin with remaining cap 3.\n        priorities[eligible_bins_indices] = -bins_remain_cap[eligible_bins_indices]\n\n    return priorities",
    "response_id": 13,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 85.11011351724513,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response14.txt_stdout.txt",
    "code_path": "problem_iter1_code14.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    The Exact Fit First strategy aims to find a bin where the item fits perfectly,\n    or if no perfect fit exists, it picks the bin with the smallest remaining\n    capacity that can still accommodate the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher priority means a better fit.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If there are bins that can fit the item\n    if np.any(can_fit_mask):\n        # Calculate the \"tightness\" of the fit. A smaller remaining capacity\n        # after adding the item means a tighter fit.\n        # We add a small epsilon to the item to avoid division by zero if\n        # item itself is zero, and also to ensure that a perfect fit (remaining_cap - item == 0)\n        # gets a very high priority (effectively infinity from the perspective of 1/remaining).\n        # However, a direct calculation of remaining_cap - item can be problematic\n        # as it might be negative for bins that don't fit.\n        # Instead, we'll assign priority based on how close the remaining capacity is to the item size.\n\n        # Priority for exact fits: Very high value\n        exact_fit_mask = (bins_remain_cap == item)\n        priorities[exact_fit_mask] = 1e10  # Assign a very high priority for exact fits\n\n        # Priority for non-exact fits: Higher priority for smaller remaining capacity\n        # that is still greater than or equal to the item size.\n        # We can use the negative of the remaining capacity to sort ascendingly\n        # or 1/remaining capacity. Using 1/(remaining_cap - item + epsilon) might be better\n        # to give higher score to smaller remaining capacity after fit.\n        # However, the problem states Exact Fit *First*, so we want to prioritize bins\n        # that *exactly* fit. If none exist, then we want the *best* fit among the rest.\n\n        # Let's refine the strategy:\n        # 1. Perfect fits get the highest priority.\n        # 2. Among bins that are not perfect fits but can accommodate the item,\n        #    the one with the smallest remaining capacity *after* placing the item\n        #    gets the next highest priority. This means bins with `remaining_cap - item` being smallest positive.\n        #    Equivalently, we want to maximize `item / remaining_cap` among those that fit,\n        #    but this doesn't capture the \"exact fit\" aspect as well as\n        #    minimizing `remaining_cap - item`.\n\n        # Let's assign priorities such that perfect fits are highest,\n        # then bins with the smallest `remaining_cap - item` (positive values).\n\n        # Filter out bins that can fit the item and are not exact fits\n        non_exact_fits_mask = can_fit_mask & ~exact_fit_mask\n\n        if np.any(non_exact_fits_mask):\n            remaining_capacities_for_fit = bins_remain_cap[non_exact_fits_mask]\n            # Calculate the difference (how much space is left after putting the item)\n            diffs = remaining_capacities_for_fit - item\n\n            # We want to prioritize bins where this difference is smallest and positive.\n            # Assign priorities inversely proportional to the difference, but\n            # scaled down from exact fits.\n            # A simple approach: use 1 / (diff + epsilon) and scale it.\n            # Or, simply sort them and assign ranks.\n            # Let's try to create a scoring mechanism where smaller positive differences get higher scores.\n            # For example, priorities can be `1 / (diff + small_constant)` or directly related to rank.\n\n            # A more robust approach is to assign priorities based on their rank after filtering.\n            # Get the remaining capacities for bins that can fit and are not exact fits.\n            suitable_bins_caps = bins_remain_cap[non_exact_fits_mask]\n            # Calculate the leftover space in these bins\n            leftovers = suitable_bins_caps - item\n\n            # Sort these leftover spaces in ascending order\n            sorted_indices_of_leftovers = np.argsort(leftovers)\n\n            # Assign priorities to these bins. The bin with the smallest leftover\n            # gets the highest priority among this group.\n            # We assign priorities decreasingly from a slightly lower value than perfect fits.\n            current_priority = 1e9 # Start just below perfect fits\n\n            for i in sorted_indices_of_leftovers:\n                # Find the original index of this suitable bin\n                original_index = np.where(non_exact_fits_mask)[0][i]\n                priorities[original_index] = current_priority\n                current_priority -= 1 # Decrease priority for the next best fit\n\n    # Bins that cannot fit the item will have a priority of 0 (as initialized)\n    return priorities",
    "response_id": 14,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Command '['python3', '-u', '/home/dokhanhnam1199/QD/problems/bpp_online/eval.py', '5000', '/home/dokhanhnam1199/QD', 'train']' timed out after 49.99997549399268 seconds"
  },
  {
    "stdout_filepath": "problem_iter1_response15.txt_stdout.txt",
    "code_path": "problem_iter1_code15.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit) strategy.\n\n    The Inverse Distance (Proximity Fit) strategy prioritizes bins that have a remaining capacity\n    closest to the item's size, but still large enough to fit the item. A higher score\n    means a better fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Consider only bins that can fit the item\n    eligible_bins_mask = bins_remain_cap >= item\n    eligible_bins_capacities = bins_remain_cap[eligible_bins_mask]\n\n    if np.sum(eligible_bins_mask) > 0:\n        # Calculate the difference between bin capacity and item size\n        # We want this difference to be small (i.e., a tight fit)\n        differences = eligible_bins_capacities - item\n        \n        # Invert the differences to get priority. Smaller differences should have higher priority.\n        # Add a small epsilon to avoid division by zero if a bin's remaining capacity\n        # is exactly equal to the item's size.\n        epsilon = 1e-9\n        priorities[eligible_bins_mask] = 1.0 / (differences + epsilon)\n        \n        # Normalize priorities to have a maximum of 1 (optional, but can help in some scenarios)\n        # max_priority = np.max(priorities[eligible_bins_mask])\n        # if max_priority > 0:\n        #     priorities[eligible_bins_mask] /= max_priority\n            \n    return priorities",
    "response_id": 15,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 57.110323830864054,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response16.txt_stdout.txt",
    "code_path": "problem_iter1_code16.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score prioritizes bins that are a \"good fit\" for the item.\n    A good fit is defined as a bin where the remaining capacity is slightly larger\n    than the item size. This strategy aims to leave larger gaps in other bins\n    for potentially larger future items, while efficiently using space in the\n    current bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Ensure item size is not larger than any bin's capacity (otherwise it can't fit)\n    # For items that cannot fit, assign a very low priority (-inf)\n    valid_bins_mask = bins_remain_cap >= item\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # For bins where the item can fit\n    remaining_capacities_for_valid_bins = bins_remain_cap[valid_bins_mask]\n\n    # Calculate the difference between remaining capacity and item size\n    # This represents how much \"extra\" space is left after placing the item.\n    # We want this difference to be small but positive for a good fit.\n    diff = remaining_capacities_for_valid_bins - item\n\n    # Apply a sigmoid-like function to map the differences to priorities.\n    # A steep sigmoid centered around 0 (meaning diff=0) would give the highest score.\n    # We want to penalize bins that are too small (negative diff, item doesn't fit, already handled by -inf)\n    # and bins that are too large (large positive diff, leaving a lot of wasted space).\n    # A common approach is to use a sigmoid scaled and shifted.\n    # Let's center the sigmoid around 0. The logistic function is 1 / (1 + exp(-x)).\n    # To prioritize bins with diff close to 0, we can use exp(-abs(diff)).\n    # Alternatively, we can use a transformed diff in the sigmoid:\n    # sigmoid(k * (target_diff - diff)). We want target_diff to be close to 0.\n    # So, exp(-k * diff) or exp(-k * abs(diff)) might be suitable.\n    # Let's try exp(-k * diff). If diff is small and positive, exp(-k*diff) is close to 1.\n    # If diff is negative (item doesn't fit), exp(-k*diff) would be large, which is good for negative values,\n    # but we've already filtered those.\n    #\n    # A more robust sigmoid fit could be:\n    # priority = 1 / (1 + exp(-k * (remaining_capacity - item - margin)))\n    # where 'margin' is a desired small positive buffer.\n    # For simplicity, let's use a Gaussian-like shape centered around zero difference,\n    # which is achieved by exp(-diff^2) or similar.\n\n    # Let's use exp(-k * diff) where k is a scaling factor. A larger k makes the preference for\n    # a tight fit more pronounced.\n    k = 2.0  # Sensitivity parameter, adjust as needed. Higher k means tighter fit preferred.\n    priorities[valid_bins_mask] = np.exp(-k * diff)\n\n    # To make it more \"priority-like\" where higher means more desirable,\n    # and we want to strongly prefer bins that are just big enough,\n    # we can also consider how much space is left.\n    # A simple logistic function centered around the \"ideal\" remaining capacity might be:\n    # ideal_remaining_capacity = item  (or item + a small buffer)\n    # Let's try a function that is high when remaining_capacity is slightly larger than item, and decreases otherwise.\n    # For example, a Gaussian-like shape: exp(-((remaining_capacity - item - buffer)**2) / sigma**2)\n    # Let's use a simpler approach: 1 / (1 + exp(k * (item - remaining_capacity)))\n    # If remaining_capacity is slightly larger than item, (item - remaining_capacity) is small negative, exp is close to 0, priority is close to 1.\n    # If remaining_capacity is much larger than item, (item - remaining_capacity) is large negative, exp is close to 0, priority is close to 1. This is not ideal.\n    #\n    # Let's re-evaluate the goal: we want to select bins where the remaining capacity is *just* enough.\n    # This means `remaining_capacity - item` should be small and positive.\n    # So, `diff = remaining_capacity - item` should be close to zero.\n    # A function that peaks at diff=0 would be `exp(-c * diff^2)` or `1 / (1 + exp(-c * diff))`.\n    #\n    # Let's refine using the sigmoid idea for \"fit score\":\n    # Consider the \"waste\" ratio: `(remaining_capacity - item) / remaining_capacity` if remaining_capacity > 0.\n    # We want this ratio to be small.\n    # Let's try prioritizing bins where `remaining_capacity` is close to `item`.\n    # This means `remaining_capacity / item` is close to 1.\n\n    # Let's try a sigmoid applied to the difference:\n    # sigmoid(a * (ideal_fit - (remaining_capacity - item)))\n    # ideal_fit = 0 (meaning we want remaining_capacity - item = 0)\n    # So, sigmoid(a * (- (remaining_capacity - item))) = sigmoid(a * (item - remaining_capacity))\n    # If remaining_capacity is just slightly larger than item, `item - remaining_capacity` is small negative, sigmoid output is ~0.5\n    # If remaining_capacity is much larger than item, `item - remaining_capacity` is large negative, sigmoid output is ~0.\n    # If remaining_capacity is less than item, `item - remaining_capacity` is positive, sigmoid output is ~1. This is also not good as it implies larger bins are better when item doesn't fit.\n\n    # Let's use a sigmoid that peaks at `remaining_capacity == item` (i.e., `diff == 0`).\n    # A common choice for this is a scaled Gaussian-like function or a logistic function focused on the difference.\n    # Let's use `1 / (1 + exp(k * abs(diff)))`. This peaks at diff=0 (score=1) and decays as diff increases.\n    # We already filtered for diff >= 0. So, `abs(diff)` is just `diff`.\n    # `1 / (1 + exp(k * diff))`.\n    # If diff = 0, score = 1.\n    # If diff is small positive (e.g., 0.1), score is slightly less than 1.\n    # If diff is large positive, score is close to 0.\n    # This means bins that *exactly* fit the item are prioritized the most, and bins with much larger capacity are de-prioritized.\n\n    k_sigmoid = 5.0 # Controls how sharply the priority drops as the fit becomes less tight.\n    priorities[valid_bins_mask] = 1.0 / (1.0 + np.exp(k_sigmoid * diff))\n\n    # Optional: Normalize priorities or add a small constant to avoid zero priorities if needed,\n    # but for this strategy, zero is a valid low priority if nothing fits.\n    # The current approach ensures items that can fit have priority > 0 and <= 1.\n\n    return priorities",
    "response_id": 16,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 89.92418250750748,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response17.txt_stdout.txt",
    "code_path": "problem_iter1_code17.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Random Fit strategy.\n\n    The Random Fit strategy assigns a random priority to each bin that can accommodate the item.\n    This encourages exploration of different bin assignments.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Assign random priorities to bins that can fit the item\n    # A positive random number ensures these bins are preferred over those that can't fit (priority 0)\n    priorities[can_fit_mask] = np.random.rand(np.sum(can_fit_mask))\n    \n    return priorities",
    "response_id": 17,
    "obj": 4.487435181491823,
    "cyclomatic_complexity": 1.0,
    "halstead": 4.754887502163469,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response18.txt_stdout.txt",
    "code_path": "problem_iter1_code18.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Exploration parameter\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Identify bins that can fit the item\n    suitable_bins_indices = np.where(bins_remain_cap >= item)[0]\n\n    if len(suitable_bins_indices) == 0:\n        # If no bin can fit the item, return all zeros (or handle as an error)\n        return priorities\n\n    # Greedy part: favor bins with less remaining capacity (best fit)\n    # We use a negative value so that a higher value (less remaining capacity) results in a higher priority score\n    greedy_scores = -bins_remain_cap[suitable_bins_indices]\n\n    # Exploration part: with probability epsilon, choose a random suitable bin\n    # We want to assign a high, uniform priority to these bins\n    exploration_scores = np.ones(len(suitable_bins_indices)) * 1e9  # Arbitrarily high score\n\n    # Combine greedy and exploration\n    # With probability 1-epsilon, use greedy scores\n    # With probability epsilon, use exploration scores\n    # We can achieve this by generating a random number for each suitable bin and comparing with epsilon\n\n    rand_nums = np.random.rand(len(suitable_bins_indices))\n    combined_scores = np.where(rand_nums < epsilon, exploration_scores, greedy_scores)\n\n    # Assign the calculated scores to the correct bins in the priorities array\n    priorities[suitable_bins_indices] = combined_scores\n\n    return priorities",
    "response_id": 18,
    "obj": 5.873554048663751,
    "cyclomatic_complexity": 2.0,
    "halstead": 53.30296890880645,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    This heuristic prioritizes bins that are a \"good fit\" for the item,\n    meaning they have just enough remaining capacity. It uses a Softmax\n    function to convert these fit scores into probabilities (priorities).\n    A higher priority is given to bins where the remaining capacity is\n    closer to the item size.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the difference between remaining capacity and item size.\n    # We are looking for bins where this difference is small and non-negative.\n    # Add a small epsilon to avoid division by zero if item size is exactly 0\n    # and also to push values away from zero if they are exactly zero.\n    diffs = bins_remain_cap - item + 1e-9\n\n    # Only consider bins that have enough capacity for the item\n    # For bins with insufficient capacity, assign a very low priority score\n    # (effectively 0 after Softmax, as negative values in exp are small)\n    # A very large negative number will result in a very small exponent.\n    valid_fit_scores = np.where(diffs >= 0, diffs, -np.inf)\n\n    # Apply the Softmax function.\n    # exp(x) can become very large or very small.\n    # We want to maximize the probability for bins that are a good fit (small positive diffs).\n    # To achieve this with Softmax, we can either use the differences directly or transform them.\n    # A common approach for \"good fit\" is to favor smaller differences.\n    # Let's consider the inverse of the difference as a measure of \"goodness of fit\",\n    # but only for bins that can fit the item.\n    # To avoid division by zero and make smaller differences have higher values,\n    # we can use 1 / (diff + 1) or similar.\n\n    # Let's redefine the \"fit score\" to be higher for better fits.\n    # A good fit is when `bins_remain_cap` is close to `item`.\n    # So, we want to penalize bins with large remaining capacity.\n    # We can use `1.0 / (bins_remain_cap - item + epsilon)` for bins that fit.\n    # However, Softmax works better with values that can be large and positive.\n    # Let's consider `item / bins_remain_cap` for bins where `bins_remain_cap >= item`.\n    # This would prioritize bins that are more full relative to their remaining capacity\n    # after placing the item. This interpretation is more like \"First Fit Decreasing\" style.\n\n    # For Softmax-Based Fit, a common approach is to convert the \"badness of fit\"\n    # (e.g., remaining capacity - item_size) into a probability.\n    # A smaller positive difference is better.\n    # We can use `exp(- (bins_remain_cap - item))` for bins that fit.\n    # However, this will favor bins with slightly *more* capacity left.\n    # To favor bins that are *closer*, we want to maximize `exp(-abs(bins_remain_cap - item))`.\n    # But we only care about `bins_remain_cap >= item`.\n\n    # Let's try to prioritize bins where the remaining capacity is JUST enough.\n    # This means `bins_remain_cap - item` should be small and non-negative.\n    # A simple way to convert this to a score that Softmax can work with\n    # (i.e., values that are generally higher for better fits) is to use:\n    # score = 1 / (bins_remain_cap - item + epsilon) for valid bins.\n    # This gives higher scores to bins with less remaining space after placing the item.\n\n    # Alternative interpretation: Prioritize bins that are \"almost full\" with the item.\n    # This means `bins_remain_cap - item` should be small and non-negative.\n    # Let's use `exp(- (bins_remain_cap - item))` and then normalize.\n    # This would mean smaller positive differences get higher scores.\n\n    # Let's try a different approach that focuses on maximizing the utilization\n    # of the bin *after* placing the item, while still ensuring the item fits.\n    # The remaining capacity after placing the item is `bins_remain_cap - item`.\n    # We want to prioritize bins where `bins_remain_cap - item` is small and non-negative.\n    # A good \"fit score\" could be `1.0 / (bins_remain_cap - item + epsilon)`.\n    # However, Softmax is `exp(score)`. If score is small and positive, exp(score) is still small.\n    # If score is large and positive, exp(score) is large.\n    # So, we want scores to be *large* for good fits.\n    # This implies that `-(bins_remain_cap - item)` should be large, i.e., `item - bins_remain_cap` should be large.\n    # This is the opposite of what we want (small non-negative difference).\n\n    # Let's go back to the idea that a good fit means `bins_remain_cap - item` is minimal and non-negative.\n    # Let `fit_score = item / bins_remain_cap` for bins that can fit the item.\n    # This prioritizes bins that will be more filled *after* the item is placed relative to their original capacity.\n    # This is similar to Best Fit.\n    # However, Softmax expects scores, not probabilities directly.\n    # Let's map \"closeness\" to a positive score.\n    # The \"closeness\" is `bins_remain_cap - item`. We want this to be small and >= 0.\n    # We can transform this to `1.0 / (bins_remain_cap - item + epsilon)` as a \"goodness score\".\n    # Then use `exp(goodness_score)`.\n\n    # Let's try this:\n    # For each bin, calculate the \"remaining space ratio\" if the item is placed:\n    # `ratio = (bins_remain_cap - item) / item` if item > 0.\n    # We want to minimize this ratio (or have it close to zero).\n    # This is tricky for Softmax where we want to maximize values.\n    # Let's use `exp(-(bins_remain_cap - item))` for bins where `bins_remain_cap >= item`.\n    # This penalizes larger remaining spaces.\n\n    # Initialize priorities to a very small negative number (effectively zero after softmax)\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Consider only bins with enough capacity for the item\n    sufficient_capacity_indices = np.where(bins_remain_cap >= item)[0]\n\n    if len(sufficient_capacity_indices) > 0:\n        # Calculate the \"fitness\" as the negative difference between\n        # remaining capacity and item size. This makes smaller positive\n        # differences have higher (less negative) fitness values.\n        # Adding a small epsilon to `bins_remain_cap` before calculating the difference\n        # can help avoid issues if `bins_remain_cap` is exactly `item`, and also\n        # helps differentiate bins that are very close.\n        # `item - (bins_remain_cap - item)` or `2*item - bins_remain_cap` if we want to maximize.\n        # Let's use `bins_remain_cap - item` as the measure of excess space.\n        # We want this excess space to be small and non-negative.\n        # For Softmax, we want higher scores for better fits.\n        # A good fit has small `bins_remain_cap - item`.\n        # So, `-(bins_remain_cap - item)` would give higher scores for better fits.\n        # We want to maximize `item / bins_remain_cap` is like best fit if item is large relative to capacity.\n        # Let's use the inverse of the remaining capacity after placing the item,\n        # capped for positive fits.\n        # score = 1.0 / (bins_remain_cap[sufficient_capacity_indices] - item + 1e-9)\n        # This would give higher scores to bins that are nearly full.\n        # Then apply exp.\n\n        # A common interpretation for \"fit\" strategies like Best Fit is to\n        # minimize `bins_remain_cap - item`. For Softmax, we want to map\n        # this to a score where lower `bins_remain_cap - item` yields a higher score.\n        # Let `score_component = item / bins_remain_cap[sufficient_capacity_indices]`\n        # This prioritizes bins that are more full relative to their original capacity,\n        # after placing the item.\n\n        # Let's define the \"fit value\" as how much capacity remains after placing the item.\n        # We want this to be small and non-negative.\n        # So, `remaining_after_fit = bins_remain_cap[sufficient_capacity_indices] - item`.\n        # We want to give high scores when `remaining_after_fit` is small.\n        # Let's use `exp(-(remaining_after_fit))` to make smaller `remaining_after_fit` have higher scores.\n        # We can also add the item size as a factor to potentially favor bins that\n        # are more utilized overall. `exp(-(bins_remain_cap[sufficient_capacity_indices] - item) / item)` if item > 0.\n\n        # Let's consider the simple \"gap\" strategy.\n        # The gap is `bins_remain_cap - item`. We want this to be minimized.\n        # For Softmax, we want higher scores for smaller gaps.\n        # `score = - (bins_remain_cap[sufficient_capacity_indices] - item)` will do this.\n        # But this can result in very large negative scores if capacities are very different.\n\n        # A more robust approach is to use a transformation that keeps values within a reasonable range.\n        # Consider `1 / (bins_remain_cap[sufficient_capacity_indices] - item + epsilon)` which gives higher scores for bins closer to fitting.\n        # Or `item / bins_remain_cap[sufficient_capacity_indices]` which is like Best Fit.\n\n        # Let's try a combination: Prioritize bins that have *some* space left but not too much.\n        # `bins_remain_cap - item` is the remaining capacity after placing the item.\n        # We want this to be close to 0.\n        # Let's define a \"fitness score\" such that smaller non-negative differences get higher scores.\n        # `fit_score = 1.0 / (bins_remain_cap[sufficient_capacity_indices] - item + 1e-3)`\n        # Using `1e-3` as a small constant to avoid division by zero and create distinct scores.\n        # Higher fit_score means the bin is a better fit.\n\n        # For Softmax: exp(score)\n        # To maximize the probability for good fits, scores for good fits should be high.\n        # Let's use `bins_remain_cap[sufficient_capacity_indices] - item`.\n        # We want to minimize this. So, `-(bins_remain_cap[sufficient_capacity_indices] - item)`\n        # becomes our score candidate.\n\n        # Let's normalize the item size and bin capacities by some factor to keep numbers reasonable.\n        # Or simply use a scaled difference.\n        # Example: `score = - (bins_remain_cap[sufficient_capacity_indices] - item) / item` (if item > 0)\n        # This makes the penalty proportional to the item size.\n\n        # Let's refine the \"good fit\" idea: we want `bins_remain_cap` to be slightly larger than `item`.\n        # So, `bins_remain_cap - item` should be small and positive.\n        # Let's map `bins_remain_cap - item` to a value that is higher for smaller positive values.\n        # `score = 1.0 / (bins_remain_cap[sufficient_capacity_indices] - item + epsilon)` works.\n        # Let's use `exp(1.0 / (bins_remain_cap[sufficient_capacity_indices] - item + 1e-9))`\n\n        # Applying Softmax: exp(scores) / sum(exp(scores))\n        # If we use `exp(1.0 / (bins_remain_cap[sufficient_capacity_indices] - item + 1e-9))`,\n        # then bins with a small positive difference will have a large `1.0 / (...)`,\n        # and thus a large exponential, leading to a high priority.\n\n        # Alternative: Consider the ratio of remaining capacity to item size.\n        # `ratio = (bins_remain_cap[sufficient_capacity_indices] - item) / item`\n        # We want this ratio to be small. So `-(ratio)` would work.\n\n        # Let's go with a simple \"gap\" approach for fit: prioritize bins where `bins_remain_cap` is close to `item`.\n        # We can use `1.0 / (bins_remain_cap - item + epsilon)` as a measure of \"how good the fit is\".\n        # A smaller difference `bins_remain_cap - item` leads to a larger `1.0 / (...)`.\n        # Then, we can exponentiate this for Softmax.\n\n        # The actual remaining capacity after placing the item.\n        remaining_after_fit = bins_remain_cap[sufficient_capacity_indices] - item\n\n        # Create a \"fitness score\" where smaller `remaining_after_fit` values yield higher scores.\n        # Use `1.0 / (remaining_after_fit + epsilon)` to map small positive values to large values.\n        # Add `1e-6` to `remaining_after_fit` to avoid division by zero if `remaining_after_fit` is exactly 0.\n        # A value of 0 remaining capacity is a perfect fit.\n        # A very small positive remaining capacity is also a good fit.\n        # So, `1 / (small_positive_number)` is large.\n\n        fitness_scores = 1.0 / (remaining_after_fit + 1e-6)\n\n        # Apply the exponential function to convert fitness scores into values suitable for Softmax.\n        # Larger fitness scores will result in larger exponential values.\n        exp_scores = np.exp(fitness_scores)\n\n        # Assign these calculated priorities to the corresponding bins\n        priorities[sufficient_capacity_indices] = exp_scores\n\n    # Normalize the priorities using Softmax.\n    # If all bins were insufficient, priorities will be all -inf, resulting in division by zero.\n    # Handle this case by returning zeros or a uniform distribution.\n    # If only one bin is sufficient, its priority will be 1.0.\n    sum_exp_scores = np.sum(priorities)\n\n    if sum_exp_scores == 0 or np.isinf(sum_exp_scores):\n        # If no bins fit or all scores are infinite (e.g., item size 0 and capacity large)\n        # return a uniform distribution or zeros if no bins can fit.\n        # If all scores are -inf, sum is 0. We need to handle this to avoid division by zero.\n        # If all were -inf, it means no bin had sufficient capacity.\n        return np.zeros_like(bins_remain_cap)\n    else:\n        # Softmax normalization: exp(score) / sum(exp(scores))\n        # However, our `priorities` array already contains `exp(score)`\n        # (or -inf for non-fitting bins).\n        # So, we just need to divide by the sum of these exponential values.\n        normalized_priorities = priorities / sum_exp_scores\n        return normalized_priorities",
    "response_id": 19,
    "obj": 4.517351416035098,
    "cyclomatic_complexity": 4.0,
    "halstead": 171.8226790216648,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response20.txt_stdout.txt",
    "code_path": "problem_iter1_code20.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a First Fit strategy.\n\n    The First Fit strategy aims to place the item in the first bin that has enough\n    remaining capacity. For a priority function, we can assign a higher priority\n    to bins that can accommodate the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Assign a high priority to bins that have enough capacity for the item.\n    # Assign a low priority to bins that do not have enough capacity.\n    # This simple approach prioritizes bins that can fit the item,\n    # and the 'first fit' nature is handled by the selection mechanism\n    # that iterates through these priorities.\n    priorities = np.where(bins_remain_cap >= item, 1.0, 0.0)\n\n    # A slightly more nuanced approach could consider how \"tight\" the fit is.\n    # However, for pure First Fit, just being able to fit is the primary criterion.\n    # For this 'priority' function to work with a 'highest priority score'\n    # selection, we can assign higher scores to bins that fit.\n\n    # A common way to implement First Fit with a priority queue\n    # is to ensure that the bins that are encountered first and have\n    # capacity are chosen. The prompt asks for a priority score where\n    # the *highest* score is chosen. This is slightly contradictory\n    # to a strict \"first encountered\" rule unless we can encode that\n    # order.\n\n    # Let's assume the selection process *iterates* through the returned\n    # priorities array and picks the first one with a non-zero (or highest)\n    # priority. To mimic First Fit directly: we want the *earliest* bin\n    # that fits to have the highest priority.\n\n    # A simple way to encode \"first fit\" into a priority score where higher is better:\n    # Give a high priority to the *first* bin that can fit the item, and lower\n    # or zero priority to others. This still doesn't quite capture \"first fit\"\n    # if multiple bins fit.\n\n    # Let's re-interpret: the priority function gives a score to each bin.\n    # The *selection* mechanism then picks the bin based on these scores.\n    # If we want to strictly follow First Fit, the priority should reflect\n    # the *order* of the bins.\n\n    # Let's assign priority based on index if it fits. Lower index = higher priority.\n    # However, the prompt says \"highest priority score\". So, if bin 0 fits,\n    # it should have a higher score than bin 5 if bin 5 also fits.\n\n    # A direct implementation of \"highest priority score\" where higher means better,\n    # and we want to select the *first* bin that fits.\n    # We can give a very large number to the first bin that fits, and a smaller number\n    # to subsequent bins that fit, and 0 to those that don't.\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    bin_indices = np.arange(len(bins_remain_cap))\n\n    # Assign a score that favors earlier bins that fit.\n    # We can give a score that decreases with index for bins that fit.\n    # This way, the first bin that fits (smallest index) will have the highest score.\n    fits_mask = bins_remain_cap >= item\n    \n    # If a bin fits, assign a priority. Higher priority for earlier bins.\n    # Let's assign a priority score based on (number_of_bins - index).\n    # This ensures that smaller indices (earlier bins) get higher priority scores.\n    # For example, if there are 10 bins, index 0 gets 10, index 1 gets 9, ..., index 9 gets 1.\n    num_bins = len(bins_remain_cap)\n    priorities[fits_mask] = num_bins - bin_indices[fits_mask]\n\n    # Alternatively, if we want to prioritize the *tightest* fit among those that fit,\n    # but still honor the \"first fit\" concept by giving a bonus to earlier bins.\n    # However, the prompt focuses on First Fit strategy. The core of First Fit is\n    # to pick the first available bin. Encoding this directly into a priority score\n    # where \"highest score wins\" is tricky without knowing the selection method.\n\n    # Assuming selection iterates and picks the highest score:\n    # To simulate \"First Fit\" behaviour (picking the *first* bin that fits),\n    # we can assign a higher priority to bins earlier in the array if they fit.\n    # Example: if bins_remain_cap = [5, 3, 7] and item = 4.\n    # Bin 0 fits (5 >= 4)\n    # Bin 1 does not fit (3 < 4)\n    # Bin 2 fits (7 >= 4)\n    # For First Fit, we'd pick Bin 0.\n    # To achieve this with highest score: Bin 0 needs highest score.\n    # We can assign priority based on (MaxPossiblePriority - Index), for bins that fit.\n    # Let's use a large constant for \"MaxPossiblePriority\".\n    # If we use `num_bins - bin_indices[fits_mask]`, then for the example above:\n    # Bin 0: fits, index=0, num_bins=3. Priority = 3 - 0 = 3.\n    # Bin 1: doesn't fit. Priority = 0.\n    # Bin 2: fits, index=2, num_bins=3. Priority = 3 - 2 = 1.\n    # Priorities: [3, 0, 1]. The highest score is 3, corresponding to bin 0.\n\n    return priorities",
    "response_id": 20,
    "obj": 4.487435181491823,
    "cyclomatic_complexity": 1.0,
    "halstead": 23.264662506490403,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response21.txt_stdout.txt",
    "code_path": "problem_iter1_code21.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function implements the Best Fit heuristic for the online Bin Packing Problem.\n    The priority is higher for bins that have just enough remaining capacity to fit the item,\n    effectively aiming to \"fill up\" bins before opening new ones.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that can fit the item will have a priority score related to the\n        \"tightness\" of the fit (closer remaining capacity to item size).\n        Bins that cannot fit the item will have a priority score of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate priority based on how tightly they fit.\n    # A higher priority is given to bins where the remaining capacity is closest to the item size.\n    # This is achieved by using the inverse of the difference, effectively minimizing the wasted space.\n    # We add a small epsilon to the denominator to avoid division by zero if a bin has exactly 'item' remaining capacity.\n    priorities[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)\n\n    return priorities",
    "response_id": 21,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 1.0,
    "halstead": 41.51317942364757,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response22.txt_stdout.txt",
    "code_path": "problem_iter1_code22.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    In Worst Fit, we want to place the item in the bin that has the most remaining capacity.\n    This heuristic aims to leave smaller capacities in other bins for potentially smaller items later.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # For bins where the item can fit\n    # The priority is simply the remaining capacity itself.\n    # The higher the remaining capacity, the higher the priority for Worst Fit.\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask]\n\n    # For bins where the item cannot fit, the priority is effectively zero (or a very low value)\n    # as they are not valid choices. The np.zeros_like initialization already handles this.\n\n    return priorities",
    "response_id": 22,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 1.0,
    "halstead": 4.754887502163469,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response23.txt_stdout.txt",
    "code_path": "problem_iter1_code23.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using the Almost Full Fit strategy.\n\n    The strategy prioritizes bins that will be \"almost full\" after packing the item.\n    A bin is considered \"almost full\" if its remaining capacity after packing the item\n    is small. We want to maximize this small remaining capacity (i.e., minimize it).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity if the item were placed in each bin\n    potential_remain_cap = bins_remain_cap - item\n\n    # We want to prioritize bins where potential_remain_cap is small.\n    # However, we can only place items in bins where they fit.\n    # So, for bins where the item doesn't fit, assign a very low priority (e.g., negative infinity)\n    # to effectively exclude them from selection.\n    # For bins where the item fits, the priority is higher if potential_remain_cap is smaller.\n    # A simple way to achieve this is to take the negative of the potential remaining capacity.\n    # This makes smaller positive remaining capacities result in larger (less negative) scores,\n    # but we want to *minimize* remaining capacity.\n    # A better approach for \"almost full\" is to consider the *inverse* or some function\n    # that grows as remaining capacity shrinks towards zero.\n    # For simplicity and to capture the \"almost full\" idea, let's prioritize bins that have\n    # just enough space for the item, aiming for the smallest non-negative remaining capacity.\n\n    # Initialize priorities to a very low value for bins that cannot accommodate the item\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Find indices of bins where the item fits\n    fit_indices = np.where(bins_remain_cap >= item)[0]\n\n    # For bins where the item fits, calculate the priority.\n    # The \"almost full\" strategy aims to leave the smallest possible remaining capacity.\n    # So, we want to maximize the negative of the remaining capacity after fitting.\n    # This means a remaining capacity closer to 0 will have a higher priority score.\n    if len(fit_indices) > 0:\n        priorities[fit_indices] = -(bins_remain_cap[fit_indices] - item)\n\n    # An alternative interpretation of \"almost full fit\" could be to favor bins\n    # that become full (remaining capacity 0) or nearly full.\n    # If we want to strictly favor bins that result in the *smallest positive* remaining capacity,\n    # the above `-(bins_remain_cap[fit_indices] - item)` works.\n    # If we want to slightly favor bins that are *already* close to full and can still take the item,\n    # we could add a term related to the current remaining capacity.\n    # For this specific problem statement, \"Almost Full Fit\" implies leaving minimal remainder.\n\n    # To make it slightly more robust, we can consider a small penalty for bins that are\n    # excessively large, to avoid filling up bins that could potentially hold larger items later.\n    # However, the core \"Almost Full Fit\" is about minimizing leftover space.\n\n    # Let's refine the priority to directly reflect \"almost full\".\n    # We want to pick the bin that, after fitting the item, has the smallest remaining capacity,\n    # but importantly, that remaining capacity should be non-negative.\n    # If multiple bins have the same smallest remaining capacity, any selection is fine by this heuristic.\n\n    # Re-calculate priorities: For fitting bins, priority is `1 / (1 + remaining_capacity_after_fit)`.\n    # This maps smaller remaining capacities to higher priorities. Adding 1 prevents division by zero\n    # if remaining capacity becomes exactly 0, and ensures a positive priority.\n    if len(fit_indices) > 0:\n        # Calculate remaining capacity for fitting bins\n        remaining_after_fit = bins_remain_cap[fit_indices] - item\n        # Assign priorities: higher for smaller remaining capacity\n        # Using 1/(1+x) makes values closer to 1 for smaller x.\n        priorities[fit_indices] = 1.0 / (1.0 + remaining_after_fit)\n\n    # The -np.inf for non-fitting bins ensures they are never chosen.\n\n    return priorities",
    "response_id": 23,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 3.0,
    "halstead": 116.75790004038474,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response24.txt_stdout.txt",
    "code_path": "problem_iter1_code24.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    The Exact Fit First (EFF) strategy prioritizes bins that can accommodate the item\n    and have the smallest remaining capacity after placement, as this aims to leave\n    larger remaining capacities in other bins for potentially larger future items.\n    A large positive value is assigned to bins that can fit the item, and a\n    negative value (or zero) for bins that cannot.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Create a mask for bins that have enough capacity for the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities to a low value (e.g., negative infinity)\n    # to indicate that bins that cannot fit the item have no priority.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # For bins that can fit the item, assign a priority based on the\n    # Exact Fit First strategy: the bin with the smallest remaining capacity\n    # *after* placing the item gets the highest priority.\n    # This is equivalent to prioritizing bins with the smallest current remaining capacity.\n    # We negate the remaining capacity so that smaller capacities result in higher (less negative) priority scores.\n    # We add a small epsilon to the priority of bins that perfectly fit,\n    # to prefer perfect fits over bins that have just enough space and more leftover.\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # To strictly implement \"Exact Fit First\", we can also give a slight bonus\n    # to bins where the remaining capacity *after* placement is exactly zero.\n    perfect_fit_mask = (bins_remain_cap - item) == 0\n    priorities[perfect_fit_mask] += 0.1 # A small bonus for a perfect fit\n\n    return priorities",
    "response_id": 24,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 38, in priority_v2\nnumpy._core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'\n1\n72.33974351909447\n"
  },
  {
    "stdout_filepath": "problem_iter1_response25.txt_stdout.txt",
    "code_path": "problem_iter1_code25.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit) strategy.\n\n    The priority is higher for bins that have a remaining capacity closer to the item size.\n    Specifically, it calculates 1 / (1 + |remaining_capacity - item_size|) for each bin.\n    A small epsilon is added to the denominator to avoid division by zero if remaining_capacity == item_size.\n    Bins that cannot fit the item have a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities to zero for all bins\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate priorities only for bins that can fit the item\n    if np.any(can_fit_mask):\n        # Calculate the difference between remaining capacity and item size\n        diff = bins_remain_cap[can_fit_mask] - item\n\n        # The priority is the inverse of the distance (plus a small epsilon to avoid division by zero)\n        # Higher priority for smaller absolute differences, meaning the bin is a closer fit.\n        # We use 1 + abs(diff) because we want smaller differences to result in higher priorities.\n        # The '+ 1' is to ensure that a perfect fit (diff=0) still has a non-zero priority,\n        # and also avoids division by zero if diff is 0.\n        priorities[can_fit_mask] = 1.0 / (1.0 + np.abs(diff))\n\n    return priorities",
    "response_id": 25,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 39.863137138648355,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response26.txt_stdout.txt",
    "code_path": "problem_iter1_code26.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score strategy.\n\n    The Sigmoid Fit Score strategy aims to prioritize bins that are a \"good fit\" for the item,\n    meaning they have enough remaining capacity but not an excessive amount, to minimize wasted space.\n    This is achieved by using a sigmoid function to map the ratio of remaining capacity to\n    the item size to a priority score.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We want to prioritize bins where bins_remain_cap is slightly larger than item\n    # to minimize wasted space. A perfect fit would be bins_remain_cap == item.\n    # If bins_remain_cap < item, the item cannot fit, so we assign a very low priority.\n    # If bins_remain_cap is much larger than item, the waste is high, so priority decreases.\n\n    # To avoid division by zero if item is 0 (though unlikely in BPP), or very small bins_remain_cap\n    # that are still >= item, we can offset the ratio calculation slightly or handle cases.\n    # For simplicity here, we assume item > 0 and bins_remain_cap are reasonable.\n\n    # Calculate the \"fit ratio\": remaining capacity relative to the item size.\n    # A ratio close to 1 (remaining_cap / item \u2248 1) suggests a good fit.\n    # We'll consider `remaining_cap - item` as a measure of slack.\n    # The sigmoid function typically maps values from (-inf, inf) to (0, 1).\n    # We want the \"peak\" of our priority around when remaining_cap - item is small and positive.\n\n    # Let's use the sigmoid of a transformation of `remaining_cap - item`.\n    # A common sigmoid is 1 / (1 + exp(-x)).\n    # If `remaining_cap - item` is large and positive, `exp(-x)` is small, sigmoid is close to 1.\n    # If `remaining_cap - item` is large and negative (item too big), `exp(-x)` is large, sigmoid is close to 0.\n    # If `remaining_cap - item` is 0, sigmoid is 0.5.\n\n    # We want higher priority when `remaining_cap` is just enough or slightly more than `item`.\n    # So, we want to map `remaining_cap - item` such that small positive values are mapped to high scores,\n    # and large positive/negative values are mapped to low scores.\n\n    # Consider the difference `bins_remain_cap - item`.\n    # If `bins_remain_cap < item`, the difference is negative. We want a score of 0.\n    # If `bins_remain_cap >= item`, the difference is non-negative.\n    # We want higher scores when this difference is small.\n\n    # Let's transform `bins_remain_cap - item` into something that has a peak.\n    # A Gaussian-like shape or a bell curve could work. The sigmoid itself doesn't create a peak directly.\n    # However, the \"Sigmoid Fit Score\" typically implies using the sigmoid to represent the likelihood\n    # of a \"good fit\". A common interpretation is that a bin is \"good\" if it has enough capacity,\n    # and among those, the one that leaves the least waste is preferred.\n\n    # Let's re-interpret \"Sigmoid Fit Score\" as a heuristic that prioritizes bins that are\n    # \"almost full\" but still able to accommodate the item.\n    # We can model this by:\n    # 1. Discarding bins that cannot fit the item.\n    # 2. For bins that can fit, assign a score based on how full they become after adding the item.\n    #    The fuller they become (closer to full bin capacity), the better.\n    #    This implies prioritizing bins where `bins_remain_cap - item` is minimized.\n\n    # Let's try mapping `item / bins_remain_cap` to a sigmoid.\n    # If `item / bins_remain_cap` is close to 1, it's a good fit (but `bins_remain_cap` must be >= `item`).\n    # This ratio is <= 1 for bins that can fit the item.\n\n    # If `bins_remain_cap < item`, the item cannot fit. We assign a priority of 0.\n    # If `bins_remain_cap >= item`, we can calculate a score.\n    # A common Sigmoid approach to measure \"goodness\" of a value X might be:\n    # sigmoid(k * (X - threshold)) which peaks around X = threshold.\n    # We want the \"fit\" to be good when `bins_remain_cap` is just above `item`.\n    # So, we want the `threshold` to be `item`.\n\n    # Let X be `bins_remain_cap`.\n    # We want a score that is high when `bins_remain_cap` is near `item` (but >= `item`).\n    # Sigmoid `1 / (1 + exp(-k * (bins_remain_cap - item)))` has an inflection point at `bins_remain_cap = item`.\n    # If `k > 0`, the sigmoid is increasing. So it's high when `bins_remain_cap` is large. Not what we want.\n\n    # Let's consider the \"slack\": `bins_remain_cap - item`. We want this to be small and positive.\n    # Consider mapping `-(bins_remain_cap - item)` = `item - bins_remain_cap`.\n    # If `bins_remain_cap < item`, this is positive.\n    # If `bins_remain_cap >= item`, this is zero or negative.\n\n    # Let's try to maximize the score when `bins_remain_cap - item` is 0 or very small.\n    # Let the score be based on `1 / (1 + exp(k * (bins_remain_cap - item)))`.\n    # If `k > 0`:\n    #   - `bins_remain_cap - item` small positive: exp(large positive) -> score ~ 0\n    #   - `bins_remain_cap - item` is 0: exp(0) = 1 -> score = 0.5\n    #   - `bins_remain_cap - item` small negative: exp(small negative) -> score ~ 1\n\n    # This is inverted. We want small positive `bins_remain_cap - item` to yield high scores.\n    # So, perhaps map `-(bins_remain_cap - item)` such that large negative values are high.\n    # `1 / (1 + exp(-k * -(bins_remain_cap - item)))` = `1 / (1 + exp(k * (bins_remain_cap - item)))`\n    # This again gives high scores for negative differences (item too big).\n\n    # The standard \"Sigmoid Fit\" or \"Best Fit\" heuristic for online BPP usually means picking\n    # the bin that has the smallest remaining capacity *after* the item is placed, provided\n    # it can accommodate the item. This is essentially minimizing `bins_remain_cap - item`.\n\n    # Let's model this using a sigmoid function, but ensuring the peak is where we want it.\n    # A common way to get a peak is `exp(-x^2)` or a similar function.\n    # However, if we are to strictly use Sigmoid as the *primary* transformation:\n    # We can consider the \"degree of fit\".\n    # A high degree of fit means `bins_remain_cap` is just enough for `item`.\n\n    # Let's define a \"fit quality\" function.\n    # We want to prioritize bins where `bins_remain_cap >= item`.\n    # Among these, we prefer bins with smaller `bins_remain_cap - item`.\n\n    # Consider a score that decreases as `bins_remain_cap - item` increases.\n    # And is 0 if `bins_remain_cap < item`.\n\n    # Option 1: Gaussian-like score on the slack, scaled and shifted.\n    # `exp(-k * (bins_remain_cap - item)^2)`\n    # But this is not a sigmoid.\n\n    # Option 2: Use sigmoid on a metric that has a peak.\n    # Consider the ratio `item / bins_remain_cap`. This is good when close to 1, but `bins_remain_cap >= item`.\n    # If `bins_remain_cap < item`, ratio is > 1.\n    # Let's analyze `item / bins_remain_cap` for valid bins (`bins_remain_cap >= item`).\n    # The ideal ratio is 1. We want a score that is high when this ratio is close to 1.\n\n    # A sigmoid function like `1 / (1 + exp(-k * (ratio - 1)))` where `k > 0` would be high when `ratio > 1`.\n    # We need it high when `ratio <= 1` and close to 1.\n    # Consider `1 / (1 + exp(-k * (1 - ratio)))`.\n    # If `ratio = 1`, score is 0.5.\n    # If `ratio < 1` (meaning `item < bins_remain_cap`), `1 - ratio` is positive, `exp` is large, score ~ 0.\n    # If `ratio` is very small (large `bins_remain_cap`), `1 - ratio` is large, score ~ 0.\n\n    # This implies that if we want to directly apply a sigmoid to a metric that should have a peak,\n    # we need to transform the metric itself.\n\n    # A pragmatic approach for \"Sigmoid Fit Score\" can be:\n    # Prioritize bins that are \"close\" to fitting the item.\n    # This means `bins_remain_cap` is slightly larger than `item`.\n    # Let's define a \"goodness\" parameter `g = bins_remain_cap - item`.\n    # We want `g` to be small and positive.\n    # Sigmoid function is monotonic. We need to get a peak.\n\n    # Let's interpret \"Sigmoid Fit Score\" as using a logistic function to\n    # assign higher priority to bins that are \"sufficiently full\" after placing the item.\n    # Consider the new remaining capacity: `new_rem_cap = bins_remain_cap - item`.\n    # We want to prioritize bins where `new_rem_cap` is small and non-negative.\n\n    # Let's try to maximize `sigmoid(some_function_of(new_rem_cap))`.\n    # A score that is high for small non-negative `new_rem_cap`:\n    # For `new_rem_cap < 0` (item doesn't fit), score = 0.\n    # For `new_rem_cap >= 0`:\n    # We want score to decrease as `new_rem_cap` increases.\n    # A simple transformation for `new_rem_cap` to map it to a decreasing range:\n    # Let `score_input = -new_rem_cap`.\n    # Then use sigmoid: `1 / (1 + exp(-k * score_input))` = `1 / (1 + exp(k * new_rem_cap))`\n    # If `k > 0`:\n    #   - `new_rem_cap` is small positive: `exp` is slightly > 1, score is slightly < 0.5.\n    #   - `new_rem_cap` is 0: `exp` is 1, score is 0.5.\n    #   - `new_rem_cap` is large positive: `exp` is large, score ~ 0.\n\n    # This is still inverted if we want high score for small positive slack.\n    # Let's invert the sigmoid mapping: `1 / (1 + exp(k * (bins_remain_cap - item)))`\n    # If `k > 0`:\n    #   - `bins_remain_cap - item` small positive: score slightly < 0.5\n    #   - `bins_remain_cap - item` is 0: score = 0.5\n    #   - `bins_remain_cap - item` small negative: score slightly > 0.5\n\n    # To achieve the \"Best Fit\" idea (smallest positive slack) with a Sigmoid,\n    # we need to construct the input to the sigmoid carefully.\n\n    # Consider `k` as a steepness parameter for the sigmoid.\n    # Let's use the ratio of remaining capacity *after* packing to the bin's original capacity.\n    # Or perhaps, the ratio of *used* capacity to the bin's capacity.\n    # This would be `item / initial_bin_capacity`. Not available.\n\n    # The \"Sigmoid Fit\" heuristic often refers to a specific family of heuristics,\n    # where a score is derived from `remaining_capacity / item_size`.\n    # Let `ratio = bins_remain_cap / item`.\n    # We want high scores when `ratio` is close to 1, and `ratio >= 1`.\n    # `sigmoid(k * (ratio - 1))` with `k > 0` gives high scores for `ratio > 1`.\n\n    # Let's consider the slack as the input to the sigmoid.\n    # `slack = bins_remain_cap - item`. We want to penalize large slack.\n    # So, we want the score to be high for small *non-negative* slack.\n\n    # We can model this by mapping slack to a value that, when fed into a sigmoid,\n    # produces the desired priority.\n\n    # Let's define a custom score transformation:\n    # For bins that cannot fit: priority = 0\n    # For bins that can fit: priority = f(bins_remain_cap - item) where f is high for small args.\n    # Let `slack = bins_remain_cap - item`.\n\n    # Try this mapping:\n    # score = 1 - sigmoid(k * slack)  or sigmoid(-k * slack)\n    # `sigmoid(-k * slack)` where k > 0:\n    #   - slack small positive: exp(-large positive) -> score ~ 0\n    #   - slack is 0: exp(0) = 1 -> score = 0.5\n    #   - slack small negative: exp(-small negative) -> score ~ 1\n\n    # This is also inverted. The key is to find the correct input for the sigmoid.\n\n    # Let's use the transformed ratio `(bins_remain_cap - item) / item`.\n    # Or `bins_remain_cap / item - 1`.\n    # Let `x = bins_remain_cap / item - 1`.\n    # We want high scores when `x` is small and non-negative.\n    # `sigmoid(k * (-x))` = `sigmoid(k * (1 - bins_remain_cap / item))`\n    # With `k > 0`:\n    #   - `bins_remain_cap / item = 1` (perfect fit): `k * 0`, sigmoid is 0.5\n    #   - `bins_remain_cap / item > 1` (slack): `bins_remain_cap / item` is > 1, so `1 - ...` is negative. `k * negative`, exp is small, sigmoid ~ 1.\n    #   - `bins_remain_cap / item` large (very large bin): `1 - ...` is large negative, exp is very small, sigmoid ~ 1.\n    # This gives high scores for large bins, not ideal.\n\n    # The commonly cited \"Sigmoid Fit\" heuristic involves prioritizing bins\n    # where the ratio `remaining_capacity / item_size` is closest to 1.\n    # Let `score_candidate = bins_remain_cap / item`.\n    # We need a score that peaks at `score_candidate = 1`.\n    # Sigmoid functions are monotonic. To get a peak, we'd typically combine them or use another function.\n\n    # A common approach for \"Best Fit\" is to simply pick the bin with the minimum `bins_remain_cap - item`.\n    # If we *must* use a sigmoid, perhaps we can use it as a weighting factor.\n    # Let's consider bins that *can* fit the item.\n    # For these, the \"quality\" of the fit is inversely proportional to `bins_remain_cap - item`.\n\n    # Let's use a sigmoid to \"activate\" the priority for bins that fit,\n    # and then use the slack itself as the score, or transform the slack.\n\n    # A simpler interpretation of \"Sigmoid Fit Score\" might be:\n    # Prioritize bins that are \"almost full\" but can fit the item.\n    # A bin is \"almost full\" if its remaining capacity is small.\n    # So, we want to pick the bin with the minimum `bins_remain_cap` that is still `>= item`.\n\n    # If we want to use a sigmoid:\n    # `sigmoid(k * (item - bins_remain_cap))`\n    # With `k > 0`:\n    #   - `bins_remain_cap < item`: `item - bins_remain_cap` is positive. `k * positive`, sigmoid > 0.5.\n    #   - `bins_remain_cap = item`: `item - bins_remain_cap` is 0. sigmoid = 0.5.\n    #   - `bins_remain_cap > item`: `item - bins_remain_cap` is negative. `k * negative`, sigmoid < 0.5.\n\n    # This still doesn't give a clear peak at `bins_remain_cap == item`.\n\n    # Let's try a different perspective: focus on the item.\n    # The item needs `item` space.\n    # A bin with `bins_remain_cap` offers `bins_remain_cap` space.\n    # The \"waste\" is `bins_remain_cap - item` for valid bins.\n    # We want to minimize this waste.\n\n    # Let's define a penalty for wasted space.\n    # A sigmoid can be used to define a threshold.\n\n    # For a truly \"Sigmoid Fit Score\" that aims for a peak:\n    # Consider the function `f(x) = 1 / (1 + exp(-k * (x - threshold)))`.\n    # This function increases around `x = threshold`.\n    # We want a score that is high when `bins_remain_cap` is around `item`.\n\n    # Let's create a \"score for slack\":\n    # For bins that cannot fit (`bins_remain_cap < item`), set a very low score.\n    # For bins that can fit (`bins_remain_cap >= item`):\n    # We want to maximize priority when `bins_remain_cap - item` is small.\n\n    # Let's create a value that is inversely related to slack for fitting bins.\n    # `fit_quality_metric = - (bins_remain_cap - item)` for `bins_remain_cap >= item`.\n    # This metric is `item - bins_remain_cap`. This is maximized when `bins_remain_cap` is minimized.\n    # Applying sigmoid: `sigmoid(k * (item - bins_remain_cap))` where k > 0.\n    # This is what we tried above and it gives highest scores for negative slack.\n\n    # The \"Sigmoid Fit\" is often a misnomer or refers to a specific formula.\n    # A common heuristic associated with \"fitting\" is \"Best Fit\" - choosing the bin\n    # that leaves the minimum remaining capacity.\n\n    # If we interpret \"Sigmoid Fit Score\" as a general way to use a sigmoid\n    # to score bins based on their \"fit\" to the item:\n    # Let's create a score where bins that can fit are prioritized, and among them,\n    # those with less remaining capacity are preferred.\n\n    # Consider bins that *can* fit: `valid_bins_mask = bins_remain_cap >= item`.\n    # For these bins, we want to prioritize those with `bins_remain_cap` as small as possible.\n    # Let's create a metric: `(bins_remain_cap - item)`. This should be minimized.\n\n    # Let's use the sigmoid to \"shape\" the priority.\n    # Consider the value `x = bins_remain_cap - item`.\n    # We want high priority for `x` small and non-negative.\n\n    # Let's define a function `g(x)` such that `g(x)` is maximized for `x` near 0 (and non-negative).\n    # Then we can do `sigmoid(k * g(x))`. But g(x) would need to peak.\n\n    # A common strategy for \"Sigmoid Fit\" in other contexts involves scoring based on the ratio `item / capacity`.\n    # For BPP, let's adapt this. We have `item` and `bins_remain_cap`.\n    # A bin is a \"good fit\" if `bins_remain_cap` is close to `item`.\n\n    # Let's define a \"suitability\" score based on `bins_remain_cap`.\n    # A simple way to use sigmoid for prioritization that emphasizes *filling up*:\n    # Prioritize bins that become \"more full\" after item placement.\n    # The new fullness can be measured by `1 - (bins_remain_cap - item) / initial_bin_capacity`.\n    # But `initial_bin_capacity` is not directly available in the function signature.\n\n    # Let's focus on the remaining capacity.\n    # Consider a \"fitting score\" for each bin:\n    # If `bins_remain_cap < item`, score = 0.\n    # If `bins_remain_cap >= item`, score = `sigmoid_function(bins_remain_cap)`.\n    # We want `sigmoid_function(bins_remain_cap)` to be higher for smaller `bins_remain_cap`.\n    # This means the sigmoid function needs to be decreasing with `bins_remain_cap`.\n    # Let `sigmoid(k * (threshold - bins_remain_cap))` where `k > 0`.\n    # If `threshold` is chosen appropriately (e.g., average remaining capacity, or related to item size),\n    # this could work.\n\n    # Let's try to map `bins_remain_cap` directly.\n    # For a bin to be chosen, `bins_remain_cap >= item`.\n    # Among these, we want the smallest `bins_remain_cap`.\n    # This is \"Best Fit\".\n\n    # Let's try a direct application of sigmoid to the ratio of `item` to `bins_remain_cap`.\n    # `ratio = item / bins_remain_cap`.\n    # We want high scores when `ratio` is close to 1, but only if `bins_remain_cap >= item`.\n\n    # A commonly cited \"Sigmoid Fit\" heuristic for online BPP often tries to balance\n    # \"fitting into a small gap\" versus \"not being too tight\".\n    # It's about finding a bin that is \"just right\".\n\n    # Let's construct a metric `m` such that `sigmoid(m)` is maximized when `bins_remain_cap`\n    # is slightly larger than `item`.\n    # Let `slack = bins_remain_cap - item`. We want `m` such that `sigmoid(m)` peaks at `slack = 0`.\n    # `sigmoid(k * (-slack))` -> peaks at `slack = 0`.\n    # So, `m = k * (item - bins_remain_cap)`.\n    # The priority is `sigmoid(k * (item - bins_remain_cap))`.\n\n    # Let's test this:\n    # `k` controls the steepness. Let `k = 10`.\n    # Bin A: `bins_remain_cap = 5`, `item = 4`. Slack = 1. Metric = 10 * (4 - 5) = -10. Sigmoid(-10) ~ 0.\n    # Bin B: `bins_remain_cap = 4`, `item = 4`. Slack = 0. Metric = 10 * (4 - 4) = 0. Sigmoid(0) = 0.5.\n    # Bin C: `bins_remain_cap = 3.5`, `item = 4`. Slack = -0.5 (cannot fit). Metric = 10 * (4 - 3.5) = 5. Sigmoid(5) ~ 1.\n\n    # This prioritizes bins that are too small (which will be rejected anyway)\n    # and then bins that are just right, but the score for larger bins is too low.\n\n    # The standard way to apply a sigmoid to a \"best fit\" concept where the peak is at a certain value:\n    # Use the distance from the ideal fit.\n    # Ideal fit: `bins_remain_cap` is exactly `item`.\n    # Distance: `abs(bins_remain_cap - item)`. We want to minimize this.\n\n    # Let's use `sigmoid(k * -(bins_remain_cap - item))` = `sigmoid(k * (item - bins_remain_cap))`.\n    # This has a maximum at `bins_remain_cap = item`.\n    # We need to ensure that bins where `bins_remain_cap < item` get a very low priority.\n\n    # Let's incorporate the fitting constraint explicitly.\n    # For bins where `bins_remain_cap < item`, set priority to a very small number (e.g., -inf or a large negative).\n    # For bins where `bins_remain_cap >= item`, we want a score that is high for small `bins_remain_cap - item`.\n    # The sigmoid `1 / (1 + exp(-k * x))` is increasing.\n    # If we set `x = -(bins_remain_cap - item)`, then the sigmoid is increasing as `bins_remain_cap` decreases.\n    # So, high score for small `bins_remain_cap`. This matches \"Best Fit\".\n\n    # Let's refine the \"Sigmoid Fit Score\" to mean using the sigmoid function on\n    # a metric that favors bins that are 'close enough' but not excessively large.\n    # The standard Best Fit heuristic is to choose the bin that minimizes `bins_remain_cap - item`.\n    # This suggests we want a score that is higher for smaller non-negative values of `bins_remain_cap - item`.\n\n    # Let `slack = bins_remain_cap - item`.\n    # For bins where `slack < 0`, priority = 0.\n    # For bins where `slack >= 0`:\n    # We want a score that is high when `slack` is small.\n    # Let's use a decreasing sigmoid.\n    # `priority = sigmoid(k * (-slack))` where `k` is a positive parameter.\n    # This will map `slack = 0` to 0.5, `slack > 0` to values between 0 and 0.5,\n    # and `slack < 0` to values between 0.5 and 1. This is still not right.\n\n    # Consider this formulation:\n    # Prioritize bins that are \"almost full\", but can still accept the item.\n    # A bin is \"almost full\" if its remaining capacity is small.\n    # So, we want to prioritize bins with the smallest `bins_remain_cap` that is `>= item`.\n\n    # Let's construct a score for `bins_remain_cap >= item`.\n    # We want the score to be higher as `bins_remain_cap` gets closer to `item`.\n    # So, let `fit_score_input = -(bins_remain_cap - item) = item - bins_remain_cap`.\n    # Then, `priority = sigmoid(k * fit_score_input)` with `k > 0`.\n    # This ensures the priority increases as `bins_remain_cap` decreases (towards `item`).\n\n    # To handle the case where `bins_remain_cap < item`:\n    # We can set these priorities to a very low value, or 0, effectively disqualifying them.\n\n    # Let's define `k` as a sensitivity parameter. A larger `k` means the sigmoid is steeper,\n    # making the priority very sensitive to small changes in `bins_remain_cap`.\n    # A moderate `k` might be 5 or 10.\n\n    # We want `sigmoid(k * (item - bins_remain_cap))`.\n    # If `bins_remain_cap < item`, then `item - bins_remain_cap` is positive.\n    # `k * (positive)` is positive. `sigmoid` will be > 0.5.\n    # If `bins_remain_cap == item`, then `item - bins_remain_cap` is 0. `sigmoid` is 0.5.\n    # If `bins_remain_cap > item`, then `item - bins_remain_cap` is negative.\n    # `k * (negative)` is negative. `sigmoid` will be < 0.5.\n\n    # This function `sigmoid(k * (item - bins_remain_cap))` is maximized when `bins_remain_cap` is smallest.\n    # However, it gives high scores to bins that are too small.\n\n    # A robust approach:\n    # 1. Create a mask for bins that can fit the item.\n    # 2. For bins that can fit, calculate a \"fit quality\" score that is high for minimal slack.\n    # 3. For bins that cannot fit, assign a priority of 0.\n\n    # Let's use the sigmoid to create a \"penalty\" for slack.\n    # The \"perfect fit\" is when `bins_remain_cap == item`.\n    # `slack = bins_remain_cap - item`. We want to minimize `slack` when `slack >= 0`.\n\n    # Consider a metric that peaks at `slack = 0`.\n    # Let `x = bins_remain_cap - item`.\n    # We want `f(x)` to be high for `x` near 0, and 0 for `x < 0`.\n    # Sigmoid: `1 / (1 + exp(-k * x))` is increasing.\n\n    # Let's transform the slack:\n    # `transformed_slack = - (bins_remain_cap - item)` = `item - bins_remain_cap`.\n    # `priority = sigmoid(k * transformed_slack)`\n    # This priority is:\n    # - High for `item - bins_remain_cap` large positive (i.e., `bins_remain_cap` small, potentially < item)\n    # - Medium (0.5) for `item - bins_remain_cap` = 0 (i.e., `bins_remain_cap = item`)\n    # - Low for `item - bins_remain_cap` small positive (i.e., `bins_remain_cap` slightly < item)\n    # - Very low for `item - bins_remain_cap` large negative (i.e., `bins_remain_cap` large >> item)\n\n    # This gives highest scores to bins that are too small, and then smaller scores as bins become larger.\n    # We need to combine this with the condition `bins_remain_cap >= item`.\n\n    # Let's make the \"Sigmoid Fit Score\" represent the \"goodness\" of a fit,\n    # where \"goodness\" is high if `bins_remain_cap` is slightly larger than `item`.\n    # `k` determines how quickly the priority drops as `bins_remain_cap` increases beyond `item`.\n\n    # Define `k` (steepness parameter). A higher `k` means priority drops faster for larger bins.\n    k_steepness = 10.0\n\n    # Calculate the input for the sigmoid. We want highest score when `bins_remain_cap` is close to `item`.\n    # Let's use the deviation from the item size as the input.\n    # The metric `item - bins_remain_cap` is maximized when `bins_remain_cap` is minimized.\n    # We want to maximize priority when `bins_remain_cap` is slightly *larger* than `item`.\n    # This means `bins_remain_cap - item` should be small and positive.\n\n    # Consider the transformation: `item / bins_remain_cap`.\n    # We want this ratio to be close to 1, but `bins_remain_cap >= item` (so ratio <= 1).\n    # `sigmoid(k * (1 - ratio))` with `k > 0` will give high scores when `ratio` is < 1.\n    # As `ratio` decreases (i.e., `bins_remain_cap` increases), the score goes to 0.\n    # This means it prioritizes bins with minimal slack.\n\n    # Let's use `bins_remain_cap - item` as the base.\n    # We want to map this to a score that's high when `bins_remain_cap - item` is small and non-negative.\n    # Let `slack = bins_remain_cap - item`.\n    # We want to calculate `score = sigmoid_peak_func(slack)`.\n    # Sigmoid itself is monotonic. To get a peak, we need `sigmoid(k * (f(slack)))` where `f(slack)` peaks.\n    # Or, use a sigmoid to *clip* or *activate* a scoring based on slack.\n\n    # A pragmatic Sigmoid Fit approach for BPP:\n    # Prioritize bins that can fit the item (`bins_remain_cap >= item`).\n    # For these bins, give a score that is higher if `bins_remain_cap` is smaller.\n    # This is \"Best Fit\".\n\n    # Let's define the sigmoid input `x` such that `sigmoid(x)` is high for small `bins_remain_cap`.\n    # If `sigmoid(k * (target - value))`, this peaks at `value = target`.\n    # We want to peak at `bins_remain_cap = item`.\n    # So, `sigmoid(k * (item - bins_remain_cap))`.\n\n    # However, this gives high scores to bins that are too small.\n    # We must ensure bins that cannot fit (`bins_remain_cap < item`) get zero priority.\n\n    # So, `priorities = np.zeros_like(bins_remain_cap)`\n    # `can_fit_mask = bins_remain_cap >= item`\n    # `fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]`\n    # `score_input = item - fitting_bins_remain_cap`\n    # `sigmoid_scores = 1 / (1 + np.exp(-k_steepness * score_input))`\n    # `priorities[can_fit_mask] = sigmoid_scores`\n\n    # Let's test this:\n    # `bins_remain_cap = [5, 4, 6]`, `item = 4`\n    # `can_fit_mask = [True, True, True]`\n    # `fitting_bins_remain_cap = [5, 4, 6]`\n    # `score_input = [4-5, 4-4, 4-6] = [-1, 0, -2]`\n    # `sigmoid_scores = [sigmoid(-10), sigmoid(0), sigmoid(-20)]` (with k=10)\n    # `sigmoid_scores = [~0.0, 0.5, ~0.0]`\n    # This correctly prioritizes the bin that is exactly full (0.5), and other bins are very low.\n\n    # Example 2: `bins_remain_cap = [7, 5, 6]`, `item = 4`\n    # `can_fit_mask = [True, True, True]`\n    # `fitting_bins_remain_cap = [7, 5, 6]`\n    # `score_input = [4-7, 4-5, 4-6] = [-3, -1, -2]`\n    # `sigmoid_scores = [sigmoid(-30), sigmoid(-10), sigmoid(-20)]`\n    # `sigmoid_scores = [~0.0, ~0.0, ~0.0]`\n    # This is also not giving the best fit. It's giving low scores for bins that fit.\n\n    # The core problem is that `sigmoid(k * (target - value))` is *increasing* with `target`\n    # and *decreasing* with `value`. We want to maximize priority as `value` (`bins_remain_cap`)\n    # gets closer to `target` (`item`).\n\n    # Let's reconsider the slack: `slack = bins_remain_cap - item`.\n    # We want high priority for `slack` near 0 and non-negative.\n    # A metric that peaks at `slack = 0`: `exp(-k * slack^2)`. But that's not sigmoid.\n\n    # Let's use the sigmoid to represent the \"chance\" of being a good fit.\n    # The good fit is when `bins_remain_cap` is just enough.\n    # This is \"Best Fit\" behavior.\n    # Minimum `bins_remain_cap - item` for `bins_remain_cap >= item`.\n\n    # Let's use a sigmoid to transform the \"amount of slack\".\n    # `slack = bins_remain_cap - item`.\n    # For `slack < 0`: priority = 0.\n    # For `slack >= 0`: we want priority to decrease as `slack` increases.\n    # Consider `priority = sigmoid(k * (-slack))`.\n    # This means:\n    #   - `slack = 0`: `sigmoid(0)` = 0.5\n    #   - `slack > 0` small: `sigmoid(negative)` ~ < 0.5\n    #   - `slack > 0` large: `sigmoid(large negative)` ~ 0\n\n    # This is inverted for `slack >= 0`. We want high priority for small `slack`.\n    # So, we need the argument of sigmoid to be larger for smaller slack.\n    # This implies `k * (-slack)` should be larger for smaller slack. This is true if `k > 0`.\n    # Ah, the previous reasoning was correct. For `slack >= 0`:\n    # `sigmoid(-k * slack)` maps `0` to `0.5`, positive slack to values below `0.5`.\n    # This *is* decreasing, which is what we want (as slack increases, priority decreases).\n\n    # So, the formula for `bins_remain_cap >= item` is `sigmoid(k * (item - bins_remain_cap))`.\n    # Let's try this again carefully:\n    # `k_steepness = 10.0`\n    # `bins_remain_cap = [5, 4, 6]`, `item = 4`\n    # `can_fit_mask = [True, True, True]`\n    # `fitting_bins_remain_cap = [5, 4, 6]`\n    # `score_input = item - fitting_bins_remain_cap = [4-5, 4-4, 4-6] = [-1, 0, -2]`\n    # `sigmoid_scores = sigmoid(k_steepness * score_input)`\n    # `sigmoid_scores = [sigmoid(-10), sigmoid(0), sigmoid(-20)]`\n    # `sigmoid_scores = [very_small, 0.5, very_small]`\n\n    # This prioritizes the bin that is perfectly full (0.5).\n    # The bins with remaining capacity (5 and 6) are getting very low scores.\n    # This is the \"Best Fit\" behavior using a sigmoid.\n    # This seems like a reasonable interpretation of \"Sigmoid Fit Score\" in this context.\n\n    # Let's check for bins that cannot fit.\n    # `bins_remain_cap = [3, 4, 5]`, `item = 4`\n    # `can_fit_mask = [False, True, True]`\n    # `fitting_bins_remain_cap = [4, 5]`\n    # `score_input_for_fitting = [4-4, 4-5] = [0, -1]`\n    # `sigmoid_scores_for_fitting = [sigmoid(0), sigmoid(-10)] = [0.5, very_small]`\n    # `priorities = np.zeros_like(bins_remain_cap)`\n    # `priorities[can_fit_mask] = sigmoid_scores_for_fitting`\n    # `priorities` becomes `[0.0, 0.5, very_small]`\n\n    # This correctly gives 0 to the first bin, 0.5 to the bin that is exactly full,\n    # and a very low score to the bin that has extra space.\n    # The bin with 0.5 will be chosen.\n\n    # This heuristic prioritizes bins that have exactly enough capacity for the item.\n    # Among bins that can fit, it assigns higher priority to those with less remaining capacity.\n\n    k = 10.0  # Steepness parameter for the sigmoid\n\n    # Initialize priorities to zero (for bins that cannot fit or are not considered)\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Create a mask for bins that have enough capacity to fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Get the remaining capacities for bins that can fit\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # Calculate the input for the sigmoid function.\n    # We want to maximize priority when `bins_remain_cap` is as close to `item` as possible,\n    # specifically, when `bins_remain_cap - item` is minimal and non-negative.\n    # The expression `item - bins_remain_cap` will be maximized when `bins_remain_cap` is minimized.\n    # This expression will be 0 when `bins_remain_cap == item`.\n    # It will be negative if `bins_remain_cap > item`.\n    # We are effectively trying to make `sigmoid(k * (item - bins_remain_cap))` high.\n    # With k > 0, sigmoid is increasing.\n    # Thus, `item - bins_remain_cap` being larger gives a higher sigmoid value.\n    # This means `bins_remain_cap` being *smaller* gives a higher sigmoid value.\n    # This matches the \"Best Fit\" strategy.\n\n    score_input = item - fitting_bins_remain_cap\n\n    # Apply the sigmoid function: 1 / (1 + exp(-x))\n    # The sigmoid function maps the score_input to a range between 0 and 1.\n    # A higher input value yields a higher sigmoid output.\n    # With `score_input = item - fitting_bins_remain_cap`, larger values of this input\n    # correspond to smaller values of `fitting_bins_remain_cap`.\n    sigmoid_scores = 1 / (1 + np.exp(-k * score_input))\n\n    # Assign these scores to the priorities array for the bins that can fit\n    priorities[can_fit_mask] = sigmoid_scores\n\n    return priorities",
    "response_id": 26,
    "obj": 4.487435181491823,
    "cyclomatic_complexity": 1.0,
    "halstead": 66.41714012534482,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response27.txt_stdout.txt",
    "code_path": "problem_iter1_code27.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Random Fit.\n\n    In Random Fit, we consider all bins that can accommodate the item.\n    Then, we assign a random priority to these eligible bins.\n    The higher the priority, the more likely it is to be chosen.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Identify bins that can accommodate the item\n    eligible_bins_mask = bins_remain_cap >= item\n\n    # Initialize priorities to zero\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Assign random priorities to eligible bins\n    # We use a large number (e.g., 1000) to ensure that eligible bins have\n    # non-zero and distinct priorities in most practical scenarios.\n    num_eligible_bins = np.sum(eligible_bins_mask)\n    if num_eligible_bins > 0:\n        random_priorities = np.random.rand(num_eligible_bins) * 1000\n        priorities[eligible_bins_mask] = random_priorities\n\n    return priorities",
    "response_id": 27,
    "obj": 73.1850817710411,
    "cyclomatic_complexity": 2.0,
    "halstead": 28.529325012980813,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response28.txt_stdout.txt",
    "code_path": "problem_iter1_code28.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements the Epsilon-Greedy strategy for online Bin Packing.\n\n    The priority is determined by a balance between greedy exploration (picking\n    the bin with the most remaining capacity) and random exploration.\n\n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: A numpy array where each element is the remaining\n                         capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for packing the item into the corresponding bin.\n    \"\"\"\n    epsilon = 0.1  # Exploration rate\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify valid bins (those that can fit the item)\n    valid_bins_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return zero priorities\n    if not np.any(valid_bins_mask):\n        return priorities\n\n    # Calculate greedy priorities for valid bins\n    # Higher remaining capacity means higher priority\n    greedy_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    greedy_priorities[valid_bins_mask] = bins_remain_cap[valid_bins_mask]\n\n    # Introduce randomness for exploration\n    random_priorities = np.random.rand(num_bins) * 0.1 # Small random exploration bonus\n\n    # Combine greedy and random choices using epsilon-greedy logic\n    # With probability epsilon, choose a random valid bin\n    # With probability 1-epsilon, choose the best greedy valid bin\n    # For simplicity in priority assignment, we can think of it as:\n    # most bins get a base greedy score, some get an extra boost for randomness.\n\n    # Assign base priorities based on greedy choice (if valid)\n    priorities[valid_bins_mask] = greedy_priorities[valid_bins_mask]\n\n    # Add a small random component to all bins for exploration\n    priorities += random_priorities\n\n    # Normalize priorities to prevent excessively large values and ensure\n    # a more stable distribution, especially if capacities are very large.\n    # We'll normalize based on the maximum possible priority to keep relative\n    # differences meaningful.\n    max_priority = np.max(priorities[valid_bins_mask]) if np.any(valid_bins_mask) else 1.0\n    if max_priority > 0:\n        priorities /= max_priority\n\n    # Ensure that bins that cannot fit the item have zero priority\n    priorities[~valid_bins_mask] = 0.0\n\n    return priorities",
    "response_id": 28,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 4.0,
    "halstead": 77.66179398375645,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response29.txt_stdout.txt",
    "code_path": "problem_iter1_code29.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.\n\n    This strategy prioritizes bins that have a remaining capacity closely matching the item's size,\n    aiming to minimize wasted space. The Softmax function is used to convert these \"fits\" into\n    probabilities (priorities), where a better fit results in a higher priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the \"fitness\" for each bin: how well the item fits.\n    # A good fit means the remaining capacity is just enough for the item.\n    # We want to minimize (remaining_capacity - item_size).\n    # However, we only consider bins where the item can actually fit (remaining_capacity >= item_size).\n    # For bins where the item doesn't fit, we assign a very low fitness (or effectively zero priority).\n\n    # Calculate potential remaining capacity after placing the item\n    potential_remaining_caps = bins_remain_cap - item\n\n    # For bins where the item fits (potential_remaining_caps >= 0)\n    # we want to minimize the potential_remaining_caps (i.e., get closer to 0).\n    # The \"closeness\" can be represented by the absolute value of potential_remaining_caps.\n    # For bins where the item does not fit, we assign a large penalty or ensure they have zero priority.\n\n    # Option 1: Simple \"goodness of fit\" where smaller leftover is better\n    # For bins that can fit the item\n    fits = bins_remain_cap - item\n    # Only consider bins where the item fits\n    valid_fits = fits[fits >= 0]\n    invalid_indices = np.where(fits < 0)[0]\n\n    # Transform fits into priorities using softmax.\n    # We want higher priority for smaller 'fits' (less leftover space).\n    # So, we can use negative of the fit, or simply the fit and let softmax handle it.\n    # Using a \"temperature\" parameter can control the sharpness of the distribution.\n    # A smaller temperature makes the distribution sharper (more emphasis on the best fit).\n    # A larger temperature makes it flatter (more exploration).\n    temperature = 1.0 # Can be tuned\n\n    # For valid bins, the \"value\" for softmax can be the negative of the remaining capacity after fit.\n    # This way, a smaller remaining capacity (closer fit) gets a larger positive value.\n    # Alternatively, we can use the probability of fit directly, but that's less about priority for *that specific* bin.\n    # Let's consider the remaining capacity itself as a factor, and penalize large remaining capacities.\n    # A good fit is when bins_remain_cap is close to item.\n    # So, we can use something like exp(-(bins_remain_cap - item) / temperature) for valid bins.\n    # This gives higher values when bins_remain_cap - item is small.\n\n    # Let's focus on the negative difference: if bins_remain_cap[i] - item is small and positive, that's good.\n    # If it's large and positive, that's bad. If it's negative, it's impossible.\n    # We can define a score where a smaller remaining capacity is better.\n    # Score = - (bins_remain_cap - item) if item fits, else a very small number (or NaN for softmax handling).\n\n    scores = np.full_like(bins_remain_cap, -np.inf) # Initialize with a very low score\n\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            # The \"closeness\" metric: how much space is left. We want to minimize this.\n            # So, a smaller remaining capacity is better.\n            # To use softmax, we want higher values for better \"fits\".\n            # Let's use the negative of the remaining capacity *after* packing.\n            # Smaller remaining capacity (good fit) leads to a less negative value, thus higher priority.\n            scores[i] = -(bins_remain_cap[i] - item)\n\n    # Apply Softmax to get probabilities (priorities)\n    # Handle cases where all scores might be -inf (if item doesn't fit anywhere)\n    if np.all(np.isinf(scores)):\n        return np.zeros_like(bins_remain_cap) # Or handle as error/no placement\n\n    # Avoid numerical instability with very large negative numbers\n    # We can shift the scores by subtracting the maximum score before exp\n    max_score = np.max(scores)\n    shifted_scores = scores - max_score\n\n    # Calculate the exponential of the shifted scores\n    exp_scores = np.exp(shifted_scores / temperature)\n\n    # Calculate the sum of exponentials for normalization\n    sum_exp_scores = np.sum(exp_scores)\n\n    # Calculate the final priorities (probabilities)\n    if sum_exp_scores > 0:\n        priorities = exp_scores / sum_exp_scores\n    else:\n        # This case should ideally not happen if scores are handled correctly\n        # but as a fallback, assign equal probability if sum is zero.\n        priorities = np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n\n    return priorities",
    "response_id": 29,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 5.0,
    "halstead": 164.99896988958,
    "exec_success": true
  }
]