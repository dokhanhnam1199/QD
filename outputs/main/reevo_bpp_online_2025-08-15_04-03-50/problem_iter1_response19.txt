```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Softmax-Based Fit.

    This heuristic prioritizes bins that are a "good fit" for the item,
    meaning they have just enough remaining capacity. It uses a Softmax
    function to convert these fit scores into probabilities (priorities).
    A higher priority is given to bins where the remaining capacity is
    closer to the item size.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Calculate the difference between remaining capacity and item size.
    # We are looking for bins where this difference is small and non-negative.
    # Add a small epsilon to avoid division by zero if item size is exactly 0
    # and also to push values away from zero if they are exactly zero.
    diffs = bins_remain_cap - item + 1e-9

    # Only consider bins that have enough capacity for the item
    # For bins with insufficient capacity, assign a very low priority score
    # (effectively 0 after Softmax, as negative values in exp are small)
    # A very large negative number will result in a very small exponent.
    valid_fit_scores = np.where(diffs >= 0, diffs, -np.inf)

    # Apply the Softmax function.
    # exp(x) can become very large or very small.
    # We want to maximize the probability for bins that are a good fit (small positive diffs).
    # To achieve this with Softmax, we can either use the differences directly or transform them.
    # A common approach for "good fit" is to favor smaller differences.
    # Let's consider the inverse of the difference as a measure of "goodness of fit",
    # but only for bins that can fit the item.
    # To avoid division by zero and make smaller differences have higher values,
    # we can use 1 / (diff + 1) or similar.

    # Let's redefine the "fit score" to be higher for better fits.
    # A good fit is when `bins_remain_cap` is close to `item`.
    # So, we want to penalize bins with large remaining capacity.
    # We can use `1.0 / (bins_remain_cap - item + epsilon)` for bins that fit.
    # However, Softmax works better with values that can be large and positive.
    # Let's consider `item / bins_remain_cap` for bins where `bins_remain_cap >= item`.
    # This would prioritize bins that are more full relative to their remaining capacity
    # after placing the item. This interpretation is more like "First Fit Decreasing" style.

    # For Softmax-Based Fit, a common approach is to convert the "badness of fit"
    # (e.g., remaining capacity - item_size) into a probability.
    # A smaller positive difference is better.
    # We can use `exp(- (bins_remain_cap - item))` for bins that fit.
    # However, this will favor bins with slightly *more* capacity left.
    # To favor bins that are *closer*, we want to maximize `exp(-abs(bins_remain_cap - item))`.
    # But we only care about `bins_remain_cap >= item`.

    # Let's try to prioritize bins where the remaining capacity is JUST enough.
    # This means `bins_remain_cap - item` should be small and non-negative.
    # A simple way to convert this to a score that Softmax can work with
    # (i.e., values that are generally higher for better fits) is to use:
    # score = 1 / (bins_remain_cap - item + epsilon) for valid bins.
    # This gives higher scores to bins with less remaining space after placing the item.

    # Alternative interpretation: Prioritize bins that are "almost full" with the item.
    # This means `bins_remain_cap - item` should be small and non-negative.
    # Let's use `exp(- (bins_remain_cap - item))` and then normalize.
    # This would mean smaller positive differences get higher scores.

    # Let's try a different approach that focuses on maximizing the utilization
    # of the bin *after* placing the item, while still ensuring the item fits.
    # The remaining capacity after placing the item is `bins_remain_cap - item`.
    # We want to prioritize bins where `bins_remain_cap - item` is small and non-negative.
    # A good "fit score" could be `1.0 / (bins_remain_cap - item + epsilon)`.
    # However, Softmax is `exp(score)`. If score is small and positive, exp(score) is still small.
    # If score is large and positive, exp(score) is large.
    # So, we want scores to be *large* for good fits.
    # This implies that `-(bins_remain_cap - item)` should be large, i.e., `item - bins_remain_cap` should be large.
    # This is the opposite of what we want (small non-negative difference).

    # Let's go back to the idea that a good fit means `bins_remain_cap - item` is minimal and non-negative.
    # Let `fit_score = item / bins_remain_cap` for bins that can fit the item.
    # This prioritizes bins that will be more filled *after* the item is placed relative to their original capacity.
    # This is similar to Best Fit.
    # However, Softmax expects scores, not probabilities directly.
    # Let's map "closeness" to a positive score.
    # The "closeness" is `bins_remain_cap - item`. We want this to be small and >= 0.
    # We can transform this to `1.0 / (bins_remain_cap - item + epsilon)` as a "goodness score".
    # Then use `exp(goodness_score)`.

    # Let's try this:
    # For each bin, calculate the "remaining space ratio" if the item is placed:
    # `ratio = (bins_remain_cap - item) / item` if item > 0.
    # We want to minimize this ratio (or have it close to zero).
    # This is tricky for Softmax where we want to maximize values.
    # Let's use `exp(-(bins_remain_cap - item))` for bins where `bins_remain_cap >= item`.
    # This penalizes larger remaining spaces.

    # Initialize priorities to a very small negative number (effectively zero after softmax)
    priorities = np.full_like(bins_remain_cap, -np.inf)

    # Consider only bins with enough capacity for the item
    sufficient_capacity_indices = np.where(bins_remain_cap >= item)[0]

    if len(sufficient_capacity_indices) > 0:
        # Calculate the "fitness" as the negative difference between
        # remaining capacity and item size. This makes smaller positive
        # differences have higher (less negative) fitness values.
        # Adding a small epsilon to `bins_remain_cap` before calculating the difference
        # can help avoid issues if `bins_remain_cap` is exactly `item`, and also
        # helps differentiate bins that are very close.
        # `item - (bins_remain_cap - item)` or `2*item - bins_remain_cap` if we want to maximize.
        # Let's use `bins_remain_cap - item` as the measure of excess space.
        # We want this excess space to be small and non-negative.
        # For Softmax, we want higher scores for better fits.
        # A good fit has small `bins_remain_cap - item`.
        # So, `-(bins_remain_cap - item)` would give higher scores for better fits.
        # We want to maximize `item / bins_remain_cap` is like best fit if item is large relative to capacity.
        # Let's use the inverse of the remaining capacity after placing the item,
        # capped for positive fits.
        # score = 1.0 / (bins_remain_cap[sufficient_capacity_indices] - item + 1e-9)
        # This would give higher scores to bins that are nearly full.
        # Then apply exp.

        # A common interpretation for "fit" strategies like Best Fit is to
        # minimize `bins_remain_cap - item`. For Softmax, we want to map
        # this to a score where lower `bins_remain_cap - item` yields a higher score.
        # Let `score_component = item / bins_remain_cap[sufficient_capacity_indices]`
        # This prioritizes bins that are more full relative to their original capacity,
        # after placing the item.

        # Let's define the "fit value" as how much capacity remains after placing the item.
        # We want this to be small and non-negative.
        # So, `remaining_after_fit = bins_remain_cap[sufficient_capacity_indices] - item`.
        # We want to give high scores when `remaining_after_fit` is small.
        # Let's use `exp(-(remaining_after_fit))` to make smaller `remaining_after_fit` have higher scores.
        # We can also add the item size as a factor to potentially favor bins that
        # are more utilized overall. `exp(-(bins_remain_cap[sufficient_capacity_indices] - item) / item)` if item > 0.

        # Let's consider the simple "gap" strategy.
        # The gap is `bins_remain_cap - item`. We want this to be minimized.
        # For Softmax, we want higher scores for smaller gaps.
        # `score = - (bins_remain_cap[sufficient_capacity_indices] - item)` will do this.
        # But this can result in very large negative scores if capacities are very different.

        # A more robust approach is to use a transformation that keeps values within a reasonable range.
        # Consider `1 / (bins_remain_cap[sufficient_capacity_indices] - item + epsilon)` which gives higher scores for bins closer to fitting.
        # Or `item / bins_remain_cap[sufficient_capacity_indices]` which is like Best Fit.

        # Let's try a combination: Prioritize bins that have *some* space left but not too much.
        # `bins_remain_cap - item` is the remaining capacity after placing the item.
        # We want this to be close to 0.
        # Let's define a "fitness score" such that smaller non-negative differences get higher scores.
        # `fit_score = 1.0 / (bins_remain_cap[sufficient_capacity_indices] - item + 1e-3)`
        # Using `1e-3` as a small constant to avoid division by zero and create distinct scores.
        # Higher fit_score means the bin is a better fit.

        # For Softmax: exp(score)
        # To maximize the probability for good fits, scores for good fits should be high.
        # Let's use `bins_remain_cap[sufficient_capacity_indices] - item`.
        # We want to minimize this. So, `-(bins_remain_cap[sufficient_capacity_indices] - item)`
        # becomes our score candidate.

        # Let's normalize the item size and bin capacities by some factor to keep numbers reasonable.
        # Or simply use a scaled difference.
        # Example: `score = - (bins_remain_cap[sufficient_capacity_indices] - item) / item` (if item > 0)
        # This makes the penalty proportional to the item size.

        # Let's refine the "good fit" idea: we want `bins_remain_cap` to be slightly larger than `item`.
        # So, `bins_remain_cap - item` should be small and positive.
        # Let's map `bins_remain_cap - item` to a value that is higher for smaller positive values.
        # `score = 1.0 / (bins_remain_cap[sufficient_capacity_indices] - item + epsilon)` works.
        # Let's use `exp(1.0 / (bins_remain_cap[sufficient_capacity_indices] - item + 1e-9))`

        # Applying Softmax: exp(scores) / sum(exp(scores))
        # If we use `exp(1.0 / (bins_remain_cap[sufficient_capacity_indices] - item + 1e-9))`,
        # then bins with a small positive difference will have a large `1.0 / (...)`,
        # and thus a large exponential, leading to a high priority.

        # Alternative: Consider the ratio of remaining capacity to item size.
        # `ratio = (bins_remain_cap[sufficient_capacity_indices] - item) / item`
        # We want this ratio to be small. So `-(ratio)` would work.

        # Let's go with a simple "gap" approach for fit: prioritize bins where `bins_remain_cap` is close to `item`.
        # We can use `1.0 / (bins_remain_cap - item + epsilon)` as a measure of "how good the fit is".
        # A smaller difference `bins_remain_cap - item` leads to a larger `1.0 / (...)`.
        # Then, we can exponentiate this for Softmax.

        # The actual remaining capacity after placing the item.
        remaining_after_fit = bins_remain_cap[sufficient_capacity_indices] - item

        # Create a "fitness score" where smaller `remaining_after_fit` values yield higher scores.
        # Use `1.0 / (remaining_after_fit + epsilon)` to map small positive values to large values.
        # Add `1e-6` to `remaining_after_fit` to avoid division by zero if `remaining_after_fit` is exactly 0.
        # A value of 0 remaining capacity is a perfect fit.
        # A very small positive remaining capacity is also a good fit.
        # So, `1 / (small_positive_number)` is large.

        fitness_scores = 1.0 / (remaining_after_fit + 1e-6)

        # Apply the exponential function to convert fitness scores into values suitable for Softmax.
        # Larger fitness scores will result in larger exponential values.
        exp_scores = np.exp(fitness_scores)

        # Assign these calculated priorities to the corresponding bins
        priorities[sufficient_capacity_indices] = exp_scores

    # Normalize the priorities using Softmax.
    # If all bins were insufficient, priorities will be all -inf, resulting in division by zero.
    # Handle this case by returning zeros or a uniform distribution.
    # If only one bin is sufficient, its priority will be 1.0.
    sum_exp_scores = np.sum(priorities)

    if sum_exp_scores == 0 or np.isinf(sum_exp_scores):
        # If no bins fit or all scores are infinite (e.g., item size 0 and capacity large)
        # return a uniform distribution or zeros if no bins can fit.
        # If all scores are -inf, sum is 0. We need to handle this to avoid division by zero.
        # If all were -inf, it means no bin had sufficient capacity.
        return np.zeros_like(bins_remain_cap)
    else:
        # Softmax normalization: exp(score) / sum(exp(scores))
        # However, our `priorities` array already contains `exp(score)`
        # (or -inf for non-fitting bins).
        # So, we just need to divide by the sum of these exponential values.
        normalized_priorities = priorities / sum_exp_scores
        return normalized_priorities

```
