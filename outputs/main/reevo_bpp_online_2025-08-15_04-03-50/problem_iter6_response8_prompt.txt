{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a refined strategy.\n\n    This strategy prioritizes bins that offer a \"tight fit\" for the item,\n    meaning the remaining capacity after insertion is minimized. It achieves this\n    by assigning a priority score based on the inverse of (1 + slack), where slack\n    is the difference between the bin's remaining capacity and the item's size.\n\n    To further balance this with the \"fullness\" of bins (prioritizing bins that\n    are already closer to full, i.e., have less remaining capacity overall), a small\n    secondary score is added. This secondary score is proportional to the negative\n    of the bin's remaining capacity, effectively giving a slight preference to\n    smaller bins when tight fits are comparable.\n\n    A small amount of noise is added to the priority scores to introduce a stochastic\n    element, promoting exploration and helping to escape local optima, as suggested\n    by the \"balance with random exploration for robustness\" aspect of the reflection.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities\n\n    # Calculate slack for bins that can fit the item\n    # Slack = remaining_capacity - item_size\n    slack = bins_remain_cap[fit_mask] - item\n\n    # Primary scoring: Favor tight fits by using 1 / (1 + slack).\n    # This assigns higher scores to bins with smaller positive slack.\n    tight_fit_scores = 1.0 / (1.0 + slack)\n\n    # Secondary scoring: Favor \"fuller\" bins (those with less initial remaining capacity).\n    # We use a small negative term proportional to the bin's remaining capacity.\n    # Maximizing this term means minimizing bins_remain_cap for fitting bins.\n    # The small multiplier ensures tight fit is primary.\n    fullness_scores = -0.1 * bins_remain_cap[fit_mask]\n\n    # Combine scores\n    combined_scores = tight_fit_scores + fullness_scores\n\n    # Add a small amount of random noise to encourage exploration.\n    # This helps to break ties and explore different packing configurations.\n    noise = np.random.normal(0, 0.01, size=combined_scores.shape)\n    priorities[fit_mask] = combined_scores + noise\n\n    # Normalize priorities so they sum to 1 over the fittable bins.\n    # This converts scores into probabilities if using a probabilistic selection method.\n    # If deterministic selection (e.g., argmax) is used, normalization isn't strictly necessary\n    # but can help in interpreting relative preferences.\n    current_priorities = priorities[fit_mask]\n    if np.any(current_priorities):\n        # Ensure no division by zero if all priorities are zero (though unlikely with noise)\n        sum_priorities = np.sum(current_priorities)\n        if sum_priorities > 0:\n            priorities[fit_mask] = current_priorities / sum_priorities\n        else:\n            # If for some reason all priorities are non-positive, assign equal probability\n            # to all fittable bins.\n            priorities[fit_mask] = 1.0 / len(current_priorities) if len(current_priorities) > 0 else 0\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a hybrid strategy.\n\n    This strategy aims to balance the preference for tight fits (minimizing wasted space\n    in the immediate bin) with a slight preference for bins that offer a bit more\n    room, promoting better global packing. It uses a combination of a \"Best Fit\"\n    (minimal slack) component and a \"First Fit\"-like component (favoring bins\n    that are not too empty).\n\n    The priority is calculated based on the negative slack (prioritizing bins\n    where `bins_remain_cap - item` is small and positive). To encourage exploration\n    and prevent premature filling of slightly-too-small gaps that might be better\n    suited for smaller items, a small penalty is applied to bins with extremely\n    tight fits, and a bonus is given to bins with a moderate amount of slack.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    fittable_bins_mask = bins_remain_cap >= item\n\n    if np.any(fittable_bins_mask):\n        fittable_bins_capacities = bins_remain_cap[fittable_bins_mask]\n\n        # Calculate slack: remaining capacity after fitting the item.\n        # We want slack to be as close to 0 as possible.\n        slack = fittable_bins_capacities - item\n\n        # Heuristic:\n        # 1. Prioritize bins with small positive slack (tight fits).\n        # 2. Avoid extremely tight fits (slack very close to 0) by slightly penalizing them.\n        # 3. Give a small bonus to bins with moderate slack to encourage using \"less full\" bins sometimes.\n        # This can be achieved by mapping slack to a score where a small positive value is optimal.\n\n        # Using a quadratic function: -(slack - ideal_slack)^2\n        # Let's aim for an \"ideal\" slack of, say, 10% of the item size,\n        # but with a floor to avoid negative ideal slack for very small items.\n        ideal_slack = max(0.1 * item, 0.1) # Aim for ~10% of item size, minimum 0.1\n        \n        # Penalize deviations from the ideal slack.\n        # A higher value means closer to ideal_slack is better.\n        # We want to maximize this value.\n        # The function `-(slack - ideal_slack)**2` will be maximized when slack is close to ideal_slack.\n        # However, we also want to prioritize *tight* fits, meaning slack near 0 should be good.\n        \n        # Let's try a different approach: Reward tightness, but with diminishing returns.\n        # Prioritize bins where (bin_capacity - item) is small.\n        # A simple reward for tightness: 1 / (1 + slack)\n        # To balance with exploration: maybe add a small bonus if slack is small but not zero.\n        \n        # Hybrid approach:\n        # Prioritize bins with minimal slack (close to 0). This is the \"Best Fit\" aspect.\n        # But also give a slight preference to bins with a moderate amount of slack\n        # to avoid leaving many bins in a state that's hard to fill.\n\n        # A scoring function that peaks at slack = 0 and then decreases,\n        # but perhaps less sharply for very small slacks.\n        \n        # Let's use a scaled inverse relationship with slack, and add a small bonus for moderate slack.\n        # For very small slack (tight fit): prioritize.\n        # For moderate slack: also consider it favorable.\n        # For large slack: disfavor.\n        \n        # Strategy: Base priority on inverse slack (favoring small slack),\n        # but add a small bonus proportional to slack up to a certain point,\n        # and then penalize larger slacks.\n\n        # Let's try a function that is high for slack near 0, then decreases.\n        # Consider a penalty for slack that is too small (e.g., < 0.1) and too large.\n        \n        # A common heuristic for balancing is to use a function that is high for \"just right\".\n        # For bin packing, \"just right\" is usually a tight fit.\n        # To encourage exploration, we can slightly boost bins that aren't *exactly* the tightest.\n        \n        # Let's use a function that rewards closeness to 0 slack, but less aggressively for very small slacks.\n        # And perhaps a small bonus for slack that is not excessively large.\n\n        # Option 1: Gamma distribution-like scoring based on slack\n        # Prioritize small slack, with a peak somewhere slightly above zero.\n        # Example: (slack^a) * exp(-b*slack)\n        # This can be tricky to tune.\n\n        # Option 2: Combining a \"Best Fit\" component with a \"Least Full\" component.\n        # Best Fit: Maximize (1 / (1 + slack))\n        # Least Full: Maximize (bins_remain_cap) -- but we want to avoid *large* bins, so this isn't direct.\n        \n        # Let's try to directly score bins based on how \"good\" their remaining space is.\n        # A good remaining space is small, but not so small that it becomes unusable.\n        \n        # Score: Maximize (1 / (1 + slack)) - prioritize tight fits.\n        # Bonus for moderate slack: add a small value if slack is between X and Y.\n        \n        # Let's consider slack values.\n        # slack = 0: Best.\n        # slack = 0.1: Good.\n        # slack = 1.0: Okay.\n        # slack = 5.0: Not great.\n        \n        # A function like: `exp(-slack / constant)` will heavily favor small slacks.\n        # To add exploration: `exp(-slack / constant) + bonus_if_slack_is_moderate`\n        \n        # Let's try a concave function that peaks at slack=0, but with a flattened initial slope.\n        # This can be achieved by `1 - exp(-k*slack)`. This maps slack=0 to 0, slack=inf to 1.\n        # We want the opposite: slack=0 should be high.\n        \n        # Revisit the \"Worse code\" logic: `1 / (1 + slack)` is good for favoring tight fits.\n        # The reflection asks to favor tight fits but explore slightly larger gaps.\n        # This means we want the function to be high for small slack, but not drop off *too* quickly.\n        \n        # Let's use a sigmoidal function that starts high for small slacks and gradually decreases.\n        # A sigmoid shifted and scaled.\n        \n        # Consider slack values:\n        # If slack is very small (e.g., 0.01), we want a high score.\n        # If slack is moderate (e.g., 0.5), we want a reasonably high score, maybe slightly lower than 0.01.\n        # If slack is large (e.g., 2.0), we want a low score.\n        \n        # A function like `1 / (1 + slack**p)` where p is between 0 and 1 might work.\n        # If p=1, it's `1/(1+slack)` (original logic).\n        # If p=0.5, it's `1/(1 + sqrt(slack))`. This penalizes large slacks less.\n        \n        # Let's use a scaling factor for slack to control the steepness of the decay.\n        # And perhaps add a small constant to slack to avoid division by zero and\n        # give a base priority to bins with zero slack.\n        \n        # Let's try a heuristic that penalizes large remaining capacities more heavily.\n        # We want to prefer bins that, after packing, have *some* remaining capacity, but not too much.\n        \n        # Consider the remaining capacity *after* packing: `bins_remain_cap - item`.\n        # We want this to be small and positive.\n        \n        # Score = f(bins_remain_cap - item)\n        # f(x) should be high for small positive x.\n        \n        # Let's use `1 / (1 + slack)` as a base, but then add a small bonus for slacks within a certain range.\n        # Or, modify the `slack` value itself before applying `1 / (1 + slack)`.\n        \n        # Idea: Apply a transformation to slack that emphasizes values close to 0.\n        # Let `transformed_slack = slack / (slack + constant)`\n        # As slack -> 0, transformed_slack -> 0.\n        # As slack -> inf, transformed_slack -> 1.\n        # So, priority could be `1 - transformed_slack = constant / (slack + constant)`\n        # This is similar to `1 / (1 + slack/constant)`.\n        \n        # Let's try to incorporate the \"just right\" idea.\n        # If slack is too small, it's bad (maybe the item almost doesn't fit).\n        # If slack is too large, it's bad (wasted space).\n        # If slack is moderate, it's good.\n        \n        # This suggests a function with a peak.\n        # However, for Bin Packing, the primary goal is still to fit items efficiently.\n        # The \"exploration\" part should probably not override the fundamental \"fit tightly\" heuristic too much.\n        \n        # Let's use a softened Best Fit approach.\n        # Instead of `1 / (1 + slack)`, use a function that is less steep for small slacks.\n        # Consider `log(1 + 1/slack)` for slack > 0. This also favors small slack.\n        \n        # Let's go back to the idea of preferring bins with small positive remaining capacity.\n        # Priority is inversely related to `bins_remain_cap - item`.\n        \n        # Consider a function `g(remaining_capacity)` where `remaining_capacity = bins_remain_cap - item`.\n        # We want `g(x)` to be high for small positive `x`.\n        \n        # `g(x) = 1 / (1 + x)`: Peaks at 1 for x=0, decreases.\n        # `g(x) = exp(-k*x)`: Peaks at 1 for x=0, decreases.\n        \n        # To encourage exploration of slightly larger gaps:\n        # We want the function to decrease slower for small `x`.\n        \n        # Let's try a hyperbolic tangent (tanh) based approach.\n        # `tanh(a - b*slack)`:\n        # If `a` is large and `b` is positive, it will be close to 1 for small slack, and decrease.\n        # Example: `tanh(5 - 2*slack)`\n        # slack=0 -> tanh(5) ~ 1\n        # slack=1 -> tanh(3) ~ 1\n        # slack=2 -> tanh(1) ~ 0.76\n        # slack=3 -> tanh(-1) ~ -0.76\n        \n        # This favors small slack, but also gives reasonable scores for moderate slack.\n        # It penalizes large slack significantly.\n        \n        # Let's refine this: `tanh(a * (1 - slack/ideal_slack))`\n        # This will peak when slack = ideal_slack. We want peak at slack = 0.\n        \n        # Let's try a function that combines \"tightness\" and \"not-too-empty\".\n        # Priority = (1 / (1 + slack)) * (slack / (slack + C))  -- this will go to 0.\n        \n        # How about favoring bins that are not too close to full, nor too empty?\n        # This is more like \"Second Fit\".\n        \n        # The reflection: \"Favor tight fits, but explore slightly larger gaps for flexibility.\"\n        # This implies a function that is high for small slack, but the decay is gentle initially.\n        \n        # Let's use a modified inverse relationship.\n        # Instead of `1 / (1 + slack)`, consider `1 / (1 + slack^p)` with `p < 1`.\n        # Or `1 / (1 + sqrt(slack))`. This gives a higher score for small slacks compared to `1/slack`.\n        # Let's try `1 / (1 + slack**0.5)`\n        \n        # slack = 0.01: 1 / (1 + 0.1) = 0.909\n        # slack = 0.1:  1 / (1 + 0.316) = 0.76\n        # slack = 0.5:  1 / (1 + 0.707) = 0.58\n        # slack = 1.0:  1 / (1 + 1.0)   = 0.5\n        \n        # Compared to `1 / (1 + slack)`:\n        # slack = 0.01: 1 / (1 + 0.01) = 0.99\n        # slack = 0.1:  1 / (1 + 0.1)  = 0.909\n        # slack = 0.5:  1 / (1 + 0.5)  = 0.667\n        # slack = 1.0:  1 / (1 + 1.0)  = 0.5\n        \n        # The `slack**0.5` version penalizes larger slacks less harshly. This seems to align with \"explore slightly larger gaps\".\n        \n        # Let's add a small constant to the denominator to ensure non-zero priority even for zero slack.\n        # And maybe scale the slack to control the sensitivity.\n        \n        # Let `scaled_slack = slack / item` (fractional slack)\n        # Priority ~ `1 / (1 + scaled_slack**p)`\n        \n        # Alternative: Focus on the remaining capacity `rc = bins_remain_cap - item`.\n        # We want `rc` to be small and positive.\n        # Let's define a penalty function for `rc`.\n        # Penalty is 0 if `rc` is ideal (e.g., 0), increases as `rc` deviates.\n        # But we want to favor small `rc`.\n        \n        # How about: `exp(-k * rc)`? This is similar to sigmoid.\n        \n        # Let's try a combination: a base score for tightness, plus a bonus for moderate non-zero slack.\n        \n        # Base score: `1 / (1 + slack)`\n        # Bonus: Apply a Gaussian-like function centered at a small positive slack, e.g., 0.5.\n        # `bonus = exp(-(slack - 0.5)**2 / (2 * sigma**2))`\n        \n        # This could get complex. Let's simplify.\n        \n        # The reflection is key: \"Favor tight fits, but explore slightly larger gaps for flexibility.\"\n        # This means the priority function should decrease as slack increases, but not too rapidly.\n        \n        # A function like `1 / (1 + slack^p)` with `0 < p < 1` is a good candidate.\n        # Let's choose `p = 0.5` (square root) for a start.\n        \n        # `priorities[fittable_bins_mask] = 1.0 / (1.0 + np.sqrt(slack))`\n        \n        # To prevent division by zero if slack could be negative (which it can't here since we filter `bins_remain_cap >= item`),\n        # and to avoid extremely high priorities for zero slack, let's add a small epsilon.\n        \n        # Let's also consider scaling slack by item size to make it more relative.\n        # `relative_slack = slack / item` (handle item=0 case)\n        # `priority = 1.0 / (1.0 + np.sqrt(relative_slack))`\n        \n        # Let's use the absolute slack but scale its influence.\n        # `score = 1.0 / (1.0 + (slack / scale_factor)**p)`\n        \n        # A simple approach that balances tight fits with some room:\n        # Favor bins where `bins_remain_cap - item` is small.\n        # Also, consider the absolute `bins_remain_cap`. Larger bins might be useful for larger items later.\n        \n        # Let's consider the \"quality\" of the remaining space.\n        # A very small remaining space (tight fit) is good for immediate packing.\n        # A slightly larger remaining space is also good because it leaves room.\n        # A very large remaining space is less good, as it might indicate a poor fit for the current item.\n        \n        # Let's try a function that peaks at a small positive slack value.\n        # A log-normal or gamma-like shape can achieve this.\n        # Example: `slack**alpha * exp(-beta * slack)`\n        # To maximize this, take derivative and set to zero.\n        # `alpha*slack**(alpha-1)*exp(-beta*slack) - beta*slack**alpha*exp(-beta*slack) = 0`\n        # `alpha*slack**(-1) - beta = 0`\n        # `alpha/slack = beta` => `slack = alpha / beta`\n        # So, we can tune `alpha` and `beta` to have the peak at a desired slack.\n        \n        # Let `alpha = 2`, `beta = 4`. Peak at `slack = 2/4 = 0.5`.\n        # `priorities[fittable_bins_mask] = slack**2 * np.exp(-4*slack)`\n        # This function is 0 at slack=0, peaks at 0.5, and goes to 0 as slack increases.\n        # This might be *too* much exploration and not enough tight fit preference.\n        \n        # Let's stick to favoring small slack primarily, but with a less aggressive decay.\n        # `1 / (1 + slack^p)` with `p` around 0.5 seems a good balance.\n        \n        # Consider `p = 0.7`.\n        # slack = 0.01: 1 / (1 + 0.0046) = 0.995\n        # slack = 0.1:  1 / (1 + 0.1778) = 0.845\n        # slack = 0.5:  1 / (1 + 0.421)  = 0.703\n        # slack = 1.0:  1 / (1 + 1.0)    = 0.5\n        \n        # This function is higher for small slacks than `1/(1+slack^0.5)`, and decays slower.\n        # This means it favors tight fits, but also gives good scores to moderately tight fits,\n        # promoting exploration of slightly larger gaps.\n        \n        # Let's use `p = 0.7` as a parameter.\n        p_value = 0.7\n        \n        # Add a small epsilon to slack to avoid potential issues with `slack=0` and `p<1` leading to undefined behavior\n        # or excessively high values if `slack` could be extremely close to zero.\n        # However, `0**p` is 0 for `p>0`. So, `1/(1+0)` is 1. This is fine.\n        \n        # Let's use a scaling factor for slack to control the \"tightness\" preference.\n        # `scaled_slack = slack / item_size_scale`\n        # A scale factor related to average item size might be good.\n        # For now, let's use absolute slack with `p=0.7`.\n        \n        priorities[fittable_bins_mask] = 1.0 / (1.0 + slack**p_value)\n        \n        # Ensure priorities are not NaN or Inf (though unlikely with this formula and non-negative slack)\n        priorities[fittable_bins_mask] = np.nan_to_num(priorities[fittable_bins_mask], nan=0.0, posinf=1.0, neginf=0.0)\n        \n    return priorities\n\n[Reflection]\nFavor tighter fits with a gentler decay for exploration.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}