```python
import numpy as np
import math

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a refined Almost Full Fit strategy.

    This version refines the Almost Full Fit (AFF) strategy by:
    1. Prioritizing bins with the smallest *positive* remaining capacity *after* placing the item.
    2. Using a sigmoid function to create a smoother preference for bins that are closer to being full,
       while still ensuring items always fit. This avoids overly aggressive prioritization of very full bins
       if slightly less full bins offer a better fit overall.
    3. Breaking ties by favoring bins with lower indices (implicitly handled by numpy operations).

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
        A higher score indicates a higher priority.
    """
    # Initialize priorities to a very low value (effectively -infinity for maximization)
    priorities = np.full_like(bins_remain_cap, -np.inf)

    # Identify bins that can fit the item
    can_fit_mask = bins_remain_cap >= item

    # For bins that can fit the item, calculate the remaining capacity after placing the item.
    remaining_after_item = bins_remain_cap[can_fit_mask] - item

    # We want to prioritize bins with the smallest *positive* remaining capacity.
    # A smaller positive remaining capacity indicates a "better fit" in the context of AFF.
    #
    # To achieve this, we can use a transformation that maps smaller positive remaining capacities
    # to higher scores. A sigmoid function can provide a smooth transition.
    #
    # We can map `remaining_after_item` to a score. A common approach is to center the sigmoid
    # around a value that represents a "good fit", or to use the raw remaining capacity.
    #
    # Let's consider the negative of remaining capacity, which naturally gives higher values to smaller capacities.
    # `-(remaining_after_item)`: Smaller remaining -> Larger negative value -> Higher priority.
    #
    # To make it smoother and more robust, we can apply a sigmoid.
    # `sigmoid(x) = 1 / (1 + exp(-x))`
    # If `x` is small (highly negative, meaning very small remaining capacity), sigmoid(x) approaches 0.
    # If `x` is large (positive, meaning large remaining capacity), sigmoid(x) approaches 1.
    # We want the opposite: small remaining capacity -> high priority.
    #
    # Let's try `sigmoid(-remaining_after_item)`.
    # If `remaining_after_item` is small (e.g., 0.1), `-remaining_after_item` is small negative. `sigmoid(-0.1)` is ~0.47.
    # If `remaining_after_item` is larger (e.g., 2), `-remaining_after_item` is -2. `sigmoid(-2)` is ~0.12.
    # This isn't quite right, as we want smaller remaining capacity to give a HIGHER priority score.
    #
    # Let's rethink the goal: prioritize bins with smallest *positive* remaining capacity.
    # This implies that if remaining_after_item is 0.1, it's better than 0.2.
    #
    # Consider `priority = -remaining_after_item`.
    # If remaining = 0.1, priority = -0.1
    # If remaining = 0.2, priority = -0.2
    # This correctly assigns higher priority (less negative) to smaller remaining capacities.
    #
    # To introduce smoothness and potentially a slight bias towards "closer to zero" remaining capacity,
    # we can use the sigmoid function on a transformed value of `remaining_after_item`.
    #
    # Let's try mapping `remaining_after_item` directly. A smaller `remaining_after_item` should
    # yield a higher priority.
    #
    # Option 1: Use negative remaining capacity directly, perhaps scaled.
    # `priorities[can_fit_mask] = -remaining_after_item` - This is like the original v1.
    #
    # Option 2: Use sigmoid on a value that is inverted or scaled to favor small positive numbers.
    # Consider `exp(-k * remaining_after_item)`. For small `remaining_after_item`, this is close to 1. For larger, it's closer to 0.
    # `k` controls the steepness. A larger `k` means more preference for smaller remaining capacities.
    # Let's pick a reasonable `k`, say `k=10`.
    # `exp(-10 * remaining_after_item)`:
    # If remaining = 0.01, exp(-0.1) ~ 0.90
    # If remaining = 0.1, exp(-1) ~ 0.37
    # If remaining = 0.5, exp(-5) ~ 0.006
    # This seems to capture the essence of preferring smaller positive remaining capacities.

    # Let's apply the exponential decay function. A small positive remaining capacity should yield a high score.
    # The parameter 'scale' controls how quickly the priority drops off as remaining capacity increases.
    # A smaller 'scale' means priority drops faster. We want to prioritize bins that are *almost* full,
    # so we want high priority for small remaining capacities.
    # We can use `exp(-remaining_capacity / scale)`.
    # To ensure we are prioritizing smallest *positive* remaining capacities, we can scale `remaining_after_item`
    # and then apply an exponential function that maps smaller positive values to higher outputs.
    # A simple inversion might be `1 / (1 + remaining_after_item)` but that's not very smooth.
    #
    # Let's use a sigmoid applied to the *negative* of the remaining capacity, scaled.
    # `sigmoid(-remaining_after_item * scale)`
    # If `remaining_after_item` is small (e.g., 0.01) and `scale` is large (e.g., 100):
    #   `sigmoid(-0.01 * 100) = sigmoid(-1) ~ 0.27` (This is low, not what we want)
    #
    # Let's reconsider the reflection: "prioritizing bins with the smallest *positive* remaining capacity".
    # This means if remaining is 0.1, it's better than 0.2.
    # We want a function `f(x)` where `x` is remaining capacity, such that `f(0.1) > f(0.2)`.
    # The function `f(x) = 1 / (1 + x)` works. `1/(1+0.1) = 0.909`, `1/(1+0.2) = 0.833`.
    # Or `f(x) = exp(-k*x)`. `exp(-10*0.1) = 0.36`, `exp(-10*0.2) = 0.13`.
    #
    # The sigmoid function in the reflection: "sigmoid functions for smooth preference".
    # A common use of sigmoid is to map values to [0, 1].
    # If we map `remaining_after_item` to `[0, 1]` and then invert the sense.
    #
    # Let's try mapping `remaining_after_item` to a desirability score.
    # Desirability is high when `remaining_after_item` is small.
    # We can use `1 - sigmoid(remaining_after_item * scale)`.
    # If `remaining_after_item` is small (e.g., 0.01) and `scale` is large (e.g., 100):
    #   `sigmoid(0.01 * 100) = sigmoid(1) ~ 0.73`
    #   `1 - 0.73 = 0.27` (Still low)
    #
    # The core idea is that smaller positive values are better.
    # Let's use a sigmoid that maps smaller positive values to higher outputs.
    # This is achieved by passing a *negative* value to sigmoid.
    # Consider `sigmoid(-remaining_after_item * scale)`.
    # To make small positive `remaining_after_item` map to high values, the argument to sigmoid should be large positive.
    # This means we need to transform `remaining_after_item` so that small positive values become large positive values.
    #
    # A simple approach is to use `exp(-remaining_after_item * scale)`.
    # For `remaining_after_item = 0.01`, `scale = 100`: `exp(-1) ~ 0.36`.
    # For `remaining_after_item = 0.1`, `scale = 100`: `exp(-10) ~ 0.000045`.
    # This works but might not be smooth enough if `scale` is too large.
    #
    # Let's try a sigmoid again.
    # We want `f(x)` where `x` is remaining capacity, `f(small_positive) > f(large_positive)`.
    # The function `1 / (1 + x)` is monotonic decreasing for `x > 0`.
    # The function `exp(-k*x)` is also monotonic decreasing for `x > 0`.
    #
    # The reflection mentions "sigmoid functions for smooth preference".
    # A sigmoid function typically has an S-shape. If we want small positive values to have high priority,
    # we can apply sigmoid to a transformation of remaining capacity.
    #
    # Let's try mapping `remaining_after_item` to a negative range that is then fed into sigmoid.
    # `sigmoid(-remaining_after_item * scale)`:
    # `remaining_after_item` (x) | `scale` | `-x*scale` | `sigmoid(-x*scale)`
    # ---------------------------|---------|------------|--------------------
    # 0.01                       | 10      | -0.1       | 0.47
    # 0.1                        | 10      | -1         | 0.27
    # 0.5                        | 10      | -5         | 0.006
    #
    # This seems reasonable. Higher priority for smaller remaining capacities.
    # The scale factor determines how sensitive the priority is to the remaining capacity.
    # A larger `scale` will make the function steeper, meaning it will more aggressively
    # favor bins with very small remaining capacities.
    # Let's use `scale = 10.0` as a starting point for smooth preference.

    scale = 10.0  # Controls the steepness of the sigmoid preference. Higher scale means stronger preference for smaller remaining capacities.

    # Apply sigmoid to the negative of remaining capacity scaled by 'scale'.
    # This maps smaller positive remaining capacities to higher sigmoid outputs (closer to 0.5 or higher),
    # and larger remaining capacities to lower sigmoid outputs (closer to 0).
    # We want highest priority for smallest positive remaining capacity.
    # The sigmoid function `1 / (1 + exp(-z))` gives values between 0 and 1.
    # If `z = -remaining_after_item * scale`:
    #   - `remaining_after_item` small positive (e.g., 0.01), `z` is small negative (e.g., -0.1). sigmoid(z) ~ 0.47.
    #   - `remaining_after_item` larger positive (e.g., 0.5), `z` is larger negative (e.g., -5). sigmoid(z) ~ 0.006.
    # This means smaller `remaining_after_item` gets higher priority.

    # Ensure remaining_after_item is not negative due to float precision issues
    # (though the can_fit_mask should prevent this, it's good practice)
    safe_remaining_after_item = np.maximum(remaining_after_item, 0)

    # Calculate sigmoid of the scaled negative remaining capacity
    # Add a small epsilon to the argument of exp to avoid potential overflow with very large negative numbers if scale is huge.
    # Or ensure the argument to sigmoid is not too extreme.
    # Using `sigmoid(-x * scale)` directly is generally fine.
    sigmoid_scores = 1 / (1 + np.exp(-safe_remaining_after_item * scale))

    # Update priorities for bins that can fit the item
    priorities[can_fit_mask] = sigmoid_scores

    # Tie-breaking: Numpy operations implicitly favor elements with lower indices
    # when values are identical. So, if two bins have the same priority score,
    # the one appearing earlier in `bins_remain_cap` (lower index) will be preferred
    # by `np.argmax` if they end up being the maximum.

    return priorities
```
