{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First strategy.\n\n    The Exact Fit First strategy prioritizes bins that have a remaining capacity\n    exactly equal to the item's size. If no such bin exists, it prioritizes\n    bins that have the smallest remaining capacity greater than or equal to\n    the item's size. This aims to minimize wasted space by finding the tightest fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the difference between remaining capacity and item size for bins that can fit\n    differences = bins_remain_cap[can_fit_mask] - item\n\n    # Exact Fit: Assign a high priority if the remaining capacity exactly matches the item size\n    exact_fit_mask = (differences == 0)\n    priorities[can_fit_mask][exact_fit_mask] = 1000.0  # High priority for exact fit\n\n    # Approximate Fit: For bins that don't offer an exact fit, prioritize those with the smallest positive difference.\n    # We can achieve this by inverting the difference (larger difference becomes smaller priority)\n    # and adding a small offset to distinguish them from exact fits.\n    non_exact_fit_mask = ~exact_fit_mask\n    if np.any(can_fit_mask[can_fit_mask][non_exact_fit_mask]):\n        # Calculate a score based on how \"close\" the fit is.\n        # Smaller positive difference is better. We can use 1/(difference + epsilon)\n        # or simply a large negative number for differences to sort them.\n        # A simpler approach is to use a value that decreases as the difference increases,\n        # but still higher than the non-fitting bins (which have priority 0).\n        # We can use a large negative number for difference, and then \"invert\" it\n        # to make smaller positive differences have higher priority.\n\n        # Example: If item is 5 and capacities are [10, 7, 12, 8]\n        # Can fit: [True, True, True, True]\n        # Differences: [5, 2, 7, 3]\n        # Exact fit: None\n        # We want to prioritize bins with differences [2, 3, 5, 7].\n        # Smallest positive difference should have highest priority among non-exact fits.\n\n        # A simple scoring mechanism: assign a score that is inversely proportional to the difference.\n        # To make smaller differences have higher priority, we can use a formula like:\n        # score = MAX_PRIORITY - difference\n        # where MAX_PRIORITY is a value larger than the exact fit priority, or a value\n        # that allows for differentiation between non-exact fits.\n        # Here, we use a scoring based on the inverse of the difference, scaled.\n        # Add a small epsilon to avoid division by zero if the difference were zero (though handled by exact_fit_mask)\n\n        # Assign priority for non-exact fits, giving higher priority to smaller remaining capacity\n        # The values should be less than the exact fit priority (1000)\n        relevant_indices = np.where(can_fit_mask)[0]\n        relevant_non_exact_indices = np.where(can_fit_mask & ~exact_fit_mask)[0]\n\n        if len(relevant_non_exact_indices) > 0:\n            # Assign priorities inversely proportional to the remaining capacity that is greater than item size.\n            # Or, more directly, proportional to the negative of the difference, but this would give\n            # negative priorities which might be confusing.\n            # A better approach: make priority decrease as the difference increases.\n            # A score like `1 / (difference + 1e-9)` would work, but then we need to scale it.\n            # Alternatively, sort the differences and assign decreasing priorities.\n            \n            # Let's assign priority such that smaller difference means higher priority among non-exact fits.\n            # A simple way is to assign a score like `1000 - difference`. This makes smaller differences\n            # closer to 1000, and larger differences further away, while still below the exact fit.\n            \n            # Get the differences for the non-exact fits\n            non_exact_differences = bins_remain_cap[relevant_non_exact_indices] - item\n            \n            # Assign priorities. A higher value means higher priority.\n            # We want smaller `non_exact_differences` to have higher priority.\n            # So, `K - non_exact_differences` where K is a large enough constant.\n            # Let's use K = 100, so priorities range from 99 (for difference 1) down to values near 0.\n            priorities[relevant_non_exact_indices] = 100 - non_exact_differences\n\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit strategy.\n\n    The Softmax-Based Fit strategy assigns a priority to each bin based on how well an item fits into it,\n    using a softmax function to convert these \"fitness\" scores into probabilities (priorities).\n    A higher score (closer to 1.0) means the bin is a better fit for the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate fitness scores for each bin. A higher score means a better fit.\n    # We consider bins that can fit the item. For bins that cannot fit, assign a very low score.\n    # A simple fitness metric could be the remaining capacity if the item fits,\n    # otherwise, a very small negative number to ensure it's not chosen by softmax.\n    fitness_scores = np.where(bins_remain_cap >= item, bins_remain_cap - item, -np.inf)\n\n    # Apply the softmax function to convert fitness scores into priorities.\n    # Add a small epsilon to avoid issues with all scores being -inf (though unlikely here\n    # as we expect at least one bin to fit in typical scenarios or the problem is ill-posed).\n    # We add 1 to the scores before softmax because the softmax function operates on positive values,\n    # and a direct application of softmax on potentially negative 'fitness_scores' could lead to\n    # numerically unstable results if not handled carefully.\n    # An alternative approach is to shift the scores so they are all non-negative before applying softmax.\n    # However, the core idea of softmax is about relative differences.\n    # Let's try a simpler approach: treat `bins_remain_cap - item` directly as scores\n    # and use `np.exp` for higher values. We want bins with *less* remaining capacity after packing\n    # to be prioritized if they still fit (to achieve fuller bins).\n    # So, a good fit means `bins_remain_cap - item` is close to zero.\n    # Let's define a \"goodness\" score as a decreasing function of `bins_remain_cap - item`.\n    # For example, `- (bins_remain_cap - item)`.\n    # We only consider bins where `bins_remain_cap >= item`.\n\n    # Calculate the difference between remaining capacity and item size.\n    # Bins with smaller positive differences are better fits (closer to zero).\n    diffs = bins_remain_cap - item\n\n    # Create a \"desirability\" score: higher for bins with small positive differences.\n    # If an item doesn't fit, its desirability is very low (large negative number).\n    # We want bins with diffs closer to 0 to have higher desirability.\n    # Let's use -(diffs) for bins that fit, and a very small negative number for those that don't.\n    desirability_scores = np.where(bins_remain_cap >= item, -diffs, -1e9) # Use a large negative number\n\n    # Apply softmax. To ensure positivity for exp, we can shift scores or use a base for exponentiation.\n    # A common trick is to shift all scores by subtracting the maximum score before exponentiating.\n    # This makes the largest score 0, and others negative, which is numerically stable for softmax.\n    # However, we want to directly translate how *good* the fit is into a probability.\n    # Let's re-think the \"fit\" for BPP. We want to put items into bins.\n    # A \"good fit\" means the remaining capacity is *small* after the item is placed,\n    # because this suggests the bin is getting full and we are efficiently using space.\n    # So, `bins_remain_cap - item` should be minimized for bins that can fit.\n\n    # Let's define a score `s_i` for bin `i`:\n    # If bin `i` can fit the item (bins_remain_cap[i] >= item):\n    #   s_i = 1 / (bins_remain_cap[i] - item + epsilon)  -- higher score for smaller remaining capacity. Add epsilon to avoid division by zero.\n    # If bin `i` cannot fit the item:\n    #   s_i = 0 (or a very small value)\n\n    epsilon = 1e-6 # Small constant to avoid division by zero\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Calculate scores only for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    fit_capacities = bins_remain_cap[can_fit_mask]\n    \n    if np.any(can_fit_mask):\n        # Calculate a \"fitness\" score: prioritize bins with less remaining capacity after placing the item.\n        # We want to minimize (bins_remain_cap - item). So, a higher score for smaller difference.\n        # Let's use -(bins_remain_cap - item) as the score, or something that is inversely proportional to the difference.\n        # A common approach in related fields is to use `exp(score)`.\n        # So, we want `exp(-k * (bins_remain_cap[i] - item))` where k is a scaling factor.\n        # For simplicity, let's use `exp(- (bins_remain_cap[i] - item))` as the raw score.\n        # Bins that fit should have a positive score. Bins that don't fit should have a zero or very low score.\n        \n        # A robust way to use softmax for preference:\n        # For each bin i, calculate a \"fit_value_i\".\n        # If bin i can fit the item, fit_value_i = some_positive_measure_of_fit.\n        # If bin i cannot fit, fit_value_i = 0.\n        # Then apply softmax on these fit_values.\n        \n        # Let's consider the remaining capacity after fitting the item. We want this to be small.\n        # So, a good fit corresponds to a small value of `bins_remain_cap[i] - item`.\n        # We can use `1 / (bins_remain_cap[i] - item + epsilon)` as a measure of \"goodness of fit\".\n        # The higher this value, the better the fit.\n        \n        # Let's map the 'remaining capacity after fitting' to a \"desirability\" score.\n        # We want smaller remaining capacity to be more desirable.\n        # Consider `bins_remain_cap[i] - item`.\n        # If this is 0, it's a perfect fit. If it's positive and large, it's a bad fit.\n        \n        # We want to give higher probability to bins where `bins_remain_cap[i] - item` is small and positive.\n        # Let's use `exp(alpha * (bins_remain_cap[i] - item))` as a measure, but this would\n        # prioritize bins with *larger* remaining capacity. We want the opposite.\n        \n        # Let's use `exp(alpha * - (bins_remain_cap[i] - item))` for bins that fit.\n        # `alpha` controls the steepness of the softmax. A higher alpha means\n        # smaller differences in remaining capacity lead to larger differences in probabilities.\n        alpha = 1.0 # Sensitivity parameter\n\n        # Calculate scores for bins that can fit the item\n        # We want to prioritize bins where `bins_remain_cap[i] - item` is close to 0.\n        # So, a score proportional to `1 / (bins_remain_cap[i] - item)` could work,\n        # but we need to make it suitable for softmax (non-negative or shifted).\n        \n        # A simpler approach: `fit_scores = bins_remain_cap[can_fit_mask] - item`.\n        # We want small values of `fit_scores` to be prioritized.\n        # To use softmax, we can use `exp(-alpha * fit_scores)`.\n        \n        fit_scores = fit_capacities - item\n        \n        # Ensure scores are not too large negative if alpha is large\n        # Clip scores to prevent overflow issues if -alpha * fit_scores is very large negative\n        # (though this is less of a concern if we're using softmax on positive values or shifted values)\n        \n        # Let's define a raw score for softmax:\n        # For bins that fit, raw_score = 1.0 / (bins_remain_cap[i] - item + epsilon)\n        # This gives higher scores to bins that are almost full.\n        \n        raw_scores = np.zeros_like(bins_remain_cap)\n        raw_scores[can_fit_mask] = 1.0 / (fit_capacities - item + epsilon)\n        \n        # Apply softmax to these raw_scores\n        # To use np.exp directly, scores must be handled carefully.\n        # A common practice for softmax is to use `exp(score - max_score)`\n        # for numerical stability, which essentially makes the highest score 1.\n        \n        # Let's try a strategy that directly maps a \"good fit\" to a positive value.\n        # Good fit => remaining capacity after packing is small and non-negative.\n        # Value = 1.0 / (remaining_capacity_after_packing + epsilon)\n        \n        # Calculate remaining capacity after placing the item\n        remaining_after_fit = bins_remain_cap - item\n        \n        # Initialize a 'desirability' array. Assign 0 to bins that cannot fit the item.\n        desirability = np.zeros_like(bins_remain_cap)\n        \n        # For bins that can fit, assign a desirability score.\n        # We want to prioritize bins where `remaining_after_fit` is small and positive.\n        # A simple mapping: desirability = 1.0 / (remaining_after_fit + epsilon)\n        # This makes bins that are almost full (small positive remaining_after_fit) more desirable.\n        \n        desirability[can_fit_mask] = 1.0 / (remaining_after_fit[can_fit_mask] + epsilon)\n        \n        # Apply softmax function to the desirability scores.\n        # `np.exp(desirability)` creates exponentials of scores.\n        # Normalizing by the sum of exponentials converts these into probabilities.\n        \n        # To ensure numerical stability for `np.exp`, it's good practice to shift\n        # the scores so that the maximum score is 0.\n        # This is done by subtracting the maximum score from all scores.\n        \n        max_desirability = np.max(desirability)\n        shifted_desirability = desirability - max_desirability\n        \n        # Calculate exponentials of the shifted desirability scores\n        exp_scores = np.exp(shifted_desirability)\n        \n        # Normalize by the sum of the exponentials to get probabilities (priorities)\n        sum_exp_scores = np.sum(exp_scores)\n        \n        # Handle the case where all bins are unable to fit the item (sum_exp_scores would be 0).\n        # In such a case, all priorities should be 0 or some default.\n        if sum_exp_scores == 0:\n            priorities = np.zeros_like(bins_remain_cap)\n        else:\n            priorities = exp_scores / sum_exp_scores\n            \n    else:\n        # If no bin can fit the item, return all zeros\n        priorities = np.zeros_like(bins_remain_cap)\n\n    return priorities\n\n[Reflection]\nPrioritize tight fits, then use softmax for smoother probability distributions.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}