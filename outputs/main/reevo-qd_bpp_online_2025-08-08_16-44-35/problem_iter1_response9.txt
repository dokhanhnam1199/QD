```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Softmax-Based Fit strategy.

    The Softmax-Based Fit strategy assigns a priority to each bin based on how well an item fits into it,
    using a softmax function to convert these "fitness" scores into probabilities (priorities).
    A higher score (closer to 1.0) means the bin is a better fit for the item.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Returns:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Calculate fitness scores for each bin. A higher score means a better fit.
    # We consider bins that can fit the item. For bins that cannot fit, assign a very low score.
    # A simple fitness metric could be the remaining capacity if the item fits,
    # otherwise, a very small negative number to ensure it's not chosen by softmax.
    fitness_scores = np.where(bins_remain_cap >= item, bins_remain_cap - item, -np.inf)

    # Apply the softmax function to convert fitness scores into priorities.
    # Add a small epsilon to avoid issues with all scores being -inf (though unlikely here
    # as we expect at least one bin to fit in typical scenarios or the problem is ill-posed).
    # We add 1 to the scores before softmax because the softmax function operates on positive values,
    # and a direct application of softmax on potentially negative 'fitness_scores' could lead to
    # numerically unstable results if not handled carefully.
    # An alternative approach is to shift the scores so they are all non-negative before applying softmax.
    # However, the core idea of softmax is about relative differences.
    # Let's try a simpler approach: treat `bins_remain_cap - item` directly as scores
    # and use `np.exp` for higher values. We want bins with *less* remaining capacity after packing
    # to be prioritized if they still fit (to achieve fuller bins).
    # So, a good fit means `bins_remain_cap - item` is close to zero.
    # Let's define a "goodness" score as a decreasing function of `bins_remain_cap - item`.
    # For example, `- (bins_remain_cap - item)`.
    # We only consider bins where `bins_remain_cap >= item`.

    # Calculate the difference between remaining capacity and item size.
    # Bins with smaller positive differences are better fits (closer to zero).
    diffs = bins_remain_cap - item

    # Create a "desirability" score: higher for bins with small positive differences.
    # If an item doesn't fit, its desirability is very low (large negative number).
    # We want bins with diffs closer to 0 to have higher desirability.
    # Let's use -(diffs) for bins that fit, and a very small negative number for those that don't.
    desirability_scores = np.where(bins_remain_cap >= item, -diffs, -1e9) # Use a large negative number

    # Apply softmax. To ensure positivity for exp, we can shift scores or use a base for exponentiation.
    # A common trick is to shift all scores by subtracting the maximum score before exponentiating.
    # This makes the largest score 0, and others negative, which is numerically stable for softmax.
    # However, we want to directly translate how *good* the fit is into a probability.
    # Let's re-think the "fit" for BPP. We want to put items into bins.
    # A "good fit" means the remaining capacity is *small* after the item is placed,
    # because this suggests the bin is getting full and we are efficiently using space.
    # So, `bins_remain_cap - item` should be minimized for bins that can fit.

    # Let's define a score `s_i` for bin `i`:
    # If bin `i` can fit the item (bins_remain_cap[i] >= item):
    #   s_i = 1 / (bins_remain_cap[i] - item + epsilon)  -- higher score for smaller remaining capacity. Add epsilon to avoid division by zero.
    # If bin `i` cannot fit the item:
    #   s_i = 0 (or a very small value)

    epsilon = 1e-6 # Small constant to avoid division by zero
    priorities = np.zeros_like(bins_remain_cap)

    # Calculate scores only for bins that can fit the item
    can_fit_mask = bins_remain_cap >= item
    fit_capacities = bins_remain_cap[can_fit_mask]
    
    if np.any(can_fit_mask):
        # Calculate a "fitness" score: prioritize bins with less remaining capacity after placing the item.
        # We want to minimize (bins_remain_cap - item). So, a higher score for smaller difference.
        # Let's use -(bins_remain_cap - item) as the score, or something that is inversely proportional to the difference.
        # A common approach in related fields is to use `exp(score)`.
        # So, we want `exp(-k * (bins_remain_cap[i] - item))` where k is a scaling factor.
        # For simplicity, let's use `exp(- (bins_remain_cap[i] - item))` as the raw score.
        # Bins that fit should have a positive score. Bins that don't fit should have a zero or very low score.
        
        # A robust way to use softmax for preference:
        # For each bin i, calculate a "fit_value_i".
        # If bin i can fit the item, fit_value_i = some_positive_measure_of_fit.
        # If bin i cannot fit, fit_value_i = 0.
        # Then apply softmax on these fit_values.
        
        # Let's consider the remaining capacity after fitting the item. We want this to be small.
        # So, a good fit corresponds to a small value of `bins_remain_cap[i] - item`.
        # We can use `1 / (bins_remain_cap[i] - item + epsilon)` as a measure of "goodness of fit".
        # The higher this value, the better the fit.
        
        # Let's map the 'remaining capacity after fitting' to a "desirability" score.
        # We want smaller remaining capacity to be more desirable.
        # Consider `bins_remain_cap[i] - item`.
        # If this is 0, it's a perfect fit. If it's positive and large, it's a bad fit.
        
        # We want to give higher probability to bins where `bins_remain_cap[i] - item` is small and positive.
        # Let's use `exp(alpha * (bins_remain_cap[i] - item))` as a measure, but this would
        # prioritize bins with *larger* remaining capacity. We want the opposite.
        
        # Let's use `exp(alpha * - (bins_remain_cap[i] - item))` for bins that fit.
        # `alpha` controls the steepness of the softmax. A higher alpha means
        # smaller differences in remaining capacity lead to larger differences in probabilities.
        alpha = 1.0 # Sensitivity parameter

        # Calculate scores for bins that can fit the item
        # We want to prioritize bins where `bins_remain_cap[i] - item` is close to 0.
        # So, a score proportional to `1 / (bins_remain_cap[i] - item)` could work,
        # but we need to make it suitable for softmax (non-negative or shifted).
        
        # A simpler approach: `fit_scores = bins_remain_cap[can_fit_mask] - item`.
        # We want small values of `fit_scores` to be prioritized.
        # To use softmax, we can use `exp(-alpha * fit_scores)`.
        
        fit_scores = fit_capacities - item
        
        # Ensure scores are not too large negative if alpha is large
        # Clip scores to prevent overflow issues if -alpha * fit_scores is very large negative
        # (though this is less of a concern if we're using softmax on positive values or shifted values)
        
        # Let's define a raw score for softmax:
        # For bins that fit, raw_score = 1.0 / (bins_remain_cap[i] - item + epsilon)
        # This gives higher scores to bins that are almost full.
        
        raw_scores = np.zeros_like(bins_remain_cap)
        raw_scores[can_fit_mask] = 1.0 / (fit_capacities - item + epsilon)
        
        # Apply softmax to these raw_scores
        # To use np.exp directly, scores must be handled carefully.
        # A common practice for softmax is to use `exp(score - max_score)`
        # for numerical stability, which essentially makes the highest score 1.
        
        # Let's try a strategy that directly maps a "good fit" to a positive value.
        # Good fit => remaining capacity after packing is small and non-negative.
        # Value = 1.0 / (remaining_capacity_after_packing + epsilon)
        
        # Calculate remaining capacity after placing the item
        remaining_after_fit = bins_remain_cap - item
        
        # Initialize a 'desirability' array. Assign 0 to bins that cannot fit the item.
        desirability = np.zeros_like(bins_remain_cap)
        
        # For bins that can fit, assign a desirability score.
        # We want to prioritize bins where `remaining_after_fit` is small and positive.
        # A simple mapping: desirability = 1.0 / (remaining_after_fit + epsilon)
        # This makes bins that are almost full (small positive remaining_after_fit) more desirable.
        
        desirability[can_fit_mask] = 1.0 / (remaining_after_fit[can_fit_mask] + epsilon)
        
        # Apply softmax function to the desirability scores.
        # `np.exp(desirability)` creates exponentials of scores.
        # Normalizing by the sum of exponentials converts these into probabilities.
        
        # To ensure numerical stability for `np.exp`, it's good practice to shift
        # the scores so that the maximum score is 0.
        # This is done by subtracting the maximum score from all scores.
        
        max_desirability = np.max(desirability)
        shifted_desirability = desirability - max_desirability
        
        # Calculate exponentials of the shifted desirability scores
        exp_scores = np.exp(shifted_desirability)
        
        # Normalize by the sum of the exponentials to get probabilities (priorities)
        sum_exp_scores = np.sum(exp_scores)
        
        # Handle the case where all bins are unable to fit the item (sum_exp_scores would be 0).
        # In such a case, all priorities should be 0 or some default.
        if sum_exp_scores == 0:
            priorities = np.zeros_like(bins_remain_cap)
        else:
            priorities = exp_scores / sum_exp_scores
            
    else:
        # If no bin can fit the item, return all zeros
        priorities = np.zeros_like(bins_remain_cap)

    return priorities
```
