[
  {
    "stdout_filepath": "problem_iter1_response3.txt_stdout.txt",
    "code_path": "problem_iter1_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    The Almost Full Fit (AFF) strategy prioritizes bins that are almost full.\n    It selects the bin with the smallest remaining capacity that can still\n    accommodate the item, encouraging fuller bins to be used first, which\n    can lead to fewer bins being opened overall.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority.\n    \"\"\"\n    # Initialize priorities to a very low value (effectively -infinity for maximization)\n    # This ensures that bins that cannot fit the item receive no priority.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, assign a priority based on their remaining capacity.\n    # The strategy is to prioritize bins that are *almost full*.\n    # This means we want to select bins with the smallest positive remaining capacity\n    # after placing the item.\n    # A higher priority should be given to bins with *less* remaining capacity\n    # after the item is placed, but only if they can still accommodate it.\n    #\n    # The remaining capacity after placing the item is `bins_remain_cap - item`.\n    # To give higher priority to smaller remaining capacities, we can use the\n    # negative of the remaining capacity.\n    #\n    # Example:\n    # If bin_remain_cap = [5, 3, 7] and item = 2:\n    #   - Bin 0: can_fit = True, remaining_after_item = 3. Priority: -3\n    #   - Bin 1: can_fit = True, remaining_after_item = 1. Priority: -1 (HIGHER PRIORITY)\n    #   - Bin 2: can_fit = True, remaining_after_item = 5. Priority: -5\n    #\n    # So, the priority is `-(bins_remain_cap - item)`.\n\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # To further refine AFF, we might want to break ties.\n    # A common tie-breaking mechanism for bin packing is to prefer bins that have\n    # been used for longer, or simply the first encountered bin.\n    # In this implementation, numpy's default behavior will handle ties by\n    # returning the indices in the order they appear, which implicitly favors\n    # earlier bins in case of identical priority scores.\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit strategy.\n\n    In this First Fit strategy, we prioritize bins that can accommodate the item.\n    Among those that can, we prefer bins that will have the least remaining capacity\n    after placing the item (to try and fill bins as much as possible). If multiple\n    bins offer the same \"tightest fit\", the one with the lower index is prioritized.\n    Bins that cannot accommodate the item are given a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity if the item is placed in a suitable bin\n    remaining_caps_if_placed = bins_remain_cap[suitable_bins_mask] - item\n\n    # We want to prioritize bins that result in the smallest remaining capacity\n    # This means we want the smallest (remaining_caps_if_placed) to have the highest priority.\n    # To achieve this with maximization, we can use a negative of the remaining capacity,\n    # or a large number minus the remaining capacity. Let's use a large number minus\n    # the remaining capacity to ensure positive priorities for suitable bins.\n    # We also need to ensure that bins that CANNOT fit the item get a priority of 0.\n\n    # Assign a priority based on the \"tightest fit\" for suitable bins\n    # A higher score indicates a better fit (less remaining capacity)\n    # We can use 1 / (remaining_cap + 1) to give higher priority to smaller remaining caps,\n    # ensuring that the denominator is not zero.\n    # Adding a small epsilon to the remaining capacity before division avoids division by zero\n    # and also ensures that bins with exactly 0 remaining capacity after placement are prioritized.\n    epsilon = 1e-9\n    priorities[suitable_bins_mask] = 1.0 / (remaining_caps_if_placed + epsilon)\n\n    # To implement the \"first fit\" aspect (preferring lower index bins for ties),\n    # we can introduce a small penalty based on the bin index.\n    # A lower index should have a slightly higher priority if all else is equal.\n    # We can subtract a small value proportional to the index from the priority.\n    # Let's make this penalty small enough not to override the fit priority.\n    bin_indices = np.arange(len(bins_remain_cap))\n    priorities[suitable_bins_mask] -= bin_indices[suitable_bins_mask] * 1e-7 # Small penalty for higher index\n\n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit.\n\n    The Best Fit strategy aims to place the item into the bin where it fits\n    most snugly, leaving the smallest remaining capacity. This heuristic tries\n    to minimize wasted space in each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher priority means a better fit. Bins that cannot accommodate the\n        item are given a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Calculate the remaining capacity if the item is placed in each bin\n    potential_remaining_capacities = bins_remain_cap - item\n\n    # Only consider bins that have enough capacity for the item\n    # For these bins, the priority is the *negative* of the remaining capacity.\n    # This is because we want to minimize the remaining capacity (best fit).\n    # A larger negative number (smaller remaining capacity) will have a higher\n    # priority score.\n    fit_mask = bins_remain_cap >= item\n    priorities[fit_mask] = -potential_remaining_capacities[fit_mask]\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response20.txt_stdout.txt",
    "code_path": "problem_iter1_code20.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit.\n\n    The First Fit strategy prioritizes bins that have just enough capacity to\n    fit the current item. This is achieved by calculating a score that is\n    high when the remaining capacity is close to the item size, and decreases\n    as the remaining capacity becomes much larger than the item size.\n    A small epsilon is used to avoid division by zero if an item perfectly fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We want to prioritize bins where the remaining capacity is just enough\n    # for the item. This means `bins_remain_cap - item` should be close to 0.\n    # A higher score for smaller positive differences, and lower for larger ones.\n    # We also want to penalize bins that are too small to fit the item.\n\n    # Filter out bins that are too small to fit the item. These will have a priority of 0.\n    # Add a small epsilon to avoid division by zero when item perfectly fits.\n    # The formula `1.0 / (bins_remain_cap - item + epsilon)` gives higher priority\n    # to bins where `bins_remain_cap - item` is smaller (closer to 0).\n    epsilon = 1e-9\n    possible_fits = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(possible_fits):\n        # Calculate the \"slack\" after fitting the item.\n        slack = bins_remain_cap[possible_fits] - item\n        # The priority is inversely proportional to the slack. Smaller slack means higher priority.\n        # Adding epsilon avoids division by zero if slack is exactly 0.\n        priorities[possible_fits] = 1.0 / (slack + epsilon)\n\n    return priorities",
    "response_id": 20,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response9.txt_stdout.txt",
    "code_path": "problem_iter1_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit strategy.\n\n    The Softmax-Based Fit strategy assigns a priority to each bin based on how well an item fits into it,\n    using a softmax function to convert these \"fitness\" scores into probabilities (priorities).\n    A higher score (closer to 1.0) means the bin is a better fit for the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate fitness scores for each bin. A higher score means a better fit.\n    # We consider bins that can fit the item. For bins that cannot fit, assign a very low score.\n    # A simple fitness metric could be the remaining capacity if the item fits,\n    # otherwise, a very small negative number to ensure it's not chosen by softmax.\n    fitness_scores = np.where(bins_remain_cap >= item, bins_remain_cap - item, -np.inf)\n\n    # Apply the softmax function to convert fitness scores into priorities.\n    # Add a small epsilon to avoid issues with all scores being -inf (though unlikely here\n    # as we expect at least one bin to fit in typical scenarios or the problem is ill-posed).\n    # We add 1 to the scores before softmax because the softmax function operates on positive values,\n    # and a direct application of softmax on potentially negative 'fitness_scores' could lead to\n    # numerically unstable results if not handled carefully.\n    # An alternative approach is to shift the scores so they are all non-negative before applying softmax.\n    # However, the core idea of softmax is about relative differences.\n    # Let's try a simpler approach: treat `bins_remain_cap - item` directly as scores\n    # and use `np.exp` for higher values. We want bins with *less* remaining capacity after packing\n    # to be prioritized if they still fit (to achieve fuller bins).\n    # So, a good fit means `bins_remain_cap - item` is close to zero.\n    # Let's define a \"goodness\" score as a decreasing function of `bins_remain_cap - item`.\n    # For example, `- (bins_remain_cap - item)`.\n    # We only consider bins where `bins_remain_cap >= item`.\n\n    # Calculate the difference between remaining capacity and item size.\n    # Bins with smaller positive differences are better fits (closer to zero).\n    diffs = bins_remain_cap - item\n\n    # Create a \"desirability\" score: higher for bins with small positive differences.\n    # If an item doesn't fit, its desirability is very low (large negative number).\n    # We want bins with diffs closer to 0 to have higher desirability.\n    # Let's use -(diffs) for bins that fit, and a very small negative number for those that don't.\n    desirability_scores = np.where(bins_remain_cap >= item, -diffs, -1e9) # Use a large negative number\n\n    # Apply softmax. To ensure positivity for exp, we can shift scores or use a base for exponentiation.\n    # A common trick is to shift all scores by subtracting the maximum score before exponentiating.\n    # This makes the largest score 0, and others negative, which is numerically stable for softmax.\n    # However, we want to directly translate how *good* the fit is into a probability.\n    # Let's re-think the \"fit\" for BPP. We want to put items into bins.\n    # A \"good fit\" means the remaining capacity is *small* after the item is placed,\n    # because this suggests the bin is getting full and we are efficiently using space.\n    # So, `bins_remain_cap - item` should be minimized for bins that can fit.\n\n    # Let's define a score `s_i` for bin `i`:\n    # If bin `i` can fit the item (bins_remain_cap[i] >= item):\n    #   s_i = 1 / (bins_remain_cap[i] - item + epsilon)  -- higher score for smaller remaining capacity. Add epsilon to avoid division by zero.\n    # If bin `i` cannot fit the item:\n    #   s_i = 0 (or a very small value)\n\n    epsilon = 1e-6 # Small constant to avoid division by zero\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Calculate scores only for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    fit_capacities = bins_remain_cap[can_fit_mask]\n    \n    if np.any(can_fit_mask):\n        # Calculate a \"fitness\" score: prioritize bins with less remaining capacity after placing the item.\n        # We want to minimize (bins_remain_cap - item). So, a higher score for smaller difference.\n        # Let's use -(bins_remain_cap - item) as the score, or something that is inversely proportional to the difference.\n        # A common approach in related fields is to use `exp(score)`.\n        # So, we want `exp(-k * (bins_remain_cap[i] - item))` where k is a scaling factor.\n        # For simplicity, let's use `exp(- (bins_remain_cap[i] - item))` as the raw score.\n        # Bins that fit should have a positive score. Bins that don't fit should have a zero or very low score.\n        \n        # A robust way to use softmax for preference:\n        # For each bin i, calculate a \"fit_value_i\".\n        # If bin i can fit the item, fit_value_i = some_positive_measure_of_fit.\n        # If bin i cannot fit, fit_value_i = 0.\n        # Then apply softmax on these fit_values.\n        \n        # Let's consider the remaining capacity after fitting the item. We want this to be small.\n        # So, a good fit corresponds to a small value of `bins_remain_cap[i] - item`.\n        # We can use `1 / (bins_remain_cap[i] - item + epsilon)` as a measure of \"goodness of fit\".\n        # The higher this value, the better the fit.\n        \n        # Let's map the 'remaining capacity after fitting' to a \"desirability\" score.\n        # We want smaller remaining capacity to be more desirable.\n        # Consider `bins_remain_cap[i] - item`.\n        # If this is 0, it's a perfect fit. If it's positive and large, it's a bad fit.\n        \n        # We want to give higher probability to bins where `bins_remain_cap[i] - item` is small and positive.\n        # Let's use `exp(alpha * (bins_remain_cap[i] - item))` as a measure, but this would\n        # prioritize bins with *larger* remaining capacity. We want the opposite.\n        \n        # Let's use `exp(alpha * - (bins_remain_cap[i] - item))` for bins that fit.\n        # `alpha` controls the steepness of the softmax. A higher alpha means\n        # smaller differences in remaining capacity lead to larger differences in probabilities.\n        alpha = 1.0 # Sensitivity parameter\n\n        # Calculate scores for bins that can fit the item\n        # We want to prioritize bins where `bins_remain_cap[i] - item` is close to 0.\n        # So, a score proportional to `1 / (bins_remain_cap[i] - item)` could work,\n        # but we need to make it suitable for softmax (non-negative or shifted).\n        \n        # A simpler approach: `fit_scores = bins_remain_cap[can_fit_mask] - item`.\n        # We want small values of `fit_scores` to be prioritized.\n        # To use softmax, we can use `exp(-alpha * fit_scores)`.\n        \n        fit_scores = fit_capacities - item\n        \n        # Ensure scores are not too large negative if alpha is large\n        # Clip scores to prevent overflow issues if -alpha * fit_scores is very large negative\n        # (though this is less of a concern if we're using softmax on positive values or shifted values)\n        \n        # Let's define a raw score for softmax:\n        # For bins that fit, raw_score = 1.0 / (bins_remain_cap[i] - item + epsilon)\n        # This gives higher scores to bins that are almost full.\n        \n        raw_scores = np.zeros_like(bins_remain_cap)\n        raw_scores[can_fit_mask] = 1.0 / (fit_capacities - item + epsilon)\n        \n        # Apply softmax to these raw_scores\n        # To use np.exp directly, scores must be handled carefully.\n        # A common practice for softmax is to use `exp(score - max_score)`\n        # for numerical stability, which essentially makes the highest score 1.\n        \n        # Let's try a strategy that directly maps a \"good fit\" to a positive value.\n        # Good fit => remaining capacity after packing is small and non-negative.\n        # Value = 1.0 / (remaining_capacity_after_packing + epsilon)\n        \n        # Calculate remaining capacity after placing the item\n        remaining_after_fit = bins_remain_cap - item\n        \n        # Initialize a 'desirability' array. Assign 0 to bins that cannot fit the item.\n        desirability = np.zeros_like(bins_remain_cap)\n        \n        # For bins that can fit, assign a desirability score.\n        # We want to prioritize bins where `remaining_after_fit` is small and positive.\n        # A simple mapping: desirability = 1.0 / (remaining_after_fit + epsilon)\n        # This makes bins that are almost full (small positive remaining_after_fit) more desirable.\n        \n        desirability[can_fit_mask] = 1.0 / (remaining_after_fit[can_fit_mask] + epsilon)\n        \n        # Apply softmax function to the desirability scores.\n        # `np.exp(desirability)` creates exponentials of scores.\n        # Normalizing by the sum of exponentials converts these into probabilities.\n        \n        # To ensure numerical stability for `np.exp`, it's good practice to shift\n        # the scores so that the maximum score is 0.\n        # This is done by subtracting the maximum score from all scores.\n        \n        max_desirability = np.max(desirability)\n        shifted_desirability = desirability - max_desirability\n        \n        # Calculate exponentials of the shifted desirability scores\n        exp_scores = np.exp(shifted_desirability)\n        \n        # Normalize by the sum of the exponentials to get probabilities (priorities)\n        sum_exp_scores = np.sum(exp_scores)\n        \n        # Handle the case where all bins are unable to fit the item (sum_exp_scores would be 0).\n        # In such a case, all priorities should be 0 or some default.\n        if sum_exp_scores == 0:\n            priorities = np.zeros_like(bins_remain_cap)\n        else:\n            priorities = exp_scores / sum_exp_scores\n            \n    else:\n        # If no bin can fit the item, return all zeros\n        priorities = np.zeros_like(bins_remain_cap)\n\n    return priorities",
    "response_id": 9,
    "obj": 4.198244914240141,
    "SLOC": 27.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response14.txt_stdout.txt",
    "code_path": "problem_iter1_code14.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First strategy.\n\n    The Exact Fit First strategy prioritizes bins that have a remaining capacity\n    exactly equal to the item's size. If no such bin exists, it prioritizes\n    bins that have the smallest remaining capacity greater than or equal to\n    the item's size. This aims to minimize wasted space by finding the tightest fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the difference between remaining capacity and item size for bins that can fit\n    differences = bins_remain_cap[can_fit_mask] - item\n\n    # Exact Fit: Assign a high priority if the remaining capacity exactly matches the item size\n    exact_fit_mask = (differences == 0)\n    priorities[can_fit_mask][exact_fit_mask] = 1000.0  # High priority for exact fit\n\n    # Approximate Fit: For bins that don't offer an exact fit, prioritize those with the smallest positive difference.\n    # We can achieve this by inverting the difference (larger difference becomes smaller priority)\n    # and adding a small offset to distinguish them from exact fits.\n    non_exact_fit_mask = ~exact_fit_mask\n    if np.any(can_fit_mask[can_fit_mask][non_exact_fit_mask]):\n        # Calculate a score based on how \"close\" the fit is.\n        # Smaller positive difference is better. We can use 1/(difference + epsilon)\n        # or simply a large negative number for differences to sort them.\n        # A simpler approach is to use a value that decreases as the difference increases,\n        # but still higher than the non-fitting bins (which have priority 0).\n        # We can use a large negative number for difference, and then \"invert\" it\n        # to make smaller positive differences have higher priority.\n\n        # Example: If item is 5 and capacities are [10, 7, 12, 8]\n        # Can fit: [True, True, True, True]\n        # Differences: [5, 2, 7, 3]\n        # Exact fit: None\n        # We want to prioritize bins with differences [2, 3, 5, 7].\n        # Smallest positive difference should have highest priority among non-exact fits.\n\n        # A simple scoring mechanism: assign a score that is inversely proportional to the difference.\n        # To make smaller differences have higher priority, we can use a formula like:\n        # score = MAX_PRIORITY - difference\n        # where MAX_PRIORITY is a value larger than the exact fit priority, or a value\n        # that allows for differentiation between non-exact fits.\n        # Here, we use a scoring based on the inverse of the difference, scaled.\n        # Add a small epsilon to avoid division by zero if the difference were zero (though handled by exact_fit_mask)\n\n        # Assign priority for non-exact fits, giving higher priority to smaller remaining capacity\n        # The values should be less than the exact fit priority (1000)\n        relevant_indices = np.where(can_fit_mask)[0]\n        relevant_non_exact_indices = np.where(can_fit_mask & ~exact_fit_mask)[0]\n\n        if len(relevant_non_exact_indices) > 0:\n            # Assign priorities inversely proportional to the remaining capacity that is greater than item size.\n            # Or, more directly, proportional to the negative of the difference, but this would give\n            # negative priorities which might be confusing.\n            # A better approach: make priority decrease as the difference increases.\n            # A score like `1 / (difference + 1e-9)` would work, but then we need to scale it.\n            # Alternatively, sort the differences and assign decreasing priorities.\n            \n            # Let's assign priority such that smaller difference means higher priority among non-exact fits.\n            # A simple way is to assign a score like `1000 - difference`. This makes smaller differences\n            # closer to 1000, and larger differences further away, while still below the exact fit.\n            \n            # Get the differences for the non-exact fits\n            non_exact_differences = bins_remain_cap[relevant_non_exact_indices] - item\n            \n            # Assign priorities. A higher value means higher priority.\n            # We want smaller `non_exact_differences` to have higher priority.\n            # So, `K - non_exact_differences` where K is a large enough constant.\n            # Let's use K = 100, so priorities range from 99 (for difference 1) down to values near 0.\n            priorities[relevant_non_exact_indices] = 100 - non_exact_differences\n\n\n    return priorities",
    "response_id": 14,
    "obj": 5.195452732349436,
    "SLOC": 14.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response16.txt_stdout.txt",
    "code_path": "problem_iter1_code16.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score strategy prioritizes bins based on how well an item fits.\n    It uses a sigmoid function to map the ratio of remaining capacity to item size\n    to a priority score. Higher scores indicate a better fit (less wasted space).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    # Consider only bins that can accommodate the item\n    available_bins_mask = bins_remain_cap >= item\n\n    # Calculate the \"slack\" or remaining capacity after placing the item\n    slack = bins_remain_cap[available_bins_mask] - item\n\n    # If there are no available bins, return all zeros\n    if slack.size == 0:\n        return priorities\n\n    # Use the sigmoid function to convert slack to a priority score.\n    # We want to minimize slack, so we want to maximize the negative slack.\n    # A common approach is to use exp(-k * slack) where k controls the steepness.\n    # A higher score means less slack (better fit).\n    # Let's map slack to a score such that small slack (good fit) gets high priority.\n    # We can use 1 / (1 + slack) or exp(-slack/scale). Let's use the exp approach for flexibility.\n    # We'll use a \"steepness\" parameter (k) that can be tuned.\n    # A higher k makes the sigmoid steeper, thus more sensitive to small differences in slack.\n    # Let's assume a reasonable default for k.\n    k = 10.0  # Steepness parameter\n\n    # Calculate scores for available bins. A smaller slack should result in a higher priority.\n    # The sigmoid function usually maps to [0, 1]. We want high priority for low slack.\n    # So, we can use exp(-k * slack). This maps low slack to values closer to 1, high slack to values closer to 0.\n    scores = np.exp(-k * slack)\n\n    # Place these scores back into the priority array at the correct indices\n    priorities[available_bins_mask] = scores\n\n    return priorities",
    "response_id": 16,
    "obj": 4.198244914240141,
    "SLOC": 10.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response26.txt_stdout.txt",
    "code_path": "problem_iter1_code26.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    This heuristic prioritizes bins that have a remaining capacity close to the item's size,\n    using a sigmoid function to smooth the preference. Bins with very large or very small\n    remaining capacities relative to the item size are penalized.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Avoid division by zero if bins_remain_cap is all zeros\n    if np.all(bins_remain_cap == 0):\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the difference between remaining capacity and item size\n    diff = bins_remain_cap - item\n\n    # Use a sigmoid function to map the difference to a priority score.\n    # We want bins where diff is close to zero to have higher priority.\n    # A common sigmoid form is 1 / (1 + exp(-x)).\n    # To make higher diffs (meaning more empty space) less preferred,\n    # we can map diff to exp(-abs(diff)). This gives higher scores when diff is small.\n    # However, the request asks for Sigmoid Fit Score, implying a direct sigmoid use.\n    # A sigmoid `s(x) = 1 / (1 + exp(-k*x))` squashes values between 0 and 1.\n    # We want to prioritize bins where `bins_remain_cap >= item`.\n    # Let's define a score where `bins_remain_cap - item` is the input.\n    # If `bins_remain_cap - item` is negative, the item doesn't fit. We should\n    # assign a very low priority. If it's positive, we want to find a good fit.\n    # A good fit would be when `bins_remain_cap - item` is small (close to 0),\n    # indicating minimal wasted space.\n\n    # Let's try mapping `bins_remain_cap` to a score.\n    # Bins with `bins_remain_cap >= item` are candidates.\n    # Among these, we prefer those closer to `item`.\n\n    # Option 1: Sigmoid of (item - bins_remain_cap), for bins that can fit the item\n    # This will give values close to 1 for bins where item is slightly less than bin capacity,\n    # and values close to 0 for bins where item is much less than bin capacity (lots of slack).\n    # For bins where item doesn't fit (bins_remain_cap < item), the argument becomes positive,\n    # resulting in very low sigmoid values, effectively giving them low priority.\n\n    # To prevent potential issues with large negative numbers in exp,\n    # we can shift the input. A common approach for fitting is to center around zero.\n    # Let's consider the 'slack' `bins_remain_cap - item`.\n    # If slack is negative, the item doesn't fit, priority should be 0.\n    # If slack is 0, priority should be high (e.g., 1).\n    # If slack is positive and small, priority should be high.\n    # If slack is positive and large, priority should be lower.\n\n    # Let's use the `bins_remain_cap` directly and adapt the sigmoid.\n    # We want a function f(cap) such that f(item) is high, and f(cap) is low for cap << item or cap >> item.\n\n    # A simple sigmoid: 1 / (1 + exp(-k * (target - x)))\n    # Here, our 'target' is the `item` size, and 'x' is `bins_remain_cap`.\n    # We want a high score when `bins_remain_cap` is close to `item`.\n    # So, we can use `k * (item - bins_remain_cap)`.\n    # Let k be a sensitivity parameter, say 1.0.\n\n    # To handle cases where the item doesn't fit (`bins_remain_cap < item`),\n    # we can set their priority to 0 explicitly.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        # Apply sigmoid to the bins that can fit the item.\n        # We want higher scores when `bins_remain_cap` is closer to `item`.\n        # Let's use a sigmoid on the difference `item - bins_remain_cap`.\n        # This difference is non-positive for bins that can fit the item.\n        # `item - bins_remain_cap` is 0 when `bins_remain_cap == item` (perfect fit).\n        # As `bins_remain_cap` increases, `item - bins_remain_cap` becomes more negative,\n        # and the sigmoid value approaches 0.\n\n        # To make it more robust to extreme differences, we can clip or scale.\n        # Let's use the 'slack' `bins_remain_cap - item`. We want small positive slack to be good.\n        # A function like `exp(-slack)` might work, but it's not a sigmoid.\n\n        # Let's stick to a sigmoid applied to `bins_remain_cap`.\n        # The sigmoid `1 / (1 + exp(-k*x))` increases with x.\n        # We want `bins_remain_cap` to be close to `item`.\n\n        # Let's consider a transformed value that is small for bad fits and large for good fits.\n        # How about `abs(bins_remain_cap - item)`? We want this to be small.\n        # Sigmoid of `-abs(bins_remain_cap - item)` will be high when the difference is small.\n\n        # Let's consider a standard logistic sigmoid function:\n        # f(x) = 1 / (1 + exp(-beta * (x - alpha)))\n        # We want this function to peak when bins_remain_cap is close to item.\n        # Let alpha = item (the desired capacity).\n        # Let beta control the steepness. A larger beta means a sharper peak.\n        # For bins where bins_remain_cap < item, the function should ideally be 0.\n\n        # A simpler approach related to sigmoid behavior for fitting:\n        # We want the priority to be high when `bins_remain_cap` is `item`, and\n        # decrease as `bins_remain_cap` deviates from `item` in either direction.\n        # However, for bin packing, deviation towards larger remaining capacity is less bad\n        # than deviation towards smaller capacity (which means item doesn't fit).\n\n        # Let's redefine the goal: prioritize bins where `bins_remain_cap` is \"just enough\" for the item.\n        # This implies a preference for smaller remaining capacities among those that can fit the item.\n        # The \"First Fit Decreasing\" strategy (though this is online) suggests fitting into the first bin that works.\n        # The \"Best Fit\" strategy suggests fitting into the bin with the smallest remaining capacity that can still fit the item.\n        # Our Sigmoid Fit Score should aim for something like \"Best Fit\" but smoothed.\n\n        # Let's apply a sigmoid to the inverse of the slack `1 / (bins_remain_cap - item + epsilon)`.\n        # Or, to the 'tightness' `item / bins_remain_cap` for bins that fit.\n\n        # Let's use the `bins_remain_cap - item` as the argument to the sigmoid,\n        # but scale and shift it to put the \"sweet spot\" for the sigmoid's steepest\n        # part around zero difference.\n\n        # `sigmoid(x) = 1 / (1 + exp(-x))`\n        # If we use `x = bins_remain_cap - item`, then for perfect fit x=0, sigmoid is 0.5.\n        # For larger remaining capacity (x > 0), sigmoid increases towards 1.\n        # For smaller remaining capacity (x < 0), sigmoid decreases towards 0.\n        # This doesn't quite capture \"best fit\" preference.\n\n        # Let's try `sigmoid(- (bins_remain_cap - item)) = sigmoid(item - bins_remain_cap)`.\n        # Argument is `item - bins_remain_cap`.\n        # If `bins_remain_cap == item`, arg is 0, sigmoid is 0.5.\n        # If `bins_remain_cap > item`, arg is negative, sigmoid < 0.5. (Less preferred as it wastes more space).\n        # If `bins_remain_cap < item`, arg is positive, sigmoid > 0.5. (This would be preferred, which is wrong if item doesn't fit).\n\n        # We must ensure that items only go into bins where they fit.\n        # We can achieve this by multiplying the sigmoid score by a 'fit' indicator (1 if fits, 0 if not).\n\n        # Let's use the sigmoid to represent how \"close\" the remaining capacity is to the item size,\n        # while ensuring the item fits.\n        # We want the score to be high when `bins_remain_cap` is small positive, and closer to `item`.\n        # This is the \"Best Fit\" criterion.\n\n        # Consider a function that peaks at `bins_remain_cap = item`.\n        # Let's define `f(cap) = 1 / (1 + exp(k * (cap - item)))`.\n        # This function decreases as `cap` increases from `item`.\n        # If `cap = item`, `f(item) = 0.5`.\n        # If `cap > item`, `f(cap) < 0.5`. (Less preferred for wasted space)\n        # If `cap < item`, `f(cap) > 0.5`. (This is bad if the item doesn't fit).\n\n        # Let's refine:\n        # Score for bins where `bins_remain_cap >= item`.\n        # Among these, we want to maximize the score when `bins_remain_cap` is minimized.\n        # So, we want a function that decreases as `bins_remain_cap` increases.\n\n        # A common transformation for 'best fit' is `bins_remain_cap - item`.\n        # We want to minimize this difference.\n        # To use a sigmoid, let's map this difference.\n        # Let `score_component = item - bins_remain_cap`.\n        # This is `0` for perfect fit, positive for more space, negative for less space.\n        # `sigmoid(score_component)`:\n        # `bins_remain_cap = item` -> `item - item = 0` -> `sigmoid(0) = 0.5`\n        # `bins_remain_cap > item` -> `item - bins_remain_cap < 0` -> `sigmoid(<0) < 0.5` (lower priority)\n        # `bins_remain_cap < item` -> `item - bins_remain_cap > 0` -> `sigmoid(>0) > 0.5` (higher priority)\n\n        # This still prioritizes bins where the item *doesn't* fit if we use it naively.\n        # The \"Sigmoid Fit Score\" usually implies finding a \"good fit\" using a sigmoid curve.\n        # The key is often a normalized difference.\n\n        # Let's re-interpret \"Sigmoid Fit Score\" as a measure of how well the item *fits* into the bin.\n        # A good fit means the remaining capacity is just slightly larger than the item.\n        # Let `diff = bins_remain_cap - item`.\n        # If `diff < 0`, the item doesn't fit. Priority is 0.\n        # If `diff == 0`, perfect fit. Max priority.\n        # If `diff > 0` and small, high priority.\n        # If `diff > 0` and large, lower priority (waste of space).\n\n        # We can model this with a sigmoid that is peaked.\n        # A bell-shaped curve can be approximated by `sigmoid(x) * (1 - sigmoid(x))`.\n        # Or a Gaussian. But the request is \"Sigmoid Fit Score\".\n\n        # Let's use `sigmoid(k * (item - bins_remain_cap))` and filter for valid bins.\n        # This function is higher when `bins_remain_cap` is smaller (and less than item).\n        # This is the opposite of what we want for \"Best Fit\".\n\n        # A better candidate for \"Best Fit\" using sigmoid:\n        # We want high score when `bins_remain_cap - item` is small and non-negative.\n        # Let `x = bins_remain_cap - item`.\n        # We want `g(x)` to be high for `x` close to 0 (and `x>=0`).\n        # Consider the sigmoid `sigmoid(alpha * (item - cap))`.\n        # For `cap >= item`, `item - cap <= 0`.\n        # `sigmoid(alpha * (item - cap))` will be >= 0.5.\n        # As `cap` increases (more slack), `item - cap` becomes more negative,\n        # and `sigmoid` approaches 0. This gives lower priority for more slack, which is good for \"Best Fit\".\n\n        # Let's use `sigmoid(k * (item - bins_remain_cap))`.\n        # `k` controls how sensitive the score is to the difference.\n        # Let's scale the `item - bins_remain_cap` to map it effectively to the sigmoid's useful range.\n        # If we expect `bins_remain_cap - item` to range from 0 to, say, `bin_capacity`,\n        # then `item - bins_remain_cap` ranges from 0 to `-bin_capacity`.\n        # The sigmoid is sensitive around 0. We want our \"best fit\" to be near 0.\n\n        # Let's normalize the difference by the item size or bin capacity, perhaps.\n        # `scaled_diff = (item - bins_remain_cap) / item` (if item > 0)\n        # `scaled_diff = (item - bins_remain_cap) / max(1, item)`\n        # `priorities[can_fit_mask] = 1 / (1 + np.exp(-scaled_diff[can_fit_mask]))`\n\n        # Let's simplify: The idea is to prioritize bins that have remaining capacity\n        # closest to the item size, provided they are large enough.\n        # `Best Fit` heuristic: Select the bin with the minimum `bins_remain_cap` such that `bins_remain_cap >= item`.\n\n        # Sigmoid Fit Score: model this preference using a sigmoid.\n        # A sigmoid typically maps to [0, 1].\n        # Let's aim for:\n        # - High priority for bins where `bins_remain_cap` is slightly larger than `item`.\n        # - Lower priority for bins where `bins_remain_cap` is much larger than `item`.\n        # - Zero priority for bins where `bins_remain_cap < item`.\n\n        # Consider `f(cap) = sigmoid(a * (b - cap))`.\n        # We want the \"sweet spot\" of the sigmoid (around `cap = b`) to align with `item`.\n        # Let's choose `b = item`.\n        # `f(cap) = sigmoid(a * (item - cap))`.\n        # If `cap = item`, `f(item) = sigmoid(0) = 0.5`.\n        # If `cap > item`, `item - cap < 0`, `f(cap) < 0.5`.\n        # If `cap < item`, `item - cap > 0`, `f(cap) > 0.5`.\n\n        # To ensure items don't go into bins where they don't fit (where `cap < item`),\n        # we can set their sigmoid score to a very low value, or simply 0.\n        # And for bins where `cap >= item`, we want the score to decrease as `cap` increases.\n        # This means `f(cap) = sigmoid(a * (item - cap))` works if `a > 0`.\n\n        # Let `a = 1.0` (a default sensitivity).\n        # We need to handle potential numerical issues with `exp`.\n\n        # Calculate `item - bins_remain_cap` for fitting bins.\n        # `arg = item - bins_remain_cap[can_fit_mask]`\n        # `priorities[can_fit_mask] = 1 / (1 + np.exp(-arg))`\n\n        # To make it more \"Sigmoid Fit Score\" and capture the \"best fit\" idea,\n        # let's normalize `item - bins_remain_cap` to control the steepness.\n        # A common scaling factor is related to the typical range of differences.\n        # If `bins_remain_cap` can vary significantly, a fixed `a` might be too steep or too shallow.\n\n        # Let's assume `item` is positive and `bins_remain_cap` are non-negative.\n        # For bins where `bins_remain_cap >= item`:\n        # Let `relative_slack = (bins_remain_cap - item) / item` (if item > 0) or `bins_remain_cap / max(1, item)`.\n        # We want smaller `relative_slack` to give higher scores.\n        # So, `sigmoid(k * (-relative_slack))` might be a good candidate.\n        # `k` is a sensitivity parameter.\n\n        # A simpler interpretation of \"Sigmoid Fit Score\" often involves using the\n        # ratio `item / bins_remain_cap`. This ratio is close to 1 for good fits.\n        # However, this doesn't handle the \"item does not fit\" case gracefully unless filtered.\n\n        # Let's try using the normalized difference scaled by `item`:\n        # `diff_from_item = bins_remain_cap[can_fit_mask] - item`\n        # `normalized_diff = diff_from_item / item` (if item > 0)\n        # A good fit means `normalized_diff` is close to 0.\n        # `sigmoid(-k * normalized_diff)` or `sigmoid(k * (item - bins_remain_cap) / item)`\n        # Let `k=1.0` for simplicity.\n        # `priorities[can_fit_mask] = 1 / (1 + np.exp(- (item - bins_remain_cap[can_fit_mask]) / max(1.0, item)))`\n        # Adding `max(1.0, item)` to denominator prevents division by zero if item is 0 and makes scaling reasonable if item is very small.\n\n        # Let's simplify the argument to avoid division by item, which can be zero.\n        # Use `item - bins_remain_cap` directly as the argument.\n        # To make the sigmoid sensitive around the \"perfect fit\" point (`bins_remain_cap = item`),\n        # we need to scale the argument.\n        # Consider a standard sigmoid transformation: `1 / (1 + exp(-x))`.\n        # We want `x = item - bins_remain_cap` to be around 0 for a good fit.\n        # If `bins_remain_cap` can be very large, `item - bins_remain_cap` can be a large negative number.\n        # If `bins_remain_cap` is just slightly larger than `item`, `item - bins_remain_cap` is a small negative number.\n\n        # Let's try mapping `bins_remain_cap` such that `item` maps to the center (0.5),\n        # values slightly larger than `item` map to values less than 0.5 (but still positive),\n        # and values much larger than `item` map to values close to 0.\n        # And for `bins_remain_cap < item`, the score is 0.\n\n        # `sigmoid(k * (item - bins_remain_cap))` seems to be the most direct interpretation for \"Sigmoid Fit Score\" targeting \"Best Fit\".\n        # The sensitivity `k` is crucial. Let's set it to a reasonable value.\n        # `k = 1.0` is a common starting point.\n        # To avoid potential issues where `item - bins_remain_cap` is extremely large negative or positive,\n        # we can clip the argument to the sigmoid, or use a scaled version.\n        # Let's scale by `item` if `item > 0`. If `item` is 0, `item - bins_remain_cap` is `-bins_remain_cap`.\n\n        # A common heuristic for \"best fit\" using a smoothed approach involves penalizing slack.\n        # The priority for a bin `i` could be proportional to `sigmoid(C - (bins_remain_cap[i] - item))`,\n        # where `C` is a constant. If `bins_remain_cap[i] - item` is small, the argument is large.\n\n        # Let's refine the sigmoid function to ensure the output is reasonable.\n        # We are applying it to the difference `item - bins_remain_cap`.\n        # Let `val = item - bins_remain_cap[can_fit_mask]`.\n        # We want high scores when `val` is close to 0 or slightly negative (meaning `bins_remain_cap` is slightly larger than `item`).\n        # `sigmoid(val)`:\n        # If `val = 0` (`bins_remain_cap = item`), score = 0.5.\n        # If `val < 0` (`bins_remain_cap > item`), score < 0.5.\n        # If `val > 0` (`bins_remain_cap < item`), score > 0.5.\n\n        # To align with \"Best Fit\" (preferring minimum sufficient capacity):\n        # We want score to be high when `bins_remain_cap` is close to `item` from above.\n        # `f(cap) = 1 / (1 + exp(k * (cap - item)))`\n        # If `cap = item`, `f(item) = 0.5`.\n        # If `cap > item`, `cap - item > 0`, `f(cap) < 0.5`. This score decreases as slack increases. Good.\n        # If `cap < item`, `cap - item < 0`, `f(cap) > 0.5`. This implies preference for bins that are too small, which is wrong.\n\n        # Let's combine the \"fit\" check and the score calculation.\n        # For bins where `bins_remain_cap < item`, priority is 0.\n        # For bins where `bins_remain_cap >= item`:\n        # We want to prioritize smaller `bins_remain_cap`.\n        # Let's use `bins_remain_cap - item` as the input to a function that decays.\n        # The sigmoid `1 / (1 + exp(-x))` increases with `x`.\n        # So we need an input that increases as `bins_remain_cap` decreases.\n        # Let the input be `item - bins_remain_cap`. This is still problematic if `bins_remain_cap < item`.\n\n        # Final approach: Use the standard sigmoid form `1 / (1 + exp(-x))`.\n        # Let `x = k * (target_value - actual_value)`.\n        # Our `actual_value` is `bins_remain_cap`.\n        # Our `target_value` for a perfect fit is `item`.\n        # So, `x = k * (item - bins_remain_cap)`.\n        # This provides:\n        # - `x > 0` when `bins_remain_cap < item` (high score, bad if they don't fit)\n        # - `x = 0` when `bins_remain_cap = item` (score = 0.5)\n        # - `x < 0` when `bins_remain_cap > item` (score < 0.5, lower as bins_remain_cap increases)\n\n        # To make this \"Sigmoid Fit Score\" and correctly handle non-fitting bins:\n        # 1. Only calculate scores for bins where `bins_remain_cap >= item`.\n        # 2. For these bins, use a sigmoid that penalizes excess capacity.\n        # The function `1 / (1 + exp(k * (bins_remain_cap - item)))` does this.\n        # Here, if `bins_remain_cap = item`, arg = 0, score = 0.5.\n        # If `bins_remain_cap > item`, arg > 0, score < 0.5. Score decreases as `bins_remain_cap` increases.\n        # If `bins_remain_cap` is slightly larger than `item`, score is slightly less than 0.5.\n        # If `bins_remain_cap` is much larger than `item`, score is close to 0.\n\n        # The sensitivity `k` determines how quickly the priority drops for bins with excess capacity.\n        # Let's choose `k=1.0` as a default.\n\n        # Compute the argument for the sigmoid.\n        # We only care about bins where `bins_remain_cap >= item`.\n        # Let `diff_slack = bins_remain_cap[can_fit_mask] - item`\n        # We want a function of `diff_slack` that is high for small `diff_slack`.\n        # Sigmoid of `-diff_slack` could work.\n        # `sigmoid(-diff_slack)` = `1 / (1 + exp(diff_slack))`.\n\n        # Let's try to map `bins_remain_cap` to a score where the peak is at `item`.\n        # Use `bins_remain_cap` directly in a sigmoid but shifted and scaled.\n        # Let `k` be a sensitivity parameter.\n        # `priorities[can_fit_mask] = sigmoid(k * (item - bins_remain_cap[can_fit_mask]))`\n        # This means for `bins_remain_cap = item`, we get 0.5.\n        # For `bins_remain_cap > item` (more slack), we get less than 0.5.\n        # For `bins_remain_cap < item` (doesn't fit), we get more than 0.5. This is the issue.\n\n        # Let's combine the best-fit idea with the sigmoid.\n        # For bins where `bins_remain_cap >= item`:\n        # Priority is inversely related to the remaining capacity.\n        # `priorities[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-6)` - Not sigmoid.\n\n        # Final decision for `priority_v2`:\n        # Apply sigmoid to the `bins_remain_cap - item` difference for bins that fit.\n        # We want higher scores for smaller differences.\n        # Sigmoid of `-(bins_remain_cap - item)` or `item - bins_remain_cap`.\n        # To make this effective for \"Best Fit\", we need to ensure that:\n        # 1. Items only go into bins they fit in.\n        # 2. Among fitting bins, smaller remaining capacity is preferred.\n\n        # Let's use `score = 1 / (1 + exp(k * (bins_remain_cap - item)))`\n        # Where `k` is a positive sensitivity parameter.\n        # If `bins_remain_cap < item`: The expression `bins_remain_cap - item` is negative.\n        # `exp(negative)` is small. `1 + small` is close to 1. `1 / (1 + small)` is close to 1.\n        # This means bins that are too small get high priority, which is incorrect.\n\n        # A better formulation for best fit using sigmoid:\n        # Consider the ratio of item size to remaining capacity: `item / bins_remain_cap`.\n        # For perfect fit, this is 1. For more capacity, this is < 1. For less capacity, this is > 1.\n        # We want values close to 1, but also handle the \"doesn't fit\" case.\n        # Let `ratios = item / bins_remain_cap`.\n        # We are interested in `ratios` near 1, for cases where `bins_remain_cap >= item`.\n        # Let `score = 1 / (1 + exp(-k * (ratios - 1)))`.\n        # If `bins_remain_cap >= item`:\n        #   `ratios <= 1`.\n        #   `ratios - 1 <= 0`.\n        #   If `ratios = 1` (perfect fit), `exp(0)=1`, score = 0.5.\n        #   If `ratios < 1` (more capacity), `ratios - 1 < 0`, `exp(<0) < 1`, score > 0.5.\n        #   This prioritizes bins with *more* capacity, opposite of best fit.\n\n        # Let's try `score = 1 / (1 + exp(k * (ratios - 1)))`\n        # If `bins_remain_cap >= item`:\n        #   `ratios <= 1`.\n        #   `ratios - 1 <= 0`.\n        #   If `ratios = 1` (perfect fit), `exp(0)=1`, score = 0.5.\n        #   If `ratios < 1` (more capacity), `ratios - 1 < 0`, `exp(<0) < 1`, score > 0.5. Still problematic.\n\n        # The problem is that sigmoid increases. We need something that decreases.\n        # Let's use the inverse: `1 / (1 + exp(-k * (1 - ratios)))` = `1 / (1 + exp(k * (ratios - 1)))`.\n        # This is the same function, still doesn't capture best fit.\n\n        # The key is often to transform the variable to be something that, when put into a standard sigmoid,\n        # results in the desired priority.\n        # For \"Best Fit\", we want to minimize `bins_remain_cap - item` for `bins_remain_cap >= item`.\n        # Let `slack = bins_remain_cap - item`. We want to minimize `slack`.\n        # Let `k` be a sensitivity parameter.\n        # Sigmoid of `-k * slack`.\n        # `sigmoid(-k * slack)` = `1 / (1 + exp(k * slack))`\n        # If `bins_remain_cap >= item`, then `slack >= 0`.\n        #   If `slack = 0` (perfect fit), `exp(0)=1`, score = 0.5.\n        #   If `slack > 0` (more capacity), `exp(>0) > 1`, score < 0.5. Score decreases as slack increases. This is good for Best Fit.\n        #   If `slack` is very large, score approaches 0.\n\n        # This seems like the most suitable interpretation for \"Sigmoid Fit Score\" for \"Best Fit\" strategy.\n        # We apply this only to bins that can fit the item.\n\n        sensitivity = 1.0  # Controls how sharp the priority drop is for slack\n\n        # Calculate slack for fitting bins\n        slack = bins_remain_cap[can_fit_mask] - item\n\n        # Calculate priority using sigmoid: 1 / (1 + exp(sensitivity * slack))\n        # This means:\n        # - Perfect fit (slack=0): priority = 0.5\n        # - Positive slack (bins_remain_cap > item): priority < 0.5, decreasing with slack\n        # - Negative slack (item doesn't fit): This case is excluded by `can_fit_mask`.\n        #\n        # We need to handle cases where `bins_remain_cap - item` can be very large or very small.\n        # If `slack` is very large positive, `exp(sensitivity * slack)` becomes very large, score ~ 0.\n        # If `slack` is very large negative (e.g., item almost fits, but `bins_remain_cap` is slightly larger than `item`),\n        # `exp(sensitivity * slack)` becomes very small, score ~ 1. This seems counter-intuitive for \"best fit\".\n\n        # Re-thinking the goal: \"Bin with the highest priority score will be selected.\"\n        # For Best Fit, we want the bin with the *least* remaining capacity that still fits the item.\n        # This means we want to maximize a function that is high when `bins_remain_cap` is minimal and `bins_remain_cap >= item`.\n\n        # Let's consider the complementary problem: penalize slack.\n        # The sigmoid `1 / (1 + exp(-x))` increases.\n        # We want to input something that increases as `slack` decreases.\n        # Input `k * (C - slack)`. Let `C = 0`. Input `-k * slack`.\n        # `score = 1 / (1 + exp(-k * slack))`\n        # `slack = bins_remain_cap[can_fit_mask] - item`\n        # If `slack = 0` (perfect fit): score = 0.5\n        # If `slack > 0` (more slack): `-k * slack < 0`. `exp(<0) < 1`. `score > 0.5`.\n        # This prioritizes bins with MORE slack. This is NOT \"Best Fit\".\n\n        # The common interpretation of Sigmoid Fit Score in some contexts aims for a balance.\n        # It doesn't strictly adhere to \"Best Fit\" but favors bins that are \"close enough\".\n\n        # Let's use the structure `1 / (1 + exp(k * (value)))` where `value` is engineered.\n        # We want to prioritize `bins_remain_cap` close to `item`.\n        # Let's transform `bins_remain_cap` by subtracting `item`.\n        # For `bins_remain_cap >= item`, `bins_remain_cap - item` is `slack >= 0`.\n        # We want a score that is high for small `slack`.\n        # `score = 1 / (1 + exp(k * (bins_remain_cap - item)))` with `k>0`\n        # If `bins_remain_cap = item`, score = 0.5.\n        # If `bins_remain_cap > item`, `bins_remain_cap - item > 0`, score < 0.5.\n        # This score decreases as remaining capacity increases. This favors \"Best Fit\".\n\n        # Let's consider the range of `bins_remain_cap - item`.\n        # If `item` is 10, and `bins_remain_cap` can be 10, 11, 15, 20.\n        # Differences: 0, 1, 5, 10.\n        # Sigmoid of `-k * diff`:\n        # k=1:\n        # diff=0: sigmoid(0) = 0.5\n        # diff=1: sigmoid(-1) = 0.2689\n        # diff=5: sigmoid(-5) = 0.0067\n        # diff=10: sigmoid(-10) = 0.000045\n\n        # This looks good. It strongly penalizes bins with significant excess capacity.\n        # We need to select `k` appropriately. A larger `k` means a sharper drop.\n        # A value of `k=1.0` seems reasonable as a starting point.\n\n        # Calculate argument for sigmoid: `sensitivity * (item - bins_remain_cap)`\n        # For bins that can fit, `item - bins_remain_cap` is non-positive.\n        arg_values = sensitivity * (item - bins_remain_cap[can_fit_mask])\n\n        # Apply sigmoid function. `1 / (1 + exp(-x))`.\n        # Where `x = arg_values`.\n        # The formula is `1 / (1 + np.exp(-arg_values))`\n        # This is equivalent to `1 / (1 + np.exp( -sensitivity * (item - bins_remain_cap[can_fit_mask]) ))`\n        # which simplifies to `1 / (1 + np.exp( sensitivity * (bins_remain_cap[can_fit_mask] - item) ))`.\n        # This is indeed the function that decreases as slack (`bins_remain_cap - item`) increases.\n\n        # Ensure stability for `np.exp`. Arguments to `np.exp` should not be extremely large positive or negative.\n        # `bins_remain_cap - item` could be large.\n        # If `bins_remain_cap - item` is very large positive, `exp` could overflow.\n        # If `bins_remain_cap - item` is very large negative, `exp` can underflow to 0.\n\n        # Let's cap the exponent to prevent overflow.\n        # The typical range for `exp(x)` is about -700 to 700.\n        # If `sensitivity * (bins_remain_cap[can_fit_mask] - item)` is `z`.\n        # We want `exp(z)` to be calculated.\n        # If `z` is very large positive, `exp(z)` will overflow. This happens when `bins_remain_cap` is much larger than `item`.\n        # In this case, the score should approach 0.\n        # If `z` is very large negative, `exp(z)` is ~0. This happens when `bins_remain_cap` is slightly larger than `item`.\n        # In this case, the score approaches 1. This is still problematic.\n\n        # The issue is that `1 / (1 + exp(large_positive))` is close to 0.\n        # And `1 / (1 + exp(large_negative))` is close to 1.\n\n        # Let's re-evaluate `score = 1 / (1 + exp(k * (bins_remain_cap - item)))`.\n        # `k=1`, `item=10`.\n        # `cap=10`: slack=0, score = 1/(1+exp(0)) = 0.5\n        # `cap=11`: slack=1, score = 1/(1+exp(1)) = 1/(1+2.718) = 0.2689\n        # `cap=15`: slack=5, score = 1/(1+exp(5)) = 1/(1+148.4) = 0.0067\n        # `cap=20`: slack=10, score = 1/(1+exp(10)) = 1/(1+22026) = 0.000045\n        # This is prioritizing smaller slack, which is Best Fit.\n\n        # The problematic case is when `bins_remain_cap - item` becomes very negative.\n        # Example: item=100, bin_cap=10. `bins_remain_cap=10`.\n        # `bins_remain_cap - item` = -90.\n        # `score = 1 / (1 + exp(k * -90))` = `1 / (1 + exp(-90))`. `exp(-90)` is extremely small.\n        # Score becomes ~1. This implies a very small bin (that doesn't fit the item) gets high priority if we didn't filter.\n\n        # Since we filter `can_fit_mask`, we only apply this to `bins_remain_cap >= item`.\n        # So `bins_remain_cap - item >= 0`.\n        # `k * (bins_remain_cap - item)` will always be non-negative if `k > 0`.\n        # So `exp` will be `>= 1`.\n        # The problem of `exp` overflowing can happen if `k * (bins_remain_cap - item)` is very large.\n        # If `k * slack > 700`, `exp` might overflow.\n        # For example, if `k=10` and `slack=71`, `exp(710)` overflows.\n        # If `k=1` and `slack=701`, `exp(701)` overflows.\n        # The score should approach 0 in these cases.\n\n        # We can cap the argument to `np.exp`.\n        # Let `exponent_arg = sensitivity * slack`.\n        # `capped_exp_arg = np.clip(exponent_arg, -700, 700)` (adjust range as needed for robustness)\n        # But we only care about `slack >= 0`. So `exponent_arg >= 0`.\n        # We only need to worry about large positive `exponent_arg` causing overflow.\n        # If `exponent_arg` is very large, `exp(exponent_arg)` becomes effectively infinity, and the score becomes 0.\n        # This is desired.\n\n        # Let's cap the argument at a value that ensures `exp` doesn't overflow but stays large.\n        # `max_exp_arg = 100` (or some suitable value).\n        # If `exponent_arg > max_exp_arg`, we can treat `exp(exponent_arg)` as infinity.\n\n        capped_slack_term = sensitivity * slack\n        # If `bins_remain_cap` is very large, `slack` is large, `capped_slack_term` is large.\n        # `np.exp(large_positive)` -> overflow. Result is inf.\n        # `1 / (1 + inf)` -> 0. This is the correct behavior.\n        # So no explicit capping might be needed for overflow if `np.inf` is handled correctly.\n\n        # To avoid `np.inf` in the denominator: `1 + np.inf` is `np.inf`.\n        # `1 / np.inf` is `0`. This is fine.\n\n        # Let's explicitly set priority to 0 for cases where `slack` is extremely large,\n        # to avoid potential `inf` calculations and ensure behavior.\n        # If `bins_remain_cap - item` is >, say, 1000 (if sensitivity is 1), it's a very bad fit.\n        # Let's clip `bins_remain_cap - item` to a maximum value before multiplying by sensitivity.\n        clipped_slack = np.clip(slack, 0, 1000.0) # Cap slack at 1000.0\n\n        exponent_argument = sensitivity * clipped_slack\n        # Ensure exponent_argument is not excessively large.\n        # We are calculating `1 / (1 + exp(x))` where `x >= 0`.\n        # `x` can be large. If `x=709`, `exp(x)` is near max float.\n        # If `x > 709`, `exp(x)` might be `inf`.\n        # A simple way to manage is to use `np.finfo(float).max` for `exp` argument.\n        # Or if `exponent_argument` is large, the result is effectively 0.\n\n        # Let's simplify and rely on numpy's handling of large exponents.\n        # If `exponent_argument` leads to `inf` in `exp`, the result is `1 / (1 + inf) = 0`.\n\n        # The `priority_v2` is a Sigmoid Best Fit score.\n        # It prioritizes bins with remaining capacity closest to the item size,\n        # preferring minimal waste.\n        # The formula: `1 / (1 + exp(sensitivity * (bins_remain_cap - item)))`\n        # applied to bins where `bins_remain_cap >= item`.\n\n        priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(sensitivity * slack))\n\n        # To ensure that bins with large remaining capacity (large slack) get very low priority,\n        # and to potentially avoid `inf` in `np.exp`, we can clip the argument.\n        # If `slack` is very large, say `> 100` and `sensitivity=1`, `exp(100)` is huge, score is ~0.\n        # If `bins_remain_cap` is large, like the bin capacity, and item is small, slack can be large.\n        # E.g., item=1, bin_cap=100. slack=99. exp(99) is huge.\n        # The score should indeed be near 0.\n\n        # Let's re-evaluate sensitivity. If `sensitivity = 0.1`, the drop is slower.\n        # if `sensitivity = 10`, the drop is very fast.\n        # `k=1` is a reasonable balance.\n\n    return priorities",
    "response_id": 26,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response23.txt_stdout.txt",
    "code_path": "problem_iter1_code23.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit.\n\n    Almost Full Fit prioritizes bins that are close to full but can still accommodate the item.\n    This heuristic aims to leave more space in other bins for potentially larger items later.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We want to prioritize bins that have just enough space for the item,\n    # but not too much extra space.\n    # A high priority should be given to bins where bins_remain_cap - item is small and non-negative.\n    # We can achieve this by considering the negative of (bins_remain_cap - item)\n    # or equivalently, item - bins_remain_cap.\n\n    # Initialize priorities to a very low value (negative infinity)\n    # to indicate that bins that cannot fit the item have zero priority.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the priority.\n    # A higher priority is given when the remaining capacity is just enough or slightly more.\n    # This means we want to minimize (bins_remain_cap - item).\n    # A good score would be the negative of this difference.\n    # So, score = -(bins_remain_cap - item) = item - bins_remain_cap.\n    # A bin that fits the item with very little leftover space (bins_remain_cap - item close to 0)\n    # will have a priority close to 0.\n    # A bin that fits the item with a lot of leftover space (bins_remain_cap - item large)\n    # will have a large negative priority.\n\n    priorities[can_fit_mask] = item - bins_remain_cap[can_fit_mask]\n\n    # It can also be beneficial to penalize bins that are already very full.\n    # However, the core idea of \"Almost Full Fit\" is to select the tightest fit.\n    # The calculation `item - bins_remain_cap` already favors tighter fits.\n    # If we have multiple bins with the same `item - bins_remain_cap` value,\n    # the current implementation will pick the first one encountered in the array.\n    # If we wanted to break ties by picking the bin that is *most* full,\n    # we could add a small positive term proportional to `bins_remain_cap`\n    # but only for those that fit.\n    # For simplicity and adhering to the \"almost full\" idea, the `item - bins_remain_cap`\n    # on fitting bins is a strong indicator.\n\n    # Let's refine this. We want bins where `bins_remain_cap - item` is minimal and >= 0.\n    # The difference `bins_remain_cap - item` represents the \"slack\" or \"wasted space\"\n    # after placing the item. We want to minimize this slack.\n    # So, we can set priority to `- (bins_remain_cap - item)` for fitting bins.\n    # `priorities = -(bins_remain_cap - item)` => `priorities = item - bins_remain_cap`\n\n    # To ensure that bins that *just* fit are preferred over bins that fit with\n    # a lot of remaining capacity, we want `item - bins_remain_cap` to be as close to 0\n    # as possible from below.\n\n    # Let's consider a slightly different approach to emphasize \"almost full\" more directly.\n    # We can use the proportion of space used by the item in the bin if it were to fit.\n    # `item / bins_remain_cap` for bins where `bins_remain_cap >= item`.\n    # However, this doesn't directly align with \"almost full\" in terms of *remaining* capacity.\n\n    # Let's go back to minimizing the leftover space: `bins_remain_cap - item`.\n    # The negative of this is `item - bins_remain_cap`.\n    # A bin with remaining capacity `R` will have a score `item - R`.\n    # If `R = item`, score = 0.\n    # If `R = item + 1`, score = -1.\n    # If `R = item + 10`, score = -10.\n    # This means bins that are *more* full (smaller R) get higher scores.\n\n    # An alternative interpretation of \"Almost Full Fit\" might be to prioritize\n    # bins that have a remaining capacity that is slightly larger than the item.\n    # For example, if the item size is `s`, we might prefer bins with remaining capacity\n    # `R` such that `R` is small and `R >= s`.\n    # The difference `R - s` is the slack. Minimizing `R - s` is the goal.\n    # So, maximizing `s - R` is also a goal.\n\n    # Let's set priorities for bins that can fit the item.\n    # The priority is `item - bins_remain_cap`.\n    # For bins that don't fit, the priority is `-inf`.\n    # This means the highest priority will be for the bin with the smallest non-negative `bins_remain_cap - item`.\n\n    priorities_for_fitting = item - bins_remain_cap[can_fit_mask]\n\n    # Let's try to make the priority directly represent \"how close to fitting it is\".\n    # For bins that can fit, we want them to be as \"tight\" as possible.\n    # The tightest fit means `bins_remain_cap` is closest to `item`.\n    # So, we want to maximize `item / bins_remain_cap` for bins that can fit,\n    # assuming `bins_remain_cap` is the total capacity.\n    # But here `bins_remain_cap` is the *remaining* capacity.\n\n    # Consider the `bins_remain_cap` that are slightly larger than `item`.\n    # For these bins, `bins_remain_cap` could be `item + epsilon`.\n    # We want to maximize the priority for smaller `epsilon`.\n    # So, let's prioritize based on `item / bins_remain_cap` IF the bins are not too empty.\n\n    # Let's stick to the interpretation: Minimize the leftover space.\n    # `bins_remain_cap - item` should be minimized for `bins_remain_cap >= item`.\n    # Maximizing `item - bins_remain_cap` achieves this.\n    # The highest score will be 0 for a perfect fit, and increasingly negative for more slack.\n\n    # Another angle: How \"full\" is the bin if we put the item in?\n    # This relates to the *original* capacity, which we don't have here.\n    # So we work with remaining capacity.\n\n    # What if we define \"almost full\" as `bins_remain_cap` being in a certain range?\n    # E.g., `item <= bins_remain_cap < item + margin`.\n    # Within this range, we pick the smallest `bins_remain_cap`.\n    # This is equivalent to maximizing `item - bins_remain_cap`.\n\n    # Consider this: we want to find a bin `j` such that `bins_remain_cap[j] >= item` and\n    # `bins_remain_cap[j]` is minimized.\n    # This means we want to maximize `bins_remain_cap[j]` such that `bins_remain_cap[j] - item` is minimized and non-negative.\n\n    # Let's modify `item - bins_remain_cap` to ensure it's monotonic.\n    # If `bins_remain_cap[j]` is slightly larger than `bins_remain_cap[k]` (and both fit `item`),\n    # then `item - bins_remain_cap[j]` will be smaller than `item - bins_remain_cap[k]`.\n    # This means the bin with less remaining capacity (tighter fit) gets a higher score.\n\n    # A slight tweak to emphasize being \"almost full\" might involve scaling or shifting.\n    # For example, what if we normalize the remaining capacity?\n    # `bins_remain_cap / MAX_CAPACITY`? But we don't have MAX_CAPACITY.\n\n    # Let's try to ensure that bins that are \"too empty\" get lower priority.\n    # For instance, if `bins_remain_cap` is much larger than `item`, maybe the priority should be lower.\n    # Let `slack = bins_remain_cap - item`. We want to minimize `slack`.\n    # Prioritize `-slack` for `slack >= 0`.\n\n    # What if we define priority as `item / bins_remain_cap` for those that fit?\n    # If `bins_remain_cap = item`, score is 1.\n    # If `bins_remain_cap = 2 * item`, score is 0.5.\n    # If `bins_remain_cap = 1.1 * item`, score is `item / (1.1 * item) = 1 / 1.1 \\approx 0.909`.\n    # This prioritizes bins that are closer to being full (smaller `bins_remain_cap`) *if they can fit the item*.\n\n    # Let's combine these: Prioritize bins that fit, and among those, prioritize the ones with less remaining capacity.\n    # This still leads to `item - bins_remain_cap`.\n\n    # Let's consider the *percentage* of remaining capacity that the item would occupy.\n    # For a bin `j` with `bins_remain_cap[j] >= item`:\n    # The item takes up `item / (bin_total_capacity)` of the bin.\n    # This doesn't seem right since we're working with remaining capacity.\n\n    # Let's try a heuristic that penalizes bins that have a lot of remaining space,\n    # but still allows them to be picked if no tighter fit exists.\n    # We are looking for the smallest `bins_remain_cap[j]` such that `bins_remain_cap[j] >= item`.\n\n    # Consider the difference `bins_remain_cap[j] - item`. We want to minimize this.\n    # Let's use a penalty for large positive differences.\n    # The priority could be related to the inverse of `bins_remain_cap[j] - item + delta`, where `delta` is small.\n    # E.g., `1.0 / (bins_remain_cap[j] - item + 1e-9)` for fitting bins.\n    # This would give a high score when `bins_remain_cap[j] - item` is close to 0.\n\n    # Let's refine this to ensure higher priority for smaller remaining capacities:\n    # For bins that can fit the item:\n    # We want to prioritize `bins_remain_cap` that are small.\n    # Let's transform `bins_remain_cap` for fitting bins so that smaller values yield higher priority.\n    # `priority = 1 / (bins_remain_cap + 1)`? No, this doesn't consider the item size.\n\n    # Let's go back to the core idea: find the smallest `bins_remain_cap[j]` where `bins_remain_cap[j] >= item`.\n    # This is equivalent to maximizing `item - bins_remain_cap[j]` for `bins_remain_cap[j] >= item`.\n    # For bins where `bins_remain_cap[j] < item`, their priority should be zero (or negative infinity) as they cannot fit.\n\n    # So, `priorities[can_fit_mask] = item - bins_remain_cap[can_fit_mask]` is a good starting point.\n    # This assigns values like 0, -1, -2, etc.\n    # What if we want to ensure that the \"almost full\" aspect means the remaining capacity isn't excessively large?\n    # For instance, if the bin's remaining capacity is `C`, and item is `I`, we prefer `C` to be `I+epsilon`.\n\n    # Let's consider an alternative that penalizes bins that are \"too empty\" more aggressively.\n    # For bins that can fit:\n    # Priority = `item - bins_remain_cap`  (This favors tighter fits)\n    # Let's add a term that is more sensitive to larger remaining capacities.\n    # Example: `priority = (item - bins_remain_cap) - alpha * (bins_remain_cap - item)` for `bins_remain_cap >= item`.\n    # This doesn't seem right.\n\n    # A simpler approach: For bins that can fit, we want to pick the one that results in the least wasted space.\n    # Wasted space = `bins_remain_cap - item`.\n    # So we want to minimize `bins_remain_cap - item`.\n    # Therefore, we want to maximize `item - bins_remain_cap`.\n\n    # Let's think about the *degree* to which a bin is \"almost full\" given the item.\n    # If a bin has remaining capacity `R`, and item is `I`:\n    # The ratio `I / R` could indicate how much of the *current remaining space* the item would occupy.\n    # If `R` is just slightly larger than `I`, then `I/R` is close to 1.\n    # E.g., `R = I + 0.1`, `I/R = I/(I+0.1) \\approx 1`.\n    # E.g., `R = 2I`, `I/R = I/(2I) = 0.5`.\n    # This metric prioritizes bins where the item takes up a large fraction of the remaining space.\n    # This seems more aligned with \"almost full\".\n\n    # Let's set priorities for fitting bins using `item / bins_remain_cap`.\n    # And for bins that don't fit, set to a very low value.\n\n    priorities_for_fitting_alt = np.zeros_like(bins_remain_cap)\n    can_fit_mask = bins_remain_cap >= item\n\n    # For fitting bins, prioritize those with smaller remaining capacity.\n    # We can use `item / bins_remain_cap`.\n    # This assumes `bins_remain_cap` is not zero (which is guaranteed by `item >= 0` and `bins_remain_cap >= item`).\n    # The closer `bins_remain_cap` is to `item`, the higher the ratio `item / bins_remain_cap`.\n    priorities_for_fitting_alt[can_fit_mask] = item / bins_remain_cap[can_fit_mask]\n\n    # Now, let's refine this. If `item` is 0, this ratio is always 0.\n    # The problem states `item: float`, implying it could be 0. If `item = 0`, any bin can fit it.\n    # If `item = 0`, any bin with `bins_remain_cap >= 0` can fit.\n    # In this case, `item / bins_remain_cap` would be 0 for all fitting bins.\n    # This means if `item=0`, the tie-breaking among fitting bins would be arbitrary (based on index).\n\n    # Let's consider the \"almost full\" aspect directly by prioritizing bins where\n    # `bins_remain_cap` is just enough to fit `item`.\n    # This means `bins_remain_cap - item` should be minimal and non-negative.\n    # Let's use the inverse of `bins_remain_cap - item + epsilon` for fitting bins.\n    # `epsilon` is a small constant to avoid division by zero and to ensure that smaller\n    # (bins_remain_cap - item) values yield higher priorities.\n\n    priorities = np.full_like(bins_remain_cap, -np.inf) # Initialize with a very low priority\n\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the priority.\n    # We want to minimize the \"slack\" or wasted space: `bins_remain_cap - item`.\n    # Let `slack = bins_remain_cap - item`.\n    # A priority of `1 / (slack + small_epsilon)` would be high for small slack.\n    # Using `item / bins_remain_cap` also works because if `R` is small and `R >= I`, then `I/R` is high.\n    # Let's use `item / bins_remain_cap` as it's more direct and avoids introducing an arbitrary epsilon if `item` itself is very small.\n    # The logic is: prioritize bins where the item occupies a larger fraction of the *available remaining space*.\n\n    # To ensure the \"almost full\" idea is emphasized, we might want to boost priorities for bins that are already quite full in absolute terms.\n    # However, without total capacity, this is hard.\n\n    # Let's use the `item / bins_remain_cap` metric.\n    # If `item` is very small (e.g., 0.01) and `bins_remain_cap` is large (e.g., 100), the ratio is tiny.\n    # If `item` is 10 and `bins_remain_cap` is 11, ratio is ~0.9.\n    # If `item` is 10 and `bins_remain_cap` is 100, ratio is 0.1.\n    # This correctly prioritizes the bin with `11` remaining capacity.\n\n    # What if we want to also penalize bins that are \"too full\" already, meaning they have very little remaining capacity *even before* adding the item?\n    # The prompt implies we want to place the item into an \"almost full\" bin, meaning a bin that has a remaining capacity `R` such that `R` is not much larger than `item`.\n\n    # Let's refine the `item / bins_remain_cap` approach.\n    # For bins where `bins_remain_cap < item`, the priority is `-np.inf`.\n    # For bins where `bins_remain_cap >= item`:\n    # The priority is `item / bins_remain_cap`.\n    # This works as intended: a higher ratio means a smaller `bins_remain_cap` relative to `item`, which is the \"almost full\" fit.\n\n    # Let's consider `priority_v1` and how to improve it.\n    # `priority_v1` returns zeros, meaning all bins have equal priority if they can fit.\n    # Our goal is to create a priority based on the \"Almost Full Fit\" strategy.\n\n    # The core idea of \"Almost Full Fit\" (AFF) or \"Best Fit\" (BF) in bin packing is to select the bin\n    # where placing the current item leaves the minimum remaining capacity.\n    # If the current item size is `s`, and the bin remaining capacities are `C_j`:\n    # We want to choose bin `j` such that `C_j >= s` and `C_j - s` is minimized.\n    # This is equivalent to choosing bin `j` such that `C_j >= s` and `C_j` is minimized.\n\n    # So, we should prioritize bins with smaller remaining capacities, provided they can fit the item.\n    # A simple way to achieve this is to use `1 / (bins_remain_cap - item + epsilon)` or simply `item / bins_remain_cap`\n    # for fitting bins.\n    # The `item / bins_remain_cap` metric naturally gives higher values to bins with smaller remaining capacities that fit the item.\n\n    # Let's make sure our implementation reflects this:\n    priorities = np.full_like(bins_remain_cap, -np.inf)  # Initialize with a very low priority\n\n    # Create a mask for bins that have enough remaining capacity for the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, assign a priority.\n    # The priority is designed to favor bins with less remaining capacity (i.e., \"almost full\" bins).\n    # The ratio `item / bins_remain_cap` achieves this:\n    # - If `bins_remain_cap` is just slightly larger than `item`, the ratio is close to 1.\n    # - If `bins_remain_cap` is much larger than `item`, the ratio is closer to 0.\n    # This makes bins with tighter fits have higher priority scores.\n    # We use a small epsilon to avoid division by zero if `bins_remain_cap` could be 0,\n    # but given `bins_remain_cap >= item` and `item` is usually positive, `bins_remain_cap` will be >= 0.\n    # If `item = 0`, `bins_remain_cap >= 0`. If `bins_remain_cap` is 0, `0/0` is undefined.\n    # However, if `item = 0`, placing it requires `bins_remain_cap >= 0`.\n    # If `bins_remain_cap = 0` and `item = 0`, `0/0` is a problem.\n    # Let's handle the `item = 0` case. If item is 0, any bin with `bins_remain_cap >= 0` can take it.\n    # The ratio `0 / bins_remain_cap` is 0.\n    # To distinguish between fitting bins when item is 0, we might need a secondary criterion.\n    # Or, if `item=0`, any bin is equally \"almost full\" in terms of space used by the item.\n\n    # A safe approach for the division:\n    # If `item` is positive and `bins_remain_cap` is positive, `item / bins_remain_cap` is fine.\n    # If `item` is 0:\n    # If `bins_remain_cap > 0`, `0 / bins_remain_cap = 0`.\n    # If `bins_remain_cap = 0` and `item = 0`, this bin can fit the item, but division by zero occurs.\n    # We need to assign a priority in this specific `item=0, bins_remain_cap=0` case.\n    # Such a bin is \"full\" and can barely take a 0-sized item. The ratio is undefined but represents a \"full\" state.\n\n    # Let's define priority for the case `item=0` separately for clarity,\n    # or use a robust calculation.\n    # A bin is \"almost full\" if its remaining capacity is *just enough* for the item.\n    # So, `bins_remain_cap - item` should be minimal.\n    # Let `slack = bins_remain_cap - item`.\n    # Priority = `-slack` for `slack >= 0`. This ensures that minimum slack gets max priority.\n    # If `slack=0`, priority=0. If `slack=1`, priority=-1.\n    # This is `item - bins_remain_cap`.\n\n    # This `item - bins_remain_cap` strategy is often called \"Best Fit Decreasing\" or \"Worst Fit Decreasing\" depending on how you rank them.\n    # For \"Best Fit\", we select the bin that minimizes `bins_remain_cap - item`.\n    # Thus, we want to maximize `item - bins_remain_cap`.\n\n    # Let's implement this:\n    priorities[can_fit_mask] = item - bins_remain_cap[can_fit_mask]\n\n    # This strategy assigns higher scores to bins that are tighter fits.\n    # For example:\n    # item = 5\n    # bins_remain_cap = [10, 7, 12, 5, 8]\n    # can_fit_mask = [True, True, True, True, True]\n    # priorities = [5-10, 5-7, 5-12, 5-5, 5-8]\n    # priorities = [-5, -2, -7, 0, -3]\n    # The highest priority is 0, corresponding to the bin with remaining capacity 5 (a perfect fit).\n    # The next highest is -2, corresponding to the bin with remaining capacity 7.\n    # This seems to be a correct interpretation of \"Best Fit\", which implies \"almost full\".\n\n    # To further emphasize \"almost full\", one might penalize bins that are too empty.\n    # However, the prompt asks for \"Almost Full Fit\", implying a preference for tighter fits among those that can accommodate the item.\n    # The `item - bins_remain_cap` metric effectively does this.\n\n    # Final check on the logic for `item = 0`:\n    # item = 0\n    # bins_remain_cap = [10, 0, 5, -2] (though negative capacity is unlikely in BPP context)\n    # Assume `bins_remain_cap` are always non-negative.\n    # bins_remain_cap = [10, 0, 5]\n    # can_fit_mask = [True, True, True] (assuming 0 can fit in 0 or more)\n    # priorities = [0-10, 0-0, 0-5]\n    # priorities = [-10, 0, -5]\n    # The bin with 0 remaining capacity gets the highest priority (0), followed by 5 (-5), then 10 (-10).\n    # This aligns: the bin with the least remaining capacity gets the highest priority.\n\n    # This approach seems robust and directly addresses the goal of minimizing wasted space for the current item.\n    # The \"Almost Full Fit\" strategy is essentially Best Fit.\n\n    return priorities",
    "response_id": 23,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  }
]