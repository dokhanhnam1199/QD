{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit strategy.\n\n    The Softmax-Based Fit strategy assigns a priority to each bin based on how well an item fits into it,\n    using a softmax function to convert these \"fitness\" scores into probabilities (priorities).\n    A higher score (closer to 1.0) means the bin is a better fit for the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate fitness scores for each bin. A higher score means a better fit.\n    # We consider bins that can fit the item. For bins that cannot fit, assign a very low score.\n    # A simple fitness metric could be the remaining capacity if the item fits,\n    # otherwise, a very small negative number to ensure it's not chosen by softmax.\n    fitness_scores = np.where(bins_remain_cap >= item, bins_remain_cap - item, -np.inf)\n\n    # Apply the softmax function to convert fitness scores into priorities.\n    # Add a small epsilon to avoid issues with all scores being -inf (though unlikely here\n    # as we expect at least one bin to fit in typical scenarios or the problem is ill-posed).\n    # We add 1 to the scores before softmax because the softmax function operates on positive values,\n    # and a direct application of softmax on potentially negative 'fitness_scores' could lead to\n    # numerically unstable results if not handled carefully.\n    # An alternative approach is to shift the scores so they are all non-negative before applying softmax.\n    # However, the core idea of softmax is about relative differences.\n    # Let's try a simpler approach: treat `bins_remain_cap - item` directly as scores\n    # and use `np.exp` for higher values. We want bins with *less* remaining capacity after packing\n    # to be prioritized if they still fit (to achieve fuller bins).\n    # So, a good fit means `bins_remain_cap - item` is close to zero.\n    # Let's define a \"goodness\" score as a decreasing function of `bins_remain_cap - item`.\n    # For example, `- (bins_remain_cap - item)`.\n    # We only consider bins where `bins_remain_cap >= item`.\n\n    # Calculate the difference between remaining capacity and item size.\n    # Bins with smaller positive differences are better fits (closer to zero).\n    diffs = bins_remain_cap - item\n\n    # Create a \"desirability\" score: higher for bins with small positive differences.\n    # If an item doesn't fit, its desirability is very low (large negative number).\n    # We want bins with diffs closer to 0 to have higher desirability.\n    # Let's use -(diffs) for bins that fit, and a very small negative number for those that don't.\n    desirability_scores = np.where(bins_remain_cap >= item, -diffs, -1e9) # Use a large negative number\n\n    # Apply softmax. To ensure positivity for exp, we can shift scores or use a base for exponentiation.\n    # A common trick is to shift all scores by subtracting the maximum score before exponentiating.\n    # This makes the largest score 0, and others negative, which is numerically stable for softmax.\n    # However, we want to directly translate how *good* the fit is into a probability.\n    # Let's re-think the \"fit\" for BPP. We want to put items into bins.\n    # A \"good fit\" means the remaining capacity is *small* after the item is placed,\n    # because this suggests the bin is getting full and we are efficiently using space.\n    # So, `bins_remain_cap - item` should be minimized for bins that can fit.\n\n    # Let's define a score `s_i` for bin `i`:\n    # If bin `i` can fit the item (bins_remain_cap[i] >= item):\n    #   s_i = 1 / (bins_remain_cap[i] - item + epsilon)  -- higher score for smaller remaining capacity. Add epsilon to avoid division by zero.\n    # If bin `i` cannot fit the item:\n    #   s_i = 0 (or a very small value)\n\n    epsilon = 1e-6 # Small constant to avoid division by zero\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Calculate scores only for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    fit_capacities = bins_remain_cap[can_fit_mask]\n    \n    if np.any(can_fit_mask):\n        # Calculate a \"fitness\" score: prioritize bins with less remaining capacity after placing the item.\n        # We want to minimize (bins_remain_cap - item). So, a higher score for smaller difference.\n        # Let's use -(bins_remain_cap - item) as the score, or something that is inversely proportional to the difference.\n        # A common approach in related fields is to use `exp(score)`.\n        # So, we want `exp(-k * (bins_remain_cap[i] - item))` where k is a scaling factor.\n        # For simplicity, let's use `exp(- (bins_remain_cap[i] - item))` as the raw score.\n        # Bins that fit should have a positive score. Bins that don't fit should have a zero or very low score.\n        \n        # A robust way to use softmax for preference:\n        # For each bin i, calculate a \"fit_value_i\".\n        # If bin i can fit the item, fit_value_i = some_positive_measure_of_fit.\n        # If bin i cannot fit, fit_value_i = 0.\n        # Then apply softmax on these fit_values.\n        \n        # Let's consider the remaining capacity after fitting the item. We want this to be small.\n        # So, a good fit corresponds to a small value of `bins_remain_cap[i] - item`.\n        # We can use `1 / (bins_remain_cap[i] - item + epsilon)` as a measure of \"goodness of fit\".\n        # The higher this value, the better the fit.\n        \n        # Let's map the 'remaining capacity after fitting' to a \"desirability\" score.\n        # We want smaller remaining capacity to be more desirable.\n        # Consider `bins_remain_cap[i] - item`.\n        # If this is 0, it's a perfect fit. If it's positive and large, it's a bad fit.\n        \n        # We want to give higher probability to bins where `bins_remain_cap[i] - item` is small and positive.\n        # Let's use `exp(alpha * (bins_remain_cap[i] - item))` as a measure, but this would\n        # prioritize bins with *larger* remaining capacity. We want the opposite.\n        \n        # Let's use `exp(alpha * - (bins_remain_cap[i] - item))` for bins that fit.\n        # `alpha` controls the steepness of the softmax. A higher alpha means\n        # smaller differences in remaining capacity lead to larger differences in probabilities.\n        alpha = 1.0 # Sensitivity parameter\n\n        # Calculate scores for bins that can fit the item\n        # We want to prioritize bins where `bins_remain_cap[i] - item` is close to 0.\n        # So, a score proportional to `1 / (bins_remain_cap[i] - item)` could work,\n        # but we need to make it suitable for softmax (non-negative or shifted).\n        \n        # A simpler approach: `fit_scores = bins_remain_cap[can_fit_mask] - item`.\n        # We want small values of `fit_scores` to be prioritized.\n        # To use softmax, we can use `exp(-alpha * fit_scores)`.\n        \n        fit_scores = fit_capacities - item\n        \n        # Ensure scores are not too large negative if alpha is large\n        # Clip scores to prevent overflow issues if -alpha * fit_scores is very large negative\n        # (though this is less of a concern if we're using softmax on positive values or shifted values)\n        \n        # Let's define a raw score for softmax:\n        # For bins that fit, raw_score = 1.0 / (bins_remain_cap[i] - item + epsilon)\n        # This gives higher scores to bins that are almost full.\n        \n        raw_scores = np.zeros_like(bins_remain_cap)\n        raw_scores[can_fit_mask] = 1.0 / (fit_capacities - item + epsilon)\n        \n        # Apply softmax to these raw_scores\n        # To use np.exp directly, scores must be handled carefully.\n        # A common practice for softmax is to use `exp(score - max_score)`\n        # for numerical stability, which essentially makes the highest score 1.\n        \n        # Let's try a strategy that directly maps a \"good fit\" to a positive value.\n        # Good fit => remaining capacity after packing is small and non-negative.\n        # Value = 1.0 / (remaining_capacity_after_packing + epsilon)\n        \n        # Calculate remaining capacity after placing the item\n        remaining_after_fit = bins_remain_cap - item\n        \n        # Initialize a 'desirability' array. Assign 0 to bins that cannot fit the item.\n        desirability = np.zeros_like(bins_remain_cap)\n        \n        # For bins that can fit, assign a desirability score.\n        # We want to prioritize bins where `remaining_after_fit` is small and positive.\n        # A simple mapping: desirability = 1.0 / (remaining_after_fit + epsilon)\n        # This makes bins that are almost full (small positive remaining_after_fit) more desirable.\n        \n        desirability[can_fit_mask] = 1.0 / (remaining_after_fit[can_fit_mask] + epsilon)\n        \n        # Apply softmax function to the desirability scores.\n        # `np.exp(desirability)` creates exponentials of scores.\n        # Normalizing by the sum of exponentials converts these into probabilities.\n        \n        # To ensure numerical stability for `np.exp`, it's good practice to shift\n        # the scores so that the maximum score is 0.\n        # This is done by subtracting the maximum score from all scores.\n        \n        max_desirability = np.max(desirability)\n        shifted_desirability = desirability - max_desirability\n        \n        # Calculate exponentials of the shifted desirability scores\n        exp_scores = np.exp(shifted_desirability)\n        \n        # Normalize by the sum of the exponentials to get probabilities (priorities)\n        sum_exp_scores = np.sum(exp_scores)\n        \n        # Handle the case where all bins are unable to fit the item (sum_exp_scores would be 0).\n        # In such a case, all priorities should be 0 or some default.\n        if sum_exp_scores == 0:\n            priorities = np.zeros_like(bins_remain_cap)\n        else:\n            priorities = exp_scores / sum_exp_scores\n            \n    else:\n        # If no bin can fit the item, return all zeros\n        priorities = np.zeros_like(bins_remain_cap)\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit.\n\n    The Best Fit strategy aims to place the item into the bin where it fits\n    most snugly, leaving the smallest remaining capacity. This heuristic tries\n    to minimize wasted space in each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher priority means a better fit. Bins that cannot accommodate the\n        item are given a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Calculate the remaining capacity if the item is placed in each bin\n    potential_remaining_capacities = bins_remain_cap - item\n\n    # Only consider bins that have enough capacity for the item\n    # For these bins, the priority is the *negative* of the remaining capacity.\n    # This is because we want to minimize the remaining capacity (best fit).\n    # A larger negative number (smaller remaining capacity) will have a higher\n    # priority score.\n    fit_mask = bins_remain_cap >= item\n    priorities[fit_mask] = -potential_remaining_capacities[fit_mask]\n\n    return priorities\n\n[Reflection]\nPrioritize bins with minimal remaining capacity after placement.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}