{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit strategy.\n\n    The Softmax-Based Fit strategy assigns a priority to each bin based on how well an item fits into it,\n    using a softmax function to convert these \"fitness\" scores into probabilities (priorities).\n    A higher score (closer to 1.0) means the bin is a better fit for the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate fitness scores for each bin. A higher score means a better fit.\n    # We consider bins that can fit the item. For bins that cannot fit, assign a very low score.\n    # A simple fitness metric could be the remaining capacity if the item fits,\n    # otherwise, a very small negative number to ensure it's not chosen by softmax.\n    fitness_scores = np.where(bins_remain_cap >= item, bins_remain_cap - item, -np.inf)\n\n    # Apply the softmax function to convert fitness scores into priorities.\n    # Add a small epsilon to avoid issues with all scores being -inf (though unlikely here\n    # as we expect at least one bin to fit in typical scenarios or the problem is ill-posed).\n    # We add 1 to the scores before softmax because the softmax function operates on positive values,\n    # and a direct application of softmax on potentially negative 'fitness_scores' could lead to\n    # numerically unstable results if not handled carefully.\n    # An alternative approach is to shift the scores so they are all non-negative before applying softmax.\n    # However, the core idea of softmax is about relative differences.\n    # Let's try a simpler approach: treat `bins_remain_cap - item` directly as scores\n    # and use `np.exp` for higher values. We want bins with *less* remaining capacity after packing\n    # to be prioritized if they still fit (to achieve fuller bins).\n    # So, a good fit means `bins_remain_cap - item` is close to zero.\n    # Let's define a \"goodness\" score as a decreasing function of `bins_remain_cap - item`.\n    # For example, `- (bins_remain_cap - item)`.\n    # We only consider bins where `bins_remain_cap >= item`.\n\n    # Calculate the difference between remaining capacity and item size.\n    # Bins with smaller positive differences are better fits (closer to zero).\n    diffs = bins_remain_cap - item\n\n    # Create a \"desirability\" score: higher for bins with small positive differences.\n    # If an item doesn't fit, its desirability is very low (large negative number).\n    # We want bins with diffs closer to 0 to have higher desirability.\n    # Let's use -(diffs) for bins that fit, and a very small negative number for those that don't.\n    desirability_scores = np.where(bins_remain_cap >= item, -diffs, -1e9) # Use a large negative number\n\n    # Apply softmax. To ensure positivity for exp, we can shift scores or use a base for exponentiation.\n    # A common trick is to shift all scores by subtracting the maximum score before exponentiating.\n    # This makes the largest score 0, and others negative, which is numerically stable for softmax.\n    # However, we want to directly translate how *good* the fit is into a probability.\n    # Let's re-think the \"fit\" for BPP. We want to put items into bins.\n    # A \"good fit\" means the remaining capacity is *small* after the item is placed,\n    # because this suggests the bin is getting full and we are efficiently using space.\n    # So, `bins_remain_cap - item` should be minimized for bins that can fit.\n\n    # Let's define a score `s_i` for bin `i`:\n    # If bin `i` can fit the item (bins_remain_cap[i] >= item):\n    #   s_i = 1 / (bins_remain_cap[i] - item + epsilon)  -- higher score for smaller remaining capacity. Add epsilon to avoid division by zero.\n    # If bin `i` cannot fit the item:\n    #   s_i = 0 (or a very small value)\n\n    epsilon = 1e-6 # Small constant to avoid division by zero\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Calculate scores only for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    fit_capacities = bins_remain_cap[can_fit_mask]\n    \n    if np.any(can_fit_mask):\n        # Calculate a \"fitness\" score: prioritize bins with less remaining capacity after placing the item.\n        # We want to minimize (bins_remain_cap - item). So, a higher score for smaller difference.\n        # Let's use -(bins_remain_cap - item) as the score, or something that is inversely proportional to the difference.\n        # A common approach in related fields is to use `exp(score)`.\n        # So, we want `exp(-k * (bins_remain_cap[i] - item))` where k is a scaling factor.\n        # For simplicity, let's use `exp(- (bins_remain_cap[i] - item))` as the raw score.\n        # Bins that fit should have a positive score. Bins that don't fit should have a zero or very low score.\n        \n        # A robust way to use softmax for preference:\n        # For each bin i, calculate a \"fit_value_i\".\n        # If bin i can fit the item, fit_value_i = some_positive_measure_of_fit.\n        # If bin i cannot fit, fit_value_i = 0.\n        # Then apply softmax on these fit_values.\n        \n        # Let's consider the remaining capacity after fitting the item. We want this to be small.\n        # So, a good fit corresponds to a small value of `bins_remain_cap[i] - item`.\n        # We can use `1 / (bins_remain_cap[i] - item + epsilon)` as a measure of \"goodness of fit\".\n        # The higher this value, the better the fit.\n        \n        # Let's map the 'remaining capacity after fitting' to a \"desirability\" score.\n        # We want smaller remaining capacity to be more desirable.\n        # Consider `bins_remain_cap[i] - item`.\n        # If this is 0, it's a perfect fit. If it's positive and large, it's a bad fit.\n        \n        # We want to give higher probability to bins where `bins_remain_cap[i] - item` is small and positive.\n        # Let's use `exp(alpha * (bins_remain_cap[i] - item))` as a measure, but this would\n        # prioritize bins with *larger* remaining capacity. We want the opposite.\n        \n        # Let's use `exp(alpha * - (bins_remain_cap[i] - item))` for bins that fit.\n        # `alpha` controls the steepness of the softmax. A higher alpha means\n        # smaller differences in remaining capacity lead to larger differences in probabilities.\n        alpha = 1.0 # Sensitivity parameter\n\n        # Calculate scores for bins that can fit the item\n        # We want to prioritize bins where `bins_remain_cap[i] - item` is close to 0.\n        # So, a score proportional to `1 / (bins_remain_cap[i] - item)` could work,\n        # but we need to make it suitable for softmax (non-negative or shifted).\n        \n        # A simpler approach: `fit_scores = bins_remain_cap[can_fit_mask] - item`.\n        # We want small values of `fit_scores` to be prioritized.\n        # To use softmax, we can use `exp(-alpha * fit_scores)`.\n        \n        fit_scores = fit_capacities - item\n        \n        # Ensure scores are not too large negative if alpha is large\n        # Clip scores to prevent overflow issues if -alpha * fit_scores is very large negative\n        # (though this is less of a concern if we're using softmax on positive values or shifted values)\n        \n        # Let's define a raw score for softmax:\n        # For bins that fit, raw_score = 1.0 / (bins_remain_cap[i] - item + epsilon)\n        # This gives higher scores to bins that are almost full.\n        \n        raw_scores = np.zeros_like(bins_remain_cap)\n        raw_scores[can_fit_mask] = 1.0 / (fit_capacities - item + epsilon)\n        \n        # Apply softmax to these raw_scores\n        # To use np.exp directly, scores must be handled carefully.\n        # A common practice for softmax is to use `exp(score - max_score)`\n        # for numerical stability, which essentially makes the highest score 1.\n        \n        # Let's try a strategy that directly maps a \"good fit\" to a positive value.\n        # Good fit => remaining capacity after packing is small and non-negative.\n        # Value = 1.0 / (remaining_capacity_after_packing + epsilon)\n        \n        # Calculate remaining capacity after placing the item\n        remaining_after_fit = bins_remain_cap - item\n        \n        # Initialize a 'desirability' array. Assign 0 to bins that cannot fit the item.\n        desirability = np.zeros_like(bins_remain_cap)\n        \n        # For bins that can fit, assign a desirability score.\n        # We want to prioritize bins where `remaining_after_fit` is small and positive.\n        # A simple mapping: desirability = 1.0 / (remaining_after_fit + epsilon)\n        # This makes bins that are almost full (small positive remaining_after_fit) more desirable.\n        \n        desirability[can_fit_mask] = 1.0 / (remaining_after_fit[can_fit_mask] + epsilon)\n        \n        # Apply softmax function to the desirability scores.\n        # `np.exp(desirability)` creates exponentials of scores.\n        # Normalizing by the sum of exponentials converts these into probabilities.\n        \n        # To ensure numerical stability for `np.exp`, it's good practice to shift\n        # the scores so that the maximum score is 0.\n        # This is done by subtracting the maximum score from all scores.\n        \n        max_desirability = np.max(desirability)\n        shifted_desirability = desirability - max_desirability\n        \n        # Calculate exponentials of the shifted desirability scores\n        exp_scores = np.exp(shifted_desirability)\n        \n        # Normalize by the sum of the exponentials to get probabilities (priorities)\n        sum_exp_scores = np.sum(exp_scores)\n        \n        # Handle the case where all bins are unable to fit the item (sum_exp_scores would be 0).\n        # In such a case, all priorities should be 0 or some default.\n        if sum_exp_scores == 0:\n            priorities = np.zeros_like(bins_remain_cap)\n        else:\n            priorities = exp_scores / sum_exp_scores\n            \n    else:\n        # If no bin can fit the item, return all zeros\n        priorities = np.zeros_like(bins_remain_cap)\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit.\n\n    Almost Full Fit prioritizes bins that are close to full but can still accommodate the item.\n    This heuristic aims to leave more space in other bins for potentially larger items later.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We want to prioritize bins that have just enough space for the item,\n    # but not too much extra space.\n    # A high priority should be given to bins where bins_remain_cap - item is small and non-negative.\n    # We can achieve this by considering the negative of (bins_remain_cap - item)\n    # or equivalently, item - bins_remain_cap.\n\n    # Initialize priorities to a very low value (negative infinity)\n    # to indicate that bins that cannot fit the item have zero priority.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the priority.\n    # A higher priority is given when the remaining capacity is just enough or slightly more.\n    # This means we want to minimize (bins_remain_cap - item).\n    # A good score would be the negative of this difference.\n    # So, score = -(bins_remain_cap - item) = item - bins_remain_cap.\n    # A bin that fits the item with very little leftover space (bins_remain_cap - item close to 0)\n    # will have a priority close to 0.\n    # A bin that fits the item with a lot of leftover space (bins_remain_cap - item large)\n    # will have a large negative priority.\n\n    priorities[can_fit_mask] = item - bins_remain_cap[can_fit_mask]\n\n    # It can also be beneficial to penalize bins that are already very full.\n    # However, the core idea of \"Almost Full Fit\" is to select the tightest fit.\n    # The calculation `item - bins_remain_cap` already favors tighter fits.\n    # If we have multiple bins with the same `item - bins_remain_cap` value,\n    # the current implementation will pick the first one encountered in the array.\n    # If we wanted to break ties by picking the bin that is *most* full,\n    # we could add a small positive term proportional to `bins_remain_cap`\n    # but only for those that fit.\n    # For simplicity and adhering to the \"almost full\" idea, the `item - bins_remain_cap`\n    # on fitting bins is a strong indicator.\n\n    # Let's refine this. We want bins where `bins_remain_cap - item` is minimal and >= 0.\n    # The difference `bins_remain_cap - item` represents the \"slack\" or \"wasted space\"\n    # after placing the item. We want to minimize this slack.\n    # So, we can set priority to `- (bins_remain_cap - item)` for fitting bins.\n    # `priorities = -(bins_remain_cap - item)` => `priorities = item - bins_remain_cap`\n\n    # To ensure that bins that *just* fit are preferred over bins that fit with\n    # a lot of remaining capacity, we want `item - bins_remain_cap` to be as close to 0\n    # as possible from below.\n\n    # Let's consider a slightly different approach to emphasize \"almost full\" more directly.\n    # We can use the proportion of space used by the item in the bin if it were to fit.\n    # `item / bins_remain_cap` for bins where `bins_remain_cap >= item`.\n    # However, this doesn't directly align with \"almost full\" in terms of *remaining* capacity.\n\n    # Let's go back to minimizing the leftover space: `bins_remain_cap - item`.\n    # The negative of this is `item - bins_remain_cap`.\n    # A bin with remaining capacity `R` will have a score `item - R`.\n    # If `R = item`, score = 0.\n    # If `R = item + 1`, score = -1.\n    # If `R = item + 10`, score = -10.\n    # This means bins that are *more* full (smaller R) get higher scores.\n\n    # An alternative interpretation of \"Almost Full Fit\" might be to prioritize\n    # bins that have a remaining capacity that is slightly larger than the item.\n    # For example, if the item size is `s`, we might prefer bins with remaining capacity\n    # `R` such that `R` is small and `R >= s`.\n    # The difference `R - s` is the slack. Minimizing `R - s` is the goal.\n    # So, maximizing `s - R` is also a goal.\n\n    # Let's set priorities for bins that can fit the item.\n    # The priority is `item - bins_remain_cap`.\n    # For bins that don't fit, the priority is `-inf`.\n    # This means the highest priority will be for the bin with the smallest non-negative `bins_remain_cap - item`.\n\n    priorities_for_fitting = item - bins_remain_cap[can_fit_mask]\n\n    # Let's try to make the priority directly represent \"how close to fitting it is\".\n    # For bins that can fit, we want them to be as \"tight\" as possible.\n    # The tightest fit means `bins_remain_cap` is closest to `item`.\n    # So, we want to maximize `item / bins_remain_cap` for bins that can fit,\n    # assuming `bins_remain_cap` is the total capacity.\n    # But here `bins_remain_cap` is the *remaining* capacity.\n\n    # Consider the `bins_remain_cap` that are slightly larger than `item`.\n    # For these bins, `bins_remain_cap` could be `item + epsilon`.\n    # We want to maximize the priority for smaller `epsilon`.\n    # So, let's prioritize based on `item / bins_remain_cap` IF the bins are not too empty.\n\n    # Let's stick to the interpretation: Minimize the leftover space.\n    # `bins_remain_cap - item` should be minimized for `bins_remain_cap >= item`.\n    # Maximizing `item - bins_remain_cap` achieves this.\n    # The highest score will be 0 for a perfect fit, and increasingly negative for more slack.\n\n    # Another angle: How \"full\" is the bin if we put the item in?\n    # This relates to the *original* capacity, which we don't have here.\n    # So we work with remaining capacity.\n\n    # What if we define \"almost full\" as `bins_remain_cap` being in a certain range?\n    # E.g., `item <= bins_remain_cap < item + margin`.\n    # Within this range, we pick the smallest `bins_remain_cap`.\n    # This is equivalent to maximizing `item - bins_remain_cap`.\n\n    # Consider this: we want to find a bin `j` such that `bins_remain_cap[j] >= item` and\n    # `bins_remain_cap[j]` is minimized.\n    # This means we want to maximize `bins_remain_cap[j]` such that `bins_remain_cap[j] - item` is minimized and non-negative.\n\n    # Let's modify `item - bins_remain_cap` to ensure it's monotonic.\n    # If `bins_remain_cap[j]` is slightly larger than `bins_remain_cap[k]` (and both fit `item`),\n    # then `item - bins_remain_cap[j]` will be smaller than `item - bins_remain_cap[k]`.\n    # This means the bin with less remaining capacity (tighter fit) gets a higher score.\n\n    # A slight tweak to emphasize being \"almost full\" might involve scaling or shifting.\n    # For example, what if we normalize the remaining capacity?\n    # `bins_remain_cap / MAX_CAPACITY`? But we don't have MAX_CAPACITY.\n\n    # Let's try to ensure that bins that are \"too empty\" get lower priority.\n    # For instance, if `bins_remain_cap` is much larger than `item`, maybe the priority should be lower.\n    # Let `slack = bins_remain_cap - item`. We want to minimize `slack`.\n    # Prioritize `-slack` for `slack >= 0`.\n\n    # What if we define priority as `item / bins_remain_cap` for those that fit?\n    # If `bins_remain_cap = item`, score is 1.\n    # If `bins_remain_cap = 2 * item`, score is 0.5.\n    # If `bins_remain_cap = 1.1 * item`, score is `item / (1.1 * item) = 1 / 1.1 \\approx 0.909`.\n    # This prioritizes bins that are closer to being full (smaller `bins_remain_cap`) *if they can fit the item*.\n\n    # Let's combine these: Prioritize bins that fit, and among those, prioritize the ones with less remaining capacity.\n    # This still leads to `item - bins_remain_cap`.\n\n    # Let's consider the *percentage* of remaining capacity that the item would occupy.\n    # For a bin `j` with `bins_remain_cap[j] >= item`:\n    # The item takes up `item / (bin_total_capacity)` of the bin.\n    # This doesn't seem right since we're working with remaining capacity.\n\n    # Let's try a heuristic that penalizes bins that have a lot of remaining space,\n    # but still allows them to be picked if no tighter fit exists.\n    # We are looking for the smallest `bins_remain_cap[j]` such that `bins_remain_cap[j] >= item`.\n\n    # Consider the difference `bins_remain_cap[j] - item`. We want to minimize this.\n    # Let's use a penalty for large positive differences.\n    # The priority could be related to the inverse of `bins_remain_cap[j] - item + delta`, where `delta` is small.\n    # E.g., `1.0 / (bins_remain_cap[j] - item + 1e-9)` for fitting bins.\n    # This would give a high score when `bins_remain_cap[j] - item` is close to 0.\n\n    # Let's refine this to ensure higher priority for smaller remaining capacities:\n    # For bins that can fit the item:\n    # We want to prioritize `bins_remain_cap` that are small.\n    # Let's transform `bins_remain_cap` for fitting bins so that smaller values yield higher priority.\n    # `priority = 1 / (bins_remain_cap + 1)`? No, this doesn't consider the item size.\n\n    # Let's go back to the core idea: find the smallest `bins_remain_cap[j]` where `bins_remain_cap[j] >= item`.\n    # This is equivalent to maximizing `item - bins_remain_cap[j]` for `bins_remain_cap[j] >= item`.\n    # For bins where `bins_remain_cap[j] < item`, their priority should be zero (or negative infinity) as they cannot fit.\n\n    # So, `priorities[can_fit_mask] = item - bins_remain_cap[can_fit_mask]` is a good starting point.\n    # This assigns values like 0, -1, -2, etc.\n    # What if we want to ensure that the \"almost full\" aspect means the remaining capacity isn't excessively large?\n    # For instance, if the bin's remaining capacity is `C`, and item is `I`, we prefer `C` to be `I+epsilon`.\n\n    # Let's consider an alternative that penalizes bins that are \"too empty\" more aggressively.\n    # For bins that can fit:\n    # Priority = `item - bins_remain_cap`  (This favors tighter fits)\n    # Let's add a term that is more sensitive to larger remaining capacities.\n    # Example: `priority = (item - bins_remain_cap) - alpha * (bins_remain_cap - item)` for `bins_remain_cap >= item`.\n    # This doesn't seem right.\n\n    # A simpler approach: For bins that can fit, we want to pick the one that results in the least wasted space.\n    # Wasted space = `bins_remain_cap - item`.\n    # So we want to minimize `bins_remain_cap - item`.\n    # Therefore, we want to maximize `item - bins_remain_cap`.\n\n    # Let's think about the *degree* to which a bin is \"almost full\" given the item.\n    # If a bin has remaining capacity `R`, and item is `I`:\n    # The ratio `I / R` could indicate how much of the *current remaining space* the item would occupy.\n    # If `R` is just slightly larger than `I`, then `I/R` is close to 1.\n    # E.g., `R = I + 0.1`, `I/R = I/(I+0.1) \\approx 1`.\n    # E.g., `R = 2I`, `I/R = I/(2I) = 0.5`.\n    # This metric prioritizes bins where the item takes up a large fraction of the remaining space.\n    # This seems more aligned with \"almost full\".\n\n    # Let's set priorities for fitting bins using `item / bins_remain_cap`.\n    # And for bins that don't fit, set to a very low value.\n\n    priorities_for_fitting_alt = np.zeros_like(bins_remain_cap)\n    can_fit_mask = bins_remain_cap >= item\n\n    # For fitting bins, prioritize those with smaller remaining capacity.\n    # We can use `item / bins_remain_cap`.\n    # This assumes `bins_remain_cap` is not zero (which is guaranteed by `item >= 0` and `bins_remain_cap >= item`).\n    # The closer `bins_remain_cap` is to `item`, the higher the ratio `item / bins_remain_cap`.\n    priorities_for_fitting_alt[can_fit_mask] = item / bins_remain_cap[can_fit_mask]\n\n    # Now, let's refine this. If `item` is 0, this ratio is always 0.\n    # The problem states `item: float`, implying it could be 0. If `item = 0`, any bin can fit it.\n    # If `item = 0`, any bin with `bins_remain_cap >= 0` can fit.\n    # In this case, `item / bins_remain_cap` would be 0 for all fitting bins.\n    # This means if `item=0`, the tie-breaking among fitting bins would be arbitrary (based on index).\n\n    # Let's consider the \"almost full\" aspect directly by prioritizing bins where\n    # `bins_remain_cap` is just enough to fit `item`.\n    # This means `bins_remain_cap - item` should be minimal and non-negative.\n    # Let's use the inverse of `bins_remain_cap - item + epsilon` for fitting bins.\n    # `epsilon` is a small constant to avoid division by zero and to ensure that smaller\n    # (bins_remain_cap - item) values yield higher priorities.\n\n    priorities = np.full_like(bins_remain_cap, -np.inf) # Initialize with a very low priority\n\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the priority.\n    # We want to minimize the \"slack\" or wasted space: `bins_remain_cap - item`.\n    # Let `slack = bins_remain_cap - item`.\n    # A priority of `1 / (slack + small_epsilon)` would be high for small slack.\n    # Using `item / bins_remain_cap` also works because if `R` is small and `R >= I`, then `I/R` is high.\n    # Let's use `item / bins_remain_cap` as it's more direct and avoids introducing an arbitrary epsilon if `item` itself is very small.\n    # The logic is: prioritize bins where the item occupies a larger fraction of the *available remaining space*.\n\n    # To ensure the \"almost full\" idea is emphasized, we might want to boost priorities for bins that are already quite full in absolute terms.\n    # However, without total capacity, this is hard.\n\n    # Let's use the `item / bins_remain_cap` metric.\n    # If `item` is very small (e.g., 0.01) and `bins_remain_cap` is large (e.g., 100), the ratio is tiny.\n    # If `item` is 10 and `bins_remain_cap` is 11, ratio is ~0.9.\n    # If `item` is 10 and `bins_remain_cap` is 100, ratio is 0.1.\n    # This correctly prioritizes the bin with `11` remaining capacity.\n\n    # What if we want to also penalize bins that are \"too full\" already, meaning they have very little remaining capacity *even before* adding the item?\n    # The prompt implies we want to place the item into an \"almost full\" bin, meaning a bin that has a remaining capacity `R` such that `R` is not much larger than `item`.\n\n    # Let's refine the `item / bins_remain_cap` approach.\n    # For bins where `bins_remain_cap < item`, the priority is `-np.inf`.\n    # For bins where `bins_remain_cap >= item`:\n    # The priority is `item / bins_remain_cap`.\n    # This works as intended: a higher ratio means a smaller `bins_remain_cap` relative to `item`, which is the \"almost full\" fit.\n\n    # Let's consider `priority_v1` and how to improve it.\n    # `priority_v1` returns zeros, meaning all bins have equal priority if they can fit.\n    # Our goal is to create a priority based on the \"Almost Full Fit\" strategy.\n\n    # The core idea of \"Almost Full Fit\" (AFF) or \"Best Fit\" (BF) in bin packing is to select the bin\n    # where placing the current item leaves the minimum remaining capacity.\n    # If the current item size is `s`, and the bin remaining capacities are `C_j`:\n    # We want to choose bin `j` such that `C_j >= s` and `C_j - s` is minimized.\n    # This is equivalent to choosing bin `j` such that `C_j >= s` and `C_j` is minimized.\n\n    # So, we should prioritize bins with smaller remaining capacities, provided they can fit the item.\n    # A simple way to achieve this is to use `1 / (bins_remain_cap - item + epsilon)` or simply `item / bins_remain_cap`\n    # for fitting bins.\n    # The `item / bins_remain_cap` metric naturally gives higher values to bins with smaller remaining capacities that fit the item.\n\n    # Let's make sure our implementation reflects this:\n    priorities = np.full_like(bins_remain_cap, -np.inf)  # Initialize with a very low priority\n\n    # Create a mask for bins that have enough remaining capacity for the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, assign a priority.\n    # The priority is designed to favor bins with less remaining capacity (i.e., \"almost full\" bins).\n    # The ratio `item / bins_remain_cap` achieves this:\n    # - If `bins_remain_cap` is just slightly larger than `item`, the ratio is close to 1.\n    # - If `bins_remain_cap` is much larger than `item`, the ratio is closer to 0.\n    # This makes bins with tighter fits have higher priority scores.\n    # We use a small epsilon to avoid division by zero if `bins_remain_cap` could be 0,\n    # but given `bins_remain_cap >= item` and `item` is usually positive, `bins_remain_cap` will be >= 0.\n    # If `item = 0`, `bins_remain_cap >= 0`. If `bins_remain_cap` is 0, `0/0` is undefined.\n    # However, if `item = 0`, placing it requires `bins_remain_cap >= 0`.\n    # If `bins_remain_cap = 0` and `item = 0`, `0/0` is a problem.\n    # Let's handle the `item = 0` case. If item is 0, any bin with `bins_remain_cap >= 0` can take it.\n    # The ratio `0 / bins_remain_cap` is 0.\n    # To distinguish between fitting bins when item is 0, we might need a secondary criterion.\n    # Or, if `item=0`, any bin is equally \"almost full\" in terms of space used by the item.\n\n    # A safe approach for the division:\n    # If `item` is positive and `bins_remain_cap` is positive, `item / bins_remain_cap` is fine.\n    # If `item` is 0:\n    # If `bins_remain_cap > 0`, `0 / bins_remain_cap = 0`.\n    # If `bins_remain_cap = 0` and `item = 0`, this bin can fit the item, but division by zero occurs.\n    # We need to assign a priority in this specific `item=0, bins_remain_cap=0` case.\n    # Such a bin is \"full\" and can barely take a 0-sized item. The ratio is undefined but represents a \"full\" state.\n\n    # Let's define priority for the case `item=0` separately for clarity,\n    # or use a robust calculation.\n    # A bin is \"almost full\" if its remaining capacity is *just enough* for the item.\n    # So, `bins_remain_cap - item` should be minimal.\n    # Let `slack = bins_remain_cap - item`.\n    # Priority = `-slack` for `slack >= 0`. This ensures that minimum slack gets max priority.\n    # If `slack=0`, priority=0. If `slack=1`, priority=-1.\n    # This is `item - bins_remain_cap`.\n\n    # This `item - bins_remain_cap` strategy is often called \"Best Fit Decreasing\" or \"Worst Fit Decreasing\" depending on how you rank them.\n    # For \"Best Fit\", we select the bin that minimizes `bins_remain_cap - item`.\n    # Thus, we want to maximize `item - bins_remain_cap`.\n\n    # Let's implement this:\n    priorities[can_fit_mask] = item - bins_remain_cap[can_fit_mask]\n\n    # This strategy assigns higher scores to bins that are tighter fits.\n    # For example:\n    # item = 5\n    # bins_remain_cap = [10, 7, 12, 5, 8]\n    # can_fit_mask = [True, True, True, True, True]\n    # priorities = [5-10, 5-7, 5-12, 5-5, 5-8]\n    # priorities = [-5, -2, -7, 0, -3]\n    # The highest priority is 0, corresponding to the bin with remaining capacity 5 (a perfect fit).\n    # The next highest is -2, corresponding to the bin with remaining capacity 7.\n    # This seems to be a correct interpretation of \"Best Fit\", which implies \"almost full\".\n\n    # To further emphasize \"almost full\", one might penalize bins that are too empty.\n    # However, the prompt asks for \"Almost Full Fit\", implying a preference for tighter fits among those that can accommodate the item.\n    # The `item - bins_remain_cap` metric effectively does this.\n\n    # Final check on the logic for `item = 0`:\n    # item = 0\n    # bins_remain_cap = [10, 0, 5, -2] (though negative capacity is unlikely in BPP context)\n    # Assume `bins_remain_cap` are always non-negative.\n    # bins_remain_cap = [10, 0, 5]\n    # can_fit_mask = [True, True, True] (assuming 0 can fit in 0 or more)\n    # priorities = [0-10, 0-0, 0-5]\n    # priorities = [-10, 0, -5]\n    # The bin with 0 remaining capacity gets the highest priority (0), followed by 5 (-5), then 10 (-10).\n    # This aligns: the bin with the least remaining capacity gets the highest priority.\n\n    # This approach seems robust and directly addresses the goal of minimizing wasted space for the current item.\n    # The \"Almost Full Fit\" strategy is essentially Best Fit.\n\n    return priorities\n\n[Reflection]\nPrioritize bins minimizing `remaining_capacity - item`. Focus on \"tightest fit\".\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}