```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Softmax-Based Fit.

    The strategy prioritizes bins that have a remaining capacity close to the item size,
    while also considering bins with more remaining capacity as potentially "better"
    fits to allow for future larger items. The softmax function is used to convert
    these preferences into probabilities (priority scores).

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Calculate a "fitness" score for each bin.
    # A bin is more "fit" if its remaining capacity is close to the item size,
    # but also rewards bins with more remaining capacity overall to leave space
    # for potentially larger future items.
    # We add a small epsilon to avoid division by zero if a bin is full.
    epsilon = 1e-9
    # If remaining capacity is less than item, it's an invalid fit, assign a very low score.
    # If remaining capacity is exactly the item size, it's a perfect fit.
    # If remaining capacity is much larger, it's also a potential fit, but perhaps less immediate.
    # The goal here is to balance fitting the current item well with leaving future options.

    # Let's create a score that is high for bins that can fit the item and
    # has a decaying effect as remaining capacity increases beyond the item size.
    # However, we also want to slightly favor bins with more remaining capacity in general.

    # Candidate 1: Focus on closeness. Higher score if remaining_cap is close to item.
    # This could be achieved by inverse of absolute difference, but capped.
    # closeness_score = 1.0 / (np.abs(bins_remain_cap - item) + epsilon)

    # Candidate 2: Favor bins that fit, and among those, favor those with more space.
    # Let's combine a "can fit" boolean with a "remaining capacity bonus".
    can_fit = bins_remain_cap >= item
    
    # For bins that can fit, we want a score that reflects how well they fit.
    # A good fit would be when remaining_cap is just enough or slightly more.
    # However, to be creative and leverage softmax, let's consider a score that is
    # higher when remaining capacity is closer to 'item' but also rewards more capacity.

    # Let's define a "desirability" score.
    # If a bin cannot fit the item, its desirability is very low (e.g., -infinity).
    # If a bin can fit the item, its desirability is based on the remaining capacity.
    # We want to reward bins with remaining capacity equal to `item` or slightly above it,
    # but also consider bins with significantly more remaining capacity as good candidates
    # for leaving future room.

    # Let's try a score that rewards fitting the item well, and for bins that can fit,
    # we give a score that is higher when remaining_cap is closer to 'item', but
    # then adds a bonus for larger remaining capacity to encourage "emptying" bins
    # less aggressively if there's a good alternative.

    # A simple approach:
    # For bins that can fit, let's assign a score based on (remaining_cap - item).
    # This means bins with exactly `item` remaining will have a score of 0.
    # Bins with `item + k` remaining will have a score of `k`.
    # This might favor larger remaining capacities too much.

    # Let's try a score that is inversely related to how much 'excess' capacity there is,
    # but positively related to having *some* capacity.

    # Consider a score that penalizes excess capacity but still values bins that can fit.
    # Let's use `bins_remain_cap` directly as a base, but only for valid fits.
    # To incorporate the "closeness" idea:
    # We want to favor bins where `bins_remain_cap` is `item` or slightly larger.
    # If `bins_remain_cap` is much larger, it's still a fit, but maybe less preferred
    # than a bin that fits snugly.

    # Let's define a score based on remaining capacity, but make it non-linear.
    # We want to map remaining capacities to priorities.
    # High remaining capacity is generally good, but fitting the item is paramount.

    # Let's try a score that is `bins_remain_cap` itself, but penalized if it's
    # excessively large compared to `item`.
    # Or, let's use `bins_remain_cap - item`. Bins that fit will have `> 0`.
    # Bins that fit perfectly will have `0`.
    # Bins that have lots of space will have large positive values.

    # Alternative: prioritize bins that have the smallest `remaining_cap` greater than or equal to `item`.
    # This is a "Best Fit" strategy. For Softmax, we need to map these to scores.

    # Let's construct a score that is high when `bins_remain_cap` is close to `item`,
    # and decreases as `bins_remain_cap` moves away from `item`, but stays positive for valid fits.
    # A Gaussian-like function centered around `item` could work, but is sensitive to variance.

    # A simpler Softmax-Based Fit:
    # For bins that can fit the item, we calculate a "preference" score.
    # Bins with remaining capacity equal to `item` are maximally preferred.
    # As remaining capacity increases beyond `item`, the preference decreases.
    # As remaining capacity decreases below `item` (invalid fit), preference is very low.

    # Let's consider the "slack" in the bin: `slack = bins_remain_cap - item`.
    # If `slack < 0`, the bin cannot fit.
    # If `slack == 0`, it's a perfect fit.
    # If `slack > 0`, it's a fit with some excess space.

    # We want to prioritize bins with `slack` close to 0.
    # Let's define a "fit score" for valid bins:
    # For bins where `bins_remain_cap >= item`:
    #   We can use `1.0 / (bins_remain_cap - item + epsilon)` which rewards smaller slacks.
    #   However, this penalizes bins with slightly more slack too much, and doesn't
    #   directly use `bins_remain_cap` in a way that Softmax intuitively handles
    #   larger values as "more available".

    # Let's try a score that uses `bins_remain_cap` directly, but biases it.
    # We can scale `bins_remain_cap` and add a term that boosts bins closer to `item`.

    # A robust way for Softmax: create scores that reflect desirability.
    # Higher remaining capacity is generally more desirable for future items.
    # However, we must fit the current item.

    # Let's assign a base score of 1 to all bins.
    # If a bin can fit the item, let's add a bonus proportional to its remaining capacity.
    # But this might excessively favor very large bins even if they don't fit snugly.

    # Creative Softmax-Based Fit:
    # Consider the "opportunity cost". If a bin has a lot of remaining capacity,
    # putting a small item there might be suboptimal if a tighter fit exists.
    # Conversely, if a bin has just enough capacity, it's a good fit.

    # Let's define a score that encourages fitting the item into bins that have
    # "just enough" capacity, but also gives a moderate boost to bins with
    # more capacity to encourage usage.

    # Score logic:
    # 1. For bins that cannot fit the item, assign a very low score (e.g., negative infinity conceptually,
    #    or a very small negative number to avoid issues with softmax if we don't mask).
    # 2. For bins that can fit:
    #    a. If remaining capacity is exactly `item`, this is a prime candidate.
    #    b. If remaining capacity is `item + k` (k > 0), this is also a candidate.
    #       We want the score to decrease as `k` increases, but not too rapidly.
    #    c. Let's consider a score proportional to `bins_remain_cap`. This favors larger bins.
    #       But we want to slightly bias towards closer fits.

    # Let's try a score that is `bins_remain_cap` itself for valid bins.
    # Then, apply a penalty for having "too much" remaining capacity if a tighter fit is available.
    # This is getting complex for a direct softmax score.

    # Simpler approach for Softmax:
    # Map `bins_remain_cap` to a desirability score.
    # Higher `bins_remain_cap` means more "room", which is good.
    # So, the raw `bins_remain_cap` for valid fits is a good starting point.

    # Let's consider a transformation of `bins_remain_cap`.
    # The softmax works well with positive values.
    # `np.log1p(bins_remain_cap)` can smooth out large differences.

    # Let's focus on fitting the item well:
    # The ideal remaining capacity is `item`.
    # So, a score could be `1 / (bins_remain_cap - item + epsilon)` for valid bins.
    # This gives higher scores to bins with less slack.

    # Let's refine: prioritize bins that CAN fit, and among those, prioritize those
    # with remaining capacity CLOSEST to item size. But ALSO slightly favor bins
    # with more capacity as a secondary objective.

    # Consider a score where `f(x) = x` for `x >= item`.
    # This makes larger capacity bins more "prioritized" if they can fit.
    # To bias towards closer fits, we could subtract a penalty based on `bins_remain_cap - item`.
    # But the penalty needs to be carefully chosen.

    # Let's use the `bins_remain_cap` directly, but only for valid fits.
    # This means bins with more remaining capacity get higher raw scores if they can fit the item.
    # Softmax will then turn these raw scores into probabilities.
    # This strategy inherently favors larger bins that can accommodate the item.

    # To incorporate the "closeness" idea without a hard rule like Best Fit:
    # If remaining capacity is `R`, and item is `I`.
    # We want a score that is higher when `R` is closer to `I`.
    # Let's try `R - abs(R - I)`.
    # If R=10, I=8: Score = 10 - abs(10-8) = 10 - 2 = 8
    # If R=8, I=8: Score = 8 - abs(8-8) = 8 - 0 = 8
    # If R=6, I=8: Score = 6 - abs(6-8) = 6 - 2 = 4 (Invalid, should be very low)

    # Let's combine "can fit" with a score related to remaining capacity.
    # We can assign a score of 0 to bins that cannot fit, and a positive score to bins that can.

    # Let `scores = bins_remain_cap`. This favors larger bins.
    # For bins where `bins_remain_cap < item`, we need to set their scores to a value
    # that will result in a low probability after softmax.

    # Option: Assign a constant very small value to invalid bins, and `bins_remain_cap` to valid bins.
    # This is not very "creative" with Softmax.

    # Creative Softmax-Based Fit Idea:
    # Bias the remaining capacity scores based on how "close" they are to the item size.
    # For a bin with remaining capacity `R` and item size `I`:
    # - If `R < I`: Assign a score that is very small or negative.
    # - If `R >= I`: Assign a score that reflects desirability.
    #   Let's make the score higher for `R` close to `I`.

    # Let's define a function `desirability(R, I)`:
    # `desirability(R, I) = max(0, R)` - base availability
    # Add a bonus for closeness: `bonus = some_function(R, I)`
    # Total score = `max(0, R) + bonus`

    # How about a score that is the `bins_remain_cap` itself, but we "shift"
    # the distribution of scores to favor bins that are closer fits, while still
    # allowing larger bins to be picked.

    # Let's use a logistic function to map the "slack" (`R - I`) to a preference.
    # `slack = bins_remain_cap - item`
    # If `slack < 0`, preference is very low.
    # If `slack = 0`, preference is high.
    # If `slack > 0`, preference decreases as slack increases.

    # We can use `1 / (1 + exp(k * slack))` for slack > 0.
    # For slack = 0, this is 0.5. We want it higher.
    # For slack < 0, we want it high.

    # Let's use a score that's `bins_remain_cap` but penalized by `(bins_remain_cap - item)^2`.
    # The penalty is highest for the worst fits (large `R - I`) and 0 for perfect fits (`R = I`).
    # This still doesn't strongly encourage the tightest fit.

    # Softmax-based strategy: Focus on the _proportion_ of remaining capacity.
    # Or the _proportion_ of the item relative to remaining capacity.

    # Let's consider `score = bins_remain_cap` as the base.
    # For bins where `bins_remain_cap < item`, set their scores very low (e.g., -1e9).
    # For bins where `bins_remain_cap >= item`, we want to favor those closer to `item`.
    # So, we can modify their scores.

    # Creative idea: Apply a scaling factor to `bins_remain_cap` that is inversely
    # related to the "excess capacity" (`bins_remain_cap - item`).
    # For a bin with remaining capacity `R` and item size `I`:
    # Let `excess = R - I`.
    # Let `scaled_R = R / (1 + excess * weight)` where `weight` is a tuning parameter.
    # This reduces the score of bins with large excess capacity.

    # Let's use a simpler, yet effective, Softmax-based approach.
    # The core idea is to create a score that reflects the desirability of putting
    # the item into a bin. A bin is desirable if it can fit the item. Among bins
    # that can fit, bins with less remaining capacity (tighter fits) are often
    # preferred to leave more space in other bins for future items. However,
    # sometimes bins with more capacity are preferred if they are "empty enough"
    # and the tightness is not crucial.

    # Let's define a score for valid bins (`bins_remain_cap >= item`) that is:
    # `1 / (bins_remain_cap - item + epsilon)` - This prioritizes tightest fits.
    # And for invalid bins, a very low score.

    # Let's try to combine the idea of "having capacity" with "closeness".
    # Assign a "closeness bonus" which is higher when `bins_remain_cap` is close to `item`.
    # And a "capacity bonus" which is higher for more `bins_remain_cap`.

    # Consider a score that is `bins_remain_cap` itself, but for bins that are
    # valid fits. For invalid bins, assign a very low score.
    # This makes larger bins with capacity more likely to be chosen.
    # To introduce the "closeness" preference subtly:
    # Let's scale `bins_remain_cap` by a factor that slightly boosts those closer to `item`.

    # Let's use the raw `bins_remain_cap` for valid bins as a primary component,
    # and add a term that slightly favors smaller excesses.

    # Creative idea: For valid bins, `score = bins_remain_cap - alpha * (bins_remain_cap - item)`
    # where `alpha` is a small positive value (e.g., 0.1).
    # This slightly reduces the score of bins with large excess capacity.
    # If `alpha=0`, it's just `bins_remain_cap`.
    # If `alpha=1`, it's `bins_remain_cap - (bins_remain_cap - item) = item`. So it prioritizes bins
    # with remaining capacity equal to item size.

    alpha = 0.2 # Tuning parameter: influences bias towards tighter fits.
                # Higher alpha means stronger bias towards remaining capacity == item.
    
    # Initialize scores to a very low value to represent non-fits or low priority.
    # Using a large negative number will make its exponential close to zero.
    scores = np.full_like(bins_remain_cap, -1e9) 

    # Identify bins that can fit the item.
    can_fit_mask = bins_remain_cap >= item

    # For bins that can fit, calculate a desirability score.
    # We want to favor bins with remaining capacity close to 'item'.
    # Let's create a score that is high when `bins_remain_cap` is near `item`.
    # A score of `bins_remain_cap` rewards larger bins.
    # To add the "closeness" aspect, we can subtract a penalty that increases
    # with the excess capacity (`bins_remain_cap - item`).

    # For bins that fit: `score = bins_remain_cap - alpha * (bins_remain_cap - item)`
    # This is equivalent to: `score = item + (1 - alpha) * (bins_remain_cap - item)`
    # If alpha = 0.2: `score = item + 0.8 * (bins_remain_cap - item)`
    # This means:
    # - If `bins_remain_cap == item`, score is `item`.
    # - If `bins_remain_cap == item + 10`, score is `item + 8`.
    # - If `bins_remain_cap == item + 50`, score is `item + 40`.
    # This subtly favors tighter fits because their scores are closer to `item`.

    # Ensure scores are positive for softmax, or use robust softmax if scores can be negative.
    # However, it's generally better to have scores that are intrinsically positive if possible.
    # Let's adjust the scores for valid fits to be non-negative and reflect desirability.

    # A better way for Softmax with "closeness":
    # Use a score that is inversely related to the slack, but not overly sensitive.
    # Consider `1.0 / (bins_remain_cap - item + epsilon)`.
    # If `bins_remain_cap = item`, score is `1/epsilon` (very high).
    # If `bins_remain_cap = item + 5`, score is `1/5`.
    # If `bins_remain_cap = item + 10`, score is `1/10`.
    # This prioritizes tightest fits.

    # Let's add a small constant bonus to all valid bins so their scores are positive,
    # and then scale by the inverse slack.
    
    # Creative approach: Softmax on a score derived from remaining capacity that
    # incorporates a preference for "sufficient" capacity without being "excessive".

    # For each bin `i` with remaining capacity `r_i`:
    # If `r_i < item`, score is `0`.
    # If `r_i >= item`, score is `f(r_i, item)`.
    # Let `f(r_i, item) = r_i * (1 - some_penalty(r_i, item))`.
    # The penalty should be small for `r_i` close to `item`, and increase as `r_i` grows.

    # Let's use the transformed remaining capacity: `(bins_remain_cap - item)`.
    # We want to reward values near 0, and have smaller rewards as values increase.
    # Use `exp(-k * (bins_remain_cap - item))` for `bins_remain_cap >= item`.
    # For `bins_remain_cap = item`, score is `exp(0) = 1`.
    # For `bins_remain_cap = item + 5`, score is `exp(-5k)`.
    # For `bins_remain_cap = item + 10`, score is `exp(-10k)`.
    # This heavily favors tightest fits.

    # To balance: use `bins_remain_cap` as the base, and add the exponential term.
    # `score = bins_remain_cap + bonus * exp(-k * (bins_remain_cap - item))`

    # Let's try a score that is `bins_remain_cap` itself, but we modify it for
    # bins that are "too full" of remaining capacity compared to the item.
    # For valid fits: `score = bins_remain_cap - gamma * max(0, bins_remain_cap - item - threshold)`
    # `gamma` and `threshold` are parameters. This penalizes bins with `remaining_cap > item + threshold`.

    # For a Softmax-based strategy, we want to assign values that represent
    # the desirability of placing the item in each bin.
    # Let's make the score proportional to `bins_remain_cap`, but also boost bins
    # where `bins_remain_cap` is close to `item`.

    # A simple and interpretable Softmax approach:
    # For bins that can fit the item (`r >= item`):
    #   Let the raw score be `r`. This naturally favors larger bins.
    # To introduce a bias towards tighter fits, we can modify this raw score.
    # For `r >= item`, let `score = r * (1.0 / (1.0 + abs(r - item)))`. This is not ideal.

    # Let's use `bins_remain_cap` directly as the "base value" for valid bins.
    # This means bins with more space get a higher raw score.
    # Softmax will convert these into probabilities.
    # To make it "Softmax-Based Fit", we need to ensure the scores reflect some "fit" logic.

    # Creative twist for Softmax-Based Fit:
    # Let the score be related to the *proportion* of remaining capacity relative to item size,
    # and also related to the *absolute* remaining capacity.

    # Let's combine `bins_remain_cap` and `bins_remain_cap - item`.
    # For valid bins: `score = bins_remain_cap + constant * exp(-(bins_remain_cap - item)^2 / variance)`
    # This gives a Gaussian-like boost to bins around `item` remaining capacity,
    # but still rewards larger bins.
    
    # Let's try a score that is simply `bins_remain_cap`, but for bins that
    # cannot fit, assign a very low score. This is a form of "Worst Fit Decreasing"
    # where larger bins are preferred. Softmax will then normalize these.
    # To be more "Fit" related:

    # Let's assign a score based on `bins_remain_cap`.
    # We want to reward bins that can fit the item.
    # And among those, perhaps slightly prefer those with less "excess" capacity.

    # Final creative idea:
    # For each bin `i` with remaining capacity `r_i`:
    # If `r_i < item`: score is `-inf` (conceptually, very low value).
    # If `r_i >= item`:
    #   Calculate a "fit quality" score. This score should be higher when `r_i` is closer to `item`.
    #   Let `fit_quality = 1.0 / (r_i - item + epsilon)`. This prioritizes tight fits.
    #   Then, let the total score be `r_i * fit_quality`. This combines
    #   "having capacity" (`r_i`) with "tightness" (`fit_quality`).
    #   So, `score = r_i / (r_i - item + epsilon)`.

    # Example:
    # Item = 5
    # Bin 1: R=5. Score = 5 / (5 - 5 + eps) = 5 / eps (very high)
    # Bin 2: R=7. Score = 7 / (7 - 5 + eps) = 7 / (2 + eps) ~ 3.5
    # Bin 3: R=10. Score = 10 / (10 - 5 + eps) = 10 / (5 + eps) ~ 2.0
    # Bin 4: R=3. Score = -1e9 (cannot fit)
    
    # This seems like a good candidate for Softmax-Based Fit.
    # It favors tightest fits, but also considers overall capacity.
    # The `epsilon` is crucial to prevent division by zero.

    epsilon = 1e-9
    # Assign a very low score to bins that cannot fit the item.
    scores = np.where(bins_remain_cap >= item, bins_remain_cap / (bins_remain_cap - item + epsilon), -1e9)

    # Apply softmax to get probabilities (priorities)
    # exp_scores = np.exp(scores - np.max(scores)) # For numerical stability
    # priorities = exp_scores / np.sum(exp_scores)

    # The question asks for the priority score *for each bin*, which is typically the input to softmax.
    # The direct output of the priority function is usually the "logits" or unnormalized scores.
    # So, the `scores` calculated above are the priorities.
    
    return scores

```
