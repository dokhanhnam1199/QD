[
  {
    "stdout_filepath": "problem_iter8_response0.txt_stdout.txt",
    "code_path": "problem_iter8_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation normalizes the remaining capacities, penalizes unused bins,\n    boosts bins where the item fits well, considers gaps for good fits,\n    and normalizes by item size. It also penalizes bins where the item cannot fit.\n    Additionally, it uses a weighted strategy and considers order penalties.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    total_capacity = np.sum(bins_remain_cap)\n    if total_capacity == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / total_capacity\n    \n    # Penalize unused bins (higher penalty for bins with less remaining capacity)\n    unused_bin_penalty = np.exp(-bins_remain_cap / np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 0)\n    \n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well and penalize if not fitting perfectly\n    fit_boost = np.where(available_cap_after >= 0, available_cap_after / bins_remain_cap, 0)\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n    \n    # Consider gaps for good fits\n    gap_factor = np.where(available_cap_after >= 0, 1 / (1 + available_cap_after), 0)\n    \n    # Weighted strategy: prioritize bins with larger gaps and better fits\n    gap_weight = 0.6\n    fit_weight = 0.4\n    weighted_score = gap_factor * gap_weight + fit_boost * fit_weight\n    \n    # Combine scores: normalize by item size, consider fit boost and fit gaps, apply penalties\n    priority_scores = remain_cap_normalized + weighted_score\n    priority_scores *= unused_bin_penalty\n    priority_scores += non_fit_penalty\n    \n    # Final normalization by item size for balance\n    priority_scores /= (item if item > 0 else 1)\n    \n    # Prevent negative scores that might lead to selection of invalid bins\n    priority_scores[priority_scores < 0] = 0\n    \n    return priority_scores",
    "response_id": 0,
    "obj": 1.7550857598723597,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response1.txt_stdout.txt",
    "code_path": "problem_iter8_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on normalized capacities, penalizes unused bins,\n    prioritizes fitting with large gaps, ensures positive priorities, and uses\n    a simpler and more efficient approach.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    max_cap = np.max(bins_remain_cap)\n    if max_cap == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / max_cap\n    \n    # Penalize unused bins based on their index (earlier bins get a penalty)\n    bin_indices = np.arange(len(bins_remain_cap))\n    order_penalties = np.exp(-bin_indices * 0.5)\n    \n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well (considering gaps)\n    fit_boost = np.where(available_cap_after >= 0, available_cap_after / bins_remain_cap, 0)\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n    \n    # Combine scores: normalize by item size, consider fit boost, and apply penalties\n    priority_scores = (remain_cap_normalized + fit_boost) * order_penalties + non_fit_penalty\n    \n    # Ensure positive priorities\n    priority_scores = np.maximum(priority_scores, 0)\n    \n    # Normalize the final scores by item size to balance priorities\n    priority_scores /= (item if item > 0 else 1)\n    \n    return priority_scores",
    "response_id": 1,
    "obj": 5.384922217790187,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response2.txt_stdout.txt",
    "code_path": "problem_iter8_code2.py",
    "code": "def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin.\n\n    This implementation normalizes the remaining capacities, penalizes gaps,\n    boosts bins where the item fits perfectly, and applies weighted penalties.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    import numpy as np\n    \n    # Normalize remaining capacities\n    total_capacity = np.sum(bins_remain_cap)\n    if total_capacity == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / total_capacity\n\n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n\n    # Penalize gaps: higher penalty for larger gaps\n    gap_penalty = np.where(available_cap_after >= 0, available_cap_after / bins_remain_cap, 0)\n    gap_penalty = np.nan_to_num(gap_penalty, posinf=0, neginf=0)\n    \n    # Boost bins where the item fits perfectly\n    fit_boost = np.where(available_cap_after == 0, 1, 0)\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Weighted strategy: prioritize bins with better fits and penalize gaps\n    gap_weight = 0.2\n    fit_weight = 0.8\n    weighted_score = fit_weight * fit_boost - gap_weight * gap_penalty\n    \n    # Combine scores: normalize by item size, consider fit boost and fit gaps, apply penalties\n    priority_scores = remain_cap_normalized + weighted_score + non_fit_penalty\n    \n    # Ensure all scores are non-negative\n    priority_scores[priority_scores < 0] = 0\n    \n    return priority_scores",
    "response_id": 2,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 1, in <module>\n    def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\nNameError: name 'np' is not defined\n14\n2\n"
  },
  {
    "stdout_filepath": "problem_iter8_response3.txt_stdout.txt",
    "code_path": "problem_iter8_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on normalized capacities, penalizes unused bins,\n    and boosts bins where the item fits with large gaps. It ensures all priority scores\n    are positive to avoid invalid bin selections.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    max_cap = np.max(bins_remain_cap)\n    if max_cap == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / max_cap\n    \n    # Penalize unused bins (smaller the index, the higher the penalty)\n    order_penalties = np.exp(-np.arange(len(bins_remain_cap)) * 0.5)\n    \n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well and consider large gaps\n    fit_boost = np.where(available_cap_after >= 0, available_cap_after / bins_remain_cap, 0)\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n    \n    # Combine scores: normalize by item size, consider fit boost, and apply penalties\n    priority_scores = (remain_cap_normalized + fit_boost) * order_penalties + non_fit_penalty\n    \n    # Final normalization by item size for balance\n    if item > 0:\n        priority_scores /= item\n    \n    # Prevent negative scores that might lead to selection of invalid bins\n    priority_scores[priority_scores < 0] = 0\n    \n    return priority_scores",
    "response_id": 3,
    "obj": 5.384922217790187,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response4.txt_stdout.txt",
    "code_path": "problem_iter8_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on normalized capacities, penalizes unused bins,\n    and boosts bins where the item fits well, considering large gaps and penalizing\n    non-fits. It normalizes priorities by item size and reduces complexity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    max_cap = np.max(bins_remain_cap)\n    if max_cap == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / max_cap\n    \n    # Penalize unused bins (smaller the index, the higher the penalty)\n    order_penalties = np.exp(-np.arange(len(bins_remain_cap)) * 0.5)\n    \n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well and penalize if not fitting perfectly\n    fit_boost = np.where(available_cap_after >= 0, available_cap_after / item, 0)\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n    \n    # Combine scores: normalize by item size, consider fit boost, and apply penalties\n    priority_scores = (remain_cap_normalized + fit_boost) * order_penalties + non_fit_penalty\n    \n    # Ensure scores remain non-negative\n    priority_scores = np.maximum(priority_scores, 0)\n    \n    return priority_scores",
    "response_id": 4,
    "obj": 5.604307937774236,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response5.txt_stdout.txt",
    "code_path": "problem_iter8_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin.\n\n    This implementation normalizes the remaining capacities, penalizes gaps,\n    boosts bins where the item fits perfectly, and applies weighted penalties.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Normalize remaining capacities\n    total_capacity = np.sum(bins_remain_cap)\n    if total_capacity == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / total_capacity\n\n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n\n    # Penalize gaps: smaller gaps receive higher scores\n    gap_penalties = np.where(available_cap_after > 0, 1 / (1 + available_cap_after), -np.inf)\n\n    # Boost perfect fits\n    perfect_fit_boosts = np.where(available_cap_after == 0, 1, 0)\n\n    # Gap and perfect fit weights\n    gap_weight = 0.4\n    perfect_fit_weight = 0.6\n\n    # Calculate weighted score\n    weighted_scores = gap_penalties * gap_weight + perfect_fit_boosts * perfect_fit_weight\n\n    # Combine scores: normalize by item size, consider fit boosts and gap penalties\n    priority_scores = remain_cap_normalized + weighted_scores\n\n    return priority_scores",
    "response_id": 5,
    "obj": 5.195452732349436,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response6.txt_stdout.txt",
    "code_path": "problem_iter8_code6.py",
    "code": "def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin.\n\n    This implementation normalizes the remaining capacities, penalizes gaps,\n    boosts bins where the item fits perfectly, and applies weighted penalties.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    import numpy as np\n    \n    # Normalize remaining capacities\n    total_capacity = np.sum(bins_remain_cap)\n    if total_capacity == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / total_capacity\n\n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n\n    # Penalize gaps: higher penalty for larger gaps\n    gap_penalty = np.where(available_cap_after >= 0, available_cap_after / bins_remain_cap, 0)\n    gap_penalty = np.nan_to_num(gap_penalty, posinf=0, neginf=0)\n    \n    # Boost bins where the item fits perfectly\n    fit_boost = np.where(available_cap_after == 0, 1, 0)\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Weighted strategy: prioritize bins with better fits and penalize gaps\n    gap_weight = 0.2\n    fit_weight = 0.8\n    weighted_score = fit_weight * fit_boost - gap_weight * gap_penalty\n    \n    # Penalize unused bins (smaller the index, the higher the penalty)\n   order_penalties = np.exp(-np.arange(len(bins_remain_cap)) * 0.5)\n    \n    # Combine scores: normalize by item size, consider fit boost and fit gaps, apply penalties\n    priority_scores = (remain_cap_normalized + weighted_score + non_fit_penalty) * order_penalties\n    \n    # Prevent negative scores that might lead to selection of invalid bins\n    priority_scores[priority_scores < 0] = 0\n    \n    return priority_scores",
    "response_id": 6,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 43\n    order_penalties = np.exp(-np.arange(len(bins_remain_cap)) * 0.5)\n                                                                    ^\nIndentationError: unindent does not match any outer indentation level\n14\n2\n"
  },
  {
    "stdout_filepath": "problem_iter8_response7.txt_stdout.txt",
    "code_path": "problem_iter8_code7.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on normalized capacities, penalizes unused bins,\n    and boosts bins where the item fits well, considering large gaps and penalizing\n    non-fits. It normalizes priorities by item size and reduces complexity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    max_cap = np.max(bins_remain_cap)\n    if max_cap == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / max_cap\n    \n    # Penalize unused bins (smaller the index, the higher the penalty)\n    order_penalties = np.exp(-np.arange(len(bins_remain_cap)) * 0.5)\n    \n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well and penalize if not fitting perfectly\n    fit_boost = np.where(available_cap_after >= 0, available_cap_after / item, 0)\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n    \n    # Normalize fit boosting by item size to avoid bias towards larger items\n    fit_boost_normalized = fit_boost / (item if item > 0 else 1)\n    \n    # Combine scores: normalize by item size, consider fit boost, and apply penalties\n    priority_scores = (remain_cap_normalized + fit_boost_normalized) * order_penalties + non_fit_penalty\n    \n    # Prevent negative scores that might lead to selection of invalid bins\n    priority_scores[priority_scores < 0] = 0\n    \n    return priority_scores",
    "response_id": 7,
    "obj": 4.646988432389324,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response8.txt_stdout.txt",
    "code_path": "problem_iter8_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on normalized capacities, penalizes unused bins,\n    prioritizes fitting with large gaps, ensures positive priorities, and simplifies logic.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    max_cap = np.max(bins_remain_cap)\n    if max_cap == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / max_cap\n    \n    # Penalize unused bins (smaller the index, the higher the penalty)\n    order_penalties = np.exp(-np.arange(len(bins_remain_cap)) * 0.5)\n    \n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well and penalize if not fitting perfectly\n    fit_boost = np.where(available_cap_after >= 0, available_cap_after, 0)\n    \n    # Combine scores: normalize by item size, consider fit boost, and apply penalties\n    priority_scores = remain_cap_normalized + fit_boost * order_penalties + non_fit_penalty\n    \n    # Final normalization by item size for balance\n    priority_scores /= (item if item > 0 else 1)\n    \n    # Prevent negative scores that might lead to selection of invalid bins\n    priority_scores[priority_scores < 0] = 0\n    \n    return priority_scores",
    "response_id": 8,
    "obj": 19.73474272038294,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response9.txt_stdout.txt",
    "code_path": "problem_iter8_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on normalized capacities, penalizes unused bins,\n    and boosts bins where the item fits well, considering large gaps and penalizing\n    non-fits. It normalizes priorities by item size and reduces complexity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    max_cap = np.max(bins_remain_cap)\n    if max_cap == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / max_cap\n    \n    # Penalize unused bins (smaller the index, the higher the penalty)\n    order_penalties = np.exp(-np.arange(len(bins_remain_cap)) * 0.5)\n    \n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well and penalize if not fitting perfectly\n    fit_boost = np.where(available_cap_after >= 0, available_cap_after / item, 0)\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n    \n    # Combine scores: normalize by item size, consider fit boost, and apply penalties\n    priority_scores = (remain_cap_normalized + fit_boost) * order_penalties + non_fit_penalty\n    \n    # Ensure scores remain non-negative\n    priority_scores = np.maximum(priority_scores, 0)\n    \n    return priority_scores",
    "response_id": 9,
    "obj": 5.604307937774236,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  }
]