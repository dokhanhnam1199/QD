[
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation normalizes the remaining capacity, penalizes unused bins,\n    ensures non-negative priority scores, and favors bins that can fit the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    max_cap = np.max(bins_remain_cap)\n    if max_cap == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / max_cap\n    \n    # Penalize for each unused bin to encourage filling existing bins\n    num_bins_penalty = 1 / (np.count_nonzero(bins_remain_cap) + 1e-6)\n    \n    # Initialize priority scores\n    priority_scores = remain_cap_normalized - num_bins_penalty\n    \n    # Avoid negative priority scores\n    priority_scores = np.maximum(priority_scores, 0)\n    \n    # Encourage using bins with large enough space for the item\n    priority_scores[bins_remain_cap >= item] += 1\n    \n    return priority_scores",
    "response_id": 0,
    "obj": 149.30195452732352,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response1.txt_stdout.txt",
    "code_path": "problem_iter2_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation considers both the remaining capacity and the number of\n    bins used using a weighted score. It normalizes the priority by the item size,\n    penalizes bins that cannot fit the item, and boosts bins with the second smallest gaps\n    to encourage less fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priority as zero\n    priority = np.zeros_like(bins_remain_cap)\n\n    # Calculate base priority for bins that can fit the item\n    can_fit_item_mask = bins_remain_cap >= item\n    priority[can_fit_item_mask] = (bins_remain_cap[can_fit_item_mask] - item) / item\n\n    # Penalize bins that cannot fit the item\n    priority[~can_fit_item_mask] -= 2\n\n    # Boost bins with the second smallest gaps (excluding the largest gap)\n    if np.any(can_fit_item_mask):\n        # Sort the remaining capacities of bins that can fit the item\n        priority_after_check = bins_remain_cap[can_fit_item_mask] - item\n        sorted_indices = np.argsort(priority_after_check)\n        \n        # Boost the second smallest gap if it exists\n        if len(sorted_indices) > 1:\n            second_smallest_index = sorted_indices[1]\n            priority[can_fit_item_mask][second_smallest_index] += 1\n\n    return priority",
    "response_id": 1,
    "obj": 35.41084962106105,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response2.txt_stdout.txt",
    "code_path": "problem_iter2_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation prioritizes bins with larger gaps, penalizes bins that cannot fit the item,\n    and subtly boosts the second smallest gaps to minimize fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity after adding the item\n    remaining_after_item = bins_remain_cap - item\n    \n    # Initialize priority scores with a baseline of remaining capacity\n    priority_scores = remaining_after_item\n    \n    # Penalize bins that cannot fit the item\n    can_fit_item_mask = bins_remain_cap >= item\n    priority_scores[~can_fit_item_mask] -= 2 * np.max(bins_remain_cap, initial=1)\n    \n    # Find the second smallest gap for boosting\n    if np.any(can_fit_item_mask):\n        # Get remaining capacities of bins that can fit the item\n        valid_remaining_caps = remaining_after_item[can_fit_item_mask]\n        \n        # Partition and find the two smallest values\n        sorted_valid_caps = np.partition(valid_remaining_caps, 1)[:2]\n        \n        # Check if there are at least two valid capacities\n        if len(sorted_valid_caps) > 1:\n            second_smallest_gap = sorted_valid_caps[1]\n            \n            # Boost the priority of bins with the second smallest gap\n            second_smallest_indices = np.where(np.isclose(valid_remaining_caps, second_smallest_gap))[0]\n            for idx in second_smallest_indices:\n                priority_scores[can_fit_item_mask][idx] += 0.1  # Boost slightly\n\n    return priority_scores",
    "response_id": 2,
    "obj": 149.30195452732352,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation prefers bins that can fit the item, penalizes incapacity heavily,\n    and prioritizes filling larger gaps slightly.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priority with remaining capacity divided by item size\n    priority = bins_remain_cap / item\n    \n    # Penalize bins that cannot fit the item heavily\n    priority[bins_remain_cap < item] -= 10\n    \n    # Prefer filling larger gaps slightly by boosting bins with higher remaining capacity\n    priority[bins_remain_cap >= item] += (bins_remain_cap[bins_remain_cap >= item] - item) / item\n    \n    return priority",
    "response_id": 3,
    "obj": 149.30195452732352,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response4.txt_stdout.txt",
    "code_path": "problem_iter2_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on normalized capacities, penalizes unused bins,\n    avoids negative priority scores, and boosts bins with enough space for the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    remain_cap_normalized = bins_remain_cap / np.max(bins_remain_cap, initial=1.0)\n    \n    # Penalize for each unused bin to encourage filling existing bins\n    used_bins_count = np.count_nonzero(bins_remain_cap < np.inf)\n    num_bins_penalty = 1 / (used_bins_count + 1e-6) if used_bins_count > 0 else 0\n    \n    # Calculate base priority as normalized remaining capacity plus penalty\n    priority_scores = remain_cap_normalized + num_bins_penalty\n    \n    # Avoid negative priority scores (e.g., if item is larger than a bin)\n    priority_scores[priority_scores < 0] = 0\n    \n    # Boost bins that have enough space for the item\n    priority_scores[bins_remain_cap >= item] += 1\n    \n    return priority_scores",
    "response_id": 4,
    "obj": 149.30195452732352,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response5.txt_stdout.txt",
    "code_path": "problem_iter2_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on normalized capacities, penalizes unused bins,\n    avoids negative priority scores, and boosts bins with enough space for the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    remain_cap_normalized = bins_remain_cap / np.max(bins_remain_cap, initial=1.0)\n    \n    # Penalize for each unused bin to encourage filling existing bins\n    used_bins_count = np.count_nonzero(bins_remain_cap < np.inf)\n    num_bins_penalty = 1 / (used_bins_count + 1e-6) if used_bins_count > 0 else 0\n    \n    # Calculate base priority as normalized remaining capacity plus penalty\n    priority_scores = remain_cap_normalized + num_bins_penalty\n    \n    # Avoid negative priority scores (e.g., if item is larger than a bin)\n    priority_scores[priority_scores < 0] = 0\n    \n    # Boost bins that have enough space for the item\n    priority_scores[bins_remain_cap >= item] += 1\n    \n    return priority_scores",
    "response_id": 5,
    "obj": 149.30195452732352,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response6.txt_stdout.txt",
    "code_path": "problem_iter2_code6.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation considers both the remaining capacity and the number of\n    bins used using a weighted score. It normalizes the priority by the item size,\n    penalizes bins that cannot fit the item, and boosts bins with the second smallest gaps\n    to encourage less fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priority as zero\n    priority = np.zeros_like(bins_remain_cap)\n\n    # Calculate base priority for bins that can fit the item\n    can_fit_item_mask = bins_remain_cap >= item\n    priority[can_fit_item_mask] = (bins_remain_cap[can_fit_item_mask] - item) / item\n\n    # Penalize bins that cannot fit the item\n    priority[~can_fit_item_mask] -= 2\n\n    # Boost bins with the second smallest gaps (excluding the largest gap)\n    if np.any(can_fit_item_mask):\n        # Sort the remaining capacities of bins that can fit the item\n        priority_after_check = bins_remain_cap[can_fit_item_mask] - item\n        sorted_indices = np.argsort(priority_after_check)\n        \n        # Boost the second smallest gap if it exists\n        if len(sorted_indices) > 1:\n            second_smallest_index = sorted_indices[1]\n            priority[can_fit_item_mask][second_smallest_index] += 1\n\n    return priority",
    "response_id": 6,
    "obj": 35.41084962106105,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response7.txt_stdout.txt",
    "code_path": "problem_iter2_code7.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on prioritizing bins with higher remaining capacity,\n    penalizing gaps, preventing unused bins, and penalizing single small fragment bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priority scores\n    priority = np.zeros_like(bins_remain_cap)\n    \n    # Bins that can fit the item\n    can_fit_item_mask = bins_remain_cap >= item\n    \n    # Prioritize bins with higher remaining capacity that can fit the item\n    priority[can_fit_item_mask] = bins_remain_cap[can_fit_item_mask] / item\n    \n    # Penalize bins that cannot fit the item\n    priority[~can_fit_item_mask] -= 2\n    \n    # Encourage filling bigger gaps to prevent fragmentation\n    if np.any(can_fit_item_mask):\n        gaps = bins_remain_cap[can_fit_item_mask] - item\n        sorted_gaps_indices = np.argsort(gaps)\n        second_smallest_gap_index = sorted_gaps_indices[1] if len(sorted_gaps_indices) > 1 else None\n        \n        if second_smallest_gap_index is not None:\n            second_smallest_gap = gaps[second_smallest_gap_index]\n            priority[can_fit_item_mask][second_smallest_gap_index] += 1\n    \n    # Penalize single small fragment bins\n    very_small_fragment_threshold = item * 0.1  # Define a threshold for very small fragments\n    very_small_fragments_mask = (bins_remain_cap - item) < very_small_fragment_threshold\n    priority[very_small_fragments_mask] -= 1\n    \n    return priority",
    "response_id": 7,
    "obj": 47.61667331471879,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response8.txt_stdout.txt",
    "code_path": "problem_iter2_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation considers both the remaining capacity and the number of\n    bins used penalties to encourage efficient packing while distributing load.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Penalize for number of bins used (smaller the index, the more penalty)\n    order_penalties = np.exp(-np.arange(len(bins_remain_cap)))\n    \n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    can_fit = available_cap_after >= 0\n    priority_scores = np.where(can_fit, available_cap_after / bins_remain_cap, 0)\n    \n    # Penalize bins inversely with how much remains if the item is too large to fit perfectly\n    deviation_from_perfect_fit = np.abs(available_cap_after / bins_remain_cap)\n    priority_scores -= (bins_remain_cap != 0) * deviation_from_perfect_fit\n    \n    # Normalize remaining capacity\n    max_remaining_capacity = np.max(bins_remain_cap)\n    if max_remaining_capacity > 0:\n        priority_scores /= max_remaining_capacity\n    \n    # Apply order penalties\n    priority_scores *= order_penalties\n    \n    return priority_scores",
    "response_id": 8,
    "obj": 4.487435181491823,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on normalized capacities, penalizes unused bins,\n    avoids negative priority scores, and boosts bins with enough space for the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    remain_cap_normalized = bins_remain_cap / np.max(bins_remain_cap, initial=1.0)\n    \n    # Penalize for each unused bin to encourage filling existing bins\n    used_bins_count = np.count_nonzero(bins_remain_cap < np.inf)\n    num_bins_penalty = 1 / (used_bins_count + 1e-6) if used_bins_count > 0 else 0\n    \n    # Calculate base priority as normalized remaining capacity plus penalty\n    priority_scores = remain_cap_normalized + num_bins_penalty\n    \n    # Avoid negative priority scores (e.g., if item is larger than a bin)\n    priority_scores[priority_scores < 0] = 0\n    \n    # Boost bins that have enough space for the item\n    priority_scores[bins_remain_cap >= item] += 1\n    \n    return priority_scores",
    "response_id": 9,
    "obj": 149.30195452732352,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  }
]