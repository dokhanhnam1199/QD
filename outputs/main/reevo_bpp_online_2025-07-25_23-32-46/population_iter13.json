[
  {
    "stdout_filepath": "problem_iter12_response0.txt_stdout.txt",
    "code_path": "problem_iter12_code0.py",
    "code": "def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on a weighted strategy that normalizes remaining capacities,\n    penalizes unused bins using exponential decay, boosts bins where the item fits well,\n    and considers gaps for good fits. It also normalizes by item size and ensures no negative scores.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    import numpy as np\n    \n    # Calculate normalized remaining capacity\n    total_capacity = np.sum(bins_remain_cap)\n    if total_capacity == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / total_capacity\n    \n    # Penalize unused bins by considering their remaining capacity with exponential decay\n    unused_bin_penalty = np.exp(-bins_remain_cap / np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 0)\n    \n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well and penalize if not fitting perfectly\n    fit_boost = np.where(available_cap_after >= 0, available_cap_after / item, 0)\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n    \n    # Consider gaps for good fits\n    gap_factor = np.where(available_cap_after >= 0, 1 / (1 + available_cap_after / item), 0)\n    \n    # Weighted strategy: prioritize bins with larger gaps and better fits\n    gap_weight = 0.6\n    fit_weight = 0.4\n    weighted_score = gap_factor * gap_weight + fit_boost * fit_weight\n    \n    # Combine scores: normalize by item size, consider fit boost and fit gaps, apply penalties\n    priority_scores = remain_cap_normalized + weighted_score\n    priority_scores *= unused_bin_penalty\n    priority_scores += non_fit_penalty\n    \n    # Final normalization by item size for balance\n    priority_scores /= (item if item > 0 else 1)\n    \n    # Ensure scores remain non-negative\n    priority_scores[priority_scores < 0] = 0\n    \n    return priority_scores",
    "response_id": 0,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 1, in <module>\n    def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\nNameError: name 'np' is not defined\n25\n4\n"
  },
  {
    "stdout_filepath": "problem_iter12_response1.txt_stdout.txt",
    "code_path": "problem_iter12_code1.py",
    "code": "def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin.\n\n    This implementation normalizes the remaining capacities, penalizes unused bins,\n    boosts bins where the item fits well, considers gaps for good fits,\n    and normalizes by item size. It also penalizes bins where the item cannot fit.\n    Additionally, it uses a weighted strategy and ensures robustness to edge cases.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    import numpy as np\n\n    # Calculate total capacity for normalization\n    total_capacity = np.sum(bins_remain_cap)\n    if total_capacity == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / total_capacity\n    \n    # Penalize unused bins (higher penalty for bins with less remaining capacity)\n    max_cap = np.max(bins_remain_cap)\n    if max_cap == 0:\n        unused_bin_penalty = np.ones_like(bins_remain_cap)\n    else:\n        unused_bin_penalty = np.exp(-bins_remain_cap / max_cap)\n    \n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well\n    fit_boost = np.where(available_cap_after >= 0, available_cap_after / bins_remain_cap, 0)\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n    \n    # Consider gaps for good fits\n    gap_factor = np.where(available_cap_after >= 0, 1 / (1 + available_cap_after), 0)\n    \n    # Weighted strategy: prioritize bins with larger gaps and better fits\n    gap_weight = 0.6\n    fit_weight = 0.4\n    weighted_score = gap_factor * gap_weight + fit_boost * fit_weight\n    \n    # Combine scores\n    priority_scores = remain_cap_normalized + weighted_score\n    priority_scores *= unused_bin_penalty\n    priority_scores += non_fit_penalty\n    \n    # Final normalization by item size for balance\n    priority_scores /= (item if item > 0 else 1)\n    \n    # Prevent negative scores that might lead to selection of invalid bins\n    priority_scores[priority_scores < 0] = 0\n    \n    return priority_scores",
    "response_id": 1,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 1, in <module>\n    def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\nNameError: name 'np' is not defined\n25\n4\n"
  },
  {
    "stdout_filepath": "problem_iter12_response2.txt_stdout.txt",
    "code_path": "problem_iter12_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on normalized capacities, penalizes unused bins,\n    and boosts bins where the item fits well with large gaps. It simplifies the\n    boosting logic and ensures all scores are non-negative.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    max_cap = np.max(bins_remain_cap)\n    remain_cap_normalized = bins_remain_cap / max_cap if max_cap > 0 else np.zeros_like(bins_remain_cap)\n    \n    # Penalize unused bins based on their index (smaller index, higher penalty)\n    order_penalties = np.exp(-np.arange(len(bins_remain_cap)) * 0.1)\n    \n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well with large gaps\n    fit_boost = np.where(available_cap_after >= 0, available_cap_after / item, 0) if item > 0 else np.zeros_like(bins_remain_cap)\n    \n    # Combine scores: normalize by item size, consider fit boost, and apply penalties\n    priority_scores = (remain_cap_normalized + fit_boost) * order_penalties + non_fit_penalty\n    \n    # Ensure all scores are non-negative\n    priority_scores[priority_scores < 0] = 0\n    \n    return priority_scores",
    "response_id": 2,
    "obj": 9.46350219385721,
    "SLOC": 25.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response3.txt_stdout.txt",
    "code_path": "problem_iter12_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority score for adding an item to each bin in the online bin packing problem.\n\n    This version simplifies the logic, focuses on clear weighting, normalizes scores, and\n    sharply penalizes bins where the item cannot fit.\n\n    Args:\n        item: Size of the item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of the same size as bins_remain_cap with priority score for each bin.\n    \"\"\"\n    # Assign zero priority to bins where the item cannot fit\n    can_fit = bins_remain_cap >= item\n    non_fit_penalty = np.where(can_fit, 0, -np.inf)\n    \n    # Calculate remaining capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Boost bins where the item fits well, penalize gaps\n    fit_boost = np.where(can_fit, available_cap_after / bins_remain_cap, 0)\n    gap_penalty = np.where(can_fit, 1 / (1 + available_cap_after), 0)\n    \n    # Weighted strategy: prioritize bins with better fits and smaller gaps\n    fit_weight = 0.7\n    gap_weight = 0.3\n    weighted_score = fit_boost * fit_weight - gap_penalty * gap_weight\n    \n    # Combine scores and apply penalties\n    priority_scores = weighted_score\n    \n    # Add penalties for non-fitting bins\n    priority_scores += non_fit_penalty\n    \n    # Normalize scores by item size\n    priority_scores /= (item if item > 0 else 1)\n    \n    # Ensure scores are non-negative\n    priority_scores = np.maximum(priority_scores, 0)\n    \n    return priority_scores",
    "response_id": 3,
    "obj": 149.2919824491424,
    "SLOC": 25.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response4.txt_stdout.txt",
    "code_path": "problem_iter12_code4.py",
    "code": "def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on normalized capacities, penalizes gaps,\n    boosts bins where the item fits well, ensures positive priorities,\n    and uses a weighted strategy to balance fits and gaps.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    import numpy as np\n    \n    # Calculate normalized remaining capacity\n    total_capacity = np.sum(bins_remain_cap)\n    if total_capacity == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / total_capacity\n    \n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well\n    fit_boost = np.where(available_cap_after >= 0, (bins_remain_cap - available_cap_after) / bins_remain_cap, 0)\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n    \n    # Penalize gaps\n    gap_penalty = np.where(available_cap_after >= 0, available_cap_after / bins_remain_cap, 0)\n    gap_penalty = np.nan_to_num(gap_penalty, posinf=0, neginf=0)\n\n    # Weighted strategy: prioritize bins with less gaps and better fits\n    gap_weight = 0.4\n    fit_weight = 0.6\n    weighted_score = fit_boost * fit_weight - gap_penalty * gap_weight\n\n    # Combine scores\n    priority_scores = remain_cap_normalized + weighted_score + non_fit_penalty\n    \n    # Ensure positive priorities\n    priority_scores = np.maximum(priority_scores, 0)\n    \n    return priority_scores",
    "response_id": 4,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 1, in <module>\n    def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\nNameError: name 'np' is not defined\n25\n4\n"
  },
  {
    "stdout_filepath": "problem_iter12_response5.txt_stdout.txt",
    "code_path": "problem_iter12_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This improved implementation normalizes the remaining capacities, penalizes unused bins,\n    boosts bins where the item fits well, considers gaps for good fits,\n    and normalizes by item size. It also penalizes bins where the item cannot fit.\n    Additionally, it uses a weighted strategy and considers order penalties.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    total_capacity = np.sum(bins_remain_cap)\n    if total_capacity == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / total_capacity\n    \n    # Penalize unused bins (higher penalty for bins with less remaining capacity)\n    max_cap = np.max(bins_remain_cap)\n    unused_bin_penalty = np.exp(-bins_remain_cap / (max_cap if max_cap > 0 else 1))\n\n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well\n    fit_boost = np.where(available_cap_after >= 0, 1 - available_cap_after / bins_remain_cap, 0)\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n    \n    # Penalize gaps for good fits\n    gap_penalty = np.where(available_cap_after >= 0, available_cap_after / bins_remain_cap, 0)\n    gap_penalty = np.nan_to_num(gap_penalty, posinf=0, neginf=0)\n\n    # Weighted strategy: prioritize bins with less gaps and better fits\n    gap_weight = 0.4\n    fit_weight = 0.6\n    weighted_score = fit_boost * fit_weight - gap_penalty * gap_weight\n\n    # Combine scores: consider fit boost and fit gaps, apply penalties and unused bin penalty\n    priority_scores = remain_cap_normalized + weighted_score * unused_bin_penalty + non_fit_penalty\n    \n    # Prevent negative scores that might lead to selection of invalid bins\n    priority_scores[priority_scores < 0] = 0\n    \n    return priority_scores",
    "response_id": 5,
    "obj": 4.028719585161557,
    "SLOC": 25.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response6.txt_stdout.txt",
    "code_path": "problem_iter12_code6.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation further refines the approach by enhancing penalties, normalizing by item size,\n    using weights for different criteria, and penalizing gaps properly. It also considers a decay factor\n    for unused bins to prioritize filling bins over creating new ones.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    total_capacity = np.sum(bins_remain_cap)\n    if total_capacity == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / total_capacity\n    \n    # Penalize unused bins with a decay factor\n    max_cap = np.max(bins_remain_cap)\n    unused_bin_penalty = np.exp(-0.5 * bins_remain_cap / (max_cap if max_cap > 0 else 1))\n\n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well and penalize if not fitting perfectly\n    fit_boost = np.where(available_cap_after >= 0, 1 - available_cap_after / bins_remain_cap, 0)\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n    \n    # Penalize gaps for good fits\n    gap_penalty = np.where(available_cap_after >= 0, available_cap_after / bins_remain_cap, 0)\n    gap_penalty = np.nan_to_num(gap_penalty, posinf=0, neginf=0)\n\n    # Weighted strategy: prioritize bins with less gaps and better fits\n    gap_weight = 0.4\n    fit_weight = 0.6\n    weighted_score = fit_boost * fit_weight - gap_penalty * gap_weight\n\n    # Normalize by item size to ensure consistency\n    normalized_item_size = item / np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 1\n    normalized_score = weighted_score / (normalized_item_size + 1e-6)\n\n    # Combine scores: normalize by item size, consider fit boost and fit gaps, apply penalties\n    priority_scores = (remain_cap_normalized + normalized_score) * unused_bin_penalty + non_fit_penalty\n    \n    # Prevent negative scores that might lead to selection of invalid bins\n    priority_scores[priority_scores < 0] = 0\n    \n    return priority_scores",
    "response_id": 6,
    "obj": 4.028719585161557,
    "SLOC": 25.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response7.txt_stdout.txt",
    "code_path": "problem_iter12_code7.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on a weighted strategy that considers normalized\n    remaining capacities, penalizes unused bins, penalizes bins where the item\n    cannot fit, and boosts bins where the item fits well with consideration of gaps.\n    It also ensures a balance by normalizing scores by the item size and incorporating a penalty for unused bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    total_capacity = np.sum(bins_remain_cap)\n    if total_capacity == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / total_capacity\n    \n    # Penalize unused bins (higher penalty for bins with more remaining capacity)\n    unused_bin_penalty = np.exp(-bins_remain_cap / np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 0)\n    \n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well and penalize if not fitting perfectly\n    fit_boost = np.where(available_cap_after >= 0, available_cap_after / bins_remain_cap, 0)\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n    \n    # Consider gaps for good fits\n    gaps = np.where(available_cap_after >= 0, available_cap_after, 0)\n    gap_factor = np.where(gaps > 0, 1 / (1 + gaps), 0)\n    \n    # Weighted strategy: prioritize bins with larger gaps and better fits\n    gap_weight = 0.5\n    fit_weight = 0.5\n    weighted_score = gap_factor * gap_weight + fit_boost * fit_weight\n    \n    # Combine scores: normalize by item size, consider fit boost and fit gaps, apply penalties\n    priority_scores = remain_cap_normalized + weighted_score\n    priority_scores *= unused_bin_penalty\n    priority_scores += non_fit_penalty\n    \n    # Final normalization by item size for balance\n    priority_scores /= (item if item > 0 else 1)\n    \n    # Prevent negative scores that might lead to selection of invalid bins\n    priority_scores[priority_scores < 0] = 0\n    \n    return priority_scores",
    "response_id": 7,
    "obj": 3.490227363382529,
    "SLOC": 25.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response8.txt_stdout.txt",
    "code_path": "problem_iter12_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on a weighted strategy that normalizes remaining capacities,\n    penalizes unused bins using exponential decay, boosts bins where the item fits well,\n    and considers gaps for good fits. It also normalizes by item size and ensures no negative scores.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    total_capacity = np.sum(bins_remain_cap)\n    if total_capacity == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / total_capacity\n    \n    # Penalize unused bins using exponential decay\n    unused_bin_penalty = np.exp(-bins_remain_cap / np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 0)\n    \n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well and penalize if not fitting perfectly\n    fit_boost = np.where(available_cap_after >= 0, available_cap_after / item, 0)\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n    \n    # Consider gaps for good fits\n    gap_factor = np.where(available_cap_after >= 0, 1 / (1 + available_cap_after), 0)\n    \n    # Weighted strategy: prioritize bins with larger gaps and better fits\n    gap_weight = 0.6\n    fit_weight = 0.4\n    weighted_score = gap_factor * gap_weight + fit_boost * fit_weight\n    \n    # Combine scores: normalize by item size, consider fit boost and fit gaps, apply penalties\n    priority_scores = remain_cap_normalized + weighted_score\n    priority_scores *= unused_bin_penalty\n    priority_scores += non_fit_penalty\n    \n    # Final normalization by item size for balance\n    priority_scores /= (item if item > 0 else 1)\n    \n    # Ensure scores remain non-negative\n    priority_scores[priority_scores < 0] = 0\n    \n    return priority_scores",
    "response_id": 8,
    "obj": 85.00199441563623,
    "SLOC": 25.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response9.txt_stdout.txt",
    "code_path": "problem_iter12_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on minimizing gaps by prioritizing bins where the item fits well with\n    minimal leftover capacity. It normalizes penalties effectively, balances weights, and avoids excessive\n    normalizations.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    total_capacity = np.sum(bins_remain_cap)\n    if total_capacity == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / total_capacity\n\n    # Penalize unused bins (higher penalty for bins with less remaining capacity)\n    max_cap = np.max(bins_remain_cap)\n    if max_cap == 0:\n        unused_bin_penalty = np.ones_like(bins_remain_cap)\n    else:\n        unused_bin_penalty = np.exp(-bins_remain_cap / max_cap)\n\n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n\n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n\n    # Boost bins where the item fits well and penalize if not fitting perfectly\n    fit_boost = np.where(available_cap_after >= 0, available_cap_after / bins_remain_cap, 0)\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n\n    # Consider gaps for good fits\n    gap_factor = np.where(available_cap_after >= 0,  available_cap_after / bins_remain_cap, 0)\n\n    # Weighted strategy: prioritize bins with larger gaps and better fits\n    gap_weight = 0.5\n    fit_weight = 0.5\n    weighted_score = (1 - gap_factor) * gap_weight + fit_boost * fit_weight\n\n    # Combine scores: normalize by item size, consider fit boost and fit gaps, apply penalties\n    priority_scores = remain_cap_normalized + weighted_score\n    priority_scores *= unused_bin_penalty\n    priority_scores += non_fit_penalty\n\n    # Final normalization by item size for balance, ensuring no division by zero\n    priority_scores /= (item if item > 0 else 1)\n\n    # Prevent negative scores that might lead to selection of invalid bins\n    priority_scores[priority_scores < 0] = 0\n\n    return priority_scores",
    "response_id": 9,
    "obj": 4.048663741523748,
    "SLOC": 25.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter13_response0.txt_stdout.txt",
    "code_path": "problem_iter13_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on a weighted strategy that considers normalized\n    remaining capacities, penalizes unused bins, and balances fit boosts with gap penalties.\n    It ensures a balance by normalizing scores by the item size and avoids negative scores.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    total_capacity = np.sum(bins_remain_cap)\n    if total_capacity == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / total_capacity\n    \n    # Penalize unused bins (higher penalty for bins with less remaining capacity)\n    max_cap = np.max(bins_remain_cap)\n    unused_bin_penalty = np.exp(-bins_remain_cap / max_cap) if max_cap > 0 else np.ones_like(bins_remain_cap)\n    \n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well and penalize if not fitting perfectly\n    fit_boost = np.where(available_cap_after >= 0, available_cap_after / bins_remain_cap, 0)\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n    \n    # Consider gaps for good fits\n    gap_factor = np.where(available_cap_after >= 0, min(1, available_cap_after / item), 0)\n    \n    # Weighted strategy: prioritize bins with larger gaps and better fits\n    gap_weight = 0.5\n    fit_weight = 0.5\n    weighted_score = gap_factor * gap_weight + fit_boost * fit_weight\n    \n    # Combine scores: normalize by item size, consider fit boost and fit gaps, apply penalties\n    priority_scores = remain_cap_normalized + weighted_score\n    priority_scores *= unused_bin_penalty\n    priority_scores += non_fit_penalty\n    \n    # Normalize by item size for balance\n    priority_scores /= (item if item > 0 else 1)\n    \n    # Prevent negative scores that might lead to selection of invalid bins\n    priority_scores[priority_scores < 0] = 0\n    \n    return priority_scores",
    "response_id": 0,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 42, in priority_v2\n    gap_weight = 0.5\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n18\n3\n"
  },
  {
    "stdout_filepath": "problem_iter13_response1.txt_stdout.txt",
    "code_path": "problem_iter13_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on a weighted strategy that considers normalized\n    remaining capacities, penalizes unused bins, and boosts bins where the item\n    fits well with consideration of gaps. It ensures a balance by normalizing scores\n    by the item size and avoids negative scores.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    total_capacity = np.sum(bins_remain_cap)\n    remain_cap_normalized = bins_remain_cap / total_capacity if total_capacity > 0 else np.zeros_like(bins_remain_cap)\n    \n    # Penalize unused bins (higher penalty for bins with less remaining capacity)\n    unused_bin_penalty = np.exp(-bins_remain_cap / np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 0)\n    \n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well\n    fit_boost = np.where(available_cap_after >= 0, 1 - available_cap_after / bins_remain_cap, 0)\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n    \n    # Consider gaps for good fits\n    gap_factor = np.where(available_cap_after >= 0, 1 / (1 + available_cap_after), 0)\n\n    # Weighted strategy: prioritize bins with larger gaps and better fits\n    gap_weight = 0.5\n    fit_weight = 0.5\n    weighted_score = gap_factor * gap_weight + fit_boost * fit_weight\n\n    # Combine scores: normalize by item size, consider fit boost and fit gaps, apply penalties\n    priority_scores = remain_cap_normalized + weighted_score\n    priority_scores *= unused_bin_penalty\n    priority_scores += non_fit_penalty\n\n    # Final normalization by item size for balance\n    priority_scores /= item if item > 0 else 1\n    \n    # Prevent negative scores that might lead to selection of invalid bins\n    priority_scores[priority_scores < 0] = 0\n\n    return priority_scores",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter13_response2.txt_stdout.txt",
    "code_path": "problem_iter13_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on a weighted strategy that considers normalized\n    remaining capacities, uses exponential decay for unused bins, and balances fit\n    boosts with gap penalties. It ensures a balance by normalizing scores by the\n    item size and avoids negative scores.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    total_capacity = np.sum(bins_remain_cap)\n    if total_capacity == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / total_capacity\n    \n    # Penalize unused bins using exponential decay\n    max_cap = np.max(bins_remain_cap)\n    unused_bin_penalty = np.exp(-bins_remain_cap / max_cap) if max_cap > 0 else 1\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(bins_remain_cap - item < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well\n    fit_boost = np.where(bins_remain_cap - item >= 0, 1 - (bins_remain_cap - item) / bins_remain_cap, 0)\n    \n    # Calculate gap factor\n    gap_factor = np.where(bins_remain_cap - item >= 0, 1 / (1 + bins_remain_cap - item), 0)\n    \n    # Weighted score combining fit boost and gap factor\n    weighted_score = 0.5 * fit_boost + 0.5 * gap_factor\n    \n    # Combine scores considering normalized capacity, gaps, and fit\n    priority_scores = remain_cap_normalized + weighted_score\n    priority_scores *= unused_bin_penalty\n    priority_scores += non_fit_penalty\n    \n    # Normalize scores by item size to balance weights\n    priority_scores /= (item if item > 0 else 1)\n    \n    # Ensure no negative scores\n    priority_scores = np.maximum(priority_scores, 0)\n    \n    return priority_scores",
    "response_id": 2,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter13_response3.txt_stdout.txt",
    "code_path": "problem_iter13_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on a weighted strategy that considers normalized\n    remaining capacities, slightly penalizes unused bins, penalizes bins where the item\n    cannot fit, and boosts bins where the item fits well with consideration of gaps.\n    It ensures a balance by normalizing scores by the item size and avoids negative scores.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    total_capacity = np.sum(bins_remain_cap)\n    if total_capacity == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / total_capacity\n    \n    # Mild penalty for unused bins (higher penalty for bins with more remaining capacity)\n    unused_bin_penalty = np.exp(-bins_remain_cap / total_capacity) if total_capacity > 0 else np.ones_like(bins_remain_cap)\n    \n    # Assign zero priority to bins where the item cannot fit\n    can_fit_mask = bins_remain_cap >= item\n    non_fit_penalty = np.where(can_fit_mask, 0, -np.inf)\n    \n    # Boost bins where the item fits well and penalize if not fitting perfectly\n    fit_boost = np.where(can_fit_mask, bins_remain_cap - item, 0) / bins_remain_cap\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n    \n    # Consider gaps for good fits\n    gap_factor = np.where(can_fit_mask, 1 / (1 + (bins_remain_cap - item)), 0)\n    \n    # Weighted strategy: prioritize bins with larger gaps and better fits\n    gap_weight = 0.5\n    fit_weight = 0.5\n    weighted_score = gap_factor * gap_weight + fit_boost * fit_weight\n    \n    # Combine scores: prioritize bins with larger gaps and better fits\n    priority_scores = remain_cap_normalized + weighted_score\n    \n    # Apply unused bin penalty\n    priority_scores *= unused_bin_penalty\n    \n    # Add non fit penalties\n    priority_scores += non_fit_penalty\n    \n    # Final normalization by item size for balance\n    priority_scores /= (item if item > 0 else 1)\n    \n    # Prevent negative scores that might lead to selection of invalid bins\n    priority_scores[priority_scores < 0] = 0\n    \n    return priority_scores",
    "response_id": 3,
    "obj": 86.27842042281613,
    "SLOC": 18.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter13_response4.txt_stdout.txt",
    "code_path": "problem_iter13_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on a weighted strategy that considers normalized\n    remaining capacities, balances fit boosts with gap penalties, avoids negative scores,\n    and simplifies logic.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate normalized remaining capacity\n    total_capacity = np.sum(bins_remain_cap)\n    if total_capacity == 0:\n        remain_cap_normalized = np.zeros_like(bins_remain_cap)\n    else:\n        remain_cap_normalized = bins_remain_cap / total_capacity\n    \n    # Calculate available capacity after placing the item\n    available_cap_after = bins_remain_cap - item\n    \n    # Assign zero priority to bins where the item cannot fit\n    non_fit_penalty = np.where(available_cap_after < 0, -np.inf, 0)\n    \n    # Boost bins where the item fits well and penalize if not fitting perfectly\n    fit_boost = np.where(available_cap_after >= 0, 1 - (available_cap_after / bins_remain_cap), 0)\n    fit_boost = np.nan_to_num(fit_boost, posinf=0, neginf=0)\n    \n    # Consider gaps for good fits\n    gap_factor = np.where(available_cap_after >= 0, available_cap_after / bins_remain_cap, 0)\n    \n    # Weighted strategy: prioritize bins with larger gaps and better fits\n    gap_weight = 0.5\n    fit_weight = 0.5\n    weighted_score = gap_factor * gap_weight + fit_boost * fit_weight\n    \n    # Normalize by item size for balance\n    weighted_score /= (item if item > 0 else 1)\n    \n    # Combine scores: consider normalized remaining capacity, fit boost, and fit gaps\n    priority_scores = remain_cap_normalized + weighted_score\n    \n    # Final check to avoid negative scores\n    priority_scores[priority_scores < 0] = 0\n    \n    return priority_scores",
    "response_id": 4,
    "obj": 149.30195452732352,
    "SLOC": 18.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  }
]