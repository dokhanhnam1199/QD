{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A more sophisticated heuristics function for the Traveling Salesman Problem (TSP).\n\n    This version combines several ideas inspired by physics and common-sense heuristics\n    to provide more informative edge priors.  It aims to balance exploration\n    (allowing for non-obvious edges) with exploitation (favoring short, promising edges).\n\n    Specifically, it uses:\n    1.  Inverse distance: Shorter edges are generally more desirable.\n    2.  Node degree desirability: Nodes with fewer short connections are made more desirable\n        as endpoints of edges.  This encourages exploring parts of the graph that\n        are less well-connected initially. This simulates a sort of 'attractive force'.\n    3.  Distance normalization: The 'temperature' variable adjusts how strongly we adhere to\n        short distances. At higher temperatures, we're more willing to explore longer edges.\n\n    Args:\n        distance_matrix (np.ndarray): The distance matrix representing the distances\n                                         between cities.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, representing the\n                     prior probabilities of including each edge in a solution. Higher\n                     values indicate a higher prior probability.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n    temperature = np.mean(distance_matrix) / 2  # Adjust as needed for optimal performance\n\n    # Inverse distance, but avoid division by zero\n    inverse_distance = 1.0 / (distance_matrix + np.eye(n))\n\n    # Node degree desirability (attractiveness)\n    node_attractiveness = np.sum(inverse_distance, axis=0)\n    node_attractiveness = 1.0 / (node_attractiveness / np.mean(node_attractiveness)) #Inverse normalized attractiveness to drive toward \"isolated\" nodes\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = (inverse_distance[i, j]**2) * (node_attractiveness[i] * node_attractiveness[j]) * np.exp(-distance_matrix[i, j] / temperature)\n\n    return heuristics\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Enhanced heuristics for the Traveling Salesman Problem, inspired by radioactive decay principles.\n\n    This version combines inverse distance with a normalized rank-based approach and a probabilistic factor\n    to favor shorter distances and prevent getting stuck in local optima.  It considers edge 'half-life',\n    a concept from radioactivity, where shorter edges 'decay' slower (are more promising).\n\n    Args:\n        distance_matrix (np.ndarray): A square matrix where distance_matrix[i, j] is the distance\n                                         between city i and city j. Diagonal elements are ignored\n                                         and often contain 0 or np.inf.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, representing the heuristic values\n                      for each edge. Higher values indicate more promising edges.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n\n    # Handle potential division by zero by adding a small epsilon\n    epsilon = 1e-9\n    distance_matrix = distance_matrix + epsilon\n\n    # 1. Inverse Distance (Basic Attraction): Shorter distances are initially more attractive\n    inverse_distance = 1 / distance_matrix\n\n    # 2. Rank-Based Normalization:  Emphasize relative edge importance, independent of scale.\n    #    For each city, rank the edges by distance and normalize these ranks.\n\n    rank_matrix = np.zeros((n, n))\n    for i in range(n):\n        distances = distance_matrix[i, :]  # Distances from city i to all others\n        ranks = np.argsort(distances) # indices sorted by increasing distance.\n\n        # Store indices to efficiently populate the rank_matrix\n        for j, r in enumerate(ranks):\n           rank_matrix[i, r] = (n - j) / n   # Shorter edges get higher rank (close to 1)\n\n    # 3.  Radioactive Decay Analogy (Edge Half-Life): Favor shorter edges using a probabilistic component\n    #     Simulate half-life decay based on distance.  Shorter edges 'decay' slower, remain promising longer.\n\n    decay_factor = np.exp(-distance_matrix / np.mean(distance_matrix))  # Shorter edges decay slower (higher value).\n\n    # 4. Combined Heuristic:  Balance attraction, relative importance, and half-life.\n    heuristic_matrix = inverse_distance * rank_matrix * decay_factor\n\n    # Optional: Enhance exploitation by emphasizing the edges with already higher chances\n\n    #Normalize between 0-1:\n    min_val = np.min(heuristic_matrix)\n    max_val = np.max(heuristic_matrix)\n    heuristic_matrix = (heuristic_matrix - min_val) / (max_val - min_val)\n\n    return heuristic_matrix\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Ada Lovelace's Heuristic for TSP.\n\n    This heuristic combines several strategies to estimate the desirability of including each edge in the final TSP tour.\n\n    1.  Inverse Distance:  Shorter distances are initially favored.\n    2.  Nearest Neighbor Influence: Edges connecting nodes to their nearest neighbors are prioritized.\n    3.  Avoidance of Long Edges:  Penalizes long edges by raising the inverse distance to a power.\n    4.  Start/End Point Bias: Heuristically encourage the solutions to go to or back from specific start/end nodes.\n        This may speed up the algorithm given a fixed start point.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # 1. Inverse Distance\n    heuristics = 1.0 / (distance_matrix + 1e-9) # Adding a small value to avoid division by zero\n    # 2. Nearest Neighbor Influence\n    for i in range(n):\n        #Find closest neighbor for node i, excluding self-loops.\n\n        temp_distances = distance_matrix[i].copy()\n        temp_distances[i] = np.inf # Ensure that the node is not selected as its own nearest neighbor.\n\n        nearest_neighbor = np.argmin(temp_distances) # argmin returns the index of the smallest value\n\n        heuristics[i, nearest_neighbor] *= 2.0 # Boost the priority for connections to the nearest neighbor.\n        heuristics[nearest_neighbor, i] *= 2.0 # Ensure symmetry of heuristics.\n\n    # 3. Avoidance of Long Edges\n    heuristics = heuristics ** 1.5 # Further amplify the priority of shorter edges\n\n    # 4. Start/End point bias: assume index 0 is the start point\n    heuristics[0, :] *= 1.2\n    heuristics[:, 0] *= 1.2\n\n    #Ensure no inf values\n    heuristics[np.isinf(heuristics)] = 0\n    return heuristics\n\n[Heuristics 4th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Tesla's Electrifying Heuristic for the Traveling Salesman!\n\n    This heuristic combines several ingenious insights:\n\n    1.  Inverse Distance:  Nodes closer together are naturally more promising.\n\n    2.  Gravity-Inspired Attraction:  Nodes exert a \"gravitational\" pull\n        proportional to the inverse square of their distance, enhancing\n        the attraction of closer nodes.  We use inverse-square for the \"attractive force\", but avoid 0 division.\n\n    3.  Global Connectivity Boost:  A small constant is added to ensure\n        all edges have a non-zero probability, encouraging global\n        exploration and preventing premature convergence.\n\n    4.  Local Optimization Influence: A factor based on the row and column means is applied to give edges connected to nodes with higher average proximity a slight preference.\n\n    Args:\n        distance_matrix (np.ndarray): A square matrix where distance_matrix[i, j]\n                                       represents the distance between node i and node j.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, where each\n                    element represents the heuristic score for including that edge\n                    in the TSP tour.  Higher values indicate a more promising edge.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9  # A small constant to avoid division by zero\n\n    # 1. Inverse Distance\n    inverse_distance = 1 / (distance_matrix + epsilon)\n\n    # 2. Gravity-Inspired Attraction\n    gravitational_attraction = 1 / ((distance_matrix**2) + epsilon)\n\n    # 3. Global Connectivity Boost\n    boost_factor = 0.1\n    global_connectivity = np.ones((n, n)) * boost_factor\n\n    # 4. Local Optimization Influence\n    row_means = np.mean(inverse_distance, axis=1, keepdims=True)\n    col_means = np.mean(inverse_distance, axis=0, keepdims=True)\n    local_influence = np.sqrt(row_means * col_means) # Geometric mean\n\n    # Combine the heuristics with carefully chosen weights\n    heuristic_matrix = (\n        0.6 * inverse_distance +\n        0.3 * gravitational_attraction +\n        0.05 * global_connectivity +\n        0.05 * local_influence\n    )\n\n    return heuristic_matrix\n\n[Heuristics 5th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Tesla's Electrifying Heuristic for the Traveling Salesman!\n\n    This heuristic combines several ingenious insights:\n\n    1.  Inverse Distance:  Nodes closer together are naturally more promising.\n\n    2.  Gravity-Inspired Attraction:  Nodes exert a \"gravitational\" pull\n        proportional to the inverse square of their distance, enhancing\n        the attraction of closer nodes.  We use inverse-square for the \"attractive force\", but avoid 0 division.\n\n    3.  Global Connectivity Boost:  A small constant is added to ensure\n        all edges have a non-zero probability, encouraging global\n        exploration and preventing premature convergence.\n\n    4.  Local Optimization Influence: A factor based on the row and column means is applied to give edges connected to nodes with higher average proximity a slight preference.\n\n    Args:\n        distance_matrix (np.ndarray): A square matrix where distance_matrix[i, j]\n                                       represents the distance between node i and node j.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, where each\n                    element represents the heuristic score for including that edge\n                    in the TSP tour.  Higher values indicate a more promising edge.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9  # A small constant to avoid division by zero\n\n    # 1. Inverse Distance\n    inverse_distance = 1 / (distance_matrix + epsilon)\n\n    # 2. Gravity-Inspired Attraction\n    gravitational_attraction = 1 / ((distance_matrix**2) + epsilon)\n\n    # 3. Global Connectivity Boost\n    boost_factor = 0.1\n    global_connectivity = np.ones((n, n)) * boost_factor\n\n    # 4. Local Optimization Influence\n    row_means = np.mean(inverse_distance, axis=1, keepdims=True)\n    col_means = np.mean(inverse_distance, axis=0, keepdims=True)\n    local_influence = np.sqrt(row_means * col_means) # Geometric mean\n\n    # Combine the heuristics with carefully chosen weights\n    heuristic_matrix = (\n        0.6 * inverse_distance +\n        0.3 * gravitational_attraction +\n        0.05 * global_connectivity +\n        0.05 * local_influence\n    )\n\n    return heuristic_matrix\n\n[Heuristics 6th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Tesla's Electrifying Heuristic for the Traveling Salesman!\n\n    This heuristic combines several ingenious insights:\n\n    1.  Inverse Distance:  Nodes closer together are naturally more promising.\n\n    2.  Gravity-Inspired Attraction:  Nodes exert a \"gravitational\" pull\n        proportional to the inverse square of their distance, enhancing\n        the attraction of closer nodes.  We use inverse-square for the \"attractive force\", but avoid 0 division.\n\n    3.  Global Connectivity Boost:  A small constant is added to ensure\n        all edges have a non-zero probability, encouraging global\n        exploration and preventing premature convergence.\n\n    4.  Local Optimization Influence: A factor based on the row and column means is applied to give edges connected to nodes with higher average proximity a slight preference.\n\n    Args:\n        distance_matrix (np.ndarray): A square matrix where distance_matrix[i, j]\n                                       represents the distance between node i and node j.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, where each\n                    element represents the heuristic score for including that edge\n                    in the TSP tour.  Higher values indicate a more promising edge.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9  # A small constant to avoid division by zero\n\n    # 1. Inverse Distance\n    inverse_distance = 1 / (distance_matrix + epsilon)\n\n    # 2. Gravity-Inspired Attraction\n    gravitational_attraction = 1 / ((distance_matrix**2) + epsilon)\n\n    # 3. Global Connectivity Boost\n    boost_factor = 0.1\n    global_connectivity = np.ones((n, n)) * boost_factor\n\n    # 4. Local Optimization Influence\n    row_means = np.mean(inverse_distance, axis=1, keepdims=True)\n    col_means = np.mean(inverse_distance, axis=0, keepdims=True)\n    local_influence = np.sqrt(row_means * col_means) # Geometric mean\n\n    # Combine the heuristics with carefully chosen weights\n    heuristic_matrix = (\n        0.6 * inverse_distance +\n        0.3 * gravitational_attraction +\n        0.05 * global_connectivity +\n        0.05 * local_influence\n    )\n\n    return heuristic_matrix\n\n[Heuristics 7th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Tesla's Electrifying Heuristic for the Traveling Salesman!\n\n    This heuristic combines several ingenious insights:\n\n    1.  Inverse Distance:  Nodes closer together are naturally more promising.\n\n    2.  Gravity-Inspired Attraction:  Nodes exert a \"gravitational\" pull\n        proportional to the inverse square of their distance, enhancing\n        the attraction of closer nodes.  We use inverse-square for the \"attractive force\", but avoid 0 division.\n\n    3.  Global Connectivity Boost:  A small constant is added to ensure\n        all edges have a non-zero probability, encouraging global\n        exploration and preventing premature convergence.\n\n    4.  Local Optimization Influence: A factor based on the row and column means is applied to give edges connected to nodes with higher average proximity a slight preference.\n\n    Args:\n        distance_matrix (np.ndarray): A square matrix where distance_matrix[i, j]\n                                       represents the distance between node i and node j.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, where each\n                    element represents the heuristic score for including that edge\n                    in the TSP tour.  Higher values indicate a more promising edge.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9  # A small constant to avoid division by zero\n\n    # 1. Inverse Distance\n    inverse_distance = 1 / (distance_matrix + epsilon)\n\n    # 2. Gravity-Inspired Attraction\n    gravitational_attraction = 1 / ((distance_matrix**2) + epsilon)\n\n    # 3. Global Connectivity Boost\n    boost_factor = 0.1\n    global_connectivity = np.ones((n, n)) * boost_factor\n\n    # 4. Local Optimization Influence\n    row_means = np.mean(inverse_distance, axis=1, keepdims=True)\n    col_means = np.mean(inverse_distance, axis=0, keepdims=True)\n    local_influence = np.sqrt(row_means * col_means) # Geometric mean\n\n    # Combine the heuristics with carefully chosen weights\n    heuristic_matrix = (\n        0.6 * inverse_distance +\n        0.3 * gravitational_attraction +\n        0.05 * global_connectivity +\n        0.05 * local_influence\n    )\n\n    return heuristic_matrix\n\n[Heuristics 8th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Tesla's Electrifying Heuristic for the Traveling Salesman!\n\n    This heuristic combines several ingenious insights:\n\n    1.  Inverse Distance:  Nodes closer together are naturally more promising.\n\n    2.  Gravity-Inspired Attraction:  Nodes exert a \"gravitational\" pull\n        proportional to the inverse square of their distance, enhancing\n        the attraction of closer nodes.  We use inverse-square for the \"attractive force\", but avoid 0 division.\n\n    3.  Global Connectivity Boost:  A small constant is added to ensure\n        all edges have a non-zero probability, encouraging global\n        exploration and preventing premature convergence.\n\n    4.  Local Optimization Influence: A factor based on the row and column means is applied to give edges connected to nodes with higher average proximity a slight preference.\n\n    Args:\n        distance_matrix (np.ndarray): A square matrix where distance_matrix[i, j]\n                                       represents the distance between node i and node j.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, where each\n                    element represents the heuristic score for including that edge\n                    in the TSP tour.  Higher values indicate a more promising edge.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9  # A small constant to avoid division by zero\n\n    # 1. Inverse Distance\n    inverse_distance = 1 / (distance_matrix + epsilon)\n\n    # 2. Gravity-Inspired Attraction\n    gravitational_attraction = 1 / ((distance_matrix**2) + epsilon)\n\n    # 3. Global Connectivity Boost\n    boost_factor = 0.1\n    global_connectivity = np.ones((n, n)) * boost_factor\n\n    # 4. Local Optimization Influence\n    row_means = np.mean(inverse_distance, axis=1, keepdims=True)\n    col_means = np.mean(inverse_distance, axis=0, keepdims=True)\n    local_influence = np.sqrt(row_means * col_means) # Geometric mean\n\n    # Combine the heuristics with carefully chosen weights\n    heuristic_matrix = (\n        0.6 * inverse_distance +\n        0.3 * gravitational_attraction +\n        0.05 * global_connectivity +\n        0.05 * local_influence\n    )\n\n    return heuristic_matrix\n\n[Heuristics 9th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Generates heuristic indicators for TSP edges based on a combination of distance,\n    nearest neighbors, and global average distance.\n\n    Args:\n        distance_matrix (np.ndarray): A square matrix where distance_matrix[i, j]\n            is the distance between node i and node j.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix,\n            containing heuristic indicators for each edge. Higher values indicate\n            more promising edges. Diagonal elements will be 0.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # Calculate nearest neighbors for each node\n    nearest_neighbors = np.argsort(distance_matrix, axis=1)[:, 1:4]  # Exclude self\n    \n    # Calculate global average distance\n    avg_distance = np.mean(distance_matrix[np.triu_indices_from(distance_matrix, k=1)])\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                distance = distance_matrix[i, j]\n\n                # Heuristic component 1: Inverse distance (basic desirability)\n                heuristic_value = 1 / (distance + 1e-6) #add a small number to avoid dividing by zero.\n\n                # Heuristic component 2: Nearest neighbor bonus\n                if j in nearest_neighbors[i]:\n                    heuristic_value += 0.5  # Bonus for being a nearest neighbor\n                    #Further boost if i is also j's nearest neighbour.\n                    nearest_neighbours_of_j = np.argsort(distance_matrix, axis=1)[:, 1:4][j]\n                    if i in nearest_neighbours_of_j:\n                        heuristic_value += 0.2\n\n                # Heuristic component 3: Encourage selection if distance is smaller than global average distance.\n                if distance < avg_distance:\n                    heuristic_value += 0.2\n                \n\n                heuristics[i, j] = heuristic_value\n                \n    return heuristics\n\n[Heuristics 10th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Stephen Hawking's Heuristic for TSP, inspired by black hole physics and stochastic sampling.\n\n    This heuristic aims to guide TSP solvers by creating a 'gravitational potential'\n    based on edge lengths, node degree (proximity to other nodes), and a touch\n    of simulated Hawking radiation for exploration.\n\n    Args:\n        distance_matrix (np.ndarray):  A square, symmetric matrix where distance_matrix[i, j]\n                                     is the distance between node i and node j. Diagonal elements\n                                     (distance_matrix[i, i]) should be zero.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, representing\n                    the heuristic values (probabilities or weights) for including each edge\n                    in the TSP tour. Higher values indicate a more promising edge.\n    \"\"\"\n\n    n = distance_matrix.shape[0]  # Number of nodes\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Node Degree (Proximity): Nodes close to many other nodes are likely\n    # to be good connectors in the optimal tour.\n    node_degrees = np.sum(1 / (distance_matrix + np.eye(n)), axis=1)  # Add eye to prevent division by zero\n    degree_matrix = np.tile(node_degrees, (n, 1)) + np.tile(node_degrees, (n, 1)).T\n\n    # Inverse Distance (Gravity):  Shorter distances are more attractive.\n    inverse_distance = 1 / (distance_matrix + np.eye(n))\n\n    # Hawking Radiation (Exploration):  Introduce randomness, especially for longer edges\n    # to prevent premature convergence.  Simulate thermal fluctuations.\n    temperature = np.mean(distance_matrix)  # Scale temperature by average distance\n    hawking_radiation = np.random.normal(0, temperature / (distance_matrix + np.eye(n)), size=(n, n))\n    hawking_radiation = np.abs(hawking_radiation)  # Ensure positive contribution\n\n    # Combine factors:\n    heuristic_matrix = inverse_distance * degree_matrix + hawking_radiation\n    heuristic_matrix = (heuristic_matrix + heuristic_matrix.T)/2 #Ensure symmetry\n    heuristic_matrix = np.nan_to_num(heuristic_matrix, nan=0.0)\n    # Normalize (optional, but often helpful for stochastic sampling)\n    heuristic_matrix = (heuristic_matrix - np.min(heuristic_matrix)) / (np.max(heuristic_matrix) - np.min(heuristic_matrix) + 1e-9) #normalizes to 0 to 1\n\n    return heuristic_matrix\n\n[Heuristics 11th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Stephen Hawking's Heuristic for TSP, inspired by black hole physics and stochastic sampling.\n\n    This heuristic aims to guide TSP solvers by creating a 'gravitational potential'\n    based on edge lengths, node degree (proximity to other nodes), and a touch\n    of simulated Hawking radiation for exploration.\n\n    Args:\n        distance_matrix (np.ndarray):  A square, symmetric matrix where distance_matrix[i, j]\n                                     is the distance between node i and node j. Diagonal elements\n                                     (distance_matrix[i, i]) should be zero.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, representing\n                    the heuristic values (probabilities or weights) for including each edge\n                    in the TSP tour. Higher values indicate a more promising edge.\n    \"\"\"\n\n    n = distance_matrix.shape[0]  # Number of nodes\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Node Degree (Proximity): Nodes close to many other nodes are likely\n    # to be good connectors in the optimal tour.\n    node_degrees = np.sum(1 / (distance_matrix + np.eye(n)), axis=1)  # Add eye to prevent division by zero\n    degree_matrix = np.tile(node_degrees, (n, 1)) + np.tile(node_degrees, (n, 1)).T\n\n    # Inverse Distance (Gravity):  Shorter distances are more attractive.\n    inverse_distance = 1 / (distance_matrix + np.eye(n))\n\n    # Hawking Radiation (Exploration):  Introduce randomness, especially for longer edges\n    # to prevent premature convergence.  Simulate thermal fluctuations.\n    temperature = np.mean(distance_matrix)  # Scale temperature by average distance\n    hawking_radiation = np.random.normal(0, temperature / (distance_matrix + np.eye(n)), size=(n, n))\n    hawking_radiation = np.abs(hawking_radiation)  # Ensure positive contribution\n\n    # Combine factors:\n    heuristic_matrix = inverse_distance * degree_matrix + hawking_radiation\n    heuristic_matrix = (heuristic_matrix + heuristic_matrix.T)/2 #Ensure symmetry\n    heuristic_matrix = np.nan_to_num(heuristic_matrix, nan=0.0)\n    # Normalize (optional, but often helpful for stochastic sampling)\n    heuristic_matrix = (heuristic_matrix - np.min(heuristic_matrix)) / (np.max(heuristic_matrix) - np.min(heuristic_matrix) + 1e-9) #normalizes to 0 to 1\n\n    return heuristic_matrix\n\n[Heuristics 12th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Heuristic function for the Traveling Salesman Problem (TSP) based on\n    a combination of inverse distance and a nearest neighbor component.\n\n    Args:\n        distance_matrix (np.ndarray): A square matrix where distance_matrix[i][j]\n                                      represents the distance between city i and city j.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, where each\n                     element represents a heuristic value indicating the\n                     desirability of including the corresponding edge in the TSP tour.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Inverse distance component (as in v1)\n    inverse_distance = 1.0 / (distance_matrix + 1e-9)  # Adding a small constant to avoid division by zero\n\n    # Nearest neighbor component\n    for i in range(n):\n        # Find the nearest neighbors for each city\n        nearest_neighbors = np.argsort(distance_matrix[i, :])[1:4]  # Exclude itself, take top 3\n\n        for j in range(n):\n            if i != j:\n                #Boost heuristic value if j is one of the nearest neighbors of i\n                if j in nearest_neighbors:\n                     heuristic_matrix[i, j] += 0.5 # increase the prob for edges towards NN\n                heuristic_matrix[i, j] += inverse_distance[i, j]\n\n    # Normalize the heuristic values to be between 0 and 1\n    max_heuristic = np.max(heuristic_matrix)\n    min_heuristic = np.min(heuristic_matrix)\n\n    if max_heuristic > min_heuristic:\n        heuristic_matrix = (heuristic_matrix - min_heuristic) / (max_heuristic - min_heuristic)\n    else:\n        heuristic_matrix = np.ones_like(heuristic_matrix) # all the same values, meaning no prior knowledge\n\n    return heuristic_matrix\n\n[Heuristics 13th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Tesla's Electrifying TSP Heuristic - Version 2: Electromagnetically Enhanced Edge Evaluation.\n\n    This function evaluates the promise of each edge in a Traveling Salesman Problem\n    by considering not only the direct distance but also the context of neighboring nodes.\n    Edges connected to nodes with long average distances to others are penalized,\n    reflecting the idea that these nodes are likely to be on the periphery of the optimal path.\n\n    The heuristic leverages a combination of inverse distance (attraction) and a node centrality measure\n    based on average distance to all other nodes (repulsion).\n\n    Args:\n        distance_matrix (np.ndarray): A square matrix representing the distances between nodes.\n                                         distance_matrix[i][j] is the distance from node i to node j.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, containing heuristic values\n                      indicating the promise of each edge. Higher values indicate a more promising edge.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Calculate average distance from each node to all other nodes.\n    node_centrality = np.mean(distance_matrix, axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                # Base attraction: Inverse of the distance (shorter distances are more attractive).\n                attraction = 1 / distance_matrix[i, j]\n\n                # Node centrality penalty (nodes far from center penalized).\n                # Applying a normalized exponential penalty based on the centrality of both nodes.\n\n                penalty = np.exp(-0.5 * (node_centrality[i] + node_centrality[j]) / np.mean(node_centrality)) # Normalize by mean\n\n                heuristic_matrix[i, j] = attraction * penalty\n            else:\n                heuristic_matrix[i, j] = 0  # No self-loops\n\n    return heuristic_matrix\n\n[Heuristics 14th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Heuristic for Traveling Salesman Problem (TSP) edge selection.\n    Combines distance-based weighting with node degree and randomness.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # Inverse distance component (close cities are generally good)\n    inverse_distance = 1.0 / (distance_matrix + 1e-6)  # Add small value to avoid division by zero\n\n    # Node degree component (discourage high degree nodes early on)\n    # Initially all nodes have same potential.  Modify based on edge selection.\n\n    # Randomness component (explore diverse solutions).  Scale with problem size\n    randomness = np.random.rand(n, n) * (1.0 / n)  # Scaled randomness\n    randomness = (randomness + randomness.T)/2 #Make sure it is symmetric\n\n    # Initial Heuristics: weighted combination of inverse distance and randomness\n    heuristics = 0.7 * inverse_distance + 0.3 * randomness\n\n    # Zero out the diagonals.  Traveling from node i to node i isn't allowed.\n    np.fill_diagonal(heuristics, 0)\n\n    return heuristics\n\n[Heuristics 15th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Einstein's heuristic for the Traveling Salesman Problem (TSP).\n\n    This function calculates a heuristic matrix based on a combination of\n    distance, nearest neighbors, and a dash of randomness inspired by\n    Brownian motion. The goal is to guide stochastic TSP solvers towards\n    promising edges.\n\n    Args:\n        distance_matrix (np.ndarray): A square matrix representing the distances\n                                       between cities. distance_matrix[i, j] is the\n                                       distance between city i and city j.\n\n    Returns:\n        np.ndarray: A heuristic matrix of the same shape as the distance matrix.\n                      Higher values indicate more promising edges.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # 1. Inverse distance (shorter is better)\n    heuristic_matrix = 1.0 / (distance_matrix + 1e-9)  # Add small value to avoid division by zero\n\n    # 2. Nearest Neighbor bonus: Cities close to many others get a boost.\n    nearest_neighbors_bonus = np.zeros(n)\n    for i in range(n):\n        # Find the k nearest neighbors (excluding itself)\n        distances = distance_matrix[i, :]\n        nearest_neighbors_indices = np.argsort(distances)[1:6] # Top 5 nearest\n        nearest_neighbors_bonus[i] = np.sum(1.0 / distances[nearest_neighbors_indices])\n\n\n    #  incorporate into heuristic matrix. Cities with short distances from\n    # the current one and high neighbor bonuses receive strong heuristic value\n\n    for i in range(n):\n      for j in range(n):\n        if i != j:\n           heuristic_matrix[i,j] = heuristic_matrix[i,j] * (nearest_neighbors_bonus[i] + nearest_neighbors_bonus[j])\n\n\n    # 3. Introduce a bit of \"Brownian Motion\" (randomness)\n    randomness = np.random.rand(n, n) * 0.1\n    heuristic_matrix += randomness\n\n    # Normalize the heuristic matrix to values between 0 and 1 (optional, but good practice)\n    heuristic_matrix = (heuristic_matrix - np.min(heuristic_matrix)) / (np.max(heuristic_matrix) - np.min(heuristic_matrix))\n\n\n    return heuristic_matrix\n\n[Heuristics 16th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Heuristics for TSP based on a combination of distance, node degree,\n    and a stochastic component to encourage exploration.\n\n    Args:\n        distance_matrix: A numpy array representing the distance matrix between cities.\n\n    Returns:\n        A numpy array of the same shape as distance_matrix, representing the\n        heuristic values for each edge. Higher values indicate more promising edges.\n    \"\"\"\n    n = distance_matrix.shape[0]\n\n    # Avoid division by zero and self-loops by setting diagonal to infinity\n    temp_matrix = distance_matrix.copy()\n    np.fill_diagonal(temp_matrix, np.inf)\n\n    # 1. Inverse Distance:  Shorter distances are generally better.\n    inverse_distance = 1 / temp_matrix\n\n    # 2. Node Degree Preference: Prefer edges connected to nodes with fewer connections\n    #    already selected (simulating a form of constraint satisfaction).\n    #    This is approximated using the sum of inverse distances from each node.\n    node_degree_preference = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:  # Avoid self-loops\n                node_degree_preference[i, j] = (np.sum(inverse_distance[i, :]) + np.sum(inverse_distance[j, :]))/2\n\n    # 3. Stochastic Perturbation: Add a small amount of random noise to encourage exploration\n    #    and prevent getting stuck in local optima.  The magnitude of the noise is scaled\n    #    by the mean inverse distance to avoid dominating the signal.\n    mean_inverse_distance = np.mean(inverse_distance[np.isfinite(inverse_distance)])\n    stochastic_perturbation = np.random.normal(0, 0.1 * mean_inverse_distance, size=(n, n))\n\n\n    # 4. Combine the heuristics.  Experiment with weights to adjust the influence of each component.\n    heuristic_matrix = (0.7 * inverse_distance +\n                         0.2 * (1/node_degree_preference) + #inverse as higher node degree indicates less preference\n                         0.1 * stochastic_perturbation)\n    # Set diagonal to zero to avoid self-loops\n    np.fill_diagonal(heuristic_matrix, 0)\n\n    return heuristic_matrix\n\n[Heuristics 17th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Marie Curie's Heuristic for TSP using Stochastic Solution Sampling:\n\n    Combines inverse distance with node connectivity estimates for edge prioritization.\n    This approach favors shorter distances but also considers how \"central\" a node is\n    within the overall graph, making it more likely to be included in a good tour.\n\n    Args:\n        distance_matrix (np.ndarray): A numpy array representing the distance matrix\n                                      where distance_matrix[i][j] is the distance\n                                      between node i and node j.\n\n    Returns:\n        np.ndarray: A numpy array of the same shape as distance_matrix, representing\n                      the heuristic values for each edge.  Higher values indicate\n                      more promising edges.\n    \"\"\"\n    n = distance_matrix.shape[0]\n\n    # Inverse distance as a base heuristic (shorter distances are better).\n    inverse_distance = 1.0 / (distance_matrix + 1e-9)  # Add a small value to avoid division by zero\n\n    # Node connectivity estimate (how well-connected each node is).\n    # We can estimate this based on the sum of inverse distances to other nodes.\n    node_connectivity = np.sum(inverse_distance, axis=1)\n\n    # Normalize node connectivity to a reasonable range.\n    normalized_connectivity = (node_connectivity - np.min(node_connectivity)) / (np.max(node_connectivity) - np.min(node_connectivity) + 1e-9)\n\n\n    # Combine inverse distance with connectivity to prioritize edges.\n    # Edges connecting well-connected nodes and having short distances are favored.\n\n    heuristic_matrix = inverse_distance * (np.tile(normalized_connectivity, (n, 1)).T + np.tile(normalized_connectivity, (n, 1)))/2 # average connectivity of two ends.\n\n    # Further refine based on stochastic sampling results (simulated annealing inspiration).\n    # Initially, we don't have actual sampling results, so we'll simulate some stochastic\n    # influence using random noise. In a real application, this section would be replaced\n    # by updates based on the performance of edges within sampled TSP solutions.\n\n    # Scale heuristics and add some random variation\n    heuristic_matrix = (heuristic_matrix - np.min(heuristic_matrix))/(np.max(heuristic_matrix)-np.min(heuristic_matrix)+1e-9)\n    heuristic_matrix += 0.01 * np.random.rand(n, n)  # Add a small random factor\n    return heuristic_matrix\n\n[Heuristics 18th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Generates heuristics for the Traveling Salesman Problem (TSP) based on stochastic solution sampling principles.\n    This version incorporates several ideas inspired by physics and observations.\n\n    Args:\n        distance_matrix: A numpy array representing the distance between cities.\n                         distance_matrix[i, j] is the distance from city i to city j.\n\n    Returns:\n        A numpy array of the same shape as distance_matrix, representing the desirability of including each edge in the solution.\n        Higher values indicate more desirable edges.\n    \"\"\"\n    n = distance_matrix.shape[0]  # Number of cities\n\n    # 1. Inverse Distance (Gravitational Analogy): Closer cities are more likely to be connected.\n    heuristic_matrix = 1.0 / (distance_matrix + 1e-9)  # Adding a small value to prevent division by zero\n\n    # 2. City Centrality (Hubs): Cities that are \"central\" (e.g., have short average distances to other cities) are more likely to be hubs.\n    city_centrality = np.sum(distance_matrix, axis=0)  # Sum of distances from each city to all other cities (lower is better)\n    city_centrality = 1.0 / (city_centrality + 1e-9)\n    # Strengthen connections to/from central cities\n    for i in range(n):\n        heuristic_matrix[i, :] *= city_centrality[i]\n        heuristic_matrix[:, i] *= city_centrality[i]\n\n    # 3. Random Perturbation (Thermodynamic Fluctuations): Introduce some randomness to allow exploration of suboptimal solutions, similar to simulated annealing\n    random_noise = np.random.rand(n, n) * 0.1  # Small random values\n    heuristic_matrix += random_noise\n\n    # 4. \"Momentum\" (Short-term memory): If an edge is good, keep it good. This is a very crude attempt to keep useful components after perturbation\n    # Not implemented this time for complexity and focus on other ideas\n\n    # 5. Normalize the values: Scales all the heuristics to the range [0, 1] for better stability.\n    heuristic_matrix = (heuristic_matrix - np.min(heuristic_matrix)) / (np.max(heuristic_matrix) - np.min(heuristic_matrix) + 1e-9)\n\n    return heuristic_matrix\n\n[Heuristics 19th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Heuristics for the Traveling Salesman Problem (TSP) based on a combination of\n    inverse distance, node degree, and a simulated annealing-inspired exploration.\n    This is analogous to path integrals by considering possible trajectories.\n\n    Args:\n        distance_matrix: A numpy ndarray representing the distance matrix.\n\n    Returns:\n        A numpy ndarray of the same shape as distance_matrix, representing\n        prior indicators of how promising it is to include each edge in a solution.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # Inverse distance (close nodes are more attractive)\n    inverse_distance = 1.0 / (distance_matrix + 1e-9)  # Adding a small value to avoid division by zero\n\n    # Node degree heuristic (favor nodes with fewer connections)\n    node_degrees = np.sum(inverse_distance, axis=0)\n    degree_heuristic = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_heuristic[i, j] = 1.0 / (node_degrees[i] + node_degrees[j] + 1e-9) # Sum of degrees, add epsilon for smoothing.\n\n    # Simulated annealing-inspired exploration (temperature parameter). Decrease over iterations.\n    # This creates a stochastic acceptance criteria, like the exp(-dE/kT).\n\n    temperature = 1.0  # Initial temperature. Think about adjusting this according to problem size (n).\n\n    # Edge combination score.\n    heuristics = inverse_distance * degree_heuristic\n    # Add a random component to simulate \"exploration\" at high temps. Reduce temperature later.\n    # High \"temperature\" means more randomness.\n\n    random_matrix = np.random.rand(n, n) * temperature\n    heuristics = heuristics + random_matrix\n    \n    # Normalize for scaling, preventing large numbers.\n    heuristics = heuristics / np.max(heuristics)\n    \n    return heuristics\n\n[Heuristics 20th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Heuristics for the Traveling Salesman Problem (TSP) based on a combination of\n    inverse distance, node degree, and a simulated annealing-inspired exploration.\n    This is analogous to path integrals by considering possible trajectories.\n\n    Args:\n        distance_matrix: A numpy ndarray representing the distance matrix.\n\n    Returns:\n        A numpy ndarray of the same shape as distance_matrix, representing\n        prior indicators of how promising it is to include each edge in a solution.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # Inverse distance (close nodes are more attractive)\n    inverse_distance = 1.0 / (distance_matrix + 1e-9)  # Adding a small value to avoid division by zero\n\n    # Node degree heuristic (favor nodes with fewer connections)\n    node_degrees = np.sum(inverse_distance, axis=0)\n    degree_heuristic = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_heuristic[i, j] = 1.0 / (node_degrees[i] + node_degrees[j] + 1e-9) # Sum of degrees, add epsilon for smoothing.\n\n    # Simulated annealing-inspired exploration (temperature parameter). Decrease over iterations.\n    # This creates a stochastic acceptance criteria, like the exp(-dE/kT).\n\n    temperature = 1.0  # Initial temperature. Think about adjusting this according to problem size (n).\n\n    # Edge combination score.\n    heuristics = inverse_distance * degree_heuristic\n    # Add a random component to simulate \"exploration\" at high temps. Reduce temperature later.\n    # High \"temperature\" means more randomness.\n\n    random_matrix = np.random.rand(n, n) * temperature\n    heuristics = heuristics + random_matrix\n    \n    # Normalize for scaling, preventing large numbers.\n    heuristics = heuristics / np.max(heuristics)\n    \n    return heuristics\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}