{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, attractiveness_exponent: float = 2.7089643959566816,\n                  sparsification_factor: float = 4.733362140013519) -> np.ndarray:\n    \"\"\"\n    A more sophisticated heuristics function for the Traveling Salesman Problem (TSP).\n\n    This version builds upon heuristics_v1 by incorporating several enhancements:\n\n    1.  Adaptive Temperature: The temperature parameter is adjusted dynamically based on\n        the distribution of distances in the matrix. This allows the heuristic to adapt\n        to different problem scales and structures.\n\n    2.  Node Centrality Penalty: Nodes with high centrality (i.e., nodes that are close to\n        many other nodes) are penalized. This encourages the algorithm to explore paths that\n        don't necessarily pass through the most central locations, potentially leading to\n        more efficient routes.\n\n    3.  Sparsification: Edges with very low desirability are set to zero, effectively\n        sparsifying the heuristics matrix. This can help to focus the search on a smaller\n        set of promising edges and improve computational efficiency.\n\n    4. Edge-Betweenness Prior: Edges that bridge disparate clusters are favored.\n\n    Args:\n        distance_matrix (np.ndarray): The distance matrix representing the distances\n                                         between cities.\n        attractiveness_exponent (float): Exponent for inverse distance. Default is 2.0.\n        sparsification_factor (float): Divisor for the mean heuristic value to determine the threshold for sparsification. Default is 5.0.\n\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, representing the\n                     prior probabilities of including each edge in a solution. Higher\n                     values indicate a higher prior probability.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # Adaptive Temperature\n    temperature = np.median(distance_matrix)  # Use median for robustness to outliers\n\n    # Inverse distance, avoid division by zero\n    inverse_distance = 1.0 / (distance_matrix + np.eye(n))\n\n    # Node Attractiveness (Desirability) - as in v1\n    node_attractiveness = np.sum(inverse_distance, axis=0)\n    node_attractiveness = 1.0 / (node_attractiveness / np.mean(node_attractiveness))\n\n    # Node Centrality Penalty\n    node_centrality = np.sum(inverse_distance, axis=1)  # Sum of inverse distances from each node\n    node_centrality = node_centrality / np.mean(node_centrality) # normalized\n    node_centrality_penalty = 1.0 / node_centrality  # Penalize high centrality\n\n    # Edge Betweenness (approximation)\n    edge_betweenness = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                #This is a simplified approximation. Ideally, calculate shortest paths.\n                edge_betweenness[i,j] = node_attractiveness[i] + node_attractiveness[j]\n\n    # Combine factors\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = (inverse_distance[i, j]**attractiveness_exponent) * \\\n                                   (node_attractiveness[i] * node_attractiveness[j]) * \\\n                                   (node_centrality_penalty[i] * node_centrality_penalty[j]) * \\\n                                   np.exp(-distance_matrix[i, j] / temperature) + edge_betweenness[i,j]\n    # Sparsification - remove low probability edges\n    threshold = np.mean(heuristics) / sparsification_factor # Dynamic threshold\n    heuristics[heuristics < threshold] = 0\n\n    return heuristics\n\n[Heuristics 2nd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, attractiveness_exponent: float = 2.7089643959566816,\n                  sparsification_factor: float = 4.733362140013519) -> np.ndarray:\n    \"\"\"\n    A more sophisticated heuristics function for the Traveling Salesman Problem (TSP).\n\n    This version builds upon heuristics_v1 by incorporating several enhancements:\n\n    1.  Adaptive Temperature: The temperature parameter is adjusted dynamically based on\n        the distribution of distances in the matrix. This allows the heuristic to adapt\n        to different problem scales and structures.\n\n    2.  Node Centrality Penalty: Nodes with high centrality (i.e., nodes that are close to\n        many other nodes) are penalized. This encourages the algorithm to explore paths that\n        don't necessarily pass through the most central locations, potentially leading to\n        more efficient routes.\n\n    3.  Sparsification: Edges with very low desirability are set to zero, effectively\n        sparsifying the heuristics matrix. This can help to focus the search on a smaller\n        set of promising edges and improve computational efficiency.\n\n    4. Edge-Betweenness Prior: Edges that bridge disparate clusters are favored.\n\n    Args:\n        distance_matrix (np.ndarray): The distance matrix representing the distances\n                                         between cities.\n        attractiveness_exponent (float): Exponent for inverse distance. Default is 2.0.\n        sparsification_factor (float): Divisor for the mean heuristic value to determine the threshold for sparsification. Default is 5.0.\n\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, representing the\n                     prior probabilities of including each edge in a solution. Higher\n                     values indicate a higher prior probability.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # Adaptive Temperature\n    temperature = np.median(distance_matrix)  # Use median for robustness to outliers\n\n    # Inverse distance, avoid division by zero\n    inverse_distance = 1.0 / (distance_matrix + np.eye(n))\n\n    # Node Attractiveness (Desirability) - as in v1\n    node_attractiveness = np.sum(inverse_distance, axis=0)\n    node_attractiveness = 1.0 / (node_attractiveness / np.mean(node_attractiveness))\n\n    # Node Centrality Penalty\n    node_centrality = np.sum(inverse_distance, axis=1)  # Sum of inverse distances from each node\n    node_centrality = node_centrality / np.mean(node_centrality) # normalized\n    node_centrality_penalty = 1.0 / node_centrality  # Penalize high centrality\n\n    # Edge Betweenness (approximation)\n    edge_betweenness = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                #This is a simplified approximation. Ideally, calculate shortest paths.\n                edge_betweenness[i,j] = node_attractiveness[i] + node_attractiveness[j]\n\n    # Combine factors\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = (inverse_distance[i, j]**attractiveness_exponent) * \\\n                                   (node_attractiveness[i] * node_attractiveness[j]) * \\\n                                   (node_centrality_penalty[i] * node_centrality_penalty[j]) * \\\n                                   np.exp(-distance_matrix[i, j] / temperature) + edge_betweenness[i,j]\n    # Sparsification - remove low probability edges\n    threshold = np.mean(heuristics) / sparsification_factor # Dynamic threshold\n    heuristics[heuristics < threshold] = 0\n\n    return heuristics\n\n[Heuristics 3rd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, attractiveness_exponent: float = 2.7089643959566816,\n                  sparsification_factor: float = 4.733362140013519) -> np.ndarray:\n    \"\"\"\n    A more sophisticated heuristics function for the Traveling Salesman Problem (TSP).\n\n    This version builds upon heuristics_v1 by incorporating several enhancements:\n\n    1.  Adaptive Temperature: The temperature parameter is adjusted dynamically based on\n        the distribution of distances in the matrix. This allows the heuristic to adapt\n        to different problem scales and structures.\n\n    2.  Node Centrality Penalty: Nodes with high centrality (i.e., nodes that are close to\n        many other nodes) are penalized. This encourages the algorithm to explore paths that\n        don't necessarily pass through the most central locations, potentially leading to\n        more efficient routes.\n\n    3.  Sparsification: Edges with very low desirability are set to zero, effectively\n        sparsifying the heuristics matrix. This can help to focus the search on a smaller\n        set of promising edges and improve computational efficiency.\n\n    4. Edge-Betweenness Prior: Edges that bridge disparate clusters are favored.\n\n    Args:\n        distance_matrix (np.ndarray): The distance matrix representing the distances\n                                         between cities.\n        attractiveness_exponent (float): Exponent for inverse distance. Default is 2.0.\n        sparsification_factor (float): Divisor for the mean heuristic value to determine the threshold for sparsification. Default is 5.0.\n\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, representing the\n                     prior probabilities of including each edge in a solution. Higher\n                     values indicate a higher prior probability.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # Adaptive Temperature\n    temperature = np.median(distance_matrix)  # Use median for robustness to outliers\n\n    # Inverse distance, avoid division by zero\n    inverse_distance = 1.0 / (distance_matrix + np.eye(n))\n\n    # Node Attractiveness (Desirability) - as in v1\n    node_attractiveness = np.sum(inverse_distance, axis=0)\n    node_attractiveness = 1.0 / (node_attractiveness / np.mean(node_attractiveness))\n\n    # Node Centrality Penalty\n    node_centrality = np.sum(inverse_distance, axis=1)  # Sum of inverse distances from each node\n    node_centrality = node_centrality / np.mean(node_centrality) # normalized\n    node_centrality_penalty = 1.0 / node_centrality  # Penalize high centrality\n\n    # Edge Betweenness (approximation)\n    edge_betweenness = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                #This is a simplified approximation. Ideally, calculate shortest paths.\n                edge_betweenness[i,j] = node_attractiveness[i] + node_attractiveness[j]\n\n    # Combine factors\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = (inverse_distance[i, j]**attractiveness_exponent) * \\\n                                   (node_attractiveness[i] * node_attractiveness[j]) * \\\n                                   (node_centrality_penalty[i] * node_centrality_penalty[j]) * \\\n                                   np.exp(-distance_matrix[i, j] / temperature) + edge_betweenness[i,j]\n    # Sparsification - remove low probability edges\n    threshold = np.mean(heuristics) / sparsification_factor # Dynamic threshold\n    heuristics[heuristics < threshold] = 0\n\n    return heuristics\n\n[Heuristics 4th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A more sophisticated heuristics function for the Traveling Salesman Problem (TSP).\n\n    This version builds upon heuristics_v1 by incorporating several enhancements:\n\n    1.  Adaptive Temperature: The temperature parameter is adjusted dynamically based on\n        the distribution of distances in the matrix. This allows the heuristic to adapt\n        to different problem scales and structures.\n\n    2.  Node Centrality Penalty: Nodes with high centrality (i.e., nodes that are close to\n        many other nodes) are penalized. This encourages the algorithm to explore paths that\n        don't necessarily pass through the most central locations, potentially leading to\n        more efficient routes.\n\n    3.  Sparsification: Edges with very low desirability are set to zero, effectively\n        sparsifying the heuristics matrix. This can help to focus the search on a smaller\n        set of promising edges and improve computational efficiency.\n\n    4. Edge-Betweenness Prior: Edges that bridge disparate clusters are favored.\n\n    Args:\n        distance_matrix (np.ndarray): The distance matrix representing the distances\n                                         between cities.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, representing the\n                     prior probabilities of including each edge in a solution. Higher\n                     values indicate a higher prior probability.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # Adaptive Temperature\n    temperature = np.median(distance_matrix)  # Use median for robustness to outliers\n\n    # Inverse distance, avoid division by zero\n    inverse_distance = 1.0 / (distance_matrix + np.eye(n))\n\n    # Node Attractiveness (Desirability) - as in v1\n    node_attractiveness = np.sum(inverse_distance, axis=0)\n    node_attractiveness = 1.0 / (node_attractiveness / np.mean(node_attractiveness))\n\n    # Node Centrality Penalty\n    node_centrality = np.sum(inverse_distance, axis=1)  # Sum of inverse distances from each node\n    node_centrality = node_centrality / np.mean(node_centrality) # normalized\n    node_centrality_penalty = 1.0 / node_centrality  # Penalize high centrality\n\n    # Edge Betweenness (approximation)\n    edge_betweenness = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                #This is a simplified approximation. Ideally, calculate shortest paths.\n                edge_betweenness[i,j] = node_attractiveness[i] + node_attractiveness[j]\n\n    # Combine factors\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = (inverse_distance[i, j]**2) * \\\n                                   (node_attractiveness[i] * node_attractiveness[j]) * \\\n                                   (node_centrality_penalty[i] * node_centrality_penalty[j]) * \\\n                                   np.exp(-distance_matrix[i, j] / temperature) + edge_betweenness[i,j]\n    # Sparsification - remove low probability edges\n    threshold = np.mean(heuristics) / 5 # Dynamic threshold\n    heuristics[heuristics < threshold] = 0\n\n    return heuristics\n\n[Heuristics 5th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A heuristics function for the Traveling Salesman Problem (TSP) that combines\n    distance, node degree, and a sparse connectivity bias.\n\n    This version builds upon heuristics_v1 by adding a sparse connectivity bias.\n    It identifies and prioritizes edges that connect relatively isolated parts of the graph,\n    while still favoring short distances and considering node degree. It also incorporates a pheromone concept.\n\n    Args:\n        distance_matrix (np.ndarray): The distance matrix representing the distances\n                                         between cities.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, representing the\n                     prior probabilities of including each edge in a solution. Higher\n                     values indicate a higher prior probability.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n    temperature = np.mean(distance_matrix) / 2  # Adjust as needed for optimal performance\n\n    # Inverse distance, avoid division by zero\n    inverse_distance = 1.0 / (distance_matrix + np.eye(n))\n\n    # Node degree desirability (attractiveness)\n    node_attractiveness = np.sum(inverse_distance, axis=0)\n    node_attractiveness = 1.0 / (node_attractiveness / np.mean(node_attractiveness)) #Inverse normalized attractiveness to drive toward \"isolated\" nodes\n\n    # Sparse connectivity bias: penalize edges connecting already well-connected nodes.\n    connectivity = np.sum(inverse_distance, axis=0)\n    sparse_connectivity_bias = np.outer(connectivity, connectivity)  # connectivity[i] * connectivity[j]\n\n    #Pheromone concept - a simple memory of good edges from shortest distances\n    pheromone_level = np.zeros_like(distance_matrix, dtype=float)\n    shortest_distance = np.partition(distance_matrix.flatten(), n+1)[n+1]\n    pheromone_level[distance_matrix <= shortest_distance * 1.2] = 1.0\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = ((inverse_distance[i, j]**2) *\n                                     (node_attractiveness[i] * node_attractiveness[j]) *\n                                     np.exp(-distance_matrix[i, j] / temperature) /\n                                     (sparse_connectivity_bias[i, j] + 1e-6)) + pheromone_level[i, j] # Adding pheromone boost\n\n\n    # Sparsify: Remove less promising edges. Adjust the threshold as needed.\n    threshold = np.percentile(heuristics[heuristics > 0], 20) # Keep top 80%\n    heuristics[heuristics < threshold] = 0\n\n    return heuristics\n\n[Heuristics 6th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, degree desirability, gravity, and temperature.\n    Normalizes for robust performance.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9\n\n    # Inverse distance & Gravity\n    inverse_distance = 1.0 / (distance_matrix + epsilon)\n    gravity = 1.0 / (distance_matrix**2 + epsilon)\n\n    # Node degree desirability\n    node_degrees = np.sum(inverse_distance, axis=0)\n    degree_heuristic = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_heuristic[i, j] = 1.0 / (node_degrees[i] + node_degrees[j] + epsilon)\n\n    # Temperature-controlled exploration\n    temperature = 0.1 # Reduced initial temp for faster convergence\n    random_matrix = np.random.rand(n, n) * temperature\n\n    # Combine and normalize\n    heuristic = inverse_distance + gravity + degree_heuristic + random_matrix\n    heuristic = heuristic / np.max(heuristic)\n\n    return heuristic\n\n[Heuristics 7th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, gravitational attraction, and randomness.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9\n\n    inverse_distance = 1 / (distance_matrix + epsilon)\n    gravitational_attraction = 1 / ((distance_matrix**2) + epsilon)\n    randomness = np.random.rand(n, n) * (1.0 / n)\n    randomness = (randomness + randomness.T) / 2\n\n    heuristic_matrix = (\n        0.5 * inverse_distance +\n        0.3 * gravitational_attraction +\n        0.2 * randomness\n    )\n\n    np.fill_diagonal(heuristic_matrix, 0)\n    return heuristic_matrix\n\n[Heuristics 8th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, gravitational attraction, and randomness.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9\n\n    inverse_distance = 1 / (distance_matrix + epsilon)\n    gravitational_attraction = 1 / ((distance_matrix**2) + epsilon)\n    randomness = np.random.rand(n, n) * (1.0 / n)\n    randomness = (randomness + randomness.T) / 2\n\n    heuristic_matrix = (\n        0.5 * inverse_distance +\n        0.3 * gravitational_attraction +\n        0.2 * randomness\n    )\n\n    np.fill_diagonal(heuristic_matrix, 0)\n    return heuristic_matrix\n\n[Heuristics 9th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, nearest neighbor, and exploration.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # 1. Inverse Distance\n    heuristics = 1.0 / (distance_matrix + 1e-9)\n\n    # 2. Nearest Neighbor Influence\n    for i in range(n):\n        temp_distances = distance_matrix[i].copy()\n        temp_distances[i] = np.inf\n        nearest_neighbor = np.argmin(temp_distances)\n        heuristics[i, nearest_neighbor] *= 1.75\n        heuristics[nearest_neighbor, i] *= 1.75\n\n    # 3. Node Degree Preference with exploration\n    node_degree_preference = np.zeros_like(distance_matrix)\n    inverse_distance = 1 / (distance_matrix + 1e-9)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                node_degree_preference[i, j] = (np.sum(inverse_distance[i, :]) + np.sum(inverse_distance[j, :]))/2\n\n    stochastic_perturbation = np.random.normal(0, 0.05, size=(n, n))\n    heuristics = (0.8 * heuristics +\n                         0.2 * (1/node_degree_preference) +\n                         0.1 * stochastic_perturbation)\n    heuristics[np.isinf(heuristics)] = 0\n\n    return heuristics\n\n[Heuristics 10th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, degree, and exploration for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9\n\n    inverse_distance = 1 / (distance_matrix + epsilon)\n    node_degrees = np.sum(1 / (distance_matrix + np.eye(n)), axis=1)\n    degree_matrix = np.tile(node_degrees, (n, 1)) + np.tile(node_degrees, (n, 1)).T\n    temperature = np.mean(distance_matrix)\n    hawking_radiation = np.random.normal(0, temperature / (distance_matrix + np.eye(n)), size=(n, n))\n    hawking_radiation = np.abs(hawking_radiation)\n\n    heuristic_matrix = inverse_distance * degree_matrix + hawking_radiation\n    heuristic_matrix = (heuristic_matrix + heuristic_matrix.T)/2\n    heuristic_matrix = np.nan_to_num(heuristic_matrix, nan=0.0)\n    heuristic_matrix = (heuristic_matrix - np.min(heuristic_matrix)) / (np.max(heuristic_matrix) - np.min(heuristic_matrix) + epsilon)\n\n    return heuristic_matrix\n\n[Heuristics 11th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, degree, and exploration for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9\n\n    inverse_distance = 1 / (distance_matrix + epsilon)\n    node_degrees = np.sum(1 / (distance_matrix + np.eye(n)), axis=1)\n    degree_matrix = np.tile(node_degrees, (n, 1)) + np.tile(node_degrees, (n, 1)).T\n    temperature = np.mean(distance_matrix)\n    hawking_radiation = np.random.normal(0, temperature / (distance_matrix + np.eye(n)), size=(n, n))\n    hawking_radiation = np.abs(hawking_radiation)\n\n    heuristic_matrix = inverse_distance * degree_matrix + hawking_radiation\n    heuristic_matrix = (heuristic_matrix + heuristic_matrix.T)/2\n    heuristic_matrix = np.nan_to_num(heuristic_matrix, nan=0.0)\n    heuristic_matrix = (heuristic_matrix - np.min(heuristic_matrix)) / (np.max(heuristic_matrix) - np.min(heuristic_matrix) + epsilon)\n\n    return heuristic_matrix\n\n[Heuristics 12th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, degree, and exploration for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9\n\n    inverse_distance = 1 / (distance_matrix + epsilon)\n    node_degrees = np.sum(1 / (distance_matrix + np.eye(n)), axis=1)\n    degree_matrix = np.tile(node_degrees, (n, 1)) + np.tile(node_degrees, (n, 1)).T\n    temperature = np.mean(distance_matrix)\n    hawking_radiation = np.random.normal(0, temperature / (distance_matrix + np.eye(n)), size=(n, n))\n    hawking_radiation = np.abs(hawking_radiation)\n\n    heuristic_matrix = inverse_distance * degree_matrix + hawking_radiation\n    heuristic_matrix = (heuristic_matrix + heuristic_matrix.T)/2\n    heuristic_matrix = np.nan_to_num(heuristic_matrix, nan=0.0)\n    heuristic_matrix = (heuristic_matrix - np.min(heuristic_matrix)) / (np.max(heuristic_matrix) - np.min(heuristic_matrix) + epsilon)\n\n    return heuristic_matrix\n\n[Heuristics 13th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, node connectivity, and Hawking radiation-inspired exploration for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    inverse_distance = 1.0 / (distance_matrix + np.eye(n))\n    node_degrees = np.sum(inverse_distance, axis=1)\n    degree_matrix = np.tile(node_degrees, (n, 1)) + np.tile(node_degrees, (n, 1)).T\n\n    temperature = np.mean(distance_matrix)\n    hawking_radiation = np.random.normal(0, temperature / (distance_matrix + np.eye(n)), size=(n, n))\n    hawking_radiation = np.abs(hawking_radiation)\n\n    heuristic_matrix = inverse_distance * degree_matrix + hawking_radiation\n    heuristic_matrix = (heuristic_matrix + heuristic_matrix.T) / 2\n    heuristic_matrix = np.nan_to_num(heuristic_matrix, nan=0.0)\n    heuristic_matrix = (heuristic_matrix - np.min(heuristic_matrix)) / (np.max(heuristic_matrix) - np.min(heuristic_matrix) + 1e-9)\n\n    return heuristic_matrix\n\n[Heuristics 14th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, nearest neighbors, and global distance for heuristics.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n    nearest_neighbors = np.argsort(distance_matrix, axis=1)[:, 1:6]\n    avg_distance = np.mean(distance_matrix[np.triu_indices_from(distance_matrix, k=1)])\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                distance = distance_matrix[i, j]\n                heuristic_value = 1 / (distance + 1e-6)\n                if j in nearest_neighbors[i]:\n                    heuristic_value += 0.5\n                    if i in np.argsort(distance_matrix, axis=1)[:, 1:6][j]:\n                        heuristic_value += 0.2\n                if distance < avg_distance:\n                    heuristic_value += 0.2\n                heuristics[i, j] = heuristic_value\n\n    #Optional: Normalize the heuristic matrix\n    max_h = np.max(heuristics)\n    min_h = np.min(heuristics)\n    if max_h > min_h:\n        heuristics = (heuristics - min_h) / (max_h - min_h)\n\n    return heuristics\n\n[Heuristics 15th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, nearest neighbors, and global distance for heuristics.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n    nearest_neighbors = np.argsort(distance_matrix, axis=1)[:, 1:6]\n    avg_distance = np.mean(distance_matrix[np.triu_indices_from(distance_matrix, k=1)])\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                distance = distance_matrix[i, j]\n                heuristic_value = 1 / (distance + 1e-6)\n                if j in nearest_neighbors[i]:\n                    heuristic_value += 0.5\n                    if i in np.argsort(distance_matrix, axis=1)[:, 1:6][j]:\n                        heuristic_value += 0.2\n                if distance < avg_distance:\n                    heuristic_value += 0.2\n                heuristics[i, j] = heuristic_value\n\n    #Optional: Normalize the heuristic matrix\n    max_h = np.max(heuristics)\n    min_h = np.min(heuristics)\n    if max_h > min_h:\n        heuristics = (heuristics - min_h) / (max_h - min_h)\n\n    return heuristics\n\n[Heuristics 16th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    An enhanced heuristics function for the Traveling Salesman Problem (TSP),\n    building upon heuristics_v1 with added features and refinements.\n\n    This version aims to improve solution quality by incorporating:\n\n    1.  Adaptive Temperature: Temperature is calculated for each node based on its\n        average distance to other nodes, leading to a more nuanced exploration strategy.\n    2.  Reinforcement Learning inspired heuristic: Favor edges that have been historically included in short tours.\n        This is approximated using an inverse measure of how many times a node has been part of longer edges.\n    3.  Sparsification:  A threshold is applied to the heuristic matrix to eliminate less\n        promising edges, reducing the search space for subsequent algorithms.\n        This threshold is dynamically adjusted based on the distribution of heuristic values.\n    4. Reciprocal edges: Ensures that if an edge (i,j) is considered important, so is the edge (j,i)\n\n    Args:\n        distance_matrix (np.ndarray): The distance matrix representing the distances\n                                         between cities.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, representing the\n                     prior probabilities of including each edge in a solution. Higher\n                     values indicate a higher prior probability.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # 1. Adaptive Temperature\n    node_avg_distances = np.mean(distance_matrix, axis=1)\n    temperature_matrix = np.tile(node_avg_distances, (n, 1)) + np.tile(node_avg_distances, (n, 1)).T\n    temperature_matrix /= 4 # Divide by a constant to adjust the scale of temperature\n    temperature_matrix = np.clip(temperature_matrix, np.min(distance_matrix) / 2, np.max(distance_matrix) * 2) # Clip values to a reasonable range\n\n    # 2. Inverse distance, avoiding division by zero\n    inverse_distance = 1.0 / (distance_matrix + np.eye(n))\n\n    # 3. Node degree desirability (attractiveness) + Reinforcement learning inspired component\n    node_edge_usage = np.sum(distance_matrix, axis=0)\n    node_attractiveness = 1.0 / (node_edge_usage / np.mean(node_edge_usage))\n\n    #Adjust temperature based on node_attractiveness\n    adjusted_temperature_matrix = temperature_matrix * (1 + 0.1 * np.abs(node_attractiveness - 1))\n\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = (inverse_distance[i, j]**2) * (node_attractiveness[i] * node_attractiveness[j]) * np.exp(-distance_matrix[i, j] / adjusted_temperature_matrix[i,j])\n\n    # 4. Sparsification\n    threshold = np.mean(heuristics[heuristics > 0]) / 3 # Dynamic threshold based on the mean of non-zero elements\n\n    heuristics[heuristics < threshold] = 0\n\n    # 5. Enforce reciprocal edges.\n    heuristics = (heuristics + heuristics.T)/2\n\n    return heuristics\n\n[Heuristics 17th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, node centrality, and rank-based normalization.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Inverse distance\n    inverse_distance = 1.0 / (distance_matrix + 1e-9)\n\n    # Node centrality (average distance to other nodes)\n    node_centrality = np.mean(distance_matrix, axis=1)\n    penalty = np.exp(-0.5 * (node_centrality[:, None] + node_centrality[None, :]) / np.mean(node_centrality))\n    heuristic_matrix = inverse_distance * penalty\n\n    # Rank-based normalization\n    ranks = np.argsort(heuristic_matrix, axis=1)\n    n_ranks = np.argsort(ranks, axis=1)\n    heuristic_matrix = n_ranks / n\n\n    return heuristic_matrix\n\n[Heuristics 18th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"TSP heuristic combining inverse distance, degree desirability, & decay.\"\"\"\n\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9\n\n    # Inverse distance\n    inverse_distance = 1.0 / (distance_matrix + epsilon)\n\n    # Node degree desirability (attractiveness of less-connected nodes)\n    node_degrees = np.sum(inverse_distance, axis=0)\n    degree_heuristic = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            degree_heuristic[i, j] = 1.0 / (node_degrees[i] + node_degrees[j] + epsilon)\n\n    # Combine inverse distance and degree desirability\n    combined_heuristic = inverse_distance * degree_heuristic\n\n    # Simulate \"radioactive decay\" to favor shorter edges probabilistically\n    half_life = np.median(distance_matrix) # Use median distance as a reference for decay\n    decay_factor = np.log(2) / half_life\n    edge_probabilities = combined_heuristic * np.exp(-decay_factor * distance_matrix)\n\n    # Rank-based normalization\n    ranks = np.argsort(edge_probabilities, axis=None)\n    ranks = np.unravel_index(ranks, edge_probabilities.shape)\n    normalized_heuristic = np.zeros_like(distance_matrix, dtype=float)\n    normalized_heuristic[ranks] = np.linspace(0, 1, n * n)\n\n    return normalized_heuristic\n\n[Heuristics 19th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"TSP heuristic combining inverse distance, degree desirability, & decay.\"\"\"\n\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9\n\n    # Inverse distance\n    inverse_distance = 1.0 / (distance_matrix + epsilon)\n\n    # Node degree desirability (attractiveness of less-connected nodes)\n    node_degrees = np.sum(inverse_distance, axis=0)\n    degree_heuristic = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            degree_heuristic[i, j] = 1.0 / (node_degrees[i] + node_degrees[j] + epsilon)\n\n    # Combine inverse distance and degree desirability\n    combined_heuristic = inverse_distance * degree_heuristic\n\n    # Simulate \"radioactive decay\" to favor shorter edges probabilistically\n    half_life = np.median(distance_matrix) # Use median distance as a reference for decay\n    decay_factor = np.log(2) / half_life\n    edge_probabilities = combined_heuristic * np.exp(-decay_factor * distance_matrix)\n\n    # Rank-based normalization\n    ranks = np.argsort(edge_probabilities, axis=None)\n    ranks = np.unravel_index(ranks, edge_probabilities.shape)\n    normalized_heuristic = np.zeros_like(distance_matrix, dtype=float)\n    normalized_heuristic[ranks] = np.linspace(0, 1, n * n)\n\n    return normalized_heuristic\n\n[Heuristics 20th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, node attractiveness, and rank-based normalization\n    for better edge prioritization in TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9\n\n    # Inverse distance\n    inverse_distance = 1.0 / (distance_matrix + epsilon)\n\n    # Node attractiveness (favor less-connected nodes)\n    node_attractiveness = np.sum(inverse_distance, axis=0)\n    node_attractiveness = 1.0 / (node_attractiveness / np.mean(node_attractiveness) + epsilon)\n\n    # Combine inverse distance and node attractiveness\n    heuristic_matrix = inverse_distance * (node_attractiveness[:, None] * node_attractiveness[None, :])\n\n    # Rank-based normalization\n    ranks = np.argsort(heuristic_matrix, axis=1)\n    normalized_matrix = np.zeros_like(heuristic_matrix, dtype=float)\n    for i in range(n):\n        normalized_matrix[i, ranks[i]] = np.arange(1, n + 1)\n    normalized_matrix = normalized_matrix / n\n\n    return normalized_matrix\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}