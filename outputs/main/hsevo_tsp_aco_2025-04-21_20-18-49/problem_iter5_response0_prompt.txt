{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n                  sparsification_factor: float = 4.733362140013519) -> np.ndarray:\n    \"\"\"\n    A more sophisticated heuristics function for the Traveling Salesman Problem (TSP).\n\n    This version builds upon heuristics_v1 by incorporating several enhancements:\n\n    1.  Adaptive Temperature: The temperature parameter is adjusted dynamically based on\n        the distribution of distances in the matrix. This allows the heuristic to adapt\n        to different problem scales and structures.\n\n    2.  Node Centrality Penalty: Nodes with high centrality (i.e., nodes that are close to\n        many other nodes) are penalized. This encourages the algorithm to explore paths that\n        don't necessarily pass through the most central locations, potentially leading to\n        more efficient routes.\n\n    3.  Sparsification: Edges with very low desirability are set to zero, effectively\n        sparsifying the heuristics matrix. This can help to focus the search on a smaller\n        set of promising edges and improve computational efficiency.\n\n    4. Edge-Betweenness Prior: Edges that bridge disparate clusters are favored.\n\n    Args:\n        distance_matrix (np.ndarray): The distance matrix representing the distances\n                                         between cities.\n        attractiveness_exponent (float): Exponent for inverse distance. Default is 2.0.\n        sparsification_factor (float): Divisor for the mean heuristic value to determine the threshold for sparsification. Default is 5.0.\n\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, representing the\n                     prior probabilities of including each edge in a solution. Higher\n                     values indicate a higher prior probability.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # Adaptive Temperature\n    temperature = np.median(distance_matrix)  # Use median for robustness to outliers\n\n    # Inverse distance, avoid division by zero\n    inverse_distance = 1.0 / (distance_matrix + np.eye(n))\n\n    # Node Attractiveness (Desirability) - as in v1\n    node_attractiveness = np.sum(inverse_distance, axis=0)\n    node_attractiveness = 1.0 / (node_attractiveness / np.mean(node_attractiveness))\n\n    # Node Centrality Penalty\n    node_centrality = np.sum(inverse_distance, axis=1)  # Sum of inverse distances from each node\n    node_centrality = node_centrality / np.mean(node_centrality) # normalized\n    node_centrality_penalty = 1.0 / node_centrality  # Penalize high centrality\n\n    # Edge Betweenness (approximation)\n    edge_betweenness = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                #This is a simplified approximation. Ideally, calculate shortest paths.\n                edge_betweenness[i,j] = node_attractiveness[i] + node_attractiveness[j]\n\n    # Combine factors\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = (inverse_distance[i, j]**attractiveness_exponent) * \\\n                                   (node_attractiveness[i] * node_attractiveness[j]) * \\\n                                   (node_centrality_penalty[i] * node_centrality_penalty[j]) * \\\n                                   np.exp(-distance_matrix[i, j] / temperature) + edge_betweenness[i,j]\n    # Sparsification - remove low probability edges\n    threshold = np.mean(heuristics) / sparsification_factor # Dynamic threshold\n    heuristics[heuristics < threshold] = 0\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"TSP heuristic combining inverse distance, degree desirability, & decay.\"\"\"\n\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9\n\n    # Inverse distance\n    inverse_distance = 1.0 / (distance_matrix + epsilon)\n\n    # Node degree desirability (attractiveness of less-connected nodes)\n    node_degrees = np.sum(inverse_distance, axis=0)\n    degree_heuristic = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            degree_heuristic[i, j] = 1.0 / (node_degrees[i] + node_degrees[j] + epsilon)\n\n    # Combine inverse distance and degree desirability\n    combined_heuristic = inverse_distance * degree_heuristic\n\n    # Simulate \"radioactive decay\" to favor shorter edges probabilistically\n    half_life = np.median(distance_matrix) # Use median distance as a reference for decay\n    decay_factor = np.log(2) / half_life\n    edge_probabilities = combined_heuristic * np.exp(-decay_factor * distance_matrix)\n\n    # Rank-based normalization\n    ranks = np.argsort(edge_probabilities, axis=None)\n    ranks = np.unravel_index(ranks, edge_probabilities.shape)\n    normalized_heuristic = np.zeros_like(distance_matrix, dtype=float)\n    normalized_heuristic[ranks] = np.linspace(0, 1, n * n)\n\n    return normalized_heuristic\n\n### Analyze & experience\n- *   Comparing (1st) vs (2nd), we see they are identical. This suggests redundancy or that these are the same heuristic implementation repeated.\n\n*   Comparing (1st) vs (4th), we observe that (1st) uses default values for `attractiveness_exponent` and `sparsification_factor`, while (4th) hardcodes these values (2 and 5 respectively).  (1st) also imports `random`, `math`, `scipy`, and `torch` which are not used, suggesting a lack of code cleanup. The parameters in (1st) add flexibility.\n\n*   Comparing (4th) vs (5th), (4th) combines inverse distance, node attractiveness, centrality penalty, and a temperature factor, along with sparsification. (5th) replaces the centrality penalty with a sparse connectivity bias and a pheromone level, and uses percentile-based sparsification. (5th) incorporates a \"pheromone\" concept and a temperature adjusted to `np.mean(distance_matrix) / 2`.\n\n*   Comparing (5th) vs (6th), (5th) employs node attractiveness, sparse connectivity bias, and pheromone levels with sparsification, while (6th) uses inverse distance, gravity, node degree desirability, a fixed temperature for exploration, and normalization. (6th) uses a fixed temperature and normalizes the heuristic. (5th) uses more adaptive components.\n\n*   Comparing (6th) vs (7th), (6th) uses inverse distance, gravity, node degree desirability, and a temperature-controlled random matrix. It normalizes the result. (7th) uses inverse distance, gravitational attraction and a fixed randomness component with specific weights. (7th) uses fixed weights for combining factors.\n\n*   Comparing (second worst) vs (worst), (19th) and (20th) both combine inverse distance, degree desirability, and decay. They differ primarily in how they combine these factors and normalize the results. They both use rank based normalization.\n\nOverall: The better heuristics tend to incorporate more adaptive elements (like adaptive temperature, dynamic thresholds for sparsification) and combinations of different factors (distance, node properties, edge properties). Normalization and avoidance of division by zero are common good practices. Poorer heuristics tend to use fixed weights and less adaptive parameters.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, let's redefine \"Current Self-Reflection\" to provide more actionable advice for designing better heuristics, while avoiding the pitfalls of \"Ineffective Self-Reflection.\" Here's a breakdown:\n\n*   **Keywords:** Parameterization, Normalization, Dynamic Adaptation, Factor Combination.\n\n*   **Advice:** Focus on creating modular heuristic components with adjustable parameters. Normalize inputs to allow fair comparisons. Design mechanisms that dynamically adapt to problem characteristics during runtime.\n\n*   **Avoid:** Blindly combining factors without understanding their interactions. Over-reliance on single, static threshold values.\n\n*   **Explanation:** Robust heuristics result from carefully integrating adaptable components. Parameterization facilitates tuning and generalization. Dynamic adaptation allows the heuristic to respond to the problem's nuances. Normalization ensures fair evaluation of different factors.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}