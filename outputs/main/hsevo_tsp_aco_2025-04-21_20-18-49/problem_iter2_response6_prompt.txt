{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Enhanced heuristics for the Traveling Salesman Problem, inspired by radioactive decay principles.\n\n    This version combines inverse distance with a normalized rank-based approach and a probabilistic factor\n    to favor shorter distances and prevent getting stuck in local optima.  It considers edge 'half-life',\n    a concept from radioactivity, where shorter edges 'decay' slower (are more promising).\n\n    Args:\n        distance_matrix (np.ndarray): A square matrix where distance_matrix[i, j] is the distance\n                                         between city i and city j. Diagonal elements are ignored\n                                         and often contain 0 or np.inf.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, representing the heuristic values\n                      for each edge. Higher values indicate more promising edges.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n\n    # Handle potential division by zero by adding a small epsilon\n    epsilon = 1e-9\n    distance_matrix = distance_matrix + epsilon\n\n    # 1. Inverse Distance (Basic Attraction): Shorter distances are initially more attractive\n    inverse_distance = 1 / distance_matrix\n\n    # 2. Rank-Based Normalization:  Emphasize relative edge importance, independent of scale.\n    #    For each city, rank the edges by distance and normalize these ranks.\n\n    rank_matrix = np.zeros((n, n))\n    for i in range(n):\n        distances = distance_matrix[i, :]  # Distances from city i to all others\n        ranks = np.argsort(distances) # indices sorted by increasing distance.\n\n        # Store indices to efficiently populate the rank_matrix\n        for j, r in enumerate(ranks):\n           rank_matrix[i, r] = (n - j) / n   # Shorter edges get higher rank (close to 1)\n\n    # 3.  Radioactive Decay Analogy (Edge Half-Life): Favor shorter edges using a probabilistic component\n    #     Simulate half-life decay based on distance.  Shorter edges 'decay' slower, remain promising longer.\n\n    decay_factor = np.exp(-distance_matrix / np.mean(distance_matrix))  # Shorter edges decay slower (higher value).\n\n    # 4. Combined Heuristic:  Balance attraction, relative importance, and half-life.\n    heuristic_matrix = inverse_distance * rank_matrix * decay_factor\n\n    # Optional: Enhance exploitation by emphasizing the edges with already higher chances\n\n    #Normalize between 0-1:\n    min_val = np.min(heuristic_matrix)\n    max_val = np.max(heuristic_matrix)\n    heuristic_matrix = (heuristic_matrix - min_val) / (max_val - min_val)\n\n    return heuristic_matrix\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Heuristic function for the Traveling Salesman Problem (TSP) based on\n    a combination of inverse distance and a nearest neighbor component.\n\n    Args:\n        distance_matrix (np.ndarray): A square matrix where distance_matrix[i][j]\n                                      represents the distance between city i and city j.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, where each\n                     element represents a heuristic value indicating the\n                     desirability of including the corresponding edge in the TSP tour.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Inverse distance component (as in v1)\n    inverse_distance = 1.0 / (distance_matrix + 1e-9)  # Adding a small constant to avoid division by zero\n\n    # Nearest neighbor component\n    for i in range(n):\n        # Find the nearest neighbors for each city\n        nearest_neighbors = np.argsort(distance_matrix[i, :])[1:4]  # Exclude itself, take top 3\n\n        for j in range(n):\n            if i != j:\n                #Boost heuristic value if j is one of the nearest neighbors of i\n                if j in nearest_neighbors:\n                     heuristic_matrix[i, j] += 0.5 # increase the prob for edges towards NN\n                heuristic_matrix[i, j] += inverse_distance[i, j]\n\n    # Normalize the heuristic values to be between 0 and 1\n    max_heuristic = np.max(heuristic_matrix)\n    min_heuristic = np.min(heuristic_matrix)\n\n    if max_heuristic > min_heuristic:\n        heuristic_matrix = (heuristic_matrix - min_heuristic) / (max_heuristic - min_heuristic)\n    else:\n        heuristic_matrix = np.ones_like(heuristic_matrix) # all the same values, meaning no prior knowledge\n\n    return heuristic_matrix\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see that the best heuristic uses inverse distance, node degree desirability, and distance normalization with a temperature parameter, while the worst uses inverse distance, node degree (favoring nodes with fewer connections), and simulated annealing-inspired exploration with a temperature parameter, and normalization. The key difference lies in how node degree is used and the exploration strategy; the best uses \"attractiveness\" to less-connected nodes, while the worst uses a simpler inverse degree and a more direct random addition.\n\nComparing (2nd best) vs (second worst), the second best heuristic uses inverse distance, rank-based normalization, and radioactive decay analogy (edge half-life), and then normalizes the heuristic matrix between 0 and 1; The second worst heuristic combines inverse distance with node degree and randomness with a weight. Key difference here is rank-based normalization that emphasize relative edge importance, independent of scale and the decay factor.\n\nComparing (1st) vs (2nd), the first heuristic focuses on a temperature-controlled balance between exploitation and exploration, normalizing node attractiveness to drive towards isolated nodes, while the second employs rank-based normalization and a radioactive decay analogy to favor shorter edges probabilistically. The first heuristic is more grounded in a simulated physical process, while the second adopts concepts from radioactivity.\n\nComparing (3rd) vs (4th), the third heuristic uses inverse distance, nearest neighbor influence, avoidance of long edges, and start/end point bias; the fourth uses inverse distance, gravity-inspired attraction, global connectivity boost, and local optimization influence (row and column means). The third favors nearest neighbors explicitly and penalizes long edges aggressively, while the fourth balances inverse distance with gravitational attraction and local optimization metrics, adding a constant boost to all edges.\n\nComparing (second worst) vs (worst), the 19th and 20th are identical. They use inverse distance, node degree heuristic (favor nodes with fewer connections), and simulated annealing-inspired exploration, and then normalizing.\n\nOverall: The better heuristics combine multiple factors, including inverse distance, node degree information, and some form of exploration strategy (temperature, randomness, rank-based normalization, radioactive decay). They often normalize the heuristic matrix to a specific range. The use of more sophisticated metaphors and analogies (radioactive decay, gravity) appears correlated with better performance. Emphasizing relative edge importance independent of scales might be more useful. Adding some kind of exploration strategy is vital for escaping local optima.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a refined concept of \"Current Self-Reflection\" aimed at improving heuristic design, focusing on actionable insights and avoiding common pitfalls:\n\n*   **Keywords:** Iterative Improvement, Performance Analysis, Adaptability, Bias Mitigation.\n*   **Advice:** Critically analyze heuristic performance across diverse problem instances. Identify strengths and weaknesses. Quantify the impact of individual components.\n*   **Avoid:** Over-reliance on intuition or anecdotal evidence. Neglecting edge cases or focusing solely on average performance. Premature optimization.\n*   **Explanation:** Effective reflection involves data-driven analysis, constant questioning of assumptions, and a commitment to refining heuristics based on empirical results. Focus on adaptability and identifying biases.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}