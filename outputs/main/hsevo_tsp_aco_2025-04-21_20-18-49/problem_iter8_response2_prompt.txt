{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    TSP heuristic: Combines inverse distance, node attractiveness,\n    centrality penalty, temperature scaling, and sparsification.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n    temperature = np.median(distance_matrix)\n    inverse_distance = 1.0 / (distance_matrix + np.eye(n))\n    node_attractiveness = np.sum(inverse_distance, axis=0)\n    node_attractiveness = 1.0 / (node_attractiveness / np.mean(node_attractiveness))\n    node_centrality = np.sum(inverse_distance, axis=1)\n    node_centrality = node_centrality / np.mean(node_centrality)\n    node_centrality_penalty = 1.0 / node_centrality\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = (inverse_distance[i, j]**2.0) * \\\n                                   (node_attractiveness[i] * node_attractiveness[j]) * \\\n                                   (node_centrality_penalty[i] * node_centrality_penalty[j]) * \\\n                                   np.exp(-distance_matrix[i, j] / temperature)\n\n    threshold = np.mean(heuristics) / 4.733362140013519\n    heuristics[heuristics < threshold] = 0\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines inverse distance, node attractiveness, and adaptive temperature for TSP.\n    Sparsifies based on percentile of combined heuristic matrix.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9\n\n    inverse_distance = 1.0 / (distance_matrix + epsilon)\n    node_attractiveness = np.sum(inverse_distance, axis=0)\n    node_attractiveness = 1.0 / (node_attractiveness / np.mean(node_attractiveness) + epsilon)\n    heuristic_matrix = inverse_distance * (node_attractiveness[:, None] * node_attractiveness[None, :])\n    temperature = np.mean(distance_matrix) / 2\n    randomness = np.random.normal(0, temperature, size=(n, n))\n    heuristic_matrix = heuristic_matrix + np.abs(randomness)\n\n    #Adaptive Sparsification\n    threshold = np.percentile(heuristic_matrix, 75)\n    heuristic_matrix[heuristic_matrix < threshold] = 0\n    heuristic_matrix = (heuristic_matrix + heuristic_matrix.T)/2\n    heuristic_matrix = (heuristic_matrix - np.min(heuristic_matrix)) / (np.max(heuristic_matrix) - np.min(heuristic_matrix) + epsilon)\n    return heuristic_matrix\n\n### Analyze & experience\n- Comparing (1st) vs (8th), we see that the first one uses default values for `attractiveness_exponent` and `sparsification_factor`, while the second one hardcodes these values. The first one is more flexible.\n\nComparing (6th) vs (7th), we see that both are exactly the same.\n\nComparing (1st) vs (6th), the 6th introduces more parameters (`inverse_distance_offset`, `node_attractiveness_scaling`, `node_centrality_scaling`, `temperature_scaling`) that control the heuristic calculation, offering finer-grained control. The 6th also imports `random`, `math`, `scipy`, and `torch` which are not used, indicating potential future extensions or debugging artifacts.\n\nComparing (11th) vs (12th), we see that both are exactly the same.\n\nComparing (1st) vs (11th), the first uses node centrality penalty, while the second one does not.\n\nComparing (19th) vs (20th), we see that both are exactly the same.\n\nComparing (second worst) vs (worst), (15th) vs (14th), the 15th normalizes the combined heuristic matrix before sparsification, leading to a more robust and stable performance, and the 14th use different weights for `inverse_distance`, `gravitational_attraction` and `randomness`.\n\nOverall: The better heuristics introduce more parameters for fine-grained control, incorporate centrality penalties, and normalize heuristics matrix for stable performance. They also focus on adaptive sparsification.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, I understand. Here's a redefined \"Current self-reflection\" aimed at designing better heuristics, avoiding the pitfalls of \"Ineffective self-reflection,\" and focusing on actionable insights:\n\n*   **Keywords:** Parameterization, Normalization, Adaptive Sparsification, Dynamic thresholds, Modular design.\n\n*   **Advice:** Design heuristics with tunable parameters and dynamic thresholds for robustness. Implement normalization techniques to mitigate scale issues when combining diverse factors. Explore adaptive sparsification for efficient solution space exploration by focusing computational effort.\n\n*   **Avoid:** Hardcoding constants and over-reliance on single analogies. Neglecting validation of components regardless of inspiration.\n\n*   **Explanation:** Parameterization allows fine-tuning, normalization ensures fair comparison, and adaptive sparsification focuses search. Dynamic thresholds provide robustness. Modular design promotes flexibility.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}