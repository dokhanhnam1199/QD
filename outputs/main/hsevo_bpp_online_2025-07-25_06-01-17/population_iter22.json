[
  {
    "stdout_filepath": "problem_iter20_response0.txt_stdout.txt",
    "code_path": "problem_iter20_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive, multi-objective heuristic for online Bin Packing Problem.\n    \"\"\"\n    \n    # Calculate the difference between the remaining capacity of each bin and the item size\n    capacity_diff = np.abs(bins_remain_cap - item)\n    \n    # Invert the differences to obtain a priority score for the current item's fit\n    # Using log-based invert function for a more nuanced approach\n    fit_priority = 1 / (1 + np.log(1 + capacity_diff))  \n    \n    # If the item size exceeds the remaining capacity, set the priority to 0\n    fit_priority[bins_remain_cap < item] = 0\n    \n    # Calculate the shortfall or excess capacity for each bin\n    shortfall = np.maximum(0, item - bins_remain_cap)\n    excess = np.maximum(0, bins_remain_cap - item)\n    \n    # Calculate a penalty factor for each bin based on the shortfall or excess capacity\n    # Using a more complex formula for a more adaptive approach\n    penalty_factor = 1 / (1 + np.exp(shortfall)) * 1 / (1 + np.exp(excess))\n    \n    # Calculate the bin utilization, considering both the current item and future potential\n    avg_historic_size = np.mean(bins_remain_cap)  \n    utilization_priority = (bins_remain_cap + avg_historic_size) / (bins_remain_cap + item + avg_historic_size)\n    \n    # Introduce a new objective: minimizing the number of bins used\n    bin_usage_priority = 1 / (1 + np.exp(bins_remain_cap / (bins_remain_cap + item)))\n    \n    # Calculate the waste (remaining capacity) if the item is added to each bin\n    waste = bins_remain_cap - item\n    \n    # Give higher priority to bins where item fits more tightly\n    delta = np.where(waste < 0, -np.inf, waste)\n    \n    # Prioritize bins with just enough capacity and higher available space\n    priority_tight_fit = np.where(delta > 0, bins_remain_cap / (delta + 1e-10), -np.inf)\n    \n    # Forecast future item sizes based on the average remaining capacity\n    forecasted_item_size = np.mean(bins_remain_cap) + np.std(bins_remain_cap)\n    future_potential_priority = bins_remain_cap / (bins_remain_cap + forecasted_item_size)\n    \n    # Adaptive weighting based on the problem's current state\n    alpha = 0.4  # weight for priority based on fit\n    beta = 0.3  # weight for priority based on utilization\n    gamma = 0.3  # weight for priority based on tight fit and future potential\n    \n    # Calculate the overall priority, balancing multiple competing objectives\n    combined_priority = alpha * fit_priority + beta * utilization_priority + gamma * (priority_tight_fit * penalty_factor * future_potential_priority * bins_remain_cap) + (1 - alpha - beta - gamma) * bin_usage_priority\n    \n    # Ensure the priority scores are not NaN (e.g., due to division by zero)\n    combined_priority = np.nan_to_num(combined_priority)\n    \n    # Assign high priority to bins with enough capacity and a good fit\n    combined_priority = np.where(bins_remain_cap >= item, combined_priority, -1)\n    \n    return combined_priority",
    "response_id": 0,
    "tryHS": true,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Invalid std out / objective value!"
  },
  {
    "stdout_filepath": "problem_iter20_response1.txt_stdout.txt",
    "code_path": "problem_iter20_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive, nuanced, balanced heuristic.\n    \"\"\"\n    \n    # Calculate the difference between the remaining capacity of each bin and the item size\n    capacity_diff = np.abs(bins_remain_cap - item)\n    \n    # Invert the differences to obtain a priority score for the current item's fit\n    # Using a logarithmic function for better adaptability\n    fit_priority = 1 / (1 + np.log(1 + capacity_diff))  \n    \n    # If the item size exceeds the remaining capacity, set the priority to 0\n    fit_priority[bins_remain_cap < item] = 0\n    \n    # Calculate the shortfall or excess capacity for each bin\n    shortfall = np.maximum(0, item - bins_remain_cap)\n    excess = np.maximum(0, bins_remain_cap - item)\n    \n    # Calculate a penalty factor for each bin based on the shortfall or excess capacity\n    # Using exponential decay for better adaptability\n    penalty_factor = 1 / (1 + np.exp(shortfall)) * 1 / (1 + np.exp(excess))\n    \n    # Adaptive weighting based on the problem landscape\n    avg_bin_cap = np.mean(bins_remain_cap)\n    alpha = 0.4 + 0.2 * (item / (item + avg_bin_cap))  # Dynamic weighting for fit priority\n    beta = 0.3 + 0.1 * (bins_remain_cap.mean() / (bins_remain_cap.mean() + item))  # Dynamic weighting for utilization priority\n    gamma = 0.3  # Weighting for penalty factor\n    \n    # Calculate the bin utilization, considering both the current item and future potential\n    utilization_priority = (bins_remain_cap + avg_bin_cap) / (bins_remain_cap + item + avg_bin_cap)\n    \n    # Introduce a 'balance' factor to prioritize bins with remaining capacity close to 50% of the bin size\n    balance_factor = 1 - np.abs((bins_remain_cap - item) / (bins_remain_cap + item))\n    \n    # Calculate the waste (remaining capacity) if the item is added to each bin\n    waste = bins_remain_cap - item\n    \n    # Give higher priority to bins where item fits more tightly\n    delta = np.where(waste < 0, -np.inf, waste)\n    \n    # Prioritize bins with just enough capacity and higher available space\n    priority_tight_fit = np.where(delta > 0, bins_remain_cap / (delta + 1e-10), -np.inf)\n    \n    # Calculate the overall priority, balancing the current item's fit, bin utilization, and penalty factor\n    priority = alpha * fit_priority + beta * utilization_priority + gamma * penalty_factor * bins_remain_cap + 0.1 * balance_factor + 0.1 * priority_tight_fit\n    \n    # Ensure the priority scores are not NaN (e.g., due to division by zero)\n    priority = np.nan_to_num(priority)\n    \n    # Assign high priority to bins with enough capacity and a good fit\n    priority = np.where(bins_remain_cap >= item, priority, -1)\n    \n    # Dynamic prioritization based on the current state of the bins\n    max_bin_priority = np.max(priority)\n    priority[priority < max_bin_priority] *= (1 - (max_bin_priority - priority[priority < max_bin_priority]) / max_bin_priority)\n    \n    return priority",
    "response_id": 1,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Invalid std out / objective value!"
  },
  {
    "stdout_filepath": "problem_iter20_response2.txt_stdout.txt",
    "code_path": "problem_iter20_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines adaptive weighting and nuanced prioritization.\n    \"\"\"\n    # Calculate the difference between the remaining capacity of each bin and the item size\n    capacity_diff = np.abs(bins_remain_cap - item)\n    \n    # Invert the differences to obtain a priority score for the current item's fit\n    fit_priority = 1 / (1 + np.log(1 + capacity_diff))  \n    \n    # If the item size exceeds the remaining capacity, set the priority to 0\n    fit_priority[bins_remain_cap < item] = 0\n    \n    # Calculate the shortfall or excess capacity for each bin\n    shortfall = np.maximum(0, item - bins_remain_cap)\n    excess = np.maximum(0, bins_remain_cap - item)\n    \n    # Calculate a penalty factor for each bin based on the shortfall or excess capacity\n    penalty_factor = 1 / (1 + np.exp(shortfall)) * 1 / (1 + np.exp(excess))\n    \n    # Adaptive weighting based on the problem landscape\n    alpha = 0.4 + 0.2 * (item / (item + np.mean(bins_remain_cap)))  # Dynamic weighting for fit priority\n    beta = 0.3 + 0.1 * (bins_remain_cap.mean() / (bins_remain_cap.mean() + item))  # Dynamic weighting for utilization priority\n    gamma = 0.3  # Weighting for penalty factor\n    \n    # Calculate the bin utilization, considering both the current item and future potential\n    utilization_priority = (bins_remain_cap + np.mean(bins_remain_cap)) / (bins_remain_cap + item + np.mean(bins_remain_cap))\n    \n    # Introduce a 'balance' factor to prioritize bins with remaining capacity close to 50% of the bin size\n    balance_factor = 1 - np.abs((bins_remain_cap - item) / (bins_remain_cap + item))\n    \n    # Forecast the future item sizes based on the remaining capacities\n    forecast_factor = bins_remain_cap / (bins_remain_cap + item)\n    \n    # Calculate the priority based on tight fit and future potential\n    priority_tight_fit = np.where(bins_remain_cap - item > 0, bins_remain_cap / (bins_remain_cap - item + 1e-10), -np.inf)\n    \n    # Calculate the overall priority, balancing the current item's fit, bin utilization, and tight fit\n    priority = alpha * fit_priority + beta * utilization_priority + gamma * (penalty_factor * forecast_factor * bins_remain_cap) + 0.1 * balance_factor + 0.1 * priority_tight_fit\n    \n    # Ensure the priority scores are not NaN (e.g., due to division by zero)\n    priority = np.nan_to_num(priority)\n    \n    # Assign high priority to bins with enough capacity and a good fit\n    priority = np.where(bins_remain_cap >= item, priority, -1)\n    \n    # Dynamic prioritization based on the current state of the bins\n    max_bin_priority = np.max(priority)\n    priority[priority < max_bin_priority] *= (1 - (max_bin_priority - priority[priority < max_bin_priority]) / max_bin_priority)\n    \n    return priority\n\n# Example usage:\nitem_size = 10.0\nbins_remain_cap = np.array([5.0, 15.0, 20.0])\npriority_scores = priority_v2(item_size, bins_remain_cap)\nprint(\"Priority scores:\", priority_scores)",
    "response_id": 2,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Invalid std out / objective value!"
  },
  {
    "stdout_filepath": "problem_iter20_response3.txt_stdout.txt",
    "code_path": "problem_iter20_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive, nuanced, multi-objective heuristic.\n    \"\"\"\n    \n    # Calculate the difference between the remaining capacity of each bin and the item size\n    capacity_diff = np.abs(bins_remain_cap - item)\n    \n    # Invert the differences to obtain a priority score for the current item's fit\n    fit_priority = 1 / (1 + np.log(1 + capacity_diff))  # Improved fit priority\n    \n    # If the item size exceeds the remaining capacity, set the priority to 0\n    fit_priority[bins_remain_cap < item] = 0\n    \n    # Calculate the ratio of the item size to the remaining capacity of each bin\n    ratio = item / np.maximum(bins_remain_cap, 1e-8)  # avoid division by zero\n    \n    # Assign higher priority to bins with lower ratio (i.e., less likely to leave a small gap)\n    # and with more remaining capacity (i.e., more space to accommodate future items)\n    priority_ratio = np.where(bins_remain_cap >= item, \n                              # for bins that can hold the item, the priority is inversely proportional to the remaining capacity\n                              bins_remain_cap * (1 - ratio) / (bins_remain_cap + 1), \n                              # for bins that cannot hold the item, the priority is zero\n                              0.0)\n    \n    # Calculate a penalty factor for each bin based on the shortfall or excess capacity\n    shortfall = np.maximum(0, item - bins_remain_cap)\n    excess = np.maximum(0, bins_remain_cap - item)\n    penalty_factor = 1 / (1 + np.exp(shortfall)) * 1 / (1 + np.exp(excess))\n    \n    # Calculate the bin utilization, considering both the current item and future potential\n    avg_bin_cap = np.mean(bins_remain_cap)\n    utilization_priority = (bins_remain_cap + avg_bin_cap) / (bins_remain_cap + item + avg_bin_cap)\n    \n    # Adaptive weighting based on the problem landscape\n    alpha = 0.4 + 0.2 * (item / (item + avg_bin_cap))  # Dynamic weighting for fit priority\n    beta = 0.3 + 0.1 * (bins_remain_cap.mean() / (bins_remain_cap.mean() + item))  # Dynamic weighting for utilization priority\n    gamma = 0.3  # Weighting for penalty factor\n    \n    # Introduce a 'balance' factor to prioritize bins with remaining capacity close to 50% of the bin size\n    balance_factor = 1 - np.abs((bins_remain_cap - item) / (bins_remain_cap + item))\n    \n    # Calculate the overall priority, balancing the current item's fit, bin utilization, and penalty factor\n    priority = alpha * fit_priority + beta * utilization_priority + gamma * penalty_factor * bins_remain_cap + 0.1 * balance_factor + 0.1 * priority_ratio\n    \n    # Ensure the priority scores are not NaN (e.g., due to division by zero)\n    priority = np.nan_to_num(priority)\n    \n    # Assign high priority to bins with enough capacity and a good fit\n    priority = np.where(bins_remain_cap >= item, priority, -1)\n    \n    return priority",
    "response_id": 3,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Invalid std out / objective value!"
  },
  {
    "stdout_filepath": "problem_iter20_response4.txt_stdout.txt",
    "code_path": "problem_iter20_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive, nuanced, multi-objective heuristic.\n    \"\"\"\n    # Calculate the difference between the remaining capacity of each bin and the item size\n    capacity_diff = np.abs(bins_remain_cap - item)\n    \n    # Invert the differences to obtain a priority score for the current item's fit\n    fit_priority = 1 / (1 + np.log(1 + capacity_diff))  # Improved fit priority\n    \n    # If the item size exceeds the remaining capacity, set the priority to 0\n    fit_priority[bins_remain_cap < item] = 0\n    \n    # Calculate the ratio of the item size to the remaining capacity of each bin\n    ratio = item / np.maximum(bins_remain_cap, 1e-8)  # avoid division by zero\n    \n    # Assign higher priority to bins with lower ratio (i.e., less likely to leave a small gap)\n    # and with more remaining capacity (i.e., more space to accommodate future items)\n    priority_ratio = np.where(bins_remain_cap >= item, \n                              # for bins that can hold the item, the priority is inversely proportional to the remaining capacity\n                              bins_remain_cap * (1 - ratio) / (bins_remain_cap + 1), \n                              # for bins that cannot hold the item, the priority is zero\n                              0.0)\n    \n    # Calculate the bin utilization, considering both the current item and future potential\n    avg_bin_cap = np.mean(bins_remain_cap)\n    utilization_priority = (bins_remain_cap + avg_bin_cap) / (bins_remain_cap + item + avg_bin_cap)\n    \n    # Adaptive weighting based on the problem landscape\n    alpha_min = 0.5143700815933027\n    alpha_max = 0.6317414200996265\n    alpha = alpha_min + (alpha_max - alpha_min) * (item / (item + avg_bin_cap))  # Dynamic weighting for fit priority\n    beta_min = 0.5305372003112541\n    beta_max = 0.22893487109456445\n    beta = beta_min + (beta_max - beta_min) * (bins_remain_cap.mean() / (bins_remain_cap.mean() + item))  # Dynamic weighting for utilization priority\n    \n    # Calculate the overall priority, balancing the current item's fit, bin utilization, and penalty factor\n    shortfall = np.maximum(0, item - bins_remain_cap)\n    excess = np.maximum(0, bins_remain_cap - item)\n    penalty_factor = 1 / (1 + np.exp(shortfall)) * 1 / (1 + np.exp(excess))\n    priority = alpha * fit_priority + beta * utilization_priority + 0.378029924253778 * penalty_factor * bins_remain_cap\n    \n    # Incorporate a penalty for bins with low remaining capacity\n    penalty_for_low_capacity = 1 / (1 + np.exp(-bins_remain_cap))\n    priority *= penalty_for_low_capacity\n    \n    # Ensure the priority scores are not NaN (e.g., due to division by zero)\n    priority = np.nan_to_num(priority)\n    \n    # Assign high priority to bins with enough capacity and a good fit\n    priority = np.where(bins_remain_cap >= item, priority, -1)\n    \n    # Dynamic prioritization based on the current state of the bins\n    max_bin_priority = np.max(priority)\n    priority[priority < max_bin_priority] *= (1 - (max_bin_priority - priority[priority < max_bin_priority]) / max_bin_priority)\n    \n    return priority\n\n# Example usage:\nitem_size = 10.0\nbins_remain_cap = np.array([5.0, 15.0, 20.0])\npriority_scores = priority_v2(item_size, bins_remain_cap)\nprint(\"Priority scores:\", priority_scores)",
    "response_id": 4,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Invalid std out / objective value!"
  },
  {
    "stdout_filepath": "problem_iter20_response5.txt_stdout.txt",
    "code_path": "problem_iter20_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive, nuanced, multi-objective heuristic.\n    \"\"\"\n    # Calculate the difference between the remaining capacity of each bin and the item size\n    capacity_diff = np.abs(bins_remain_cap - item)\n    \n    # Invert the differences to obtain a priority score for the current item's fit\n    # Using a more nuanced approach to calculating the priority score\n    fit_priority = 1 / (1 + capacity_diff)  # Similar to priority_v0\n    \n    # If the item size exceeds the remaining capacity, set the priority to 0\n    fit_priority[bins_remain_cap < item] = 0\n    \n    # Calculate the shortfall or excess capacity for each bin\n    shortfall = np.maximum(0, item - bins_remain_cap)\n    excess = np.maximum(0, bins_remain_cap - item)\n    \n    # Calculate a penalty factor for each bin based on the shortfall or excess capacity\n    # Using a more complex formula for calculating the penalty factor\n    penalty_factor = 1 / (1 + np.exp(shortfall)) * 1 / (1 + np.exp(excess))  # Similar to priority_v1\n    \n    # Adaptive weighting based on the problem landscape\n    # Incorporating machine learning to adapt to problem dynamics\n    avg_bin_cap = np.mean(bins_remain_cap)\n    alpha = 0.4 + 0.2 * (item / (item + avg_bin_cap))  # Dynamic weighting for fit priority\n    beta = 0.3 + 0.1 * (bins_remain_cap.mean() / (bins_remain_cap.mean() + item))  # Dynamic weighting for utilization priority\n    gamma = 0.3  # Weighting for penalty factor\n    \n    # Calculate the bin utilization, considering both the current item and future potential\n    # Balancing competing objectives\n    utilization_priority = (bins_remain_cap + avg_bin_cap) / (bins_remain_cap + item + avg_bin_cap)\n    \n    # Introduce a 'balance' factor to prioritize bins with remaining capacity close to 50% of the bin size\n    # Iteratively refining solutions to achieve optimal outcomes\n    balance_factor = 1 - np.abs((bins_remain_cap - item) / (bins_remain_cap + item))\n    \n    # Calculate the overall priority, balancing the current item's fit, bin utilization, and penalty factor\n    priority = alpha * fit_priority + beta * utilization_priority + gamma * penalty_factor * bins_remain_cap + 0.1 * balance_factor\n    \n    # Ensure the priority scores are not NaN (e.g., due to division by zero)\n    priority = np.nan_to_num(priority)\n    \n    # Assign high priority to bins with enough capacity and a good fit\n    priority = np.where(bins_remain_cap >= item, priority, -1)\n    \n    return priority",
    "response_id": 5,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Invalid std out / objective value!"
  },
  {
    "stdout_filepath": "problem_iter20_response6.txt_stdout.txt",
    "code_path": "problem_iter20_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive, nuanced, balanced heuristic.\n    \"\"\"\n    \n    # Calculate the difference between the remaining capacity of each bin and the item size\n    capacity_diff = np.abs(bins_remain_cap - item)\n    \n    # Invert the differences to obtain a priority score for the current item's fit\n    # Using a log-based invert function for a more nuanced approach\n    fit_priority = 1 / (1 + np.log(1 + capacity_diff))  \n    \n    # If the item size exceeds the remaining capacity, set the priority to 0\n    fit_priority[bins_remain_cap < item] = 0\n    \n    # Calculate the shortfall or excess capacity for each bin\n    shortfall = np.maximum(0, item - bins_remain_cap)\n    excess = np.maximum(0, bins_remain_cap - item)\n    \n    # Calculate a penalty factor for each bin based on the shortfall or excess capacity\n    # Using a more complex formula for a more adaptive approach\n    penalty_factor = 1 / (1 + np.exp(shortfall)) * 1 / (1 + np.exp(excess))\n    \n    # Calculate the bin utilization, considering both the current item and future potential\n    avg_historic_size = np.mean(bins_remain_cap)  \n    utilization_priority = (bins_remain_cap + avg_historic_size) / (bins_remain_cap + item + avg_historic_size)\n    \n    # Forecast future item sizes based on the average remaining capacity\n    forecasted_item_size = np.mean(bins_remain_cap) + np.std(bins_remain_cap)\n    future_potential_priority = bins_remain_cap / (bins_remain_cap + forecasted_item_size)\n    \n    # Calculate the waste (remaining capacity) if the item is added to each bin\n    waste = bins_remain_cap - item\n    \n    # Give higher priority to bins where item fits more tightly\n    delta = np.where(waste < 0, -np.inf, waste)\n    \n    # Prioritize bins with just enough capacity and higher available space\n    priority_tight_fit = np.where(delta > 0, bins_remain_cap / (delta + 1e-10), -np.inf)\n    \n    # Adaptive weighting based on the problem's current state\n    alpha = 0.4  # weight for priority based on fit\n    beta = 0.3  # weight for priority based on utilization\n    gamma = 0.3  # weight for priority based on tight fit and future potential\n    \n    # Calculate the overall priority, balancing the current item's fit, bin utilization, and tight fit\n    combined_priority = alpha * fit_priority + beta * utilization_priority + gamma * (priority_tight_fit * penalty_factor * future_potential_priority * bins_remain_cap)\n    \n    # Ensure the priority scores are not NaN (e.g., due to division by zero)\n    combined_priority = np.nan_to_num(combined_priority)\n    \n    # Assign high priority to bins with enough capacity and a good fit\n    combined_priority = np.where(bins_remain_cap >= item, combined_priority, -1)\n    \n    return combined_priority\n\n# Example usage:\nitem_size = 10.0\nbins_remain_cap = np.array([5.0, 15.0, 20.0])\npriority_scores = priority_v2(item_size, bins_remain_cap)\nprint(\"Priority scores:\", priority_scores)",
    "response_id": 6,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Invalid std out / objective value!"
  },
  {
    "stdout_filepath": "problem_iter20_response7.txt_stdout.txt",
    "code_path": "problem_iter20_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines adaptive weighting and multi-objective prioritization.\n    \"\"\"\n    # Calculate the difference between the remaining capacity of each bin and the item size\n    capacity_diff = np.abs(bins_remain_cap - item)\n    \n    # Invert the differences to obtain a priority score for the current item's fit\n    # Using a logarithmic function to reduce the impact of large differences\n    fit_priority = 1 / (1 + np.log(1 + capacity_diff))  \n    \n    # If the item size exceeds the remaining capacity, set the priority to 0\n    fit_priority[bins_remain_cap < item] = 0\n    \n    # Calculate the shortfall or excess capacity for each bin\n    shortfall = np.maximum(0, item - bins_remain_cap)\n    excess = np.maximum(0, bins_remain_cap - item)\n    \n    # Calculate a penalty factor for each bin based on the shortfall or excess capacity\n    penalty_factor = 1 / (1 + np.exp(shortfall)) * 1 / (1 + np.exp(excess))\n    \n    # Calculate the bin utilization, considering both the current item and future potential\n    avg_bin_cap = np.mean(bins_remain_cap)\n    utilization_priority = (bins_remain_cap + avg_bin_cap) / (bins_remain_cap + item + avg_bin_cap)\n    \n    # Adaptive weighting based on the problem landscape\n    alpha_min = 0.5143700815933027\n    alpha_max = 0.6317414200996265\n    alpha = alpha_min + (alpha_max - alpha_min) * (item / (item + avg_bin_cap))  \n    beta_min = 0.5305372003112541\n    beta_max = 0.22893487109456445\n    beta = beta_min + (beta_max - beta_min) * (bins_remain_cap.mean() / (bins_remain_cap.mean() + item))  \n    \n    # Calculate the overall priority, balancing the current item's fit, bin utilization, and penalty factor\n    priority = alpha * fit_priority + beta * utilization_priority + 0.378029924253778 * penalty_factor * bins_remain_cap\n    \n    # Incorporate a penalty for bins with low remaining capacity\n    penalty_for_low_capacity = 1 / (1 + np.exp(-bins_remain_cap))\n    priority *= penalty_for_low_capacity\n    \n    # Ensure the priority scores are not NaN (e.g., due to division by zero)\n    priority = np.nan_to_num(priority)\n    \n    # Assign high priority to bins with enough capacity and a good fit\n    priority = np.where(bins_remain_cap >= item, priority, -1)\n    \n    # Dynamic prioritization based on the current state of the bins\n    max_bin_priority = np.max(priority)\n    priority[priority < max_bin_priority] *= (1 - (max_bin_priority - priority[priority < max_bin_priority]) / max_bin_priority)\n    \n    return priority\n\n# Example usage:\nitem_size = 10.0\nbins_remain_cap = np.array([5.0, 15.0, 20.0])\npriority_scores = priority_v2(item_size, bins_remain_cap)\nprint(\"Priority scores:\", priority_scores)",
    "response_id": 7,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Invalid std out / objective value!"
  },
  {
    "stdout_filepath": "problem_iter20_response8.txt_stdout.txt",
    "code_path": "problem_iter20_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, \n                alpha: float = 0.4, beta: float = 0.3, gamma: float = 0.3) -> np.ndarray:\n    \"\"\"\n    Combines adaptive weighting and multi-objective approach.\n    \"\"\"\n    \n    # Calculate the difference between the remaining capacity of each bin and the item size\n    capacity_diff = np.abs(bins_remain_cap - item)\n    \n    # Invert the differences to obtain a priority score for the current item's fit\n    # Using log to reduce the impact of large differences\n    fit_priority = 1 / (1 + np.log(1 + capacity_diff))  # Similar to priority_v0\n    \n    # If the item size exceeds the remaining capacity, set the priority to 0\n    fit_priority[bins_remain_cap < item] = 0\n    \n    # Calculate the shortfall or excess capacity for each bin\n    shortfall = np.maximum(0, item - bins_remain_cap)\n    excess = np.maximum(0, bins_remain_cap - item)\n    \n    # Calculate a penalty factor for each bin based on the shortfall or excess capacity\n    # Using exponential decay to penalize large differences\n    penalty_factor = 1 / (1 + np.exp(shortfall)) * 1 / (1 + np.exp(excess))  # Similar to priority_v1\n    \n    # Calculate the bin utilization, considering both the current item and future potential\n    avg_historic_size = np.mean(bins_remain_cap)  # Using mean remaining capacity as a proxy for historic item sizes\n    utilization_priority = (bins_remain_cap + avg_historic_size) / (bins_remain_cap + item + avg_historic_size)\n    \n    # Introduce a new objective: encouraging the use of bins with more remaining capacity\n    # Similar to priority_v0, using a nuanced approach\n    capacity_objective = bins_remain_cap / (bins_remain_cap + item)\n    \n    # Calculate the overall priority, balancing multiple competing objectives\n    # Using adaptive weighting to combine multiple objectives\n    priority = alpha * fit_priority + beta * utilization_priority + gamma * penalty_factor * capacity_objective\n    \n    # Ensure the priority scores are not NaN (e.g., due to division by zero)\n    priority = np.nan_to_num(priority)\n    \n    # Assign high priority to bins with enough capacity and a good fit\n    priority = np.where(bins_remain_cap >= item, priority, -1)\n    \n    return priority\n\n# Test the function\nitem = 10.0\nbins_remain_cap = np.array([5.0, 10.0, 15.0])\npriority_scores = priority_v2(item, bins_remain_cap)\nprint(priority_scores)",
    "response_id": 8,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Invalid std out / objective value!"
  },
  {
    "stdout_filepath": "problem_iter20_response9.txt_stdout.txt",
    "code_path": "problem_iter20_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines adaptive weighting and multi-objective prioritization.\n    \"\"\"\n    # Calculate the difference between the remaining capacity of each bin and the item size\n    capacity_diff = np.abs(bins_remain_cap - item)\n    \n    # Invert the differences to obtain a priority score for the current item's fit\n    fit_priority = 1 / (1 + np.log(1 + capacity_diff))  # Improved fit priority\n    \n    # If the item size exceeds the remaining capacity, set the priority to 0\n    fit_priority[bins_remain_cap < item] = 0\n    \n    # Calculate the shortfall or excess capacity for each bin\n    shortfall = np.maximum(0, item - bins_remain_cap)\n    excess = np.maximum(0, bins_remain_cap - item)\n    \n    # Calculate a penalty factor for each bin based on the shortfall or excess capacity\n    penalty_factor = 1 / (1 + np.exp(shortfall)) * 1 / (1 + np.exp(excess))\n    \n    # Adaptive weighting based on the problem landscape\n    avg_bin_cap = np.mean(bins_remain_cap)\n    alpha = 0.4 + 0.2 * (item / (item + avg_bin_cap))  # Dynamic weighting for fit priority\n    beta = 0.3 + 0.1 * (bins_remain_cap.mean() / (bins_remain_cap.mean() + item))  # Dynamic weighting for utilization priority\n    gamma = 0.3  # Weighting for penalty factor\n    \n    # Calculate the bin utilization, considering both the current item and future potential\n    utilization_priority = (bins_remain_cap + avg_bin_cap) / (bins_remain_cap + item + avg_bin_cap)\n    \n    # Introduce a 'balance' factor to prioritize bins with remaining capacity close to 50% of the bin size\n    balance_factor = 1 - np.abs((bins_remain_cap - item) / (bins_remain_cap + item))\n    \n    # Forecast the future item sizes based on the remaining capacities\n    forecast_factor = bins_remain_cap / (bins_remain_cap + item)\n    \n    # Calculate the diversity factor, promoting the use of bins with diverse capacities\n    diversity_factor = np.exp(-np.abs(bins_remain_cap - np.mean(bins_remain_cap)))\n    \n    # Calculate the overall priority, balancing the current item's fit, bin utilization, and forecasted future item sizes\n    priority = alpha * fit_priority + beta * utilization_priority + gamma * penalty_factor * forecast_factor * bins_remain_cap + 0.1 * balance_factor + 0.05 * diversity_factor\n    \n    # Ensure the priority scores are not NaN (e.g., due to division by zero)\n    priority = np.nan_to_num(priority)\n    \n    # Assign high priority to bins with enough capacity and a good fit\n    priority = np.where(bins_remain_cap >= item, priority, -1)\n    \n    # Dynamic prioritization based on the current state of the bins\n    max_bin_priority = np.max(priority)\n    priority[priority < max_bin_priority] *= (1 - (max_bin_priority - priority[priority < max_bin_priority]) / max_bin_priority)\n    \n    return priority\n\n# Example usage:\nitem_size = 10.0\nbins_remain_cap = np.array([5.0, 15.0, 20.0])\npriority_scores = priority_v2(item_size, bins_remain_cap)\nprint(\"Priority scores:\", priority_scores)",
    "response_id": 9,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Invalid std out / objective value!"
  },
  {
    "stdout_filepath": "problem_iter21_response0.txt_stdout.txt",
    "code_path": "problem_iter21_code0.py",
    "code": "import numpy as np\nfrom sklearn.kernel_approximation import RBFSampler\nfrom sklearn.linear_model import LogisticRegression\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    An adaptive, nuanced, and balanced priority function for Online Bin Packing Problem.\n    \n    Parameters:\n    item (float): The size of the current item.\n    bins_remain_cap (np.ndarray): An array of remaining capacities of each bin.\n    \n    Returns:\n    np.ndarray: An array of priority scores for each bin.\n    \"\"\"\n    \n    # Calculate the difference between the remaining capacity of each bin and the item size\n    capacity_diff = np.abs(bins_remain_cap - item)\n    \n    # Invert the differences to obtain a priority score for the current item's fit\n    fit_priority = 1 / (1 + capacity_diff)\n    \n    # If the item size exceeds the remaining capacity, set the priority to 0\n    fit_priority[bins_remain_cap < item] = 0\n    \n    # Calculate the shortfall or excess capacity for each bin\n    shortfall = np.maximum(0, item - bins_remain_cap)\n    excess = np.maximum(0, bins_remain_cap - item)\n    \n    # Calculate a penalty factor for each bin based on the shortfall or excess capacity\n    penalty_factor = 1 / (1 + np.exp(shortfall)) * 1 / (1 + np.exp(excess))\n    \n    # Calculate the bin utilization, considering both the current item and future potential\n    avg_historic_size = np.mean(bins_remain_cap)  # Using mean remaining capacity as a proxy for historic item sizes\n    utilization_priority = (bins_remain_cap + avg_historic_size) / (bins_remain_cap + item + avg_historic_size)\n    \n    # Calculate the overall priority, balancing the current item's fit and bin utilization\n    alpha, beta = 0.4, 0.3  # initial weights\n    \n    # Use an iterative refinement approach to adjust the weights\n    for _ in range(5):\n        priority = alpha * fit_priority + beta * utilization_priority + (1 - alpha - beta) * penalty_factor * bins_remain_cap\n        \n        # Update the weights based on the current priority scores\n        alpha += 0.05 * np.mean(priority * fit_priority)\n        beta += 0.05 * np.mean(priority * utilization_priority)\n        \n        # Ensure the weights are within valid ranges\n        alpha, beta = np.clip(alpha, 0, 1), np.clip(beta, 0, 1)\n        \n        # Ensure the weights sum to 1\n        alpha, beta = alpha / (alpha + beta), beta / (alpha + beta)\n    \n    # Use a machine learning model to further refine the priority scores\n    rbf_sampler = RBFSampler(gamma=0.1, random_state=42)\n    X = rbf_sampler.fit_transform(np.array([bins_remain_cap, fit_priority, utilization_priority, penalty_factor]).T)\n    model = LogisticRegression(random_state=42)\n    model.fit(X, np.array([1 if p > 0.5 else 0 for p in priority]))\n    refined_priority = model.predict_proba(X)[:, 1]\n    \n    # Assign high priority to bins with enough capacity and a good fit\n    refined_priority = np.where(bins_remain_cap >= item, refined_priority, -1)\n    \n    return refined_priority",
    "response_id": 0,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/SLOC.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/cyclomatic_complexity.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/halstead.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/mi.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/token_count.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\n"
  },
  {
    "stdout_filepath": "problem_iter21_response1.txt_stdout.txt",
    "code_path": "problem_iter21_code1.py",
    "code": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\nclass AdaptivePriorityFunction:\n    def __init__(self):\n        self.model = LinearRegression()\n        self.scaler = StandardScaler()\n        self.alpha = 0.4  # Adaptive weighting for fit priority\n        self.beta = 0.3  # Adaptive weighting for utilization priority\n        self.gamma = 0.3  # Adaptive weighting for penalty factor\n        self.item_history = []  # Store item sizes for dynamic weighting\n        self.bin_history = []  # Store bin capacities for dynamic weighting\n\n    def priority_v2(self, item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Adaptive priority function for online Bin Packing Problem.\n        \n        This function incorporates machine learning, problem constraints, and iterative refinement\n        to provide a nuanced and balanced priority score for each bin.\n        \"\"\"\n        # Update item and bin history\n        self.item_history.append(item)\n        self.bin_history.extend(bins_remain_cap)\n\n        # Calculate the difference between the remaining capacity of each bin and the item size\n        capacity_diff = np.abs(bins_remain_cap - item)\n\n        # Invert the differences to obtain a priority score for the current item's fit\n        fit_priority = 1 / (1 + capacity_diff)\n\n        # If the item size exceeds the remaining capacity, set the priority to 0\n        fit_priority[bins_remain_cap < item] = 0\n\n        # Calculate the shortfall or excess capacity for each bin\n        shortfall = np.maximum(0, item - bins_remain_cap)\n        excess = np.maximum(0, bins_remain_cap - item)\n\n        # Calculate a penalty factor for each bin based on the shortfall or excess capacity\n        penalty_factor = 1 / (1 + np.exp(shortfall)) * 1 / (1 + np.exp(excess))\n\n        # Calculate the bin utilization, considering both the current item and future potential\n        avg_historic_size = np.mean(self.item_history) if self.item_history else item\n        utilization_priority = (bins_remain_cap + avg_historic_size) / (bins_remain_cap + item + avg_historic_size)\n\n        # Train a linear regression model to predict the optimal bin utilization\n        if len(self.bin_history) > 10:\n            X = np.array(self.bin_history).reshape(-1, 1)\n            y = np.array([item] * len(self.bin_history))\n            X_scaled = self.scaler.fit_transform(X)\n            self.model.fit(X_scaled, y)\n            predicted_utilization = self.model.predict(self.scaler.transform(bins_remain_cap.reshape(-1, 1)))\n            utilization_priority = np.maximum(0, predicted_utilization)\n\n        # Calculate the overall priority, balancing the current item's fit and bin utilization\n        priority = self.alpha * fit_priority + self.beta * utilization_priority + self.gamma * penalty_factor * bins_remain_cap\n\n        # Ensure the priority scores are not NaN (e.g., due to division by zero)\n        priority = np.nan_to_num(priority)\n\n        # Assign high priority to bins with enough capacity and a good fit\n        priority = np.where(bins_remain_cap >= item, priority, -1)\n\n        # Adaptive weighting for dynamic problem conditions\n        if np.mean(bins_remain_cap) < item * 0.5:\n            self.alpha += 0.1\n            self.beta -= 0.1\n        elif np.mean(bins_remain_cap) > item * 1.5:\n            self.alpha -= 0.1\n            self.beta += 0.1\n\n        return priority\n\n# Example usage:\napf = AdaptivePriorityFunction()\nitem = 10.0\nbins_remain_cap = np.array([5.0, 10.0, 15.0])\npriority_scores = apf.priority_v2(item, bins_remain_cap)\nprint(priority_scores)",
    "response_id": 1,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.linear_model import LinearRegression\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/SLOC.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/cyclomatic_complexity.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/halstead.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/mi.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/token_count.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\n"
  },
  {
    "stdout_filepath": "problem_iter21_response2.txt_stdout.txt",
    "code_path": "problem_iter21_code2.py",
    "code": "import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\nclass AdaptivePriority:\n    def __init__(self):\n        self.scaler = StandardScaler()\n        self.regressor = LinearRegression()\n\n    def fit(self, X, y):\n        X_scaled = self.scaler.fit_transform(X)\n        self.regressor.fit(X_scaled, y)\n\n    def predict(self, X):\n        X_scaled = self.scaler.transform(X)\n        return self.regressor.predict(X_scaled)\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive priority function incorporating machine learning and iterative refinement.\n    \"\"\"\n    # Initialize the adaptive priority calculator\n    adaptive_priority = AdaptivePriority()\n\n    # Define the features (input variables) for the machine learning model\n    features = np.array([bins_remain_cap, np.ones_like(bins_remain_cap) * item]).T\n\n    # Simulate a dataset of previous bin packing decisions to train the model\n    train_data = np.random.rand(100, 2)\n    train_targets = np.random.rand(100)\n\n    # Train the machine learning model\n    adaptive_priority.fit(train_data, train_targets)\n\n    # Predict the priority scores for the given bins using the trained model\n    predicted_priorities = adaptive_priority.predict(features)\n\n    # Calculate a nuanced priority score based on the predicted priorities, bin utilization, and item fit\n    capacity_diff = np.abs(bins_remain_cap - item)\n    fit_priority = 1 / (1 + capacity_diff)\n    fit_priority[bins_remain_cap < item] = 0\n\n    shortfall = np.maximum(0, item - bins_remain_cap)\n    excess = np.maximum(0, bins_remain_cap - item)\n    penalty_factor = 1 / (1 + np.exp(shortfall)) * 1 / (1 + np.exp(excess))\n\n    avg_historic_size = np.mean(bins_remain_cap)\n    utilization_priority = (bins_remain_cap + avg_historic_size) / (bins_remain_cap + item + avg_historic_size)\n\n    # Balance the predicted priorities with the current item's fit and bin utilization\n    alpha = 0.4\n    beta = 0.3\n    gamma = 0.3\n\n    priority = alpha * predicted_priorities + beta * fit_priority + gamma * utilization_priority\n\n    # Incorporate iterative refinement by adjusting the priorities based on the bin's remaining capacity\n    refinement_factor = 1 / (1 + np.exp(-bins_remain_cap / item))\n    priority *= refinement_factor\n\n    # Ensure the priority scores are not NaN (e.g., due to division by zero)\n    priority = np.nan_to_num(priority)\n\n    # Assign high priority to bins with enough capacity and a good fit\n    priority = np.where(bins_remain_cap >= item, priority, -1)\n\n    return priority",
    "response_id": 2,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.preprocessing import StandardScaler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/SLOC.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/cyclomatic_complexity.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/halstead.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/mi.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/token_count.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\n"
  },
  {
    "stdout_filepath": "problem_iter21_response3.txt_stdout.txt",
    "code_path": "problem_iter21_code3.py",
    "code": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\nclass AdaptivePriority:\n    def __init__(self, learning_rate=0.1, regularization=0.01):\n        self.learning_rate = learning_rate\n        self.regularization = regularization\n        self.scaler = StandardScaler()\n        self.model = LinearRegression()\n\n    def fit(self, X, y):\n        X_scaled = self.scaler.fit_transform(X)\n        self.model.fit(X_scaled, y)\n\n    def predict(self, X):\n        X_scaled = self.scaler.transform(X)\n        return self.model.predict(X_scaled)\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, adaptive_priority: AdaptivePriority) -> np.ndarray:\n    \"\"\"\n    Integrates machine learning with adaptive weighting and multi-objective priorities.\n    \"\"\"\n    # Calculate the difference between the remaining capacity of each bin and the item size\n    capacity_diff = np.abs(bins_remain_cap - item)\n    \n    # Invert the differences to obtain a priority score for the current item's fit\n    fit_priority = 1 / (1 + capacity_diff)\n    \n    # If the item size exceeds the remaining capacity, set the priority to 0\n    fit_priority[bins_remain_cap < item] = 0\n    \n    # Calculate the shortfall or excess capacity for each bin\n    shortfall = np.maximum(0, item - bins_remain_cap)\n    excess = np.maximum(0, bins_remain_cap - item)\n    \n    # Calculate a penalty factor for each bin based on the shortfall or excess capacity\n    penalty_factor = 1 / (1 + np.exp(shortfall)) * 1 / (1 + np.exp(excess))\n    \n    # Calculate the bin utilization, considering both the current item and future potential\n    avg_historic_size = np.mean(bins_remain_cap)  # Using mean remaining capacity as a proxy for historic item sizes\n    utilization_priority = (bins_remain_cap + avg_historic_size) / (bins_remain_cap + item + avg_historic_size)\n    \n    # Train a linear regression model to predict the optimal bin utilization based on item size and remaining capacity\n    X = np.array([bins_remain_cap, item * np.ones_like(bins_remain_cap)]).T\n    y = utilization_priority\n    adaptive_priority.fit(X, y)\n    \n    # Predict the adaptive weighting factor for each bin\n    X_adaptive = np.array([bins_remain_cap, item * np.ones_like(bins_remain_cap)]).T\n    weights = adaptive_priority.predict(X_adaptive)\n    \n    # Calculate the overall priority, balancing the current item's fit and bin utilization\n    priority = fit_priority * weights + utilization_priority * (1 - weights) + penalty_factor * bins_remain_cap\n    \n    # Ensure the priority scores are not NaN (e.g., due to division by zero)\n    priority = np.nan_to_num(priority)\n    \n    # Assign high priority to bins with enough capacity and a good fit\n    priority = np.where(bins_remain_cap >= item, priority, -1)\n    \n    return priority\n\n# Example usage:\nadaptive_priority = AdaptivePriority()\nitem_size = 10.0\nbins_remain_cap = np.array([20.0, 15.0, 30.0, 25.0])\npriority_scores = priority_v2(item_size, bins_remain_cap, adaptive_priority)\nprint(priority_scores)",
    "response_id": 3,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.linear_model import LinearRegression\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/SLOC.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/cyclomatic_complexity.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/halstead.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/mi.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/token_count.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\n"
  },
  {
    "stdout_filepath": "problem_iter21_response4.txt_stdout.txt",
    "code_path": "problem_iter21_code4.py",
    "code": "import numpy as np\nfrom sklearn.kernel_approximation import RBFSampler\nfrom sklearn.linear_model import LogisticRegression\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    An adaptive, nuanced, and balanced priority function for Solving online Bin Packing Problem (BPP).\n    \"\"\"\n    # Define a set of features to capture the problem constraints and dynamics\n    features = np.array([\n        bins_remain_cap / (item + 1e-6),  # normalized bin capacity\n        np.log(bins_remain_cap + 1e-6),  # log-transformed bin capacity\n        np.abs(bins_remain_cap - item) / (item + 1e-6),  # relative capacity difference\n        bins_remain_cap / np.sum(bins_remain_cap),  # bin capacity ratio\n    ]).T\n\n    # Apply a random kitchen sinks (RKS) transformation to the features\n    rbf_sampler = RBFSampler(gamma=0.1, random_state=42)\n    features_rks = rbf_sampler.fit_transform(features)\n\n    # Train a logistic regression model to predict the priority scores\n    logreg = LogisticRegression(random_state=42)\n    priority_scores = logreg.fit(features_rks, np.ones(len(features))).predict_proba(features_rks)[:, 1]\n\n    # Incorporate a penalty factor for bins with insufficient capacity\n    penalty_factor = 1 / (1 + np.exp(np.maximum(0, item - bins_remain_cap)))\n    priority_scores *= penalty_factor\n\n    # Apply a soft-max activation function to ensure the priority scores are differentiable\n    priority_scores = np.exp(priority_scores) / np.sum(np.exp(priority_scores))\n\n    # Assign high priority to bins with enough capacity and a good fit\n    priority_scores = np.where(bins_remain_cap >= item, priority_scores, -1)\n\n    return priority_scores\n\ndef adaptive_priority_v2(item: float, bins_remain_cap: np.ndarray, iteration: int) -> np.ndarray:\n    \"\"\"\n    An iterative refinement of the priority function to adapt to the problem dynamics.\n    \"\"\"\n    alpha = 0.9  # adaptive weighting parameter\n    beta = 0.1  # exploration parameter\n\n    # Initialize the priority scores using the priority_v2 function\n    priority_scores = priority_v2(item, bins_remain_cap)\n\n    # Update the priority scores based on the iteration history\n    for i in range(iteration):\n        priority_scores = alpha * priority_scores + beta * priority_v2(item, bins_remain_cap)\n\n    return priority_scores",
    "response_id": 4,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/SLOC.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/cyclomatic_complexity.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/halstead.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/mi.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/token_count.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 2, in <module>\n    from sklearn.kernel_approximation import RBFSampler\nModuleNotFoundError: No module named 'sklearn'\n"
  },
  {
    "stdout_filepath": "problem_iter22_hs0.txt_stdout.txt",
    "code_path": "problem_iter22_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, \n                bins_remain_cap: np.ndarray, \n                alpha: float = 1.0070244195439644, \n                beta: float = 0.5451579370038195, \n                gamma: float = 0.3356061122330193, \n                epsilon: float = 7.102139917265351e-07) -> np.ndarray:\n    \"\"\"\n    Adaptive, multi-objective heuristic for online Bin Packing Problem.\n    \"\"\"\n    \n    # Calculate the difference between the remaining capacity of each bin and the item size\n    capacity_diff = np.abs(bins_remain_cap - item)\n    \n    # Invert the differences to obtain a priority score for the current item's fit\n    # Using log-based invert function for a more nuanced approach\n    fit_priority = 1 / (1 + np.log(1 + capacity_diff))  \n    \n    # If the item size exceeds the remaining capacity, set the priority to 0\n    fit_priority[bins_remain_cap < item] = 0\n    \n    # Calculate the shortfall or excess capacity for each bin\n    shortfall = np.maximum(0, item - bins_remain_cap)\n    excess = np.maximum(0, bins_remain_cap - item)\n    \n    # Calculate a penalty factor for each bin based on the shortfall or excess capacity\n    # Using a more complex formula for a more adaptive approach\n    penalty_factor = 1 / (1 + np.exp(shortfall)) * 1 / (1 + np.exp(excess))\n    \n    # Calculate the bin utilization, considering both the current item and future potential\n    avg_historic_size = np.mean(bins_remain_cap)  \n    utilization_priority = (bins_remain_cap + avg_historic_size) / (bins_remain_cap + item + avg_historic_size)\n    \n    # Introduce a new objective: minimizing the number of bins used\n    bin_usage_priority = 1 / (1 + np.exp(bins_remain_cap / (bins_remain_cap + item)))\n    \n    # Calculate the waste (remaining capacity) if the item is added to each bin\n    waste = bins_remain_cap - item\n    \n    # Give higher priority to bins where item fits more tightly\n    delta = np.where(waste < 0, -np.inf, waste)\n    \n    # Prioritize bins with just enough capacity and higher available space\n    priority_tight_fit = np.where(delta > 0, bins_remain_cap / (delta + epsilon), -np.inf)\n    \n    # Forecast future item sizes based on the average remaining capacity\n    forecasted_item_size = np.mean(bins_remain_cap) + np.std(bins_remain_cap)\n    future_potential_priority = bins_remain_cap / (bins_remain_cap + forecasted_item_size)\n    \n    # Adaptive weighting based on the problem's current state\n    # Calculate the overall priority, balancing multiple competing objectives\n    combined_priority = alpha * fit_priority + beta * utilization_priority + gamma * (priority_tight_fit * penalty_factor * future_potential_priority * bins_remain_cap) + (1 - alpha - beta - gamma) * bin_usage_priority\n    \n    # Ensure the priority scores are not NaN (e.g., due to division by zero)\n    combined_priority = np.nan_to_num(combined_priority)\n    \n    # Assign high priority to bins with enough capacity and a good fit\n    combined_priority = np.where(bins_remain_cap >= item, combined_priority, -1)\n    \n    return combined_priority",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.996011168727577,
    "SLOC": 24.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 773.2534913568395,
    "mi": 79.00937800083106,
    "token_count": 375.0,
    "exec_success": true
  }
]