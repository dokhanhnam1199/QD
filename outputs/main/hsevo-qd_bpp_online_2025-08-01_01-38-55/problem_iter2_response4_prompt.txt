{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This heuristic implements a \"Best Fit\" strategy, prioritizing bins\n    that, after accommodating the item, would leave the smallest amount of\n    remaining space. This aims to keep bins as \"tightly packed\" as possible,\n    reserving larger empty spaces for potentially larger future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining space in each bin if the item were placed there.\n    # A negative value here means the item does not fit.\n    potential_remain_after_placement = bins_remain_cap - item\n\n    # Initialize priority scores.\n    # We use a float array to allow for -np.inf.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify which bins can actually fit the item (remaining capacity >= 0).\n    can_fit_mask = potential_remain_after_placement >= 0\n\n    # For bins that can fit the item:\n    # The priority is the negative of the remaining space.\n    # A smaller remaining space (e.g., 0.1) will result in a larger negative score (-0.1).\n    # A larger remaining space (e.g., 0.5) will result in a smaller negative score (-0.5).\n    # Since we want to choose the bin with the HIGHEST priority score,\n    # minimizing `potential_remain_after_placement` corresponds to maximizing\n    # `-potential_remain_after_placement`. This is the core of Best Fit.\n    priorities[can_fit_mask] = -potential_remain_after_placement[can_fit_mask]\n\n    # For bins that cannot fit the item:\n    # Assign a very low priority (negative infinity) to ensure they are never\n    # selected if there is any bin that can accommodate the item.\n    priorities[~can_fit_mask] = -np.inf\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), we observe a crucial difference in how \"tightness of fit\" is translated into a priority score. Heuristic 1st employs an inverse relationship (`1.0 / (residual_space + epsilon)`), causing priority to skyrocket as residual space approaches zero. This non-linear scaling gives a disproportionately high preference to perfect or near-perfect fits, aggressively pushing for bin completion. In contrast, Heuristic 2nd (and 3rd-8th, 10th) uses a linear negative relationship (`-residual_capacity` or `-slack`). While still implementing \"Best Fit\" by favoring smaller residuals, the priority difference between a near-perfect fit (e.g., 0.01 residual) and a slightly larger one (e.g., 0.1 residual) is much smaller in the linear scheme, making it less aggressive in seeking out extremely tight configurations. This explains why the inverse weighting (1st and 9th) is ranked higher.\n\nComparing (3rd) vs (4th), and similarly (5th) vs (6th), and (7th) vs (8th): these pairs are identical in their implementation logic. Their ranking suggests that minor variations in docstring descriptions or variable naming do not impact performance if the core heuristic calculation remains the same. This reinforces that the *mathematical formulation* of the priority function is paramount.\n\nComparing (10th) vs (11th), and consistently across (11th-20th): Heuristic 10th still attempts a \"Best Fit\" by minimizing residual space. However, Heuristics 11th through 20th are all identical and severely flawed; they simply return `np.zeros_like(bins_remain_cap)`. This effectively means all bins that can fit the item have an equal, non-discriminating priority. When `np.argmax` is applied, it will consistently pick the bin with the lowest index that can accommodate the item, which is essentially a \"First Fit\" strategy among eligible bins. This lack of any intelligent prioritization based on the item's size relative to available space explains their consistently low ranking.\n\nOverall: The ranking reveals a clear hierarchy of effectiveness. Strategies that strongly prioritize precise fits (like 1st/9th with inverse weighting) are superior. Standard \"Best Fit\" strategies (like 2nd-8th, 10th, using negative residuals) are good but less aggressive. The worst heuristics are those that provide no meaningful prioritization, defaulting to a \"First Fit\" or arbitrary choice.\n- \n### Current self-reflection\n\n*   **Keywords:** Adaptive Scoring, Multi-Objective, Granular Incentives, Contextual Exploitation.\n*   **Advice:** Implement **adaptive, multi-objective scoring** sensitive to problem state. Employ **non-linear reward functions** to strongly incentivize critical, high-value outcomes (e.g., perfect fits). **Exploit problem-specific structures** for informed greedy decisions.\n*   **Avoid:** Static, rigid scoring; generic rule application; ignoring dynamic solution evolution.\n*   **Explanation:** This fosters robust heuristics by enabling dynamic adaptation, precise steering towards complex goals via differentiated rewards, and intelligent, problem-aware decision-making, optimizing resource utilization.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}