{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    As Newton, I observe that nature abhors a vacuum and tends towards the most\n    efficient arrangement, minimizing wasted space. For our bins, this translates\n    to the principle of 'Best Fit'. We seek to place an item into the bin where it\n    will occupy the space most compactly, leaving the smallest possible remainder.\n    This action effectively \"fills\" the bin most completely, making it less likely\n    to accommodate future large items and pushing it towards completion, thereby\n    conserving the larger, more versatile spaces in other bins.\n\n    A bin that precisely fits an item (leaving zero remaining capacity) is akin to\n    matter perfectly settling into its equilibrium position, achieving maximum density.\n    Thus, such a fit should be of paramount priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher score indicates higher priority.\n    \"\"\"\n    \n    # Initialize priorities. Bins that cannot fit the item are assigned a\n    # very low priority, effectively making them ineligible.\n    # This prevents the item from being \"forced\" into an incompatible space.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify all bins where the item can physically fit.\n    # This is our domain of possible interactions.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the 'residual space' if the item were placed into the bins\n    # that can accommodate it. This is the \"void\" left behind.\n    residual_space = bins_remain_cap[can_fit_mask] - item\n\n    # To ensure perfect fits (residual_space == 0) receive an extremely high,\n    # but finite, priority without causing division by zero, we add a tiny epsilon.\n    # This models an infinite preference for perfect compaction.\n    epsilon = 1e-9 \n\n    # The priority is inversely proportional to the residual space.\n    # A smaller residual space indicates a 'tighter fit' and thus a higher priority.\n    # This mirrors the natural tendency towards minimum potential energy or maximum density.\n    priorities[can_fit_mask] = 1.0 / (residual_space + epsilon)\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), we observe a crucial difference in how \"tightness of fit\" is translated into a priority score. Heuristic 1st employs an inverse relationship (`1.0 / (residual_space + epsilon)`), causing priority to skyrocket as residual space approaches zero. This non-linear scaling gives a disproportionately high preference to perfect or near-perfect fits, aggressively pushing for bin completion. In contrast, Heuristic 2nd (and 3rd-8th, 10th) uses a linear negative relationship (`-residual_capacity` or `-slack`). While still implementing \"Best Fit\" by favoring smaller residuals, the priority difference between a near-perfect fit (e.g., 0.01 residual) and a slightly larger one (e.g., 0.1 residual) is much smaller in the linear scheme, making it less aggressive in seeking out extremely tight configurations. This explains why the inverse weighting (1st and 9th) is ranked higher.\n\nComparing (3rd) vs (4th), and similarly (5th) vs (6th), and (7th) vs (8th): these pairs are identical in their implementation logic. Their ranking suggests that minor variations in docstring descriptions or variable naming do not impact performance if the core heuristic calculation remains the same. This reinforces that the *mathematical formulation* of the priority function is paramount.\n\nComparing (10th) vs (11th), and consistently across (11th-20th): Heuristic 10th still attempts a \"Best Fit\" by minimizing residual space. However, Heuristics 11th through 20th are all identical and severely flawed; they simply return `np.zeros_like(bins_remain_cap)`. This effectively means all bins that can fit the item have an equal, non-discriminating priority. When `np.argmax` is applied, it will consistently pick the bin with the lowest index that can accommodate the item, which is essentially a \"First Fit\" strategy among eligible bins. This lack of any intelligent prioritization based on the item's size relative to available space explains their consistently low ranking.\n\nOverall: The ranking reveals a clear hierarchy of effectiveness. Strategies that strongly prioritize precise fits (like 1st/9th with inverse weighting) are superior. Standard \"Best Fit\" strategies (like 2nd-8th, 10th, using negative residuals) are good but less aggressive. The worst heuristics are those that provide no meaningful prioritization, defaulting to a \"First Fit\" or arbitrary choice.\n- \n### Current self-reflection\n\n*   **Keywords:** Adaptive Scoring, Multi-Objective, Granular Incentives, Contextual Exploitation.\n*   **Advice:** Implement **adaptive, multi-objective scoring** sensitive to problem state. Employ **non-linear reward functions** to strongly incentivize critical, high-value outcomes (e.g., perfect fits). **Exploit problem-specific structures** for informed greedy decisions.\n*   **Avoid:** Static, rigid scoring; generic rule application; ignoring dynamic solution evolution.\n*   **Explanation:** This fosters robust heuristics by enabling dynamic adaptation, precise steering towards complex goals via differentiated rewards, and intelligent, problem-aware decision-making, optimizing resource utilization.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}