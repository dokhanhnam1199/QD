{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n    This version enhances the Best-Fit heuristic by incorporating a \"consolidation\"\n    bias. It subtly prioritizes placing items into bins that are already partially\n    filled, over opening entirely new bins (or using effectively \"new\" bins that are\n    still at their maximum initial capacity), provided the fit is comparable.\n    This promotes filling existing bins first to reduce the total bin count,\n    aligning with the goal of \"Global Flexibility\" and \"overall solution quality\"\n    by preventing fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize all priorities to a very low number. Bins that cannot fit\n    # the item will effectively not be chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins where the item can actually fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return the deprioritized array.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # --- Best-Fit (Base Logic) ---\n    # Calculate the remaining capacity if the item is placed.\n    # A smaller remaining capacity indicates a tighter fit.\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # The base score is the negative of the remaining capacity.\n    # A perfect fit (0 remaining) gets a score of 0. Tighter fits (smaller positive\n    # remaining capacity) get scores closer to 0 (less negative), making them higher priority.\n    base_scores = -remaining_capacity_after_fit\n\n    # --- Consolidation Bias (Domain Intelligence & Global Flexibility) ---\n    # To encourage consolidation, we add a small bonus to bins that are already\n    # partially filled. This nudges the algorithm to prefer an existing bin\n    # over a new one (or one that's still at its maximum capacity) if the\n    # Best-Fit scores are very close.\n\n    # Infer \"newly opened\" bins: We assume that any bin whose remaining capacity\n    # is equal to the maximum remaining capacity among all *currently available*\n    # bins (that can fit the item) is considered effectively \"new\" or \"empty\".\n    # This heuristic works well if bins are opened with a fixed capacity.\n    max_current_capacity = np.max(bins_remain_cap[can_fit_mask])\n\n    # Identify bins that are NOT \"newly opened\" (i.e., they are already partially filled).\n    # This is true if their current capacity is strictly less than the maximum observed capacity.\n    is_partially_filled = bins_remain_cap[can_fit_mask] < max_current_capacity\n\n    # Define a small positive bonus. This value should be small enough not to\n    # override a significantly better Best-Fit score, but large enough to\n    # differentiate between closely scoring bins or break ties.\n    # The choice of 0.01 is a simple, robust constant for floating-point comparisons.\n    consolidation_bonus = 0.01\n\n    # Apply the bonus to partially filled bins.\n    adjusted_scores = base_scores\n    adjusted_scores[is_partially_filled] += consolidation_bonus\n\n    # Assign the calculated scores to the fitting bins in the main priority array.\n    priorities[can_fit_mask] = adjusted_scores\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin,\n    implementing a hybrid heuristic that prioritizes perfect fits,\n    then encourages leaving large, versatile spaces, and heavily\n    penalizes leaving very small, potentially useless spaces.\n\n    This heuristic is designed to be more \"contextual\" and \"adaptive\"\n    than a simple Best-Fit. It aims to avoid local optima (e.g.,\n    a tight fit that leaves a fragmented, unusable space) by shaping\n    the remaining capacities in a more strategic way.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize all priorities to a very low number. Bins that cannot\n    # accommodate the item are effectively deprioritized.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Create a boolean mask for bins where the item can actually fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return the deprioritized array.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the remaining capacity after placing the item for fitting bins.\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_fit = fitting_bins_remain_cap - item\n\n    # --- Heuristic Parameters (Tunable) ---\n    # Assuming standard bin capacity is 1.0. This is a common assumption\n    # for normalized item sizes in BPP unless specified otherwise.\n    BIN_CAPACITY = 1.0\n\n    # Score for a perfect fit (remaining_capacity_after_fit == 0).\n    # This should be the highest possible score, ensuring it's always chosen.\n    PERFECT_FIT_SCORE = 1000.0\n\n    # Threshold for what constitutes a \"small, potentially useless\" remainder.\n    # If the remaining space is less than this fraction of the bin capacity,\n    # it's considered poor and heavily penalized.\n    # This avoids creating many bins with tiny, unusable gaps.\n    FRAGMENT_THRESHOLD = 0.05 * BIN_CAPACITY # e.g., 5% of bin capacity\n\n    # Multiplier for the penalty applied to small, non-zero remainders.\n    # Higher values lead to stronger discouragement of such fits.\n    SMALL_REMAINDER_PENALTY_MULTIPLIER = 50.0\n\n    # Multiplier for the score applied to larger, versatile remainders.\n    # This encourages leaving substantial space in a bin for future items,\n    # akin to a \"Worst-Fit\" approach for non-tight fits.\n    LARGE_REMAINDER_MULTIPLIER = 2.0\n    # --- End Heuristic Parameters ---\n\n    # Apply scores based on different conditions for `remaining_after_fit`:\n\n    # 1. Perfect Fit: `remaining_after_fit` is approximately zero.\n    perfect_fit_mask = np.isclose(remaining_after_fit, 0.0, atol=1e-9)\n    priorities[can_fit_mask][perfect_fit_mask] = PERFECT_FIT_SCORE\n\n    # 2. Small, Potentially Useless Remainder: `0 < remaining_after_fit < FRAGMENT_THRESHOLD`.\n    # These are fits that are not perfect, but leave very little space,\n    # which might be too small for most subsequent items, leading to fragmentation.\n    small_remainder_mask = (remaining_after_fit > 1e-9) & (remaining_after_fit < FRAGMENT_THRESHOLD)\n    \n    # The penalty increases as `remaining_after_fit` gets closer to zero (from the positive side).\n    # This creates a \"valley\" in the scoring function just after zero.\n    penalty_scores = - (FRAGMENT_THRESHOLD - remaining_after_fit[small_remainder_mask]) * SMALL_REMAINDER_PENALTY_MULTIPLIER\n    priorities[can_fit_mask][small_remainder_mask] = penalty_scores\n\n    # 3. Large, Versatile Remainder: `remaining_after_fit >= FRAGMENT_THRESHOLD`.\n    # For these cases, we prefer leaving larger remaining spaces, as they are\n    # more likely to accommodate future, larger items, maintaining bin versatility.\n    # This is a Worst-Fit-like component for non-tight fits.\n    large_remainder_mask = remaining_after_fit >= FRAGMENT_THRESHOLD\n    \n    # Linear scoring: higher `remaining_after_fit` leads to a higher score.\n    # These scores are designed to be positive but lower than the `PERFECT_FIT_SCORE`.\n    large_remainder_scores = remaining_after_fit[large_remainder_mask] * LARGE_REMAINDER_MULTIPLIER\n    priorities[can_fit_mask][large_remainder_mask] = large_remainder_scores\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), the best heuristic `priority_v2` (1st) implements a Best-Fit strategy enhanced with a \"consolidation bias\" that subtly prioritizes placing items into already partially filled bins. In stark contrast, the worst heuristic (20th) returns a zero-filled array, effectively making an arbitrary choice among fitting bins, leading to highly suboptimal packing. This highlights that any intelligent prioritization is vastly superior to no specific strategy.\n\nComparing (2nd) vs (19th), the second-best heuristic (2nd) is functionally identical to the best (1st) but introduces tunable parameters for its Best-Fit and consolidation components. The second-worst (19th) is also a \"no-op\" heuristic returning zeros. This reinforces that strategic guidance, even with generic tunable defaults, drastically outperforms arbitrary placement.\n\nComparing (1st) vs (2nd), both heuristics employ the same logic: Best-Fit combined with a discrete \"consolidation bonus\" for partially filled bins. The key difference is that 1st uses fixed, hardcoded values (e.g., `consolidation_bonus = 0.01`), while 2nd exposes these as tunable parameters. The higher rank of 1st suggests that its specific fixed parameters were particularly well-suited or near-optimal for the problem instances evaluated, potentially outperforming a more general but untuned version (2nd).\n\nComparing (3rd) vs (4th), the 3rd heuristic is a pure Best-Fit heuristic with tunable weights. The 4th heuristic is identical to the 2nd (Best-Fit plus consolidation bias with tunable parameters). The fact that 4th outranks 3rd clearly demonstrates the significant performance benefit of incorporating a \"consolidation bias\" (encouraging the filling of existing bins) in addition to basic Best-Fit. This \"consolidation\" principle helps reduce the total number of bins by preventing fragmentation.\n\nComparing (6th) vs (11th), both introduce advanced concepts like prioritizing perfect fits and penalizing small fragments. However, 6th then reverts to a Best-Fit approach for other valid fits, while 11th encourages leaving *large* versatile spaces (a \"Worst-Fit\" tendency). The higher ranking of 6th indicates that after handling perfect fits and fragmentation, maintaining a Best-Fit preference for remaining cases is generally more effective than aiming for larger, \"versatile\" gaps, as the latter can lead to less dense packing.\n\nComparing (19th) vs (20th), both are identical \"no-op\" heuristics that return zero priorities for all bins. Their adjacent ranking (19th and 20th) is expected, as they offer no intelligent decision-making.\n\nOverall: The analysis reveals a clear progression from arbitrary decisions to increasingly sophisticated strategies. Effective heuristics balance local optimization (tight fits) with global concerns like minimizing bin count and preventing fragmentation. Explicitly encouraging consolidation (filling existing bins) and penalizing unusable small gaps are crucial. Perfect fits should be highly prioritized. While tunability offers flexibility, well-chosen fixed parameters can perform exceptionally well for specific problem distributions.\n- \nHere's a redefined self-reflection focusing purely on the heuristic's strategic objectives and outcomes, avoiding forbidden concepts:\n\n*   **Keywords**: Local/global balance, tight fit, bin consolidation, fragmentation avoidance, perfect fits, minimal waste.\n*   **Advice**: Strategically combine immediate item placement with long-term container organization. Emphasize complete space utilization and prevent unusable voids.\n*   **Avoid**: Creating small, unfillable gaps or leaving large, unproductive empty regions. Neglecting overall resource structure for minor, immediate gains.\n*   **Explanation**: Effective heuristics must consider both the immediate fit and the global arrangement of resources to maximize efficiency and prevent future placement difficulties, ensuring comprehensive utility.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}