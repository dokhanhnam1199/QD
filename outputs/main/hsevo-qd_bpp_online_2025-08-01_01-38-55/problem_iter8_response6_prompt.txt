{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin,\n    implementing a Best-Fit-like heuristic.\n    Bins that fit the item are prioritized based on how little space\n    would be left after placing the item (i.e., tighter fits get higher scores).\n    Bins that cannot fit the item receive a very low priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize all priorities to a very low number. This ensures that\n    # bins which cannot accommodate the item are effectively deprioritized.\n    # Using -np.inf makes them guaranteed to not be chosen if any valid bin exists.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Create a boolean mask for bins where the item can actually fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity after placement.\n    # We want to minimize this remaining capacity to achieve a \"best fit\".\n    # By taking the negative of the remaining capacity, a smaller positive remainder\n    # (i.e., a tighter fit) results in a larger (less negative) priority score.\n    # A perfect fit (remaining_capacity == 0) results in a score of 0.\n    # A bin that is barely larger than the item will get a score close to 0.\n    # A bin much larger than the item will get a more negative score.\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    priorities[can_fit_mask] = -remaining_capacity_after_fit\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (1st) vs (11th), we see that the best heuristic implements a Best-Fit strategy with tunable weighting (`fit_score_weight`) and a very low priority for unfit bins, allowing for precise optimization of remaining capacity. In contrast, the worst heuristic (11th and all others from 11th to 20th, which are identical) provides no actual prioritization, returning zeros for all bins, which effectively leads to arbitrary or first-fit item placement, highlighting the critical need for any intelligent decision-making.\n\nComparing (2nd) vs (10th), we observe that these two heuristics are identical in their code implementation: both apply a Best-Fit logic with an added \"consolidation bias\" (a fixed `consolidation_bonus`). Their differing ranks despite identical logic suggest that the ranking might be influenced by external factors, specific test cases leading to subtle tie-breaking differences, or the result of multiple runs within a hyperparameter search.\n\nComparing (1st) vs (2nd), the top-ranked heuristic (1st) employs a parameterized Best-Fit (where the weighting of remaining capacity is a tunable `fit_score_weight`). The second-ranked (2nd) uses a Best-Fit base with a fixed \"consolidation bias.\" The superior performance of 1st suggests that allowing the primary Best-Fit weighting to be optimized (via `fit_score_weight`) is more beneficial than introducing a fixed, domain-specific rule like the consolidation bias, or that the bias itself needs tuning.\n\nComparing (3rd) vs (4th), these heuristics are functionally identical to each other and to Heuristic 1st, only differing in their default parameter values (`default_low_priority` and `fit_score_weight`). Their distinct ranks underscore the high sensitivity of heuristic performance to the specific numerical values of their parameters, emphasizing the importance of precise hyperparameter tuning. Heuristic 6th, 7th, and 9th are also identical, representing a pure Best-Fit (equivalent to `fit_score_weight = -1`).\n\nOverall: The analysis reveals that Best-Fit is a strong foundational strategy for this problem. The most successful heuristics leverage this base but crucially expose key weighting factors as tunable parameters, allowing for specific optimization. More complex fixed-logic additions (like the consolidation bias) do not guarantee superiority over simpler, well-tuned base heuristics. Finally, any intelligent prioritization, even if not perfectly optimal, drastically outperforms a heuristic that provides no differentiation between choices.\n- \nHere's the redefined 'Current self-reflection':\n\n*   **Keywords:** Adaptable Baselines, Parameterized Mechanisms, Contextual Tuning, Robust Simplicity.\n*   **Advice:** Develop adaptable heuristic baselines. Externalize *mechanism weights* for tuning. Prioritize context-aware logic. Rigorous, problem-specific tuning on simpler models is key.\n*   **Avoid:** Fixed, overly precise scoring functions. Arbitrary \"magic number\" parameters. Explicitly rewarding perfect sub-solutions if detrimental to global optimality. Over-engineered complexity.\n*   **Explanation:** Design foundational, tunable heuristics focusing on adaptable logic and empirical validation, avoiding rigid definitions or local optima from premature sub-component incentives.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}