{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin,\n    implementing a Best-Fit-like heuristic.\n    Bins that fit the item are prioritized based on how little space\n    would be left after placing the item (i.e., tighter fits get higher scores).\n    Bins that cannot fit the item receive a very low priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n        unfit_bin_priority_value: The priority score assigned to bins that\n                                   cannot accommodate the item. Default is -np.inf,\n                                   ensuring they are never chosen if any valid bin exists.\n        fit_score_weight: A weighting factor applied to the remaining capacity\n                          of fitting bins. A negative value ensures that tighter fits\n                          (smaller remaining capacity) result in higher priority scores.\n                          Default is -1.0, which means a perfect fit (0 remaining capacity)\n                          scores 0, and larger remaining capacities get more negative scores.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize all priorities to a very low number. This ensures that\n    # bins which cannot accommodate the item are effectively deprioritized.\n    priorities = np.full_like(bins_remain_cap, unfit_bin_priority_value, dtype=float)\n\n    # Create a boolean mask for bins where the item can actually fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity after placement.\n    # We want to minimize this remaining capacity to achieve a \"best fit\".\n    # By applying the fit_score_weight to the remaining capacity, a smaller positive remainder\n    # (i.e., a tighter fit) results in a larger priority score when fit_score_weight is negative.\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    priorities[can_fit_mask] = fit_score_weight * remaining_capacity_after_fit\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (Heuristics 1st) vs (Heuristics 2nd), Heuristics 1st, which applies a fixed small bonus (0.01) for consolidation by identifying partially filled bins (those with remaining capacity less than the maximum current capacity among fitting bins), slightly outperforms Heuristics 2nd. Heuristics 2nd uses a continuously tunable weighted sum of Best-Fit (`remaining_capacity_after_fit`) and `current_fullness` (current remaining capacity), with specific tuned weights (`-4.72...`, `-1.0`). Both successfully integrate a consolidation bias, but the simple, hard-coded conditional bonus in 1st proved marginally more effective in this ranking than 2nd's continuously weighted sum, even with its tuned parameters.\n\nComparing (Heuristics 2nd) vs (Heuristics 3rd), Heuristics 2nd (tuned weighted sum including bin fullness) is superior to Heuristics 3rd (tuned pure Best-Fit). Heuristics 3rd's `fit_score_weight` of `-5.04...` indicates it's a strongly tuned Best-Fit. However, the explicit inclusion of the `current_fullness_weight` component in Heuristics 2nd, favoring bins that are already fuller, demonstrates that a multi-objective scoring combining tight fit with bin consolidation (filling existing bins) provides better performance than a purely Best-Fit approach, even when both are highly tuned.\n\nComparing (Heuristics 3rd) vs (Heuristics 8th), Heuristics 3rd (tuned pure Best-Fit) significantly outperforms Heuristics 8th, which is also a pure Best-Fit but with a generic `fit_score_weight` of `-1.0`. This stark difference highlights the immense value of hyperparameter tuning: even a simple heuristic can become very powerful when its parameters are meticulously optimized for the problem instance.\n\nComparing (Heuristics 4th) vs (Heuristics 5th), both are \"tunable\" with default parameters. Heuristics 4th implements a Best-Fit with a *binary* consolidation bonus based on whether a bin is \"partially filled\" relative to the maximum capacity. Heuristics 5th employs a *continuous* weighted sum of \"tight fit\" and \"bin fullness.\" Heuristics 5th (continuous weighting) outranks Heuristics 4th (binary bonus with similar defaults), suggesting that a continuous scoring for fullness is generally more robust or effective than a binary bonus when parameters are not specifically tuned.\n\nComparing (Heuristics 5th) vs (Heuristics 7th), Heuristics 5th (tunable weighted sum, default weights) performs better than Heuristics 7th, which uses a non-linear scoring function with a significant bonus for perfect fits and an inverse relationship for other tight fits. This indicates that while the *idea* of strongly prioritizing perfect fits and non-linear rewards might seem beneficial, the specific implementation in Heuristics 7th might lead to sub-optimal choices globally, perhaps by overly favoring very rare perfect fits or by creating less versatile remaining spaces compared to a more balanced linear combination.\n\nComparing (Heuristics 11th) vs (Heuristics 13th), Heuristics 11th, a highly complex \"hybrid adaptive\" heuristic that attempts to manage fragmentation by penalizing small remainders and rewarding large ones, is still vastly superior to Heuristics 13th. Heuristics 13th simply assigns zero priority to all bins, effectively leading to arbitrary or random bin selection among fitting bins. This confirms that any structured heuristic, no matter how poorly optimized or overly complex, offers more strategic guidance than a complete lack of strategy. The repeated instances of Heuristics 11th (11th, 12th, 14th, 16th, 17th, 19th, 20th) at the bottom ranks (excluding the dummy zero function) suggest its complex logic with its default parameters is often counter-productive, possibly opening too many bins prematurely or leaving too much usable space unused to avoid \"fragmentation,\" thereby increasing the overall bin count.\n\nOverall: The best-performing heuristics are those that combine the core \"Best-Fit\" principle with a mechanism for \"consolidation\" or \"bin fullness.\" Crucially, even simple heuristics can achieve top performance if their parameters are meticulously tuned. Overly complex or non-linear scoring functions, while conceptually appealing, can underperform if not perfectly calibrated, sometimes leading to worse results than simpler, robust approaches.\n- \nTo redefine 'Current self-reflection' avoiding 'Ineffective self-reflection' points:\n\n*   **Keywords:** Emergent Properties, Adaptive Design, Global Context, Structural Integrity.\n*   **Advice:** Design heuristics to foster beneficial emergent behaviors and maintain solution flexibility, prioritizing adaptive decision rules that consider global context over rigid local optimizations.\n*   **Avoid:** Over-reliance on isolated parameter tuning, prescriptive scoring functions for local \"perfect fits,\" or specific algorithmic choices as universal solutions.\n*   **Explanation:** A focus on systemic design and adaptability enables heuristics to navigate complex landscapes more effectively, preventing local optima traps and fostering robust, globally superior outcomes.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}