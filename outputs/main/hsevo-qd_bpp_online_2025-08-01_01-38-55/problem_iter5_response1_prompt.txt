{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin,\n    implementing an enhanced Best-Fit-like heuristic with a non-linear\n    scoring function that strongly incentivizes perfect fits and applies\n    a decaying reward for other tight fits.\n\n    This version goes beyond a simple linear 'minimizing leftover space'\n    by introducing a multi-objective perspective through:\n    1. A significant bonus for perfect utilization of a bin.\n    2. A non-linear (inverse) reward for non-perfect fits,\n       making very tight fits significantly more desirable than slightly looser ones.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize all priorities to a very low number. This ensures that\n    # bins which cannot accommodate the item are effectively deprioritized.\n    # Using -np.inf makes them guaranteed to not be chosen if any valid bin exists.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Create a boolean mask for bins where the item can actually fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity after placing the item for fitting bins.\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # Define a small epsilon for robust floating-point comparisons\n    # and to prevent division by zero for actual perfect fits.\n    epsilon = 1e-9\n\n    # --- Multi-Objective / Non-Linear Scoring Strategy ---\n\n    # 1. Identify \"perfect fits\" (remaining capacity is effectively zero).\n    #    These are critical outcomes as they fully utilize a bin and reduce fragmentation.\n    is_perfect_fit = remaining_capacity_after_fit < epsilon\n\n    # 2. Calculate scores for non-perfect fits using an inverse relationship.\n    #    This ensures a non-linear decay: smaller remaining capacities get\n    #    disproportionately higher positive scores. Adding epsilon to the denominator\n    #    ensures numerical stability and provides a very high but finite score\n    #    for nearly perfect non-zero fits.\n    scores_for_non_perfect_fits = 1.0 / (remaining_capacity_after_fit + epsilon)\n\n    # Initialize combined scores with the non-perfect fit calculations.\n    combined_scores = scores_for_non_perfect_fits\n\n    # 3. Apply a significant, overriding bonus for truly perfect fits.\n    #    This ensures that a perfect fit is always chosen over any non-perfect fit,\n    #    no matter how small the remaining capacity in other bins might be.\n    #    The value (e.g., 2.0 / epsilon) is chosen to be orders of magnitude\n    #    larger than the highest possible score from `1.0 / (remaining + epsilon)`.\n    PERFECT_FIT_SCORE_BONUS = 2.0 / epsilon\n    combined_scores[is_perfect_fit] = PERFECT_FIT_SCORE_BONUS\n\n    # Assign the calculated scores to the bins that can fit the item.\n    priorities[can_fit_mask] = combined_scores\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), we see that the simpler linear Best-Fit (Heuristic 1) outperforms the more complex non-linear approach with perfect fit bonus (Heuristic 2). This suggests that while perfect fits are intuitively desirable and can reduce fragmentation, an overly aggressive non-linear weighting or bonus might not lead to globally optimal solutions for the specific problem instances tested. This could be due to the bonus causing an item to be placed in a bin for a perfect fit, even if that bin could have held a larger future item, leading to worse overall packing.\n\nComparing (2nd) vs (3rd), Heuristic 2 (non-linear with bonus) is better than Heuristic 3 (parameterized linear Best-Fit). This indicates that the specific hardcoded parameters in Heuristic 3 (`default_low_priority`, `fit_score_weight`) likely lead to suboptimal behavior for the problem at hand, or that the non-linear weighting of Heuristic 2, despite being less effective than Heuristic 1, is still more robust than a poorly tuned linear model. The `fit_score_weight` of -4.72 likely amplifies the best-fit preference more aggressively than necessary.\n\nComparing the \"Best-Fit\" variants (1st, 2nd, 3rd) vs the \"Worst\" (11th-20th), it's evident that any attempt to prioritize bins based on a \"best fit\" criterion, even if the specific scoring function is not perfectly optimal (as in 2nd and 3rd), provides significantly better results than a heuristic that assigns uniform priority to all bins (Heuristics 11-20). The latter simply returns zero priorities, effectively making the choice arbitrary among valid bins, demonstrating a complete lack of strategic intelligence.\n\nOverall: The ranking implies that a straightforward, well-understood heuristic like simple linear Best-Fit can be highly effective and robust. Introducing complexity (non-linear scoring, perfect-fit bonuses) or untuned parameters can unexpectedly degrade performance, potentially due to over-specialization, sub-optimal trade-offs, or an inability to generalize across problem instances.\n- \n*   **Keywords**: Simplicity, Robustness, Global Flexibility, Domain Intelligence.\n*   **Advice**: Favor simple, resilient designs over intricate ones. Prioritize overall solution quality, not just perfect sub-solutions that limit future options. Parameterization must be context-driven, not arbitrary. Always infuse problem-specific insights.\n*   **Avoid**: Prescribing specific scoring methodologies, comparing particular algorithms (e.g., Best Fit vs. First Fit), or incentivizing specific outcomes with complex non-linear scoring.\n*   **Explanation**: This promotes heuristics that are adaptable, less prone to over-fitting, and maintain a broad solution space, preventing premature commitment to local optima through rigid, component-level scoring.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}