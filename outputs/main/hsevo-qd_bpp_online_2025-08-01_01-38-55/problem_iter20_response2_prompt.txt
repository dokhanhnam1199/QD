{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n                default_low_priority: float = -13.732882263687515,\n                fit_score_weight: float = -4.7273623240749325) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin,\n    implementing a Best-Fit-like heuristic.\n    Bins that fit the item are prioritized based on how little space\n    would be left after placing the item (i.e., tighter fits get higher scores).\n    Bins that cannot fit the item receive a very low priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n        default_low_priority: The priority assigned to bins that cannot accommodate the item.\n                              Should be a very low number (e.g., -np.inf) to ensure they are\n                              not chosen if any valid bin exists.\n        fit_score_weight: A multiplier applied to the negative remaining capacity after fit.\n                          A negative value (e.g., -1.0) ensures that tighter fits (smaller\n                          remaining capacity) receive higher scores.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize all priorities to a very low number. This ensures that\n    # bins which cannot accommodate the item are effectively deprioritized.\n    # Using default_low_priority makes them guaranteed to not be chosen if any valid bin exists.\n    priorities = np.full_like(bins_remain_cap, default_low_priority, dtype=float)\n\n    # Create a boolean mask for bins where the item can actually fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity after placement.\n    # We want to minimize this remaining capacity to achieve a \"best fit\".\n    # By taking the remaining capacity and multiplying by fit_score_weight,\n    # a smaller positive remainder (i.e., a tighter fit) results in a larger priority score\n    # when fit_score_weight is negative.\n    # A perfect fit (remaining_capacity == 0) results in a score of 0.\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    priorities[can_fit_mask] = remaining_capacity_after_fit * fit_score_weight\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin,\n    implementing a hybrid heuristic that prioritizes perfect fits,\n    then encourages leaving large, versatile spaces, and heavily\n    penalizes leaving very small, potentially useless spaces.\n\n    This heuristic is designed to be more \"contextual\" and \"adaptive\"\n    than a simple Best-Fit. It aims to avoid local optima (e.g.,\n    a tight fit that leaves a fragmented, unusable space) by shaping\n    the remaining capacities in a more strategic way.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize all priorities to a very low number. Bins that cannot\n    # accommodate the item are effectively deprioritized.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Create a boolean mask for bins where the item can actually fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return the deprioritized array.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the remaining capacity after placing the item for fitting bins.\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_fit = fitting_bins_remain_cap - item\n\n    # --- Heuristic Parameters (Tunable) ---\n    # Assuming standard bin capacity is 1.0. This is a common assumption\n    # for normalized item sizes in BPP unless specified otherwise.\n    BIN_CAPACITY = 1.0\n\n    # Score for a perfect fit (remaining_capacity_after_fit == 0).\n    # This should be the highest possible score, ensuring it's always chosen.\n    PERFECT_FIT_SCORE = 1000.0\n\n    # Threshold for what constitutes a \"small, potentially useless\" remainder.\n    # If the remaining space is less than this fraction of the bin capacity,\n    # it's considered poor and heavily penalized.\n    # This avoids creating many bins with tiny, unusable gaps.\n    FRAGMENT_THRESHOLD = 0.05 * BIN_CAPACITY # e.g., 5% of bin capacity\n\n    # Multiplier for the penalty applied to small, non-zero remainders.\n    # Higher values lead to stronger discouragement of such fits.\n    SMALL_REMAINDER_PENALTY_MULTIPLIER = 50.0\n\n    # Multiplier for the score applied to larger, versatile remainders.\n    # This encourages leaving substantial space in a bin for future items,\n    # akin to a \"Worst-Fit\" approach for non-tight fits.\n    LARGE_REMAINDER_MULTIPLIER = 2.0\n    # --- End Heuristic Parameters ---\n\n    # Apply scores based on different conditions for `remaining_after_fit`:\n\n    # 1. Perfect Fit: `remaining_after_fit` is approximately zero.\n    perfect_fit_mask = np.isclose(remaining_after_fit, 0.0, atol=1e-9)\n    priorities[can_fit_mask][perfect_fit_mask] = PERFECT_FIT_SCORE\n\n    # 2. Small, Potentially Useless Remainder: `0 < remaining_after_fit < FRAGMENT_THRESHOLD`.\n    # These are fits that are not perfect, but leave very little space,\n    # which might be too small for most subsequent items, leading to fragmentation.\n    small_remainder_mask = (remaining_after_fit > 1e-9) & (remaining_after_fit < FRAGMENT_THRESHOLD)\n    \n    # The penalty increases as `remaining_after_fit` gets closer to zero (from the positive side).\n    # This creates a \"valley\" in the scoring function just after zero.\n    penalty_scores = - (FRAGMENT_THRESHOLD - remaining_after_fit[small_remainder_mask]) * SMALL_REMAINDER_PENALTY_MULTIPLIER\n    priorities[can_fit_mask][small_remainder_mask] = penalty_scores\n\n    # 3. Large, Versatile Remainder: `remaining_after_fit >= FRAGMENT_THRESHOLD`.\n    # For these cases, we prefer leaving larger remaining spaces, as they are\n    # more likely to accommodate future, larger items, maintaining bin versatility.\n    # This is a Worst-Fit-like component for non-tight fits.\n    large_remainder_mask = remaining_after_fit >= FRAGMENT_THRESHOLD\n    \n    # Linear scoring: higher `remaining_after_fit` leads to a higher score.\n    # These scores are designed to be positive but lower than the `PERFECT_FIT_SCORE`.\n    large_remainder_scores = remaining_after_fit[large_remainder_mask] * LARGE_REMAINDER_MULTIPLIER\n    priorities[can_fit_mask][large_remainder_mask] = large_remainder_scores\n\n    return priorities\n\n### Analyze & experience\n- Comparing (best) [Heuristics 1st] vs (worst) [Heuristics 11th], we observe that explicit heuristic logic, even a simple Best-Fit, is vastly superior to no logic at all. Heuristic 1st combines Best-Fit with a consolidation bias, while 11th simply returns zero priorities, leading to arbitrary bin selection.\n\nComparing (second best) [Heuristics 2nd] vs (second worst) [Heuristics 13th], Heuristic 2nd employs a continuous weighted sum of tight-fit and bin-fullness scores. Heuristic 13th uses a complex, piecewise function with distinct bonuses for perfect fits, penalties for small fragments, and a preference for large remainders. The higher ranking of the simpler, linear combination (2nd) over the more complex, threshold-dependent one (13th) suggests that continuous scoring might be more robust or easier to tune.\n\nComparing (1st) [Heuristics 1st] vs (2nd) [Heuristics 2nd], both combine Best-Fit with consolidation. Heuristic 1st's consolidation targets bins not of maximal available capacity, while 2nd directly targets fuller bins. The subtle difference in consolidation strategy (encouraging use of non-largest bins vs. already-fuller bins) appears to influence performance.\n\nComparing (3rd) [Heuristics 3rd] vs (4th) [Heuristics 4th], this reveals a critical point: Heuristic 4th is an exact duplicate of 1st, yet it ranks lower than 3rd (a pure Best-Fit). This strong contradiction indicates that default parameter values, problem-specific tuning, or environmental factors (e.g., test data distribution, experimental noise) heavily influence performance, potentially outweighing algorithmic sophistication if not properly configured.\n\nComparing (second worst) [Heuristics 13th] vs (worst) [Heuritsics 11th], even a complex heuristic with potential \"valleys\" due to penalties (13th) is far better than a non-heuristic approach (11th). This reinforces the value of any informed decision-making over random placement.\n\nOverall: Effective heuristics combine Best-Fit with strategies for bin consolidation and fragmentation avoidance. Simpler, continuously weighted combinations often outperform complex, piecewise functions, possibly due to robustness or ease of tuning. Critically, parameter optimization is paramount; an algorithmically strong heuristic can perform poorly if its weights are not well-suited for the problem instance.\n- \nHere's a redefined 'Current self-reflection':\n\n*   **Keywords**: Adaptive strategies, emergent properties, state-aware, dynamic.\n*   **Advice**: Design heuristics with inherent adaptability, allowing them to dynamically adjust priorities based on real-time state evolution. Favor simple, local rules that collectively yield robust global behavior. Incorporate mechanisms for recognizing and rectifying accumulating sub-optimality.\n*   **Avoid**: Prescribing fixed scoring biases, over-engineering perfect fit rewards, relying solely on pre-tuned parameters, or generic comparisons of complexity.\n*   **Explanation**: Focusing on dynamic adaptation and emergent properties fosters resilience. Rather than static optimization, heuristics should fluidly respond to changing problem landscapes, moving beyond rigid, pre-defined rules.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}