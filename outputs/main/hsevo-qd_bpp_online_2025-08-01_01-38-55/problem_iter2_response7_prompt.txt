{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Inspired by the principle of elegant efficiency and the 'relativity'\n    of an item's volume to the available space within a container, this\n    heuristic aims for the 'best fit'. We seek to minimize the void space\n    that remains after an item is placed, thereby ensuring the most\n    compact packing possible for the individual placement decision.\n\n    A bin is prioritized if, after accommodating the item, it leaves the\n    smallest positive residual capacity. This preserves larger gaps in other\n    bins for future, potentially larger, items, or simply achieves a tighter\n    overall packing. Bins that cannot fit the item are given an infinitesimally\n    small priority, effectively disqualifying them.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A lower positive difference (bins_remain_cap - item) results in a\n        higher (less negative) priority score.\n    \"\"\"\n    # Calculate the theoretical remaining capacity for each bin after placing the item.\n    # This is a measure of the \"excess space\" or \"tightness of fit\".\n    remaining_after_fit = bins_remain_cap - item\n\n    # Initialize all scores to a very low value. This effectively disqualifies\n    # any bin that cannot accommodate the item, as their remaining_after_fit\n    # would be negative, and we only update positive or zero remaining_after_fit.\n    # We choose a sufficiently small negative number to ensure it's always lower\n    # than any valid score.\n    scores = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify which bins can actually fit the item.\n    # These are the bins where remaining_after_fit is non-negative.\n    can_fit_mask = remaining_after_fit >= 0\n\n    # For the bins that can fit the item, we assign a score.\n    # The score is the negative of the remaining space.\n    # This means a smaller remaining space (e.g., 0.1) yields a higher score (-0.1)\n    # compared to a larger remaining space (e.g., 0.5 which yields -0.5).\n    # The bin with the largest (least negative) score will be the one\n    # with the smallest positive remaining capacity after placement (the best fit).\n    scores[can_fit_mask] = -remaining_after_fit[can_fit_mask]\n\n    return scores\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), we observe a crucial difference in how \"tightness of fit\" is translated into a priority score. Heuristic 1st employs an inverse relationship (`1.0 / (residual_space + epsilon)`), causing priority to skyrocket as residual space approaches zero. This non-linear scaling gives a disproportionately high preference to perfect or near-perfect fits, aggressively pushing for bin completion. In contrast, Heuristic 2nd (and 3rd-8th, 10th) uses a linear negative relationship (`-residual_capacity` or `-slack`). While still implementing \"Best Fit\" by favoring smaller residuals, the priority difference between a near-perfect fit (e.g., 0.01 residual) and a slightly larger one (e.g., 0.1 residual) is much smaller in the linear scheme, making it less aggressive in seeking out extremely tight configurations. This explains why the inverse weighting (1st and 9th) is ranked higher.\n\nComparing (3rd) vs (4th), and similarly (5th) vs (6th), and (7th) vs (8th): these pairs are identical in their implementation logic. Their ranking suggests that minor variations in docstring descriptions or variable naming do not impact performance if the core heuristic calculation remains the same. This reinforces that the *mathematical formulation* of the priority function is paramount.\n\nComparing (10th) vs (11th), and consistently across (11th-20th): Heuristic 10th still attempts a \"Best Fit\" by minimizing residual space. However, Heuristics 11th through 20th are all identical and severely flawed; they simply return `np.zeros_like(bins_remain_cap)`. This effectively means all bins that can fit the item have an equal, non-discriminating priority. When `np.argmax` is applied, it will consistently pick the bin with the lowest index that can accommodate the item, which is essentially a \"First Fit\" strategy among eligible bins. This lack of any intelligent prioritization based on the item's size relative to available space explains their consistently low ranking.\n\nOverall: The ranking reveals a clear hierarchy of effectiveness. Strategies that strongly prioritize precise fits (like 1st/9th with inverse weighting) are superior. Standard \"Best Fit\" strategies (like 2nd-8th, 10th, using negative residuals) are good but less aggressive. The worst heuristics are those that provide no meaningful prioritization, defaulting to a \"First Fit\" or arbitrary choice.\n- \n### Current self-reflection\n\n*   **Keywords:** Adaptive Scoring, Multi-Objective, Granular Incentives, Contextual Exploitation.\n*   **Advice:** Implement **adaptive, multi-objective scoring** sensitive to problem state. Employ **non-linear reward functions** to strongly incentivize critical, high-value outcomes (e.g., perfect fits). **Exploit problem-specific structures** for informed greedy decisions.\n*   **Avoid:** Static, rigid scoring; generic rule application; ignoring dynamic solution evolution.\n*   **Explanation:** This fosters robust heuristics by enabling dynamic adaptation, precise steering towards complex goals via differentiated rewards, and intelligent, problem-aware decision-making, optimizing resource utilization.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}