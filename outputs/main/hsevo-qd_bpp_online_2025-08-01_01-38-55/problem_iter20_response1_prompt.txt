{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n                w_tight_fit: float = 1.0, w_fullness: float = 0.1) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin,\n    implementing a tunable heuristic combining a Best-Fit-like approach with a\n    preference for fuller bins.\n\n    Bins that fit the item are prioritized based on two weighted components:\n    1.  **Tight Fit:** How little space would be left after placing the item (i.e., tighter fits get higher scores).\n        This component directly relates to the Best-Fit heuristic.\n    2.  **Bin Fullness:** How full the bin already is (i.e., smaller remaining capacity means a fuller bin, getting a higher score).\n        This component encourages \"closing\" bins by filling those that are already significantly utilized.\n\n    Bins that cannot fit the item receive a very low priority, effectively preventing their selection.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n        w_tight_fit: Weight for the \"tight fit\" component. A higher value emphasizes\n                     minimizing leftover space after placement.\n        w_fullness: Weight for the \"bin fullness\" component. A higher value emphasizes\n                    using bins that are already more filled.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize all priorities to a very low number. This ensures that\n    # bins which cannot accommodate the item are effectively deprioritized.\n    # Using -np.inf makes them guaranteed to not be chosen if any valid bin exists.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Create a boolean mask for bins where the item can actually fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate scores only for bins that can fit the item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # Component 1: Tight Fit (Best-Fit-like)\n    # The goal is to minimize (bins_remain_cap - item), which is the space left after placement.\n    # To maximize this, we use the negative: -(bins_remain_cap - item) = item - bins_remain_cap.\n    # A smaller remaining capacity after fit results in a higher score.\n    score_tight_fit = item - fitting_bins_remain_cap\n\n    # Component 2: Bin Fullness\n    # The goal is to prefer bins that already have a smaller remaining capacity\n    # (i.e., are closer to being full).\n    # To maximize this, we use the negative of remaining capacity: -fitting_bins_remain_cap.\n    # A smaller current remaining capacity results in a higher score.\n    score_fullness = -fitting_bins_remain_cap\n\n    # Combine the scores using tunable weights.\n    # The overall priority is a weighted sum of these two components.\n    combined_score = (w_tight_fit * score_tight_fit) + (w_fullness * score_fullness)\n\n    # Apply the calculated scores to the valid bins in the priorities array.\n    priorities[can_fit_mask] = combined_score\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (best) [Heuristics 1st] vs (worst) [Heuristics 11th], we observe that explicit heuristic logic, even a simple Best-Fit, is vastly superior to no logic at all. Heuristic 1st combines Best-Fit with a consolidation bias, while 11th simply returns zero priorities, leading to arbitrary bin selection.\n\nComparing (second best) [Heuristics 2nd] vs (second worst) [Heuristics 13th], Heuristic 2nd employs a continuous weighted sum of tight-fit and bin-fullness scores. Heuristic 13th uses a complex, piecewise function with distinct bonuses for perfect fits, penalties for small fragments, and a preference for large remainders. The higher ranking of the simpler, linear combination (2nd) over the more complex, threshold-dependent one (13th) suggests that continuous scoring might be more robust or easier to tune.\n\nComparing (1st) [Heuristics 1st] vs (2nd) [Heuristics 2nd], both combine Best-Fit with consolidation. Heuristic 1st's consolidation targets bins not of maximal available capacity, while 2nd directly targets fuller bins. The subtle difference in consolidation strategy (encouraging use of non-largest bins vs. already-fuller bins) appears to influence performance.\n\nComparing (3rd) [Heuristics 3rd] vs (4th) [Heuristics 4th], this reveals a critical point: Heuristic 4th is an exact duplicate of 1st, yet it ranks lower than 3rd (a pure Best-Fit). This strong contradiction indicates that default parameter values, problem-specific tuning, or environmental factors (e.g., test data distribution, experimental noise) heavily influence performance, potentially outweighing algorithmic sophistication if not properly configured.\n\nComparing (second worst) [Heuristics 13th] vs (worst) [Heuritsics 11th], even a complex heuristic with potential \"valleys\" due to penalties (13th) is far better than a non-heuristic approach (11th). This reinforces the value of any informed decision-making over random placement.\n\nOverall: Effective heuristics combine Best-Fit with strategies for bin consolidation and fragmentation avoidance. Simpler, continuously weighted combinations often outperform complex, piecewise functions, possibly due to robustness or ease of tuning. Critically, parameter optimization is paramount; an algorithmically strong heuristic can perform poorly if its weights are not well-suited for the problem instance.\n- \nHere's a redefined 'Current self-reflection':\n\n*   **Keywords**: Adaptive strategies, emergent properties, state-aware, dynamic.\n*   **Advice**: Design heuristics with inherent adaptability, allowing them to dynamically adjust priorities based on real-time state evolution. Favor simple, local rules that collectively yield robust global behavior. Incorporate mechanisms for recognizing and rectifying accumulating sub-optimality.\n*   **Avoid**: Prescribing fixed scoring biases, over-engineering perfect fit rewards, relying solely on pre-tuned parameters, or generic comparisons of complexity.\n*   **Explanation**: Focusing on dynamic adaptation and emergent properties fosters resilience. Rather than static optimization, heuristics should fluidly respond to changing problem landscapes, moving beyond rigid, pre-defined rules.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}