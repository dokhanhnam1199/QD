{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add an item to each bin,\n    implementing a sophisticated heuristic that combines the Best-Fit principle\n    with strategic considerations for bin consolidation, perfect fits, and\n    the avoidance of small, unfillable gaps.\n\n    This heuristic aims to:\n    1. Strongly favor perfect fits to maximize complete bin utilization.\n    2. Prioritize tight fits (Best-Fit) to minimize immediate waste.\n    3. Penalize placements that result in very small, potentially unusable\n       remaining capacities, preventing \"fragmentation\" of bin space.\n    4. Provide a bonus for placing items in bins that are already highly\n       utilized, encouraging the \"consolidation\" and closure of bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n                         Assumes capacities are normalized (e.g., bin capacity = 1.0).\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins where the item can physically fit\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # Calculate the remaining capacity for bins after placing the item\n    remaining_capacity_after_fit = fitting_bins_remain_cap - item\n\n    # --- Core Scoring Components ---\n\n    # 1. Base Score (Best-Fit principle):\n    #   A smaller remaining capacity (tighter fit) yields a higher base score.\n    #   Scores range from 0 (perfect fit) to negative values.\n    base_scores = -remaining_capacity_after_fit\n\n    # Define constants for heuristic weighting. These can be tuned.\n    PERFECT_FIT_BONUS = 1000.0  # Large bonus for a perfect fit, making it highly desirable.\n    SMALL_GAP_THRESHOLD = 0.05  # A fractional threshold for what constitutes a \"small\" gap.\n                                # E.g., if bin capacity is 1.0, 0.05 means 5% of bin.\n    SMALL_GAP_PENALTY_FACTOR = 200.0 # Multiplier for the penalty applied to small gaps.\n                                    # Higher values mean stronger discouragement.\n    CONSOLIDATION_THRESHOLD = 0.75 # Threshold (as a fraction of total capacity)\n                                   # for a bin to be considered \"highly utilized\".\n    CONSOLIDATION_BONUS_FACTOR = 50.0 # Multiplier for the bonus applied to highly utilized bins.\n\n    # 2. Perfect Fit Bonus:\n    #    If an item perfectly fills a bin, give a substantial bonus. This is a\n    #    highly desirable outcome for bin packing efficiency.\n    perfect_fit_mask = remaining_capacity_after_fit == 0\n    base_scores[perfect_fit_mask] += PERFECT_FIT_BONUS\n\n    # 3. Small Gap Penalty:\n    #    Apply a penalty for placements that leave a very small, non-zero\n    #    remaining capacity. Such small gaps are often \"unfillable\" and\n    #    lead to wasted space/fragmentation. The penalty is higher for smaller gaps.\n    small_gap_mask = (remaining_capacity_after_fit > 0) & (remaining_capacity_after_fit < SMALL_GAP_THRESHOLD)\n    # Penalty magnitude is inversely proportional to the gap size within the threshold\n    penalty_values = (SMALL_GAP_THRESHOLD - remaining_capacity_after_fit[small_gap_mask]) * SMALL_GAP_PENALTY_FACTOR\n    base_scores[small_gap_mask] -= penalty_values\n\n    # 4. Consolidation Bonus:\n    #    Reward placing items into bins that are already significantly full.\n    #    This encourages \"closing\" bins and reduces the total number of\n    #    active bins, contributing to bin consolidation.\n    #    Assumes a nominal total bin capacity (e.g., 1.0) if not explicitly provided.\n    NOMINAL_BIN_CAPACITY = 1.0 # This needs to be consistent with how bins_remain_cap are scaled.\n    current_fill_level = (NOMINAL_BIN_CAPACITY - fitting_bins_remain_cap) / NOMINAL_BIN_CAPACITY\n\n    consolidation_mask = current_fill_level >= CONSOLIDATION_THRESHOLD\n    # The bonus can scale with how full the bin is beyond the threshold\n    consolidation_bonus = current_fill_level[consolidation_mask] * CONSOLIDATION_BONUS_FACTOR\n    base_scores[consolidation_mask] += consolidation_bonus\n\n    # Assign the calculated scores to the respective bins.\n    priorities[can_fit_mask] = base_scores\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (best) [Heuristics 1st] vs (worst) [Heuristics 11th], we observe that explicit heuristic logic, even a simple Best-Fit, is vastly superior to no logic at all. Heuristic 1st combines Best-Fit with a consolidation bias, while 11th simply returns zero priorities, leading to arbitrary bin selection.\n\nComparing (second best) [Heuristics 2nd] vs (second worst) [Heuristics 13th], Heuristic 2nd employs a continuous weighted sum of tight-fit and bin-fullness scores. Heuristic 13th uses a complex, piecewise function with distinct bonuses for perfect fits, penalties for small fragments, and a preference for large remainders. The higher ranking of the simpler, linear combination (2nd) over the more complex, threshold-dependent one (13th) suggests that continuous scoring might be more robust or easier to tune.\n\nComparing (1st) [Heuristics 1st] vs (2nd) [Heuristics 2nd], both combine Best-Fit with consolidation. Heuristic 1st's consolidation targets bins not of maximal available capacity, while 2nd directly targets fuller bins. The subtle difference in consolidation strategy (encouraging use of non-largest bins vs. already-fuller bins) appears to influence performance.\n\nComparing (3rd) [Heuristics 3rd] vs (4th) [Heuristics 4th], this reveals a critical point: Heuristic 4th is an exact duplicate of 1st, yet it ranks lower than 3rd (a pure Best-Fit). This strong contradiction indicates that default parameter values, problem-specific tuning, or environmental factors (e.g., test data distribution, experimental noise) heavily influence performance, potentially outweighing algorithmic sophistication if not properly configured.\n\nComparing (second worst) [Heuristics 13th] vs (worst) [Heuritsics 11th], even a complex heuristic with potential \"valleys\" due to penalties (13th) is far better than a non-heuristic approach (11th). This reinforces the value of any informed decision-making over random placement.\n\nOverall: Effective heuristics combine Best-Fit with strategies for bin consolidation and fragmentation avoidance. Simpler, continuously weighted combinations often outperform complex, piecewise functions, possibly due to robustness or ease of tuning. Critically, parameter optimization is paramount; an algorithmically strong heuristic can perform poorly if its weights are not well-suited for the problem instance.\n- \nHere's a redefined 'Current self-reflection':\n\n*   **Keywords**: Adaptive strategies, emergent properties, state-aware, dynamic.\n*   **Advice**: Design heuristics with inherent adaptability, allowing them to dynamically adjust priorities based on real-time state evolution. Favor simple, local rules that collectively yield robust global behavior. Incorporate mechanisms for recognizing and rectifying accumulating sub-optimality.\n*   **Avoid**: Prescribing fixed scoring biases, over-engineering perfect fit rewards, relying solely on pre-tuned parameters, or generic comparisons of complexity.\n*   **Explanation**: Focusing on dynamic adaptation and emergent properties fosters resilience. Rather than static optimization, heuristics should fluidly respond to changing problem landscapes, moving beyond rigid, pre-defined rules.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}