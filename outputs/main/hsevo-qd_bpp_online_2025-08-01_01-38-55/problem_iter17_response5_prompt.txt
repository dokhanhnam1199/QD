{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add an item to each bin.\n    This heuristic is designed to be an adaptive variant of Best-Fit,\n    considering 'Structural Integrity' and 'Emergent Properties' of the\n    overall bin packing solution, moving beyond rigid local optimization.\n\n    It improves upon a simple Best-Fit by:\n    1.  **Strongly Prioritizing Perfect Fits:** Bins that can precisely fit the\n        item (leaving zero remainder) receive the highest possible score.\n    2.  **Penalizing Fragmentation:** It applies a significant penalty to bins\n        where placing the item would leave a very small, non-zero, and\n        potentially unusable remainder (a \"fragment\"). This aims to prevent\n        the creation of many tiny, difficult-to-fill pockets of space, which\n        can lead to an increased total number of bins used (a negative\n        emergent property for the global solution).\n    3.  **Adaptive Thresholding:** The definition of a \"small fragment\" is\n        adaptive, considering both a small absolute value (relative to the\n        bin's maximum capacity, providing 'Global Context') and a value relative\n        to the current item's size ('Adaptive Design').\n    4.  **Best-Fit Tendency for Other Cases:** For all other valid fits (perfect\n        fits, or fits leaving remainders larger than the fragmentation threshold),\n        it reverts to the Best-Fit principle, favoring tighter fits to minimize\n        wasted space, while implicitly encouraging closure of bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: A NumPy array containing the remaining capacities\n                         for each bin.\n        BIN_MAX_CAPACITY: The maximum capacity of any bin. This parameter\n                          provides crucial global context for evaluating\n                          remainder sizes and setting penalty scales.\n                          Defaults to 1.0, assuming item and bin sizes are normalized.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each\n        element is the calculated priority score for the corresponding bin.\n        Bins that cannot fit the item will have a very low priority (-np.inf).\n    \"\"\"\n    # Initialize all priorities to a very low number. This ensures that\n    # bins which cannot accommodate the item are effectively deprioritized.\n    # Using -np.inf makes them guaranteed not to be chosen if any valid bin exists.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Create a boolean mask for bins where the item can actually fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        # If no bin can accommodate the item, return the initialized priorities.\n        # This implies that a new bin must be opened in the broader packing algorithm.\n        return priorities\n\n    # Calculate the remaining capacity for bins where the item can be placed.\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # --- Component 1: Best-Fit Base Score ---\n    # Tighter fits (smaller `remaining_capacity_after_fit`) are more desirable.\n    # A perfect fit (remainder = 0) gets a score of 0. Larger remainders get\n    # increasingly negative scores.\n    base_fit_score = -remaining_capacity_after_fit\n\n    # --- Component 2: Fragmentation Penalty ---\n    # This component penalizes solutions that lead to 'Structural Degradation'\n    # by creating very small, likely unusable, fragments of space.\n    # Define an adaptive threshold for what constitutes a \"small fragment\".\n    # It considers both an absolute minimum size (e.g., 1% of max bin capacity)\n    # and a relative minimum size (e.g., 5% of the current item's size).\n    # Using `max` ensures it meets at least a minimum absolute size criterion,\n    # while `min` prevents the threshold from becoming too large for very small items.\n    FRAGMENT_THRESHOLD = max(0.01 * BIN_MAX_CAPACITY, 0.05 * item)\n    FRAGMENT_THRESHOLD = min(FRAGMENT_THRESHOLD, 0.5 * item) # Cap threshold to avoid penalizing useful mid-range remainders\n\n    fragment_penalty = np.zeros_like(remaining_capacity_after_fit)\n\n    # Identify bins where the item fits, but leaves a small, non-zero fragment.\n    # Using a small epsilon (1e-9) to account for floating-point inaccuracies\n    # when checking for truly zero remainder.\n    is_fragmented_remainder = (remaining_capacity_after_fit > 1e-9) & \\\n                              (remaining_capacity_after_fit < FRAGMENT_THRESHOLD)\n\n    # Determine the magnitude of the penalty. This value is relative to the\n    # `BIN_MAX_CAPACITY` to scale appropriately across different problem sizes.\n    # A larger magnitude means fragmented bins are heavily discouraged.\n    PENALTY_MAGNITUDE = 0.2 * BIN_MAX_CAPACITY  # This is a key tunable parameter\n\n    # Apply the penalty to identified fragmented remainders.\n    fragment_penalty[is_fragmented_remainder] = -PENALTY_MAGNITUDE\n\n    # --- Combine Scores ---\n    # The final priority combines the best-fit preference with the fragmentation penalty.\n    # This guides the heuristic to make decisions that lead to better 'Emergent Properties'\n    # for the overall packing solution.\n    combined_score = base_fit_score + fragment_penalty\n\n    # Assign the calculated combined scores back to the appropriate bins.\n    priorities[can_fit_mask] = combined_score\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (1st) vs (20th), the best heuristic `priority_v2` (1st) implements a Best-Fit strategy enhanced with a \"consolidation bias\" that subtly prioritizes placing items into already partially filled bins. In stark contrast, the worst heuristic (20th) returns a zero-filled array, effectively making an arbitrary choice among fitting bins, leading to highly suboptimal packing. This highlights that any intelligent prioritization is vastly superior to no specific strategy.\n\nComparing (2nd) vs (19th), the second-best heuristic (2nd) is functionally identical to the best (1st) but introduces tunable parameters for its Best-Fit and consolidation components. The second-worst (19th) is also a \"no-op\" heuristic returning zeros. This reinforces that strategic guidance, even with generic tunable defaults, drastically outperforms arbitrary placement.\n\nComparing (1st) vs (2nd), both heuristics employ the same logic: Best-Fit combined with a discrete \"consolidation bonus\" for partially filled bins. The key difference is that 1st uses fixed, hardcoded values (e.g., `consolidation_bonus = 0.01`), while 2nd exposes these as tunable parameters. The higher rank of 1st suggests that its specific fixed parameters were particularly well-suited or near-optimal for the problem instances evaluated, potentially outperforming a more general but untuned version (2nd).\n\nComparing (3rd) vs (4th), the 3rd heuristic is a pure Best-Fit heuristic with tunable weights. The 4th heuristic is identical to the 2nd (Best-Fit plus consolidation bias with tunable parameters). The fact that 4th outranks 3rd clearly demonstrates the significant performance benefit of incorporating a \"consolidation bias\" (encouraging the filling of existing bins) in addition to basic Best-Fit. This \"consolidation\" principle helps reduce the total number of bins by preventing fragmentation.\n\nComparing (6th) vs (11th), both introduce advanced concepts like prioritizing perfect fits and penalizing small fragments. However, 6th then reverts to a Best-Fit approach for other valid fits, while 11th encourages leaving *large* versatile spaces (a \"Worst-Fit\" tendency). The higher ranking of 6th indicates that after handling perfect fits and fragmentation, maintaining a Best-Fit preference for remaining cases is generally more effective than aiming for larger, \"versatile\" gaps, as the latter can lead to less dense packing.\n\nComparing (19th) vs (20th), both are identical \"no-op\" heuristics that return zero priorities for all bins. Their adjacent ranking (19th and 20th) is expected, as they offer no intelligent decision-making.\n\nOverall: The analysis reveals a clear progression from arbitrary decisions to increasingly sophisticated strategies. Effective heuristics balance local optimization (tight fits) with global concerns like minimizing bin count and preventing fragmentation. Explicitly encouraging consolidation (filling existing bins) and penalizing unusable small gaps are crucial. Perfect fits should be highly prioritized. While tunability offers flexibility, well-chosen fixed parameters can perform exceptionally well for specific problem distributions.\n- \nHere's a redefined self-reflection focusing purely on the heuristic's strategic objectives and outcomes, avoiding forbidden concepts:\n\n*   **Keywords**: Local/global balance, tight fit, bin consolidation, fragmentation avoidance, perfect fits, minimal waste.\n*   **Advice**: Strategically combine immediate item placement with long-term container organization. Emphasize complete space utilization and prevent unusable voids.\n*   **Avoid**: Creating small, unfillable gaps or leaving large, unproductive empty regions. Neglecting overall resource structure for minor, immediate gains.\n*   **Explanation**: Effective heuristics must consider both the immediate fit and the global arrangement of resources to maximize efficiency and prevent future placement difficulties, ensuring comprehensive utility.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}