{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n    This version enhances the Best-Fit heuristic by incorporating a \"consolidation\"\n    bias. It subtly prioritizes placing items into bins that are already partially\n    filled, over opening entirely new bins (or using effectively \"new\" bins that are\n    still at their maximum initial capacity), provided the fit is comparable.\n    This promotes filling existing bins first to reduce the total bin count,\n    aligning with the goal of \"Global Flexibility\" and \"overall solution quality\"\n    by preventing fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize all priorities to a very low number. Bins that cannot fit\n    # the item will effectively not be chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins where the item can actually fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return the deprioritized array.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # --- Best-Fit (Base Logic) ---\n    # Calculate the remaining capacity if the item is placed.\n    # A smaller remaining capacity indicates a tighter fit.\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # The base score is the negative of the remaining capacity.\n    # A perfect fit (0 remaining) gets a score of 0. Tighter fits (smaller positive\n    # remaining capacity) get scores closer to 0 (less negative), making them higher priority.\n    base_scores = -remaining_capacity_after_fit\n\n    # --- Consolidation Bias (Domain Intelligence & Global Flexibility) ---\n    # To encourage consolidation, we add a small bonus to bins that are already\n    # partially filled. This nudges the algorithm to prefer an existing bin\n    # over a new one (or one that's still at its maximum capacity) if the\n    # Best-Fit scores are very close.\n\n    # Infer \"newly opened\" bins: We assume that any bin whose remaining capacity\n    # is equal to the maximum remaining capacity among all *currently available*\n    # bins (that can fit the item) is considered effectively \"new\" or \"empty\".\n    # This heuristic works well if bins are opened with a fixed capacity.\n    max_current_capacity = np.max(bins_remain_cap[can_fit_mask])\n\n    # Identify bins that are NOT \"newly opened\" (i.e., they are already partially filled).\n    # This is true if their current capacity is strictly less than the maximum observed capacity.\n    is_partially_filled = bins_remain_cap[can_fit_mask] < max_current_capacity\n\n    # Define a small positive bonus. This value should be small enough not to\n    # override a significantly better Best-Fit score, but large enough to\n    # differentiate between closely scoring bins or break ties.\n    # The choice of 0.01 is a simple, robust constant for floating-point comparisons.\n    consolidation_bonus = 0.01\n\n    # Apply the bonus to partially filled bins.\n    adjusted_scores = base_scores\n    adjusted_scores[is_partially_filled] += consolidation_bonus\n\n    # Assign the calculated scores to the fitting bins in the main priority array.\n    priorities[can_fit_mask] = adjusted_scores\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (1st) vs (11th), we see that the best heuristic implements a Best-Fit strategy with tunable weighting (`fit_score_weight`) and a very low priority for unfit bins, allowing for precise optimization of remaining capacity. In contrast, the worst heuristic (11th and all others from 11th to 20th, which are identical) provides no actual prioritization, returning zeros for all bins, which effectively leads to arbitrary or first-fit item placement, highlighting the critical need for any intelligent decision-making.\n\nComparing (2nd) vs (10th), we observe that these two heuristics are identical in their code implementation: both apply a Best-Fit logic with an added \"consolidation bias\" (a fixed `consolidation_bonus`). Their differing ranks despite identical logic suggest that the ranking might be influenced by external factors, specific test cases leading to subtle tie-breaking differences, or the result of multiple runs within a hyperparameter search.\n\nComparing (1st) vs (2nd), the top-ranked heuristic (1st) employs a parameterized Best-Fit (where the weighting of remaining capacity is a tunable `fit_score_weight`). The second-ranked (2nd) uses a Best-Fit base with a fixed \"consolidation bias.\" The superior performance of 1st suggests that allowing the primary Best-Fit weighting to be optimized (via `fit_score_weight`) is more beneficial than introducing a fixed, domain-specific rule like the consolidation bias, or that the bias itself needs tuning.\n\nComparing (3rd) vs (4th), these heuristics are functionally identical to each other and to Heuristic 1st, only differing in their default parameter values (`default_low_priority` and `fit_score_weight`). Their distinct ranks underscore the high sensitivity of heuristic performance to the specific numerical values of their parameters, emphasizing the importance of precise hyperparameter tuning. Heuristic 6th, 7th, and 9th are also identical, representing a pure Best-Fit (equivalent to `fit_score_weight = -1`).\n\nOverall: The analysis reveals that Best-Fit is a strong foundational strategy for this problem. The most successful heuristics leverage this base but crucially expose key weighting factors as tunable parameters, allowing for specific optimization. More complex fixed-logic additions (like the consolidation bias) do not guarantee superiority over simpler, well-tuned base heuristics. Finally, any intelligent prioritization, even if not perfectly optimal, drastically outperforms a heuristic that provides no differentiation between choices.\n- \nHere's the redefined 'Current self-reflection':\n\n*   **Keywords:** Adaptable Baselines, Parameterized Mechanisms, Contextual Tuning, Robust Simplicity.\n*   **Advice:** Develop adaptable heuristic baselines. Externalize *mechanism weights* for tuning. Prioritize context-aware logic. Rigorous, problem-specific tuning on simpler models is key.\n*   **Avoid:** Fixed, overly precise scoring functions. Arbitrary \"magic number\" parameters. Explicitly rewarding perfect sub-solutions if detrimental to global optimality. Over-engineered complexity.\n*   **Explanation:** Design foundational, tunable heuristics focusing on adaptable logic and empirical validation, avoiding rigid definitions or local optima from premature sub-component incentives.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}