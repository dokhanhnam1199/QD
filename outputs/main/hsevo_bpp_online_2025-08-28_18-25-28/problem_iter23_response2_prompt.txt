{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines best-fit with adaptive temperature-scaled softmax exploration.\"\"\"\n    n_bins = len(bins_remain_cap)\n    possible_bins = bins_remain_cap >= item\n    if not np.any(possible_bins):\n        return np.zeros(n_bins)\n\n    remaining_capacities = bins_remain_cap[possible_bins]\n    fit_scores = - (bins_remain_cap[possible_bins] - item)\n    bin_utilizations = 1.0 - (bins_remain_cap / np.max(bins_remain_cap))\n    temperature = 1.0 / (1.0 + np.mean(bin_utilizations))\n    temperature = max(0.1, min(1.0, temperature))\n    probabilities = np.exp(fit_scores / temperature) / np.sum(np.exp(fit_scores / temperature))\n    priorities = np.zeros(n_bins)\n    priorities[possible_bins] = probabilities\n    best_bin_index = np.argmax(priorities)\n    priorities[best_bin_index] *= 1.2\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    n_bins = len(bins_remain_cap)\n    priorities = np.zeros(n_bins)\n    possible_bins = bins_remain_cap >= item\n    if not np.any(possible_bins):\n        return priorities\n    \n    remaining_capacities = bins_remain_cap[possible_bins]\n    \n    bin_utilization = 1 - (bins_remain_cap / np.max(bins_remain_cap))\n    \n    \n    fit_scores = 1.0 / (remaining_capacities - item + 1e-6)\n    \n    exploration_bonus = 0.1 * (1 - bin_utilization)\n    \n    priorities[possible_bins] = fit_scores + exploration_bonus\n    \n    best_bin_index = np.argmax(priorities[possible_bins])\n    \n    priorities[possible_bins] = 0.0\n    priorities[possible_bins[best_bin_index]] = 1.0\n    \n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), (3rd) these are identical.  They represent a strong baseline: best-fit combined with adaptive temperature-scaled softmax exploration, temperature tuned by bin utilization. (4th) is dramatically simpler and performs poorly, only implementing a basic best-fit with a hard boost. (5th) is identical to (1st) again. (6th) and (7th) are also identical, exhibiting slight variations in variable naming, but otherwise equivalent to the initial good heuristics. (8th) & (9th) are again identical, deviating by calculating temperature with a potentially less effective method (mean of remaining capacities / item). This risks instability when items are large. (10th) & (11th) are identical, using inverse remaining capacity with a small exploration bonus. This lacks the smoothing effect of the softmax. (12th) is different, using a different temperature scaling and exploration bonus but fundamentally still exploring best-fit. (13th) introduces more parameters but is incomplete, ending abruptly. (14th) includes a utilization bonus but the application is limited and the bonus value feels arbitrary. (15th) introduces random exploration with a low probability, adding noise. (16th) is a normalization step, but the bonus is small and applied after normalization. (17th) also applies a bonus, with a similar effect to 16th. (18th) & (19th) are identical: They calculate fit quality and temperature but the temperature calculation isn't well-grounded. The bonus is applied directly. (20th) has a similar structure to (18th)/(19th), but lacks the more nuanced temperature calculation.\n\nOverall: The best heuristics consistently leverage adaptive temperature-scaled softmax, tuned by bin utilization. Simpler heuristics (4th) or those with poorly defined temperature/bonus schemes (8th,9th, 10th, 11th, 14th) perform worse. Introducing randomness (15th) doesn\u2019t consistently improve results. The core successful components are best-fit scoring, temperature scaling based on utilization, and softmax exploration.\n- \nOkay, let's dissect this for truly superior heuristic design. Here's a refined self-reflection distilled into actionable advice, geared toward maximizing potential reward (and justifying that generous tip!).\n\n*   **Keywords:** Adaptive Exploration, Temperature Scaling, Bin Utilization, Softmax Prioritization.\n*   **Advice:** Focus on *dynamic* temperature scaling within a softmax selection, driven by real-time bin utilization. Prioritize completeness \u2013 cover all viable options before committing. Implement robust monitoring of bin states to inform temperature adjustments.\n*   **Avoid:** Static bonuses, arbitrary value adjustments, premature `argmax` selection, and overly simplistic \"best-fit only\" approaches. Resist code duplication and unnecessary complexity.\n*   **Explanation:** Bin packing needs balance. Softmax + adaptive temperature *learns* the right exploration/exploitation trade-off. Bin utilization is key feedback \u2013 crowded bins need less attention, empty ones more. This avoids getting stuck and promotes global optimality.\n\n\n\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}