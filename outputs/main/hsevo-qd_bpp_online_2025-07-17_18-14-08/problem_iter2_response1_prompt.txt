{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function prioritizes bins based on a combination of factors:\n    1. How well the item fits (smaller wasted space is better).\n    2. A probabilistic element to explore different bin choices (inspired by quantum randomness).\n    3. A penalty for bins that are almost full (risk of overfilling).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Avoid division by zero\n    bins_remain_cap = np.clip(bins_remain_cap, 1e-6, None)\n\n    # 1. Fit Score: Smaller wasted space is better\n    wasted_space = bins_remain_cap - item\n    fit_score = np.exp(-np.abs(wasted_space)) # Gaussian-like, peaked at 0 wasted space. exp(-waste) more sensitive to small wastes\n\n    # Adjust fit score: Penalize if waste is very high. Use the original capacity for scaling\n    fit_score = np.where(wasted_space < 0, -1e9, fit_score)  # Assign extremely low score if overfill.  Make really small\n\n    # 2. Probabilistic Element: \"Quantum Tunneling\" - Allow some exploration\n    # Inspired by the idea that a particle can tunnel through barriers.\n    #  Add small random numbers (proportional to the item size). This encourages exploration.\n    probabilistic_boost = np.random.rand(len(bins_remain_cap)) * item * 0.01  # Small boost\n\n    # 3. Risk Factor: Penalize bins that are close to full (Higher risk of overfilling with subsequent items).\n    # This is an exponentially decaying penalty near capacity.\n    capacity_ratio = item / bins_remain_cap\n    risk_penalty = np.exp(10 * (capacity_ratio - 1))\n    risk_penalty = np.clip(risk_penalty, 0, 1e9)  # limit penalty effect to prevent nan. The lower this upper bound, the less sensitive\n\n    # Combine all factors\n    priorities = fit_score + probabilistic_boost - risk_penalty\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n    Considers both remaining capacity and a \"gravitational pull\"\n    towards bins that are already somewhat full.\n    Inspired by gravitational lensing and spacetime curvature -\n    items \"prefer\" to be near existing \"mass\" (filled bins).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Avoid division by zero:\n    bins_remain_cap = np.clip(bins_remain_cap, 1e-9, None)\n\n    # 1. Capacity Consideration (Similar to First Fit Decreasing):\n    capacity_priority = bins_remain_cap - item  # Higher is better\n\n    # If the item doesn't fit, give very low priority.\n    capacity_priority[capacity_priority < 0] = -np.inf\n\n    # 2. \"Gravitational\" Factor (Encourages filling nearly-full bins):\n    # Higher remaining cap means weaker \"gravity\", so use inverse.\n    # Small constant added to avoid division by zero, and to tweak the influence\n    # Note, the greater the item, the larger its effect is to \"pull\" to itself\n    gravitational_constant = 0.1\n    gravity = item / (bins_remain_cap + gravitational_constant)  # The closer it is to filled, the higher priority\n    gravity = np.nan_to_num(gravity, neginf=0)\n\n    # 3. Combine Priorities (Relativistic Addition - loosely inspired):\n    # Add some small factor such that a large change to either parameter doesn't change the total by as much.\n    #   - We should favor adding to any bin with cap sufficiently large rather than prioritizing the full bins.\n\n    alpha = 0.9 # Tunable constant\n\n    priorities = alpha * capacity_priority + (1-alpha) * gravity\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see 1st uses a combined score of capacity and waste, with exponential decay for waste close to 0, while 20th prioritizes based on remaining capacity, item/cap ratio, and penalizes similar sized bins. 1st's approach of combining capacity and waste appears more effective.\n\nComparing (2nd) vs (19th), we see 2nd prioritizes bins that fit the item well and aren't too empty afterwards with scaling and negative penalization, while 19th simply uses log ratios of item size to remaining capacity. The more complex scaling and penalization in 2nd are beneficial.\n\nComparing (1st) vs (2nd), we see 1st uses exponential decay for waste minimization and combines capacity and waste scores, while 2nd focuses on scaling based on bin fullness after packing and penalizing bins that are too large or empty. The difference lies in the specific functions used to combine capacity and waste.\n\nComparing (3rd) vs (4th), we see 3rd gives high priority to bins that can fit the item with small waste and penalizes bins that cannot fit, while 4th prioritizes based on perfect fit, best fit, utilization ratio, and avoids nearly full bins. 4th uses specific bonuses and penalties, while 3rd is simpler with waste minimization.\n\nComparing (2nd) vs (3rd), we see that 2nd uses a squared ratio and capacity normalization, while 3rd uses simple inverse of waste and assigns binary priorities based on nearly full condition.\n\nComparing (19th) vs (20th), we see that 19th uses the log ratio, while 20th computes the priority based on remaining capacity, item/cap ratio and penalizes similar sized bins as the items.\n\nOverall: The better heuristics incorporate a balance between remaining capacity, waste minimization, and avoidance of nearly full bins, often using exponential functions or scaling to fine-tune the priorities. Simpler approaches such as just log ratio are less effective. The best heuristics contain several components of First-Fit-Decreasing and Best-Fit. The more elaborate and fine-grained that the penalties were, the better.\n- \nOkay, let's redefine current self-reflection for heuristic design, focusing on actionable insights and avoiding pitfalls.\n\n*   **Keywords:** Adaptive, Iterative, Performance Metrics, Solution Space Analysis.\n*   **Advice:** Design heuristics iteratively, using performance metrics to guide refinements. Analyze the solution space to understand heuristic behavior. Adapt the heuristic's parameters based on feedback.\n*   **Avoid:** Premature optimization, neglecting edge cases, rigid parameter settings, lack of performance tracking.\n*   **Explanation:** Move beyond static combinations. Focus on adaptive strategies that learn from past performance, explore the solution space effectively, and dynamically adjust to different problem instances.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}