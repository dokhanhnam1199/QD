```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray, temperature: float = 1.0, k: float = 5.0, gamma: float = 0.1, delta: float = 0.01, epsilon: float = 1e-6) -> np.ndarray:
    """
    Returns priority probabilities for packing an item into bins using a
    combination of tight fit preference, bin scarcity, earlier bin preference,
    and Softmax for probabilistic selection.

    This heuristic prioritizes:
    1.  **Tight Fits:** Favors bins where the remaining capacity is close to the item size.
        A sigmoid function on the "mismatch" (remaining_capacity - item) is used,
        where smaller mismatch yields a higher score.
    2.  **Bin Scarcity:** Slightly favors bins that have less remaining capacity overall,
        as these are scarcer resources. A bonus is added inversely proportional to
        the remaining capacity.
    3.  **Earlier Bin Preference:** As a tie-breaker, favors bins that were opened earlier
        (i.e., appear earlier in the `bins_remain_cap` array).
    4.  **Probabilistic Exploration (Softmax):** Converts the combined priority scores
        into probabilities using the Softmax function. The `temperature` parameter
        controls the exploration-exploitation trade-off:
        - Low temperature (close to 0): Primarily exploits the highest-scoring bins.
        - High temperature (large value): Leads to more uniform probabilities,
          encouraging exploration of less optimal bins.

    The raw score for each bin `i` is calculated as:
    `Score_i = SigmoidFit(bins_remain_cap[i], item, k) + gamma * (1 / (bins_remain_cap[i] + epsilon)) + delta * (1 / (i + 1))`
    where `SigmoidFit` is `1 / (1 + exp(k * (bins_remain_cap[i] - item)))`.
    Then, probabilities are derived using Softmax:
    `P_i = exp(Score_i / temperature) / sum(exp(Score_j / temperature))`

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.
        temperature: Controls the Softmax exploration/exploitation balance.
                     Higher values lead to more exploration. Defaults to 1.0.
        k: Sensitivity parameter for the sigmoid function (tightest fit preference).
           Higher `k` increases preference for tighter fits. Defaults to 5.0.
        gamma: Weight for the bin scarcity bonus. Higher `gamma` increases
               preference for less empty bins. Defaults to 0.1.
        delta: Weight for the earlier bin preference tie-breaker. Higher `delta`
               increases preference for earlier bins. Defaults to 0.01.
        epsilon: Small value to prevent division by zero in scarcity calculation.
                 Defaults to 1e-6.

    Returns:
        A NumPy array of probabilities, same size as `bins_remain_cap`.
        Bins that cannot fit the item will have a probability of 0.
    """
    num_bins = len(bins_remain_cap)
    raw_scores = np.full(num_bins, -np.inf, dtype=float) # Initialize with -inf for invalid bins

    # Identify bins that can accommodate the item
    suitable_bins_mask = bins_remain_cap >= item
    suitable_bin_indices = np.where(suitable_bins_mask)[0]

    if suitable_bin_indices.size > 0:
        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]
        
        # 1. Sigmoid Fit Score: Prioritize tight fits.
        # Calculate mismatch (wasted space)
        mismatch = suitable_bins_cap - item
        
        # Cap exponent argument to prevent overflow in np.exp.
        # A large positive mismatch should result in a score close to 0.
        # A mismatch of 0 should result in a score close to 0.5.
        # Sigmoid(x) = 1 / (1 + exp(x)). We want smaller mismatch (closer to 0) to be better.
        # Let's use `k * mismatch`. If mismatch is small (e.g., 0), exp(0)=1, sigmoid=0.5.
        # If mismatch is larger, exp(k*mismatch) increases, sigmoid decreases.
        # To make tighter fits higher score, we want `1 - sigmoid` or `sigmoid(-k*mismatch)`.
        # Let's define fit score such that smaller `mismatch` is higher score.
        # Option: 1 / (1 + exp(k * mismatch)). This gives higher score for larger mismatch.
        # Option: 1 / (1 + exp(-k * mismatch)). This gives higher score for smaller mismatch.
        # We want small positive mismatch (tight fit) to be prioritized.
        # So, we want score to be high when `mismatch` is close to 0.
        # Let's use `exp(-k * mismatch)`. Then normalize it.
        # Or, use `1 / (1 + exp(k * mismatch))` and invert it, or simply use the original formulation
        # and recognize that lower `mismatch` leads to `1/(1+exp(small_positive))` which is higher.
        # Let's stick with `1 / (1 + exp(k * mismatch))` and interpret it directly.
        # Larger values mean smaller `mismatch` if we use `k * mismatch` as is.
        
        # To ensure higher score for tighter fit, we want function to be decreasing with mismatch.
        # `sigmoid_score = 1 / (1 + np.exp(k * mismatch))`
        # This means larger mismatch -> larger exponent -> smaller score. This is good.
        
        max_exponent_arg = 35.0 # Prevent overflow
        capped_exponent_arg = np.minimum(k * mismatch, max_exponent_arg)
        sigmoid_scores = 1.0 / (1.0 + np.exp(capped_exponent_arg))

        # 2. Bin Scarcity Bonus: Favor less empty bins.
        # Use 1 / (capacity + epsilon) as a proxy for "fullness".
        scarcity_bonus = gamma * (1.0 / (suitable_bins_cap + epsilon))

        # 3. Earlier Bin Preference: Tie-breaker for bins opened earlier.
        # Indices are 0-based, so we add 1 to avoid division by zero and shift index.
        tie_breaker_bonus = delta * (1.0 / (suitable_bin_indices + 1.0))

        # Combine scores for suitable bins
        combined_scores = sigmoid_scores + scarcity_bonus + tie_breaker_bonus
        
        # Assign combined scores back to the raw_scores array
        raw_scores[suitable_bins_mask] = combined_scores

    # If temperature is very low (close to 0), it's pure exploitation.
    # Avoid division by zero if temperature is 0.
    if temperature <= epsilon:
        if np.all(raw_scores == -np.inf): # No suitable bins
            return np.zeros(num_bins)
            
        max_score = np.max(raw_scores)
        # Assign probability 1 to the bin(s) with the maximum score
        probabilities = np.where(raw_scores == max_score, 1.0, 0.0)
        
        # Normalize to ensure sum is 1 if multiple max scores exist
        num_max_scores = np.sum(probabilities)
        if num_max_scores > 0:
            probabilities /= num_max_scores
        return probabilities

    # Apply Softmax to convert scores to probabilities for exploration.
    # Shift scores by subtracting the max score for numerical stability.
    # exp(x_i / T) / sum(exp(x_j / T)) = exp((x_i - max(x)) / T) / sum(exp((x_j - max(x)) / T))
    
    # If all scores are -inf (no suitable bins), max_raw_score will be -inf.
    # In this case, return all zeros.
    if np.all(raw_scores == -np.inf):
        return np.zeros(num_bins)

    max_raw_score = np.max(raw_scores)
    
    # Handle potential case where max_raw_score is -inf (should be caught above, but for safety)
    if not np.isfinite(max_raw_score):
         return np.zeros(num_bins)

    shifted_scores = (raw_scores - max_raw_score) / temperature
    
    # Calculate exponential of shifted scores.
    # exp_scores can be 0 if shifted_scores are very negative.
    exp_scores = np.exp(shifted_scores)
    
    # Calculate sum of exponential scores for normalization.
    sum_exp_scores = np.sum(exp_scores)
    
    # Calculate probabilities. Handle case where sum_exp_scores is 0 (e.g., all exp_scores were 0).
    if sum_exp_scores > 0:
        probabilities = exp_scores / sum_exp_scores
    else:
        # This implies all exp_scores were effectively zero, meaning all shifted scores were extremely negative.
        # In this scenario, all probabilities should be zero, or distributed uniformly if
        # we interpret the lack of finite scores as equal unsuitability.
        # Given our `raw_scores` initialization with -inf, this should ideally not happen
        # unless `temperature` is extremely large or all `raw_scores` are -inf.
        # If `raw_scores` contained finite values but `sum_exp_scores` is 0, it's a numerical issue.
        # Setting to zeros is a safe fallback.
        probabilities = np.zeros(num_bins)

    # Ensure probabilities sum to 1 (due to potential floating point inaccuracies)
    # and handle any NaN values that might arise from edge cases.
    probabilities = np.nan_to_num(probabilities)
    if np.sum(probabilities) > epsilon: # Re-normalize if sum is significantly greater than 0
        probabilities /= np.sum(probabilities)
    elif not np.all(raw_scores == -np.inf): # If there were valid scores but sum is 0
        # This suggests all valid scores resulted in near-zero exp_scores.
        # A uniform distribution over suitable bins might be better if possible.
        # However, given the Softmax logic, zero sum means zero probabilities.
        pass # Keep probabilities as zeros

    return probabilities
```
