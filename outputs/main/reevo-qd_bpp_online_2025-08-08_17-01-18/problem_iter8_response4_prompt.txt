{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a refined priority strategy.\n\n    This heuristic prioritizes bins based on several factors:\n    1.  **Tight Fit:** Prioritizes bins that have just enough remaining capacity for the item,\n        minimizing waste.\n    2.  **Bin Scarcity:** Prefers bins that have less remaining capacity overall, making them\n        \"scarcer\" and potentially needing to be filled to capacity sooner.\n    3.  **Earlier Bin Preference (Tie-breaking):** If multiple bins offer similar priority,\n        bins that appear earlier in the `bins_remain_cap` array are slightly preferred.\n    4.  **Probabilistic Exploration (Softmax):** Uses Softmax to convert scores into probabilities,\n        allowing for probabilistic selection. This balances exploitation (choosing the best fit)\n        with exploration (trying less optimal bins occasionally).\n\n    The scoring mechanism combines the tight-fit score with a scarcity factor.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, representing the\n        probability (or weighted priority) of selecting each bin. Bins that cannot\n        fit the item will have a probability of 0.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities  # No bin can fit the item\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    # --- Scoring Components ---\n\n    # 1. Tight Fit Score (Sigmoid-based, similar to v1 but adjusted for Softmax)\n    # We want a score that is high for tight fits and decreases as capacity increases.\n    # A score close to 1 for perfect fit, decreasing towards 0.\n    # Let's use `1 / (1 + exp(k * (capacity - item)))`\n    # For perfect fit (capacity - item = 0), score = 0.5\n    # For capacity - item > 0, score decreases.\n    # To work well with Softmax, scores should be non-negative.\n    # We can shift the sigmoid output: `sigmoid_score = 1 / (1 + exp(k * mismatch))`.\n    # A perfect fit gives 0.5. A slightly larger fit gives slightly less than 0.5.\n    # Let's ensure scores are always positive. Maybe add a small constant or use a different transformation.\n    # Alternative: `exp(-k * mismatch)`. Perfect fit = exp(0) = 1. Larger mismatch = smaller score.\n    # This aligns better with Softmax. Let's use this.\n    k_tightness = 5.0  # Sensitivity to tightness. Higher k -> stronger preference for tight fits.\n    mismatch = suitable_bins_cap - item\n    \n    # Cap exponent to prevent overflow/underflow issues with exp, especially for large k or mismatch\n    max_exponent_val = 700.0 # exp(700) is very large\n    min_exponent_val = -700.0 # exp(-700) is very close to 0\n    \n    tightness_scores = np.exp(-k_tightness * mismatch)\n    \n    # Ensure scores are within a reasonable range if needed, though exp(-x) is usually fine.\n    # For robustness, one could clamp the argument to exp.\n    # capped_exponent = np.clip(-k_tightness * mismatch, min_exponent_val, max_exponent_val)\n    # tightness_scores = np.exp(capped_exponent)\n\n\n    # 2. Bin Scarcity Score\n    # Prioritize bins with less remaining capacity. A simple inverse relationship works.\n    # Add a small epsilon to avoid division by zero if a bin somehow has 0 capacity.\n    epsilon = 1e-6\n    scarcity_scores = 1.0 / (suitable_bins_cap + epsilon)\n    \n    # Normalize scarcity scores to be in a similar range or complement tightness score.\n    # Let's combine them additively, scaled appropriately.\n    # We want tight fit to dominate, but scarcity to break ties or influence choices.\n    # A simple weighted sum: `score = w1 * tightness + w2 * scarcity`\n    # Or, since both are positive and indicate preference, we can multiply them\n    # if we want them to be strong together, or add if we want them to be independent factors.\n    # Let's try combining them additively, scaled to avoid one dominating too much.\n    \n    # Normalize scores before combining to manage scales:\n    # Max possible tightness score is 1 (perfect fit). Min depends on max mismatch.\n    # Max possible scarcity score is 1/epsilon. Min is 1/max_capacity.\n    # This suggests scarcity might dominate if not scaled.\n    \n    # Let's scale scarcity by the inverse of the typical bin capacity or by a factor.\n    # A simpler approach: add a \"bonus\" for being scarce.\n    # Consider the total capacity of suitable bins. A bin with less capacity is scarcer.\n    # Let's scale scarcity scores based on the mean capacity of suitable bins.\n    # Or, we can think of scarcity as a \"bonus\" for having less space.\n    # Let's try making scarcity a multiplier for tightness, but capped.\n    # Or, add scarcity as a bonus.\n    \n    # Let's use additive combination with some scaling for scarcity.\n    # Scarcity score: Higher for less capacity.\n    # We want to favor bins that are nearly full.\n    # Consider the remaining capacity relative to the item size.\n    # Alternative scarcity: `1.0 - (suitable_bins_cap / max_possible_capacity)`\n    # Or simply use the inverse: `1.0 / suitable_bins_cap`\n    \n    # Let's normalize the scarcity scores to be between 0 and 1.\n    min_cap = np.min(suitable_bins_cap)\n    max_cap = np.max(suitable_bins_cap)\n    \n    if max_cap - min_cap > epsilon: # Avoid division by zero if all capacities are the same\n        normalized_scarcity = (suitable_bins_cap - min_cap) / (max_cap - min_cap)\n        # Invert for scarcity: higher score for less capacity\n        scarcity_bonus = 1.0 - normalized_scarcity\n    else:\n        scarcity_bonus = np.ones_like(suitable_bins_cap) * 0.5 # All bins equally scarce/plentiful\n\n    # 3. Combine tightness and scarcity\n    # Weighted sum: prioritize tightness, add scarcity as a secondary factor.\n    weight_tightness = 1.0\n    weight_scarcity = 0.3 # Give scarcity a moderate influence\n    \n    combined_scores = (weight_tightness * tightness_scores) + (weight_scarcity * scarcity_bonus)\n\n    # 4. Tie-breaking (Earlier Bin Preference)\n    # Add a small bonus based on index.\n    # The index itself can be used, scaled down.\n    index_bonus_scale = 0.01\n    index_bonus = suitable_bins_indices * index_bonus_scale\n    \n    final_scores = combined_scores + index_bonus\n\n    # Apply Softmax to get probabilities\n    # Softmax is `exp(score) / sum(exp(scores))`\n    # We need to handle potential large values in `final_scores` causing `exp` overflow.\n    # A common trick is to subtract the maximum score before exponentiating.\n    # `exp(x_i - max(x)) / sum(exp(x_j - max(x)))`\n    # This doesn't change the resulting probabilities.\n    \n    if final_scores.size > 0:\n        max_score = np.max(final_scores)\n        # Ensure scores passed to exp are not excessively large after subtraction\n        # Although subtracting max should handle it, numerical stability can be an issue.\n        # Let's cap the intermediate values before exp.\n        \n        shifted_scores = final_scores - max_score\n        # Cap shifted scores to prevent exp overflow even after subtraction if differences are large\n        # A value like 50-100 is usually safe.\n        capped_shifted_scores = np.clip(shifted_scores, -100.0, 100.0)\n        \n        exp_scores = np.exp(capped_shifted_scores)\n        sum_exp_scores = np.sum(exp_scores)\n        \n        if sum_exp_scores > 0:\n            probabilities = exp_scores / sum_exp_scores\n        else:\n            # This case should ideally not happen if scores are finite, but for safety:\n            probabilities = np.ones_like(exp_scores) / len(exp_scores) if len(exp_scores) > 0 else np.array([])\n    else:\n        probabilities = np.array([])\n\n    # Place the calculated probabilities back into the main priorities array\n    priorities[suitable_bins_mask] = probabilities\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    This heuristic prioritizes bins that can fit the item and are \"nearly full\"\n    (i.e., have little remaining capacity after fitting the item), as this\n    encourages tighter packing. A small random exploration component is added\n    to avoid getting stuck in local optima. The exploration rate (epsilon) is\n    reduced to favor greedy choices more. Scores are normalized to a [0, 1] range\n    for a more stable epsilon-greedy behavior.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.05  # Reduced probability of random exploration for more greedy behavior\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate scores for bins that can fit the item\n    # Prioritize bins that leave little remaining capacity after fitting the item.\n    # Score is the inverse of the remaining capacity after fitting.\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Add a small constant to the denominator to prevent division by zero and overly large scores\n    # for bins that perfectly fit the item.\n    scores = 1.0 / (remaining_capacity_after_fit + 1e-6) \n    \n    # Normalize scores to be between 0 and 1.\n    # This makes the epsilon-greedy selection more balanced.\n    if scores.size > 0:\n        min_score = np.min(scores)\n        max_score = np.max(scores)\n        if max_score > min_score:\n            priorities[can_fit_mask] = (scores - min_score) / (max_score - min_score)\n        else:\n            # If all scores are the same (e.g., all bins have the same remaining capacity after fit),\n            # assign a neutral priority, or simply a value representing the best fit.\n            # Assigning 0.5 could be seen as neutral, but since they are all equally good,\n            # a higher uniform value (like 1.0) might be more indicative of a good fit.\n            # Let's stick to a normalized 1.0 for \"equally best\" fit.\n            priorities[can_fit_mask] = 1.0\n    \n    # Epsilon-Greedy exploration: with probability epsilon, choose a random bin that fits.\n    # This allows for exploration of less optimal (but still valid) bins.\n    if np.random.rand() < epsilon:\n        possible_bins_indices = np.where(can_fit_mask)[0]\n        if possible_bins_indices.size > 0:\n            # Select a random bin among those that can fit the item.\n            random_bin_index = np.random.choice(possible_bins_indices)\n            \n            # Reset priorities and assign the highest priority to the randomly chosen bin.\n            # This ensures that the exploration step effectively picks a random bin.\n            priorities = np.zeros_like(bins_remain_cap, dtype=float)\n            priorities[random_bin_index] = 1.0  # Highest priority\n\n    return priorities\n\n[Reflection]\nFocus on clear objectives, simple scoring, and controlled exploration.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}