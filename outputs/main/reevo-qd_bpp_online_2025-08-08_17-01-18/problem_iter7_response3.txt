```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a Softmax-based unified scoring mechanism.

    This heuristic prioritizes bins that offer the "tightest fit" for an incoming item,
    while also considering bin scarcity. It uses a unified scoring mechanism that
    balances the preference for a tight fit with a general preference for bins that
    are not excessively large. The scores are then normalized using a Softmax function
    with a tunable temperature parameter, allowing for adaptive exploration/exploitation.

    The core scoring for a suitable bin (where `remaining_capacity >= item`) is:
    `score = exp(-k * (remaining_capacity - item))`

    Here:
    - `remaining_capacity - item` represents the "mismatch" or wasted space.
    - `k` is a parameter controlling the sensitivity to the mismatch. A higher `k`
      amplifies the penalty for larger mismatches, favoring tighter fits more strongly.
      A lower `k` is more forgiving of larger mismatches.
    - The negative sign ensures that smaller mismatches (tighter fits) result in
      larger exponent values and thus higher scores before Softmax.

    After calculating raw scores for all suitable bins, a Softmax function is applied
    to normalize these scores into probabilities (priorities).
    `priority = exp(raw_score / temperature) / sum(exp(raw_score / temperature))`

    - `temperature` controls the shape of the probability distribution.
      - A low temperature makes the distribution "sharper," strongly favoring
        bins with the highest raw scores (exploitation).
      - A high temperature makes the distribution "flatter," increasing the
        probability of selecting bins with lower raw scores (exploration).

    Bins that cannot fit the item are assigned a priority of 0.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.

    Returns:
        A NumPy array of the same size as `bins_remain_cap`, where each element
        is the priority score (probability) for the corresponding bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins that can accommodate the item
    suitable_bins_mask = bins_remain_cap >= item
    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]

    # If no bin can fit the item, return all zeros
    if suitable_bins_cap.size == 0:
        return priorities

    # --- Tunable Parameters ---
    # Sensitivity to mismatch: higher k means stronger preference for tighter fits.
    k = 1.0
    # Softmax temperature: controls exploration/exploitation.
    # Lower temp = more exploitation, higher temp = more exploration.
    temperature = 0.5
    # --------------------------

    # Calculate the "mismatch" for suitable bins
    mismatch = suitable_bins_cap - item

    # Calculate raw scores: prefer smaller mismatches (tighter fits)
    # Using exp(-k * mismatch) means smaller mismatch -> larger score
    # To prevent overflow for very small mismatches (exp(large_positive)),
    # we can cap the negative mismatch if it's very large (which shouldn't happen if k>0 and mismatch>=0)
    # The primary concern is exp() of large negative numbers if k or mismatch were negative,
    # but here k>=0 and mismatch>=0.
    # However, for very large positive `k * mismatch`, `exp(-k * mismatch)` will be close to 0.
    # We might want to avoid `exp` of extremely large negative numbers if `k * mismatch` is very large.
    # Capping `k * mismatch` to a maximum value is a good practice.
    max_positive_mismatch_score_arg = 10.0 # exp(-10) is small but not zero
    capped_mismatch = np.minimum(mismatch, max_positive_mismatch_score_arg / k if k > 0 else np.inf)
    raw_scores = np.exp(-k * capped_mismatch)

    # Apply Softmax to normalize scores into probabilities (priorities)
    if temperature <= 0:
        # If temperature is zero or negative, select the bin with the highest raw score deterministically.
        # This is equivalent to a greedy approach among suitable bins.
        # We can find the index of the max score and assign probability 1 to it.
        max_score_idx = np.argmax(raw_scores)
        softmax_priorities = np.zeros_like(raw_scores)
        softmax_priorities[max_score_idx] = 1.0
    else:
        # Standard Softmax calculation
        # Add a small epsilon to the denominator to prevent division by zero if all raw_scores are -inf (not possible here)
        # or if the sum of exponents is zero.
        try:
            # Using log-sum-exp trick for numerical stability if needed, but for exp(-k*mismatch)
            # where mismatch >= 0 and k >= 0, values are between 0 and 1.
            # Direct calculation is likely fine.
            exp_scores = np.exp(raw_scores / temperature)
            sum_exp_scores = np.sum(exp_scores)
            if sum_exp_scores == 0: # Handle case where all exp_scores are ~0
                softmax_priorities = np.ones_like(raw_scores) / len(raw_scores) if len(raw_scores) > 0 else np.array([])
            else:
                softmax_priorities = exp_scores / sum_exp_scores
        except FloatingPointError:
            # Fallback for extreme values if exp calculation fails
            softmax_priorities = np.zeros_like(raw_scores)
            if len(raw_scores) > 0:
                max_score_idx = np.argmax(raw_scores)
                softmax_priorities[max_score_idx] = 1.0


    # Place the calculated softmax priorities back into the main priorities array
    priorities[suitable_bins_mask] = softmax_priorities

    return priorities
```
