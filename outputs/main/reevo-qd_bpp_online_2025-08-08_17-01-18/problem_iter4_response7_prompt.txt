{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    This heuristic prioritizes bins that can fit the item and are \"nearly full\"\n    (i.e., have little remaining capacity after fitting the item), as this\n    encourages tighter packing. A small random exploration component is added\n    to avoid getting stuck in local optima. The exploration rate (epsilon) is\n    reduced to favor greedy choices more. Scores are normalized to a [0, 1] range\n    for a more stable epsilon-greedy behavior.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.05  # Reduced probability of random exploration for more greedy behavior\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate scores for bins that can fit the item\n    # Prioritize bins that leave little remaining capacity after fitting the item.\n    # Score is the inverse of the remaining capacity after fitting.\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Add a small constant to the denominator to prevent division by zero and overly large scores\n    # for bins that perfectly fit the item.\n    scores = 1.0 / (remaining_capacity_after_fit + 1e-6) \n    \n    # Normalize scores to be between 0 and 1.\n    # This makes the epsilon-greedy selection more balanced.\n    if scores.size > 0:\n        min_score = np.min(scores)\n        max_score = np.max(scores)\n        if max_score > min_score:\n            priorities[can_fit_mask] = (scores - min_score) / (max_score - min_score)\n        else:\n            # If all scores are the same (e.g., all bins have the same remaining capacity after fit),\n            # assign a neutral priority, or simply a value representing the best fit.\n            # Assigning 0.5 could be seen as neutral, but since they are all equally good,\n            # a higher uniform value (like 1.0) might be more indicative of a good fit.\n            # Let's stick to a normalized 1.0 for \"equally best\" fit.\n            priorities[can_fit_mask] = 1.0\n    \n    # Epsilon-Greedy exploration: with probability epsilon, choose a random bin that fits.\n    # This allows for exploration of less optimal (but still valid) bins.\n    if np.random.rand() < epsilon:\n        possible_bins_indices = np.where(can_fit_mask)[0]\n        if possible_bins_indices.size > 0:\n            # Select a random bin among those that can fit the item.\n            random_bin_index = np.random.choice(possible_bins_indices)\n            \n            # Reset priorities and assign the highest priority to the randomly chosen bin.\n            # This ensures that the exploration step effectively picks a random bin.\n            priorities = np.zeros_like(bins_remain_cap, dtype=float)\n            priorities[random_bin_index] = 1.0  # Highest priority\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a blended strategy.\n\n    This strategy combines the preference for exact fits with the soft preference\n    for near-exact fits using a Softmax-like approach. It prioritizes bins\n    where the remaining capacity is exactly the item size, and then assigns\n    decreasing priority to bins with slightly larger remaining capacities.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if not eligible_bins_cap.size:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit score\": how much space is left after placing the item.\n    # Smaller values indicate a tighter fit.\n    fit_scores = eligible_bins_cap - item\n\n    # Assign a base priority of 1.0 to exact fits and a lower base priority to partial fits.\n    # This ensures exact fits are always preferred over partial fits before softmax.\n    base_priorities = np.where(fit_scores == 0, 1.0, 0.5)\n\n    # To incorporate the \"near-exact\" preference smoothly, we can modify the\n    # base priorities based on how close the fit is, using a logistic or exponential decay.\n    # Here, we use a simple exponential decay for remaining space.\n    # Smaller remaining space (after fitting) should get higher priority.\n    # We add 1 to fit_scores to handle the case of exact fit (fit_score=0) gracefully\n    # and ensure positive values for exponentiation.\n    # The scaling factor (e.g., 1.0) and the decay rate (e.g., 0.1) can be tuned.\n    decay_rate = 0.1\n    near_fit_scores = np.exp(-decay_rate * fit_scores)\n\n    # Blend the base priority with the near-fit score.\n    # Exact fits should retain their high base priority, while near-fits get a boost.\n    # We prioritize exact fits (score 1.0) and then near-exact fits.\n    # A simple way to blend is to amplify the scores of exact fits and give\n    # a scaled score to near-fits.\n    combined_scores = np.where(fit_scores == 0, 1.0, near_fit_scores)\n\n    # Apply a Softmax-like transformation to normalize and create probabilities.\n    # This smooths the preferences, giving higher probability to better fits.\n    # The temperature parameter controls the \"softness\".\n    temperature = 0.5\n    try:\n        # Add a small epsilon to prevent log(0) or division by zero issues\n        epsilon = 1e-9\n        scaled_scores = combined_scores / temperature\n        exp_scores = np.exp(scaled_scores)\n        softmax_priorities = exp_scores / np.sum(exp_scores)\n    except OverflowError:\n        # Fallback for overflow: assign uniform probability if scores are too extreme\n        softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size\n\n\n    # Map the calculated priorities back to the original bin indices\n    priorities[can_fit_mask] = softmax_priorities\n\n    return priorities\n\n[Reflection]\nPrioritize exact fits, smooth preferences with a decay, and use Softmax for balanced probabilities.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}