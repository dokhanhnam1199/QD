```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin, prioritizing tight fits and considering bin scarcity.

    This heuristic prioritizes bins that can accommodate the item, favoring "tight fits"
    (smaller remaining capacity after packing) to minimize wasted space.
    It also introduces a penalty for bins that are already very full (scarce),
    encouraging the use of bins with more remaining capacity if the fit is not extremely tight.
    A softmax function is used to convert these preferences into probabilities,
    allowing for some exploration. The temperature parameter controls the
    aggressiveness of the selection. Bins that are too small for the item receive a priority of 0.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    temperature = 0.5  # Tune this parameter: lower for more greedy, higher for more exploration
    scarcity_penalty_factor = 0.1 # Factor to penalize bins that are already nearly full

    # Identify bins that can fit the item
    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        return priorities # No bins can fit the item

    fitting_bins_caps = bins_remain_cap[can_fit_mask]

    # Calculate a score based on how tight the fit is.
    # A smaller remaining capacity after packing means a tighter fit.
    # Add a small epsilon to avoid division by zero if a bin has exactly the item's size.
    tight_fit_scores = 1.0 / (fitting_bins_caps - item + 1e-9)

    # Introduce a scarcity penalty: Bins with less remaining capacity overall are less preferred
    # unless the fit is extremely tight. We can scale the inverse of remaining capacity.
    # We want to penalize bins that are already very full, so we use 1/capacity.
    # However, we only want this penalty to apply if the fit isn't perfectly tight.
    # A simple approach is to add a small term that decreases with remaining capacity.
    # We'll use a factor that scales the inverse of the bin's *total* capacity
    # to penalize those that are less likely to accommodate future items.
    # For simplicity, we'll use the current remaining capacity as a proxy for scarcity.
    # Bins with less remaining capacity are considered more scarce.
    scarcity_scores = scarcity_penalty_factor * (1.0 / (fitting_bins_caps + 1e-9))

    # Combine tight fit preference with scarcity penalty.
    # We want to favor tight fits, so tight_fit_scores are generally good.
    # Scarcity scores penalize bins, so we subtract them.
    # The `tight_fit_scores` should dominate, so we might need to tune the factor.
    # Let's refine: we want to increase preference for tight fits,
    # and decrease preference for bins that are already scarce (low remaining capacity).
    # So, we'll combine the "goodness" of the fit with a penalty for scarcity.
    # A better approach might be to prioritize bins that have *just enough* space
    # but also have substantial remaining capacity for future items.
    # Let's try a score that rewards tight fits and penalizes low remaining capacity.

    # Calculate preference for tight fit (higher is better)
    fit_preference = 1.0 / (fitting_bins_caps - item + 1e-9)

    # Calculate penalty for scarcity (lower remaining capacity is worse)
    # We want to *discourage* using very full bins, so higher scarcity_penalty is worse.
    # Let's invert it to get a desirability score: bins with more remaining capacity are more desirable.
    scarcity_desirability = fitting_bins_caps

    # Combine them: prioritize tight fits, but also consider overall bin capacity.
    # A good heuristic might be to prioritize bins that are tight fits BUT still have a decent amount of space left.
    # This is a bit contradictory. Let's re-think the reflection.
    # "prioritize tight fits; explore with softmax; consider bin scarcity."
    # Bin scarcity means we don't want to fill up bins too quickly if we have many options.
    # This implies favoring bins that have more space.

    # Let's re-frame: score should be high for bins that are tight fits AND have sufficient capacity remaining.
    # Score = (1 / (remaining_capacity - item)) * (remaining_capacity)  -- this amplifies larger capacities for tight fits.
    # Or, to favor tight fits more strongly, we can make the second term more influential.
    # Let's try a weighted sum or multiplicative approach.

    # Option 1: Weighted sum, favoring tight fit, penalizing scarcity (low remaining capacity)
    # score = weight_fit * fit_preference - weight_scarcity * (1 / (fitting_bins_caps + 1e-9))
    # This means a very low remaining capacity (high scarcity) will result in a more negative score.

    # Option 2: Consider the remaining capacity after packing. We want this to be small (tight fit).
    # And we also want the *original* remaining capacity to be not too small (to avoid scarcity).
    # Let's define a score that is high for bins that are tight AND have ample space.
    # This sounds like prioritizing bins that can fit the item with minimal waste, but also aren't already almost full.

    # Let's use a score that prioritizes tight fits, but also gives a slight boost to bins with more remaining capacity.
    # This can be achieved by adding a term related to the remaining capacity itself.
    # The intuition is: prefer tight fits, but if multiple bins offer tight fits, pick the one that leaves more space.
    # This might be counter-intuitive to "tight fit" as it encourages leaving more space.

    # Let's go back to the core idea: minimize wasted space. Tightest fits do this.
    # Bin scarcity: don't fill up bins too quickly. This means maybe spreading items across bins.
    # If we have many bins with similar tight fits, we might want to pick the one with more original capacity.

    # Let's combine:
    # 1. Tightness of fit: `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` (higher is better)
    # 2. Bin capacity: `bins_remain_cap[can_fit_mask]` (higher is better, to avoid scarcity)
    # We can weight these. A simple product might work: `tight_fit_scores * bins_remain_cap[can_fit_mask]`
    # Or a weighted sum: `w1 * tight_fit_scores + w2 * bins_remain_cap[can_fit_mask]`
    # Let's try a multiplicative approach that rewards tightness and penalizes low capacity.
    # The previous `priority_v1` used `1.0 / (fitting_bins_caps - item + 1e-9)`. Let's modify that.

    # Consider the value of a bin.
    # A bin is good if it fits the item snugly.
    # A bin is also good if it has a lot of remaining capacity for future items.
    # Let's try to combine these. The "value" could be related to the remaining capacity *after* packing.
    # We want this to be small. So, `remaining_capacity - item`.
    # But we also want to avoid scarcity, meaning bins with very low original capacity are less desirable.

    # Let's refine the score:
    # We want bins where `bins_remain_cap - item` is small (tight fit).
    # We also want bins where `bins_remain_cap` is not too small (to avoid scarcity).
    # A score that rewards small `bins_remain_cap - item` and penalizes small `bins_remain_cap` could be:
    # `score = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9) * (bins_remain_cap[can_fit_mask] / BIN_CAPACITY)`
    # If we don't know BIN_CAPACITY, we can use the average remaining capacity, or simply the remaining capacity itself.
    # Let's use the remaining capacity directly as a multiplier for the tight-fit score.
    # This gives higher scores to tight fits in bins that also have more space overall.

    # Refined score: prioritize tight fits, but if multiple bins are tightly fitting,
    # prefer the one that still has more capacity.
    # This implicitly encourages spreading items if fits are equally tight.
    combined_scores = (1.0 / (fitting_bins_caps - item + 1e-9)) * fitting_bins_caps

    # Apply softmax
    if np.sum(combined_scores) > 0:
        max_score = np.max(combined_scores)
        exp_scores = np.exp((combined_scores - max_score) / temperature)
        sum_exp_scores = np.sum(exp_scores)

        if sum_exp_scores > 0:
            priorities[can_fit_mask] = exp_scores / sum_exp_scores
        else:
            # Fallback if exp_scores sum to zero (e.g., extreme values or temperature)
            num_fitting_bins = np.sum(can_fit_mask)
            if num_fitting_bins > 0:
                priorities[can_fit_mask] = 1.0 / num_fitting_bins
    else:
        # If no fitting bins or scores are zero, priorities remain zero.
        pass

    return priorities
```
