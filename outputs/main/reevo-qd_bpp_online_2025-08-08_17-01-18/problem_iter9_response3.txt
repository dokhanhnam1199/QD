[Prior reflection]
The current heuristic `priority_v1` focuses solely on the "best fit" aspect using a sigmoid function. To incorporate "scarcity" and create a unified score that balances both, we need to modify the scoring mechanism.

**Scarcity:** A scarcer bin is one with less remaining capacity overall, but still enough to fit the current item. This means smaller bins are more "scarce" and should be prioritized. The current sigmoid already implicitly favors bins with less remaining capacity *among suitable bins*. However, it doesn't explicitly consider the *absolute* scarcity of bins. We can define scarcity as the inverse of remaining capacity (or some transformation of it).

**Unified Score:** We can combine a "fit score" (related to how well the item fits) and a "scarcity score" using a weighted sum. The "fit score" can still be based on the sigmoid, or a simpler measure like `1 / (1 + mismatch)`. The "scarcity score" could be `1 / (1 + remaining_capacity)` or `1 - (remaining_capacity / max_capacity)` if we have a notion of max capacity.

**Adaptive Exploration/Exploitation:** The weights for combining scores can be learned or dynamically adjusted. For this exercise, we'll use fixed weights but acknowledge that tuning them is crucial. A Softmax temperature parameter can be introduced to control the randomness of selection (exploitation vs. exploration).

**Refined Approach for `priority_v2`:**

1.  **Best Fit Component:** Use a measure that rewards bins where `remaining_capacity - item` is minimized (i.e., the tightest fit). The sigmoid function is good for this, but a simpler inverse of the waste `1 / (1 + (remaining_capacity - item))` might also work and be less prone to extreme values. Let's stick with a sigmoid-like behavior but perhaps a simpler form or a different parameterization. A common way to represent "best fit" is `1 / (1 + waste)`, where `waste = remaining_capacity - item`.
2.  **Scarcity Component:** Prioritize bins with less remaining capacity. A simple scarcity score could be `1 / (1 + remaining_capacity)`. This gives higher scores to bins with smaller remaining capacities.
3.  **Combined Score:** A weighted sum: `score = w_fit * fit_score + w_scarcity * scarcity_score`.
4.  **Normalization/Softmax:** Apply a Softmax function to the combined scores to get probabilities or priorities, and use a temperature parameter to control the exploration/exploitation trade-off.
    `priority_i = exp(score_i / temperature) / sum(exp(score_j / temperature))`

Let's implement `priority_v2` using a combined score and Softmax.

**Detailed Plan for `priority_v2`:**

*   **Input:** `item` (float), `bins_remain_cap` (np.ndarray).
*   **Output:** `priorities` (np.ndarray).
*   **Steps:**
    1.  Identify suitable bins (`bins_remain_cap >= item`).
    2.  For suitable bins:
        *   Calculate "fit score": `fit_score = 1 / (1 + (bins_remain_cap[suitable] - item))`. This rewards bins where `waste` is small.
        *   Calculate "scarcity score": `scarcity_score = 1 / (1 + bins_remain_cap[suitable])`. This rewards bins with less remaining capacity.
        *   Define weights: `w_fit` and `w_scarcity`. Let's start with `w_fit = 0.7`, `w_scarcity = 0.3`.
        *   Calculate combined score: `combined_score = w_fit * fit_score + w_scarcity * scarcity_score`.
        *   Introduce a temperature parameter, `temperature`. Let's start with `temperature = 1.0`.
        *   Apply Softmax: `priorities[suitable] = softmax(combined_score / temperature)`.
    3.  Return the `priorities` array (unsuitable bins get 0 priority).

*Self-correction/Refinement:*
The `softmax` function is typically applied to a vector to get a probability distribution. Here, we're calculating a priority score for each bin, and then the *selection* process (e.g., argmax or sampling) would use these scores. The request asks for a priority score for *each bin*, where the bin with the *highest* score is selected. This implies we don't necessarily need Softmax at the priority calculation stage, but rather the priority scores themselves should reflect the desired preference.

If the goal is to return a score and then pick the max, applying Softmax to the scores might be counter-productive as it normalizes them. Instead, we should directly use the weighted sum. The "adaptive exploration/exploitation" might then refer to how the *selection* is made (e.g., epsilon-greedy based on these scores, or sampling according to their normalized values). Since the request is for a *priority function*, returning the calculated scores before Softmax seems more aligned with "returning a priority score for each bin".

Let's simplify: Combine "best fit" and "scarcity" with weights directly. The "adaptive" part can be seen in the *choice* of weights and the specific functions used.

**Revised Plan for `priority_v2`:**

1.  Identify suitable bins (`bins_remain_cap >= item`).
2.  For suitable bins:
    *   **Fit Component:** `fit_score = 1.0 / (1.0 + (bins_remain_cap[suitable] - item))`. This value ranges from (0, 1]. Perfect fit (waste=0) gives 1.0. Larger waste gives scores closer to 0.
    *   **Scarcity Component:** `scarcity_score = 1.0 / (1.0 + bins_remain_cap[suitable])`. This value ranges from (0, 1]. Minimum remaining capacity (if 0) gives 1.0. Larger capacity gives scores closer to 0.
    *   **Weights:** `w_fit = 0.7`, `w_scarcity = 0.3`. These are tunable parameters that balance the two criteria. Higher `w_fit` emphasizes fitting snugly. Higher `w_scarcity` emphasizes using up smaller bins first.
    *   **Combined Score:** `combined_score = w_fit * fit_score + w_scarcity * scarcity_score`.
3.  Initialize `priorities` to zeros.
4.  Assign `combined_score` to `priorities[suitable_bins_mask]`.
5.  Return `priorities`.

This approach provides a single numerical score for each bin, where higher scores mean higher priority. The "adaptive exploration/exploitation" is embedded in the choice of weights and the functional forms. The prompt mentions "Softmax temperature", which implies a probabilistic selection. If the intention is to return scores that can be used for probabilistic selection, then applying Softmax to the scores is indeed appropriate.

Let's revisit the Softmax idea. If the prompt implies that the output scores should be interpretable as probabilities or weights for sampling, then Softmax is needed.

**Final Plan for `priority_v2` (incorporating Softmax for probabilistic selection interpretation):**

1.  Define `w_fit`, `w_scarcity`, `temperature`.
2.  Identify suitable bins (`bins_remain_cap >= item`).
3.  If no suitable bins, return zeros.
4.  Calculate `fit_scores` for suitable bins: `1.0 / (1.0 + (bins_remain_cap[suitable] - item))`. Handle potential division by zero if `1 + waste` is zero (though waste is always >=0 here).
5.  Calculate `scarcity_scores` for suitable bins: `1.0 / (1.0 + bins_remain_cap[suitable])`. Handle potential division by zero if `1 + capacity` is zero (capacity is always >=0).
6.  Calculate `combined_scores = w_fit * fit_scores + w_scarcity * scarcity_scores`.
7.  Apply Softmax to `combined_scores / temperature` to get normalized priorities.
8.  Place normalized priorities into the result array.

Consider the case where `bins_remain_cap[suitable]` can be very small (e.g., close to `item`). `bins_remain_cap[suitable] - item` will be close to 0, `fit_score` close to 1. `bins_remain_cap[suitable]` will also be small, `scarcity_score` close to 1.
If `bins_remain_cap[suitable]` is very large, `fit_score` will be close to 0, `scarcity_score` close to 0.

Let's refine the "fit score" to be more like the original sigmoid to capture the *degree* of "goodness of fit". A simple `1/(1+waste)` is monotonic, but sigmoid has an inflection point. The original sigmoid `1 / (1 + exp(k * (remaining_capacity - item)))` gives 0.5 for perfect fit, which might be less intuitive than giving a high score for perfect fit.

Alternative "fit score": A function that peaks at `remaining_capacity == item`. E.g., a Gaussian centered at `item` or a Laplace distribution. Or, simply `1.0 / (1.0 + abs(remaining_capacity - item))`. Let's use `1.0 / (1.0 + (remaining_capacity - item))` for simplicity as it's a common representation of "waste".

The prompt mentions "unified score" and "adaptive exploration/exploitation". This strongly suggests a softmax-based approach for selection.

```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority scores for bins, combining "best fit" and "scarcity"
    using a weighted sum, and then normalizing using Softmax for probabilistic
    selection interpretation.

    This heuristic prioritizes bins based on two criteria:
    1. Best Fit: Bins where the remaining capacity is closest to the item size,
       minimizing wasted space.
    2. Scarcity: Bins with less overall remaining capacity are preferred, as they
       are more "scarce".

    The scores are combined using weights, and then a Softmax function is applied
    with a temperature parameter. This allows for balancing exploitation (picking
    the best bin) and exploration (giving a chance to less optimal bins).

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.

    Returns:
        A NumPy array of the same size as `bins_remain_cap`, where each element
        is the normalized priority score for the corresponding bin. Bins that
        cannot fit the item will have a priority of 0.
    """
    
    # Parameters for balancing fit and scarcity, and for Softmax temperature
    w_fit = 0.7       # Weight for the "best fit" component (0.0 to 1.0)
    w_scarcity = 0.3  # Weight for the "scarcity" component (0.0 to 1.0)
    temperature = 1.0 # Softmax temperature: higher -> more exploration, lower -> more exploitation

    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins that can accommodate the item
    suitable_bins_mask = bins_remain_cap >= item
    
    # If no bin can fit the item, return all zeros
    if not np.any(suitable_bins_mask):
        return priorities

    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]

    # --- Calculate "Best Fit" Score ---
    # We want to maximize this score when (remaining_capacity - item) is small.
    # A simple function is 1 / (1 + waste).
    # waste = suitable_bins_cap - item
    # fit_scores = 1.0 / (1.0 + waste)
    # This gives a score of 1.0 for a perfect fit, and approaches 0 for large waste.
    waste = suitable_bins_cap - item
    
    # To avoid potential division by zero if waste were negative (not possible here)
    # or if we wanted to cap the score range, we can add a small epsilon or use np.maximum.
    # For robustness, ensure waste is non-negative (already guaranteed by suitable_bins_mask)
    # and handle potential very large waste that could make 1/(1+waste) extremely small.
    # We can clip the waste to avoid underflow issues with its reciprocal,
    # although with typical capacities, this is less of a concern.
    # Let's use np.maximum to ensure the denominator is at least 1.
    fit_scores = 1.0 / (1.0 + np.maximum(0, waste)) # Ensure waste is non-negative

    # --- Calculate "Scarcity" Score ---
    # We want to maximize this score when remaining_capacity is small.
    # A simple function is 1 / (1 + remaining_capacity).
    # scarcity_scores = 1.0 / (1.0 + suitable_bins_cap)
    # This gives a score of 1.0 for a bin with 0 remaining capacity, and approaches 0 for large capacities.
    scarcity_scores = 1.0 / (1.0 + np.maximum(0, suitable_bins_cap)) # Ensure capacity is non-negative

    # --- Combine Scores ---
    # Weighted sum of fit and scarcity scores
    combined_scores = (w_fit * fit_scores) + (w_scarcity * scarcity_scores)

    # --- Apply Softmax for Probabilistic Priorities ---
    # Divide by temperature to control the sharpness of the distribution.
    # Higher temperature -> probabilities are more uniform (more exploration).
    # Lower temperature -> probabilities are more skewed towards high scores (more exploitation).
    if temperature <= 0:
        # Handle non-positive temperature by making it very small positive
        # or by returning argmax-like behavior (deterministic best choice).
        # For now, let's treat temperature <= 0 as effectively zero temperature
        # which would make the softmax behave like an argmax if not handled.
        # A robust softmax implementation typically expects positive temperature.
        # If temperature is 0, exp(score/0) is inf for max score, 0 for others.
        # This would lead to a single bin having probability 1.
        # For this function, returning raw scores before softmax for T=0 makes sense.
        # Or, if we must use softmax, a very small positive number.
        effective_temperature = 1e-9 # Small positive to avoid division by zero
    else:
        effective_temperature = temperature

    # Calculate exponent terms: scores / temperature
    exponent_terms = combined_scores / effective_temperature
    
    # Calculate Softmax probabilities
    # To avoid overflow with large exponent_terms, subtract the maximum value.
    # max_exponent = np.max(exponent_terms)
    # exp_values = np.exp(exponent_terms - max_exponent)
    # probabilities = exp_values / np.sum(exp_values)
    
    # Using scipy.special.softmax is more numerically stable
    try:
        from scipy.special import softmax
        # Softmax requires a 1D array
        probabilities = softmax(exponent_terms)
    except ImportError:
        # Fallback if scipy is not available
        # Stable softmax implementation
        max_val = np.max(exponent_terms)
        exps = np.exp(exponent_terms - max_val)
        probabilities = exps / np.sum(exps)
        
    # Assign the calculated probabilities to the priorities array
    priorities[suitable_bins_mask] = probabilities

    return priorities
```
