```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority scores for packing an item into bins using a refined
    priority function incorporating "Best Fit" preference, "Bin Scarcity",
    and Softmax for probabilistic exploration.

    This heuristic aims to balance:
    1.  **Best Fit Preference:** Prioritize bins where the remaining capacity is
        closest to the item size, minimizing waste. This is achieved by
        assigning a higher score to bins with smaller `remaining_capacity - item`.
    2.  **Bin Scarcity:** Favor bins that have less remaining capacity overall,
        as these are considered "scarcer" resources. This is achieved by
        assigning higher scores to bins with smaller `remaining_capacity`.
    3.  **Softmax for Probabilistic Selection:** Uses Softmax to convert combined
        heuristic scores into probabilities, allowing for exploration of less
        optimal bins. The temperature parameter controls the degree of exploration.

    The combined score for a suitable bin is a weighted sum of the
    "anti-waste" score and the bin scarcity score.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.

    Returns:
        A NumPy array of the same size as `bins_remain_cap`, where each element
        is the probability score (from Softmax) for the corresponding bin.
        Bins that cannot fit the item will have a priority of 0.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins that can accommodate the item
    suitable_bins_mask = bins_remain_cap >= item
    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]
    suitable_bin_indices = np.where(suitable_bins_mask)[0]

    if suitable_bins_cap.size == 0:
        return priorities

    # --- Heuristic Component 1: "Anti-Waste" Score ---
    # Maximize `1 / (1 + waste)` where `waste = bins_remain_cap - item`.
    # This maps waste in [0, inf) to scores in (0, 1].
    # A small constant `epsilon_waste` can be added to the denominator to
    # prevent division by zero if waste is 0, although the `1+` already handles this.
    # We use a steepness parameter `k_waste` to control sensitivity.
    k_waste = 5.0  # Controls how quickly the score drops with increasing waste
    waste = suitable_bins_cap - item
    # Using exp decay: exp(-k * waste). Higher score for lower waste.
    # We use a base score of 1 for zero waste and decays exponentially.
    anti_waste_scores = np.exp(-k_waste * waste)

    # --- Heuristic Component 2: Bin Scarcity Score ---
    # Prioritize bins with less remaining capacity.
    # Use `1 / (capacity + epsilon)` to get higher scores for smaller capacities.
    epsilon_scarcity = 1e-6
    scarcity_scores = 1.0 / (suitable_bins_cap + epsilon_scarcity)

    # Normalize scarcity scores to a [0, 1] range for consistent weighting.
    min_scarcity = np.min(scarcity_scores)
    max_scarcity = np.max(scarcity_scores)

    if max_scarcity - min_scarcity > epsilon_scarcity:
        normalized_scarcity_scores = (scarcity_scores - min_scarcity) / (max_scarcity - min_scarcity)
    else:
        # If all suitable bins have the same scarcity, this component doesn't differentiate.
        # Assigning 0 or a neutral value. 0 emphasizes that scarcity isn't a deciding factor.
        normalized_scarcity_scores = np.zeros_like(scarcity_scores)

    # --- Combine Heuristics ---
    # Weights for combining the scores. These are hyperparameters and can be tuned.
    weight_anti_waste = 0.6
    weight_scarcity = 0.4

    combined_scores = (weight_anti_waste * anti_waste_scores) + (weight_scarcity * normalized_scarcity_scores)

    # --- Softmax for Probabilistic Exploration ---
    # Temperature parameter: Controls exploration vs. exploitation.
    # Lower temperature leads to more greedy choices (closer to argmax).
    # Higher temperature leads to more uniform probabilities (more exploration).
    temperature = 0.5  # Tunable parameter

    # Apply Softmax with numerical stability
    # Divide scores by temperature before exponentiation
    scores_for_softmax = combined_scores / temperature

    # Subtract the maximum score to prevent overflow in np.exp
    # This shifts the distribution but preserves the relative differences
    stable_scores = scores_for_softmax - np.max(scores_for_softmax)

    exp_scores = np.exp(stable_scores)
    sum_exp_scores = np.sum(exp_scores)

    # Calculate probabilities
    if sum_exp_scores > 0:
        probabilities = exp_scores / sum_exp_scores
    else:
        # Fallback for cases where all exp_scores are zero (e.g., extreme negative inputs)
        # This is highly unlikely with the stable_scores preprocessing, but good practice.
        probabilities = np.ones_like(combined_scores) / combined_scores.size

    # Assign the calculated probabilities to the corresponding bins
    priorities[suitable_bins_mask] = probabilities

    return priorities
```
