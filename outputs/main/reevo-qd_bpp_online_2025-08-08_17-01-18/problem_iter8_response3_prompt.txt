{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a blended strategy.\n\n    This strategy prioritizes exact fits, uses a decay function for near-exact fits,\n    and applies a Softmax-like normalization for smooth probability distribution.\n    It aims to favor bins where the remaining capacity is closest to the item size,\n    with a strong preference for exact matches.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if not eligible_bins_cap.size:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit residual\": how much space is left after placing the item.\n    # Smaller values indicate a tighter fit.\n    fit_residuals = eligible_bins_cap - item\n\n    # Prioritize exact fits (residual = 0). For near-exact fits, use an exponential\n    # decay based on the residual. Bins with smaller residuals get higher scores.\n    # A small epsilon is added to the residual to ensure that exact fits (residual=0)\n    # get a distinct, higher score than bins that leave a tiny positive residual.\n    # The decay_factor controls how quickly the priority drops as the residual increases.\n    decay_factor = 0.5\n    # We want smaller residuals to have higher scores.\n    # An exponential decay is suitable: exp(-decay_factor * residual)\n    # For residual = 0 (exact fit), score is exp(0) = 1.0.\n    # For residual > 0, score decreases.\n    # Add a small constant to the exponent to ensure that even exact fits have a score\n    # that can be part of a meaningful softmax, avoiding potential issues if all residuals are 0.\n    # Alternatively, we can explicitly set exact fits to a high base value.\n    \n    # Strategy:\n    # 1. Exact fits get a high score (e.g., 1.0).\n    # 2. Near-exact fits get a score based on exponential decay of the residual.\n    # 3. Use softmax to normalize these scores into probabilities.\n\n    # Base scores: 1.0 for exact fits, and an exponentially decaying score for others.\n    # The decay_rate ensures that bins with residuals closer to 0 are preferred.\n    # We use `fit_residuals + 1e-6` to ensure that even for exact fits (residual=0),\n    # we have a non-zero value to pass to exp, and to differentiate exact from very close fits slightly.\n    # However, a cleaner approach is to handle exact fits explicitly.\n    \n    scores = np.where(fit_residuals == 0,\n                      1.0,  # High priority for exact fits\n                      np.exp(-decay_factor * fit_residuals)) # Decreasing priority for near-fits\n\n    # Apply Softmax-like normalization to convert scores into probabilities.\n    # This ensures that the priorities sum to 1 across the eligible bins and\n    # that preferences are smoothly distributed.\n    # The temperature parameter controls the \"sharpness\" of the distribution.\n    # A lower temperature makes the probabilities sharper (more emphasis on best fits).\n    temperature = 0.2\n    \n    try:\n        # Ensure scores are not excessively large before exponentiation\n        # Clipping can help prevent overflow, but Softmax should handle it better with exp\n        # Adding a small constant to the scores before softmax can also help stabilize.\n        # A common practice is to subtract the maximum score before exponentiating to avoid overflow.\n        max_score = np.max(scores)\n        normalized_scores = (scores - max_score) / temperature\n        exp_scores = np.exp(normalized_scores)\n        softmax_priorities = exp_scores / np.sum(exp_scores)\n    except OverflowError:\n        # In case of extreme values leading to overflow, fall back to a uniform distribution\n        # or a simpler heuristic if softmax fails. For now, uniform is a safe fallback.\n        softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size\n    except ZeroDivisionError:\n        # If sum of exp_scores is zero (highly unlikely with positive scores), fallback\n        softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size\n\n    # Map the calculated priorities back to the original bin indices\n    priorities[can_fit_mask] = softmax_priorities\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority scores for packing an item into bins using a simplified\n    and adaptive heuristic.\n\n    This heuristic prioritizes bins that have just enough remaining capacity\n    for the item, aiming for tight fits. It uses a simple linear scoring\n    mechanism for bins that can fit the item, preferring those with less\n    remaining capacity after packing. A small random component is added\n    to encourage exploration, with the probability of exploration decreasing\n    as more items are packed (implicitly, as the number of bins grows or\n    bins become more full).\n\n    The scoring is designed to be interpretable and efficient.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of priority scores, same size as `bins_remain_cap`.\n        Bins that cannot fit the item will have a score of 0.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate scores for bins that can fit the item\n    if np.any(can_fit_mask):\n        suitable_bins_cap = bins_remain_cap[can_fit_mask]\n\n        # Score: Prioritize bins that leave minimal remaining capacity.\n        # A simple inverse relationship with (remaining_capacity - item) works well.\n        # Adding 1 to the denominator prevents division by zero for perfect fits\n        # and dampens extremely high scores for perfect fits.\n        scores = 1.0 / (suitable_bins_cap - item + 1.0)\n\n        # Normalize scores to a [0, 1] range to make them comparable and stable.\n        min_score = np.min(scores)\n        max_score = np.max(scores)\n\n        if max_score > min_score:\n            normalized_scores = (scores - min_score) / (max_score - min_score)\n        else:\n            # If all suitable bins yield the same score (e.g., all have same remaining capacity after fit),\n            # assign a uniform high score (e.g., 1.0) to all of them.\n            normalized_scores = np.ones(scores.shape)\n\n        priorities[can_fit_mask] = normalized_scores\n\n    # Simple exploration: With a small, fixed probability, pick a random bin\n    # among the ones that can fit. This is simpler than Softmax or complex\n    # adaptive epsilon, balancing exploration and exploitation.\n    exploration_prob = 0.05 # Fixed small probability for exploration\n    if np.random.rand() < exploration_prob:\n        possible_bins_indices = np.where(can_fit_mask)[0]\n        if possible_bins_indices.size > 0:\n            # Select a random bin among those that can fit.\n            random_bin_index = np.random.choice(possible_bins_indices)\n            \n            # Assign the highest priority to the randomly chosen bin.\n            # This effectively makes the random choice override the calculated priority.\n            priorities = np.zeros(num_bins, dtype=float) # Reset all priorities\n            priorities[random_bin_index] = 1.0         # Give highest priority to random bin\n\n    return priorities\n\n[Reflection]\nSimplicity, clear objectives, and occasional exploration often lead to better heuristics.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}