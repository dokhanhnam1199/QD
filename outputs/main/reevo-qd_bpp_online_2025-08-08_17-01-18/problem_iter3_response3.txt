```python
import numpy as np
from scipy.special import softmax

def priority_v2(item: float, bins_remain_cap: np.ndarray, temperature: float = 1.0) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a softmax-based heuristic.

    This heuristic prioritizes bins that can accommodate the item and are "tight fits"
    to minimize wasted space. It uses softmax to assign probabilities based on how well
    an item fits into a bin. Bins that are too small for the item receive a priority of 0.
    A higher temperature leads to more uniform probabilities (more exploration),
    while a lower temperature leads to more focused probabilities on the best fits.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.
        temperature: Controls the randomness of the selection. Higher values mean more exploration.

    Return:
        Array of same size as bins_remain_cap with priority score (probability) of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins that can fit the item
    can_fit_mask = bins_remain_cap >= item

    # Calculate scores for bins that can fit.
    # We want to prioritize bins where the remaining capacity is close to the item size.
    # A good measure for "closeness" is the difference (bins_remain_cap - item).
    # Smaller difference means a tighter fit. To use softmax effectively (which
    # favors larger values), we can use the negative of this difference, or
    # equivalently, invert the difference (1 / (difference + epsilon)).
    # Here, we'll use the negative difference, scaled by temperature.
    # We also add a small epsilon to avoid issues with perfect fits causing very large negative numbers.
    if np.any(can_fit_mask):
        scores = -(bins_remain_cap[can_fit_mask] - item + 1e-9)

        # Apply softmax to convert scores into probabilities.
        # Softmax output is always non-negative and sums to 1.
        # A higher temperature will smooth the probabilities.
        probabilities = softmax(scores / temperature)

        # Assign probabilities to the corresponding bins
        priorities[can_fit_mask] = probabilities

    # Tie-breaking: Favor earlier bins if probabilities are identical (due to softmax or exact fits).
    # This can be implicitly handled by the order of `bins_remain_cap` if `softmax`
    # implementation has stable sorting, or explicitly if needed. For most softmax
    # implementations, the original index order is preserved for equal scores.
    # If explicit tie-breaking is needed, one could add a small negative value
    # to scores of later bins, proportional to their index, before softmax.
    # For example: `scores_with_tiebreak = scores - np.arange(len(scores)) * 1e-3`

    return priorities
```
