{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority probabilities for packing an item into bins using a\n    combination of tight fit, bin scarcity, tie-breaking, and Softmax for exploration.\n\n    This heuristic prioritizes:\n    1.  **Tight Fits:** Bins that have just enough remaining capacity for the item.\n        This is handled by a sigmoid function on the \"mismatch\" (remaining_capacity - item).\n    2.  **Bin Scarcity:** Slightly favors bins that are less empty (have less remaining capacity),\n        as these are scarcer resources. A bonus is added inversely proportional to remaining capacity.\n    3.  **Earlier Bin Preference:** If multiple bins have similar scores, favors bins that\n        appear earlier in the `bins_remain_cap` array (i.e., were opened earlier).\n    4.  **Probabilistic Exploration (Softmax):** Converts the computed priority scores\n        into probabilities using the Softmax function. The `temperature` parameter\n        controls the exploration-exploitation trade-off:\n        - Low temperature (close to 0): Exploitation (favors the highest score).\n        - High temperature (large value): Exploration (probabilities are more uniform).\n\n    The final priority for each bin is calculated as:\n    `Score_i = SigmoidFit(bins_remain_cap[i], item, k) + gamma * (1 / (bins_remain_cap[i] + epsilon)) + delta * (1 / (i + 1))`\n    Then, probabilities are derived using Softmax:\n    `P_i = exp(Score_i / temperature) / sum(exp(Score_j / temperature))`\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n        temperature: Controls the Softmax exploration/exploitation balance.\n                     Higher values lead to more exploration. Defaults to 1.0.\n        k: Sensitivity parameter for the sigmoid function (tightest fit preference).\n           Higher `k` increases preference for tighter fits. Defaults to 5.0.\n        gamma: Weight for the bin scarcity bonus. Higher `gamma` increases\n               preference for less empty bins. Defaults to 0.1.\n        delta: Weight for the earlier bin preference tie-breaker. Higher `delta`\n               increases preference for earlier bins. Defaults to 0.01.\n        epsilon: Small value to prevent division by zero in scarcity calculation.\n                 Defaults to 1e-6.\n\n    Returns:\n        A NumPy array of probabilities, same size as `bins_remain_cap`.\n        Bins that cannot fit the item will have a probability of 0.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    raw_scores = np.zeros(num_bins, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    # Calculate base sigmoid fit score for suitable bins\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    if suitable_bins_cap.size > 0:\n        # Calculate the \"mismatch\" or wasted space\n        mismatch = suitable_bins_cap - item\n        \n        # Cap exponent argument to prevent overflow in np.exp\n        max_exponent_arg = 35.0\n        capped_exponent_arg = np.minimum(k * mismatch, max_exponent_arg)\n        \n        # Sigmoid score: Higher for tighter fits (smaller mismatch)\n        sigmoid_scores = 1 / (1 + np.exp(capped_exponent_arg))\n\n        # Apply scarcity bonus: Add bonus for bins with less remaining capacity\n        # Using 1 / (capacity + epsilon) as a proxy for \"fullness\"\n        scarcity_bonus = gamma * (1.0 / (suitable_bins_cap + epsilon))\n\n        # Apply tie-breaking bonus: Add bonus for earlier bins\n        # Find the indices of the suitable bins in the original array\n        suitable_bin_indices = np.where(suitable_bins_mask)[0]\n        tie_breaker_bonus = delta * (1.0 / (suitable_bin_indices + 1.0))\n\n        # Combine scores for suitable bins\n        combined_scores = sigmoid_scores + scarcity_bonus + tie_breaker_bonus\n        \n        # Assign combined scores back to the raw_scores array\n        raw_scores[suitable_bins_mask] = combined_scores\n\n    # If temperature is very low (close to 0), effectively select the max score bin.\n    # Avoid division by zero if temperature is 0.\n    if temperature <= epsilon:\n        if np.max(raw_scores) > -np.inf: # Check if there's at least one valid score\n             # Assign probability 1 to the bin(s) with the maximum score\n             max_score = np.max(raw_scores)\n             probabilities = np.where(raw_scores == max_score, 1.0, 0.0)\n             # Normalize to ensure sum is 1 if multiple max scores exist\n             num_max_scores = np.sum(probabilities)\n             if num_max_scores > 0:\n                 probabilities /= num_max_scores\n        else: # All scores are -inf (e.g., no suitable bins)\n             probabilities = np.zeros(num_bins)\n        return probabilities\n\n    # Apply Softmax to convert scores to probabilities\n    # Add a small constant to scores before exp to avoid issues with very small negative scores\n    # Or simply handle potential underflow/overflow.\n    # A common trick is to subtract the max score before exponentiation:\n    # exp(x_i / T) / sum(exp(x_j / T)) = exp((x_i - max(x)) / T) / sum(exp((x_j - max(x)) / T))\n    # This stabilizes calculations.\n    \n    # Find the maximum score to shift all scores down for numerical stability\n    max_raw_score = np.max(raw_scores)\n    \n    # Ensure we don't get NaN or inf if all scores are -inf (e.g., no suitable bins)\n    if max_raw_score == -np.inf:\n        return np.zeros(num_bins)\n\n    shifted_scores = (raw_scores - max_raw_score) / temperature\n    \n    # Calculate exponential of shifted scores\n    exp_scores = np.exp(shifted_scores)\n    \n    # Calculate sum of exponential scores for normalization\n    sum_exp_scores = np.sum(exp_scores)\n    \n    # Calculate probabilities\n    probabilities = exp_scores / sum_exp_scores\n    \n    # Ensure probabilities sum to 1 (due to potential floating point inaccuracies)\n    # And handle cases where sum_exp_scores might be 0 (e.g., all shifted scores were -inf)\n    if sum_exp_scores > 0:\n        probabilities /= np.sum(probabilities) # Re-normalize\n    else:\n        # This case should ideally not happen if max_raw_score was handled correctly,\n        # but as a fallback, if all exp_scores resulted in 0, distribute uniformly or zero out.\n        # Given our max_raw_score shift, this implies all shifted scores were extremely negative.\n        probabilities = np.zeros(num_bins)\n\n    # Ensure probabilities are not NaN\n    probabilities = np.nan_to_num(probabilities)\n\n    return probabilities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a Softmax-based approach\n    that prioritizes tight fits, considers bin scarcity, and encourages earlier bin usage.\n\n    This heuristic aims to balance several factors:\n    1.  **Tight Fit Preference:** Prioritize bins that leave minimal remaining capacity\n        after packing the item. This is achieved by assigning higher scores to bins\n        where `remaining_capacity - item` is small and non-negative.\n    2.  **Bin Scarcity/Fullness:** Bins that are already quite full (i.e., have less\n        remaining capacity) should generally be preferred over those with ample space,\n        as this helps in utilizing bins more efficiently and potentially opening up\n        space in other bins for larger items later.\n    3.  **Exploration (Softmax):** Using Softmax allows for probabilistic selection,\n        meaning even bins that are not the absolute \"best fit\" have a chance of being chosen.\n        This can prevent getting stuck in local optima and discover better packings.\n    4.  **Tie-breaking:** Implicitly, bins that are encountered earlier in the `bins_remain_cap`\n        array might receive slightly higher priority if their scores are identical to later bins,\n        due to how the scores are processed or due to inherent ordering.\n\n    The scoring for suitable bins is designed as follows:\n    For bins where `remaining_capacity >= item`:\n    - We calculate a \"fit score\" based on how tightly the item fits. A simple inverse\n      relationship with the remaining capacity is used, favoring smaller capacities.\n    - A \"scarcity score\" related to the inverse of remaining capacity can be incorporated.\n    - A common approach for balancing is to use a weighted sum or a transformation\n      that combines these aspects.\n\n    Here, we'll adapt a common approach for online bin packing heuristics that focuses on\n    the \"best fit\" principle, but modulated by Softmax for exploration.\n    A common score for \"best fit\" is the difference `remaining_capacity - item`.\n    We want to *minimize* this difference. For Softmax, we need to transform this into\n    scores where higher means more preferred.\n\n    We'll use a score inversely related to the remaining capacity for suitable bins.\n    A simple heuristic could be `1 / remaining_capacity`.\n    To incorporate the \"tight fit\" preference more directly, we can consider the\n    inverse of `(remaining_capacity - item) + epsilon` to avoid division by zero,\n    or use a sigmoid-like function on this difference.\n\n    Let's refine the reflection's idea:\n    - **Tight Fit Score:** `1 / (1 + exp(k * (remaining_capacity - item)))` as in v1. This assigns\n      higher scores to smaller positive differences.\n    - **Exploration (Softmax):** Apply Softmax to these scores to get probabilities.\n    - **Bin Scarcity/Earlier Bin Preference:** The original reflection mentioned \"earlier bin preference\".\n      This can be achieved by adding a small constant to the score of earlier bins, or by\n      ranking bins and adding a bonus based on rank. For simplicity and focusing on the\n      fit/scarcity, we'll rely on Softmax's inherent exploration. The \"scarcity\" is implicitly\n      handled by favoring bins with less `remaining_capacity`.\n\n    Let's re-evaluate the scoring to directly favor smaller remaining capacities for Softmax.\n    A score like `max_capacity - remaining_capacity` would favor fuller bins.\n    However, we also need the item to fit.\n\n    Consider the following score for suitable bins:\n    `score = some_function(remaining_capacity - item)`\n    We want `remaining_capacity - item` to be small.\n    Let's try `score = - (remaining_capacity - item)` which directly rewards smaller differences.\n    To make it suitable for Softmax (where we want positive scores), we can use:\n    `score = C - (remaining_capacity - item)` where C is a large constant.\n    Or, perhaps more intuitively, we want to *minimize* `remaining_capacity`.\n    So, a score proportional to `-remaining_capacity` or `max_capacity - remaining_capacity`\n    could work.\n\n    Let's combine:\n    1. Prioritize bins where `remaining_capacity` is small (scarcity/fullness).\n    2. Among those, prioritize tighter fits (`remaining_capacity - item` is small).\n\n    A score that captures this is to prioritize bins with smaller `remaining_capacity`.\n    So, `score = -remaining_capacity`. For Softmax, we'd use `np.exp(-remaining_capacity)`.\n    However, this doesn't directly incorporate the item size into the fit.\n\n    Let's use the \"tight fit\" score from v1, but apply Softmax to it, potentially\n    adding a small boost for bins that are generally less full (though the prompt\n    suggests prioritizing tighter fits and scarcity, which might conflict).\n\n    Let's focus on the reflection's core idea: tight fits and minimal waste,\n    and Softmax for exploration.\n    A score that reflects tight fit is `1 / (1 + exp(k * (remaining_capacity - item)))`.\n    To make this suitable for Softmax (where larger is better), we can directly use this value.\n    Bins that cannot fit the item get a score of 0.\n\n    We'll use a parameter `temperature` for the Softmax function to control exploration.\n    A higher temperature leads to more uniform probabilities (more exploration).\n    A lower temperature leads to more concentrated probabilities on the highest scores (more exploitation).\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is the probability score for the corresponding bin, derived from Softmax.\n        Bins that cannot fit the item will have a score of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n    # If no bin can fit the item, return all zeros\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # --- Scoring Mechanism ---\n    # We want to assign higher scores to bins that offer a tighter fit.\n    # The difference `remaining_capacity - item` represents the wasted space.\n    # We want to minimize this difference.\n    # A score that rewards smaller differences is `1 / (1 + exp(k * (remaining_capacity - item)))`.\n    # Let's define k. A higher k emphasizes tighter fits.\n    k = 5.0  # Sensitivity parameter for the tight fit preference\n\n    # Calculate the \"mismatch\" or wasted space for suitable bins\n    mismatch = suitable_bins_cap - item\n\n    # Calculate the raw \"preference\" score. Higher means more preferred.\n    # Using the sigmoid score: 1 / (1 + exp(k * mismatch))\n    # This score is between 0 and 1. A perfect fit (mismatch=0) gives 0.5.\n    # Smaller positive mismatches give scores closer to 0.5.\n    # Larger positive mismatches give scores closer to 0.\n    # We want to favor smaller mismatches, so larger scores are better.\n    # The current sigmoid score `1/(1+exp(k*mismatch))` correctly assigns higher values to smaller `mismatch`.\n\n    # To prevent numerical issues with exp, cap the argument.\n    max_exponent_arg = 35.0 # Corresponds to exp(35)\n    \n    capped_exponent_arg = np.minimum(k * mismatch, max_exponent_arg)\n    \n    # The raw scores: higher values mean better fits (smaller mismatch)\n    raw_scores = 1 / (1 + np.exp(capped_exponent_arg))\n\n    # --- Softmax Application ---\n    # Apply Softmax to the raw scores to get probabilities.\n    # Softmax(z_i) = exp(z_i) / sum(exp(z_j))\n    # To control exploration, we can scale the scores by a temperature parameter.\n    # `temperature` > 0.\n    # If temperature is very low (e.g., close to 0), it's like argmax (exploitation).\n    # If temperature is high, it's like uniform distribution (exploration).\n    \n    # Let's use a tunable temperature. A value like 0.1-1.0 is common.\n    # If scores are already in [0,1], scaling might not be strictly necessary,\n    # but Softmax generally expects values that can be exponentiated.\n    # We can use the raw_scores directly or scale them.\n    # Scaling by temperature: `exp(score / temperature)`.\n    # Let's try a temperature that makes the differences more pronounced or smoothed.\n    # A temperature of 1.0 means we use the raw scores directly in exp.\n    # A temperature > 1 will smooth probabilities.\n    # A temperature < 1 will sharpen probabilities.\n\n    # Let's use temperature to control the \"sharpness\" of preference.\n    # A lower temperature will favor the best fits more strongly.\n    # A higher temperature will distribute preference more evenly.\n    # Let's start with a moderate temperature, e.g., 0.5, to slightly favor better fits.\n    temperature = 0.5 # Tunable parameter for exploration/exploitation balance\n\n    # Apply exponentiation with temperature scaling\n    # Ensure we don't have issues if temperature is very close to zero or zero.\n    # If temperature is very small, `raw_scores / temperature` can become very large.\n    # We can clip the scaled scores before exp to prevent overflow if temperature is tiny.\n    # However, if temperature is 0, this is problematic. Assume temperature > 0.\n\n    # Avoid division by zero if temperature is 0 or very small and scores are high.\n    # If temperature is very small, scores with slight differences will be amplified.\n    # Let's scale the scores by `1/temperature` before `exp` for a sharper distribution.\n    # Or scale by `temperature` for a smoother distribution. The reflection says \"tuning temperature for exploration/exploitation balance\".\n    # Higher temperature -> more exploration (smoother distribution).\n    # Lower temperature -> more exploitation (sharper distribution).\n\n    # We want to explore, so let's make temperature a factor that increases probability spread.\n    # Use `exp(score / temperature)` where higher temperature spreads probabilities.\n    # So, let's use `temperature = 0.5` (lower means sharper), `temperature = 2.0` (higher means flatter).\n    # Let's set temperature to 1.0 initially for no scaling, and test.\n    # If we want to favor tighter fits more, we want smaller `mismatch` to have higher probability.\n    # `raw_scores` are already designed for this. Softmax will spread these.\n    # Higher temperature -> more uniform probability distribution.\n    # Lower temperature -> probability concentrated on the highest `raw_scores`.\n\n    # Let's use a temperature that favors exploitation slightly, i.e., a lower temperature.\n    # A temperature around 0.1-0.5 might be good for demonstrating preference.\n    # Or, a temperature of 1.0 is standard softmax. Let's try to emphasize the preference.\n    # If `raw_scores` are e.g., [0.6, 0.5, 0.4], exp([0.6, 0.5, 0.4]) = [1.82, 1.65, 1.49]\n    # Sum = 4.96. Probs = [0.36, 0.33, 0.30]. Not very sharp.\n    # If we scale by 1/temp: temp=0.1 => exp([6, 5, 4]) = [403, 148, 54]. Sum = 605. Probs = [0.66, 0.24, 0.08]. Much sharper.\n\n    # Let's use temperature `T` in `exp(score / T)`.\n    # Smaller `T` leads to sharper probabilities.\n    temperature_param = 0.5 # Lower T = more exploitation of \"best fit\"\n\n    # Ensure scores are not excessively large before exp to prevent overflow.\n    # `raw_scores` are already capped at 1.\n    # If `temperature_param` is very small, `raw_scores / temperature_param` can still be large.\n    # Let's ensure `raw_scores / temperature_param` doesn't exceed a threshold before `exp`.\n    # For example, threshold of 35 for the exponent argument.\n    scaled_scores = raw_scores / temperature_param\n    \n    # Cap the scaled scores before exponentiation to prevent overflow\n    max_scaled_score_exponent = 35.0 \n    capped_scaled_scores = np.minimum(scaled_scores, max_scaled_score_exponent)\n    \n    # Calculate the exponentiated values\n    exponentials = np.exp(capped_scaled_scores)\n\n    # Calculate the sum of exponentiated values for normalization\n    sum_exponentials = np.sum(exponentials)\n\n    # Avoid division by zero if all exponentiated values are zero (e.g., due to capping or very low scores)\n    if sum_exponentials == 0:\n        # In this case, all suitable bins are equally (un)preferred or there was an issue.\n        # A fallback could be uniform probability among suitable bins, or all zeros if it implies no good fit.\n        # Given our scores are >= 0, sum_exponentials should only be 0 if exponentials are all 0.\n        # This might happen if all raw_scores were extremely small and negative after some transformation,\n        # or if capping resulted in zero exp. But our raw_scores are positive.\n        # If sum is 0, it means all `capped_scaled_scores` were -inf (which shouldn't happen here).\n        # As a fallback, assign equal probability to suitable bins if sum is zero.\n        if suitable_bins_cap.size > 0:\n            priorities[suitable_bins_mask] = 1.0 / suitable_bins_cap.size\n        return priorities\n\n    # Calculate the final probabilities using Softmax\n    softmax_probabilities = exponentials / sum_exponentials\n\n    # Place the calculated softmax probabilities back into the main priorities array\n    priorities[suitable_bins_mask] = softmax_probabilities\n\n    return priorities\n\n[Reflection]\nPrioritize tight fits, then scarcity. Use Softmax for controlled exploration/exploitation.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}