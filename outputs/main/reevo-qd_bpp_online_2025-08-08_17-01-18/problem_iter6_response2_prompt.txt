{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-based approach\n    that also incorporates an epsilon-greedy exploration strategy.\n\n    This heuristic first calculates a \"fit score\" for each bin. A better fit is defined as a bin\n    where the remaining capacity is just enough for the item (i.e., `bins_remain_cap - item` is small and positive).\n    The score is designed such that a smaller positive difference yields a higher score.\n    These scores are then transformed using softmax to generate probabilities.\n    An epsilon-greedy component is integrated: with a small probability `epsilon`, a bin is chosen randomly\n    from the *suitable* bins to encourage exploration. Otherwise, the greedy (softmax-derived)\n    priorities are used.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploring a random suitable bin\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    n_bins = len(bins_remain_cap)\n    priorities = np.zeros(n_bins)\n\n    if len(suitable_bin_indices) == 0:\n        # No bin can fit the item, return all zeros\n        return priorities\n\n    # Epsilon-greedy choice: With probability epsilon, pick a random suitable bin\n    if np.random.rand() < epsilon:\n        # Choose one suitable bin randomly\n        chosen_index = np.random.choice(suitable_bin_indices)\n        # Assign a high priority to this randomly chosen bin\n        priorities[chosen_index] = 1.0\n        # For other suitable bins, assign a very low priority to make the chosen one stand out\n        priorities[suitable_bin_indices] = 1e-9\n        return priorities\n\n    # If not exploring, use a greedy strategy based on how well the item fits.\n    # We want bins where the remaining capacity is just enough for the item.\n    # The score should be higher for bins with `bins_remain_cap - item` closer to 0 (but positive).\n    # A good scoring function would be related to `1 / (bins_remain_cap - item + small_constant)`\n    # or `-(bins_remain_cap - item)` which is `item - bins_remain_cap`.\n    # Let's use `item - bins_remain_cap` to prioritize smaller positive differences.\n    scores = item - bins_remain_cap\n\n    # For bins that cannot fit the item, assign a very low score (effectively -infinity for softmax)\n    # This ensures their softmax probability is negligible.\n    scores[~suitable_bins_mask] = -np.inf\n\n    # Calculate softmax probabilities\n    # To prevent potential overflow with large positive scores, we shift scores\n    # so that the maximum score becomes 0.\n    # We only consider scores for suitable bins for the maximum calculation.\n    max_score_for_suitable = np.max(scores[suitable_bins_mask]) if np.any(suitable_bins_mask) else 0\n    \n    shifted_scores = scores - max_score_for_suitable\n    \n    # Calculate exponentiated scores. Bins with -inf will result in 0.\n    exp_scores = np.exp(shifted_scores)\n    \n    # Calculate the sum of exponentiated scores. Ensure it's not zero.\n    sum_exp_scores = np.sum(exp_scores)\n    \n    if sum_exp_scores == 0:\n        # This can happen if all suitable bins had scores that resulted in exp_scores close to zero\n        # or if somehow all suitable bins were removed or became invalid.\n        # As a fallback, assign uniform probability to all suitable bins.\n        if np.any(suitable_bins_mask):\n            priorities[suitable_bins_mask] = 1.0 / len(suitable_bin_indices)\n        return priorities\n    \n    priorities = exp_scores / sum_exp_scores\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority scores for packing an item into bins using a refined\n    priority function incorporating Softmax for exploration, bin scarcity,\n    and tie-breaking.\n\n    This heuristic aims to balance tight fits with exploration and efficiency\n    by considering:\n    1.  **Tight Fit Score (Sigmoid):** Prioritizes bins with remaining capacity\n        closest to the item size, minimizing waste.\n    2.  **Bin Scarcity:** Favors bins that have less remaining capacity overall,\n        as these are \"scarcer\" resources. This is modeled using the inverse\n        of remaining capacity.\n    3.  **Softmax for Probabilistic Selection:** Uses Softmax to convert scores\n        into probabilities, allowing for exploration of less optimal bins.\n        The temperature parameter controls the degree of exploration.\n    4.  **Tie-breaking:** Implicitly handled by the sorting order or original\n        index if scores are identical, favoring bins that appear earlier in\n        the array when scores are equal.\n\n    The combined priority for a suitable bin is a weighted sum of the tight\n    fit score and the bin scarcity score, transformed by Softmax.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is the probability score (from Softmax) for the corresponding bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # --- Heuristic Component 1: Tight Fit Score (Sigmoid) ---\n    # Parameter for the sigmoid function's steepness.\n    # Higher k means sharper preference for tighter fits.\n    k_fit = 5.0\n    mismatch = suitable_bins_cap - item\n    \n    # Use a capped sigmoid for numerical stability\n    max_exponent_arg = 35.0\n    capped_exponent_arg = np.minimum(k_fit * mismatch, max_exponent_arg)\n    tight_fit_scores = 1 / (1 + np.exp(capped_exponent_arg))\n\n    # --- Heuristic Component 2: Bin Scarcity Score ---\n    # Prioritize bins with less remaining capacity (scarcer bins).\n    # Using 1 / (capacity + epsilon) to avoid division by zero and give higher score to smaller capacities.\n    # Adding a small epsilon to avoid division by zero if a bin has 0 capacity (though unlikely if it fits the item).\n    epsilon = 1e-6\n    scarcity_scores = 1 / (suitable_bins_cap + epsilon)\n    \n    # Normalize scarcity scores to be comparable to tight_fit_scores (0 to 1 range)\n    # A simple min-max scaling can work.\n    min_scarcity = np.min(scarcity_scores)\n    max_scarcity = np.max(scarcity_scores)\n    if max_scarcity - min_scarcity > epsilon: # Avoid division by zero if all scarcity scores are the same\n        normalized_scarcity_scores = (scarcity_scores - min_scarcity) / (max_scarcity - min_scarcity)\n    else:\n        normalized_scarcity_scores = np.zeros_like(scarcity_scores) # Or assign 0.5 if all are equal and non-zero\n\n    # --- Combine Heuristics ---\n    # Weighted sum of tight fit and scarcity. Weights can be tuned.\n    # Here, we give equal weight, but this could be adjusted.\n    weight_fit = 0.5\n    weight_scarcity = 0.5\n    \n    combined_scores = (weight_fit * tight_fit_scores) + (weight_scarcity * normalized_scarcity_scores)\n\n    # --- Softmax for Probabilistic Exploration ---\n    # Temperature parameter: higher temp -> more exploration (probabilities closer to uniform)\n    # lower temp -> less exploration (probabilities closer to argmax)\n    temperature = 0.5  # Tunable parameter\n\n    # Apply Softmax\n    # Softmax(z)_i = exp(z_i / T) / sum(exp(z_j / T))\n    # Ensure combined_scores are not excessively large to avoid exp overflow even after capping.\n    # A common practice is to subtract the maximum score before exponentiation for numerical stability.\n    scores_for_softmax = combined_scores / temperature\n    \n    # Subtract max score for numerical stability in exp\n    stable_scores = scores_for_softmax - np.max(scores_for_softmax)\n    \n    exp_scores = np.exp(stable_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Place the calculated probabilities back into the main priorities array\n    priorities[suitable_bins_mask] = probabilities\n\n    return priorities\n\n[Reflection]\nBalance fit, scarcity, and exploration with tunable parameters.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}