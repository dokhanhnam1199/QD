{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    The strategy favors bins that are a \"good fit\" for the item (i.e., leaving\n    a small remaining capacity), but with a probability epsilon, it assigns a\n    consistent exploration score to encourage trying less optimal bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.2  # Probability of exploration\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if len(suitable_bins_indices) == 0:\n        return priorities  # No bin can fit the item\n\n    # --- Exploitation Component (Good Fit) ---\n    # Calculate a score based on how well the item fits.\n    # We want to prioritize bins that leave minimal remaining capacity.\n    # Score = 1 / (remaining_capacity - item + epsilon)\n    # A smaller difference means a higher score.\n    fit_scores = 1.0 / (bins_remain_cap[suitable_bins_indices] - item + 1e-6)\n\n    # Normalize fit_scores to a 0-1 range to represent the \"exploitation\" priority.\n    # Higher score means better fit.\n    if fit_scores.max() > fit_scores.min():\n        exploitation_priorities = (fit_scores - fit_scores.min()) / (fit_scores.max() - fit_scores.min())\n    else:\n        exploitation_priorities = np.ones(len(suitable_bins_indices)) # All fits are equally good\n\n    # Assign the exploitation priorities to the suitable bins\n    priorities[suitable_bins_indices] = exploitation_priorities\n\n    # --- Exploration Component ---\n    # With probability epsilon, overwrite the exploitation priority with a\n    # consistent, lower exploration score for a random subset of suitable bins.\n    # This encourages exploration of bins that might not be the immediate best fit.\n    exploration_score = 0.1 # A fixed low score for exploration\n\n    # Determine which suitable bins will be subject to exploration\n    num_suitable = len(suitable_bins_indices)\n    explore_indices_in_suitable = np.random.choice(\n        num_suitable,\n        size=int(np.ceil(epsilon * num_suitable)),\n        replace=False\n    )\n    \n    # Get the actual indices in the original bins_remain_cap array\n    bins_to_explore_indices = suitable_bins_indices[explore_indices_in_suitable]\n\n    # Assign the exploration score to these bins\n    priorities[bins_to_explore_indices] = exploration_score\n\n    # Ensure that bins that cannot fit the item have zero priority\n    priorities[~suitable_bins_mask] = 0\n\n    # Optional: Normalize final priorities if a specific range is required by downstream logic.\n    # For now, we return the scores as calculated, where higher means more preferred.\n    # A simple max-min normalization can be applied if needed:\n    # if priorities.max() > priorities.min():\n    #     final_priorities = (priorities - priorities.min()) / (priorities.max() - priorities.min())\n    # else:\n    #     final_priorities = np.ones(num_bins) * 0.5\n    # return final_priorities\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a softmax-based heuristic.\n\n    This heuristic prioritizes bins that can accommodate the item, with a stronger\n    preference for \"tight fits\" (bins with remaining capacity close to the item size).\n    The `temperature` parameter controls the exploration vs. exploitation trade-off.\n    Higher temperatures lead to more uniform probabilities (more exploration), while\n    lower temperatures focus on the best-fitting bins (more exploitation).\n    Bins that are too small for the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n        temperature: Controls the sharpness of the softmax distribution.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score (probability) of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate a \"goodness\" score.\n    # We want to prioritize bins where the remaining capacity is close to the item size.\n    # A common approach is to use the difference (remaining_capacity - item).\n    # To prioritize smaller differences (tighter fits), we can use the negative of this difference.\n    # We add a small epsilon to avoid issues when remaining_capacity == item.\n    # A larger negative value (more negative) indicates a worse fit, a value closer to zero is a better fit.\n    # To make better fits have higher scores for softmax, we can invert this or use a different metric.\n    # Let's try prioritizing based on how much capacity is LEFT OVER after packing.\n    # So, (remaining_capacity - item) is what we want to minimize.\n    # For softmax, higher values mean higher probability. So, we want a metric that is\n    # higher for better fits. A good metric would be the negative of the leftover capacity,\n    # or a Gaussian-like function centered at 0 difference.\n    # Let's use negative difference, then scale it to make it more sensitive to near fits.\n    # A simple transformation that boosts near-fits and reduces others:\n    # Consider -(bins_remain_cap[can_fit_mask] - item) which is (item - bins_remain_cap[can_fit_mask]).\n    # This value is negative or zero. Higher values (closer to zero) are better fits.\n    # To make it suitable for softmax where higher is better, we can use `-(bins_remain_cap[can_fit_mask] - item)`.\n    # However, this might still be too sensitive to very small items.\n    # Let's consider a score that is high when `bins_remain_cap[can_fit_mask] - item` is small and positive.\n    # The inverse `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` from v1 is good.\n    # Let's refine this. We want to reward bins where `bins_remain_cap - item` is small.\n    # A Gaussian-like kernel centered at 0 difference could work: exp(- (diff^2) / (2 * sigma^2))\n    # Or, a simpler approach: consider `1 / (1 + diff)` where diff is `bins_remain_cap - item`.\n    # If diff is small and positive, score is close to 1. If diff is large, score approaches 0.\n\n    # Let's try a score that emphasizes small positive differences.\n    # We want a high score when `bins_remain_cap[can_fit_mask] - item` is small and positive.\n    # Consider `1.0 / (1.0 + (bins_remain_cap[can_fit_mask] - item))`\n    # This gives scores between (0, 1] for valid fits. 1 for perfect fits.\n\n    # Alternative: Prioritize bins with minimum remaining capacity that can fit the item.\n    # This is essentially the \"Best Fit\" strategy. For a heuristic priority, we can\n    # use the inverse of the remaining capacity for fitting bins.\n    # `priorities[can_fit_mask] = 1.0 / bins_remain_cap[can_fit_mask]` - This prioritizes smallest bins.\n    # If we want to prioritize tight fits, we are looking for bins where `bins_remain_cap - item` is small.\n    # So, we want to maximize `- (bins_remain_cap[can_fit_mask] - item)`.\n    # Or, a score that is high for small positive `bins_remain_cap[can_fit_mask] - item`.\n    # Let's use `-(bins_remain_cap[can_fit_mask] - item)` directly, then rescale or apply softmax.\n    # The intuition of `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` was good.\n    # Let's enhance it for \"nearness\".\n\n    # Consider the \"wasted space\" after packing: `wasted_space = bins_remain_cap - item`.\n    # We want to minimize `wasted_space`. So, a higher priority should be given to bins with smaller `wasted_space`.\n    # Let's transform `wasted_space` into a score where smaller `wasted_space` yields a higher score.\n    # A simple transformation: `score = 1.0 / (1.0 + wasted_space)`.\n    # This gives scores in the range (0, 1]. Perfect fit -> score 1. Large wasted space -> score close to 0.\n    # This should provide a good signal for \"tight fits\".\n\n    wasted_space = bins_remain_cap[can_fit_mask] - item\n    # Using `1.0 / (1.0 + wasted_space)` maps small positive wasted space to values close to 1.\n    # For a perfect fit (wasted_space = 0), score is 1.\n    # For larger wasted_space, score decreases.\n    scores_for_softmax = 1.0 / (1.0 + wasted_space)\n\n    # Apply softmax to get probabilities.\n    # Ensure temperature is positive to avoid division by zero or invalid operations.\n    if temperature <= 0:\n        raise ValueError(\"Temperature must be positive.\")\n\n    # Calculate exponentiated scores, scaled by temperature\n    # Lower temperature means sharper distribution, higher temperature means flatter.\n    exp_scores = np.exp(scores_for_softmax / temperature)\n\n    # Normalize to get probabilities\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Assign probabilities to the original priorities array\n    priorities[can_fit_mask] = probabilities\n\n    # Normalize priorities to sum to 1, ensuring valid probability distribution\n    # This is already handled by the softmax if there's at least one bin that can fit.\n    # If no bins can fit, priorities remains all zeros, which is correct.\n    if np.sum(priorities) > 0:\n        priorities /= np.sum(priorities)\n\n    return priorities\n\n[Reflection]\n1. **Smooth Selection:** Softmax offers smoother transitions than strict epsilon-greedy.\n2. **Tunable Exploration:** Temperature parameter in softmax provides fine-grained control.\n3. **Focus on Fit:** Prioritize bins with minimal wasted space for better packing.\n4. **Unified Scoring:** Softmax converts scores into probabilities for selection.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}