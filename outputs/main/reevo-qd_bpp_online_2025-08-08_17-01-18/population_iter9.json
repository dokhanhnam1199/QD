[
  {
    "stdout_filepath": "problem_iter1_response20.txt_stdout.txt",
    "code_path": "problem_iter1_code20.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit Decreasing logic.\n\n    This heuristic prioritizes bins that can accommodate the item and are \"tight fits\"\n    to minimize wasted space. Bins that are too small for the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate priority as the inverse of the remaining capacity\n    # A smaller remaining capacity means a tighter fit, hence higher priority.\n    # We add a small epsilon to avoid division by zero if a bin has exactly the item's size.\n    priorities[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)\n\n    return priorities",
    "response_id": 20,
    "obj": 4.048663741523748,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This heuristic implements a \"Best Fit Decreasing\" like strategy for the\n    priority function within an online First Fit context. For each item, it\n    prioritizes bins that can accommodate the item, and among those, it\n    prefers bins that will have the least remaining capacity after the item\n    is placed. This aims to \"tightly pack\" bins, potentially leaving larger\n    spaces in other bins for future larger items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Calculate remaining capacity after placing the item\n    potential_remaining_cap = bins_remain_cap - item\n    \n    # Bins that can accommodate the item have a non-negative remaining capacity\n    can_accommodate = potential_remaining_cap >= 0\n    \n    # Assign a priority:\n    # For bins that can accommodate the item, the priority is the inverse of\n    # the remaining capacity after placement. Smaller remaining capacity gets higher priority.\n    # We use a large number (e.g., 1e6) for bins that cannot accommodate the item\n    # to ensure they have a much lower priority.\n    \n    # To make smaller remaining capacities have higher priority, we can use negative.\n    # The more negative, the better.\n    priorities[can_accommodate] = -potential_remaining_cap[can_accommodate]\n    \n    # Bins that cannot accommodate the item get a very low priority (a large positive number)\n    # We can also set it to 0 if we want to strictly exclude them if a better option exists.\n    # However, a very low negative number makes more sense for sorting if we want to be\n    # sure they are not picked. For First Fit, it's about *which* bin to try first.\n    # A common way to handle \"cannot accommodate\" in priority systems is to give them a\n    # very low score. If we want to avoid them entirely, we can assign a score that will\n    # never be chosen if any other valid option exists. Let's use a value that is\n    # significantly less desirable than any valid remaining capacity.\n    # Since remaining capacities are non-negative, negative values are good.\n    # We can use a large negative number for those that cannot fit.\n    priorities[~can_accommodate] = -1e9 # A very low priority\n    \n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit strategy.\n\n    The Best Fit strategy aims to place the item into the bin that has the least remaining\n    capacity after the item is placed, provided it fits. This helps in keeping larger\n    bins available for larger items later.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a more desirable bin. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    # Calculate remaining capacity after placing the item\n    potential_remaining_caps = bins_remain_cap - item\n\n    # Initialize priorities to zero\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Find bins where the item can fit\n    fit_indices = np.where(potential_remaining_caps >= 0)[0]\n\n    if len(fit_indices) > 0:\n        # For bins that can fit the item, assign a priority.\n        # The \"best fit\" is the one that leaves the *least* remaining capacity.\n        # To make this a \"highest priority\" score, we can invert the remaining capacity\n        # (or use a large number minus remaining capacity).\n        # A simple approach is to use the negative of the potential remaining capacity.\n        # The more negative, the better the fit (i.e., less remaining space).\n        priorities[fit_indices] = -potential_remaining_caps[fit_indices]\n\n        # To ensure that a bin that is a \"perfect fit\" (leaves 0 remaining capacity)\n        # is prioritized over one that leaves, say, -1 remaining capacity (meaning it was\n        # a bit too large), we can further refine the priority.\n        # If multiple bins have the same minimum remaining capacity, any of them is fine.\n        # The current negative remaining capacity already ranks them appropriately.\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response3.txt_stdout.txt",
    "code_path": "problem_iter1_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    The Almost Full Fit strategy prioritizes bins that, after placing the item,\n    will have the least remaining capacity among bins that can still accommodate the item.\n    This aims to fill bins as much as possible before opening new ones.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot accommodate the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find bins that can accommodate the item\n    possible_bins_mask = bins_remain_cap >= item\n\n    # Calculate remaining capacity after placing the item in possible bins\n    remaining_after_placement = bins_remain_cap[possible_bins_mask] - item\n\n    # If there are no possible bins, return all zeros\n    if remaining_after_placement.size == 0:\n        return priorities\n\n    # Calculate the \"tightness\" score for possible bins.\n    # We want to minimize the remaining capacity, so a smaller remaining capacity\n    # should result in a higher priority.\n    # We use the inverse of the remaining capacity. To avoid division by zero\n    # or extremely high priorities for bins that become exactly full, we can\n    # add a small epsilon or use a scaled inverse.\n    # A common approach is to consider bins that leave little to no space.\n    # We want to maximize the chance of the bin becoming \"almost full\".\n\n    # Let's aim for a score where smaller remaining capacity is better.\n    # A simple approach is to use the negative of the remaining capacity.\n    # However, to make it a \"priority\" score (higher is better), we can\n    # invert it and potentially scale it.\n\n    # To prioritize bins that leave minimal remaining space, we can take\n    # the negative of the remaining capacity. The larger (less negative)\n    # the value, the less space is left, thus higher priority.\n    # Let's refine this: We want bins that, after placing the item, will have\n    # the *least* remaining capacity. This means we want to *minimize*\n    # `bins_remain_cap - item`.\n    # If we want higher scores to be better, we can assign a score based on\n    # the inverse of the remaining capacity.\n    # However, the goal is to fill bins. So bins that will be closest to full\n    # after placing the item are preferred.\n\n    # Consider the difference: max_capacity - (remaining_after_placement)\n    # This is effectively how much space is used. We want to maximize this.\n    # So, `item` is constant. Maximizing `bins_remain_cap[i] - remaining_after_placement[i]`\n    # means maximizing `bins_remain_cap[i] - (bins_remain_cap[i] - item)` which is just `item`.\n    # This isn't quite right.\n\n    # \"Almost Full Fit\" suggests bins that are ALMOST full.\n    # After placing the item, a bin is \"almost full\" if its remaining capacity is small.\n    # So, we want to minimize `bins_remain_cap[i] - item`.\n    # To convert this to a priority score (higher is better), we can:\n    # 1. Use `1 / (remaining_after_placement + epsilon)` where epsilon is a small number to avoid division by zero.\n    # 2. Use `-(remaining_after_placement)`\n    # 3. Use `max_possible_remaining - remaining_after_placement` for some large `max_possible_remaining`\n    #    which is equivalent to `some_constant - remaining_capacity`.\n\n    # Let's try option 2: higher priority for smaller remaining capacity.\n    # So, `priority = -remaining_capacity`. This means a bin with remaining_capacity=1\n    # gets priority -1, and a bin with remaining_capacity=0 gets priority 0.\n    # This seems to fit the idea of \"smallest remaining capacity\".\n\n    # To make it more \"priority-like\" (higher is better), we can use:\n    # `priority = C - remaining_capacity`, where C is a large constant, or\n    # `priority = 1 / (remaining_capacity + epsilon)`\n\n    # Let's use the concept that the *difference* between what's remaining and what's desired (a full bin)\n    # should be minimized. So, `remaining_capacity` should be small.\n    # A score that reflects this: `max(0, C - remaining_capacity)`.\n    # If we want to prioritize bins that become *most* full after the item,\n    # this means the remaining capacity is minimized.\n\n    # Consider the bin that would become \"most full\". This is the bin where\n    # `bins_remain_cap[i] - item` is minimized.\n    # So, higher priority for smaller `bins_remain_cap[i] - item`.\n    # Let's transform `bins_remain_cap[i] - item` into a priority:\n    # `priority = some_large_value - (bins_remain_cap[i] - item)`\n    # This is equivalent to `some_large_value - bins_remain_cap[i] + item`.\n    # Since `item` is constant for all bins, this is `constant - bins_remain_cap[i]`.\n    # We want to maximize this, so we want to minimize `bins_remain_cap[i]`.\n\n    # Another perspective on \"almost full\":\n    # We want to pick the bin that, after placing the item, is closest to full.\n    # This means `bins_remain_cap[i] - item` is minimal.\n    # Let's assign a priority based on this minimum value.\n\n    # A common heuristic for \"Best Fit\" or \"Almost Full Fit\" is to try to minimize\n    # the remaining capacity of the bin *after* placing the item.\n    # This means we want to select the bin `i` that minimizes `bins_remain_cap[i] - item`.\n    # To convert this into a priority score (where higher means more desirable),\n    # we can use `-(bins_remain_cap[i] - item)` or `1 / (bins_remain_cap[i] - item + epsilon)`.\n\n    # Let's try using `1 / (remaining_after_placement + epsilon)` where epsilon is small.\n    # This ensures that bins with very small remaining capacity get very high priorities.\n\n    epsilon = 1e-6\n    priorities[possible_bins_mask] = 1.0 / (remaining_after_placement + epsilon)\n\n    # Alternatively, if we want to give a higher score to bins that leave *less*\n    # remaining space, we can simply use the negative of the remaining space,\n    # or a scaled version of it.\n\n    # Let's consider the original \"Almost Full Fit\" idea: prioritize bins that are\n    # already close to full. However, the problem statement for this function\n    # implies prioritizing based on the *outcome* after placing the item.\n    # \"priority score for each bin ... bin with the highest priority score will be selected for the item.\"\n\n    # If the goal is to fill bins, then after placing the item, we want the bin\n    # with the *least* remaining capacity.\n    # So, `priority = f(remaining_capacity)` where `f` is decreasing.\n    # `f(x) = 1/(x + epsilon)` or `f(x) = -x`.\n    # Let's use the `1/(x + epsilon)` approach for a strong preference towards tighter fits.\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response6.txt_stdout.txt",
    "code_path": "problem_iter4_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Sigmoid Fit Score.\n\n    This heuristic prioritizes bins that offer the \"tightest fit\" for an incoming item.\n    A tight fit means the bin has just enough remaining capacity to accommodate the item,\n    minimizing wasted space. Bins that are too small are excluded, and among the\n    suitable bins, those with less remaining capacity (but still sufficient) are preferred.\n\n    The scoring is based on a sigmoid function applied to the difference between\n    the bin's remaining capacity and the item's size. Specifically, for bins where\n    `remaining_capacity >= item`, the score is calculated as:\n\n    `score = 1 / (1 + exp(k * (remaining_capacity - item)))`\n\n    Here:\n    - `k` is a sensitivity parameter that controls how quickly the priority drops\n      as the remaining capacity exceeds the item size. A higher `k` means a sharper\n      preference for tighter fits.\n    - `remaining_capacity - item` is the \"mismatch\" or wasted space.\n    - When `remaining_capacity == item` (perfect fit), the exponent is 0, `exp(0)=1`,\n      and the score is `1 / (1 + 1) = 0.5`.\n    - When `remaining_capacity > item` (mismatch > 0), the exponent is positive.\n      As `remaining_capacity` increases, the exponent `k * (remaining_capacity - item)`\n      increases, `exp(...)` increases, `1 + exp(...)` increases, and thus the score\n      decreases (approaching 0 for very large capacities). This correctly penalizes\n      bins with excessive remaining space.\n\n    This approach ensures that bins with smaller positive mismatches (tighter fits)\n    receive higher priority scores than bins with larger positive mismatches.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is the priority score for the corresponding bin. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n    # If no bin can fit the item, return all zeros\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # Parameter for the sigmoid function's steepness.\n    # A higher value makes the function drop faster as capacity increases past the item size.\n    # This encourages selecting bins that are closer to the item size.\n    k = 5.0  # Tunable parameter\n\n    # Calculate the \"mismatch\" or wasted space for suitable bins\n    # mismatch = suitable_bins_cap - item\n    # We want to give higher priority when mismatch is small (close to 0).\n    # The function 1 / (1 + exp(k * mismatch)) achieves this:\n    # - If mismatch = 0, score = 1 / (1 + exp(0)) = 0.5\n    # - If mismatch > 0 (but small), exp(k*mismatch) is slightly > 1, score is slightly < 0.5\n    # - If mismatch is large positive, exp(k*mismatch) is very large, score approaches 0.\n\n    # Calculate the sigmoid scores for the suitable bins\n    # To avoid potential overflow with exp(k * mismatch) if mismatch is very large,\n    # we can consider the range of `suitable_bins_cap`. If `suitable_bins_cap`\n    # can be extremely large compared to `item`, `k * (suitable_bins_cap - item)`\n    # can be a very large positive number, leading to `exp()` overflowing.\n    # A robust way to handle this is to clip the argument to the exponential or\n    # use a more numerically stable sigmoid implementation if necessary.\n    # For typical BPP scenarios, direct calculation might be acceptable.\n    # If `suitable_bins_cap - item` becomes very large, `exp` might overflow.\n    # We can cap the argument to `exp` to prevent overflow.\n    # A practical upper bound for `k * (capacity - item)` can be set.\n    # For example, if `k=5`, `exp(35)` is already very large. Let's cap at 35.\n    mismatch = suitable_bins_cap - item\n    exponent_arg = k * mismatch\n    \n    # Cap the exponent argument to prevent overflow in np.exp\n    # A value of 700 is a common threshold for `exp` to return inf.\n    # If `k * mismatch` is, say, 40, `exp(40)` is large but manageable.\n    # If `k * mismatch` is 1000, `exp(1000)` is infinity.\n    # Let's cap the argument to a reasonable value, say 35, to keep `exp` within range,\n    # or handle `inf` gracefully. If `exp` becomes `inf`, the score becomes 0.\n    # A simpler approach is to ensure `k` and `mismatch` product doesn't exceed a threshold.\n    # Let's assume typical capacities and k are such that direct calculation is fine,\n    # but for robustness, we'll consider capping.\n    \n    # Let's use a threshold for `k * mismatch`. If `k * mismatch > threshold`,\n    # then `exp(k * mismatch)` will be very large, and the score will be close to 0.\n    # A threshold like 30-40 for the exponent is usually sufficient to make `exp` very large.\n    # Let's use a maximum argument to exp to prevent overflow.\n    max_exponent_arg = 35.0 # Corresponds to exp(35) which is ~3.4e15\n    \n    capped_exponent_arg = np.minimum(exponent_arg, max_exponent_arg)\n    \n    sigmoid_scores = 1 / (1 + np.exp(capped_exponent_arg))\n\n    # Place the calculated sigmoid scores back into the main priorities array\n    priorities[suitable_bins_mask] = sigmoid_scores\n\n    return priorities",
    "response_id": 6,
    "obj": 3.948942959712818,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response4.txt_stdout.txt",
    "code_path": "problem_iter3_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, temperature: float = 1.0) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a softmax-based heuristic.\n\n    This heuristic prioritizes bins that can accommodate the item, with a stronger\n    preference for \"tight fits\" (bins with remaining capacity close to the item size).\n    The `temperature` parameter controls the exploration vs. exploitation trade-off.\n    Higher temperatures lead to more uniform probabilities (more exploration), while\n    lower temperatures focus on the best-fitting bins (more exploitation).\n    Bins that are too small for the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n        temperature: Controls the sharpness of the softmax distribution.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score (probability) of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate a \"goodness\" score.\n    # We want to prioritize bins where the remaining capacity is close to the item size.\n    # A common approach is to use the difference (remaining_capacity - item).\n    # To prioritize smaller differences (tighter fits), we can use the negative of this difference.\n    # We add a small epsilon to avoid issues when remaining_capacity == item.\n    # A larger negative value (more negative) indicates a worse fit, a value closer to zero is a better fit.\n    # To make better fits have higher scores for softmax, we can invert this or use a different metric.\n    # Let's try prioritizing based on how much capacity is LEFT OVER after packing.\n    # So, (remaining_capacity - item) is what we want to minimize.\n    # For softmax, higher values mean higher probability. So, we want a metric that is\n    # higher for better fits. A good metric would be the negative of the leftover capacity,\n    # or a Gaussian-like function centered at 0 difference.\n    # Let's use negative difference, then scale it to make it more sensitive to near fits.\n    # A simple transformation that boosts near-fits and reduces others:\n    # Consider -(bins_remain_cap[can_fit_mask] - item) which is (item - bins_remain_cap[can_fit_mask]).\n    # This value is negative or zero. Higher values (closer to zero) are better fits.\n    # To make it suitable for softmax where higher is better, we can use `-(bins_remain_cap[can_fit_mask] - item)`.\n    # However, this might still be too sensitive to very small items.\n    # Let's consider a score that is high when `bins_remain_cap[can_fit_mask] - item` is small and positive.\n    # The inverse `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` from v1 is good.\n    # Let's refine this. We want to reward bins where `bins_remain_cap - item` is small.\n    # A Gaussian-like kernel centered at 0 difference could work: exp(- (diff^2) / (2 * sigma^2))\n    # Or, a simpler approach: consider `1 / (1 + diff)` where diff is `bins_remain_cap - item`.\n    # If diff is small and positive, score is close to 1. If diff is large, score approaches 0.\n\n    # Let's try a score that emphasizes small positive differences.\n    # We want a high score when `bins_remain_cap[can_fit_mask] - item` is small and positive.\n    # Consider `1.0 / (1.0 + (bins_remain_cap[can_fit_mask] - item))`\n    # This gives scores between (0, 1] for valid fits. 1 for perfect fits.\n\n    # Alternative: Prioritize bins with minimum remaining capacity that can fit the item.\n    # This is essentially the \"Best Fit\" strategy. For a heuristic priority, we can\n    # use the inverse of the remaining capacity for fitting bins.\n    # `priorities[can_fit_mask] = 1.0 / bins_remain_cap[can_fit_mask]` - This prioritizes smallest bins.\n    # If we want to prioritize tight fits, we are looking for bins where `bins_remain_cap - item` is small.\n    # So, we want to maximize `- (bins_remain_cap[can_fit_mask] - item)`.\n    # Or, a score that is high for small positive `bins_remain_cap[can_fit_mask] - item`.\n    # Let's use `-(bins_remain_cap[can_fit_mask] - item)` directly, then rescale or apply softmax.\n    # The intuition of `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` was good.\n    # Let's enhance it for \"nearness\".\n\n    # Consider the \"wasted space\" after packing: `wasted_space = bins_remain_cap - item`.\n    # We want to minimize `wasted_space`. So, a higher priority should be given to bins with smaller `wasted_space`.\n    # Let's transform `wasted_space` into a score where smaller `wasted_space` yields a higher score.\n    # A simple transformation: `score = 1.0 / (1.0 + wasted_space)`.\n    # This gives scores in the range (0, 1]. Perfect fit -> score 1. Large wasted space -> score close to 0.\n    # This should provide a good signal for \"tight fits\".\n\n    wasted_space = bins_remain_cap[can_fit_mask] - item\n    # Using `1.0 / (1.0 + wasted_space)` maps small positive wasted space to values close to 1.\n    # For a perfect fit (wasted_space = 0), score is 1.\n    # For larger wasted_space, score decreases.\n    scores_for_softmax = 1.0 / (1.0 + wasted_space)\n\n    # Apply softmax to get probabilities.\n    # Ensure temperature is positive to avoid division by zero or invalid operations.\n    if temperature <= 0:\n        raise ValueError(\"Temperature must be positive.\")\n\n    # Calculate exponentiated scores, scaled by temperature\n    # Lower temperature means sharper distribution, higher temperature means flatter.\n    exp_scores = np.exp(scores_for_softmax / temperature)\n\n    # Normalize to get probabilities\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Assign probabilities to the original priorities array\n    priorities[can_fit_mask] = probabilities\n\n    # Normalize priorities to sum to 1, ensuring valid probability distribution\n    # This is already handled by the softmax if there's at least one bin that can fit.\n    # If no bins can fit, priorities remains all zeros, which is correct.\n    if np.sum(priorities) > 0:\n        priorities /= np.sum(priorities)\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.\n\n    The strategy assigns higher priority to bins that have a remaining capacity\n    just slightly larger than the item size, aiming to fill bins more compactly.\n    A temperature parameter controls the \"softness\" of the softmax, influencing\n    how aggressively we favor these \"almost fitting\" bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Filter out bins that cannot fit the item\n    can_fit_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if not eligible_bins_cap.size:\n        # If no bin can fit the item, return zeros for all original bins\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit score\": how much space is left after placing the item.\n    # Smaller values indicate a tighter fit.\n    fit_scores = eligible_bins_cap - item\n\n    # Use a Softmax-like approach to convert fit scores to priorities.\n    # We invert the fit scores to give higher priority to smaller remaining capacities (tighter fits).\n    # Adding a small epsilon to avoid division by zero or log(0) if all fit_scores are 0.\n    epsilon = 1e-9\n    inverted_fit_scores = 1.0 / (fit_scores + epsilon)\n\n    # The temperature parameter controls the \"softness\" of the softmax.\n    # A lower temperature makes the distribution sharper (more peaky),\n    # favoring the best fitting bins more strongly.\n    # A higher temperature makes it more uniform.\n    temperature = 0.5  # This can be tuned as a hyperparameter\n\n    # Apply softmax to the inverted fit scores\n    try:\n        exp_scores = np.exp(inverted_fit_scores / temperature)\n        softmax_priorities = exp_scores / np.sum(exp_scores)\n    except OverflowError:\n        # Handle potential overflow if scores become too large\n        # In such cases, a simple proportional scaling might be better\n        # or clamping the input to exp. For simplicity here, we can\n        # assign equal high probability to all if overflow occurs.\n        softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size\n\n\n    # Create the final priority array, mapping priorities back to original bin indices\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[can_fit_mask] = softmax_priorities\n\n    return priorities",
    "response_id": 19,
    "obj": 4.048663741523748,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response10.txt_stdout.txt",
    "code_path": "problem_iter1_code10.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit Decreasing-like heuristic.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # First Fit strategy: find the first bin that can accommodate the item.\n    # For priority, we want to favor bins that are a \"tight fit\" but still fit.\n    # This means we prefer bins where the remaining capacity is just enough for the item.\n    # A bin with remaining capacity exactly equal to the item size is ideal.\n    # Bins that are too small should have a priority of 0.\n\n    # Create a mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities to a very low value (effectively zero for unusable bins)\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # For bins that can fit the item, calculate a priority score.\n    # We want to prioritize bins where the remaining capacity is closest to the item size,\n    # without going below the item size.\n    # The difference (bins_remain_cap - item) represents the \"slack\".\n    # We want to minimize slack, so a smaller difference is better.\n    # However, we want the *first* such bin in the array to be prioritized in case of ties,\n    # which is naturally handled by numpy's vectorized operations if we consider\n    # negative of the slack as a priority. A smaller slack means a larger negative slack,\n    # which translates to a higher priority in a max-priority queue sense.\n\n    # Calculate slack for bins that can fit the item\n    slack = bins_remain_cap[can_fit_mask] - item\n\n    # The priority is the negative of the slack.\n    # Smaller slack -> larger negative slack -> higher priority.\n    # If multiple bins have the same slack, their relative order in the original\n    # bins_remain_cap array will be preserved in terms of priority calculation.\n    priorities[can_fit_mask] = -slack\n\n    # In a true First Fit, we'd just take the first bin that fits.\n    # To simulate this \"first fit\" behavior in a priority context,\n    # we can add a small bonus to earlier bins with good fits, or more directly,\n    # simply return priorities such that the first available bin with the \"best\" fit\n    # (smallest slack) gets the highest priority. The `-slack` already achieves this.\n    # If multiple bins have the same minimal slack, the one appearing first in the\n    # `bins_remain_cap` array will naturally get the higher priority due to the way\n    # numpy operations often preserve order in selection when values are equal.\n    # For absolute certainty of 'first fit' logic within this priority framework,\n    # we can make bins that are \"exact fits\" have a slightly higher priority\n    # than slightly looser fits.\n\n    # Refined priority: Exact fits (slack=0) get highest priority.\n    # Then, among bins that fit, prioritize smaller slack (tighter fit).\n    # The current -slack already prioritizes smaller slack.\n    # To enforce the \"first fit\" aspect: consider the index.\n    # A bin with smaller index is preferred if slack is equal.\n\n    # Let's use a more explicit priority:\n    # 1. Highest priority for exact fits.\n    # 2. Then, prioritize bins with smaller slack.\n    # 3. If slack is equal, prioritize the bin with the smaller index.\n\n    # Initialize priorities for fitting bins\n    fit_priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Calculate slack for fitting bins\n    slack_values = bins_remain_cap[can_fit_mask] - item\n\n    # Assign priorities:\n    # For exact fits (slack_values == 0), assign a very high priority (e.g., 1e9)\n    # For other fits, assign priority based on negative slack.\n    # To break ties and enforce \"first fit\", we can penalize later bins.\n    # Let's assign priority = 10000 - slack - (index * 0.1) for fitting bins.\n    # This way, smaller slack is better, and smaller index is better for same slack.\n\n    indices = np.where(can_fit_mask)[0]\n    # Create a score: (ideal_fit - slack) + (bonus_for_early_bins)\n    # We want to maximize this score.\n    # Ideal fit = 0 (when remaining_capacity == item)\n    # So, priority component from slack is -slack.\n    # Bonus for early bins: -index * small_constant.\n    # We want to maximize priority.\n    # Maximize: (-slack) - (index * 0.01)\n    \n    # A simpler way is to use a very large number for exact fits, then -slack.\n    # Let's consider the \"best\" fit for the priority.\n    # The best fit is the one that minimizes `remaining_capacity - item`.\n    # This is equivalent to maximizing `-(remaining_capacity - item)`.\n    # So, `priority = -(remaining_capacity - item)` for fitting bins.\n    # To ensure first-fit, if there are multiple bins with the same minimal slack,\n    # the one with the lower index should be preferred.\n    # We can achieve this by adding a very small penalty to the priority based on index.\n    # `priority = -(remaining_capacity - item) - index * epsilon`\n    # where epsilon is a very small positive number. This ensures that a bin at a\n    # lower index with the same slack gets a slightly higher priority.\n\n    epsilon = 1e-6  # Small value to break ties for first-fit\n    fit_priorities[can_fit_mask] = -(slack_values) - (indices * epsilon)\n    \n    # We want the bin with the highest priority score to be selected.\n    # The current calculation `-(slack_values) - (indices * epsilon)` will work.\n    # A more direct \"First Fit Decreasing-like\" priority would be to assign\n    # priorities such that the smallest slack is maximized.\n    # And for ties in slack, the lowest index is maximized.\n    # So, `priority = (some_large_number - slack) - index * epsilon`.\n    # Or simply, `priority = -slack - index * epsilon`.\n    # A higher value means higher priority.\n\n    # Final check: The priority should reflect our preference.\n    # We prefer bins that are a tight fit, and among tight fits, the earliest one.\n    # This means a bin where `bins_remain_cap - item` is small is good.\n    # And smaller index is good for ties.\n    # So, the score should be high for small `bins_remain_cap - item` and small `index`.\n    # Let's use a score that is large for best fits:\n    # Priority = MAX_SCORE - (bins_remain_cap - item) - (index * penalty)\n    # A large MAX_SCORE ensures any fitting bin is better than non-fitting.\n    \n    MAX_BENEFIT_SCORE = 1000000  # A large number to indicate a good fit\n    INDEX_PENALTY_FACTOR = 1000  # Penalty for later bins\n\n    # Prioritize bins that can fit the item.\n    # For bins that fit, the score is determined by how \"tight\" the fit is\n    # (smaller remaining capacity after placement is better) and by their index\n    # (earlier bins are preferred for first-fit).\n\n    # Calculate the \"tightness\" score: a larger value means a tighter fit (less waste).\n    # We want to maximize `MAX_BENEFIT_SCORE - slack`.\n    tightness_score = np.zeros_like(bins_remain_cap)\n    tightness_score[can_fit_mask] = MAX_BENEFIT_SCORE - slack\n\n    # Introduce a penalty for bins with higher indices to enforce the \"first fit\" logic.\n    # Subtract a scaled index. Smaller index should have higher priority.\n    index_penalty = indices * INDEX_PENALTY_FACTOR\n    \n    # Combine scores. We want to maximize the overall priority.\n    # Prioritize based on tightness, then index.\n    # This means higher tightness_score is better.\n    # Lower index_penalty is better.\n    # So, we want to MAXIMIZE: `tightness_score - index_penalty`\n    \n    priorities = np.full_like(bins_remain_cap, -np.inf) # Initialize with low priority\n\n    if np.any(can_fit_mask):\n        fitting_indices = np.where(can_fit_mask)[0]\n        fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        \n        # Calculate the \"fit quality\": how close the remaining capacity is to the item size.\n        # A smaller difference (bins_remain_cap - item) is better.\n        fit_quality = -(fitting_bins_remain_cap - item) # Maximize this (smaller difference is better)\n\n        # To implement \"First Fit\", we prefer earlier bins when fit quality is the same.\n        # We can achieve this by adding a small bonus for earlier indices.\n        # This means we want to maximize `fit_quality + bonus_for_early_bins`.\n        # The bonus should decrease with index. So, `bonus = constant - index * small_factor`.\n        # Let's use a simple approach: a very high priority for the tightest fits,\n        # and then penalize for looser fits and later indices.\n\n        # Consider the inverse: we want to minimize waste (slack) and index.\n        # So, we want to minimize `slack + index * epsilon`.\n        # Priority should be inverse of this minimization.\n        # Priority = - (slack + index * epsilon)\n\n        epsilon_tiebreaker = 1e-6\n        priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item) - (np.arange(len(bins_remain_cap))[can_fit_mask] * epsilon_tiebreaker)\n\n    # The current priority definition maximizes (tight fit) + (early index).\n    # This correctly aligns with First Fit logic.\n    return priorities",
    "response_id": 10,
    "obj": 4.048663741523748,
    "SLOC": 23.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response14.txt_stdout.txt",
    "code_path": "problem_iter1_code14.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First strategy.\n\n    The Exact Fit First strategy prioritizes bins where the remaining capacity is exactly equal\n    to the item size. Bins that can fit the item but not exactly are given a lower priority,\n    with larger remaining capacities being less preferred. Bins that cannot fit the item\n    receive zero priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Prioritize bins where remaining capacity is exactly the item size\n    exact_fit_mask = (bins_remain_cap == item) & can_fit_mask\n    priorities[exact_fit_mask] = 1.0\n\n    # For bins that can fit but not exactly, give a lower priority\n    # We want to penalize bins with a lot of leftover space after placing the item.\n    # So, we assign a priority that decreases as (remaining_capacity - item_size) increases.\n    # A simple way is to use 1 / (remaining_capacity - item_size + 1) to avoid division by zero\n    # and ensure non-zero priorities for valid fits.\n    partial_fit_mask = (~exact_fit_mask) & can_fit_mask\n    remaining_space_after_fit = bins_remain_cap[partial_fit_mask] - item\n    priorities[partial_fit_mask] = 1.0 / (remaining_space_after_fit + 1) # Adding 1 to avoid division by zero if remaining_space is 0, which is handled by exact_fit_mask anyway.\n\n    # Ensure that exact fits have higher priority than partial fits.\n    # Since we set exact fits to 1.0, and partial fits to values < 1.0 (as remaining_space_after_fit >= 0),\n    # this condition is naturally met.\n\n    return priorities",
    "response_id": 14,
    "obj": 4.198244914240141,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response29.txt_stdout.txt",
    "code_path": "problem_iter1_code29.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    This heuristic prioritizes bins that have a remaining capacity slightly larger than the item,\n    aiming to minimize wasted space. It uses a softmax function to convert these differences\n    into probabilities, effectively assigning higher priority to bins that are a \"good fit\".\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the \"fit\" of the item into each bin.\n    # We want bins where bins_remain_cap is just enough or slightly more than the item.\n    # A negative value here means the item doesn't fit. We can clamp these to a small\n    # positive value or 0 to avoid issues with softmax if all items don't fit.\n    # A large positive difference (item fits easily) is also not ideal as it wastes space.\n    # So we want the difference (bins_remain_cap - item) to be close to zero.\n    # We can use the negative of this difference as the exponent in softmax,\n    # so smaller (bins_remain_cap - item) results in a higher exponent.\n    fits = bins_remain_cap - item\n\n    # Filter out bins where the item does not fit (remaining capacity < item size)\n    # Assign a very low priority (or effectively zero) to these bins.\n    # A large negative number in softmax exponent will result in a value close to 0.\n    # We can also directly set their fits to a very low value before softmax.\n    fits[fits < 0] = -np.inf # Effectively 0 probability after softmax\n\n    # Apply the softmax function. The exponent in softmax should reflect desirability.\n    # We want bins with (bins_remain_cap - item) close to 0 to have high priority.\n    # So, we can use -(bins_remain_cap - item) as the exponent, or more intuitively,\n    # (item - bins_remain_cap) as the exponent, ensuring negative values become smaller.\n    # Or even better, prioritize bins with a positive difference close to zero.\n    # Let's define desirability as: higher is better if remaining_cap is slightly > item.\n    # Consider a transformation: exp(-abs(fits)). This would favor fits near 0.\n    # However, softmax typically takes logits (raw scores).\n    # Let's consider the score `s_i = -(bins_remain_cap_i - item)^2`. This penalizes\n    # being too small or too large. But we only care about `bins_remain_cap_i >= item`.\n    # So, if `bins_remain_cap_i < item`, priority should be 0.\n    # If `bins_remain_cap_i >= item`, we want `bins_remain_cap_i - item` to be small.\n    # A good transformation for softmax could be to map `bins_remain_cap_i - item` to a\n    # desirability score. Higher `bins_remain_cap_i` than `item` is good, but not too high.\n    # Let's use `bins_remain_cap_i - item` directly as logits, but only for bins that can fit.\n    # For bins that cannot fit, their logit should be extremely low.\n\n    # Option 1: Softmax on (bins_remain_cap - item) for fitting bins.\n    # Prioritize bins where remaining capacity is *exactly* the item size.\n    # Using -fits will invert the ordering, so smaller positive diffs become larger.\n    # exp(-fits) where fits = bins_remain_cap - item\n    # If bins_remain_cap = 5, item = 3, fit = 2. exp(-2) = 0.135\n    # If bins_remain_cap = 3, item = 3, fit = 0. exp(0) = 1.0\n    # If bins_remain_cap = 6, item = 3, fit = 3. exp(-3) = 0.049\n    # This prioritizes exact fits.\n\n    # We need to ensure that we don't assign probabilities to bins where the item doesn't fit.\n    # A common way to do this with softmax is to assign a very low logit (large negative number).\n    logits = bins_remain_cap - item\n    # For bins that cannot fit the item, set their logit to a very small number.\n    # This will make their softmax probability close to zero.\n    logits[logits < 0] = -1e9  # A large negative number\n\n    # For bins that can fit, we want to prioritize those with smaller remaining capacity (closer to item size).\n    # So, we want to exponentiate -(bins_remain_cap - item) which is (item - bins_remain_cap).\n    # However, the softmax input should ideally be positive values that represent scores.\n    # Let's redefine `scores`: a higher score means better fit.\n    # A perfect fit would have score X.\n    # A slightly larger capacity would have score X - epsilon.\n    # A much larger capacity would have score X - delta.\n    # A capacity smaller than item would have score -infinity.\n    # So, we can use `item - bins_remain_cap` as our underlying measure for exponentiation,\n    # but we need to handle the case where `bins_remain_cap < item`.\n\n    # Let's use a modified approach. We want bins that *can* fit, and among those,\n    # we prefer bins with less excess capacity.\n    # So, a \"goodness\" score could be:\n    # -infinity if item > bins_remain_cap\n    # -(bins_remain_cap - item) otherwise (prioritizing smaller differences)\n\n    scores = -(bins_remain_cap - item)\n    scores[bins_remain_cap < item] = -np.inf # Ensure items not fitting get no chance\n\n    # Now apply softmax. Softmax(x_i) = exp(x_i) / sum(exp(x_j)).\n    # The `scores` directly go into the exponent of softmax.\n    # Higher scores (closer to 0, or less negative) mean higher probability.\n    # If scores = [-inf, -inf, 0, -2, -5],\n    # exp(scores) = [0, 0, 1, exp(-2), exp(-5)]\n    # Softmax will normalize these.\n\n    # Add a small constant to the score for bins that can fit, to ensure\n    # that even a small positive difference doesn't get zero probability.\n    # For instance, if we use -(bins_remain_cap - item), a difference of 10\n    # gives exp(-10) which is tiny.\n    # Perhaps a linear scaling or a different transformation is better.\n    # Let's try mapping `bins_remain_cap - item` to a desirability score.\n    # If diff = 0, score = K (high)\n    # If diff = 1, score = K - epsilon\n    # If diff = 10, score = K - delta\n    # If diff < 0, score = -infinity\n\n    # Let's re-think the logit construction for softmax.\n    # We want the probability P_i proportional to exp(logit_i).\n    # Desirability of bin i for item: D_i\n    # P_i = exp(alpha * D_i) / sum(exp(alpha * D_j))\n    # If item doesn't fit: D_i = -infinity\n    # If item fits and diff = bins_remain_cap - item:\n    # We want smaller diffs to have higher D_i.\n    # So, D_i = -(bins_remain_cap - item) = item - bins_remain_cap.\n\n    # This approach correctly prioritizes bins with less remaining capacity over\n    # bins with more remaining capacity, for those that can fit the item.\n    # The `-np.inf` for non-fitting bins ensures they get 0 probability.\n    # The `alpha` parameter (implicitly 1 here) controls the \"peakiness\" of the distribution.\n\n    # Calculate the underlying scores for softmax.\n    # A higher score means more desirable.\n    # If bins_remain_cap >= item, we want bins_remain_cap - item to be small.\n    # So, let's use `-(bins_remain_cap - item)` which is `item - bins_remain_cap`.\n    # For bins where `bins_remain_cap < item`, the score should be very low.\n\n    scores = item - bins_remain_cap\n\n    # Set scores to a very low value for bins where the item does not fit.\n    # This ensures their probability contribution in softmax is negligible.\n    scores[bins_remain_cap < item] = -1e9 # Effectively -infinity for softmax\n\n    # Calculate probabilities using softmax.\n    # Adding 1 to scores for fitting bins can help avoid all zeros if all diffs are large negative\n    # but we've handled that with -1e9 for non-fitting bins.\n    # The softmax calculation itself: exp(scores) / sum(exp(scores))\n    # We can use np.exp directly, but be mindful of overflow/underflow if scores are very large/small.\n    # If scores = [-1e9, -1e9, 0, -2, -5], then exp(scores) = [0, 0, 1, exp(-2), exp(-5)]\n    # This seems correct.\n    exp_scores = np.exp(scores)\n\n    # If all items resulted in -1e9 (meaning item doesn't fit any bin), exp_scores will be all zeros.\n    # In this case, the sum will be zero, leading to division by zero. Handle this edge case.\n    sum_exp_scores = np.sum(exp_scores)\n    if sum_exp_scores == 0:\n        # This means the item cannot fit into any available bin.\n        # Return zero priorities for all bins.\n        return np.zeros_like(bins_remain_cap)\n\n    priorities = exp_scores / sum_exp_scores\n\n    return priorities",
    "response_id": 29,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response7.txt_stdout.txt",
    "code_path": "problem_iter4_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a blended strategy.\n\n    This strategy prioritizes exact fits, uses a decay function for near-exact fits,\n    and applies a Softmax-like normalization for smooth probability distribution.\n    It aims to favor bins where the remaining capacity is closest to the item size,\n    with a strong preference for exact matches.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if not eligible_bins_cap.size:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit residual\": how much space is left after placing the item.\n    # Smaller values indicate a tighter fit.\n    fit_residuals = eligible_bins_cap - item\n\n    # Prioritize exact fits (residual = 0). For near-exact fits, use an exponential\n    # decay based on the residual. Bins with smaller residuals get higher scores.\n    # A small epsilon is added to the residual to ensure that exact fits (residual=0)\n    # get a distinct, higher score than bins that leave a tiny positive residual.\n    # The decay_factor controls how quickly the priority drops as the residual increases.\n    decay_factor = 0.5\n    # We want smaller residuals to have higher scores.\n    # An exponential decay is suitable: exp(-decay_factor * residual)\n    # For residual = 0 (exact fit), score is exp(0) = 1.0.\n    # For residual > 0, score decreases.\n    # Add a small constant to the exponent to ensure that even exact fits have a score\n    # that can be part of a meaningful softmax, avoiding potential issues if all residuals are 0.\n    # Alternatively, we can explicitly set exact fits to a high base value.\n    \n    # Strategy:\n    # 1. Exact fits get a high score (e.g., 1.0).\n    # 2. Near-exact fits get a score based on exponential decay of the residual.\n    # 3. Use softmax to normalize these scores into probabilities.\n\n    # Base scores: 1.0 for exact fits, and an exponentially decaying score for others.\n    # The decay_rate ensures that bins with residuals closer to 0 are preferred.\n    # We use `fit_residuals + 1e-6` to ensure that even for exact fits (residual=0),\n    # we have a non-zero value to pass to exp, and to differentiate exact from very close fits slightly.\n    # However, a cleaner approach is to handle exact fits explicitly.\n    \n    scores = np.where(fit_residuals == 0,\n                      1.0,  # High priority for exact fits\n                      np.exp(-decay_factor * fit_residuals)) # Decreasing priority for near-fits\n\n    # Apply Softmax-like normalization to convert scores into probabilities.\n    # This ensures that the priorities sum to 1 across the eligible bins and\n    # that preferences are smoothly distributed.\n    # The temperature parameter controls the \"sharpness\" of the distribution.\n    # A lower temperature makes the probabilities sharper (more emphasis on best fits).\n    temperature = 0.2\n    \n    try:\n        # Ensure scores are not excessively large before exponentiation\n        # Clipping can help prevent overflow, but Softmax should handle it better with exp\n        # Adding a small constant to the scores before softmax can also help stabilize.\n        # A common practice is to subtract the maximum score before exponentiating to avoid overflow.\n        max_score = np.max(scores)\n        normalized_scores = (scores - max_score) / temperature\n        exp_scores = np.exp(normalized_scores)\n        softmax_priorities = exp_scores / np.sum(exp_scores)\n    except OverflowError:\n        # In case of extreme values leading to overflow, fall back to a uniform distribution\n        # or a simpler heuristic if softmax fails. For now, uniform is a safe fallback.\n        softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size\n    except ZeroDivisionError:\n        # If sum of exp_scores is zero (highly unlikely with positive scores), fallback\n        softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size\n\n    # Map the calculated priorities back to the original bin indices\n    priorities[can_fit_mask] = softmax_priorities\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "SLOC": 23.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response5.txt_stdout.txt",
    "code_path": "problem_iter2_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    This heuristic prioritizes bins that can fit the item and are \"nearly full\"\n    (i.e., have little remaining capacity after fitting the item), as this\n    encourages tighter packing. A small random exploration component is added\n    to avoid getting stuck in local optima. The exploration rate (epsilon) is\n    reduced to favor greedy choices more. Scores are normalized to a [0, 1] range\n    for a more stable epsilon-greedy behavior.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.05  # Reduced probability of random exploration for more greedy behavior\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate scores for bins that can fit the item\n    # Prioritize bins that leave little remaining capacity after fitting the item.\n    # Score is the inverse of the remaining capacity after fitting.\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Add a small constant to the denominator to prevent division by zero and overly large scores\n    # for bins that perfectly fit the item.\n    scores = 1.0 / (remaining_capacity_after_fit + 1e-6) \n    \n    # Normalize scores to be between 0 and 1.\n    # This makes the epsilon-greedy selection more balanced.\n    if scores.size > 0:\n        min_score = np.min(scores)\n        max_score = np.max(scores)\n        if max_score > min_score:\n            priorities[can_fit_mask] = (scores - min_score) / (max_score - min_score)\n        else:\n            # If all scores are the same (e.g., all bins have the same remaining capacity after fit),\n            # assign a neutral priority, or simply a value representing the best fit.\n            # Assigning 0.5 could be seen as neutral, but since they are all equally good,\n            # a higher uniform value (like 1.0) might be more indicative of a good fit.\n            # Let's stick to a normalized 1.0 for \"equally best\" fit.\n            priorities[can_fit_mask] = 1.0\n    \n    # Epsilon-Greedy exploration: with probability epsilon, choose a random bin that fits.\n    # This allows for exploration of less optimal (but still valid) bins.\n    if np.random.rand() < epsilon:\n        possible_bins_indices = np.where(can_fit_mask)[0]\n        if possible_bins_indices.size > 0:\n            # Select a random bin among those that can fit the item.\n            random_bin_index = np.random.choice(possible_bins_indices)\n            \n            # Reset priorities and assign the highest priority to the randomly chosen bin.\n            # This ensures that the exploration step effectively picks a random bin.\n            priorities = np.zeros_like(bins_remain_cap, dtype=float)\n            priorities[random_bin_index] = 1.0  # Highest priority\n\n    return priorities",
    "response_id": 5,
    "obj": 4.068607897885915,
    "SLOC": 20.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response3.txt_stdout.txt",
    "code_path": "problem_iter8_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority scores for packing an item into bins using an improved\n    heuristic that balances tight fits with a controlled exploration strategy.\n\n    This heuristic prioritizes bins that are closer to fitting the item exactly,\n    using a hyperbolic ranking that favors tighter fits more strongly than\n    linear approaches. It also incorporates a probability-based exploration\n    mechanism to occasionally choose a random eligible bin, preventing\n    getting stuck in local optima.\n\n    The exploration probability can be seen as inversely related to the\n    number of available suitable bins, encouraging exploration when options\n    are plentiful and exploitation when options are limited.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of priority scores, same size as `bins_remain_cap`.\n        Bins that cannot fit the item will have a score of 0.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate scores for bins that can fit the item\n    if np.any(can_fit_mask):\n        suitable_bins_cap = bins_remain_cap[can_fit_mask]\n\n        # Hyperbolic scoring: Prioritize bins that leave minimal remaining capacity.\n        # The score is inversely proportional to the remaining capacity after packing.\n        # Adding a small constant (epsilon) to the denominator prevents division by zero\n        # for perfect fits and ensures scores remain finite.\n        # A higher \"strength\" parameter (e.g., 1.0 here) can control the steepness\n        # of the preference for tighter fits.\n        strength = 1.0\n        epsilon = 1e-6\n        scores = 1.0 / (suitable_bins_cap - item + epsilon) ** strength\n\n        # Normalize scores to a [0, 1] range for consistency.\n        min_score = np.min(scores)\n        max_score = np.max(scores)\n\n        if max_score > min_score:\n            normalized_scores = (scores - min_score) / (max_score - min_score)\n        else:\n            # If all suitable bins yield the same score, assign uniform high scores.\n            normalized_scores = np.ones(scores.shape)\n\n        priorities[can_fit_mask] = normalized_scores\n\n        # Controlled Exploration:\n        # The probability of random exploration decreases with the number of\n        # eligible bins. When there are many options, we are more likely to\n        # pick a random one to explore different packing strategies.\n        num_eligible_bins = np.sum(can_fit_mask)\n        # A simple inverse relationship: exploration_prob = 1 / num_eligible_bins\n        # but capped to prevent extremely high probabilities and ensure it's\n        # not zero when there are many options.\n        exploration_prob = 0.1 / np.sqrt(num_eligible_bins) if num_eligible_bins > 0 else 0.0\n        exploration_prob = min(exploration_prob, 0.3) # Cap exploration probability\n\n        if np.random.rand() < exploration_prob:\n            possible_bins_indices = np.where(can_fit_mask)[0]\n            if possible_bins_indices.size > 0:\n                # Select a random bin among those that can fit.\n                random_bin_index = np.random.choice(possible_bins_indices)\n\n                # Assign the highest priority to the randomly chosen bin.\n                priorities = np.zeros(num_bins, dtype=float)\n                priorities[random_bin_index] = 1.0\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "SLOC": 26.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a blended strategy.\n\n    This strategy combines the preference for exact fits with the soft preference\n    for near-exact fits using a Softmax-like approach. It prioritizes bins\n    where the remaining capacity is exactly the item size, and then assigns\n    decreasing priority to bins with slightly larger remaining capacities.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if not eligible_bins_cap.size:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit score\": how much space is left after placing the item.\n    # Smaller values indicate a tighter fit.\n    fit_scores = eligible_bins_cap - item\n\n    # Assign a base priority of 1.0 to exact fits and a lower base priority to partial fits.\n    # This ensures exact fits are always preferred over partial fits before softmax.\n    base_priorities = np.where(fit_scores == 0, 1.0, 0.5)\n\n    # To incorporate the \"near-exact\" preference smoothly, we can modify the\n    # base priorities based on how close the fit is, using a logistic or exponential decay.\n    # Here, we use a simple exponential decay for remaining space.\n    # Smaller remaining space (after fitting) should get higher priority.\n    # We add 1 to fit_scores to handle the case of exact fit (fit_score=0) gracefully\n    # and ensure positive values for exponentiation.\n    # The scaling factor (e.g., 1.0) and the decay rate (e.g., 0.1) can be tuned.\n    decay_rate = 0.1\n    near_fit_scores = np.exp(-decay_rate * fit_scores)\n\n    # Blend the base priority with the near-fit score.\n    # Exact fits should retain their high base priority, while near-fits get a boost.\n    # We prioritize exact fits (score 1.0) and then near-exact fits.\n    # A simple way to blend is to amplify the scores of exact fits and give\n    # a scaled score to near-fits.\n    combined_scores = np.where(fit_scores == 0, 1.0, near_fit_scores)\n\n    # Apply a Softmax-like transformation to normalize and create probabilities.\n    # This smooths the preferences, giving higher probability to better fits.\n    # The temperature parameter controls the \"softness\".\n    temperature = 0.5\n    try:\n        # Add a small epsilon to prevent log(0) or division by zero issues\n        epsilon = 1e-9\n        scaled_scores = combined_scores / temperature\n        exp_scores = np.exp(scaled_scores)\n        softmax_priorities = exp_scores / np.sum(exp_scores)\n    except OverflowError:\n        # Fallback for overflow: assign uniform probability if scores are too extreme\n        softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size\n\n\n    # Map the calculated priorities back to the original bin indices\n    priorities[can_fit_mask] = softmax_priorities\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "SLOC": 21.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response0.txt_stdout.txt",
    "code_path": "problem_iter7_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority for packing an item into bins using a unified scoring mechanism\n    that balances fit tightness and bin scarcity, employing a Softmax-like approach.\n\n    This heuristic prioritizes bins that offer a \"tightest fit\" for an incoming item,\n    meaning the bin has just enough remaining capacity to accommodate the item,\n    minimizing wasted space. It also considers the overall scarcity of suitable bins\n    using a Softmax-like function.\n\n    The scoring mechanism works as follows:\n    1. For bins that can fit the item (`remaining_capacity >= item`), calculate a\n       \"fit score\" that is higher for tighter fits. This is achieved using a\n       sigmoid function: `1 / (1 + exp(k * (remaining_capacity - item)))`.\n       - `k` is a sensitivity parameter (tunable) controlling the preference for\n         tighter fits. Higher `k` means sharper preference.\n       - A perfect fit (`remaining_capacity == item`) results in a score of 0.5.\n       - Bins with larger remaining capacity (greater \"mismatch\") get lower scores.\n    2. Apply a Softmax-like transformation to these fit scores. This considers\n       the relative desirability of each suitable bin. The Softmax function,\n       `exp(score) / sum(exp(scores))`, converts scores into probabilities,\n       where bins with higher fit scores get a proportionally higher probability\n       (priority). This also implicitly handles bin scarcity by normalizing\n       across all suitable bins.\n    3. Bins that cannot fit the item receive a priority of 0.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is the priority score for the corresponding bin. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n    # If no bin can fit the item, return all zeros\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # Tunable parameter for the sigmoid's steepness.\n    # Higher k emphasizes tighter fits more strongly.\n    k_fit = 5.0\n\n    # Calculate the \"mismatch\" for suitable bins\n    mismatch = suitable_bins_cap - item\n\n    # Calculate fit scores using sigmoid: higher score for smaller mismatch\n    # Score is in [0, 1], with 0.5 for a perfect fit.\n    # Cap the exponent argument to prevent overflow in np.exp.\n    # A value of 35.0 for `k * mismatch` yields `exp(35)`, which is large.\n    # Scores below this threshold will still be distinguishable.\n    max_exponent_arg = 35.0\n    capped_exponent_arg = np.minimum(k_fit * mismatch, max_exponent_arg)\n    fit_scores = 1 / (1 + np.exp(capped_exponent_arg))\n\n    # Softmax-like transformation to get relative priorities\n    # This converts fit scores into a distribution, where higher fit scores\n    # get proportionally higher probabilities (priorities).\n    # It also balances exploration/exploitation by considering all suitable bins.\n    exp_fit_scores = np.exp(fit_scores)\n    \n    # Avoid division by zero if all exp_fit_scores are zero (highly unlikely with sigmoid)\n    sum_exp_fit_scores = np.sum(exp_fit_scores)\n    if sum_exp_fit_scores == 0:\n        # If for some reason sum is zero, distribute uniformly among suitable bins\n        softmax_priorities = np.ones_like(fit_scores) / fit_scores.size\n    else:\n        softmax_priorities = exp_fit_scores / sum_exp_fit_scores\n\n    # Place the calculated softmax priorities back into the main priorities array\n    priorities[suitable_bins_mask] = softmax_priorities\n\n    return priorities",
    "response_id": 0,
    "obj": 3.948942959712818,
    "SLOC": 19.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response2.txt_stdout.txt",
    "code_path": "problem_iter7_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority scores for bins using a unified scoring mechanism\n    that balances tight fits with bin scarcity, employing a Softmax function\n    with a tunable temperature.\n\n    This heuristic prioritizes bins based on two factors:\n    1. Tightness of fit: Bins that have just enough remaining capacity\n       for the item are preferred.\n    2. Bin scarcity: Bins with less remaining capacity overall are considered\n       more valuable, as they are closer to being full.\n\n    The scoring mechanism uses a combination of the \"mismatch\" (remaining capacity - item)\n    and the bin's remaining capacity itself. A Softmax function is applied to these\n    scores to create a probability distribution over the bins, effectively\n    balancing exploration (trying less tight fits) and exploitation (going for the tightest fit).\n\n    For each bin `i`:\n    - `mismatch_i = bins_remain_cap[i] - item` (if `bins_remain_cap[i] >= item`, else infinity)\n    - `scarce_score_i = -bins_remain_cap[i]` (higher score for smaller capacity)\n    - `unified_score_i = scarce_score_i - mismatch_i` (prioritize low mismatch and low capacity)\n\n    A Softmax function is then applied to `unified_score_i` for all suitable bins:\n    `probability_i = exp(temperature * unified_score_i) / sum(exp(temperature * unified_score_j))`\n\n    The `temperature` parameter controls the exploration-exploitation trade-off:\n    - High temperature: Explores more, scores are closer to uniform.\n    - Low temperature: Exploits more, favors bins with the absolute highest unified score.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is a probability score (between 0 and 1) for the corresponding bin.\n        Bins that cannot fit the item will have a score of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n    # If no bin can fit the item, return all zeros\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # Tunable temperature parameter for Softmax.\n    # Controls the balance between exploiting the best fit and exploring other options.\n    # A value of 1.0 is a good starting point. Higher values lead to more uniform\n    # probabilities (more exploration), lower values lead to more peaked probabilities\n    # (more exploitation).\n    temperature = 1.0\n\n    # Calculate scores for suitable bins\n    # Score is based on prioritizing bins with less remaining capacity (scarce_score)\n    # and a tighter fit (lower mismatch).\n    # We want to maximize `scarce_score - mismatch`.\n    # `scarce_score` is `-bins_remain_cap[i]` (higher score for lower capacity).\n    # `mismatch` is `bins_remain_cap[i] - item`.\n    # So, `unified_score_i` = `-bins_remain_cap[i] - (bins_remain_cap[i] - item)`\n    #                     = `item - 2 * bins_remain_cap[i]`\n    # This simple formulation prioritizes bins that are smaller, and among equally\n    # sized bins, it prioritizes those that are closer to the item size.\n    # Let's refine this to directly use mismatch and scarcity in a balanced way.\n\n    # Let's define components:\n    # 1. Fit Score: Penalize larger mismatches. A simple way is `-mismatch`.\n    #    To be more robust, use a function that drops quickly. E.g., `-(bins_remain_cap[i] - item)^2`.\n    #    Or, for a \"tight fit\" focus, we can use the previous sigmoid idea.\n    #    However, for Softmax unification, let's use a simple linear penalty on mismatch.\n    #    `fit_score = -(suitable_bins_cap - item)`\n    #\n    # 2. Scarcity Score: Penalize larger remaining capacities.\n    #    `scarce_score = -suitable_bins_cap`\n\n    # Unified score combines fit and scarcity.\n    # We want to prefer low mismatch AND low capacity.\n    # A simple sum of negative values works: `unified_score = fit_score + scarce_score`\n    # `unified_score = -(suitable_bins_cap - item) - suitable_bins_cap`\n    # `unified_score = -suitable_bins_cap + item - suitable_bins_cap`\n    # `unified_score = item - 2 * suitable_bins_cap`\n    # This means bins with smaller remaining capacity get higher scores.\n    # Let's re-evaluate the reflection: \"prioritize tight fits using a unified scoring mechanism.\n    # Employ Softmax with tunable temperature for adaptive exploration/exploitation,\n    # balancing fit and bin scarcity.\"\n\n    # The prompt implies balancing *tight fits* with *bin scarcity*.\n    # Tight fit: Small `bins_remain_cap[i] - item`.\n    # Bin scarcity: Small `bins_remain_cap[i]`.\n\n    # Let's try to create scores where higher is better for both.\n    # For tight fit: Higher score for smaller `(bins_remain_cap[i] - item)`.\n    # For bin scarcity: Higher score for smaller `bins_remain_cap[i]`.\n\n    # Example scores:\n    # `fit_priority_component = - (suitable_bins_cap - item)`\n    # `scarcity_priority_component = - suitable_bins_cap`\n    # `unified_score = fit_priority_component + scarcity_priority_component`\n    # `unified_score = -(suitable_bins_cap - item) - suitable_bins_cap`\n    # `unified_score = -suitable_bins_cap + item - suitable_bins_cap`\n    # `unified_score = item - 2 * suitable_bins_cap`\n\n    # Alternative: Use the \"goodness\" of the fit and \"how much capacity is left\".\n    # Goodness of fit: Higher for smaller `(bins_remain_cap[i] - item)`.\n    # How much capacity is left: Higher for smaller `bins_remain_cap[i]`.\n    # Let's scale them to be comparable.\n    # We can think of `bins_remain_cap[i]` as a measure of bin fullness (inversely).\n    # A bin that is almost full (low `bins_remain_cap[i]`) is scarce.\n    # A bin that fits the item snugly (low `bins_remain_cap[i] - item`) is a tight fit.\n\n    # Let's consider the remaining capacity `R` and item size `S`.\n    # Tight fit: `R - S` is small and non-negative.\n    # Scarcity: `R` is small.\n\n    # Proposed unified score: Prioritize bins where `R` is small, and among those,\n    # where `R - S` is small.\n    # A simple score that captures this could be a decreasing function of `R` and `R-S`.\n    # Let's try: `score = - R - (R - S)` for `R >= S`.\n    # `score = -2R + S`\n    # This penalizes larger `R` more heavily.\n\n    # Let's consider the *benefit* of placing the item in a bin.\n    # Benefit = saving a bin (if it's the last item for that bin) + reduced wasted space.\n    # This is complex for an online setting.\n\n    # Let's go back to balancing fit and scarcity.\n    # How about giving a high score to bins that are *almost* full, but can still fit the item?\n    # And among those, prioritize the ones that fit snugly.\n\n    # A score that reflects \"how much capacity is left relative to the item size\"\n    # and \"how much capacity is left in total\".\n    # Let `mismatch = suitable_bins_cap - item`\n    # Let `capacity = suitable_bins_cap`\n\n    # Score idea: `f(mismatch, capacity)`. We want `f` to increase as `mismatch` and `capacity` decrease.\n    # `score = -mismatch - capacity`\n    # `score = -(suitable_bins_cap - item) - suitable_bins_cap`\n    # `score = -suitable_bins_cap + item - suitable_bins_cap`\n    # `score = item - 2 * suitable_bins_cap`\n\n    # Let's scale these components if needed, but for Softmax, relative values matter.\n    # Let's use the components directly.\n    # `fit_score_component = -(suitable_bins_cap - item)`\n    # `scarcity_score_component = -suitable_bins_cap`\n    # `unified_score = fit_score_component + scarcity_score_component`\n\n    # To ensure numerical stability and meaningful distribution from Softmax,\n    # we might want to normalize or shift these scores.\n    # A common practice is to ensure scores are not excessively large or small.\n\n    # Let's consider the components:\n    # `mismatches = suitable_bins_cap - item` (non-negative)\n    # `capacities = suitable_bins_cap` (non-negative)\n\n    # We want to maximize: `-mismatches` and `-capacities`.\n    # Let's create two terms, one favoring tight fits and one favoring scarcity.\n    # Term 1 (Tight Fit): Higher score for smaller `mismatch`.\n    # E.g., `term1 = -mismatches`. A large mismatch gives a large negative score.\n    # Term 2 (Scarcity): Higher score for smaller `capacity`.\n    # E.g., `term2 = -capacities`. A large capacity gives a large negative score.\n\n    # Unified Score = `w1 * term1 + w2 * term2`\n    # Let's try equal weighting for now: `w1 = 1, w2 = 1`.\n    # `unified_score = -mismatches - capacities`\n    # `unified_score = -(suitable_bins_cap - item) - suitable_bins_cap`\n    # `unified_score = item - 2 * suitable_bins_cap`\n\n    # This score implies that if two bins have the same remaining capacity,\n    # they will have the same score regardless of the item size.\n    # If we want the tightness of fit to play a role even for bins with the same remaining capacity,\n    # we need to structure it differently.\n\n    # Let's think about the criteria:\n    # 1. Minimal remaining capacity (`R`) such that `R >= item`. This is scarcity.\n    # 2. Minimal `R - item`. This is tight fit.\n\n    # A common approach for combining criteria is to use a weighted sum of functions\n    # that represent each criterion.\n    # Let `f_scarce(R) = -R` (higher for smaller R)\n    # Let `f_fit(R, S) = -(R - S)` (higher for smaller R-S)\n\n    # Unified score: `w_s * f_scarce(R) + w_f * f_fit(R, S)`\n    # `w_s * (-R) + w_f * (-(R - S))`\n    # `-(w_s * R + w_f * R - w_f * S)`\n    # `-( (w_s + w_f) * R - w_f * S )`\n    # If `w_s = w_f = 1`: `-(2R - S) = S - 2R`. Same as before.\n\n    # The issue with `S - 2R` is that it might not differentiate well if `R` is very large.\n    # For example, if `R` is huge, the score becomes very negative.\n    # Let's consider the components separately and then combine.\n\n    # Component 1: How \"scarce\" is the bin? (Higher for less capacity)\n    # `scarce_value = 1.0 / (1.0 + suitable_bins_cap)`  # Decreasing function of capacity\n    # or `scarce_value = -suitable_bins_cap`\n\n    # Component 2: How \"tight\" is the fit? (Higher for less mismatch)\n    # `mismatch = suitable_bins_cap - item`\n    # `fit_value = 1.0 / (1.0 + mismatch)` # Decreasing function of mismatch\n    # or `fit_value = -mismatch`\n\n    # Let's try `fit_value = 1.0 / (1.0 + mismatch)` and `scarce_value = 1.0 / (1.0 + suitable_bins_cap)`.\n    # These are normalized between 0 and 1.\n    # `unified_score = fit_value + scarce_value`\n    # `unified_score = (1.0 / (1.0 + suitable_bins_cap - item)) + (1.0 / (1.0 + suitable_bins_cap))`\n\n    # This looks promising. Both terms are higher when their respective values are lower.\n    # Let's use these as the scores that will be fed into Softmax.\n\n    mismatches = suitable_bins_cap - item\n    capacities = suitable_bins_cap\n\n    # To avoid division by zero if mismatch or capacity is 0, we add 1.\n    # The terms are:\n    # fit_term = 1 / (1 + mismatch) -> higher for smaller mismatch\n    # scarce_term = 1 / (1 + capacity) -> higher for smaller capacity\n    # We want to maximize both.\n    fit_term = 1.0 / (1.0 + mismatches)\n    scarce_term = 1.0 / (1.0 + capacities)\n\n    # Unified score: A simple sum of these terms.\n    unified_scores = fit_term + scarce_term\n\n    # Apply Softmax to get probabilities\n    # Need to handle cases where scores might be very large or very small.\n    # Softmax requires exponentiation, so large positive scores will dominate.\n    # If `unified_scores` are all very small negative, `exp` might underflow.\n    # If `unified_scores` are very large positive, `exp` might overflow.\n\n    # It's good practice to shift scores so the max is 0 before exponentiating for Softmax.\n    # `max_score = np.max(unified_scores)`\n    # `shifted_scores = unified_scores - max_score`\n    # `exp_scores = np.exp(temperature * shifted_scores)`\n    # `probabilities = exp_scores / np.sum(exp_scores)`\n\n    # Let's directly compute using temperature, and handle potential issues.\n    # A high temperature will smooth out differences.\n    # A low temperature will make the highest score dominate.\n\n    # Ensure scores are not too extreme before exponentiation.\n    # We can clip the `temperature * unified_scores` argument.\n    # For example, clip to [-10, 10].\n    exponent_argument = temperature * unified_scores\n    max_exponent_val = 10.0 # Corresponds to exp(10) approx 22000\n    min_exponent_val = -10.0 # Corresponds to exp(-10) approx 4.5e-5\n\n    capped_exponent_argument = np.clip(exponent_argument, min_exponent_val, max_exponent_val)\n\n    exp_scores = np.exp(capped_exponent_argument)\n    sum_exp_scores = np.sum(exp_scores)\n\n    # Avoid division by zero if all scores were somehow zero or exp resulted in zero.\n    if sum_exp_scores == 0:\n        # Fallback: if all scores are 0 or lead to 0, assign uniform probability.\n        probabilities = np.ones_like(unified_scores) / len(unified_scores)\n    else:\n        probabilities = exp_scores / sum_exp_scores\n\n    # Place the calculated probabilities back into the main priorities array\n    priorities[suitable_bins_mask] = probabilities\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "SLOC": 24.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response2.txt_stdout.txt",
    "code_path": "problem_iter3_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, prioritizing tight fits.\n\n    This heuristic prioritizes bins that can accommodate the item, favoring \"tight fits\"\n    (smaller remaining capacity after packing) to minimize wasted space.\n    A softmax function is used to convert these preferences into probabilities,\n    allowing for some exploration. The temperature parameter controls the\n    aggressiveness of the selection. Bins that are too small for the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    temperature = 1.0  # Tune this parameter: lower for more greedy, higher for more exploration\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate a score based on how tight the fit is.\n    # A smaller remaining capacity after packing means a tighter fit.\n    # We add a small epsilon to avoid division by zero if a bin has exactly the item's size.\n    tight_fit_scores = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)\n\n    # Apply softmax to convert scores into probabilities/priorities\n    # Ensure no division by zero if all scores are effectively zero (e.g., no bin fits)\n    if np.sum(tight_fit_scores) > 0:\n        # Softmax calculation: exp(score / temperature) / sum(exp(score / temperature))\n        # To avoid numerical instability with large scores, we can subtract the max score.\n        max_score = np.max(tight_fit_scores)\n        exp_scores = np.exp((tight_fit_scores - max_score) / temperature)\n        sum_exp_scores = np.sum(exp_scores)\n\n        if sum_exp_scores > 0:\n            priorities[can_fit_mask] = exp_scores / sum_exp_scores\n        else:\n            # Handle cases where sum of exp_scores might be zero due to extreme values or temperature\n            # In such rare cases, fall back to uniform distribution among fitting bins\n            num_fitting_bins = np.sum(can_fit_mask)\n            if num_fitting_bins > 0:\n                priorities[can_fit_mask] = 1.0 / num_fitting_bins\n    else:\n        # If no bins can fit, priorities remain zero.\n        pass\n\n    # Tie-breaking: Favor earlier bins (lower index) if priorities are very close.\n    # This can be implicitly handled by the order of processing or explicitly added\n    # by adding a small negative value based on index to the score before softmax,\n    # e.g., score - index * epsilon_tiebreaker. For simplicity here, we rely on\n    # the original order and potential floating point differences.\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response2.txt_stdout.txt",
    "code_path": "problem_iter4_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, prioritizing tight fits and considering bin scarcity.\n\n    This heuristic prioritizes bins that can accommodate the item, favoring \"tight fits\"\n    (smaller remaining capacity after packing) to minimize wasted space.\n    It also introduces a penalty for bins that are already very full (scarce),\n    encouraging the use of bins with more remaining capacity if the fit is not extremely tight.\n    A softmax function is used to convert these preferences into probabilities,\n    allowing for some exploration. The temperature parameter controls the\n    aggressiveness of the selection. Bins that are too small for the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    temperature = 0.5  # Tune this parameter: lower for more greedy, higher for more exploration\n    scarcity_penalty_factor = 0.1 # Factor to penalize bins that are already nearly full\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities # No bins can fit the item\n\n    fitting_bins_caps = bins_remain_cap[can_fit_mask]\n\n    # Calculate a score based on how tight the fit is.\n    # A smaller remaining capacity after packing means a tighter fit.\n    # Add a small epsilon to avoid division by zero if a bin has exactly the item's size.\n    tight_fit_scores = 1.0 / (fitting_bins_caps - item + 1e-9)\n\n    # Introduce a scarcity penalty: Bins with less remaining capacity overall are less preferred\n    # unless the fit is extremely tight. We can scale the inverse of remaining capacity.\n    # We want to penalize bins that are already very full, so we use 1/capacity.\n    # However, we only want this penalty to apply if the fit isn't perfectly tight.\n    # A simple approach is to add a small term that decreases with remaining capacity.\n    # We'll use a factor that scales the inverse of the bin's *total* capacity\n    # to penalize those that are less likely to accommodate future items.\n    # For simplicity, we'll use the current remaining capacity as a proxy for scarcity.\n    # Bins with less remaining capacity are considered more scarce.\n    scarcity_scores = scarcity_penalty_factor * (1.0 / (fitting_bins_caps + 1e-9))\n\n    # Combine tight fit preference with scarcity penalty.\n    # We want to favor tight fits, so tight_fit_scores are generally good.\n    # Scarcity scores penalize bins, so we subtract them.\n    # The `tight_fit_scores` should dominate, so we might need to tune the factor.\n    # Let's refine: we want to increase preference for tight fits,\n    # and decrease preference for bins that are already scarce (low remaining capacity).\n    # So, we'll combine the \"goodness\" of the fit with a penalty for scarcity.\n    # A better approach might be to prioritize bins that have *just enough* space\n    # but also have substantial remaining capacity for future items.\n    # Let's try a score that rewards tight fits and penalizes low remaining capacity.\n\n    # Calculate preference for tight fit (higher is better)\n    fit_preference = 1.0 / (fitting_bins_caps - item + 1e-9)\n\n    # Calculate penalty for scarcity (lower remaining capacity is worse)\n    # We want to *discourage* using very full bins, so higher scarcity_penalty is worse.\n    # Let's invert it to get a desirability score: bins with more remaining capacity are more desirable.\n    scarcity_desirability = fitting_bins_caps\n\n    # Combine them: prioritize tight fits, but also consider overall bin capacity.\n    # A good heuristic might be to prioritize bins that are tight fits BUT still have a decent amount of space left.\n    # This is a bit contradictory. Let's re-think the reflection.\n    # \"prioritize tight fits; explore with softmax; consider bin scarcity.\"\n    # Bin scarcity means we don't want to fill up bins too quickly if we have many options.\n    # This implies favoring bins that have more space.\n\n    # Let's re-frame: score should be high for bins that are tight fits AND have sufficient capacity remaining.\n    # Score = (1 / (remaining_capacity - item)) * (remaining_capacity)  -- this amplifies larger capacities for tight fits.\n    # Or, to favor tight fits more strongly, we can make the second term more influential.\n    # Let's try a weighted sum or multiplicative approach.\n\n    # Option 1: Weighted sum, favoring tight fit, penalizing scarcity (low remaining capacity)\n    # score = weight_fit * fit_preference - weight_scarcity * (1 / (fitting_bins_caps + 1e-9))\n    # This means a very low remaining capacity (high scarcity) will result in a more negative score.\n\n    # Option 2: Consider the remaining capacity after packing. We want this to be small (tight fit).\n    # And we also want the *original* remaining capacity to be not too small (to avoid scarcity).\n    # Let's define a score that is high for bins that are tight AND have ample space.\n    # This sounds like prioritizing bins that can fit the item with minimal waste, but also aren't already almost full.\n\n    # Let's use a score that prioritizes tight fits, but also gives a slight boost to bins with more remaining capacity.\n    # This can be achieved by adding a term related to the remaining capacity itself.\n    # The intuition is: prefer tight fits, but if multiple bins offer tight fits, pick the one that leaves more space.\n    # This might be counter-intuitive to \"tight fit\" as it encourages leaving more space.\n\n    # Let's go back to the core idea: minimize wasted space. Tightest fits do this.\n    # Bin scarcity: don't fill up bins too quickly. This means maybe spreading items across bins.\n    # If we have many bins with similar tight fits, we might want to pick the one with more original capacity.\n\n    # Let's combine:\n    # 1. Tightness of fit: `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` (higher is better)\n    # 2. Bin capacity: `bins_remain_cap[can_fit_mask]` (higher is better, to avoid scarcity)\n    # We can weight these. A simple product might work: `tight_fit_scores * bins_remain_cap[can_fit_mask]`\n    # Or a weighted sum: `w1 * tight_fit_scores + w2 * bins_remain_cap[can_fit_mask]`\n    # Let's try a multiplicative approach that rewards tightness and penalizes low capacity.\n    # The previous `priority_v1` used `1.0 / (fitting_bins_caps - item + 1e-9)`. Let's modify that.\n\n    # Consider the value of a bin.\n    # A bin is good if it fits the item snugly.\n    # A bin is also good if it has a lot of remaining capacity for future items.\n    # Let's try to combine these. The \"value\" could be related to the remaining capacity *after* packing.\n    # We want this to be small. So, `remaining_capacity - item`.\n    # But we also want to avoid scarcity, meaning bins with very low original capacity are less desirable.\n\n    # Let's refine the score:\n    # We want bins where `bins_remain_cap - item` is small (tight fit).\n    # We also want bins where `bins_remain_cap` is not too small (to avoid scarcity).\n    # A score that rewards small `bins_remain_cap - item` and penalizes small `bins_remain_cap` could be:\n    # `score = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9) * (bins_remain_cap[can_fit_mask] / BIN_CAPACITY)`\n    # If we don't know BIN_CAPACITY, we can use the average remaining capacity, or simply the remaining capacity itself.\n    # Let's use the remaining capacity directly as a multiplier for the tight-fit score.\n    # This gives higher scores to tight fits in bins that also have more space overall.\n\n    # Refined score: prioritize tight fits, but if multiple bins are tightly fitting,\n    # prefer the one that still has more capacity.\n    # This implicitly encourages spreading items if fits are equally tight.\n    combined_scores = (1.0 / (fitting_bins_caps - item + 1e-9)) * fitting_bins_caps\n\n    # Apply softmax\n    if np.sum(combined_scores) > 0:\n        max_score = np.max(combined_scores)\n        exp_scores = np.exp((combined_scores - max_score) / temperature)\n        sum_exp_scores = np.sum(exp_scores)\n\n        if sum_exp_scores > 0:\n            priorities[can_fit_mask] = exp_scores / sum_exp_scores\n        else:\n            # Fallback if exp_scores sum to zero (e.g., extreme values or temperature)\n            num_fitting_bins = np.sum(can_fit_mask)\n            if num_fitting_bins > 0:\n                priorities[can_fit_mask] = 1.0 / num_fitting_bins\n    else:\n        # If no fitting bins or scores are zero, priorities remain zero.\n        pass\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "SLOC": 26.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response5.txt_stdout.txt",
    "code_path": "problem_iter4_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This heuristic prioritizes bins that can fit the item and are \"nearly full\"\n    after the item is placed, encouraging tighter packing. It also incorporates a\n    mechanism to slightly favor bins that have been used less (i.e., have more\n    remaining capacity *before* adding the item) if they still offer a good fit,\n    to promote better distribution and avoid premature bin exhaustion.\n    The priority is calculated based on the remaining capacity after placing the item,\n    inverted to make smaller remaining capacities yield higher priorities.\n    A small additive term is used to break ties, favoring bins that had more\n    initial capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a more desirable bin. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    # Calculate remaining capacity after placing the item for all bins\n    potential_remaining_caps = bins_remain_cap - item\n\n    # Initialize priorities to a very low value (or 0) for bins that cannot fit\n    # This ensures they are never chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = potential_remaining_caps >= 0\n\n    # For bins that can fit, calculate priority:\n    # The core idea is to prioritize bins where the remaining capacity after placing\n    # the item is minimized (best fit). We invert this to get a higher score for\n    # a tighter fit.\n    # We add a small penalty based on the *original* remaining capacity. This is\n    # to slightly favor bins that had more capacity initially if they still provide\n    # a good fit. This can help in distributing items better.\n    # The inversion of `potential_remaining_caps` means smaller values are better.\n    # Adding a small positive value for `bins_remain_cap` can slightly break ties.\n    # `bins_remain_cap` is used here for its larger values to be slightly preferred\n    # when the `potential_remaining_caps` are equal or very close.\n    # A simple approach is to use the negative of `potential_remaining_caps`\n    # and then add a small bonus for larger `bins_remain_cap`.\n\n    # We want to maximize (smaller remaining capacity after fit) AND (larger initial capacity if fits are similar)\n    # So, we can aim for a score like: -potential_remaining_caps + C * bins_remain_cap\n    # Where C is a small positive constant to give weight to original capacity.\n    # Let's normalize the original capacity to avoid it dominating too much.\n    # A simpler approach without explicit normalization:\n    # Prioritize by the inverse of (remaining capacity after fit + small epsilon)\n    # and add a small bonus for larger original capacity.\n    # Let's use the negative remaining capacity as primary driver and add the original capacity as a tie-breaker.\n\n    if np.any(can_fit_mask):\n        # Calculate the primary score: inverse of remaining capacity after fit.\n        # Add a small epsilon to avoid division by zero and to give a slight preference\n        # to bins that are not *perfectly* full (though this is subtle and may need tuning).\n        # A more direct way for \"best fit\" is to use the negative of the remaining capacity.\n        # Negative remaining capacity: lower value means better fit.\n        # To get higher priority, we invert it: 1 / (residual + epsilon) or similar.\n        # Let's stick to the negative remaining capacity and add original capacity.\n        \n        # Primary scoring: -potential_remaining_caps. Smaller (more negative) is better.\n        # To convert to higher priority: use positive values.\n        # We can use `max_residual - residual` or `1 / (residual + epsilon)`.\n        # Using `-potential_remaining_caps` directly works if we interpret higher values as better.\n        # Let's refine to make it clearer: High priority means a good fit.\n        # Good fit = small `potential_remaining_caps`.\n        # So, let's use `some_large_value - potential_remaining_caps`.\n        # Example: Bin Capacity = 10. Item = 3. Remaining_cap = 7.\n        # If item = 7, Remaining_cap = 3. This is a better fit.\n        # Score for 7: large_val - 3. Score for 3: large_val - 7. Higher score for better fit.\n\n        # Let's use a scaled inverse of the remaining capacity after fit.\n        # `1 / (potential_remaining_caps + 1e-6)` would give higher score to smaller remaining.\n        # To incorporate original capacity as a secondary criterion:\n        # We want to maximize (-potential_remaining_caps) primarily, and (bins_remain_cap) secondarily.\n        # A common way is to combine them linearly, e.g., `a * (-potential_remaining_caps) + b * bins_remain_cap`.\n        # For simplicity and direct \"best fit\" interpretation:\n        # Score = `(max_possible_residual - potential_remaining_caps)` + `0.01 * bins_remain_cap`\n        # where `max_possible_residual` is the maximum possible remaining capacity for a fit.\n        # Alternatively, using the negative remaining capacity is good for sorting.\n        \n        # Let's use `1.0 / (potential_remaining_caps + 1e-6)` for primary priority\n        # and add a small term related to original capacity.\n        \n        # If we want to prioritize bins that are *closer* to fitting the item perfectly,\n        # `potential_remaining_caps` should be as close to 0 as possible.\n        # So, `1.0 / (potential_remaining_caps + epsilon)` works well for that.\n        \n        # Consider `potential_remaining_caps = [2, 0, 3]`\n        # Scores: `1/2.000001`, `1/0.000001`, `1/3.000001` -> approx `0.5`, `1000000`, `0.33`\n        # This clearly prioritizes the best fit.\n        \n        # Now, adding the secondary criterion: slight preference for bins with more original capacity.\n        # Let's normalize `bins_remain_cap` to a small range, e.g., [0, 1] or [0, 0.1]\n        # to avoid it overpowering the primary criterion.\n        \n        # Let's combine: priority = (1 / (potential_remaining_caps + epsilon)) + (bins_remain_cap / MAX_CAPACITY_OR_MEAN)\n        # For simplicity, we can just add a scaled version of original capacity.\n        \n        # Final approach:\n        # Primary: Maximize `-potential_remaining_caps`. (Best fit)\n        # Secondary: Maximize `bins_remain_cap`. (Slight preference for larger original capacity bins)\n        # Combine: `priorities[can_fit_mask] = -potential_remaining_caps[can_fit_mask] + 0.1 * bins_remain_cap[can_fit_mask]`\n        # The `0.1` factor is heuristic. It means a difference of 1 in `potential_remaining_caps`\n        # is equivalent to a difference of 10 in `bins_remain_cap`. This gives strong preference\n        # to best fit.\n\n        priorities[can_fit_mask] = -potential_remaining_caps[can_fit_mask] + 0.05 * bins_remain_cap[can_fit_mask]\n        \n    # Ensure bins that can't fit have a priority of -inf (or a very small number)\n    # We already initialized to -inf, so this step is covered.\n    # If we were using 0 for non-fitting bins, we would need to ensure it.\n\n    # Handle the case where all potential priorities are -inf (no bin can fit)\n    if not np.any(np.isfinite(priorities)):\n        return np.zeros_like(bins_remain_cap) # Return all zeros if no bin can fit\n\n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response3.txt_stdout.txt",
    "code_path": "problem_iter9_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority scores for bins, combining \"best fit\" and \"scarcity\"\n    using a weighted sum, and then normalizing using Softmax for probabilistic\n    selection interpretation.\n\n    This heuristic prioritizes bins based on two criteria:\n    1. Best Fit: Bins where the remaining capacity is closest to the item size,\n       minimizing wasted space.\n    2. Scarcity: Bins with less overall remaining capacity are preferred, as they\n       are more \"scarce\".\n\n    The scores are combined using weights, and then a Softmax function is applied\n    with a temperature parameter. This allows for balancing exploitation (picking\n    the best bin) and exploration (giving a chance to less optimal bins).\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is the normalized priority score for the corresponding bin. Bins that\n        cannot fit the item will have a priority of 0.\n    \"\"\"\n    \n    # Parameters for balancing fit and scarcity, and for Softmax temperature\n    w_fit = 0.7       # Weight for the \"best fit\" component (0.0 to 1.0)\n    w_scarcity = 0.3  # Weight for the \"scarcity\" component (0.0 to 1.0)\n    temperature = 1.0 # Softmax temperature: higher -> more exploration, lower -> more exploitation\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    # If no bin can fit the item, return all zeros\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n    # --- Calculate \"Best Fit\" Score ---\n    # We want to maximize this score when (remaining_capacity - item) is small.\n    # A simple function is 1 / (1 + waste).\n    # waste = suitable_bins_cap - item\n    # fit_scores = 1.0 / (1.0 + waste)\n    # This gives a score of 1.0 for a perfect fit, and approaches 0 for large waste.\n    waste = suitable_bins_cap - item\n    \n    # To avoid potential division by zero if waste were negative (not possible here)\n    # or if we wanted to cap the score range, we can add a small epsilon or use np.maximum.\n    # For robustness, ensure waste is non-negative (already guaranteed by suitable_bins_mask)\n    # and handle potential very large waste that could make 1/(1+waste) extremely small.\n    # We can clip the waste to avoid underflow issues with its reciprocal,\n    # although with typical capacities, this is less of a concern.\n    # Let's use np.maximum to ensure the denominator is at least 1.\n    fit_scores = 1.0 / (1.0 + np.maximum(0, waste)) # Ensure waste is non-negative\n\n    # --- Calculate \"Scarcity\" Score ---\n    # We want to maximize this score when remaining_capacity is small.\n    # A simple function is 1 / (1 + remaining_capacity).\n    # scarcity_scores = 1.0 / (1.0 + suitable_bins_cap)\n    # This gives a score of 1.0 for a bin with 0 remaining capacity, and approaches 0 for large capacities.\n    scarcity_scores = 1.0 / (1.0 + np.maximum(0, suitable_bins_cap)) # Ensure capacity is non-negative\n\n    # --- Combine Scores ---\n    # Weighted sum of fit and scarcity scores\n    combined_scores = (w_fit * fit_scores) + (w_scarcity * scarcity_scores)\n\n    # --- Apply Softmax for Probabilistic Priorities ---\n    # Divide by temperature to control the sharpness of the distribution.\n    # Higher temperature -> probabilities are more uniform (more exploration).\n    # Lower temperature -> probabilities are more skewed towards high scores (more exploitation).\n    if temperature <= 0:\n        # Handle non-positive temperature by making it very small positive\n        # or by returning argmax-like behavior (deterministic best choice).\n        # For now, let's treat temperature <= 0 as effectively zero temperature\n        # which would make the softmax behave like an argmax if not handled.\n        # A robust softmax implementation typically expects positive temperature.\n        # If temperature is 0, exp(score/0) is inf for max score, 0 for others.\n        # This would lead to a single bin having probability 1.\n        # For this function, returning raw scores before softmax for T=0 makes sense.\n        # Or, if we must use softmax, a very small positive number.\n        effective_temperature = 1e-9 # Small positive to avoid division by zero\n    else:\n        effective_temperature = temperature\n\n    # Calculate exponent terms: scores / temperature\n    exponent_terms = combined_scores / effective_temperature\n    \n    # Calculate Softmax probabilities\n    # To avoid overflow with large exponent_terms, subtract the maximum value.\n    # max_exponent = np.max(exponent_terms)\n    # exp_values = np.exp(exponent_terms - max_exponent)\n    # probabilities = exp_values / np.sum(exp_values)\n    \n    # Using scipy.special.softmax is more numerically stable\n    try:\n        from scipy.special import softmax\n        # Softmax requires a 1D array\n        probabilities = softmax(exponent_terms)\n    except ImportError:\n        # Fallback if scipy is not available\n        # Stable softmax implementation\n        max_val = np.max(exponent_terms)\n        exps = np.exp(exponent_terms - max_val)\n        probabilities = exps / np.sum(exps)\n        \n    # Assign the calculated probabilities to the priorities array\n    priorities[suitable_bins_mask] = probabilities\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "SLOC": 27.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response0.txt_stdout.txt",
    "code_path": "problem_iter5_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority scores for packing an item into bins using a refined\n    priority function incorporating Softmax for exploration, bin scarcity,\n    and tie-breaking.\n\n    This heuristic aims to balance tight fits with exploration and efficiency\n    by considering:\n    1.  **Tight Fit Score (Sigmoid):** Prioritizes bins with remaining capacity\n        closest to the item size, minimizing waste.\n    2.  **Bin Scarcity:** Favors bins that have less remaining capacity overall,\n        as these are \"scarcer\" resources. This is modeled using the inverse\n        of remaining capacity.\n    3.  **Softmax for Probabilistic Selection:** Uses Softmax to convert scores\n        into probabilities, allowing for exploration of less optimal bins.\n        The temperature parameter controls the degree of exploration.\n    4.  **Tie-breaking:** Implicitly handled by the sorting order or original\n        index if scores are identical, favoring bins that appear earlier in\n        the array when scores are equal.\n\n    The combined priority for a suitable bin is a weighted sum of the tight\n    fit score and the bin scarcity score, transformed by Softmax.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is the probability score (from Softmax) for the corresponding bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # --- Heuristic Component 1: Tight Fit Score (Sigmoid) ---\n    # Parameter for the sigmoid function's steepness.\n    # Higher k means sharper preference for tighter fits.\n    k_fit = 5.0\n    mismatch = suitable_bins_cap - item\n    \n    # Use a capped sigmoid for numerical stability\n    max_exponent_arg = 35.0\n    capped_exponent_arg = np.minimum(k_fit * mismatch, max_exponent_arg)\n    tight_fit_scores = 1 / (1 + np.exp(capped_exponent_arg))\n\n    # --- Heuristic Component 2: Bin Scarcity Score ---\n    # Prioritize bins with less remaining capacity (scarcer bins).\n    # Using 1 / (capacity + epsilon) to avoid division by zero and give higher score to smaller capacities.\n    # Adding a small epsilon to avoid division by zero if a bin has 0 capacity (though unlikely if it fits the item).\n    epsilon = 1e-6\n    scarcity_scores = 1 / (suitable_bins_cap + epsilon)\n    \n    # Normalize scarcity scores to be comparable to tight_fit_scores (0 to 1 range)\n    # A simple min-max scaling can work.\n    min_scarcity = np.min(scarcity_scores)\n    max_scarcity = np.max(scarcity_scores)\n    if max_scarcity - min_scarcity > epsilon: # Avoid division by zero if all scarcity scores are the same\n        normalized_scarcity_scores = (scarcity_scores - min_scarcity) / (max_scarcity - min_scarcity)\n    else:\n        normalized_scarcity_scores = np.zeros_like(scarcity_scores) # Or assign 0.5 if all are equal and non-zero\n\n    # --- Combine Heuristics ---\n    # Weighted sum of tight fit and scarcity. Weights can be tuned.\n    # Here, we give equal weight, but this could be adjusted.\n    weight_fit = 0.5\n    weight_scarcity = 0.5\n    \n    combined_scores = (weight_fit * tight_fit_scores) + (weight_scarcity * normalized_scarcity_scores)\n\n    # --- Softmax for Probabilistic Exploration ---\n    # Temperature parameter: higher temp -> more exploration (probabilities closer to uniform)\n    # lower temp -> less exploration (probabilities closer to argmax)\n    temperature = 0.5  # Tunable parameter\n\n    # Apply Softmax\n    # Softmax(z)_i = exp(z_i / T) / sum(exp(z_j / T))\n    # Ensure combined_scores are not excessively large to avoid exp overflow even after capping.\n    # A common practice is to subtract the maximum score before exponentiation for numerical stability.\n    scores_for_softmax = combined_scores / temperature\n    \n    # Subtract max score for numerical stability in exp\n    stable_scores = scores_for_softmax - np.max(scores_for_softmax)\n    \n    exp_scores = np.exp(stable_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Place the calculated probabilities back into the main priorities array\n    priorities[suitable_bins_mask] = probabilities\n\n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "SLOC": 30.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response1.txt_stdout.txt",
    "code_path": "problem_iter5_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a\n    softmax-based approach that considers tight fits, bin scarcity, and tie-breaking.\n\n    This heuristic prioritizes bins that offer a \"tight fit\" for an incoming item,\n    minimizing wasted space. It also incorporates a preference for bins with less\n    remaining capacity (i.e., scarcer bins) and uses a softmax function to convert\n    these scores into probabilities, allowing for probabilistic exploration.\n\n    The score for each bin is calculated as follows:\n    1. For bins that cannot fit the item, the score is 0.\n    2. For bins that can fit the item, the score is influenced by:\n       a. Tightness: A score that is higher for smaller remaining capacities\n          (after fitting the item). This is calculated using a sigmoid-like\n          function `exp(-k * (remaining_capacity - item))`. A perfect fit\n          (remaining_capacity - item = 0) gets a score of 1.\n       b. Scarcity: A score that is higher for bins with less remaining capacity overall.\n          This is calculated as `exp(-s * remaining_capacity)`.\n       c. Tie-breaking: A small preference for earlier bins in the array. This is\n          handled implicitly by the softmax or can be explicitly added as a small bonus.\n          Here, we'll implicitly favor earlier bins in the softmax if scores are equal.\n\n    The final priority for each bin is determined by applying a softmax function\n    to a combined score. The combined score for a suitable bin `i` is:\n    `combined_score_i = tightness_score_i + scarcity_score_i`\n\n    Softmax function: `priority_i = exp(combined_score_i / temperature) / sum(exp(combined_score_j / temperature))`\n    where the sum is over all suitable bins `j`.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is the priority score (probability) for the corresponding bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_indices = np.where(suitable_bins_mask)[0]\n\n    # If no bin can fit the item, return all zeros\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # --- Tunable Parameters ---\n    # k: Controls the preference for tight fits. Higher k means stronger preference for exact fits.\n    #    A small positive value is used so that `exp(-k * mismatch)` is high when mismatch is small.\n    k_tightness = 10.0\n    # s: Controls the preference for scarce bins (bins with less remaining capacity).\n    #    Higher s means stronger preference for bins that are already more full.\n    s_scarcity = 0.1\n    # temperature: Controls the exploration vs. exploitation trade-off in softmax.\n    #    Lower temperature means more exploitation (picking the highest scored bin).\n    #    Higher temperature means more exploration (more uniform probability distribution).\n    temperature = 1.0\n    # -------------------------\n\n    # Calculate scores for suitable bins\n    # Score component 1: Tightness (prefer smaller remaining capacity after packing)\n    # We want `remaining_capacity - item` to be small.\n    # Using `exp(-k * (remaining_capacity - item))` means:\n    # - Perfect fit (diff=0): exp(0) = 1 (highest tightness score)\n    # - Larger diff: exp(-positive) < 1 (lower tightness score)\n    mismatch = suitable_bins_cap - item\n    # Cap mismatch to prevent extreme negative exponents if `k` is large and mismatch is very small\n    # (though mismatch should be non-negative here by definition).\n    # More importantly, cap the argument to `exp` to avoid potential issues if `k` is large.\n    # A negative exponent in `exp` is generally safe.\n    tightness_scores = np.exp(-k_tightness * mismatch)\n\n    # Score component 2: Scarcity (prefer bins with less remaining capacity overall)\n    # We want `remaining_capacity` to be small.\n    # Using `exp(-s * remaining_capacity)` means:\n    # - Smaller capacity: exp(-small_positive) closer to 1 (higher scarcity score)\n    # - Larger capacity: exp(-large_positive) closer to 0 (lower scarcity score)\n    scarcity_scores = np.exp(-s_scarcity * suitable_bins_cap)\n\n    # Combined score: Sum of tightness and scarcity preferences\n    combined_scores = tightness_scores + scarcity_scores\n\n    # Apply softmax to get probabilities\n    # Normalize scores by temperature to control exploration\n    # Ensure scores are finite before applying softmax\n    exp_scores = np.exp(combined_scores / temperature)\n\n    # Handle potential infinities or NaNs if combined_scores are too large/small\n    # Although unlikely with exp(-k*mismatch) and exp(-s*capacity) if k, s, cap are reasonable.\n    # If exp_scores become inf, we might want to rescale or cap them.\n    # For simplicity, assuming reasonable inputs for now.\n    \n    # Calculate probabilities using softmax\n    softmax_probs = exp_scores / np.sum(exp_scores)\n\n    # Place the calculated probabilities back into the main priorities array\n    priorities[suitable_indices] = softmax_probs\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response2.txt_stdout.txt",
    "code_path": "problem_iter5_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined priority strategy.\n\n    This heuristic prioritizes bins based on several factors:\n    1.  **Tight Fit:** Prioritizes bins that have just enough remaining capacity for the item,\n        minimizing waste.\n    2.  **Bin Scarcity:** Prefers bins that have less remaining capacity overall, making them\n        \"scarcer\" and potentially needing to be filled to capacity sooner.\n    3.  **Earlier Bin Preference (Tie-breaking):** If multiple bins offer similar priority,\n        bins that appear earlier in the `bins_remain_cap` array are slightly preferred.\n    4.  **Probabilistic Exploration (Softmax):** Uses Softmax to convert scores into probabilities,\n        allowing for probabilistic selection. This balances exploitation (choosing the best fit)\n        with exploration (trying less optimal bins occasionally).\n\n    The scoring mechanism combines the tight-fit score with a scarcity factor.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, representing the\n        probability (or weighted priority) of selecting each bin. Bins that cannot\n        fit the item will have a probability of 0.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities  # No bin can fit the item\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    # --- Scoring Components ---\n\n    # 1. Tight Fit Score (Sigmoid-based, similar to v1 but adjusted for Softmax)\n    # We want a score that is high for tight fits and decreases as capacity increases.\n    # A score close to 1 for perfect fit, decreasing towards 0.\n    # Let's use `1 / (1 + exp(k * (capacity - item)))`\n    # For perfect fit (capacity - item = 0), score = 0.5\n    # For capacity - item > 0, score decreases.\n    # To work well with Softmax, scores should be non-negative.\n    # We can shift the sigmoid output: `sigmoid_score = 1 / (1 + exp(k * mismatch))`.\n    # A perfect fit gives 0.5. A slightly larger fit gives slightly less than 0.5.\n    # Let's ensure scores are always positive. Maybe add a small constant or use a different transformation.\n    # Alternative: `exp(-k * mismatch)`. Perfect fit = exp(0) = 1. Larger mismatch = smaller score.\n    # This aligns better with Softmax. Let's use this.\n    k_tightness = 5.0  # Sensitivity to tightness. Higher k -> stronger preference for tight fits.\n    mismatch = suitable_bins_cap - item\n    \n    # Cap exponent to prevent overflow/underflow issues with exp, especially for large k or mismatch\n    max_exponent_val = 700.0 # exp(700) is very large\n    min_exponent_val = -700.0 # exp(-700) is very close to 0\n    \n    tightness_scores = np.exp(-k_tightness * mismatch)\n    \n    # Ensure scores are within a reasonable range if needed, though exp(-x) is usually fine.\n    # For robustness, one could clamp the argument to exp.\n    # capped_exponent = np.clip(-k_tightness * mismatch, min_exponent_val, max_exponent_val)\n    # tightness_scores = np.exp(capped_exponent)\n\n\n    # 2. Bin Scarcity Score\n    # Prioritize bins with less remaining capacity. A simple inverse relationship works.\n    # Add a small epsilon to avoid division by zero if a bin somehow has 0 capacity.\n    epsilon = 1e-6\n    scarcity_scores = 1.0 / (suitable_bins_cap + epsilon)\n    \n    # Normalize scarcity scores to be in a similar range or complement tightness score.\n    # Let's combine them additively, scaled appropriately.\n    # We want tight fit to dominate, but scarcity to break ties or influence choices.\n    # A simple weighted sum: `score = w1 * tightness + w2 * scarcity`\n    # Or, since both are positive and indicate preference, we can multiply them\n    # if we want them to be strong together, or add if we want them to be independent factors.\n    # Let's try combining them additively, scaled to avoid one dominating too much.\n    \n    # Normalize scores before combining to manage scales:\n    # Max possible tightness score is 1 (perfect fit). Min depends on max mismatch.\n    # Max possible scarcity score is 1/epsilon. Min is 1/max_capacity.\n    # This suggests scarcity might dominate if not scaled.\n    \n    # Let's scale scarcity by the inverse of the typical bin capacity or by a factor.\n    # A simpler approach: add a \"bonus\" for being scarce.\n    # Consider the total capacity of suitable bins. A bin with less capacity is scarcer.\n    # Let's scale scarcity scores based on the mean capacity of suitable bins.\n    # Or, we can think of scarcity as a \"bonus\" for having less space.\n    # Let's try making scarcity a multiplier for tightness, but capped.\n    # Or, add scarcity as a bonus.\n    \n    # Let's use additive combination with some scaling for scarcity.\n    # Scarcity score: Higher for less capacity.\n    # We want to favor bins that are nearly full.\n    # Consider the remaining capacity relative to the item size.\n    # Alternative scarcity: `1.0 - (suitable_bins_cap / max_possible_capacity)`\n    # Or simply use the inverse: `1.0 / suitable_bins_cap`\n    \n    # Let's normalize the scarcity scores to be between 0 and 1.\n    min_cap = np.min(suitable_bins_cap)\n    max_cap = np.max(suitable_bins_cap)\n    \n    if max_cap - min_cap > epsilon: # Avoid division by zero if all capacities are the same\n        normalized_scarcity = (suitable_bins_cap - min_cap) / (max_cap - min_cap)\n        # Invert for scarcity: higher score for less capacity\n        scarcity_bonus = 1.0 - normalized_scarcity\n    else:\n        scarcity_bonus = np.ones_like(suitable_bins_cap) * 0.5 # All bins equally scarce/plentiful\n\n    # 3. Combine tightness and scarcity\n    # Weighted sum: prioritize tightness, add scarcity as a secondary factor.\n    weight_tightness = 1.0\n    weight_scarcity = 0.3 # Give scarcity a moderate influence\n    \n    combined_scores = (weight_tightness * tightness_scores) + (weight_scarcity * scarcity_bonus)\n\n    # 4. Tie-breaking (Earlier Bin Preference)\n    # Add a small bonus based on index.\n    # The index itself can be used, scaled down.\n    index_bonus_scale = 0.01\n    index_bonus = suitable_bins_indices * index_bonus_scale\n    \n    final_scores = combined_scores + index_bonus\n\n    # Apply Softmax to get probabilities\n    # Softmax is `exp(score) / sum(exp(scores))`\n    # We need to handle potential large values in `final_scores` causing `exp` overflow.\n    # A common trick is to subtract the maximum score before exponentiating.\n    # `exp(x_i - max(x)) / sum(exp(x_j - max(x)))`\n    # This doesn't change the resulting probabilities.\n    \n    if final_scores.size > 0:\n        max_score = np.max(final_scores)\n        # Ensure scores passed to exp are not excessively large after subtraction\n        # Although subtracting max should handle it, numerical stability can be an issue.\n        # Let's cap the intermediate values before exp.\n        \n        shifted_scores = final_scores - max_score\n        # Cap shifted scores to prevent exp overflow even after subtraction if differences are large\n        # A value like 50-100 is usually safe.\n        capped_shifted_scores = np.clip(shifted_scores, -100.0, 100.0)\n        \n        exp_scores = np.exp(capped_shifted_scores)\n        sum_exp_scores = np.sum(exp_scores)\n        \n        if sum_exp_scores > 0:\n            probabilities = exp_scores / sum_exp_scores\n        else:\n            # This case should ideally not happen if scores are finite, but for safety:\n            probabilities = np.ones_like(exp_scores) / len(exp_scores) if len(exp_scores) > 0 else np.array([])\n    else:\n        probabilities = np.array([])\n\n    # Place the calculated probabilities back into the main priorities array\n    priorities[suitable_bins_mask] = probabilities\n\n    return priorities",
    "response_id": 2,
    "obj": 4.1284403669724865,
    "SLOC": 42.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response3.txt_stdout.txt",
    "code_path": "problem_iter5_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a Softmax-based approach\n    that prioritizes tight fits, considers bin scarcity, and encourages earlier bin usage.\n\n    This heuristic aims to balance several factors:\n    1.  **Tight Fit Preference:** Prioritize bins that leave minimal remaining capacity\n        after packing the item. This is achieved by assigning higher scores to bins\n        where `remaining_capacity - item` is small and non-negative.\n    2.  **Bin Scarcity/Fullness:** Bins that are already quite full (i.e., have less\n        remaining capacity) should generally be preferred over those with ample space,\n        as this helps in utilizing bins more efficiently and potentially opening up\n        space in other bins for larger items later.\n    3.  **Exploration (Softmax):** Using Softmax allows for probabilistic selection,\n        meaning even bins that are not the absolute \"best fit\" have a chance of being chosen.\n        This can prevent getting stuck in local optima and discover better packings.\n    4.  **Tie-breaking:** Implicitly, bins that are encountered earlier in the `bins_remain_cap`\n        array might receive slightly higher priority if their scores are identical to later bins,\n        due to how the scores are processed or due to inherent ordering.\n\n    The scoring for suitable bins is designed as follows:\n    For bins where `remaining_capacity >= item`:\n    - We calculate a \"fit score\" based on how tightly the item fits. A simple inverse\n      relationship with the remaining capacity is used, favoring smaller capacities.\n    - A \"scarcity score\" related to the inverse of remaining capacity can be incorporated.\n    - A common approach for balancing is to use a weighted sum or a transformation\n      that combines these aspects.\n\n    Here, we'll adapt a common approach for online bin packing heuristics that focuses on\n    the \"best fit\" principle, but modulated by Softmax for exploration.\n    A common score for \"best fit\" is the difference `remaining_capacity - item`.\n    We want to *minimize* this difference. For Softmax, we need to transform this into\n    scores where higher means more preferred.\n\n    We'll use a score inversely related to the remaining capacity for suitable bins.\n    A simple heuristic could be `1 / remaining_capacity`.\n    To incorporate the \"tight fit\" preference more directly, we can consider the\n    inverse of `(remaining_capacity - item) + epsilon` to avoid division by zero,\n    or use a sigmoid-like function on this difference.\n\n    Let's refine the reflection's idea:\n    - **Tight Fit Score:** `1 / (1 + exp(k * (remaining_capacity - item)))` as in v1. This assigns\n      higher scores to smaller positive differences.\n    - **Exploration (Softmax):** Apply Softmax to these scores to get probabilities.\n    - **Bin Scarcity/Earlier Bin Preference:** The original reflection mentioned \"earlier bin preference\".\n      This can be achieved by adding a small constant to the score of earlier bins, or by\n      ranking bins and adding a bonus based on rank. For simplicity and focusing on the\n      fit/scarcity, we'll rely on Softmax's inherent exploration. The \"scarcity\" is implicitly\n      handled by favoring bins with less `remaining_capacity`.\n\n    Let's re-evaluate the scoring to directly favor smaller remaining capacities for Softmax.\n    A score like `max_capacity - remaining_capacity` would favor fuller bins.\n    However, we also need the item to fit.\n\n    Consider the following score for suitable bins:\n    `score = some_function(remaining_capacity - item)`\n    We want `remaining_capacity - item` to be small.\n    Let's try `score = - (remaining_capacity - item)` which directly rewards smaller differences.\n    To make it suitable for Softmax (where we want positive scores), we can use:\n    `score = C - (remaining_capacity - item)` where C is a large constant.\n    Or, perhaps more intuitively, we want to *minimize* `remaining_capacity`.\n    So, a score proportional to `-remaining_capacity` or `max_capacity - remaining_capacity`\n    could work.\n\n    Let's combine:\n    1. Prioritize bins where `remaining_capacity` is small (scarcity/fullness).\n    2. Among those, prioritize tighter fits (`remaining_capacity - item` is small).\n\n    A score that captures this is to prioritize bins with smaller `remaining_capacity`.\n    So, `score = -remaining_capacity`. For Softmax, we'd use `np.exp(-remaining_capacity)`.\n    However, this doesn't directly incorporate the item size into the fit.\n\n    Let's use the \"tight fit\" score from v1, but apply Softmax to it, potentially\n    adding a small boost for bins that are generally less full (though the prompt\n    suggests prioritizing tighter fits and scarcity, which might conflict).\n\n    Let's focus on the reflection's core idea: tight fits and minimal waste,\n    and Softmax for exploration.\n    A score that reflects tight fit is `1 / (1 + exp(k * (remaining_capacity - item)))`.\n    To make this suitable for Softmax (where larger is better), we can directly use this value.\n    Bins that cannot fit the item get a score of 0.\n\n    We'll use a parameter `temperature` for the Softmax function to control exploration.\n    A higher temperature leads to more uniform probabilities (more exploration).\n    A lower temperature leads to more concentrated probabilities on the highest scores (more exploitation).\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is the probability score for the corresponding bin, derived from Softmax.\n        Bins that cannot fit the item will have a score of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n    # If no bin can fit the item, return all zeros\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # --- Scoring Mechanism ---\n    # We want to assign higher scores to bins that offer a tighter fit.\n    # The difference `remaining_capacity - item` represents the wasted space.\n    # We want to minimize this difference.\n    # A score that rewards smaller differences is `1 / (1 + exp(k * (remaining_capacity - item)))`.\n    # Let's define k. A higher k emphasizes tighter fits.\n    k = 5.0  # Sensitivity parameter for the tight fit preference\n\n    # Calculate the \"mismatch\" or wasted space for suitable bins\n    mismatch = suitable_bins_cap - item\n\n    # Calculate the raw \"preference\" score. Higher means more preferred.\n    # Using the sigmoid score: 1 / (1 + exp(k * mismatch))\n    # This score is between 0 and 1. A perfect fit (mismatch=0) gives 0.5.\n    # Smaller positive mismatches give scores closer to 0.5.\n    # Larger positive mismatches give scores closer to 0.\n    # We want to favor smaller mismatches, so larger scores are better.\n    # The current sigmoid score `1/(1+exp(k*mismatch))` correctly assigns higher values to smaller `mismatch`.\n\n    # To prevent numerical issues with exp, cap the argument.\n    max_exponent_arg = 35.0 # Corresponds to exp(35)\n    \n    capped_exponent_arg = np.minimum(k * mismatch, max_exponent_arg)\n    \n    # The raw scores: higher values mean better fits (smaller mismatch)\n    raw_scores = 1 / (1 + np.exp(capped_exponent_arg))\n\n    # --- Softmax Application ---\n    # Apply Softmax to the raw scores to get probabilities.\n    # Softmax(z_i) = exp(z_i) / sum(exp(z_j))\n    # To control exploration, we can scale the scores by a temperature parameter.\n    # `temperature` > 0.\n    # If temperature is very low (e.g., close to 0), it's like argmax (exploitation).\n    # If temperature is high, it's like uniform distribution (exploration).\n    \n    # Let's use a tunable temperature. A value like 0.1-1.0 is common.\n    # If scores are already in [0,1], scaling might not be strictly necessary,\n    # but Softmax generally expects values that can be exponentiated.\n    # We can use the raw_scores directly or scale them.\n    # Scaling by temperature: `exp(score / temperature)`.\n    # Let's try a temperature that makes the differences more pronounced or smoothed.\n    # A temperature of 1.0 means we use the raw scores directly in exp.\n    # A temperature > 1 will smooth probabilities.\n    # A temperature < 1 will sharpen probabilities.\n\n    # Let's use temperature to control the \"sharpness\" of preference.\n    # A lower temperature will favor the best fits more strongly.\n    # A higher temperature will distribute preference more evenly.\n    # Let's start with a moderate temperature, e.g., 0.5, to slightly favor better fits.\n    temperature = 0.5 # Tunable parameter for exploration/exploitation balance\n\n    # Apply exponentiation with temperature scaling\n    # Ensure we don't have issues if temperature is very close to zero or zero.\n    # If temperature is very small, `raw_scores / temperature` can become very large.\n    # We can clip the scaled scores before exp to prevent overflow if temperature is tiny.\n    # However, if temperature is 0, this is problematic. Assume temperature > 0.\n\n    # Avoid division by zero if temperature is 0 or very small and scores are high.\n    # If temperature is very small, scores with slight differences will be amplified.\n    # Let's scale the scores by `1/temperature` before `exp` for a sharper distribution.\n    # Or scale by `temperature` for a smoother distribution. The reflection says \"tuning temperature for exploration/exploitation balance\".\n    # Higher temperature -> more exploration (smoother distribution).\n    # Lower temperature -> more exploitation (sharper distribution).\n\n    # We want to explore, so let's make temperature a factor that increases probability spread.\n    # Use `exp(score / temperature)` where higher temperature spreads probabilities.\n    # So, let's use `temperature = 0.5` (lower means sharper), `temperature = 2.0` (higher means flatter).\n    # Let's set temperature to 1.0 initially for no scaling, and test.\n    # If we want to favor tighter fits more, we want smaller `mismatch` to have higher probability.\n    # `raw_scores` are already designed for this. Softmax will spread these.\n    # Higher temperature -> more uniform probability distribution.\n    # Lower temperature -> probability concentrated on the highest `raw_scores`.\n\n    # Let's use a temperature that favors exploitation slightly, i.e., a lower temperature.\n    # A temperature around 0.1-0.5 might be good for demonstrating preference.\n    # Or, a temperature of 1.0 is standard softmax. Let's try to emphasize the preference.\n    # If `raw_scores` are e.g., [0.6, 0.5, 0.4], exp([0.6, 0.5, 0.4]) = [1.82, 1.65, 1.49]\n    # Sum = 4.96. Probs = [0.36, 0.33, 0.30]. Not very sharp.\n    # If we scale by 1/temp: temp=0.1 => exp([6, 5, 4]) = [403, 148, 54]. Sum = 605. Probs = [0.66, 0.24, 0.08]. Much sharper.\n\n    # Let's use temperature `T` in `exp(score / T)`.\n    # Smaller `T` leads to sharper probabilities.\n    temperature_param = 0.5 # Lower T = more exploitation of \"best fit\"\n\n    # Ensure scores are not excessively large before exp to prevent overflow.\n    # `raw_scores` are already capped at 1.\n    # If `temperature_param` is very small, `raw_scores / temperature_param` can still be large.\n    # Let's ensure `raw_scores / temperature_param` doesn't exceed a threshold before `exp`.\n    # For example, threshold of 35 for the exponent argument.\n    scaled_scores = raw_scores / temperature_param\n    \n    # Cap the scaled scores before exponentiation to prevent overflow\n    max_scaled_score_exponent = 35.0 \n    capped_scaled_scores = np.minimum(scaled_scores, max_scaled_score_exponent)\n    \n    # Calculate the exponentiated values\n    exponentials = np.exp(capped_scaled_scores)\n\n    # Calculate the sum of exponentiated values for normalization\n    sum_exponentials = np.sum(exponentials)\n\n    # Avoid division by zero if all exponentiated values are zero (e.g., due to capping or very low scores)\n    if sum_exponentials == 0:\n        # In this case, all suitable bins are equally (un)preferred or there was an issue.\n        # A fallback could be uniform probability among suitable bins, or all zeros if it implies no good fit.\n        # Given our scores are >= 0, sum_exponentials should only be 0 if exponentials are all 0.\n        # This might happen if all raw_scores were extremely small and negative after some transformation,\n        # or if capping resulted in zero exp. But our raw_scores are positive.\n        # If sum is 0, it means all `capped_scaled_scores` were -inf (which shouldn't happen here).\n        # As a fallback, assign equal probability to suitable bins if sum is zero.\n        if suitable_bins_cap.size > 0:\n            priorities[suitable_bins_mask] = 1.0 / suitable_bins_cap.size\n        return priorities\n\n    # Calculate the final probabilities using Softmax\n    softmax_probabilities = exponentials / sum_exponentials\n\n    # Place the calculated softmax probabilities back into the main priorities array\n    priorities[suitable_bins_mask] = softmax_probabilities\n\n    return priorities",
    "response_id": 3,
    "obj": 3.948942959712818,
    "SLOC": 25.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response4.txt_stdout.txt",
    "code_path": "problem_iter5_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, temperature: float = 1.0, k: float = 5.0, gamma: float = 0.1, delta: float = 0.01, epsilon: float = 1e-6) -> np.ndarray:\n    \"\"\"\n    Returns priority probabilities for packing an item into bins using a\n    combination of tight fit, bin scarcity, tie-breaking, and Softmax for exploration.\n\n    This heuristic prioritizes:\n    1.  **Tight Fits:** Bins that have just enough remaining capacity for the item.\n        This is handled by a sigmoid function on the \"mismatch\" (remaining_capacity - item).\n    2.  **Bin Scarcity:** Slightly favors bins that are less empty (have less remaining capacity),\n        as these are scarcer resources. A bonus is added inversely proportional to remaining capacity.\n    3.  **Earlier Bin Preference:** If multiple bins have similar scores, favors bins that\n        appear earlier in the `bins_remain_cap` array (i.e., were opened earlier).\n    4.  **Probabilistic Exploration (Softmax):** Converts the computed priority scores\n        into probabilities using the Softmax function. The `temperature` parameter\n        controls the exploration-exploitation trade-off:\n        - Low temperature (close to 0): Exploitation (favors the highest score).\n        - High temperature (large value): Exploration (probabilities are more uniform).\n\n    The final priority for each bin is calculated as:\n    `Score_i = SigmoidFit(bins_remain_cap[i], item, k) + gamma * (1 / (bins_remain_cap[i] + epsilon)) + delta * (1 / (i + 1))`\n    Then, probabilities are derived using Softmax:\n    `P_i = exp(Score_i / temperature) / sum(exp(Score_j / temperature))`\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n        temperature: Controls the Softmax exploration/exploitation balance.\n                     Higher values lead to more exploration. Defaults to 1.0.\n        k: Sensitivity parameter for the sigmoid function (tightest fit preference).\n           Higher `k` increases preference for tighter fits. Defaults to 5.0.\n        gamma: Weight for the bin scarcity bonus. Higher `gamma` increases\n               preference for less empty bins. Defaults to 0.1.\n        delta: Weight for the earlier bin preference tie-breaker. Higher `delta`\n               increases preference for earlier bins. Defaults to 0.01.\n        epsilon: Small value to prevent division by zero in scarcity calculation.\n                 Defaults to 1e-6.\n\n    Returns:\n        A NumPy array of probabilities, same size as `bins_remain_cap`.\n        Bins that cannot fit the item will have a probability of 0.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    raw_scores = np.zeros(num_bins, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    # Calculate base sigmoid fit score for suitable bins\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    if suitable_bins_cap.size > 0:\n        # Calculate the \"mismatch\" or wasted space\n        mismatch = suitable_bins_cap - item\n        \n        # Cap exponent argument to prevent overflow in np.exp\n        max_exponent_arg = 35.0\n        capped_exponent_arg = np.minimum(k * mismatch, max_exponent_arg)\n        \n        # Sigmoid score: Higher for tighter fits (smaller mismatch)\n        sigmoid_scores = 1 / (1 + np.exp(capped_exponent_arg))\n\n        # Apply scarcity bonus: Add bonus for bins with less remaining capacity\n        # Using 1 / (capacity + epsilon) as a proxy for \"fullness\"\n        scarcity_bonus = gamma * (1.0 / (suitable_bins_cap + epsilon))\n\n        # Apply tie-breaking bonus: Add bonus for earlier bins\n        # Find the indices of the suitable bins in the original array\n        suitable_bin_indices = np.where(suitable_bins_mask)[0]\n        tie_breaker_bonus = delta * (1.0 / (suitable_bin_indices + 1.0))\n\n        # Combine scores for suitable bins\n        combined_scores = sigmoid_scores + scarcity_bonus + tie_breaker_bonus\n        \n        # Assign combined scores back to the raw_scores array\n        raw_scores[suitable_bins_mask] = combined_scores\n\n    # If temperature is very low (close to 0), effectively select the max score bin.\n    # Avoid division by zero if temperature is 0.\n    if temperature <= epsilon:\n        if np.max(raw_scores) > -np.inf: # Check if there's at least one valid score\n             # Assign probability 1 to the bin(s) with the maximum score\n             max_score = np.max(raw_scores)\n             probabilities = np.where(raw_scores == max_score, 1.0, 0.0)\n             # Normalize to ensure sum is 1 if multiple max scores exist\n             num_max_scores = np.sum(probabilities)\n             if num_max_scores > 0:\n                 probabilities /= num_max_scores\n        else: # All scores are -inf (e.g., no suitable bins)\n             probabilities = np.zeros(num_bins)\n        return probabilities\n\n    # Apply Softmax to convert scores to probabilities\n    # Add a small constant to scores before exp to avoid issues with very small negative scores\n    # Or simply handle potential underflow/overflow.\n    # A common trick is to subtract the max score before exponentiation:\n    # exp(x_i / T) / sum(exp(x_j / T)) = exp((x_i - max(x)) / T) / sum(exp((x_j - max(x)) / T))\n    # This stabilizes calculations.\n    \n    # Find the maximum score to shift all scores down for numerical stability\n    max_raw_score = np.max(raw_scores)\n    \n    # Ensure we don't get NaN or inf if all scores are -inf (e.g., no suitable bins)\n    if max_raw_score == -np.inf:\n        return np.zeros(num_bins)\n\n    shifted_scores = (raw_scores - max_raw_score) / temperature\n    \n    # Calculate exponential of shifted scores\n    exp_scores = np.exp(shifted_scores)\n    \n    # Calculate sum of exponential scores for normalization\n    sum_exp_scores = np.sum(exp_scores)\n    \n    # Calculate probabilities\n    probabilities = exp_scores / sum_exp_scores\n    \n    # Ensure probabilities sum to 1 (due to potential floating point inaccuracies)\n    # And handle cases where sum_exp_scores might be 0 (e.g., all shifted scores were -inf)\n    if sum_exp_scores > 0:\n        probabilities /= np.sum(probabilities) # Re-normalize\n    else:\n        # This case should ideally not happen if max_raw_score was handled correctly,\n        # but as a fallback, if all exp_scores resulted in 0, distribute uniformly or zero out.\n        # Given our max_raw_score shift, this implies all shifted scores were extremely negative.\n        probabilities = np.zeros(num_bins)\n\n    # Ensure probabilities are not NaN\n    probabilities = np.nan_to_num(probabilities)\n\n    return probabilities",
    "response_id": 4,
    "obj": 4.168328679696844,
    "SLOC": 38.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response0.txt_stdout.txt",
    "code_path": "problem_iter6_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority scores for packing an item into bins using a simplified\n    and adaptive heuristic.\n\n    This heuristic prioritizes bins that have just enough remaining capacity\n    for the item, aiming for tight fits. It uses a simple linear scoring\n    mechanism for bins that can fit the item, preferring those with less\n    remaining capacity after packing. A small random component is added\n    to encourage exploration, with the probability of exploration decreasing\n    as more items are packed (implicitly, as the number of bins grows or\n    bins become more full).\n\n    The scoring is designed to be interpretable and efficient.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of priority scores, same size as `bins_remain_cap`.\n        Bins that cannot fit the item will have a score of 0.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate scores for bins that can fit the item\n    if np.any(can_fit_mask):\n        suitable_bins_cap = bins_remain_cap[can_fit_mask]\n\n        # Score: Prioritize bins that leave minimal remaining capacity.\n        # A simple inverse relationship with (remaining_capacity - item) works well.\n        # Adding 1 to the denominator prevents division by zero for perfect fits\n        # and dampens extremely high scores for perfect fits.\n        scores = 1.0 / (suitable_bins_cap - item + 1.0)\n\n        # Normalize scores to a [0, 1] range to make them comparable and stable.\n        min_score = np.min(scores)\n        max_score = np.max(scores)\n\n        if max_score > min_score:\n            normalized_scores = (scores - min_score) / (max_score - min_score)\n        else:\n            # If all suitable bins yield the same score (e.g., all have same remaining capacity after fit),\n            # assign a uniform high score (e.g., 1.0) to all of them.\n            normalized_scores = np.ones(scores.shape)\n\n        priorities[can_fit_mask] = normalized_scores\n\n    # Simple exploration: With a small, fixed probability, pick a random bin\n    # among the ones that can fit. This is simpler than Softmax or complex\n    # adaptive epsilon, balancing exploration and exploitation.\n    exploration_prob = 0.05 # Fixed small probability for exploration\n    if np.random.rand() < exploration_prob:\n        possible_bins_indices = np.where(can_fit_mask)[0]\n        if possible_bins_indices.size > 0:\n            # Select a random bin among those that can fit.\n            random_bin_index = np.random.choice(possible_bins_indices)\n            \n            # Assign the highest priority to the randomly chosen bin.\n            # This effectively makes the random choice override the calculated priority.\n            priorities = np.zeros(num_bins, dtype=float) # Reset all priorities\n            priorities[random_bin_index] = 1.0         # Give highest priority to random bin\n\n    return priorities",
    "response_id": 0,
    "obj": 4.028719585161557,
    "SLOC": 22.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response2.txt_stdout.txt",
    "code_path": "problem_iter6_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority scores for packing an item into bins using a refined\n    priority function incorporating Softmax for exploration, bin scarcity,\n    and a \"best fit\" preference.\n\n    This heuristic aims to balance:\n    1.  **Best Fit Preference:** Prioritize bins where the remaining capacity is\n        closest to the item size, minimizing waste. This is achieved by\n        penalizing excess capacity.\n    2.  **Bin Scarcity:** Favor bins that have less remaining capacity overall,\n        as these are considered \"scarcer\" resources.\n    3.  **Softmax for Probabilistic Selection:** Uses Softmax to convert scores\n        into probabilities, allowing for exploration of less optimal bins.\n        The temperature parameter controls the degree of exploration.\n\n    The combined score for a suitable bin is a weighted sum of the\n    \"anti-waste\" score and the bin scarcity score.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is the probability score (from Softmax) for the corresponding bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # --- Heuristic Component 1: \"Anti-Waste\" Score ---\n    # We want bins where `bins_remain_cap - item` is minimized.\n    # A good score would be the negative difference, so smaller differences are less negative (higher score).\n    # Or, we can use a penalty function that increases with the difference.\n    # Let's use the negative difference for simplicity, which is `item - bins_remain_cap`.\n    # To make it more conducive to softmax (avoiding large negative numbers), we can transform it.\n    # A simple approach is `-(bins_remain_cap - item)`.\n    # To ensure scores are positive and have a decreasing trend as difference increases,\n    # we can use something like `1 / (1 + (bins_remain_cap - item))`.\n    # Let's use `item - suitable_bins_cap` as a base score, where smaller positive values are better.\n    # We'll apply a transformation to ensure scores are in a reasonable range for softmax.\n    \n    # Calculate the \"waste\"\n    waste = suitable_bins_cap - item\n    \n    # Transform waste into an \"anti-waste\" score. We want smaller waste to have higher scores.\n    # Using a steepness parameter `k_waste` to control sensitivity to waste.\n    k_waste = 5.0\n    \n    # Employ a function that maps small positive waste to high scores and larger waste to lower scores.\n    # An exponential decay or a sigmoid-like function is suitable.\n    # Let's use `exp(-k_waste * waste)`. This maps [0, inf) to (0, 1].\n    # Clamp waste to avoid extremely small negative values that could lead to exp overflow.\n    # However, waste is already guaranteed to be >= 0 here.\n    anti_waste_scores = np.exp(-k_waste * waste)\n\n    # --- Heuristic Component 2: Bin Scarcity Score ---\n    # Prioritize bins with less remaining capacity.\n    # Using `1 / (capacity + epsilon)` gives higher scores to smaller capacities.\n    epsilon_scarcity = 1e-6\n    scarcity_scores = 1 / (suitable_bins_cap + epsilon_scarcity)\n    \n    # Normalize scarcity scores to a [0, 1] range to be comparable with anti_waste_scores.\n    min_scarcity = np.min(scarcity_scores)\n    max_scarcity = np.max(scarcity_scores)\n    \n    # Handle case where all scarcity scores are identical to avoid division by zero\n    if max_scarcity - min_scarcity > epsilon_scarcity:\n        normalized_scarcity_scores = (scarcity_scores - min_scarcity) / (max_scarcity - min_scarcity)\n    else:\n        # If all suitable bins have the same scarcity, assign a neutral score (e.g., 0.5)\n        # or 0 if we want to de-emphasize them when they are not distinct.\n        # Let's assign 0, as scarcity doesn't differentiate them.\n        normalized_scarcity_scores = np.zeros_like(scarcity_scores)\n\n    # --- Combine Heuristics ---\n    # Weighted sum. These weights can be tuned based on empirical performance.\n    # Let's give more weight to minimizing waste, as it's often a primary goal in BPP.\n    weight_anti_waste = 0.7\n    weight_scarcity = 0.3\n    \n    combined_scores = (weight_anti_waste * anti_waste_scores) + (weight_scarcity * normalized_scarcity_scores)\n\n    # --- Softmax for Probabilistic Exploration ---\n    # Temperature parameter: Controls exploration.\n    # Lower temperature -> greedy (closer to argmax).\n    # Higher temperature -> more exploration (probabilities closer to uniform).\n    temperature = 0.7  # Tunable parameter\n\n    # Apply Softmax with numerical stability\n    scores_for_softmax = combined_scores / temperature\n    \n    # Subtract max score for numerical stability in exp.\n    # This ensures the largest value in `stable_scores` is 0.\n    stable_scores = scores_for_softmax - np.max(scores_for_softmax)\n    \n    exp_scores = np.exp(stable_scores)\n    sum_exp_scores = np.sum(exp_scores)\n    \n    # Handle cases where sum_exp_scores might be zero due to underflow (very unlikely with stable_scores)\n    if sum_exp_scores == 0:\n        # Fallback: uniform probability if all exp_scores are effectively zero.\n        probabilities = np.ones_like(combined_scores) / combined_scores.size\n    else:\n        probabilities = exp_scores / sum_exp_scores\n\n    # Place the calculated probabilities back into the main priorities array\n    priorities[suitable_bins_mask] = probabilities\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "SLOC": 32.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response4.txt_stdout.txt",
    "code_path": "problem_iter6_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, temperature: float = 1.0, k: float = 5.0, gamma: float = 0.1, delta: float = 0.01, epsilon: float = 1e-6) -> np.ndarray:\n    \"\"\"\n    Returns priority probabilities for packing an item into bins using a\n    combination of tight fit preference, bin scarcity, earlier bin preference,\n    and Softmax for probabilistic selection.\n\n    This heuristic prioritizes:\n    1.  **Tight Fits:** Favors bins where the remaining capacity is close to the item size.\n        A sigmoid function on the \"mismatch\" (remaining_capacity - item) is used,\n        where smaller mismatch yields a higher score.\n    2.  **Bin Scarcity:** Slightly favors bins that have less remaining capacity overall,\n        as these are scarcer resources. A bonus is added inversely proportional to\n        the remaining capacity.\n    3.  **Earlier Bin Preference:** As a tie-breaker, favors bins that were opened earlier\n        (i.e., appear earlier in the `bins_remain_cap` array).\n    4.  **Probabilistic Exploration (Softmax):** Converts the combined priority scores\n        into probabilities using the Softmax function. The `temperature` parameter\n        controls the exploration-exploitation trade-off:\n        - Low temperature (close to 0): Primarily exploits the highest-scoring bins.\n        - High temperature (large value): Leads to more uniform probabilities,\n          encouraging exploration of less optimal bins.\n\n    The raw score for each bin `i` is calculated as:\n    `Score_i = SigmoidFit(bins_remain_cap[i], item, k) + gamma * (1 / (bins_remain_cap[i] + epsilon)) + delta * (1 / (i + 1))`\n    where `SigmoidFit` is `1 / (1 + exp(k * (bins_remain_cap[i] - item)))`.\n    Then, probabilities are derived using Softmax:\n    `P_i = exp(Score_i / temperature) / sum(exp(Score_j / temperature))`\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n        temperature: Controls the Softmax exploration/exploitation balance.\n                     Higher values lead to more exploration. Defaults to 1.0.\n        k: Sensitivity parameter for the sigmoid function (tightest fit preference).\n           Higher `k` increases preference for tighter fits. Defaults to 5.0.\n        gamma: Weight for the bin scarcity bonus. Higher `gamma` increases\n               preference for less empty bins. Defaults to 0.1.\n        delta: Weight for the earlier bin preference tie-breaker. Higher `delta`\n               increases preference for earlier bins. Defaults to 0.01.\n        epsilon: Small value to prevent division by zero in scarcity calculation.\n                 Defaults to 1e-6.\n\n    Returns:\n        A NumPy array of probabilities, same size as `bins_remain_cap`.\n        Bins that cannot fit the item will have a probability of 0.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    raw_scores = np.full(num_bins, -np.inf, dtype=float) # Initialize with -inf for invalid bins\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bin_indices.size > 0:\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # 1. Sigmoid Fit Score: Prioritize tight fits.\n        # Calculate mismatch (wasted space)\n        mismatch = suitable_bins_cap - item\n        \n        # Cap exponent argument to prevent overflow in np.exp.\n        # A large positive mismatch should result in a score close to 0.\n        # A mismatch of 0 should result in a score close to 0.5.\n        # Sigmoid(x) = 1 / (1 + exp(x)). We want smaller mismatch (closer to 0) to be better.\n        # Let's use `k * mismatch`. If mismatch is small (e.g., 0), exp(0)=1, sigmoid=0.5.\n        # If mismatch is larger, exp(k*mismatch) increases, sigmoid decreases.\n        # To make tighter fits higher score, we want `1 - sigmoid` or `sigmoid(-k*mismatch)`.\n        # Let's define fit score such that smaller `mismatch` is higher score.\n        # Option: 1 / (1 + exp(k * mismatch)). This gives higher score for larger mismatch.\n        # Option: 1 / (1 + exp(-k * mismatch)). This gives higher score for smaller mismatch.\n        # We want small positive mismatch (tight fit) to be prioritized.\n        # So, we want score to be high when `mismatch` is close to 0.\n        # Let's use `exp(-k * mismatch)`. Then normalize it.\n        # Or, use `1 / (1 + exp(k * mismatch))` and invert it, or simply use the original formulation\n        # and recognize that lower `mismatch` leads to `1/(1+exp(small_positive))` which is higher.\n        # Let's stick with `1 / (1 + exp(k * mismatch))` and interpret it directly.\n        # Larger values mean smaller `mismatch` if we use `k * mismatch` as is.\n        \n        # To ensure higher score for tighter fit, we want function to be decreasing with mismatch.\n        # `sigmoid_score = 1 / (1 + np.exp(k * mismatch))`\n        # This means larger mismatch -> larger exponent -> smaller score. This is good.\n        \n        max_exponent_arg = 35.0 # Prevent overflow\n        capped_exponent_arg = np.minimum(k * mismatch, max_exponent_arg)\n        sigmoid_scores = 1.0 / (1.0 + np.exp(capped_exponent_arg))\n\n        # 2. Bin Scarcity Bonus: Favor less empty bins.\n        # Use 1 / (capacity + epsilon) as a proxy for \"fullness\".\n        scarcity_bonus = gamma * (1.0 / (suitable_bins_cap + epsilon))\n\n        # 3. Earlier Bin Preference: Tie-breaker for bins opened earlier.\n        # Indices are 0-based, so we add 1 to avoid division by zero and shift index.\n        tie_breaker_bonus = delta * (1.0 / (suitable_bin_indices + 1.0))\n\n        # Combine scores for suitable bins\n        combined_scores = sigmoid_scores + scarcity_bonus + tie_breaker_bonus\n        \n        # Assign combined scores back to the raw_scores array\n        raw_scores[suitable_bins_mask] = combined_scores\n\n    # If temperature is very low (close to 0), it's pure exploitation.\n    # Avoid division by zero if temperature is 0.\n    if temperature <= epsilon:\n        if np.all(raw_scores == -np.inf): # No suitable bins\n            return np.zeros(num_bins)\n            \n        max_score = np.max(raw_scores)\n        # Assign probability 1 to the bin(s) with the maximum score\n        probabilities = np.where(raw_scores == max_score, 1.0, 0.0)\n        \n        # Normalize to ensure sum is 1 if multiple max scores exist\n        num_max_scores = np.sum(probabilities)\n        if num_max_scores > 0:\n            probabilities /= num_max_scores\n        return probabilities\n\n    # Apply Softmax to convert scores to probabilities for exploration.\n    # Shift scores by subtracting the max score for numerical stability.\n    # exp(x_i / T) / sum(exp(x_j / T)) = exp((x_i - max(x)) / T) / sum(exp((x_j - max(x)) / T))\n    \n    # If all scores are -inf (no suitable bins), max_raw_score will be -inf.\n    # In this case, return all zeros.\n    if np.all(raw_scores == -np.inf):\n        return np.zeros(num_bins)\n\n    max_raw_score = np.max(raw_scores)\n    \n    # Handle potential case where max_raw_score is -inf (should be caught above, but for safety)\n    if not np.isfinite(max_raw_score):\n         return np.zeros(num_bins)\n\n    shifted_scores = (raw_scores - max_raw_score) / temperature\n    \n    # Calculate exponential of shifted scores.\n    # exp_scores can be 0 if shifted_scores are very negative.\n    exp_scores = np.exp(shifted_scores)\n    \n    # Calculate sum of exponential scores for normalization.\n    sum_exp_scores = np.sum(exp_scores)\n    \n    # Calculate probabilities. Handle case where sum_exp_scores is 0 (e.g., all exp_scores were 0).\n    if sum_exp_scores > 0:\n        probabilities = exp_scores / sum_exp_scores\n    else:\n        # This implies all exp_scores were effectively zero, meaning all shifted scores were extremely negative.\n        # In this scenario, all probabilities should be zero, or distributed uniformly if\n        # we interpret the lack of finite scores as equal unsuitability.\n        # Given our `raw_scores` initialization with -inf, this should ideally not happen\n        # unless `temperature` is extremely large or all `raw_scores` are -inf.\n        # If `raw_scores` contained finite values but `sum_exp_scores` is 0, it's a numerical issue.\n        # Setting to zeros is a safe fallback.\n        probabilities = np.zeros(num_bins)\n\n    # Ensure probabilities sum to 1 (due to potential floating point inaccuracies)\n    # and handle any NaN values that might arise from edge cases.\n    probabilities = np.nan_to_num(probabilities)\n    if np.sum(probabilities) > epsilon: # Re-normalize if sum is significantly greater than 0\n        probabilities /= np.sum(probabilities)\n    elif not np.all(raw_scores == -np.inf): # If there were valid scores but sum is 0\n        # This suggests all valid scores resulted in near-zero exp_scores.\n        # A uniform distribution over suitable bins might be better if possible.\n        # However, given the Softmax logic, zero sum means zero probabilities.\n        pass # Keep probabilities as zeros\n\n    return probabilities",
    "response_id": 4,
    "obj": 4.168328679696844,
    "SLOC": 42.0,
    "cyclomatic_complexity": 10.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response5.txt_stdout.txt",
    "code_path": "problem_iter6_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined strategy:\n    Exact Fit First, then Best Fit (smallest positive remaining capacity), and finally First Fit.\n    Incorporates an epsilon-greedy exploration.\n\n    The strategy prioritizes bins where the remaining capacity is exactly equal to the item size.\n    If no exact fit is found, it prioritizes bins that can fit the item with the smallest positive\n    remaining capacity after placement (Best Fit).\n    If no bins can fit exactly or with a small positive gap, it falls back to selecting the first\n    available bin that can fit the item (First Fit principle applied through uniform probability).\n    An epsilon-greedy component is integrated: with a small probability `epsilon`, a bin is chosen randomly\n    from the *suitable* bins to encourage exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.05  # Probability of exploring a random suitable bin\n\n    n_bins = len(bins_remain_cap)\n    priorities = np.zeros(n_bins)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    if len(suitable_bin_indices) == 0:\n        # No bin can fit the item\n        return priorities\n\n    # Epsilon-greedy choice: With probability epsilon, pick a random suitable bin\n    if np.random.rand() < epsilon:\n        chosen_index = np.random.choice(suitable_bin_indices)\n        # Assign a high priority to this randomly chosen bin\n        priorities[chosen_index] = 1.0\n        # For other suitable bins, assign a very low priority\n        priorities[suitable_bin_indices] = 1e-9\n        return priorities\n\n    # Greedy strategy:\n    # 1. Exact Fit: Prioritize bins where remaining capacity is exactly the item size.\n    exact_fit_mask = (bins_remain_cap == item) & suitable_bins_mask\n    exact_fit_indices = np.where(exact_fit_mask)[0]\n    if len(exact_fit_indices) > 0:\n        priorities[exact_fit_indices] = 1.0\n        # If there are exact fits, we only consider them as the highest priority.\n        # To ensure they are *strictly* higher, we can assign a very high value and then\n        # normalize later if needed, or set other bins to very low values.\n        # For simplicity, let's assign 1.0 to exact fits and focus on making them stand out.\n        # If other strategies are used, ensure their scores are less than 1.0.\n        # For now, we'll return here if exact fits are found, as they are the highest priority.\n        return priorities # Or normalize if we want a distribution\n\n    # 2. Best Fit: If no exact fit, prioritize bins with the smallest positive remaining capacity.\n    # Calculate the remaining space *after* placing the item.\n    remaining_space_after_fit = bins_remain_cap[suitable_bins_mask] - item\n    \n    # Find the minimum positive remaining space.\n    positive_remaining_space = remaining_space_after_fit[remaining_space_after_fit >= 0]\n\n    if len(positive_remaining_space) > 0:\n        min_positive_space = np.min(positive_remaining_space)\n        \n        # Bins that result in exactly this minimum positive space get the highest priority among non-exact fits.\n        # We can assign a score that decreases as the gap increases.\n        # Score = 1 / (gap + 1) or similar. Let's use a score that is high for small gaps.\n        # For bins that fit exactly (gap=0), this would be 1.0. For small positive gaps, it's < 1.0.\n        \n        # Calculate scores for suitable bins: higher score for smaller positive gap\n        # Use a value that is less than 1.0 but still high.\n        # A simple approach: 1.0 - (gap / (max_capacity - item + 1)) to keep it between [0, 1)\n        # More direct: assign a score based on the inverse of the gap.\n        \n        # Let's assign a priority based on the inverse of the gap.\n        # For a gap of 'g', priority = 1/(g + 1) to avoid division by zero.\n        # This ensures smaller gaps get higher priorities.\n        \n        # Apply to suitable bins only\n        best_fit_scores = np.zeros_like(bins_remain_cap)\n        best_fit_scores[suitable_bins_mask] = 1.0 / (remaining_space_after_fit + 1.0)\n        \n        # To make these distinct from exact fits (which would have a score of 1.0),\n        # we can scale them down or ensure they are less than the exact fit priority.\n        # A simple way to differentiate: add a small offset to the exact fit priority.\n        # Here, since we already returned for exact fits, we can just assign these scores.\n        \n        # To ensure Best Fit is preferred over others, we can set their priority to a\n        # value slightly less than 1, e.g., 0.9. Or use a scaled value.\n        # Let's use a scaled value that decreases with the gap.\n        \n        # A simple scaling: 1 - (gap / (average_gap + 1)) for suitable bins.\n        # A more direct approach: assign a score that favors smaller positive gaps.\n        # We can use the 'remaining_space_after_fit' directly for ranking.\n        \n        # Let's try a score that is inversely proportional to the gap, capped at a high value.\n        # For example, score = 1 - (gap / (max_possible_gap + epsilon))\n        # Or, simply rank them and assign priorities.\n        \n        # A practical approach: assign a rank-based score, or a continuous score.\n        # Let's assign a continuous score: higher for smaller positive gaps.\n        # We want to map `remaining_space_after_fit` (which is >= 0) to priorities.\n        # A mapping like `exp(-remaining_space_after_fit)` could work, but needs scaling.\n        # Let's stick to `1 / (gap + 1)` for simplicity and effectiveness.\n\n        # We are prioritizing bins where `remaining_space_after_fit` is minimal and positive.\n        # Let's re-evaluate the priority assignment for these bins.\n        # We can assign priority `1 - (gap / (max_gap_for_suitable_bins + 1))`\n        \n        # A common heuristic for Best Fit is to assign a score that is highest for the smallest gap.\n        # Let's assign a priority that is inversely proportional to the remaining capacity *after* fit.\n        # This means `1 / (bins_remain_cap[i] - item + 1e-6)` for suitable bins.\n        \n        scores_best_fit = np.zeros_like(bins_remain_cap)\n        # Ensure we only apply to suitable bins and avoid division by zero with a small epsilon.\n        gaps = bins_remain_cap[suitable_bins_mask] - item\n        \n        # The smallest *positive* gap should get the highest priority.\n        # We can assign scores that are higher for smaller positive gaps.\n        # For a gap 'g', a score like `1 / (g + epsilon)` works.\n        # Let's use a value that is clearly less than 1.0 (for exact fits).\n        # Example: 0.8 - (gap / (max_gap + 1))\n        \n        # Let's use a simple penalty for waste. A smaller penalty is better.\n        # Priority = 1 - (waste / max_waste)\n        \n        # For best fit, we want to prioritize the smallest *positive* gap.\n        # We can assign a score like `1 / (gap + 1)` to suitable bins.\n        # This will naturally give higher scores to smaller gaps.\n        \n        # Let's calculate these scores and assign them.\n        # Note: The prior 'exact_fit' handling already returned. If we reach here, no exact fit.\n        \n        scores = np.zeros_like(bins_remain_cap)\n        \n        # Calculate the gap for all suitable bins\n        gaps_for_suitable = bins_remain_cap[suitable_bins_mask] - item\n        \n        # For bins that have a positive gap, assign a priority based on the inverse of the gap.\n        # This favors smaller gaps. We add 1 to avoid division by zero if gap is 0 (which is handled by exact fit).\n        # To ensure these priorities are distinct and generally lower than a hypothetical exact fit priority,\n        # we can scale them. Let's assign a score between (0, 1) where smaller gap is closer to 1.\n        \n        # We can use `1.0 / (gaps_for_suitable + 1.0)` and then scale these values if needed.\n        # Let's try to create a priority distribution.\n        \n        # A pragmatic approach: rank the suitable bins by their gap and assign priorities.\n        # Or, assign a score that is a decreasing function of the gap.\n        \n        # For Best Fit, we want `bins_remain_cap[i] - item` to be minimized and positive.\n        # Let's use the reciprocal of the gap.\n        # `scores[suitable_bins_mask] = 1.0 / (gaps_for_suitable + 1e-6)`\n        # This would give very high scores for very small gaps.\n        # To ensure these are less than 1.0 (for exact fit), we can scale.\n        \n        # Let's use a linear decay for the priority as the gap increases.\n        # Max possible gap for a suitable bin: bin_capacity - item.\n        # Assuming bin_capacity is fixed and known, or we use the max `bins_remain_cap`.\n        \n        # A simpler approach: Assign a high priority (e.g., 0.8) to bins with the smallest positive gap,\n        # and then slightly lower priorities to others.\n        \n        # Let's assign priority based on the inverse of the gap, scaled.\n        # `priority = max_priority_best_fit * (1 - (gap / (max_gap_possible + epsilon)))`\n        # Let's try `priority = 0.8 * (1.0 / (gap + 1.0))`\n        \n        # Calculate scores for best fit: higher for smaller positive gaps.\n        # Use a value that is less than 1.0.\n        \n        scores_for_best_fit = np.zeros_like(bins_remain_cap)\n        positive_gaps_indices = np.where(gaps_for_suitable >= 0)[0] # Re-filter for non-negative gaps\n        \n        if len(positive_gaps_indices) > 0:\n            # For all suitable bins, calculate a score based on the gap.\n            # The score should be higher for smaller gaps.\n            # Let's map the gap to a score in [0, 0.9].\n            # `score = 0.9 * (1.0 - (gap / (max_gap_across_suitable_bins + 1e-6)))`\n            \n            # Let's use `1.0 / (gap + 1.0)` and then normalize or scale it.\n            # `inverse_gap_scores = 1.0 / (gaps_for_suitable + 1.0)`\n            \n            # To make these priorities reasonable, let's scale them.\n            # The maximum value of `1.0 / (gaps_for_suitable + 1.0)` occurs at the minimum gap.\n            # Let `min_positive_gap` be the smallest non-negative gap.\n            # Max score ~ `1.0 / (min_positive_gap + 1.0)`\n            \n            # Assign a base priority for best fit, e.g., 0.7. Then adjust based on gap.\n            \n            # Let's use a simpler method: Prioritize bins with smallest positive gap.\n            # We can assign priorities directly to these bins.\n            \n            # Find the minimum positive gap among suitable bins\n            min_positive_gap = np.min(gaps_for_suitable)\n            \n            # Assign a high priority to bins matching this minimum positive gap\n            # We need to be careful if multiple bins have the same minimum gap.\n            \n            # Let's assign a score that is higher for smaller gaps.\n            # A score like `1.0 - (gap / (max_suitable_gap + 1))` can work.\n            \n            # Calculate the gap for all suitable bins\n            gaps_for_suitable = bins_remain_cap[suitable_bins_mask] - item\n            \n            # We want to prioritize bins with smaller gaps.\n            # Create scores for suitable bins: higher for smaller gaps.\n            # A simple heuristic: score = 1.0 / (gap + 1.0)\n            # Let's scale these so they are distinctly less than 1.0.\n            \n            # Example: Map gaps to priorities in the range [0.5, 0.9].\n            # Max gap among suitable bins for scaling:\n            max_gap_suitable = np.max(gaps_for_suitable) if len(gaps_for_suitable) > 0 else 0\n            \n            # If max_gap_suitable is 0, all suitable bins are exact fits (already handled).\n            # So, we assume max_gap_suitable > 0 if we reach here.\n            \n            # For bins with gap `g`: priority = 0.9 - (g / (max_gap_suitable + 1.0)) * 0.4\n            # This maps 0 gap to 0.9 and max_gap_suitable to 0.5.\n            \n            priorities_for_best_fit = np.zeros_like(bins_remain_cap)\n            \n            # For each suitable bin\n            for i, bin_idx in enumerate(suitable_bin_indices):\n                gap = bins_remain_cap[bin_idx] - item\n                \n                if gap == 0: # This should not happen if exact fit is handled first\n                    continue\n                \n                # Calculate a score that is higher for smaller positive gaps\n                # Map the gap to a priority value.\n                # Let's use `1.0 / (gap + 1.0)` and scale it to be below 1.0.\n                # Example: score = 0.8 * (1.0 / (gap + 1.0))\n                # This ensures smaller gaps get higher scores.\n                \n                # Consider the range of gaps for suitable bins\n                # Let's use the difference from the minimum positive gap.\n                \n                # Simple approach: Assign priority 0.7 to all bins that fit.\n                # Then, if we want to differentiate Best Fit, we can try to adjust.\n                \n                # Let's go back to the idea of inverse of gap.\n                # `score = 1.0 / (gap + 1.0)`\n                \n                # Scale these scores to be in a range like [0.5, 0.9]\n                # Find the minimum positive gap among these suitable bins.\n                min_gap_among_suitable = np.min(gaps_for_suitable)\n                \n                # If `min_gap_among_suitable` is very small, `1.0 / (gap + 1.0)` can be large.\n                # Let's consider the range of `gaps_for_suitable`.\n                \n                # Let's try a simpler priority assignment:\n                # Prioritize bins with the smallest positive remaining capacity.\n                \n                # For all suitable bins, calculate their gap.\n                gaps = bins_remain_cap[suitable_bins_mask] - item\n                \n                # Find the minimum positive gap.\n                min_pos_gap = np.min(gaps[gaps >= 0]) # Find min non-negative gap\n                \n                # Assign a high priority to bins with this minimum positive gap.\n                # These are the best fit candidates.\n                best_fit_candidates_mask = (bins_remain_cap == item + min_pos_gap) & suitable_bins_mask\n                priorities[best_fit_candidates_mask] = 0.9 # High priority for best fit\n                \n                # For other suitable bins (that are not exact fits and not best fits)\n                # Assign a lower priority. This would be a form of \"Worst Fit\" if we wanted that,\n                # but for First Fit we'd just pick the first one.\n                # Here, we can assign a moderate priority to other suitable bins.\n                \n                other_suitable_mask = suitable_bins_mask & ~best_fit_candidates_mask\n                priorities[other_suitable_mask] = 0.5 # Moderate priority for other fits\n                \n                return priorities # Return after Best Fit assignment.\n            \n    # 3. First Fit fallback: If no exact or best fit identified (e.g., only large gaps or no suitable bins),\n    # this part ensures some priority is given if any suitable bins exist.\n    # The above logic for best fit should cover all suitable bins.\n    # If no suitable bins were found, we already returned zeros.\n    # If suitable bins were found, the `priorities` array should have non-zero values.\n\n    # If the logic above didn't assign any priorities (e.g., if only one suitable bin and it's not best/exact)\n    # or if we want a fallback for \"First Fit\" where any fitting bin is okay.\n    # If `np.sum(priorities) == 0` and `len(suitable_bin_indices) > 0`:\n    # This means no exact fit, and perhaps no clear \"best fit\" was prioritized distinctly.\n    # In such cases, we can assign uniform priorities to all suitable bins.\n    if np.sum(priorities) == 0 and len(suitable_bin_indices) > 0:\n        priorities[suitable_bin_indices] = 1.0 / len(suitable_bin_indices)\n\n    return priorities",
    "response_id": 5,
    "obj": 4.078579976067022,
    "SLOC": 50.0,
    "cyclomatic_complexity": 11.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response8.txt_stdout.txt",
    "code_path": "problem_iter6_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, temperature: float = 1.0) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a softmax-based heuristic.\n\n    This heuristic prioritizes bins that can accommodate the item, with a stronger\n    preference for \"tight fits\" (bins with remaining capacity close to the item size).\n    It also incorporates a \"scarcity\" factor, favoring bins that are closer to being full\n    among those that can fit the item. The `temperature` parameter controls the\n    exploration vs. exploitation trade-off. Higher temperatures lead to more uniform\n    probabilities (more exploration), while lower temperatures focus on the best-fitting\n    bins (more exploitation). Bins that are too small for the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n        temperature: Controls the sharpness of the softmax distribution. Must be positive.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score (probability) of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Scoring Components ---\n\n    # 1. Tight Fit Score: Prioritize bins where remaining capacity is close to item size.\n    #    We want a score that is high when `fitting_bins_cap - item` is small and positive.\n    #    Using `1.0 / (1.0 + (fitting_bins_cap - item))` maps small positive wasted space to values close to 1.\n    #    A perfect fit (wasted_space = 0) results in a score of 1.0.\n    wasted_space = fitting_bins_cap - item\n    tightness_scores = 1.0 / (1.0 + wasted_space)\n\n    # 2. Scarcity Score: Prioritize bins that have less remaining capacity overall among fitting bins.\n    #    We use the inverse of the remaining capacity. To avoid division by zero and\n    #    to make it comparable to tightness_scores, we can normalize it.\n    #    A simple approach is to map the minimum fitting capacity to a higher score.\n    #    We want a score that is high for small `fitting_bins_cap`.\n    #    Let's use a score inversely proportional to capacity, but capped to avoid extreme values.\n    #    `1.0 / (fitting_bins_cap + epsilon)` is a common way.\n    #    To make scarcity a secondary factor, we can scale it.\n    #    Consider scarcity as `1.0 - normalized_capacity`. High scarcity means low capacity.\n    #    Normalize capacity to [0, 1] range for fitting bins.\n    min_cap = np.min(fitting_bins_cap)\n    max_cap = np.max(fitting_bins_cap)\n    epsilon_cap = 1e-9 # for stable division\n\n    if max_cap - min_cap > epsilon_cap: # Avoid division by zero if all capacities are the same\n        normalized_capacity = (fitting_bins_cap - min_cap) / (max_cap - min_cap)\n        # Scarcity is higher for lower capacity, so invert normalized capacity.\n        scarcity_scores = 1.0 - normalized_capacity\n    else:\n        # If all fitting bins have the same capacity, scarcity is uniform.\n        scarcity_scores = np.ones_like(fitting_bins_cap) * 0.5\n\n    # 3. Combine Scores: Weighted sum of tightness and scarcity.\n    #    Prioritize tightness, with scarcity as a secondary preference.\n    weight_tightness = 1.0\n    weight_scarcity = 0.4  # Scarcity has a moderate influence\n    combined_scores = (weight_tightness * tightness_scores) + (weight_scarcity * scarcity_scores)\n\n    # Apply Softmax to get probabilities.\n    if temperature <= 0:\n        raise ValueError(\"Temperature must be positive.\")\n\n    # Scale scores by temperature to control distribution sharpness.\n    # Subtract max score before exponentiation for numerical stability.\n    max_score = np.max(combined_scores)\n    scaled_scores = (combined_scores - max_score) / temperature\n\n    # Ensure scores passed to exp are within a safe range (e.g., [-100, 100])\n    scaled_scores = np.clip(scaled_scores, -100.0, 100.0)\n\n    exp_scores = np.exp(scaled_scores)\n    sum_exp_scores = np.sum(exp_scores)\n\n    if sum_exp_scores > 0:\n        probabilities = exp_scores / sum_exp_scores\n    else:\n        # Fallback for extreme cases, though unlikely with clipping.\n        probabilities = np.ones_like(exp_scores) / len(exp_scores)\n\n    # Assign probabilities to the original priorities array\n    priorities[can_fit_mask] = probabilities\n\n    return priorities",
    "response_id": 8,
    "obj": 4.048663741523748,
    "SLOC": 32.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response3.txt_stdout.txt",
    "code_path": "problem_iter7_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-based unified scoring mechanism.\n\n    This heuristic prioritizes bins that offer the \"tightest fit\" for an incoming item,\n    while also considering bin scarcity. It uses a unified scoring mechanism that\n    balances the preference for a tight fit with a general preference for bins that\n    are not excessively large. The scores are then normalized using a Softmax function\n    with a tunable temperature parameter, allowing for adaptive exploration/exploitation.\n\n    The core scoring for a suitable bin (where `remaining_capacity >= item`) is:\n    `score = exp(-k * (remaining_capacity - item))`\n\n    Here:\n    - `remaining_capacity - item` represents the \"mismatch\" or wasted space.\n    - `k` is a parameter controlling the sensitivity to the mismatch. A higher `k`\n      amplifies the penalty for larger mismatches, favoring tighter fits more strongly.\n      A lower `k` is more forgiving of larger mismatches.\n    - The negative sign ensures that smaller mismatches (tighter fits) result in\n      larger exponent values and thus higher scores before Softmax.\n\n    After calculating raw scores for all suitable bins, a Softmax function is applied\n    to normalize these scores into probabilities (priorities).\n    `priority = exp(raw_score / temperature) / sum(exp(raw_score / temperature))`\n\n    - `temperature` controls the shape of the probability distribution.\n      - A low temperature makes the distribution \"sharper,\" strongly favoring\n        bins with the highest raw scores (exploitation).\n      - A high temperature makes the distribution \"flatter,\" increasing the\n        probability of selecting bins with lower raw scores (exploration).\n\n    Bins that cannot fit the item are assigned a priority of 0.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is the priority score (probability) for the corresponding bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n    # If no bin can fit the item, return all zeros\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # --- Tunable Parameters ---\n    # Sensitivity to mismatch: higher k means stronger preference for tighter fits.\n    k = 1.0\n    # Softmax temperature: controls exploration/exploitation.\n    # Lower temp = more exploitation, higher temp = more exploration.\n    temperature = 0.5\n    # --------------------------\n\n    # Calculate the \"mismatch\" for suitable bins\n    mismatch = suitable_bins_cap - item\n\n    # Calculate raw scores: prefer smaller mismatches (tighter fits)\n    # Using exp(-k * mismatch) means smaller mismatch -> larger score\n    # To prevent overflow for very small mismatches (exp(large_positive)),\n    # we can cap the negative mismatch if it's very large (which shouldn't happen if k>0 and mismatch>=0)\n    # The primary concern is exp() of large negative numbers if k or mismatch were negative,\n    # but here k>=0 and mismatch>=0.\n    # However, for very large positive `k * mismatch`, `exp(-k * mismatch)` will be close to 0.\n    # We might want to avoid `exp` of extremely large negative numbers if `k * mismatch` is very large.\n    # Capping `k * mismatch` to a maximum value is a good practice.\n    max_positive_mismatch_score_arg = 10.0 # exp(-10) is small but not zero\n    capped_mismatch = np.minimum(mismatch, max_positive_mismatch_score_arg / k if k > 0 else np.inf)\n    raw_scores = np.exp(-k * capped_mismatch)\n\n    # Apply Softmax to normalize scores into probabilities (priorities)\n    if temperature <= 0:\n        # If temperature is zero or negative, select the bin with the highest raw score deterministically.\n        # This is equivalent to a greedy approach among suitable bins.\n        # We can find the index of the max score and assign probability 1 to it.\n        max_score_idx = np.argmax(raw_scores)\n        softmax_priorities = np.zeros_like(raw_scores)\n        softmax_priorities[max_score_idx] = 1.0\n    else:\n        # Standard Softmax calculation\n        # Add a small epsilon to the denominator to prevent division by zero if all raw_scores are -inf (not possible here)\n        # or if the sum of exponents is zero.\n        try:\n            # Using log-sum-exp trick for numerical stability if needed, but for exp(-k*mismatch)\n            # where mismatch >= 0 and k >= 0, values are between 0 and 1.\n            # Direct calculation is likely fine.\n            exp_scores = np.exp(raw_scores / temperature)\n            sum_exp_scores = np.sum(exp_scores)\n            if sum_exp_scores == 0: # Handle case where all exp_scores are ~0\n                softmax_priorities = np.ones_like(raw_scores) / len(raw_scores) if len(raw_scores) > 0 else np.array([])\n            else:\n                softmax_priorities = exp_scores / sum_exp_scores\n        except FloatingPointError:\n            # Fallback for extreme values if exp calculation fails\n            softmax_priorities = np.zeros_like(raw_scores)\n            if len(raw_scores) > 0:\n                max_score_idx = np.argmax(raw_scores)\n                softmax_priorities[max_score_idx] = 1.0\n\n\n    # Place the calculated softmax priorities back into the main priorities array\n    priorities[suitable_bins_mask] = softmax_priorities\n\n    return priorities",
    "response_id": 3,
    "obj": 3.9788591942560925,
    "SLOC": 31.0,
    "cyclomatic_complexity": 8.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response0.txt_stdout.txt",
    "code_path": "problem_iter8_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority scores for bins using a Softmax-based approach,\n    balancing tight fits with bin scarcity using tunable weights and temperature.\n\n    This heuristic prioritizes bins based on two factors, combined into a unified score\n    and then transformed into probabilities using Softmax:\n\n    1.  **Tightness of Fit:** Prefers bins where the remaining capacity is just enough\n        to accommodate the item, minimizing wasted space (`remaining_capacity - item`).\n        This is modeled by a term that increases as the mismatch decreases.\n    2.  **Bin Scarcity:** Prefers bins that have less remaining capacity overall. This\n        encourages the usage of less full bins, potentially leaving more space in\n        moderately full bins for future items. This is modeled by a term that\n        increases as the remaining capacity decreases.\n\n    The unified score for a suitable bin `i` is a weighted sum of these two components:\n    `unified_score_i = w_fit * fit_score_i + w_scarcity * scarcity_score_i`\n\n    - `fit_score_i`: Higher for smaller `bins_remain_cap[i] - item`. We use `1.0 / (1.0 + mismatch_i)`.\n    - `scarcity_score_i`: Higher for smaller `bins_remain_cap[i]`. We use `1.0 / (1.0 + bins_remain_cap[i])`.\n\n    The `temperature` parameter in the Softmax function controls the exploration-exploitation\n    trade-off:\n    - High temperature: Smoother probability distribution (more exploration).\n    - Low temperature: Sharper probability distribution (more exploitation of best bins).\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is a probability score (between 0 and 1) for the corresponding bin.\n        Bins that cannot fit the item will have a score of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n    # If no bin can fit the item, return all zeros\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # --- Tuning Parameters ---\n    # Weights for balancing fit and scarcity. Sum doesn't need to be 1.\n    # Higher weight means that component has more influence.\n    weight_fit = 0.7\n    weight_scarcity = 0.3\n\n    # Temperature for Softmax: controls exploration vs. exploitation.\n    # Lower temperature -> favors bins with highest score (exploitation).\n    # Higher temperature -> more uniform distribution (exploration).\n    temperature = 0.5 # Tunable parameter: 0.1 (sharp) to 2.0 (smooth)\n\n    # --- Scoring Mechanism ---\n    mismatches = suitable_bins_cap - item\n    capacities = suitable_bins_cap\n\n    # Fit score: Higher for smaller mismatch. Use 1/(1+x) to map [0, inf) to (0, 1].\n    # Add a small epsilon if we want to ensure scores are never exactly 0 or 1,\n    # but 1/(1+x) already maps to (0, 1].\n    fit_scores = 1.0 / (1.0 + mismatches)\n\n    # Scarcity score: Higher for smaller capacity. Use 1/(1+x) to map [0, inf) to (0, 1].\n    scarcity_scores = 1.0 / (1.0 + capacities)\n\n    # Unified score: Weighted sum of fit and scarcity scores.\n    unified_scores = weight_fit * fit_scores + weight_scarcity * scarcity_scores\n\n    # --- Softmax Application ---\n    # Apply Softmax to `unified_scores`. The formula is exp(score / temperature) / sum(exp(score / temperature)).\n    # To prevent numerical overflow from exp(), it's best to shift scores so the maximum is 0.\n    # `scaled_scores = unified_scores / temperature`\n    # `shifted_scaled_scores = scaled_scores - np.max(scaled_scores)`\n    # `exponentials = np.exp(shifted_scaled_scores)`\n    # `sum_exponentials = np.sum(exponentials)`\n\n    # Let's use direct scaling and then careful exponentiation.\n    # Avoid division by zero if temperature is zero or extremely small.\n    if temperature <= 0:\n        # If temperature is zero or negative, it implies pure exploitation (argmax).\n        # The bin with the highest unified score gets probability 1.\n        # However, Softmax is generally defined for temperature > 0.\n        # For simplicity and robustness, if temperature is invalid, treat as moderate.\n        temperature = 1e-6 # Small positive number to avoid division by zero\n\n    # Scale scores by temperature for Softmax. Lower temp sharpens.\n    scaled_scores = unified_scores / temperature\n\n    # Shift scores for numerical stability before exponentiation.\n    # The maximum value in `scaled_scores` will become 0 after subtraction.\n    # This prevents `exp` from overflowing for large positive scores.\n    max_scaled_score = np.max(scaled_scores)\n    shifted_scaled_scores = scaled_scores - max_scaled_score\n\n    # Compute exponentials. Even after shifting, very small negative scores can lead to underflow.\n    # `np.exp` handles small values gracefully (approaching 0).\n    exponentials = np.exp(shifted_scaled_scores)\n\n    # Calculate the sum of exponentials for normalization.\n    sum_exponentials = np.sum(exponentials)\n\n    # Avoid division by zero if all exponentiated values are zero.\n    if sum_exponentials == 0:\n        # This can happen if all `shifted_scaled_scores` were extremely negative,\n        # causing `exp` to underflow to zero. In such cases, all bins are effectively\n        # equally undesirable or equally preferred. Assign uniform probability.\n        if suitable_bins_cap.size > 0:\n            priorities[suitable_bins_mask] = 1.0 / suitable_bins_cap.size\n        return priorities\n\n    # Calculate the final probabilities using Softmax.\n    softmax_probabilities = exponentials / sum_exponentials\n\n    # Place the calculated softmax probabilities back into the main priorities array.\n    priorities[suitable_bins_mask] = softmax_probabilities\n\n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "SLOC": 28.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response4.txt_stdout.txt",
    "code_path": "problem_iter8_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority for each bin using a refined Best Fit Decreasing (BFD)-like strategy.\n\n    This heuristic aims to replicate the spirit of BFD in an online setting,\n    prioritizing bins that are the \"best fit\" (i.e., have the least remaining\n    capacity after placing the item), while also incorporating a mechanism to\n    avoid getting stuck in a purely greedy approach.\n\n    The scoring mechanism is designed to:\n    1.  **Prioritize tightest fit:** Bins that leave the least remaining capacity\n        after placing the item receive the highest scores. This is the primary\n        criterion.\n    2.  **Encourage using less full bins (secondary):** If multiple bins offer\n        a similar \"tight fit,\" the one with more remaining capacity is slightly\n        preferred. This acts as a tie-breaker and encourages distributing items\n        more evenly rather than filling up a few bins too quickly.\n    3.  **Controlled exploration:** A small probability of choosing a random\n        valid bin is included to prevent premature convergence to suboptimal\n        solutions. The probability is kept low to favor the greedy strategy.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, representing the\n        priority score for each bin. Bins that cannot fit the item will have a\n        score of 0. Scores are roughly scaled between 0 and 1, where higher is better.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    suitable_bins_cap = bins_remain_cap[can_fit_mask]\n    suitable_bins_indices = np.where(can_fit_mask)[0]\n\n    # --- Scoring Components ---\n\n    # 1. Tightest Fit Score: (Maximum remaining capacity after packing) - (actual remaining capacity after packing)\n    # A higher score means less remaining capacity, i.e., a tighter fit.\n    # We want to maximize `bins_remain_cap - item`.\n    # So, we can use `bins_remain_cap[suitable_bins_mask] - item`.\n    # To make it a priority score (higher is better), we can use the negative of this,\n    # or a transformation that maps smaller values to larger scores.\n    # Let's use `1.0 / (remaining_capacity_after_fit + epsilon)` as in v1,\n    # but normalize it to avoid extreme values.\n\n    remaining_capacity_after_fit = suitable_bins_cap - item\n    \n    # Avoid division by zero, and ensure small remaining capacities get high scores.\n    # Add a small constant epsilon.\n    tight_fit_scores = 1.0 / (remaining_capacity_after_fit + 1e-6)\n\n    # Normalize tight_fit_scores to be roughly in [0, 1].\n    # Higher values mean a tighter fit.\n    min_tight_score = np.min(tight_fit_scores)\n    max_tight_score = np.max(tight_fit_scores)\n\n    if max_tight_score > min_tight_score:\n        normalized_tight_fit = (tight_fit_scores - min_tight_score) / (max_tight_score - min_tight_score)\n    else:\n        # All fits are equally \"tight\" in terms of remaining capacity difference\n        normalized_tight_fit = np.ones_like(tight_fit_scores) * 0.5 # Neutral score\n\n    # 2. Secondary Tie-breaker: Prefer bins with *more* remaining capacity if fits are similar.\n    # This helps distribute items better. So, score should increase with `suitable_bins_cap`.\n    # Normalize suitable_bins_cap to be in [0, 1].\n    min_bin_cap = np.min(suitable_bins_cap)\n    max_bin_cap = np.max(suitable_bins_cap)\n    \n    if max_bin_cap > min_bin_cap:\n        normalized_capacity_preference = (suitable_bins_cap - min_bin_cap) / (max_bin_cap - min_bin_cap)\n    else:\n        # All suitable bins have the same capacity. No preference based on this.\n        normalized_capacity_preference = np.ones_like(suitable_bins_cap) * 0.5 # Neutral score\n\n    # Combine scores: Primarily tight fit, secondarily capacity preference.\n    # Weight the secondary factor to ensure tight fit dominates.\n    weight_tightness = 1.0\n    weight_capacity = 0.2  # Lower weight for the secondary criterion\n    \n    combined_scores = (weight_tightness * normalized_tight_fit) + (weight_capacity * normalized_capacity_preference)\n\n    # --- Controlled Exploration ---\n    # Use a small epsilon for random choice among valid bins.\n    exploration_epsilon = 0.05  # Small probability of random selection\n\n    # Calculate actual probabilities using Softmax to allow for nuanced selection\n    # while still favoring higher scores.\n    # Shift scores to prevent overflow with exp, by subtracting the max score.\n    if combined_scores.size > 0:\n        max_score = np.max(combined_scores)\n        shifted_scores = combined_scores - max_score\n        # Clamp values to prevent numerical issues with exp\n        capped_shifted_scores = np.clip(shifted_scores, -100.0, 100.0)\n        \n        exp_scores = np.exp(capped_shifted_scores)\n        sum_exp_scores = np.sum(exp_scores)\n        \n        if sum_exp_scores > 0:\n            greedy_probabilities = exp_scores / sum_exp_scores\n        else:\n            greedy_probabilities = np.ones_like(exp_scores) / len(exp_scores) if len(exp_scores) > 0 else np.array([])\n    else:\n        greedy_probabilities = np.array([])\n\n    # Integrate exploration: For a small epsilon, pick a random bin.\n    # This means we override the calculated probabilities with a uniform distribution\n    # over valid bins with probability epsilon.\n    final_probabilities = np.copy(greedy_probabilities)\n    \n    if np.random.rand() < exploration_epsilon:\n        # Choose a random bin among those that can fit\n        possible_bins_indices = np.where(can_fit_mask)[0]\n        if possible_bins_indices.size > 0:\n            random_choice_index = np.random.choice(len(possible_bins_indices))\n            random_bin_global_index = possible_bins_indices[random_choice_index]\n            \n            # Reset probabilities and assign 1.0 to the randomly chosen bin, 0 to others.\n            # This is one way to implement epsilon-greedy: with prob epsilon, pick a random *action*.\n            # Here, actions are bins.\n            priorities = np.zeros_like(bins_remain_cap, dtype=float)\n            priorities[random_bin_global_index] = 1.0\n            return priorities # Exit early if exploration happened\n\n    # If not exploring randomly, use the calculated greedy probabilities\n    priorities[can_fit_mask] = final_probabilities\n\n    return priorities",
    "response_id": 4,
    "obj": 4.168328679696844,
    "SLOC": 49.0,
    "cyclomatic_complexity": 9.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response5.txt_stdout.txt",
    "code_path": "problem_iter8_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, temperature: float = 0.2, decay_rate: float = 0.5) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a blended strategy.\n\n    This strategy prioritizes exact fits, uses a smooth exponential decay for near-exact fits,\n    and applies a Softmax normalization for a tunable probability distribution.\n    It aims to favor bins where the remaining capacity is closest to the item size,\n    with a strong preference for exact matches.\n\n    The priority score for a bin `i` that can fit the item is calculated as:\n    1. If `bins_remain_cap[i] == item`: score = 1.0 (exact fit)\n    2. If `bins_remain_cap[i] > item`: score = exp(-decay_rate * (bins_remain_cap[i] - item))\n    \n    These scores are then converted into probabilities using the Softmax function:\n    `P_i = exp(score_i / temperature) / sum(exp(score_j / temperature))`\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n        temperature: Controls the Softmax exploration/exploitation balance.\n                     Higher values lead to more exploration (more uniform probabilities).\n                     Defaults to 0.2.\n        decay_rate: Controls how quickly priority decreases as the residual\n                    (remaining capacity - item) increases. Higher values mean\n                    faster decay. Defaults to 0.5.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score (probability) of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if not eligible_bins_cap.size:\n        return priorities  # Return all zeros if no bin can fit the item\n\n    # Calculate the \"fit residual\": how much space is left after placing the item.\n    fit_residuals = eligible_bins_cap - item\n\n    # Calculate base scores for eligible bins:\n    # - Exact fits (residual == 0) get a score of 1.0.\n    # - Near-exact fits get a score based on exponential decay of the residual.\n    #   The term `fit_residuals + 1e-9` is used to prevent `exp(0)` for exact fits\n    #   and to ensure numerical stability, slightly differentiating exact fits from\n    #   very close fits if `decay_rate` is high, though the `np.where` handles exact fits explicitly.\n    #   Using `np.where` directly for exact fits is cleaner.\n    scores = np.where(fit_residuals == 0,\n                      1.0,  # High priority for exact fits\n                      np.exp(-decay_rate * fit_residuals)) # Decreasing priority for near-fits\n\n    # Apply Softmax to convert scores into probabilities.\n    # Subtracting the maximum score before exponentiation improves numerical stability.\n    if scores.size > 0:\n        try:\n            max_score = np.max(scores)\n            # Ensure temperature is not zero to avoid division by zero\n            effective_temperature = max(temperature, 1e-9)\n            \n            shifted_scores = (scores - max_score) / effective_temperature\n            exp_scores = np.exp(shifted_scores)\n            sum_exp_scores = np.sum(exp_scores)\n\n            if sum_exp_scores > 1e-9: # Avoid division by zero or near-zero sums\n                softmax_priorities = exp_scores / sum_exp_scores\n            else:\n                # If sum is zero or near-zero (e.g., all scores were -inf or extremely negative),\n                # fall back to uniform distribution over eligible bins.\n                softmax_priorities = np.ones_like(scores) / scores.size\n\n        except (OverflowError, FloatingPointError):\n            # In case of extreme values leading to overflow or other numerical issues,\n            # fall back to a uniform distribution over eligible bins.\n            softmax_priorities = np.ones_like(scores) / scores.size\n    else:\n        softmax_priorities = np.array([], dtype=float)\n\n\n    # Map the calculated priorities back to the original bin indices\n    priorities[can_fit_mask] = softmax_priorities\n    \n    # Ensure probabilities sum to 1 (due to potential floating point inaccuracies)\n    # and handle any NaN values that might arise from edge cases.\n    priorities = np.nan_to_num(priorities)\n    current_sum = np.sum(priorities)\n    if current_sum > 1e-9:\n        priorities /= current_sum\n    elif not np.all(can_fit_mask): # If there are eligible bins but sum is ~0\n        # This might happen if temperature is very high and all scores are very small\n        # or if there's a numerical issue. Distribute uniformly among eligible bins.\n        num_eligible = np.sum(can_fit_mask)\n        if num_eligible > 0:\n            priorities[can_fit_mask] = 1.0 / num_eligible\n\n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "SLOC": 36.0,
    "cyclomatic_complexity": 8.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response6.txt_stdout.txt",
    "code_path": "problem_iter8_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a tunable\n    combination of First Fit (FF) preference and a scarcity-aware preference.\n\n    This heuristic aims to balance two objectives:\n    1. Tight fit preference: Prioritize bins where the item leaves minimal remaining space.\n       This is similar to First Fit Decreasing's intuition of packing densely.\n    2. Scarcity preference: Prioritize bins that are less empty overall. This can help\n       in scenarios where starting new bins is undesirable or when trying to consolidate\n       items.\n\n    The priority for a suitable bin is calculated as a weighted sum of scores reflecting\n    these two objectives. A Softmax function is then applied to convert these scores into\n    probabilities, allowing for probabilistic selection.\n\n    Scores for a suitable bin (where `remaining_capacity >= item`):\n\n    - Tight Fit Score: `exp(-k_tight * (remaining_capacity - item))`\n      This score is high for bins with small `remaining_capacity - item` (tight fits).\n      `k_tight` controls the sensitivity to the tightness.\n\n    - Scarcity Score: `exp(k_scarcity * (bin_capacity - remaining_capacity))`\n      Assuming `bin_capacity` is known or can be approximated (e.g., by the bin's initial capacity\n      or an average capacity if not explicitly provided). For this implementation, we'll assume\n      bins are of a uniform `BIN_CAPACITY`. A higher score indicates less remaining space,\n      i.e., a scarcer bin. `k_scarcity` controls sensitivity to scarcity.\n\n    Combined Raw Score: `w_tight * tight_fit_score + w_scarcity * scarcity_score`\n      `w_tight` and `w_scarcity` are weights determining the relative importance of each factor.\n\n    Final Priority: Softmax of the combined raw scores.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is the priority score (probability) for the corresponding bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n    # If no bin can fit the item, return all zeros\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # --- Tunable Parameters ---\n    # Weight for tight fit preference\n    w_tight = 0.7\n    # Weight for scarcity preference\n    w_scarcity = 0.3\n    # Sensitivity to mismatch (tightness)\n    k_tight = 1.5\n    # Sensitivity to remaining capacity (scarcity)\n    # Assuming a default bin capacity for calculating scarcity. In a real scenario,\n    # this might be passed or derived. Let's assume a standard capacity for demonstration.\n    BIN_CAPACITY = 1.0 # Example: if items are fractions of 1.0\n    k_scarcity = 1.0\n    # Softmax temperature: controls exploration/exploitation.\n    temperature = 0.5\n    # --------------------------\n\n    # Calculate scores for suitable bins\n\n    # 1. Tight Fit Score Component\n    mismatch = suitable_bins_cap - item\n    # Cap `k_tight * mismatch` to prevent potential overflow with exp() if values are very large.\n    # Since mismatch >= 0, we are concerned about `exp(-large_positive_number)` -> near zero.\n    # Capping ensures that the score doesn't become exactly zero too quickly.\n    max_positive_mismatch_score_arg = 10.0 # exp(-10) is very small\n    capped_mismatch = np.minimum(mismatch, max_positive_mismatch_score_arg / k_tight if k_tight > 0 else np.inf)\n    tight_fit_scores = np.exp(-k_tight * capped_mismatch)\n\n    # 2. Scarcity Score Component\n    # Score based on how much capacity is *used* or how *little* is remaining.\n    # Higher score for bins with less remaining capacity (i.e., scarcer bins).\n    # We use `BIN_CAPACITY - suitable_bins_cap` to represent the \"used\" space.\n    # A higher `k_scarcity` makes bins with less remaining capacity more desirable.\n    # Cap `k_scarcity * used_space` for stability.\n    used_space = BIN_CAPACITY - suitable_bins_cap\n    max_positive_used_space_score_arg = 10.0\n    capped_used_space = np.minimum(used_space, max_positive_used_space_score_arg / k_scarcity if k_scarcity > 0 else np.inf)\n    scarcity_scores = np.exp(k_scarcity * capped_used_space)\n\n    # Combine scores\n    raw_scores = (w_tight * tight_fit_scores) + (w_scarcity * scarcity_scores)\n\n    # Apply Softmax to normalize scores into probabilities (priorities)\n    if temperature <= 0:\n        # Deterministic selection: pick the best bin according to combined score\n        max_score_idx = np.argmax(raw_scores)\n        softmax_priorities = np.zeros_like(raw_scores)\n        softmax_priorities[max_score_idx] = 1.0\n    else:\n        try:\n            exp_scores = np.exp(raw_scores / temperature)\n            sum_exp_scores = np.sum(exp_scores)\n            if sum_exp_scores == 0:\n                # If all exponentiated scores are near zero, assign uniform probability\n                softmax_priorities = np.ones_like(raw_scores) / len(raw_scores) if len(raw_scores) > 0 else np.array([])\n            else:\n                softmax_priorities = exp_scores / sum_exp_scores\n        except FloatingPointError:\n            # Fallback for extreme values\n            softmax_priorities = np.zeros_like(raw_scores)\n            if len(raw_scores) > 0:\n                max_score_idx = np.argmax(raw_scores)\n                softmax_priorities[max_score_idx] = 1.0\n\n    # Place the calculated softmax priorities back into the main priorities array\n    priorities[suitable_bins_mask] = softmax_priorities\n\n    return priorities",
    "response_id": 6,
    "obj": 3.9888312724371757,
    "SLOC": 40.0,
    "cyclomatic_complexity": 9.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response9.txt_stdout.txt",
    "code_path": "problem_iter8_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority probabilities for packing an item into bins using a\n    combination of tight fit preference, bin scarcity consideration, and Softmax\n    for controlled exploration.\n\n    This heuristic prioritizes:\n    1.  **Tight Fits:** Bins that have a small remaining capacity after packing\n        the item are preferred. This minimizes wasted space.\n    2.  **Bin Scarcity:** Bins that are already less empty (have less remaining\n        capacity) are slightly preferred. This encourages fuller utilization\n        of existing bins.\n    3.  **Probabilistic Selection (Softmax):** The computed scores are converted\n        into probabilities using the Softmax function. A `temperature` parameter\n        controls the exploration-exploitation trade-off:\n        - Low temperature (e.g., < 1.0): Favors bins with higher scores more strongly\n          (more exploitation of preferred bins).\n        - High temperature (e.g., > 1.0): Probabilities are more uniform, leading\n          to more exploration of less-preferred bins.\n\n    The score for each suitable bin is calculated as a weighted sum:\n    `Score_i = w_fit * FitScore_i + w_scarcity * ScarcityScore_i`\n\n    -   **FitScore:** Designed to be high for small positive `remaining_capacity - item`.\n        A common choice is `1 / (1 + exp(k * (remaining_capacity - item)))`.\n        Here, we use `1 - (mismatch / (mismatch + item))` for a score from 0 to 1\n        that increases as mismatch decreases, and is capped by item size.\n        Alternatively, a simpler approach is to use a logistic function on negative mismatch.\n        We will use `1 / (1 + exp(k * (remaining_capacity - item)))` for its clear interpretation.\n    -   **ScarcityScore:** Designed to be high for bins with less remaining capacity.\n        A simple choice is `1 / (remaining_capacity + epsilon)`.\n\n    Then, probabilities are derived using Softmax:\n    `P_i = exp(Score_i / temperature) / sum(exp(Score_j / temperature))`\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of probabilities, same size as `bins_remain_cap`.\n        Bins that cannot fit the item will have a probability of 0.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(suitable_bins_mask):\n        return priorities # No bins can fit the item\n\n    # Parameters for scoring\n    k = 5.0          # Sensitivity for tight fit preference (higher k = stronger preference)\n    w_fit = 0.8      # Weight for the tight fit score\n    w_scarcity = 0.2 # Weight for the bin scarcity score\n    temperature = 0.5 # Controls Softmax exploration (lower = more exploitation)\n    epsilon = 1e-6   # Small value to prevent division by zero\n\n    # --- Calculate Scores for Suitable Bins ---\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    \n    # 1. Fit Score: Higher for smaller `remaining_capacity - item` (mismatch)\n    mismatch = suitable_bins_cap - item\n    # Cap exponent argument to prevent overflow in np.exp\n    max_exponent_arg = 35.0\n    capped_exponent_arg = np.minimum(k * mismatch, max_exponent_arg)\n    # FitScore ranges from ~0 (large mismatch) to 0.5 (zero mismatch) to ~1 (negative mismatch).\n    # We want high scores for small positive mismatch. The sigmoid `1/(1+exp(x))` gives\n    # higher values for smaller x. So, `1/(1+exp(k*mismatch))` works well.\n    fit_scores = 1 / (1 + np.exp(capped_exponent_arg))\n\n    # 2. Scarcity Score: Higher for bins with less remaining capacity\n    # Using `1 / (capacity + epsilon)` as a measure of fullness.\n    scarcity_scores = 1.0 / (suitable_bins_cap + epsilon)\n    \n    # Combine scores with weights\n    # Ensure scores are positive and well-behaved for Softmax.\n    # Fit scores are in [0, 1). Scarcity scores are > 0.\n    # Normalize scarcity scores to be in a similar range if needed, or rely on weights.\n    # For simplicity, let's use them as is and rely on Softmax.\n    \n    # Adjust scarcity score to be more comparable to fit score range, e.g., normalize if needed.\n    # A simple approach: scale scarcity to roughly [0, 1].\n    # Max possible scarcity score is 1/epsilon. Min is 1/(max_capacity+epsilon).\n    # Let's consider the typical range of `suitable_bins_cap`. If capacities are, say, up to 100,\n    # then scarcity is up to 1/100 = 0.01. This is much smaller than fit scores.\n    # To make scarcity impactful, we might need to adjust its weight or its scoring function.\n    # A common practice might be to normalize scarcity scores or use a bounded utility function.\n    # For now, let's assume the weights handle the scale difference.\n    \n    raw_scores = w_fit * fit_scores + w_scarcity * scarcity_scores\n\n    # --- Softmax Application ---\n    # Apply Softmax with temperature scaling.\n    # `exp(score / temperature)` where higher temperature leads to more exploration.\n    \n    # Handle potential numerical instability if temperature is very close to zero.\n    # Ensure scores don't become excessively large after scaling.\n    if temperature <= epsilon: # Effectively argmax\n        max_score = np.max(raw_scores)\n        # Assign probability 1 to the bin(s) with the maximum score\n        best_bin_indices = np.where(raw_scores == max_score)[0]\n        priorities[suitable_bins_mask][best_bin_indices] = 1.0 / len(best_bin_indices)\n    else:\n        # Scale scores by temperature for Softmax\n        # Subtract max score for numerical stability before exponentiation\n        max_raw_score = np.max(raw_scores)\n        scaled_scores = (raw_scores - max_raw_score) / temperature\n        \n        # Cap the scaled scores argument to avoid exp overflow\n        max_scaled_score_exponent = 35.0 \n        capped_scaled_scores = np.minimum(scaled_scores, max_scaled_score_exponent)\n        \n        exponentials = np.exp(capped_scaled_scores)\n        sum_exponentials = np.sum(exponentials)\n        \n        if sum_exponentials > 0:\n            softmax_probabilities = exponentials / sum_exponentials\n            priorities[suitable_bins_mask] = softmax_probabilities\n        else:\n            # Fallback: if sum is zero (e.g., all capped scores were -inf),\n            # distribute probability uniformly among suitable bins.\n            if suitable_bins_cap.size > 0:\n                priorities[suitable_bins_mask] = 1.0 / suitable_bins_cap.size\n\n    # Ensure probabilities sum to 1 and handle NaNs (though unlikely with current approach)\n    priorities = np.nan_to_num(priorities)\n    if np.sum(priorities) > 0:\n        priorities /= np.sum(priorities) # Re-normalize for float precision issues\n\n    return priorities",
    "response_id": 9,
    "obj": 4.048663741523748,
    "SLOC": 39.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response1.txt_stdout.txt",
    "code_path": "problem_iter9_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority scores for bins using a combined \"best fit\" and \"scarcity\" heuristic.\n\n    This function calculates a priority score for each bin based on how well it\n    fits the incoming item, considering both the tightness of the fit and the\n    remaining capacity of the bin. Bins that can fit the item are evaluated.\n\n    The score for a suitable bin is calculated as a weighted sum of two components:\n    1. Best Fit Component: Prioritizes bins where the remaining capacity is\n       closest to the item size. This is modeled using a sigmoid function applied\n       to the \"mismatch\" (remaining_capacity - item). A smaller mismatch yields\n       a higher score.\n    2. Scarcity Component: Prioritizes bins with less remaining capacity overall,\n       as these are considered \"scarcer\" resources. This is modeled by simply\n       inverting the remaining capacity, normalized by the item size to keep\n       the scale comparable.\n\n    The final score for a suitable bin is:\n    `score = w_fit * sigmoid_score + w_scarce * scarcity_score`\n\n    where:\n    - `sigmoid_score = 1 / (1 + exp(k * (remaining_capacity - item)))`\n      The sensitivity parameter `k` controls the steepness of the preference for\n      tighter fits.\n    - `scarcity_score = (max_possible_capacity - remaining_capacity) / item`\n      This term increases as `remaining_capacity` decreases. `max_possible_capacity`\n      is a proxy for the initial bin capacity. We use a large value if not known,\n      or a reasonable estimate. For simplicity, we can normalize by item size to\n      keep the magnitudes somewhat aligned with the sigmoid score. A simpler form\n      can be `1.0 / remaining_capacity` if `remaining_capacity` is always positive.\n      Here, we'll use `1.0 / (remaining_capacity / item)` to scale it.\n\n    The weights `w_fit` and `w_scarce` control the balance between the two objectives.\n    The final priorities are passed through a Softmax function to convert scores\n    into probabilities, allowing for adaptive exploration.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of priority scores for each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # --- Tunable Parameters ---\n    # Sensitivity parameter for the sigmoid function (Best Fit component)\n    # Higher k means stronger preference for exact fits.\n    k_fit = 5.0\n\n    # Weights for combining the two components\n    w_fit = 0.7\n    w_scarce = 0.3\n\n    # Softmax temperature for exploration/exploitation balance\n    # Higher temperature -> more uniform probabilities (more exploration)\n    # Lower temperature -> more peaked probabilities (more exploitation)\n    softmax_temp = 1.0\n\n    # --- Calculate Best Fit Component ---\n    mismatch = suitable_bins_cap - item\n    # Use a capped sigmoid to prevent numerical issues with large mismatches\n    max_exponent_arg = 35.0\n    capped_exponent_arg = np.minimum(k_fit * mismatch, max_exponent_arg)\n    sigmoid_scores = 1 / (1 + np.exp(capped_exponent_arg))\n\n    # --- Calculate Scarcity Component ---\n    # Prioritize bins with less remaining capacity.\n    # Normalize by item size to make scores comparable with sigmoid scores.\n    # Add a small epsilon to avoid division by zero if a bin has 0 remaining capacity (though unlikely here)\n    epsilon = 1e-6\n    # A higher score means less capacity. We want to invert this relationship.\n    # Higher score for smaller remaining capacity.\n    # Normalize to keep values in a reasonable range.\n    # Consider a maximum \"useful\" capacity as a reference, e.g., if items are usually small.\n    # For simplicity, let's use 1.0 / (remaining_capacity / item) which is item / remaining_capacity\n    # We want higher score when remaining_capacity is small.\n    # So, we want a score that increases as remaining_capacity decreases.\n    # A simple inverse relationship: 1 / (remaining_capacity + epsilon)\n    # To make it more comparable to sigmoid scores, let's scale it.\n    # Let's use `item / (suitable_bins_cap + epsilon)` as a measure of \"how much of the item fits\"\n    # relative to the bin's remaining capacity. Small remaining capacity means high ratio.\n    scarcity_scores = item / (suitable_bins_cap + epsilon)\n\n    # Normalize scarcity scores to be roughly in the same range as sigmoid scores (0 to 1)\n    # Find max and min scarcity scores among suitable bins.\n    if scarcity_scores.size > 0:\n        min_scarcity = np.min(scarcity_scores)\n        max_scarcity = np.max(scarcity_scores)\n        if max_scarcity > min_scarcity:\n            normalized_scarcity_scores = (scarcity_scores - min_scarcity) / (max_scarcity - min_scarcity)\n        else:\n            normalized_scarcity_scores = np.ones_like(scarcity_scores) * 0.5 # All equal, assign mid value\n    else:\n        normalized_scarcity_scores = np.array([])\n\n\n    # --- Combine Components ---\n    combined_scores = w_fit * sigmoid_scores + w_scarce * normalized_scarcity_scores\n\n    # --- Apply Softmax for Probabilistic Selection ---\n    # Ensure scores are not all identical for softmax to work meaningfully,\n    # though it handles identical values gracefully.\n    if combined_scores.size > 0:\n        # Prevent division by zero in softmax if temperature is zero or scores are identical.\n        if softmax_temp <= 0:\n            # If temperature is non-positive, select the max score bin deterministically.\n            max_score_idx = np.argmax(combined_scores)\n            priorities[suitable_bin_indices[max_score_idx]] = 1.0\n        else:\n            # Normalize scores before softmax to prevent large exponents\n            exp_scores = np.exp(combined_scores / softmax_temp)\n            probabilities = exp_scores / np.sum(exp_scores)\n            priorities[suitable_bin_indices] = probabilities\n    \n    # Ensure no NaN values (can happen if sum of exp_scores is 0, though unlikely here)\n    priorities = np.nan_to_num(priorities)\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 37.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  }
]