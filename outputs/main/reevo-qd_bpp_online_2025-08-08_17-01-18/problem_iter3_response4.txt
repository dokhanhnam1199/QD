```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray, temperature: float = 1.0) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a softmax-based heuristic.

    This heuristic prioritizes bins that can accommodate the item, with a stronger
    preference for "tight fits" (bins with remaining capacity close to the item size).
    The `temperature` parameter controls the exploration vs. exploitation trade-off.
    Higher temperatures lead to more uniform probabilities (more exploration), while
    lower temperatures focus on the best-fitting bins (more exploitation).
    Bins that are too small for the item receive a priority of 0.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.
        temperature: Controls the sharpness of the softmax distribution.

    Return:
        Array of same size as bins_remain_cap with priority score (probability) of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins that can fit the item
    can_fit_mask = bins_remain_cap >= item

    # For bins that can fit, calculate a "goodness" score.
    # We want to prioritize bins where the remaining capacity is close to the item size.
    # A common approach is to use the difference (remaining_capacity - item).
    # To prioritize smaller differences (tighter fits), we can use the negative of this difference.
    # We add a small epsilon to avoid issues when remaining_capacity == item.
    # A larger negative value (more negative) indicates a worse fit, a value closer to zero is a better fit.
    # To make better fits have higher scores for softmax, we can invert this or use a different metric.
    # Let's try prioritizing based on how much capacity is LEFT OVER after packing.
    # So, (remaining_capacity - item) is what we want to minimize.
    # For softmax, higher values mean higher probability. So, we want a metric that is
    # higher for better fits. A good metric would be the negative of the leftover capacity,
    # or a Gaussian-like function centered at 0 difference.
    # Let's use negative difference, then scale it to make it more sensitive to near fits.
    # A simple transformation that boosts near-fits and reduces others:
    # Consider -(bins_remain_cap[can_fit_mask] - item) which is (item - bins_remain_cap[can_fit_mask]).
    # This value is negative or zero. Higher values (closer to zero) are better fits.
    # To make it suitable for softmax where higher is better, we can use `-(bins_remain_cap[can_fit_mask] - item)`.
    # However, this might still be too sensitive to very small items.
    # Let's consider a score that is high when `bins_remain_cap[can_fit_mask] - item` is small and positive.
    # The inverse `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` from v1 is good.
    # Let's refine this. We want to reward bins where `bins_remain_cap - item` is small.
    # A Gaussian-like kernel centered at 0 difference could work: exp(- (diff^2) / (2 * sigma^2))
    # Or, a simpler approach: consider `1 / (1 + diff)` where diff is `bins_remain_cap - item`.
    # If diff is small and positive, score is close to 1. If diff is large, score approaches 0.

    # Let's try a score that emphasizes small positive differences.
    # We want a high score when `bins_remain_cap[can_fit_mask] - item` is small and positive.
    # Consider `1.0 / (1.0 + (bins_remain_cap[can_fit_mask] - item))`
    # This gives scores between (0, 1] for valid fits. 1 for perfect fits.

    # Alternative: Prioritize bins with minimum remaining capacity that can fit the item.
    # This is essentially the "Best Fit" strategy. For a heuristic priority, we can
    # use the inverse of the remaining capacity for fitting bins.
    # `priorities[can_fit_mask] = 1.0 / bins_remain_cap[can_fit_mask]` - This prioritizes smallest bins.
    # If we want to prioritize tight fits, we are looking for bins where `bins_remain_cap - item` is small.
    # So, we want to maximize `- (bins_remain_cap[can_fit_mask] - item)`.
    # Or, a score that is high for small positive `bins_remain_cap[can_fit_mask] - item`.
    # Let's use `-(bins_remain_cap[can_fit_mask] - item)` directly, then rescale or apply softmax.
    # The intuition of `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` was good.
    # Let's enhance it for "nearness".

    # Consider the "wasted space" after packing: `wasted_space = bins_remain_cap - item`.
    # We want to minimize `wasted_space`. So, a higher priority should be given to bins with smaller `wasted_space`.
    # Let's transform `wasted_space` into a score where smaller `wasted_space` yields a higher score.
    # A simple transformation: `score = 1.0 / (1.0 + wasted_space)`.
    # This gives scores in the range (0, 1]. Perfect fit -> score 1. Large wasted space -> score close to 0.
    # This should provide a good signal for "tight fits".

    wasted_space = bins_remain_cap[can_fit_mask] - item
    # Using `1.0 / (1.0 + wasted_space)` maps small positive wasted space to values close to 1.
    # For a perfect fit (wasted_space = 0), score is 1.
    # For larger wasted_space, score decreases.
    scores_for_softmax = 1.0 / (1.0 + wasted_space)

    # Apply softmax to get probabilities.
    # Ensure temperature is positive to avoid division by zero or invalid operations.
    if temperature <= 0:
        raise ValueError("Temperature must be positive.")

    # Calculate exponentiated scores, scaled by temperature
    # Lower temperature means sharper distribution, higher temperature means flatter.
    exp_scores = np.exp(scores_for_softmax / temperature)

    # Normalize to get probabilities
    probabilities = exp_scores / np.sum(exp_scores)

    # Assign probabilities to the original priorities array
    priorities[can_fit_mask] = probabilities

    # Normalize priorities to sum to 1, ensuring valid probability distribution
    # This is already handled by the softmax if there's at least one bin that can fit.
    # If no bins can fit, priorities remains all zeros, which is correct.
    if np.sum(priorities) > 0:
        priorities /= np.sum(priorities)

    return priorities
```
