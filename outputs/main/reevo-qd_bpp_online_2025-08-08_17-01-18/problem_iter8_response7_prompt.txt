{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First strategy.\n\n    The Exact Fit First strategy prioritizes bins where the remaining capacity is exactly equal\n    to the item size. Bins that can fit the item but not exactly are given a lower priority,\n    with larger remaining capacities being less preferred. Bins that cannot fit the item\n    receive zero priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Prioritize bins where remaining capacity is exactly the item size\n    exact_fit_mask = (bins_remain_cap == item) & can_fit_mask\n    priorities[exact_fit_mask] = 1.0\n\n    # For bins that can fit but not exactly, give a lower priority\n    # We want to penalize bins with a lot of leftover space after placing the item.\n    # So, we assign a priority that decreases as (remaining_capacity - item_size) increases.\n    # A simple way is to use 1 / (remaining_capacity - item_size + 1) to avoid division by zero\n    # and ensure non-zero priorities for valid fits.\n    partial_fit_mask = (~exact_fit_mask) & can_fit_mask\n    remaining_space_after_fit = bins_remain_cap[partial_fit_mask] - item\n    priorities[partial_fit_mask] = 1.0 / (remaining_space_after_fit + 1) # Adding 1 to avoid division by zero if remaining_space is 0, which is handled by exact_fit_mask anyway.\n\n    # Ensure that exact fits have higher priority than partial fits.\n    # Since we set exact fits to 1.0, and partial fits to values < 1.0 (as remaining_space_after_fit >= 0),\n    # this condition is naturally met.\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a softmax-based heuristic.\n\n    This heuristic prioritizes bins that can accommodate the item, with a stronger\n    preference for \"tight fits\" (bins with remaining capacity close to the item size).\n    The `temperature` parameter controls the exploration vs. exploitation trade-off.\n    Higher temperatures lead to more uniform probabilities (more exploration), while\n    lower temperatures focus on the best-fitting bins (more exploitation).\n    Bins that are too small for the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n        temperature: Controls the sharpness of the softmax distribution.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score (probability) of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate a \"goodness\" score.\n    # We want to prioritize bins where the remaining capacity is close to the item size.\n    # A common approach is to use the difference (remaining_capacity - item).\n    # To prioritize smaller differences (tighter fits), we can use the negative of this difference.\n    # We add a small epsilon to avoid issues when remaining_capacity == item.\n    # A larger negative value (more negative) indicates a worse fit, a value closer to zero is a better fit.\n    # To make better fits have higher scores for softmax, we can invert this or use a different metric.\n    # Let's try prioritizing based on how much capacity is LEFT OVER after packing.\n    # So, (remaining_capacity - item) is what we want to minimize.\n    # For softmax, higher values mean higher probability. So, we want a metric that is\n    # higher for better fits. A good metric would be the negative of the leftover capacity,\n    # or a Gaussian-like function centered at 0 difference.\n    # Let's use negative difference, then scale it to make it more sensitive to near fits.\n    # A simple transformation that boosts near-fits and reduces others:\n    # Consider -(bins_remain_cap[can_fit_mask] - item) which is (item - bins_remain_cap[can_fit_mask]).\n    # This value is negative or zero. Higher values (closer to zero) are better fits.\n    # To make it suitable for softmax where higher is better, we can use `-(bins_remain_cap[can_fit_mask] - item)`.\n    # However, this might still be too sensitive to very small items.\n    # Let's consider a score that is high when `bins_remain_cap[can_fit_mask] - item` is small and positive.\n    # The inverse `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` from v1 is good.\n    # Let's refine this. We want to reward bins where `bins_remain_cap - item` is small.\n    # A Gaussian-like kernel centered at 0 difference could work: exp(- (diff^2) / (2 * sigma^2))\n    # Or, a simpler approach: consider `1 / (1 + diff)` where diff is `bins_remain_cap - item`.\n    # If diff is small and positive, score is close to 1. If diff is large, score approaches 0.\n\n    # Let's try a score that emphasizes small positive differences.\n    # We want a high score when `bins_remain_cap[can_fit_mask] - item` is small and positive.\n    # Consider `1.0 / (1.0 + (bins_remain_cap[can_fit_mask] - item))`\n    # This gives scores between (0, 1] for valid fits. 1 for perfect fits.\n\n    # Alternative: Prioritize bins with minimum remaining capacity that can fit the item.\n    # This is essentially the \"Best Fit\" strategy. For a heuristic priority, we can\n    # use the inverse of the remaining capacity for fitting bins.\n    # `priorities[can_fit_mask] = 1.0 / bins_remain_cap[can_fit_mask]` - This prioritizes smallest bins.\n    # If we want to prioritize tight fits, we are looking for bins where `bins_remain_cap - item` is small.\n    # So, we want to maximize `- (bins_remain_cap[can_fit_mask] - item)`.\n    # Or, a score that is high for small positive `bins_remain_cap[can_fit_mask] - item`.\n    # Let's use `-(bins_remain_cap[can_fit_mask] - item)` directly, then rescale or apply softmax.\n    # The intuition of `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` was good.\n    # Let's enhance it for \"nearness\".\n\n    # Consider the \"wasted space\" after packing: `wasted_space = bins_remain_cap - item`.\n    # We want to minimize `wasted_space`. So, a higher priority should be given to bins with smaller `wasted_space`.\n    # Let's transform `wasted_space` into a score where smaller `wasted_space` yields a higher score.\n    # A simple transformation: `score = 1.0 / (1.0 + wasted_space)`.\n    # This gives scores in the range (0, 1]. Perfect fit -> score 1. Large wasted space -> score close to 0.\n    # This should provide a good signal for \"tight fits\".\n\n    wasted_space = bins_remain_cap[can_fit_mask] - item\n    # Using `1.0 / (1.0 + wasted_space)` maps small positive wasted space to values close to 1.\n    # For a perfect fit (wasted_space = 0), score is 1.\n    # For larger wasted_space, score decreases.\n    scores_for_softmax = 1.0 / (1.0 + wasted_space)\n\n    # Apply softmax to get probabilities.\n    # Ensure temperature is positive to avoid division by zero or invalid operations.\n    if temperature <= 0:\n        raise ValueError(\"Temperature must be positive.\")\n\n    # Calculate exponentiated scores, scaled by temperature\n    # Lower temperature means sharper distribution, higher temperature means flatter.\n    exp_scores = np.exp(scores_for_softmax / temperature)\n\n    # Normalize to get probabilities\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Assign probabilities to the original priorities array\n    priorities[can_fit_mask] = probabilities\n\n    # Normalize priorities to sum to 1, ensuring valid probability distribution\n    # This is already handled by the softmax if there's at least one bin that can fit.\n    # If no bins can fit, priorities remains all zeros, which is correct.\n    if np.sum(priorities) > 0:\n        priorities /= np.sum(priorities)\n\n    return priorities\n\n[Reflection]\nFocus on \"tight fits\" and explore parameterization for adaptable behavior.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}