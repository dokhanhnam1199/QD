```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using an enhanced blended strategy.

    This strategy prioritizes exact fits, uses a steeper exponential decay for near-exact fits,
    and applies a Softmax normalization with a temperature parameter for smooth probability distribution.
    It aims to strongly favor bins where the remaining capacity is closest to the item size,
    with an absolute preference for exact matches.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins that can fit the item
    can_fit_mask = bins_remain_cap >= item
    eligible_bins_cap = bins_remain_cap[can_fit_mask]

    if not eligible_bins_cap.size:
        return np.zeros_like(bins_remain_cap)

    # Calculate the "fit residual": how much space is left after placing the item.
    # Smaller values indicate a tighter fit.
    fit_residuals = eligible_bins_cap - item

    # Prioritize exact fits (residual = 0) with a very high score.
    # For near-exact fits, use a sharper exponential decay based on the residual.
    # Bins with smaller residuals get higher scores.
    # The decay_rate controls how quickly the priority drops as the residual increases.
    # A higher decay_rate makes the preference for closer fits stronger.
    decay_rate = 1.0  # Increased decay rate for stronger preference for closer fits

    # Calculate raw scores:
    # Exact fits (residual == 0) get a base score of 1.0.
    # For others, use exp(-decay_rate * residual).
    # Add a small epsilon to residual for non-exact fits to ensure they get a score strictly less than 1.0
    # but also to avoid potential division by zero if residual is negative (though that's handled by can_fit_mask).
    # A better approach for differentiation is to use a base value for exact fits and then the decay for others.
    
    raw_scores = np.where(fit_residuals == 0,
                          1.0,  # High priority for exact fits
                          np.exp(-decay_rate * fit_residuals)) # Decreasing priority for near-fits

    # Apply Softmax normalization to convert scores into probabilities.
    # This ensures that the priorities sum to 1 across the eligible bins and
    # that preferences are smoothly distributed.
    # The temperature parameter controls the "sharpness" of the distribution.
    # A lower temperature makes the probabilities sharper (more emphasis on best fits).
    temperature = 0.1 # Lower temperature for sharper distribution
    
    try:
        # Stabilize softmax calculation by subtracting the maximum score from all scores
        # before exponentiation. This prevents potential overflow.
        max_score = np.max(raw_scores)
        stable_scores = (raw_scores - max_score) / temperature
        exp_scores = np.exp(stable_scores)
        softmax_priorities = exp_scores / np.sum(exp_scores)
    except (OverflowError, FloatingPointError):
        # Fallback strategy if softmax calculation fails (e.g., due to extreme values)
        # A simple fallback could be to assign higher priority to exact fits and uniform for others,
        # or just a uniform distribution. Here, we'll try a slightly more robust version.
        # If exact fits exist, prioritize them, otherwise uniform.
        if 0 in fit_residuals:
            softmax_priorities = np.where(fit_residuals == 0, 1.0, 0.0)
            softmax_priorities /= np.sum(softmax_priorities)
        else:
            softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size

    # Map the calculated priorities back to the original bin indices
    priorities[can_fit_mask] = softmax_priorities

    return priorities
```
