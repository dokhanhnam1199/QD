{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority scores for bins using a unified scoring mechanism\n    that balances tight fits with bin scarcity, employing a Softmax function\n    with a tunable temperature.\n\n    This heuristic prioritizes bins based on two factors:\n    1. Tightness of fit: Bins that have just enough remaining capacity\n       for the item are preferred.\n    2. Bin scarcity: Bins with less remaining capacity overall are considered\n       more valuable, as they are closer to being full.\n\n    The scoring mechanism uses a combination of the \"mismatch\" (remaining capacity - item)\n    and the bin's remaining capacity itself. A Softmax function is applied to these\n    scores to create a probability distribution over the bins, effectively\n    balancing exploration (trying less tight fits) and exploitation (going for the tightest fit).\n\n    For each bin `i`:\n    - `mismatch_i = bins_remain_cap[i] - item` (if `bins_remain_cap[i] >= item`, else infinity)\n    - `scarce_score_i = -bins_remain_cap[i]` (higher score for smaller capacity)\n    - `unified_score_i = scarce_score_i - mismatch_i` (prioritize low mismatch and low capacity)\n\n    A Softmax function is then applied to `unified_score_i` for all suitable bins:\n    `probability_i = exp(temperature * unified_score_i) / sum(exp(temperature * unified_score_j))`\n\n    The `temperature` parameter controls the exploration-exploitation trade-off:\n    - High temperature: Explores more, scores are closer to uniform.\n    - Low temperature: Exploits more, favors bins with the absolute highest unified score.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is a probability score (between 0 and 1) for the corresponding bin.\n        Bins that cannot fit the item will have a score of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n    # If no bin can fit the item, return all zeros\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # Tunable temperature parameter for Softmax.\n    # Controls the balance between exploiting the best fit and exploring other options.\n    # A value of 1.0 is a good starting point. Higher values lead to more uniform\n    # probabilities (more exploration), lower values lead to more peaked probabilities\n    # (more exploitation).\n    temperature = 1.0\n\n    # Calculate scores for suitable bins\n    # Score is based on prioritizing bins with less remaining capacity (scarce_score)\n    # and a tighter fit (lower mismatch).\n    # We want to maximize `scarce_score - mismatch`.\n    # `scarce_score` is `-bins_remain_cap[i]` (higher score for lower capacity).\n    # `mismatch` is `bins_remain_cap[i] - item`.\n    # So, `unified_score_i` = `-bins_remain_cap[i] - (bins_remain_cap[i] - item)`\n    #                     = `item - 2 * bins_remain_cap[i]`\n    # This simple formulation prioritizes bins that are smaller, and among equally\n    # sized bins, it prioritizes those that are closer to the item size.\n    # Let's refine this to directly use mismatch and scarcity in a balanced way.\n\n    # Let's define components:\n    # 1. Fit Score: Penalize larger mismatches. A simple way is `-mismatch`.\n    #    To be more robust, use a function that drops quickly. E.g., `-(bins_remain_cap[i] - item)^2`.\n    #    Or, for a \"tight fit\" focus, we can use the previous sigmoid idea.\n    #    However, for Softmax unification, let's use a simple linear penalty on mismatch.\n    #    `fit_score = -(suitable_bins_cap - item)`\n    #\n    # 2. Scarcity Score: Penalize larger remaining capacities.\n    #    `scarce_score = -suitable_bins_cap`\n\n    # Unified score combines fit and scarcity.\n    # We want to prefer low mismatch AND low capacity.\n    # A simple sum of negative values works: `unified_score = fit_score + scarce_score`\n    # `unified_score = -(suitable_bins_cap - item) - suitable_bins_cap`\n    # `unified_score = -suitable_bins_cap + item - suitable_bins_cap`\n    # `unified_score = item - 2 * suitable_bins_cap`\n    # This means bins with smaller remaining capacity get higher scores.\n    # Let's re-evaluate the reflection: \"prioritize tight fits using a unified scoring mechanism.\n    # Employ Softmax with tunable temperature for adaptive exploration/exploitation,\n    # balancing fit and bin scarcity.\"\n\n    # The prompt implies balancing *tight fits* with *bin scarcity*.\n    # Tight fit: Small `bins_remain_cap[i] - item`.\n    # Bin scarcity: Small `bins_remain_cap[i]`.\n\n    # Let's try to create scores where higher is better for both.\n    # For tight fit: Higher score for smaller `(bins_remain_cap[i] - item)`.\n    # For bin scarcity: Higher score for smaller `bins_remain_cap[i]`.\n\n    # Example scores:\n    # `fit_priority_component = - (suitable_bins_cap - item)`\n    # `scarcity_priority_component = - suitable_bins_cap`\n    # `unified_score = fit_priority_component + scarcity_priority_component`\n    # `unified_score = -(suitable_bins_cap - item) - suitable_bins_cap`\n    # `unified_score = -suitable_bins_cap + item - suitable_bins_cap`\n    # `unified_score = item - 2 * suitable_bins_cap`\n\n    # Alternative: Use the \"goodness\" of the fit and \"how much capacity is left\".\n    # Goodness of fit: Higher for smaller `(bins_remain_cap[i] - item)`.\n    # How much capacity is left: Higher for smaller `bins_remain_cap[i]`.\n    # Let's scale them to be comparable.\n    # We can think of `bins_remain_cap[i]` as a measure of bin fullness (inversely).\n    # A bin that is almost full (low `bins_remain_cap[i]`) is scarce.\n    # A bin that fits the item snugly (low `bins_remain_cap[i] - item`) is a tight fit.\n\n    # Let's consider the remaining capacity `R` and item size `S`.\n    # Tight fit: `R - S` is small and non-negative.\n    # Scarcity: `R` is small.\n\n    # Proposed unified score: Prioritize bins where `R` is small, and among those,\n    # where `R - S` is small.\n    # A simple score that captures this could be a decreasing function of `R` and `R-S`.\n    # Let's try: `score = - R - (R - S)` for `R >= S`.\n    # `score = -2R + S`\n    # This penalizes larger `R` more heavily.\n\n    # Let's consider the *benefit* of placing the item in a bin.\n    # Benefit = saving a bin (if it's the last item for that bin) + reduced wasted space.\n    # This is complex for an online setting.\n\n    # Let's go back to balancing fit and scarcity.\n    # How about giving a high score to bins that are *almost* full, but can still fit the item?\n    # And among those, prioritize the ones that fit snugly.\n\n    # A score that reflects \"how much capacity is left relative to the item size\"\n    # and \"how much capacity is left in total\".\n    # Let `mismatch = suitable_bins_cap - item`\n    # Let `capacity = suitable_bins_cap`\n\n    # Score idea: `f(mismatch, capacity)`. We want `f` to increase as `mismatch` and `capacity` decrease.\n    # `score = -mismatch - capacity`\n    # `score = -(suitable_bins_cap - item) - suitable_bins_cap`\n    # `score = -suitable_bins_cap + item - suitable_bins_cap`\n    # `score = item - 2 * suitable_bins_cap`\n\n    # Let's scale these components if needed, but for Softmax, relative values matter.\n    # Let's use the components directly.\n    # `fit_score_component = -(suitable_bins_cap - item)`\n    # `scarcity_score_component = -suitable_bins_cap`\n    # `unified_score = fit_score_component + scarcity_score_component`\n\n    # To ensure numerical stability and meaningful distribution from Softmax,\n    # we might want to normalize or shift these scores.\n    # A common practice is to ensure scores are not excessively large or small.\n\n    # Let's consider the components:\n    # `mismatches = suitable_bins_cap - item` (non-negative)\n    # `capacities = suitable_bins_cap` (non-negative)\n\n    # We want to maximize: `-mismatches` and `-capacities`.\n    # Let's create two terms, one favoring tight fits and one favoring scarcity.\n    # Term 1 (Tight Fit): Higher score for smaller `mismatch`.\n    # E.g., `term1 = -mismatches`. A large mismatch gives a large negative score.\n    # Term 2 (Scarcity): Higher score for smaller `capacity`.\n    # E.g., `term2 = -capacities`. A large capacity gives a large negative score.\n\n    # Unified Score = `w1 * term1 + w2 * term2`\n    # Let's try equal weighting for now: `w1 = 1, w2 = 1`.\n    # `unified_score = -mismatches - capacities`\n    # `unified_score = -(suitable_bins_cap - item) - suitable_bins_cap`\n    # `unified_score = item - 2 * suitable_bins_cap`\n\n    # This score implies that if two bins have the same remaining capacity,\n    # they will have the same score regardless of the item size.\n    # If we want the tightness of fit to play a role even for bins with the same remaining capacity,\n    # we need to structure it differently.\n\n    # Let's think about the criteria:\n    # 1. Minimal remaining capacity (`R`) such that `R >= item`. This is scarcity.\n    # 2. Minimal `R - item`. This is tight fit.\n\n    # A common approach for combining criteria is to use a weighted sum of functions\n    # that represent each criterion.\n    # Let `f_scarce(R) = -R` (higher for smaller R)\n    # Let `f_fit(R, S) = -(R - S)` (higher for smaller R-S)\n\n    # Unified score: `w_s * f_scarce(R) + w_f * f_fit(R, S)`\n    # `w_s * (-R) + w_f * (-(R - S))`\n    # `-(w_s * R + w_f * R - w_f * S)`\n    # `-( (w_s + w_f) * R - w_f * S )`\n    # If `w_s = w_f = 1`: `-(2R - S) = S - 2R`. Same as before.\n\n    # The issue with `S - 2R` is that it might not differentiate well if `R` is very large.\n    # For example, if `R` is huge, the score becomes very negative.\n    # Let's consider the components separately and then combine.\n\n    # Component 1: How \"scarce\" is the bin? (Higher for less capacity)\n    # `scarce_value = 1.0 / (1.0 + suitable_bins_cap)`  # Decreasing function of capacity\n    # or `scarce_value = -suitable_bins_cap`\n\n    # Component 2: How \"tight\" is the fit? (Higher for less mismatch)\n    # `mismatch = suitable_bins_cap - item`\n    # `fit_value = 1.0 / (1.0 + mismatch)` # Decreasing function of mismatch\n    # or `fit_value = -mismatch`\n\n    # Let's try `fit_value = 1.0 / (1.0 + mismatch)` and `scarce_value = 1.0 / (1.0 + suitable_bins_cap)`.\n    # These are normalized between 0 and 1.\n    # `unified_score = fit_value + scarce_value`\n    # `unified_score = (1.0 / (1.0 + suitable_bins_cap - item)) + (1.0 / (1.0 + suitable_bins_cap))`\n\n    # This looks promising. Both terms are higher when their respective values are lower.\n    # Let's use these as the scores that will be fed into Softmax.\n\n    mismatches = suitable_bins_cap - item\n    capacities = suitable_bins_cap\n\n    # To avoid division by zero if mismatch or capacity is 0, we add 1.\n    # The terms are:\n    # fit_term = 1 / (1 + mismatch) -> higher for smaller mismatch\n    # scarce_term = 1 / (1 + capacity) -> higher for smaller capacity\n    # We want to maximize both.\n    fit_term = 1.0 / (1.0 + mismatches)\n    scarce_term = 1.0 / (1.0 + capacities)\n\n    # Unified score: A simple sum of these terms.\n    unified_scores = fit_term + scarce_term\n\n    # Apply Softmax to get probabilities\n    # Need to handle cases where scores might be very large or very small.\n    # Softmax requires exponentiation, so large positive scores will dominate.\n    # If `unified_scores` are all very small negative, `exp` might underflow.\n    # If `unified_scores` are very large positive, `exp` might overflow.\n\n    # It's good practice to shift scores so the max is 0 before exponentiating for Softmax.\n    # `max_score = np.max(unified_scores)`\n    # `shifted_scores = unified_scores - max_score`\n    # `exp_scores = np.exp(temperature * shifted_scores)`\n    # `probabilities = exp_scores / np.sum(exp_scores)`\n\n    # Let's directly compute using temperature, and handle potential issues.\n    # A high temperature will smooth out differences.\n    # A low temperature will make the highest score dominate.\n\n    # Ensure scores are not too extreme before exponentiation.\n    # We can clip the `temperature * unified_scores` argument.\n    # For example, clip to [-10, 10].\n    exponent_argument = temperature * unified_scores\n    max_exponent_val = 10.0 # Corresponds to exp(10) approx 22000\n    min_exponent_val = -10.0 # Corresponds to exp(-10) approx 4.5e-5\n\n    capped_exponent_argument = np.clip(exponent_argument, min_exponent_val, max_exponent_val)\n\n    exp_scores = np.exp(capped_exponent_argument)\n    sum_exp_scores = np.sum(exp_scores)\n\n    # Avoid division by zero if all scores were somehow zero or exp resulted in zero.\n    if sum_exp_scores == 0:\n        # Fallback: if all scores are 0 or lead to 0, assign uniform probability.\n        probabilities = np.ones_like(unified_scores) / len(unified_scores)\n    else:\n        probabilities = exp_scores / sum_exp_scores\n\n    # Place the calculated probabilities back into the main priorities array\n    priorities[suitable_bins_mask] = probabilities\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a Softmax-based approach\n    that prioritizes tight fits, considers bin scarcity, and encourages earlier bin usage.\n\n    This heuristic aims to balance several factors:\n    1.  **Tight Fit Preference:** Prioritize bins that leave minimal remaining capacity\n        after packing the item. This is achieved by assigning higher scores to bins\n        where `remaining_capacity - item` is small and non-negative.\n    2.  **Bin Scarcity/Fullness:** Bins that are already quite full (i.e., have less\n        remaining capacity) should generally be preferred over those with ample space,\n        as this helps in utilizing bins more efficiently and potentially opening up\n        space in other bins for larger items later.\n    3.  **Exploration (Softmax):** Using Softmax allows for probabilistic selection,\n        meaning even bins that are not the absolute \"best fit\" have a chance of being chosen.\n        This can prevent getting stuck in local optima and discover better packings.\n    4.  **Tie-breaking:** Implicitly, bins that are encountered earlier in the `bins_remain_cap`\n        array might receive slightly higher priority if their scores are identical to later bins,\n        due to how the scores are processed or due to inherent ordering.\n\n    The scoring for suitable bins is designed as follows:\n    For bins where `remaining_capacity >= item`:\n    - We calculate a \"fit score\" based on how tightly the item fits. A simple inverse\n      relationship with the remaining capacity is used, favoring smaller capacities.\n    - A \"scarcity score\" related to the inverse of remaining capacity can be incorporated.\n    - A common approach for balancing is to use a weighted sum or a transformation\n      that combines these aspects.\n\n    Here, we'll adapt a common approach for online bin packing heuristics that focuses on\n    the \"best fit\" principle, but modulated by Softmax for exploration.\n    A common score for \"best fit\" is the difference `remaining_capacity - item`.\n    We want to *minimize* this difference. For Softmax, we need to transform this into\n    scores where higher means more preferred.\n\n    We'll use a score inversely related to the remaining capacity for suitable bins.\n    A simple heuristic could be `1 / remaining_capacity`.\n    To incorporate the \"tight fit\" preference more directly, we can consider the\n    inverse of `(remaining_capacity - item) + epsilon` to avoid division by zero,\n    or use a sigmoid-like function on this difference.\n\n    Let's refine the reflection's idea:\n    - **Tight Fit Score:** `1 / (1 + exp(k * (remaining_capacity - item)))` as in v1. This assigns\n      higher scores to smaller positive differences.\n    - **Exploration (Softmax):** Apply Softmax to these scores to get probabilities.\n    - **Bin Scarcity/Earlier Bin Preference:** The original reflection mentioned \"earlier bin preference\".\n      This can be achieved by adding a small constant to the score of earlier bins, or by\n      ranking bins and adding a bonus based on rank. For simplicity and focusing on the\n      fit/scarcity, we'll rely on Softmax's inherent exploration. The \"scarcity\" is implicitly\n      handled by favoring bins with less `remaining_capacity`.\n\n    Let's re-evaluate the scoring to directly favor smaller remaining capacities for Softmax.\n    A score like `max_capacity - remaining_capacity` would favor fuller bins.\n    However, we also need the item to fit.\n\n    Consider the following score for suitable bins:\n    `score = some_function(remaining_capacity - item)`\n    We want `remaining_capacity - item` to be small.\n    Let's try `score = - (remaining_capacity - item)` which directly rewards smaller differences.\n    To make it suitable for Softmax (where we want positive scores), we can use:\n    `score = C - (remaining_capacity - item)` where C is a large constant.\n    Or, perhaps more intuitively, we want to *minimize* `remaining_capacity`.\n    So, a score proportional to `-remaining_capacity` or `max_capacity - remaining_capacity`\n    could work.\n\n    Let's combine:\n    1. Prioritize bins where `remaining_capacity` is small (scarcity/fullness).\n    2. Among those, prioritize tighter fits (`remaining_capacity - item` is small).\n\n    A score that captures this is to prioritize bins with smaller `remaining_capacity`.\n    So, `score = -remaining_capacity`. For Softmax, we'd use `np.exp(-remaining_capacity)`.\n    However, this doesn't directly incorporate the item size into the fit.\n\n    Let's use the \"tight fit\" score from v1, but apply Softmax to it, potentially\n    adding a small boost for bins that are generally less full (though the prompt\n    suggests prioritizing tighter fits and scarcity, which might conflict).\n\n    Let's focus on the reflection's core idea: tight fits and minimal waste,\n    and Softmax for exploration.\n    A score that reflects tight fit is `1 / (1 + exp(k * (remaining_capacity - item)))`.\n    To make this suitable for Softmax (where larger is better), we can directly use this value.\n    Bins that cannot fit the item get a score of 0.\n\n    We'll use a parameter `temperature` for the Softmax function to control exploration.\n    A higher temperature leads to more uniform probabilities (more exploration).\n    A lower temperature leads to more concentrated probabilities on the highest scores (more exploitation).\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is the probability score for the corresponding bin, derived from Softmax.\n        Bins that cannot fit the item will have a score of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n    # If no bin can fit the item, return all zeros\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # --- Scoring Mechanism ---\n    # We want to assign higher scores to bins that offer a tighter fit.\n    # The difference `remaining_capacity - item` represents the wasted space.\n    # We want to minimize this difference.\n    # A score that rewards smaller differences is `1 / (1 + exp(k * (remaining_capacity - item)))`.\n    # Let's define k. A higher k emphasizes tighter fits.\n    k = 5.0  # Sensitivity parameter for the tight fit preference\n\n    # Calculate the \"mismatch\" or wasted space for suitable bins\n    mismatch = suitable_bins_cap - item\n\n    # Calculate the raw \"preference\" score. Higher means more preferred.\n    # Using the sigmoid score: 1 / (1 + exp(k * mismatch))\n    # This score is between 0 and 1. A perfect fit (mismatch=0) gives 0.5.\n    # Smaller positive mismatches give scores closer to 0.5.\n    # Larger positive mismatches give scores closer to 0.\n    # We want to favor smaller mismatches, so larger scores are better.\n    # The current sigmoid score `1/(1+exp(k*mismatch))` correctly assigns higher values to smaller `mismatch`.\n\n    # To prevent numerical issues with exp, cap the argument.\n    max_exponent_arg = 35.0 # Corresponds to exp(35)\n    \n    capped_exponent_arg = np.minimum(k * mismatch, max_exponent_arg)\n    \n    # The raw scores: higher values mean better fits (smaller mismatch)\n    raw_scores = 1 / (1 + np.exp(capped_exponent_arg))\n\n    # --- Softmax Application ---\n    # Apply Softmax to the raw scores to get probabilities.\n    # Softmax(z_i) = exp(z_i) / sum(exp(z_j))\n    # To control exploration, we can scale the scores by a temperature parameter.\n    # `temperature` > 0.\n    # If temperature is very low (e.g., close to 0), it's like argmax (exploitation).\n    # If temperature is high, it's like uniform distribution (exploration).\n    \n    # Let's use a tunable temperature. A value like 0.1-1.0 is common.\n    # If scores are already in [0,1], scaling might not be strictly necessary,\n    # but Softmax generally expects values that can be exponentiated.\n    # We can use the raw_scores directly or scale them.\n    # Scaling by temperature: `exp(score / temperature)`.\n    # Let's try a temperature that makes the differences more pronounced or smoothed.\n    # A temperature of 1.0 means we use the raw scores directly in exp.\n    # A temperature > 1 will smooth probabilities.\n    # A temperature < 1 will sharpen probabilities.\n\n    # Let's use temperature to control the \"sharpness\" of preference.\n    # A lower temperature will favor the best fits more strongly.\n    # A higher temperature will distribute preference more evenly.\n    # Let's start with a moderate temperature, e.g., 0.5, to slightly favor better fits.\n    temperature = 0.5 # Tunable parameter for exploration/exploitation balance\n\n    # Apply exponentiation with temperature scaling\n    # Ensure we don't have issues if temperature is very close to zero or zero.\n    # If temperature is very small, `raw_scores / temperature` can become very large.\n    # We can clip the scaled scores before exp to prevent overflow if temperature is tiny.\n    # However, if temperature is 0, this is problematic. Assume temperature > 0.\n\n    # Avoid division by zero if temperature is 0 or very small and scores are high.\n    # If temperature is very small, scores with slight differences will be amplified.\n    # Let's scale the scores by `1/temperature` before `exp` for a sharper distribution.\n    # Or scale by `temperature` for a smoother distribution. The reflection says \"tuning temperature for exploration/exploitation balance\".\n    # Higher temperature -> more exploration (smoother distribution).\n    # Lower temperature -> more exploitation (sharper distribution).\n\n    # We want to explore, so let's make temperature a factor that increases probability spread.\n    # Use `exp(score / temperature)` where higher temperature spreads probabilities.\n    # So, let's use `temperature = 0.5` (lower means sharper), `temperature = 2.0` (higher means flatter).\n    # Let's set temperature to 1.0 initially for no scaling, and test.\n    # If we want to favor tighter fits more, we want smaller `mismatch` to have higher probability.\n    # `raw_scores` are already designed for this. Softmax will spread these.\n    # Higher temperature -> more uniform probability distribution.\n    # Lower temperature -> probability concentrated on the highest `raw_scores`.\n\n    # Let's use a temperature that favors exploitation slightly, i.e., a lower temperature.\n    # A temperature around 0.1-0.5 might be good for demonstrating preference.\n    # Or, a temperature of 1.0 is standard softmax. Let's try to emphasize the preference.\n    # If `raw_scores` are e.g., [0.6, 0.5, 0.4], exp([0.6, 0.5, 0.4]) = [1.82, 1.65, 1.49]\n    # Sum = 4.96. Probs = [0.36, 0.33, 0.30]. Not very sharp.\n    # If we scale by 1/temp: temp=0.1 => exp([6, 5, 4]) = [403, 148, 54]. Sum = 605. Probs = [0.66, 0.24, 0.08]. Much sharper.\n\n    # Let's use temperature `T` in `exp(score / T)`.\n    # Smaller `T` leads to sharper probabilities.\n    temperature_param = 0.5 # Lower T = more exploitation of \"best fit\"\n\n    # Ensure scores are not excessively large before exp to prevent overflow.\n    # `raw_scores` are already capped at 1.\n    # If `temperature_param` is very small, `raw_scores / temperature_param` can still be large.\n    # Let's ensure `raw_scores / temperature_param` doesn't exceed a threshold before `exp`.\n    # For example, threshold of 35 for the exponent argument.\n    scaled_scores = raw_scores / temperature_param\n    \n    # Cap the scaled scores before exponentiation to prevent overflow\n    max_scaled_score_exponent = 35.0 \n    capped_scaled_scores = np.minimum(scaled_scores, max_scaled_score_exponent)\n    \n    # Calculate the exponentiated values\n    exponentials = np.exp(capped_scaled_scores)\n\n    # Calculate the sum of exponentiated values for normalization\n    sum_exponentials = np.sum(exponentials)\n\n    # Avoid division by zero if all exponentiated values are zero (e.g., due to capping or very low scores)\n    if sum_exponentials == 0:\n        # In this case, all suitable bins are equally (un)preferred or there was an issue.\n        # A fallback could be uniform probability among suitable bins, or all zeros if it implies no good fit.\n        # Given our scores are >= 0, sum_exponentials should only be 0 if exponentials are all 0.\n        # This might happen if all raw_scores were extremely small and negative after some transformation,\n        # or if capping resulted in zero exp. But our raw_scores are positive.\n        # If sum is 0, it means all `capped_scaled_scores` were -inf (which shouldn't happen here).\n        # As a fallback, assign equal probability to suitable bins if sum is zero.\n        if suitable_bins_cap.size > 0:\n            priorities[suitable_bins_mask] = 1.0 / suitable_bins_cap.size\n        return priorities\n\n    # Calculate the final probabilities using Softmax\n    softmax_probabilities = exponentials / sum_exponentials\n\n    # Place the calculated softmax probabilities back into the main priorities array\n    priorities[suitable_bins_mask] = softmax_probabilities\n\n    return priorities\n\n[Reflection]\nFocus on precise weighting of fit and scarcity. Experiment with score normalization and temperature effects.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}