{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a refined strategy:\n    Exact Fit First, then Best Fit (smallest positive remaining capacity), and finally First Fit.\n    Incorporates an epsilon-greedy exploration.\n\n    The strategy prioritizes bins where the remaining capacity is exactly equal to the item size.\n    If no exact fit is found, it prioritizes bins that can fit the item with the smallest positive\n    remaining capacity after placement (Best Fit).\n    If no bins can fit exactly or with a small positive gap, it falls back to selecting the first\n    available bin that can fit the item (First Fit principle applied through uniform probability).\n    An epsilon-greedy component is integrated: with a small probability `epsilon`, a bin is chosen randomly\n    from the *suitable* bins to encourage exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.05  # Probability of exploring a random suitable bin\n\n    n_bins = len(bins_remain_cap)\n    priorities = np.zeros(n_bins)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    if len(suitable_bin_indices) == 0:\n        # No bin can fit the item\n        return priorities\n\n    # Epsilon-greedy choice: With probability epsilon, pick a random suitable bin\n    if np.random.rand() < epsilon:\n        chosen_index = np.random.choice(suitable_bin_indices)\n        # Assign a high priority to this randomly chosen bin\n        priorities[chosen_index] = 1.0\n        # For other suitable bins, assign a very low priority\n        priorities[suitable_bin_indices] = 1e-9\n        return priorities\n\n    # Greedy strategy:\n    # 1. Exact Fit: Prioritize bins where remaining capacity is exactly the item size.\n    exact_fit_mask = (bins_remain_cap == item) & suitable_bins_mask\n    exact_fit_indices = np.where(exact_fit_mask)[0]\n    if len(exact_fit_indices) > 0:\n        priorities[exact_fit_indices] = 1.0\n        # If there are exact fits, we only consider them as the highest priority.\n        # To ensure they are *strictly* higher, we can assign a very high value and then\n        # normalize later if needed, or set other bins to very low values.\n        # For simplicity, let's assign 1.0 to exact fits and focus on making them stand out.\n        # If other strategies are used, ensure their scores are less than 1.0.\n        # For now, we'll return here if exact fits are found, as they are the highest priority.\n        return priorities # Or normalize if we want a distribution\n\n    # 2. Best Fit: If no exact fit, prioritize bins with the smallest positive remaining capacity.\n    # Calculate the remaining space *after* placing the item.\n    remaining_space_after_fit = bins_remain_cap[suitable_bins_mask] - item\n    \n    # Find the minimum positive remaining space.\n    positive_remaining_space = remaining_space_after_fit[remaining_space_after_fit >= 0]\n\n    if len(positive_remaining_space) > 0:\n        min_positive_space = np.min(positive_remaining_space)\n        \n        # Bins that result in exactly this minimum positive space get the highest priority among non-exact fits.\n        # We can assign a score that decreases as the gap increases.\n        # Score = 1 / (gap + 1) or similar. Let's use a score that is high for small gaps.\n        # For bins that fit exactly (gap=0), this would be 1.0. For small positive gaps, it's < 1.0.\n        \n        # Calculate scores for suitable bins: higher score for smaller positive gap\n        # Use a value that is less than 1.0 but still high.\n        # A simple approach: 1.0 - (gap / (max_capacity - item + 1)) to keep it between [0, 1)\n        # More direct: assign a score based on the inverse of the gap.\n        \n        # Let's assign a priority based on the inverse of the gap.\n        # For a gap of 'g', priority = 1/(g + 1) to avoid division by zero.\n        # This ensures smaller gaps get higher priorities.\n        \n        # Apply to suitable bins only\n        best_fit_scores = np.zeros_like(bins_remain_cap)\n        best_fit_scores[suitable_bins_mask] = 1.0 / (remaining_space_after_fit + 1.0)\n        \n        # To make these distinct from exact fits (which would have a score of 1.0),\n        # we can scale them down or ensure they are less than the exact fit priority.\n        # A simple way to differentiate: add a small offset to the exact fit priority.\n        # Here, since we already returned for exact fits, we can just assign these scores.\n        \n        # To ensure Best Fit is preferred over others, we can set their priority to a\n        # value slightly less than 1, e.g., 0.9. Or use a scaled value.\n        # Let's use a scaled value that decreases with the gap.\n        \n        # A simple scaling: 1 - (gap / (average_gap + 1)) for suitable bins.\n        # A more direct approach: assign a score that favors smaller positive gaps.\n        # We can use the 'remaining_space_after_fit' directly for ranking.\n        \n        # Let's try a score that is inversely proportional to the gap, capped at a high value.\n        # For example, score = 1 - (gap / (max_possible_gap + epsilon))\n        # Or, simply rank them and assign priorities.\n        \n        # A practical approach: assign a rank-based score, or a continuous score.\n        # Let's assign a continuous score: higher for smaller positive gaps.\n        # We want to map `remaining_space_after_fit` (which is >= 0) to priorities.\n        # A mapping like `exp(-remaining_space_after_fit)` could work, but needs scaling.\n        # Let's stick to `1 / (gap + 1)` for simplicity and effectiveness.\n\n        # We are prioritizing bins where `remaining_space_after_fit` is minimal and positive.\n        # Let's re-evaluate the priority assignment for these bins.\n        # We can assign priority `1 - (gap / (max_gap_for_suitable_bins + 1))`\n        \n        # A common heuristic for Best Fit is to assign a score that is highest for the smallest gap.\n        # Let's assign a priority that is inversely proportional to the remaining capacity *after* fit.\n        # This means `1 / (bins_remain_cap[i] - item + 1e-6)` for suitable bins.\n        \n        scores_best_fit = np.zeros_like(bins_remain_cap)\n        # Ensure we only apply to suitable bins and avoid division by zero with a small epsilon.\n        gaps = bins_remain_cap[suitable_bins_mask] - item\n        \n        # The smallest *positive* gap should get the highest priority.\n        # We can assign scores that are higher for smaller positive gaps.\n        # For a gap 'g', a score like `1 / (g + epsilon)` works.\n        # Let's use a value that is clearly less than 1.0 (for exact fits).\n        # Example: 0.8 - (gap / (max_gap + 1))\n        \n        # Let's use a simple penalty for waste. A smaller penalty is better.\n        # Priority = 1 - (waste / max_waste)\n        \n        # For best fit, we want to prioritize the smallest *positive* gap.\n        # We can assign a score like `1 / (gap + 1)` to suitable bins.\n        # This will naturally give higher scores to smaller gaps.\n        \n        # Let's calculate these scores and assign them.\n        # Note: The prior 'exact_fit' handling already returned. If we reach here, no exact fit.\n        \n        scores = np.zeros_like(bins_remain_cap)\n        \n        # Calculate the gap for all suitable bins\n        gaps_for_suitable = bins_remain_cap[suitable_bins_mask] - item\n        \n        # For bins that have a positive gap, assign a priority based on the inverse of the gap.\n        # This favors smaller gaps. We add 1 to avoid division by zero if gap is 0 (which is handled by exact fit).\n        # To ensure these priorities are distinct and generally lower than a hypothetical exact fit priority,\n        # we can scale them. Let's assign a score between (0, 1) where smaller gap is closer to 1.\n        \n        # We can use `1.0 / (gaps_for_suitable + 1.0)` and then scale these values if needed.\n        # Let's try to create a priority distribution.\n        \n        # A pragmatic approach: rank the suitable bins by their gap and assign priorities.\n        # Or, assign a score that is a decreasing function of the gap.\n        \n        # For Best Fit, we want `bins_remain_cap[i] - item` to be minimized and positive.\n        # Let's use the reciprocal of the gap.\n        # `scores[suitable_bins_mask] = 1.0 / (gaps_for_suitable + 1e-6)`\n        # This would give very high scores for very small gaps.\n        # To ensure these are less than 1.0 (for exact fit), we can scale.\n        \n        # Let's use a linear decay for the priority as the gap increases.\n        # Max possible gap for a suitable bin: bin_capacity - item.\n        # Assuming bin_capacity is fixed and known, or we use the max `bins_remain_cap`.\n        \n        # A simpler approach: Assign a high priority (e.g., 0.8) to bins with the smallest positive gap,\n        # and then slightly lower priorities to others.\n        \n        # Let's assign priority based on the inverse of the gap, scaled.\n        # `priority = max_priority_best_fit * (1 - (gap / (max_gap_possible + epsilon)))`\n        # Let's try `priority = 0.8 * (1.0 / (gap + 1.0))`\n        \n        # Calculate scores for best fit: higher for smaller positive gaps.\n        # Use a value that is less than 1.0.\n        \n        scores_for_best_fit = np.zeros_like(bins_remain_cap)\n        positive_gaps_indices = np.where(gaps_for_suitable >= 0)[0] # Re-filter for non-negative gaps\n        \n        if len(positive_gaps_indices) > 0:\n            # For all suitable bins, calculate a score based on the gap.\n            # The score should be higher for smaller gaps.\n            # Let's map the gap to a score in [0, 0.9].\n            # `score = 0.9 * (1.0 - (gap / (max_gap_across_suitable_bins + 1e-6)))`\n            \n            # Let's use `1.0 / (gap + 1.0)` and then normalize or scale it.\n            # `inverse_gap_scores = 1.0 / (gaps_for_suitable + 1.0)`\n            \n            # To make these priorities reasonable, let's scale them.\n            # The maximum value of `1.0 / (gaps_for_suitable + 1.0)` occurs at the minimum gap.\n            # Let `min_positive_gap` be the smallest non-negative gap.\n            # Max score ~ `1.0 / (min_positive_gap + 1.0)`\n            \n            # Assign a base priority for best fit, e.g., 0.7. Then adjust based on gap.\n            \n            # Let's use a simpler method: Prioritize bins with smallest positive gap.\n            # We can assign priorities directly to these bins.\n            \n            # Find the minimum positive gap among suitable bins\n            min_positive_gap = np.min(gaps_for_suitable)\n            \n            # Assign a high priority to bins matching this minimum positive gap\n            # We need to be careful if multiple bins have the same minimum gap.\n            \n            # Let's assign a score that is higher for smaller gaps.\n            # A score like `1.0 - (gap / (max_suitable_gap + 1))` can work.\n            \n            # Calculate the gap for all suitable bins\n            gaps_for_suitable = bins_remain_cap[suitable_bins_mask] - item\n            \n            # We want to prioritize bins with smaller gaps.\n            # Create scores for suitable bins: higher for smaller gaps.\n            # A simple heuristic: score = 1.0 / (gap + 1.0)\n            # Let's scale these so they are distinctly less than 1.0.\n            \n            # Example: Map gaps to priorities in the range [0.5, 0.9].\n            # Max gap among suitable bins for scaling:\n            max_gap_suitable = np.max(gaps_for_suitable) if len(gaps_for_suitable) > 0 else 0\n            \n            # If max_gap_suitable is 0, all suitable bins are exact fits (already handled).\n            # So, we assume max_gap_suitable > 0 if we reach here.\n            \n            # For bins with gap `g`: priority = 0.9 - (g / (max_gap_suitable + 1.0)) * 0.4\n            # This maps 0 gap to 0.9 and max_gap_suitable to 0.5.\n            \n            priorities_for_best_fit = np.zeros_like(bins_remain_cap)\n            \n            # For each suitable bin\n            for i, bin_idx in enumerate(suitable_bin_indices):\n                gap = bins_remain_cap[bin_idx] - item\n                \n                if gap == 0: # This should not happen if exact fit is handled first\n                    continue\n                \n                # Calculate a score that is higher for smaller positive gaps\n                # Map the gap to a priority value.\n                # Let's use `1.0 / (gap + 1.0)` and scale it to be below 1.0.\n                # Example: score = 0.8 * (1.0 / (gap + 1.0))\n                # This ensures smaller gaps get higher scores.\n                \n                # Consider the range of gaps for suitable bins\n                # Let's use the difference from the minimum positive gap.\n                \n                # Simple approach: Assign priority 0.7 to all bins that fit.\n                # Then, if we want to differentiate Best Fit, we can try to adjust.\n                \n                # Let's go back to the idea of inverse of gap.\n                # `score = 1.0 / (gap + 1.0)`\n                \n                # Scale these scores to be in a range like [0.5, 0.9]\n                # Find the minimum positive gap among these suitable bins.\n                min_gap_among_suitable = np.min(gaps_for_suitable)\n                \n                # If `min_gap_among_suitable` is very small, `1.0 / (gap + 1.0)` can be large.\n                # Let's consider the range of `gaps_for_suitable`.\n                \n                # Let's try a simpler priority assignment:\n                # Prioritize bins with the smallest positive remaining capacity.\n                \n                # For all suitable bins, calculate their gap.\n                gaps = bins_remain_cap[suitable_bins_mask] - item\n                \n                # Find the minimum positive gap.\n                min_pos_gap = np.min(gaps[gaps >= 0]) # Find min non-negative gap\n                \n                # Assign a high priority to bins with this minimum positive gap.\n                # These are the best fit candidates.\n                best_fit_candidates_mask = (bins_remain_cap == item + min_pos_gap) & suitable_bins_mask\n                priorities[best_fit_candidates_mask] = 0.9 # High priority for best fit\n                \n                # For other suitable bins (that are not exact fits and not best fits)\n                # Assign a lower priority. This would be a form of \"Worst Fit\" if we wanted that,\n                # but for First Fit we'd just pick the first one.\n                # Here, we can assign a moderate priority to other suitable bins.\n                \n                other_suitable_mask = suitable_bins_mask & ~best_fit_candidates_mask\n                priorities[other_suitable_mask] = 0.5 # Moderate priority for other fits\n                \n                return priorities # Return after Best Fit assignment.\n            \n    # 3. First Fit fallback: If no exact or best fit identified (e.g., only large gaps or no suitable bins),\n    # this part ensures some priority is given if any suitable bins exist.\n    # The above logic for best fit should cover all suitable bins.\n    # If no suitable bins were found, we already returned zeros.\n    # If suitable bins were found, the `priorities` array should have non-zero values.\n\n    # If the logic above didn't assign any priorities (e.g., if only one suitable bin and it's not best/exact)\n    # or if we want a fallback for \"First Fit\" where any fitting bin is okay.\n    # If `np.sum(priorities) == 0` and `len(suitable_bin_indices) > 0`:\n    # This means no exact fit, and perhaps no clear \"best fit\" was prioritized distinctly.\n    # In such cases, we can assign uniform priorities to all suitable bins.\n    if np.sum(priorities) == 0 and len(suitable_bin_indices) > 0:\n        priorities[suitable_bin_indices] = 1.0 / len(suitable_bin_indices)\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority scores for packing an item into bins using a refined\n    priority function incorporating Softmax for exploration, bin scarcity,\n    and tie-breaking.\n\n    This heuristic aims to balance tight fits with exploration and efficiency\n    by considering:\n    1.  **Tight Fit Score (Sigmoid):** Prioritizes bins with remaining capacity\n        closest to the item size, minimizing waste.\n    2.  **Bin Scarcity:** Favors bins that have less remaining capacity overall,\n        as these are \"scarcer\" resources. This is modeled using the inverse\n        of remaining capacity.\n    3.  **Softmax for Probabilistic Selection:** Uses Softmax to convert scores\n        into probabilities, allowing for exploration of less optimal bins.\n        The temperature parameter controls the degree of exploration.\n    4.  **Tie-breaking:** Implicitly handled by the sorting order or original\n        index if scores are identical, favoring bins that appear earlier in\n        the array when scores are equal.\n\n    The combined priority for a suitable bin is a weighted sum of the tight\n    fit score and the bin scarcity score, transformed by Softmax.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is the probability score (from Softmax) for the corresponding bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # --- Heuristic Component 1: Tight Fit Score (Sigmoid) ---\n    # Parameter for the sigmoid function's steepness.\n    # Higher k means sharper preference for tighter fits.\n    k_fit = 5.0\n    mismatch = suitable_bins_cap - item\n    \n    # Use a capped sigmoid for numerical stability\n    max_exponent_arg = 35.0\n    capped_exponent_arg = np.minimum(k_fit * mismatch, max_exponent_arg)\n    tight_fit_scores = 1 / (1 + np.exp(capped_exponent_arg))\n\n    # --- Heuristic Component 2: Bin Scarcity Score ---\n    # Prioritize bins with less remaining capacity (scarcer bins).\n    # Using 1 / (capacity + epsilon) to avoid division by zero and give higher score to smaller capacities.\n    # Adding a small epsilon to avoid division by zero if a bin has 0 capacity (though unlikely if it fits the item).\n    epsilon = 1e-6\n    scarcity_scores = 1 / (suitable_bins_cap + epsilon)\n    \n    # Normalize scarcity scores to be comparable to tight_fit_scores (0 to 1 range)\n    # A simple min-max scaling can work.\n    min_scarcity = np.min(scarcity_scores)\n    max_scarcity = np.max(scarcity_scores)\n    if max_scarcity - min_scarcity > epsilon: # Avoid division by zero if all scarcity scores are the same\n        normalized_scarcity_scores = (scarcity_scores - min_scarcity) / (max_scarcity - min_scarcity)\n    else:\n        normalized_scarcity_scores = np.zeros_like(scarcity_scores) # Or assign 0.5 if all are equal and non-zero\n\n    # --- Combine Heuristics ---\n    # Weighted sum of tight fit and scarcity. Weights can be tuned.\n    # Here, we give equal weight, but this could be adjusted.\n    weight_fit = 0.5\n    weight_scarcity = 0.5\n    \n    combined_scores = (weight_fit * tight_fit_scores) + (weight_scarcity * normalized_scarcity_scores)\n\n    # --- Softmax for Probabilistic Exploration ---\n    # Temperature parameter: higher temp -> more exploration (probabilities closer to uniform)\n    # lower temp -> less exploration (probabilities closer to argmax)\n    temperature = 0.5  # Tunable parameter\n\n    # Apply Softmax\n    # Softmax(z)_i = exp(z_i / T) / sum(exp(z_j / T))\n    # Ensure combined_scores are not excessively large to avoid exp overflow even after capping.\n    # A common practice is to subtract the maximum score before exponentiation for numerical stability.\n    scores_for_softmax = combined_scores / temperature\n    \n    # Subtract max score for numerical stability in exp\n    stable_scores = scores_for_softmax - np.max(scores_for_softmax)\n    \n    exp_scores = np.exp(stable_scores)\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Place the calculated probabilities back into the main priorities array\n    priorities[suitable_bins_mask] = probabilities\n\n    return priorities\n\n[Reflection]\n1.  **Combine heuristics:** Blend multiple factors for robustness.\n2.  **Probabilistic selection:** Use Softmax for exploration.\n3.  **Data-driven tuning:** Adjust weights and parameters.\n4.  **Clear logic:** Avoid complex conditional returns for clarity.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}