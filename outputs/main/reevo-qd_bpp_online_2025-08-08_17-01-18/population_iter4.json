[
  {
    "stdout_filepath": "problem_iter1_response20.txt_stdout.txt",
    "code_path": "problem_iter1_code20.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit Decreasing logic.\n\n    This heuristic prioritizes bins that can accommodate the item and are \"tight fits\"\n    to minimize wasted space. Bins that are too small for the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate priority as the inverse of the remaining capacity\n    # A smaller remaining capacity means a tighter fit, hence higher priority.\n    # We add a small epsilon to avoid division by zero if a bin has exactly the item's size.\n    priorities[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)\n\n    return priorities",
    "response_id": 20,
    "obj": 4.048663741523748,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This heuristic implements a \"Best Fit Decreasing\" like strategy for the\n    priority function within an online First Fit context. For each item, it\n    prioritizes bins that can accommodate the item, and among those, it\n    prefers bins that will have the least remaining capacity after the item\n    is placed. This aims to \"tightly pack\" bins, potentially leaving larger\n    spaces in other bins for future larger items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Calculate remaining capacity after placing the item\n    potential_remaining_cap = bins_remain_cap - item\n    \n    # Bins that can accommodate the item have a non-negative remaining capacity\n    can_accommodate = potential_remaining_cap >= 0\n    \n    # Assign a priority:\n    # For bins that can accommodate the item, the priority is the inverse of\n    # the remaining capacity after placement. Smaller remaining capacity gets higher priority.\n    # We use a large number (e.g., 1e6) for bins that cannot accommodate the item\n    # to ensure they have a much lower priority.\n    \n    # To make smaller remaining capacities have higher priority, we can use negative.\n    # The more negative, the better.\n    priorities[can_accommodate] = -potential_remaining_cap[can_accommodate]\n    \n    # Bins that cannot accommodate the item get a very low priority (a large positive number)\n    # We can also set it to 0 if we want to strictly exclude them if a better option exists.\n    # However, a very low negative number makes more sense for sorting if we want to be\n    # sure they are not picked. For First Fit, it's about *which* bin to try first.\n    # A common way to handle \"cannot accommodate\" in priority systems is to give them a\n    # very low score. If we want to avoid them entirely, we can assign a score that will\n    # never be chosen if any other valid option exists. Let's use a value that is\n    # significantly less desirable than any valid remaining capacity.\n    # Since remaining capacities are non-negative, negative values are good.\n    # We can use a large negative number for those that cannot fit.\n    priorities[~can_accommodate] = -1e9 # A very low priority\n    \n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit strategy.\n\n    The Best Fit strategy aims to place the item into the bin that has the least remaining\n    capacity after the item is placed, provided it fits. This helps in keeping larger\n    bins available for larger items later.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a more desirable bin. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    # Calculate remaining capacity after placing the item\n    potential_remaining_caps = bins_remain_cap - item\n\n    # Initialize priorities to zero\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Find bins where the item can fit\n    fit_indices = np.where(potential_remaining_caps >= 0)[0]\n\n    if len(fit_indices) > 0:\n        # For bins that can fit the item, assign a priority.\n        # The \"best fit\" is the one that leaves the *least* remaining capacity.\n        # To make this a \"highest priority\" score, we can invert the remaining capacity\n        # (or use a large number minus remaining capacity).\n        # A simple approach is to use the negative of the potential remaining capacity.\n        # The more negative, the better the fit (i.e., less remaining space).\n        priorities[fit_indices] = -potential_remaining_caps[fit_indices]\n\n        # To ensure that a bin that is a \"perfect fit\" (leaves 0 remaining capacity)\n        # is prioritized over one that leaves, say, -1 remaining capacity (meaning it was\n        # a bit too large), we can further refine the priority.\n        # If multiple bins have the same minimum remaining capacity, any of them is fine.\n        # The current negative remaining capacity already ranks them appropriately.\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response3.txt_stdout.txt",
    "code_path": "problem_iter1_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    The Almost Full Fit strategy prioritizes bins that, after placing the item,\n    will have the least remaining capacity among bins that can still accommodate the item.\n    This aims to fill bins as much as possible before opening new ones.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot accommodate the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find bins that can accommodate the item\n    possible_bins_mask = bins_remain_cap >= item\n\n    # Calculate remaining capacity after placing the item in possible bins\n    remaining_after_placement = bins_remain_cap[possible_bins_mask] - item\n\n    # If there are no possible bins, return all zeros\n    if remaining_after_placement.size == 0:\n        return priorities\n\n    # Calculate the \"tightness\" score for possible bins.\n    # We want to minimize the remaining capacity, so a smaller remaining capacity\n    # should result in a higher priority.\n    # We use the inverse of the remaining capacity. To avoid division by zero\n    # or extremely high priorities for bins that become exactly full, we can\n    # add a small epsilon or use a scaled inverse.\n    # A common approach is to consider bins that leave little to no space.\n    # We want to maximize the chance of the bin becoming \"almost full\".\n\n    # Let's aim for a score where smaller remaining capacity is better.\n    # A simple approach is to use the negative of the remaining capacity.\n    # However, to make it a \"priority\" score (higher is better), we can\n    # invert it and potentially scale it.\n\n    # To prioritize bins that leave minimal remaining space, we can take\n    # the negative of the remaining capacity. The larger (less negative)\n    # the value, the less space is left, thus higher priority.\n    # Let's refine this: We want bins that, after placing the item, will have\n    # the *least* remaining capacity. This means we want to *minimize*\n    # `bins_remain_cap - item`.\n    # If we want higher scores to be better, we can assign a score based on\n    # the inverse of the remaining capacity.\n    # However, the goal is to fill bins. So bins that will be closest to full\n    # after placing the item are preferred.\n\n    # Consider the difference: max_capacity - (remaining_after_placement)\n    # This is effectively how much space is used. We want to maximize this.\n    # So, `item` is constant. Maximizing `bins_remain_cap[i] - remaining_after_placement[i]`\n    # means maximizing `bins_remain_cap[i] - (bins_remain_cap[i] - item)` which is just `item`.\n    # This isn't quite right.\n\n    # \"Almost Full Fit\" suggests bins that are ALMOST full.\n    # After placing the item, a bin is \"almost full\" if its remaining capacity is small.\n    # So, we want to minimize `bins_remain_cap[i] - item`.\n    # To convert this to a priority score (higher is better), we can:\n    # 1. Use `1 / (remaining_after_placement + epsilon)` where epsilon is a small number to avoid division by zero.\n    # 2. Use `-(remaining_after_placement)`\n    # 3. Use `max_possible_remaining - remaining_after_placement` for some large `max_possible_remaining`\n    #    which is equivalent to `some_constant - remaining_capacity`.\n\n    # Let's try option 2: higher priority for smaller remaining capacity.\n    # So, `priority = -remaining_capacity`. This means a bin with remaining_capacity=1\n    # gets priority -1, and a bin with remaining_capacity=0 gets priority 0.\n    # This seems to fit the idea of \"smallest remaining capacity\".\n\n    # To make it more \"priority-like\" (higher is better), we can use:\n    # `priority = C - remaining_capacity`, where C is a large constant, or\n    # `priority = 1 / (remaining_capacity + epsilon)`\n\n    # Let's use the concept that the *difference* between what's remaining and what's desired (a full bin)\n    # should be minimized. So, `remaining_capacity` should be small.\n    # A score that reflects this: `max(0, C - remaining_capacity)`.\n    # If we want to prioritize bins that become *most* full after the item,\n    # this means the remaining capacity is minimized.\n\n    # Consider the bin that would become \"most full\". This is the bin where\n    # `bins_remain_cap[i] - item` is minimized.\n    # So, higher priority for smaller `bins_remain_cap[i] - item`.\n    # Let's transform `bins_remain_cap[i] - item` into a priority:\n    # `priority = some_large_value - (bins_remain_cap[i] - item)`\n    # This is equivalent to `some_large_value - bins_remain_cap[i] + item`.\n    # Since `item` is constant for all bins, this is `constant - bins_remain_cap[i]`.\n    # We want to maximize this, so we want to minimize `bins_remain_cap[i]`.\n\n    # Another perspective on \"almost full\":\n    # We want to pick the bin that, after placing the item, is closest to full.\n    # This means `bins_remain_cap[i] - item` is minimal.\n    # Let's assign a priority based on this minimum value.\n\n    # A common heuristic for \"Best Fit\" or \"Almost Full Fit\" is to try to minimize\n    # the remaining capacity of the bin *after* placing the item.\n    # This means we want to select the bin `i` that minimizes `bins_remain_cap[i] - item`.\n    # To convert this into a priority score (where higher means more desirable),\n    # we can use `-(bins_remain_cap[i] - item)` or `1 / (bins_remain_cap[i] - item + epsilon)`.\n\n    # Let's try using `1 / (remaining_after_placement + epsilon)` where epsilon is small.\n    # This ensures that bins with very small remaining capacity get very high priorities.\n\n    epsilon = 1e-6\n    priorities[possible_bins_mask] = 1.0 / (remaining_after_placement + epsilon)\n\n    # Alternatively, if we want to give a higher score to bins that leave *less*\n    # remaining space, we can simply use the negative of the remaining space,\n    # or a scaled version of it.\n\n    # Let's consider the original \"Almost Full Fit\" idea: prioritize bins that are\n    # already close to full. However, the problem statement for this function\n    # implies prioritizing based on the *outcome* after placing the item.\n    # \"priority score for each bin ... bin with the highest priority score will be selected for the item.\"\n\n    # If the goal is to fill bins, then after placing the item, we want the bin\n    # with the *least* remaining capacity.\n    # So, `priority = f(remaining_capacity)` where `f` is decreasing.\n    # `f(x) = 1/(x + epsilon)` or `f(x) = -x`.\n    # Let's use the `1/(x + epsilon)` approach for a strong preference towards tighter fits.\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response6.txt_stdout.txt",
    "code_path": "problem_iter4_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Sigmoid Fit Score.\n\n    This heuristic prioritizes bins that offer the \"tightest fit\" for an incoming item.\n    A tight fit means the bin has just enough remaining capacity to accommodate the item,\n    minimizing wasted space. Bins that are too small are excluded, and among the\n    suitable bins, those with less remaining capacity (but still sufficient) are preferred.\n\n    The scoring is based on a sigmoid function applied to the difference between\n    the bin's remaining capacity and the item's size. Specifically, for bins where\n    `remaining_capacity >= item`, the score is calculated as:\n\n    `score = 1 / (1 + exp(k * (remaining_capacity - item)))`\n\n    Here:\n    - `k` is a sensitivity parameter that controls how quickly the priority drops\n      as the remaining capacity exceeds the item size. A higher `k` means a sharper\n      preference for tighter fits.\n    - `remaining_capacity - item` is the \"mismatch\" or wasted space.\n    - When `remaining_capacity == item` (perfect fit), the exponent is 0, `exp(0)=1`,\n      and the score is `1 / (1 + 1) = 0.5`.\n    - When `remaining_capacity > item` (mismatch > 0), the exponent is positive.\n      As `remaining_capacity` increases, the exponent `k * (remaining_capacity - item)`\n      increases, `exp(...)` increases, `1 + exp(...)` increases, and thus the score\n      decreases (approaching 0 for very large capacities). This correctly penalizes\n      bins with excessive remaining space.\n\n    This approach ensures that bins with smaller positive mismatches (tighter fits)\n    receive higher priority scores than bins with larger positive mismatches.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is the priority score for the corresponding bin. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n    # If no bin can fit the item, return all zeros\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # Parameter for the sigmoid function's steepness.\n    # A higher value makes the function drop faster as capacity increases past the item size.\n    # This encourages selecting bins that are closer to the item size.\n    k = 5.0  # Tunable parameter\n\n    # Calculate the \"mismatch\" or wasted space for suitable bins\n    # mismatch = suitable_bins_cap - item\n    # We want to give higher priority when mismatch is small (close to 0).\n    # The function 1 / (1 + exp(k * mismatch)) achieves this:\n    # - If mismatch = 0, score = 1 / (1 + exp(0)) = 0.5\n    # - If mismatch > 0 (but small), exp(k*mismatch) is slightly > 1, score is slightly < 0.5\n    # - If mismatch is large positive, exp(k*mismatch) is very large, score approaches 0.\n\n    # Calculate the sigmoid scores for the suitable bins\n    # To avoid potential overflow with exp(k * mismatch) if mismatch is very large,\n    # we can consider the range of `suitable_bins_cap`. If `suitable_bins_cap`\n    # can be extremely large compared to `item`, `k * (suitable_bins_cap - item)`\n    # can be a very large positive number, leading to `exp()` overflowing.\n    # A robust way to handle this is to clip the argument to the exponential or\n    # use a more numerically stable sigmoid implementation if necessary.\n    # For typical BPP scenarios, direct calculation might be acceptable.\n    # If `suitable_bins_cap - item` becomes very large, `exp` might overflow.\n    # We can cap the argument to `exp` to prevent overflow.\n    # A practical upper bound for `k * (capacity - item)` can be set.\n    # For example, if `k=5`, `exp(35)` is already very large. Let's cap at 35.\n    mismatch = suitable_bins_cap - item\n    exponent_arg = k * mismatch\n    \n    # Cap the exponent argument to prevent overflow in np.exp\n    # A value of 700 is a common threshold for `exp` to return inf.\n    # If `k * mismatch` is, say, 40, `exp(40)` is large but manageable.\n    # If `k * mismatch` is 1000, `exp(1000)` is infinity.\n    # Let's cap the argument to a reasonable value, say 35, to keep `exp` within range,\n    # or handle `inf` gracefully. If `exp` becomes `inf`, the score becomes 0.\n    # A simpler approach is to ensure `k` and `mismatch` product doesn't exceed a threshold.\n    # Let's assume typical capacities and k are such that direct calculation is fine,\n    # but for robustness, we'll consider capping.\n    \n    # Let's use a threshold for `k * mismatch`. If `k * mismatch > threshold`,\n    # then `exp(k * mismatch)` will be very large, and the score will be close to 0.\n    # A threshold like 30-40 for the exponent is usually sufficient to make `exp` very large.\n    # Let's use a maximum argument to exp to prevent overflow.\n    max_exponent_arg = 35.0 # Corresponds to exp(35) which is ~3.4e15\n    \n    capped_exponent_arg = np.minimum(exponent_arg, max_exponent_arg)\n    \n    sigmoid_scores = 1 / (1 + np.exp(capped_exponent_arg))\n\n    # Place the calculated sigmoid scores back into the main priorities array\n    priorities[suitable_bins_mask] = sigmoid_scores\n\n    return priorities",
    "response_id": 6,
    "obj": 3.948942959712818,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response4.txt_stdout.txt",
    "code_path": "problem_iter3_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, temperature: float = 1.0) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a softmax-based heuristic.\n\n    This heuristic prioritizes bins that can accommodate the item, with a stronger\n    preference for \"tight fits\" (bins with remaining capacity close to the item size).\n    The `temperature` parameter controls the exploration vs. exploitation trade-off.\n    Higher temperatures lead to more uniform probabilities (more exploration), while\n    lower temperatures focus on the best-fitting bins (more exploitation).\n    Bins that are too small for the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n        temperature: Controls the sharpness of the softmax distribution.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score (probability) of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate a \"goodness\" score.\n    # We want to prioritize bins where the remaining capacity is close to the item size.\n    # A common approach is to use the difference (remaining_capacity - item).\n    # To prioritize smaller differences (tighter fits), we can use the negative of this difference.\n    # We add a small epsilon to avoid issues when remaining_capacity == item.\n    # A larger negative value (more negative) indicates a worse fit, a value closer to zero is a better fit.\n    # To make better fits have higher scores for softmax, we can invert this or use a different metric.\n    # Let's try prioritizing based on how much capacity is LEFT OVER after packing.\n    # So, (remaining_capacity - item) is what we want to minimize.\n    # For softmax, higher values mean higher probability. So, we want a metric that is\n    # higher for better fits. A good metric would be the negative of the leftover capacity,\n    # or a Gaussian-like function centered at 0 difference.\n    # Let's use negative difference, then scale it to make it more sensitive to near fits.\n    # A simple transformation that boosts near-fits and reduces others:\n    # Consider -(bins_remain_cap[can_fit_mask] - item) which is (item - bins_remain_cap[can_fit_mask]).\n    # This value is negative or zero. Higher values (closer to zero) are better fits.\n    # To make it suitable for softmax where higher is better, we can use `-(bins_remain_cap[can_fit_mask] - item)`.\n    # However, this might still be too sensitive to very small items.\n    # Let's consider a score that is high when `bins_remain_cap[can_fit_mask] - item` is small and positive.\n    # The inverse `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` from v1 is good.\n    # Let's refine this. We want to reward bins where `bins_remain_cap - item` is small.\n    # A Gaussian-like kernel centered at 0 difference could work: exp(- (diff^2) / (2 * sigma^2))\n    # Or, a simpler approach: consider `1 / (1 + diff)` where diff is `bins_remain_cap - item`.\n    # If diff is small and positive, score is close to 1. If diff is large, score approaches 0.\n\n    # Let's try a score that emphasizes small positive differences.\n    # We want a high score when `bins_remain_cap[can_fit_mask] - item` is small and positive.\n    # Consider `1.0 / (1.0 + (bins_remain_cap[can_fit_mask] - item))`\n    # This gives scores between (0, 1] for valid fits. 1 for perfect fits.\n\n    # Alternative: Prioritize bins with minimum remaining capacity that can fit the item.\n    # This is essentially the \"Best Fit\" strategy. For a heuristic priority, we can\n    # use the inverse of the remaining capacity for fitting bins.\n    # `priorities[can_fit_mask] = 1.0 / bins_remain_cap[can_fit_mask]` - This prioritizes smallest bins.\n    # If we want to prioritize tight fits, we are looking for bins where `bins_remain_cap - item` is small.\n    # So, we want to maximize `- (bins_remain_cap[can_fit_mask] - item)`.\n    # Or, a score that is high for small positive `bins_remain_cap[can_fit_mask] - item`.\n    # Let's use `-(bins_remain_cap[can_fit_mask] - item)` directly, then rescale or apply softmax.\n    # The intuition of `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` was good.\n    # Let's enhance it for \"nearness\".\n\n    # Consider the \"wasted space\" after packing: `wasted_space = bins_remain_cap - item`.\n    # We want to minimize `wasted_space`. So, a higher priority should be given to bins with smaller `wasted_space`.\n    # Let's transform `wasted_space` into a score where smaller `wasted_space` yields a higher score.\n    # A simple transformation: `score = 1.0 / (1.0 + wasted_space)`.\n    # This gives scores in the range (0, 1]. Perfect fit -> score 1. Large wasted space -> score close to 0.\n    # This should provide a good signal for \"tight fits\".\n\n    wasted_space = bins_remain_cap[can_fit_mask] - item\n    # Using `1.0 / (1.0 + wasted_space)` maps small positive wasted space to values close to 1.\n    # For a perfect fit (wasted_space = 0), score is 1.\n    # For larger wasted_space, score decreases.\n    scores_for_softmax = 1.0 / (1.0 + wasted_space)\n\n    # Apply softmax to get probabilities.\n    # Ensure temperature is positive to avoid division by zero or invalid operations.\n    if temperature <= 0:\n        raise ValueError(\"Temperature must be positive.\")\n\n    # Calculate exponentiated scores, scaled by temperature\n    # Lower temperature means sharper distribution, higher temperature means flatter.\n    exp_scores = np.exp(scores_for_softmax / temperature)\n\n    # Normalize to get probabilities\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Assign probabilities to the original priorities array\n    priorities[can_fit_mask] = probabilities\n\n    # Normalize priorities to sum to 1, ensuring valid probability distribution\n    # This is already handled by the softmax if there's at least one bin that can fit.\n    # If no bins can fit, priorities remains all zeros, which is correct.\n    if np.sum(priorities) > 0:\n        priorities /= np.sum(priorities)\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.\n\n    The strategy assigns higher priority to bins that have a remaining capacity\n    just slightly larger than the item size, aiming to fill bins more compactly.\n    A temperature parameter controls the \"softness\" of the softmax, influencing\n    how aggressively we favor these \"almost fitting\" bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Filter out bins that cannot fit the item\n    can_fit_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if not eligible_bins_cap.size:\n        # If no bin can fit the item, return zeros for all original bins\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit score\": how much space is left after placing the item.\n    # Smaller values indicate a tighter fit.\n    fit_scores = eligible_bins_cap - item\n\n    # Use a Softmax-like approach to convert fit scores to priorities.\n    # We invert the fit scores to give higher priority to smaller remaining capacities (tighter fits).\n    # Adding a small epsilon to avoid division by zero or log(0) if all fit_scores are 0.\n    epsilon = 1e-9\n    inverted_fit_scores = 1.0 / (fit_scores + epsilon)\n\n    # The temperature parameter controls the \"softness\" of the softmax.\n    # A lower temperature makes the distribution sharper (more peaky),\n    # favoring the best fitting bins more strongly.\n    # A higher temperature makes it more uniform.\n    temperature = 0.5  # This can be tuned as a hyperparameter\n\n    # Apply softmax to the inverted fit scores\n    try:\n        exp_scores = np.exp(inverted_fit_scores / temperature)\n        softmax_priorities = exp_scores / np.sum(exp_scores)\n    except OverflowError:\n        # Handle potential overflow if scores become too large\n        # In such cases, a simple proportional scaling might be better\n        # or clamping the input to exp. For simplicity here, we can\n        # assign equal high probability to all if overflow occurs.\n        softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size\n\n\n    # Create the final priority array, mapping priorities back to original bin indices\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[can_fit_mask] = softmax_priorities\n\n    return priorities",
    "response_id": 19,
    "obj": 4.048663741523748,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response10.txt_stdout.txt",
    "code_path": "problem_iter1_code10.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit Decreasing-like heuristic.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # First Fit strategy: find the first bin that can accommodate the item.\n    # For priority, we want to favor bins that are a \"tight fit\" but still fit.\n    # This means we prefer bins where the remaining capacity is just enough for the item.\n    # A bin with remaining capacity exactly equal to the item size is ideal.\n    # Bins that are too small should have a priority of 0.\n\n    # Create a mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities to a very low value (effectively zero for unusable bins)\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # For bins that can fit the item, calculate a priority score.\n    # We want to prioritize bins where the remaining capacity is closest to the item size,\n    # without going below the item size.\n    # The difference (bins_remain_cap - item) represents the \"slack\".\n    # We want to minimize slack, so a smaller difference is better.\n    # However, we want the *first* such bin in the array to be prioritized in case of ties,\n    # which is naturally handled by numpy's vectorized operations if we consider\n    # negative of the slack as a priority. A smaller slack means a larger negative slack,\n    # which translates to a higher priority in a max-priority queue sense.\n\n    # Calculate slack for bins that can fit the item\n    slack = bins_remain_cap[can_fit_mask] - item\n\n    # The priority is the negative of the slack.\n    # Smaller slack -> larger negative slack -> higher priority.\n    # If multiple bins have the same slack, their relative order in the original\n    # bins_remain_cap array will be preserved in terms of priority calculation.\n    priorities[can_fit_mask] = -slack\n\n    # In a true First Fit, we'd just take the first bin that fits.\n    # To simulate this \"first fit\" behavior in a priority context,\n    # we can add a small bonus to earlier bins with good fits, or more directly,\n    # simply return priorities such that the first available bin with the \"best\" fit\n    # (smallest slack) gets the highest priority. The `-slack` already achieves this.\n    # If multiple bins have the same minimal slack, the one appearing first in the\n    # `bins_remain_cap` array will naturally get the higher priority due to the way\n    # numpy operations often preserve order in selection when values are equal.\n    # For absolute certainty of 'first fit' logic within this priority framework,\n    # we can make bins that are \"exact fits\" have a slightly higher priority\n    # than slightly looser fits.\n\n    # Refined priority: Exact fits (slack=0) get highest priority.\n    # Then, among bins that fit, prioritize smaller slack (tighter fit).\n    # The current -slack already prioritizes smaller slack.\n    # To enforce the \"first fit\" aspect: consider the index.\n    # A bin with smaller index is preferred if slack is equal.\n\n    # Let's use a more explicit priority:\n    # 1. Highest priority for exact fits.\n    # 2. Then, prioritize bins with smaller slack.\n    # 3. If slack is equal, prioritize the bin with the smaller index.\n\n    # Initialize priorities for fitting bins\n    fit_priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Calculate slack for fitting bins\n    slack_values = bins_remain_cap[can_fit_mask] - item\n\n    # Assign priorities:\n    # For exact fits (slack_values == 0), assign a very high priority (e.g., 1e9)\n    # For other fits, assign priority based on negative slack.\n    # To break ties and enforce \"first fit\", we can penalize later bins.\n    # Let's assign priority = 10000 - slack - (index * 0.1) for fitting bins.\n    # This way, smaller slack is better, and smaller index is better for same slack.\n\n    indices = np.where(can_fit_mask)[0]\n    # Create a score: (ideal_fit - slack) + (bonus_for_early_bins)\n    # We want to maximize this score.\n    # Ideal fit = 0 (when remaining_capacity == item)\n    # So, priority component from slack is -slack.\n    # Bonus for early bins: -index * small_constant.\n    # We want to maximize priority.\n    # Maximize: (-slack) - (index * 0.01)\n    \n    # A simpler way is to use a very large number for exact fits, then -slack.\n    # Let's consider the \"best\" fit for the priority.\n    # The best fit is the one that minimizes `remaining_capacity - item`.\n    # This is equivalent to maximizing `-(remaining_capacity - item)`.\n    # So, `priority = -(remaining_capacity - item)` for fitting bins.\n    # To ensure first-fit, if there are multiple bins with the same minimal slack,\n    # the one with the lower index should be preferred.\n    # We can achieve this by adding a very small penalty to the priority based on index.\n    # `priority = -(remaining_capacity - item) - index * epsilon`\n    # where epsilon is a very small positive number. This ensures that a bin at a\n    # lower index with the same slack gets a slightly higher priority.\n\n    epsilon = 1e-6  # Small value to break ties for first-fit\n    fit_priorities[can_fit_mask] = -(slack_values) - (indices * epsilon)\n    \n    # We want the bin with the highest priority score to be selected.\n    # The current calculation `-(slack_values) - (indices * epsilon)` will work.\n    # A more direct \"First Fit Decreasing-like\" priority would be to assign\n    # priorities such that the smallest slack is maximized.\n    # And for ties in slack, the lowest index is maximized.\n    # So, `priority = (some_large_number - slack) - index * epsilon`.\n    # Or simply, `priority = -slack - index * epsilon`.\n    # A higher value means higher priority.\n\n    # Final check: The priority should reflect our preference.\n    # We prefer bins that are a tight fit, and among tight fits, the earliest one.\n    # This means a bin where `bins_remain_cap - item` is small is good.\n    # And smaller index is good for ties.\n    # So, the score should be high for small `bins_remain_cap - item` and small `index`.\n    # Let's use a score that is large for best fits:\n    # Priority = MAX_SCORE - (bins_remain_cap - item) - (index * penalty)\n    # A large MAX_SCORE ensures any fitting bin is better than non-fitting.\n    \n    MAX_BENEFIT_SCORE = 1000000  # A large number to indicate a good fit\n    INDEX_PENALTY_FACTOR = 1000  # Penalty for later bins\n\n    # Prioritize bins that can fit the item.\n    # For bins that fit, the score is determined by how \"tight\" the fit is\n    # (smaller remaining capacity after placement is better) and by their index\n    # (earlier bins are preferred for first-fit).\n\n    # Calculate the \"tightness\" score: a larger value means a tighter fit (less waste).\n    # We want to maximize `MAX_BENEFIT_SCORE - slack`.\n    tightness_score = np.zeros_like(bins_remain_cap)\n    tightness_score[can_fit_mask] = MAX_BENEFIT_SCORE - slack\n\n    # Introduce a penalty for bins with higher indices to enforce the \"first fit\" logic.\n    # Subtract a scaled index. Smaller index should have higher priority.\n    index_penalty = indices * INDEX_PENALTY_FACTOR\n    \n    # Combine scores. We want to maximize the overall priority.\n    # Prioritize based on tightness, then index.\n    # This means higher tightness_score is better.\n    # Lower index_penalty is better.\n    # So, we want to MAXIMIZE: `tightness_score - index_penalty`\n    \n    priorities = np.full_like(bins_remain_cap, -np.inf) # Initialize with low priority\n\n    if np.any(can_fit_mask):\n        fitting_indices = np.where(can_fit_mask)[0]\n        fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        \n        # Calculate the \"fit quality\": how close the remaining capacity is to the item size.\n        # A smaller difference (bins_remain_cap - item) is better.\n        fit_quality = -(fitting_bins_remain_cap - item) # Maximize this (smaller difference is better)\n\n        # To implement \"First Fit\", we prefer earlier bins when fit quality is the same.\n        # We can achieve this by adding a small bonus for earlier indices.\n        # This means we want to maximize `fit_quality + bonus_for_early_bins`.\n        # The bonus should decrease with index. So, `bonus = constant - index * small_factor`.\n        # Let's use a simple approach: a very high priority for the tightest fits,\n        # and then penalize for looser fits and later indices.\n\n        # Consider the inverse: we want to minimize waste (slack) and index.\n        # So, we want to minimize `slack + index * epsilon`.\n        # Priority should be inverse of this minimization.\n        # Priority = - (slack + index * epsilon)\n\n        epsilon_tiebreaker = 1e-6\n        priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item) - (np.arange(len(bins_remain_cap))[can_fit_mask] * epsilon_tiebreaker)\n\n    # The current priority definition maximizes (tight fit) + (early index).\n    # This correctly aligns with First Fit logic.\n    return priorities",
    "response_id": 10,
    "obj": 4.048663741523748,
    "SLOC": 23.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response14.txt_stdout.txt",
    "code_path": "problem_iter1_code14.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First strategy.\n\n    The Exact Fit First strategy prioritizes bins where the remaining capacity is exactly equal\n    to the item size. Bins that can fit the item but not exactly are given a lower priority,\n    with larger remaining capacities being less preferred. Bins that cannot fit the item\n    receive zero priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Prioritize bins where remaining capacity is exactly the item size\n    exact_fit_mask = (bins_remain_cap == item) & can_fit_mask\n    priorities[exact_fit_mask] = 1.0\n\n    # For bins that can fit but not exactly, give a lower priority\n    # We want to penalize bins with a lot of leftover space after placing the item.\n    # So, we assign a priority that decreases as (remaining_capacity - item_size) increases.\n    # A simple way is to use 1 / (remaining_capacity - item_size + 1) to avoid division by zero\n    # and ensure non-zero priorities for valid fits.\n    partial_fit_mask = (~exact_fit_mask) & can_fit_mask\n    remaining_space_after_fit = bins_remain_cap[partial_fit_mask] - item\n    priorities[partial_fit_mask] = 1.0 / (remaining_space_after_fit + 1) # Adding 1 to avoid division by zero if remaining_space is 0, which is handled by exact_fit_mask anyway.\n\n    # Ensure that exact fits have higher priority than partial fits.\n    # Since we set exact fits to 1.0, and partial fits to values < 1.0 (as remaining_space_after_fit >= 0),\n    # this condition is naturally met.\n\n    return priorities",
    "response_id": 14,
    "obj": 4.198244914240141,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response29.txt_stdout.txt",
    "code_path": "problem_iter1_code29.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    This heuristic prioritizes bins that have a remaining capacity slightly larger than the item,\n    aiming to minimize wasted space. It uses a softmax function to convert these differences\n    into probabilities, effectively assigning higher priority to bins that are a \"good fit\".\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the \"fit\" of the item into each bin.\n    # We want bins where bins_remain_cap is just enough or slightly more than the item.\n    # A negative value here means the item doesn't fit. We can clamp these to a small\n    # positive value or 0 to avoid issues with softmax if all items don't fit.\n    # A large positive difference (item fits easily) is also not ideal as it wastes space.\n    # So we want the difference (bins_remain_cap - item) to be close to zero.\n    # We can use the negative of this difference as the exponent in softmax,\n    # so smaller (bins_remain_cap - item) results in a higher exponent.\n    fits = bins_remain_cap - item\n\n    # Filter out bins where the item does not fit (remaining capacity < item size)\n    # Assign a very low priority (or effectively zero) to these bins.\n    # A large negative number in softmax exponent will result in a value close to 0.\n    # We can also directly set their fits to a very low value before softmax.\n    fits[fits < 0] = -np.inf # Effectively 0 probability after softmax\n\n    # Apply the softmax function. The exponent in softmax should reflect desirability.\n    # We want bins with (bins_remain_cap - item) close to 0 to have high priority.\n    # So, we can use -(bins_remain_cap - item) as the exponent, or more intuitively,\n    # (item - bins_remain_cap) as the exponent, ensuring negative values become smaller.\n    # Or even better, prioritize bins with a positive difference close to zero.\n    # Let's define desirability as: higher is better if remaining_cap is slightly > item.\n    # Consider a transformation: exp(-abs(fits)). This would favor fits near 0.\n    # However, softmax typically takes logits (raw scores).\n    # Let's consider the score `s_i = -(bins_remain_cap_i - item)^2`. This penalizes\n    # being too small or too large. But we only care about `bins_remain_cap_i >= item`.\n    # So, if `bins_remain_cap_i < item`, priority should be 0.\n    # If `bins_remain_cap_i >= item`, we want `bins_remain_cap_i - item` to be small.\n    # A good transformation for softmax could be to map `bins_remain_cap_i - item` to a\n    # desirability score. Higher `bins_remain_cap_i` than `item` is good, but not too high.\n    # Let's use `bins_remain_cap_i - item` directly as logits, but only for bins that can fit.\n    # For bins that cannot fit, their logit should be extremely low.\n\n    # Option 1: Softmax on (bins_remain_cap - item) for fitting bins.\n    # Prioritize bins where remaining capacity is *exactly* the item size.\n    # Using -fits will invert the ordering, so smaller positive diffs become larger.\n    # exp(-fits) where fits = bins_remain_cap - item\n    # If bins_remain_cap = 5, item = 3, fit = 2. exp(-2) = 0.135\n    # If bins_remain_cap = 3, item = 3, fit = 0. exp(0) = 1.0\n    # If bins_remain_cap = 6, item = 3, fit = 3. exp(-3) = 0.049\n    # This prioritizes exact fits.\n\n    # We need to ensure that we don't assign probabilities to bins where the item doesn't fit.\n    # A common way to do this with softmax is to assign a very low logit (large negative number).\n    logits = bins_remain_cap - item\n    # For bins that cannot fit the item, set their logit to a very small number.\n    # This will make their softmax probability close to zero.\n    logits[logits < 0] = -1e9  # A large negative number\n\n    # For bins that can fit, we want to prioritize those with smaller remaining capacity (closer to item size).\n    # So, we want to exponentiate -(bins_remain_cap - item) which is (item - bins_remain_cap).\n    # However, the softmax input should ideally be positive values that represent scores.\n    # Let's redefine `scores`: a higher score means better fit.\n    # A perfect fit would have score X.\n    # A slightly larger capacity would have score X - epsilon.\n    # A much larger capacity would have score X - delta.\n    # A capacity smaller than item would have score -infinity.\n    # So, we can use `item - bins_remain_cap` as our underlying measure for exponentiation,\n    # but we need to handle the case where `bins_remain_cap < item`.\n\n    # Let's use a modified approach. We want bins that *can* fit, and among those,\n    # we prefer bins with less excess capacity.\n    # So, a \"goodness\" score could be:\n    # -infinity if item > bins_remain_cap\n    # -(bins_remain_cap - item) otherwise (prioritizing smaller differences)\n\n    scores = -(bins_remain_cap - item)\n    scores[bins_remain_cap < item] = -np.inf # Ensure items not fitting get no chance\n\n    # Now apply softmax. Softmax(x_i) = exp(x_i) / sum(exp(x_j)).\n    # The `scores` directly go into the exponent of softmax.\n    # Higher scores (closer to 0, or less negative) mean higher probability.\n    # If scores = [-inf, -inf, 0, -2, -5],\n    # exp(scores) = [0, 0, 1, exp(-2), exp(-5)]\n    # Softmax will normalize these.\n\n    # Add a small constant to the score for bins that can fit, to ensure\n    # that even a small positive difference doesn't get zero probability.\n    # For instance, if we use -(bins_remain_cap - item), a difference of 10\n    # gives exp(-10) which is tiny.\n    # Perhaps a linear scaling or a different transformation is better.\n    # Let's try mapping `bins_remain_cap - item` to a desirability score.\n    # If diff = 0, score = K (high)\n    # If diff = 1, score = K - epsilon\n    # If diff = 10, score = K - delta\n    # If diff < 0, score = -infinity\n\n    # Let's re-think the logit construction for softmax.\n    # We want the probability P_i proportional to exp(logit_i).\n    # Desirability of bin i for item: D_i\n    # P_i = exp(alpha * D_i) / sum(exp(alpha * D_j))\n    # If item doesn't fit: D_i = -infinity\n    # If item fits and diff = bins_remain_cap - item:\n    # We want smaller diffs to have higher D_i.\n    # So, D_i = -(bins_remain_cap - item) = item - bins_remain_cap.\n\n    # This approach correctly prioritizes bins with less remaining capacity over\n    # bins with more remaining capacity, for those that can fit the item.\n    # The `-np.inf` for non-fitting bins ensures they get 0 probability.\n    # The `alpha` parameter (implicitly 1 here) controls the \"peakiness\" of the distribution.\n\n    # Calculate the underlying scores for softmax.\n    # A higher score means more desirable.\n    # If bins_remain_cap >= item, we want bins_remain_cap - item to be small.\n    # So, let's use `-(bins_remain_cap - item)` which is `item - bins_remain_cap`.\n    # For bins where `bins_remain_cap < item`, the score should be very low.\n\n    scores = item - bins_remain_cap\n\n    # Set scores to a very low value for bins where the item does not fit.\n    # This ensures their probability contribution in softmax is negligible.\n    scores[bins_remain_cap < item] = -1e9 # Effectively -infinity for softmax\n\n    # Calculate probabilities using softmax.\n    # Adding 1 to scores for fitting bins can help avoid all zeros if all diffs are large negative\n    # but we've handled that with -1e9 for non-fitting bins.\n    # The softmax calculation itself: exp(scores) / sum(exp(scores))\n    # We can use np.exp directly, but be mindful of overflow/underflow if scores are very large/small.\n    # If scores = [-1e9, -1e9, 0, -2, -5], then exp(scores) = [0, 0, 1, exp(-2), exp(-5)]\n    # This seems correct.\n    exp_scores = np.exp(scores)\n\n    # If all items resulted in -1e9 (meaning item doesn't fit any bin), exp_scores will be all zeros.\n    # In this case, the sum will be zero, leading to division by zero. Handle this edge case.\n    sum_exp_scores = np.sum(exp_scores)\n    if sum_exp_scores == 0:\n        # This means the item cannot fit into any available bin.\n        # Return zero priorities for all bins.\n        return np.zeros_like(bins_remain_cap)\n\n    priorities = exp_scores / sum_exp_scores\n\n    return priorities",
    "response_id": 29,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response7.txt_stdout.txt",
    "code_path": "problem_iter4_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a blended strategy.\n\n    This strategy prioritizes exact fits, uses a decay function for near-exact fits,\n    and applies a Softmax-like normalization for smooth probability distribution.\n    It aims to favor bins where the remaining capacity is closest to the item size,\n    with a strong preference for exact matches.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if not eligible_bins_cap.size:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit residual\": how much space is left after placing the item.\n    # Smaller values indicate a tighter fit.\n    fit_residuals = eligible_bins_cap - item\n\n    # Prioritize exact fits (residual = 0). For near-exact fits, use an exponential\n    # decay based on the residual. Bins with smaller residuals get higher scores.\n    # A small epsilon is added to the residual to ensure that exact fits (residual=0)\n    # get a distinct, higher score than bins that leave a tiny positive residual.\n    # The decay_factor controls how quickly the priority drops as the residual increases.\n    decay_factor = 0.5\n    # We want smaller residuals to have higher scores.\n    # An exponential decay is suitable: exp(-decay_factor * residual)\n    # For residual = 0 (exact fit), score is exp(0) = 1.0.\n    # For residual > 0, score decreases.\n    # Add a small constant to the exponent to ensure that even exact fits have a score\n    # that can be part of a meaningful softmax, avoiding potential issues if all residuals are 0.\n    # Alternatively, we can explicitly set exact fits to a high base value.\n    \n    # Strategy:\n    # 1. Exact fits get a high score (e.g., 1.0).\n    # 2. Near-exact fits get a score based on exponential decay of the residual.\n    # 3. Use softmax to normalize these scores into probabilities.\n\n    # Base scores: 1.0 for exact fits, and an exponentially decaying score for others.\n    # The decay_rate ensures that bins with residuals closer to 0 are preferred.\n    # We use `fit_residuals + 1e-6` to ensure that even for exact fits (residual=0),\n    # we have a non-zero value to pass to exp, and to differentiate exact from very close fits slightly.\n    # However, a cleaner approach is to handle exact fits explicitly.\n    \n    scores = np.where(fit_residuals == 0,\n                      1.0,  # High priority for exact fits\n                      np.exp(-decay_factor * fit_residuals)) # Decreasing priority for near-fits\n\n    # Apply Softmax-like normalization to convert scores into probabilities.\n    # This ensures that the priorities sum to 1 across the eligible bins and\n    # that preferences are smoothly distributed.\n    # The temperature parameter controls the \"sharpness\" of the distribution.\n    # A lower temperature makes the probabilities sharper (more emphasis on best fits).\n    temperature = 0.2\n    \n    try:\n        # Ensure scores are not excessively large before exponentiation\n        # Clipping can help prevent overflow, but Softmax should handle it better with exp\n        # Adding a small constant to the scores before softmax can also help stabilize.\n        # A common practice is to subtract the maximum score before exponentiating to avoid overflow.\n        max_score = np.max(scores)\n        normalized_scores = (scores - max_score) / temperature\n        exp_scores = np.exp(normalized_scores)\n        softmax_priorities = exp_scores / np.sum(exp_scores)\n    except OverflowError:\n        # In case of extreme values leading to overflow, fall back to a uniform distribution\n        # or a simpler heuristic if softmax fails. For now, uniform is a safe fallback.\n        softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size\n    except ZeroDivisionError:\n        # If sum of exp_scores is zero (highly unlikely with positive scores), fallback\n        softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size\n\n    # Map the calculated priorities back to the original bin indices\n    priorities[can_fit_mask] = softmax_priorities\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "SLOC": 23.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response5.txt_stdout.txt",
    "code_path": "problem_iter2_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    This heuristic prioritizes bins that can fit the item and are \"nearly full\"\n    (i.e., have little remaining capacity after fitting the item), as this\n    encourages tighter packing. A small random exploration component is added\n    to avoid getting stuck in local optima. The exploration rate (epsilon) is\n    reduced to favor greedy choices more. Scores are normalized to a [0, 1] range\n    for a more stable epsilon-greedy behavior.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.05  # Reduced probability of random exploration for more greedy behavior\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate scores for bins that can fit the item\n    # Prioritize bins that leave little remaining capacity after fitting the item.\n    # Score is the inverse of the remaining capacity after fitting.\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Add a small constant to the denominator to prevent division by zero and overly large scores\n    # for bins that perfectly fit the item.\n    scores = 1.0 / (remaining_capacity_after_fit + 1e-6) \n    \n    # Normalize scores to be between 0 and 1.\n    # This makes the epsilon-greedy selection more balanced.\n    if scores.size > 0:\n        min_score = np.min(scores)\n        max_score = np.max(scores)\n        if max_score > min_score:\n            priorities[can_fit_mask] = (scores - min_score) / (max_score - min_score)\n        else:\n            # If all scores are the same (e.g., all bins have the same remaining capacity after fit),\n            # assign a neutral priority, or simply a value representing the best fit.\n            # Assigning 0.5 could be seen as neutral, but since they are all equally good,\n            # a higher uniform value (like 1.0) might be more indicative of a good fit.\n            # Let's stick to a normalized 1.0 for \"equally best\" fit.\n            priorities[can_fit_mask] = 1.0\n    \n    # Epsilon-Greedy exploration: with probability epsilon, choose a random bin that fits.\n    # This allows for exploration of less optimal (but still valid) bins.\n    if np.random.rand() < epsilon:\n        possible_bins_indices = np.where(can_fit_mask)[0]\n        if possible_bins_indices.size > 0:\n            # Select a random bin among those that can fit the item.\n            random_bin_index = np.random.choice(possible_bins_indices)\n            \n            # Reset priorities and assign the highest priority to the randomly chosen bin.\n            # This ensures that the exploration step effectively picks a random bin.\n            priorities = np.zeros_like(bins_remain_cap, dtype=float)\n            priorities[random_bin_index] = 1.0  # Highest priority\n\n    return priorities",
    "response_id": 5,
    "obj": 4.068607897885915,
    "SLOC": 20.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response4.txt_stdout.txt",
    "code_path": "problem_iter4_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-based approach\n    that also incorporates an epsilon-greedy exploration strategy.\n\n    This heuristic first calculates a \"fit score\" for each bin. A better fit is defined as a bin\n    where the remaining capacity is just enough for the item (i.e., `bins_remain_cap - item` is small and positive).\n    The score is designed such that a smaller positive difference yields a higher score.\n    These scores are then transformed using softmax to generate probabilities.\n    An epsilon-greedy component is integrated: with a small probability `epsilon`, a bin is chosen randomly\n    from the *suitable* bins to encourage exploration. Otherwise, the greedy (softmax-derived)\n    priorities are used.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploring a random suitable bin\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    n_bins = len(bins_remain_cap)\n    priorities = np.zeros(n_bins)\n\n    if len(suitable_bin_indices) == 0:\n        # No bin can fit the item, return all zeros\n        return priorities\n\n    # Epsilon-greedy choice: With probability epsilon, pick a random suitable bin\n    if np.random.rand() < epsilon:\n        # Choose one suitable bin randomly\n        chosen_index = np.random.choice(suitable_bin_indices)\n        # Assign a high priority to this randomly chosen bin\n        priorities[chosen_index] = 1.0\n        # For other suitable bins, assign a very low priority to make the chosen one stand out\n        priorities[suitable_bin_indices] = 1e-9\n        return priorities\n\n    # If not exploring, use a greedy strategy based on how well the item fits.\n    # We want bins where the remaining capacity is just enough for the item.\n    # The score should be higher for bins with `bins_remain_cap - item` closer to 0 (but positive).\n    # A good scoring function would be related to `1 / (bins_remain_cap - item + small_constant)`\n    # or `-(bins_remain_cap - item)` which is `item - bins_remain_cap`.\n    # Let's use `item - bins_remain_cap` to prioritize smaller positive differences.\n    scores = item - bins_remain_cap\n\n    # For bins that cannot fit the item, assign a very low score (effectively -infinity for softmax)\n    # This ensures their softmax probability is negligible.\n    scores[~suitable_bins_mask] = -np.inf\n\n    # Calculate softmax probabilities\n    # To prevent potential overflow with large positive scores, we shift scores\n    # so that the maximum score becomes 0.\n    # We only consider scores for suitable bins for the maximum calculation.\n    max_score_for_suitable = np.max(scores[suitable_bins_mask]) if np.any(suitable_bins_mask) else 0\n    \n    shifted_scores = scores - max_score_for_suitable\n    \n    # Calculate exponentiated scores. Bins with -inf will result in 0.\n    exp_scores = np.exp(shifted_scores)\n    \n    # Calculate the sum of exponentiated scores. Ensure it's not zero.\n    sum_exp_scores = np.sum(exp_scores)\n    \n    if sum_exp_scores == 0:\n        # This can happen if all suitable bins had scores that resulted in exp_scores close to zero\n        # or if somehow all suitable bins were removed or became invalid.\n        # As a fallback, assign uniform probability to all suitable bins.\n        if np.any(suitable_bins_mask):\n            priorities[suitable_bins_mask] = 1.0 / len(suitable_bin_indices)\n        return priorities\n    \n    priorities = exp_scores / sum_exp_scores\n\n    return priorities",
    "response_id": 4,
    "obj": 4.068607897885915,
    "SLOC": 25.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a blended strategy.\n\n    This strategy combines the preference for exact fits with the soft preference\n    for near-exact fits using a Softmax-like approach. It prioritizes bins\n    where the remaining capacity is exactly the item size, and then assigns\n    decreasing priority to bins with slightly larger remaining capacities.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if not eligible_bins_cap.size:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit score\": how much space is left after placing the item.\n    # Smaller values indicate a tighter fit.\n    fit_scores = eligible_bins_cap - item\n\n    # Assign a base priority of 1.0 to exact fits and a lower base priority to partial fits.\n    # This ensures exact fits are always preferred over partial fits before softmax.\n    base_priorities = np.where(fit_scores == 0, 1.0, 0.5)\n\n    # To incorporate the \"near-exact\" preference smoothly, we can modify the\n    # base priorities based on how close the fit is, using a logistic or exponential decay.\n    # Here, we use a simple exponential decay for remaining space.\n    # Smaller remaining space (after fitting) should get higher priority.\n    # We add 1 to fit_scores to handle the case of exact fit (fit_score=0) gracefully\n    # and ensure positive values for exponentiation.\n    # The scaling factor (e.g., 1.0) and the decay rate (e.g., 0.1) can be tuned.\n    decay_rate = 0.1\n    near_fit_scores = np.exp(-decay_rate * fit_scores)\n\n    # Blend the base priority with the near-fit score.\n    # Exact fits should retain their high base priority, while near-fits get a boost.\n    # We prioritize exact fits (score 1.0) and then near-exact fits.\n    # A simple way to blend is to amplify the scores of exact fits and give\n    # a scaled score to near-fits.\n    combined_scores = np.where(fit_scores == 0, 1.0, near_fit_scores)\n\n    # Apply a Softmax-like transformation to normalize and create probabilities.\n    # This smooths the preferences, giving higher probability to better fits.\n    # The temperature parameter controls the \"softness\".\n    temperature = 0.5\n    try:\n        # Add a small epsilon to prevent log(0) or division by zero issues\n        epsilon = 1e-9\n        scaled_scores = combined_scores / temperature\n        exp_scores = np.exp(scaled_scores)\n        softmax_priorities = exp_scores / np.sum(exp_scores)\n    except OverflowError:\n        # Fallback for overflow: assign uniform probability if scores are too extreme\n        softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size\n\n\n    # Map the calculated priorities back to the original bin indices\n    priorities[can_fit_mask] = softmax_priorities\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "SLOC": 21.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response3.txt_stdout.txt",
    "code_path": "problem_iter4_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a hybrid approach.\n\n    This heuristic combines the \"Best Fit\" (minimizing remaining capacity) with an\n    element of exploration. It prioritizes bins that leave minimal remaining capacity\n    after placing the item, but also assigns a small, non-zero priority to bins that\n    are not perfect fits to encourage exploration of potentially better long-term solutions.\n\n    The \"goodness\" of a fit is measured by the remaining capacity after placing the item.\n    A smaller remaining capacity is considered a better fit. To convert this into a\n    priority score, we use an exponential decay based on the slack.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if len(suitable_bins_indices) == 0:\n        return priorities  # No bin can fit the item\n\n    # --- Calculate \"goodness of fit\" score ---\n    # We want to minimize (bins_remain_cap - item), which is the slack.\n    # A smaller slack means a better fit.\n    slack_values = bins_remain_cap[suitable_bins_indices] - item\n\n    # Transform slack into an attractiveness score.\n    # We use an exponential function: exp(-slack / T). A smaller slack leads to a higher score.\n    # T (temperature) controls how strongly we favor the best fit. Smaller T = stronger preference.\n    temperature = 0.3  # Hyperparameter to tune exploration vs. exploitation balance\n    \n    # Add a small epsilon to avoid potential issues with exp(0) if slack is exactly 0,\n    # though exp(0) is 1, which is fine. The main goal is to have scores that\n    # decrease as slack increases.\n    fit_scores = np.exp(-slack_values / temperature)\n\n    # --- Exploration component ---\n    # Add a small base score to all suitable bins to ensure some level of exploration.\n    # This prevents bins that are not the absolute best fit from having zero chance.\n    exploration_bonus = 0.1  # A small constant added to all suitable bins\n\n    # Combine fit scores with exploration bonus\n    # We add the bonus to all suitable bins, so the relative preference of the best fits is maintained.\n    combined_scores = fit_scores + exploration_bonus\n\n    # Normalize scores to sum to 1 (or to be interpretable as probabilities)\n    # This ensures that the priorities are on a consistent scale.\n    # Using softmax-like normalization for the combined scores.\n    # Add a small epsilon to the sum to prevent division by zero if all scores are very small.\n    sum_of_scores = np.sum(combined_scores)\n    if sum_of_scores > 1e-9:\n        normalized_priorities = combined_scores / sum_of_scores\n    else:\n        # If all combined scores are near zero, distribute equally among suitable bins.\n        normalized_priorities = np.ones_like(combined_scores) / len(combined_scores)\n\n    # Assign the calculated priorities back to the original bins\n    priorities[suitable_bins_indices] = normalized_priorities\n\n    return priorities",
    "response_id": 3,
    "obj": 3.9788591942560925,
    "SLOC": 19.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    The strategy favors bins that are a \"good fit\" for the item (i.e., leaving\n    a small remaining capacity), but with a probability epsilon, it assigns a\n    consistent exploration score to encourage trying less optimal bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.2  # Probability of exploration\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if len(suitable_bins_indices) == 0:\n        return priorities  # No bin can fit the item\n\n    # --- Exploitation Component (Good Fit) ---\n    # Calculate a score based on how well the item fits.\n    # We want to prioritize bins that leave minimal remaining capacity.\n    # Score = 1 / (remaining_capacity - item + epsilon)\n    # A smaller difference means a higher score.\n    fit_scores = 1.0 / (bins_remain_cap[suitable_bins_indices] - item + 1e-6)\n\n    # Normalize fit_scores to a 0-1 range to represent the \"exploitation\" priority.\n    # Higher score means better fit.\n    if fit_scores.max() > fit_scores.min():\n        exploitation_priorities = (fit_scores - fit_scores.min()) / (fit_scores.max() - fit_scores.min())\n    else:\n        exploitation_priorities = np.ones(len(suitable_bins_indices)) # All fits are equally good\n\n    # Assign the exploitation priorities to the suitable bins\n    priorities[suitable_bins_indices] = exploitation_priorities\n\n    # --- Exploration Component ---\n    # With probability epsilon, overwrite the exploitation priority with a\n    # consistent, lower exploration score for a random subset of suitable bins.\n    # This encourages exploration of bins that might not be the immediate best fit.\n    exploration_score = 0.1 # A fixed low score for exploration\n\n    # Determine which suitable bins will be subject to exploration\n    num_suitable = len(suitable_bins_indices)\n    explore_indices_in_suitable = np.random.choice(\n        num_suitable,\n        size=int(np.ceil(epsilon * num_suitable)),\n        replace=False\n    )\n    \n    # Get the actual indices in the original bins_remain_cap array\n    bins_to_explore_indices = suitable_bins_indices[explore_indices_in_suitable]\n\n    # Assign the exploration score to these bins\n    priorities[bins_to_explore_indices] = exploration_score\n\n    # Ensure that bins that cannot fit the item have zero priority\n    priorities[~suitable_bins_mask] = 0\n\n    # Optional: Normalize final priorities if a specific range is required by downstream logic.\n    # For now, we return the scores as calculated, where higher means more preferred.\n    # A simple max-min normalization can be applied if needed:\n    # if priorities.max() > priorities.min():\n    #     final_priorities = (priorities - priorities.min()) / (priorities.max() - priorities.min())\n    # else:\n    #     final_priorities = np.ones(num_bins) * 0.5\n    # return final_priorities\n\n    return priorities",
    "response_id": 9,
    "obj": 4.178300757877951,
    "SLOC": 25.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response2.txt_stdout.txt",
    "code_path": "problem_iter3_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, prioritizing tight fits.\n\n    This heuristic prioritizes bins that can accommodate the item, favoring \"tight fits\"\n    (smaller remaining capacity after packing) to minimize wasted space.\n    A softmax function is used to convert these preferences into probabilities,\n    allowing for some exploration. The temperature parameter controls the\n    aggressiveness of the selection. Bins that are too small for the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    temperature = 1.0  # Tune this parameter: lower for more greedy, higher for more exploration\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate a score based on how tight the fit is.\n    # A smaller remaining capacity after packing means a tighter fit.\n    # We add a small epsilon to avoid division by zero if a bin has exactly the item's size.\n    tight_fit_scores = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)\n\n    # Apply softmax to convert scores into probabilities/priorities\n    # Ensure no division by zero if all scores are effectively zero (e.g., no bin fits)\n    if np.sum(tight_fit_scores) > 0:\n        # Softmax calculation: exp(score / temperature) / sum(exp(score / temperature))\n        # To avoid numerical instability with large scores, we can subtract the max score.\n        max_score = np.max(tight_fit_scores)\n        exp_scores = np.exp((tight_fit_scores - max_score) / temperature)\n        sum_exp_scores = np.sum(exp_scores)\n\n        if sum_exp_scores > 0:\n            priorities[can_fit_mask] = exp_scores / sum_exp_scores\n        else:\n            # Handle cases where sum of exp_scores might be zero due to extreme values or temperature\n            # In such rare cases, fall back to uniform distribution among fitting bins\n            num_fitting_bins = np.sum(can_fit_mask)\n            if num_fitting_bins > 0:\n                priorities[can_fit_mask] = 1.0 / num_fitting_bins\n    else:\n        # If no bins can fit, priorities remain zero.\n        pass\n\n    # Tie-breaking: Favor earlier bins (lower index) if priorities are very close.\n    # This can be implicitly handled by the order of processing or explicitly added\n    # by adding a small negative value based on index to the score before softmax,\n    # e.g., score - index * epsilon_tiebreaker. For simplicity here, we rely on\n    # the original order and potential floating point differences.\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response2.txt_stdout.txt",
    "code_path": "problem_iter4_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, prioritizing tight fits and considering bin scarcity.\n\n    This heuristic prioritizes bins that can accommodate the item, favoring \"tight fits\"\n    (smaller remaining capacity after packing) to minimize wasted space.\n    It also introduces a penalty for bins that are already very full (scarce),\n    encouraging the use of bins with more remaining capacity if the fit is not extremely tight.\n    A softmax function is used to convert these preferences into probabilities,\n    allowing for some exploration. The temperature parameter controls the\n    aggressiveness of the selection. Bins that are too small for the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    temperature = 0.5  # Tune this parameter: lower for more greedy, higher for more exploration\n    scarcity_penalty_factor = 0.1 # Factor to penalize bins that are already nearly full\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities # No bins can fit the item\n\n    fitting_bins_caps = bins_remain_cap[can_fit_mask]\n\n    # Calculate a score based on how tight the fit is.\n    # A smaller remaining capacity after packing means a tighter fit.\n    # Add a small epsilon to avoid division by zero if a bin has exactly the item's size.\n    tight_fit_scores = 1.0 / (fitting_bins_caps - item + 1e-9)\n\n    # Introduce a scarcity penalty: Bins with less remaining capacity overall are less preferred\n    # unless the fit is extremely tight. We can scale the inverse of remaining capacity.\n    # We want to penalize bins that are already very full, so we use 1/capacity.\n    # However, we only want this penalty to apply if the fit isn't perfectly tight.\n    # A simple approach is to add a small term that decreases with remaining capacity.\n    # We'll use a factor that scales the inverse of the bin's *total* capacity\n    # to penalize those that are less likely to accommodate future items.\n    # For simplicity, we'll use the current remaining capacity as a proxy for scarcity.\n    # Bins with less remaining capacity are considered more scarce.\n    scarcity_scores = scarcity_penalty_factor * (1.0 / (fitting_bins_caps + 1e-9))\n\n    # Combine tight fit preference with scarcity penalty.\n    # We want to favor tight fits, so tight_fit_scores are generally good.\n    # Scarcity scores penalize bins, so we subtract them.\n    # The `tight_fit_scores` should dominate, so we might need to tune the factor.\n    # Let's refine: we want to increase preference for tight fits,\n    # and decrease preference for bins that are already scarce (low remaining capacity).\n    # So, we'll combine the \"goodness\" of the fit with a penalty for scarcity.\n    # A better approach might be to prioritize bins that have *just enough* space\n    # but also have substantial remaining capacity for future items.\n    # Let's try a score that rewards tight fits and penalizes low remaining capacity.\n\n    # Calculate preference for tight fit (higher is better)\n    fit_preference = 1.0 / (fitting_bins_caps - item + 1e-9)\n\n    # Calculate penalty for scarcity (lower remaining capacity is worse)\n    # We want to *discourage* using very full bins, so higher scarcity_penalty is worse.\n    # Let's invert it to get a desirability score: bins with more remaining capacity are more desirable.\n    scarcity_desirability = fitting_bins_caps\n\n    # Combine them: prioritize tight fits, but also consider overall bin capacity.\n    # A good heuristic might be to prioritize bins that are tight fits BUT still have a decent amount of space left.\n    # This is a bit contradictory. Let's re-think the reflection.\n    # \"prioritize tight fits; explore with softmax; consider bin scarcity.\"\n    # Bin scarcity means we don't want to fill up bins too quickly if we have many options.\n    # This implies favoring bins that have more space.\n\n    # Let's re-frame: score should be high for bins that are tight fits AND have sufficient capacity remaining.\n    # Score = (1 / (remaining_capacity - item)) * (remaining_capacity)  -- this amplifies larger capacities for tight fits.\n    # Or, to favor tight fits more strongly, we can make the second term more influential.\n    # Let's try a weighted sum or multiplicative approach.\n\n    # Option 1: Weighted sum, favoring tight fit, penalizing scarcity (low remaining capacity)\n    # score = weight_fit * fit_preference - weight_scarcity * (1 / (fitting_bins_caps + 1e-9))\n    # This means a very low remaining capacity (high scarcity) will result in a more negative score.\n\n    # Option 2: Consider the remaining capacity after packing. We want this to be small (tight fit).\n    # And we also want the *original* remaining capacity to be not too small (to avoid scarcity).\n    # Let's define a score that is high for bins that are tight AND have ample space.\n    # This sounds like prioritizing bins that can fit the item with minimal waste, but also aren't already almost full.\n\n    # Let's use a score that prioritizes tight fits, but also gives a slight boost to bins with more remaining capacity.\n    # This can be achieved by adding a term related to the remaining capacity itself.\n    # The intuition is: prefer tight fits, but if multiple bins offer tight fits, pick the one that leaves more space.\n    # This might be counter-intuitive to \"tight fit\" as it encourages leaving more space.\n\n    # Let's go back to the core idea: minimize wasted space. Tightest fits do this.\n    # Bin scarcity: don't fill up bins too quickly. This means maybe spreading items across bins.\n    # If we have many bins with similar tight fits, we might want to pick the one with more original capacity.\n\n    # Let's combine:\n    # 1. Tightness of fit: `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` (higher is better)\n    # 2. Bin capacity: `bins_remain_cap[can_fit_mask]` (higher is better, to avoid scarcity)\n    # We can weight these. A simple product might work: `tight_fit_scores * bins_remain_cap[can_fit_mask]`\n    # Or a weighted sum: `w1 * tight_fit_scores + w2 * bins_remain_cap[can_fit_mask]`\n    # Let's try a multiplicative approach that rewards tightness and penalizes low capacity.\n    # The previous `priority_v1` used `1.0 / (fitting_bins_caps - item + 1e-9)`. Let's modify that.\n\n    # Consider the value of a bin.\n    # A bin is good if it fits the item snugly.\n    # A bin is also good if it has a lot of remaining capacity for future items.\n    # Let's try to combine these. The \"value\" could be related to the remaining capacity *after* packing.\n    # We want this to be small. So, `remaining_capacity - item`.\n    # But we also want to avoid scarcity, meaning bins with very low original capacity are less desirable.\n\n    # Let's refine the score:\n    # We want bins where `bins_remain_cap - item` is small (tight fit).\n    # We also want bins where `bins_remain_cap` is not too small (to avoid scarcity).\n    # A score that rewards small `bins_remain_cap - item` and penalizes small `bins_remain_cap` could be:\n    # `score = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9) * (bins_remain_cap[can_fit_mask] / BIN_CAPACITY)`\n    # If we don't know BIN_CAPACITY, we can use the average remaining capacity, or simply the remaining capacity itself.\n    # Let's use the remaining capacity directly as a multiplier for the tight-fit score.\n    # This gives higher scores to tight fits in bins that also have more space overall.\n\n    # Refined score: prioritize tight fits, but if multiple bins are tightly fitting,\n    # prefer the one that still has more capacity.\n    # This implicitly encourages spreading items if fits are equally tight.\n    combined_scores = (1.0 / (fitting_bins_caps - item + 1e-9)) * fitting_bins_caps\n\n    # Apply softmax\n    if np.sum(combined_scores) > 0:\n        max_score = np.max(combined_scores)\n        exp_scores = np.exp((combined_scores - max_score) / temperature)\n        sum_exp_scores = np.sum(exp_scores)\n\n        if sum_exp_scores > 0:\n            priorities[can_fit_mask] = exp_scores / sum_exp_scores\n        else:\n            # Fallback if exp_scores sum to zero (e.g., extreme values or temperature)\n            num_fitting_bins = np.sum(can_fit_mask)\n            if num_fitting_bins > 0:\n                priorities[can_fit_mask] = 1.0 / num_fitting_bins\n    else:\n        # If no fitting bins or scores are zero, priorities remain zero.\n        pass\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "SLOC": 26.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response5.txt_stdout.txt",
    "code_path": "problem_iter4_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This heuristic prioritizes bins that can fit the item and are \"nearly full\"\n    after the item is placed, encouraging tighter packing. It also incorporates a\n    mechanism to slightly favor bins that have been used less (i.e., have more\n    remaining capacity *before* adding the item) if they still offer a good fit,\n    to promote better distribution and avoid premature bin exhaustion.\n    The priority is calculated based on the remaining capacity after placing the item,\n    inverted to make smaller remaining capacities yield higher priorities.\n    A small additive term is used to break ties, favoring bins that had more\n    initial capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a more desirable bin. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    # Calculate remaining capacity after placing the item for all bins\n    potential_remaining_caps = bins_remain_cap - item\n\n    # Initialize priorities to a very low value (or 0) for bins that cannot fit\n    # This ensures they are never chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = potential_remaining_caps >= 0\n\n    # For bins that can fit, calculate priority:\n    # The core idea is to prioritize bins where the remaining capacity after placing\n    # the item is minimized (best fit). We invert this to get a higher score for\n    # a tighter fit.\n    # We add a small penalty based on the *original* remaining capacity. This is\n    # to slightly favor bins that had more capacity initially if they still provide\n    # a good fit. This can help in distributing items better.\n    # The inversion of `potential_remaining_caps` means smaller values are better.\n    # Adding a small positive value for `bins_remain_cap` can slightly break ties.\n    # `bins_remain_cap` is used here for its larger values to be slightly preferred\n    # when the `potential_remaining_caps` are equal or very close.\n    # A simple approach is to use the negative of `potential_remaining_caps`\n    # and then add a small bonus for larger `bins_remain_cap`.\n\n    # We want to maximize (smaller remaining capacity after fit) AND (larger initial capacity if fits are similar)\n    # So, we can aim for a score like: -potential_remaining_caps + C * bins_remain_cap\n    # Where C is a small positive constant to give weight to original capacity.\n    # Let's normalize the original capacity to avoid it dominating too much.\n    # A simpler approach without explicit normalization:\n    # Prioritize by the inverse of (remaining capacity after fit + small epsilon)\n    # and add a small bonus for larger original capacity.\n    # Let's use the negative remaining capacity as primary driver and add the original capacity as a tie-breaker.\n\n    if np.any(can_fit_mask):\n        # Calculate the primary score: inverse of remaining capacity after fit.\n        # Add a small epsilon to avoid division by zero and to give a slight preference\n        # to bins that are not *perfectly* full (though this is subtle and may need tuning).\n        # A more direct way for \"best fit\" is to use the negative of the remaining capacity.\n        # Negative remaining capacity: lower value means better fit.\n        # To get higher priority, we invert it: 1 / (residual + epsilon) or similar.\n        # Let's stick to the negative remaining capacity and add original capacity.\n        \n        # Primary scoring: -potential_remaining_caps. Smaller (more negative) is better.\n        # To convert to higher priority: use positive values.\n        # We can use `max_residual - residual` or `1 / (residual + epsilon)`.\n        # Using `-potential_remaining_caps` directly works if we interpret higher values as better.\n        # Let's refine to make it clearer: High priority means a good fit.\n        # Good fit = small `potential_remaining_caps`.\n        # So, let's use `some_large_value - potential_remaining_caps`.\n        # Example: Bin Capacity = 10. Item = 3. Remaining_cap = 7.\n        # If item = 7, Remaining_cap = 3. This is a better fit.\n        # Score for 7: large_val - 3. Score for 3: large_val - 7. Higher score for better fit.\n\n        # Let's use a scaled inverse of the remaining capacity after fit.\n        # `1 / (potential_remaining_caps + 1e-6)` would give higher score to smaller remaining.\n        # To incorporate original capacity as a secondary criterion:\n        # We want to maximize (-potential_remaining_caps) primarily, and (bins_remain_cap) secondarily.\n        # A common way is to combine them linearly, e.g., `a * (-potential_remaining_caps) + b * bins_remain_cap`.\n        # For simplicity and direct \"best fit\" interpretation:\n        # Score = `(max_possible_residual - potential_remaining_caps)` + `0.01 * bins_remain_cap`\n        # where `max_possible_residual` is the maximum possible remaining capacity for a fit.\n        # Alternatively, using the negative remaining capacity is good for sorting.\n        \n        # Let's use `1.0 / (potential_remaining_caps + 1e-6)` for primary priority\n        # and add a small term related to original capacity.\n        \n        # If we want to prioritize bins that are *closer* to fitting the item perfectly,\n        # `potential_remaining_caps` should be as close to 0 as possible.\n        # So, `1.0 / (potential_remaining_caps + epsilon)` works well for that.\n        \n        # Consider `potential_remaining_caps = [2, 0, 3]`\n        # Scores: `1/2.000001`, `1/0.000001`, `1/3.000001` -> approx `0.5`, `1000000`, `0.33`\n        # This clearly prioritizes the best fit.\n        \n        # Now, adding the secondary criterion: slight preference for bins with more original capacity.\n        # Let's normalize `bins_remain_cap` to a small range, e.g., [0, 1] or [0, 0.1]\n        # to avoid it overpowering the primary criterion.\n        \n        # Let's combine: priority = (1 / (potential_remaining_caps + epsilon)) + (bins_remain_cap / MAX_CAPACITY_OR_MEAN)\n        # For simplicity, we can just add a scaled version of original capacity.\n        \n        # Final approach:\n        # Primary: Maximize `-potential_remaining_caps`. (Best fit)\n        # Secondary: Maximize `bins_remain_cap`. (Slight preference for larger original capacity bins)\n        # Combine: `priorities[can_fit_mask] = -potential_remaining_caps[can_fit_mask] + 0.1 * bins_remain_cap[can_fit_mask]`\n        # The `0.1` factor is heuristic. It means a difference of 1 in `potential_remaining_caps`\n        # is equivalent to a difference of 10 in `bins_remain_cap`. This gives strong preference\n        # to best fit.\n\n        priorities[can_fit_mask] = -potential_remaining_caps[can_fit_mask] + 0.05 * bins_remain_cap[can_fit_mask]\n        \n    # Ensure bins that can't fit have a priority of -inf (or a very small number)\n    # We already initialized to -inf, so this step is covered.\n    # If we were using 0 for non-fitting bins, we would need to ensure it.\n\n    # Handle the case where all potential priorities are -inf (no bin can fit)\n    if not np.any(np.isfinite(priorities)):\n        return np.zeros_like(bins_remain_cap) # Return all zeros if no bin can fit\n\n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response8.txt_stdout.txt",
    "code_path": "problem_iter4_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using an enhanced Almost Full Fit strategy.\n\n    This strategy prioritizes bins that, after placing the item, will have the least\n    remaining capacity among bins that can still accommodate the item. This aims\n    to fill bins as much as possible before opening new ones.\n    To mitigate getting stuck in local optima, a small probability `epsilon` is used\n    to introduce randomness and explore less optimal, but potentially better fitting,\n    bins in the long run.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot accommodate the item will have a priority of 0.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins, dtype=float)\n    epsilon = 0.1  # Probability for exploration\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if len(suitable_bins_indices) == 0:\n        return priorities  # No bin can fit the item\n\n    # --- Exploitation Component (Almost Full Fit) ---\n    # Prioritize bins that leave the minimum remaining capacity after placing the item.\n    # A smaller remaining capacity means a \"better fit\" or a bin that becomes \"more full\".\n    # We use the inverse of the remaining capacity plus a small epsilon to avoid division by zero\n    # and ensure higher scores for tighter fits.\n    remaining_after_placement = bins_remain_cap[suitable_bins_indices] - item\n    epsilon_fit = 1e-6  # Small epsilon for numerical stability\n    fit_scores = 1.0 / (remaining_after_placement + epsilon_fit)\n\n    # Normalize fit_scores to a 0-1 range for a consistent priority scale.\n    # Higher normalized score means a better fit (less remaining capacity).\n    if fit_scores.max() > fit_scores.min():\n        exploitation_priorities = (fit_scores - fit_scores.min()) / (fit_scores.max() - fit_scores.min())\n    else:\n        # If all fits are equally good (e.g., only one suitable bin), assign max priority.\n        exploitation_priorities = np.ones(len(suitable_bins_indices))\n\n    # Assign the calculated exploitation priorities to the suitable bins\n    priorities[suitable_bins_indices] = exploitation_priorities\n\n    # --- Exploration Component ---\n    # With probability epsilon, we assign a lower, consistent exploration score\n    # to a random subset of suitable bins. This encourages trying bins that\n    # might not be the immediate best fit according to the \"Almost Full Fit\" criteria.\n    exploration_score = 0.05  # A fixed low score to encourage exploration\n\n    # Determine how many suitable bins to explore\n    num_suitable = len(suitable_bins_indices)\n    num_explore = int(np.ceil(epsilon * num_suitable))\n\n    if num_explore > 0:\n        # Randomly select indices within the suitable_bins_indices array\n        explore_indices_in_suitable = np.random.choice(\n            num_suitable,\n            size=min(num_explore, num_suitable), # Ensure we don't ask for more than available\n            replace=False\n        )\n        \n        # Get the actual indices in the original bins_remain_cap array\n        bins_to_explore_indices = suitable_bins_indices[explore_indices_in_suitable]\n\n        # Assign the exploration score to these selected bins, potentially overwriting\n        # their exploitation priority.\n        priorities[bins_to_explore_indices] = exploration_score\n\n    # Ensure bins that cannot fit the item still have zero priority\n    priorities[~suitable_bins_mask] = 0\n\n    return priorities",
    "response_id": 8,
    "obj": 4.058635819704831,
    "SLOC": 29.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  }
]