```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority for each bin using a refined Best Fit Decreasing (BFD)-like strategy.

    This heuristic aims to replicate the spirit of BFD in an online setting,
    prioritizing bins that are the "best fit" (i.e., have the least remaining
    capacity after placing the item), while also incorporating a mechanism to
    avoid getting stuck in a purely greedy approach.

    The scoring mechanism is designed to:
    1.  **Prioritize tightest fit:** Bins that leave the least remaining capacity
        after placing the item receive the highest scores. This is the primary
        criterion.
    2.  **Encourage using less full bins (secondary):** If multiple bins offer
        a similar "tight fit," the one with more remaining capacity is slightly
        preferred. This acts as a tie-breaker and encourages distributing items
        more evenly rather than filling up a few bins too quickly.
    3.  **Controlled exploration:** A small probability of choosing a random
        valid bin is included to prevent premature convergence to suboptimal
        solutions. The probability is kept low to favor the greedy strategy.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.

    Returns:
        A NumPy array of the same size as `bins_remain_cap`, representing the
        priority score for each bin. Bins that cannot fit the item will have a
        score of 0. Scores are roughly scaled between 0 and 1, where higher is better.
    """
    num_bins = len(bins_remain_cap)
    priorities = np.zeros(num_bins, dtype=float)

    # Identify bins that can accommodate the item
    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        return priorities  # No bin can fit the item

    suitable_bins_cap = bins_remain_cap[can_fit_mask]
    suitable_bins_indices = np.where(can_fit_mask)[0]

    # --- Scoring Components ---

    # 1. Tightest Fit Score: (Maximum remaining capacity after packing) - (actual remaining capacity after packing)
    # A higher score means less remaining capacity, i.e., a tighter fit.
    # We want to maximize `bins_remain_cap - item`.
    # So, we can use `bins_remain_cap[suitable_bins_mask] - item`.
    # To make it a priority score (higher is better), we can use the negative of this,
    # or a transformation that maps smaller values to larger scores.
    # Let's use `1.0 / (remaining_capacity_after_fit + epsilon)` as in v1,
    # but normalize it to avoid extreme values.

    remaining_capacity_after_fit = suitable_bins_cap - item
    
    # Avoid division by zero, and ensure small remaining capacities get high scores.
    # Add a small constant epsilon.
    tight_fit_scores = 1.0 / (remaining_capacity_after_fit + 1e-6)

    # Normalize tight_fit_scores to be roughly in [0, 1].
    # Higher values mean a tighter fit.
    min_tight_score = np.min(tight_fit_scores)
    max_tight_score = np.max(tight_fit_scores)

    if max_tight_score > min_tight_score:
        normalized_tight_fit = (tight_fit_scores - min_tight_score) / (max_tight_score - min_tight_score)
    else:
        # All fits are equally "tight" in terms of remaining capacity difference
        normalized_tight_fit = np.ones_like(tight_fit_scores) * 0.5 # Neutral score

    # 2. Secondary Tie-breaker: Prefer bins with *more* remaining capacity if fits are similar.
    # This helps distribute items better. So, score should increase with `suitable_bins_cap`.
    # Normalize suitable_bins_cap to be in [0, 1].
    min_bin_cap = np.min(suitable_bins_cap)
    max_bin_cap = np.max(suitable_bins_cap)
    
    if max_bin_cap > min_bin_cap:
        normalized_capacity_preference = (suitable_bins_cap - min_bin_cap) / (max_bin_cap - min_bin_cap)
    else:
        # All suitable bins have the same capacity. No preference based on this.
        normalized_capacity_preference = np.ones_like(suitable_bins_cap) * 0.5 # Neutral score

    # Combine scores: Primarily tight fit, secondarily capacity preference.
    # Weight the secondary factor to ensure tight fit dominates.
    weight_tightness = 1.0
    weight_capacity = 0.2  # Lower weight for the secondary criterion
    
    combined_scores = (weight_tightness * normalized_tight_fit) + (weight_capacity * normalized_capacity_preference)

    # --- Controlled Exploration ---
    # Use a small epsilon for random choice among valid bins.
    exploration_epsilon = 0.05  # Small probability of random selection

    # Calculate actual probabilities using Softmax to allow for nuanced selection
    # while still favoring higher scores.
    # Shift scores to prevent overflow with exp, by subtracting the max score.
    if combined_scores.size > 0:
        max_score = np.max(combined_scores)
        shifted_scores = combined_scores - max_score
        # Clamp values to prevent numerical issues with exp
        capped_shifted_scores = np.clip(shifted_scores, -100.0, 100.0)
        
        exp_scores = np.exp(capped_shifted_scores)
        sum_exp_scores = np.sum(exp_scores)
        
        if sum_exp_scores > 0:
            greedy_probabilities = exp_scores / sum_exp_scores
        else:
            greedy_probabilities = np.ones_like(exp_scores) / len(exp_scores) if len(exp_scores) > 0 else np.array([])
    else:
        greedy_probabilities = np.array([])

    # Integrate exploration: For a small epsilon, pick a random bin.
    # This means we override the calculated probabilities with a uniform distribution
    # over valid bins with probability epsilon.
    final_probabilities = np.copy(greedy_probabilities)
    
    if np.random.rand() < exploration_epsilon:
        # Choose a random bin among those that can fit
        possible_bins_indices = np.where(can_fit_mask)[0]
        if possible_bins_indices.size > 0:
            random_choice_index = np.random.choice(len(possible_bins_indices))
            random_bin_global_index = possible_bins_indices[random_choice_index]
            
            # Reset probabilities and assign 1.0 to the randomly chosen bin, 0 to others.
            # This is one way to implement epsilon-greedy: with prob epsilon, pick a random *action*.
            # Here, actions are bins.
            priorities = np.zeros_like(bins_remain_cap, dtype=float)
            priorities[random_bin_global_index] = 1.0
            return priorities # Exit early if exploration happened

    # If not exploring randomly, use the calculated greedy probabilities
    priorities[can_fit_mask] = final_probabilities

    return priorities
```
