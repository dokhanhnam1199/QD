```python
import numpy as np
import scipy.stats as stats

def priority_v2(item: float, bins_remain_cap: np.ndarray, temperature: float = 1.0) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a softmax-based
    approach that prioritizes "good fits" and allows for exploration.

    This heuristic prioritizes bins that can accommodate the item and are "tight fits"
    to minimize wasted space. Bins that are too small for the item receive a priority of 0.
    The priority scores are then transformed using a softmax function controlled by a
    temperature parameter, allowing for tunable exploration of bins.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.
        temperature: Controls the randomness of the priority. Higher values lead to
                     more uniform probabilities (exploration), lower values to more
                     deterministic choices (exploitation of best fits).

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins that can fit the item
    can_fit_mask = bins_remain_cap >= item

    # For bins that can fit, calculate a score based on how "tight" the fit is.
    # A smaller difference (bins_remain_cap - item) means a tighter fit.
    # We add a small epsilon to avoid division by zero or log(0) issues.
    # Using the negative difference as input to softmax implicitly prioritizes smaller differences.
    # We want to maximize (bins_remain_cap - item) for softmax to pick smaller values.
    # A common approach is to use the negative of the difference, or the difference itself
    # and let softmax handle the selection. Let's use the difference directly.
    # A smaller difference means a better fit. To use softmax for choosing best fits,
    # we want the "values" for softmax to be higher for better fits.
    # So, we can use negative of the difference.
    fit_diff = bins_remain_cap[can_fit_mask] - item
    
    # Calculate scores for fitting bins. We want smaller differences to have higher priority.
    # Using 1 / (fit_diff + epsilon) is one way, but softmax works better with unbounded values.
    # Let's use the negative of the fit_diff. This means smaller fit_diff (better fit)
    # will result in a less negative number, which softmax will prefer.
    scores = -fit_diff

    # Apply softmax to get probabilities, scaled by temperature.
    # We only apply softmax to bins that can fit the item.
    if np.any(can_fit_mask):
        # Use log_softmax to avoid numerical instability if scores are very large/small
        log_probabilities = stats.log_softmax(scores / temperature)
        probabilities = np.exp(log_probabilities)
        priorities[can_fit_mask] = probabilities

    # For tie-breaking, we can further refine priorities.
    # A simple tie-breaker is to favor earlier bins.
    # We can add a small penalty based on the index.
    # However, softmax already handles relative preferences.
    # For explicit tie-breaking favouring earlier bins, we can add a small value based on index.
    # Let's modify the scores before softmax if we want to incorporate this explicitly.
    # For simplicity, let's keep the softmax on fit difference and consider tie-breaking
    # as part of the selection process after priorities are generated, or as a post-processing step.
    # If we want to bake it into the priority score directly before softmax:
    # We would need to create a vector of indices for the fitting bins and subtract a scaled version.
    # E.g., indices = np.arange(len(bins_remain_cap))[can_fit_mask]
    # tie_breaker_scores = -indices * epsilon_for_tie_breaker
    # scores = scores + tie_breaker_scores

    return priorities
```
