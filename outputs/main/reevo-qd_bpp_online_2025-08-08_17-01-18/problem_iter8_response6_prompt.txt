{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit Decreasing logic.\n\n    This heuristic prioritizes bins that can accommodate the item and are \"tight fits\"\n    to minimize wasted space. Bins that are too small for the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate priority as the inverse of the remaining capacity\n    # A smaller remaining capacity means a tighter fit, hence higher priority.\n    # We add a small epsilon to avoid division by zero if a bin has exactly the item's size.\n    priorities[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-based unified scoring mechanism.\n\n    This heuristic prioritizes bins that offer the \"tightest fit\" for an incoming item,\n    while also considering bin scarcity. It uses a unified scoring mechanism that\n    balances the preference for a tight fit with a general preference for bins that\n    are not excessively large. The scores are then normalized using a Softmax function\n    with a tunable temperature parameter, allowing for adaptive exploration/exploitation.\n\n    The core scoring for a suitable bin (where `remaining_capacity >= item`) is:\n    `score = exp(-k * (remaining_capacity - item))`\n\n    Here:\n    - `remaining_capacity - item` represents the \"mismatch\" or wasted space.\n    - `k` is a parameter controlling the sensitivity to the mismatch. A higher `k`\n      amplifies the penalty for larger mismatches, favoring tighter fits more strongly.\n      A lower `k` is more forgiving of larger mismatches.\n    - The negative sign ensures that smaller mismatches (tighter fits) result in\n      larger exponent values and thus higher scores before Softmax.\n\n    After calculating raw scores for all suitable bins, a Softmax function is applied\n    to normalize these scores into probabilities (priorities).\n    `priority = exp(raw_score / temperature) / sum(exp(raw_score / temperature))`\n\n    - `temperature` controls the shape of the probability distribution.\n      - A low temperature makes the distribution \"sharper,\" strongly favoring\n        bins with the highest raw scores (exploitation).\n      - A high temperature makes the distribution \"flatter,\" increasing the\n        probability of selecting bins with lower raw scores (exploration).\n\n    Bins that cannot fit the item are assigned a priority of 0.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is the priority score (probability) for the corresponding bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n\n    # If no bin can fit the item, return all zeros\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # --- Tunable Parameters ---\n    # Sensitivity to mismatch: higher k means stronger preference for tighter fits.\n    k = 1.0\n    # Softmax temperature: controls exploration/exploitation.\n    # Lower temp = more exploitation, higher temp = more exploration.\n    temperature = 0.5\n    # --------------------------\n\n    # Calculate the \"mismatch\" for suitable bins\n    mismatch = suitable_bins_cap - item\n\n    # Calculate raw scores: prefer smaller mismatches (tighter fits)\n    # Using exp(-k * mismatch) means smaller mismatch -> larger score\n    # To prevent overflow for very small mismatches (exp(large_positive)),\n    # we can cap the negative mismatch if it's very large (which shouldn't happen if k>0 and mismatch>=0)\n    # The primary concern is exp() of large negative numbers if k or mismatch were negative,\n    # but here k>=0 and mismatch>=0.\n    # However, for very large positive `k * mismatch`, `exp(-k * mismatch)` will be close to 0.\n    # We might want to avoid `exp` of extremely large negative numbers if `k * mismatch` is very large.\n    # Capping `k * mismatch` to a maximum value is a good practice.\n    max_positive_mismatch_score_arg = 10.0 # exp(-10) is small but not zero\n    capped_mismatch = np.minimum(mismatch, max_positive_mismatch_score_arg / k if k > 0 else np.inf)\n    raw_scores = np.exp(-k * capped_mismatch)\n\n    # Apply Softmax to normalize scores into probabilities (priorities)\n    if temperature <= 0:\n        # If temperature is zero or negative, select the bin with the highest raw score deterministically.\n        # This is equivalent to a greedy approach among suitable bins.\n        # We can find the index of the max score and assign probability 1 to it.\n        max_score_idx = np.argmax(raw_scores)\n        softmax_priorities = np.zeros_like(raw_scores)\n        softmax_priorities[max_score_idx] = 1.0\n    else:\n        # Standard Softmax calculation\n        # Add a small epsilon to the denominator to prevent division by zero if all raw_scores are -inf (not possible here)\n        # or if the sum of exponents is zero.\n        try:\n            # Using log-sum-exp trick for numerical stability if needed, but for exp(-k*mismatch)\n            # where mismatch >= 0 and k >= 0, values are between 0 and 1.\n            # Direct calculation is likely fine.\n            exp_scores = np.exp(raw_scores / temperature)\n            sum_exp_scores = np.sum(exp_scores)\n            if sum_exp_scores == 0: # Handle case where all exp_scores are ~0\n                softmax_priorities = np.ones_like(raw_scores) / len(raw_scores) if len(raw_scores) > 0 else np.array([])\n            else:\n                softmax_priorities = exp_scores / sum_exp_scores\n        except FloatingPointError:\n            # Fallback for extreme values if exp calculation fails\n            softmax_priorities = np.zeros_like(raw_scores)\n            if len(raw_scores) > 0:\n                max_score_idx = np.argmax(raw_scores)\n                softmax_priorities[max_score_idx] = 1.0\n\n\n    # Place the calculated softmax priorities back into the main priorities array\n    priorities[suitable_bins_mask] = softmax_priorities\n\n    return priorities\n\n[Reflection]\nBalance exploitation (tight fits) with exploration (diverse bin choices).\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}