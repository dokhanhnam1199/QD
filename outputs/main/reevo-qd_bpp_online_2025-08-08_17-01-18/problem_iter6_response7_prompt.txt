{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a refined priority strategy.\n\n    This heuristic prioritizes bins based on several factors:\n    1.  **Tight Fit:** Prioritizes bins that have just enough remaining capacity for the item,\n        minimizing waste.\n    2.  **Bin Scarcity:** Prefers bins that have less remaining capacity overall, making them\n        \"scarcer\" and potentially needing to be filled to capacity sooner.\n    3.  **Earlier Bin Preference (Tie-breaking):** If multiple bins offer similar priority,\n        bins that appear earlier in the `bins_remain_cap` array are slightly preferred.\n    4.  **Probabilistic Exploration (Softmax):** Uses Softmax to convert scores into probabilities,\n        allowing for probabilistic selection. This balances exploitation (choosing the best fit)\n        with exploration (trying less optimal bins occasionally).\n\n    The scoring mechanism combines the tight-fit score with a scarcity factor.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, representing the\n        probability (or weighted priority) of selecting each bin. Bins that cannot\n        fit the item will have a probability of 0.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities  # No bin can fit the item\n\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    # --- Scoring Components ---\n\n    # 1. Tight Fit Score (Sigmoid-based, similar to v1 but adjusted for Softmax)\n    # We want a score that is high for tight fits and decreases as capacity increases.\n    # A score close to 1 for perfect fit, decreasing towards 0.\n    # Let's use `1 / (1 + exp(k * (capacity - item)))`\n    # For perfect fit (capacity - item = 0), score = 0.5\n    # For capacity - item > 0, score decreases.\n    # To work well with Softmax, scores should be non-negative.\n    # We can shift the sigmoid output: `sigmoid_score = 1 / (1 + exp(k * mismatch))`.\n    # A perfect fit gives 0.5. A slightly larger fit gives slightly less than 0.5.\n    # Let's ensure scores are always positive. Maybe add a small constant or use a different transformation.\n    # Alternative: `exp(-k * mismatch)`. Perfect fit = exp(0) = 1. Larger mismatch = smaller score.\n    # This aligns better with Softmax. Let's use this.\n    k_tightness = 5.0  # Sensitivity to tightness. Higher k -> stronger preference for tight fits.\n    mismatch = suitable_bins_cap - item\n    \n    # Cap exponent to prevent overflow/underflow issues with exp, especially for large k or mismatch\n    max_exponent_val = 700.0 # exp(700) is very large\n    min_exponent_val = -700.0 # exp(-700) is very close to 0\n    \n    tightness_scores = np.exp(-k_tightness * mismatch)\n    \n    # Ensure scores are within a reasonable range if needed, though exp(-x) is usually fine.\n    # For robustness, one could clamp the argument to exp.\n    # capped_exponent = np.clip(-k_tightness * mismatch, min_exponent_val, max_exponent_val)\n    # tightness_scores = np.exp(capped_exponent)\n\n\n    # 2. Bin Scarcity Score\n    # Prioritize bins with less remaining capacity. A simple inverse relationship works.\n    # Add a small epsilon to avoid division by zero if a bin somehow has 0 capacity.\n    epsilon = 1e-6\n    scarcity_scores = 1.0 / (suitable_bins_cap + epsilon)\n    \n    # Normalize scarcity scores to be in a similar range or complement tightness score.\n    # Let's combine them additively, scaled appropriately.\n    # We want tight fit to dominate, but scarcity to break ties or influence choices.\n    # A simple weighted sum: `score = w1 * tightness + w2 * scarcity`\n    # Or, since both are positive and indicate preference, we can multiply them\n    # if we want them to be strong together, or add if we want them to be independent factors.\n    # Let's try combining them additively, scaled to avoid one dominating too much.\n    \n    # Normalize scores before combining to manage scales:\n    # Max possible tightness score is 1 (perfect fit). Min depends on max mismatch.\n    # Max possible scarcity score is 1/epsilon. Min is 1/max_capacity.\n    # This suggests scarcity might dominate if not scaled.\n    \n    # Let's scale scarcity by the inverse of the typical bin capacity or by a factor.\n    # A simpler approach: add a \"bonus\" for being scarce.\n    # Consider the total capacity of suitable bins. A bin with less capacity is scarcer.\n    # Let's scale scarcity scores based on the mean capacity of suitable bins.\n    # Or, we can think of scarcity as a \"bonus\" for having less space.\n    # Let's try making scarcity a multiplier for tightness, but capped.\n    # Or, add scarcity as a bonus.\n    \n    # Let's use additive combination with some scaling for scarcity.\n    # Scarcity score: Higher for less capacity.\n    # We want to favor bins that are nearly full.\n    # Consider the remaining capacity relative to the item size.\n    # Alternative scarcity: `1.0 - (suitable_bins_cap / max_possible_capacity)`\n    # Or simply use the inverse: `1.0 / suitable_bins_cap`\n    \n    # Let's normalize the scarcity scores to be between 0 and 1.\n    min_cap = np.min(suitable_bins_cap)\n    max_cap = np.max(suitable_bins_cap)\n    \n    if max_cap - min_cap > epsilon: # Avoid division by zero if all capacities are the same\n        normalized_scarcity = (suitable_bins_cap - min_cap) / (max_cap - min_cap)\n        # Invert for scarcity: higher score for less capacity\n        scarcity_bonus = 1.0 - normalized_scarcity\n    else:\n        scarcity_bonus = np.ones_like(suitable_bins_cap) * 0.5 # All bins equally scarce/plentiful\n\n    # 3. Combine tightness and scarcity\n    # Weighted sum: prioritize tightness, add scarcity as a secondary factor.\n    weight_tightness = 1.0\n    weight_scarcity = 0.3 # Give scarcity a moderate influence\n    \n    combined_scores = (weight_tightness * tightness_scores) + (weight_scarcity * scarcity_bonus)\n\n    # 4. Tie-breaking (Earlier Bin Preference)\n    # Add a small bonus based on index.\n    # The index itself can be used, scaled down.\n    index_bonus_scale = 0.01\n    index_bonus = suitable_bins_indices * index_bonus_scale\n    \n    final_scores = combined_scores + index_bonus\n\n    # Apply Softmax to get probabilities\n    # Softmax is `exp(score) / sum(exp(scores))`\n    # We need to handle potential large values in `final_scores` causing `exp` overflow.\n    # A common trick is to subtract the maximum score before exponentiating.\n    # `exp(x_i - max(x)) / sum(exp(x_j - max(x)))`\n    # This doesn't change the resulting probabilities.\n    \n    if final_scores.size > 0:\n        max_score = np.max(final_scores)\n        # Ensure scores passed to exp are not excessively large after subtraction\n        # Although subtracting max should handle it, numerical stability can be an issue.\n        # Let's cap the intermediate values before exp.\n        \n        shifted_scores = final_scores - max_score\n        # Cap shifted scores to prevent exp overflow even after subtraction if differences are large\n        # A value like 50-100 is usually safe.\n        capped_shifted_scores = np.clip(shifted_scores, -100.0, 100.0)\n        \n        exp_scores = np.exp(capped_shifted_scores)\n        sum_exp_scores = np.sum(exp_scores)\n        \n        if sum_exp_scores > 0:\n            probabilities = exp_scores / sum_exp_scores\n        else:\n            # This case should ideally not happen if scores are finite, but for safety:\n            probabilities = np.ones_like(exp_scores) / len(exp_scores) if len(exp_scores) > 0 else np.array([])\n    else:\n        probabilities = np.array([])\n\n    # Place the calculated probabilities back into the main priorities array\n    priorities[suitable_bins_mask] = probabilities\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a softmax-based heuristic.\n\n    This heuristic prioritizes bins that can accommodate the item, with a stronger\n    preference for \"tight fits\" (bins with remaining capacity close to the item size).\n    The `temperature` parameter controls the exploration vs. exploitation trade-off.\n    Higher temperatures lead to more uniform probabilities (more exploration), while\n    lower temperatures focus on the best-fitting bins (more exploitation).\n    Bins that are too small for the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n        temperature: Controls the sharpness of the softmax distribution.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score (probability) of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate a \"goodness\" score.\n    # We want to prioritize bins where the remaining capacity is close to the item size.\n    # A common approach is to use the difference (remaining_capacity - item).\n    # To prioritize smaller differences (tighter fits), we can use the negative of this difference.\n    # We add a small epsilon to avoid issues when remaining_capacity == item.\n    # A larger negative value (more negative) indicates a worse fit, a value closer to zero is a better fit.\n    # To make better fits have higher scores for softmax, we can invert this or use a different metric.\n    # Let's try prioritizing based on how much capacity is LEFT OVER after packing.\n    # So, (remaining_capacity - item) is what we want to minimize.\n    # For softmax, higher values mean higher probability. So, we want a metric that is\n    # higher for better fits. A good metric would be the negative of the leftover capacity,\n    # or a Gaussian-like function centered at 0 difference.\n    # Let's use negative difference, then scale it to make it more sensitive to near fits.\n    # A simple transformation that boosts near-fits and reduces others:\n    # Consider -(bins_remain_cap[can_fit_mask] - item) which is (item - bins_remain_cap[can_fit_mask]).\n    # This value is negative or zero. Higher values (closer to zero) are better fits.\n    # To make it suitable for softmax where higher is better, we can use `-(bins_remain_cap[can_fit_mask] - item)`.\n    # However, this might still be too sensitive to very small items.\n    # Let's consider a score that is high when `bins_remain_cap[can_fit_mask] - item` is small and positive.\n    # The inverse `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` from v1 is good.\n    # Let's refine this. We want to reward bins where `bins_remain_cap - item` is small.\n    # A Gaussian-like kernel centered at 0 difference could work: exp(- (diff^2) / (2 * sigma^2))\n    # Or, a simpler approach: consider `1 / (1 + diff)` where diff is `bins_remain_cap - item`.\n    # If diff is small and positive, score is close to 1. If diff is large, score approaches 0.\n\n    # Let's try a score that emphasizes small positive differences.\n    # We want a high score when `bins_remain_cap[can_fit_mask] - item` is small and positive.\n    # Consider `1.0 / (1.0 + (bins_remain_cap[can_fit_mask] - item))`\n    # This gives scores between (0, 1] for valid fits. 1 for perfect fits.\n\n    # Alternative: Prioritize bins with minimum remaining capacity that can fit the item.\n    # This is essentially the \"Best Fit\" strategy. For a heuristic priority, we can\n    # use the inverse of the remaining capacity for fitting bins.\n    # `priorities[can_fit_mask] = 1.0 / bins_remain_cap[can_fit_mask]` - This prioritizes smallest bins.\n    # If we want to prioritize tight fits, we are looking for bins where `bins_remain_cap - item` is small.\n    # So, we want to maximize `- (bins_remain_cap[can_fit_mask] - item)`.\n    # Or, a score that is high for small positive `bins_remain_cap[can_fit_mask] - item`.\n    # Let's use `-(bins_remain_cap[can_fit_mask] - item)` directly, then rescale or apply softmax.\n    # The intuition of `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` was good.\n    # Let's enhance it for \"nearness\".\n\n    # Consider the \"wasted space\" after packing: `wasted_space = bins_remain_cap - item`.\n    # We want to minimize `wasted_space`. So, a higher priority should be given to bins with smaller `wasted_space`.\n    # Let's transform `wasted_space` into a score where smaller `wasted_space` yields a higher score.\n    # A simple transformation: `score = 1.0 / (1.0 + wasted_space)`.\n    # This gives scores in the range (0, 1]. Perfect fit -> score 1. Large wasted space -> score close to 0.\n    # This should provide a good signal for \"tight fits\".\n\n    wasted_space = bins_remain_cap[can_fit_mask] - item\n    # Using `1.0 / (1.0 + wasted_space)` maps small positive wasted space to values close to 1.\n    # For a perfect fit (wasted_space = 0), score is 1.\n    # For larger wasted_space, score decreases.\n    scores_for_softmax = 1.0 / (1.0 + wasted_space)\n\n    # Apply softmax to get probabilities.\n    # Ensure temperature is positive to avoid division by zero or invalid operations.\n    if temperature <= 0:\n        raise ValueError(\"Temperature must be positive.\")\n\n    # Calculate exponentiated scores, scaled by temperature\n    # Lower temperature means sharper distribution, higher temperature means flatter.\n    exp_scores = np.exp(scores_for_softmax / temperature)\n\n    # Normalize to get probabilities\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Assign probabilities to the original priorities array\n    priorities[can_fit_mask] = probabilities\n\n    # Normalize priorities to sum to 1, ensuring valid probability distribution\n    # This is already handled by the softmax if there's at least one bin that can fit.\n    # If no bins can fit, priorities remains all zeros, which is correct.\n    if np.sum(priorities) > 0:\n        priorities /= np.sum(priorities)\n\n    return priorities\n\n[Reflection]\nPrioritize tight fits, use temperature for exploration/exploitation balance.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}