```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority scores for packing an item into bins using a refined
    priority function incorporating Softmax for exploration, bin scarcity,
    and a "best fit" preference.

    This heuristic aims to balance:
    1.  **Best Fit Preference:** Prioritize bins where the remaining capacity is
        closest to the item size, minimizing waste. This is achieved by
        penalizing excess capacity.
    2.  **Bin Scarcity:** Favor bins that have less remaining capacity overall,
        as these are considered "scarcer" resources.
    3.  **Softmax for Probabilistic Selection:** Uses Softmax to convert scores
        into probabilities, allowing for exploration of less optimal bins.
        The temperature parameter controls the degree of exploration.

    The combined score for a suitable bin is a weighted sum of the
    "anti-waste" score and the bin scarcity score.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.

    Returns:
        A NumPy array of the same size as `bins_remain_cap`, where each element
        is the probability score (from Softmax) for the corresponding bin.
        Bins that cannot fit the item will have a priority of 0.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    
    # Identify bins that can accommodate the item
    suitable_bins_mask = bins_remain_cap >= item
    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]
    suitable_bin_indices = np.where(suitable_bins_mask)[0]

    if suitable_bins_cap.size == 0:
        return priorities

    # --- Heuristic Component 1: "Anti-Waste" Score ---
    # We want bins where `bins_remain_cap - item` is minimized.
    # A good score would be the negative difference, so smaller differences are less negative (higher score).
    # Or, we can use a penalty function that increases with the difference.
    # Let's use the negative difference for simplicity, which is `item - bins_remain_cap`.
    # To make it more conducive to softmax (avoiding large negative numbers), we can transform it.
    # A simple approach is `-(bins_remain_cap - item)`.
    # To ensure scores are positive and have a decreasing trend as difference increases,
    # we can use something like `1 / (1 + (bins_remain_cap - item))`.
    # Let's use `item - suitable_bins_cap` as a base score, where smaller positive values are better.
    # We'll apply a transformation to ensure scores are in a reasonable range for softmax.
    
    # Calculate the "waste"
    waste = suitable_bins_cap - item
    
    # Transform waste into an "anti-waste" score. We want smaller waste to have higher scores.
    # Using a steepness parameter `k_waste` to control sensitivity to waste.
    k_waste = 5.0
    
    # Employ a function that maps small positive waste to high scores and larger waste to lower scores.
    # An exponential decay or a sigmoid-like function is suitable.
    # Let's use `exp(-k_waste * waste)`. This maps [0, inf) to (0, 1].
    # Clamp waste to avoid extremely small negative values that could lead to exp overflow.
    # However, waste is already guaranteed to be >= 0 here.
    anti_waste_scores = np.exp(-k_waste * waste)

    # --- Heuristic Component 2: Bin Scarcity Score ---
    # Prioritize bins with less remaining capacity.
    # Using `1 / (capacity + epsilon)` gives higher scores to smaller capacities.
    epsilon_scarcity = 1e-6
    scarcity_scores = 1 / (suitable_bins_cap + epsilon_scarcity)
    
    # Normalize scarcity scores to a [0, 1] range to be comparable with anti_waste_scores.
    min_scarcity = np.min(scarcity_scores)
    max_scarcity = np.max(scarcity_scores)
    
    # Handle case where all scarcity scores are identical to avoid division by zero
    if max_scarcity - min_scarcity > epsilon_scarcity:
        normalized_scarcity_scores = (scarcity_scores - min_scarcity) / (max_scarcity - min_scarcity)
    else:
        # If all suitable bins have the same scarcity, assign a neutral score (e.g., 0.5)
        # or 0 if we want to de-emphasize them when they are not distinct.
        # Let's assign 0, as scarcity doesn't differentiate them.
        normalized_scarcity_scores = np.zeros_like(scarcity_scores)

    # --- Combine Heuristics ---
    # Weighted sum. These weights can be tuned based on empirical performance.
    # Let's give more weight to minimizing waste, as it's often a primary goal in BPP.
    weight_anti_waste = 0.7
    weight_scarcity = 0.3
    
    combined_scores = (weight_anti_waste * anti_waste_scores) + (weight_scarcity * normalized_scarcity_scores)

    # --- Softmax for Probabilistic Exploration ---
    # Temperature parameter: Controls exploration.
    # Lower temperature -> greedy (closer to argmax).
    # Higher temperature -> more exploration (probabilities closer to uniform).
    temperature = 0.7  # Tunable parameter

    # Apply Softmax with numerical stability
    scores_for_softmax = combined_scores / temperature
    
    # Subtract max score for numerical stability in exp.
    # This ensures the largest value in `stable_scores` is 0.
    stable_scores = scores_for_softmax - np.max(scores_for_softmax)
    
    exp_scores = np.exp(stable_scores)
    sum_exp_scores = np.sum(exp_scores)
    
    # Handle cases where sum_exp_scores might be zero due to underflow (very unlikely with stable_scores)
    if sum_exp_scores == 0:
        # Fallback: uniform probability if all exp_scores are effectively zero.
        probabilities = np.ones_like(combined_scores) / combined_scores.size
    else:
        probabilities = exp_scores / sum_exp_scores

    # Place the calculated probabilities back into the main priorities array
    priorities[suitable_bins_mask] = probabilities

    return priorities
```
