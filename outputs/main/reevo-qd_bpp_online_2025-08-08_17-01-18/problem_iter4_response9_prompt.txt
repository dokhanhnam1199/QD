{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    The strategy favors bins that are a \"good fit\" for the item (i.e., leaving\n    a small remaining capacity), but with a probability epsilon, it assigns a\n    consistent exploration score to encourage trying less optimal bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.2  # Probability of exploration\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if len(suitable_bins_indices) == 0:\n        return priorities  # No bin can fit the item\n\n    # --- Exploitation Component (Good Fit) ---\n    # Calculate a score based on how well the item fits.\n    # We want to prioritize bins that leave minimal remaining capacity.\n    # Score = 1 / (remaining_capacity - item + epsilon)\n    # A smaller difference means a higher score.\n    fit_scores = 1.0 / (bins_remain_cap[suitable_bins_indices] - item + 1e-6)\n\n    # Normalize fit_scores to a 0-1 range to represent the \"exploitation\" priority.\n    # Higher score means better fit.\n    if fit_scores.max() > fit_scores.min():\n        exploitation_priorities = (fit_scores - fit_scores.min()) / (fit_scores.max() - fit_scores.min())\n    else:\n        exploitation_priorities = np.ones(len(suitable_bins_indices)) # All fits are equally good\n\n    # Assign the exploitation priorities to the suitable bins\n    priorities[suitable_bins_indices] = exploitation_priorities\n\n    # --- Exploration Component ---\n    # With probability epsilon, overwrite the exploitation priority with a\n    # consistent, lower exploration score for a random subset of suitable bins.\n    # This encourages exploration of bins that might not be the immediate best fit.\n    exploration_score = 0.1 # A fixed low score for exploration\n\n    # Determine which suitable bins will be subject to exploration\n    num_suitable = len(suitable_bins_indices)\n    explore_indices_in_suitable = np.random.choice(\n        num_suitable,\n        size=int(np.ceil(epsilon * num_suitable)),\n        replace=False\n    )\n    \n    # Get the actual indices in the original bins_remain_cap array\n    bins_to_explore_indices = suitable_bins_indices[explore_indices_in_suitable]\n\n    # Assign the exploration score to these bins\n    priorities[bins_to_explore_indices] = exploration_score\n\n    # Ensure that bins that cannot fit the item have zero priority\n    priorities[~suitable_bins_mask] = 0\n\n    # Optional: Normalize final priorities if a specific range is required by downstream logic.\n    # For now, we return the scores as calculated, where higher means more preferred.\n    # A simple max-min normalization can be applied if needed:\n    # if priorities.max() > priorities.min():\n    #     final_priorities = (priorities - priorities.min()) / (priorities.max() - priorities.min())\n    # else:\n    #     final_priorities = np.ones(num_bins) * 0.5\n    # return final_priorities\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score aims to prioritize bins that are a \"good fit\" for the item.\n    A good fit is generally considered to be a bin where the remaining capacity\n    is slightly larger than the item size, avoiding both empty bins and bins that\n    are almost full.\n\n    The sigmoid function `1 / (1 + exp(-k * (x - x0)))` maps any real number\n    to a value between 0 and 1. We use it here to score how \"close\" the remaining\n    capacity is to the item size.\n\n    We want bins where `bin_remain_cap - item` is close to 0.\n    So, a bin with `bin_remain_cap >= item` is a candidate.\n    Among these candidates, we prefer those where `bin_remain_cap` is just enough\n    to fit the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Consider only bins that have enough capacity for the item\n    available_bins_mask = bins_remain_cap >= item\n    available_bins_cap = bins_remain_cap[available_bins_mask]\n\n    if available_bins_cap.size == 0:\n        return priorities  # No bins can accommodate the item\n\n    # Calculate the \"mismatch\" for available bins: remaining_cap - item\n    mismatch = available_bins_cap - item\n\n    # We want to give higher scores to bins where mismatch is close to 0.\n    # The sigmoid function can be used to create a score that peaks around 0.\n    # We can transform the mismatch values. A negative value for mismatch\n    # means the bin is too small (already handled by the mask), so we focus on\n    # non-negative mismatches.\n\n    # A simple approach is to invert the mismatch so that smaller mismatches\n    # become larger values, and then apply a sigmoid.\n    # However, a direct sigmoid on mismatch might not be ideal because it\n    # emphasizes very large remaining capacities.\n\n    # Let's try to design a sigmoid function that peaks when mismatch is zero.\n    # The function f(x) = 1 / (1 + exp(-k * x)) peaks at x=0 if we shift it or\n    # use it as is. We want to reward bins where `mismatch` is small (close to 0).\n\n    # Let's map `mismatch` to a value where 0 mismatch is the optimal value.\n    # Consider `score = sigmoid(k * (threshold - mismatch))`\n    # Where `threshold` is the ideal mismatch (e.g., 0).\n    # A large `k` makes the transition steeper.\n    # We want to give a high score if `mismatch` is small.\n    # So, if `mismatch` is 0, we want a high score.\n    # If `mismatch` is large, we want a low score.\n\n    # Let's try `sigmoid(k * (-(mismatch)))` which is `sigmoid(-k * mismatch)`.\n    # If mismatch is 0, score is 0.5.\n    # If mismatch is positive, score is < 0.5.\n    # If mismatch is negative, score is > 0.5.\n\n    # This is not quite right as it gives lower scores for small positive mismatches.\n    # We want to incentivize fitting tightly but still fitting.\n\n    # Let's use a shifted and scaled sigmoid.\n    # We can define a score function that is high when `mismatch` is small and positive.\n    # Consider `score = 1 / (1 + exp(-k * (ideal_mismatch - mismatch)))`.\n    # If `ideal_mismatch` is 0, this is `1 / (1 + exp(-k * (-mismatch)))`.\n    # This will give higher scores for negative mismatch (bins too small, which we filter).\n\n    # Alternative: Focus on `item / bin_remain_cap`.\n    # If `bin_remain_cap` is very large, this ratio is small.\n    # If `bin_remain_cap` is just above `item`, this ratio is close to 1.\n    # We want to maximize this ratio, but not exceed 1.\n    # We can use a sigmoid that maps values close to 1 (but less than 1) to high scores.\n\n    # Let's refine the mismatch approach:\n    # We want to reward bins where `bin_remain_cap` is close to `item`.\n    # The difference `bin_remain_cap - item` should be small.\n    # A value close to 0 for this difference is good.\n    # Let's scale and shift the mismatch: `scaled_mismatch = (mismatch - mean_mismatch) / std_mismatch`\n    # Or, a simpler approach: normalize mismatch relative to some max possible mismatch.\n\n    # Sigmoid strategy: score = 1 / (1 + exp(-k * (target_val - current_val)))\n    # We want `current_val` (which is `mismatch`) to be close to `target_val` (e.g., 0).\n    # If `target_val = 0`, then `score = 1 / (1 + exp(-k * (-mismatch)))`.\n    # This gives high scores for negative `mismatch` (too small).\n    # This is not ideal because we already filter `mismatch < 0`.\n\n    # Let's try: `score = 1 / (1 + exp(-k * (mismatch)))`\n    # Mismatch = 0 -> score = 0.5\n    # Mismatch > 0 -> score < 0.5 (worse fit)\n    # Mismatch < 0 -> score > 0.5 (better fit)\n\n    # This still seems to penalize small positive mismatches.\n    # We need a function that peaks at mismatch = 0.\n\n    # Consider a Gaussian-like shape using exp(-x^2).\n    # `score = exp(-k * mismatch**2)`\n    # Mismatch = 0 -> score = 1\n    # Mismatch != 0 -> score < 1. This is a good candidate.\n    # This can be approximated with a sigmoid.\n\n    # Let's re-think the sigmoid target.\n    # We want `bin_remain_cap` to be just enough.\n    # If we focus on `bin_remain_cap`, we want it to be close to `item`.\n    # `score = sigmoid(k * (item - bin_remain_cap))`\n    # If `bin_remain_cap` is slightly larger than `item`, `item - bin_remain_cap` is small and negative.\n    # `sigmoid(small_negative_val)` is close to 0.\n    # If `bin_remain_cap` is much larger than `item`, `item - bin_remain_cap` is a large negative.\n    # `sigmoid(large_negative_val)` is close to 0.\n    # If `bin_remain_cap` is exactly `item`, `item - bin_remain_cap` is 0.\n    # `sigmoid(0)` is 0.5.\n\n    # This function favors bins with `item - bin_remain_cap` being small negative,\n    # which means `bin_remain_cap` is slightly larger than `item`.\n\n    # Let's define k as a parameter controlling sensitivity. A higher k means\n    # a sharper peak around the ideal fit.\n    k = 5.0  # Sensitivity parameter, can be tuned.\n\n    # Calculate the sigmoid score for available bins.\n    # We want `bin_remain_cap` to be as close to `item` as possible, but >= `item`.\n    # This means we want `bin_remain_cap - item` to be small and non-negative.\n    # Let's transform this difference to get a score.\n\n    # A score that peaks when `bin_remain_cap - item` is 0.\n    # `score = 1 / (1 + exp(-k * (max_possible_item_size - (bin_remain_cap - item))))`\n    # This seems complicated.\n\n    # Let's use the mismatch and map it.\n    # We want to reward small positive mismatch values.\n    # We can use a sigmoid with a shift.\n    # `score = 1 / (1 + exp(-k * (mismatch_target - mismatch)))`\n    # If `mismatch_target` is 0, then `score = 1 / (1 + exp(-k * (-mismatch)))`.\n    # This would give high scores for negative mismatch values.\n\n    # Let's consider what `bins_remain_cap - item` represents:\n    # Small positive value: good fit\n    # Large positive value: too much space, potentially wasted\n    # Zero: perfect fit\n\n    # We can try to penalize large positive values.\n    # `score = 1 - sigmoid(k * (mismatch))`\n    # `score = 1 - 1 / (1 + exp(-k * mismatch))`\n    # `score = exp(-k * mismatch) / (1 + exp(-k * mismatch))`\n    # If mismatch is 0, score = 0.5.\n    # If mismatch is large positive, score -> 0.\n    # If mismatch is large negative, score -> 1.\n\n    # This gives high scores to bins that are too small, which is counter-intuitive.\n\n    # The common way to implement a \"best fit\" using sigmoid is to target\n    # the difference `bin_remain_cap - item` being close to 0.\n    # `sigmoid(k * (target_val - x))` where x is the value being measured.\n    # Here, we want to measure `bin_remain_cap`.\n    # `target_val` would be `item`.\n    # So, `score = 1 / (1 + exp(-k * (item - bin_remain_cap)))`\n\n    # Let's re-evaluate `score = 1 / (1 + exp(-k * (bin_remain_cap - item)))`.\n    # If `bin_remain_cap == item` (mismatch=0), score = 0.5.\n    # If `bin_remain_cap` > `item` (mismatch > 0), e.g., `bin_remain_cap = item + delta`\n    # `score = 1 / (1 + exp(-k * delta))`. If delta > 0, exp(-k*delta) < 1.\n    # So `1 + exp < 2`, and `score > 0.5`. The larger delta, the smaller exp(-k*delta), closer to 0. So score gets closer to 1.\n    # This penalizes larger remaining capacity, which is WRONG.\n\n    # Let's flip it: `score = 1 / (1 + exp(-k * (item - bin_remain_cap)))`\n    # If `bin_remain_cap == item` (mismatch=0), score = 0.5.\n    # If `bin_remain_cap` > `item` (mismatch > 0), e.g., `bin_remain_cap = item + delta`\n    # `score = 1 / (1 + exp(-k * (-delta))) = 1 / (1 + exp(k * delta))`.\n    # If delta > 0, k*delta > 0. exp(k*delta) > 1. `1 + exp > 2`.\n    # So `score < 0.5`. The larger delta, the smaller the score. This is CORRECT.\n    # If `bin_remain_cap` < `item` (mismatch < 0), this case is filtered.\n    # If we didn't filter, and `bin_remain_cap = item - delta'`, where delta' > 0.\n    # `score = 1 / (1 + exp(-k * (item - (item - delta')))) = 1 / (1 + exp(k * delta'))`.\n    # If delta' > 0, k*delta' > 0. exp(k*delta') > 1. `1 + exp > 2`. So `score < 0.5`.\n    # This would give low scores to bins that are too small. This is also fine,\n    # but we already use a mask for this.\n\n    # So, the expression `1 / (1 + exp(-k * (item - bin_remain_cap)))` for `bin_remain_cap >= item`\n    # appears to be a reasonable Sigmoid Fit Score. It assigns higher scores to bins\n    # where `bin_remain_cap` is closer to `item`.\n\n    # We want to make sure the range of arguments to sigmoid is reasonable.\n    # If `bin_remain_cap` is very large, `item - bin_remain_cap` can be a large negative number.\n    # `exp(large_positive)` can overflow.\n    # If `bin_remain_cap` is exactly `item`, argument is 0.\n    # If `bin_remain_cap` is slightly larger than `item`, argument is small negative.\n\n    # To prevent potential issues with very large capacities and the sigmoid argument:\n    # We can clip the `bin_remain_cap` for calculating the argument, or scale.\n    # Alternatively, we can use `1 / (1 + exp(k * (bin_remain_cap - item)))`\n    # Let's check this:\n    # `score = 1 / (1 + exp(k * (bin_remain_cap - item)))`\n    # If `bin_remain_cap == item` (mismatch=0), score = 1 / (1 + exp(0)) = 1 / (1 + 1) = 0.5.\n    # If `bin_remain_cap` > `item` (mismatch > 0), e.g., `bin_remain_cap = item + delta`.\n    # `score = 1 / (1 + exp(k * delta))`. If delta > 0, k*delta > 0. exp(k*delta) > 1.\n    # `1 + exp > 2`. So `score < 0.5`. The larger delta, the smaller the score. CORRECT.\n    # This expression seems more numerically stable for large positive differences.\n\n    # Let's use this expression.\n    # We'll compute it for the available bins.\n\n    # Parameters for sigmoid: k (steepness)\n    k = 5.0 # Sensitivity. Higher k means score drops faster as capacity increases beyond item.\n\n    # Calculate the argument for the sigmoid: k * (remaining_capacity - item)\n    # We are interested in `bins_remain_cap >= item`.\n    # `argument = k * (available_bins_cap - item)`\n\n    # The sigmoid function: `sigmoid(x) = 1 / (1 + exp(-x))`\n    # Our chosen formula is `1 / (1 + exp(k * (bin_remain_cap - item)))`.\n    # Let `x = k * (bin_remain_cap - item)`. Then `sigmoid_val = 1 / (1 + exp(x))`.\n    # This is also equivalent to `1 - sigmoid(-x) = 1 - 1 / (1 + exp(-(-x))) = 1 - 1 / (1 + exp(x))`.\n    # So, it's 1 minus a standard sigmoid applied to `k * (bin_remain_cap - item)`.\n    # This shape is suitable: peaks at 0.5, decreases for positive arguments.\n\n    # Applying the sigmoid to the mismatches for the available bins\n    # We want to assign these calculated priorities back to the original `priorities` array.\n    sigmoid_scores = 1 / (1 + np.exp(k * (available_bins_cap - item)))\n\n    # Place the calculated sigmoid scores back into the priorities array\n    priorities[available_bins_mask] = sigmoid_scores\n\n    return priorities\n\n[Reflection]\nThe better heuristic uses a sigmoid to reward bins that are a \"good fit,\" avoiding exploration bias.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}