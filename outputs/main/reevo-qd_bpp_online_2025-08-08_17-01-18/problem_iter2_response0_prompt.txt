{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using an epsilon-greedy strategy.\n\n    The strategy favors bins that are a \"good fit\" for the item (i.e., leaving\n    a small remaining capacity), but with a probability epsilon, it explores\n    other bins randomly.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.2  # Probability of exploring\n\n    n_bins = len(bins_remain_cap)\n    priorities = np.zeros(n_bins)\n\n    # Calculate a \"goodness of fit\" score for each bin\n    # A smaller remaining capacity after fitting the item is considered better.\n    # We use 1 / (remaining_capacity - item + 1e-9) to ensure division by zero is avoided\n    # and to give higher scores to bins with less remaining capacity.\n    # Only consider bins that can actually fit the item.\n    suitable_bins_mask = bins_remain_cap >= item\n    priorities[suitable_bins_mask] = 1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)\n\n    # Epsilon-greedy part: with probability epsilon, choose a random suitable bin\n    if np.random.rand() < epsilon:\n        # Get indices of suitable bins\n        suitable_indices = np.where(suitable_bins_mask)[0]\n        if len(suitable_indices) > 0:\n            # Randomly pick one suitable bin\n            chosen_index = np.random.choice(suitable_indices)\n            # Assign a very high priority to the randomly chosen bin\n            priorities[chosen_index] = np.max(priorities) + 1.0 # Give it a slightly higher priority than the best greedy choice\n    else:\n        # Otherwise, follow the greedy approach (already calculated in `priorities`)\n        pass # priorities are already set to the greedy scores\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    This heuristic prioritizes bins that have a remaining capacity slightly larger than the item,\n    aiming to minimize wasted space. It uses a softmax function to convert these differences\n    into probabilities, effectively assigning higher priority to bins that are a \"good fit\".\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the \"fit\" of the item into each bin.\n    # We want bins where bins_remain_cap is just enough or slightly more than the item.\n    # A negative value here means the item doesn't fit. We can clamp these to a small\n    # positive value or 0 to avoid issues with softmax if all items don't fit.\n    # A large positive difference (item fits easily) is also not ideal as it wastes space.\n    # So we want the difference (bins_remain_cap - item) to be close to zero.\n    # We can use the negative of this difference as the exponent in softmax,\n    # so smaller (bins_remain_cap - item) results in a higher exponent.\n    fits = bins_remain_cap - item\n\n    # Filter out bins where the item does not fit (remaining capacity < item size)\n    # Assign a very low priority (or effectively zero) to these bins.\n    # A large negative number in softmax exponent will result in a value close to 0.\n    # We can also directly set their fits to a very low value before softmax.\n    fits[fits < 0] = -np.inf # Effectively 0 probability after softmax\n\n    # Apply the softmax function. The exponent in softmax should reflect desirability.\n    # We want bins with (bins_remain_cap - item) close to 0 to have high priority.\n    # So, we can use -(bins_remain_cap - item) as the exponent, or more intuitively,\n    # (item - bins_remain_cap) as the exponent, ensuring negative values become smaller.\n    # Or even better, prioritize bins with a positive difference close to zero.\n    # Let's define desirability as: higher is better if remaining_cap is slightly > item.\n    # Consider a transformation: exp(-abs(fits)). This would favor fits near 0.\n    # However, softmax typically takes logits (raw scores).\n    # Let's consider the score `s_i = -(bins_remain_cap_i - item)^2`. This penalizes\n    # being too small or too large. But we only care about `bins_remain_cap_i >= item`.\n    # So, if `bins_remain_cap_i < item`, priority should be 0.\n    # If `bins_remain_cap_i >= item`, we want `bins_remain_cap_i - item` to be small.\n    # A good transformation for softmax could be to map `bins_remain_cap_i - item` to a\n    # desirability score. Higher `bins_remain_cap_i` than `item` is good, but not too high.\n    # Let's use `bins_remain_cap_i - item` directly as logits, but only for bins that can fit.\n    # For bins that cannot fit, their logit should be extremely low.\n\n    # Option 1: Softmax on (bins_remain_cap - item) for fitting bins.\n    # Prioritize bins where remaining capacity is *exactly* the item size.\n    # Using -fits will invert the ordering, so smaller positive diffs become larger.\n    # exp(-fits) where fits = bins_remain_cap - item\n    # If bins_remain_cap = 5, item = 3, fit = 2. exp(-2) = 0.135\n    # If bins_remain_cap = 3, item = 3, fit = 0. exp(0) = 1.0\n    # If bins_remain_cap = 6, item = 3, fit = 3. exp(-3) = 0.049\n    # This prioritizes exact fits.\n\n    # We need to ensure that we don't assign probabilities to bins where the item doesn't fit.\n    # A common way to do this with softmax is to assign a very low logit (large negative number).\n    logits = bins_remain_cap - item\n    # For bins that cannot fit the item, set their logit to a very small number.\n    # This will make their softmax probability close to zero.\n    logits[logits < 0] = -1e9  # A large negative number\n\n    # For bins that can fit, we want to prioritize those with smaller remaining capacity (closer to item size).\n    # So, we want to exponentiate -(bins_remain_cap - item) which is (item - bins_remain_cap).\n    # However, the softmax input should ideally be positive values that represent scores.\n    # Let's redefine `scores`: a higher score means better fit.\n    # A perfect fit would have score X.\n    # A slightly larger capacity would have score X - epsilon.\n    # A much larger capacity would have score X - delta.\n    # A capacity smaller than item would have score -infinity.\n    # So, we can use `item - bins_remain_cap` as our underlying measure for exponentiation,\n    # but we need to handle the case where `bins_remain_cap < item`.\n\n    # Let's use a modified approach. We want bins that *can* fit, and among those,\n    # we prefer bins with less excess capacity.\n    # So, a \"goodness\" score could be:\n    # -infinity if item > bins_remain_cap\n    # -(bins_remain_cap - item) otherwise (prioritizing smaller differences)\n\n    scores = -(bins_remain_cap - item)\n    scores[bins_remain_cap < item] = -np.inf # Ensure items not fitting get no chance\n\n    # Now apply softmax. Softmax(x_i) = exp(x_i) / sum(exp(x_j)).\n    # The `scores` directly go into the exponent of softmax.\n    # Higher scores (closer to 0, or less negative) mean higher probability.\n    # If scores = [-inf, -inf, 0, -2, -5],\n    # exp(scores) = [0, 0, 1, exp(-2), exp(-5)]\n    # Softmax will normalize these.\n\n    # Add a small constant to the score for bins that can fit, to ensure\n    # that even a small positive difference doesn't get zero probability.\n    # For instance, if we use -(bins_remain_cap - item), a difference of 10\n    # gives exp(-10) which is tiny.\n    # Perhaps a linear scaling or a different transformation is better.\n    # Let's try mapping `bins_remain_cap - item` to a desirability score.\n    # If diff = 0, score = K (high)\n    # If diff = 1, score = K - epsilon\n    # If diff = 10, score = K - delta\n    # If diff < 0, score = -infinity\n\n    # Let's re-think the logit construction for softmax.\n    # We want the probability P_i proportional to exp(logit_i).\n    # Desirability of bin i for item: D_i\n    # P_i = exp(alpha * D_i) / sum(exp(alpha * D_j))\n    # If item doesn't fit: D_i = -infinity\n    # If item fits and diff = bins_remain_cap - item:\n    # We want smaller diffs to have higher D_i.\n    # So, D_i = -(bins_remain_cap - item) = item - bins_remain_cap.\n\n    # This approach correctly prioritizes bins with less remaining capacity over\n    # bins with more remaining capacity, for those that can fit the item.\n    # The `-np.inf` for non-fitting bins ensures they get 0 probability.\n    # The `alpha` parameter (implicitly 1 here) controls the \"peakiness\" of the distribution.\n\n    # Calculate the underlying scores for softmax.\n    # A higher score means more desirable.\n    # If bins_remain_cap >= item, we want bins_remain_cap - item to be small.\n    # So, let's use `-(bins_remain_cap - item)` which is `item - bins_remain_cap`.\n    # For bins where `bins_remain_cap < item`, the score should be very low.\n\n    scores = item - bins_remain_cap\n\n    # Set scores to a very low value for bins where the item does not fit.\n    # This ensures their probability contribution in softmax is negligible.\n    scores[bins_remain_cap < item] = -1e9 # Effectively -infinity for softmax\n\n    # Calculate probabilities using softmax.\n    # Adding 1 to scores for fitting bins can help avoid all zeros if all diffs are large negative\n    # but we've handled that with -1e9 for non-fitting bins.\n    # The softmax calculation itself: exp(scores) / sum(exp(scores))\n    # We can use np.exp directly, but be mindful of overflow/underflow if scores are very large/small.\n    # If scores = [-1e9, -1e9, 0, -2, -5], then exp(scores) = [0, 0, 1, exp(-2), exp(-5)]\n    # This seems correct.\n    exp_scores = np.exp(scores)\n\n    # If all items resulted in -1e9 (meaning item doesn't fit any bin), exp_scores will be all zeros.\n    # In this case, the sum will be zero, leading to division by zero. Handle this edge case.\n    sum_exp_scores = np.sum(exp_scores)\n    if sum_exp_scores == 0:\n        # This means the item cannot fit into any available bin.\n        # Return zero priorities for all bins.\n        return np.zeros_like(bins_remain_cap)\n\n    priorities = exp_scores / sum_exp_scores\n\n    return priorities\n\n[Reflection]\nPrioritize \"good fits\" with softmax; explore randomly only sparingly.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}