[
  {
    "stdout_filepath": "problem_iter1_response20.txt_stdout.txt",
    "code_path": "problem_iter1_code20.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit Decreasing logic.\n\n    This heuristic prioritizes bins that can accommodate the item and are \"tight fits\"\n    to minimize wasted space. Bins that are too small for the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate priority as the inverse of the remaining capacity\n    # A smaller remaining capacity means a tighter fit, hence higher priority.\n    # We add a small epsilon to avoid division by zero if a bin has exactly the item's size.\n    priorities[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)\n\n    return priorities",
    "response_id": 20,
    "obj": 4.048663741523748,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This heuristic implements a \"Best Fit Decreasing\" like strategy for the\n    priority function within an online First Fit context. For each item, it\n    prioritizes bins that can accommodate the item, and among those, it\n    prefers bins that will have the least remaining capacity after the item\n    is placed. This aims to \"tightly pack\" bins, potentially leaving larger\n    spaces in other bins for future larger items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Calculate remaining capacity after placing the item\n    potential_remaining_cap = bins_remain_cap - item\n    \n    # Bins that can accommodate the item have a non-negative remaining capacity\n    can_accommodate = potential_remaining_cap >= 0\n    \n    # Assign a priority:\n    # For bins that can accommodate the item, the priority is the inverse of\n    # the remaining capacity after placement. Smaller remaining capacity gets higher priority.\n    # We use a large number (e.g., 1e6) for bins that cannot accommodate the item\n    # to ensure they have a much lower priority.\n    \n    # To make smaller remaining capacities have higher priority, we can use negative.\n    # The more negative, the better.\n    priorities[can_accommodate] = -potential_remaining_cap[can_accommodate]\n    \n    # Bins that cannot accommodate the item get a very low priority (a large positive number)\n    # We can also set it to 0 if we want to strictly exclude them if a better option exists.\n    # However, a very low negative number makes more sense for sorting if we want to be\n    # sure they are not picked. For First Fit, it's about *which* bin to try first.\n    # A common way to handle \"cannot accommodate\" in priority systems is to give them a\n    # very low score. If we want to avoid them entirely, we can assign a score that will\n    # never be chosen if any other valid option exists. Let's use a value that is\n    # significantly less desirable than any valid remaining capacity.\n    # Since remaining capacities are non-negative, negative values are good.\n    # We can use a large negative number for those that cannot fit.\n    priorities[~can_accommodate] = -1e9 # A very low priority\n    \n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit strategy.\n\n    The Best Fit strategy aims to place the item into the bin that has the least remaining\n    capacity after the item is placed, provided it fits. This helps in keeping larger\n    bins available for larger items later.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a more desirable bin. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    # Calculate remaining capacity after placing the item\n    potential_remaining_caps = bins_remain_cap - item\n\n    # Initialize priorities to zero\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Find bins where the item can fit\n    fit_indices = np.where(potential_remaining_caps >= 0)[0]\n\n    if len(fit_indices) > 0:\n        # For bins that can fit the item, assign a priority.\n        # The \"best fit\" is the one that leaves the *least* remaining capacity.\n        # To make this a \"highest priority\" score, we can invert the remaining capacity\n        # (or use a large number minus remaining capacity).\n        # A simple approach is to use the negative of the potential remaining capacity.\n        # The more negative, the better the fit (i.e., less remaining space).\n        priorities[fit_indices] = -potential_remaining_caps[fit_indices]\n\n        # To ensure that a bin that is a \"perfect fit\" (leaves 0 remaining capacity)\n        # is prioritized over one that leaves, say, -1 remaining capacity (meaning it was\n        # a bit too large), we can further refine the priority.\n        # If multiple bins have the same minimum remaining capacity, any of them is fine.\n        # The current negative remaining capacity already ranks them appropriately.\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response3.txt_stdout.txt",
    "code_path": "problem_iter1_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    The Almost Full Fit strategy prioritizes bins that, after placing the item,\n    will have the least remaining capacity among bins that can still accommodate the item.\n    This aims to fill bins as much as possible before opening new ones.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot accommodate the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find bins that can accommodate the item\n    possible_bins_mask = bins_remain_cap >= item\n\n    # Calculate remaining capacity after placing the item in possible bins\n    remaining_after_placement = bins_remain_cap[possible_bins_mask] - item\n\n    # If there are no possible bins, return all zeros\n    if remaining_after_placement.size == 0:\n        return priorities\n\n    # Calculate the \"tightness\" score for possible bins.\n    # We want to minimize the remaining capacity, so a smaller remaining capacity\n    # should result in a higher priority.\n    # We use the inverse of the remaining capacity. To avoid division by zero\n    # or extremely high priorities for bins that become exactly full, we can\n    # add a small epsilon or use a scaled inverse.\n    # A common approach is to consider bins that leave little to no space.\n    # We want to maximize the chance of the bin becoming \"almost full\".\n\n    # Let's aim for a score where smaller remaining capacity is better.\n    # A simple approach is to use the negative of the remaining capacity.\n    # However, to make it a \"priority\" score (higher is better), we can\n    # invert it and potentially scale it.\n\n    # To prioritize bins that leave minimal remaining space, we can take\n    # the negative of the remaining capacity. The larger (less negative)\n    # the value, the less space is left, thus higher priority.\n    # Let's refine this: We want bins that, after placing the item, will have\n    # the *least* remaining capacity. This means we want to *minimize*\n    # `bins_remain_cap - item`.\n    # If we want higher scores to be better, we can assign a score based on\n    # the inverse of the remaining capacity.\n    # However, the goal is to fill bins. So bins that will be closest to full\n    # after placing the item are preferred.\n\n    # Consider the difference: max_capacity - (remaining_after_placement)\n    # This is effectively how much space is used. We want to maximize this.\n    # So, `item` is constant. Maximizing `bins_remain_cap[i] - remaining_after_placement[i]`\n    # means maximizing `bins_remain_cap[i] - (bins_remain_cap[i] - item)` which is just `item`.\n    # This isn't quite right.\n\n    # \"Almost Full Fit\" suggests bins that are ALMOST full.\n    # After placing the item, a bin is \"almost full\" if its remaining capacity is small.\n    # So, we want to minimize `bins_remain_cap[i] - item`.\n    # To convert this to a priority score (higher is better), we can:\n    # 1. Use `1 / (remaining_after_placement + epsilon)` where epsilon is a small number to avoid division by zero.\n    # 2. Use `-(remaining_after_placement)`\n    # 3. Use `max_possible_remaining - remaining_after_placement` for some large `max_possible_remaining`\n    #    which is equivalent to `some_constant - remaining_capacity`.\n\n    # Let's try option 2: higher priority for smaller remaining capacity.\n    # So, `priority = -remaining_capacity`. This means a bin with remaining_capacity=1\n    # gets priority -1, and a bin with remaining_capacity=0 gets priority 0.\n    # This seems to fit the idea of \"smallest remaining capacity\".\n\n    # To make it more \"priority-like\" (higher is better), we can use:\n    # `priority = C - remaining_capacity`, where C is a large constant, or\n    # `priority = 1 / (remaining_capacity + epsilon)`\n\n    # Let's use the concept that the *difference* between what's remaining and what's desired (a full bin)\n    # should be minimized. So, `remaining_capacity` should be small.\n    # A score that reflects this: `max(0, C - remaining_capacity)`.\n    # If we want to prioritize bins that become *most* full after the item,\n    # this means the remaining capacity is minimized.\n\n    # Consider the bin that would become \"most full\". This is the bin where\n    # `bins_remain_cap[i] - item` is minimized.\n    # So, higher priority for smaller `bins_remain_cap[i] - item`.\n    # Let's transform `bins_remain_cap[i] - item` into a priority:\n    # `priority = some_large_value - (bins_remain_cap[i] - item)`\n    # This is equivalent to `some_large_value - bins_remain_cap[i] + item`.\n    # Since `item` is constant for all bins, this is `constant - bins_remain_cap[i]`.\n    # We want to maximize this, so we want to minimize `bins_remain_cap[i]`.\n\n    # Another perspective on \"almost full\":\n    # We want to pick the bin that, after placing the item, is closest to full.\n    # This means `bins_remain_cap[i] - item` is minimal.\n    # Let's assign a priority based on this minimum value.\n\n    # A common heuristic for \"Best Fit\" or \"Almost Full Fit\" is to try to minimize\n    # the remaining capacity of the bin *after* placing the item.\n    # This means we want to select the bin `i` that minimizes `bins_remain_cap[i] - item`.\n    # To convert this into a priority score (where higher means more desirable),\n    # we can use `-(bins_remain_cap[i] - item)` or `1 / (bins_remain_cap[i] - item + epsilon)`.\n\n    # Let's try using `1 / (remaining_after_placement + epsilon)` where epsilon is small.\n    # This ensures that bins with very small remaining capacity get very high priorities.\n\n    epsilon = 1e-6\n    priorities[possible_bins_mask] = 1.0 / (remaining_after_placement + epsilon)\n\n    # Alternatively, if we want to give a higher score to bins that leave *less*\n    # remaining space, we can simply use the negative of the remaining space,\n    # or a scaled version of it.\n\n    # Let's consider the original \"Almost Full Fit\" idea: prioritize bins that are\n    # already close to full. However, the problem statement for this function\n    # implies prioritizing based on the *outcome* after placing the item.\n    # \"priority score for each bin ... bin with the highest priority score will be selected for the item.\"\n\n    # If the goal is to fill bins, then after placing the item, we want the bin\n    # with the *least* remaining capacity.\n    # So, `priority = f(remaining_capacity)` where `f` is decreasing.\n    # `f(x) = 1/(x + epsilon)` or `f(x) = -x`.\n    # Let's use the `1/(x + epsilon)` approach for a strong preference towards tighter fits.\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response26.txt_stdout.txt",
    "code_path": "problem_iter1_code26.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score aims to prioritize bins that are a \"good fit\" for the item.\n    A good fit is generally considered to be a bin where the remaining capacity\n    is slightly larger than the item size, avoiding both empty bins and bins that\n    are almost full.\n\n    The sigmoid function `1 / (1 + exp(-k * (x - x0)))` maps any real number\n    to a value between 0 and 1. We use it here to score how \"close\" the remaining\n    capacity is to the item size.\n\n    We want bins where `bin_remain_cap - item` is close to 0.\n    So, a bin with `bin_remain_cap >= item` is a candidate.\n    Among these candidates, we prefer those where `bin_remain_cap` is just enough\n    to fit the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Consider only bins that have enough capacity for the item\n    available_bins_mask = bins_remain_cap >= item\n    available_bins_cap = bins_remain_cap[available_bins_mask]\n\n    if available_bins_cap.size == 0:\n        return priorities  # No bins can accommodate the item\n\n    # Calculate the \"mismatch\" for available bins: remaining_cap - item\n    mismatch = available_bins_cap - item\n\n    # We want to give higher scores to bins where mismatch is close to 0.\n    # The sigmoid function can be used to create a score that peaks around 0.\n    # We can transform the mismatch values. A negative value for mismatch\n    # means the bin is too small (already handled by the mask), so we focus on\n    # non-negative mismatches.\n\n    # A simple approach is to invert the mismatch so that smaller mismatches\n    # become larger values, and then apply a sigmoid.\n    # However, a direct sigmoid on mismatch might not be ideal because it\n    # emphasizes very large remaining capacities.\n\n    # Let's try to design a sigmoid function that peaks when mismatch is zero.\n    # The function f(x) = 1 / (1 + exp(-k * x)) peaks at x=0 if we shift it or\n    # use it as is. We want to reward bins where `mismatch` is small (close to 0).\n\n    # Let's map `mismatch` to a value where 0 mismatch is the optimal value.\n    # Consider `score = sigmoid(k * (threshold - mismatch))`\n    # Where `threshold` is the ideal mismatch (e.g., 0).\n    # A large `k` makes the transition steeper.\n    # We want to give a high score if `mismatch` is small.\n    # So, if `mismatch` is 0, we want a high score.\n    # If `mismatch` is large, we want a low score.\n\n    # Let's try `sigmoid(k * (-(mismatch)))` which is `sigmoid(-k * mismatch)`.\n    # If mismatch is 0, score is 0.5.\n    # If mismatch is positive, score is < 0.5.\n    # If mismatch is negative, score is > 0.5.\n\n    # This is not quite right as it gives lower scores for small positive mismatches.\n    # We want to incentivize fitting tightly but still fitting.\n\n    # Let's use a shifted and scaled sigmoid.\n    # We can define a score function that is high when `mismatch` is small and positive.\n    # Consider `score = 1 / (1 + exp(-k * (ideal_mismatch - mismatch)))`.\n    # If `ideal_mismatch` is 0, this is `1 / (1 + exp(-k * (-mismatch)))`.\n    # This will give higher scores for negative mismatch (bins too small, which we filter).\n\n    # Alternative: Focus on `item / bin_remain_cap`.\n    # If `bin_remain_cap` is very large, this ratio is small.\n    # If `bin_remain_cap` is just above `item`, this ratio is close to 1.\n    # We want to maximize this ratio, but not exceed 1.\n    # We can use a sigmoid that maps values close to 1 (but less than 1) to high scores.\n\n    # Let's refine the mismatch approach:\n    # We want to reward bins where `bin_remain_cap` is close to `item`.\n    # The difference `bin_remain_cap - item` should be small.\n    # A value close to 0 for this difference is good.\n    # Let's scale and shift the mismatch: `scaled_mismatch = (mismatch - mean_mismatch) / std_mismatch`\n    # Or, a simpler approach: normalize mismatch relative to some max possible mismatch.\n\n    # Sigmoid strategy: score = 1 / (1 + exp(-k * (target_val - current_val)))\n    # We want `current_val` (which is `mismatch`) to be close to `target_val` (e.g., 0).\n    # If `target_val = 0`, then `score = 1 / (1 + exp(-k * (-mismatch)))`.\n    # This gives high scores for negative `mismatch` (too small).\n    # This is not ideal because we already filter `mismatch < 0`.\n\n    # Let's try: `score = 1 / (1 + exp(-k * (mismatch)))`\n    # Mismatch = 0 -> score = 0.5\n    # Mismatch > 0 -> score < 0.5 (worse fit)\n    # Mismatch < 0 -> score > 0.5 (better fit)\n\n    # This still seems to penalize small positive mismatches.\n    # We need a function that peaks at mismatch = 0.\n\n    # Consider a Gaussian-like shape using exp(-x^2).\n    # `score = exp(-k * mismatch**2)`\n    # Mismatch = 0 -> score = 1\n    # Mismatch != 0 -> score < 1. This is a good candidate.\n    # This can be approximated with a sigmoid.\n\n    # Let's re-think the sigmoid target.\n    # We want `bin_remain_cap` to be just enough.\n    # If we focus on `bin_remain_cap`, we want it to be close to `item`.\n    # `score = sigmoid(k * (item - bin_remain_cap))`\n    # If `bin_remain_cap` is slightly larger than `item`, `item - bin_remain_cap` is small and negative.\n    # `sigmoid(small_negative_val)` is close to 0.\n    # If `bin_remain_cap` is much larger than `item`, `item - bin_remain_cap` is a large negative.\n    # `sigmoid(large_negative_val)` is close to 0.\n    # If `bin_remain_cap` is exactly `item`, `item - bin_remain_cap` is 0.\n    # `sigmoid(0)` is 0.5.\n\n    # This function favors bins with `item - bin_remain_cap` being small negative,\n    # which means `bin_remain_cap` is slightly larger than `item`.\n\n    # Let's define k as a parameter controlling sensitivity. A higher k means\n    # a sharper peak around the ideal fit.\n    k = 5.0  # Sensitivity parameter, can be tuned.\n\n    # Calculate the sigmoid score for available bins.\n    # We want `bin_remain_cap` to be as close to `item` as possible, but >= `item`.\n    # This means we want `bin_remain_cap - item` to be small and non-negative.\n    # Let's transform this difference to get a score.\n\n    # A score that peaks when `bin_remain_cap - item` is 0.\n    # `score = 1 / (1 + exp(-k * (max_possible_item_size - (bin_remain_cap - item))))`\n    # This seems complicated.\n\n    # Let's use the mismatch and map it.\n    # We want to reward small positive mismatch values.\n    # We can use a sigmoid with a shift.\n    # `score = 1 / (1 + exp(-k * (mismatch_target - mismatch)))`\n    # If `mismatch_target` is 0, then `score = 1 / (1 + exp(-k * (-mismatch)))`.\n    # This would give high scores for negative mismatch values.\n\n    # Let's consider what `bins_remain_cap - item` represents:\n    # Small positive value: good fit\n    # Large positive value: too much space, potentially wasted\n    # Zero: perfect fit\n\n    # We can try to penalize large positive values.\n    # `score = 1 - sigmoid(k * (mismatch))`\n    # `score = 1 - 1 / (1 + exp(-k * mismatch))`\n    # `score = exp(-k * mismatch) / (1 + exp(-k * mismatch))`\n    # If mismatch is 0, score = 0.5.\n    # If mismatch is large positive, score -> 0.\n    # If mismatch is large negative, score -> 1.\n\n    # This gives high scores to bins that are too small, which is counter-intuitive.\n\n    # The common way to implement a \"best fit\" using sigmoid is to target\n    # the difference `bin_remain_cap - item` being close to 0.\n    # `sigmoid(k * (target_val - x))` where x is the value being measured.\n    # Here, we want to measure `bin_remain_cap`.\n    # `target_val` would be `item`.\n    # So, `score = 1 / (1 + exp(-k * (item - bin_remain_cap)))`\n\n    # Let's re-evaluate `score = 1 / (1 + exp(-k * (bin_remain_cap - item)))`.\n    # If `bin_remain_cap == item` (mismatch=0), score = 0.5.\n    # If `bin_remain_cap` > `item` (mismatch > 0), e.g., `bin_remain_cap = item + delta`\n    # `score = 1 / (1 + exp(-k * delta))`. If delta > 0, exp(-k*delta) < 1.\n    # So `1 + exp < 2`, and `score > 0.5`. The larger delta, the smaller exp(-k*delta), closer to 0. So score gets closer to 1.\n    # This penalizes larger remaining capacity, which is WRONG.\n\n    # Let's flip it: `score = 1 / (1 + exp(-k * (item - bin_remain_cap)))`\n    # If `bin_remain_cap == item` (mismatch=0), score = 0.5.\n    # If `bin_remain_cap` > `item` (mismatch > 0), e.g., `bin_remain_cap = item + delta`\n    # `score = 1 / (1 + exp(-k * (-delta))) = 1 / (1 + exp(k * delta))`.\n    # If delta > 0, k*delta > 0. exp(k*delta) > 1. `1 + exp > 2`.\n    # So `score < 0.5`. The larger delta, the smaller the score. This is CORRECT.\n    # If `bin_remain_cap` < `item` (mismatch < 0), this case is filtered.\n    # If we didn't filter, and `bin_remain_cap = item - delta'`, where delta' > 0.\n    # `score = 1 / (1 + exp(-k * (item - (item - delta')))) = 1 / (1 + exp(k * delta'))`.\n    # If delta' > 0, k*delta' > 0. exp(k*delta') > 1. `1 + exp > 2`. So `score < 0.5`.\n    # This would give low scores to bins that are too small. This is also fine,\n    # but we already use a mask for this.\n\n    # So, the expression `1 / (1 + exp(-k * (item - bin_remain_cap)))` for `bin_remain_cap >= item`\n    # appears to be a reasonable Sigmoid Fit Score. It assigns higher scores to bins\n    # where `bin_remain_cap` is closer to `item`.\n\n    # We want to make sure the range of arguments to sigmoid is reasonable.\n    # If `bin_remain_cap` is very large, `item - bin_remain_cap` can be a large negative number.\n    # `exp(large_positive)` can overflow.\n    # If `bin_remain_cap` is exactly `item`, argument is 0.\n    # If `bin_remain_cap` is slightly larger than `item`, argument is small negative.\n\n    # To prevent potential issues with very large capacities and the sigmoid argument:\n    # We can clip the `bin_remain_cap` for calculating the argument, or scale.\n    # Alternatively, we can use `1 / (1 + exp(k * (bin_remain_cap - item)))`\n    # Let's check this:\n    # `score = 1 / (1 + exp(k * (bin_remain_cap - item)))`\n    # If `bin_remain_cap == item` (mismatch=0), score = 1 / (1 + exp(0)) = 1 / (1 + 1) = 0.5.\n    # If `bin_remain_cap` > `item` (mismatch > 0), e.g., `bin_remain_cap = item + delta`.\n    # `score = 1 / (1 + exp(k * delta))`. If delta > 0, k*delta > 0. exp(k*delta) > 1.\n    # `1 + exp > 2`. So `score < 0.5`. The larger delta, the smaller the score. CORRECT.\n    # This expression seems more numerically stable for large positive differences.\n\n    # Let's use this expression.\n    # We'll compute it for the available bins.\n\n    # Parameters for sigmoid: k (steepness)\n    k = 5.0 # Sensitivity. Higher k means score drops faster as capacity increases beyond item.\n\n    # Calculate the argument for the sigmoid: k * (remaining_capacity - item)\n    # We are interested in `bins_remain_cap >= item`.\n    # `argument = k * (available_bins_cap - item)`\n\n    # The sigmoid function: `sigmoid(x) = 1 / (1 + exp(-x))`\n    # Our chosen formula is `1 / (1 + exp(k * (bin_remain_cap - item)))`.\n    # Let `x = k * (bin_remain_cap - item)`. Then `sigmoid_val = 1 / (1 + exp(x))`.\n    # This is also equivalent to `1 - sigmoid(-x) = 1 - 1 / (1 + exp(-(-x))) = 1 - 1 / (1 + exp(x))`.\n    # So, it's 1 minus a standard sigmoid applied to `k * (bin_remain_cap - item)`.\n    # This shape is suitable: peaks at 0.5, decreases for positive arguments.\n\n    # Applying the sigmoid to the mismatches for the available bins\n    # We want to assign these calculated priorities back to the original `priorities` array.\n    sigmoid_scores = 1 / (1 + np.exp(k * (available_bins_cap - item)))\n\n    # Place the calculated sigmoid scores back into the priorities array\n    priorities[available_bins_mask] = sigmoid_scores\n\n    return priorities",
    "response_id": 26,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response4.txt_stdout.txt",
    "code_path": "problem_iter3_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, temperature: float = 1.0) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a softmax-based heuristic.\n\n    This heuristic prioritizes bins that can accommodate the item, with a stronger\n    preference for \"tight fits\" (bins with remaining capacity close to the item size).\n    The `temperature` parameter controls the exploration vs. exploitation trade-off.\n    Higher temperatures lead to more uniform probabilities (more exploration), while\n    lower temperatures focus on the best-fitting bins (more exploitation).\n    Bins that are too small for the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n        temperature: Controls the sharpness of the softmax distribution.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score (probability) of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate a \"goodness\" score.\n    # We want to prioritize bins where the remaining capacity is close to the item size.\n    # A common approach is to use the difference (remaining_capacity - item).\n    # To prioritize smaller differences (tighter fits), we can use the negative of this difference.\n    # We add a small epsilon to avoid issues when remaining_capacity == item.\n    # A larger negative value (more negative) indicates a worse fit, a value closer to zero is a better fit.\n    # To make better fits have higher scores for softmax, we can invert this or use a different metric.\n    # Let's try prioritizing based on how much capacity is LEFT OVER after packing.\n    # So, (remaining_capacity - item) is what we want to minimize.\n    # For softmax, higher values mean higher probability. So, we want a metric that is\n    # higher for better fits. A good metric would be the negative of the leftover capacity,\n    # or a Gaussian-like function centered at 0 difference.\n    # Let's use negative difference, then scale it to make it more sensitive to near fits.\n    # A simple transformation that boosts near-fits and reduces others:\n    # Consider -(bins_remain_cap[can_fit_mask] - item) which is (item - bins_remain_cap[can_fit_mask]).\n    # This value is negative or zero. Higher values (closer to zero) are better fits.\n    # To make it suitable for softmax where higher is better, we can use `-(bins_remain_cap[can_fit_mask] - item)`.\n    # However, this might still be too sensitive to very small items.\n    # Let's consider a score that is high when `bins_remain_cap[can_fit_mask] - item` is small and positive.\n    # The inverse `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` from v1 is good.\n    # Let's refine this. We want to reward bins where `bins_remain_cap - item` is small.\n    # A Gaussian-like kernel centered at 0 difference could work: exp(- (diff^2) / (2 * sigma^2))\n    # Or, a simpler approach: consider `1 / (1 + diff)` where diff is `bins_remain_cap - item`.\n    # If diff is small and positive, score is close to 1. If diff is large, score approaches 0.\n\n    # Let's try a score that emphasizes small positive differences.\n    # We want a high score when `bins_remain_cap[can_fit_mask] - item` is small and positive.\n    # Consider `1.0 / (1.0 + (bins_remain_cap[can_fit_mask] - item))`\n    # This gives scores between (0, 1] for valid fits. 1 for perfect fits.\n\n    # Alternative: Prioritize bins with minimum remaining capacity that can fit the item.\n    # This is essentially the \"Best Fit\" strategy. For a heuristic priority, we can\n    # use the inverse of the remaining capacity for fitting bins.\n    # `priorities[can_fit_mask] = 1.0 / bins_remain_cap[can_fit_mask]` - This prioritizes smallest bins.\n    # If we want to prioritize tight fits, we are looking for bins where `bins_remain_cap - item` is small.\n    # So, we want to maximize `- (bins_remain_cap[can_fit_mask] - item)`.\n    # Or, a score that is high for small positive `bins_remain_cap[can_fit_mask] - item`.\n    # Let's use `-(bins_remain_cap[can_fit_mask] - item)` directly, then rescale or apply softmax.\n    # The intuition of `1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)` was good.\n    # Let's enhance it for \"nearness\".\n\n    # Consider the \"wasted space\" after packing: `wasted_space = bins_remain_cap - item`.\n    # We want to minimize `wasted_space`. So, a higher priority should be given to bins with smaller `wasted_space`.\n    # Let's transform `wasted_space` into a score where smaller `wasted_space` yields a higher score.\n    # A simple transformation: `score = 1.0 / (1.0 + wasted_space)`.\n    # This gives scores in the range (0, 1]. Perfect fit -> score 1. Large wasted space -> score close to 0.\n    # This should provide a good signal for \"tight fits\".\n\n    wasted_space = bins_remain_cap[can_fit_mask] - item\n    # Using `1.0 / (1.0 + wasted_space)` maps small positive wasted space to values close to 1.\n    # For a perfect fit (wasted_space = 0), score is 1.\n    # For larger wasted_space, score decreases.\n    scores_for_softmax = 1.0 / (1.0 + wasted_space)\n\n    # Apply softmax to get probabilities.\n    # Ensure temperature is positive to avoid division by zero or invalid operations.\n    if temperature <= 0:\n        raise ValueError(\"Temperature must be positive.\")\n\n    # Calculate exponentiated scores, scaled by temperature\n    # Lower temperature means sharper distribution, higher temperature means flatter.\n    exp_scores = np.exp(scores_for_softmax / temperature)\n\n    # Normalize to get probabilities\n    probabilities = exp_scores / np.sum(exp_scores)\n\n    # Assign probabilities to the original priorities array\n    priorities[can_fit_mask] = probabilities\n\n    # Normalize priorities to sum to 1, ensuring valid probability distribution\n    # This is already handled by the softmax if there's at least one bin that can fit.\n    # If no bins can fit, priorities remains all zeros, which is correct.\n    if np.sum(priorities) > 0:\n        priorities /= np.sum(priorities)\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.\n\n    The strategy assigns higher priority to bins that have a remaining capacity\n    just slightly larger than the item size, aiming to fill bins more compactly.\n    A temperature parameter controls the \"softness\" of the softmax, influencing\n    how aggressively we favor these \"almost fitting\" bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Filter out bins that cannot fit the item\n    can_fit_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if not eligible_bins_cap.size:\n        # If no bin can fit the item, return zeros for all original bins\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit score\": how much space is left after placing the item.\n    # Smaller values indicate a tighter fit.\n    fit_scores = eligible_bins_cap - item\n\n    # Use a Softmax-like approach to convert fit scores to priorities.\n    # We invert the fit scores to give higher priority to smaller remaining capacities (tighter fits).\n    # Adding a small epsilon to avoid division by zero or log(0) if all fit_scores are 0.\n    epsilon = 1e-9\n    inverted_fit_scores = 1.0 / (fit_scores + epsilon)\n\n    # The temperature parameter controls the \"softness\" of the softmax.\n    # A lower temperature makes the distribution sharper (more peaky),\n    # favoring the best fitting bins more strongly.\n    # A higher temperature makes it more uniform.\n    temperature = 0.5  # This can be tuned as a hyperparameter\n\n    # Apply softmax to the inverted fit scores\n    try:\n        exp_scores = np.exp(inverted_fit_scores / temperature)\n        softmax_priorities = exp_scores / np.sum(exp_scores)\n    except OverflowError:\n        # Handle potential overflow if scores become too large\n        # In such cases, a simple proportional scaling might be better\n        # or clamping the input to exp. For simplicity here, we can\n        # assign equal high probability to all if overflow occurs.\n        softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size\n\n\n    # Create the final priority array, mapping priorities back to original bin indices\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[can_fit_mask] = softmax_priorities\n\n    return priorities",
    "response_id": 19,
    "obj": 4.048663741523748,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response10.txt_stdout.txt",
    "code_path": "problem_iter1_code10.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit Decreasing-like heuristic.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # First Fit strategy: find the first bin that can accommodate the item.\n    # For priority, we want to favor bins that are a \"tight fit\" but still fit.\n    # This means we prefer bins where the remaining capacity is just enough for the item.\n    # A bin with remaining capacity exactly equal to the item size is ideal.\n    # Bins that are too small should have a priority of 0.\n\n    # Create a mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities to a very low value (effectively zero for unusable bins)\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # For bins that can fit the item, calculate a priority score.\n    # We want to prioritize bins where the remaining capacity is closest to the item size,\n    # without going below the item size.\n    # The difference (bins_remain_cap - item) represents the \"slack\".\n    # We want to minimize slack, so a smaller difference is better.\n    # However, we want the *first* such bin in the array to be prioritized in case of ties,\n    # which is naturally handled by numpy's vectorized operations if we consider\n    # negative of the slack as a priority. A smaller slack means a larger negative slack,\n    # which translates to a higher priority in a max-priority queue sense.\n\n    # Calculate slack for bins that can fit the item\n    slack = bins_remain_cap[can_fit_mask] - item\n\n    # The priority is the negative of the slack.\n    # Smaller slack -> larger negative slack -> higher priority.\n    # If multiple bins have the same slack, their relative order in the original\n    # bins_remain_cap array will be preserved in terms of priority calculation.\n    priorities[can_fit_mask] = -slack\n\n    # In a true First Fit, we'd just take the first bin that fits.\n    # To simulate this \"first fit\" behavior in a priority context,\n    # we can add a small bonus to earlier bins with good fits, or more directly,\n    # simply return priorities such that the first available bin with the \"best\" fit\n    # (smallest slack) gets the highest priority. The `-slack` already achieves this.\n    # If multiple bins have the same minimal slack, the one appearing first in the\n    # `bins_remain_cap` array will naturally get the higher priority due to the way\n    # numpy operations often preserve order in selection when values are equal.\n    # For absolute certainty of 'first fit' logic within this priority framework,\n    # we can make bins that are \"exact fits\" have a slightly higher priority\n    # than slightly looser fits.\n\n    # Refined priority: Exact fits (slack=0) get highest priority.\n    # Then, among bins that fit, prioritize smaller slack (tighter fit).\n    # The current -slack already prioritizes smaller slack.\n    # To enforce the \"first fit\" aspect: consider the index.\n    # A bin with smaller index is preferred if slack is equal.\n\n    # Let's use a more explicit priority:\n    # 1. Highest priority for exact fits.\n    # 2. Then, prioritize bins with smaller slack.\n    # 3. If slack is equal, prioritize the bin with the smaller index.\n\n    # Initialize priorities for fitting bins\n    fit_priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Calculate slack for fitting bins\n    slack_values = bins_remain_cap[can_fit_mask] - item\n\n    # Assign priorities:\n    # For exact fits (slack_values == 0), assign a very high priority (e.g., 1e9)\n    # For other fits, assign priority based on negative slack.\n    # To break ties and enforce \"first fit\", we can penalize later bins.\n    # Let's assign priority = 10000 - slack - (index * 0.1) for fitting bins.\n    # This way, smaller slack is better, and smaller index is better for same slack.\n\n    indices = np.where(can_fit_mask)[0]\n    # Create a score: (ideal_fit - slack) + (bonus_for_early_bins)\n    # We want to maximize this score.\n    # Ideal fit = 0 (when remaining_capacity == item)\n    # So, priority component from slack is -slack.\n    # Bonus for early bins: -index * small_constant.\n    # We want to maximize priority.\n    # Maximize: (-slack) - (index * 0.01)\n    \n    # A simpler way is to use a very large number for exact fits, then -slack.\n    # Let's consider the \"best\" fit for the priority.\n    # The best fit is the one that minimizes `remaining_capacity - item`.\n    # This is equivalent to maximizing `-(remaining_capacity - item)`.\n    # So, `priority = -(remaining_capacity - item)` for fitting bins.\n    # To ensure first-fit, if there are multiple bins with the same minimal slack,\n    # the one with the lower index should be preferred.\n    # We can achieve this by adding a very small penalty to the priority based on index.\n    # `priority = -(remaining_capacity - item) - index * epsilon`\n    # where epsilon is a very small positive number. This ensures that a bin at a\n    # lower index with the same slack gets a slightly higher priority.\n\n    epsilon = 1e-6  # Small value to break ties for first-fit\n    fit_priorities[can_fit_mask] = -(slack_values) - (indices * epsilon)\n    \n    # We want the bin with the highest priority score to be selected.\n    # The current calculation `-(slack_values) - (indices * epsilon)` will work.\n    # A more direct \"First Fit Decreasing-like\" priority would be to assign\n    # priorities such that the smallest slack is maximized.\n    # And for ties in slack, the lowest index is maximized.\n    # So, `priority = (some_large_number - slack) - index * epsilon`.\n    # Or simply, `priority = -slack - index * epsilon`.\n    # A higher value means higher priority.\n\n    # Final check: The priority should reflect our preference.\n    # We prefer bins that are a tight fit, and among tight fits, the earliest one.\n    # This means a bin where `bins_remain_cap - item` is small is good.\n    # And smaller index is good for ties.\n    # So, the score should be high for small `bins_remain_cap - item` and small `index`.\n    # Let's use a score that is large for best fits:\n    # Priority = MAX_SCORE - (bins_remain_cap - item) - (index * penalty)\n    # A large MAX_SCORE ensures any fitting bin is better than non-fitting.\n    \n    MAX_BENEFIT_SCORE = 1000000  # A large number to indicate a good fit\n    INDEX_PENALTY_FACTOR = 1000  # Penalty for later bins\n\n    # Prioritize bins that can fit the item.\n    # For bins that fit, the score is determined by how \"tight\" the fit is\n    # (smaller remaining capacity after placement is better) and by their index\n    # (earlier bins are preferred for first-fit).\n\n    # Calculate the \"tightness\" score: a larger value means a tighter fit (less waste).\n    # We want to maximize `MAX_BENEFIT_SCORE - slack`.\n    tightness_score = np.zeros_like(bins_remain_cap)\n    tightness_score[can_fit_mask] = MAX_BENEFIT_SCORE - slack\n\n    # Introduce a penalty for bins with higher indices to enforce the \"first fit\" logic.\n    # Subtract a scaled index. Smaller index should have higher priority.\n    index_penalty = indices * INDEX_PENALTY_FACTOR\n    \n    # Combine scores. We want to maximize the overall priority.\n    # Prioritize based on tightness, then index.\n    # This means higher tightness_score is better.\n    # Lower index_penalty is better.\n    # So, we want to MAXIMIZE: `tightness_score - index_penalty`\n    \n    priorities = np.full_like(bins_remain_cap, -np.inf) # Initialize with low priority\n\n    if np.any(can_fit_mask):\n        fitting_indices = np.where(can_fit_mask)[0]\n        fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        \n        # Calculate the \"fit quality\": how close the remaining capacity is to the item size.\n        # A smaller difference (bins_remain_cap - item) is better.\n        fit_quality = -(fitting_bins_remain_cap - item) # Maximize this (smaller difference is better)\n\n        # To implement \"First Fit\", we prefer earlier bins when fit quality is the same.\n        # We can achieve this by adding a small bonus for earlier indices.\n        # This means we want to maximize `fit_quality + bonus_for_early_bins`.\n        # The bonus should decrease with index. So, `bonus = constant - index * small_factor`.\n        # Let's use a simple approach: a very high priority for the tightest fits,\n        # and then penalize for looser fits and later indices.\n\n        # Consider the inverse: we want to minimize waste (slack) and index.\n        # So, we want to minimize `slack + index * epsilon`.\n        # Priority should be inverse of this minimization.\n        # Priority = - (slack + index * epsilon)\n\n        epsilon_tiebreaker = 1e-6\n        priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item) - (np.arange(len(bins_remain_cap))[can_fit_mask] * epsilon_tiebreaker)\n\n    # The current priority definition maximizes (tight fit) + (early index).\n    # This correctly aligns with First Fit logic.\n    return priorities",
    "response_id": 10,
    "obj": 4.048663741523748,
    "SLOC": 23.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response14.txt_stdout.txt",
    "code_path": "problem_iter1_code14.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First strategy.\n\n    The Exact Fit First strategy prioritizes bins where the remaining capacity is exactly equal\n    to the item size. Bins that can fit the item but not exactly are given a lower priority,\n    with larger remaining capacities being less preferred. Bins that cannot fit the item\n    receive zero priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Prioritize bins where remaining capacity is exactly the item size\n    exact_fit_mask = (bins_remain_cap == item) & can_fit_mask\n    priorities[exact_fit_mask] = 1.0\n\n    # For bins that can fit but not exactly, give a lower priority\n    # We want to penalize bins with a lot of leftover space after placing the item.\n    # So, we assign a priority that decreases as (remaining_capacity - item_size) increases.\n    # A simple way is to use 1 / (remaining_capacity - item_size + 1) to avoid division by zero\n    # and ensure non-zero priorities for valid fits.\n    partial_fit_mask = (~exact_fit_mask) & can_fit_mask\n    remaining_space_after_fit = bins_remain_cap[partial_fit_mask] - item\n    priorities[partial_fit_mask] = 1.0 / (remaining_space_after_fit + 1) # Adding 1 to avoid division by zero if remaining_space is 0, which is handled by exact_fit_mask anyway.\n\n    # Ensure that exact fits have higher priority than partial fits.\n    # Since we set exact fits to 1.0, and partial fits to values < 1.0 (as remaining_space_after_fit >= 0),\n    # this condition is naturally met.\n\n    return priorities",
    "response_id": 14,
    "obj": 4.198244914240141,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response29.txt_stdout.txt",
    "code_path": "problem_iter1_code29.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    This heuristic prioritizes bins that have a remaining capacity slightly larger than the item,\n    aiming to minimize wasted space. It uses a softmax function to convert these differences\n    into probabilities, effectively assigning higher priority to bins that are a \"good fit\".\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the \"fit\" of the item into each bin.\n    # We want bins where bins_remain_cap is just enough or slightly more than the item.\n    # A negative value here means the item doesn't fit. We can clamp these to a small\n    # positive value or 0 to avoid issues with softmax if all items don't fit.\n    # A large positive difference (item fits easily) is also not ideal as it wastes space.\n    # So we want the difference (bins_remain_cap - item) to be close to zero.\n    # We can use the negative of this difference as the exponent in softmax,\n    # so smaller (bins_remain_cap - item) results in a higher exponent.\n    fits = bins_remain_cap - item\n\n    # Filter out bins where the item does not fit (remaining capacity < item size)\n    # Assign a very low priority (or effectively zero) to these bins.\n    # A large negative number in softmax exponent will result in a value close to 0.\n    # We can also directly set their fits to a very low value before softmax.\n    fits[fits < 0] = -np.inf # Effectively 0 probability after softmax\n\n    # Apply the softmax function. The exponent in softmax should reflect desirability.\n    # We want bins with (bins_remain_cap - item) close to 0 to have high priority.\n    # So, we can use -(bins_remain_cap - item) as the exponent, or more intuitively,\n    # (item - bins_remain_cap) as the exponent, ensuring negative values become smaller.\n    # Or even better, prioritize bins with a positive difference close to zero.\n    # Let's define desirability as: higher is better if remaining_cap is slightly > item.\n    # Consider a transformation: exp(-abs(fits)). This would favor fits near 0.\n    # However, softmax typically takes logits (raw scores).\n    # Let's consider the score `s_i = -(bins_remain_cap_i - item)^2`. This penalizes\n    # being too small or too large. But we only care about `bins_remain_cap_i >= item`.\n    # So, if `bins_remain_cap_i < item`, priority should be 0.\n    # If `bins_remain_cap_i >= item`, we want `bins_remain_cap_i - item` to be small.\n    # A good transformation for softmax could be to map `bins_remain_cap_i - item` to a\n    # desirability score. Higher `bins_remain_cap_i` than `item` is good, but not too high.\n    # Let's use `bins_remain_cap_i - item` directly as logits, but only for bins that can fit.\n    # For bins that cannot fit, their logit should be extremely low.\n\n    # Option 1: Softmax on (bins_remain_cap - item) for fitting bins.\n    # Prioritize bins where remaining capacity is *exactly* the item size.\n    # Using -fits will invert the ordering, so smaller positive diffs become larger.\n    # exp(-fits) where fits = bins_remain_cap - item\n    # If bins_remain_cap = 5, item = 3, fit = 2. exp(-2) = 0.135\n    # If bins_remain_cap = 3, item = 3, fit = 0. exp(0) = 1.0\n    # If bins_remain_cap = 6, item = 3, fit = 3. exp(-3) = 0.049\n    # This prioritizes exact fits.\n\n    # We need to ensure that we don't assign probabilities to bins where the item doesn't fit.\n    # A common way to do this with softmax is to assign a very low logit (large negative number).\n    logits = bins_remain_cap - item\n    # For bins that cannot fit the item, set their logit to a very small number.\n    # This will make their softmax probability close to zero.\n    logits[logits < 0] = -1e9  # A large negative number\n\n    # For bins that can fit, we want to prioritize those with smaller remaining capacity (closer to item size).\n    # So, we want to exponentiate -(bins_remain_cap - item) which is (item - bins_remain_cap).\n    # However, the softmax input should ideally be positive values that represent scores.\n    # Let's redefine `scores`: a higher score means better fit.\n    # A perfect fit would have score X.\n    # A slightly larger capacity would have score X - epsilon.\n    # A much larger capacity would have score X - delta.\n    # A capacity smaller than item would have score -infinity.\n    # So, we can use `item - bins_remain_cap` as our underlying measure for exponentiation,\n    # but we need to handle the case where `bins_remain_cap < item`.\n\n    # Let's use a modified approach. We want bins that *can* fit, and among those,\n    # we prefer bins with less excess capacity.\n    # So, a \"goodness\" score could be:\n    # -infinity if item > bins_remain_cap\n    # -(bins_remain_cap - item) otherwise (prioritizing smaller differences)\n\n    scores = -(bins_remain_cap - item)\n    scores[bins_remain_cap < item] = -np.inf # Ensure items not fitting get no chance\n\n    # Now apply softmax. Softmax(x_i) = exp(x_i) / sum(exp(x_j)).\n    # The `scores` directly go into the exponent of softmax.\n    # Higher scores (closer to 0, or less negative) mean higher probability.\n    # If scores = [-inf, -inf, 0, -2, -5],\n    # exp(scores) = [0, 0, 1, exp(-2), exp(-5)]\n    # Softmax will normalize these.\n\n    # Add a small constant to the score for bins that can fit, to ensure\n    # that even a small positive difference doesn't get zero probability.\n    # For instance, if we use -(bins_remain_cap - item), a difference of 10\n    # gives exp(-10) which is tiny.\n    # Perhaps a linear scaling or a different transformation is better.\n    # Let's try mapping `bins_remain_cap - item` to a desirability score.\n    # If diff = 0, score = K (high)\n    # If diff = 1, score = K - epsilon\n    # If diff = 10, score = K - delta\n    # If diff < 0, score = -infinity\n\n    # Let's re-think the logit construction for softmax.\n    # We want the probability P_i proportional to exp(logit_i).\n    # Desirability of bin i for item: D_i\n    # P_i = exp(alpha * D_i) / sum(exp(alpha * D_j))\n    # If item doesn't fit: D_i = -infinity\n    # If item fits and diff = bins_remain_cap - item:\n    # We want smaller diffs to have higher D_i.\n    # So, D_i = -(bins_remain_cap - item) = item - bins_remain_cap.\n\n    # This approach correctly prioritizes bins with less remaining capacity over\n    # bins with more remaining capacity, for those that can fit the item.\n    # The `-np.inf` for non-fitting bins ensures they get 0 probability.\n    # The `alpha` parameter (implicitly 1 here) controls the \"peakiness\" of the distribution.\n\n    # Calculate the underlying scores for softmax.\n    # A higher score means more desirable.\n    # If bins_remain_cap >= item, we want bins_remain_cap - item to be small.\n    # So, let's use `-(bins_remain_cap - item)` which is `item - bins_remain_cap`.\n    # For bins where `bins_remain_cap < item`, the score should be very low.\n\n    scores = item - bins_remain_cap\n\n    # Set scores to a very low value for bins where the item does not fit.\n    # This ensures their probability contribution in softmax is negligible.\n    scores[bins_remain_cap < item] = -1e9 # Effectively -infinity for softmax\n\n    # Calculate probabilities using softmax.\n    # Adding 1 to scores for fitting bins can help avoid all zeros if all diffs are large negative\n    # but we've handled that with -1e9 for non-fitting bins.\n    # The softmax calculation itself: exp(scores) / sum(exp(scores))\n    # We can use np.exp directly, but be mindful of overflow/underflow if scores are very large/small.\n    # If scores = [-1e9, -1e9, 0, -2, -5], then exp(scores) = [0, 0, 1, exp(-2), exp(-5)]\n    # This seems correct.\n    exp_scores = np.exp(scores)\n\n    # If all items resulted in -1e9 (meaning item doesn't fit any bin), exp_scores will be all zeros.\n    # In this case, the sum will be zero, leading to division by zero. Handle this edge case.\n    sum_exp_scores = np.sum(exp_scores)\n    if sum_exp_scores == 0:\n        # This means the item cannot fit into any available bin.\n        # Return zero priorities for all bins.\n        return np.zeros_like(bins_remain_cap)\n\n    priorities = exp_scores / sum_exp_scores\n\n    return priorities",
    "response_id": 29,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response18.txt_stdout.txt",
    "code_path": "problem_iter1_code18.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    The Epsilon-Greedy strategy aims to balance exploration (trying less optimal bins)\n    and exploitation (choosing the best bin).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.2  # Probability of exploration\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins = np.where(suitable_bins_mask)[0]\n\n    if len(suitable_bins) == 0:\n        # If no bin can fit the item, return all zeros (or handle as an error)\n        return priorities\n\n    # --- Exploitation Component ---\n    # Calculate a \"goodness\" score for suitable bins.\n    # A common heuristic is the \"Best Fit\" approach: prioritize bins with\n    # the least remaining capacity after placing the item (minimizing waste).\n    # Here, we want the *highest* priority for the *best* fit, so we\n    # transform the remaining capacity difference into a positive score.\n    # A simple approach is (max_capacity - item) - remaining_capacity\n    # or more directly, prioritize smaller remaining capacities after fitting.\n    # We can use a value inversely related to remaining capacity, e.g., 1 / (remaining_capacity - item + 1e-6)\n    # to give higher priority to bins with less slack.\n\n    # Calculate the 'fit_score' for suitable bins: higher is better fit (less wasted space)\n    # This is 1 / (remaining_capacity - item + small_epsilon)\n    # A bin with remaining_capacity = item will have the highest score.\n    fit_scores = 1 / (bins_remain_cap[suitable_bins] - item + 1e-6)\n\n    # Normalize fit_scores to a 0-1 range (optional but can be helpful)\n    if fit_scores.max() > fit_scores.min():\n        exploitation_priorities = (fit_scores - fit_scores.min()) / (fit_scores.max() - fit_scores.min())\n    else:\n        exploitation_priorities = np.ones(len(suitable_bins)) # All are equally \"good\"\n\n    # Assign exploitation priorities to the suitable bins\n    priorities[suitable_bins] = exploitation_priorities\n\n    # --- Exploration Component ---\n    # Introduce randomness: with probability epsilon, choose a random suitable bin.\n    # Assign a small, uniform \"exploration\" priority to all suitable bins.\n    # This encourages trying out bins that might not be the immediate \"best fit\".\n    exploration_priority_value = 0.1 # A small constant value to represent exploration\n\n    # For the bins that are suitable, decide whether to explore\n    explore_mask = np.random.rand(len(suitable_bins)) < epsilon\n\n    # Update priorities for bins chosen for exploration\n    priorities[suitable_bins[explore_mask]] = exploration_priority_value\n\n    # Normalize the final priorities to ensure a meaningful range, e.g., 0 to 1\n    # (This step might be adjusted based on how the priority scores are used elsewhere)\n    if priorities.max() > priorities.min():\n        final_priorities = (priorities - priorities.min()) / (priorities.max() - priorities.min())\n    else:\n        final_priorities = np.ones(num_bins) * 0.5 # Default if all are same\n\n    # Ensure that bins that cannot fit the item have zero priority\n    final_priorities[~suitable_bins_mask] = 0\n\n    return final_priorities",
    "response_id": 18,
    "obj": 4.15835660151576,
    "SLOC": 23.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response5.txt_stdout.txt",
    "code_path": "problem_iter2_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    This heuristic prioritizes bins that can fit the item and are \"nearly full\"\n    (i.e., have little remaining capacity after fitting the item), as this\n    encourages tighter packing. A small random exploration component is added\n    to avoid getting stuck in local optima. The exploration rate (epsilon) is\n    reduced to favor greedy choices more. Scores are normalized to a [0, 1] range\n    for a more stable epsilon-greedy behavior.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.05  # Reduced probability of random exploration for more greedy behavior\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate scores for bins that can fit the item\n    # Prioritize bins that leave little remaining capacity after fitting the item.\n    # Score is the inverse of the remaining capacity after fitting.\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Add a small constant to the denominator to prevent division by zero and overly large scores\n    # for bins that perfectly fit the item.\n    scores = 1.0 / (remaining_capacity_after_fit + 1e-6) \n    \n    # Normalize scores to be between 0 and 1.\n    # This makes the epsilon-greedy selection more balanced.\n    if scores.size > 0:\n        min_score = np.min(scores)\n        max_score = np.max(scores)\n        if max_score > min_score:\n            priorities[can_fit_mask] = (scores - min_score) / (max_score - min_score)\n        else:\n            # If all scores are the same (e.g., all bins have the same remaining capacity after fit),\n            # assign a neutral priority, or simply a value representing the best fit.\n            # Assigning 0.5 could be seen as neutral, but since they are all equally good,\n            # a higher uniform value (like 1.0) might be more indicative of a good fit.\n            # Let's stick to a normalized 1.0 for \"equally best\" fit.\n            priorities[can_fit_mask] = 1.0\n    \n    # Epsilon-Greedy exploration: with probability epsilon, choose a random bin that fits.\n    # This allows for exploration of less optimal (but still valid) bins.\n    if np.random.rand() < epsilon:\n        possible_bins_indices = np.where(can_fit_mask)[0]\n        if possible_bins_indices.size > 0:\n            # Select a random bin among those that can fit the item.\n            random_bin_index = np.random.choice(possible_bins_indices)\n            \n            # Reset priorities and assign the highest priority to the randomly chosen bin.\n            # This ensures that the exploration step effectively picks a random bin.\n            priorities = np.zeros_like(bins_remain_cap, dtype=float)\n            priorities[random_bin_index] = 1.0  # Highest priority\n\n    return priorities",
    "response_id": 5,
    "obj": 4.068607897885915,
    "SLOC": 20.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-based approach\n    that also incorporates an epsilon-greedy exploration strategy.\n\n    This heuristic first calculates a \"fit score\" for each bin, where a better fit (less remaining capacity)\n    results in a higher score. These scores are then transformed using softmax to get probabilities.\n    An epsilon-greedy component is added: with a small probability `epsilon`, a bin is chosen randomly\n    from the *suitable* bins to encourage exploration. Otherwise, the greedy (softmax-derived)\n    priorities are used.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploring a random suitable bin\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    n_bins = len(bins_remain_cap)\n    priorities = np.zeros(n_bins)\n\n    if len(suitable_bin_indices) == 0:\n        # No bin can fit the item, return all zeros\n        return priorities\n\n    # Epsilon-greedy choice: With probability epsilon, pick a random suitable bin\n    if np.random.rand() < epsilon:\n        # Choose one suitable bin randomly\n        chosen_index = np.random.choice(suitable_bin_indices)\n        # Assign a high priority to this randomly chosen bin\n        priorities[chosen_index] = 1.0\n        # For other bins, assign a very low priority to make the chosen one stand out\n        priorities[suitable_bin_indices] = 1e-9\n        # Ensure non-suitable bins remain at 0\n        return priorities\n\n    # If not exploring, use a greedy strategy based on how well the item fits.\n    # We want bins where the remaining capacity is just enough for the item.\n    # The score should be higher for bins with `bins_remain_cap - item` closer to 0.\n    # Use `-(bins_remain_cap - item)` as the score, which is `item - bins_remain_cap`.\n    # This makes smaller positive differences (better fits) have higher scores.\n\n    scores = item - bins_remain_cap\n\n    # For bins that cannot fit the item, assign a very low score.\n    # This ensures their softmax probability is negligible.\n    scores[~suitable_bins_mask] = -np.inf  # Effectively -infinity for softmax\n\n    # Calculate softmax probabilities\n    # exp_scores = np.exp(scores)\n    # To prevent potential overflow with large positive scores or underflow with large negative scores,\n    # we can use `np.exp(scores - np.max(scores))` if scores are not all -inf.\n    # Since we set non-suitable bins to -inf, and suitable bins have scores >= 0,\n    # np.max(scores) will be at least 0 for suitable bins.\n    \n    # Calculate logits, ensuring they are not all -inf\n    max_score = np.max(scores[suitable_bins_mask]) if np.any(suitable_bins_mask) else 0\n    \n    # Shift scores so that the maximum is 0 to avoid large exponents\n    shifted_scores = scores - max_score\n    \n    # Ensure exp_scores are non-negative and sum is positive before division\n    exp_scores = np.exp(shifted_scores)\n    \n    sum_exp_scores = np.sum(exp_scores)\n    \n    if sum_exp_scores == 0:\n        # This case should ideally not be reached if there's at least one suitable bin\n        # but as a safeguard, return uniform probabilities for suitable bins.\n        # This would mean assigning equal probability to all bins that can fit.\n        if np.any(suitable_bins_mask):\n            priorities[suitable_bins_mask] = 1.0 / len(suitable_bin_indices)\n        return priorities\n    \n    priorities = exp_scores / sum_exp_scores\n\n    return priorities",
    "response_id": 0,
    "obj": 4.108496210610296,
    "SLOC": 25.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a blended strategy.\n\n    This strategy combines the preference for exact fits with the soft preference\n    for near-exact fits using a Softmax-like approach. It prioritizes bins\n    where the remaining capacity is exactly the item size, and then assigns\n    decreasing priority to bins with slightly larger remaining capacities.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if not eligible_bins_cap.size:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit score\": how much space is left after placing the item.\n    # Smaller values indicate a tighter fit.\n    fit_scores = eligible_bins_cap - item\n\n    # Assign a base priority of 1.0 to exact fits and a lower base priority to partial fits.\n    # This ensures exact fits are always preferred over partial fits before softmax.\n    base_priorities = np.where(fit_scores == 0, 1.0, 0.5)\n\n    # To incorporate the \"near-exact\" preference smoothly, we can modify the\n    # base priorities based on how close the fit is, using a logistic or exponential decay.\n    # Here, we use a simple exponential decay for remaining space.\n    # Smaller remaining space (after fitting) should get higher priority.\n    # We add 1 to fit_scores to handle the case of exact fit (fit_score=0) gracefully\n    # and ensure positive values for exponentiation.\n    # The scaling factor (e.g., 1.0) and the decay rate (e.g., 0.1) can be tuned.\n    decay_rate = 0.1\n    near_fit_scores = np.exp(-decay_rate * fit_scores)\n\n    # Blend the base priority with the near-fit score.\n    # Exact fits should retain their high base priority, while near-fits get a boost.\n    # We prioritize exact fits (score 1.0) and then near-exact fits.\n    # A simple way to blend is to amplify the scores of exact fits and give\n    # a scaled score to near-fits.\n    combined_scores = np.where(fit_scores == 0, 1.0, near_fit_scores)\n\n    # Apply a Softmax-like transformation to normalize and create probabilities.\n    # This smooths the preferences, giving higher probability to better fits.\n    # The temperature parameter controls the \"softness\".\n    temperature = 0.5\n    try:\n        # Add a small epsilon to prevent log(0) or division by zero issues\n        epsilon = 1e-9\n        scaled_scores = combined_scores / temperature\n        exp_scores = np.exp(scaled_scores)\n        softmax_priorities = exp_scores / np.sum(exp_scores)\n    except OverflowError:\n        # Fallback for overflow: assign uniform probability if scores are too extreme\n        softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size\n\n\n    # Map the calculated priorities back to the original bin indices\n    priorities[can_fit_mask] = softmax_priorities\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "SLOC": 21.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response7.txt_stdout.txt",
    "code_path": "problem_iter2_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-Based Best Fit strategy.\n\n    This strategy prioritizes bins that have a remaining capacity closest to the item size,\n    encouraging tighter packing. It uses a softmax function to convert the \"closeness\"\n    score into a probability-like priority, controlled by a temperature parameter.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if len(suitable_bins_indices) == 0:\n        # If no bin can fit the item, return all zeros\n        return priorities\n\n    # Calculate the \"closeness\" to a perfect fit.\n    # We want to minimize the remaining capacity after placing the item.\n    # A smaller difference (bins_remain_cap - item) is a better fit.\n    # To use softmax effectively, we convert this to a score where higher is better.\n    # We can use the negative of the difference, or a transformation that maps\n    # smaller differences to larger positive scores.\n    # A simple approach is to consider the inverse of the remaining capacity after fit.\n    # To prioritize tighter fits, we want bins where (bins_remain_cap - item) is small.\n    # Let's calculate the slack: slack = bins_remain_cap[suitable_bins_indices] - item\n    # We want to give higher priority to smaller slacks.\n    slack_values = bins_remain_cap[suitable_bins_indices] - item\n\n    # Transform slack values into scores where smaller slack means higher score.\n    # We can use 1 / (slack + epsilon) or exp(-slack / temperature) etc.\n    # Using exp(-slack / temperature) directly relates to Softmax and is more robust.\n    # A lower temperature will favor bins with very little slack more strongly.\n    temperature = 0.2  # Hyperparameter: controls the \"sharpness\" of the preference for tight fits\n    epsilon = 1e-9     # Small value to avoid division by zero or log(0) if using other transformations\n\n    # Calculate scores for softmax. Higher scores for tighter fits.\n    # Using exp(-slack / temperature) makes smaller slack values result in larger scores.\n    # We add epsilon to slack_values before division to prevent potential issues if slack is exactly 0.\n    # Alternatively, directly use exp(-slack / temperature).\n    # Let's use exp(-slack / temperature) for a more direct softmax interpretation.\n    try:\n        softmax_input_scores = -slack_values / temperature\n        exp_scores = np.exp(softmax_input_scores)\n        # Normalize scores to get probabilities (priorities)\n        probabilities = exp_scores / np.sum(exp_scores)\n    except OverflowError:\n        # If scores are too large, it means some bins are extremely good fits.\n        # In such cases, a simpler approach might be to give a high score to the best fit(s).\n        # For now, we can distribute probabilities more evenly if overflow occurs,\n        # or assign a very high probability to the bin with minimum slack.\n        min_slack_idx = np.argmin(slack_values)\n        probabilities = np.zeros_like(slack_values)\n        probabilities[min_slack_idx] = 1.0\n\n    # Assign these calculated priorities back to the original bins\n    priorities[suitable_bins_indices] = probabilities\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "SLOC": 20.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    The strategy favors bins that are a \"good fit\" for the item (i.e., leaving\n    a small remaining capacity), but with a probability epsilon, it assigns a\n    consistent exploration score to encourage trying less optimal bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.2  # Probability of exploration\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if len(suitable_bins_indices) == 0:\n        return priorities  # No bin can fit the item\n\n    # --- Exploitation Component (Good Fit) ---\n    # Calculate a score based on how well the item fits.\n    # We want to prioritize bins that leave minimal remaining capacity.\n    # Score = 1 / (remaining_capacity - item + epsilon)\n    # A smaller difference means a higher score.\n    fit_scores = 1.0 / (bins_remain_cap[suitable_bins_indices] - item + 1e-6)\n\n    # Normalize fit_scores to a 0-1 range to represent the \"exploitation\" priority.\n    # Higher score means better fit.\n    if fit_scores.max() > fit_scores.min():\n        exploitation_priorities = (fit_scores - fit_scores.min()) / (fit_scores.max() - fit_scores.min())\n    else:\n        exploitation_priorities = np.ones(len(suitable_bins_indices)) # All fits are equally good\n\n    # Assign the exploitation priorities to the suitable bins\n    priorities[suitable_bins_indices] = exploitation_priorities\n\n    # --- Exploration Component ---\n    # With probability epsilon, overwrite the exploitation priority with a\n    # consistent, lower exploration score for a random subset of suitable bins.\n    # This encourages exploration of bins that might not be the immediate best fit.\n    exploration_score = 0.1 # A fixed low score for exploration\n\n    # Determine which suitable bins will be subject to exploration\n    num_suitable = len(suitable_bins_indices)\n    explore_indices_in_suitable = np.random.choice(\n        num_suitable,\n        size=int(np.ceil(epsilon * num_suitable)),\n        replace=False\n    )\n    \n    # Get the actual indices in the original bins_remain_cap array\n    bins_to_explore_indices = suitable_bins_indices[explore_indices_in_suitable]\n\n    # Assign the exploration score to these bins\n    priorities[bins_to_explore_indices] = exploration_score\n\n    # Ensure that bins that cannot fit the item have zero priority\n    priorities[~suitable_bins_mask] = 0\n\n    # Optional: Normalize final priorities if a specific range is required by downstream logic.\n    # For now, we return the scores as calculated, where higher means more preferred.\n    # A simple max-min normalization can be applied if needed:\n    # if priorities.max() > priorities.min():\n    #     final_priorities = (priorities - priorities.min()) / (priorities.max() - priorities.min())\n    # else:\n    #     final_priorities = np.ones(num_bins) * 0.5\n    # return final_priorities\n\n    return priorities",
    "response_id": 9,
    "obj": 4.178300757877951,
    "SLOC": 25.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response2.txt_stdout.txt",
    "code_path": "problem_iter3_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, prioritizing tight fits.\n\n    This heuristic prioritizes bins that can accommodate the item, favoring \"tight fits\"\n    (smaller remaining capacity after packing) to minimize wasted space.\n    A softmax function is used to convert these preferences into probabilities,\n    allowing for some exploration. The temperature parameter controls the\n    aggressiveness of the selection. Bins that are too small for the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    temperature = 1.0  # Tune this parameter: lower for more greedy, higher for more exploration\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate a score based on how tight the fit is.\n    # A smaller remaining capacity after packing means a tighter fit.\n    # We add a small epsilon to avoid division by zero if a bin has exactly the item's size.\n    tight_fit_scores = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)\n\n    # Apply softmax to convert scores into probabilities/priorities\n    # Ensure no division by zero if all scores are effectively zero (e.g., no bin fits)\n    if np.sum(tight_fit_scores) > 0:\n        # Softmax calculation: exp(score / temperature) / sum(exp(score / temperature))\n        # To avoid numerical instability with large scores, we can subtract the max score.\n        max_score = np.max(tight_fit_scores)\n        exp_scores = np.exp((tight_fit_scores - max_score) / temperature)\n        sum_exp_scores = np.sum(exp_scores)\n\n        if sum_exp_scores > 0:\n            priorities[can_fit_mask] = exp_scores / sum_exp_scores\n        else:\n            # Handle cases where sum of exp_scores might be zero due to extreme values or temperature\n            # In such rare cases, fall back to uniform distribution among fitting bins\n            num_fitting_bins = np.sum(can_fit_mask)\n            if num_fitting_bins > 0:\n                priorities[can_fit_mask] = 1.0 / num_fitting_bins\n    else:\n        # If no bins can fit, priorities remain zero.\n        pass\n\n    # Tie-breaking: Favor earlier bins (lower index) if priorities are very close.\n    # This can be implicitly handled by the order of processing or explicitly added\n    # by adding a small negative value based on index to the score before softmax,\n    # e.g., score - index * epsilon_tiebreaker. For simplicity here, we rely on\n    # the original order and potential floating point differences.\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  }
]