```python
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin.

    This heuristic prioritizes bins that can fit the item and are "nearly full"
    after the item is placed, encouraging tighter packing. It also incorporates a
    mechanism to slightly favor bins that have been used less (i.e., have more
    remaining capacity *before* adding the item) if they still offer a good fit,
    to promote better distribution and avoid premature bin exhaustion.
    The priority is calculated based on the remaining capacity after placing the item,
    inverted to make smaller remaining capacities yield higher priorities.
    A small additive term is used to break ties, favoring bins that had more
    initial capacity.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
        A higher score indicates a more desirable bin. Bins that cannot fit the item
        will have a priority of 0.
    """
    # Calculate remaining capacity after placing the item for all bins
    potential_remaining_caps = bins_remain_cap - item

    # Initialize priorities to a very low value (or 0) for bins that cannot fit
    # This ensures they are never chosen.
    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)

    # Identify bins that can fit the item
    can_fit_mask = potential_remaining_caps >= 0

    # For bins that can fit, calculate priority:
    # The core idea is to prioritize bins where the remaining capacity after placing
    # the item is minimized (best fit). We invert this to get a higher score for
    # a tighter fit.
    # We add a small penalty based on the *original* remaining capacity. This is
    # to slightly favor bins that had more capacity initially if they still provide
    # a good fit. This can help in distributing items better.
    # The inversion of `potential_remaining_caps` means smaller values are better.
    # Adding a small positive value for `bins_remain_cap` can slightly break ties.
    # `bins_remain_cap` is used here for its larger values to be slightly preferred
    # when the `potential_remaining_caps` are equal or very close.
    # A simple approach is to use the negative of `potential_remaining_caps`
    # and then add a small bonus for larger `bins_remain_cap`.

    # We want to maximize (smaller remaining capacity after fit) AND (larger initial capacity if fits are similar)
    # So, we can aim for a score like: -potential_remaining_caps + C * bins_remain_cap
    # Where C is a small positive constant to give weight to original capacity.
    # Let's normalize the original capacity to avoid it dominating too much.
    # A simpler approach without explicit normalization:
    # Prioritize by the inverse of (remaining capacity after fit + small epsilon)
    # and add a small bonus for larger original capacity.
    # Let's use the negative remaining capacity as primary driver and add the original capacity as a tie-breaker.

    if np.any(can_fit_mask):
        # Calculate the primary score: inverse of remaining capacity after fit.
        # Add a small epsilon to avoid division by zero and to give a slight preference
        # to bins that are not *perfectly* full (though this is subtle and may need tuning).
        # A more direct way for "best fit" is to use the negative of the remaining capacity.
        # Negative remaining capacity: lower value means better fit.
        # To get higher priority, we invert it: 1 / (residual + epsilon) or similar.
        # Let's stick to the negative remaining capacity and add original capacity.
        
        # Primary scoring: -potential_remaining_caps. Smaller (more negative) is better.
        # To convert to higher priority: use positive values.
        # We can use `max_residual - residual` or `1 / (residual + epsilon)`.
        # Using `-potential_remaining_caps` directly works if we interpret higher values as better.
        # Let's refine to make it clearer: High priority means a good fit.
        # Good fit = small `potential_remaining_caps`.
        # So, let's use `some_large_value - potential_remaining_caps`.
        # Example: Bin Capacity = 10. Item = 3. Remaining_cap = 7.
        # If item = 7, Remaining_cap = 3. This is a better fit.
        # Score for 7: large_val - 3. Score for 3: large_val - 7. Higher score for better fit.

        # Let's use a scaled inverse of the remaining capacity after fit.
        # `1 / (potential_remaining_caps + 1e-6)` would give higher score to smaller remaining.
        # To incorporate original capacity as a secondary criterion:
        # We want to maximize (-potential_remaining_caps) primarily, and (bins_remain_cap) secondarily.
        # A common way is to combine them linearly, e.g., `a * (-potential_remaining_caps) + b * bins_remain_cap`.
        # For simplicity and direct "best fit" interpretation:
        # Score = `(max_possible_residual - potential_remaining_caps)` + `0.01 * bins_remain_cap`
        # where `max_possible_residual` is the maximum possible remaining capacity for a fit.
        # Alternatively, using the negative remaining capacity is good for sorting.
        
        # Let's use `1.0 / (potential_remaining_caps + 1e-6)` for primary priority
        # and add a small term related to original capacity.
        
        # If we want to prioritize bins that are *closer* to fitting the item perfectly,
        # `potential_remaining_caps` should be as close to 0 as possible.
        # So, `1.0 / (potential_remaining_caps + epsilon)` works well for that.
        
        # Consider `potential_remaining_caps = [2, 0, 3]`
        # Scores: `1/2.000001`, `1/0.000001`, `1/3.000001` -> approx `0.5`, `1000000`, `0.33`
        # This clearly prioritizes the best fit.
        
        # Now, adding the secondary criterion: slight preference for bins with more original capacity.
        # Let's normalize `bins_remain_cap` to a small range, e.g., [0, 1] or [0, 0.1]
        # to avoid it overpowering the primary criterion.
        
        # Let's combine: priority = (1 / (potential_remaining_caps + epsilon)) + (bins_remain_cap / MAX_CAPACITY_OR_MEAN)
        # For simplicity, we can just add a scaled version of original capacity.
        
        # Final approach:
        # Primary: Maximize `-potential_remaining_caps`. (Best fit)
        # Secondary: Maximize `bins_remain_cap`. (Slight preference for larger original capacity bins)
        # Combine: `priorities[can_fit_mask] = -potential_remaining_caps[can_fit_mask] + 0.1 * bins_remain_cap[can_fit_mask]`
        # The `0.1` factor is heuristic. It means a difference of 1 in `potential_remaining_caps`
        # is equivalent to a difference of 10 in `bins_remain_cap`. This gives strong preference
        # to best fit.

        priorities[can_fit_mask] = -potential_remaining_caps[can_fit_mask] + 0.05 * bins_remain_cap[can_fit_mask]
        
    # Ensure bins that can't fit have a priority of -inf (or a very small number)
    # We already initialized to -inf, so this step is covered.
    # If we were using 0 for non-fitting bins, we would need to ensure it.

    # Handle the case where all potential priorities are -inf (no bin can fit)
    if not np.any(np.isfinite(priorities)):
        return np.zeros_like(bins_remain_cap) # Return all zeros if no bin can fit

    return priorities
```
