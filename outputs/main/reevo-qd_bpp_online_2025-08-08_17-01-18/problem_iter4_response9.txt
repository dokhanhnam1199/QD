```python
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a normalized Sigmoid Fit Score.

    This heuristic prioritizes bins that are a "good fit" for the item, meaning
    the remaining capacity is just slightly larger than the item size. It uses
    a sigmoid function to score bins where `bin_remain_cap - item` is close to zero.
    The score is normalized to a [0, 1] range.

    The function used is `1 / (1 + exp(k * (bin_remain_cap - item)))`.
    This function yields a score of 0.5 when `bin_remain_cap == item`.
    For `bin_remain_cap > item`, the score decreases as the remaining capacity increases,
    penalizing bins with excessive empty space.
    For `bin_remain_cap < item`, the score approaches 0, but these bins are filtered out.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin,
        normalized to the range [0, 1].
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Consider only bins that have enough capacity for the item
    available_bins_mask = bins_remain_cap >= item
    available_bins_cap = bins_remain_cap[available_bins_mask]

    if available_bins_cap.size == 0:
        return priorities  # No bins can accommodate the item

    # --- Sigmoid Fit Score Calculation ---
    # We want to prioritize bins where the remaining capacity is close to the item size.
    # The function `1 / (1 + exp(k * (remaining_capacity - item)))` achieves this:
    # - If remaining_capacity == item, score is 0.5.
    # - If remaining_capacity > item, score is < 0.5, decreasing as capacity increases.
    # - If remaining_capacity < item, score is > 0.5, increasing as capacity decreases (but these are filtered out).
    # `k` controls the steepness of the sigmoid. A higher `k` means a sharper preference for bins closer to the item size.
    k = 5.0  # Sensitivity parameter, can be tuned.

    # Calculate the sigmoid scores for the available bins
    # Use `np.clip` on the argument to `exp` to avoid potential overflow/underflow issues
    # with very large or small differences, though for typical bin packing scenarios,
    # this might not be strictly necessary. A large positive difference (oversized bin)
    # will result in a large positive argument to exp, making `exp(arg)` very large,
    # so `1 / (1 + large_num)` approaches 0. A small positive difference (tight fit)
    # will result in a small positive argument, making `exp(arg)` slightly larger than 1,
    # so `1 / (1 + slightly_larger)` is slightly less than 0.5.
    # The argument `k * (available_bins_cap - item)` can be large negative if `available_bins_cap` is only slightly larger than `item`
    # and `k` is small, or if `k` is large and `available_bins_cap` is much larger than `item`.
    # For `available_bins_cap >= item`, `available_bins_cap - item >= 0`.
    # So the argument `k * (available_bins_cap - item)` will always be non-negative for available bins.
    # This ensures `exp()` will not have a negative large argument.
    
    argument = k * (available_bins_cap - item)
    
    # To handle potentially very large positive arguments to exp which can lead to overflow
    # (though Python's float typically handles very large numbers, np.exp can return inf)
    # We can clip the argument to a reasonable upper bound.
    # For example, if argument > 700, exp(arg) is roughly 10^304, then 1/(1+exp) is near 0.
    # A practical limit might be around 10-20 for argument to avoid inf.
    # Let's set a clipping limit for the argument to exp.
    # The value `x` in `1/(1+exp(x))` where `x >= 0`.
    # If `x=10`, `exp(10)` ~ 22026, score ~ 1/22027 ~ 0.000045
    # If `x=20`, `exp(20)` ~ 4.8e8, score ~ 2e-9
    # So, even moderate values of `k * (capacity - item)` quickly drive the score to zero.
    # A clipping limit for `k * (capacity - item)` can be set, e.g., 20.
    clipped_argument = np.clip(argument, a_min=None, a_max=20.0)
    
    sigmoid_scores = 1 / (1 + np.exp(clipped_argument))

    # Assign the calculated scores to the corresponding bins
    priorities[available_bins_mask] = sigmoid_scores

    # The scores are already in the range [0, 0.5] for available bins.
    # To get a [0, 1] range, we can consider a transformation,
    # but the relative ordering is what matters for priority.
    # If the goal is a strict [0,1] range where 1 is best, we might need
    # to rescale. However, the current scores indicate relative preference.
    # The current maximum score is 0.5 (for perfect fits), and it decreases.
    # If we want the "best fit" to be 1, we can invert and scale:
    # `normalized_score = 1 - sigmoid_scores` maps the best fit (0.5) to 0.5,
    # and worse fits (near 0) to near 1. This is inverted.
    #
    # Let's stick to the interpretation that smaller `bin_remain_cap - item` is better.
    # The function `1 / (1 + exp(k * (capacity - item)))` outputs values in [0, 0.5].
    # To map this to a [0, 1] range where the best fit (smallest `capacity - item`)
    # gets the highest score, we can do `1 - (sigmoid_scores * 2)`.
    # If `sigmoid_scores = 0.5` (perfect fit), then `1 - (0.5 * 2) = 0`. This is not right.
    #
    # Let's use the definition `score = 1 / (1 + exp(-k * (target - value)))`
    # Here, `value` is `bin_remain_cap`, `target` is `item`.
    # `score = 1 / (1 + exp(-k * (item - bin_remain_cap)))`
    # For `bin_remain_cap >= item`:
    # If `bin_remain_cap == item`, `item - bin_remain_cap = 0`, `exp(0)=1`, `score = 1/(1+1) = 0.5`.
    # If `bin_remain_cap > item`, `item - bin_remain_cap < 0`. Let `item - bin_remain_cap = -delta` where `delta > 0`.
    # `score = 1 / (1 + exp(-k * -delta)) = 1 / (1 + exp(k * delta))`.
    # As `delta` increases, `k * delta` increases, `exp(k * delta)` increases, so `score` decreases towards 0.
    # This function correctly maps:
    # Perfect fit (mismatch 0) -> 0.5
    # Slight overshoot (small positive mismatch) -> score slightly less than 0.5
    # Large overshoot (large positive mismatch) -> score approaches 0.
    #
    # This means our previous implementation `1 / (1 + np.exp(k * (available_bins_cap - item)))`
    # actually does produce scores from 0 to 0.5, where 0.5 is the "best fit".
    #
    # To normalize this to [0, 1] where best fit is 1:
    # `best_fit_score = 0.5`
    # `worst_fit_score_for_available = min(sigmoid_scores)` (close to 0)
    #
    # A simple way to map [0, 0.5] to [0, 1] where 0.5 maps to 1 is `score * 2`.
    # So, `priorities[available_bins_mask] = sigmoid_scores * 2.0`
    # This maps the perfect fits (0.5) to 1.0, and bins with large remaining capacity (score near 0) to near 0.0.

    priorities[available_bins_mask] = sigmoid_scores * 2.0

    return priorities
```
