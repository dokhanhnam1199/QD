```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.

    The strategy assigns higher priority to bins that have a remaining capacity
    just slightly larger than the item size, aiming to fill bins more compactly.
    A temperature parameter controls the "softness" of the softmax, influencing
    how aggressively we favor these "almost fitting" bins.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Filter out bins that cannot fit the item
    can_fit_mask = bins_remain_cap >= item
    eligible_bins_cap = bins_remain_cap[can_fit_mask]

    if not eligible_bins_cap.size:
        # If no bin can fit the item, return zeros for all original bins
        return np.zeros_like(bins_remain_cap)

    # Calculate the "fit score": how much space is left after placing the item.
    # Smaller values indicate a tighter fit.
    fit_scores = eligible_bins_cap - item

    # Use a Softmax-like approach to convert fit scores to priorities.
    # We invert the fit scores to give higher priority to smaller remaining capacities (tighter fits).
    # Adding a small epsilon to avoid division by zero or log(0) if all fit_scores are 0.
    epsilon = 1e-9
    inverted_fit_scores = 1.0 / (fit_scores + epsilon)

    # The temperature parameter controls the "softness" of the softmax.
    # A lower temperature makes the distribution sharper (more peaky),
    # favoring the best fitting bins more strongly.
    # A higher temperature makes it more uniform.
    temperature = 0.5  # This can be tuned as a hyperparameter

    # Apply softmax to the inverted fit scores
    try:
        exp_scores = np.exp(inverted_fit_scores / temperature)
        softmax_priorities = exp_scores / np.sum(exp_scores)
    except OverflowError:
        # Handle potential overflow if scores become too large
        # In such cases, a simple proportional scaling might be better
        # or clamping the input to exp. For simplicity here, we can
        # assign equal high probability to all if overflow occurs.
        softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size


    # Create the final priority array, mapping priorities back to original bin indices
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    priorities[can_fit_mask] = softmax_priorities

    return priorities
```
