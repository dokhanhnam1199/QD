```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority scores for packing an item into bins using a refined
    priority function incorporating Softmax for exploration, bin scarcity,
    and tie-breaking.

    This heuristic aims to balance tight fits with exploration and efficiency
    by considering:
    1.  **Tight Fit Score (Sigmoid):** Prioritizes bins with remaining capacity
        closest to the item size, minimizing waste.
    2.  **Bin Scarcity:** Favors bins that have less remaining capacity overall,
        as these are "scarcer" resources. This is modeled using the inverse
        of remaining capacity.
    3.  **Softmax for Probabilistic Selection:** Uses Softmax to convert scores
        into probabilities, allowing for exploration of less optimal bins.
        The temperature parameter controls the degree of exploration.
    4.  **Tie-breaking:** Implicitly handled by the sorting order or original
        index if scores are identical, favoring bins that appear earlier in
        the array when scores are equal.

    The combined priority for a suitable bin is a weighted sum of the tight
    fit score and the bin scarcity score, transformed by Softmax.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.

    Returns:
        A NumPy array of the same size as `bins_remain_cap`, where each element
        is the probability score (from Softmax) for the corresponding bin.
        Bins that cannot fit the item will have a priority of 0.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    
    # Identify bins that can accommodate the item
    suitable_bins_mask = bins_remain_cap >= item
    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]
    suitable_bin_indices = np.where(suitable_bins_mask)[0]

    if suitable_bins_cap.size == 0:
        return priorities

    # --- Heuristic Component 1: Tight Fit Score (Sigmoid) ---
    # Parameter for the sigmoid function's steepness.
    # Higher k means sharper preference for tighter fits.
    k_fit = 5.0
    mismatch = suitable_bins_cap - item
    
    # Use a capped sigmoid for numerical stability
    max_exponent_arg = 35.0
    capped_exponent_arg = np.minimum(k_fit * mismatch, max_exponent_arg)
    tight_fit_scores = 1 / (1 + np.exp(capped_exponent_arg))

    # --- Heuristic Component 2: Bin Scarcity Score ---
    # Prioritize bins with less remaining capacity (scarcer bins).
    # Using 1 / (capacity + epsilon) to avoid division by zero and give higher score to smaller capacities.
    # Adding a small epsilon to avoid division by zero if a bin has 0 capacity (though unlikely if it fits the item).
    epsilon = 1e-6
    scarcity_scores = 1 / (suitable_bins_cap + epsilon)
    
    # Normalize scarcity scores to be comparable to tight_fit_scores (0 to 1 range)
    # A simple min-max scaling can work.
    min_scarcity = np.min(scarcity_scores)
    max_scarcity = np.max(scarcity_scores)
    if max_scarcity - min_scarcity > epsilon: # Avoid division by zero if all scarcity scores are the same
        normalized_scarcity_scores = (scarcity_scores - min_scarcity) / (max_scarcity - min_scarcity)
    else:
        normalized_scarcity_scores = np.zeros_like(scarcity_scores) # Or assign 0.5 if all are equal and non-zero

    # --- Combine Heuristics ---
    # Weighted sum of tight fit and scarcity. Weights can be tuned.
    # Here, we give equal weight, but this could be adjusted.
    weight_fit = 0.5
    weight_scarcity = 0.5
    
    combined_scores = (weight_fit * tight_fit_scores) + (weight_scarcity * normalized_scarcity_scores)

    # --- Softmax for Probabilistic Exploration ---
    # Temperature parameter: higher temp -> more exploration (probabilities closer to uniform)
    # lower temp -> less exploration (probabilities closer to argmax)
    temperature = 0.5  # Tunable parameter

    # Apply Softmax
    # Softmax(z)_i = exp(z_i / T) / sum(exp(z_j / T))
    # Ensure combined_scores are not excessively large to avoid exp overflow even after capping.
    # A common practice is to subtract the maximum score before exponentiation for numerical stability.
    scores_for_softmax = combined_scores / temperature
    
    # Subtract max score for numerical stability in exp
    stable_scores = scores_for_softmax - np.max(scores_for_softmax)
    
    exp_scores = np.exp(stable_scores)
    probabilities = exp_scores / np.sum(exp_scores)

    # Place the calculated probabilities back into the main priorities array
    priorities[suitable_bins_mask] = probabilities

    return priorities
```
