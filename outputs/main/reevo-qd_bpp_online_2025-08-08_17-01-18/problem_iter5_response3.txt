```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority with which we want to add item to each bin using a Softmax-based approach
    that prioritizes tight fits, considers bin scarcity, and encourages earlier bin usage.

    This heuristic aims to balance several factors:
    1.  **Tight Fit Preference:** Prioritize bins that leave minimal remaining capacity
        after packing the item. This is achieved by assigning higher scores to bins
        where `remaining_capacity - item` is small and non-negative.
    2.  **Bin Scarcity/Fullness:** Bins that are already quite full (i.e., have less
        remaining capacity) should generally be preferred over those with ample space,
        as this helps in utilizing bins more efficiently and potentially opening up
        space in other bins for larger items later.
    3.  **Exploration (Softmax):** Using Softmax allows for probabilistic selection,
        meaning even bins that are not the absolute "best fit" have a chance of being chosen.
        This can prevent getting stuck in local optima and discover better packings.
    4.  **Tie-breaking:** Implicitly, bins that are encountered earlier in the `bins_remain_cap`
        array might receive slightly higher priority if their scores are identical to later bins,
        due to how the scores are processed or due to inherent ordering.

    The scoring for suitable bins is designed as follows:
    For bins where `remaining_capacity >= item`:
    - We calculate a "fit score" based on how tightly the item fits. A simple inverse
      relationship with the remaining capacity is used, favoring smaller capacities.
    - A "scarcity score" related to the inverse of remaining capacity can be incorporated.
    - A common approach for balancing is to use a weighted sum or a transformation
      that combines these aspects.

    Here, we'll adapt a common approach for online bin packing heuristics that focuses on
    the "best fit" principle, but modulated by Softmax for exploration.
    A common score for "best fit" is the difference `remaining_capacity - item`.
    We want to *minimize* this difference. For Softmax, we need to transform this into
    scores where higher means more preferred.

    We'll use a score inversely related to the remaining capacity for suitable bins.
    A simple heuristic could be `1 / remaining_capacity`.
    To incorporate the "tight fit" preference more directly, we can consider the
    inverse of `(remaining_capacity - item) + epsilon` to avoid division by zero,
    or use a sigmoid-like function on this difference.

    Let's refine the reflection's idea:
    - **Tight Fit Score:** `1 / (1 + exp(k * (remaining_capacity - item)))` as in v1. This assigns
      higher scores to smaller positive differences.
    - **Exploration (Softmax):** Apply Softmax to these scores to get probabilities.
    - **Bin Scarcity/Earlier Bin Preference:** The original reflection mentioned "earlier bin preference".
      This can be achieved by adding a small constant to the score of earlier bins, or by
      ranking bins and adding a bonus based on rank. For simplicity and focusing on the
      fit/scarcity, we'll rely on Softmax's inherent exploration. The "scarcity" is implicitly
      handled by favoring bins with less `remaining_capacity`.

    Let's re-evaluate the scoring to directly favor smaller remaining capacities for Softmax.
    A score like `max_capacity - remaining_capacity` would favor fuller bins.
    However, we also need the item to fit.

    Consider the following score for suitable bins:
    `score = some_function(remaining_capacity - item)`
    We want `remaining_capacity - item` to be small.
    Let's try `score = - (remaining_capacity - item)` which directly rewards smaller differences.
    To make it suitable for Softmax (where we want positive scores), we can use:
    `score = C - (remaining_capacity - item)` where C is a large constant.
    Or, perhaps more intuitively, we want to *minimize* `remaining_capacity`.
    So, a score proportional to `-remaining_capacity` or `max_capacity - remaining_capacity`
    could work.

    Let's combine:
    1. Prioritize bins where `remaining_capacity` is small (scarcity/fullness).
    2. Among those, prioritize tighter fits (`remaining_capacity - item` is small).

    A score that captures this is to prioritize bins with smaller `remaining_capacity`.
    So, `score = -remaining_capacity`. For Softmax, we'd use `np.exp(-remaining_capacity)`.
    However, this doesn't directly incorporate the item size into the fit.

    Let's use the "tight fit" score from v1, but apply Softmax to it, potentially
    adding a small boost for bins that are generally less full (though the prompt
    suggests prioritizing tighter fits and scarcity, which might conflict).

    Let's focus on the reflection's core idea: tight fits and minimal waste,
    and Softmax for exploration.
    A score that reflects tight fit is `1 / (1 + exp(k * (remaining_capacity - item)))`.
    To make this suitable for Softmax (where larger is better), we can directly use this value.
    Bins that cannot fit the item get a score of 0.

    We'll use a parameter `temperature` for the Softmax function to control exploration.
    A higher temperature leads to more uniform probabilities (more exploration).
    A lower temperature leads to more concentrated probabilities on the highest scores (more exploitation).

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.

    Returns:
        A NumPy array of the same size as `bins_remain_cap`, where each element
        is the probability score for the corresponding bin, derived from Softmax.
        Bins that cannot fit the item will have a score of 0.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins that can accommodate the item
    suitable_bins_mask = bins_remain_cap >= item
    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]

    # If no bin can fit the item, return all zeros
    if suitable_bins_cap.size == 0:
        return priorities

    # --- Scoring Mechanism ---
    # We want to assign higher scores to bins that offer a tighter fit.
    # The difference `remaining_capacity - item` represents the wasted space.
    # We want to minimize this difference.
    # A score that rewards smaller differences is `1 / (1 + exp(k * (remaining_capacity - item)))`.
    # Let's define k. A higher k emphasizes tighter fits.
    k = 5.0  # Sensitivity parameter for the tight fit preference

    # Calculate the "mismatch" or wasted space for suitable bins
    mismatch = suitable_bins_cap - item

    # Calculate the raw "preference" score. Higher means more preferred.
    # Using the sigmoid score: 1 / (1 + exp(k * mismatch))
    # This score is between 0 and 1. A perfect fit (mismatch=0) gives 0.5.
    # Smaller positive mismatches give scores closer to 0.5.
    # Larger positive mismatches give scores closer to 0.
    # We want to favor smaller mismatches, so larger scores are better.
    # The current sigmoid score `1/(1+exp(k*mismatch))` correctly assigns higher values to smaller `mismatch`.

    # To prevent numerical issues with exp, cap the argument.
    max_exponent_arg = 35.0 # Corresponds to exp(35)
    
    capped_exponent_arg = np.minimum(k * mismatch, max_exponent_arg)
    
    # The raw scores: higher values mean better fits (smaller mismatch)
    raw_scores = 1 / (1 + np.exp(capped_exponent_arg))

    # --- Softmax Application ---
    # Apply Softmax to the raw scores to get probabilities.
    # Softmax(z_i) = exp(z_i) / sum(exp(z_j))
    # To control exploration, we can scale the scores by a temperature parameter.
    # `temperature` > 0.
    # If temperature is very low (e.g., close to 0), it's like argmax (exploitation).
    # If temperature is high, it's like uniform distribution (exploration).
    
    # Let's use a tunable temperature. A value like 0.1-1.0 is common.
    # If scores are already in [0,1], scaling might not be strictly necessary,
    # but Softmax generally expects values that can be exponentiated.
    # We can use the raw_scores directly or scale them.
    # Scaling by temperature: `exp(score / temperature)`.
    # Let's try a temperature that makes the differences more pronounced or smoothed.
    # A temperature of 1.0 means we use the raw scores directly in exp.
    # A temperature > 1 will smooth probabilities.
    # A temperature < 1 will sharpen probabilities.

    # Let's use temperature to control the "sharpness" of preference.
    # A lower temperature will favor the best fits more strongly.
    # A higher temperature will distribute preference more evenly.
    # Let's start with a moderate temperature, e.g., 0.5, to slightly favor better fits.
    temperature = 0.5 # Tunable parameter for exploration/exploitation balance

    # Apply exponentiation with temperature scaling
    # Ensure we don't have issues if temperature is very close to zero or zero.
    # If temperature is very small, `raw_scores / temperature` can become very large.
    # We can clip the scaled scores before exp to prevent overflow if temperature is tiny.
    # However, if temperature is 0, this is problematic. Assume temperature > 0.

    # Avoid division by zero if temperature is 0 or very small and scores are high.
    # If temperature is very small, scores with slight differences will be amplified.
    # Let's scale the scores by `1/temperature` before `exp` for a sharper distribution.
    # Or scale by `temperature` for a smoother distribution. The reflection says "tuning temperature for exploration/exploitation balance".
    # Higher temperature -> more exploration (smoother distribution).
    # Lower temperature -> more exploitation (sharper distribution).

    # We want to explore, so let's make temperature a factor that increases probability spread.
    # Use `exp(score / temperature)` where higher temperature spreads probabilities.
    # So, let's use `temperature = 0.5` (lower means sharper), `temperature = 2.0` (higher means flatter).
    # Let's set temperature to 1.0 initially for no scaling, and test.
    # If we want to favor tighter fits more, we want smaller `mismatch` to have higher probability.
    # `raw_scores` are already designed for this. Softmax will spread these.
    # Higher temperature -> more uniform probability distribution.
    # Lower temperature -> probability concentrated on the highest `raw_scores`.

    # Let's use a temperature that favors exploitation slightly, i.e., a lower temperature.
    # A temperature around 0.1-0.5 might be good for demonstrating preference.
    # Or, a temperature of 1.0 is standard softmax. Let's try to emphasize the preference.
    # If `raw_scores` are e.g., [0.6, 0.5, 0.4], exp([0.6, 0.5, 0.4]) = [1.82, 1.65, 1.49]
    # Sum = 4.96. Probs = [0.36, 0.33, 0.30]. Not very sharp.
    # If we scale by 1/temp: temp=0.1 => exp([6, 5, 4]) = [403, 148, 54]. Sum = 605. Probs = [0.66, 0.24, 0.08]. Much sharper.

    # Let's use temperature `T` in `exp(score / T)`.
    # Smaller `T` leads to sharper probabilities.
    temperature_param = 0.5 # Lower T = more exploitation of "best fit"

    # Ensure scores are not excessively large before exp to prevent overflow.
    # `raw_scores` are already capped at 1.
    # If `temperature_param` is very small, `raw_scores / temperature_param` can still be large.
    # Let's ensure `raw_scores / temperature_param` doesn't exceed a threshold before `exp`.
    # For example, threshold of 35 for the exponent argument.
    scaled_scores = raw_scores / temperature_param
    
    # Cap the scaled scores before exponentiation to prevent overflow
    max_scaled_score_exponent = 35.0 
    capped_scaled_scores = np.minimum(scaled_scores, max_scaled_score_exponent)
    
    # Calculate the exponentiated values
    exponentials = np.exp(capped_scaled_scores)

    # Calculate the sum of exponentiated values for normalization
    sum_exponentials = np.sum(exponentials)

    # Avoid division by zero if all exponentiated values are zero (e.g., due to capping or very low scores)
    if sum_exponentials == 0:
        # In this case, all suitable bins are equally (un)preferred or there was an issue.
        # A fallback could be uniform probability among suitable bins, or all zeros if it implies no good fit.
        # Given our scores are >= 0, sum_exponentials should only be 0 if exponentials are all 0.
        # This might happen if all raw_scores were extremely small and negative after some transformation,
        # or if capping resulted in zero exp. But our raw_scores are positive.
        # If sum is 0, it means all `capped_scaled_scores` were -inf (which shouldn't happen here).
        # As a fallback, assign equal probability to suitable bins if sum is zero.
        if suitable_bins_cap.size > 0:
            priorities[suitable_bins_mask] = 1.0 / suitable_bins_cap.size
        return priorities

    # Calculate the final probabilities using Softmax
    softmax_probabilities = exponentials / sum_exponentials

    # Place the calculated softmax probabilities back into the main priorities array
    priorities[suitable_bins_mask] = softmax_probabilities

    return priorities
```
