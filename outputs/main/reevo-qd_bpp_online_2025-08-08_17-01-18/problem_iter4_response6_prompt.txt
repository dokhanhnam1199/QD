{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-based approach\n    that also incorporates an epsilon-greedy exploration strategy.\n\n    This heuristic first calculates a \"fit score\" for each bin, where a better fit (less remaining capacity)\n    results in a higher score. These scores are then transformed using softmax to get probabilities.\n    An epsilon-greedy component is added: with a small probability `epsilon`, a bin is chosen randomly\n    from the *suitable* bins to encourage exploration. Otherwise, the greedy (softmax-derived)\n    priorities are used.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploring a random suitable bin\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    n_bins = len(bins_remain_cap)\n    priorities = np.zeros(n_bins)\n\n    if len(suitable_bin_indices) == 0:\n        # No bin can fit the item, return all zeros\n        return priorities\n\n    # Epsilon-greedy choice: With probability epsilon, pick a random suitable bin\n    if np.random.rand() < epsilon:\n        # Choose one suitable bin randomly\n        chosen_index = np.random.choice(suitable_bin_indices)\n        # Assign a high priority to this randomly chosen bin\n        priorities[chosen_index] = 1.0\n        # For other bins, assign a very low priority to make the chosen one stand out\n        priorities[suitable_bin_indices] = 1e-9\n        # Ensure non-suitable bins remain at 0\n        return priorities\n\n    # If not exploring, use a greedy strategy based on how well the item fits.\n    # We want bins where the remaining capacity is just enough for the item.\n    # The score should be higher for bins with `bins_remain_cap - item` closer to 0.\n    # Use `-(bins_remain_cap - item)` as the score, which is `item - bins_remain_cap`.\n    # This makes smaller positive differences (better fits) have higher scores.\n\n    scores = item - bins_remain_cap\n\n    # For bins that cannot fit the item, assign a very low score.\n    # This ensures their softmax probability is negligible.\n    scores[~suitable_bins_mask] = -np.inf  # Effectively -infinity for softmax\n\n    # Calculate softmax probabilities\n    # exp_scores = np.exp(scores)\n    # To prevent potential overflow with large positive scores or underflow with large negative scores,\n    # we can use `np.exp(scores - np.max(scores))` if scores are not all -inf.\n    # Since we set non-suitable bins to -inf, and suitable bins have scores >= 0,\n    # np.max(scores) will be at least 0 for suitable bins.\n    \n    # Calculate logits, ensuring they are not all -inf\n    max_score = np.max(scores[suitable_bins_mask]) if np.any(suitable_bins_mask) else 0\n    \n    # Shift scores so that the maximum is 0 to avoid large exponents\n    shifted_scores = scores - max_score\n    \n    # Ensure exp_scores are non-negative and sum is positive before division\n    exp_scores = np.exp(shifted_scores)\n    \n    sum_exp_scores = np.sum(exp_scores)\n    \n    if sum_exp_scores == 0:\n        # This case should ideally not be reached if there's at least one suitable bin\n        # but as a safeguard, return uniform probabilities for suitable bins.\n        # This would mean assigning equal probability to all bins that can fit.\n        if np.any(suitable_bins_mask):\n            priorities[suitable_bins_mask] = 1.0 / len(suitable_bin_indices)\n        return priorities\n    \n    priorities = exp_scores / sum_exp_scores\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score aims to prioritize bins that are a \"good fit\" for the item.\n    A good fit is generally considered to be a bin where the remaining capacity\n    is slightly larger than the item size, avoiding both empty bins and bins that\n    are almost full.\n\n    The sigmoid function `1 / (1 + exp(-k * (x - x0)))` maps any real number\n    to a value between 0 and 1. We use it here to score how \"close\" the remaining\n    capacity is to the item size.\n\n    We want bins where `bin_remain_cap - item` is close to 0.\n    So, a bin with `bin_remain_cap >= item` is a candidate.\n    Among these candidates, we prefer those where `bin_remain_cap` is just enough\n    to fit the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Consider only bins that have enough capacity for the item\n    available_bins_mask = bins_remain_cap >= item\n    available_bins_cap = bins_remain_cap[available_bins_mask]\n\n    if available_bins_cap.size == 0:\n        return priorities  # No bins can accommodate the item\n\n    # Calculate the \"mismatch\" for available bins: remaining_cap - item\n    mismatch = available_bins_cap - item\n\n    # We want to give higher scores to bins where mismatch is close to 0.\n    # The sigmoid function can be used to create a score that peaks around 0.\n    # We can transform the mismatch values. A negative value for mismatch\n    # means the bin is too small (already handled by the mask), so we focus on\n    # non-negative mismatches.\n\n    # A simple approach is to invert the mismatch so that smaller mismatches\n    # become larger values, and then apply a sigmoid.\n    # However, a direct sigmoid on mismatch might not be ideal because it\n    # emphasizes very large remaining capacities.\n\n    # Let's try to design a sigmoid function that peaks when mismatch is zero.\n    # The function f(x) = 1 / (1 + exp(-k * x)) peaks at x=0 if we shift it or\n    # use it as is. We want to reward bins where `mismatch` is small (close to 0).\n\n    # Let's map `mismatch` to a value where 0 mismatch is the optimal value.\n    # Consider `score = sigmoid(k * (threshold - mismatch))`\n    # Where `threshold` is the ideal mismatch (e.g., 0).\n    # A large `k` makes the transition steeper.\n    # We want to give a high score if `mismatch` is small.\n    # So, if `mismatch` is 0, we want a high score.\n    # If `mismatch` is large, we want a low score.\n\n    # Let's try `sigmoid(k * (-(mismatch)))` which is `sigmoid(-k * mismatch)`.\n    # If mismatch is 0, score is 0.5.\n    # If mismatch is positive, score is < 0.5.\n    # If mismatch is negative, score is > 0.5.\n\n    # This is not quite right as it gives lower scores for small positive mismatches.\n    # We want to incentivize fitting tightly but still fitting.\n\n    # Let's use a shifted and scaled sigmoid.\n    # We can define a score function that is high when `mismatch` is small and positive.\n    # Consider `score = 1 / (1 + exp(-k * (ideal_mismatch - mismatch)))`.\n    # If `ideal_mismatch` is 0, this is `1 / (1 + exp(-k * (-mismatch)))`.\n    # This will give higher scores for negative mismatch (bins too small, which we filter).\n\n    # Alternative: Focus on `item / bin_remain_cap`.\n    # If `bin_remain_cap` is very large, this ratio is small.\n    # If `bin_remain_cap` is just above `item`, this ratio is close to 1.\n    # We want to maximize this ratio, but not exceed 1.\n    # We can use a sigmoid that maps values close to 1 (but less than 1) to high scores.\n\n    # Let's refine the mismatch approach:\n    # We want to reward bins where `bin_remain_cap` is close to `item`.\n    # The difference `bin_remain_cap - item` should be small.\n    # A value close to 0 for this difference is good.\n    # Let's scale and shift the mismatch: `scaled_mismatch = (mismatch - mean_mismatch) / std_mismatch`\n    # Or, a simpler approach: normalize mismatch relative to some max possible mismatch.\n\n    # Sigmoid strategy: score = 1 / (1 + exp(-k * (target_val - current_val)))\n    # We want `current_val` (which is `mismatch`) to be close to `target_val` (e.g., 0).\n    # If `target_val = 0`, then `score = 1 / (1 + exp(-k * (-mismatch)))`.\n    # This gives high scores for negative `mismatch` (too small).\n    # This is not ideal because we already filter `mismatch < 0`.\n\n    # Let's try: `score = 1 / (1 + exp(-k * (mismatch)))`\n    # Mismatch = 0 -> score = 0.5\n    # Mismatch > 0 -> score < 0.5 (worse fit)\n    # Mismatch < 0 -> score > 0.5 (better fit)\n\n    # This still seems to penalize small positive mismatches.\n    # We need a function that peaks at mismatch = 0.\n\n    # Consider a Gaussian-like shape using exp(-x^2).\n    # `score = exp(-k * mismatch**2)`\n    # Mismatch = 0 -> score = 1\n    # Mismatch != 0 -> score < 1. This is a good candidate.\n    # This can be approximated with a sigmoid.\n\n    # Let's re-think the sigmoid target.\n    # We want `bin_remain_cap` to be just enough.\n    # If we focus on `bin_remain_cap`, we want it to be close to `item`.\n    # `score = sigmoid(k * (item - bin_remain_cap))`\n    # If `bin_remain_cap` is slightly larger than `item`, `item - bin_remain_cap` is small and negative.\n    # `sigmoid(small_negative_val)` is close to 0.\n    # If `bin_remain_cap` is much larger than `item`, `item - bin_remain_cap` is a large negative.\n    # `sigmoid(large_negative_val)` is close to 0.\n    # If `bin_remain_cap` is exactly `item`, `item - bin_remain_cap` is 0.\n    # `sigmoid(0)` is 0.5.\n\n    # This function favors bins with `item - bin_remain_cap` being small negative,\n    # which means `bin_remain_cap` is slightly larger than `item`.\n\n    # Let's define k as a parameter controlling sensitivity. A higher k means\n    # a sharper peak around the ideal fit.\n    k = 5.0  # Sensitivity parameter, can be tuned.\n\n    # Calculate the sigmoid score for available bins.\n    # We want `bin_remain_cap` to be as close to `item` as possible, but >= `item`.\n    # This means we want `bin_remain_cap - item` to be small and non-negative.\n    # Let's transform this difference to get a score.\n\n    # A score that peaks when `bin_remain_cap - item` is 0.\n    # `score = 1 / (1 + exp(-k * (max_possible_item_size - (bin_remain_cap - item))))`\n    # This seems complicated.\n\n    # Let's use the mismatch and map it.\n    # We want to reward small positive mismatch values.\n    # We can use a sigmoid with a shift.\n    # `score = 1 / (1 + exp(-k * (mismatch_target - mismatch)))`\n    # If `mismatch_target` is 0, then `score = 1 / (1 + exp(-k * (-mismatch)))`.\n    # This would give high scores for negative mismatch values.\n\n    # Let's consider what `bins_remain_cap - item` represents:\n    # Small positive value: good fit\n    # Large positive value: too much space, potentially wasted\n    # Zero: perfect fit\n\n    # We can try to penalize large positive values.\n    # `score = 1 - sigmoid(k * (mismatch))`\n    # `score = 1 - 1 / (1 + exp(-k * mismatch))`\n    # `score = exp(-k * mismatch) / (1 + exp(-k * mismatch))`\n    # If mismatch is 0, score = 0.5.\n    # If mismatch is large positive, score -> 0.\n    # If mismatch is large negative, score -> 1.\n\n    # This gives high scores to bins that are too small, which is counter-intuitive.\n\n    # The common way to implement a \"best fit\" using sigmoid is to target\n    # the difference `bin_remain_cap - item` being close to 0.\n    # `sigmoid(k * (target_val - x))` where x is the value being measured.\n    # Here, we want to measure `bin_remain_cap`.\n    # `target_val` would be `item`.\n    # So, `score = 1 / (1 + exp(-k * (item - bin_remain_cap)))`\n\n    # Let's re-evaluate `score = 1 / (1 + exp(-k * (bin_remain_cap - item)))`.\n    # If `bin_remain_cap == item` (mismatch=0), score = 0.5.\n    # If `bin_remain_cap` > `item` (mismatch > 0), e.g., `bin_remain_cap = item + delta`\n    # `score = 1 / (1 + exp(-k * delta))`. If delta > 0, exp(-k*delta) < 1.\n    # So `1 + exp < 2`, and `score > 0.5`. The larger delta, the smaller exp(-k*delta), closer to 0. So score gets closer to 1.\n    # This penalizes larger remaining capacity, which is WRONG.\n\n    # Let's flip it: `score = 1 / (1 + exp(-k * (item - bin_remain_cap)))`\n    # If `bin_remain_cap == item` (mismatch=0), score = 0.5.\n    # If `bin_remain_cap` > `item` (mismatch > 0), e.g., `bin_remain_cap = item + delta`\n    # `score = 1 / (1 + exp(-k * (-delta))) = 1 / (1 + exp(k * delta))`.\n    # If delta > 0, k*delta > 0. exp(k*delta) > 1. `1 + exp > 2`.\n    # So `score < 0.5`. The larger delta, the smaller the score. This is CORRECT.\n    # If `bin_remain_cap` < `item` (mismatch < 0), this case is filtered.\n    # If we didn't filter, and `bin_remain_cap = item - delta'`, where delta' > 0.\n    # `score = 1 / (1 + exp(-k * (item - (item - delta')))) = 1 / (1 + exp(k * delta'))`.\n    # If delta' > 0, k*delta' > 0. exp(k*delta') > 1. `1 + exp > 2`. So `score < 0.5`.\n    # This would give low scores to bins that are too small. This is also fine,\n    # but we already use a mask for this.\n\n    # So, the expression `1 / (1 + exp(-k * (item - bin_remain_cap)))` for `bin_remain_cap >= item`\n    # appears to be a reasonable Sigmoid Fit Score. It assigns higher scores to bins\n    # where `bin_remain_cap` is closer to `item`.\n\n    # We want to make sure the range of arguments to sigmoid is reasonable.\n    # If `bin_remain_cap` is very large, `item - bin_remain_cap` can be a large negative number.\n    # `exp(large_positive)` can overflow.\n    # If `bin_remain_cap` is exactly `item`, argument is 0.\n    # If `bin_remain_cap` is slightly larger than `item`, argument is small negative.\n\n    # To prevent potential issues with very large capacities and the sigmoid argument:\n    # We can clip the `bin_remain_cap` for calculating the argument, or scale.\n    # Alternatively, we can use `1 / (1 + exp(k * (bin_remain_cap - item)))`\n    # Let's check this:\n    # `score = 1 / (1 + exp(k * (bin_remain_cap - item)))`\n    # If `bin_remain_cap == item` (mismatch=0), score = 1 / (1 + exp(0)) = 1 / (1 + 1) = 0.5.\n    # If `bin_remain_cap` > `item` (mismatch > 0), e.g., `bin_remain_cap = item + delta`.\n    # `score = 1 / (1 + exp(k * delta))`. If delta > 0, k*delta > 0. exp(k*delta) > 1.\n    # `1 + exp > 2`. So `score < 0.5`. The larger delta, the smaller the score. CORRECT.\n    # This expression seems more numerically stable for large positive differences.\n\n    # Let's use this expression.\n    # We'll compute it for the available bins.\n\n    # Parameters for sigmoid: k (steepness)\n    k = 5.0 # Sensitivity. Higher k means score drops faster as capacity increases beyond item.\n\n    # Calculate the argument for the sigmoid: k * (remaining_capacity - item)\n    # We are interested in `bins_remain_cap >= item`.\n    # `argument = k * (available_bins_cap - item)`\n\n    # The sigmoid function: `sigmoid(x) = 1 / (1 + exp(-x))`\n    # Our chosen formula is `1 / (1 + exp(k * (bin_remain_cap - item)))`.\n    # Let `x = k * (bin_remain_cap - item)`. Then `sigmoid_val = 1 / (1 + exp(x))`.\n    # This is also equivalent to `1 - sigmoid(-x) = 1 - 1 / (1 + exp(-(-x))) = 1 - 1 / (1 + exp(x))`.\n    # So, it's 1 minus a standard sigmoid applied to `k * (bin_remain_cap - item)`.\n    # This shape is suitable: peaks at 0.5, decreases for positive arguments.\n\n    # Applying the sigmoid to the mismatches for the available bins\n    # We want to assign these calculated priorities back to the original `priorities` array.\n    sigmoid_scores = 1 / (1 + np.exp(k * (available_bins_cap - item)))\n\n    # Place the calculated sigmoid scores back into the priorities array\n    priorities[available_bins_mask] = sigmoid_scores\n\n    return priorities\n\n[Reflection]\nPrioritize bins with minimal remaining capacity *beyond* item size.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}