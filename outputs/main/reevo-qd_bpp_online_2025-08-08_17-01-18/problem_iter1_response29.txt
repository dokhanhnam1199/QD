```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Softmax-Based Fit.

    This heuristic prioritizes bins that have a remaining capacity slightly larger than the item,
    aiming to minimize wasted space. It uses a softmax function to convert these differences
    into probabilities, effectively assigning higher priority to bins that are a "good fit".

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Calculate the "fit" of the item into each bin.
    # We want bins where bins_remain_cap is just enough or slightly more than the item.
    # A negative value here means the item doesn't fit. We can clamp these to a small
    # positive value or 0 to avoid issues with softmax if all items don't fit.
    # A large positive difference (item fits easily) is also not ideal as it wastes space.
    # So we want the difference (bins_remain_cap - item) to be close to zero.
    # We can use the negative of this difference as the exponent in softmax,
    # so smaller (bins_remain_cap - item) results in a higher exponent.
    fits = bins_remain_cap - item

    # Filter out bins where the item does not fit (remaining capacity < item size)
    # Assign a very low priority (or effectively zero) to these bins.
    # A large negative number in softmax exponent will result in a value close to 0.
    # We can also directly set their fits to a very low value before softmax.
    fits[fits < 0] = -np.inf # Effectively 0 probability after softmax

    # Apply the softmax function. The exponent in softmax should reflect desirability.
    # We want bins with (bins_remain_cap - item) close to 0 to have high priority.
    # So, we can use -(bins_remain_cap - item) as the exponent, or more intuitively,
    # (item - bins_remain_cap) as the exponent, ensuring negative values become smaller.
    # Or even better, prioritize bins with a positive difference close to zero.
    # Let's define desirability as: higher is better if remaining_cap is slightly > item.
    # Consider a transformation: exp(-abs(fits)). This would favor fits near 0.
    # However, softmax typically takes logits (raw scores).
    # Let's consider the score `s_i = -(bins_remain_cap_i - item)^2`. This penalizes
    # being too small or too large. But we only care about `bins_remain_cap_i >= item`.
    # So, if `bins_remain_cap_i < item`, priority should be 0.
    # If `bins_remain_cap_i >= item`, we want `bins_remain_cap_i - item` to be small.
    # A good transformation for softmax could be to map `bins_remain_cap_i - item` to a
    # desirability score. Higher `bins_remain_cap_i` than `item` is good, but not too high.
    # Let's use `bins_remain_cap_i - item` directly as logits, but only for bins that can fit.
    # For bins that cannot fit, their logit should be extremely low.

    # Option 1: Softmax on (bins_remain_cap - item) for fitting bins.
    # Prioritize bins where remaining capacity is *exactly* the item size.
    # Using -fits will invert the ordering, so smaller positive diffs become larger.
    # exp(-fits) where fits = bins_remain_cap - item
    # If bins_remain_cap = 5, item = 3, fit = 2. exp(-2) = 0.135
    # If bins_remain_cap = 3, item = 3, fit = 0. exp(0) = 1.0
    # If bins_remain_cap = 6, item = 3, fit = 3. exp(-3) = 0.049
    # This prioritizes exact fits.

    # We need to ensure that we don't assign probabilities to bins where the item doesn't fit.
    # A common way to do this with softmax is to assign a very low logit (large negative number).
    logits = bins_remain_cap - item
    # For bins that cannot fit the item, set their logit to a very small number.
    # This will make their softmax probability close to zero.
    logits[logits < 0] = -1e9  # A large negative number

    # For bins that can fit, we want to prioritize those with smaller remaining capacity (closer to item size).
    # So, we want to exponentiate -(bins_remain_cap - item) which is (item - bins_remain_cap).
    # However, the softmax input should ideally be positive values that represent scores.
    # Let's redefine `scores`: a higher score means better fit.
    # A perfect fit would have score X.
    # A slightly larger capacity would have score X - epsilon.
    # A much larger capacity would have score X - delta.
    # A capacity smaller than item would have score -infinity.
    # So, we can use `item - bins_remain_cap` as our underlying measure for exponentiation,
    # but we need to handle the case where `bins_remain_cap < item`.

    # Let's use a modified approach. We want bins that *can* fit, and among those,
    # we prefer bins with less excess capacity.
    # So, a "goodness" score could be:
    # -infinity if item > bins_remain_cap
    # -(bins_remain_cap - item) otherwise (prioritizing smaller differences)

    scores = -(bins_remain_cap - item)
    scores[bins_remain_cap < item] = -np.inf # Ensure items not fitting get no chance

    # Now apply softmax. Softmax(x_i) = exp(x_i) / sum(exp(x_j)).
    # The `scores` directly go into the exponent of softmax.
    # Higher scores (closer to 0, or less negative) mean higher probability.
    # If scores = [-inf, -inf, 0, -2, -5],
    # exp(scores) = [0, 0, 1, exp(-2), exp(-5)]
    # Softmax will normalize these.

    # Add a small constant to the score for bins that can fit, to ensure
    # that even a small positive difference doesn't get zero probability.
    # For instance, if we use -(bins_remain_cap - item), a difference of 10
    # gives exp(-10) which is tiny.
    # Perhaps a linear scaling or a different transformation is better.
    # Let's try mapping `bins_remain_cap - item` to a desirability score.
    # If diff = 0, score = K (high)
    # If diff = 1, score = K - epsilon
    # If diff = 10, score = K - delta
    # If diff < 0, score = -infinity

    # Let's re-think the logit construction for softmax.
    # We want the probability P_i proportional to exp(logit_i).
    # Desirability of bin i for item: D_i
    # P_i = exp(alpha * D_i) / sum(exp(alpha * D_j))
    # If item doesn't fit: D_i = -infinity
    # If item fits and diff = bins_remain_cap - item:
    # We want smaller diffs to have higher D_i.
    # So, D_i = -(bins_remain_cap - item) = item - bins_remain_cap.

    # This approach correctly prioritizes bins with less remaining capacity over
    # bins with more remaining capacity, for those that can fit the item.
    # The `-np.inf` for non-fitting bins ensures they get 0 probability.
    # The `alpha` parameter (implicitly 1 here) controls the "peakiness" of the distribution.

    # Calculate the underlying scores for softmax.
    # A higher score means more desirable.
    # If bins_remain_cap >= item, we want bins_remain_cap - item to be small.
    # So, let's use `-(bins_remain_cap - item)` which is `item - bins_remain_cap`.
    # For bins where `bins_remain_cap < item`, the score should be very low.

    scores = item - bins_remain_cap

    # Set scores to a very low value for bins where the item does not fit.
    # This ensures their probability contribution in softmax is negligible.
    scores[bins_remain_cap < item] = -1e9 # Effectively -infinity for softmax

    # Calculate probabilities using softmax.
    # Adding 1 to scores for fitting bins can help avoid all zeros if all diffs are large negative
    # but we've handled that with -1e9 for non-fitting bins.
    # The softmax calculation itself: exp(scores) / sum(exp(scores))
    # We can use np.exp directly, but be mindful of overflow/underflow if scores are very large/small.
    # If scores = [-1e9, -1e9, 0, -2, -5], then exp(scores) = [0, 0, 1, exp(-2), exp(-5)]
    # This seems correct.
    exp_scores = np.exp(scores)

    # If all items resulted in -1e9 (meaning item doesn't fit any bin), exp_scores will be all zeros.
    # In this case, the sum will be zero, leading to division by zero. Handle this edge case.
    sum_exp_scores = np.sum(exp_scores)
    if sum_exp_scores == 0:
        # This means the item cannot fit into any available bin.
        # Return zero priorities for all bins.
        return np.zeros_like(bins_remain_cap)

    priorities = exp_scores / sum_exp_scores

    return priorities
```
