[
  {
    "stdout_filepath": "problem_iter1_response20.txt_stdout.txt",
    "code_path": "problem_iter1_code20.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit Decreasing logic.\n\n    This heuristic prioritizes bins that can accommodate the item and are \"tight fits\"\n    to minimize wasted space. Bins that are too small for the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate priority as the inverse of the remaining capacity\n    # A smaller remaining capacity means a tighter fit, hence higher priority.\n    # We add a small epsilon to avoid division by zero if a bin has exactly the item's size.\n    priorities[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9)\n\n    return priorities",
    "response_id": 20,
    "obj": 4.048663741523748,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This heuristic implements a \"Best Fit Decreasing\" like strategy for the\n    priority function within an online First Fit context. For each item, it\n    prioritizes bins that can accommodate the item, and among those, it\n    prefers bins that will have the least remaining capacity after the item\n    is placed. This aims to \"tightly pack\" bins, potentially leaving larger\n    spaces in other bins for future larger items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Calculate remaining capacity after placing the item\n    potential_remaining_cap = bins_remain_cap - item\n    \n    # Bins that can accommodate the item have a non-negative remaining capacity\n    can_accommodate = potential_remaining_cap >= 0\n    \n    # Assign a priority:\n    # For bins that can accommodate the item, the priority is the inverse of\n    # the remaining capacity after placement. Smaller remaining capacity gets higher priority.\n    # We use a large number (e.g., 1e6) for bins that cannot accommodate the item\n    # to ensure they have a much lower priority.\n    \n    # To make smaller remaining capacities have higher priority, we can use negative.\n    # The more negative, the better.\n    priorities[can_accommodate] = -potential_remaining_cap[can_accommodate]\n    \n    # Bins that cannot accommodate the item get a very low priority (a large positive number)\n    # We can also set it to 0 if we want to strictly exclude them if a better option exists.\n    # However, a very low negative number makes more sense for sorting if we want to be\n    # sure they are not picked. For First Fit, it's about *which* bin to try first.\n    # A common way to handle \"cannot accommodate\" in priority systems is to give them a\n    # very low score. If we want to avoid them entirely, we can assign a score that will\n    # never be chosen if any other valid option exists. Let's use a value that is\n    # significantly less desirable than any valid remaining capacity.\n    # Since remaining capacities are non-negative, negative values are good.\n    # We can use a large negative number for those that cannot fit.\n    priorities[~can_accommodate] = -1e9 # A very low priority\n    \n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit strategy.\n\n    The Best Fit strategy aims to place the item into the bin that has the least remaining\n    capacity after the item is placed, provided it fits. This helps in keeping larger\n    bins available for larger items later.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a more desirable bin. Bins that cannot fit the item\n        will have a priority of 0.\n    \"\"\"\n    # Calculate remaining capacity after placing the item\n    potential_remaining_caps = bins_remain_cap - item\n\n    # Initialize priorities to zero\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Find bins where the item can fit\n    fit_indices = np.where(potential_remaining_caps >= 0)[0]\n\n    if len(fit_indices) > 0:\n        # For bins that can fit the item, assign a priority.\n        # The \"best fit\" is the one that leaves the *least* remaining capacity.\n        # To make this a \"highest priority\" score, we can invert the remaining capacity\n        # (or use a large number minus remaining capacity).\n        # A simple approach is to use the negative of the potential remaining capacity.\n        # The more negative, the better the fit (i.e., less remaining space).\n        priorities[fit_indices] = -potential_remaining_caps[fit_indices]\n\n        # To ensure that a bin that is a \"perfect fit\" (leaves 0 remaining capacity)\n        # is prioritized over one that leaves, say, -1 remaining capacity (meaning it was\n        # a bit too large), we can further refine the priority.\n        # If multiple bins have the same minimum remaining capacity, any of them is fine.\n        # The current negative remaining capacity already ranks them appropriately.\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response3.txt_stdout.txt",
    "code_path": "problem_iter1_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    The Almost Full Fit strategy prioritizes bins that, after placing the item,\n    will have the least remaining capacity among bins that can still accommodate the item.\n    This aims to fill bins as much as possible before opening new ones.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot accommodate the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find bins that can accommodate the item\n    possible_bins_mask = bins_remain_cap >= item\n\n    # Calculate remaining capacity after placing the item in possible bins\n    remaining_after_placement = bins_remain_cap[possible_bins_mask] - item\n\n    # If there are no possible bins, return all zeros\n    if remaining_after_placement.size == 0:\n        return priorities\n\n    # Calculate the \"tightness\" score for possible bins.\n    # We want to minimize the remaining capacity, so a smaller remaining capacity\n    # should result in a higher priority.\n    # We use the inverse of the remaining capacity. To avoid division by zero\n    # or extremely high priorities for bins that become exactly full, we can\n    # add a small epsilon or use a scaled inverse.\n    # A common approach is to consider bins that leave little to no space.\n    # We want to maximize the chance of the bin becoming \"almost full\".\n\n    # Let's aim for a score where smaller remaining capacity is better.\n    # A simple approach is to use the negative of the remaining capacity.\n    # However, to make it a \"priority\" score (higher is better), we can\n    # invert it and potentially scale it.\n\n    # To prioritize bins that leave minimal remaining space, we can take\n    # the negative of the remaining capacity. The larger (less negative)\n    # the value, the less space is left, thus higher priority.\n    # Let's refine this: We want bins that, after placing the item, will have\n    # the *least* remaining capacity. This means we want to *minimize*\n    # `bins_remain_cap - item`.\n    # If we want higher scores to be better, we can assign a score based on\n    # the inverse of the remaining capacity.\n    # However, the goal is to fill bins. So bins that will be closest to full\n    # after placing the item are preferred.\n\n    # Consider the difference: max_capacity - (remaining_after_placement)\n    # This is effectively how much space is used. We want to maximize this.\n    # So, `item` is constant. Maximizing `bins_remain_cap[i] - remaining_after_placement[i]`\n    # means maximizing `bins_remain_cap[i] - (bins_remain_cap[i] - item)` which is just `item`.\n    # This isn't quite right.\n\n    # \"Almost Full Fit\" suggests bins that are ALMOST full.\n    # After placing the item, a bin is \"almost full\" if its remaining capacity is small.\n    # So, we want to minimize `bins_remain_cap[i] - item`.\n    # To convert this to a priority score (higher is better), we can:\n    # 1. Use `1 / (remaining_after_placement + epsilon)` where epsilon is a small number to avoid division by zero.\n    # 2. Use `-(remaining_after_placement)`\n    # 3. Use `max_possible_remaining - remaining_after_placement` for some large `max_possible_remaining`\n    #    which is equivalent to `some_constant - remaining_capacity`.\n\n    # Let's try option 2: higher priority for smaller remaining capacity.\n    # So, `priority = -remaining_capacity`. This means a bin with remaining_capacity=1\n    # gets priority -1, and a bin with remaining_capacity=0 gets priority 0.\n    # This seems to fit the idea of \"smallest remaining capacity\".\n\n    # To make it more \"priority-like\" (higher is better), we can use:\n    # `priority = C - remaining_capacity`, where C is a large constant, or\n    # `priority = 1 / (remaining_capacity + epsilon)`\n\n    # Let's use the concept that the *difference* between what's remaining and what's desired (a full bin)\n    # should be minimized. So, `remaining_capacity` should be small.\n    # A score that reflects this: `max(0, C - remaining_capacity)`.\n    # If we want to prioritize bins that become *most* full after the item,\n    # this means the remaining capacity is minimized.\n\n    # Consider the bin that would become \"most full\". This is the bin where\n    # `bins_remain_cap[i] - item` is minimized.\n    # So, higher priority for smaller `bins_remain_cap[i] - item`.\n    # Let's transform `bins_remain_cap[i] - item` into a priority:\n    # `priority = some_large_value - (bins_remain_cap[i] - item)`\n    # This is equivalent to `some_large_value - bins_remain_cap[i] + item`.\n    # Since `item` is constant for all bins, this is `constant - bins_remain_cap[i]`.\n    # We want to maximize this, so we want to minimize `bins_remain_cap[i]`.\n\n    # Another perspective on \"almost full\":\n    # We want to pick the bin that, after placing the item, is closest to full.\n    # This means `bins_remain_cap[i] - item` is minimal.\n    # Let's assign a priority based on this minimum value.\n\n    # A common heuristic for \"Best Fit\" or \"Almost Full Fit\" is to try to minimize\n    # the remaining capacity of the bin *after* placing the item.\n    # This means we want to select the bin `i` that minimizes `bins_remain_cap[i] - item`.\n    # To convert this into a priority score (where higher means more desirable),\n    # we can use `-(bins_remain_cap[i] - item)` or `1 / (bins_remain_cap[i] - item + epsilon)`.\n\n    # Let's try using `1 / (remaining_after_placement + epsilon)` where epsilon is small.\n    # This ensures that bins with very small remaining capacity get very high priorities.\n\n    epsilon = 1e-6\n    priorities[possible_bins_mask] = 1.0 / (remaining_after_placement + epsilon)\n\n    # Alternatively, if we want to give a higher score to bins that leave *less*\n    # remaining space, we can simply use the negative of the remaining space,\n    # or a scaled version of it.\n\n    # Let's consider the original \"Almost Full Fit\" idea: prioritize bins that are\n    # already close to full. However, the problem statement for this function\n    # implies prioritizing based on the *outcome* after placing the item.\n    # \"priority score for each bin ... bin with the highest priority score will be selected for the item.\"\n\n    # If the goal is to fill bins, then after placing the item, we want the bin\n    # with the *least* remaining capacity.\n    # So, `priority = f(remaining_capacity)` where `f` is decreasing.\n    # `f(x) = 1/(x + epsilon)` or `f(x) = -x`.\n    # Let's use the `1/(x + epsilon)` approach for a strong preference towards tighter fits.\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response26.txt_stdout.txt",
    "code_path": "problem_iter1_code26.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score aims to prioritize bins that are a \"good fit\" for the item.\n    A good fit is generally considered to be a bin where the remaining capacity\n    is slightly larger than the item size, avoiding both empty bins and bins that\n    are almost full.\n\n    The sigmoid function `1 / (1 + exp(-k * (x - x0)))` maps any real number\n    to a value between 0 and 1. We use it here to score how \"close\" the remaining\n    capacity is to the item size.\n\n    We want bins where `bin_remain_cap - item` is close to 0.\n    So, a bin with `bin_remain_cap >= item` is a candidate.\n    Among these candidates, we prefer those where `bin_remain_cap` is just enough\n    to fit the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Consider only bins that have enough capacity for the item\n    available_bins_mask = bins_remain_cap >= item\n    available_bins_cap = bins_remain_cap[available_bins_mask]\n\n    if available_bins_cap.size == 0:\n        return priorities  # No bins can accommodate the item\n\n    # Calculate the \"mismatch\" for available bins: remaining_cap - item\n    mismatch = available_bins_cap - item\n\n    # We want to give higher scores to bins where mismatch is close to 0.\n    # The sigmoid function can be used to create a score that peaks around 0.\n    # We can transform the mismatch values. A negative value for mismatch\n    # means the bin is too small (already handled by the mask), so we focus on\n    # non-negative mismatches.\n\n    # A simple approach is to invert the mismatch so that smaller mismatches\n    # become larger values, and then apply a sigmoid.\n    # However, a direct sigmoid on mismatch might not be ideal because it\n    # emphasizes very large remaining capacities.\n\n    # Let's try to design a sigmoid function that peaks when mismatch is zero.\n    # The function f(x) = 1 / (1 + exp(-k * x)) peaks at x=0 if we shift it or\n    # use it as is. We want to reward bins where `mismatch` is small (close to 0).\n\n    # Let's map `mismatch` to a value where 0 mismatch is the optimal value.\n    # Consider `score = sigmoid(k * (threshold - mismatch))`\n    # Where `threshold` is the ideal mismatch (e.g., 0).\n    # A large `k` makes the transition steeper.\n    # We want to give a high score if `mismatch` is small.\n    # So, if `mismatch` is 0, we want a high score.\n    # If `mismatch` is large, we want a low score.\n\n    # Let's try `sigmoid(k * (-(mismatch)))` which is `sigmoid(-k * mismatch)`.\n    # If mismatch is 0, score is 0.5.\n    # If mismatch is positive, score is < 0.5.\n    # If mismatch is negative, score is > 0.5.\n\n    # This is not quite right as it gives lower scores for small positive mismatches.\n    # We want to incentivize fitting tightly but still fitting.\n\n    # Let's use a shifted and scaled sigmoid.\n    # We can define a score function that is high when `mismatch` is small and positive.\n    # Consider `score = 1 / (1 + exp(-k * (ideal_mismatch - mismatch)))`.\n    # If `ideal_mismatch` is 0, this is `1 / (1 + exp(-k * (-mismatch)))`.\n    # This will give higher scores for negative mismatch (bins too small, which we filter).\n\n    # Alternative: Focus on `item / bin_remain_cap`.\n    # If `bin_remain_cap` is very large, this ratio is small.\n    # If `bin_remain_cap` is just above `item`, this ratio is close to 1.\n    # We want to maximize this ratio, but not exceed 1.\n    # We can use a sigmoid that maps values close to 1 (but less than 1) to high scores.\n\n    # Let's refine the mismatch approach:\n    # We want to reward bins where `bin_remain_cap` is close to `item`.\n    # The difference `bin_remain_cap - item` should be small.\n    # A value close to 0 for this difference is good.\n    # Let's scale and shift the mismatch: `scaled_mismatch = (mismatch - mean_mismatch) / std_mismatch`\n    # Or, a simpler approach: normalize mismatch relative to some max possible mismatch.\n\n    # Sigmoid strategy: score = 1 / (1 + exp(-k * (target_val - current_val)))\n    # We want `current_val` (which is `mismatch`) to be close to `target_val` (e.g., 0).\n    # If `target_val = 0`, then `score = 1 / (1 + exp(-k * (-mismatch)))`.\n    # This gives high scores for negative `mismatch` (too small).\n    # This is not ideal because we already filter `mismatch < 0`.\n\n    # Let's try: `score = 1 / (1 + exp(-k * (mismatch)))`\n    # Mismatch = 0 -> score = 0.5\n    # Mismatch > 0 -> score < 0.5 (worse fit)\n    # Mismatch < 0 -> score > 0.5 (better fit)\n\n    # This still seems to penalize small positive mismatches.\n    # We need a function that peaks at mismatch = 0.\n\n    # Consider a Gaussian-like shape using exp(-x^2).\n    # `score = exp(-k * mismatch**2)`\n    # Mismatch = 0 -> score = 1\n    # Mismatch != 0 -> score < 1. This is a good candidate.\n    # This can be approximated with a sigmoid.\n\n    # Let's re-think the sigmoid target.\n    # We want `bin_remain_cap` to be just enough.\n    # If we focus on `bin_remain_cap`, we want it to be close to `item`.\n    # `score = sigmoid(k * (item - bin_remain_cap))`\n    # If `bin_remain_cap` is slightly larger than `item`, `item - bin_remain_cap` is small and negative.\n    # `sigmoid(small_negative_val)` is close to 0.\n    # If `bin_remain_cap` is much larger than `item`, `item - bin_remain_cap` is a large negative.\n    # `sigmoid(large_negative_val)` is close to 0.\n    # If `bin_remain_cap` is exactly `item`, `item - bin_remain_cap` is 0.\n    # `sigmoid(0)` is 0.5.\n\n    # This function favors bins with `item - bin_remain_cap` being small negative,\n    # which means `bin_remain_cap` is slightly larger than `item`.\n\n    # Let's define k as a parameter controlling sensitivity. A higher k means\n    # a sharper peak around the ideal fit.\n    k = 5.0  # Sensitivity parameter, can be tuned.\n\n    # Calculate the sigmoid score for available bins.\n    # We want `bin_remain_cap` to be as close to `item` as possible, but >= `item`.\n    # This means we want `bin_remain_cap - item` to be small and non-negative.\n    # Let's transform this difference to get a score.\n\n    # A score that peaks when `bin_remain_cap - item` is 0.\n    # `score = 1 / (1 + exp(-k * (max_possible_item_size - (bin_remain_cap - item))))`\n    # This seems complicated.\n\n    # Let's use the mismatch and map it.\n    # We want to reward small positive mismatch values.\n    # We can use a sigmoid with a shift.\n    # `score = 1 / (1 + exp(-k * (mismatch_target - mismatch)))`\n    # If `mismatch_target` is 0, then `score = 1 / (1 + exp(-k * (-mismatch)))`.\n    # This would give high scores for negative mismatch values.\n\n    # Let's consider what `bins_remain_cap - item` represents:\n    # Small positive value: good fit\n    # Large positive value: too much space, potentially wasted\n    # Zero: perfect fit\n\n    # We can try to penalize large positive values.\n    # `score = 1 - sigmoid(k * (mismatch))`\n    # `score = 1 - 1 / (1 + exp(-k * mismatch))`\n    # `score = exp(-k * mismatch) / (1 + exp(-k * mismatch))`\n    # If mismatch is 0, score = 0.5.\n    # If mismatch is large positive, score -> 0.\n    # If mismatch is large negative, score -> 1.\n\n    # This gives high scores to bins that are too small, which is counter-intuitive.\n\n    # The common way to implement a \"best fit\" using sigmoid is to target\n    # the difference `bin_remain_cap - item` being close to 0.\n    # `sigmoid(k * (target_val - x))` where x is the value being measured.\n    # Here, we want to measure `bin_remain_cap`.\n    # `target_val` would be `item`.\n    # So, `score = 1 / (1 + exp(-k * (item - bin_remain_cap)))`\n\n    # Let's re-evaluate `score = 1 / (1 + exp(-k * (bin_remain_cap - item)))`.\n    # If `bin_remain_cap == item` (mismatch=0), score = 0.5.\n    # If `bin_remain_cap` > `item` (mismatch > 0), e.g., `bin_remain_cap = item + delta`\n    # `score = 1 / (1 + exp(-k * delta))`. If delta > 0, exp(-k*delta) < 1.\n    # So `1 + exp < 2`, and `score > 0.5`. The larger delta, the smaller exp(-k*delta), closer to 0. So score gets closer to 1.\n    # This penalizes larger remaining capacity, which is WRONG.\n\n    # Let's flip it: `score = 1 / (1 + exp(-k * (item - bin_remain_cap)))`\n    # If `bin_remain_cap == item` (mismatch=0), score = 0.5.\n    # If `bin_remain_cap` > `item` (mismatch > 0), e.g., `bin_remain_cap = item + delta`\n    # `score = 1 / (1 + exp(-k * (-delta))) = 1 / (1 + exp(k * delta))`.\n    # If delta > 0, k*delta > 0. exp(k*delta) > 1. `1 + exp > 2`.\n    # So `score < 0.5`. The larger delta, the smaller the score. This is CORRECT.\n    # If `bin_remain_cap` < `item` (mismatch < 0), this case is filtered.\n    # If we didn't filter, and `bin_remain_cap = item - delta'`, where delta' > 0.\n    # `score = 1 / (1 + exp(-k * (item - (item - delta')))) = 1 / (1 + exp(k * delta'))`.\n    # If delta' > 0, k*delta' > 0. exp(k*delta') > 1. `1 + exp > 2`. So `score < 0.5`.\n    # This would give low scores to bins that are too small. This is also fine,\n    # but we already use a mask for this.\n\n    # So, the expression `1 / (1 + exp(-k * (item - bin_remain_cap)))` for `bin_remain_cap >= item`\n    # appears to be a reasonable Sigmoid Fit Score. It assigns higher scores to bins\n    # where `bin_remain_cap` is closer to `item`.\n\n    # We want to make sure the range of arguments to sigmoid is reasonable.\n    # If `bin_remain_cap` is very large, `item - bin_remain_cap` can be a large negative number.\n    # `exp(large_positive)` can overflow.\n    # If `bin_remain_cap` is exactly `item`, argument is 0.\n    # If `bin_remain_cap` is slightly larger than `item`, argument is small negative.\n\n    # To prevent potential issues with very large capacities and the sigmoid argument:\n    # We can clip the `bin_remain_cap` for calculating the argument, or scale.\n    # Alternatively, we can use `1 / (1 + exp(k * (bin_remain_cap - item)))`\n    # Let's check this:\n    # `score = 1 / (1 + exp(k * (bin_remain_cap - item)))`\n    # If `bin_remain_cap == item` (mismatch=0), score = 1 / (1 + exp(0)) = 1 / (1 + 1) = 0.5.\n    # If `bin_remain_cap` > `item` (mismatch > 0), e.g., `bin_remain_cap = item + delta`.\n    # `score = 1 / (1 + exp(k * delta))`. If delta > 0, k*delta > 0. exp(k*delta) > 1.\n    # `1 + exp > 2`. So `score < 0.5`. The larger delta, the smaller the score. CORRECT.\n    # This expression seems more numerically stable for large positive differences.\n\n    # Let's use this expression.\n    # We'll compute it for the available bins.\n\n    # Parameters for sigmoid: k (steepness)\n    k = 5.0 # Sensitivity. Higher k means score drops faster as capacity increases beyond item.\n\n    # Calculate the argument for the sigmoid: k * (remaining_capacity - item)\n    # We are interested in `bins_remain_cap >= item`.\n    # `argument = k * (available_bins_cap - item)`\n\n    # The sigmoid function: `sigmoid(x) = 1 / (1 + exp(-x))`\n    # Our chosen formula is `1 / (1 + exp(k * (bin_remain_cap - item)))`.\n    # Let `x = k * (bin_remain_cap - item)`. Then `sigmoid_val = 1 / (1 + exp(x))`.\n    # This is also equivalent to `1 - sigmoid(-x) = 1 - 1 / (1 + exp(-(-x))) = 1 - 1 / (1 + exp(x))`.\n    # So, it's 1 minus a standard sigmoid applied to `k * (bin_remain_cap - item)`.\n    # This shape is suitable: peaks at 0.5, decreases for positive arguments.\n\n    # Applying the sigmoid to the mismatches for the available bins\n    # We want to assign these calculated priorities back to the original `priorities` array.\n    sigmoid_scores = 1 / (1 + np.exp(k * (available_bins_cap - item)))\n\n    # Place the calculated sigmoid scores back into the priorities array\n    priorities[available_bins_mask] = sigmoid_scores\n\n    return priorities",
    "response_id": 26,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response8.txt_stdout.txt",
    "code_path": "problem_iter1_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using an epsilon-greedy strategy.\n\n    The strategy favors bins that are a \"good fit\" for the item (i.e., leaving\n    a small remaining capacity), but with a probability epsilon, it explores\n    other bins randomly.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.2  # Probability of exploring\n\n    n_bins = len(bins_remain_cap)\n    priorities = np.zeros(n_bins)\n\n    # Calculate a \"goodness of fit\" score for each bin\n    # A smaller remaining capacity after fitting the item is considered better.\n    # We use 1 / (remaining_capacity - item + 1e-9) to ensure division by zero is avoided\n    # and to give higher scores to bins with less remaining capacity.\n    # Only consider bins that can actually fit the item.\n    suitable_bins_mask = bins_remain_cap >= item\n    priorities[suitable_bins_mask] = 1.0 / (bins_remain_cap[suitable_bins_mask] - item + 1e-9)\n\n    # Epsilon-greedy part: with probability epsilon, choose a random suitable bin\n    if np.random.rand() < epsilon:\n        # Get indices of suitable bins\n        suitable_indices = np.where(suitable_bins_mask)[0]\n        if len(suitable_indices) > 0:\n            # Randomly pick one suitable bin\n            chosen_index = np.random.choice(suitable_indices)\n            # Assign a very high priority to the randomly chosen bin\n            priorities[chosen_index] = np.max(priorities) + 1.0 # Give it a slightly higher priority than the best greedy choice\n    else:\n        # Otherwise, follow the greedy approach (already calculated in `priorities`)\n        pass # priorities are already set to the greedy scores\n\n    return priorities",
    "response_id": 8,
    "obj": 4.248105305145606,
    "SLOC": 14.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.\n\n    The strategy assigns higher priority to bins that have a remaining capacity\n    just slightly larger than the item size, aiming to fill bins more compactly.\n    A temperature parameter controls the \"softness\" of the softmax, influencing\n    how aggressively we favor these \"almost fitting\" bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Filter out bins that cannot fit the item\n    can_fit_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if not eligible_bins_cap.size:\n        # If no bin can fit the item, return zeros for all original bins\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit score\": how much space is left after placing the item.\n    # Smaller values indicate a tighter fit.\n    fit_scores = eligible_bins_cap - item\n\n    # Use a Softmax-like approach to convert fit scores to priorities.\n    # We invert the fit scores to give higher priority to smaller remaining capacities (tighter fits).\n    # Adding a small epsilon to avoid division by zero or log(0) if all fit_scores are 0.\n    epsilon = 1e-9\n    inverted_fit_scores = 1.0 / (fit_scores + epsilon)\n\n    # The temperature parameter controls the \"softness\" of the softmax.\n    # A lower temperature makes the distribution sharper (more peaky),\n    # favoring the best fitting bins more strongly.\n    # A higher temperature makes it more uniform.\n    temperature = 0.5  # This can be tuned as a hyperparameter\n\n    # Apply softmax to the inverted fit scores\n    try:\n        exp_scores = np.exp(inverted_fit_scores / temperature)\n        softmax_priorities = exp_scores / np.sum(exp_scores)\n    except OverflowError:\n        # Handle potential overflow if scores become too large\n        # In such cases, a simple proportional scaling might be better\n        # or clamping the input to exp. For simplicity here, we can\n        # assign equal high probability to all if overflow occurs.\n        softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size\n\n\n    # Create the final priority array, mapping priorities back to original bin indices\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[can_fit_mask] = softmax_priorities\n\n    return priorities",
    "response_id": 19,
    "obj": 4.048663741523748,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response10.txt_stdout.txt",
    "code_path": "problem_iter1_code10.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit Decreasing-like heuristic.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # First Fit strategy: find the first bin that can accommodate the item.\n    # For priority, we want to favor bins that are a \"tight fit\" but still fit.\n    # This means we prefer bins where the remaining capacity is just enough for the item.\n    # A bin with remaining capacity exactly equal to the item size is ideal.\n    # Bins that are too small should have a priority of 0.\n\n    # Create a mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities to a very low value (effectively zero for unusable bins)\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # For bins that can fit the item, calculate a priority score.\n    # We want to prioritize bins where the remaining capacity is closest to the item size,\n    # without going below the item size.\n    # The difference (bins_remain_cap - item) represents the \"slack\".\n    # We want to minimize slack, so a smaller difference is better.\n    # However, we want the *first* such bin in the array to be prioritized in case of ties,\n    # which is naturally handled by numpy's vectorized operations if we consider\n    # negative of the slack as a priority. A smaller slack means a larger negative slack,\n    # which translates to a higher priority in a max-priority queue sense.\n\n    # Calculate slack for bins that can fit the item\n    slack = bins_remain_cap[can_fit_mask] - item\n\n    # The priority is the negative of the slack.\n    # Smaller slack -> larger negative slack -> higher priority.\n    # If multiple bins have the same slack, their relative order in the original\n    # bins_remain_cap array will be preserved in terms of priority calculation.\n    priorities[can_fit_mask] = -slack\n\n    # In a true First Fit, we'd just take the first bin that fits.\n    # To simulate this \"first fit\" behavior in a priority context,\n    # we can add a small bonus to earlier bins with good fits, or more directly,\n    # simply return priorities such that the first available bin with the \"best\" fit\n    # (smallest slack) gets the highest priority. The `-slack` already achieves this.\n    # If multiple bins have the same minimal slack, the one appearing first in the\n    # `bins_remain_cap` array will naturally get the higher priority due to the way\n    # numpy operations often preserve order in selection when values are equal.\n    # For absolute certainty of 'first fit' logic within this priority framework,\n    # we can make bins that are \"exact fits\" have a slightly higher priority\n    # than slightly looser fits.\n\n    # Refined priority: Exact fits (slack=0) get highest priority.\n    # Then, among bins that fit, prioritize smaller slack (tighter fit).\n    # The current -slack already prioritizes smaller slack.\n    # To enforce the \"first fit\" aspect: consider the index.\n    # A bin with smaller index is preferred if slack is equal.\n\n    # Let's use a more explicit priority:\n    # 1. Highest priority for exact fits.\n    # 2. Then, prioritize bins with smaller slack.\n    # 3. If slack is equal, prioritize the bin with the smaller index.\n\n    # Initialize priorities for fitting bins\n    fit_priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Calculate slack for fitting bins\n    slack_values = bins_remain_cap[can_fit_mask] - item\n\n    # Assign priorities:\n    # For exact fits (slack_values == 0), assign a very high priority (e.g., 1e9)\n    # For other fits, assign priority based on negative slack.\n    # To break ties and enforce \"first fit\", we can penalize later bins.\n    # Let's assign priority = 10000 - slack - (index * 0.1) for fitting bins.\n    # This way, smaller slack is better, and smaller index is better for same slack.\n\n    indices = np.where(can_fit_mask)[0]\n    # Create a score: (ideal_fit - slack) + (bonus_for_early_bins)\n    # We want to maximize this score.\n    # Ideal fit = 0 (when remaining_capacity == item)\n    # So, priority component from slack is -slack.\n    # Bonus for early bins: -index * small_constant.\n    # We want to maximize priority.\n    # Maximize: (-slack) - (index * 0.01)\n    \n    # A simpler way is to use a very large number for exact fits, then -slack.\n    # Let's consider the \"best\" fit for the priority.\n    # The best fit is the one that minimizes `remaining_capacity - item`.\n    # This is equivalent to maximizing `-(remaining_capacity - item)`.\n    # So, `priority = -(remaining_capacity - item)` for fitting bins.\n    # To ensure first-fit, if there are multiple bins with the same minimal slack,\n    # the one with the lower index should be preferred.\n    # We can achieve this by adding a very small penalty to the priority based on index.\n    # `priority = -(remaining_capacity - item) - index * epsilon`\n    # where epsilon is a very small positive number. This ensures that a bin at a\n    # lower index with the same slack gets a slightly higher priority.\n\n    epsilon = 1e-6  # Small value to break ties for first-fit\n    fit_priorities[can_fit_mask] = -(slack_values) - (indices * epsilon)\n    \n    # We want the bin with the highest priority score to be selected.\n    # The current calculation `-(slack_values) - (indices * epsilon)` will work.\n    # A more direct \"First Fit Decreasing-like\" priority would be to assign\n    # priorities such that the smallest slack is maximized.\n    # And for ties in slack, the lowest index is maximized.\n    # So, `priority = (some_large_number - slack) - index * epsilon`.\n    # Or simply, `priority = -slack - index * epsilon`.\n    # A higher value means higher priority.\n\n    # Final check: The priority should reflect our preference.\n    # We prefer bins that are a tight fit, and among tight fits, the earliest one.\n    # This means a bin where `bins_remain_cap - item` is small is good.\n    # And smaller index is good for ties.\n    # So, the score should be high for small `bins_remain_cap - item` and small `index`.\n    # Let's use a score that is large for best fits:\n    # Priority = MAX_SCORE - (bins_remain_cap - item) - (index * penalty)\n    # A large MAX_SCORE ensures any fitting bin is better than non-fitting.\n    \n    MAX_BENEFIT_SCORE = 1000000  # A large number to indicate a good fit\n    INDEX_PENALTY_FACTOR = 1000  # Penalty for later bins\n\n    # Prioritize bins that can fit the item.\n    # For bins that fit, the score is determined by how \"tight\" the fit is\n    # (smaller remaining capacity after placement is better) and by their index\n    # (earlier bins are preferred for first-fit).\n\n    # Calculate the \"tightness\" score: a larger value means a tighter fit (less waste).\n    # We want to maximize `MAX_BENEFIT_SCORE - slack`.\n    tightness_score = np.zeros_like(bins_remain_cap)\n    tightness_score[can_fit_mask] = MAX_BENEFIT_SCORE - slack\n\n    # Introduce a penalty for bins with higher indices to enforce the \"first fit\" logic.\n    # Subtract a scaled index. Smaller index should have higher priority.\n    index_penalty = indices * INDEX_PENALTY_FACTOR\n    \n    # Combine scores. We want to maximize the overall priority.\n    # Prioritize based on tightness, then index.\n    # This means higher tightness_score is better.\n    # Lower index_penalty is better.\n    # So, we want to MAXIMIZE: `tightness_score - index_penalty`\n    \n    priorities = np.full_like(bins_remain_cap, -np.inf) # Initialize with low priority\n\n    if np.any(can_fit_mask):\n        fitting_indices = np.where(can_fit_mask)[0]\n        fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        \n        # Calculate the \"fit quality\": how close the remaining capacity is to the item size.\n        # A smaller difference (bins_remain_cap - item) is better.\n        fit_quality = -(fitting_bins_remain_cap - item) # Maximize this (smaller difference is better)\n\n        # To implement \"First Fit\", we prefer earlier bins when fit quality is the same.\n        # We can achieve this by adding a small bonus for earlier indices.\n        # This means we want to maximize `fit_quality + bonus_for_early_bins`.\n        # The bonus should decrease with index. So, `bonus = constant - index * small_factor`.\n        # Let's use a simple approach: a very high priority for the tightest fits,\n        # and then penalize for looser fits and later indices.\n\n        # Consider the inverse: we want to minimize waste (slack) and index.\n        # So, we want to minimize `slack + index * epsilon`.\n        # Priority should be inverse of this minimization.\n        # Priority = - (slack + index * epsilon)\n\n        epsilon_tiebreaker = 1e-6\n        priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item) - (np.arange(len(bins_remain_cap))[can_fit_mask] * epsilon_tiebreaker)\n\n    # The current priority definition maximizes (tight fit) + (early index).\n    # This correctly aligns with First Fit logic.\n    return priorities",
    "response_id": 10,
    "obj": 4.048663741523748,
    "SLOC": 23.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response14.txt_stdout.txt",
    "code_path": "problem_iter1_code14.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First strategy.\n\n    The Exact Fit First strategy prioritizes bins where the remaining capacity is exactly equal\n    to the item size. Bins that can fit the item but not exactly are given a lower priority,\n    with larger remaining capacities being less preferred. Bins that cannot fit the item\n    receive zero priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Prioritize bins where remaining capacity is exactly the item size\n    exact_fit_mask = (bins_remain_cap == item) & can_fit_mask\n    priorities[exact_fit_mask] = 1.0\n\n    # For bins that can fit but not exactly, give a lower priority\n    # We want to penalize bins with a lot of leftover space after placing the item.\n    # So, we assign a priority that decreases as (remaining_capacity - item_size) increases.\n    # A simple way is to use 1 / (remaining_capacity - item_size + 1) to avoid division by zero\n    # and ensure non-zero priorities for valid fits.\n    partial_fit_mask = (~exact_fit_mask) & can_fit_mask\n    remaining_space_after_fit = bins_remain_cap[partial_fit_mask] - item\n    priorities[partial_fit_mask] = 1.0 / (remaining_space_after_fit + 1) # Adding 1 to avoid division by zero if remaining_space is 0, which is handled by exact_fit_mask anyway.\n\n    # Ensure that exact fits have higher priority than partial fits.\n    # Since we set exact fits to 1.0, and partial fits to values < 1.0 (as remaining_space_after_fit >= 0),\n    # this condition is naturally met.\n\n    return priorities",
    "response_id": 14,
    "obj": 4.198244914240141,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response29.txt_stdout.txt",
    "code_path": "problem_iter1_code29.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    This heuristic prioritizes bins that have a remaining capacity slightly larger than the item,\n    aiming to minimize wasted space. It uses a softmax function to convert these differences\n    into probabilities, effectively assigning higher priority to bins that are a \"good fit\".\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the \"fit\" of the item into each bin.\n    # We want bins where bins_remain_cap is just enough or slightly more than the item.\n    # A negative value here means the item doesn't fit. We can clamp these to a small\n    # positive value or 0 to avoid issues with softmax if all items don't fit.\n    # A large positive difference (item fits easily) is also not ideal as it wastes space.\n    # So we want the difference (bins_remain_cap - item) to be close to zero.\n    # We can use the negative of this difference as the exponent in softmax,\n    # so smaller (bins_remain_cap - item) results in a higher exponent.\n    fits = bins_remain_cap - item\n\n    # Filter out bins where the item does not fit (remaining capacity < item size)\n    # Assign a very low priority (or effectively zero) to these bins.\n    # A large negative number in softmax exponent will result in a value close to 0.\n    # We can also directly set their fits to a very low value before softmax.\n    fits[fits < 0] = -np.inf # Effectively 0 probability after softmax\n\n    # Apply the softmax function. The exponent in softmax should reflect desirability.\n    # We want bins with (bins_remain_cap - item) close to 0 to have high priority.\n    # So, we can use -(bins_remain_cap - item) as the exponent, or more intuitively,\n    # (item - bins_remain_cap) as the exponent, ensuring negative values become smaller.\n    # Or even better, prioritize bins with a positive difference close to zero.\n    # Let's define desirability as: higher is better if remaining_cap is slightly > item.\n    # Consider a transformation: exp(-abs(fits)). This would favor fits near 0.\n    # However, softmax typically takes logits (raw scores).\n    # Let's consider the score `s_i = -(bins_remain_cap_i - item)^2`. This penalizes\n    # being too small or too large. But we only care about `bins_remain_cap_i >= item`.\n    # So, if `bins_remain_cap_i < item`, priority should be 0.\n    # If `bins_remain_cap_i >= item`, we want `bins_remain_cap_i - item` to be small.\n    # A good transformation for softmax could be to map `bins_remain_cap_i - item` to a\n    # desirability score. Higher `bins_remain_cap_i` than `item` is good, but not too high.\n    # Let's use `bins_remain_cap_i - item` directly as logits, but only for bins that can fit.\n    # For bins that cannot fit, their logit should be extremely low.\n\n    # Option 1: Softmax on (bins_remain_cap - item) for fitting bins.\n    # Prioritize bins where remaining capacity is *exactly* the item size.\n    # Using -fits will invert the ordering, so smaller positive diffs become larger.\n    # exp(-fits) where fits = bins_remain_cap - item\n    # If bins_remain_cap = 5, item = 3, fit = 2. exp(-2) = 0.135\n    # If bins_remain_cap = 3, item = 3, fit = 0. exp(0) = 1.0\n    # If bins_remain_cap = 6, item = 3, fit = 3. exp(-3) = 0.049\n    # This prioritizes exact fits.\n\n    # We need to ensure that we don't assign probabilities to bins where the item doesn't fit.\n    # A common way to do this with softmax is to assign a very low logit (large negative number).\n    logits = bins_remain_cap - item\n    # For bins that cannot fit the item, set their logit to a very small number.\n    # This will make their softmax probability close to zero.\n    logits[logits < 0] = -1e9  # A large negative number\n\n    # For bins that can fit, we want to prioritize those with smaller remaining capacity (closer to item size).\n    # So, we want to exponentiate -(bins_remain_cap - item) which is (item - bins_remain_cap).\n    # However, the softmax input should ideally be positive values that represent scores.\n    # Let's redefine `scores`: a higher score means better fit.\n    # A perfect fit would have score X.\n    # A slightly larger capacity would have score X - epsilon.\n    # A much larger capacity would have score X - delta.\n    # A capacity smaller than item would have score -infinity.\n    # So, we can use `item - bins_remain_cap` as our underlying measure for exponentiation,\n    # but we need to handle the case where `bins_remain_cap < item`.\n\n    # Let's use a modified approach. We want bins that *can* fit, and among those,\n    # we prefer bins with less excess capacity.\n    # So, a \"goodness\" score could be:\n    # -infinity if item > bins_remain_cap\n    # -(bins_remain_cap - item) otherwise (prioritizing smaller differences)\n\n    scores = -(bins_remain_cap - item)\n    scores[bins_remain_cap < item] = -np.inf # Ensure items not fitting get no chance\n\n    # Now apply softmax. Softmax(x_i) = exp(x_i) / sum(exp(x_j)).\n    # The `scores` directly go into the exponent of softmax.\n    # Higher scores (closer to 0, or less negative) mean higher probability.\n    # If scores = [-inf, -inf, 0, -2, -5],\n    # exp(scores) = [0, 0, 1, exp(-2), exp(-5)]\n    # Softmax will normalize these.\n\n    # Add a small constant to the score for bins that can fit, to ensure\n    # that even a small positive difference doesn't get zero probability.\n    # For instance, if we use -(bins_remain_cap - item), a difference of 10\n    # gives exp(-10) which is tiny.\n    # Perhaps a linear scaling or a different transformation is better.\n    # Let's try mapping `bins_remain_cap - item` to a desirability score.\n    # If diff = 0, score = K (high)\n    # If diff = 1, score = K - epsilon\n    # If diff = 10, score = K - delta\n    # If diff < 0, score = -infinity\n\n    # Let's re-think the logit construction for softmax.\n    # We want the probability P_i proportional to exp(logit_i).\n    # Desirability of bin i for item: D_i\n    # P_i = exp(alpha * D_i) / sum(exp(alpha * D_j))\n    # If item doesn't fit: D_i = -infinity\n    # If item fits and diff = bins_remain_cap - item:\n    # We want smaller diffs to have higher D_i.\n    # So, D_i = -(bins_remain_cap - item) = item - bins_remain_cap.\n\n    # This approach correctly prioritizes bins with less remaining capacity over\n    # bins with more remaining capacity, for those that can fit the item.\n    # The `-np.inf` for non-fitting bins ensures they get 0 probability.\n    # The `alpha` parameter (implicitly 1 here) controls the \"peakiness\" of the distribution.\n\n    # Calculate the underlying scores for softmax.\n    # A higher score means more desirable.\n    # If bins_remain_cap >= item, we want bins_remain_cap - item to be small.\n    # So, let's use `-(bins_remain_cap - item)` which is `item - bins_remain_cap`.\n    # For bins where `bins_remain_cap < item`, the score should be very low.\n\n    scores = item - bins_remain_cap\n\n    # Set scores to a very low value for bins where the item does not fit.\n    # This ensures their probability contribution in softmax is negligible.\n    scores[bins_remain_cap < item] = -1e9 # Effectively -infinity for softmax\n\n    # Calculate probabilities using softmax.\n    # Adding 1 to scores for fitting bins can help avoid all zeros if all diffs are large negative\n    # but we've handled that with -1e9 for non-fitting bins.\n    # The softmax calculation itself: exp(scores) / sum(exp(scores))\n    # We can use np.exp directly, but be mindful of overflow/underflow if scores are very large/small.\n    # If scores = [-1e9, -1e9, 0, -2, -5], then exp(scores) = [0, 0, 1, exp(-2), exp(-5)]\n    # This seems correct.\n    exp_scores = np.exp(scores)\n\n    # If all items resulted in -1e9 (meaning item doesn't fit any bin), exp_scores will be all zeros.\n    # In this case, the sum will be zero, leading to division by zero. Handle this edge case.\n    sum_exp_scores = np.sum(exp_scores)\n    if sum_exp_scores == 0:\n        # This means the item cannot fit into any available bin.\n        # Return zero priorities for all bins.\n        return np.zeros_like(bins_remain_cap)\n\n    priorities = exp_scores / sum_exp_scores\n\n    return priorities",
    "response_id": 29,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response18.txt_stdout.txt",
    "code_path": "problem_iter1_code18.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    The Epsilon-Greedy strategy aims to balance exploration (trying less optimal bins)\n    and exploitation (choosing the best bin).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.2  # Probability of exploration\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins = np.where(suitable_bins_mask)[0]\n\n    if len(suitable_bins) == 0:\n        # If no bin can fit the item, return all zeros (or handle as an error)\n        return priorities\n\n    # --- Exploitation Component ---\n    # Calculate a \"goodness\" score for suitable bins.\n    # A common heuristic is the \"Best Fit\" approach: prioritize bins with\n    # the least remaining capacity after placing the item (minimizing waste).\n    # Here, we want the *highest* priority for the *best* fit, so we\n    # transform the remaining capacity difference into a positive score.\n    # A simple approach is (max_capacity - item) - remaining_capacity\n    # or more directly, prioritize smaller remaining capacities after fitting.\n    # We can use a value inversely related to remaining capacity, e.g., 1 / (remaining_capacity - item + 1e-6)\n    # to give higher priority to bins with less slack.\n\n    # Calculate the 'fit_score' for suitable bins: higher is better fit (less wasted space)\n    # This is 1 / (remaining_capacity - item + small_epsilon)\n    # A bin with remaining_capacity = item will have the highest score.\n    fit_scores = 1 / (bins_remain_cap[suitable_bins] - item + 1e-6)\n\n    # Normalize fit_scores to a 0-1 range (optional but can be helpful)\n    if fit_scores.max() > fit_scores.min():\n        exploitation_priorities = (fit_scores - fit_scores.min()) / (fit_scores.max() - fit_scores.min())\n    else:\n        exploitation_priorities = np.ones(len(suitable_bins)) # All are equally \"good\"\n\n    # Assign exploitation priorities to the suitable bins\n    priorities[suitable_bins] = exploitation_priorities\n\n    # --- Exploration Component ---\n    # Introduce randomness: with probability epsilon, choose a random suitable bin.\n    # Assign a small, uniform \"exploration\" priority to all suitable bins.\n    # This encourages trying out bins that might not be the immediate \"best fit\".\n    exploration_priority_value = 0.1 # A small constant value to represent exploration\n\n    # For the bins that are suitable, decide whether to explore\n    explore_mask = np.random.rand(len(suitable_bins)) < epsilon\n\n    # Update priorities for bins chosen for exploration\n    priorities[suitable_bins[explore_mask]] = exploration_priority_value\n\n    # Normalize the final priorities to ensure a meaningful range, e.g., 0 to 1\n    # (This step might be adjusted based on how the priority scores are used elsewhere)\n    if priorities.max() > priorities.min():\n        final_priorities = (priorities - priorities.min()) / (priorities.max() - priorities.min())\n    else:\n        final_priorities = np.ones(num_bins) * 0.5 # Default if all are same\n\n    # Ensure that bins that cannot fit the item have zero priority\n    final_priorities[~suitable_bins_mask] = 0\n\n    return final_priorities",
    "response_id": 18,
    "obj": 4.15835660151576,
    "SLOC": 23.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response28.txt_stdout.txt",
    "code_path": "problem_iter1_code28.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    This heuristic prioritizes bins that can fit the item and are \"nearly full\"\n    (i.e., have little remaining capacity after fitting the item), as this\n    encourages tighter packing. A small random exploration component is added\n    to avoid getting stuck in local optima.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of random exploration\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate scores for bins that can fit the item\n    # We want to prioritize bins that leave little remaining capacity\n    # \"nearly full\" bins are good candidates.\n    # The score is the inverse of the remaining capacity after fitting the item.\n    # A smaller remaining capacity results in a higher score.\n    remaining_capacity_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Avoid division by zero or very small numbers which can lead to very large priorities.\n    # A small epsilon is added to the denominator.\n    # Alternatively, we could cap the priorities or use a different scoring mechanism.\n    scores = 1.0 / (remaining_capacity_after_fit + 1e-9) \n    \n    # Normalize scores to be between 0 and 1 for better epsilon-greedy application\n    if scores.size > 0:\n        min_score = np.min(scores)\n        max_score = np.max(scores)\n        if max_score > min_score:\n            priorities[can_fit_mask] = (scores - min_score) / (max_score - min_score)\n        else:\n            # If all scores are the same, assign a neutral priority\n            priorities[can_fit_mask] = 0.5\n    \n    # Epsilon-Greedy exploration: with probability epsilon, choose a random bin that fits\n    if np.random.rand() < epsilon:\n        possible_bins = np.where(can_fit_mask)[0]\n        if possible_bins.size > 0:\n            random_bin_index = np.random.choice(possible_bins)\n            priorities = np.zeros_like(bins_remain_cap, dtype=float)\n            priorities[random_bin_index] = 1.0 # Assign highest priority to the randomly chosen bin\n    \n    return priorities",
    "response_id": 28,
    "obj": 4.148384523334677,
    "SLOC": 20.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  }
]