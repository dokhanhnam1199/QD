{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First strategy.\n\n    The Exact Fit First strategy prioritizes bins where the remaining capacity is exactly equal\n    to the item size. Bins that can fit the item but not exactly are given a lower priority,\n    with larger remaining capacities being less preferred. Bins that cannot fit the item\n    receive zero priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Prioritize bins where remaining capacity is exactly the item size\n    exact_fit_mask = (bins_remain_cap == item) & can_fit_mask\n    priorities[exact_fit_mask] = 1.0\n\n    # For bins that can fit but not exactly, give a lower priority\n    # We want to penalize bins with a lot of leftover space after placing the item.\n    # So, we assign a priority that decreases as (remaining_capacity - item_size) increases.\n    # A simple way is to use 1 / (remaining_capacity - item_size + 1) to avoid division by zero\n    # and ensure non-zero priorities for valid fits.\n    partial_fit_mask = (~exact_fit_mask) & can_fit_mask\n    remaining_space_after_fit = bins_remain_cap[partial_fit_mask] - item\n    priorities[partial_fit_mask] = 1.0 / (remaining_space_after_fit + 1) # Adding 1 to avoid division by zero if remaining_space is 0, which is handled by exact_fit_mask anyway.\n\n    # Ensure that exact fits have higher priority than partial fits.\n    # Since we set exact fits to 1.0, and partial fits to values < 1.0 (as remaining_space_after_fit >= 0),\n    # this condition is naturally met.\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority scores for packing an item into bins using a refined\n    priority function incorporating Softmax for exploration, bin scarcity,\n    and a \"best fit\" preference.\n\n    This heuristic aims to balance:\n    1.  **Best Fit Preference:** Prioritize bins where the remaining capacity is\n        closest to the item size, minimizing waste. This is achieved by\n        penalizing excess capacity.\n    2.  **Bin Scarcity:** Favor bins that have less remaining capacity overall,\n        as these are considered \"scarcer\" resources.\n    3.  **Softmax for Probabilistic Selection:** Uses Softmax to convert scores\n        into probabilities, allowing for exploration of less optimal bins.\n        The temperature parameter controls the degree of exploration.\n\n    The combined score for a suitable bin is a weighted sum of the\n    \"anti-waste\" score and the bin scarcity score.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as `bins_remain_cap`, where each element\n        is the probability score (from Softmax) for the corresponding bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_cap.size == 0:\n        return priorities\n\n    # --- Heuristic Component 1: \"Anti-Waste\" Score ---\n    # We want bins where `bins_remain_cap - item` is minimized.\n    # A good score would be the negative difference, so smaller differences are less negative (higher score).\n    # Or, we can use a penalty function that increases with the difference.\n    # Let's use the negative difference for simplicity, which is `item - bins_remain_cap`.\n    # To make it more conducive to softmax (avoiding large negative numbers), we can transform it.\n    # A simple approach is `-(bins_remain_cap - item)`.\n    # To ensure scores are positive and have a decreasing trend as difference increases,\n    # we can use something like `1 / (1 + (bins_remain_cap - item))`.\n    # Let's use `item - suitable_bins_cap` as a base score, where smaller positive values are better.\n    # We'll apply a transformation to ensure scores are in a reasonable range for softmax.\n    \n    # Calculate the \"waste\"\n    waste = suitable_bins_cap - item\n    \n    # Transform waste into an \"anti-waste\" score. We want smaller waste to have higher scores.\n    # Using a steepness parameter `k_waste` to control sensitivity to waste.\n    k_waste = 5.0\n    \n    # Employ a function that maps small positive waste to high scores and larger waste to lower scores.\n    # An exponential decay or a sigmoid-like function is suitable.\n    # Let's use `exp(-k_waste * waste)`. This maps [0, inf) to (0, 1].\n    # Clamp waste to avoid extremely small negative values that could lead to exp overflow.\n    # However, waste is already guaranteed to be >= 0 here.\n    anti_waste_scores = np.exp(-k_waste * waste)\n\n    # --- Heuristic Component 2: Bin Scarcity Score ---\n    # Prioritize bins with less remaining capacity.\n    # Using `1 / (capacity + epsilon)` gives higher scores to smaller capacities.\n    epsilon_scarcity = 1e-6\n    scarcity_scores = 1 / (suitable_bins_cap + epsilon_scarcity)\n    \n    # Normalize scarcity scores to a [0, 1] range to be comparable with anti_waste_scores.\n    min_scarcity = np.min(scarcity_scores)\n    max_scarcity = np.max(scarcity_scores)\n    \n    # Handle case where all scarcity scores are identical to avoid division by zero\n    if max_scarcity - min_scarcity > epsilon_scarcity:\n        normalized_scarcity_scores = (scarcity_scores - min_scarcity) / (max_scarcity - min_scarcity)\n    else:\n        # If all suitable bins have the same scarcity, assign a neutral score (e.g., 0.5)\n        # or 0 if we want to de-emphasize them when they are not distinct.\n        # Let's assign 0, as scarcity doesn't differentiate them.\n        normalized_scarcity_scores = np.zeros_like(scarcity_scores)\n\n    # --- Combine Heuristics ---\n    # Weighted sum. These weights can be tuned based on empirical performance.\n    # Let's give more weight to minimizing waste, as it's often a primary goal in BPP.\n    weight_anti_waste = 0.7\n    weight_scarcity = 0.3\n    \n    combined_scores = (weight_anti_waste * anti_waste_scores) + (weight_scarcity * normalized_scarcity_scores)\n\n    # --- Softmax for Probabilistic Exploration ---\n    # Temperature parameter: Controls exploration.\n    # Lower temperature -> greedy (closer to argmax).\n    # Higher temperature -> more exploration (probabilities closer to uniform).\n    temperature = 0.7  # Tunable parameter\n\n    # Apply Softmax with numerical stability\n    scores_for_softmax = combined_scores / temperature\n    \n    # Subtract max score for numerical stability in exp.\n    # This ensures the largest value in `stable_scores` is 0.\n    stable_scores = scores_for_softmax - np.max(scores_for_softmax)\n    \n    exp_scores = np.exp(stable_scores)\n    sum_exp_scores = np.sum(exp_scores)\n    \n    # Handle cases where sum_exp_scores might be zero due to underflow (very unlikely with stable_scores)\n    if sum_exp_scores == 0:\n        # Fallback: uniform probability if all exp_scores are effectively zero.\n        probabilities = np.ones_like(combined_scores) / combined_scores.size\n    else:\n        probabilities = exp_scores / sum_exp_scores\n\n    # Place the calculated probabilities back into the main priorities array\n    priorities[suitable_bins_mask] = probabilities\n\n    return priorities\n\n[Reflection]\n1.  **Combine \"best fit\" with \"scarcity\" for a richer score.**\n2.  **Use Softmax for exploration, balancing greediness and variety.**\n3.  **Tune weights and temperature based on performance.**\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}