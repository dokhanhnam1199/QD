```python
def priority_v2(item: float, bins_remain_cap: np.ndarray, temperature: float = 1.0) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a softmax-based heuristic prioritizing tight fits.

    This heuristic prioritizes bins that can accommodate the item, with a stronger
    preference for "tight fits" (bins with remaining capacity close to the item size).
    The `temperature` parameter controls the exploration vs. exploitation trade-off.
    Higher temperatures lead to more uniform probabilities (more exploration), while
    lower temperatures focus on the best-fitting bins (more exploitation).
    Bins that are too small for the item receive a priority of 0.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.
        temperature: Controls the sharpness of the softmax distribution. A higher
                     value leads to more exploration, a lower value to more exploitation.

    Return:
        Array of same size as bins_remain_cap with priority score (probability) of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins that can fit the item
    can_fit_mask = bins_remain_cap >= item

    # For bins that can fit, calculate a "tightness" score.
    # We want to prioritize bins where the remaining capacity is just enough for the item.
    # The "wasted space" is `bins_remain_cap[can_fit_mask] - item`.
    # We want to minimize this wasted space.
    # A score that increases as wasted space decreases is desirable for softmax.
    # `1.0 / (1.0 + wasted_space)` is a good candidate:
    # - If wasted_space = 0 (perfect fit), score = 1.0
    # - If wasted_space is small positive (tight fit), score is close to 1.0
    # - If wasted_space is large, score approaches 0.0
    wasted_space = bins_remain_cap[can_fit_mask] - item
    
    # Ensure we don't have negative wasted_space (already handled by can_fit_mask)
    # and add a small epsilon to prevent division by zero if we were to use 1/wasted_space directly.
    # The `1.0 / (1.0 + wasted_space)` transformation avoids this.
    
    # Calculate exponentiated scores for softmax.
    # The `temperature` parameter controls the steepness of the probability distribution.
    # A low temperature makes the probabilities sharply peaked on the best-fitting bins (exploitation).
    # A high temperature makes the probabilities more uniform across all suitable bins (exploration).
    if temperature <= 0:
        raise ValueError("Temperature must be a positive value.")

    # Using `np.exp(score / temperature)` for softmax
    # The scores themselves represent how "good" a fit is.
    # Higher score for smaller `wasted_space`.
    # So, we want `scores_for_softmax` to be higher for tighter fits.
    # `1.0 / (1.0 + wasted_space)` does this.
    # For example, if item=5, bin_caps=[10, 12, 15]
    # Suitable bins: [10, 12, 15]
    # Wasted_space: [5, 7, 10]
    # Scores: [1/6, 1/8, 1/11] which are ~[0.167, 0.125, 0.091]
    # This prioritization might be inverted. We want tighter fits to have HIGHER priority.
    # Let's rethink the score.
    # We want to maximize `-(bins_remain_cap - item)`. This is `item - bins_remain_cap`.
    # This value is 0 for a perfect fit, and negative for other fits. Higher is better.
    # Let's use `item - bins_remain_cap[can_fit_mask]` as the raw score.
    # This directly reflects the "tightness" where 0 is the tightest.
    # For softmax, we want positive values for better fits.
    # Let's try `1.0 / (1.0 + (bins_remain_cap[can_fit_mask] - item))` again.
    # Wasted_space: [5, 7, 10]
    # Scores: [1/6, 1/8, 1/11] -> these are decreasing. This implies less preference for tighter fits.

    # Correct approach for "tight fit": prioritize bins where `bins_remain_cap - item` is minimized.
    # Let `diff = bins_remain_cap[can_fit_mask] - item`. We want to minimize `diff`.
    # For softmax, we need values where higher means better.
    # So, we need a function f(diff) such that f(diff) is higher when `diff` is smaller (and positive).
    # `1.0 / (1.0 + diff)`: If diff=0, score=1. If diff=5, score=1/6. This IS what we want.
    # The issue might be in how softmax is applied or interpreted.
    # Let's consider the negative of diff for higher values for better fits: `-(bins_remain_cap[can_fit_mask] - item)`.
    # This gives [ -5, -7, -10 ]. The largest value (-5) corresponds to the tightest fit.
    # For softmax, we want positive scores that are higher for better fits.
    # So, `1.0 - (bins_remain_cap[can_fit_mask] - item) / max_wasted_space`? Normalization is tricky.

    # Let's use a score that is directly proportional to the quality of the fit,
    # where "quality" means minimal remaining capacity after packing.
    # We want to maximize `-(bins_remain_cap[can_fit_mask] - item)`.
    # To ensure positive scores for softmax, we can shift this:
    # `max_wasted_space - (bins_remain_cap[can_fit_mask] - item)`
    # Or simpler: `item - bins_remain_cap[can_fit_mask]` (which is negative or zero).
    # To make it positive and larger for better fits: `(item - bins_remain_cap[can_fit_mask]) - min_diff`
    # where `min_diff` is the most negative value of `item - bins_remain_cap[can_fit_mask]`.
    # Or, just use `-(bins_remain_cap[can_fit_mask] - item)` directly. The exponentiation will handle it.
    # Let's try this score: `scores_for_softmax = -(bins_remain_cap[can_fit_mask] - item)`
    # Example: item=5, bin_caps=[10, 12, 15]
    # Suitable bins: [10, 12, 15]
    # Wasted_space: [5, 7, 10]
    # Scores: [-5, -7, -10]. The highest score is -5, corresponding to the tightest fit.
    
    scores_for_softmax = item - bins_remain_cap[can_fit_mask]

    # Apply softmax
    # Stable softmax: subtract max score before exp to prevent overflow
    max_score = np.max(scores_for_softmax)
    exp_scores = np.exp((scores_for_softmax - max_score) / temperature)
    
    # Normalize to get probabilities
    sum_exp_scores = np.sum(exp_scores)
    
    if sum_exp_scores > 0:
        probabilities = exp_scores / sum_exp_scores
    else:
        # This case should ideally not happen if there's at least one bin,
        # but as a safeguard, if all exp_scores are zero or NaN, assign uniform probability.
        num_suitable = len(scores_for_softmax)
        if num_suitable > 0:
            probabilities = np.ones(num_suitable) / num_suitable
        else:
            probabilities = np.array([]) # Should not occur if can_fit_mask is checked

    # Assign probabilities to the original priorities array
    priorities[can_fit_mask] = probabilities

    return priorities
```
