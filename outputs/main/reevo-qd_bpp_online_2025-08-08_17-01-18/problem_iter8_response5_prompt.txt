{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority probabilities for packing an item into bins using a\n    combination of tight fit preference, bin scarcity, earlier bin preference,\n    and Softmax for probabilistic selection.\n\n    This heuristic prioritizes:\n    1.  **Tight Fits:** Favors bins where the remaining capacity is close to the item size.\n        A sigmoid function on the \"mismatch\" (remaining_capacity - item) is used,\n        where smaller mismatch yields a higher score.\n    2.  **Bin Scarcity:** Slightly favors bins that have less remaining capacity overall,\n        as these are scarcer resources. A bonus is added inversely proportional to\n        the remaining capacity.\n    3.  **Earlier Bin Preference:** As a tie-breaker, favors bins that were opened earlier\n        (i.e., appear earlier in the `bins_remain_cap` array).\n    4.  **Probabilistic Exploration (Softmax):** Converts the combined priority scores\n        into probabilities using the Softmax function. The `temperature` parameter\n        controls the exploration-exploitation trade-off:\n        - Low temperature (close to 0): Primarily exploits the highest-scoring bins.\n        - High temperature (large value): Leads to more uniform probabilities,\n          encouraging exploration of less optimal bins.\n\n    The raw score for each bin `i` is calculated as:\n    `Score_i = SigmoidFit(bins_remain_cap[i], item, k) + gamma * (1 / (bins_remain_cap[i] + epsilon)) + delta * (1 / (i + 1))`\n    where `SigmoidFit` is `1 / (1 + exp(k * (bins_remain_cap[i] - item)))`.\n    Then, probabilities are derived using Softmax:\n    `P_i = exp(Score_i / temperature) / sum(exp(Score_j / temperature))`\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n        temperature: Controls the Softmax exploration/exploitation balance.\n                     Higher values lead to more exploration. Defaults to 1.0.\n        k: Sensitivity parameter for the sigmoid function (tightest fit preference).\n           Higher `k` increases preference for tighter fits. Defaults to 5.0.\n        gamma: Weight for the bin scarcity bonus. Higher `gamma` increases\n               preference for less empty bins. Defaults to 0.1.\n        delta: Weight for the earlier bin preference tie-breaker. Higher `delta`\n               increases preference for earlier bins. Defaults to 0.01.\n        epsilon: Small value to prevent division by zero in scarcity calculation.\n                 Defaults to 1e-6.\n\n    Returns:\n        A NumPy array of probabilities, same size as `bins_remain_cap`.\n        Bins that cannot fit the item will have a probability of 0.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    raw_scores = np.full(num_bins, -np.inf, dtype=float) # Initialize with -inf for invalid bins\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bin_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bin_indices.size > 0:\n        suitable_bins_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # 1. Sigmoid Fit Score: Prioritize tight fits.\n        # Calculate mismatch (wasted space)\n        mismatch = suitable_bins_cap - item\n        \n        # Cap exponent argument to prevent overflow in np.exp.\n        # A large positive mismatch should result in a score close to 0.\n        # A mismatch of 0 should result in a score close to 0.5.\n        # Sigmoid(x) = 1 / (1 + exp(x)). We want smaller mismatch (closer to 0) to be better.\n        # Let's use `k * mismatch`. If mismatch is small (e.g., 0), exp(0)=1, sigmoid=0.5.\n        # If mismatch is larger, exp(k*mismatch) increases, sigmoid decreases.\n        # To make tighter fits higher score, we want `1 - sigmoid` or `sigmoid(-k*mismatch)`.\n        # Let's define fit score such that smaller `mismatch` is higher score.\n        # Option: 1 / (1 + exp(k * mismatch)). This gives higher score for larger mismatch.\n        # Option: 1 / (1 + exp(-k * mismatch)). This gives higher score for smaller mismatch.\n        # We want small positive mismatch (tight fit) to be prioritized.\n        # So, we want score to be high when `mismatch` is close to 0.\n        # Let's use `exp(-k * mismatch)`. Then normalize it.\n        # Or, use `1 / (1 + exp(k * mismatch))` and invert it, or simply use the original formulation\n        # and recognize that lower `mismatch` leads to `1/(1+exp(small_positive))` which is higher.\n        # Let's stick with `1 / (1 + exp(k * mismatch))` and interpret it directly.\n        # Larger values mean smaller `mismatch` if we use `k * mismatch` as is.\n        \n        # To ensure higher score for tighter fit, we want function to be decreasing with mismatch.\n        # `sigmoid_score = 1 / (1 + np.exp(k * mismatch))`\n        # This means larger mismatch -> larger exponent -> smaller score. This is good.\n        \n        max_exponent_arg = 35.0 # Prevent overflow\n        capped_exponent_arg = np.minimum(k * mismatch, max_exponent_arg)\n        sigmoid_scores = 1.0 / (1.0 + np.exp(capped_exponent_arg))\n\n        # 2. Bin Scarcity Bonus: Favor less empty bins.\n        # Use 1 / (capacity + epsilon) as a proxy for \"fullness\".\n        scarcity_bonus = gamma * (1.0 / (suitable_bins_cap + epsilon))\n\n        # 3. Earlier Bin Preference: Tie-breaker for bins opened earlier.\n        # Indices are 0-based, so we add 1 to avoid division by zero and shift index.\n        tie_breaker_bonus = delta * (1.0 / (suitable_bin_indices + 1.0))\n\n        # Combine scores for suitable bins\n        combined_scores = sigmoid_scores + scarcity_bonus + tie_breaker_bonus\n        \n        # Assign combined scores back to the raw_scores array\n        raw_scores[suitable_bins_mask] = combined_scores\n\n    # If temperature is very low (close to 0), it's pure exploitation.\n    # Avoid division by zero if temperature is 0.\n    if temperature <= epsilon:\n        if np.all(raw_scores == -np.inf): # No suitable bins\n            return np.zeros(num_bins)\n            \n        max_score = np.max(raw_scores)\n        # Assign probability 1 to the bin(s) with the maximum score\n        probabilities = np.where(raw_scores == max_score, 1.0, 0.0)\n        \n        # Normalize to ensure sum is 1 if multiple max scores exist\n        num_max_scores = np.sum(probabilities)\n        if num_max_scores > 0:\n            probabilities /= num_max_scores\n        return probabilities\n\n    # Apply Softmax to convert scores to probabilities for exploration.\n    # Shift scores by subtracting the max score for numerical stability.\n    # exp(x_i / T) / sum(exp(x_j / T)) = exp((x_i - max(x)) / T) / sum(exp((x_j - max(x)) / T))\n    \n    # If all scores are -inf (no suitable bins), max_raw_score will be -inf.\n    # In this case, return all zeros.\n    if np.all(raw_scores == -np.inf):\n        return np.zeros(num_bins)\n\n    max_raw_score = np.max(raw_scores)\n    \n    # Handle potential case where max_raw_score is -inf (should be caught above, but for safety)\n    if not np.isfinite(max_raw_score):\n         return np.zeros(num_bins)\n\n    shifted_scores = (raw_scores - max_raw_score) / temperature\n    \n    # Calculate exponential of shifted scores.\n    # exp_scores can be 0 if shifted_scores are very negative.\n    exp_scores = np.exp(shifted_scores)\n    \n    # Calculate sum of exponential scores for normalization.\n    sum_exp_scores = np.sum(exp_scores)\n    \n    # Calculate probabilities. Handle case where sum_exp_scores is 0 (e.g., all exp_scores were 0).\n    if sum_exp_scores > 0:\n        probabilities = exp_scores / sum_exp_scores\n    else:\n        # This implies all exp_scores were effectively zero, meaning all shifted scores were extremely negative.\n        # In this scenario, all probabilities should be zero, or distributed uniformly if\n        # we interpret the lack of finite scores as equal unsuitability.\n        # Given our `raw_scores` initialization with -inf, this should ideally not happen\n        # unless `temperature` is extremely large or all `raw_scores` are -inf.\n        # If `raw_scores` contained finite values but `sum_exp_scores` is 0, it's a numerical issue.\n        # Setting to zeros is a safe fallback.\n        probabilities = np.zeros(num_bins)\n\n    # Ensure probabilities sum to 1 (due to potential floating point inaccuracies)\n    # and handle any NaN values that might arise from edge cases.\n    probabilities = np.nan_to_num(probabilities)\n    if np.sum(probabilities) > epsilon: # Re-normalize if sum is significantly greater than 0\n        probabilities /= np.sum(probabilities)\n    elif not np.all(raw_scores == -np.inf): # If there were valid scores but sum is 0\n        # This suggests all valid scores resulted in near-zero exp_scores.\n        # A uniform distribution over suitable bins might be better if possible.\n        # However, given the Softmax logic, zero sum means zero probabilities.\n        pass # Keep probabilities as zeros\n\n    return probabilities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a blended strategy.\n\n    This strategy prioritizes exact fits, uses a decay function for near-exact fits,\n    and applies a Softmax-like normalization for smooth probability distribution.\n    It aims to favor bins where the remaining capacity is closest to the item size,\n    with a strong preference for exact matches.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    eligible_bins_cap = bins_remain_cap[can_fit_mask]\n\n    if not eligible_bins_cap.size:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit residual\": how much space is left after placing the item.\n    # Smaller values indicate a tighter fit.\n    fit_residuals = eligible_bins_cap - item\n\n    # Prioritize exact fits (residual = 0). For near-exact fits, use an exponential\n    # decay based on the residual. Bins with smaller residuals get higher scores.\n    # A small epsilon is added to the residual to ensure that exact fits (residual=0)\n    # get a distinct, higher score than bins that leave a tiny positive residual.\n    # The decay_factor controls how quickly the priority drops as the residual increases.\n    decay_factor = 0.5\n    # We want smaller residuals to have higher scores.\n    # An exponential decay is suitable: exp(-decay_factor * residual)\n    # For residual = 0 (exact fit), score is exp(0) = 1.0.\n    # For residual > 0, score decreases.\n    # Add a small constant to the exponent to ensure that even exact fits have a score\n    # that can be part of a meaningful softmax, avoiding potential issues if all residuals are 0.\n    # Alternatively, we can explicitly set exact fits to a high base value.\n    \n    # Strategy:\n    # 1. Exact fits get a high score (e.g., 1.0).\n    # 2. Near-exact fits get a score based on exponential decay of the residual.\n    # 3. Use softmax to normalize these scores into probabilities.\n\n    # Base scores: 1.0 for exact fits, and an exponentially decaying score for others.\n    # The decay_rate ensures that bins with residuals closer to 0 are preferred.\n    # We use `fit_residuals + 1e-6` to ensure that even for exact fits (residual=0),\n    # we have a non-zero value to pass to exp, and to differentiate exact from very close fits slightly.\n    # However, a cleaner approach is to handle exact fits explicitly.\n    \n    scores = np.where(fit_residuals == 0,\n                      1.0,  # High priority for exact fits\n                      np.exp(-decay_factor * fit_residuals)) # Decreasing priority for near-fits\n\n    # Apply Softmax-like normalization to convert scores into probabilities.\n    # This ensures that the priorities sum to 1 across the eligible bins and\n    # that preferences are smoothly distributed.\n    # The temperature parameter controls the \"sharpness\" of the distribution.\n    # A lower temperature makes the probabilities sharper (more emphasis on best fits).\n    temperature = 0.2\n    \n    try:\n        # Ensure scores are not excessively large before exponentiation\n        # Clipping can help prevent overflow, but Softmax should handle it better with exp\n        # Adding a small constant to the scores before softmax can also help stabilize.\n        # A common practice is to subtract the maximum score before exponentiating to avoid overflow.\n        max_score = np.max(scores)\n        normalized_scores = (scores - max_score) / temperature\n        exp_scores = np.exp(normalized_scores)\n        softmax_priorities = exp_scores / np.sum(exp_scores)\n    except OverflowError:\n        # In case of extreme values leading to overflow, fall back to a uniform distribution\n        # or a simpler heuristic if softmax fails. For now, uniform is a safe fallback.\n        softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size\n    except ZeroDivisionError:\n        # If sum of exp_scores is zero (highly unlikely with positive scores), fallback\n        softmax_priorities = np.ones_like(eligible_bins_cap) / eligible_bins_cap.size\n\n    # Map the calculated priorities back to the original bin indices\n    priorities[can_fit_mask] = softmax_priorities\n\n    return priorities\n\n[Reflection]\nPrioritize exact fits, use smooth decay, and enable tunable exploration via Softmax.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}