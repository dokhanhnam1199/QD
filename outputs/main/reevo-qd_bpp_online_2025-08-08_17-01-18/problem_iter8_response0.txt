```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority scores for bins using a Softmax-based approach,
    balancing tight fits with bin scarcity using tunable weights and temperature.

    This heuristic prioritizes bins based on two factors, combined into a unified score
    and then transformed into probabilities using Softmax:

    1.  **Tightness of Fit:** Prefers bins where the remaining capacity is just enough
        to accommodate the item, minimizing wasted space (`remaining_capacity - item`).
        This is modeled by a term that increases as the mismatch decreases.
    2.  **Bin Scarcity:** Prefers bins that have less remaining capacity overall. This
        encourages the usage of less full bins, potentially leaving more space in
        moderately full bins for future items. This is modeled by a term that
        increases as the remaining capacity decreases.

    The unified score for a suitable bin `i` is a weighted sum of these two components:
    `unified_score_i = w_fit * fit_score_i + w_scarcity * scarcity_score_i`

    - `fit_score_i`: Higher for smaller `bins_remain_cap[i] - item`. We use `1.0 / (1.0 + mismatch_i)`.
    - `scarcity_score_i`: Higher for smaller `bins_remain_cap[i]`. We use `1.0 / (1.0 + bins_remain_cap[i])`.

    The `temperature` parameter in the Softmax function controls the exploration-exploitation
    trade-off:
    - High temperature: Smoother probability distribution (more exploration).
    - Low temperature: Sharper probability distribution (more exploitation of best bins).

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.

    Returns:
        A NumPy array of the same size as `bins_remain_cap`, where each element
        is a probability score (between 0 and 1) for the corresponding bin.
        Bins that cannot fit the item will have a score of 0.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins that can accommodate the item
    suitable_bins_mask = bins_remain_cap >= item
    suitable_bins_cap = bins_remain_cap[suitable_bins_mask]

    # If no bin can fit the item, return all zeros
    if suitable_bins_cap.size == 0:
        return priorities

    # --- Tuning Parameters ---
    # Weights for balancing fit and scarcity. Sum doesn't need to be 1.
    # Higher weight means that component has more influence.
    weight_fit = 0.7
    weight_scarcity = 0.3

    # Temperature for Softmax: controls exploration vs. exploitation.
    # Lower temperature -> favors bins with highest score (exploitation).
    # Higher temperature -> more uniform distribution (exploration).
    temperature = 0.5 # Tunable parameter: 0.1 (sharp) to 2.0 (smooth)

    # --- Scoring Mechanism ---
    mismatches = suitable_bins_cap - item
    capacities = suitable_bins_cap

    # Fit score: Higher for smaller mismatch. Use 1/(1+x) to map [0, inf) to (0, 1].
    # Add a small epsilon if we want to ensure scores are never exactly 0 or 1,
    # but 1/(1+x) already maps to (0, 1].
    fit_scores = 1.0 / (1.0 + mismatches)

    # Scarcity score: Higher for smaller capacity. Use 1/(1+x) to map [0, inf) to (0, 1].
    scarcity_scores = 1.0 / (1.0 + capacities)

    # Unified score: Weighted sum of fit and scarcity scores.
    unified_scores = weight_fit * fit_scores + weight_scarcity * scarcity_scores

    # --- Softmax Application ---
    # Apply Softmax to `unified_scores`. The formula is exp(score / temperature) / sum(exp(score / temperature)).
    # To prevent numerical overflow from exp(), it's best to shift scores so the maximum is 0.
    # `scaled_scores = unified_scores / temperature`
    # `shifted_scaled_scores = scaled_scores - np.max(scaled_scores)`
    # `exponentials = np.exp(shifted_scaled_scores)`
    # `sum_exponentials = np.sum(exponentials)`

    # Let's use direct scaling and then careful exponentiation.
    # Avoid division by zero if temperature is zero or extremely small.
    if temperature <= 0:
        # If temperature is zero or negative, it implies pure exploitation (argmax).
        # The bin with the highest unified score gets probability 1.
        # However, Softmax is generally defined for temperature > 0.
        # For simplicity and robustness, if temperature is invalid, treat as moderate.
        temperature = 1e-6 # Small positive number to avoid division by zero

    # Scale scores by temperature for Softmax. Lower temp sharpens.
    scaled_scores = unified_scores / temperature

    # Shift scores for numerical stability before exponentiation.
    # The maximum value in `scaled_scores` will become 0 after subtraction.
    # This prevents `exp` from overflowing for large positive scores.
    max_scaled_score = np.max(scaled_scores)
    shifted_scaled_scores = scaled_scores - max_scaled_score

    # Compute exponentials. Even after shifting, very small negative scores can lead to underflow.
    # `np.exp` handles small values gracefully (approaching 0).
    exponentials = np.exp(shifted_scaled_scores)

    # Calculate the sum of exponentials for normalization.
    sum_exponentials = np.sum(exponentials)

    # Avoid division by zero if all exponentiated values are zero.
    if sum_exponentials == 0:
        # This can happen if all `shifted_scaled_scores` were extremely negative,
        # causing `exp` to underflow to zero. In such cases, all bins are effectively
        # equally undesirable or equally preferred. Assign uniform probability.
        if suitable_bins_cap.size > 0:
            priorities[suitable_bins_mask] = 1.0 / suitable_bins_cap.size
        return priorities

    # Calculate the final probabilities using Softmax.
    softmax_probabilities = exponentials / sum_exponentials

    # Place the calculated softmax probabilities back into the main priorities array.
    priorities[suitable_bins_mask] = softmax_probabilities

    return priorities
```
