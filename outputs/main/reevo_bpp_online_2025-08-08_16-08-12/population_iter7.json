[
  {
    "stdout_filepath": "problem_iter6_response0.txt_stdout.txt",
    "code_path": "problem_iter6_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a best-fit approach with explicit prioritization of exact fits and exploration.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it first gives the highest priority to exact fits. For bins that are not\n    exact fits but can accommodate the item, it assigns priority based on\n    how much remaining capacity is left, favoring smaller remaining capacities.\n    To encourage exploration, a soft constraint using a temperature parameter\n    is applied to the non-exact fits, allowing bins with slightly more remaining\n    capacity a chance to be selected.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can accommodate the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_placement = fitting_bins_remain_cap - item\n\n    # Assign highest priority to exact fits.\n    exact_fit_mask_subset = (remaining_after_placement == 0)\n    priorities[can_fit_mask][exact_fit_mask_subset] = 100.0\n\n    # For bins that fit but are not exact fits, assign priorities.\n    near_fit_mask_subset = ~exact_fit_mask_subset\n    if np.any(near_fit_mask_subset):\n        near_fit_priorities_raw = remaining_after_placement[near_fit_mask_subset]\n        \n        # Apply a transformation to favor smaller remaining capacities (closer to zero).\n        # Using negative of remaining capacity means smaller remaining capacity gets higher raw priority.\n        # Add a small epsilon to avoid issues with zero remaining capacity if it wasn't caught as exact fit\n        # (though the exact_fit_mask_subset should handle this).\n        transformed_priorities = -near_fit_priorities_raw - 1e-9 \n\n        # Apply softmax for exploration.\n        # Higher temperature means more exploration.\n        temperature = 1.0\n        \n        # Normalize for softmax to avoid overflow/underflow and ensure numerical stability\n        # Subtracting max makes the largest value 0, exp(0) = 1.\n        if np.any(transformed_priorities):\n            normalized_transformed_priorities = transformed_priorities - np.max(transformed_priorities)\n            soft_priorities = np.exp(normalized_transformed_priorities / temperature)\n            \n            # Scale soft_priorities to be less than the exact fit priority (100.0)\n            # and positive. The maximum possible value for soft_priorities is 1 (before scaling).\n            # We want these to be lower than 100, and positive.\n            # A simple scaling factor can work, or we can ensure they are just below 100.\n            # Let's scale them such that the best near-fit is slightly less than exact fit.\n            # The current max value of soft_priorities is 1.0.\n            # We can scale it by a factor less than 100, e.g., 99.\n            scale_factor = 99.0\n            soft_priorities_scaled = soft_priorities * scale_factor\n\n            priorities[can_fit_mask][near_fit_mask_subset] = soft_priorities_scaled\n        else:\n            # If somehow no near fits had valid transformed priorities (highly unlikely with -inf initialization)\n            pass # Keep them at -inf or handle as an error case if necessary.\n\n    return priorities",
    "response_id": 0,
    "obj": 4.487435181491823,
    "SLOC": 23.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response1.txt_stdout.txt",
    "code_path": "problem_iter6_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins using a refined \"Best Fit\" heuristic, with a mild exploration component.\n\n    This heuristic aims to find the bin that, after placing the item, will have the\n    least remaining capacity (i.e., the tightest fit) among all bins that can\n    accommodate the item. It prioritizes bins that are closer to being full.\n    An exact fit (remaining capacity == item size) is implicitly handled as\n    it results in zero remaining capacity, which is the minimum possible.\n\n    The priority is calculated as:\n    - For bins that can fit the item: A score that rewards tighter fits.\n      Specifically, `-(bins_remain_cap - item)` is used, which means bins with\n      less remaining capacity after placing the item get higher scores.\n      A small penalty is added for bins that leave a *very* small positive remainder\n      to slightly encourage exploration if the best fit is only marginally better\n      than another option.\n    - For bins that cannot fit the item: A priority of negative infinity to ensure\n      they are never selected.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array containing the remaining capacity of each bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        represents the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    epsilon = 1e-6  # Small constant to slightly penalize near-zero remainders\n\n    # Identify bins that have enough capacity to fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity after packing\n    remaining_capacities_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # Assign priorities. Higher priority for smaller remaining capacity (tighter fit).\n    # We use the negative of the remaining capacity.\n    # A small epsilon is added to the remaining capacity before negation to slightly\n    # discourage extremely tight fits when there are other good options, promoting\n    # a mild form of exploration by making slightly looser fits appear more attractive\n    # if the difference is negligible.\n    # The primary goal is still best fit, but this adds a slight nudge.\n    priorities[can_fit_mask] = -(remaining_capacities_after_fit + epsilon)\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response2.txt_stdout.txt",
    "code_path": "problem_iter6_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority for each bin to pack an item.\n    Prioritizes exact fits, then near fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin. Bins that cannot fit the item\n        will have a very low priority.\n    \"\"\"\n    # Initialize priorities to a very low value.\n    priorities = np.full_like(bins_remain_cap, -float('inf'))\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    # Calculate potential remaining capacities for bins that can fit the item.\n    potential_remainders = bins_remain_cap[can_fit_mask] - item\n\n    # Assign priority:\n    # 1. Exact fits (remainder is 0) get the highest priority.\n    # 2. Near fits (smaller positive remainder) get higher priority than larger remainders.\n    # We can achieve this by assigning a high value for exact fits and then\n    # using the negative of the remainder for near fits, so smaller remainders\n    # (less negative values) get higher priority.\n\n    # For exact fits\n    exact_fit_indices = np.where(potential_remainders == 0)[0]\n    if len(exact_fit_indices) > 0:\n        priorities[can_fit_mask][exact_fit_indices] = 100.0  # High priority for exact fits\n\n    # For near fits\n    near_fit_indices = np.where(potential_remainders > 0)[0]\n    if len(near_fit_indices) > 0:\n        # Assign priorities based on the negative of the remainder.\n        # This means smaller remainders get higher (less negative) scores.\n        priorities[can_fit_mask][near_fit_indices] = -potential_remainders[near_fit_indices]\n\n    return priorities",
    "response_id": 2,
    "obj": 4.487435181491823,
    "SLOC": 13.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response3.txt_stdout.txt",
    "code_path": "problem_iter6_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a strategy that prioritizes exact fits, then near fits, and incorporates exploration.\n\n    This strategy prioritizes exact fits (leaving zero remaining capacity).\n    Among bins that can accommodate the item but are not exact fits, it prioritizes\n    those that leave the least remaining capacity (best-fit approach).\n    To encourage exploration, a softmax function is applied to the priorities of\n    near-fitting bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can accommodate the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_placement = fitting_bins_remain_cap - item\n\n    # Handle exact fits\n    exact_fit_mask_subset = (remaining_after_placement == 0)\n    exact_fit_indices = np.where(can_fit_mask)[0][exact_fit_mask_subset]\n    priorities[exact_fit_indices] = 100.0  # High priority for exact fits\n\n    # Handle near fits\n    near_fit_mask_subset = ~exact_fit_mask_subset\n    if np.any(near_fit_mask_subset):\n        near_fit_indices_subset = np.where(can_fit_mask)[0][near_fit_mask_subset]\n        near_fit_remaining = remaining_after_placement[near_fit_mask_subset]\n\n        # Base priority for near fits: prioritize smaller remaining capacity.\n        # Using negative remaining capacity to make smaller remaining capacity have higher score.\n        # Add a small epsilon to avoid issues with identical remaining capacities and for stability.\n        near_fit_base_priorities = -near_fit_remaining\n\n        # Apply softmax to the base priorities of near fits for exploration.\n        # Softmax converts scores into probabilities, giving a chance to bins with slightly more remaining space.\n        temperature = 1.0  # Tunable parameter for exploration\n        \n        # Shift priorities to avoid large negative exponents in exp\n        shifted_near_fit_priorities = near_fit_base_priorities - np.max(near_fit_base_priorities)\n        \n        # Calculate softmax probabilities\n        exp_priorities = np.exp(shifted_near_fit_priorities / temperature)\n        \n        # Normalize probabilities to sum to 1 across the near-fitting bins\n        sum_exp_priorities = np.sum(exp_priorities)\n        if sum_exp_priorities > 0:\n            softmax_probabilities = exp_priorities / sum_exp_priorities\n        else:\n            # If all exponents are effectively zero or negative infinity, assign uniform probability\n            softmax_probabilities = np.ones_like(exp_priorities) / len(exp_priorities)\n\n        # Assign these probabilities (scaled to still be lower than exact fits) to the priority array\n        # We can scale them down or add a constant to ensure they are less than the exact fit priority.\n        # For simplicity, let's scale them by a factor less than 1, but larger than the penalty for non-exact fits.\n        # The goal is that exact fits are still distinctly preferred, but exploration happens among near fits.\n        # We can map the probabilities to a range like [50, 99] for example.\n        scaled_near_fit_priorities = 50.0 + softmax_probabilities * 49.0 # Scale probabilities to a range below exact fits\n        \n        priorities[near_fit_indices_subset] = scaled_near_fit_priorities\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "SLOC": 26.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response4.txt_stdout.txt",
    "code_path": "problem_iter6_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a\n    strategy that prioritizes exact fits and then uses softmax for exploration.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it assigns a higher priority to bins that will have less remaining capacity\n    after the item is placed (closer to an exact fit). It then applies a\n    softmax function to these priorities to balance exploration and exploitation,\n    allowing for a chance to pack items into bins that are not the absolute best fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9  # Small value to avoid division by zero\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate the remaining capacity if the item is placed in a fitting bin.\n    # We want to prioritize bins that leave *less* remaining capacity.\n    # Therefore, the \"quality\" of a fit is inversely related to the remaining capacity.\n    # A small positive remainder is good. A negative remainder means it doesn't fit.\n    # To use softmax, we need positive values. A simple way is to use the\n    # inverse of the remaining capacity plus a small constant for stability.\n    # Alternatively, we can use the negative of the remaining capacity and\n    # adjust the softmax interpretation.\n    # Let's stick to the idea of inverse remaining capacity for now.\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # We want to maximize the \"goodness\" of the fit. A smaller remainder is better.\n    # The score should reflect this: higher score for smaller remainders.\n    # A common approach is 1 / (remainder + epsilon).\n    # We'll scale these scores before applying softmax to control the spread.\n    fit_scores = 1.0 / (remaining_after_placement + epsilon)\n\n    # Scale scores to prevent very large or small exponents in softmax.\n    # A simple normalization can be dividing by the maximum score among fitting bins.\n    max_fit_score = np.max(fit_scores) if fit_scores.size > 0 else 1.0\n    if max_fit_score > epsilon: # Avoid division by zero if all fit_scores are ~0\n        scaled_fit_scores = fit_scores / max_fit_score\n    else:\n        scaled_fit_scores = np.ones_like(fit_scores) # If all scores are negligible, treat them equally\n\n    # Apply softmax. This converts scores into probabilities, balancing exploration\n    # (more uniform probabilities) and exploitation (probabilities concentrated on high scores).\n    # A temperature parameter could be introduced for more fine-grained control,\n    # but here we use a standard softmax.\n    exp_scores = np.exp(scaled_fit_scores)\n    sum_exp_scores = np.sum(exp_scores)\n\n    if sum_exp_scores > epsilon:\n        probabilities = exp_scores / sum_exp_scores\n    else:\n        # This case should be rare with the scaling, but for safety,\n        # assign uniform probabilities if the sum is too small.\n        probabilities = np.ones_like(fit_scores) / len(fit_scores)\n\n    # Assign these probabilities as priorities to the bins that can fit the item.\n    priorities[can_fit_mask] = probabilities\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "SLOC": 21.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response5.txt_stdout.txt",
    "code_path": "problem_iter6_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin, prioritizing Exact Fit, then Best Fit,\n    with a small exploration component favoring bins with larger remaining capacity.\n\n    This heuristic prioritizes bins that offer an exact fit for the item.\n    If no exact fit is available, it then prioritizes bins that can accommodate the item\n    with the smallest remaining capacity (Best Fit approach among the remaining options).\n    As a secondary exploration, it slightly boosts bins that have a larger remaining capacity,\n    aiming to keep more space available for potentially larger future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Exact fits get the highest priority (e.g., 100). Bins that fit but not exactly\n        get a priority based on the \"tightness\" of the fit, inversely proportional to\n        the slack (remaining capacity - item size), with a slight bonus for larger remaining capacity.\n        Bins that cannot fit get 0 priority.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 0.05  # Probability of exploration based on remaining capacity\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n    fitting_indices = np.where(can_fit_mask)[0]\n\n    if fitting_bins_capacities.size == 0:\n        return priorities  # No bin can fit the item\n\n    # Calculate slack for bins that can fit the item\n    slacks = fitting_bins_capacities - item\n\n    # Exact fit has the highest priority\n    exact_fit_mask = slacks == 0\n    exact_fit_indices = fitting_indices[exact_fit_mask]\n    priorities[exact_fit_indices] = 100.0\n\n    # For bins that are not an exact fit, assign priority based on the slack (Best Fit)\n    # and introduce a small exploration bonus for larger remaining capacity.\n    non_exact_fit_mask = slacks > 0\n    non_exact_fit_indices = fitting_indices[non_exact_fit_mask]\n    non_exact_slacks = slacks[non_exact_fit_mask]\n    non_exact_capacities = fitting_bins_capacities[non_exact_fit_mask]\n\n    if non_exact_fit_indices.size > 0:\n        # Sort these bins by slack in ascending order.\n        sorted_slack_indices = np.argsort(non_exact_slacks)\n\n        # Assign base priorities from highest (99) to lowest based on sorted slack\n        base_priorities = 99.0 - np.arange(non_exact_slacks.size)\n\n        # Introduce a small exploration bonus for bins with larger remaining capacity.\n        # This bonus is scaled by epsilon and the relative difference in capacity.\n        # We normalize capacities to avoid issues with very large numbers and to\n        # ensure the bonus is relative.\n        min_cap = np.min(non_exact_capacities)\n        max_cap = np.max(non_exact_capacities)\n        \n        exploration_bonus = np.zeros(non_exact_slacks.size)\n        if max_cap > min_cap: # Avoid division by zero if all non-exact fits have the same capacity\n            normalized_capacities = (non_exact_capacities - min_cap) / (max_cap - min_cap)\n            # Small bonus for higher capacity, scaled by epsilon\n            exploration_bonus = epsilon * normalized_capacities\n\n        # Combine base priority and exploration bonus\n        combined_priorities = base_priorities + exploration_bonus\n\n        # Assign combined priorities\n        priorities[non_exact_fit_indices[sorted_slack_indices]] = combined_priorities\n\n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "SLOC": 28.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response6.txt_stdout.txt",
    "code_path": "problem_iter6_code6.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins using a combination of \"Best Fit\" (exploitation) and a\n    penalty for bins that are too empty (exploration component).\n\n    This heuristic aims to:\n    1. Exploit: Prioritize bins that provide a \"tight fit\" for the current item.\n       This is achieved by favoring bins where the remaining capacity after\n       placing the item is minimized (closest to zero).\n    2. Explore: Slightly penalize bins that have a large amount of remaining\n       capacity even before placing the item. This encourages using bins that\n       are already somewhat full, potentially leaving the emptier bins for\n       future larger items.\n\n    The priority is calculated as:\n    - For bins that can fit the item:\n      The score is derived from the remaining capacity after fitting.\n      A tight fit (small `bins_remain_cap - item`) yields a higher score.\n      The score is designed such that a tight fit gets a high positive value,\n      and a loose fit gets a lower value.\n    - For bins that cannot fit the item: A priority of negative infinity.\n\n    A penalty is applied to bins that are \"too empty\" to encourage using\n    bins that are already partially filled.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array containing the remaining capacity of each bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        represents the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that have enough capacity to fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity after placing the item for bins that can fit.\n    remaining_capacities_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate a base \"best fit\" score. Higher score for smaller remaining capacity.\n    # We use the negative of the remaining capacity. Smaller remaining capacity means\n    # a larger negative number, which we want to be higher priority.\n    # Example:\n    # Bin A: remaining_cap = 10, item = 7 -> remaining_after_fit = 3. Score = -3.\n    # Bin B: remaining_cap = 8, item = 7 -> remaining_after_fit = 1. Score = -1.\n    # Bin B is a tighter fit and has a higher score.\n    best_fit_scores = -remaining_capacities_after_fit\n\n    # Introduce an exploration penalty for bins that are too empty.\n    # This encourages using bins that are already somewhat occupied.\n    # We can define \"too empty\" as having a remaining capacity significantly larger\n    # than the item itself, or simply a very large absolute capacity.\n    # A simple penalty: subtract a value proportional to the original remaining capacity.\n    # This makes bins with large remaining capacity less attractive.\n    # We only apply this penalty to bins that can fit the item.\n    # The penalty should be smaller for bins that are already a tight fit.\n    # Let's define a 'emptiness_penalty' that is higher for bins with higher initial capacity.\n    # We can use a logarithmic scale or a simple linear scale, scaled to not overwhelm best_fit_scores.\n    # A common approach is to penalize bins with lots of unused space.\n    # Let's consider the \"slack\" or unused capacity if we were to fill the bin as much as possible.\n    # However, for online BPP, we only care about the remaining capacity.\n    # Let's penalize bins that have remaining_capacities_after_fit that are still large.\n    # Or even simpler, penalize based on the original bin capacity if it's much larger than the item.\n    # A simple approach: penalize the original `bins_remain_cap` itself.\n    # The penalty should be subtracted from the `best_fit_scores`.\n\n    # For bins that can fit:\n    # Score = (tight_fit_score) - (emptiness_penalty)\n    # tight_fit_score = -remaining_capacities_after_fit\n    # emptiness_penalty could be related to bins_remain_cap[can_fit_mask]\n\n    # Let's refine the scoring:\n    # We want to prioritize bins that have `bins_remain_cap - item` small and positive.\n    # The `priority_v1` did this well with `-remaining_capacities_after_fit`.\n    # Now, let's add the exploration: penalize bins that are \"too empty\".\n    # What if we define \"too empty\" as a bin whose remaining capacity is much larger than the item?\n    # Or a bin that has a lot of unused capacity relative to the bin's total capacity (if we knew it)?\n    # Since we only have `bins_remain_cap`, let's focus on that.\n    # A bin is \"too empty\" if `bins_remain_cap` is large.\n    # Let's subtract a portion of `bins_remain_cap` from the best-fit score.\n    # The larger `bins_remain_cap`, the larger the subtraction (more penalty).\n    # We need to be careful not to make the penalty too aggressive.\n    # A scaling factor might be needed.\n\n    # Let's consider a weighted sum:\n    # priority = w1 * (best_fit_score) + w2 * (exploration_score)\n    # where best_fit_score is high for tight fits, and exploration_score is low for empty bins.\n    # Or simply:\n    # priority = best_fit_score - penalty_for_emptiness\n\n    # A potential formula:\n    # priority = - (bins_remain_cap[i] - item) - alpha * bins_remain_cap[i]\n    # where alpha is a small positive constant to control the penalty.\n    # Let's try to combine these directly.\n    # We want to maximize `- (bins_remain_cap[i] - item)` (tight fit)\n    # and minimize `bins_remain_cap[i]` (avoid empty bins).\n    # So we want to maximize `-(bins_remain_cap[i] - item) - alpha * bins_remain_cap[i]`\n\n    alpha = 0.1  # Controls the penalty for empty bins. Adjust as needed.\n\n    # Calculate the combined score for bins that can fit.\n    # The first term promotes tight fits.\n    # The second term penalizes bins that are initially large (empty).\n    combined_scores = -remaining_capacities_after_fit - alpha * bins_remain_cap[can_fit_mask]\n\n    priorities[can_fit_mask] = combined_scores\n\n    # Ensure we don't have NaN or Inf from potential division by zero if alpha were in denominator etc.\n    # Our current formula is safe.\n    return priorities",
    "response_id": 6,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response7.txt_stdout.txt",
    "code_path": "problem_iter6_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins for the online Bin Packing Problem using a hybrid approach\n    that balances \"Best Fit\" (exploitation) with a small chance of \"First Fit\"\n    or random choice among fitting bins (exploration).\n\n    The \"Best Fit\" component prioritizes bins that leave the least remaining\n    capacity after the item is placed. This is achieved by assigning a priority\n    that is the negative of the remaining capacity after fitting the item.\n    A tighter fit (smaller remaining capacity) results in a higher (less negative)\n    priority.\n\n    The exploration component is introduced to prevent getting stuck in local\n    optima. With a small probability (epsilon), it favors bins that are not\n    necessarily the best fit, by assigning a random priority among those that\n    can fit the item. This allows for exploring different packing configurations.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array containing the remaining capacity of each bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        represents the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    epsilon = 0.05  # Probability of exploration\n    num_bins = len(bins_remain_cap)\n\n    # Initialize priorities to negative infinity for all bins\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that have enough capacity to fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity for bins that can fit the item\n    remaining_capacities_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # --- Exploitation Component (Best Fit) ---\n    # Assign priorities based on the tightness of the fit.\n    # The negative of the remaining capacity is used: smaller remaining capacity\n    # leads to a less negative (higher) priority.\n    exploitation_scores = np.full_like(bins_remain_cap, -np.inf)\n    exploitation_scores[can_fit_mask] = -remaining_capacities_after_fit\n\n    # --- Exploration Component ---\n    # Generate random scores for exploration among fitting bins.\n    # We want to give a chance to bins that might not be the \"best\" according to Best Fit.\n    # A simple approach is to assign a random value if exploration is chosen.\n    exploration_scores_random = np.random.rand(num_bins)\n\n    # Combine exploitation and exploration using an epsilon-greedy strategy.\n    # Generate a random choice for each bin: True for exploration, False for exploitation.\n    explore_choice_mask = np.random.rand(num_bins) < epsilon\n\n    # For bins where exploration is chosen AND they can fit the item, use the random score.\n    # For bins where exploitation is chosen AND they can fit the item, use the exploitation score.\n    # For bins that cannot fit, their priority remains -np.inf.\n    \n    # Create a mask for bins that are candidates for selection (can fit the item)\n    candidate_mask = can_fit_mask\n    \n    # Apply exploration choice: if exploration is chosen for a candidate bin, use its random score.\n    priorities[explore_choice_mask & candidate_mask] = exploration_scores_random[explore_choice_mask & candidate_mask]\n    \n    # Apply exploitation choice: if exploitation is chosen for a candidate bin, use its exploitation score.\n    # We only update if the bin was NOT chosen for exploration (or if exploration wasn't chosen for it).\n    # The condition `~explore_choice_mask` ensures we only apply exploitation if exploration wasn't picked.\n    priorities[~explore_choice_mask & candidate_mask] = exploitation_scores[~explore_choice_mask & candidate_mask]\n\n    # Ensure that bins that cannot fit remain at -np.inf\n    priorities[~can_fit_mask] = -np.inf\n\n    return priorities",
    "response_id": 7,
    "obj": 73.81332269644994,
    "SLOC": 15.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response8.txt_stdout.txt",
    "code_path": "problem_iter6_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin, prioritizing\n    minimal slack after placement while also slightly penalizing bins that\n    would become exactly full. This version aims to balance the greedy approach\n    of minimizing slack with a touch of exploration by slightly boosting bins\n    that are not the absolute best fit but still good candidates.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it assigns a higher priority to bins that will have less remaining capacity\n    after the item is placed (minimal slack). This encourages denser packing.\n    A slight penalty is applied to bins that would become completely full,\n    encouraging leaving some space for potentially larger items. To introduce\n    a mild exploratory element, bins that result in a slightly larger, but still\n    good, remainder are given a small boost to avoid over-optimization on just\n    the smallest slack.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the remaining capacity if the item is placed in a fitting bin\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # Base priority: prioritize bins with minimal slack.\n    # Higher score for smaller remainders. Add epsilon for numerical stability.\n    base_priorities = 1.0 / (remaining_after_placement + 1e-9)\n\n    # Introduce a penalty for bins that would become exactly full.\n    # Subtract a small value to slightly reduce their priority.\n    penalty_for_full = 0.1\n    fully_filled_mask = (remaining_after_placement == 0)\n    priorities[can_fit_mask] = base_priorities\n    priorities[can_fit_mask][fully_filled_mask] -= penalty_for_full\n\n    # Add a small exploratory boost: slightly favor bins that have a small positive remainder,\n    # but not necessarily the absolute minimum. This can be achieved by adding a small\n    # positive value to the priority score for a range of remainders.\n    # For example, we can add a small constant to priorities where the remainder is small but not zero.\n    # We can define a threshold for \"small positive remainder\". Let's say up to 10% of bin capacity,\n    # but this is problem-dependent. For a general heuristic, let's add a small constant to\n    # all positive remainders (that are not exactly zero) as a slight encouragement.\n    # We can make this boost decay as the remainder increases.\n    # For simplicity here, let's add a constant boost to all non-exact fits.\n    # This is a very simple form of exploration.\n    exploratory_boost_value = 0.05\n    not_fully_filled_mask = ~fully_filled_mask\n    priorities[can_fit_mask][not_fully_filled_mask] += exploratory_boost_value\n\n    # Ensure all priorities are non-negative.\n    priorities = np.clip(priorities, 0, None)\n\n    return priorities",
    "response_id": 8,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response9.txt_stdout.txt",
    "code_path": "problem_iter6_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin, prioritizing\n    minimal slack after placement while also slightly penalizing bins that\n    would become exactly full to maintain flexibility.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it assigns a higher priority to bins that will have less remaining capacity\n    after the item is placed (minimal slack). This encourages denser packing.\n    Additionally, it slightly penalizes bins that would become completely full\n    to encourage leaving some space for potentially larger items that might\n    arrive later, promoting a balance between filling bins and maintaining flexibility.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the remaining capacity if the item is placed in a fitting bin\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # Assign higher priority to bins that will have less remaining capacity (minimal slack)\n    # We invert the remaining capacity so that smaller remaining capacity gets higher priority.\n    # Add a small epsilon to avoid division by zero and to ensure that bins with zero remaining\n    # capacity after placement are still prioritized highest among the minimal slack options.\n    # A higher value indicates higher priority.\n    priorities[can_fit_mask] = 1.0 / (remaining_after_placement + 1e-9)\n\n    # Introduce a slight penalty for bins that would become exactly full after placement.\n    # This encourages keeping some space for potentially larger future items.\n    # The penalty is a small constant subtracted from the priority.\n    # We apply this penalty only to the bins that are *exactly* filled.\n    fully_filled_mask = (remaining_after_placement == 0)\n    # Reduce priority for bins that become exactly full. The magnitude of reduction\n    # should be small enough not to override the primary goal of minimizing slack,\n    # but significant enough to encourage non-full bins.\n    penalty = 0.05  # A smaller penalty than in v1 to prioritize minimal slack more strongly\n    priorities[can_fit_mask][fully_filled_mask] -= penalty\n\n    # Ensure all priorities are non-negative. If the penalty makes a priority negative,\n    # it should be capped at 0, as a negative priority doesn't make logical sense in this context.\n    priorities = np.clip(priorities, 0, None)\n\n    return priorities",
    "response_id": 9,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response0.txt_stdout.txt",
    "code_path": "problem_iter7_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit strategy with a temperature-controlled exploration.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it assigns a higher priority to bins that will have less remaining capacity\n    after the item is placed (Best Fit). A softmax function is used to introduce\n    controlled exploration, allowing slightly less optimal bins to be chosen with\n    a certain probability, which can help escape local optima. The temperature\n    parameter can be adjusted to control the degree of exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # A higher temperature means more exploration (closer to uniform probabilities)\n    # A lower temperature means more exploitation (closer to greedy best fit)\n    temperature = 0.1  # Tunable parameter\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity if the item is placed in a fitting bin\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # For bins that can fit, calculate a \"fit score\". A smaller remaining capacity\n    # after placement is better, so we use the negative of the remaining capacity.\n    # This is what the softmax will operate on.\n    fit_scores = -remaining_after_placement\n\n    # Apply softmax to get probabilities/priorities.\n    # Ensure exp(fit_scores) doesn't overflow for very small negative fit_scores.\n    # Adding a constant offset to shift the scores to be less negative, which can\n    # help with numerical stability in exp. The relative order is preserved.\n    # We can shift by the maximum value in fit_scores to make the largest element 0.\n    if fit_scores.size > 0:\n        shifted_fit_scores = fit_scores - np.max(fit_scores)\n        # Handle potential NaNs if fit_scores were all Inf or -Inf (unlikely here)\n        if np.any(np.isnan(shifted_fit_scores)):\n             shifted_fit_scores = np.nan_to_num(shifted_fit_scores, nan=-1e9) # Assign a very low score\n\n        # Softmax calculation: exp(score / temperature) / sum(exp(score / temperature))\n        # We are interested in the relative priority, not necessarily normalized probabilities.\n        # So, we can simply use exp(score / temperature). Higher values indicate higher priority.\n        # The smaller the remaining_after_placement, the more negative fit_scores,\n        # so exp(negative_value/temperature) will be smaller.\n        # We want higher priority for smaller remaining_after_placement, so we should use positive fit_scores.\n        # Let's re-evaluate: we want smaller remaining_after_placement to have higher priority.\n        # The current fit_scores are -remaining_after_placement.\n        # Softmax on -remaining_after_placement will assign higher probability to LARGER (less negative) values,\n        # which means LARGER remaining_after_placement. This is the opposite of what we want.\n        # We want smaller remaining_after_placement to have higher priority.\n        # So, we should use the reciprocal of remaining capacity for the softmax, or a score that\n        # increases as remaining_after_placement decreases.\n\n        # Let's use the inverse of remaining capacity for the score.\n        # Larger inverse means smaller remaining capacity, which is what we want.\n        # Add a small epsilon to avoid division by zero.\n        inverse_remaining = 1.0 / (remaining_after_placement + 1e-9)\n\n        # Softmax on these positive scores will give higher probabilities to bins\n        # with smaller remaining capacity.\n        # We want to prioritize tighter fits, so smaller remaining_after_placement.\n        # If remaining_after_placement is 0, inverse is large (high priority).\n        # If remaining_after_placement is large, inverse is small (low priority).\n        # So, inverse_remaining is a good score for \"goodness of fit\".\n        priorities[can_fit_mask] = np.exp(inverse_remaining / temperature)\n    else:\n        # If no bins can fit, all priorities remain 0.\n        pass\n\n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response1.txt_stdout.txt",
    "code_path": "problem_iter7_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit strategy with an exploration component.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it assigns a higher priority to bins that will have less remaining capacity\n    after the item is placed (Best Fit). To balance exploitation with exploration,\n    it uses a softmax-like approach where bins with slightly more remaining\n    capacity also have a non-negligible chance of being selected, controlled by\n    a temperature parameter. This can help avoid getting stuck in locally optimal\n    packings.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity if the item is placed in a fitting bin\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # We want to prioritize smaller remaining capacities (tighter fits).\n    # A simple way to do this is to use the negative of the remaining capacity\n    # or its reciprocal. Here, we use the reciprocal for a strong preference for tight fits.\n    # To introduce exploration, we can add a small noise or use a temperature parameter\n    # in a softmax-like way. For simplicity here, we'll adjust the values to be\n    # more distributed. A common approach is to use e^(score/temperature).\n    # Let's define a score that favors smaller remaining_after_placement.\n    # We can use -remaining_after_placement directly for a simpler form of \"best fit\" priority,\n    # or use a scaling that might encourage more variety.\n\n    # Option 1: Simple Best Fit (similar to v1 but expressed differently)\n    # Higher priority for smaller remaining_after_placement\n    # priorities[can_fit_mask] = -remaining_after_placement\n\n    # Option 2: Softmax-like exploration.\n    # We want scores to be positive and higher for better fits.\n    # Let's map remaining_after_placement to a score where smaller is better.\n    # A potential mapping: score = -remaining_after_placement.\n    # For softmax: exp(score / temperature).\n    # A temperature of 1 means exponential scaling of the negative remaining capacity.\n    # A higher temperature smooths out the probabilities, increasing exploration.\n    # Let's use a temperature that's not too aggressive, e.g., 0.5 or 1.0.\n    # For simplicity, let's consider the \"value\" of a bin as the negative remaining capacity\n    # after placement, so a higher \"value\" is better (less remaining space).\n\n    # To encourage a mix, let's use a linear transformation of remaining_after_placement.\n    # A common technique is to use `1 / (remaining_after_placement + epsilon)` as in v1,\n    # but to add some exploration, we can slightly shift or scale these values.\n    # Or, consider `remaining_after_placement` directly. Lower is better.\n    # For priority, higher should be better. So, we want to transform `remaining_after_placement`\n    # such that smaller values result in larger priorities.\n\n    # Let's try a strategy that favors tight fits but also gives some chance to bins with\n    # slightly more space.\n    # A \"goodness\" score could be `bin_capacity - item - remaining_after_placement`.\n    # But we want to prioritize bins that minimize `remaining_after_placement`.\n    # So, let's use a function of `remaining_after_placement` where smaller values are better.\n\n    # Strategy: Prioritize bins with small `remaining_after_placement`.\n    # To add exploration, we can make the scores less extreme.\n    # Instead of `1 / (remaining_after_placement + epsilon)`, consider `exp(-remaining_after_placement / temperature)`.\n    # Let's use a temperature parameter. A temperature of 1 means exp(-x).\n    # A temperature of 0.1 would make it `exp(-10x)`, very sharp.\n    # A temperature of 10 would make it `exp(-x/10)`, flatter.\n    # Let's choose a moderate temperature, say T=1.0, to balance.\n\n    # Avoid division by zero and ensure non-negative scores.\n    # We want smaller `remaining_after_placement` to result in higher priority.\n    # Let's create scores based on `remaining_after_placement`.\n    # Consider a \"score\" which is the inverse of remaining capacity after placement.\n    # To add exploration, we can use a temperature parameter.\n    # `score = 1.0 / (remaining_after_placement + 1e-9)` is good for best fit.\n    # To explore, we can perturb this or use an exponential.\n\n    # Let's consider the \"benefit\" of placing the item in a bin as the proportion of\n    # remaining capacity filled by the item. This is `item / bins_remain_cap[can_fit_mask]`.\n    # However, this is for the *original* bin capacity, not the remaining capacity.\n    # The goal is to minimize the number of bins, so filling them up is good.\n    # `remaining_after_placement` is a direct measure of how much \"wasted\" space is left.\n    # Minimizing this is good. So higher priority for smaller `remaining_after_placement`.\n\n    # Let's try a score that is a linear function of `1/remaining_after_placement`\n    # plus a small constant to ensure all fitting bins have positive priority,\n    # and then apply a scaling to control exploration.\n\n    # Using `1.0 / (remaining_after_placement + 1e-9)` as the base for tight fits.\n    # To add exploration, we can add a small, positive random noise to these scores.\n    # However, this might make the \"best fit\" less predictable.\n    # A more principled way is to use a temperature parameter with an exponential function.\n\n    # Let's map `remaining_after_placement` to a \"quality\" score where lower is better.\n    # We can use `remaining_after_placement` itself.\n    # For priority, we want higher values to be better.\n    # So, priority can be a function that maps smaller `remaining_after_placement` to higher values.\n\n    # Let's use a score related to the *inverse* of remaining capacity after placement,\n    # but with a moderating factor for exploration.\n    # `score = -remaining_after_placement` (higher is better).\n    # To add exploration, we can use `exp(score / temperature)`.\n    # Let's set a temperature, e.g., `temperature = 1.0`.\n\n    # Calculate values that are higher for better fits (smaller remaining_after_placement).\n    # We can use `1.0 / (remaining_after_placement + epsilon)` or `-remaining_after_placement`.\n    # Let's use `-remaining_after_placement` as the base score (more negative is worse).\n    # A simple way to make priorities positive and reflect \"goodness\" of fit:\n    # `priority_base = max_possible_remaining_capacity - remaining_after_placement`\n    # or simply, `priority_base = some_large_number - remaining_after_placement`.\n    # Or `priority_base = 1 / (remaining_after_placement + epsilon)`.\n\n    # Let's consider `remaining_after_placement` directly as the \"cost\". We want to minimize cost.\n    # For priority, we want to maximize it.\n    # Priority = `C - remaining_after_placement` where C is a constant.\n    # To add exploration, we can smooth this.\n\n    # Let's try this: priority is proportional to `exp(-remaining_after_placement / temperature)`.\n    # A temperature of 1.0 means we assign priorities based on the exponential of\n    # the negative remaining space. This gives higher priority to tight fits but\n    # also non-zero priority to bins with slightly more space.\n\n    temperature = 1.0  # Controls the exploration/exploitation trade-off. Higher T -> more exploration.\n    scores = -remaining_after_placement / temperature\n    priorities[can_fit_mask] = np.exp(scores)\n\n    # Normalize probabilities to sum to 1 for the fitting bins if needed for some applications,\n    # but for priority scores, raw values are often sufficient.\n    # If normalization is desired for a softmax-like selection:\n    # fitting_priorities = np.exp(scores)\n    # if np.sum(fitting_priorities) > 0:\n    #     priorities[can_fit_mask] = fitting_priorities / np.sum(fitting_priorities)\n    # else:\n    #     priorities[can_fit_mask] = 1.0 / len(priorities[can_fit_mask]) if len(priorities[can_fit_mask]) > 0 else 0\n\n    # For now, let's keep the raw exponential scores. The agent selecting the bin\n    # would typically pick the argmax or use a softmax over these scores.\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response2.txt_stdout.txt",
    "code_path": "problem_iter7_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit Decreasing-like strategy adapted for online setting.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it assigns a higher priority to bins that will have less remaining capacity\n    after the item is placed (tightest fit). To balance exploitation with some\n    exploration, a softmax function is used to convert these fit scores into\n    probabilities, controlled by a temperature parameter. This allows for\n    slightly less optimal fits to still have a chance, which can be beneficial\n    in the online setting to avoid premature blocking of good future fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    temperature = 0.5  # Tunable parameter for exploration/exploitation balance\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity if the item is placed in a fitting bin\n    # For bins that cannot fit, their \"fit score\" will be effectively negative infinity\n    # by not being included in the softmax calculation.\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # We want to prioritize bins with *less* remaining capacity after placement.\n    # A good fit means remaining_after_placement is small.\n    # To use with softmax, we can use the negative of the remaining capacity.\n    # A tighter fit (smaller remaining_after_placement) results in a less negative score,\n    # which will get a higher probability in softmax.\n    fit_scores = -remaining_after_placement\n\n    # Apply softmax to convert scores to probabilities\n    # Add a small constant to avoid log(0) if all fit_scores are identical.\n    # If all bins can fit, the relative differences matter.\n    if fit_scores.size > 0:\n        exp_scores = np.exp(fit_scores / temperature)\n        probabilities = exp_scores / np.sum(exp_scores)\n        priorities[can_fit_mask] = probabilities\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response3.txt_stdout.txt",
    "code_path": "problem_iter7_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit strategy with an exploration component.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it assigns a higher priority to bins that will have less remaining capacity\n    after the item is placed (Best Fit). Additionally, it incorporates a\n    temperature parameter for a softmax-like exploration, giving a chance\n    to bins that are not the absolute tightest fit but still viable.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        # Calculate the remaining capacity if the item is placed in a fitting bin\n        remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n        # Assign higher priority to bins that will have less remaining capacity\n        # This is equivalent to prioritizing bins where the item fills a larger\n        # proportion of the remaining space.\n        # We invert the remaining capacity so that smaller remaining capacity gets higher priority.\n        # Adding a small epsilon to avoid division by zero if remaining capacity is exactly 0.\n        # Using a score that is inversely proportional to remaining capacity.\n        scores = 1.0 / (remaining_after_placement + 1e-9)\n\n        # Introduce a temperature parameter for exploration.\n        # A higher temperature leads to more uniform probabilities (more exploration).\n        # A lower temperature leads to more deterministic selection (more exploitation).\n        temperature = 0.5  # Tunable parameter\n        \n        # Apply softmax to convert scores into probabilities/priorities\n        # Ensure numerical stability for softmax\n        exp_scores = np.exp(scores / temperature)\n        probabilities = exp_scores / np.sum(exp_scores)\n\n        priorities[can_fit_mask] = probabilities\n    \n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response4.txt_stdout.txt",
    "code_path": "problem_iter7_code4.py",
    "code": "import numpy as np\nfrom scipy.special import softmax\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it assigns a higher priority to bins that will have less remaining capacity\n    after the item is placed (exploitation). It uses a softmax function to\n    introduce a degree of exploration, where bins with good fits are more likely\n    to be chosen but not guaranteed. The temperature parameter can be tuned to\n    balance exploitation and exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        # Calculate the remaining capacity if the item is placed in a fitting bin\n        remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n        # Assign higher priority to bins that will have less remaining capacity\n        # This is equivalent to prioritizing bins where the item fills a larger\n        # proportion of the remaining space.\n        # We invert the remaining capacity so that smaller remaining capacity gets higher priority.\n        # Adding a small epsilon to avoid division by zero.\n        scores = 1.0 / (remaining_after_placement + 1e-9)\n\n        # Apply softmax to introduce exploration.\n        # A higher temperature means more exploration (probabilities are closer).\n        # A lower temperature means more exploitation (probabilities are more skewed towards the best).\n        # For this example, we'll use a fixed moderate temperature.\n        temperature = 0.5\n        probabilities = softmax(scores / temperature)\n\n        priorities[can_fit_mask] = probabilities\n    else:\n        # If no bin can fit the item, all priorities remain 0.\n        # In a real scenario, this might trigger opening a new bin,\n        # but the priority function itself just reflects suitability for existing bins.\n        pass\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  }
]