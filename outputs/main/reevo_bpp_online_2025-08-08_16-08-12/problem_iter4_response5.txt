```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a Softmax-Based Best Fit strategy with adaptive temperature.

    This heuristic prioritizes bins that have a remaining capacity closest to the item size,
    aiming for the "best fit". It calculates a fitness score based on the gap between
    the remaining capacity and the item size, preferring smaller non-negative gaps.
    The softmax function is then used to convert these fitness scores into priorities.
    The temperature parameter is adapted based on the spread of suitable gaps to control
    the exploration/exploitation balance.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Returns:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Calculate the "gap" for bins that can fit the item.
    # We want to minimize this gap (i.e., find the best fit).
    gaps = bins_remain_cap - item

    # Initialize fitness scores. Bins that cannot fit the item will have a very low score.
    # Using a large negative number to ensure they are not picked by softmax unless absolutely necessary.
    fitness_scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)

    # Identify bins that can accommodate the item.
    can_fit_mask = (gaps >= 0)
    valid_gaps = gaps[can_fit_mask]

    if np.any(can_fit_mask):
        # Adaptive temperature:
        # A smaller temperature makes the probability distribution sharper, favoring the best fit more.
        # A larger temperature makes the distribution flatter, allowing more exploration.
        # We can adapt the temperature based on the variance or range of suitable gaps.
        # If there's a wide range of gaps, a larger temperature might be suitable.
        # If gaps are very close, a smaller temperature is appropriate.
        if valid_gaps.size > 1:
            # Use standard deviation of valid gaps as a measure of spread.
            # Add a small constant to avoid division by zero if all gaps are identical.
            gap_spread = np.std(valid_gaps) + 1e-6
            # The temperature should be proportional to the spread.
            # A scaling factor can be tuned. Here, we use a simple linear relationship.
            temperature = gap_spread
        elif valid_gaps.size == 1:
            # If only one bin fits, temperature doesn't really matter for distinctiveness,
            # but setting it to a small value ensures exp(-gap/temp) is well-behaved.
            temperature = 1.0
        else:
            # This case should not be reached due to np.any(can_fit_mask) check,
            # but for robustness, set a default.
            temperature = 1.0

        # Ensure temperature is positive and not excessively small to avoid numerical instability.
        temperature = max(temperature, 1e-3)

        # Calculate fitness scores: exp(-gap / temperature). Smaller gap = higher score.
        scaled_gaps = valid_gaps / temperature
        positive_scores = np.exp(-scaled_gaps)

        fitness_scores[can_fit_mask] = positive_scores

    # Handle the case where no bins can fit the item.
    # In this scenario, all fitness scores are -np.inf. Softmax would result in NaNs.
    # A reasonable fallback is to assign uniform probabilities, as any bin choice is equally "bad"
    # and creating a new bin would be the ultimate action in a BPP solver.
    if not np.any(np.isfinite(fitness_scores)):
        # If no bins can fit, and there are bins available, assign uniform probability.
        if bins_remain_cap.size > 0:
            return np.ones_like(bins_remain_cap) / bins_remain_cap.size
        else:
            return np.array([], dtype=float) # Return empty array if no bins at all

    # Apply Softmax to convert fitness scores into probabilities (priorities).
    # Shift scores to avoid numerical issues with very small/large exponents in exp().
    # Subtracting the maximum finite score from all scores does not change the softmax output
    # but can help prevent overflow/underflow issues.
    finite_mask = np.isfinite(fitness_scores)
    if np.any(finite_mask):
        max_finite_score = np.max(fitness_scores[finite_mask])
        shifted_scores = fitness_scores - max_finite_score
    else:
        # This case is covered by the `if not np.any(np.isfinite(fitness_scores))` block,
        # but for absolute safety, handle it.
        return np.ones_like(bins_remain_cap) / bins_remain_cap.size if bins_remain_cap.size > 0 else np.array([], dtype=float)

    # Compute the exponentiated scores. Replace -inf with 0 to avoid exp(-inf) = 0 issues
    # which might incorrectly lead to a sum of zero if all were -inf.
    exp_scores = np.where(np.isfinite(shifted_scores), np.exp(shifted_scores), 0)

    sum_exp_scores = np.sum(exp_scores)

    # If the sum is zero (e.g., all original scores were -inf, or numerical issues),
    # fall back to uniform distribution among available bins.
    if sum_exp_scores == 0:
        return np.ones_like(bins_remain_cap) / bins_remain_cap.size if bins_remain_cap.size > 0 else np.array([], dtype=float)
    else:
        priorities = exp_scores / sum_exp_scores
        return priorities
```
