[
  {
    "stdout_filepath": "problem_iter8_response0.txt_stdout.txt",
    "code_path": "problem_iter8_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins for the online Bin Packing Problem.\n    This strategy prioritizes exact fits, then near fits using a soft-max approach\n    for exploration among near-fitting bins, and penalizes bins with very large remaining capacities.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array containing the remaining capacity of each bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        represents the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    num_bins = len(bins_remain_cap)\n\n    # Identify bins that can accommodate the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n    fitting_bins_remain_cap = bins_remain_cap[fitting_bins_indices]\n    remaining_after_placement = fitting_bins_remain_cap - item\n\n    # Assign very high priority for exact fits\n    exact_fit_mask_subset = (remaining_after_placement == 0)\n    exact_fit_indices = fitting_bins_indices[exact_fit_mask_subset]\n    priorities[exact_fit_indices] = 100.0\n\n    # Handle near fits with a softmax for exploration.\n    # Lower remaining capacity should lead to a higher score.\n    near_fit_mask_subset = ~exact_fit_mask_subset\n    near_fit_indices_subset = fitting_bins_indices[near_fit_mask_subset]\n\n    if np.any(near_fit_mask_subset):\n        near_fit_remaining = remaining_after_placement[near_fit_mask_subset]\n\n        # Calculate scores for near fits. We want to prioritize smaller remaining capacities.\n        # Using negative remaining capacity: smaller capacity -> less negative score -> higher priority.\n        # Add a small constant to ensure scores are not zero for softmax, and to keep them lower than exact fits.\n        # We also want to penalize bins with very large remaining capacity if they are not exact fits.\n        # A simple way is to make the priority decrease as remaining capacity increases beyond a certain point.\n        # Let's use a transformation that is high for small remaining, and tapers off.\n        # A negative exponential function could work: exp(-k * remaining_capacity)\n        # Or, simply use the negative remaining capacity and apply softmax.\n        # Let's ensure the scores are positive for softmax, and scale them.\n        \n        # Calculate \"desirability\" for near fits: higher for smaller remaining capacity.\n        # We can use a scale where 0 remaining capacity is best, and it degrades.\n        # Let's map the remaining capacities to a desirability score.\n        # Smallest remaining capacity should get highest score among near fits.\n        # Max value for near fits to avoid extreme values in exp\n        max_near_fit_remaining = np.max(near_fit_remaining) if near_fit_remaining.size > 0 else 0\n        \n        # Calculate scores that are higher for smaller remaining capacity.\n        # We scale by a factor to ensure they are less than exact fits.\n        # A simple approach: -(remaining_capacity).\n        # To make it suitable for softmax and exploration, we want values that differentiate.\n        # Let's consider the inverse of remaining capacity (plus a small epsilon to avoid division by zero).\n        # Or more simply, negative of remaining capacity, shifted and scaled.\n\n        # We want values that increase as remaining capacity decreases.\n        # `near_fit_remaining` values are >= 0.\n        # Let's transform `near_fit_remaining` into \"fitness scores\" where higher is better.\n        # Score = max_remaining_if_near_fit - current_remaining_if_near_fit\n        # This makes exact fits (0 remaining) have the highest score among near fits.\n        # But we already handled exact fits separately with a fixed high score.\n        # For near fits, we want to prioritize smaller `near_fit_remaining`.\n        \n        # Let's use -near_fit_remaining as base scores. Add a base value to shift it.\n        # A base value like 10.0 could be used.\n        # Score = 10.0 - near_fit_remaining\n        \n        # The problem is that `near_fit_remaining` can vary widely.\n        # If we want to use softmax for exploration, we need scores that can be exponentiated.\n        # Let's use the negative of remaining capacity as the base score.\n        # We need to scale and shift these to be positive for softmax and to be distinct from exact fits.\n        \n        # Calculate base priorities: prioritize smaller remaining capacity.\n        # We want to assign higher scores to bins with smaller `near_fit_remaining`.\n        # Let's use `-near_fit_remaining` as the raw score.\n        # To make it suitable for softmax, we can shift and scale it.\n        # Example: shift to make all values positive, then scale.\n        \n        # Calculate values that are higher for smaller remaining capacity.\n        # If remaining capacity is 0.1, score is higher than if it's 0.5.\n        # Let's try `1 / (near_fit_remaining + epsilon)`.\n        epsilon_small = 1e-6\n        near_fit_scores_for_softmax = 1.0 / (near_fit_remaining + epsilon_small)\n        \n        # Normalize these scores using softmax for exploration.\n        # Softmax will give higher probabilities to bins with smaller remaining capacity (larger scores).\n        temperature = 1.0  # Tunable parameter for exploration control\n\n        # Shift scores to prevent numerical underflow/overflow in exp\n        max_score = np.max(near_fit_scores_for_softmax)\n        shifted_scores = near_fit_scores_for_softmax - max_score\n        \n        exp_scores = np.exp(shifted_scores / temperature)\n        sum_exp_scores = np.sum(exp_scores)\n\n        if sum_exp_scores > 0:\n            softmax_probabilities = exp_scores / sum_exp_scores\n        else:\n            # Fallback to uniform probability if all scores are ~0 or -inf\n            softmax_probabilities = np.ones_like(exp_scores) / len(exp_scores)\n\n        # Scale these probabilities to a range that is lower than exact fits but reflects preference.\n        # For example, map probabilities to a range like [50, 90].\n        # Higher probability for smaller remaining capacity -> higher scaled score.\n        # The softmax probabilities range from ~0 to 1.\n        scaled_near_fit_priorities = 50.0 + softmax_probabilities * 40.0 # Range [50, 90)\n\n        priorities[near_fit_indices_subset] = scaled_near_fit_priorities\n\n    # Bins that can fit but were not assigned exact fit or near fit priority (shouldn't happen with current logic)\n    # or bins that could fit but had issues in scoring, will remain -np.inf.\n    # However, to be safe, we can assign a baseline priority to any bin that can fit but wasn't covered.\n    # This could happen if `near_fit_mask_subset` is empty, and `exact_fit_mask_subset` is also empty,\n    # but `can_fit_mask` is True. In this case, the item is smaller than remaining capacity but not an exact fit,\n    # and no other near fits were processed.\n    # This implies `near_fit_remaining` was empty. This scenario should be covered by the `if np.any(near_fit_mask_subset):` block.\n\n    # Any bin that *can* fit but has no priority assigned yet should get a default low priority.\n    # For example, any remaining `can_fit_mask` that are not `exact_fit_indices` and not `near_fit_indices_subset`.\n    # This should logically not occur given how near_fit_mask_subset is derived.\n    # If a bin can fit, it's either an exact fit or a near fit.\n    \n    # Let's ensure that any bin that *can* fit has at least a minimal positive priority if not an exact fit.\n    # This default priority should be lower than any near-fit priority.\n    default_low_priority = 1.0\n    unassigned_fitting_bins = np.where(can_fit_mask & (priorities == -np.inf))[0]\n    priorities[unassigned_fitting_bins] = default_low_priority\n\n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "SLOC": 34.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response1.txt_stdout.txt",
    "code_path": "problem_iter8_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit strategy with explicit prioritization of exact fits and temperature-controlled exploration.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it first gives the highest priority to exact fits. For bins that are not\n    exact fits but can accommodate the item, it assigns priority based on\n    how much remaining capacity is left, favoring smaller remaining capacities.\n    To encourage exploration, a softmax function is applied to the non-exact fits\n    with a temperature parameter. A smaller temperature leads to more exploitation (closer to greedy),\n    while a larger temperature leads to more exploration (closer to uniform).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    temperature = 0.1  # Tunable parameter for exploration. Lower = more greedy.\n\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can accommodate the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    # Calculate remaining capacity if item is placed\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_placement = fitting_bins_remain_cap - item\n\n    # Assign highest priority to exact fits\n    exact_fit_mask_subset = (remaining_after_placement == 0)\n    priorities[can_fit_mask][exact_fit_mask_subset] = 100.0\n\n    # For bins that fit but are not exact fits, calculate scores for softmax\n    non_exact_fit_indices_subset = np.where(~exact_fit_mask_subset)[0]\n\n    if non_exact_fit_indices_subset.size > 0:\n        non_exact_fitting_bins_remain_cap = fitting_bins_remain_cap[~exact_fit_mask_subset]\n        non_exact_remaining_after_placement = non_exact_fitting_bins_remain_cap - item\n\n        # We want to prioritize smaller remaining capacities.\n        # Using the negative of remaining capacity means smaller remaining capacity\n        # yields a larger (less negative) score, which is good for softmax.\n        # Add epsilon to avoid division by zero in case of floating point inaccuracies\n        # if remaining_after_placement is very close to zero but not exactly zero.\n        # However, a simpler approach is to just use the negative remaining capacity.\n        # Let's use negative remaining capacity as the score for softmax.\n        # A smaller remaining_after_placement will result in a less negative score.\n        fit_scores_for_softmax = -non_exact_remaining_after_placement\n\n        # Apply softmax for exploration. Higher score (less remaining capacity) means higher priority.\n        # To ensure numerical stability with exp, shift scores so the maximum is 0.\n        if fit_scores_for_softmax.size > 0:\n            shifted_fit_scores = fit_scores_for_softmax - np.max(fit_scores_for_softmax)\n            # Handle potential NaNs if fit_scores were all Inf or -Inf (unlikely here)\n            if np.any(np.isnan(shifted_fit_scores)):\n                 shifted_fit_scores = np.nan_to_num(shifted_fit_scores, nan=-1e9)\n\n            # Calculate the softmax values. The relative magnitude is what matters.\n            # Higher value for smaller remaining_after_placement.\n            soft_priorities = np.exp(shifted_fit_scores / temperature)\n\n            # Scale these priorities to be less than the exact fit priority (100.0)\n            # and ensure they are positive.\n            # The maximum possible value of soft_priorities (before scaling) is 1.0.\n            # We can scale them by a factor like 99.0 to ensure they are lower than exact fits.\n            scale_factor = 99.0\n            priorities[can_fit_mask][~exact_fit_mask_subset] = soft_priorities * scale_factor\n        else:\n            # If no non-exact fits (all were exact fits)\n            pass\n\n    return priorities",
    "response_id": 1,
    "obj": 4.487435181491823,
    "SLOC": 25.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response2.txt_stdout.txt",
    "code_path": "problem_iter8_code2.py",
    "code": "import numpy as np\n\ndef softmax(x, temp=1.0):\n    \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n    e_x = np.exp(x / temp)\n    return e_x / e_x.sum(axis=0)\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins for the online Bin Packing Problem using a softmax-based\n    approach to balance \"Best Fit\" (exploitation) with exploration.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it assigns a higher priority to bins that will have less remaining capacity\n    after the item is placed (exploitation). The softmax function is used to\n    convert these \"fit scores\" into probabilities, introducing a degree of\n    exploration. Bins with better fits are more likely to be chosen, but not\n    guaranteed, depending on the temperature parameter.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array containing the remaining capacity of each bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        represents the probability (priority score) of placing the item in the\n        corresponding bin.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins, dtype=float)\n\n    # Identify bins that have enough capacity to fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        # Calculate the \"goodness\" of fit for bins that can accommodate the item.\n        # A smaller remaining capacity after placement indicates a better fit.\n        # We want to maximize this \"goodness\", so we use the inverse of\n        # remaining capacity. Adding a small epsilon to avoid division by zero.\n        remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n        fit_scores = 1.0 / (remaining_after_placement + 1e-9)\n\n        # Apply softmax to convert fit scores into probabilities.\n        # The 'temperature' parameter controls the balance between exploitation and exploration.\n        # A lower temperature (e.g., 0.1) makes the probabilities sharper, favoring the best fit more.\n        # A higher temperature (e.g., 1.0 or more) makes the probabilities smoother,\n        # increasing the chance of picking less optimal fits.\n        temperature = 0.5  # Tunable parameter\n        probabilities = softmax(fit_scores, temp=temperature)\n\n        # Assign these probabilities as priorities to the bins that can fit the item.\n        priorities[can_fit_mask] = probabilities\n    \n    # Bins that cannot fit the item will have a priority of 0.\n    # In a real application, if all priorities are 0, a new bin would be opened.\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response3.txt_stdout.txt",
    "code_path": "problem_iter8_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best-Fit approach with explicit prioritization of exact fits.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it first gives the highest priority to exact fits. For bins that are not\n    exact fits but can accommodate the item, it assigns priority based on\n    how much remaining capacity is left, favoring smaller remaining capacities.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can accommodate the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_placement = fitting_bins_remain_cap - item\n\n    # Assign highest priority to exact fits.\n    exact_fit_mask_subset = (remaining_after_placement == 0)\n    # Use a very high priority for exact fits\n    priorities[can_fit_mask][exact_fit_mask_subset] = 1000.0\n\n    # For bins that fit but are not exact fits, assign priorities.\n    # Prioritize smaller remaining capacities (closer to zero).\n    near_fit_mask_subset = ~exact_fit_mask_subset\n    if np.any(near_fit_mask_subset):\n        near_fit_priorities_raw = remaining_after_placement[near_fit_mask_subset]\n        \n        # Invert remaining capacity to prioritize smaller values. Add a small epsilon for stability.\n        # The larger the value (less remaining capacity), the higher the priority.\n        priorities[can_fit_mask][near_fit_mask_subset] = 1.0 / (near_fit_priorities_raw + 1e-9)\n\n    return priorities",
    "response_id": 3,
    "obj": 4.487435181491823,
    "SLOC": 14.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response4.txt_stdout.txt",
    "code_path": "problem_iter8_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a First Fit strategy.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it assigns a higher priority to bins that will have less remaining capacity\n    after the item is placed. This is a greedy approach aiming to fill bins\n    as much as possible, encouraging denser packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to a very low value for bins that cannot fit the item.\n    # Bins that can fit will have their priorities calculated.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that have enough capacity to fit the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate their priority.\n    # The priority is the negative of the remaining capacity after placing the item.\n    # This means bins with smaller remaining capacity (tighter fits) will have higher priorities.\n    remaining_capacities_after_fit = bins_remain_cap[can_fit_mask] - item\n    priorities[can_fit_mask] = -remaining_capacities_after_fit\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response5.txt_stdout.txt",
    "code_path": "problem_iter8_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority for placing an item into bins, prioritizing minimal slack\n    while also encouraging exploration of bins with slightly more remaining\n    capacity, and considering the size of the current item.\n\n    This heuristic prioritizes bins that can fit the item. Among fitting bins,\n    it first favors those that result in minimal remaining capacity (minimal slack).\n    To encourage exploration and adaptability to future items, it uses a temperature\n    parameter to give a chance to bins that might have slightly more slack,\n    especially when the current item is small. It also scales the priority\n    based on the item size, giving higher weight to packing larger items tightly.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can accommodate the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_placement = fitting_bins_remain_cap - item\n\n    # --- Primary objective: Minimize slack ---\n    # Higher priority for smaller remaining capacity.\n    # Add epsilon to prevent division by zero and ensure non-zero slack gets a finite score.\n    slack_priorities = 1.0 / (remaining_after_placement + 1e-9)\n\n    # --- Exploration and Item Size Consideration ---\n    # Temperature for softmax. Higher temperature means more exploration.\n    # Make temperature sensitive to item size: smaller items might benefit from more exploration.\n    # A simple approach: inverse relationship with item size, capped.\n    temperature = max(0.1, 2.0 / (item + 1e-9))\n\n    # Apply softmax-like weighting to allow exploration of bins with slightly more slack.\n    # We want to amplify the differences for tight fits but not completely ignore others.\n    # Using negative slack directly as input to exp, so smaller slack (larger negative value) becomes larger exp output.\n    # Normalizing for numerical stability.\n    if np.any(slack_priorities):\n        normalized_slack_priorities = slack_priorities - np.max(slack_priorities)\n        exploration_weights = np.exp(normalized_slack_priorities / temperature)\n\n        # Combine primary objective (slack) with exploration weight.\n        # Multiply slack priorities by exploration weights. This means bins with good slack\n        # AND good exploration weight (i.e., not too much slack) get boosted.\n        # The exploration_weights are roughly between 0 and 1 (after normalization and exp).\n        # We can scale them to make sure they don't dominate the base slack priority.\n        # A simple multiplication can work.\n        combined_priorities = slack_priorities * exploration_weights\n    else:\n        combined_priorities = slack_priorities # Should not happen if any(can_fit_mask)\n\n    # --- Refinement: Penalize exact fits slightly? (Optional, based on broader strategy) ---\n    # The original v0 penalized exact fits. v1 penalizes them.\n    # Let's consider a softer approach: slightly reduce priority if it's an exact fit,\n    # but only if the item is large, to encourage packing large items tightly.\n    # If item is small, exact fits might be good to avoid fragmentation.\n    penalty_strength = 0.05 # Small penalty\n    exact_fit_mask_subset = (remaining_after_placement == 0)\n\n    # Apply a small penalty to exact fits, but less so for larger items where exact fits are valuable.\n    # Inverse of item size (scaled) to reduce penalty for larger items.\n    item_size_factor = 1.0 / (item + 1e-9)\n    penalty_applied_to_exact_fits = penalty_strength * item_size_factor\n\n    # Only apply penalty if it's beneficial (i.e., reduce priority for exact fits)\n    # The combined_priorities are already high for exact fits (due to 1/epsilon).\n    # Subtracting a small value from these high values might make them less dominant but still high.\n    # We subtract a fraction of the penalty to make it less aggressive.\n    # Let's make it a direct subtraction from the priority score.\n    combined_priorities[exact_fit_mask_subset] -= penalty_applied_to_exact_fits\n\n\n    # Assign final priorities\n    priorities[can_fit_mask] = combined_priorities\n\n    # Ensure priorities are non-negative.\n    priorities = np.clip(priorities, 0, None)\n\n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "SLOC": 23.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response6.txt_stdout.txt",
    "code_path": "problem_iter8_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins for the online Bin Packing Problem using a Best Fit strategy\n    with a temperature-controlled exploration component.\n\n    This heuristic prioritizes bins that can fit the item. Among fitting bins,\n    it assigns a higher priority to those that leave less remaining capacity\n    after the item is placed (Best Fit). To balance exploitation (tight fits)\n    with exploration, it uses an exponential function of the negative remaining\n    capacity, scaled by a temperature parameter. A higher temperature leads to\n    smoother priority distributions, increasing the chance of selecting bins\n    that are not the absolute best fit.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array containing the remaining capacity of each bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        represents the priority score for placing the item in the corresponding bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    # Temperature parameter to control the exploration/exploitation trade-off.\n    # A higher temperature leads to flatter probabilities (more exploration),\n    # while a lower temperature leads to sharper probabilities (more exploitation).\n    # We aim for a balance, let's set temperature to a moderate value like 0.5 or 1.0.\n    temperature = 0.75\n\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins, dtype=float)\n\n    # Identify bins that have enough capacity to fit the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    # Lower values here are better (tighter fits).\n    remaining_capacities_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # We want higher priority for bins with smaller remaining capacity.\n    # A common way to achieve this with exploration is using an exponential function:\n    # priority_score = exp(f(remaining_capacity))\n    # where f should be decreasing with remaining_capacity.\n    # A simple choice for f is -remaining_capacity / temperature.\n    # This way, smaller remaining_capacity leads to a less negative exponent,\n    # thus a higher exponential value (priority).\n    # Using `remaining_capacities_after_fit` directly as the \"cost\".\n    # The \"value\" of a bin is inversely related to this cost.\n    # So, we use `exp(-cost / temperature)`.\n\n    # Calculate the raw \"fitness\" score for fitting bins.\n    # We add a small epsilon to the denominator in case remaining_capacities_after_fit is 0\n    # to avoid division by zero in some formulations, though exp(-x) handles 0 fine.\n    # Using -remaining_capacities_after_fit directly for the exponent argument is cleaner.\n    \n    # The score is higher for smaller `remaining_capacities_after_fit`.\n    # Let's use `score = -remaining_capacities_after_fit`. Higher score is better.\n    # For exploration, we can use `exp(score / temperature)`.\n    # This will give higher priorities to tighter fits but also assign non-zero\n    # priorities to bins with slightly more slack, governed by the temperature.\n    \n    fitness_scores = -remaining_capacities_after_fit / temperature\n    priorities[can_fit_mask] = np.exp(fitness_scores)\n\n    # Ensure that priorities for bins that cannot fit remain 0.\n    # This is already handled by the initialization and mask.\n\n    return priorities",
    "response_id": 6,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response7.txt_stdout.txt",
    "code_path": "problem_iter8_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a\n    strategy that prioritizes exact fits and then uses softmax for exploration.\n    This version introduces a temperature parameter for softmax to control\n    exploration/exploitation balance and explicitly handles exact fits with\n    a high base priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9  # Small value for numerical stability\n    temperature = 0.1 # Temperature for softmax. Lower temp -> more exploitative.\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return np.zeros_like(bins_remain_cap, dtype=float)\n\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # Calculate the remaining capacity after placing the item.\n    # Smaller remaining capacity is better.\n    remaining_after_placement = fitting_bins_remain_cap - item\n\n    # Assign base priorities:\n    # Exact fits get a very high base priority (e.g., 100).\n    # Near fits get a priority based on how close they are to an exact fit.\n    # We use 1 / (remainder + epsilon) for near fits, making smaller remainders have higher scores.\n    # To ensure exact fits dominate and near fits have a spectrum, we can structure this.\n    base_scores = np.zeros_like(remaining_after_placement)\n\n    exact_fit_mask = (remaining_after_placement < epsilon)\n    near_fit_mask = ~exact_fit_mask\n\n    # High base score for exact fits\n    base_scores[exact_fit_mask] = 100.0\n\n    # For near fits, use inverse of remaining capacity, scaled.\n    # A simple scaling factor can be applied to keep values reasonable.\n    # A factor of 10 might be good, so a remainder of 0.1 gets a score of 10.\n    if np.any(near_fit_mask):\n        # Using 1/(remainder) as the core idea for \"goodness\".\n        # Add a small constant to avoid division by zero, though epsilon is already there.\n        # A scaling factor can also be used. Let's use the max remainder to normalize.\n        max_remainder_for_near_fits = np.max(remaining_after_placement[near_fit_mask])\n        if max_remainder_for_near_fits > epsilon:\n            # Normalize the 'goodness' by dividing by the max remainder.\n            # This makes the best near-fit have a score of 1.\n            normalized_near_fit_scores = (1.0 / (remaining_after_placement[near_fit_mask] + epsilon))\n            max_normalized_score = np.max(normalized_near_fit_scores)\n            if max_normalized_score > epsilon:\n                scaled_near_fit_scores = normalized_near_fit_scores / max_normalized_score\n            else:\n                scaled_near_fit_scores = np.ones_like(normalized_near_fit_scores) # Handle edge case\n        else:\n            scaled_near_fit_scores = np.ones_like(remaining_after_placement[near_fit_mask]) # If all near fits are ~0 remainder\n\n        base_scores[near_fit_mask] = scaled_near_fit_scores\n\n    # Apply softmax with temperature to the base scores.\n    # This balances exploitation (high base_scores get higher probability)\n    # with exploration (lower base_scores still have a chance).\n    # Softmax formula: exp(score / temperature) / sum(exp(score / temperature))\n    try:\n        # Divide by temperature before exponentiation\n        scaled_scores = base_scores / temperature\n        exp_scores = np.exp(scaled_scores)\n        sum_exp_scores = np.sum(exp_scores)\n\n        if sum_exp_scores > epsilon:\n            probabilities = exp_scores / sum_exp_scores\n        else:\n            # If sum_exp_scores is near zero (e.g., due to very low temp and all scores being very negative),\n            # assign uniform probabilities.\n            probabilities = np.ones_like(base_scores) / len(base_scores)\n    except OverflowError:\n        # Handle potential overflow if scaled_scores become too large.\n        # In such cases, effectively all probability will be on the max score.\n        # A simple way is to treat it as an argmax selection.\n        max_score_idx = np.argmax(base_scores)\n        probabilities = np.zeros_like(base_scores)\n        probabilities[max_score_idx] = 1.0\n\n    # Assign these probabilities as priorities to the bins that can fit the item.\n    priorities[can_fit_mask] = probabilities\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "SLOC": 40.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response8.txt_stdout.txt",
    "code_path": "problem_iter8_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin, prioritizing\n    minimal slack after placement. This version refines the penalty for exact\n    fills and introduces a more nuanced exploratory boost by favoring bins with\n    small positive remaining capacity.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it assigns a higher priority to bins that will have less remaining capacity\n    after the item is placed (minimal slack). This encourages denser packing.\n    A more aggressive penalty is applied to bins that would become completely full,\n    more strongly discouraging exact fits. To introduce a more structured\n    exploratory element, bins that result in a small, positive remainder (e.g.,\n    up to 10% of the item's size) are given a moderate boost to their priority.\n    This encourages leaving a useful, but not excessive, amount of space.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the remaining capacity if the item is placed in a fitting bin\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # Base priority: prioritize bins with minimal slack.\n    # Higher score for smaller remainders. Add epsilon for numerical stability.\n    # Using negative of remaining capacity to ensure higher priority for smaller values.\n    base_priorities = -remaining_after_placement\n\n    # Penalty for bins that would become exactly full.\n    # Subtract a larger value to strongly discourage exact fits.\n    penalty_for_full = 1.0\n    fully_filled_mask = (remaining_after_placement == 0)\n    priorities[can_fit_mask] = base_priorities\n    priorities[can_fit_mask][fully_filled_mask] -= penalty_for_full\n\n    # Exploratory boost: favor bins that have a small positive remainder.\n    # A small positive remainder is considered to be up to 10% of the item's size.\n    # This is a heuristic value and can be tuned.\n    exploratory_boost_threshold = 0.1 * item\n    small_positive_remainder_mask = (remaining_after_placement > 0) & (remaining_after_placement <= exploratory_boost_threshold)\n    exploratory_boost_value = 0.5  # Moderate boost value\n    priorities[can_fit_mask][small_positive_remainder_mask] += exploratory_boost_value\n\n    # Normalize priorities to be non-negative.\n    # Find the minimum priority among valid choices and shift all priorities up.\n    # This ensures that the relative ordering is maintained while making values non-negative.\n    valid_priorities = priorities[can_fit_mask]\n    if valid_priorities.size > 0:\n        min_valid_priority = np.min(valid_priorities)\n        # If min_valid_priority is negative, shift all valid priorities to be >= 0\n        if min_valid_priority < 0:\n            priorities[can_fit_mask] -= min_valid_priority\n\n    # Ensure all non-fitting bins have the lowest possible priority (effectively zero after shift)\n    priorities[~can_fit_mask] = 0.0\n\n    return priorities",
    "response_id": 8,
    "obj": 4.048663741523748,
    "SLOC": 22.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response9.txt_stdout.txt",
    "code_path": "problem_iter8_code9.py",
    "code": "import numpy as np\n\ndef softmax(x):\n    \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it assigns a higher priority to bins that will have less remaining capacity\n    after the item is placed (exploitation). It uses a softmax function to\n    introduce a degree of exploration, where bins with good fits are more likely\n    to be chosen but not guaranteed. The temperature parameter can be tuned to\n    balance exploitation and exploration.\n\n    This version refines the scoring for \"near fits\" to better differentiate\n    between them, and also explicitly handles exact fits with a very high score\n    before applying softmax to other near fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_placement = fitting_bins_cap - item\n\n    # Scores for bins that can fit the item\n    scores = np.zeros_like(fitting_bins_cap, dtype=float)\n\n    # Prioritize exact fits with a very high score\n    exact_fit_indices = np.where(remaining_after_placement == 0)[0]\n    scores[exact_fit_indices] = 1e9  # A very large score for exact fits\n\n    # For near fits, we want to prioritize smaller remaining capacities.\n    # We use a transformation that rewards smaller remainders more strongly.\n    # A simple inverse can be sensitive to very small remainders.\n    # A small negative score for remaining capacity can work,\n    # or a score that penalizes larger remaining space.\n    # Let's try to give a score that is proportional to how 'tight' the fit is,\n    # but also differentiate near fits from exact fits.\n    # We can use 1 / (remainder + epsilon) and add a bonus for exact fits.\n    near_fit_indices = np.where(remaining_after_placement > 0)[0]\n    if len(near_fit_indices) > 0:\n        # Use inverse of remaining capacity. Add a small constant to avoid\n        # huge values for tiny remainders and to slightly differentiate them.\n        # A small positive score for near fits.\n        scores[near_fit_indices] = 1.0 / (remaining_after_placement[near_fit_indices] + 0.1)\n\n    # Ensure exact fits still dominate if they exist.\n    # If there are exact fits, they have very high scores already.\n    # If only near fits, softmax will operate on those scores.\n\n    # Apply softmax to introduce exploration.\n    # A moderate temperature to balance exploration/exploitation.\n    temperature = 0.5\n    if scores.size > 0:\n        probabilities = softmax(scores / temperature)\n        priorities[can_fit_mask] = probabilities\n    # If there were no fitting bins, priorities remain 0, which is handled by the initial check.\n\n    return priorities",
    "response_id": 9,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  }
]