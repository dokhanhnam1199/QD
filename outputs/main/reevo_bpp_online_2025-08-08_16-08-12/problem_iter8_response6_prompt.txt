{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritizes bins for the online Bin Packing Problem using a hybrid approach\n    that balances \"Best Fit\" (exploitation) with a small chance of \"First Fit\"\n    or random choice among fitting bins (exploration).\n\n    The \"Best Fit\" component prioritizes bins that leave the least remaining\n    capacity after the item is placed. This is achieved by assigning a priority\n    that is the negative of the remaining capacity after fitting the item.\n    A tighter fit (smaller remaining capacity) results in a higher (less negative)\n    priority.\n\n    The exploration component is introduced to prevent getting stuck in local\n    optima. With a small probability (epsilon), it favors bins that are not\n    necessarily the best fit, by assigning a random priority among those that\n    can fit the item. This allows for exploring different packing configurations.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array containing the remaining capacity of each bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        represents the priority score for placing the item in the corresponding bin.\n    \"\"\"\n    epsilon = 0.05  # Probability of exploration\n    num_bins = len(bins_remain_cap)\n\n    # Initialize priorities to negative infinity for all bins\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that have enough capacity to fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity for bins that can fit the item\n    remaining_capacities_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # --- Exploitation Component (Best Fit) ---\n    # Assign priorities based on the tightness of the fit.\n    # The negative of the remaining capacity is used: smaller remaining capacity\n    # leads to a less negative (higher) priority.\n    exploitation_scores = np.full_like(bins_remain_cap, -np.inf)\n    exploitation_scores[can_fit_mask] = -remaining_capacities_after_fit\n\n    # --- Exploration Component ---\n    # Generate random scores for exploration among fitting bins.\n    # We want to give a chance to bins that might not be the \"best\" according to Best Fit.\n    # A simple approach is to assign a random value if exploration is chosen.\n    exploration_scores_random = np.random.rand(num_bins)\n\n    # Combine exploitation and exploration using an epsilon-greedy strategy.\n    # Generate a random choice for each bin: True for exploration, False for exploitation.\n    explore_choice_mask = np.random.rand(num_bins) < epsilon\n\n    # For bins where exploration is chosen AND they can fit the item, use the random score.\n    # For bins where exploitation is chosen AND they can fit the item, use the exploitation score.\n    # For bins that cannot fit, their priority remains -np.inf.\n    \n    # Create a mask for bins that are candidates for selection (can fit the item)\n    candidate_mask = can_fit_mask\n    \n    # Apply exploration choice: if exploration is chosen for a candidate bin, use its random score.\n    priorities[explore_choice_mask & candidate_mask] = exploration_scores_random[explore_choice_mask & candidate_mask]\n    \n    # Apply exploitation choice: if exploitation is chosen for a candidate bin, use its exploitation score.\n    # We only update if the bin was NOT chosen for exploration (or if exploration wasn't chosen for it).\n    # The condition `~explore_choice_mask` ensures we only apply exploitation if exploration wasn't picked.\n    priorities[~explore_choice_mask & candidate_mask] = exploitation_scores[~explore_choice_mask & candidate_mask]\n\n    # Ensure that bins that cannot fit remain at -np.inf\n    priorities[~can_fit_mask] = -np.inf\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit strategy with an exploration component.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it assigns a higher priority to bins that will have less remaining capacity\n    after the item is placed (Best Fit). To balance exploitation with exploration,\n    it uses a softmax-like approach where bins with slightly more remaining\n    capacity also have a non-negligible chance of being selected, controlled by\n    a temperature parameter. This can help avoid getting stuck in locally optimal\n    packings.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity if the item is placed in a fitting bin\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # We want to prioritize smaller remaining capacities (tighter fits).\n    # A simple way to do this is to use the negative of the remaining capacity\n    # or its reciprocal. Here, we use the reciprocal for a strong preference for tight fits.\n    # To introduce exploration, we can add a small noise or use a temperature parameter\n    # in a softmax-like way. For simplicity here, we'll adjust the values to be\n    # more distributed. A common approach is to use e^(score/temperature).\n    # Let's define a score that favors smaller remaining_after_placement.\n    # We can use -remaining_after_placement directly for a simpler form of \"best fit\" priority,\n    # or use a scaling that might encourage more variety.\n\n    # Option 1: Simple Best Fit (similar to v1 but expressed differently)\n    # Higher priority for smaller remaining_after_placement\n    # priorities[can_fit_mask] = -remaining_after_placement\n\n    # Option 2: Softmax-like exploration.\n    # We want scores to be positive and higher for better fits.\n    # Let's map remaining_after_placement to a score where smaller is better.\n    # A potential mapping: score = -remaining_after_placement.\n    # For softmax: exp(score / temperature).\n    # A temperature of 1 means exponential scaling of the negative remaining capacity.\n    # A higher temperature smooths out the probabilities, increasing exploration.\n    # Let's use a temperature that's not too aggressive, e.g., 0.5 or 1.0.\n    # For simplicity, let's consider the \"value\" of a bin as the negative remaining capacity\n    # after placement, so a higher \"value\" is better (less remaining space).\n\n    # To encourage a mix, let's use a linear transformation of remaining_after_placement.\n    # A common technique is to use `1 / (remaining_after_placement + epsilon)` as in v1,\n    # but to add some exploration, we can slightly shift or scale these values.\n    # Or, consider `remaining_after_placement` directly. Lower is better.\n    # For priority, higher should be better. So, we want to transform `remaining_after_placement`\n    # such that smaller values result in larger priorities.\n\n    # Let's try a strategy that favors tight fits but also gives some chance to bins with\n    # slightly more space.\n    # A \"goodness\" score could be `bin_capacity - item - remaining_after_placement`.\n    # But we want to prioritize bins that minimize `remaining_after_placement`.\n    # So, let's use a function of `remaining_after_placement` where smaller values are better.\n\n    # Strategy: Prioritize bins with small `remaining_after_placement`.\n    # To add exploration, we can make the scores less extreme.\n    # Instead of `1 / (remaining_after_placement + epsilon)`, consider `exp(-remaining_after_placement / temperature)`.\n    # Let's use a temperature parameter. A temperature of 1 means exp(-x).\n    # A temperature of 0.1 would make it `exp(-10x)`, very sharp.\n    # A temperature of 10 would make it `exp(-x/10)`, flatter.\n    # Let's choose a moderate temperature, say T=1.0, to balance.\n\n    # Avoid division by zero and ensure non-negative scores.\n    # We want smaller `remaining_after_placement` to result in higher priority.\n    # Let's create scores based on `remaining_after_placement`.\n    # Consider a \"score\" which is the inverse of remaining capacity after placement.\n    # To add exploration, we can use a temperature parameter.\n    # `score = 1.0 / (remaining_after_placement + 1e-9)` is good for best fit.\n    # To explore, we can perturb this or use an exponential.\n\n    # Let's consider the \"benefit\" of placing the item in a bin as the proportion of\n    # remaining capacity filled by the item. This is `item / bins_remain_cap[can_fit_mask]`.\n    # However, this is for the *original* bin capacity, not the remaining capacity.\n    # The goal is to minimize the number of bins, so filling them up is good.\n    # `remaining_after_placement` is a direct measure of how much \"wasted\" space is left.\n    # Minimizing this is good. So higher priority for smaller `remaining_after_placement`.\n\n    # Let's try a score that is a linear function of `1/remaining_after_placement`\n    # plus a small constant to ensure all fitting bins have positive priority,\n    # and then apply a scaling to control exploration.\n\n    # Using `1.0 / (remaining_after_placement + 1e-9)` as the base for tight fits.\n    # To add exploration, we can add a small, positive random noise to these scores.\n    # However, this might make the \"best fit\" less predictable.\n    # A more principled way is to use a temperature parameter with an exponential function.\n\n    # Let's map `remaining_after_placement` to a \"quality\" score where lower is better.\n    # We can use `remaining_after_placement` itself.\n    # For priority, we want higher values to be better.\n    # So, priority can be a function that maps smaller `remaining_after_placement` to higher values.\n\n    # Let's use a score related to the *inverse* of remaining capacity after placement,\n    # but with a moderating factor for exploration.\n    # `score = -remaining_after_placement` (higher is better).\n    # To add exploration, we can use `exp(score / temperature)`.\n    # Let's set a temperature, e.g., `temperature = 1.0`.\n\n    # Calculate values that are higher for better fits (smaller remaining_after_placement).\n    # We can use `1.0 / (remaining_after_placement + epsilon)` or `-remaining_after_placement`.\n    # Let's use `-remaining_after_placement` as the base score (more negative is worse).\n    # A simple way to make priorities positive and reflect \"goodness\" of fit:\n    # `priority_base = max_possible_remaining_capacity - remaining_after_placement`\n    # or simply, `priority_base = some_large_number - remaining_after_placement`.\n    # Or `priority_base = 1 / (remaining_after_placement + epsilon)`.\n\n    # Let's consider `remaining_after_placement` directly as the \"cost\". We want to minimize cost.\n    # For priority, we want to maximize it.\n    # Priority = `C - remaining_after_placement` where C is a constant.\n    # To add exploration, we can smooth this.\n\n    # Let's try this: priority is proportional to `exp(-remaining_after_placement / temperature)`.\n    # A temperature of 1.0 means we assign priorities based on the exponential of\n    # the negative remaining space. This gives higher priority to tight fits but\n    # also non-zero priority to bins with slightly more space.\n\n    temperature = 1.0  # Controls the exploration/exploitation trade-off. Higher T -> more exploration.\n    scores = -remaining_after_placement / temperature\n    priorities[can_fit_mask] = np.exp(scores)\n\n    # Normalize probabilities to sum to 1 for the fitting bins if needed for some applications,\n    # but for priority scores, raw values are often sufficient.\n    # If normalization is desired for a softmax-like selection:\n    # fitting_priorities = np.exp(scores)\n    # if np.sum(fitting_priorities) > 0:\n    #     priorities[can_fit_mask] = fitting_priorities / np.sum(fitting_priorities)\n    # else:\n    #     priorities[can_fit_mask] = 1.0 / len(priorities[can_fit_mask]) if len(priorities[can_fit_mask]) > 0 else 0\n\n    # For now, let's keep the raw exponential scores. The agent selecting the bin\n    # would typically pick the argmax or use a softmax over these scores.\n\n    return priorities\n\n[Reflection]\nPrioritize tight fits, but smooth for exploration using temperature or noise.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}