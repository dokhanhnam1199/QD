```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority with which we want to add item to each bin using a
    strategy that prioritizes exact fits and then uses softmax for exploration.
    This version introduces a temperature parameter for softmax to control
    exploration/exploitation balance and explicitly handles exact fits with
    a high base priority.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Returns:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    epsilon = 1e-9  # Small value for numerical stability
    temperature = 0.1 # Temperature for softmax. Lower temp -> more exploitative.

    # Identify bins that can accommodate the item
    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        return np.zeros_like(bins_remain_cap, dtype=float)

    fitting_bins_indices = np.where(can_fit_mask)[0]
    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]

    # Calculate the remaining capacity after placing the item.
    # Smaller remaining capacity is better.
    remaining_after_placement = fitting_bins_remain_cap - item

    # Assign base priorities:
    # Exact fits get a very high base priority (e.g., 100).
    # Near fits get a priority based on how close they are to an exact fit.
    # We use 1 / (remainder + epsilon) for near fits, making smaller remainders have higher scores.
    # To ensure exact fits dominate and near fits have a spectrum, we can structure this.
    base_scores = np.zeros_like(remaining_after_placement)

    exact_fit_mask = (remaining_after_placement < epsilon)
    near_fit_mask = ~exact_fit_mask

    # High base score for exact fits
    base_scores[exact_fit_mask] = 100.0

    # For near fits, use inverse of remaining capacity, scaled.
    # A simple scaling factor can be applied to keep values reasonable.
    # A factor of 10 might be good, so a remainder of 0.1 gets a score of 10.
    if np.any(near_fit_mask):
        # Using 1/(remainder) as the core idea for "goodness".
        # Add a small constant to avoid division by zero, though epsilon is already there.
        # A scaling factor can also be used. Let's use the max remainder to normalize.
        max_remainder_for_near_fits = np.max(remaining_after_placement[near_fit_mask])
        if max_remainder_for_near_fits > epsilon:
            # Normalize the 'goodness' by dividing by the max remainder.
            # This makes the best near-fit have a score of 1.
            normalized_near_fit_scores = (1.0 / (remaining_after_placement[near_fit_mask] + epsilon))
            max_normalized_score = np.max(normalized_near_fit_scores)
            if max_normalized_score > epsilon:
                scaled_near_fit_scores = normalized_near_fit_scores / max_normalized_score
            else:
                scaled_near_fit_scores = np.ones_like(normalized_near_fit_scores) # Handle edge case
        else:
            scaled_near_fit_scores = np.ones_like(remaining_after_placement[near_fit_mask]) # If all near fits are ~0 remainder

        base_scores[near_fit_mask] = scaled_near_fit_scores

    # Apply softmax with temperature to the base scores.
    # This balances exploitation (high base_scores get higher probability)
    # with exploration (lower base_scores still have a chance).
    # Softmax formula: exp(score / temperature) / sum(exp(score / temperature))
    try:
        # Divide by temperature before exponentiation
        scaled_scores = base_scores / temperature
        exp_scores = np.exp(scaled_scores)
        sum_exp_scores = np.sum(exp_scores)

        if sum_exp_scores > epsilon:
            probabilities = exp_scores / sum_exp_scores
        else:
            # If sum_exp_scores is near zero (e.g., due to very low temp and all scores being very negative),
            # assign uniform probabilities.
            probabilities = np.ones_like(base_scores) / len(base_scores)
    except OverflowError:
        # Handle potential overflow if scaled_scores become too large.
        # In such cases, effectively all probability will be on the max score.
        # A simple way is to treat it as an argmax selection.
        max_score_idx = np.argmax(base_scores)
        probabilities = np.zeros_like(base_scores)
        probabilities[max_score_idx] = 1.0

    # Assign these probabilities as priorities to the bins that can fit the item.
    priorities[can_fit_mask] = probabilities

    return priorities
```
