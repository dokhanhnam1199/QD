{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.\n\n    This strategy assigns higher priority to bins that have a remaining capacity\n    closest to the item's size. A temperature parameter controls the \"softness\"\n    of the softmax. A higher temperature makes the probabilities more uniform,\n    while a lower temperature sharpens the preference for bins that are a better fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Ensure we only consider bins that can actually fit the item\n    valid_bin_indices = bins_remain_cap >= item\n    \n    # If no bin can fit the item, return zeros (or handle as an error/new bin creation)\n    if not np.any(valid_bin_indices):\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit\" for valid bins: the closer the remaining capacity\n    # is to the item size, the better the fit. We can represent this as\n    # a score where a smaller difference is better. We want to maximize\n    # priority, so we invert the difference. A common approach is to use\n    # the negative difference, or some function of the inverse difference.\n    # Here we use (remaining_capacity - item_size) as the \"slack\".\n    # Smaller slack is better.\n    slack = bins_remain_cap[valid_bin_indices] - item\n    \n    # We want to assign higher priorities to bins with less slack.\n    # To use softmax, we need values that we want to be high to be large.\n    # So, we can use the negative slack.\n    # Also, we might want to prevent extreme values from dominating.\n    # A simple way to make it more \"soft\" is to add a small epsilon to avoid\n    # log(0) if we were using inverse slack.\n    # Or we can directly exponentiate a score that represents \"goodness\".\n    \n    # Let's define a \"goodness\" score. A good fit means slack is small.\n    # We can use -slack, so smaller slack becomes larger negative slack.\n    # To get positive scores for softmax, we can use -slack and then shift\n    # or simply use the raw slack as is and let softmax handle it.\n    # A common practice for Softmax-based selection is to have a score `s`\n    # where higher `s` is preferred.\n    # A good bin has `remaining_capacity - item` small.\n    # So, `item - remaining_capacity` is a good score (more negative is worse).\n    # Or, `remaining_capacity - item` (slack). Smaller slack is better.\n    # To use softmax, we want the best bins to have high values.\n    # Let's use a measure that is inversely related to slack, perhaps `1 / (slack + epsilon)`.\n    # However, this can lead to very large values if slack is near zero.\n    \n    # A more stable approach for Softmax-based selection often involves\n    # transforming the scores so that better choices yield higher values.\n    # If we consider `remaining_capacity - item` (slack), smaller is better.\n    # To make it suitable for softmax where higher values are preferred,\n    # we can transform it.\n    # For example, let the \"preference score\" be `-slack`.\n    # `preference_scores = -slack`\n    \n    # Another approach, often seen in contexts like attention mechanisms,\n    # is to use a \"similarity\" or \"energy\" score.\n    # We want bins that are \"close\" to fitting the item.\n    # Let's define closeness as the absolute difference.\n    # `closeness = np.abs(bins_remain_cap[valid_bin_indices] - item)`\n    # Smaller closeness is better.\n    # For softmax, we want higher values for better choices.\n    # So, we can use `-closeness`.\n    # `preference_scores = -np.abs(bins_remain_cap[valid_bin_indices] - item)`\n\n    # Let's go with a strategy that prioritizes bins that leave minimal slack.\n    # Slack = remaining_capacity - item_size.\n    # A smaller slack is better.\n    # To make it suitable for softmax (higher value = higher priority),\n    # we can use `-slack`.\n    # `preference_scores = -(bins_remain_cap[valid_bin_indices] - item)`\n\n    # To avoid issues with very small or negative scores, we can use a\n    # temperature parameter to smooth the distribution.\n    # A common form is `exp(score / temperature)`.\n    # A lower temperature emphasizes differences more.\n    \n    # Let's try a score that is proportional to how \"full\" the bin becomes.\n    # A bin that becomes almost full (item + small slack) is good.\n    # So, `item` is good if `bins_remain_cap` is `item + epsilon`.\n    # Consider the inverse of slack: `1.0 / (slack + 1e-6)` to avoid division by zero.\n    # Then apply softmax.\n    \n    # Let's define a score `s` where higher `s` is better.\n    # If `r` is remaining capacity, and `i` is item size:\n    # We want `r` to be close to `i`.\n    # So, `r-i` (slack) should be small and non-negative.\n    # Consider the score as `-slack` which means `i-r`. Small negative is better.\n    # Let's use `i - r` which becomes more positive as slack decreases.\n    # `scores = item - bins_remain_cap[valid_bin_indices]`\n    \n    # To make it more robust, especially if we have bins that are much larger,\n    # which might have a slightly larger slack but are still \"valid\",\n    # we can consider the ratio of item to remaining capacity if the remaining\n    # capacity is close to item size.\n\n    # A simple and effective approach for \"best fit\" is to consider the remaining capacity\n    # after placing the item. We want this remaining capacity to be as small as possible,\n    # but non-negative.\n    # So, `residual_capacity = bins_remain_cap[valid_bin_indices] - item`\n    # We want to maximize `-(residual_capacity)`.\n    \n    # Let's refine this for softmax. Higher values for better bins.\n    # If a bin has `remaining_capacity = r` and item is `i`:\n    # Preferred state is `r` is slightly larger than `i`.\n    # Let's define a \"preference\" `p` such that `p` is high when `r-i` is small and non-negative.\n    # One way is `p = - (r - i) = i - r`. This is maximized when `r-i` is minimized.\n    # Let's add a constant to ensure positivity before exponentiation if needed,\n    # or rely on the softmax properties.\n    \n    # Let's consider the \"tightness\" of the fit.\n    # Tightness is maximized when `bins_remain_cap[idx]` is exactly `item`.\n    # So, a score proportional to `1 / (slack + epsilon)` or `-slack`.\n    # Let's use `-slack` which is `item - bins_remain_cap[valid_bin_indices]`.\n    \n    preference_scores = item - bins_remain_cap[valid_bin_indices]\n    \n    # Apply a temperature to control the softness of the softmax.\n    # A temperature T > 0. Lower T -> harder selection, higher T -> softer selection.\n    # We want to avoid extremely large or small values before exponentiation.\n    # Let's cap the scores to prevent overflow or underflow issues if `item` is very large\n    # or `bins_remain_cap` is very small (though we filtered for `bins_remain_cap >= item`).\n    # If `item - bins_remain_cap` is very negative (large slack), it should have low priority.\n    \n    # To ensure non-negativity for the exponents, and potentially create a clearer preference:\n    # We can shift the scores by adding the maximum score. This doesn't change\n    # the softmax output but makes intermediate values positive.\n    # shifted_scores = preference_scores - np.min(preference_scores)\n    \n    # Let's directly use the preference scores, assuming `softmax` can handle\n    # potentially negative inputs by its nature of exponentiation and normalization.\n    \n    # Define a temperature parameter. A value like 1.0 is a good starting point.\n    # Lower values (e.g., 0.1) will make the selection more \"greedy\", focusing\n    # on the single best bin. Higher values (e.g., 5.0) will distribute probability\n    # more evenly across good fitting bins.\n    temperature = 1.0\n\n    # Calculate exponentiated scores\n    # We want bins with the smallest slack (largest `item - slack`) to have high scores.\n    # The formula for softmax is exp(score_i) / sum(exp(score_j)).\n    # If we use `item - slack`, then small slack leads to larger scores.\n    \n    # Ensure we don't have issues with extremely large positive or negative scores.\n    # For `item - bins_remain_cap`, if `bins_remain_cap` is very small and `item` is large,\n    # this can be large positive. If `bins_remain_cap` is much larger than `item`,\n    # this can be large negative.\n    \n    # Let's consider the \"fit\" as the negative difference: `-(bins_remain_cap - item) = item - bins_remain_cap`.\n    # A bin where `bins_remain_cap` is just enough for `item` gives a score of `0`.\n    # A bin with more capacity gives a more negative score.\n    # A bin with just enough capacity (slack 0) is ideal.\n    # If `bins_remain_cap` can be very large, `item - bins_remain_cap` can be very negative.\n    \n    # Let's try to map the remaining capacity to a desirability score.\n    # We want `bins_remain_cap` to be close to `item`.\n    # Let's map the values such that `item` maps to a high score, and values\n    # far from `item` map to lower scores.\n    \n    # For the \"best fit\" strategy, we are looking for the bin where `remaining_capacity`\n    # is closest to `item` *and* `remaining_capacity >= item`.\n    # This means we want to minimize `remaining_capacity - item`.\n    \n    # Score for softmax: higher is better.\n    # So, we want to maximize `-(bins_remain_cap[valid_bin_indices] - item)`.\n    # Let `fittness = -(bins_remain_cap[valid_bin_indices] - item)`\n    # `fittness = item - bins_remain_cap[valid_bin_indices]`\n\n    # Calculate exponential of scaled scores.\n    # Softmax expects scores that can be exponentiated.\n    # A stable way is `exp(scores / temperature)`.\n    # Consider `scores = item - bins_remain_cap[valid_bin_indices]`.\n    # If `bins_remain_cap` is very large, `scores` become very negative, exp goes to 0.\n    # If `bins_remain_cap` is slightly larger than `item`, `scores` are slightly negative.\n    # If `bins_remain_cap == item`, `scores` are 0, exp is 1.\n    # This seems reasonable: bins that perfectly fit are ideal. Bins with more space are less ideal.\n\n    # Ensure scores are not excessively large or small before exponentiation.\n    # Clip scores to avoid numerical instability.\n    # A reasonable range could be [-10, 10] for example, after scaling.\n    # If we use `item - bins_remain_cap`, and `item` can be large and `bins_remain_cap` small (but valid),\n    # the score can be large. Let's consider the *relative* difference.\n    \n    # A better heuristic for \"fit\" could be related to how \"full\" the bin becomes.\n    # We want the bin to be as full as possible without overflowing.\n    # So, if `r` is remaining capacity, we want `r-item` to be minimal.\n    # Let `score = -(r-item) = item - r`.\n    # We are selecting from bins where `r >= item`.\n    # The maximum value of `item - r` is 0 (when `r = item`).\n    # The minimum value can be large negative if `r` is much larger than `item`.\n    \n    # Example: item=5, bins_remain_cap=[7, 5, 10]\n    # Valid bins: [7, 5, 10]\n    # Scores (item - r): 5-7=-2, 5-5=0, 5-10=-5\n    # Softmax inputs: exp(-2/T), exp(0/T), exp(-5/T)\n    # If T=1: exp(-2)=0.135, exp(0)=1, exp(-5)=0.0067\n    # Probabilities: 0.135 / (0.135+1+0.0067) = 0.116, 1 / (0.135+1+0.0067) = 0.855, 0.0067 / (0.135+1+0.0067) = 0.0058\n    # This prioritizes the bin with remaining capacity exactly matching the item.\n\n    # The scores can be directly used in softmax, but for numerical stability,\n    # it's often better to center them around 0.\n    # `centered_scores = preference_scores - np.mean(preference_scores)`\n    # Or `centered_scores = preference_scores - np.max(preference_scores)` which makes max score 0.\n    \n    # Let's use `max(0, item - bins_remain_cap[valid_bin_indices])` as the score,\n    # so only bins that *could* fit are considered positively, and better fits are higher.\n    # `score = np.maximum(0, item - bins_remain_cap[valid_bin_indices])`\n    # This maps bins where `r > item` to 0, and bins where `r = item` to 0.\n    # This doesn't differentiate well between bins that are *almost* full.\n\n    # Let's stick with `item - bins_remain_cap[valid_bin_indices]` and handle potential large negative values by letting softmax turn them to near-zero probabilities.\n    \n    # Calculate exponentiated preference scores.\n    # We want higher scores for bins that leave less slack.\n    # `score = item - remaining_capacity`. Higher score is better.\n    # Maximize `score`.\n    # `score_values = item - bins_remain_cap[valid_bin_indices]`\n\n    # To prevent overflow with large positive scores (if item >> remaining_capacity, although this is filtered),\n    # or underflow with very large negative scores (if remaining_capacity >> item),\n    # we can scale or clip.\n    # Let's normalize the scores first to be in a more manageable range, e.g., [0, 1].\n    # `normalized_scores = (score_values - np.min(score_values)) / (np.max(score_values) - np.min(score_values) + 1e-8)`\n    # This maps the current range of scores to [0, 1]. The best fit (min slack) gets 1, worst fit gets 0.\n    \n    # A more common approach with softmax is direct use of scores with temperature.\n    # `exponentials = np.exp(score_values / temperature)`\n    \n    # Using the definition of \"best fit\": minimize `remaining_capacity - item`.\n    # We want bins that result in smaller positive residual capacities.\n    # `residuals = bins_remain_cap[valid_bin_indices] - item`\n    # We want to minimize `residuals`.\n    # For softmax, we want to maximize a function of `residuals`.\n    # Let `priority_score = -residuals = item - bins_remain_cap[valid_bin_indices]`\n    # Higher `priority_score` means better fit.\n\n    scores = item - bins_remain_cap[valid_bin_indices]\n\n    # If there's only one valid bin, its priority should be 1.0\n    if len(scores) == 1:\n        priorities = np.zeros_like(bins_remain_cap)\n        priorities[valid_bin_indices] = 1.0\n        return priorities\n\n    # Softmax calculation\n    # `exps = np.exp(scores / temperature)`\n    # Sum of exponentials\n    # `sum_exps = np.sum(exps)`\n    # Calculate probabilities\n    # `probabilities = exps / sum_exps`\n    \n    # For numerical stability, it's common to subtract the maximum score from all scores\n    # before exponentiation: exp(score_i - max(score_j)). This makes the maximum score 0.\n    max_score = np.max(scores)\n    stable_scores = scores - max_score\n    exps = np.exp(stable_scores / temperature)\n    sum_exps = np.sum(exps)\n    \n    # Handle case where sum_exps might be zero (e.g., all inputs to exp were -inf, which shouldn't happen here if temperature > 0 and scores are finite)\n    if sum_exps == 0:\n        # This could happen if all stable_scores were extremely negative.\n        # In such a case, perhaps assign uniform probability to valid bins.\n        probabilities = np.ones_like(scores) / len(scores)\n    else:\n        probabilities = exps / sum_exps\n\n    # Fill the final priorities array\n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bin_indices] = probabilities\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    This heuristic prioritizes bins that have just enough space for the item (tight fit),\n    with a small probability of choosing a random bin to encourage exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of choosing a random bin\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        # Calculate a \"tightness\" score for suitable bins\n        # Lower value means tighter fit (less wasted space)\n        tightness_scores = bins_remain_cap[suitable_bins_mask] - item\n        \n        # Assign a higher priority to bins with a tighter fit.\n        # We can invert the tightness score to make higher values mean higher priority.\n        # Adding a small constant to avoid division by zero if all fits are perfect.\n        priorities[suitable_bins_mask] = 1.0 / (tightness_scores + 1e-6)\n\n        # Apply epsilon-greedy strategy\n        if np.random.rand() < epsilon:\n            # Explore: Assign equal probability to all bins (including unsuitable ones)\n            # This can sometimes be useful if the \"best\" fit isn't always optimal long-term.\n            # However, for strict fitting, we might want to only explore among suitable bins.\n            # For this implementation, we'll explore among *all* bins to allow for\n            # the possibility of picking a slightly less optimal bin that might\n            # enable better packing later.\n            priorities = np.ones_like(priorities) / num_bins\n        else:\n            # Exploit: Select the bin with the highest priority score (tightest fit)\n            # For non-suitable bins, priority remains 0, so they won't be picked by max.\n            pass\n    else:\n        # If no bin can fit the item, all priorities remain 0.\n        # In a real implementation, this would likely trigger creating a new bin.\n        pass\n\n    return priorities\n\n[Reflection]\nPrioritize bins by maximizing fit and minimizing waste. Consider exploration.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}