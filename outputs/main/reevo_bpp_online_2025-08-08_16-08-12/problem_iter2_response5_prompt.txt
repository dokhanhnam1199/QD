{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Random Fit.\n\n    In Random Fit, we randomly select a bin among those that can accommodate the item.\n    This heuristic assigns a higher priority to bins that have enough remaining capacity,\n    and a lower priority to those that do not. The actual selection is randomized\n    among eligible bins. Here, we create a score that reflects this eligibility,\n    with higher scores for bins that fit the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Assign a high priority (e.g., 1) to bins that can fit the item\n    # Assign a low priority (e.g., 0) to bins that cannot fit the item.\n    # This ensures that only valid bins are considered in a subsequent random selection.\n    priorities = np.where(bins_remain_cap >= item, 1, 0)\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.\n\n    This heuristic prioritizes bins that have a remaining capacity close to the item size,\n    but also considers bins with slightly larger remaining capacity to allow for better\n    future packing. The softmax function is used to convert these 'fitness' scores\n    into probabilities (priorities).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate a \"fitness\" score for each bin.\n    # We want bins where remaining capacity is slightly larger than the item.\n    # A good score would be when remaining_capacity - item is small and non-negative.\n    # We can use a function that penalizes larger gaps, like an inverse or negative exponential.\n    # Here, we'll use a term that is high when (remaining_capacity - item) is small and positive.\n    # We'll also consider bins that can fit the item, so we set a very low score for bins\n    # that cannot fit the item.\n\n    # Maximum difference we are willing to tolerate. This can be tuned.\n    # A larger max_diff allows for more flexibility.\n    max_diff = np.max(bins_remain_cap) # A simple heuristic for max_diff\n\n    # Calculate the \"gap\" between remaining capacity and item size for bins that can fit the item\n    gaps = bins_remain_cap - item\n\n    # Initialize fitness scores. Bins that cannot fit the item will have a very low score.\n    fitness_scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # For bins that can fit the item (gaps >= 0)\n    can_fit_mask = (gaps >= 0)\n    valid_gaps = gaps[can_fit_mask]\n\n    if np.any(can_fit_mask):\n        # We want to prioritize bins with smaller gaps (closer fit).\n        # A common approach is to use an inverse function or a Gaussian-like function.\n        # Let's use a score that increases as the gap decreases (approaching zero).\n        # We can normalize the gaps to prevent extreme values and map them to a reasonable range.\n        # A simple approach: -valid_gaps / max_diff. This gives values close to 0 for small gaps\n        # and values close to -1 for large gaps. We want higher scores for smaller gaps.\n        # Let's try a linear transformation to make small gaps have higher positive values.\n        # Higher values mean higher priority.\n        # Score = (max_gap - gap) / max_gap, for gap >= 0.\n        # A simple way to map to positive values and use softmax is to transform the gap.\n        # Consider the \"waste\": remaining_capacity - item. We want to minimize waste.\n        # Let's use a score proportional to the inverse of the waste, or a negative exponential\n        # of the waste to create a smooth decay.\n\n        # Method 1: Using inverse of gap + 1 to avoid division by zero and ensure positive values.\n        # This will heavily favor bins that fit perfectly or near-perfectly.\n        # adjusted_scores = 1.0 / (valid_gaps + 1e-6)\n        # However, this can be sensitive to small gaps.\n\n        # Method 2: Using a soft-ranking based on the difference, aiming for a \"best fit\" preference.\n        # We can penalize larger differences more. For instance, we could use exp(-k * gap).\n        # A higher score means better fit.\n        # Let's use exp(-gap / temperature). Higher temperature makes it flatter (more uniform).\n        # A smaller temperature makes it sharper (more preference for best fit).\n        # Let's use a moderate temperature for diversity.\n        temperature = np.mean(bins_remain_cap) if np.mean(bins_remain_cap) > 0 else 1.0 # Dynamic temperature\n\n        # Ensure gaps are not excessively large compared to temperature,\n        # otherwise exp(-large_gap/temp) will underflow to 0.\n        # We can clip the gaps or use a scaled version.\n        # Scale the gaps relative to the temperature to control the softmax spread.\n        scaled_gaps = valid_gaps / temperature\n\n        # Calculate fitness scores using negative exponential. Higher values mean better fit.\n        # We want to give higher scores to smaller gaps.\n        # Let's invert the logic to have positive fitness for smaller gaps.\n        # For example, consider `max_useful_gap - gap` and then take exponential,\n        # or more directly `exp(-(gap / normalization_factor))`.\n        # A simple positive score that decays with gap size:\n        positive_scores = np.exp(-scaled_gaps)\n        fitness_scores[can_fit_mask] = positive_scores\n\n    # Apply Softmax to convert fitness scores into probabilities (priorities)\n    # Avoid issues with all scores being -inf or too close, which can lead to NaN in softmax.\n    # If all bins cannot fit, return uniform low priorities.\n    if not np.any(can_fit_mask):\n        return np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n\n    # Ensure that fitness_scores has at least one finite value before applying softmax\n    finite_scores = fitness_scores[np.isfinite(fitness_scores)]\n    if len(finite_scores) == 0:\n         return np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n\n    # Shift scores to avoid potential numerical instability with large negative values\n    # before exponentiation in softmax. The difference from the max score is constant.\n    max_score = np.max(fitness_scores[np.isfinite(fitness_scores)])\n    shifted_scores = fitness_scores - max_score\n\n    # Compute softmax\n    exp_scores = np.exp(shifted_scores)\n\n    # Handle potential infinities that might arise if shifted_scores still contains large positive numbers\n    # (e.g., if max_score was incorrectly calculated due to -inf).\n    # More robust approach for softmax:\n    exp_scores = np.where(np.isfinite(shifted_scores), np.exp(shifted_scores), 0)\n    sum_exp_scores = np.sum(exp_scores)\n\n    if sum_exp_scores == 0:\n        # Fallback: if all exponentiated scores are zero or non-finite, assign uniform priority\n        return np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n    else:\n        priorities = exp_scores / sum_exp_scores\n        return priorities\n\n[Reflection]\nPrioritize smaller gaps for better fit, use softmax for smooth probability distribution.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}