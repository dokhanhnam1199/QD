```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority with which we want to add item to each bin using a
    strategy that prioritizes exact fits and then uses softmax for exploration.

    This heuristic prioritizes bins that can fit the item. Among those that fit,
    it assigns a higher priority to bins that will have less remaining capacity
    after the item is placed (closer to an exact fit). It then applies a
    softmax function to these priorities to balance exploration and exploitation,
    allowing for a chance to pack items into bins that are not the absolute best fit.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Returns:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    epsilon = 1e-9  # Small value to avoid division by zero

    # Identify bins that can accommodate the item
    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        return np.zeros_like(bins_remain_cap, dtype=float)

    # Calculate the remaining capacity if the item is placed in a fitting bin
    remaining_after_placement = bins_remain_cap[can_fit_mask] - item

    # Assign higher priority to bins that will have less remaining capacity (closer to zero slack)
    # We want smaller remaining capacity to have higher priority, so we use 1/remaining.
    # Add epsilon to avoid division by zero.
    fit_priorities = 1.0 / (remaining_after_placement + epsilon)

    # Normalize priorities for softmax. This helps in controlling the 'temperature' effect.
    # A simple scaling or using the negative of remaining capacity can also work.
    # Here, we scale by the maximum possible inverse remaining capacity to keep values reasonable.
    max_inverse_remaining = np.max(fit_priorities) if fit_priorities.size > 0 else 1.0
    scaled_fit_priorities = fit_priorities / max_inverse_remaining

    # Apply softmax to introduce exploration. A higher temperature (controlled by the division
    # factor) means more uniform probabilities. A lower temperature means more exploitation
    # of the best fit. Here, we implicitly use a temperature of 1.0 by not dividing by a temperature parameter.
    # For tunable exploration, one could introduce a temperature parameter `T` and use:
    # softmax(scaled_fit_priorities / T)
    exp_priorities = np.exp(scaled_fit_priorities)
    sum_exp_priorities = np.sum(exp_priorities)

    if sum_exp_priorities > epsilon:
        probabilities = exp_priorities / sum_exp_priorities
    else:
        # If all scaled priorities are very small (e.g., all remaining capacities are huge)
        # Assign uniform probability among fitting bins.
        probabilities = np.ones_like(fit_priorities) / len(fit_priorities)

    # Assign the calculated probabilities (as priorities) to the fitting bins
    priorities[can_fit_mask] = probabilities

    return priorities
```
