[Prior reflection]
Prioritize exact fits. For near fits, use scaled softmax on inverse remaining capacity to encourage tighter packing and simpler greedy strategies. Adapt exploration for fewer, direct improvements.

[Code]
import numpy as np

def priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a First Fit strategy with a twist.

    This heuristic prioritizes bins that can fit the item. Among those that fit,
    it assigns a higher priority to bins that will have less remaining capacity
    after the item is placed. This is a greedy approach aiming to fill bins
    as much as possible, encouraging denser packing.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins that can accommodate the item
    can_fit_mask = bins_remain_cap >= item

    # Calculate the remaining capacity if the item is placed in a fitting bin
    remaining_after_placement = bins_remain_cap[can_fit_mask] - item

    # Assign higher priority to bins that will have less remaining capacity
    # This is equivalent to prioritizing bins where the item fills a larger
    # proportion of the remaining space.
    # We invert the remaining capacity so that smaller remaining capacity gets higher priority.
    # Adding a small epsilon to avoid division by zero if remaining capacity is exactly 0,
    # although with this logic, that case would already have a high priority.
    priorities[can_fit_mask] = 1.0 / (remaining_after_placement + 1e-9)

    return priorities

[Improved code]
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a modified Best Fit strategy.

    This heuristic prioritizes bins that can fit the item. It gives a very high
    priority to exact fits. For near fits, it uses a scaled softmax on the
    inverse of the remaining capacity after placement to encourage tighter packing.
    A scaling factor is used to control the steepness of the softmax.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    epsilon = 1e-9  # For numerical stability
    softmax_scale = 10.0  # Controls the steepness of the softmax

    # Identify bins that can accommodate the item
    can_fit_mask = bins_remain_cap >= item

    # Calculate potential remaining capacities for fitting bins
    potential_remaining_cap = bins_remain_cap[can_fit_mask] - item

    # Prioritize exact fits with a very high score
    exact_fit_mask = np.isclose(potential_remaining_cap, 0, atol=epsilon)
    priorities[can_fit_mask][exact_fit_mask] = 1e6  # High priority for exact fits

    # For near fits, use scaled softmax on the inverse remaining capacity
    near_fit_mask = ~exact_fit_mask
    if np.any(can_fit_mask[can_fit_mask][near_fit_mask]):
        # Invert remaining capacity, higher priority for smaller remaining capacity
        inverse_remaining_cap = 1.0 / (potential_remaining_cap[near_fit_mask] + epsilon)

        # Apply scaled softmax
        # The idea is to transform the inverse remaining capacities into probabilities
        # or scores where tighter fits get higher scores.
        # Softmax can be sensitive to large differences, so scaling helps normalize.
        scaled_inverse_remaining_cap = softmax_scale * inverse_remaining_cap
        softmax_values = np.exp(scaled_inverse_remaining_cap)

        # Normalize softmax values to prevent extremely large or small numbers
        # and to ensure they are relative.
        # We are not looking for a probability distribution here, but relative scores.
        # A simple scaling might be sufficient, but softmax can introduce a sharper
        # distinction for values that are already inversely related.
        # For simplicity, we can just use the scaled inverse directly for near fits,
        # as softmax over a single set of values is just those values scaled.
        # The reflection mentions "scaled softmax on inverse remaining capacity",
        # which suggests we might want to apply softmax to a set of values derived
        # from inverse remaining capacities. If we consider the set of 'inverse_remaining_cap'
        # for the near fits, softmax would normalize them relative to each other.
        # However, the prompt asks for a priority score for *each* bin.
        # A common interpretation of "scaled softmax on X" for priority is
        # to use exp(scale * X) directly as a score.

        # Let's interpret "scaled softmax on inverse remaining capacity" as
        # applying an exponential function with a scaling factor to the inverse
        # remaining capacity, which amplifies differences.
        priorities[can_fit_mask][near_fit_mask] = np.exp(softmax_scale * inverse_remaining_cap)

    return priorities
```
