[
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a First Fit strategy with a twist.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it assigns a higher priority to bins that will have less remaining capacity\n    after the item is placed. This is a greedy approach aiming to fill bins\n    as much as possible, encouraging denser packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity if the item is placed in a fitting bin\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # Assign higher priority to bins that will have less remaining capacity\n    # This is equivalent to prioritizing bins where the item fills a larger\n    # proportion of the remaining space.\n    # We invert the remaining capacity so that smaller remaining capacity gets higher priority.\n    # Adding a small epsilon to avoid division by zero if remaining capacity is exactly 0,\n    # although with this logic, that case would already have a high priority.\n    priorities[can_fit_mask] = 1.0 / (remaining_after_placement + 1e-9)\n\n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit strategy.\n\n    The Best Fit strategy aims to place the item in the bin that has the smallest\n    remaining capacity but can still accommodate the item. This minimizes wasted\n    space.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n        Bins that can fit the item will have a priority inversely proportional\n        to their remaining capacity (smaller remaining capacity gets higher priority).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate priority\n    # Priority is higher for bins with less remaining capacity\n    # This is achieved by taking the inverse of the remaining capacity.\n    # To avoid division by zero if remaining capacity is exactly the item size,\n    # we add a small epsilon or use a value that ensures larger remaining capacity\n    # results in lower priority. A simple inverse (1/capacity) works well here\n    # because we are looking for the *smallest* remaining capacity.\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # Assign priorities: smaller remaining capacity gets a higher priority\n    # The priority is 1 / (remaining_capacity - item_size + 1)\n    # Adding 1 to the denominator ensures that if remaining_capacity == item_size,\n    # the priority is 1, and if remaining_capacity is slightly larger, it's still high.\n    # A very large remaining capacity will result in a priority close to 0.\n    priorities[can_fit_mask] = 1.0 / (fitting_bins_remain_cap - item + 1.0)\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response2.txt_stdout.txt",
    "code_path": "problem_iter1_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit.\n\n    In Worst Fit, we aim to place the item in the bin that has the most remaining capacity.\n    This strategy is greedy and tries to leave larger spaces for potentially larger future items.\n    The priority is higher for bins with more remaining capacity, provided the item fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    # We want to prioritize bins where the item can actually fit.\n    # For bins where the item fits, the priority is directly the remaining capacity.\n    # This ensures that the bin with the *most* remaining capacity is chosen.\n    # For bins where the item does not fit, the priority is 0, effectively excluding them.\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask]\n    return priorities",
    "response_id": 2,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response3.txt_stdout.txt",
    "code_path": "problem_iter1_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    The \"Almost Full Fit\" strategy prioritizes bins that have a remaining capacity\n    just large enough to fit the current item. This aims to leave larger capacities\n    in other bins for potentially larger future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate the \"tightness\" for bins that can accommodate the item\n    # Tightness is the difference between the bin's remaining capacity and the item's size.\n    # A smaller positive difference indicates a \"tighter\" fit.\n    # We want to prioritize bins where this difference is small and positive.\n    # We use a small epsilon to avoid division by zero and to handle cases where\n    # remaining capacity is exactly equal to the item size.\n    epsilon = 1e-9\n    tightness = bins_remain_cap - item\n\n    # Only consider bins that have enough capacity for the item\n    valid_bins_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate their priority.\n    # Higher priority is given to bins with smaller positive tightness.\n    # We invert the tightness and add epsilon to avoid division by zero for bins\n    # where remaining_capacity == item.\n    # A large value means the bin is a good fit (tight).\n    priorities[valid_bins_mask] = 1.0 / (tightness[valid_bins_mask] + epsilon)\n\n    # Optionally, you could add a small penalty to bins that are \"too large\"\n    # to fit the item, or further refine the priority based on other criteria.\n    # For this \"Almost Full Fit\", we focus solely on the tightness of bins that can fit.\n\n    # Ensure that bins that cannot fit the item have a priority of 0\n    priorities[~valid_bins_mask] = 0.0\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response4.txt_stdout.txt",
    "code_path": "problem_iter1_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Exact Fit First prioritizes bins where the item fits exactly.\n    # We assign a high priority to bins where remaining capacity minus item size is 0.\n    # For other bins, the priority is lower.\n\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Find bins where the item fits exactly\n    exact_fit_mask = (bins_remain_cap - item) == 0\n    \n    # Assign a high priority to exact fit bins\n    priorities[exact_fit_mask] = 100  # A large value to indicate highest priority\n    \n    # For bins where the item fits but not exactly, assign a moderate priority.\n    # This can be the remaining capacity itself, so smaller remaining capacities get higher priority\n    # among non-exact fits, favoring fuller bins.\n    valid_fit_mask = bins_remain_cap >= item\n    priorities[valid_fit_mask & ~exact_fit_mask] = bins_remain_cap[valid_fit_mask & ~exact_fit_mask]\n\n    # Bins where the item doesn't fit get a priority of 0 (already initialized)\n    \n    return priorities",
    "response_id": 4,
    "obj": 86.58755484643,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response5.txt_stdout.txt",
    "code_path": "problem_iter1_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit) strategy.\n\n    The priority is calculated as the inverse of the remaining capacity of the bin\n    after the item is placed in it. A higher priority means the bin is a closer fit\n    (i.e., has less remaining capacity after packing). Bins where the item cannot fit\n    are given a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Calculate the remaining capacity if the item is placed in the bin\n    potential_remaining_cap = bins_remain_cap - item\n    \n    # Identify bins where the item can fit\n    fit_mask = potential_remaining_cap >= 0\n    \n    # For bins where the item fits, calculate priority as 1 / (remaining_capacity + epsilon)\n    # Add a small epsilon to avoid division by zero if a bin becomes exactly full.\n    # The inverse distance (proximity fit) aims to leave as little space as possible.\n    epsilon = 1e-9  # Small value to prevent division by zero\n    priorities[fit_mask] = 1.0 / (potential_remaining_cap[fit_mask] + epsilon)\n    \n    return priorities",
    "response_id": 5,
    "obj": 4.198244914240141,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response6.txt_stdout.txt",
    "code_path": "problem_iter1_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score prioritizes bins that have a remaining capacity\n    close to the item's size. A bin with a remaining capacity slightly larger\n    than the item's size is preferred over bins that are either too small\n    or have a very large remaining capacity. The sigmoid function helps\n    to create a smooth transition in priorities around the \"ideal\" fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Ensure we don't divide by zero or get nonsensical results for bins with 0 capacity\n    # Also, avoid very small capacities causing extremely large negative exponents.\n    # We add a small epsilon to the denominator to prevent division by zero.\n    epsilon = 1e-9\n    # Calculate the difference between bin capacity and item size.\n    # We are interested in the positive difference (slack).\n    diff = bins_remain_cap - item\n\n    # Apply a transformation that favors bins where diff is close to zero.\n    # The sigmoid function can be used here. A common form for scoring\n    # where we want to maximize a value (like closeness to zero difference) is:\n    # sigmoid(k * (ideal - x)) where k is a steepness parameter and ideal is the target value.\n    # Here, we want to penalize bins that are too small (diff < 0) and bins that\n    # have a lot of slack (large positive diff). We want to reward bins\n    # where diff is close to 0.\n    # Let's consider the ratio (item / capacity) or rather (capacity - item) / capacity.\n    # If capacity < item, the difference is negative.\n    # We want to give higher scores to bins where `diff` is close to zero.\n    # Let's use a sigmoid centered around `diff = 0`.\n    # sigmoid(x) = 1 / (1 + exp(-x))\n    # If diff is positive and small, it maps to values close to 1.\n    # If diff is negative, it maps to values close to 0.\n    # If diff is large and positive, it also maps to values close to 1.\n    # This isn't quite right for prioritizing *smaller* remaining capacity.\n\n    # A better approach is to map `bins_remain_cap` to a score.\n    # We want to prioritize bins that are \"just big enough\".\n    # Let's try a sigmoid where the center is the item size.\n    # The input to the sigmoid should be something like (item - remaining_capacity).\n    # If item > remaining_capacity, the input is positive, sigmoid is close to 1.\n    # If item < remaining_capacity, the input is negative, sigmoid is close to 0.\n    # This prioritizes bins that are too small.\n\n    # Let's rethink the \"Sigmoid Fit Score\". The idea is to create a score\n    # that is high when the remaining capacity is *just enough* or slightly more than the item.\n    # A good strategy would be to have a score that peaks when `bins_remain_cap` is\n    # approximately equal to `item`.\n    #\n    # Consider a function f(x) where x is the remaining capacity. We want f(item) to be high.\n    # A common form of sigmoid is `1 / (1 + exp(-k*(x - center)))`.\n    # If `center` is `item`, then:\n    # - If `bins_remain_cap` < `item`, then `(bins_remain_cap - item)` is negative.\n    #   The exponent `-k * (negative)` becomes positive. The `exp()` grows large. The score goes to 0. (Good, we don't want bins too small)\n    # - If `bins_remain_cap` = `item`, then `(bins_remain_cap - item)` is 0. The exponent is 0. `exp(0) = 1`. Score is `1 / (1 + 1) = 0.5`. (Okay, this is the midpoint)\n    # - If `bins_remain_cap` > `item`, then `(bins_remain_cap - item)` is positive.\n    #   The exponent `-k * (positive)` becomes negative. The `exp()` goes towards 0. The score goes towards 1. (This prioritizes bins that are too large over the ideal fit)\n\n    # This sigmoid centered at `item` prioritizes bins that are larger than the item.\n    # We want to prioritize bins that are \"tight fits\".\n    # Let's define a score that is high when `bins_remain_cap` is slightly greater than `item`.\n    # A function that looks like a Gaussian or a bell curve centered at `item`\n    # would be ideal, but sigmoid is requested.\n\n    # Let's try to engineer a sigmoid shape.\n    # We want a score that:\n    # 1. Is low if `bins_remain_cap` < `item`.\n    # 2. Is high if `bins_remain_cap` is slightly larger than `item`.\n    # 3. Decreases if `bins_remain_cap` becomes much larger than `item`.\n\n    # This kind of behavior is hard to achieve with a single sigmoid.\n    # However, \"Sigmoid Fit Score\" often implies a heuristic that uses\n    # the sigmoid function to map some fitness metric.\n    # A common interpretation for \"fit\" in bin packing is the percentage of remaining capacity.\n    #\n    # Consider the \"Best Fit\" strategy: pick the bin with the smallest remaining capacity\n    # that is still greater than or equal to the item size.\n    #\n    # Let's try to use sigmoid to represent the \"goodness\" of a fit.\n    # A common heuristic related to sigmoid could be based on how \"full\" the bin becomes.\n    # If a bin has remaining capacity `c` and we put item `i`, the new capacity is `c-i`.\n    # The \"fullness\" can be `(bin_capacity - (c-i)) / bin_capacity` or `i / bin_capacity`.\n    #\n    # Let's try to score based on the `bins_remain_cap` relative to the item.\n    # If `bins_remain_cap < item`, the bin is not usable. Score should be 0 or very low.\n    # If `bins_remain_cap >= item`, the bin is usable.\n    # Among usable bins, we want to prioritize those that have `bins_remain_cap`\n    # as close to `item` as possible (but still >= item).\n\n    # Let's use the sigmoid to penalize bins that are too empty *after* fitting.\n    # If we place item `i` into a bin with remaining capacity `c`, the new remaining capacity is `c-i`.\n    # We want to penalize if `c-i` is very large.\n    # The \"slack\" is `c-i`.\n    #\n    # Let's consider `1 - sigmoid(k * (c - i))`.\n    # If `c - i` is large positive (lots of slack), exponent is large positive, sigmoid is ~1, score is ~0. (Penalizes slack)\n    # If `c - i` is 0 (perfect fit), exponent is 0, sigmoid is 0.5, score is 0.5.\n    # If `c - i` is negative (item too large), exponent is large negative, sigmoid is ~0, score is ~1. (This favors bins that are too small - BAD!)\n\n    # We need to handle the unusable bins (`bins_remain_cap < item`) separately or\n    # ensure the score naturally becomes very low for them.\n\n    # Let's try a sigmoid that models the probability of a \"good\" fit.\n    # A \"good\" fit means `bins_remain_cap` is close to `item`.\n    # A \"very good\" fit is `bins_remain_cap == item`.\n    # A \"slightly less good\" fit is `bins_remain_cap` is slightly larger than `item`.\n    # A \"bad\" fit is `bins_remain_cap` is much larger than `item`, or `bins_remain_cap` < `item`.\n\n    # Let's try scoring based on how much \"wasted space\" is created.\n    # Wasted space = `bins_remain_cap - item`. We want this to be small and non-negative.\n    #\n    # Consider the value `item / bins_remain_cap`.\n    # If `bins_remain_cap < item`, this ratio is > 1.\n    # If `bins_remain_cap == item`, ratio is 1.\n    # If `bins_remain_cap > item`, ratio is < 1.\n    #\n    # We want to penalize bins that are unusable (`bins_remain_cap < item`).\n    # We want to reward bins where `bins_remain_cap` is just enough.\n\n    # Let's construct a score from `item` and `bins_remain_cap`.\n    # We want a high score when `bins_remain_cap` is close to `item` (and `bins_remain_cap >= item`).\n    #\n    # Try `sigmoid(k * (bins_remain_cap - item))`?\n    # If `c < i`, exponent is negative, sigmoid is near 0.\n    # If `c = i`, exponent is 0, sigmoid is 0.5.\n    # If `c > i`, exponent is positive, sigmoid is near 1.\n    # This prioritizes bins that are too large.\n\n    # Let's invert the logic or the input.\n    # Consider `sigmoid(k * (item - bins_remain_cap))`\n    # If `c < i`, `i - c > 0`, exponent is positive, sigmoid is near 1. (Prioritizes too small - BAD)\n    # If `c = i`, `i - c = 0`, exponent is 0, sigmoid is 0.5.\n    # If `c > i`, `i - c < 0`, exponent is negative, sigmoid is near 0. (Prioritizes too large - BAD)\n\n    # The standard \"Sigmoid Fit Score\" heuristic in some contexts might actually refer to:\n    # Prioritizing bins by the \"tightness\" of the fit.\n    # A measure of tightness could be `item / bin_capacity` or `item / bins_remain_cap`.\n    # However, in online bin packing, we are given `bins_remain_cap`.\n\n    # Let's try to create a function that peaks at `bins_remain_cap = item`.\n    # We can use a sigmoid to represent the \"closeness\" to the item size.\n    #\n    # Let's consider a score based on the *difference* `bins_remain_cap - item`.\n    # We want small positive differences to have high scores.\n    # A sigmoid function's shape is useful here.\n    #\n    # Let `x = bins_remain_cap - item`.\n    # We want `score(x)` such that `score(0)` is high, `score(<0)` is low, `score(>>0)` is low.\n    #\n    # A Gaussian is ideal, but using sigmoid:\n    # `sigmoid(k * (X - x))` where X is the target.\n    # We want to maximize `bins_remain_cap` that is close to `item`.\n    #\n    # Let's define a score that is 0 if the bin is unusable (`bins_remain_cap < item`).\n    # For usable bins, we want to prioritize those with `bins_remain_cap` closest to `item`.\n    #\n    # Let's try a score that is `sigmoid(k * (item - bins_remain_cap))`.\n    # If `bins_remain_cap < item`, `item - bins_remain_cap > 0`, score is near 1. (This is bad, it prioritizes bins that are too small)\n    # If `bins_remain_cap > item`, `item - bins_remain_cap < 0`, score is near 0. (This is bad, it penalizes bins that are large, including ones that might be ideal)\n\n    # Okay, let's consider what makes a bin *less* desirable:\n    # 1. Bin capacity is less than item size.\n    # 2. Bin capacity is much larger than item size (creates a lot of slack/waste).\n    #\n    # So, we want to *maximize* the score for bins where `bins_remain_cap` is close to `item`.\n\n    # A common strategy for \"fitting\" is to consider the \"slack\" `bins_remain_cap - item`.\n    # We want this slack to be small and non-negative.\n    # Let's use a sigmoid to \"curve\" this slack.\n    #\n    # Consider the metric `item / bins_remain_cap`. We want this to be close to 1, but not greater than 1.\n    #\n    # Let's try to map the *ratio* of available space to the item size.\n    #\n    # A common form of a \"sigmoid fit\" might involve mapping a criterion like\n    # `bins_remain_cap / max_bin_capacity` or `bins_remain_cap / item_size`.\n    #\n    # Let's try to model the probability that this bin is the \"best fit\".\n    # A bin is a good fit if `bins_remain_cap >= item`.\n    # Among these, a better fit if `bins_remain_cap` is smaller.\n    #\n    # Let's consider a sigmoid that captures \"how close is `bins_remain_cap` to `item`\".\n    #\n    # Let `y = bins_remain_cap`. We want to score based on `y`.\n    # We want `score(y)` to be high when `y` is near `item`.\n    #\n    # A function that captures this is `sigmoid(k * (y - item)) * sigmoid(k * (item_upper - y))`\n    # where `item_upper` is some value slightly larger than `item`. This creates a bell-like shape.\n    # However, we need a single sigmoid.\n\n    # Let's go with the idea that a high score means a good fit.\n    # A bin is unusable if `bins_remain_cap < item`. Score = 0 for these.\n    # For bins where `bins_remain_cap >= item`, we want to prioritize those where\n    # `bins_remain_cap` is as small as possible.\n\n    # Let's consider the \"inverse\" fit score. A smaller remaining capacity (but still sufficient)\n    # is a better fit.\n    #\n    # Try scoring based on `1 / (1 + exp(k * (bins_remain_cap - item)))`.\n    # If `c < i`, `c - i < 0`, exponent is negative, `exp()` is small, score is near 1. (Bad)\n    # If `c = i`, `c - i = 0`, exponent is 0, `exp()` is 1, score is 0.5.\n    # If `c > i`, `c - i > 0`, exponent is positive, `exp()` is large, score is near 0. (Good, penalizes slack)\n    #\n    # So, this `1 / (1 + exp(k * (bins_remain_cap - item)))` prioritizes bins with *less* remaining capacity.\n    #\n    # To handle unusable bins: we can set their score to a very small negative number or 0.\n    # The current formula already gives near 1 for unusable bins. We need to reverse this.\n\n    # Let's use `sigmoid(k * (item - bins_remain_cap))`.\n    # If `c < i`, `i - c > 0`, sigmoid is near 1. (Prioritizes unusable bins - BAD)\n    # If `c = i`, `i - c = 0`, sigmoid is 0.5.\n    # If `c > i`, `i - c < 0`, sigmoid is near 0. (Penalizes large slack)\n\n    # We need to filter unusable bins first.\n    usable_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # For usable bins, apply a score that favors smaller remaining capacities.\n    # Let's use the sigmoid on the difference `item - bins_remain_cap`.\n    # The closer `bins_remain_cap` is to `item`, the smaller `item - bins_remain_cap` is (close to 0).\n    # We want a high score for small `item - bins_remain_cap`.\n    #\n    # `sigmoid(k * (item - bins_remain_cap))` gives high scores when `item - bins_remain_cap` is large and positive (i.e., `bins_remain_cap` is small).\n    # So, if `item - bins_remain_cap` is positive (meaning `item > bins_remain_cap`, but we already filtered this)\n    # or negative (meaning `item < bins_remain_cap`).\n\n    # Let's try again with the interpretation of \"tight fit\".\n    # We want to maximize `1 / (1 + exp(k * (bins_remain_cap - item)))` for usable bins.\n    # This score is high when `bins_remain_cap - item` is negative (unusable - BAD) or small negative.\n    # It's low when `bins_remain_cap - item` is positive.\n    #\n    # The form that works for \"close to ideal\" using sigmoid often involves a difference term.\n    # Let `score = 1 / (1 + exp(-k * (bins_remain_cap - item)))`.\n    # This is high when `bins_remain_cap` is large.\n    #\n    # Let `score = 1 / (1 + exp(k * (bins_remain_cap - item)))`.\n    # This is high when `bins_remain_cap - item` is small (or negative).\n    #\n    # So for usable bins: `priorities[usable_bins_mask] = 1 / (1 + np.exp(k * (bins_remain_cap[usable_bins_mask] - item)))`\n    # This will assign higher scores to bins with less remaining capacity. This is the \"Best Fit\" idea.\n    #\n    # Let's make it more explicitly \"Sigmoid Fit Score\" where the sigmoid defines the \"goodness\".\n    #\n    # Consider the ratio `r = item / bins_remain_cap`.\n    # If `bins_remain_cap < item`, `r > 1`.\n    # If `bins_remain_cap == item`, `r = 1`.\n    # If `bins_remain_cap > item`, `r < 1`.\n    #\n    # We want to score the bins where `r` is close to 1, but not greater than 1.\n    #\n    # Let's use the sigmoid function to represent the \"quality of fit\".\n    #\n    # Option 1: Prioritize \"Best Fit\" (smallest remaining capacity >= item).\n    # Use sigmoid to convert difference `bins_remain_cap - item` to a score.\n    # `sigmoid(k * (item - bins_remain_cap))` maps `0` to `0.5`, large positive to `1`, large negative to `0`.\n    # If `bins_remain_cap - item` is negative (unusable), then `item - bins_remain_cap` is positive.\n    # If `bins_remain_cap - item` is small positive, then `item - bins_remain_cap` is small negative.\n    # If `bins_remain_cap - item` is large positive, then `item - bins_remain_cap` is large negative.\n    #\n    # `sigmoid(k * (item - bins_remain_cap))`\n    # - If `c < i` (unusable), `i - c > 0`, score is close to 1. (Bad)\n    # - If `c = i`, `i - c = 0`, score is 0.5.\n    # - If `c > i`, `i - c < 0`, score is close to 0. (Penalizes slack)\n    #\n    # The issue is prioritizing unusable bins.\n    # We can set the score to 0 for unusable bins.\n    # For usable bins, we want the score to be high when `c` is just above `i`.\n    # This means `c - i` is small positive.\n    # So `item - c` is small negative.\n\n    # Let's try a sigmoid centered around the \"ideal fit\" where `bins_remain_cap` equals `item`.\n    # We can use `sigmoid(k * (item - bins_remain_cap))`.\n    # However, the problem with this is that it assigns high scores when `bins_remain_cap` is much smaller than `item`.\n    #\n    # A common heuristic for \"Sigmoid Fit Score\" might aim to convert the *difference* into a score using sigmoid.\n    # Consider `score = sigmoid(k * (bins_remain_cap - item))`.\n    # If `c < i`, `c-i` negative, sigmoid is near 0.\n    # If `c = i`, `c-i` zero, sigmoid is 0.5.\n    # If `c > i`, `c-i` positive, sigmoid is near 1.\n    # This prioritizes bins that are too large.\n\n    # A more sophisticated sigmoid fit might involve prioritizing based on the *ratio* of remaining capacity to the item size,\n    # but this isn't a direct sigmoid application.\n\n    # Let's consider a practical interpretation of \"Sigmoid Fit Score\":\n    # It should be high for bins that are a \"good fit\", meaning `bins_remain_cap` is just enough or slightly more than `item`.\n    # It should be low for bins that are too small or have too much slack.\n    #\n    # Let `slack = bins_remain_cap - item`. We want `slack` to be small and positive.\n    #\n    # A sigmoid transformation can map values to (0, 1).\n    # Let's use the negative slack: `-(bins_remain_cap - item) = item - bins_remain_cap`.\n    # If `bins_remain_cap < item`, `item - bins_remain_cap` is positive.\n    # If `bins_remain_cap > item`, `item - bins_remain_cap` is negative.\n    #\n    # `sigmoid(k * (item - bins_remain_cap))`\n    # - When `bins_remain_cap` is very small compared to `item`, `item - bins_remain_cap` is large positive, score approaches 1.\n    # - When `bins_remain_cap = item`, `item - bins_remain_cap` is 0, score is 0.5.\n    # - When `bins_remain_cap` is much larger than `item`, `item - bins_remain_cap` is large negative, score approaches 0.\n    #\n    # This seems to correctly prioritize bins where `bins_remain_cap` is larger than `item`, and penalizes those that are much larger.\n    # The problem is that it gives high scores to unusable bins.\n\n    # To address the unusable bins, we can set their score to zero or a very low value.\n    # Let's create a score for usable bins.\n\n    # We can define a steepness parameter `k` for the sigmoid. A higher `k` makes the transition sharper.\n    k = 10.0  # Steepness parameter, can be tuned.\n\n    # Create scores for bins that can fit the item.\n    # We want to give higher priority to bins that have remaining capacity closer to the item size.\n    # This means minimizing the \"slack\" (bins_remain_cap - item).\n    #\n    # Let's try a score that's high when `bins_remain_cap - item` is small.\n    # Consider `1.0 / (1.0 + np.exp(k * (bins_remain_cap - item)))`.\n    # If `c < i`: `c - i` is negative. `k * (c - i)` is negative. `exp(...)` is small. Score ~ 1. (Bad)\n    # If `c = i`: `c - i` is 0. `exp(...)` is 1. Score = 0.5.\n    # If `c > i`: `c - i` is positive. `k * (c - i)` is positive. `exp(...)` is large. Score ~ 0. (Good)\n    # This prioritizes bins that are *larger* than the item. It is the inverse of what we want for \"best fit\".\n\n    # Let's consider the \"Best Fit\" heuristic: select the bin with the smallest remaining capacity that can accommodate the item.\n    # This means we want to *maximize* a score that is inversely related to `bins_remain_cap` (for usable bins).\n    #\n    # A score of `1.0 / bins_remain_cap` would favor smaller capacities, but doesn't use sigmoid and doesn't penalize unusable bins directly.\n    #\n    # Let's use the sigmoid to model how \"close\" `bins_remain_cap` is to `item`.\n    # A function like `sigmoid(k * (item - bins_remain_cap))` or `sigmoid(k * (bins_remain_cap - item))` can be used.\n    #\n    # Let's prioritize bins that have a positive \"fit margin\" (`bins_remain_cap - item`) that is small.\n    #\n    # Consider `sigmoid(k * (item - bins_remain_cap))` for usable bins:\n    # `usable_bins_mask = bins_remain_cap >= item`\n    # `scores_for_usable = sigmoid(k * (item - bins_remain_cap[usable_bins_mask]))`\n    # - If `c` is just slightly larger than `item`, `item - c` is slightly negative. Sigmoid is slightly less than 0.5.\n    # - If `c = item`, `item - c` is 0. Sigmoid is 0.5.\n    # - If `c` is much larger than `item`, `item - c` is very negative. Sigmoid is close to 0.\n    # This prioritizes bins that are closer to `item` from above.\n    #\n    # If we want to strictly follow \"Sigmoid Fit Score\", a reasonable interpretation is to use the sigmoid function\n    # to map a metric of \"fit quality\" to a priority score.\n    #\n    # Let's define \"fit quality\" as `-(bins_remain_cap - item)`. We want this to be small and positive.\n    # So we want to maximize `sigmoid(k * (-(bins_remain_cap - item))) = sigmoid(k * (item - bins_remain_cap))`.\n    #\n    # For unusable bins, the score should be 0.\n    # For usable bins, the score should be `sigmoid(k * (item - bins_remain_cap))`.\n    #\n    # Let `diff = item - bins_remain_cap`.\n    # For usable bins, `bins_remain_cap >= item`, so `diff <= 0`.\n    #\n    # `sigmoid(k * diff)`:\n    # - If `diff = 0` (`c = i`), `sigmoid(0) = 0.5`.\n    # - If `diff < 0` (`c > i`), `sigmoid(negative)` is < 0.5. Closer to 0 as `c` increases.\n    #\n    # This prioritizes bins that are exactly equal to the item size, and then bins that are slightly larger.\n    # It gives a score between 0 and 0.5 for usable bins.\n\n    # To get scores between 0 and 1 for usable bins:\n    # Let's normalize the sigmoid output.\n    # The standard sigmoid outputs values in (0, 1).\n    #\n    # `priorities[usable_bins_mask] = 1.0 / (1.0 + np.exp(-k * (item - bins_remain_cap[usable_bins_mask])))`\n    #\n    # Let's test this:\n    # If `c < i` (unusable): `item - c > 0`, `-k * (item - c)` is large negative. `exp(...)` small. Score ~ 1. (Still a problem)\n    # If `c = i`: `item - c = 0`, `-k * 0 = 0`. `exp(0) = 1`. Score = 1 / (1+1) = 0.5.\n    # If `c > i`: `item - c < 0`, `-k * (item - c)` is positive. `exp(...)` large. Score approaches 0.\n\n    # The goal is to have a score that is high for bins that are \"tight fits\".\n    # This means `bins_remain_cap` is slightly larger than `item`.\n    #\n    # Let's define the sigmoid score for a bin `j` as:\n    # `S_j = 1 / (1 + exp(-k * (bins_remain_cap_j - item)))`\n    # If `c_j < item`, score is low. If `c_j = item`, score is 0.5. If `c_j > item`, score is high.\n    # This prioritizes bins that are too large.\n\n    # The inverse:\n    # `S_j = 1 / (1 + exp(k * (bins_remain_cap_j - item)))`\n    # If `c_j < item`, score is high. If `c_j = item`, score is 0.5. If `c_j > item`, score is low.\n    # This prioritizes bins that are too small.\n\n    # This suggests that the \"Sigmoid Fit Score\" might be referring to the mapping of `item / bins_remain_cap`.\n    # However, we are given the item and bin remaining capacities.\n\n    # Let's use a heuristic that is often associated with sigmoid-like behavior for fitting:\n    # Prioritize bins where the remaining capacity is \"just enough\".\n    #\n    # A score function that peaks when `bins_remain_cap` is equal to `item`.\n    #\n    # Consider the \"normalized slack\": `slack_norm = (bins_remain_cap - item) / item`\n    # We want `slack_norm` to be small and positive.\n    #\n    # Let's use the sigmoid function to map the *difference* `bins_remain_cap - item`.\n    # We want high scores for small positive differences.\n    #\n    # `score = sigmoid(k * (item - bins_remain_cap))` where we only consider usable bins.\n    #\n    # For usable bins (`bins_remain_cap >= item`), we want to prioritize those with\n    # the smallest `bins_remain_cap`.\n    # This corresponds to the \"Best Fit\" strategy.\n    #\n    # Let's assign a score that reflects this.\n    # `1.0 / (1.0 + np.exp(k * (bins_remain_cap - item)))`\n    # For usable bins:\n    # `c = item` => `exp(0)=1` => score = 0.5\n    # `c = item + epsilon` => `exp(k*epsilon)` large => score near 0.\n    # `c = item - epsilon` => `exp(-k*epsilon)` near 1 => score near 0.5 (this is unusable)\n    #\n    # We need to ensure unusable bins get a score of 0.\n    # For usable bins, we want a score that is higher for smaller `bins_remain_cap`.\n\n    # A direct application of a sigmoid to `bins_remain_cap` for prioritizing tightness:\n    # `sigmoid(k * (item - bins_remain_cap))`\n    # If `bins_remain_cap` is very small (unusable), `item - bins_remain_cap` is large positive. Sigmoid is ~1. (Bad)\n    # If `bins_remain_cap` is large, `item - bins_remain_cap` is large negative. Sigmoid is ~0. (Good, penalizes slack)\n    # If `bins_remain_cap = item`, `item - bins_remain_cap = 0`. Sigmoid is 0.5.\n\n    # So, this form prioritizes bins that are \"not too large\".\n    # To make it prioritize *tight* fits (not too large, not too small), we'd need a bell curve.\n    # But with sigmoid, we can do:\n    # 1. Penalize unusable bins by setting their score to 0.\n    # 2. For usable bins, use `sigmoid(k * (item - bins_remain_cap))`.\n    # This will give scores between 0 and 0.5, with higher scores for bins closer to `item` from above.\n\n    # Let's adjust the range to [0, 1] for usable bins.\n    # The term `item - bins_remain_cap` for usable bins ranges from `0` (when `c=i`) down to negative infinity (when `c` is very large).\n    #\n    # If we want to maximize tightness, we want to maximize `1 / (1 + exp(k * (bins_remain_cap - item)))`.\n    # For usable bins (`c >= i`), this score is between 0.5 (for `c=i`) and 0 (for `c >> i`).\n    # This doesn't quite capture \"ideal fit\" versus \"too much slack\".\n\n    # A more effective Sigmoid Fit Score heuristic:\n    # Consider the \"wasted space\" ratio: `waste = (bins_remain_cap - item) / bins_remain_cap`.\n    # We want this to be small.\n    #\n    # Let's define a score that's high for small `bins_remain_cap - item` (for usable bins).\n    #\n    # A common strategy is to transform the remaining capacity such that it has a peak at `item`.\n    # However, sigmoid is monotonic.\n    #\n    # Let's use a simple sigmoid application for \"fit\":\n    # Prioritize bins where `bins_remain_cap` is just sufficient.\n    # This means `bins_remain_cap - item` should be small and non-negative.\n    #\n    # A score that increases as `bins_remain_cap` decreases towards `item`.\n    # `sigmoid(k * (item - bins_remain_cap))` is good for `c > i`.\n    #\n    # Let's try: `priorities[j] = sigmoid(k * (item - bins_remain_cap[j]))` for usable bins.\n    # This assigns scores from 0 (for very large `c`) up to 0.5 (for `c = i`).\n    # To get scores in [0, 1], we can adjust the sigmoid.\n    #\n    # The standard form `sigmoid(x)` maps to (0, 1).\n    # Let `x = k * (item - bins_remain_cap)`.\n    # For usable bins, `item - bins_remain_cap <= 0`. So `x <= 0`.\n    # Sigmoid of `x <= 0` ranges from 0 to 0.5.\n    #\n    # This means `sigmoid(k * (item - bins_remain_cap))` would give priorities that are 0 (for very large slack) up to 0.5 (for perfect fit).\n    # This is a valid heuristic, prioritizing best fit, but not giving high scores for slightly imperfect but still good fits.\n\n    # To include good fits where `c > i`:\n    # Consider a \"goodness\" metric that is high for `c` close to `i`.\n    # Let's define the score for a bin `j` as:\n    # `score_j = sigmoid(k * (bins_remain_cap[j] - item))`\n    # For usable bins (`c >= i`):\n    #   `c = i` => `score = 0.5`\n    #   `c = i + epsilon` => `score` near 1.\n    #   `c = i + large` => `score` near 1.\n    # This prioritizes bins that are larger than the item, penalizing no slack.\n\n    # Let's try to map the *difference* `bins_remain_cap - item` to a score using sigmoid.\n    #\n    # We want a score that is high when `bins_remain_cap - item` is small.\n    #\n    # `sigmoid(-k * (bins_remain_cap - item))` = `sigmoid(k * (item - bins_remain_cap))`\n    #\n    # Consider `k=10`.\n    #\n    # Usable bins mask: `bins_remain_cap >= item`.\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # We want to prioritize bins with the smallest remaining capacity that can fit the item.\n    # This is \"Best Fit\".\n    # We can map `bins_remain_cap` to a score using sigmoid to emphasize certain ranges.\n    #\n    # Let's assign scores from the sigmoid: `f(x) = 1 / (1 + exp(-x))`\n    # We want to use `x` such that higher scores mean better fits.\n    #\n    # A good fit is when `bins_remain_cap` is close to `item`.\n    # Consider `bins_remain_cap - item`. We want this to be small and non-negative.\n    #\n    # `sigmoid(k * (item - bins_remain_cap))` gives scores from 0 to 0.5. Higher score means closer to `item` (from above).\n    # `sigmoid(k * (bins_remain_cap - item))` gives scores from 0.5 to 1. Higher score means further from `item` (from above).\n    #\n    # Let's use the first one and scale it.\n    # We only apply this to bins where `bins_remain_cap >= item`.\n    # For these bins, `item - bins_remain_cap <= 0`.\n    # So `sigmoid(k * (item - bins_remain_cap))` gives values between 0 (for large slack) and 0.5 (for exact fit).\n    # This means the highest priority is given to the \"tightest fit\" where `c = i`.\n    #\n    # Let's choose a steepness factor `k`.\n    k = 10.0  # This factor controls how \"sharp\" the sigmoid transition is.\n\n    # Mask for bins that have enough capacity for the item.\n    # These are the \"usable\" bins.\n    usable_bins_mask = bins_remain_cap >= item\n\n    # Initialize priorities to zero. Unusable bins will keep a score of 0.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate scores only for usable bins.\n    # We want to prioritize bins with remaining capacity closest to the item size.\n    # This is a \"best fit\" criterion.\n    # The function `sigmoid(k * (item - bins_remain_cap))` maps:\n    # - If `bins_remain_cap` is very large: `item - bins_remain_cap` is large negative. Sigmoid is near 0.\n    # - If `bins_remain_cap == item`: `item - bins_remain_cap` is 0. Sigmoid is 0.5.\n    # - If `bins_remain_cap` is slightly larger than `item`: `item - bins_remain_cap` is small negative. Sigmoid is slightly less than 0.5.\n    #\n    # This means this formula gives higher scores for bins that are a tighter fit.\n    # The scores are in the range [0, 0.5) for bins where `bins_remain_cap > item` and exactly 0.5 for `bins_remain_cap == item`.\n    #\n    # To get scores in the range [0, 1] representing priority, we can potentially shift and scale this,\n    # or interpret this range as a valid priority. A higher value implies higher priority.\n    #\n    # The expression `1 / (1 + exp(x))` where `x = -k * (bins_remain_cap - item)` results in:\n    # if `c < i`: `c-i < 0`. `-k*(c-i) > 0`. `exp(>0)` is large. Score near 0. (Unusable)\n    # if `c = i`: `c-i = 0`. `-k*0 = 0`. `exp(0)=1`. Score = 1/2 = 0.5.\n    # if `c > i`: `c-i > 0`. `-k*(c-i) < 0`. `exp(<0)` small. Score near 1. (Too much slack)\n    # This prioritizes slack over tight fit.\n\n    # Let's go back to prioritizing tightness.\n    # `sigmoid(k * (item - bins_remain_cap))` for usable bins.\n    # These scores range from 0 to 0.5. Let's consider this as the priority.\n\n    # To normalize or scale, we can multiply by 2, resulting in [0, 1].\n    # `2 * sigmoid(k * (item - bins_remain_cap))` for usable bins.\n    # This maps `0` to `0` and `0.5` to `1`.\n    # So, exactly matching capacity gets priority 1.\n    # Slightly larger capacity gets priority slightly less than 1.\n    # Much larger capacity gets priority close to 0.\n\n    # This looks like a reasonable \"Sigmoid Fit Score\" heuristic for prioritizing \"best fit\".\n\n    # Calculate the argument for the sigmoid function.\n    # We are interested in `item - bins_remain_cap`. For usable bins, this is <= 0.\n    # A smaller (less negative) value indicates a better fit.\n    sigmoid_arg = k * (item - bins_remain_cap[usable_bins_mask])\n\n    # Apply the sigmoid function.\n    # sigmoid(x) = 1 / (1 + exp(-x))\n    # The scores for usable bins will be in the range [0, 0.5].\n    # Higher score means a better fit (closer to `item`).\n    sigmoid_scores = 1.0 / (1.0 + np.exp(-sigmoid_arg))\n\n    # Scale these scores to the range [0, 1] to represent priority.\n    # A score of 0.5 (exact fit) becomes 1. A score of 0 (large slack) becomes 0.\n    priorities[usable_bins_mask] = sigmoid_scores * 2.0\n\n    return priorities",
    "response_id": 6,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response7.txt_stdout.txt",
    "code_path": "problem_iter1_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Random Fit.\n\n    In Random Fit, we randomly select a bin among those that can accommodate the item.\n    This heuristic assigns a higher priority to bins that have enough remaining capacity,\n    and a lower priority to those that do not. The actual selection is randomized\n    among eligible bins. Here, we create a score that reflects this eligibility,\n    with higher scores for bins that fit the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Assign a high priority (e.g., 1) to bins that can fit the item\n    # Assign a low priority (e.g., 0) to bins that cannot fit the item.\n    # This ensures that only valid bins are considered in a subsequent random selection.\n    priorities = np.where(bins_remain_cap >= item, 1, 0)\n    return priorities",
    "response_id": 7,
    "obj": 4.487435181491823,
    "SLOC": 3.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response8.txt_stdout.txt",
    "code_path": "problem_iter1_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    This heuristic prioritizes bins that have just enough space for the item (tight fit),\n    with a small probability of choosing a random bin to encourage exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of choosing a random bin\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if np.any(suitable_bins_mask):\n        # Calculate a \"tightness\" score for suitable bins\n        # Lower value means tighter fit (less wasted space)\n        tightness_scores = bins_remain_cap[suitable_bins_mask] - item\n        \n        # Assign a higher priority to bins with a tighter fit.\n        # We can invert the tightness score to make higher values mean higher priority.\n        # Adding a small constant to avoid division by zero if all fits are perfect.\n        priorities[suitable_bins_mask] = 1.0 / (tightness_scores + 1e-6)\n\n        # Apply epsilon-greedy strategy\n        if np.random.rand() < epsilon:\n            # Explore: Assign equal probability to all bins (including unsuitable ones)\n            # This can sometimes be useful if the \"best\" fit isn't always optimal long-term.\n            # However, for strict fitting, we might want to only explore among suitable bins.\n            # For this implementation, we'll explore among *all* bins to allow for\n            # the possibility of picking a slightly less optimal bin that might\n            # enable better packing later.\n            priorities = np.ones_like(priorities) / num_bins\n        else:\n            # Exploit: Select the bin with the highest priority score (tightest fit)\n            # For non-suitable bins, priority remains 0, so they won't be picked by max.\n            pass\n    else:\n        # If no bin can fit the item, all priorities remain 0.\n        # In a real implementation, this would likely trigger creating a new bin.\n        pass\n\n    return priorities",
    "response_id": 8,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response9.txt_stdout.txt",
    "code_path": "problem_iter1_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.\n\n    This strategy assigns higher priority to bins that have a remaining capacity\n    closest to the item's size. A temperature parameter controls the \"softness\"\n    of the softmax. A higher temperature makes the probabilities more uniform,\n    while a lower temperature sharpens the preference for bins that are a better fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Ensure we only consider bins that can actually fit the item\n    valid_bin_indices = bins_remain_cap >= item\n    \n    # If no bin can fit the item, return zeros (or handle as an error/new bin creation)\n    if not np.any(valid_bin_indices):\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit\" for valid bins: the closer the remaining capacity\n    # is to the item size, the better the fit. We can represent this as\n    # a score where a smaller difference is better. We want to maximize\n    # priority, so we invert the difference. A common approach is to use\n    # the negative difference, or some function of the inverse difference.\n    # Here we use (remaining_capacity - item_size) as the \"slack\".\n    # Smaller slack is better.\n    slack = bins_remain_cap[valid_bin_indices] - item\n    \n    # We want to assign higher priorities to bins with less slack.\n    # To use softmax, we need values that we want to be high to be large.\n    # So, we can use the negative slack.\n    # Also, we might want to prevent extreme values from dominating.\n    # A simple way to make it more \"soft\" is to add a small epsilon to avoid\n    # log(0) if we were using inverse slack.\n    # Or we can directly exponentiate a score that represents \"goodness\".\n    \n    # Let's define a \"goodness\" score. A good fit means slack is small.\n    # We can use -slack, so smaller slack becomes larger negative slack.\n    # To get positive scores for softmax, we can use -slack and then shift\n    # or simply use the raw slack as is and let softmax handle it.\n    # A common practice for Softmax-based selection is to have a score `s`\n    # where higher `s` is preferred.\n    # A good bin has `remaining_capacity - item` small.\n    # So, `item - remaining_capacity` is a good score (more negative is worse).\n    # Or, `remaining_capacity - item` (slack). Smaller slack is better.\n    # To use softmax, we want the best bins to have high values.\n    # Let's use a measure that is inversely related to slack, perhaps `1 / (slack + epsilon)`.\n    # However, this can lead to very large values if slack is near zero.\n    \n    # A more stable approach for Softmax-based selection often involves\n    # transforming the scores so that better choices yield higher values.\n    # If we consider `remaining_capacity - item` (slack), smaller is better.\n    # To make it suitable for softmax where higher values are preferred,\n    # we can transform it.\n    # For example, let the \"preference score\" be `-slack`.\n    # `preference_scores = -slack`\n    \n    # Another approach, often seen in contexts like attention mechanisms,\n    # is to use a \"similarity\" or \"energy\" score.\n    # We want bins that are \"close\" to fitting the item.\n    # Let's define closeness as the absolute difference.\n    # `closeness = np.abs(bins_remain_cap[valid_bin_indices] - item)`\n    # Smaller closeness is better.\n    # For softmax, we want higher values for better choices.\n    # So, we can use `-closeness`.\n    # `preference_scores = -np.abs(bins_remain_cap[valid_bin_indices] - item)`\n\n    # Let's go with a strategy that prioritizes bins that leave minimal slack.\n    # Slack = remaining_capacity - item_size.\n    # A smaller slack is better.\n    # To make it suitable for softmax (higher value = higher priority),\n    # we can use `-slack`.\n    # `preference_scores = -(bins_remain_cap[valid_bin_indices] - item)`\n\n    # To avoid issues with very small or negative scores, we can use a\n    # temperature parameter to smooth the distribution.\n    # A common form is `exp(score / temperature)`.\n    # A lower temperature emphasizes differences more.\n    \n    # Let's try a score that is proportional to how \"full\" the bin becomes.\n    # A bin that becomes almost full (item + small slack) is good.\n    # So, `item` is good if `bins_remain_cap` is `item + epsilon`.\n    # Consider the inverse of slack: `1.0 / (slack + 1e-6)` to avoid division by zero.\n    # Then apply softmax.\n    \n    # Let's define a score `s` where higher `s` is better.\n    # If `r` is remaining capacity, and `i` is item size:\n    # We want `r` to be close to `i`.\n    # So, `r-i` (slack) should be small and non-negative.\n    # Consider the score as `-slack` which means `i-r`. Small negative is better.\n    # Let's use `i - r` which becomes more positive as slack decreases.\n    # `scores = item - bins_remain_cap[valid_bin_indices]`\n    \n    # To make it more robust, especially if we have bins that are much larger,\n    # which might have a slightly larger slack but are still \"valid\",\n    # we can consider the ratio of item to remaining capacity if the remaining\n    # capacity is close to item size.\n\n    # A simple and effective approach for \"best fit\" is to consider the remaining capacity\n    # after placing the item. We want this remaining capacity to be as small as possible,\n    # but non-negative.\n    # So, `residual_capacity = bins_remain_cap[valid_bin_indices] - item`\n    # We want to maximize `-(residual_capacity)`.\n    \n    # Let's refine this for softmax. Higher values for better bins.\n    # If a bin has `remaining_capacity = r` and item is `i`:\n    # Preferred state is `r` is slightly larger than `i`.\n    # Let's define a \"preference\" `p` such that `p` is high when `r-i` is small and non-negative.\n    # One way is `p = - (r - i) = i - r`. This is maximized when `r-i` is minimized.\n    # Let's add a constant to ensure positivity before exponentiation if needed,\n    # or rely on the softmax properties.\n    \n    # Let's consider the \"tightness\" of the fit.\n    # Tightness is maximized when `bins_remain_cap[idx]` is exactly `item`.\n    # So, a score proportional to `1 / (slack + epsilon)` or `-slack`.\n    # Let's use `-slack` which is `item - bins_remain_cap[valid_bin_indices]`.\n    \n    preference_scores = item - bins_remain_cap[valid_bin_indices]\n    \n    # Apply a temperature to control the softness of the softmax.\n    # A temperature T > 0. Lower T -> harder selection, higher T -> softer selection.\n    # We want to avoid extremely large or small values before exponentiation.\n    # Let's cap the scores to prevent overflow or underflow issues if `item` is very large\n    # or `bins_remain_cap` is very small (though we filtered for `bins_remain_cap >= item`).\n    # If `item - bins_remain_cap` is very negative (large slack), it should have low priority.\n    \n    # To ensure non-negativity for the exponents, and potentially create a clearer preference:\n    # We can shift the scores by adding the maximum score. This doesn't change\n    # the softmax output but makes intermediate values positive.\n    # shifted_scores = preference_scores - np.min(preference_scores)\n    \n    # Let's directly use the preference scores, assuming `softmax` can handle\n    # potentially negative inputs by its nature of exponentiation and normalization.\n    \n    # Define a temperature parameter. A value like 1.0 is a good starting point.\n    # Lower values (e.g., 0.1) will make the selection more \"greedy\", focusing\n    # on the single best bin. Higher values (e.g., 5.0) will distribute probability\n    # more evenly across good fitting bins.\n    temperature = 1.0\n\n    # Calculate exponentiated scores\n    # We want bins with the smallest slack (largest `item - slack`) to have high scores.\n    # The formula for softmax is exp(score_i) / sum(exp(score_j)).\n    # If we use `item - slack`, then small slack leads to larger scores.\n    \n    # Ensure we don't have issues with extremely large positive or negative scores.\n    # For `item - bins_remain_cap`, if `bins_remain_cap` is very small and `item` is large,\n    # this can be large positive. If `bins_remain_cap` is much larger than `item`,\n    # this can be large negative.\n    \n    # Let's consider the \"fit\" as the negative difference: `-(bins_remain_cap - item) = item - bins_remain_cap`.\n    # A bin where `bins_remain_cap` is just enough for `item` gives a score of `0`.\n    # A bin with more capacity gives a more negative score.\n    # A bin with just enough capacity (slack 0) is ideal.\n    # If `bins_remain_cap` can be very large, `item - bins_remain_cap` can be very negative.\n    \n    # Let's try to map the remaining capacity to a desirability score.\n    # We want `bins_remain_cap` to be close to `item`.\n    # Let's map the values such that `item` maps to a high score, and values\n    # far from `item` map to lower scores.\n    \n    # For the \"best fit\" strategy, we are looking for the bin where `remaining_capacity`\n    # is closest to `item` *and* `remaining_capacity >= item`.\n    # This means we want to minimize `remaining_capacity - item`.\n    \n    # Score for softmax: higher is better.\n    # So, we want to maximize `-(bins_remain_cap[valid_bin_indices] - item)`.\n    # Let `fittness = -(bins_remain_cap[valid_bin_indices] - item)`\n    # `fittness = item - bins_remain_cap[valid_bin_indices]`\n\n    # Calculate exponential of scaled scores.\n    # Softmax expects scores that can be exponentiated.\n    # A stable way is `exp(scores / temperature)`.\n    # Consider `scores = item - bins_remain_cap[valid_bin_indices]`.\n    # If `bins_remain_cap` is very large, `scores` become very negative, exp goes to 0.\n    # If `bins_remain_cap` is slightly larger than `item`, `scores` are slightly negative.\n    # If `bins_remain_cap == item`, `scores` are 0, exp is 1.\n    # This seems reasonable: bins that perfectly fit are ideal. Bins with more space are less ideal.\n\n    # Ensure scores are not excessively large or small before exponentiation.\n    # Clip scores to avoid numerical instability.\n    # A reasonable range could be [-10, 10] for example, after scaling.\n    # If we use `item - bins_remain_cap`, and `item` can be large and `bins_remain_cap` small (but valid),\n    # the score can be large. Let's consider the *relative* difference.\n    \n    # A better heuristic for \"fit\" could be related to how \"full\" the bin becomes.\n    # We want the bin to be as full as possible without overflowing.\n    # So, if `r` is remaining capacity, we want `r-item` to be minimal.\n    # Let `score = -(r-item) = item - r`.\n    # We are selecting from bins where `r >= item`.\n    # The maximum value of `item - r` is 0 (when `r = item`).\n    # The minimum value can be large negative if `r` is much larger than `item`.\n    \n    # Example: item=5, bins_remain_cap=[7, 5, 10]\n    # Valid bins: [7, 5, 10]\n    # Scores (item - r): 5-7=-2, 5-5=0, 5-10=-5\n    # Softmax inputs: exp(-2/T), exp(0/T), exp(-5/T)\n    # If T=1: exp(-2)=0.135, exp(0)=1, exp(-5)=0.0067\n    # Probabilities: 0.135 / (0.135+1+0.0067) = 0.116, 1 / (0.135+1+0.0067) = 0.855, 0.0067 / (0.135+1+0.0067) = 0.0058\n    # This prioritizes the bin with remaining capacity exactly matching the item.\n\n    # The scores can be directly used in softmax, but for numerical stability,\n    # it's often better to center them around 0.\n    # `centered_scores = preference_scores - np.mean(preference_scores)`\n    # Or `centered_scores = preference_scores - np.max(preference_scores)` which makes max score 0.\n    \n    # Let's use `max(0, item - bins_remain_cap[valid_bin_indices])` as the score,\n    # so only bins that *could* fit are considered positively, and better fits are higher.\n    # `score = np.maximum(0, item - bins_remain_cap[valid_bin_indices])`\n    # This maps bins where `r > item` to 0, and bins where `r = item` to 0.\n    # This doesn't differentiate well between bins that are *almost* full.\n\n    # Let's stick with `item - bins_remain_cap[valid_bin_indices]` and handle potential large negative values by letting softmax turn them to near-zero probabilities.\n    \n    # Calculate exponentiated preference scores.\n    # We want higher scores for bins that leave less slack.\n    # `score = item - remaining_capacity`. Higher score is better.\n    # Maximize `score`.\n    # `score_values = item - bins_remain_cap[valid_bin_indices]`\n\n    # To prevent overflow with large positive scores (if item >> remaining_capacity, although this is filtered),\n    # or underflow with very large negative scores (if remaining_capacity >> item),\n    # we can scale or clip.\n    # Let's normalize the scores first to be in a more manageable range, e.g., [0, 1].\n    # `normalized_scores = (score_values - np.min(score_values)) / (np.max(score_values) - np.min(score_values) + 1e-8)`\n    # This maps the current range of scores to [0, 1]. The best fit (min slack) gets 1, worst fit gets 0.\n    \n    # A more common approach with softmax is direct use of scores with temperature.\n    # `exponentials = np.exp(score_values / temperature)`\n    \n    # Using the definition of \"best fit\": minimize `remaining_capacity - item`.\n    # We want bins that result in smaller positive residual capacities.\n    # `residuals = bins_remain_cap[valid_bin_indices] - item`\n    # We want to minimize `residuals`.\n    # For softmax, we want to maximize a function of `residuals`.\n    # Let `priority_score = -residuals = item - bins_remain_cap[valid_bin_indices]`\n    # Higher `priority_score` means better fit.\n\n    scores = item - bins_remain_cap[valid_bin_indices]\n\n    # If there's only one valid bin, its priority should be 1.0\n    if len(scores) == 1:\n        priorities = np.zeros_like(bins_remain_cap)\n        priorities[valid_bin_indices] = 1.0\n        return priorities\n\n    # Softmax calculation\n    # `exps = np.exp(scores / temperature)`\n    # Sum of exponentials\n    # `sum_exps = np.sum(exps)`\n    # Calculate probabilities\n    # `probabilities = exps / sum_exps`\n    \n    # For numerical stability, it's common to subtract the maximum score from all scores\n    # before exponentiation: exp(score_i - max(score_j)). This makes the maximum score 0.\n    max_score = np.max(scores)\n    stable_scores = scores - max_score\n    exps = np.exp(stable_scores / temperature)\n    sum_exps = np.sum(exps)\n    \n    # Handle case where sum_exps might be zero (e.g., all inputs to exp were -inf, which shouldn't happen here if temperature > 0 and scores are finite)\n    if sum_exps == 0:\n        # This could happen if all stable_scores were extremely negative.\n        # In such a case, perhaps assign uniform probability to valid bins.\n        probabilities = np.ones_like(scores) / len(scores)\n    else:\n        probabilities = exps / sum_exps\n\n    # Fill the final priorities array\n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bin_indices] = probabilities\n\n    return priorities",
    "response_id": 9,
    "obj": 4.497407259672929,
    "SLOC": 23.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response10.txt_stdout.txt",
    "code_path": "problem_iter1_code10.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit.\n\n    The priority is determined by the remaining capacity of the bin after placing the item.\n    A bin is only considered if it has enough capacity for the item.\n    Among eligible bins, the one with the smallest remaining capacity AFTER placing the item\n    is prioritized (to minimize wasted space). If no bin can fit the item, a score of -1\n    is returned for all bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with -1 (ineligible)\n\n    # Find bins that can accommodate the item\n    eligible_bins_mask = bins_remain_cap >= item\n\n    if np.any(eligible_bins_mask):\n        # Calculate remaining capacity after placing the item in eligible bins\n        remaining_capacities_after_fit = bins_remain_cap[eligible_bins_mask] - item\n\n        # Higher priority for bins with less remaining capacity after fitting (First Fit preference)\n        # We want to select the bin that leaves the LEAST remaining space.\n        # So, we assign a score based on the inverse of the remaining capacity,\n        # but we want to minimize it. A simple way is to use the remaining capacity itself\n        # and take the minimum among those.\n        # To achieve \"First Fit\" behavior, we prioritize the *first* bin that meets the criteria.\n        # In this priority function, we are assigning scores. The selection logic *outside*\n        # this function will pick the highest score. So, we want to assign a higher score\n        # to the \"best fit\" bin according to First Fit.\n\n        # The goal of First Fit is to place the item in the first bin that can accommodate it.\n        # If we want to translate this to a priority score where higher means better,\n        # we should give the highest score to the *first* eligible bin.\n\n        # Let's redefine the priority for the selection algorithm:\n        # Higher score means more preferred.\n        # For First Fit, the most preferred bin is the *first* one that fits.\n        # So, let's assign a high score to the first eligible bin and lower scores to subsequent ones.\n\n        # A simple way to implement this within a priority function is to give a\n        # \"value\" to each eligible bin. The selection process (outside this function)\n        # will pick the bin with the highest value.\n        # If we want First Fit, we assign a decreasing priority to bins as we find them.\n\n        # Let's assign priorities based on the bin index for First Fit:\n        # The first bin that fits gets the highest priority, the second gets the next highest, etc.\n        # To make this concrete: if the item fits in bins at indices 0 and 2,\n        # bin 0 gets a higher priority than bin 2.\n\n        # Assign a score that prioritizes earlier bins if they are eligible.\n        # We can use a large number and subtract the index.\n        # The \"best\" fit in terms of minimizing remaining space is implicitly handled by\n        # the fact that we only consider bins that *can* fit the item.\n\n        # Let's try a simple approach: assign a base score to eligible bins.\n        # The highest score will go to the first eligible bin.\n\n        # Option 1: Score based on \"best fit\" if we consider that within First Fit's spirit.\n        # This means, among eligible bins, pick the one that leaves the least remaining space.\n        # A higher priority for a smaller remaining_capacity_after_fit.\n        # We can negate the remaining capacity to make smaller capacities have higher priority.\n        # Example: if remaining_capacities_after_fit are [2.0, 0.5], we want 0.5 to be preferred.\n        # Negating gives [-2.0, -0.5]. The maximum of these is -0.5. This corresponds to the best fit.\n        # This isn't strictly First Fit but a common heuristic variation.\n\n        # Option 2: Strict First Fit priority.\n        # The highest priority should be given to the *first* bin encountered that can fit the item.\n        # Let's assign a priority score such that the earliest available bin gets the highest score.\n        # For example, if bins_remain_cap = [5, 3, 6] and item = 2:\n        # Eligible bins are at indices 0 (rem=3), 1 (rem=1), 2 (rem=4).\n        # First Fit would pick bin 0.\n        # To make bin 0 have the highest score, we can assign a large number minus its index.\n        # Let's say we assign `N - index` for eligible bins, where N is the total number of bins.\n        # Bin 0 (eligible): N - 0 = N\n        # Bin 1 (eligible): N - 1\n        # Bin 2 (eligible): N - 2\n        # The maximum score would be N, corresponding to bin 0.\n\n        # Let's use the index-based priority for strict First Fit\n        base_priority = len(bins_remain_cap) # A large enough number\n        priorities[eligible_bins_mask] = base_priority - np.arange(len(bins_remain_cap))[eligible_bins_mask]\n\n    return priorities",
    "response_id": 10,
    "obj": 4.487435181491823,
    "SLOC": 8.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response11.txt_stdout.txt",
    "code_path": "problem_iter1_code11.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit strategy.\n\n    The Best Fit strategy aims to place the item into the bin where it fits most snugly,\n    leaving the least remaining capacity. This often leads to better overall packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher priority score indicates a better fit.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # For bins where the item fits (remaining capacity >= item size)\n    # we want to prioritize bins with the smallest remaining capacity after placement.\n    # This is equivalent to prioritizing bins with remaining capacity just enough to fit the item.\n    # The 'cost' of placing the item is the remaining capacity of the bin AFTER placing the item.\n    # We want to MINIMIZE this cost. Since priority is usually interpreted as \"more is better\",\n    # we can transform the cost. A simple transformation is to use the negative of the cost\n    # or a large number minus the cost, ensuring that bins with lower costs (better fits)\n    # get higher priority scores.\n\n    # Calculate the remaining capacity if the item is placed in each bin\n    potential_remaining_capacities = bins_remain_cap - item\n\n    # Filter out bins where the item doesn't fit\n    fits_mask = potential_remaining_capacities >= 0\n\n    # For bins where the item fits, assign a priority based on how tightly it fits.\n    # The ideal scenario is when potential_remaining_capacities is 0 (perfect fit).\n    # Bins with smaller potential_remaining_capacities are better.\n    # We can use the negative of the potential_remaining_capacities as a priority score.\n    # This means a perfect fit (0 remaining capacity) will have a priority of 0,\n    # while a tighter fit (e.g., -1 remaining capacity if it were allowed, but we filtered)\n    # would have a higher positive priority if we used a different logic.\n    # A more standard way for \"best fit\" to be directly translated to a highest priority\n    # would be to consider the difference `bin_capacity - item_size`. The smaller this\n    # difference for a valid bin, the better the fit.\n    # So, `-(bin_capacity - item_size)` will give higher scores to better fits.\n\n    # To ensure that bins where the item doesn't fit have a lower priority (or zero),\n    # we can initialize priorities to a low value or zero and then update only for valid bins.\n    \n    # Calculate the \"badness\" of the fit: how much capacity is left over.\n    # We want to minimize this.\n    fit_differences = bins_remain_cap - item\n\n    # Only consider bins where the item fits\n    valid_fits = fit_differences[fits_mask]\n\n    # Assign priorities. For a best fit heuristic, we want to assign a high score to the bin\n    # that has the smallest remaining capacity after fitting the item.\n    # The smaller `bins_remain_cap[i] - item` is (as long as it's >= 0), the better.\n    # So, a simple priority can be `- (bins_remain_cap[i] - item)`.\n    # However, this might lead to positive priorities for bins that are only slightly too small\n    # if we didn't have the `fits_mask`.\n    # A common way to implement \"best fit\" as a priority is to assign a score based on the inverse\n    # of the leftover space. Or, a very high priority for the *least* leftover space.\n    \n    # Let's use a penalty approach: the \"penalty\" is the leftover capacity.\n    # We want to minimize the penalty. So, higher priority means smaller penalty.\n    # A simple transformation: `max_possible_leftover - actual_leftover`.\n    # The maximum possible leftover could be considered the bin capacity, or simply the max\n    # of valid leftovers.\n\n    # Another approach is to assign a priority inversely proportional to the remaining capacity\n    # after placing the item, but ensure it's only for valid placements.\n    # For bins `i` where `bins_remain_cap[i] >= item`:\n    # Priority is proportional to `1 / (bins_remain_cap[i] - item)` or `-(bins_remain_cap[i] - item)`\n    # and these values should be positive for priority.\n    # Let's map `bins_remain_cap[i] - item` to a priority score such that smaller difference means higher priority.\n    # `priority = MAX_PRIORITY - (bins_remain_cap[i] - item)` could work if we know MAX_PRIORITY.\n    # Alternatively, `priority = 1.0 / (1 + (bins_remain_cap[i] - item))` for bins that fit.\n\n    # Let's try a direct score where higher is better:\n    # For valid fits, the \"score\" is how much capacity is left. We want this to be minimal.\n    # So, we can make the priority inversely related to the leftover space.\n    # A simple way: give a large base score, and subtract the leftover space.\n    # We need to handle the case where `bins_remain_cap[i] - item` is zero.\n    # Let's aim for priorities like:\n    # For bins where `bins_remain_cap[i] >= item`: priority = `C - (bins_remain_cap[i] - item)`\n    # For bins where `bins_remain_cap[i] < item`: priority = 0 or a very small negative number.\n    \n    # A robust way for Best Fit is to prioritize bins with the smallest *positive* difference\n    # between bin capacity and item size.\n    \n    # Calculate the remaining capacity for all bins, and mark invalid fits with a large negative value.\n    # This way, `np.argmax` will select the bin with the smallest positive residual, or the largest\n    # negative value if no bin fits.\n    \n    # A common pattern for \"best fit\" with `argmax` is to return `-(residual_capacity)` for valid fits,\n    # and `-infinity` or a very small number for invalid fits.\n    \n    # `residual_capacity = bins_remain_cap[i] - item`\n    # If `residual_capacity >= 0`, priority is `-residual_capacity`. This means a smaller positive\n    # residual leads to a higher (less negative) priority. E.g., residual 0 -> priority 0, residual 1 -> priority -1.\n    # If `residual_capacity < 0`, priority is `-np.inf` (or a very small number).\n    \n    # Let's assign a very low priority score to bins where the item doesn't fit.\n    # For bins where it fits, the priority is the negative of the remaining capacity after packing.\n    # This means the smallest non-negative remaining capacity will have the highest (least negative) priority.\n    \n    priorities = np.full_like(bins_remain_cap, -np.inf) # Initialize with very low priority\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate remaining capacity for bins where item can fit\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # For bins that can fit the item, the priority is the negative of the leftover space.\n    # The smaller the leftover space (closer to 0), the higher the priority score (closer to 0).\n    priorities[can_fit_mask] = -remaining_after_fit\n    \n    return priorities",
    "response_id": 11,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response12.txt_stdout.txt",
    "code_path": "problem_iter1_code12.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    The Worst Fit strategy aims to place the item in the bin that has the most remaining capacity.\n    This is generally a good heuristic for online bin packing as it tries to leave\n    smaller remaining capacities in other bins, which might be useful for packing\n    future smaller items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        The priority is set to the remaining capacity of the bin if the item fits,\n        and 0 otherwise. This encourages packing into the fullest bins.\n    \"\"\"\n    # Initialize priorities to 0 for all bins\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins where the item can fit\n    # A bin can fit the item if its remaining capacity is greater than or equal to the item's size\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins where the item can fit, set the priority to the remaining capacity\n    # This implements the Worst Fit strategy: higher remaining capacity = higher priority\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask]\n\n    return priorities",
    "response_id": 12,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response13.txt_stdout.txt",
    "code_path": "problem_iter1_code13.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    The Almost Full Fit (AFF) strategy aims to prioritize bins that are \"almost full\"\n    and can still accommodate the current item. This is a greedy approach that\n    tries to fill bins efficiently before opening new ones.\n\n    The priority is calculated as follows:\n    1. For bins that can accommodate the item (remaining capacity >= item size):\n       - The priority is higher for bins with *less* remaining capacity (i.e., closer to being full).\n       - We assign a priority score based on how much capacity is *left* after placing the item.\n         A smaller remaining capacity after placement means the bin was \"more full\" before.\n       - The priority is `bin_remain_cap - item` for these bins.\n\n    2. For bins that *cannot* accommodate the item:\n       - They receive a priority of -1, effectively making them unselectable.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher preference.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with -1 for unselectable bins\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate priority for bins that can fit the item\n    # The priority is the remaining capacity *after* the item is placed.\n    # We want to select bins that leave the *least* remaining capacity,\n    # meaning they were almost full.\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask] - item\n\n    return priorities",
    "response_id": 13,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response14.txt_stdout.txt",
    "code_path": "problem_iter1_code14.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Exact Fit First Strategy:\n    # Prioritize bins that can fit the item perfectly.\n    # If multiple bins fit perfectly, we can use a secondary criterion (e.g., smallest remaining capacity after fit)\n    # to break ties, or simply pick the first one.\n    # For this heuristic, we'll assign a very high priority (e.g., 1000) to bins that are an exact fit.\n    # For bins that don't fit perfectly, we assign a lower priority (e.g., 0).\n\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Find bins that are an exact fit\n    exact_fit_mask = (bins_remain_cap == item)\n\n    # Assign high priority to exact fit bins\n    priorities[exact_fit_mask] = 1000\n\n    # Optional: If there are multiple exact fits, you might want to refine the priority.\n    # For example, pick the one with the least remaining capacity among exact fits.\n    # However, for a strict \"Exact Fit First\", any exact fit is highly preferred.\n    # If no exact fit exists, the priorities remain 0, meaning no bin is particularly preferred by this rule.\n\n    # For a more nuanced approach if no exact fit is found, one could then consider\n    # bins that are \"close fits\" or have the smallest remaining capacity among\n    # bins that CAN fit the item (bins_remain_cap >= item).\n    # But the prompt specifically asks for \"Exact Fit First\".\n\n    return priorities",
    "response_id": 14,
    "obj": 4.198244914240141,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response15.txt_stdout.txt",
    "code_path": "problem_iter1_code15.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using inverse distance strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We want to prioritize bins that are a \"closer fit\" for the item.\n    # This means bins where the remaining capacity is just enough to fit the item,\n    # or slightly larger.\n    # A simple way to represent this is to take the difference between the\n    # bin's remaining capacity and the item's size. We want this difference to be small and non-negative.\n\n    # Calculate the difference between remaining capacity and item size.\n    # If the item doesn't fit, the difference will be negative.\n    differences = bins_remain_cap - item\n\n    # We only want to consider bins where the item actually fits.\n    # For bins where the item doesn't fit, we assign a very low priority (e.g., 0 or negative infinity).\n    # For bins where the item fits, we want to prioritize those with the smallest non-negative difference.\n    # The inverse distance strategy suggests giving higher priority to items that are \"closer\".\n    # So, a smaller positive difference should result in a higher priority.\n\n    # We can use the inverse of the difference, but we need to handle the case where the difference is zero\n    # and avoid division by zero. Also, we want to ensure that negative differences (item doesn't fit)\n    # get a very low priority.\n\n    # Let's create priorities based on the differences:\n    # - If an item doesn't fit (difference < 0), priority is 0.\n    # - If an item fits perfectly (difference == 0), priority is very high (e.g., a large positive number).\n    # - If an item fits with some space left (difference > 0), priority is inversely proportional to the difference.\n    #   To avoid division by zero and to ensure smaller positive differences get higher priority, we can use\n    #   1 / (difference + epsilon) where epsilon is a small constant.\n\n    epsilon = 1e-6  # A small constant to avoid division by zero\n\n    # Initialize priorities to a very low value (or 0) for bins that cannot fit the item.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find bins where the item fits\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins where the item fits:\n    # Calculate the \"closeness\" of the fit. We want smaller positive differences to have higher priority.\n    # A simple inverse distance can be 1 / (difference + epsilon).\n    # For perfect fits (difference == 0), this will be 1/epsilon, which is large.\n    # For small positive differences, it will be slightly smaller but still large.\n    # For larger positive differences, it will be smaller.\n    fitting_differences = differences[can_fit_mask]\n    priorities[can_fit_mask] = 1.0 / (fitting_differences + epsilon)\n\n    # Ensure that bins where the item doesn't fit have the lowest priority.\n    # By initializing with zeros, we've already achieved this.\n\n    return priorities",
    "response_id": 15,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response16.txt_stdout.txt",
    "code_path": "problem_iter1_code16.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    This heuristic prioritizes bins that, after placing the item, will have a\n    remaining capacity that is neither too large nor too small, aiming to\n    balance bin utilization. The sigmoid function scales this preference.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity if the item is placed in each bin\n    potential_remaining_caps = bins_remain_cap - item\n\n    # Filter out bins where the item doesn't fit\n    valid_bins_mask = potential_remaining_caps >= 0\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # For valid bins, calculate the sigmoid score\n    # We want to prioritize bins where the remaining capacity is close to 0\n    # after packing the item. A small remaining capacity indicates a good fit.\n    # The sigmoid function's steepness can be tuned with 'k' and its midpoint with 'x0'.\n    # Here, we'll use a strategy where we want to penalize large remaining capacities.\n    # A common approach for Sigmoid Fit is to map the remaining capacity to a score.\n    # We can invert the remaining capacity and apply sigmoid, or use a sigmoid\n    # that saturates for large values.\n\n    # Let's aim to favor bins that are nearly full after placing the item.\n    # A smaller remaining capacity is generally better.\n    # The sigmoid function typically maps values to (0, 1).\n    # We want higher scores for smaller `potential_remaining_caps`.\n    # A common sigmoid form: 1 / (1 + exp(-k * (x - x0)))\n    # If we want higher scores for smaller x, we can use -x in the exponent:\n    # 1 / (1 + exp(k * (x - x0)))\n\n    # We want the ideal scenario to have a remaining capacity of 0.\n    # Let's define 'x0' as a target remaining capacity, perhaps near zero.\n    # Let's set x0 to a small value, say 0.05 * max_bin_capacity (assuming we knew it, or a generic small value).\n    # For simplicity and to avoid external parameters for now, let's consider the absolute value of remaining capacity.\n    # We want the score to be high when potential_remaining_caps is small (close to 0).\n\n    # Strategy: Score based on how close the potential remaining capacity is to 0,\n    # scaled by a factor that emphasizes smaller remaining capacities.\n    # A common sigmoid approach for optimization is to assign a score based on\n    # how \"good\" a state is. Here, a \"good\" state is a bin that is almost full.\n\n    # Let's define the sigmoid such that the peak is at 0 remaining capacity.\n    # If potential_remaining_caps is 0, the score should be high.\n    # If potential_remaining_caps is large, the score should be low.\n\n    # A function like `sigmoid(-k * x)` would achieve this:\n    # sigmoid(-k * 0) = 0.5\n    # sigmoid(-k * small_positive) approaches 0\n    # sigmoid(-k * large_positive) approaches 0\n\n    # This means we want to offset the function.\n    # Let's consider the 'fit' aspect. A good fit means `bins_remain_cap - item` is small.\n    # The \"Sigmoid Fit Score\" can be interpreted as how well the item \"fits\" into the remaining capacity.\n    # A good fit means `bins_remain_cap - item` is close to zero.\n\n    # Let's use a sigmoid that peaks at the 'best' remaining capacity after packing.\n    # The best remaining capacity is ideally close to zero.\n    # Let's define a reference point. `x0` can be the optimal remaining capacity.\n    # `k` controls the steepness.\n\n    # To keep it simple and self-contained, let's define a characteristic remaining capacity `ideal_residual`\n    # that we'd ideally like to leave in a bin after packing. A small positive value might be good,\n    # to avoid overfilling or leaving too much space. Let's say an ideal residual is around 5-10% of the bin capacity.\n    # However, we don't have bin capacity. So, we focus on `potential_remaining_caps`.\n\n    # Let's directly use the `potential_remaining_caps`. We want small values of this to have high priority.\n    # We can invert the potential remaining capacities for valid bins and apply a sigmoid that increases with its input.\n    # `score = 1 / (1 + exp(-k * (target_value - x)))` where x is `potential_remaining_caps`.\n    # If target_value is 0, and k > 0:\n    # score(0) = 0.5\n    # score(small_pos) < 0.5\n    # score(large_pos) << 0.5\n    # This is the opposite of what we want.\n\n    # Let's try `score = 1 / (1 + exp(k * x))` where x is `potential_remaining_caps`.\n    # If x = 0, score = 0.5\n    # If x = small_pos, score < 0.5\n    # If x = large_pos, score << 0.5\n    # This still gives lower scores for smaller remaining capacities.\n\n    # Alternative perspective: a \"good\" fit leaves minimal waste. So, `bins_remain_cap - item` should be small.\n    # Let's define a \"fit score\" directly. The closer `bins_remain_cap - item` is to 0, the higher the score.\n    # Sigmoid for \"closeness to zero\": we want values near zero to map to high scores.\n    # Consider a score based on `1 - sigmoid(abs(potential_remaining_caps))`.\n    # Or a sigmoid that is high around 0 and drops off.\n\n    # Let's use the standard sigmoid form and map the potential remaining capacity to it.\n    # `score = 1 / (1 + exp(-k * (target_remaining - potential_remaining_caps)))`\n    # If we want to favor bins where `potential_remaining_caps` is *small*, we should set `target_remaining` to a small value.\n    # Let `target_remaining = 0.1 * max_item_size` (as a proxy for optimal residual).\n    # If we don't know `max_item_size`, let's consider relative values.\n    # A common way to use sigmoid for optimization is to define a function that is maximized at the optimum.\n    # If `f(x)` is our objective (e.g., remaining capacity), we want to minimize it.\n    # We can transform this into a score using sigmoid: `score = sigmoid(-k * f(x))`.\n\n    # Let's try to map `potential_remaining_caps` to the sigmoid.\n    # We want higher scores for lower `potential_remaining_caps`.\n    # Use `potential_remaining_caps` as the input `x` to a sigmoid function that *decreases* with `x`.\n    # `f(x) = 1 / (1 + exp(k * x))` is a decreasing sigmoid.\n    # The steepness `k` can be chosen. A larger `k` makes the preference for smaller remaining capacities more pronounced.\n\n    k = 5.0  # Steepness parameter. Higher k means stronger preference for smaller remaining capacities.\n\n    # Apply the decreasing sigmoid function to the potential remaining capacities of valid bins.\n    # For invalid bins, the priority is 0.\n    priorities[valid_bins_mask] = 1.0 / (1.0 + np.exp(k * potential_remaining_caps[valid_bins_mask]))\n\n    # The priority score will be in the range (0, 1) for valid bins, and 0 for invalid bins.\n    # We want the bin with the highest priority score to be chosen.\n\n    return priorities",
    "response_id": 16,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response17.txt_stdout.txt",
    "code_path": "problem_iter1_code17.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Random Fit strategy for online Bin Packing Problem.\n\n    This priority function assigns a priority to each bin based on the following logic:\n    1. If a bin has enough remaining capacity to hold the item, it's a candidate.\n    2. Among the candidate bins, priority is assigned randomly. This introduces\n       an element of exploration, as it doesn't deterministically pick the\n       'best' fitting bin every time. This can sometimes lead to better\n       overall packing by not always filling up certain bins too quickly.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n        Bins that can fit the item will have a randomly assigned priority.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins that can fit the item, assign a random priority\n    # We use a positive random number to ensure these are prioritized over bins that don't fit.\n    # A simple uniform distribution between 0 and 1 is sufficient for random selection.\n    priorities[can_fit_mask] = np.random.rand(np.sum(can_fit_mask))\n    \n    return priorities",
    "response_id": 17,
    "obj": 72.55684084563224,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response18.txt_stdout.txt",
    "code_path": "problem_iter1_code18.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using an Epsilon-Greedy strategy.\n\n    The strategy aims to balance exploration (trying less optimal but potentially good bins)\n    and exploitation (choosing the best available bin).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploration\n    num_bins = len(bins_remain_cap)\n\n    # Calculate the \"greediness\" of each bin: how well it fits the item.\n    # We want bins that are a good fit but not so tight that they become unusable\n    # for future items. A good fit leaves a small remainder.\n    # If an item doesn't fit, its \"greediness\" is set to a very low value (e.g., -inf)\n    # to effectively disqualify it.\n\n    # Calculate potential remainders if the item fits\n    potential_remainders = bins_remain_cap - item\n\n    # Identify bins where the item can fit\n    fitting_bins_mask = potential_remainders >= 0\n\n    # For bins where the item fits, calculate a score. A good score is one\n    # where the remainder is small but positive. We can invert the remainder\n    # so that smaller remainders get higher scores. Adding a small constant\n    # ensures that even a perfect fit (zero remainder) gets a positive score.\n    # We use `np.inf` for bins that don't fit to ensure they are never selected by the greedy part.\n    greediness_scores = np.full_like(bins_remain_cap, -np.inf)\n    greediness_scores[fitting_bins_mask] = 1.0 / (potential_remainders[fitting_bins_mask] + 1e-9) # Adding epsilon for stability, small remainder is good\n\n    # Normalize greediness scores to be between 0 and 1 for easier combination with epsilon\n    max_greediness = np.max(greediness_scores[fitting_bins_mask] if np.any(fitting_bins_mask) else [0])\n    if max_greediness > 0:\n        normalized_greediness = greediness_scores / max_greediness\n    else:\n        normalized_greediness = np.zeros_like(greediness_scores) # All bins might be too small\n\n    # Introduce exploration: with probability epsilon, pick a random bin among those that fit.\n    # With probability (1 - epsilon), pick the bin with the highest greediness score.\n\n    # Generate random scores for exploration\n    exploration_scores = np.random.rand(num_bins)\n\n    # Combine greediness and exploration\n    priorities = np.where(np.random.rand(num_bins) < epsilon,\n                          exploration_scores,\n                          normalized_greediness)\n\n    # Ensure that bins where the item doesn't fit have zero or negative priority\n    # (though the -np.inf in greediness_scores already handles this for the greedy part)\n    # The exploration part might pick a non-fitting bin if we're not careful.\n    # So, we explicitly set priorities to a very low value for non-fitting bins\n    # even during exploration to avoid invalid moves.\n    priorities[~fitting_bins_mask] = -np.inf\n\n    return priorities",
    "response_id": 18,
    "obj": 35.79976067012365,
    "SLOC": 18.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    This strategy aims to balance two aspects:\n    1. Prefer bins that have just enough remaining capacity to fit the item (Best Fit like behavior).\n    2. Prefer bins that have more remaining capacity overall (First Fit Decreasing like behavior, but applied to priorities).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Consider only bins that can actually fit the item\n    fitting_bins_mask = bins_remain_cap >= item\n    \n    if not np.any(fitting_bins_mask):\n        # If no bin can fit the item, return zeros (or handle as an error/special case)\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate 'fit' scores: how well the item fits into the bin.\n    # Smaller difference (remaining_cap - item) is better.\n    # We want higher scores for smaller differences.\n    # Add a small epsilon to avoid division by zero if remaining_cap == item\n    fit_scores = 1.0 / (bins_remain_cap[fitting_bins_mask] - item + 1e-6)\n\n    # Calculate 'capacity' scores: a general preference for bins with more space.\n    # This encourages using less full bins first if the fit is similar.\n    # Add a small epsilon to avoid division by zero.\n    capacity_scores = bins_remain_cap[fitting_bins_mask] + 1e-6\n\n    # Combine scores. Here, we give a weighted sum.\n    # The 'fit' score is often more critical for BPP efficiency.\n    # A common approach is to use exponential of combined scores for Softmax.\n    # Let's create a composite score: slightly favor better fit, but also capacity.\n    # We'll scale these to avoid issues with magnitudes before exponentiation.\n    \n    # Normalize fit_scores to a [0, 1] range if necessary, but raw values might be fine too.\n    # For simplicity, let's try a direct combination.\n    \n    # Create a composite score that emphasizes good fit, but also remaining capacity\n    # A simple linear combination: 0.7 * normalized_fit + 0.3 * normalized_capacity\n    # For Softmax, direct combination before exp is okay if scales are not wildly different.\n    \n    # Let's try a combination that prioritizes tighter fits more strongly.\n    # A higher fit score (smaller residual capacity) is good.\n    # A higher capacity score (more total remaining capacity) is also good.\n    \n    # A strategy could be to use log of capacity and inverse of residual capacity\n    # or simply the inverse of residual capacity and the capacity itself.\n    \n    # Let's try to amplify the difference between good fits.\n    # Residual capacity: bins_remain_cap[fitting_bins_mask] - item\n    # We want smaller residuals to be higher priority.\n    residual_capacities = bins_remain_cap[fitting_bins_mask] - item\n    \n    # We can invert this residual capacity to get a \"tightness\" score.\n    # Smallest residual should get highest score.\n    tightness_scores = -residual_capacities # Negative to use max later\n\n    # Combine tightness and total capacity.\n    # For Softmax, we want values that reflect preference.\n    # A positive score is generally preferred.\n    # A bin with item fits perfectly (residual = 0) should have a high score.\n    # A bin with lots of space and a decent fit should also have a good score.\n    \n    # Let's try to reward bins that have *just enough* capacity,\n    # and then reward bins with more overall capacity among those.\n    \n    # For Softmax, we exponentiate scores. We want to produce scores that,\n    # when exponentiated, result in probabilities reflecting our preference.\n    \n    # A common way is: exp(score_for_bin_i).\n    # The 'score_for_bin_i' should be higher for preferred bins.\n    \n    # Let's define a score based on inverse residual capacity and remaining capacity.\n    # We want smaller residual_capacity to be better.\n    # We want larger remaining_capacity to be better.\n    \n    # A possible combination for the exponent:\n    # score = alpha * (1 / (residual_capacity + epsilon)) + beta * remaining_capacity\n    # where alpha and beta are weights.\n    \n    alpha = 1.5 # Weight for 'good fit' (tightness)\n    beta = 0.5  # Weight for overall remaining capacity\n    epsilon = 1e-3 # Small value to avoid division by zero\n\n    # Calculate scores for bins that can fit the item\n    scores_for_fitting_bins = alpha * (1.0 / (residual_capacities + epsilon)) + beta * bins_remain_cap[fitting_bins_mask]\n    \n    # Initialize priorities for all bins to a very low value (or negative infinity effectively)\n    # so that only fitting bins get positive consideration after softmax.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    # Apply the scores to the fitting bins\n    priorities[fitting_bins_mask] = scores_for_fitting_bins\n    \n    # Softmax function: exp(score) / sum(exp(scores))\n    # We want to return scores that are proportional to probabilities,\n    # so we can return the exponentiated scores directly.\n    # The softmax itself would normalize them, but for priority selection,\n    # proportional scores are often what's needed.\n    \n    # Using exp directly creates higher-valued outputs for better bins,\n    # which aligns with selecting the bin with the highest priority.\n    # If we were to pass these to a softmax distribution, they would work.\n    # However, if the goal is to return scores that a highest-value selector\n    # can use, we should just return the exponentiated values.\n    \n    # Let's return the exponentiated scores.\n    # We need to handle the -np.inf cases carefully if exponentiating.\n    # exp(-inf) is 0. So, bins that don't fit will have scores of 0.\n    \n    exp_priorities = np.exp(priorities)\n    \n    # To ensure that a zero probability isn't interpreted as zero preference,\n    # and that we return scores reflecting preference for fitting bins,\n    # we might scale or simply return the scores before exponentiation,\n    # or handle the -inf to 0 conversion explicitly.\n\n    # If we return exp_priorities, the non-fitting bins will have a score of 0,\n    # and fitting bins will have positive scores. This is reasonable.\n\n    # However, for \"priority\", directly using the calculated scores before exp\n    # might be more interpretable for a \"highest priority wins\" selection.\n    # The problem statement asks for \"priority score\", which implies a value\n    # used for comparison. The Softmax-Based Fit strategy usually refers to\n    # the final normalized probabilities, but here we need a score.\n\n    # Let's consider the goal: \"The bin with the highest priority score will be selected\".\n    # This means we don't need the normalized probabilities, just values proportional\n    # to them. exp(score) is proportional. So exp_priorities is good.\n\n    # An alternative for interpretation: return the raw scores before exponentiation.\n    # The selection will pick the max.\n    \n    # Let's re-evaluate the score calculation. We want bins that fit tightly to have\n    # higher values. Residual capacity `r = remaining_cap - item`. `r=0` is best.\n    # `1/r` or `-r` can work. Using `1/(r + epsilon)` makes smaller `r` have larger score.\n    # Then add a term for overall capacity `remaining_cap`.\n    \n    # The current `scores_for_fitting_bins` are what we'd pass to `exp()`.\n    # Let's return these as the \"priority scores\". The selection process\n    # would then pick the maximum. If we want to be strictly \"Softmax-based\",\n    # we'd apply `exp` and then potentially normalize. But for a direct selector,\n    # the raw scores that go into `exp` are often used.\n\n    # Let's return `scores_for_fitting_bins` for the fitting bins, and a very low value\n    # for non-fitting bins to ensure they are never picked if a fitting bin exists.\n    \n    priorities_final = np.full_like(bins_remain_cap, -np.inf)\n    priorities_final[fitting_bins_mask] = scores_for_fitting_bins\n\n    return priorities_final",
    "response_id": 19,
    "obj": 86.58755484643,
    "SLOC": 18.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response20.txt_stdout.txt",
    "code_path": "problem_iter1_code20.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit Decreasing-like logic for online BPP.\n\n    This heuristic aims to place the item into the bin where it fits best,\n    prioritizing bins that will have minimal remaining capacity after placement.\n    This is inspired by the First Fit Decreasing (FFD) strategy but adapted for\n    the online scenario where we don't know all items in advance.\n\n    The priority is calculated as:\n    priority = 1 / (remaining_capacity - item_size + 1)\n    where the addition of 1 in the denominator prevents division by zero\n    and penalizes bins that are almost full.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate a more preferred bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can actually accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate priority only for bins that can fit the item\n    # The idea is to prioritize bins that leave the least remaining capacity\n    # after placing the item. A smaller remaining capacity is better for packing.\n    # By taking the inverse, we get higher scores for tighter fits.\n    # Adding 1 to the denominator prevents division by zero for perfectly fitting bins.\n    # A very small positive number epsilon could also be used instead of 1 for\n    # extremely tight fits, but 1 is simpler and generally effective.\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n    # We want to maximize the tightness of the fit, so we invert the remaining capacity.\n    # A larger value means a tighter fit (less remaining space).\n    # Add a small epsilon to the denominator to avoid division by zero\n    # and to ensure bins that perfectly fit get a very high, but finite, priority.\n    priorities[can_fit_mask] = 1.0 / (remaining_after_fit + 1e-9)\n\n    # Bins that cannot fit the item will have a priority of 0, meaning they are not considered.\n    return priorities",
    "response_id": 20,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response21.txt_stdout.txt",
    "code_path": "problem_iter1_code21.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit.\n\n    The Best Fit strategy aims to place the item in the bin that leaves the least\n    remaining capacity after the item is placed, effectively minimizing wasted space\n    in that specific bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher priority is given to bins where placing the item results in less\n        remaining capacity (closer to zero, but still non-negative).\n        Bins that cannot fit the item are given a priority of -1.\n    \"\"\"\n    # Calculate potential remaining capacity for each bin if the item fits\n    potential_remain_cap = bins_remain_cap - item\n\n    # Initialize priorities array with a low value (e.g., -1) indicating not a good fit or impossible to fit\n    priorities = np.full_like(bins_remain_cap, -1.0)\n\n    # Identify bins where the item can actually fit\n    can_fit_mask = potential_remain_cap >= 0\n\n    # For bins where the item fits, the priority is the negative of the remaining capacity.\n    # This means bins with smaller remaining capacity (after placing the item) get higher priority.\n    # We use the negative because we want to MAXIMIZE the priority score, and smallest remaining\n    # capacity is best fit. So, a smaller positive remaining capacity becomes a larger negative number.\n    priorities[can_fit_mask] = -potential_remain_cap[can_fit_mask]\n\n    return priorities",
    "response_id": 21,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response22.txt_stdout.txt",
    "code_path": "problem_iter1_code22.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    The Worst Fit strategy aims to place the current item into the bin that has the\n    largest remaining capacity. This leaves smaller remaining capacities in other\n    bins, potentially allowing them to be filled more efficiently by smaller items\n    later.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        The score is the remaining capacity of the bin, such that bins with\n        larger remaining capacity get higher priority.\n    \"\"\"\n    # Initialize priorities to zero.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # For each bin, calculate its priority.\n    # In Worst Fit, the priority is directly proportional to the remaining capacity.\n    # We only consider bins that can actually fit the item.\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            priorities[i] = bins_remain_cap[i]\n        else:\n            # If a bin cannot fit the item, give it a very low priority (effectively zero).\n            priorities[i] = -np.inf # Or a very small negative number to ensure it's not chosen.\n\n    return priorities",
    "response_id": 22,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Command '['python3', '-u', '/home/dokhanhnam1199/QD/problems/bpp_online/eval.py', '5000', '/home/dokhanhnam1199/QD', 'train']' timed out after 49.9999718679901 seconds"
  },
  {
    "stdout_filepath": "problem_iter1_response23.txt_stdout.txt",
    "code_path": "problem_iter1_code23.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit.\n\n    The Almost Full Fit strategy prioritizes bins that have a remaining capacity\n    slightly larger than the item. This aims to leave smaller gaps in bins that\n    are nearly full, potentially improving overall packing efficiency.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We want to prioritize bins where the remaining capacity is just slightly\n    # larger than the item. A good way to express this is by minimizing the\n    # difference (bins_remain_cap - item), but only for bins that can actually\n    # fit the item. For bins that cannot fit the item, we assign a very low priority.\n\n    # Initialize priorities to a very low value (negative infinity) to signify\n    # that these bins are not candidates if the item doesn't fit.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Find bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate a priority score.\n    # We want the remaining capacity to be as close as possible to 'item' but still\n    # greater than or equal to it. This is equivalent to minimizing (bins_remain_cap - item).\n    # However, for \"Almost Full Fit\", we want bins that are *almost* full, meaning\n    # the remaining capacity is large enough but not excessively large.\n    # A simple approach is to take the negative of the remaining capacity for\n    # bins that can fit the item. This way, bins with *less* remaining capacity\n    # (closer to 'item') will have a higher priority (less negative).\n\n    # To further refine for \"Almost Full Fit\", we can also consider how \"full\" the bin is.\n    # A bin that is *almost* full and can fit the item is desirable.\n    # We can achieve this by prioritizing bins with larger remaining capacities *among those that can fit the item*.\n    # Let's invert this. We want to select bins that are *almost* full.\n    # So, if a bin can fit the item, its priority is inversely related to its remaining capacity.\n    # A smaller remaining capacity (closer to 'item') is better for 'Almost Full Fit'.\n\n    # Let's use a scoring where a *smaller* positive difference (remaining capacity - item)\n    # is preferred. We'll use the negative of this difference as priority,\n    # so a larger negative difference (meaning smaller positive difference) is better.\n    # We add a small epsilon to avoid division by zero if we were to use reciprocals,\n    # or to ensure distinct values.\n\n    # For bins that can fit, calculate the difference. We want to penalize larger differences.\n    # A bin with remaining capacity `C` and item `I`: difference is `C - I`.\n    # We want small `C - I`. So, priority should be related to `-(C - I)`.\n    # This means higher priority for smaller `C - I`.\n\n    # Let's consider bins that can fit the item:\n    eligible_bins_capacity = bins_remain_cap[can_fit_mask]\n\n    # For these eligible bins, we want those with the smallest remaining capacity.\n    # So, we can sort them or assign priorities based on their remaining capacity.\n    # The smaller the `bins_remain_cap[i]`, the higher the priority for eligible bins.\n    # Assign priority as negative remaining capacity.\n    priorities[can_fit_mask] = -eligible_bins_capacity\n\n    # This implementation gives higher priority to bins that are \"almost full\"\n    # meaning they have the least remaining capacity among those that can fit the item.\n    # This is a common interpretation of \"Almost Full Fit\" where you try to fill\n    # bins as much as possible without overfilling, thus leaving less fragmented space.\n\n    return priorities",
    "response_id": 23,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response24.txt_stdout.txt",
    "code_path": "problem_iter1_code24.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    The Exact Fit First (EFF) strategy prioritizes bins that have just enough\n    remaining capacity to fit the current item. A perfect fit (remaining capacity == item size)\n    receives the highest priority. Among bins that are not perfect fits, we can\n    assign lower priorities, potentially based on how close their remaining capacity\n    is to the item size without exceeding it, or by favoring bins with less remaining\n    capacity to keep larger capacities open for potentially larger future items.\n\n    This implementation assigns a priority of 100 for an exact fit,\n    and a decreasing priority for bins that can fit the item but not perfectly,\n    proportional to how much \"slack\" they have (remaining capacity - item size).\n    Bins that cannot fit the item receive a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Find bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate priorities for bins that can fit the item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # Exact fit has the highest priority\n    exact_fit_mask = fitting_bins_remain_cap == item\n    priorities[can_fit_mask][exact_fit_mask] = 100\n\n    # For bins that are not an exact fit but can still fit the item,\n    # assign priority based on how \"tight\" the fit is.\n    # A smaller slack (remaining_capacity - item) gets a higher priority\n    # among the non-exact fits.\n    # We subtract the slack from a maximum possible priority (e.g., 99)\n    # to create a descending priority based on slack.\n    non_exact_fit_mask = fitting_bins_remain_cap != item\n    if np.any(non_exact_fit_mask):\n        non_fitting_bins_capacities = fitting_bins_remain_cap[non_exact_fit_mask]\n        slacks = non_fitting_bins_capacities - item\n        # Assign priorities inversely proportional to slack, from 99 down to 0.\n        # The smallest slack gets the highest priority (99), largest slack gets the lowest.\n        # To achieve this, we can map the slack values to a range.\n        # A simple way is to sort them and assign decreasing values.\n        # Or, more directly, use a formula: max_priority - proportional_slack_value.\n        # Let's use a simple inverse relation for demonstration.\n        # If there are N non-exact fitting bins, their priorities can be N-1, N-2, ..., 0.\n        # Or, let's map slack to a value between 0 and 99.\n        # Smallest slack (closest to item) gets priority closer to 99.\n        # Largest slack gets priority closer to 0.\n\n        # To assign priorities based on slack value, let's normalize the slacks\n        # and then create priorities.\n        # A more robust approach: assign a priority based on the rank of the slack.\n        sorted_indices = np.argsort(slacks)\n        for i, original_index_in_fitting in enumerate(np.where(can_fit_mask)[0][non_exact_fit_mask]):\n            # Assign priority from 99 down to 99 - (number of non-exact fits - 1)\n            priorities[original_index_in_fitting] = 99 - i\n\n\n    return priorities",
    "response_id": 24,
    "obj": 5.534503390506582,
    "SLOC": 14.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response25.txt_stdout.txt",
    "code_path": "problem_iter1_code25.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance strategy.\n\n    The Inverse Distance strategy prioritizes bins that are \"closer\" to fitting the item.\n    A higher priority is assigned to bins with less remaining capacity that can still\n    accommodate the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the \"distance\" to fitting the item.\n    # This is the remaining capacity minus the item size.\n    # A smaller positive distance means a better fit.\n    distances = bins_remain_cap - item\n\n    # Initialize priorities to a very low value (e.g., negative infinity)\n    # for bins that cannot fit the item.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # For bins that can fit the item (distances >= 0):\n    # Calculate inverse distance as priority.\n    # To avoid division by zero if a bin has exactly the remaining capacity,\n    # we add a small epsilon.\n    # The closer the remaining capacity is to the item size (smaller positive distance),\n    # the higher the priority.\n    fit_mask = distances >= 0\n    priorities[fit_mask] = 1.0 / (distances[fit_mask] + 1e-9)\n\n    return priorities",
    "response_id": 25,
    "obj": 4.198244914240141,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response26.txt_stdout.txt",
    "code_path": "problem_iter1_code26.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score strategy prioritizes bins based on how well an item fits.\n    It uses a sigmoid function to map the remaining capacity relative to the item size\n    to a priority score. Bins with remaining capacity slightly larger than the item\n    size (meaning a tight fit) receive higher priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Avoid division by zero or negative capacities\n    valid_bins_mask = bins_remain_cap >= item\n    \n    # Calculate the 'fit' for each valid bin. A smaller value indicates a tighter fit.\n    # The \"fit\" is defined as the remaining capacity minus the item size.\n    # We want to prioritize bins where (bins_remain_cap - item) is close to 0.\n    fit_values = bins_remain_cap[valid_bins_mask] - item\n    \n    # Use a sigmoid-like function to transform the fit values into priorities.\n    # The idea is to give higher priority to bins where the remaining capacity\n    # is just enough for the item.\n    # We can center the sigmoid around 0, so a fit_value of 0 (perfect fit) gets a high score.\n    # The scaling factor (e.g., 1.0 or adjusted) controls the steepness of the sigmoid.\n    # A positive steepness makes smaller fit values (tighter fits) get higher scores.\n    \n    # Normalize fit values to be within a reasonable range for the sigmoid,\n    # or simply use the fit values directly if the sigmoid can handle it.\n    # Here, we'll use the fit values directly and ensure the sigmoid maps\n    # smaller (better) fits to higher scores.\n    \n    # Using a shifted and scaled sigmoid:\n    # The goal is to have a higher score when (bin_capacity - item) is small.\n    # If bin_capacity - item = 0 (perfect fit), we want a high score.\n    # If bin_capacity - item > 0 (loose fit), we want a lower score.\n    # If bin_capacity - item < 0 (no fit), we want a very low score (handled by valid_bins_mask).\n    \n    # Let's define a transformation `t(fit) = -k * fit`, where k > 0.\n    # Then apply sigmoid: 1 / (1 + exp(-t(fit))) = 1 / (1 + exp(k * fit))\n    # This function gives 0.5 at fit=0 and decreases as fit increases.\n    # This is the opposite of what we want (higher priority for smaller fits).\n    \n    # Alternative transformation: `t(fit) = k * (item - bin_capacity)` which is `-k * fit`.\n    # Let's re-think. We want high priority when `bin_capacity` is just above `item`.\n    # This means `bin_capacity - item` should be small and positive.\n    \n    # Consider a function `f(x) = 1 / (1 + exp(-slope * (x - center)))`.\n    # If x is the remaining capacity, and we want high priority when x is close to item:\n    # Let's transform x into a measure of \"how good is the fit\".\n    # Good fit = small `bins_remain_cap - item`\n    \n    # Let's try mapping `(bins_remain_cap - item)` to a score where small positive values are high.\n    # Using the sigmoid function's property that `1/(1+e^-x)` increases with x.\n    # We need to transform `bins_remain_cap - item` such that smaller values lead to larger sigmoid inputs.\n    # Let the input to sigmoid be `- (bins_remain_cap - item) * steepness`.\n    # The steepness controls how quickly the priority drops as the fit gets looser.\n    \n    steepness = 2.0  # Controls how sharp the transition is. Higher means sharper.\n    \n    # Calculate scores for valid bins.\n    # `-(fit_values)` means tighter fits get larger (more positive) inputs to sigmoid.\n    scores = 1.0 / (1.0 + np.exp(steepness * fit_values))\n    \n    # Initialize a full-sized array for priorities.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Place the calculated scores into the correct positions.\n    priorities[valid_bins_mask] = scores\n    \n    return priorities",
    "response_id": 26,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response27.txt_stdout.txt",
    "code_path": "problem_iter1_code27.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Random Fit.\n\n    The Random Fit strategy does not strictly follow a deterministic priority.\n    Instead, it randomly selects a feasible bin. However, to fit this into the\n    priority function framework, we can assign a higher priority to bins that\n    can accommodate the item, and among those, a random selection is implicitly\n    handled by the uniform probability distribution. This implementation assigns\n    a priority of 1 to all bins that can fit the item, and 0 to those that cannot.\n    The subsequent selection mechanism would then randomly pick from the bins\n    with the highest priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    # Assign a priority of 1 to bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = 1.0\n    return priorities",
    "response_id": 27,
    "obj": 4.487435181491823,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response28.txt_stdout.txt",
    "code_path": "problem_iter1_code28.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Exploration probability\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Greedy choice: Prioritize bins that can fit the item and have least remaining capacity (Best Fit)\n    # Calculate \"greediness\" for each bin\n    greedy_scores = np.zeros(num_bins)\n    # A bin can fit if its remaining capacity is greater than or equal to the item size\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the difference between bin capacity and item size. Smaller difference is better (less wasted space).\n    # Add a small epsilon to avoid division by zero if item is exactly the remaining capacity\n    difference = bins_remain_cap - item + 1e-9\n    \n    # Calculate a \"best fit\" score: smaller difference means higher score.\n    # We use the inverse of the difference. We also want to consider only bins that can fit.\n    # Initialize with a very low score for bins that cannot fit.\n    greedy_scores[can_fit_mask] = 1.0 / difference[can_fit_mask]\n    greedy_scores[~can_fit_mask] = -np.inf  # Ensure bins that don't fit get a very low priority\n\n    # Epsilon-Greedy strategy\n    # With probability epsilon, explore (choose a random bin that can fit)\n    # With probability 1-epsilon, exploit (choose the bin with the highest greedy score)\n    \n    indices_that_can_fit = np.where(can_fit_mask)[0]\n    \n    if len(indices_that_can_fit) > 0:\n        if np.random.rand() < epsilon:\n            # Explore: choose a random bin that can fit\n            random_bin_index = np.random.choice(indices_that_can_fit)\n            priorities[random_bin_index] = 1.0\n        else:\n            # Exploit: choose the bin with the highest greedy score\n            best_bin_index = np.argmax(greedy_scores)\n            priorities[best_bin_index] = 1.0\n    else:\n        # If no bin can fit the item, this strategy won't place it.\n        # In a real online scenario, a new bin would be opened.\n        # For this priority function, we return all zeros, indicating no suitable bin.\n        pass \n\n    return priorities",
    "response_id": 28,
    "obj": 4.11846828879138,
    "SLOC": 20.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response29.txt_stdout.txt",
    "code_path": "problem_iter1_code29.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.\n\n    This heuristic prioritizes bins that have a remaining capacity close to the item size,\n    but also considers bins with slightly larger remaining capacity to allow for better\n    future packing. The softmax function is used to convert these 'fitness' scores\n    into probabilities (priorities).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate a \"fitness\" score for each bin.\n    # We want bins where remaining capacity is slightly larger than the item.\n    # A good score would be when remaining_capacity - item is small and non-negative.\n    # We can use a function that penalizes larger gaps, like an inverse or negative exponential.\n    # Here, we'll use a term that is high when (remaining_capacity - item) is small and positive.\n    # We'll also consider bins that can fit the item, so we set a very low score for bins\n    # that cannot fit the item.\n\n    # Maximum difference we are willing to tolerate. This can be tuned.\n    # A larger max_diff allows for more flexibility.\n    max_diff = np.max(bins_remain_cap) # A simple heuristic for max_diff\n\n    # Calculate the \"gap\" between remaining capacity and item size for bins that can fit the item\n    gaps = bins_remain_cap - item\n\n    # Initialize fitness scores. Bins that cannot fit the item will have a very low score.\n    fitness_scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # For bins that can fit the item (gaps >= 0)\n    can_fit_mask = (gaps >= 0)\n    valid_gaps = gaps[can_fit_mask]\n\n    if np.any(can_fit_mask):\n        # We want to prioritize bins with smaller gaps (closer fit).\n        # A common approach is to use an inverse function or a Gaussian-like function.\n        # Let's use a score that increases as the gap decreases (approaching zero).\n        # We can normalize the gaps to prevent extreme values and map them to a reasonable range.\n        # A simple approach: -valid_gaps / max_diff. This gives values close to 0 for small gaps\n        # and values close to -1 for large gaps. We want higher scores for smaller gaps.\n        # Let's try a linear transformation to make small gaps have higher positive values.\n        # Higher values mean higher priority.\n        # Score = (max_gap - gap) / max_gap, for gap >= 0.\n        # A simple way to map to positive values and use softmax is to transform the gap.\n        # Consider the \"waste\": remaining_capacity - item. We want to minimize waste.\n        # Let's use a score proportional to the inverse of the waste, or a negative exponential\n        # of the waste to create a smooth decay.\n\n        # Method 1: Using inverse of gap + 1 to avoid division by zero and ensure positive values.\n        # This will heavily favor bins that fit perfectly or near-perfectly.\n        # adjusted_scores = 1.0 / (valid_gaps + 1e-6)\n        # However, this can be sensitive to small gaps.\n\n        # Method 2: Using a soft-ranking based on the difference, aiming for a \"best fit\" preference.\n        # We can penalize larger differences more. For instance, we could use exp(-k * gap).\n        # A higher score means better fit.\n        # Let's use exp(-gap / temperature). Higher temperature makes it flatter (more uniform).\n        # A smaller temperature makes it sharper (more preference for best fit).\n        # Let's use a moderate temperature for diversity.\n        temperature = np.mean(bins_remain_cap) if np.mean(bins_remain_cap) > 0 else 1.0 # Dynamic temperature\n\n        # Ensure gaps are not excessively large compared to temperature,\n        # otherwise exp(-large_gap/temp) will underflow to 0.\n        # We can clip the gaps or use a scaled version.\n        # Scale the gaps relative to the temperature to control the softmax spread.\n        scaled_gaps = valid_gaps / temperature\n\n        # Calculate fitness scores using negative exponential. Higher values mean better fit.\n        # We want to give higher scores to smaller gaps.\n        # Let's invert the logic to have positive fitness for smaller gaps.\n        # For example, consider `max_useful_gap - gap` and then take exponential,\n        # or more directly `exp(-(gap / normalization_factor))`.\n        # A simple positive score that decays with gap size:\n        positive_scores = np.exp(-scaled_gaps)\n        fitness_scores[can_fit_mask] = positive_scores\n\n    # Apply Softmax to convert fitness scores into probabilities (priorities)\n    # Avoid issues with all scores being -inf or too close, which can lead to NaN in softmax.\n    # If all bins cannot fit, return uniform low priorities.\n    if not np.any(can_fit_mask):\n        return np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n\n    # Ensure that fitness_scores has at least one finite value before applying softmax\n    finite_scores = fitness_scores[np.isfinite(fitness_scores)]\n    if len(finite_scores) == 0:\n         return np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n\n    # Shift scores to avoid potential numerical instability with large negative values\n    # before exponentiation in softmax. The difference from the max score is constant.\n    max_score = np.max(fitness_scores[np.isfinite(fitness_scores)])\n    shifted_scores = fitness_scores - max_score\n\n    # Compute softmax\n    exp_scores = np.exp(shifted_scores)\n\n    # Handle potential infinities that might arise if shifted_scores still contains large positive numbers\n    # (e.g., if max_score was incorrectly calculated due to -inf).\n    # More robust approach for softmax:\n    exp_scores = np.where(np.isfinite(shifted_scores), np.exp(shifted_scores), 0)\n    sum_exp_scores = np.sum(exp_scores)\n\n    if sum_exp_scores == 0:\n        # Fallback: if all exponentiated scores are zero or non-finite, assign uniform priority\n        return np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n    else:\n        priorities = exp_scores / sum_exp_scores\n        return priorities",
    "response_id": 29,
    "obj": 4.048663741523748,
    "SLOC": 26.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  }
]