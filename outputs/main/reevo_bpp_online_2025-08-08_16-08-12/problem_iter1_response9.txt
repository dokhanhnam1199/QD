```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.

    This strategy assigns higher priority to bins that have a remaining capacity
    closest to the item's size. A temperature parameter controls the "softness"
    of the softmax. A higher temperature makes the probabilities more uniform,
    while a lower temperature sharpens the preference for bins that are a better fit.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Ensure we only consider bins that can actually fit the item
    valid_bin_indices = bins_remain_cap >= item
    
    # If no bin can fit the item, return zeros (or handle as an error/new bin creation)
    if not np.any(valid_bin_indices):
        return np.zeros_like(bins_remain_cap)

    # Calculate the "fit" for valid bins: the closer the remaining capacity
    # is to the item size, the better the fit. We can represent this as
    # a score where a smaller difference is better. We want to maximize
    # priority, so we invert the difference. A common approach is to use
    # the negative difference, or some function of the inverse difference.
    # Here we use (remaining_capacity - item_size) as the "slack".
    # Smaller slack is better.
    slack = bins_remain_cap[valid_bin_indices] - item
    
    # We want to assign higher priorities to bins with less slack.
    # To use softmax, we need values that we want to be high to be large.
    # So, we can use the negative slack.
    # Also, we might want to prevent extreme values from dominating.
    # A simple way to make it more "soft" is to add a small epsilon to avoid
    # log(0) if we were using inverse slack.
    # Or we can directly exponentiate a score that represents "goodness".
    
    # Let's define a "goodness" score. A good fit means slack is small.
    # We can use -slack, so smaller slack becomes larger negative slack.
    # To get positive scores for softmax, we can use -slack and then shift
    # or simply use the raw slack as is and let softmax handle it.
    # A common practice for Softmax-based selection is to have a score `s`
    # where higher `s` is preferred.
    # A good bin has `remaining_capacity - item` small.
    # So, `item - remaining_capacity` is a good score (more negative is worse).
    # Or, `remaining_capacity - item` (slack). Smaller slack is better.
    # To use softmax, we want the best bins to have high values.
    # Let's use a measure that is inversely related to slack, perhaps `1 / (slack + epsilon)`.
    # However, this can lead to very large values if slack is near zero.
    
    # A more stable approach for Softmax-based selection often involves
    # transforming the scores so that better choices yield higher values.
    # If we consider `remaining_capacity - item` (slack), smaller is better.
    # To make it suitable for softmax where higher values are preferred,
    # we can transform it.
    # For example, let the "preference score" be `-slack`.
    # `preference_scores = -slack`
    
    # Another approach, often seen in contexts like attention mechanisms,
    # is to use a "similarity" or "energy" score.
    # We want bins that are "close" to fitting the item.
    # Let's define closeness as the absolute difference.
    # `closeness = np.abs(bins_remain_cap[valid_bin_indices] - item)`
    # Smaller closeness is better.
    # For softmax, we want higher values for better choices.
    # So, we can use `-closeness`.
    # `preference_scores = -np.abs(bins_remain_cap[valid_bin_indices] - item)`

    # Let's go with a strategy that prioritizes bins that leave minimal slack.
    # Slack = remaining_capacity - item_size.
    # A smaller slack is better.
    # To make it suitable for softmax (higher value = higher priority),
    # we can use `-slack`.
    # `preference_scores = -(bins_remain_cap[valid_bin_indices] - item)`

    # To avoid issues with very small or negative scores, we can use a
    # temperature parameter to smooth the distribution.
    # A common form is `exp(score / temperature)`.
    # A lower temperature emphasizes differences more.
    
    # Let's try a score that is proportional to how "full" the bin becomes.
    # A bin that becomes almost full (item + small slack) is good.
    # So, `item` is good if `bins_remain_cap` is `item + epsilon`.
    # Consider the inverse of slack: `1.0 / (slack + 1e-6)` to avoid division by zero.
    # Then apply softmax.
    
    # Let's define a score `s` where higher `s` is better.
    # If `r` is remaining capacity, and `i` is item size:
    # We want `r` to be close to `i`.
    # So, `r-i` (slack) should be small and non-negative.
    # Consider the score as `-slack` which means `i-r`. Small negative is better.
    # Let's use `i - r` which becomes more positive as slack decreases.
    # `scores = item - bins_remain_cap[valid_bin_indices]`
    
    # To make it more robust, especially if we have bins that are much larger,
    # which might have a slightly larger slack but are still "valid",
    # we can consider the ratio of item to remaining capacity if the remaining
    # capacity is close to item size.

    # A simple and effective approach for "best fit" is to consider the remaining capacity
    # after placing the item. We want this remaining capacity to be as small as possible,
    # but non-negative.
    # So, `residual_capacity = bins_remain_cap[valid_bin_indices] - item`
    # We want to maximize `-(residual_capacity)`.
    
    # Let's refine this for softmax. Higher values for better bins.
    # If a bin has `remaining_capacity = r` and item is `i`:
    # Preferred state is `r` is slightly larger than `i`.
    # Let's define a "preference" `p` such that `p` is high when `r-i` is small and non-negative.
    # One way is `p = - (r - i) = i - r`. This is maximized when `r-i` is minimized.
    # Let's add a constant to ensure positivity before exponentiation if needed,
    # or rely on the softmax properties.
    
    # Let's consider the "tightness" of the fit.
    # Tightness is maximized when `bins_remain_cap[idx]` is exactly `item`.
    # So, a score proportional to `1 / (slack + epsilon)` or `-slack`.
    # Let's use `-slack` which is `item - bins_remain_cap[valid_bin_indices]`.
    
    preference_scores = item - bins_remain_cap[valid_bin_indices]
    
    # Apply a temperature to control the softness of the softmax.
    # A temperature T > 0. Lower T -> harder selection, higher T -> softer selection.
    # We want to avoid extremely large or small values before exponentiation.
    # Let's cap the scores to prevent overflow or underflow issues if `item` is very large
    # or `bins_remain_cap` is very small (though we filtered for `bins_remain_cap >= item`).
    # If `item - bins_remain_cap` is very negative (large slack), it should have low priority.
    
    # To ensure non-negativity for the exponents, and potentially create a clearer preference:
    # We can shift the scores by adding the maximum score. This doesn't change
    # the softmax output but makes intermediate values positive.
    # shifted_scores = preference_scores - np.min(preference_scores)
    
    # Let's directly use the preference scores, assuming `softmax` can handle
    # potentially negative inputs by its nature of exponentiation and normalization.
    
    # Define a temperature parameter. A value like 1.0 is a good starting point.
    # Lower values (e.g., 0.1) will make the selection more "greedy", focusing
    # on the single best bin. Higher values (e.g., 5.0) will distribute probability
    # more evenly across good fitting bins.
    temperature = 1.0

    # Calculate exponentiated scores
    # We want bins with the smallest slack (largest `item - slack`) to have high scores.
    # The formula for softmax is exp(score_i) / sum(exp(score_j)).
    # If we use `item - slack`, then small slack leads to larger scores.
    
    # Ensure we don't have issues with extremely large positive or negative scores.
    # For `item - bins_remain_cap`, if `bins_remain_cap` is very small and `item` is large,
    # this can be large positive. If `bins_remain_cap` is much larger than `item`,
    # this can be large negative.
    
    # Let's consider the "fit" as the negative difference: `-(bins_remain_cap - item) = item - bins_remain_cap`.
    # A bin where `bins_remain_cap` is just enough for `item` gives a score of `0`.
    # A bin with more capacity gives a more negative score.
    # A bin with just enough capacity (slack 0) is ideal.
    # If `bins_remain_cap` can be very large, `item - bins_remain_cap` can be very negative.
    
    # Let's try to map the remaining capacity to a desirability score.
    # We want `bins_remain_cap` to be close to `item`.
    # Let's map the values such that `item` maps to a high score, and values
    # far from `item` map to lower scores.
    
    # For the "best fit" strategy, we are looking for the bin where `remaining_capacity`
    # is closest to `item` *and* `remaining_capacity >= item`.
    # This means we want to minimize `remaining_capacity - item`.
    
    # Score for softmax: higher is better.
    # So, we want to maximize `-(bins_remain_cap[valid_bin_indices] - item)`.
    # Let `fittness = -(bins_remain_cap[valid_bin_indices] - item)`
    # `fittness = item - bins_remain_cap[valid_bin_indices]`

    # Calculate exponential of scaled scores.
    # Softmax expects scores that can be exponentiated.
    # A stable way is `exp(scores / temperature)`.
    # Consider `scores = item - bins_remain_cap[valid_bin_indices]`.
    # If `bins_remain_cap` is very large, `scores` become very negative, exp goes to 0.
    # If `bins_remain_cap` is slightly larger than `item`, `scores` are slightly negative.
    # If `bins_remain_cap == item`, `scores` are 0, exp is 1.
    # This seems reasonable: bins that perfectly fit are ideal. Bins with more space are less ideal.

    # Ensure scores are not excessively large or small before exponentiation.
    # Clip scores to avoid numerical instability.
    # A reasonable range could be [-10, 10] for example, after scaling.
    # If we use `item - bins_remain_cap`, and `item` can be large and `bins_remain_cap` small (but valid),
    # the score can be large. Let's consider the *relative* difference.
    
    # A better heuristic for "fit" could be related to how "full" the bin becomes.
    # We want the bin to be as full as possible without overflowing.
    # So, if `r` is remaining capacity, we want `r-item` to be minimal.
    # Let `score = -(r-item) = item - r`.
    # We are selecting from bins where `r >= item`.
    # The maximum value of `item - r` is 0 (when `r = item`).
    # The minimum value can be large negative if `r` is much larger than `item`.
    
    # Example: item=5, bins_remain_cap=[7, 5, 10]
    # Valid bins: [7, 5, 10]
    # Scores (item - r): 5-7=-2, 5-5=0, 5-10=-5
    # Softmax inputs: exp(-2/T), exp(0/T), exp(-5/T)
    # If T=1: exp(-2)=0.135, exp(0)=1, exp(-5)=0.0067
    # Probabilities: 0.135 / (0.135+1+0.0067) = 0.116, 1 / (0.135+1+0.0067) = 0.855, 0.0067 / (0.135+1+0.0067) = 0.0058
    # This prioritizes the bin with remaining capacity exactly matching the item.

    # The scores can be directly used in softmax, but for numerical stability,
    # it's often better to center them around 0.
    # `centered_scores = preference_scores - np.mean(preference_scores)`
    # Or `centered_scores = preference_scores - np.max(preference_scores)` which makes max score 0.
    
    # Let's use `max(0, item - bins_remain_cap[valid_bin_indices])` as the score,
    # so only bins that *could* fit are considered positively, and better fits are higher.
    # `score = np.maximum(0, item - bins_remain_cap[valid_bin_indices])`
    # This maps bins where `r > item` to 0, and bins where `r = item` to 0.
    # This doesn't differentiate well between bins that are *almost* full.

    # Let's stick with `item - bins_remain_cap[valid_bin_indices]` and handle potential large negative values by letting softmax turn them to near-zero probabilities.
    
    # Calculate exponentiated preference scores.
    # We want higher scores for bins that leave less slack.
    # `score = item - remaining_capacity`. Higher score is better.
    # Maximize `score`.
    # `score_values = item - bins_remain_cap[valid_bin_indices]`

    # To prevent overflow with large positive scores (if item >> remaining_capacity, although this is filtered),
    # or underflow with very large negative scores (if remaining_capacity >> item),
    # we can scale or clip.
    # Let's normalize the scores first to be in a more manageable range, e.g., [0, 1].
    # `normalized_scores = (score_values - np.min(score_values)) / (np.max(score_values) - np.min(score_values) + 1e-8)`
    # This maps the current range of scores to [0, 1]. The best fit (min slack) gets 1, worst fit gets 0.
    
    # A more common approach with softmax is direct use of scores with temperature.
    # `exponentials = np.exp(score_values / temperature)`
    
    # Using the definition of "best fit": minimize `remaining_capacity - item`.
    # We want bins that result in smaller positive residual capacities.
    # `residuals = bins_remain_cap[valid_bin_indices] - item`
    # We want to minimize `residuals`.
    # For softmax, we want to maximize a function of `residuals`.
    # Let `priority_score = -residuals = item - bins_remain_cap[valid_bin_indices]`
    # Higher `priority_score` means better fit.

    scores = item - bins_remain_cap[valid_bin_indices]

    # If there's only one valid bin, its priority should be 1.0
    if len(scores) == 1:
        priorities = np.zeros_like(bins_remain_cap)
        priorities[valid_bin_indices] = 1.0
        return priorities

    # Softmax calculation
    # `exps = np.exp(scores / temperature)`
    # Sum of exponentials
    # `sum_exps = np.sum(exps)`
    # Calculate probabilities
    # `probabilities = exps / sum_exps`
    
    # For numerical stability, it's common to subtract the maximum score from all scores
    # before exponentiation: exp(score_i - max(score_j)). This makes the maximum score 0.
    max_score = np.max(scores)
    stable_scores = scores - max_score
    exps = np.exp(stable_scores / temperature)
    sum_exps = np.sum(exps)
    
    # Handle case where sum_exps might be zero (e.g., all inputs to exp were -inf, which shouldn't happen here if temperature > 0 and scores are finite)
    if sum_exps == 0:
        # This could happen if all stable_scores were extremely negative.
        # In such a case, perhaps assign uniform probability to valid bins.
        probabilities = np.ones_like(scores) / len(scores)
    else:
        probabilities = exps / sum_exps

    # Fill the final priorities array
    priorities = np.zeros_like(bins_remain_cap)
    priorities[valid_bin_indices] = probabilities

    return priorities
```
