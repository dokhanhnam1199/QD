```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority with which we want to add item to each bin,
    prioritizing exact fits and then using softmax for exploration.

    This heuristic prioritizes bins that can fit the item. Among those that fit,
    it assigns a higher priority to bins that will have less remaining capacity
    after the item is placed (exact fits). To encourage exploration of different
    packing strategies and avoid getting stuck in local optima too early,
    a softmax function is applied to the priorities, allowing for a
    probabilistic selection that can sometimes choose bins with slightly more
    remaining capacity.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    temperature = 0.5  # Controls the randomness of the softmax distribution

    # Identify bins that can accommodate the item
    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        return priorities  # No bin can fit the item

    # Calculate a score that prioritizes tighter fits.
    # A larger negative value means a tighter fit (smaller remaining capacity).
    # We use -remaining_capacity to make tighter fits have higher scores before softmax.
    remaining_after_placement = bins_remain_cap[can_fit_mask] - item
    
    # We want to prioritize bins where `remaining_after_placement` is small.
    # So, we want to maximize `-(remaining_after_placement)`.
    # Add a small epsilon to prevent issues with exactly zero remaining capacity
    # and to ensure differentiation if multiple bins are perfect fits.
    # Inverting this can also work, but direct negation is clearer for softmax input.
    
    # Let's prioritize bins with smaller remaining capacity.
    # We can map small remaining_capacity to higher values.
    # A simple way is to use `1 / (remaining_capacity + epsilon)`.
    # For softmax, we want to work with values that are not excessively large or small.
    # If remaining_capacity is 0, the score is high. If remaining_capacity is large, the score is low.
    
    # Let's define a "preference score" where higher is better.
    # A perfect fit (remaining_capacity=0) should have the highest preference.
    # A large remaining_capacity should have a low preference.
    # We can use -remaining_capacity as the base score.
    
    scores = -remaining_after_placement
    
    # Apply softmax to get probabilities. The higher the score, the higher the probability.
    # Softmax calculation: exp(score / temperature) / sum(exp(score / temperature))
    # For our priority function, we don't need the normalization of softmax,
    # just the relative "strength" of the preference, which is exp(score / temperature).
    # The `temperature` parameter controls the "softness" of the selection.
    # Low temperature: sharp distribution, favors best bin strongly.
    # High temperature: uniform distribution, encourages exploration.
    
    # Ensure scores are not too extreme to prevent overflow/underflow in exp
    # Clipping might be too aggressive, scaling can be better.
    # For now, assume scores are within reasonable bounds or use a robust softmax implementation if needed.
    
    # Calculate exponentiated scores
    exp_scores = np.exp(scores / temperature)
    
    # Assign these to the corresponding bins
    priorities[can_fit_mask] = exp_scores
    
    return priorities
```
