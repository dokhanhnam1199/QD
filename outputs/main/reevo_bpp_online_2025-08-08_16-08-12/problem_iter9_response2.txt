```python
import numpy as np
from scipy.special import softmax

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using an enhanced
    First Fit strategy with consideration for exact fits and minimal slack,
    and adaptive exploration.

    This heuristic prioritizes bins that can fit the item. Among those that fit,
    it assigns a higher priority to bins that will have less remaining capacity
    after the item is placed (exact fits are prioritized). It then uses a
    softmax function with a temperature parameter to introduce exploration,
    favoring bins that offer a good balance between fitting the item snugly
    and leaving some room for future items. The temperature is adjusted based
    on the item size relative to bin capacity to encourage more exploration
    for smaller items or when bins have abundant space.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins that can accommodate the item
    can_fit_mask = bins_remain_cap >= item

    # Calculate potential remaining capacities and a score for each fitting bin
    if np.any(can_fit_mask):
        fitting_bins_cap = bins_remain_cap[can_fit_mask]
        remaining_after_placement = fitting_bins_cap - item

        # Score: Prioritize exact fits (remaining_after_placement = 0)
        # Then prioritize minimal slack (smaller positive remaining_after_placement)
        # We use a transformation that gives higher values to smaller remaining capacities.
        # Adding 1 to remaining_after_placement to ensure positive values and
        # to differentiate between 0 remaining capacity and other small capacities.
        # A value of 1 indicates an exact fit. Larger values indicate more slack.
        # We invert this to get higher scores for tighter fits.
        scores = 1.0 / (remaining_after_placement + 1.0)

        # Adaptive temperature for softmax
        # Higher temperature means more uniform probabilities (more exploration)
        # Lower temperature means more peaked probabilities (more greedy)
        # Consider item size relative to the average capacity of fitting bins.
        # If item is small relative to available space, we might want more exploration.
        avg_fitting_cap = np.mean(fitting_bins_cap)
        if avg_fitting_cap > 0:
            exploration_factor = item / avg_fitting_cap
        else:
            exploration_factor = 1.0 # Default if no fitting bins have positive capacity

        # Tune the temperature: lower temperature for less exploration, higher for more.
        # We want to explore more when the item is relatively small or space is abundant.
        # A simple inverse relationship with exploration_factor can work, or a more
        # complex function can be derived from problem characteristics.
        # Let's use a temperature that decreases as exploration_factor increases,
        # so smaller items get higher temp.
        temperature = 0.5 + 1.0 / (1.0 + exploration_factor * 5) # Tunable parameter

        # Apply softmax to scores with the adaptive temperature
        # Softmax converts scores into probabilities.
        # The temperature parameter controls the "smoothness" of the distribution.
        # Lower temperature -> more greedy (favors highest score)
        # Higher temperature -> more exploration (probabilities are more evenly spread)
        probabilities = softmax(scores / temperature)

        priorities[can_fit_mask] = probabilities

    return priorities
```
