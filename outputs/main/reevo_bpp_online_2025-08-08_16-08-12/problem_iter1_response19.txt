```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Softmax-Based Fit.

    This strategy aims to balance two aspects:
    1. Prefer bins that have just enough remaining capacity to fit the item (Best Fit like behavior).
    2. Prefer bins that have more remaining capacity overall (First Fit Decreasing like behavior, but applied to priorities).

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Consider only bins that can actually fit the item
    fitting_bins_mask = bins_remain_cap >= item
    
    if not np.any(fitting_bins_mask):
        # If no bin can fit the item, return zeros (or handle as an error/special case)
        return np.zeros_like(bins_remain_cap)

    # Calculate 'fit' scores: how well the item fits into the bin.
    # Smaller difference (remaining_cap - item) is better.
    # We want higher scores for smaller differences.
    # Add a small epsilon to avoid division by zero if remaining_cap == item
    fit_scores = 1.0 / (bins_remain_cap[fitting_bins_mask] - item + 1e-6)

    # Calculate 'capacity' scores: a general preference for bins with more space.
    # This encourages using less full bins first if the fit is similar.
    # Add a small epsilon to avoid division by zero.
    capacity_scores = bins_remain_cap[fitting_bins_mask] + 1e-6

    # Combine scores. Here, we give a weighted sum.
    # The 'fit' score is often more critical for BPP efficiency.
    # A common approach is to use exponential of combined scores for Softmax.
    # Let's create a composite score: slightly favor better fit, but also capacity.
    # We'll scale these to avoid issues with magnitudes before exponentiation.
    
    # Normalize fit_scores to a [0, 1] range if necessary, but raw values might be fine too.
    # For simplicity, let's try a direct combination.
    
    # Create a composite score that emphasizes good fit, but also remaining capacity
    # A simple linear combination: 0.7 * normalized_fit + 0.3 * normalized_capacity
    # For Softmax, direct combination before exp is okay if scales are not wildly different.
    
    # Let's try a combination that prioritizes tighter fits more strongly.
    # A higher fit score (smaller residual capacity) is good.
    # A higher capacity score (more total remaining capacity) is also good.
    
    # A strategy could be to use log of capacity and inverse of residual capacity
    # or simply the inverse of residual capacity and the capacity itself.
    
    # Let's try to amplify the difference between good fits.
    # Residual capacity: bins_remain_cap[fitting_bins_mask] - item
    # We want smaller residuals to be higher priority.
    residual_capacities = bins_remain_cap[fitting_bins_mask] - item
    
    # We can invert this residual capacity to get a "tightness" score.
    # Smallest residual should get highest score.
    tightness_scores = -residual_capacities # Negative to use max later

    # Combine tightness and total capacity.
    # For Softmax, we want values that reflect preference.
    # A positive score is generally preferred.
    # A bin with item fits perfectly (residual = 0) should have a high score.
    # A bin with lots of space and a decent fit should also have a good score.
    
    # Let's try to reward bins that have *just enough* capacity,
    # and then reward bins with more overall capacity among those.
    
    # For Softmax, we exponentiate scores. We want to produce scores that,
    # when exponentiated, result in probabilities reflecting our preference.
    
    # A common way is: exp(score_for_bin_i).
    # The 'score_for_bin_i' should be higher for preferred bins.
    
    # Let's define a score based on inverse residual capacity and remaining capacity.
    # We want smaller residual_capacity to be better.
    # We want larger remaining_capacity to be better.
    
    # A possible combination for the exponent:
    # score = alpha * (1 / (residual_capacity + epsilon)) + beta * remaining_capacity
    # where alpha and beta are weights.
    
    alpha = 1.5 # Weight for 'good fit' (tightness)
    beta = 0.5  # Weight for overall remaining capacity
    epsilon = 1e-3 # Small value to avoid division by zero

    # Calculate scores for bins that can fit the item
    scores_for_fitting_bins = alpha * (1.0 / (residual_capacities + epsilon)) + beta * bins_remain_cap[fitting_bins_mask]
    
    # Initialize priorities for all bins to a very low value (or negative infinity effectively)
    # so that only fitting bins get positive consideration after softmax.
    priorities = np.full_like(bins_remain_cap, -np.inf)
    
    # Apply the scores to the fitting bins
    priorities[fitting_bins_mask] = scores_for_fitting_bins
    
    # Softmax function: exp(score) / sum(exp(scores))
    # We want to return scores that are proportional to probabilities,
    # so we can return the exponentiated scores directly.
    # The softmax itself would normalize them, but for priority selection,
    # proportional scores are often what's needed.
    
    # Using exp directly creates higher-valued outputs for better bins,
    # which aligns with selecting the bin with the highest priority.
    # If we were to pass these to a softmax distribution, they would work.
    # However, if the goal is to return scores that a highest-value selector
    # can use, we should just return the exponentiated values.
    
    # Let's return the exponentiated scores.
    # We need to handle the -np.inf cases carefully if exponentiating.
    # exp(-inf) is 0. So, bins that don't fit will have scores of 0.
    
    exp_priorities = np.exp(priorities)
    
    # To ensure that a zero probability isn't interpreted as zero preference,
    # and that we return scores reflecting preference for fitting bins,
    # we might scale or simply return the scores before exponentiation,
    # or handle the -inf to 0 conversion explicitly.

    # If we return exp_priorities, the non-fitting bins will have a score of 0,
    # and fitting bins will have positive scores. This is reasonable.

    # However, for "priority", directly using the calculated scores before exp
    # might be more interpretable for a "highest priority wins" selection.
    # The problem statement asks for "priority score", which implies a value
    # used for comparison. The Softmax-Based Fit strategy usually refers to
    # the final normalized probabilities, but here we need a score.

    # Let's consider the goal: "The bin with the highest priority score will be selected".
    # This means we don't need the normalized probabilities, just values proportional
    # to them. exp(score) is proportional. So exp_priorities is good.

    # An alternative for interpretation: return the raw scores before exponentiation.
    # The selection will pick the max.
    
    # Let's re-evaluate the score calculation. We want bins that fit tightly to have
    # higher values. Residual capacity `r = remaining_cap - item`. `r=0` is best.
    # `1/r` or `-r` can work. Using `1/(r + epsilon)` makes smaller `r` have larger score.
    # Then add a term for overall capacity `remaining_cap`.
    
    # The current `scores_for_fitting_bins` are what we'd pass to `exp()`.
    # Let's return these as the "priority scores". The selection process
    # would then pick the maximum. If we want to be strictly "Softmax-based",
    # we'd apply `exp` and then potentially normalize. But for a direct selector,
    # the raw scores that go into `exp` are often used.

    # Let's return `scores_for_fitting_bins` for the fitting bins, and a very low value
    # for non-fitting bins to ensure they are never picked if a fitting bin exists.
    
    priorities_final = np.full_like(bins_remain_cap, -np.inf)
    priorities_final[fitting_bins_mask] = scores_for_fitting_bins

    return priorities_final
```
