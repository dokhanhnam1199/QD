{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin.\n\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it assigns a higher priority to bins that will have less remaining capacity\n    after the item is placed (exact fits are preferred). It then considers\n    bins with slightly more slack. Softmax is used to convert these scores into\n    probabilities, with a temperature parameter to control exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    temperature = 1.0  # Tunable parameter for exploration\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        # Calculate the remaining capacity if the item is placed in a fitting bin\n        remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n        # Prioritize exact fits (remaining capacity is 0)\n        # Then prioritize bins with minimal slack.\n        # We want smaller remaining_after_placement to have higher priority.\n        # Adding a small constant to avoid log(0) and to ensure positive values.\n        # Scaling by a factor to make differences more pronounced before softmax.\n        # An arbitrary scaling factor, could be tuned.\n        scaled_slack_scores = -1.0 / (remaining_after_placement + 1e-9)\n\n        # Apply softmax to convert scores to probabilities (priorities)\n        # Higher scores (smaller slack) will get higher probabilities\n        priorities[can_fit_mask] = scipy.special.softmax(scaled_slack_scores / temperature)\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin.\n    This heuristic prioritizes bins that can fit the item. Among those that fit,\n    it assigns a higher priority to bins that will have less remaining capacity\n    after the item is placed. This is a greedy approach aiming to fill bins\n    as much as possible, encouraging denser packing. It also introduces a\n    soft-max like behavior to allow for some exploration and preference.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate remaining capacity if item is placed in a fitting bin\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n    \n    # Assign higher priority to bins with less remaining capacity after placement.\n    # We use a transformation that favors exact fits or near-exact fits more strongly.\n    # A simple inverse might not differentiate enough between good fits.\n    # A common approach is to use something like 1 / (1 + slack) or a scaled version.\n    # Here, we'll use a slightly more aggressive inverse, but add a small constant\n    # to ensure that perfect fits (slack=0) get a very high, but not infinite, score.\n    # We can also consider a form that penalizes very large remaining capacities more.\n    \n    # Transform remaining capacity to priority: lower remaining capacity = higher priority.\n    # We invert the remaining capacity. Adding 1 avoids division by zero for exact fits\n    # and makes the priority for exact fits finite but highest.\n    # A small scaling factor can be used for temperature/exploration tuning.\n    # Let's use a scaling factor `alpha`. Higher alpha means more greedy.\n    alpha = 1.0 # Tunable parameter for exploration/greediness\n    \n    # Higher values of `remaining_after_placement` should result in lower priorities.\n    # We can use `exp(-alpha * remaining_after_placement)` or similar, but for a\n    # direct priority score that favors minimal slack, an inverse is more direct.\n    \n    # Let's try a transformation that makes smaller remaining capacities map to larger priorities.\n    # We want to prioritize exact fits, then minimal slack.\n    # For slack = 0, priority should be high.\n    # For slack = small, priority should be high.\n    # For slack = large, priority should be low.\n    \n    # A common approach in soft-max like prioritization is:\n    # priority = exp(score)\n    # Where score is something like -slack or 1/slack.\n    # To avoid extremely large values for exact fits, we can normalize or use a different function.\n    \n    # Option 1: Simple inverse with a base to avoid division by zero and provide a floor.\n    # `priorities[can_fit_mask] = 1.0 / (remaining_after_placement + 1e-6)`\n    \n    # Option 2: Using a scaled exponential for a softer distribution, closer to softmax.\n    # `priorities[can_fit_mask] = np.exp(-alpha * remaining_after_placement)`\n    \n    # Option 3: Prioritize exact fits strongly, then minimal slack.\n    # Let's create a score that is high for small remaining_after_placement.\n    # `score = 1.0 / (1.0 + remaining_after_placement)` - this gives 1 for exact fit.\n    # Or, `score = 1.0 - (remaining_after_placement / max_possible_slack)` to normalize.\n    \n    # Let's go with a transformation that heavily favors smaller remaining capacities.\n    # We can use a function like: `1 / (1 + slack)` or `1 - (slack / max_item_size)`\n    # Using `1.0 / (1.0 + remaining_after_placement)` provides a score between 0 and 1,\n    # where 1 is for an exact fit. This seems to capture the \"minimal slack\" idea well.\n    \n    priorities[can_fit_mask] = 1.0 / (1.0 + remaining_after_placement)\n    \n    # Optional: Introduce a small random noise or a soft-max like scaling to encourage exploration\n    # For a purely greedy approach, this is not needed.\n    # If exploration is desired, we could do:\n    # scaled_priorities = np.exp(priorities[can_fit_mask] * temp)\n    # For now, sticking to the core reflection of prioritizing minimal slack.\n\n    return priorities\n\n[Reflection]\nPrioritize minimal slack for dense packing. Explore varying score transformations.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}