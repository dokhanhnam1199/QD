```python
import numpy as np

def softmax(x, temp=1.0):
    """Compute softmax values for each set of scores in x."""
    e_x = np.exp(x / temp)
    return e_x / e_x.sum(axis=0)

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Prioritizes bins for the online Bin Packing Problem using a softmax-based
    approach to balance "Best Fit" (exploitation) with exploration.

    This heuristic prioritizes bins that can fit the item. Among those that fit,
    it assigns a higher priority to bins that will have less remaining capacity
    after the item is placed (exploitation). The softmax function is used to
    convert these "fit scores" into probabilities, introducing a degree of
    exploration. Bins with better fits are more likely to be chosen, but not
    guaranteed, depending on the temperature parameter.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A numpy array containing the remaining capacity of each bin.

    Returns:
        A numpy array of the same size as bins_remain_cap, where each element
        represents the probability (priority score) of placing the item in the
        corresponding bin.
    """
    num_bins = len(bins_remain_cap)
    priorities = np.zeros(num_bins, dtype=float)

    # Identify bins that have enough capacity to fit the item
    can_fit_mask = bins_remain_cap >= item

    if np.any(can_fit_mask):
        # Calculate the "goodness" of fit for bins that can accommodate the item.
        # A smaller remaining capacity after placement indicates a better fit.
        # We want to maximize this "goodness", so we use the inverse of
        # remaining capacity. Adding a small epsilon to avoid division by zero.
        remaining_after_placement = bins_remain_cap[can_fit_mask] - item
        fit_scores = 1.0 / (remaining_after_placement + 1e-9)

        # Apply softmax to convert fit scores into probabilities.
        # The 'temperature' parameter controls the balance between exploitation and exploration.
        # A lower temperature (e.g., 0.1) makes the probabilities sharper, favoring the best fit more.
        # A higher temperature (e.g., 1.0 or more) makes the probabilities smoother,
        # increasing the chance of picking less optimal fits.
        temperature = 0.5  # Tunable parameter
        probabilities = softmax(fit_scores, temp=temperature)

        # Assign these probabilities as priorities to the bins that can fit the item.
        priorities[can_fit_mask] = probabilities
    
    # Bins that cannot fit the item will have a priority of 0.
    # In a real application, if all priorities are 0, a new bin would be opened.

    return priorities
```
