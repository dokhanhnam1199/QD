```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.

    This heuristic prioritizes bins that have a remaining capacity close to the item size,
    but also considers bins with slightly larger remaining capacity to allow for better
    future packing. The softmax function is used to convert these 'fitness' scores
    into probabilities (priorities).

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Returns:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Calculate a "fitness" score for each bin.
    # We want bins where remaining capacity is slightly larger than the item.
    # A good score would be when remaining_capacity - item is small and non-negative.
    # We can use a function that penalizes larger gaps, like an inverse or negative exponential.
    # Here, we'll use a term that is high when (remaining_capacity - item) is small and positive.
    # We'll also consider bins that can fit the item, so we set a very low score for bins
    # that cannot fit the item.

    # Maximum difference we are willing to tolerate. This can be tuned.
    # A larger max_diff allows for more flexibility.
    max_diff = np.max(bins_remain_cap) # A simple heuristic for max_diff

    # Calculate the "gap" between remaining capacity and item size for bins that can fit the item
    gaps = bins_remain_cap - item

    # Initialize fitness scores. Bins that cannot fit the item will have a very low score.
    fitness_scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)

    # For bins that can fit the item (gaps >= 0)
    can_fit_mask = (gaps >= 0)
    valid_gaps = gaps[can_fit_mask]

    if np.any(can_fit_mask):
        # We want to prioritize bins with smaller gaps (closer fit).
        # A common approach is to use an inverse function or a Gaussian-like function.
        # Let's use a score that increases as the gap decreases (approaching zero).
        # We can normalize the gaps to prevent extreme values and map them to a reasonable range.
        # A simple approach: -valid_gaps / max_diff. This gives values close to 0 for small gaps
        # and values close to -1 for large gaps. We want higher scores for smaller gaps.
        # Let's try a linear transformation to make small gaps have higher positive values.
        # Higher values mean higher priority.
        # Score = (max_gap - gap) / max_gap, for gap >= 0.
        # A simple way to map to positive values and use softmax is to transform the gap.
        # Consider the "waste": remaining_capacity - item. We want to minimize waste.
        # Let's use a score proportional to the inverse of the waste, or a negative exponential
        # of the waste to create a smooth decay.

        # Method 1: Using inverse of gap + 1 to avoid division by zero and ensure positive values.
        # This will heavily favor bins that fit perfectly or near-perfectly.
        # adjusted_scores = 1.0 / (valid_gaps + 1e-6)
        # However, this can be sensitive to small gaps.

        # Method 2: Using a soft-ranking based on the difference, aiming for a "best fit" preference.
        # We can penalize larger differences more. For instance, we could use exp(-k * gap).
        # A higher score means better fit.
        # Let's use exp(-gap / temperature). Higher temperature makes it flatter (more uniform).
        # A smaller temperature makes it sharper (more preference for best fit).
        # Let's use a moderate temperature for diversity.
        temperature = np.mean(bins_remain_cap) if np.mean(bins_remain_cap) > 0 else 1.0 # Dynamic temperature

        # Ensure gaps are not excessively large compared to temperature,
        # otherwise exp(-large_gap/temp) will underflow to 0.
        # We can clip the gaps or use a scaled version.
        # Scale the gaps relative to the temperature to control the softmax spread.
        scaled_gaps = valid_gaps / temperature

        # Calculate fitness scores using negative exponential. Higher values mean better fit.
        # We want to give higher scores to smaller gaps.
        # Let's invert the logic to have positive fitness for smaller gaps.
        # For example, consider `max_useful_gap - gap` and then take exponential,
        # or more directly `exp(-(gap / normalization_factor))`.
        # A simple positive score that decays with gap size:
        positive_scores = np.exp(-scaled_gaps)
        fitness_scores[can_fit_mask] = positive_scores

    # Apply Softmax to convert fitness scores into probabilities (priorities)
    # Avoid issues with all scores being -inf or too close, which can lead to NaN in softmax.
    # If all bins cannot fit, return uniform low priorities.
    if not np.any(can_fit_mask):
        return np.ones_like(bins_remain_cap) / len(bins_remain_cap)

    # Ensure that fitness_scores has at least one finite value before applying softmax
    finite_scores = fitness_scores[np.isfinite(fitness_scores)]
    if len(finite_scores) == 0:
         return np.ones_like(bins_remain_cap) / len(bins_remain_cap)

    # Shift scores to avoid potential numerical instability with large negative values
    # before exponentiation in softmax. The difference from the max score is constant.
    max_score = np.max(fitness_scores[np.isfinite(fitness_scores)])
    shifted_scores = fitness_scores - max_score

    # Compute softmax
    exp_scores = np.exp(shifted_scores)

    # Handle potential infinities that might arise if shifted_scores still contains large positive numbers
    # (e.g., if max_score was incorrectly calculated due to -inf).
    # More robust approach for softmax:
    exp_scores = np.where(np.isfinite(shifted_scores), np.exp(shifted_scores), 0)
    sum_exp_scores = np.sum(exp_scores)

    if sum_exp_scores == 0:
        # Fallback: if all exponentiated scores are zero or non-finite, assign uniform priority
        return np.ones_like(bins_remain_cap) / len(bins_remain_cap)
    else:
        priorities = exp_scores / sum_exp_scores
        return priorities

```
