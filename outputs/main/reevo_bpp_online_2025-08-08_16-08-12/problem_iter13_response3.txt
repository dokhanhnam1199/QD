[Prior reflection]
The core idea is to make the priority function more robust and adaptable.
1.  **Exact Fits:** Keep the highest priority. This is a fundamental greedy strategy.
2.  **Near Fits:** Differentiate them more strongly by how "near" they are. Instead of a binary near/not-near, use a continuous score. A good candidate is a scaled inverse of the remaining capacity *after* placing the item, but capped to prevent extreme values and make it more stable. This encourages prioritizing bins that leave *just enough* space.
3.  **General Fits:** Assign a base priority, but perhaps slightly higher than before to ensure they are considered more readily than if they had a very low score.
4.  **Temperature-Controlled Softmax:** The previous approach used softmax on inverse remaining capacity. This is good for exploration. However, we need to ensure that the *quality* of the fit (how much space is left) influences the softmax input. Instead of just `1/remaining_capacity`, we can use a function that grows rapidly for small remaining capacities and then flattens out. `1 / (remaining_capacity + constant)` or `exp(-k * remaining_capacity)` are options. We need to keep the temperature parameter to control exploration. The scaling of softmax output needs careful tuning to ensure it doesn't overlap with exact fits or general fits too much.
5.  **Stability:** Use of epsilon is good. Capping the priority scores can also add stability.

Let's refine the strategy for "Near Fits":
-   Define "near" more explicitly, perhaps based on the item size. For instance, bins where remaining capacity is less than `item_size * threshold`.
-   For these "near" bins, assign priorities that are inversely proportional to the *remaining capacity after placement*.
-   To make this scaling stable and avoid extreme values when remaining capacity is very small (but not zero), we can use a function like `1 / (remaining_capacity + epsilon)` or `exp(-k * remaining_capacity)`.
-   Apply a temperature-controlled softmax to these scaled priorities to allow for exploration among the best "near" fits. The higher the temperature, the more uniform the probabilities become among these candidates.
-   The final priority for near fits will be a combination of a base score (to distinguish them from general fits) and the scaled softmax probability.

Consider a function for near-fit scoring: `f(r) = exp(-k * r)`, where `r` is remaining capacity after placement. This gives higher scores for smaller `r`. We can then apply softmax to `f(r)`.

Let's define the thresholds and parameters more clearly:
-   `EXACT_FIT_PRIORITY`: Very high, e.g., 100.0.
-   `GENERAL_FIT_PRIORITY`: Low positive value, e.g., 1.0.
-   `NEAR_FIT_BASE_SCORE`: A score that places near fits above general fits, e.g., 50.0.
-   `NEAR_FIT_SCALE`: A factor to scale the softmax probabilities, e.g., 40.0.
-   `NEAR_FIT_THRESHOLD_RATIO`: Define "near" based on remaining capacity relative to item size, e.g., `0.20`.
-   `NEAR_FIT_TEMPERATURE`: Controls exploration, e.g., `0.7`.
-   `NEAR_FIT_EXP_FACTOR`: Controls the steepness of the exponential decay for remaining capacity, e.g., `k` in `exp(-k*r)`. A larger `k` means faster decay for larger remaining capacities. Let's set `k` to be inversely related to the threshold, e.g., `1 / (item_size * NEAR_FIT_THRESHOLD_RATIO)`. This way, a larger item relative to the threshold gets a stronger penalty for larger remaining capacities.

The goal is to create a smooth transition from very good fits (small remaining capacity) to acceptable fits.

Refined plan:
1.  Identify exact fits. Assign `EXACT_FIT_PRIORITY`.
2.  Identify bins that can fit but are not exact fits.
3.  For these fitting bins, calculate remaining capacity `r = bins_remain_cap - item`.
4.  Define a "nearness" score for each fitting bin based on `r`. A good candidate is `exp(-NEAR_FIT_EXP_FACTOR * r)`. This gives high values for small `r`.
5.  Apply temperature-controlled softmax to these "nearness" scores. The softmax output will represent the normalized preference for each near-fitting bin.
6.  Scale these softmax outputs and add `NEAR_FIT_BASE_SCORE` to get the priority for near fits.
7.  Assign `GENERAL_FIT_PRIORITY` to fitting bins that are not considered "near" (i.e., `r` is large).

Let's re-evaluate the "near fit" criteria. A bin is "near" if `r` is small. The previous version used a threshold. Instead, let's use the scores directly. Bins with very small `r` will naturally get high scores from `exp(-k*r)`. Bins with larger `r` will get scores closer to 1. We can still differentiate between "very near" and "moderately near" by adjusting `k` or the scaling.

Alternative scoring for near fits:
Instead of softmax on `exp(-k*r)`, let's consider a direct mapping for near fits that is inverse to remaining capacity, but bounded.
Perhaps `NEAR_FIT_BASE_SCORE + NEAR_FIT_SCALE * (1 / (r + epsilon))` for bins where `r` is below some threshold, and `GENERAL_FIT_PRIORITY` for those above. The softmax approach is still interesting for exploration.

Let's stick to the softmax approach for exploration but refine the input to softmax.
Input to softmax should be high for good fits (small remaining capacity).
Let's use `1.0 / (remaining_capacity + epsilon)` scaled and shifted so that the best fit gets a high value and others get progressively lower but positive values.

Revised approach for near fits:
1.  Identify bins `b` where `bins_remain_cap[b] >= item`. Let `r = bins_remain_cap[b] - item`.
2.  For these fitting bins, calculate a raw score: `raw_score = 1.0 / (r + epsilon_small)`. Higher score means smaller `r`.
3.  We want to prioritize bins with smaller `r`. However, very large `r` should still get a positive priority, not close to zero.
4.  Instead of a binary "near" classification, let's create a spectrum of scores for all fitting bins.
5.  Use a function that maps `r` to a priority.
    -   Exact fits: `EXACT_FIT_PRIORITY`.
    -   Fitting bins: Map `r` to a score. A sigmoid-like function or a scaled inverse could work.
    -   Let's consider the remaining capacity `r` after placement.
    -   If `r == 0`: exact fit.
    -   If `0 < r < threshold_small`: near fit, high priority.
    -   If `r >= threshold_small`: general fit, lower priority.

Let's reconsider the reflection: "scale inverse remaining capacity aggressively but stably."
This suggests a function like `scale * (1 / (r + epsilon))`. To make it stable, we should cap the output.

Let's combine ideas:
1.  **Exact Fits**: Highest priority.
2.  **Fitting Bins**: Calculate `r = bins_remain_cap - item`.
3.  **Score Calculation**:
    *   For bins where `r` is very small (e.g., `r < epsilon_small`): These are practically exact fits. Assign `EXACT_FIT_PRIORITY`.
    *   For bins where `0 < r < NEAR_FIT_THRESHOLD`: These are "near fits". Assign a priority based on `1.0 / r`, but scaled and possibly capped. Let's use `NEAR_FIT_BASE_SCORE + NEAR_FIT_SCALE * stable_inverse_r(r)`.
    *   For bins where `r >= NEAR_FIT_THRESHOLD`: These are "general fits". Assign `GENERAL_FIT_PRIORITY`.

`stable_inverse_r(r)`:
A stable inverse could be `1.0 / (r + NEAR_FIT_EPSILON)`. We should cap this to prevent extremely high values if `r` is very close to zero but not exactly zero. Let's say the maximum inverse value we want is `MAX_INV_SCORE`.
So, `capped_inverse_r = min(1.0 / (r + epsilon_small), MAX_INV_SCORE)`.

Then priority for near fits: `NEAR_FIT_BASE_SCORE + NEAR_FIT_SCALE * capped_inverse_r`.
We need to ensure `NEAR_FIT_BASE_SCORE + NEAR_FIT_SCALE * MAX_INV_SCORE` is less than `EXACT_FIT_PRIORITY`.

Let's refine parameters:
-   `EXACT_FIT_PRIORITY = 100.0`
-   `GENERAL_FIT_PRIORITY = 1.0`
-   `NEAR_FIT_THRESHOLD = 5.0` (absolute threshold for 'near')
-   `NEAR_FIT_BASE_SCORE = 30.0` (base for near fits)
-   `NEAR_FIT_SCALE = 60.0` (scaling factor for inverse capacity)
-   `MAX_INV_SCORE = 5.0` (capped value for `1/(r+eps)`)
-   `epsilon_small = 1e-9`
-   `epsilon_near_fit = 1e-6` (for distinguishing near from exact, if needed)

The reflection also mentioned "temperature-controlled softmax". This implies we want exploration. If we use deterministic scores like `BASE + SCALE * capped_inverse`, there's no exploration among near fits.

Okay, let's try to integrate softmax for exploration *among near fits*.

Plan v3:
1.  **Exact Fits**: `priorities[abs(bins_remain_cap - item) < epsilon_small] = EXACT_FIT_PRIORITY`.
2.  **Fitting Bins**: Identify `can_fit_mask` and `exact_fit_mask`.
    `fitting_bin_indices = np.where(can_fit_mask & ~exact_fit_mask)[0]`
    `remaining_capacities_after_placement = bins_remain_cap[fitting_bin_indices] - item`
3.  **Categorize Fitting Bins into Near/General**:
    `near_fit_criteria = remaining_capacities_after_placement < NEAR_FIT_THRESHOLD`
    `near_fit_indices_subset = fitting_bin_indices[near_fit_criteria]`
    `non_near_fit_indices_subset = fitting_bin_indices[~near_fit_criteria]`
4.  **Assign Priorities**:
    *   **General Fits**: `priorities[non_near_fit_indices_subset] = GENERAL_FIT_PRIORITY`.
    *   **Near Fits**:
        *   Calculate raw scores: `near_fit_raw_scores = 1.0 / (remaining_capacities_after_placement[near_fit_criteria] + epsilon_small)`. These scores are higher for smaller remaining capacity.
        *   To make these scores suitable for softmax and avoid extreme values, we can normalize them or scale them. Let's use a clamped inverse.
        *   `clamped_scores = np.minimum(near_fit_raw_scores, MAX_INV_SCORE)`
        *   Apply softmax to `clamped_scores / TEMPERATURE`.
        *   `shifted_scores = clamped_scores - np.max(clamped_scores)` (for stability)
        *   `exp_scores = np.exp(shifted_scores / TEMPERATURE)`
        *   `sum_exp_scores = np.sum(exp_scores)`
        *   `softmax_probs = exp_scores / sum_exp_scores` if `sum_exp_scores > 0` else uniform probs.
        *   Final priority for near fits: `NEAR_FIT_BASE_SCORE + softmax_probs * NEAR_FIT_SCALE`.

This looks like a solid plan that incorporates the reflection's points.
- Prioritize exact fits.
- For near fits, scale inverse remaining capacity (using `1/r`).
- Scale aggressively but stably: Use `min(1/r, MAX_INV_SCORE)` for stability.
- Differentiate fit categories: Exact, Near, General.
- Tune scores: `EXACT_FIT_PRIORITY`, `NEAR_FIT_BASE_SCORE`, `NEAR_FIT_SCALE`, `GENERAL_FIT_PRIORITY`.
- Use bounded transformations: `min(..., MAX_INV_SCORE)` is a bounded transformation.
- Temperature-controlled softmax: applied to the scaled inverse capacities to balance exploitation/exploration.

Let's define parameters again for clarity.

```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Prioritizes bins for the online Bin Packing Problem with enhanced strategies,
    focusing on stable scaling of inverse remaining capacity and exploration
    among near fits.

    Heuristic Strategy:
    1. Exact Fits: Highest priority, ensuring perfect matches are always chosen.
    2. Near Fits: Bins where the remaining capacity after placement is small
       (below a defined threshold). These are prioritized based on a scaled
       inverse of the remaining capacity, controlled by a temperature parameter
       for exploration among the best near fits.
    3. General Fits: Bins where the item fits, but the remaining capacity is
       not considered 'near'. These receive a low positive priority.
    4. Unfittable: Bins where the item cannot fit, assigned negative infinity.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A numpy array containing the remaining capacity of each bin.

    Returns:
        A numpy array of the same size as bins_remain_cap, where each element
        represents the priority score for placing the item in the corresponding bin.
    """
    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)

    # --- Configuration Parameters ---
    # Priority for exact fits. Must be highest.
    EXACT_FIT_PRIORITY = 100.0
    # Base score for near fits before adding exploration-driven component.
    # Ensures near fits are preferred over general fits.
    NEAR_FIT_BASE_SCORE = 30.0
    # Scaling factor for the exploration component derived from softmax.
    NEAR_FIT_SCALE = 60.0
    # Priority for bins that fit but are not considered "near".
    GENERAL_FIT_PRIORITY = 1.0
    # Threshold to classify a bin as a "near fit". Bins with remaining capacity
    # less than this value after placement are considered near fits.
    NEAR_FIT_THRESHOLD = 5.0  # e.g., if remaining capacity < 5.0 units
    # Maximum value for the scaled inverse remaining capacity. This helps stabilize
    # priorities and prevents extremely high scores for very small remaining capacities.
    MAX_INV_SCORE = 5.0
    # Temperature parameter for the softmax function. Higher temperature leads to
    # more uniform probabilities (more exploration), lower temperature leads to
    # more skewed probabilities (more exploitation of the best near fit).
    TEMPERATURE = 0.7
    # Small constant to prevent division by zero in inverse calculations.
    epsilon_small = 1e-9

    # --- Bin Suitability Check ---
    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        return priorities # No bin can fit the item

    # --- Assign Priorities ---

    # 1. Exact Fits: Prioritize bins where the item perfectly fills the remaining capacity.
    # Use a small tolerance for floating point comparisons.
    exact_fit_mask = can_fit_mask & (np.abs(bins_remain_cap - item) < epsilon_small)
    priorities[exact_fit_mask] = EXACT_FIT_PRIORITY

    # Identify bins that can fit the item but are not exact fits.
    fitting_bin_indices = np.where(can_fit_mask & ~exact_fit_mask)[0]

    if len(fitting_bin_indices) > 0:
        # Calculate remaining capacity for all fitting bins.
        remaining_capacities_after_placement = bins_remain_cap[fitting_bin_indices] - item

        # 2. Categorize fitting bins into "Near Fits" and "General Fits".
        near_fit_criteria = remaining_capacities_after_placement < NEAR_FIT_THRESHOLD
        
        near_fit_indices_in_subset = np.where(near_fit_criteria)[0]
        general_fit_indices_in_subset = np.where(~near_fit_criteria)[0]

        # 3. Assign priorities for General Fits.
        general_fit_actual_indices = fitting_bin_indices[general_fit_indices_in_subset]
        priorities[general_fit_actual_indices] = GENERAL_FIT_PRIORITY

        # 4. Assign priorities for Near Fits using scaled inverse capacity and softmax.
        near_fit_actual_indices = fitting_bin_indices[near_fit_indices_in_subset]
        
        if len(near_fit_actual_indices) > 0:
            near_fit_remaining_caps = remaining_capacities_after_placement[near_fit_indices_in_subset]

            # Calculate raw scores: inverse of remaining capacity. Higher score for smaller capacity.
            # Add epsilon_small for numerical stability.
            near_fit_raw_scores = 1.0 / (near_fit_remaining_caps + epsilon_small)

            # Apply stable scaling by capping the raw scores.
            clamped_scores = np.minimum(near_fit_raw_scores, MAX_INV_SCORE)

            # Use temperature-controlled softmax to get exploration-driven weights.
            # We softmax over the clamped scores, scaled by temperature.
            if len(clamped_scores) > 0:
                # Shift scores for numerical stability before exponentiation.
                max_clamped_score = np.max(clamped_scores)
                shifted_clamped_scores = clamped_scores - max_clamped_score
                
                exp_scores = np.exp(shifted_clamped_scores / TEMPERATURE)
                sum_exp_scores = np.sum(exp_scores)

                if sum_exp_scores > 0:
                    softmax_probabilities = exp_scores / sum_exp_scores
                else:
                    # Fallback: if all exp_scores are ~0 or NaN, treat as uniform distribution.
                    softmax_probabilities = np.ones_like(clamped_scores) / len(clamped_scores)
                
                # Combine base score with scaled softmax probabilities for final near-fit priority.
                scaled_near_fit_priorities = NEAR_FIT_BASE_SCORE + softmax_probabilities * NEAR_FIT_SCALE
                priorities[near_fit_actual_indices] = scaled_near_fit_priorities
            else:
                 # Should not happen if near_fit_actual_indices > 0, but for safety:
                 priorities[near_fit_actual_indices] = NEAR_FIT_BASE_SCORE # Assign base score if no scores could be calculated


    return priorities

```
Testing the logic mentally:
- Item size = 10.
- Bins remaining capacity: [12, 10, 25, 18, 15]
- `can_fit_mask`: [True, True, True, True, True]
- Exact fits: Bin 1 (capacity 10) -> priority 100.0
- Fitting bins (indices 0, 2, 3, 4) with capacities [12, 25, 18, 15].
- Remaining capacities after placement: [2, 15, 8, 5] for bins with original indices [0, 2, 3, 4].
- `NEAR_FIT_THRESHOLD = 5.0`.
- Near fit criteria: `[2 < 5, 15 < 5, 8 < 5, 5 < 5]` -> `[True, False, False, True]`.
- Near fits are bins with original indices [0, 4]. Remaining caps: [2, 5].
- General fits are bins with original indices [2, 3]. Remaining caps: [15, 8].
- Priorities initialization: [-inf, -inf, -inf, -inf, -inf]
- After exact fit: [100.0, -inf, -inf, -inf, -inf] (Assuming bin 1 was index 1)
  Let's use the example: bins_remain_cap = [12, 10, 25, 18, 15]. item = 10.
  Priorities: [-inf, -inf, -inf, -inf, -inf]
  Exact fit: bin 1 (cap 10). `priorities` -> [-inf, 100.0, -inf, -inf, -inf]
  Fitting indices (original): [0, 2, 3, 4]
  Remaining caps for these: [2, 15, 8, 5]
  Near fit criteria (`< 5.0`): [True, False, False, True]
  Near fit indices (original): [0, 4]. Remaining caps: [2, 5].
  General fit indices (original): [2, 3]. Remaining caps: [15, 8].

  Assign general fit priority:
  `priorities[2] = 1.0`
  `priorities[3] = 1.0`
  `priorities` -> [-inf, 100.0, 1.0, 1.0, -inf]

  Assign near fit priority:
  Indices: [0, 4]. Remaining caps: [2, 5].
  Raw scores (1 / (r + eps)): [1/2.000000001, 1/5.000000001] approx [0.5, 0.2]
  `MAX_INV_SCORE = 5.0`.
  Clamped scores: [min(0.5, 5.0), min(0.2, 5.0)] -> [0.5, 0.2]

  Softmax calculation: `TEMPERATURE = 0.7`.
  Scores: [0.5, 0.2]. Max score = 0.5.
  Shifted scores: [0.5 - 0.5, 0.2 - 0.5] -> [0.0, -0.3]
  Exp scores: [exp(0.0/0.7), exp(-0.3/0.7)] -> [exp(0), exp(-0.428)] approx [1.0, 0.65]
  Sum exp scores: 1.0 + 0.65 = 1.65
  Softmax probs: [1.0 / 1.65, 0.65 / 1.65] approx [0.606, 0.394]

  Final near fit priorities:
  `NEAR_FIT_BASE_SCORE (30.0) + softmax_probs * NEAR_FIT_SCALE (60.0)`
  Bin 0 (remaining cap 2): `30.0 + 0.606 * 60.0 = 30.0 + 36.36 = 66.36`
  Bin 4 (remaining cap 5): `30.0 + 0.394 * 60.0 = 30.0 + 23.64 = 53.64`

  Update priorities:
  `priorities[0] = 66.36`
  `priorities[4] = 53.64`

  Final priorities: [66.36, 100.0, 1.0, 1.0, 53.64]
  This seems logical. Bin 1 is exact fit (highest). Bin 0 leaves 2 units, gets high near-fit score. Bin 4 leaves 5 units (just at threshold), gets lower near-fit score. Bins 2 and 3 leave more space, get general-fit priority.

One edge case: What if all fitting bins are near fits?
bins_remain_cap = [11, 12, 13], item = 10.
Exact: None.
Fitting: [11, 12, 13] (indices 0,1,2)
Remaining: [1, 2, 3]
Threshold = 5.0. All are near fits.
Raw scores: [1/1.000000001, 1/2.000000001, 1/3.000000001] approx [1.0, 0.5, 0.333]
Clamped scores: [min(1.0, 5.0), min(0.5, 5.0), min(0.333, 5.0)] -> [1.0, 0.5, 0.333]
Softmax calculation...
This appears correct.

Consider parameter values:
- `NEAR_FIT_THRESHOLD`: Could be relative, e.g., `item * 0.15`. This makes it adaptive to item size. The reflection hinted at this: "For near fits, scale inverse remaining capacity aggressively but stably. Differentiate fit categories, tune scores". Using an absolute threshold might be too rigid. Let's keep it absolute for now as it's simpler to reason about, but a relative threshold would be a good next iteration.
- `MAX_INV_SCORE`: If `NEAR_FIT_THRESHOLD` is 5.0, then `1/5.0` is `0.2`. `MAX_INV_SCORE = 5.0` means we are heavily prioritizing capacities close to zero. This seems fine.
- `TEMPERATURE`: `0.7` is a reasonable starting point for exploration.

The code seems robust and aligns with the reflection.
Final check on the implementation details:
- `np.where(can_fit_mask & ~exact_fit_mask)[0]` correctly gets indices of fitting but not exact bins.
- Slicing `remaining_capacities_after_placement` based on `near_fit_criteria` is correct.
- Handling of `len(near_fit_actual_indices) == 0` cases (e.g., assigning BASE_SCORE if calculations fail) is reasonable.
- Numerical stability for softmax (`shifted_scores`, `sum_exp_scores > 0` check) is included.

One final consideration: The reflection mentions "scale inverse remaining capacity aggressively but stably". The `MAX_INV_SCORE` capped `1/r` helps stability. Aggressiveness comes from the `NEAR_FIT_SCALE` and the steepness of `1/r`. Maybe `k` in `exp(-k*r)` would be better for controlling aggressiveness?
However, the prompt asks for a mutated function based on the *provided* reflection. The current `v2` implements the strategy well. The use of `1.0 / (r + epsilon_small)` directly relates to "scale inverse remaining capacity". Capping it addresses "stably".

The existing `priority_v1` already uses softmax on inverse capacity. `v2` refines this by:
1.  More explicit category definitions (near vs general based on threshold).
2.  Stable inverse scaling (`MAX_INV_SCORE`).
3.  More tuned base scores and scales.

The reflection also mentions "Differentiate fit categories, tune scores". This is addressed by the distinct `EXACT_FIT_PRIORITY`, `NEAR_FIT_BASE_SCORE`, `NEAR_FIT_SCALE`, and `GENERAL_FIT_PRIORITY`.

I believe `priority_v2` aligns well with the reflection.

```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Prioritizes bins for the online Bin Packing Problem with enhanced strategies,
    focusing on stable scaling of inverse remaining capacity and exploration
    among near fits.

    Heuristic Strategy:
    1. Exact Fits: Highest priority, ensuring perfect matches are always chosen.
    2. Near Fits: Bins where the remaining capacity after placement is small
       (below a defined threshold). These are prioritized based on a scaled
       inverse of the remaining capacity, controlled by a temperature parameter
       for exploration among the best near fits. The inverse capacity is capped
       for stability.
    3. General Fits: Bins where the item fits, but the remaining capacity is
       not considered 'near'. These receive a low positive priority.
    4. Unfittable: Bins where the item cannot fit, assigned negative infinity.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A numpy array containing the remaining capacity of each bin.

    Returns:
        A numpy array of the same size as bins_remain_cap, where each element
        represents the priority score for placing the item in the corresponding bin.
    """
    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)

    # --- Configuration Parameters ---
    # Priority for exact fits. Must be the highest.
    EXACT_FIT_PRIORITY = 100.0
    # Base score for near fits before adding the exploration-driven component.
    # Ensures near fits are preferred over general fits.
    NEAR_FIT_BASE_SCORE = 30.0
    # Scaling factor for the exploration component derived from softmax.
    # This determines how much the exploration aspect influences the priority.
    NEAR_FIT_SCALE = 60.0
    # Priority for bins that fit but are not considered "near".
    GENERAL_FIT_PRIORITY = 1.0
    # Threshold to classify a bin as a "near fit". Bins with remaining capacity
    # less than this value after placement are considered near fits.
    NEAR_FIT_THRESHOLD = 5.0  # e.g., if remaining capacity < 5.0 units
    # Maximum value for the scaled inverse remaining capacity. This helps stabilize
    # priorities and prevents extremely high scores for very small remaining capacities,
    # making the scaling more robust.
    MAX_INV_SCORE = 5.0
    # Temperature parameter for the softmax function. Higher temperature leads to
    # more uniform probabilities (more exploration), lower temperature leads to
    # more skewed probabilities (more exploitation of the best near fit).
    TEMPERATURE = 0.7
    # Small constant to prevent division by zero in inverse calculations.
    epsilon_small = 1e-9

    # --- Bin Suitability Check ---
    # Create a mask for bins that have enough capacity for the item.
    can_fit_mask = bins_remain_cap >= item

    # If no bin can fit the item, return initial priorities.
    if not np.any(can_fit_mask):
        return priorities

    # --- Assign Priorities ---

    # 1. Exact Fits: Prioritize bins where the item perfectly fills the remaining capacity.
    # Use a small tolerance for floating point comparisons to identify exact fits.
    exact_fit_mask = can_fit_mask & (np.abs(bins_remain_cap - item) < epsilon_small)
    priorities[exact_fit_mask] = EXACT_FIT_PRIORITY

    # Identify bins that can fit the item but are not exact fits.
    # These are candidates for near or general fits.
    fitting_bin_indices = np.where(can_fit_mask & ~exact_fit_mask)[0]

    # Proceed only if there are bins that can fit but are not exact fits.
    if len(fitting_bin_indices) > 0:
        # Calculate the remaining capacity in these fitting bins after placing the item.
        remaining_capacities_after_placement = bins_remain_cap[fitting_bin_indices] - item

        # 2. Categorize fitting bins into "Near Fits" and "General Fits".
        # A bin is a "near fit" if the remaining capacity is below the defined threshold.
        near_fit_criteria = remaining_capacities_after_placement < NEAR_FIT_THRESHOLD
        
        # Get indices relative to the 'fitting_bin_indices' array.
        near_fit_indices_in_subset = np.where(near_fit_criteria)[0]
        general_fit_indices_in_subset = np.where(~near_fit_criteria)[0]

        # 3. Assign priorities for General Fits.
        # These bins have sufficient capacity but leave a relatively large amount of space.
        general_fit_actual_indices = fitting_bin_indices[general_fit_indices_in_subset]
        priorities[general_fit_actual_indices] = GENERAL_FIT_PRIORITY

        # 4. Assign priorities for Near Fits using scaled inverse capacity and softmax.
        near_fit_actual_indices = fitting_bin_indices[near_fit_indices_in_subset]
        
        # Process only if there are actual near fits to evaluate.
        if len(near_fit_actual_indices) > 0:
            near_fit_remaining_caps = remaining_capacities_after_placement[near_fit_indices_in_subset]

            # Calculate raw scores: inverse of remaining capacity. Higher score for smaller capacity.
            # Add epsilon_small for numerical stability to avoid division by zero.
            near_fit_raw_scores = 1.0 / (near_fit_remaining_caps + epsilon_small)

            # Apply stable scaling by capping the raw scores at MAX_INV_SCORE.
            # This limits the influence of extremely small remaining capacities.
            clamped_scores = np.minimum(near_fit_raw_scores, MAX_INV_SCORE)

            # Use temperature-controlled softmax to generate exploration-driven weights
            # for the near-fit bins. This allows some preference for sub-optimal near fits.
            if len(clamped_scores) > 0:
                # Shift scores by the maximum value before exponentiation for numerical stability.
                # This prevents overflow when exp() is applied to large positive numbers.
                max_clamped_score = np.max(clamped_scores)
                shifted_clamped_scores = clamped_scores - max_clamped_score
                
                # Apply the exponential function, controlled by the temperature.
                exp_scores = np.exp(shifted_clamped_scores / TEMPERATURE)
                sum_exp_scores = np.sum(exp_scores)

                # Calculate softmax probabilities. If sum_exp_scores is zero (e.g., all inputs were -inf),
                # fall back to a uniform distribution.
                if sum_exp_scores > 0:
                    softmax_probabilities = exp_scores / sum_exp_scores
                else:
                    # Fallback: Treat all near fits equally if softmax calculation fails.
                    softmax_probabilities = np.ones_like(clamped_scores) / len(clamped_scores)
                
                # Combine the base score for near fits with the scaled softmax probabilities.
                # This assigns a priority that is higher than general fits and reflects
                # the exploration strategy among near fits.
                scaled_near_fit_priorities = NEAR_FIT_BASE_SCORE + softmax_probabilities * NEAR_FIT_SCALE
                priorities[near_fit_actual_indices] = scaled_near_fit_priorities
            # else: If there are near fit indices but somehow clamped_scores is empty,
            # assign just the base score as a safe default. This case is unlikely.
            # The existing logic implicitly handles this by not updating priorities if the block is skipped.

    return priorities
``````python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Prioritizes bins for the online Bin Packing Problem with enhanced strategies,
    focusing on stable scaling of inverse remaining capacity and exploration
    among near fits.

    Heuristic Strategy:
    1. Exact Fits: Highest priority, ensuring perfect matches are always chosen.
    2. Near Fits: Bins where the remaining capacity after placement is small
       (below a defined threshold). These are prioritized based on a scaled
       inverse of the remaining capacity, controlled by a temperature parameter
       for exploration among the best near fits. The inverse capacity is capped
       for stability.
    3. General Fits: Bins where the item fits, but the remaining capacity is
       not considered 'near'. These receive a low positive priority.
    4. Unfittable: Bins where the item cannot fit, assigned negative infinity.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A numpy array containing the remaining capacity of each bin.

    Returns:
        A numpy array of the same size as bins_remain_cap, where each element
        represents the priority score for placing the item in the corresponding bin.
    """
    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)

    # --- Configuration Parameters ---
    # Priority for exact fits. Must be the highest.
    EXACT_FIT_PRIORITY = 100.0
    # Base score for near fits before adding the exploration-driven component.
    # Ensures near fits are preferred over general fits.
    NEAR_FIT_BASE_SCORE = 30.0
    # Scaling factor for the exploration component derived from softmax.
    # This determines how much the exploration aspect influences the priority.
    NEAR_FIT_SCALE = 60.0
    # Priority for bins that fit but are not considered "near".
    GENERAL_FIT_PRIORITY = 1.0
    # Threshold to classify a bin as a "near fit". Bins with remaining capacity
    # less than this value after placement are considered near fits.
    NEAR_FIT_THRESHOLD = 5.0  # e.g., if remaining capacity < 5.0 units
    # Maximum value for the scaled inverse remaining capacity. This helps stabilize
    # priorities and prevents extremely high scores for very small remaining capacities,
    # making the scaling more robust.
    MAX_INV_SCORE = 5.0
    # Temperature parameter for the softmax function. Higher temperature leads to
    # more uniform probabilities (more exploration), lower temperature leads to
    # more skewed probabilities (more exploitation of the best near fit).
    TEMPERATURE = 0.7
    # Small constant to prevent division by zero in inverse calculations.
    epsilon_small = 1e-9

    # --- Bin Suitability Check ---
    # Create a mask for bins that have enough capacity for the item.
    can_fit_mask = bins_remain_cap >= item

    # If no bin can fit the item, return initial priorities.
    if not np.any(can_fit_mask):
        return priorities

    # --- Assign Priorities ---

    # 1. Exact Fits: Prioritize bins where the item perfectly fills the remaining capacity.
    # Use a small tolerance for floating point comparisons to identify exact fits.
    exact_fit_mask = can_fit_mask & (np.abs(bins_remain_cap - item) < epsilon_small)
    priorities[exact_fit_mask] = EXACT_FIT_PRIORITY

    # Identify bins that can fit the item but are not exact fits.
    # These are candidates for near or general fits.
    fitting_bin_indices = np.where(can_fit_mask & ~exact_fit_mask)[0]

    # Proceed only if there are bins that can fit but are not exact fits.
    if len(fitting_bin_indices) > 0:
        # Calculate the remaining capacity in these fitting bins after placing the item.
        remaining_capacities_after_placement = bins_remain_cap[fitting_bin_indices] - item

        # 2. Categorize fitting bins into "Near Fits" and "General Fits".
        # A bin is a "near fit" if the remaining capacity is below the defined threshold.
        near_fit_criteria = remaining_capacities_after_placement < NEAR_FIT_THRESHOLD
        
        # Get indices relative to the 'fitting_bin_indices' array.
        near_fit_indices_in_subset = np.where(near_fit_criteria)[0]
        general_fit_indices_in_subset = np.where(~near_fit_criteria)[0]

        # 3. Assign priorities for General Fits.
        # These bins have sufficient capacity but leave a relatively large amount of space.
        general_fit_actual_indices = fitting_bin_indices[general_fit_indices_in_subset]
        priorities[general_fit_actual_indices] = GENERAL_FIT_PRIORITY

        # 4. Assign priorities for Near Fits using scaled inverse capacity and softmax.
        near_fit_actual_indices = fitting_bin_indices[near_fit_indices_in_subset]
        
        # Process only if there are actual near fits to evaluate.
        if len(near_fit_actual_indices) > 0:
            near_fit_remaining_caps = remaining_capacities_after_placement[near_fit_indices_in_subset]

            # Calculate raw scores: inverse of remaining capacity. Higher score for smaller capacity.
            # Add epsilon_small for numerical stability to avoid division by zero.
            near_fit_raw_scores = 1.0 / (near_fit_remaining_caps + epsilon_small)

            # Apply stable scaling by capping the raw scores at MAX_INV_SCORE.
            # This limits the influence of extremely small remaining capacities.
            clamped_scores = np.minimum(near_fit_raw_scores, MAX_INV_SCORE)

            # Use temperature-controlled softmax to generate exploration-driven weights
            # for the near-fit bins. This allows some preference for sub-optimal near fits.
            if len(clamped_scores) > 0:
                # Shift scores by the maximum value before exponentiation for numerical stability.
                # This prevents overflow when exp() is applied to large positive numbers.
                max_clamped_score = np.max(clamped_scores)
                shifted_clamped_scores = clamped_scores - max_clamped_score
                
                # Apply the exponential function, controlled by the temperature.
                exp_scores = np.exp(shifted_clamped_scores / TEMPERATURE)
                sum_exp_scores = np.sum(exp_scores)

                # Calculate softmax probabilities. If sum_exp_scores is zero (e.g., all inputs were -inf),
                # fall back to a uniform distribution.
                if sum_exp_scores > 0:
                    softmax_probabilities = exp_scores / sum_exp_scores
                else:
                    # Fallback: Treat all near fits equally if softmax calculation fails.
                    softmax_probabilities = np.ones_like(clamped_scores) / len(clamped_scores)
                
                # Combine the base score for near fits with the scaled softmax probabilities.
                # This assigns a priority that is higher than general fits and reflects
                # the exploration strategy among near fits.
                scaled_near_fit_priorities = NEAR_FIT_BASE_SCORE + softmax_probabilities * NEAR_FIT_SCALE
                priorities[near_fit_actual_indices] = scaled_near_fit_priorities
            # else: If there are near fit indices but somehow clamped_scores is empty,
            # priorities for these indices will remain -inf, which is a safe default.

    return priorities
```
