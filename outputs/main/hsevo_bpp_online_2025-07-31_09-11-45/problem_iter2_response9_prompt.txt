{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins using a radiation-inspired 'decay balance' heuristic.\n    Combines tight packing with capacity preservation using exponential terms.\"\"\"\n    fit_mask = bins_remain_cap >= item\n    remaining_space = bins_remain_cap - item\n    \n    # Favor bins where item fills radioactive \"half-life\" proportions\n    decay_factor = np.exp(-5 * remaining_space)  # Strong preference for near-exact fits\n    capacity_preservation = np.sqrt(bins_remain_cap)  # Preserve mid-sized capacities\n    \n    scores = np.where(\n        fit_mask,\n        decay_factor * (1 + capacity_preservation),\n        -np.inf\n    )\n    \n    # Prefer creating new bins for items >0.5 capacity to prevent fragmentation\n    if item > 0.5:\n        scores = np.where(fit_mask, scores * 0.2, scores)\n    \n    return scores\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritize bins where the item fills a large portion of remaining space.\n       Uses inverse of post-placement remaining capacity with non-linear scaling.\"\"\"\n    # Calculate remaining capacity after potential placement\n    remaining_after = bins_remain_cap - item\n    \n    # Create validity mask and initialize scores\n    valid_mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    \n    # For valid bins, calculate dynamic priority score combining multiple factors\n    valid_indices = valid_mask.nonzero()[0]\n    if valid_indices.size > 0:\n        # Use inverse square of remaining space to strongly favor tight fits\n        remaining_valid = remaining_after[valid_indices]\n        density_boost = item / (remaining_valid + 1e-9)  # Base density score\n        capacity_penalty = 0.1 * bins_remain_cap[valid_indices]  # Prefer smaller bins\n        scores[valid_indices] = density_boost - capacity_penalty + np.sqrt(item)\n        \n    return scores\n\n### Analyze & experience\n- Comparing 1st (quantum-inspired) vs 20th (radioactive decay): The best heuristic combines harmonic efficiency (1/remaining) with residual potential (\u221acapacity) via geometric mean, creating balanced packing/flexibility tradeoffs. The worst uses multiplicative terms (tightness \u00d7 utilization \u00d7 decay) that over-penalize mid-range bins.  \n\n2nd (celestial harmony) vs 19th: 2nd's golden ratio weighting (0.618/0.382) between tight fits and future flexibility shows mathematically informed balance, while 19th's exponential decay bonus lacks theoretical grounding and overfits to near-exact matches.  \n\n4th (gravitational) vs 16th (black hole): Both use inverse-square prioritization, but 4th cleanly separates exact fits from others, while 16th introduces unstable density penalties that disrupt ordering.  \n\n13th (exact fit + reciprocal) vs 10th: 13th handles floating-point tolerance properly (np.isclose) and scales priorities appropriately, while 10th's 1e9 boost creates artificial score cliffs.\n- \n<think>\nOkay, let's tackle this. The user wants to redefine 'Current self-reflection' for designing better heuristics, making sure to avoid the ineffective aspects mentioned. The existing effective heuristics include things like balancing tight packing with flexibility, handling floating precision, smooth gradients, multiplicative factors for independence, and using physics/math metaphors.\n\nFirst, I need to identify what's missing in the current self-reflection. The ineffective part isn't listed, but maybe the current points could be too rigid. For example, using only multiplicative combinations when factors are independent might limit some scenarios where a different combination could work better. Also, relying solely on physics/math metaphors might exclude other useful analogies that are still interpretable.\n\nNext, the advice should build on the current strengths but address potential gaps. Maybe emphasize dynamic adaptation instead of fixed ratios. Also, ensuring that even if factors aren't perfectly independent, there's a way to combine them without forcing multiplication. Testing heuristic robustness under varying conditions could be another point. And while metaphors are good, grounding them in empirical validation would make them more reliable.\n\nAvoiding things like static ratios, over-reliance on independence assumptions, neglecting empirical checks, and forcing metaphors that don't fit. The explanation should tie these together, showing how the new advice leads to more adaptable and robust heuristics without the pitfalls of the old approach.\n</think>\n\n**Keywords**: Dynamic adaptation, empirical validation, dimensional synergy, constraint robustness  \n**Advice**: 1) Replace fixed ratios with *adaptive thresholds* that evolve with problem scale 2) Test heuristics under *extreme precision edge cases* (e.g., near-zero values) 3) Quantify independence between factors before choosing combination operators 4) Use metaphors only when they enable *actionable geometric intuition*.  \n**Avoid**: Static \"golden ratios\", blind multiplicative blending, assuming dimension independence without proof, poetic analogies without operational mappings.  \n**Explanation**: Enhances adaptability through responsive thresholds while maintaining rigor via stress-testing and explicit dependency analysis. Metaphors become tools rather than dogma when tied to verifiable spatial reasoning.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}