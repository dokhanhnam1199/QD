<think>
Okay, I need to create an improved priority function for the online bin packing problem by combining elements from the better code (priority_v0) and the worse code (priority_v1), while incorporating the analysis provided. Let me start by understanding what made priority_v0 better and priority_v1 worse.

From the analysis, the best heuristics use a focused approach with calibrated constants and avoid unnecessary complexity. The worse code had things like hyperbolic tangent, logistic transforms, and quantum-inspired terms which added noise. So, I should take the good parts from v0 and maybe avoid the overcomplicated parts from v1.

Looking at priority_v0: It calculates fill efficiency (item/bin capacity) and residual potential (sqrt of remaining space). Then combines them with a geometric mean (sqrt of product) for synergy. Also, adds a bonus for exact fits. Uses epsilons to prevent division by zero and near-zero comparisons.

Priority_v1 had dynamic residual valuation with tanh, anti-fragility terms, and entanglement of different regimes. But these added complexity without benefit. However, maybe some elements could be useful if simplified.

The analysis mentioned that the best heuristics balance simplicity with precision-tuned components. So, perhaps I can take the core idea from v0 and enhance it with some elements from v1, but simplified.

Wait, the analysis said that the better code uses geometric mean of fill efficiency and residual potential. The worse code had things like "quantum-inspired superposition" which didn't help. So, the key is to stick with the geometric mean approach but maybe refine the components.

Looking at the parameters in v0: eps_fill, eps_residual, atol_perfection, and perfection_bonus_weight. These are calibrated constants. The worse code had more arbitrary terms like log1p(item) in exact fit detection.

So, the improved version should focus on the synergy between fill efficiency and residual potential, with well-tuned constants, and avoid overcomplicating with extra terms.

Wait, the analysis also mentioned that in the better code, the exact fit bonus is applied with a weight. The worse code had an exact fit detection with log1p(item), which might not be as effective.

So, perhaps the improved version can keep the exact fit bonus but adjust how it's calculated. Or maybe use a dynamic bonus based on the item size. For example, larger items that exactly fit might be more valuable to prioritize, as they are harder to place later.

Another point from the analysis: the best heuristics use geometric mean for synergy, while worse ones use additive combinations. So, the improved function should maintain the multiplicative combination.

Wait, in v0, the synergy_score is the sqrt of fill_efficiency * residual_potential. That's the geometric mean. So, that's good.

What about the residual_potential? In v0, it's sqrt(remaining_after + eps_residual). Maybe using a different function here could help. For example, in v1, they used tanh(residual_space / item), but that's part of the worse approach. However, maybe a different residual potential calculation could be better.

Alternatively, perhaps the residual potential could be adjusted based on the item size. For instance, if the remaining space after placing the item is large relative to the item size, that's better for future flexibility. So, maybe residual_potential could be something like sqrt(remaining_after / item) instead of sqrt(remaining_after). But that's speculative.

Alternatively, the residual_potential in v0 is sqrt(remaining_after), which values larger remaining spaces more, but perhaps in a non-linear way. Maybe that's sufficient.

Another idea from the analysis: perfection_bonus_weight is a calibrated constant. In v0, it's a fixed weight. Maybe this could be adjusted based on the item size. For example, larger items that exactly fit should get a higher bonus, as they are more critical to place correctly.

But how to implement that? The exact fit bonus in v0 is a fixed weight multiplied by a boolean (whether it's an exact fit). If the item is large, maybe the bonus should be higher. So, perhaps the perfection_bonus could be item * perfection_bonus_weight instead of a fixed weight. But that might need testing.

Alternatively, the exact fit bonus could be scaled by the item size, as placing a larger item exactly is more beneficial. So, perfection_bonus = np.isclose(...) * item * some_weight.

But how to integrate that without overcomplicating? The analysis warns against overparameterization, so maybe keep it simple. However, the original v0 uses a fixed weight. Maybe adjusting the bonus based on item size could be beneficial.

Another aspect: the worse code (v1) had an 'exact_fit' term that included log1p(item), which adds a dependency on the item size. The analysis said that the best heuristics have calibrated constants, so perhaps incorporating a dynamic bonus based on item size could be better, but needs to be done carefully.

So, perhaps the perfection_bonus in v0 can be modified to include the item size. For example:

perfection_bonus = np.isclose(...) * (perfection_bonus_weight * item)

This way, larger items that exactly fit get a higher bonus, encouraging their placement in bins where they fit exactly.

But how to test this? Since we can't test, we have to rely on the analysis given. The analysis mentioned that the better code uses a perfection_bonus with a weight, which works. So maybe enhancing that part by making the bonus proportional to the item size could be better.

Another point from the analysis: the worse code had entropic smoothing which didn't help. So, avoid such terms.

Putting this together, the improved priority function could:

1. Keep the synergy_score as the geometric mean of fill_efficiency and residual_potential.

2. Adjust the residual_potential calculation, perhaps by using a different function, but keeping it simple.

3. Modify the exact fit bonus to be proportional to the item size, to prioritize larger exact fits.

4. Ensure all constants are well-calibrated, perhaps adjusting the epsilons and weights.

Wait, in v0, the fill_efficiency is item / (bin_remain_cap + eps_fill). The eps_fill is a small epsilon to prevent division by zero. Similarly for residual_potential.

So, the key components are:

- fill_efficiency: how well the item fills the bin.

- residual_potential: how much space is left for future items.

- exact_fit bonus: encourage placing items that exactly fill the bin.

The improved version could enhance these components.

Another idea: the residual_potential in v0 is sqrt(remaining_after). Maybe using a cube root or another function could better balance future flexibility. But sqrt is a good balance between linear and quadratic.

Alternatively, the residual_potential could be remaining_after raised to a power less than 1, like 0.5 (sqrt), which is what v0 does. So that's probably fine.

Another aspect: in v0, the synergy_score is the sqrt of the product of fill_efficiency and residual_potential. Maybe using a weighted geometric mean could allow tuning the balance between the two factors. For example, (fill^a * residual^b)^(1/(a+b)), where a and b are weights. But that adds parameters. However, the analysis mentioned that the best heuristics avoid overparameterization, so maybe keeping a=1 and b=1 (geometric mean) is better.

Alternatively, the analysis said that the better code uses a geometric mean, which is a multiplicative combination, so that's good.

So, the main structure of v0 is solid. The improvements could come from:

- Enhancing the exact fit bonus by making it dependent on the item size.

- Adjusting the residual_potential calculation to better reflect future flexibility.

Wait, in the worse code (v1), they used anti_fragility = 1 - exp(-5 * residual_space), which penalizes near-zero residuals. But the analysis said that such terms are overcomplicated. However, maybe a simplified version could help. For example, adding a small penalty for very small residuals to avoid leaving bins with tiny remaining spaces, which are hard to fill.

But how to do that without overcomplicating? In v0, the residual_potential is sqrt(remaining_after), which naturally values larger residuals more. However, if a bin has a very small remaining_after (but not zero), it's not useful. So, perhaps adding a penalty for small residuals could help.

But that would require another term. However, the analysis warns against adding too many terms. So, perhaps it's better to leave it as is.

Alternatively, adjust the residual_potential to be remaining_after^gamma, where gamma is a parameter. For example, gamma=0.5 (sqrt) as in v0. Maybe a slightly higher gamma would value larger residuals more. But this is speculative.

Alternatively, the residual_potential could be remaining_after + a small term, but v0 already adds eps_residual.

Hmm. Maybe the existing approach is sufficient, and the improvement comes from adjusting the exact fit bonus.

So, let's try modifying the exact fit bonus to be proportional to the item size. In v0, the bonus is a fixed weight multiplied by whether it's an exact fit. Changing it to item * weight would make larger exact fits get a higher bonus.

So, in code:

perfection_bonus = np.isclose(remaining_after, 0, atol=atol_perfection) * item * perfection_bonus_weight

This way, larger items that exactly fit get a higher bonus, which could lead to better packing.

Additionally, perhaps adjust the synergy_score. For example, instead of geometric mean, use a weighted product. But the analysis says that the geometric mean is good.

Another idea from the analysis: the best heuristics use calibrated constants. So, perhaps the parameters (eps_fill, eps_residual, etc.) can be adjusted based on the item size or other factors. But that might complicate things.

Alternatively, the parameters can remain as they are, but the exact fit bonus is made dynamic.

Putting this all together, the improved priority_v2 would:

- Calculate fill_efficiency as item / (bin_remain_cap + eps_fill)

- Calculate residual_potential as sqrt(remaining_after + eps_residual)

- Calculate synergy_score as geometric mean (sqrt of product) of fill_efficiency and residual_potential

- Add a perfection_bonus that is item * weight if it's an exact fit

- Use valid_mask to set invalid bins to -inf

So, the code would look similar to v0, but with the perfection_bonus scaled by the item.

Testing this approach: for an exact fit, the bonus is higher for larger items, encouraging the algorithm to prioritize placing large items that fit perfectly, which can reduce the number of bins used.

Another consideration: the parameters (epsilons, atol, weight) might need adjustment. But since the user can't test, we'll keep them as in v0 but modify the perfection_bonus.

So, the code for priority_v2 would be:

def priority_v2(...):
    remaining_after = bins_remain_cap - item
    valid_mask = remaining_after >=0

    fill_efficiency = item / (bins_remain_cap + eps_fill)
    residual_potential = sqrt(remaining_after + eps_residual)
    synergy_score = sqrt(fill_efficiency * residual_potential)
    perfection_bonus = np.isclose(...) * item * perfection_bonus_weight
    scores = np.where(valid_mask, synergy_score + perfection_bonus, -inf)
    return scores

This way, the main change is making the perfection bonus proportional to the item size.

Additionally, perhaps adjust the residual_potential. For example, using a cube root instead of sqrt. But that's a parameter change. However, the analysis mentioned that the better code uses sqrt, so maybe keeping it is better.

Another possible adjustment: in v0, the residual_potential is sqrt(max(remaining_after,0) + eps_residual). The max is redundant if remaining_after is already >=0 due to valid_mask. But in the code, remaining_after can be negative in invalid bins, but those are masked out. So, the max might not be necessary, but it's safe to keep.

So, the final code would incorporate the item-scaled perfection bonus and keep the rest similar to v0, with possible parameter adjustments.

Now, writing the code with these changes, including the docstring.
</think>

```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Balances tight packing with residual utility using geometric mean synergy
    and item-scaled exact fit rewards. Calibrated epsilons prevent instabilities."""
    
    remaining_after = bins_remain_cap - item
    valid_mask = remaining_after >= 0
    
    # Core synergy components
    fill_efficiency = item / (bins_remain_cap + 3.2e-7)  # Prevent zero-division
    residual_potential = np.sqrt(np.maximum(remaining_after, 0) + 1.8e-7)  # Future flexibility
    synergy_score = np.sqrt(fill_efficiency * residual_potential)  # Geometric balance
    
    # Dynamic exact fit reward proportional to item size
    exact_fit_bonus = np.isclose(remaining_after, 0, atol=4.5e-6) * item * 2.65
    
    scores = np.where(valid_mask, synergy_score + exact_fit_bonus, -np.inf)
    return scores
```
