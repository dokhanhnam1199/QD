{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Dynamic multi-objective heuristic with adaptive weight balancing and residual \n    space forecasting. Features three key innovations:\n    1. Context-aware synergy: Weighted power mean adapts to item size\n    2. Predictive residual scoring: Cube root + linear combo handles multiple future item scenarios\n    3. Precision-gradient exact fit: Smooth bonus proportional to item significance\"\"\"\n    \n    remaining_after = bins_remain_cap - item\n    valid_mask = remaining_after >= 0\n    \n    # Adaptive normalization base using item-aware sigmoid\n    norm_base = 0.5 + 1/(1 + np.exp(-10*(item-0.2)))\n    \n    # Core packing efficiency with non-linear scaling\n    fill_ratio = item / (bins_remain_cap + 1e-12)\n    fill_power = np.where(bins_remain_cap > 0.5, fill_ratio**0.7, fill_ratio**1.2)\n    \n    # Residual space value estimator with multi-horizon forecasting\n    residual_space = np.maximum(remaining_after, 0)\n    residual_value = (residual_space**0.4 + 0.6*residual_space**0.8) / (1.6 + 0.1*item)\n    \n    # Dynamic weight interpolation between objectives\n    weight = np.clip(item * 1.8, 0.2, 0.8)\n    synergy = (fill_power**weight) * (residual_value**(1-weight)) * norm_base\n    \n    # Precision-aware exact fit with smooth gradient\n    near_exact = np.exp(-(remaining_after**2)/(1e-7 + 0.01*item**2))\n    exact_bonus = near_exact * (0.5 + 1.5*item)  # Scales with item importance\n    \n    # Final score composition\n    scores = np.where(valid_mask, synergy + exact_bonus, -np.inf)\n    \n    return scores\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Balances tight packing with future flexibility using:\n    1. Exponential decay for near-exact fits\n    2. Square root residual capacity preservation\n    3. Dynamic large-item penalty with exponential scaling\"\"\"\n    \n    fit_mask = bins_remain_cap >= item\n    remaining_after = bins_remain_cap - item\n    \n    # Base score combines tightness and preservation components\n    tightness = np.exp(-6 * remaining_after)  # Strong preference for <0.1 residual\n    preservation = np.sqrt(np.maximum(remaining_after, 1e-9))  # Protect usable capacity\n    scores = np.where(fit_mask, tightness * preservation, -np.inf)\n    \n    # Dynamic penalty for items that could fragment large bins\n    if item > 0.3:  # Applies penalty gradient starting at lower threshold\n        penalty_scale = np.exp(-4 * (bins_remain_cap - item))  # Penalize bins that would leave <0.25 capacity\n        scores = np.where(fit_mask, scores * (1 - 0.6*penalty_scale), scores)\n    \n    return scores\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic uses a focused geometric mean of fill efficiency and residual potential with calibrated constants, while the worst introduces unnecessary complexity (quantum-inspired terms, hyperbolic transforms) that likely introduces noise. (2nd vs 19th) shows duplicated simple synergy scoring outperforms topological scoring with entropic smoothing. (8th vs 13th) reveals adaptive weight balancing and residual forecasting can help, but requires careful calibration to avoid overparameterization. (7th vs 17th) demonstrates that geometric mean of tightness/utilization works better than exponential boosts with arbitrary scaling. Overall: Top heuristics balance simplicity with precision-tuned components, while lower-ranked ones suffer from either under-calibration or overengineering.\n- \n<think>\nOkay, let's tackle this. The user wants to redefine 'Current self-reflection' for designing better heuristics, avoiding the ineffective aspects mentioned. First, I need to understand the existing points in Current self-reflection. They mention things like synergistic balance using multiplicative means, precision handling, calibrated epsilon, avoiding ungrounded complexity.\n\nThe effective heuristics list includes balancing packing with flexibility, rigorous floating precision, smooth gradients, multiplicative factors for independence, and using physics/math metaphors. The ineffective part is empty, so maybe the user wants to ensure that the new reflection doesn't include any of the common pitfalls that weren't listed here.\n\nSo, the task is to take the current points and rephrase or enhance them, making sure to avoid ineffective elements. The user probably wants concise, actionable advice. Let me break down each part.\n\nFirst, the original points stress multiplicative balance. Maybe emphasize that multiplicative combination should be only when variables are independent. Also, calibrated epsilon constants for numerical stability\u2014so numerical rigor is key. Avoid quantum/entropic metaphors, so stick to grounded math.\n\nThe advice should include using geometric means for balance, handling precision with care, smooth gradients over abrupt changes, and using physics-based models. Avoid additive features, ungrounded metaphors, ignoring precision, and independent variables combined additively.\n\nWait, the user's example response has keywords, advice, avoid, explanation. So each bullet should have those four parts. Let me structure each point accordingly. Need four bullet points. Let me think of the main points from the current reflection and translate them into the required format, ensuring they avoid the ineffective aspects (even though ineffective wasn't specified, but perhaps common pitfalls in heuristics design).\n\nFirst keyword could be Multiplicative Balance. Advice: Use geometric means for competing objectives. Avoid additive combinations. Explanation: Geometric means prevent dominance by single factors and maintain proportional trade-offs.\n\nSecond keyword: Numerical Rigor. Advice: Implement precision-aware epsilon calibration. Avoid ignoring floating-point errors. Explanation: Ensures stability in calculations and prevents underflow/overflow.\n\nThird: Smooth Transitions. Advice: Design gradual score gradients. Avoid abrupt threshold-based boosts. Explanation: Smoothness allows for nuanced optimization paths, avoiding local minima traps.\n\nFourth: Grounded Metaphors. Advice: Use physics/math principles. Avoid biological/cosmic analogies. Explanation: Physics-based models provide verifiable, interpretable relationships.\n\nCheck if these points cover the original current reflection and avoid the ineffective parts. The original mentions avoiding ungrounded complexity (like quantum metaphors), so Grounded Metaphors bullet addresses that. Also, precision handling and multiplicative balance are covered. The advice points should each have a keyword, advice, what to avoid, and a brief explanation. That should fit the user's request.\n</think>\n\n- **Keywords**: Multiplicative balance, Numerical rigor, Smooth transitions, Grounded metaphors  \n- **Advice**: 1) Combine independent objectives via geometric means 2) Embed precision-aware error bounds 3) Use gradual scoring to enable adaptive tradeoffs 4) Anchor in verifiable physics (e.g., spring models, friction coefficients).  \n- **Avoid**: Additive objective stacking, arbitrary \"quantum-inspired\" terms, hard thresholds, unvalidated constants.  \n- **Explanation**: Geometric balancing preserves proportionality between dimensions, numerical rigor prevents instability, smooth gradients enable robust optimization, and physics metaphors ensure testable causality over mystical analogies.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}