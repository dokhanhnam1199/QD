{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, node degree penalty, sparsification, and cycle avoidance for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n    small_constant = 1e-6\n    avg_distance_weight = 0.5\n\n    node_degrees = np.sum(distance_matrix < (avg_distance * 1.5), axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = (node_degrees[i] + node_degrees[j])**1.5 # Exponent for stronger penalty\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance) * avg_distance_weight))\n\n                #Dynamically adjust degree penalty based on local density\n                local_neighbors = np.sum(distance_matrix[i, :] < (distance_matrix[i, j] * 1.2)) + np.sum(distance_matrix[j, :] < (distance_matrix[j, i] * 1.2))\n                heuristics[i,j] /= (1 + (local_neighbors * 0.1))\n\n\n            else:\n                heuristics[i, j] = 0\n\n    # Sparsification: keep only top edges for each node\n    for i in range(n):\n        row = heuristics[i, :]\n        threshold = np.percentile(row[row > 0], 75) # Only consider top edges\n        heuristics[i, row < threshold] = 0\n\n    # Further sparsification based on global distance distribution\n    distance_threshold = np.percentile(distance_matrix[distance_matrix != 0], 25)\n    for i in range(n):\n        for j in range(n):\n          if distance_matrix[i,j] > distance_threshold and heuristics[i,j] > 0:\n             heuristics[i,j] *= 0.5\n\n    return heuristics\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, node degree penalty, sparsification, and cycle avoidance for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n    small_constant = 1e-6\n    avg_distance_weight = 0.5\n\n    node_degrees = np.sum(distance_matrix < (avg_distance * 1.5), axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = (node_degrees[i] + node_degrees[j])**1.5 # Exponent for stronger penalty\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance) * avg_distance_weight))\n\n                #Dynamically adjust degree penalty based on local density\n                local_neighbors = np.sum(distance_matrix[i, :] < (distance_matrix[i, j] * 1.2)) + np.sum(distance_matrix[j, :] < (distance_matrix[j, i] * 1.2))\n                heuristics[i,j] /= (1 + (local_neighbors * 0.1))\n\n\n            else:\n                heuristics[i, j] = 0\n\n    # Sparsification: keep only top edges for each node\n    for i in range(n):\n        row = heuristics[i, :]\n        threshold = np.percentile(row[row > 0], 75) # Only consider top edges\n        heuristics[i, row < threshold] = 0\n\n    # Further sparsification based on global distance distribution\n    distance_threshold = np.percentile(distance_matrix[distance_matrix != 0], 25)\n    for i in range(n):\n        for j in range(n):\n          if distance_matrix[i,j] > distance_threshold and heuristics[i,j] > 0:\n             heuristics[i,j] *= 0.5\n\n    return heuristics\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, node degree penalty, sparsification, and cycle avoidance for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n    small_constant = 1e-6\n    avg_distance_weight = 0.5\n\n    node_degrees = np.sum(distance_matrix < (avg_distance * 1.5), axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = (node_degrees[i] + node_degrees[j])**1.5 # Exponent for stronger penalty\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance) * avg_distance_weight))\n\n                #Dynamically adjust degree penalty based on local density\n                local_neighbors = np.sum(distance_matrix[i, :] < (distance_matrix[i, j] * 1.2)) + np.sum(distance_matrix[j, :] < (distance_matrix[j, i] * 1.2))\n                heuristics[i,j] /= (1 + (local_neighbors * 0.1))\n\n\n            else:\n                heuristics[i, j] = 0\n\n    # Sparsification: keep only top edges for each node\n    for i in range(n):\n        row = heuristics[i, :]\n        threshold = np.percentile(row[row > 0], 75) # Only consider top edges\n        heuristics[i, row < threshold] = 0\n\n    # Further sparsification based on global distance distribution\n    distance_threshold = np.percentile(distance_matrix[distance_matrix != 0], 25)\n    for i in range(n):\n        for j in range(n):\n          if distance_matrix[i,j] > distance_threshold and heuristics[i,j] > 0:\n             heuristics[i,j] *= 0.5\n\n    return heuristics\n\n[Heuristics 4th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, node degree penalty, sparsification, and cycle avoidance for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n    small_constant = 1e-6\n    avg_distance_weight = 0.5\n\n    node_degrees = np.sum(distance_matrix < (avg_distance * 1.5), axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = (node_degrees[i] + node_degrees[j])**1.5 # Exponent for stronger penalty\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance) * avg_distance_weight))\n\n                #Dynamically adjust degree penalty based on local density\n                local_neighbors = np.sum(distance_matrix[i, :] < (distance_matrix[i, j] * 1.2)) + np.sum(distance_matrix[j, :] < (distance_matrix[j, i] * 1.2))\n                heuristics[i,j] /= (1 + (local_neighbors * 0.1))\n\n\n            else:\n                heuristics[i, j] = 0\n\n    # Sparsification: keep only top edges for each node\n    for i in range(n):\n        row = heuristics[i, :]\n        threshold = np.percentile(row[row > 0], 75) # Only consider top edges\n        heuristics[i, row < threshold] = 0\n\n    # Further sparsification based on global distance distribution\n    distance_threshold = np.percentile(distance_matrix[distance_matrix != 0], 25)\n    for i in range(n):\n        for j in range(n):\n          if distance_matrix[i,j] > distance_threshold and heuristics[i,j] > 0:\n             heuristics[i,j] *= 0.5\n\n    return heuristics\n\n[Heuristics 5th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, node degree penalty, sparsification, and cycle avoidance for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n    small_constant = 1e-6\n    avg_distance_weight = 0.5\n\n    node_degrees = np.sum(distance_matrix < (avg_distance * 1.5), axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = (node_degrees[i] + node_degrees[j])**1.5 # Exponent for stronger penalty\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance) * avg_distance_weight))\n\n                #Dynamically adjust degree penalty based on local density\n                local_neighbors = np.sum(distance_matrix[i, :] < (distance_matrix[i, j] * 1.2)) + np.sum(distance_matrix[j, :] < (distance_matrix[j, i] * 1.2))\n                heuristics[i,j] /= (1 + (local_neighbors * 0.1))\n\n\n            else:\n                heuristics[i, j] = 0\n\n    # Sparsification: keep only top edges for each node\n    for i in range(n):\n        row = heuristics[i, :]\n        threshold = np.percentile(row[row > 0], 75) # Only consider top edges\n        heuristics[i, row < threshold] = 0\n\n    # Further sparsification based on global distance distribution\n    distance_threshold = np.percentile(distance_matrix[distance_matrix != 0], 25)\n    for i in range(n):\n        for j in range(n):\n          if distance_matrix[i,j] > distance_threshold and heuristics[i,j] > 0:\n             heuristics[i,j] *= 0.5\n\n    return heuristics\n\n[Heuristics 6th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, avg_distance_weight: float = 0.18749559158160367, degree_penalty_exponent: float = 1.8228003324542028, sparsification_percentile: float = 72.01995321459675, avg_distance_threshold_factor: float = 1.2582795525251842, small_constant: float = 1.7254114851896175e-07) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, node degree penalty, and sparsification for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n\n    node_degrees = np.sum(distance_matrix < (avg_distance * avg_distance_threshold_factor), axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = (node_degrees[i] + node_degrees[j])**degree_penalty_exponent # Exponent for stronger penalty\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance) * avg_distance_weight))\n\n            else:\n                heuristics[i, j] = 0\n\n    # Sparsification: keep only top edges for each node\n    for i in range(n):\n        row = heuristics[i, :]\n        threshold = np.percentile(row[row > 0], sparsification_percentile) # Only consider positive values\n        heuristics[i, row < threshold] = 0\n\n    return heuristics\n\n[Heuristics 7th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, avg_distance_weight: float = 0.18749559158160367, degree_penalty_exponent: float = 1.8228003324542028, sparsification_percentile: float = 72.01995321459675, avg_distance_threshold_factor: float = 1.2582795525251842, small_constant: float = 1.7254114851896175e-07) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, node degree penalty, and sparsification for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n\n    node_degrees = np.sum(distance_matrix < (avg_distance * avg_distance_threshold_factor), axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = (node_degrees[i] + node_degrees[j])**degree_penalty_exponent # Exponent for stronger penalty\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance) * avg_distance_weight))\n\n            else:\n                heuristics[i, j] = 0\n\n    # Sparsification: keep only top edges for each node\n    for i in range(n):\n        row = heuristics[i, :]\n        threshold = np.percentile(row[row > 0], sparsification_percentile) # Only consider positive values\n        heuristics[i, row < threshold] = 0\n\n    return heuristics\n\n[Heuristics 8th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, node degree penalty, and sparsification for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n    small_constant = 1e-6\n    avg_distance_weight = 0.5\n\n    node_degrees = np.sum(distance_matrix < (avg_distance * 1.5), axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = (node_degrees[i] + node_degrees[j])**1.5 # Exponent for stronger penalty\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance) * avg_distance_weight))\n\n            else:\n                heuristics[i, j] = 0\n\n    # Sparsification: keep only top 20% edges for each node\n    for i in range(n):\n        row = heuristics[i, :]\n        threshold = np.percentile(row[row > 0], 80) # Only consider positive values\n        heuristics[i, row < threshold] = 0\n\n    return heuristics\n\n[Heuristics 9th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, node degree penalty, and sparsification for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n    small_constant = 1e-6\n    avg_distance_weight = 0.5\n\n    node_degrees = np.sum(distance_matrix < (avg_distance * 1.5), axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = (node_degrees[i] + node_degrees[j])**1.5 # Exponent for stronger penalty\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance) * avg_distance_weight))\n\n            else:\n                heuristics[i, j] = 0\n\n    # Sparsification: keep only top 20% edges for each node\n    for i in range(n):\n        row = heuristics[i, :]\n        threshold = np.percentile(row[row > 0], 80) # Only consider positive values\n        heuristics[i, row < threshold] = 0\n\n    return heuristics\n\n[Heuristics 10th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, avg_distance_weight: float = 0.8538075872358267, small_constant: float = 3.5318233073538207e-09) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance with node degree penalty to avoid subtours.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n\n    node_degrees = np.sum(distance_matrix < avg_distance, axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = (node_degrees[i] + node_degrees[j])\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance) * avg_distance_weight))\n            else:\n                heuristics[i, j] = 0\n\n    return heuristics\n\n[Heuristics 11th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, avg_distance_weight: float = 0.8538075872358267, small_constant: float = 3.5318233073538207e-09) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance with node degree penalty to avoid subtours.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n\n    node_degrees = np.sum(distance_matrix < avg_distance, axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = (node_degrees[i] + node_degrees[j])\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance) * avg_distance_weight))\n            else:\n                heuristics[i, j] = 0\n\n    return heuristics\n\n[Heuristics 12th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, avg_distance_weight: float = 0.8538075872358267, small_constant: float = 3.5318233073538207e-09) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance with node degree penalty to avoid subtours.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n\n    node_degrees = np.sum(distance_matrix < avg_distance, axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = (node_degrees[i] + node_degrees[j])\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance) * avg_distance_weight))\n            else:\n                heuristics[i, j] = 0\n\n    return heuristics\n\n[Heuristics 13th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, avg_distance_weight: float = 0.8538075872358267, small_constant: float = 3.5318233073538207e-09) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance with node degree penalty to avoid subtours.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n\n    node_degrees = np.sum(distance_matrix < avg_distance, axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = (node_degrees[i] + node_degrees[j])\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance) * avg_distance_weight))\n            else:\n                heuristics[i, j] = 0\n\n    return heuristics\n\n[Heuristics 14th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, avg_distance_weight: float = 0.8538075872358267, small_constant: float = 3.5318233073538207e-09) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance with node degree penalty to avoid subtours.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n\n    node_degrees = np.sum(distance_matrix < avg_distance, axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = (node_degrees[i] + node_degrees[j])\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance) * avg_distance_weight))\n            else:\n                heuristics[i, j] = 0\n\n    return heuristics\n\n[Heuristics 15th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Einstein's heuristics for TSP, version 2: A symphony of inverse distances,\n    node degrees, and a touch of gravitational potential.\n\n    This heuristic combines several factors to estimate the desirability of each edge:\n\n    1. Inverse Distance:  Shorter distances are more desirable.\n    2. Node Degree Penalty:  Penalizes edges connected to nodes with a high \"degree\"\n        (number of nearby nodes). This discourages premature closing of subtours.  We're\n        analogizing this to a high \"gravitational potential\" - nodes with too many\n        connections are avoided until absolutely necessary.\n    3. Global Average Distance: Uses the average distance in the matrix to normalize\n       the effect of very short and very long distances, making it scale-invariant.\n    4.  Add a small constant to avoid division by zero\n\n    Args:\n        distance_matrix (np.ndarray):  A square matrix where distance_matrix[i, j]\n            is the distance between city i and city j.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, where each element\n            represents the desirability score of including the corresponding edge\n            in the TSP tour. Higher values indicate more desirable edges.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0]) #Avoid including self-loops when computing the mean\n    small_constant = 1e-9  # Add to the denominator for stability\n\n    # Calculate \"node degree\" based on proximity. Number of other cities within\n    # certain threshold distance. The intuition here is nodes which are very central\n    # should not be immediately visited to prevent early sub-cycles.\n    node_degrees = np.sum(distance_matrix < avg_distance, axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:  # Avoid self-loops\n                # Node degree penalty is computed to suppress visiting very central nodes before necessary.\n                degree_penalty = (node_degrees[i] + node_degrees[j])  # Total degree of the two nodes\n                # Combine inverse distance with node degree penalty. The degree penalty is scaled with avg distance so that when average distance is large\n                # the suppression due to degree penalty is also large.\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance)))\n            else:\n                heuristics[i, j] = 0  # Self-loops are undesirable\n\n    return heuristics\n\n[Heuristics 16th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Einstein's heuristics for TSP, version 2: A symphony of inverse distances,\n    node degrees, and a touch of gravitational potential.\n\n    This heuristic combines several factors to estimate the desirability of each edge:\n\n    1. Inverse Distance:  Shorter distances are more desirable.\n    2. Node Degree Penalty:  Penalizes edges connected to nodes with a high \"degree\"\n        (number of nearby nodes). This discourages premature closing of subtours.  We're\n        analogizing this to a high \"gravitational potential\" - nodes with too many\n        connections are avoided until absolutely necessary.\n    3. Global Average Distance: Uses the average distance in the matrix to normalize\n       the effect of very short and very long distances, making it scale-invariant.\n    4.  Add a small constant to avoid division by zero\n\n    Args:\n        distance_matrix (np.ndarray):  A square matrix where distance_matrix[i, j]\n            is the distance between city i and city j.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, where each element\n            represents the desirability score of including the corresponding edge\n            in the TSP tour. Higher values indicate more desirable edges.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0]) #Avoid including self-loops when computing the mean\n    small_constant = 1e-9  # Add to the denominator for stability\n\n    # Calculate \"node degree\" based on proximity. Number of other cities within\n    # certain threshold distance. The intuition here is nodes which are very central\n    # should not be immediately visited to prevent early sub-cycles.\n    node_degrees = np.sum(distance_matrix < avg_distance, axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:  # Avoid self-loops\n                # Node degree penalty is computed to suppress visiting very central nodes before necessary.\n                degree_penalty = (node_degrees[i] + node_degrees[j])  # Total degree of the two nodes\n                # Combine inverse distance with node degree penalty. The degree penalty is scaled with avg distance so that when average distance is large\n                # the suppression due to degree penalty is also large.\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance)))\n            else:\n                heuristics[i, j] = 0  # Self-loops are undesirable\n\n    return heuristics\n\n[Heuristics 17th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance with node degree penalty and a local search inspired factor.\n    Sparsifies the matrix by setting unpromising elements to zero.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n    small_constant = 1e-9\n\n    node_degrees = np.sum(distance_matrix < avg_distance * 1.5, axis=1)  # Adjusted degree threshold\n    degree_penalty_exponent = 1.5  # Tunable exponent for degree penalty\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                # Degree penalty based on average distance\n                degree_penalty = (node_degrees[i] + node_degrees[j]) ** degree_penalty_exponent\n                \n                # Local search inspired factor: favors edges that connect to distant nodes\n                neighbor_distances_i = distance_matrix[i, :]\n                neighbor_distances_j = distance_matrix[j, :]\n\n                # Avoid including self in calculation\n                neighbor_distances_i[i] = np.inf\n                neighbor_distances_j[j] = np.inf\n                \n                # Encourage edges connecting to distant/unvisited nodes. Higher value is better.\n                local_search_factor = np.mean(np.partition(neighbor_distances_i, 5)[:5]) + np.mean(np.partition(neighbor_distances_j, 5)[:5]) \n\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance))) * (local_search_factor/avg_distance)\n            else:\n                heuristics[i, j] = 0\n\n    # Sparsify: remove edges with very low heuristic value\n    threshold = np.quantile(heuristics[heuristics > 0], 0.25)  # Adjust sparsification quantile\n    heuristics[heuristics < threshold] = 0\n\n    return heuristics\n\n[Heuristics 18th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    return 1 / distance_matrix\n\n[Heuristics 19th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    return 1 / distance_matrix\n\n[Heuristics 20th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    return 1 / distance_matrix\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}