{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines inverse distance, node degree penalty, sparsification, and cycle avoidance for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n    small_constant = 1e-6\n    avg_distance_weight = 0.5\n\n    node_degrees = np.sum(distance_matrix < (avg_distance * 1.5), axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = (node_degrees[i] + node_degrees[j])**1.5 # Exponent for stronger penalty\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance) * avg_distance_weight))\n\n                #Dynamically adjust degree penalty based on local density\n                local_neighbors = np.sum(distance_matrix[i, :] < (distance_matrix[i, j] * 1.2)) + np.sum(distance_matrix[j, :] < (distance_matrix[j, i] * 1.2))\n                heuristics[i,j] /= (1 + (local_neighbors * 0.1))\n\n\n            else:\n                heuristics[i, j] = 0\n\n    # Sparsification: keep only top edges for each node\n    for i in range(n):\n        row = heuristics[i, :]\n        threshold = np.percentile(row[row > 0], 75) # Only consider top edges\n        heuristics[i, row < threshold] = 0\n\n    # Further sparsification based on global distance distribution\n    distance_threshold = np.percentile(distance_matrix[distance_matrix != 0], 25)\n    for i in range(n):\n        for j in range(n):\n          if distance_matrix[i,j] > distance_threshold and heuristics[i,j] > 0:\n             heuristics[i,j] *= 0.5\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Einstein's heuristics for TSP, version 2: A symphony of inverse distances,\n    node degrees, and a touch of gravitational potential.\n\n    This heuristic combines several factors to estimate the desirability of each edge:\n\n    1. Inverse Distance:  Shorter distances are more desirable.\n    2. Node Degree Penalty:  Penalizes edges connected to nodes with a high \"degree\"\n        (number of nearby nodes). This discourages premature closing of subtours.  We're\n        analogizing this to a high \"gravitational potential\" - nodes with too many\n        connections are avoided until absolutely necessary.\n    3. Global Average Distance: Uses the average distance in the matrix to normalize\n       the effect of very short and very long distances, making it scale-invariant.\n    4.  Add a small constant to avoid division by zero\n\n    Args:\n        distance_matrix (np.ndarray):  A square matrix where distance_matrix[i, j]\n            is the distance between city i and city j.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, where each element\n            represents the desirability score of including the corresponding edge\n            in the TSP tour. Higher values indicate more desirable edges.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0]) #Avoid including self-loops when computing the mean\n    small_constant = 1e-9  # Add to the denominator for stability\n\n    # Calculate \"node degree\" based on proximity. Number of other cities within\n    # certain threshold distance. The intuition here is nodes which are very central\n    # should not be immediately visited to prevent early sub-cycles.\n    node_degrees = np.sum(distance_matrix < avg_distance, axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:  # Avoid self-loops\n                # Node degree penalty is computed to suppress visiting very central nodes before necessary.\n                degree_penalty = (node_degrees[i] + node_degrees[j])  # Total degree of the two nodes\n                # Combine inverse distance with node degree penalty. The degree penalty is scaled with avg distance so that when average distance is large\n                # the suppression due to degree penalty is also large.\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance)))\n            else:\n                heuristics[i, j] = 0  # Self-loops are undesirable\n\n    return heuristics\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic incorporates inverse distance, node degree penalty, sparsification, and cycle avoidance, while the worst simply returns the inverse of the distance matrix. (2nd) and (3rd) are identical, indicating redundancy or a lack of experimentation with different parameters. Comparing (1st) vs (6th), we see that dynamically adjusting the degree penalty based on local density and further sparsification based on global distance distribution helps improve the solution quality in (1st). (6th) parameterizes several key values. Comparing (6th) vs (8th), we observe that the (6th) uses tunable parameters for average distance weight, degree penalty exponent, sparsification percentile, and average distance threshold factor, whereas the (8th) uses fixed values. (10th) simplifies the heuristic to only inverse distance with node degree penalty. Comparing (1st) vs (10th), the sparsification techniques used in (1st) appear to offer further improvement. Comparing (17th) vs (18th), the consideration of local search factors and sparsification contribute to the superiority of (17th). Overall: the best heuristics balance edge desirability (inverse distance) with mechanisms to avoid subtours (node degree penalty) and sparsification to reduce the search space, while parameterization offers flexibility.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, let's redefine \"Current self-reflection\" focusing on actionable insights for better heuristic design, avoiding the pitfalls of vague suggestions.\n\nHere's a refined perspective, tailored for effective heuristic development:\n\n*   **Keywords:** Iterative refinement, problem-specific knowledge, multi-objective factors, dynamic adaptation.\n*   **Advice:** Start with a simple baseline heuristic. Iteratively add complexity based on *quantifiable* problem characteristics (e.g., degree distribution skewness). Experiment with multi-objective optimization incorporating constraints directly.\n*   **Avoid:** Vague directives like \"incorporate local factors.\" Instead, define *specific* local factors and *how* they will be used.\n*   **Explanation:** Effective heuristics are built through a cycle of implementation, experimentation, and data-driven refinement, not abstract intuition. Focus on measurable improvements with each iteration.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}