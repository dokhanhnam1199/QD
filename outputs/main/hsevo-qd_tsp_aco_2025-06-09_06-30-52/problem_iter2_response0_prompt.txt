{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Einstein's heuristics for TSP, version 2: A symphony of inverse distances,\n    node degrees, and a touch of gravitational potential.\n\n    This heuristic combines several factors to estimate the desirability of each edge:\n\n    1. Inverse Distance:  Shorter distances are more desirable.\n    2. Node Degree Penalty:  Penalizes edges connected to nodes with a high \"degree\"\n        (number of nearby nodes). This discourages premature closing of subtours.  We're\n        analogizing this to a high \"gravitational potential\" - nodes with too many\n        connections are avoided until absolutely necessary.\n    3. Global Average Distance: Uses the average distance in the matrix to normalize\n       the effect of very short and very long distances, making it scale-invariant.\n    4.  Add a small constant to avoid division by zero\n\n    Args:\n        distance_matrix (np.ndarray):  A square matrix where distance_matrix[i, j]\n            is the distance between city i and city j.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, where each element\n            represents the desirability score of including the corresponding edge\n            in the TSP tour. Higher values indicate more desirable edges.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0]) #Avoid including self-loops when computing the mean\n    small_constant = 1e-9  # Add to the denominator for stability\n\n    # Calculate \"node degree\" based on proximity. Number of other cities within\n    # certain threshold distance. The intuition here is nodes which are very central\n    # should not be immediately visited to prevent early sub-cycles.\n    node_degrees = np.sum(distance_matrix < avg_distance, axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:  # Avoid self-loops\n                # Node degree penalty is computed to suppress visiting very central nodes before necessary.\n                degree_penalty = (node_degrees[i] + node_degrees[j])  # Total degree of the two nodes\n                # Combine inverse distance with node degree penalty. The degree penalty is scaled with avg distance so that when average distance is large\n                # the suppression due to degree penalty is also large.\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance)))\n            else:\n                heuristics[i, j] = 0  # Self-loops are undesirable\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n    return 1 / distance_matrix\n\n### Analyze & experience\n- Comparing (1st) vs (11th), we see the first heuristic incorporates a node degree penalty and scales it with the average distance to avoid premature subtours, while the 11th heuristic only considers the inverse distance. Comparing (2nd) vs (12th), (3rd) vs (13th), ..., (10th) vs (20th) we see the same pattern. The better heuristics attempt to incorporate more sophisticated problem-specific knowledge like node degree and average distance for scaling. The inferior heuristics all compute just the inverse of the distance matrix. These are equivalent. All of Heuristics 1st to 10th are identical.\n\nOverall: The better heuristics attempt to incorporate more sophisticated problem-specific knowledge, such as a node degree penalty to discourage early subtours and scaling using average distances to make the heuristic scale-invariant. A major difference is in trying to prevent sub-cycles.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, let's refine self-reflection for better heuristic design, avoiding common pitfalls and focusing on actionable insights.\n\nHere's a breakdown:\n\n*   **Keywords:** Iterative refinement, problem-specific knowledge, solution context, evaluation metrics, adaptive adjustment.\n*   **Advice:** After establishing a baseline heuristic, rigorously analyze its weaknesses by understanding where the algorithm makes wrong calls. Then, inject problem-specific information related to these areas. Quantify improvements with pre-defined metrics.\n*   **Avoid:** \"Blind\" refinement without analyzing current heuristic performance. Overfitting to limited training instances. Neglecting evaluation metrics.\n*   **Explanation:** Current heuristic's decision patterns and weaknesses are thoroughly examined and addressed using tailored problem-specific knowledge. Evaluate performance based on defined metrics.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}