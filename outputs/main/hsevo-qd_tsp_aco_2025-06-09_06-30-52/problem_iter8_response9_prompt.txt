{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines inverse distance, node degree penalty, and sparsification for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n    small_constant = 1e-6\n    avg_distance_weight = 0.5\n\n    node_degrees = np.sum(distance_matrix < (avg_distance * 1.5), axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = (node_degrees[i] + node_degrees[j])**1.5 # Exponent for stronger penalty\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance) * avg_distance_weight))\n\n            else:\n                heuristics[i, j] = 0\n\n    # Sparsification: keep only top 20% edges for each node\n    for i in range(n):\n        row = heuristics[i, :]\n        threshold = np.percentile(row[row > 0], 80) # Only consider positive values\n        heuristics[i, row < threshold] = 0\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance with node degree penalty to avoid subtours.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    avg_distance = np.mean(distance_matrix[distance_matrix != 0])\n\n    node_degrees = np.sum(distance_matrix < avg_distance, axis=1)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = (node_degrees[i] + node_degrees[j])\n                heuristics[i, j] = (1 / (distance_matrix[i, j] + small_constant)) / (1 + (degree_penalty * (distance_matrix[i, j]/avg_distance) * avg_distance_weight))\n            else:\n                heuristics[i, j] = 0\n\n    return heuristics\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic incorporates inverse distance, node degree penalty, sparsification, and cycle avoidance, while the worst simply returns the inverse of the distance matrix. (2nd) and (3rd) are identical, indicating redundancy or a lack of experimentation with different parameters. Comparing (1st) vs (6th), we see that dynamically adjusting the degree penalty based on local density and further sparsification based on global distance distribution helps improve the solution quality in (1st). (6th) parameterizes several key values. Comparing (6th) vs (8th), we observe that the (6th) uses tunable parameters for average distance weight, degree penalty exponent, sparsification percentile, and average distance threshold factor, whereas the (8th) uses fixed values. (10th) simplifies the heuristic to only inverse distance with node degree penalty. Comparing (1st) vs (10th), the sparsification techniques used in (1st) appear to offer further improvement. Comparing (17th) vs (18th), the consideration of local search factors and sparsification contribute to the superiority of (17th). Overall: the best heuristics balance edge desirability (inverse distance) with mechanisms to avoid subtours (node degree penalty) and sparsification to reduce the search space, while parameterization offers flexibility.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, let's redefine \"Current self-reflection\" focusing on actionable insights for better heuristic design, avoiding the pitfalls of vague suggestions.\n\nHere's a refined perspective, tailored for effective heuristic development:\n\n*   **Keywords:** Iterative refinement, problem-specific knowledge, multi-objective factors, dynamic adaptation.\n*   **Advice:** Start with a simple baseline heuristic. Iteratively add complexity based on *quantifiable* problem characteristics (e.g., degree distribution skewness). Experiment with multi-objective optimization incorporating constraints directly.\n*   **Avoid:** Vague directives like \"incorporate local factors.\" Instead, define *specific* local factors and *how* they will be used.\n*   **Explanation:** Effective heuristics are built through a cycle of implementation, experimentation, and data-driven refinement, not abstract intuition. Focus on measurable improvements with each iteration.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}