{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Newtonian attraction with inverse distance for TSP edge prioritization.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    safe_distance_matrix = distance_matrix + epsilon\n\n    # Newtonian attraction component\n    attraction = 1 / (safe_distance_matrix**2)\n    mass = np.sum(1 / safe_distance_matrix, axis=1)\n    M = np.outer(mass, mass)\n    potential_energy = safe_distance_matrix / np.mean(safe_distance_matrix)\n    newtonian = (attraction * M) / potential_energy\n\n    # Inverse distance component\n    inverse_distance = 1 / safe_distance_matrix\n\n    # Combine the two components\n    heuristics = attraction_weight * newtonian + inverse_distance_weight * inverse_distance\n\n    # Normalize\n    min_heuristic = np.min(heuristics)\n    max_heuristic = np.max(heuristics)\n    heuristics = (heuristics - min_heuristic) / (max_heuristic - min_heuristic)\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Newtonian attraction, inverse distance, node degree consideration,\n    and sparsification for TSP edge prioritization.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    epsilon = 1e-9\n    safe_distance_matrix = distance_matrix + epsilon\n\n    # 1. Newtonian Attraction Component\n    attraction = 1 / (safe_distance_matrix**2)\n    mass = np.sum(1 / safe_distance_matrix, axis=1)\n    M = np.outer(mass, mass)\n    potential_energy = safe_distance_matrix / np.mean(safe_distance_matrix)\n    newtonian = (attraction * M) / potential_energy\n\n    # 2. Inverse Distance Component\n    inverse_distance = 1 / safe_distance_matrix\n\n    # 3. Node Degree Consideration: Favor edges connecting to nodes with fewer connections\n    degree = np.sum(distance_matrix < np.mean(distance_matrix), axis=1) # Rough degree estimate. Lower is better\n    degree_matrix = np.outer(degree, degree)\n    degree_factor = 1 / (degree_matrix + epsilon)\n    # degree_factor = (degree_factor - np.min(degree_factor)) / (np.max(degree_factor) - np.min(degree_factor)) # Normalize\n\n    # 4. Combine Components (Experiment with weights)\n    heuristics = 0.5 * newtonian + 0.3 * inverse_distance + 0.2 * degree_factor\n\n    # 5. Normalize\n    min_heuristic = np.min(heuristics)\n    max_heuristic = np.max(heuristics)\n    heuristics = (heuristics - min_heuristic) / (max_heuristic - min_heuristic)\n\n    # 6. Sparsification: Set less promising edges to zero (Dynamic threshold)\n    threshold = np.percentile(heuristics, 20)  # Keep top 80% of edges, experiment with values. Could be parameter.\n    heuristics[heuristics < threshold] = 0\n\n    # Ensure diagonal is zero.\n    np.fill_diagonal(heuristics, 0)\n\n    return heuristics\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic combines Newtonian attraction and inverse distance, normalizes the result, and includes a small epsilon to avoid division by zero. The worst simply returns the inverse of the distance matrix, lacking any sophisticated combination or normalization.\nComparing (14th) vs (17th), the 14th heuristic adds node degree consideration and sparsification to the Newtonian attraction and inverse distance components, while the 17th heuristic is the same as the 20th.\nComparing (1st) vs (6th), the first uses tuned parameters while the 6th hardcodes weights.\nComparing (14th) vs (6th), the 14th incorporates degree and sparsification.\nOverall: The better heuristics consider multiple factors beyond just inverse distance, normalize results for stability, avoid division by zero, may incorporate sparsification, and may tune parameters. Simpler heuristics perform worse. Also docstrings affect.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, let's redefine \"Current Self-Reflection\" to design better heuristics, focusing on actionable steps and avoiding common pitfalls.\n\n*   **Keywords:** Feature Engineering, Normalization, Edge-Case Handling, Parameter Tuning, Sparsification, Physical Analogy, Empirical Validation, Code Efficiency.\n\n*   **Advice:** Prioritize *meaningful* feature combinations informed by domain knowledge or physical principles. Rigorously test and compare heuristic performance with statistical significance.\n\n*   **Avoid:** Redundant calculations, neglecting edge cases (e.g., division by zero), and premature optimization without proper validation.\n\n*   **Explanation:** Effective heuristics stem from insightful feature engineering, robust handling of numerical issues, and iterative refinement guided by empirical data.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}