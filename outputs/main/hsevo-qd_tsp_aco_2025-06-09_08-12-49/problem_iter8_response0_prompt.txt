{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Newtonian Heuristic for TSP Edge Prioritization.\n\n    This heuristic combines elements inspired by gravitational attraction and\n    energy minimization to guide TSP solution sampling.  Edges connecting nodes\n    that are \"attracted\" to each other more strongly (closer distance, higher\n    pseudo-mass) are favored. A term discouraging long edges (high potential\n    energy) is also included.\n\n    Args:\n        distance_matrix (np.ndarray): A symmetric numpy array representing the\n            pairwise distances between cities. distance_matrix[i, j] is the\n            distance between city i and city j.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, representing\n            the heuristic \"promise\" of each edge. Higher values indicate\n            more promising edges.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # Avoid division by zero by adding a small epsilon to distances.\n    epsilon = 1e-9\n    safe_distance_matrix = distance_matrix + epsilon\n\n    # Gravitational attraction component (inverse square law): cities closer together are more attracted\n    attraction = 1 / (safe_distance_matrix**2)\n\n    # Node \"mass\" proxy: how \"central\" is a city. We assign mass using degree centrality:\n    # sum the inverse distance to all other cities. A city close to many others is more important.\n\n    mass = np.sum(1 / safe_distance_matrix, axis=1) #mass of the city\n\n    # Create a matrix M[i][j] = mass[i]*mass[j]\n    M = np.outer(mass, mass)\n\n    # Potential energy penalty: long edges are less desirable. Scale to be compatible with the 'gravitational' term\n    potential_energy = safe_distance_matrix / np.mean(safe_distance_matrix) # normalized distance\n\n\n    heuristics = (attraction * M) / potential_energy\n\n    # Normalize to range [0, 1] for numerical stability and interpretability, and to bound effect during search\n    min_heuristic = np.min(heuristics)\n    max_heuristic = np.max(heuristics)\n    heuristics = (heuristics - min_heuristic) / (max_heuristic - min_heuristic)\n\n\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Newtonian attraction, inverse distance, node degree consideration,\n    and sparsification for TSP edge prioritization.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    epsilon = 1e-9\n    safe_distance_matrix = distance_matrix + epsilon\n\n    # 1. Newtonian Attraction Component\n    attraction = 1 / (safe_distance_matrix**2)\n    mass = np.sum(1 / safe_distance_matrix, axis=1)\n    M = np.outer(mass, mass)\n    potential_energy = safe_distance_matrix / np.mean(safe_distance_matrix)\n    newtonian = (attraction * M) / potential_energy\n\n    # 2. Inverse Distance Component\n    inverse_distance = 1 / safe_distance_matrix\n\n    # 3. Node Degree Consideration: Favor edges connecting to nodes with fewer connections\n    degree = np.sum(distance_matrix < np.mean(distance_matrix), axis=1) # Rough degree estimate. Lower is better\n    degree_matrix = np.outer(degree, degree)\n    degree_factor = 1 / (degree_matrix + epsilon)\n    # degree_factor = (degree_factor - np.min(degree_factor)) / (np.max(degree_factor) - np.min(degree_factor)) # Normalize\n\n    # 4. Combine Components (Experiment with weights)\n    heuristics = 0.5 * newtonian + 0.3 * inverse_distance + 0.2 * degree_factor\n\n    # 5. Normalize\n    min_heuristic = np.min(heuristics)\n    max_heuristic = np.max(heuristics)\n    heuristics = (heuristics - min_heuristic) / (max_heuristic - min_heuristic)\n\n    # 6. Sparsification: Set less promising edges to zero (Dynamic threshold)\n    threshold = np.percentile(heuristics, 20)  # Keep top 80% of edges, experiment with values. Could be parameter.\n    heuristics[heuristics < threshold] = 0\n\n    # Ensure diagonal is zero.\n    np.fill_diagonal(heuristics, 0)\n\n    return heuristics\n\n### Analyze & experience\n- Comparing (1st) vs (19th), we see that the best heuristic combines Newtonian attraction, inverse distance, and carefully tuned weights, along with normalization and sparsification, while the worst simply returns the inverse of the distance matrix. Comparing (2nd best) vs (2nd worst), we again observe the inclusion of Newtonian attraction, inverse distance, and node degree, as well as sparsification and normalization, contrasting with the simple inverse distance approach. Comparing (1st) vs (2nd), we see that the first one uses carefully tuned weights as parameters, and the second one uses hardcoded weights and includes a node degree component. Comparing (3rd) vs (4th), no difference. Comparing (second worst) vs (worst), no difference. Overall: The best heuristics incorporate multiple factors beyond just distance, such as node degree and a Newtonian attraction analogy, and use parameter tuning. Sparsification, normalization, and an epsilon value for numerical stability are also common features. Parameter tuning is the most important design for good heuristics.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, I'm ready to refine \"Current Self-Reflection\" for better heuristic design. Let's focus on actionable improvements, not just general reminders.\n\n*   **Keywords:** Feature Engineering, Analogy, Evaluation Rigor, Code Clarity\n*   **Advice:** Emphasize creating novel features through insightful combinations (especially physical analogies). Design robust testing procedures to truly compare heuristic effectiveness.\n*   **Avoid:** Vague statements like \"consider combining factors.\" Instead, focus on *how* to combine them creatively and meaningfully.\n*   **Explanation:** Move beyond general principles. We need concrete techniques for feature creation, rigorous evaluation, and maintainable code, not just the awareness of their importance.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}