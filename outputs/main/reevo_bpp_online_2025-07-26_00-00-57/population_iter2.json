[
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation prioritizes bins that are closer to being full but ensures\n    that items are only placed into bins where they can fit. It penalizes bins that\n    cannot fit the item and normalizes the priorities based on the original bin capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Original bin capacities, assuming all bins were initially full to the max remaining capacity found\n    bins_original_cap = np.full_like(bins_remain_cap, bins_remain_cap.max() + item)\n    \n    # Penalty for bins with less remaining capacity than the item (can't fit the item)\n    penalties = (bins_remain_cap < item) * -np.inf\n    \n    # Calculate priority: moving closer to full has a priority proportional to the fullness\n    priority = 1 - (bins_remain_cap / bins_original_cap)\n    \n    # Apply penalties to bins where the item can't be placed\n    priority += penalties\n    \n    # Inverse so that closer to full but feasible slots get highest priority\n    priority = -priority\n    \n    return priority",
    "response_id": 0,
    "obj": 4.487435181491823,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response1.txt_stdout.txt",
    "code_path": "problem_iter2_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation considers both the remaining capacity and the number of\n    bins used using a weighted score. It gives a clear preference to bins that can\n    fit the item and tries to fill bins that are closer to being full to reduce\n    fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Linear decreasing priority based on remaining capacity\n    # Use a max function to avoid negative priorities and ensure feasibility\n    priority = np.maximum(0, bins_remain_cap - item) / item\n    \n    # Give a clear preference to bins that can fit the item at all\n    can_fit_item_mask = bins_remain_cap >= item\n    priority[~can_fit_item_mask] -= 2  # Strict penalty for non-fitting bins\n    \n    # Boost smaller gaps to reduce fragmentation\n    if np.any(can_fit_item_mask):\n        # Find the second smallest gap that can fit the item\n        priority_after_check = bins_remain_cap[can_fit_item_mask] - item\n        second_smallest_gap = np.partition(priority_after_check, 1)[1] if len(priority_after_check) > 1 else 0\n        if second_smallest_gap > 0:\n            # Boost bins that have this second smallest gap\n            second_smallest_indices = np.where(np.isclose(priority_after_check, second_smallest_gap))[0]\n            for idx in second_smallest_indices:\n                priority[can_fit_item_mask][idx] += 1  # Boost priority for these bins\n\n    return priority",
    "response_id": 1,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response2.txt_stdout.txt",
    "code_path": "problem_iter2_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation considers both the remaining capacity and the number of\n    bins used using a weighted score. Prioritizes bins that are near full, penalizes\n    bins that cannot fit the item, and boosts bins that have a close-to-subtle gap.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Linear decreasing priority based on remaining capacity\n    priority = (bins_remain_cap - item) / item\n\n    # Penalize bins that cannot fit the item\n    can_fit_item_mask = bins_remain_cap >= item\n    priority[~can_fit_item_mask] = -np.inf\n\n    # Boost bins that have a close-to-subtle gap to encourage less fragmentation\n    if np.any(can_fit_item_mask):\n        remaining_capacities = bins_remain_cap[can_fit_item_mask] - item\n        second_smallest_gap = np.partition(remaining_capacities, 2)[:3]  # Get top 3 smallest gaps\n        if len(second_smallest_gap) > 1:\n            second_smallest = second_smallest_gap[1]  # This is the smallest gap after 0\n            priority[bins_remain_cap - item == second_smallest] += 0.5\n\n    return priority",
    "response_id": 2,
    "obj": 139.4395692062226,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation considers both the remaining capacity and the number of\n    bins used using a weighted score. Prioritizes bins that are near full but can\n    still fit the item, penalizes bins that cannot fit the item, and boosts bins\n    that have a close-to-subtle gap which encourages less fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priority scores\n    priority = np.zeros_like(bins_remain_cap)\n    \n    # Calculate the remaining capacity after placing the item in each bin\n    remaining_capacity = bins_remain_cap - item\n    \n    # Mark bins that can fit the item\n    can_fit_item_mask = remaining_capacity >= 0\n    \n    # Assign high priority to bins that can fit the item, scaled by remaining capacity\n    priority[can_fit_item_mask] = remaining_capacity[can_fit_item_mask] / item\n    \n    # Penalize bins that cannot fit the item\n    priority[~can_fit_item_mask] -= 2\n    \n    # Boost bins that have a close-to-subtle gap to reduce fragmentation\n    if np.any(can_fit_item_mask):\n        sorted_remaining = np.sort(remaining_capacity[can_fit_item_mask])\n        if len(sorted_remaining) > 1:\n            subtle_gap = sorted_remaining[1]  # Second smallest gap\n        else:\n            subtle_gap = sorted_remaining[0]  # Only one option if only one bin can fit the item\n        \n        # Boost bins that have this subtle gap\n        subtle_gap_indices = np.where(np.isclose(remaining_capacity, subtle_gap))[0]\n        priority[subtle_gap_indices] += 1\n    \n    return priority",
    "response_id": 3,
    "obj": 17.999601116872757,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response4.txt_stdout.txt",
    "code_path": "problem_iter2_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation uses sorting to prioritize bins with the second smallest gaps,\n    encouraging less fragmentation while ensuring items that cannot fit are penalized.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priority with linear decreasing score based on remaining capacity\n    priority = np.maximum(0, bins_remain_cap - item) / item\n    \n    # Penalize bins that cannot fit the item\n    can_fit_item_mask = bins_remain_cap >= item\n    priority[~can_fit_item_mask] -= 2\n    \n    # Identify the second smallest gap for items that can fit\n    potential_gaps = bins_remain_cap - item\n    valid_gaps = potential_gaps[can_fit_item_mask]\n    \n    if valid_gaps.size > 1:\n        second_smallest_gap = np.partition(valid_gaps, 1)[1]\n        second_smallest_indices = np.where(np.isclose(potential_gaps, second_smallest_gap))[0]\n        priority[second_smallest_indices] += 3  # Boost priority for bins with the second smallest gap\n\n    return priority",
    "response_id": 4,
    "obj": 8.675708017550859,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response5.txt_stdout.txt",
    "code_path": "problem_iter2_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation considers both the remaining capacity and the number of\n    bins used using a weighted score. It strongly penalizes bins that cannot fit the item,\n    boosts bins with the smallest gaps, and is sensitive to the item size.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Linear decreasing priority based on remaining capacity\n    priority = np.maximum(0, bins_remain_cap - item) / item\n\n    # Strongly penalize bins that cannot fit the item\n    can_fit_item_mask = bins_remain_cap >= item\n    priority[~can_fit_item_mask] -= 10  # Strong penalty for bins that cannot fit the item\n\n    # Boost bins with the smallest gaps after placing the item\n    if np.any(can_fit_item_mask):\n        gaps = bins_remain_cap[can_fit_item_mask] - item\n        if len(gaps) > 1:\n            smallest_two_gaps = np.partition(gaps, 1)[:2]\n            smallest_gap = smallest_two_gaps[0]\n            second_smallest_gap = smallest_two_gaps[1]\n            smallest_idx = np.where(np.isclose(gaps, smallest_gap))[0]\n            second_smallest_idx = np.where(np.isclose(gaps, second_smallest_gap))[0]\n            for idx in smallest_idx:\n                priority[can_fit_item_mask][idx] += 2  # Boost for the smallest gap\n            for idx in second_smallest_idx:\n                priority[can_fit_item_mask][idx] += 1  # Lesser boost for the second smallest gap\n\n    return priority",
    "response_id": 5,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response6.txt_stdout.txt",
    "code_path": "problem_iter2_code6.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation considers both the remaining capacity and the number of\n    bins used using a weighted score. Bins that are closer to being full get higher\n    priority, but the priority is slightly reduced for bins that have less remaining\n    capacity than the item to prevent unused bin scenarios. It focuses on filling\n    bins with the second smallest gap first to encourage less fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priority with remaining capacity relative to item size\n    priority = np.maximum(0, bins_remain_cap - item) / item\n    \n    # Penalize bins that cannot fit the item\n    priority[bins_remain_cap < item] -= 2\n    \n    # Encourage filling the bin with the second smallest gap\n    valid_gaps = bins_remain_cap - item\n    valid_gaps_indices = np.argsort(valid_gaps)\n    \n    # Check if there's a valid second smallest gap\n    if len(valid_gaps_indices) > 1:\n        second_smallest_gap_index = valid_gaps_indices[1]\n        priority[second_smallest_gap_index] += 3  # can be tuned\n\n    return priority",
    "response_id": 6,
    "obj": 8.426406063023535,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response7.txt_stdout.txt",
    "code_path": "problem_iter2_code7.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation gives a high priority to bins that are closer to being full\n    while ensuring that bins where the item cannot fit receive a very low priority.\n    The priority score is calculated based on the remaining capacity relative to the\n    original capacity of the bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Original bin capacities, assuming all bins start with the same capacity\n    bins_original_cap = np.max(bins_remain_cap)\n    \n    # Penalty for bins with less remaining capacity than the item (can't fit the item)\n    penalties = (bins_remain_cap < item) * -np.inf\n    \n    # Calculate priority: moving closer to full has a priority proportional to the fullness\n    priority = 1 - (bins_remain_cap / bins_original_cap)\n    \n    # Apply penalties to bins where the item can't be placed\n    priority += penalties\n    \n    # Invert so that bins closer to full but feasible get the highest priority\n    priority = -priority\n    \n    return priority",
    "response_id": 7,
    "obj": 4.487435181491823,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response8.txt_stdout.txt",
    "code_path": "problem_iter2_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on scaling, penalizing underuse, and clear item capacity checks.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Linear decreasing priority based on remaining capacity\n    base_priority = (bins_remain_cap - item) / bins_remain_cap\n\n    # Penalize bins that have less capacity than the item itself\n    penalty_for_small_bins = np.where(bins_remain_cap < item, -2.0, 0)\n\n    # Penalize underused bins to avoid creating bins too small and leftover bins\n    # This penalty increases the more the bin is under-used compared to the largest bin\n    usage_discount = (bins_remain_cap.max() - bins_remain_cap) / bins_remain_cap.max()\n\n    # Combine base priority, usage discount, and penalties into a final score\n    priority_score = base_priority - usage_discount + penalty_for_small_bins\n\n    # Ensure bins without enough space are not considered\n    priority_score[bins_remain_cap < item] = -np.inf\n\n    return priority_score",
    "response_id": 8,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation prioritizes bins based on remaining capacity, assigns a\n    very low penalty to bins that can't fit the item, and ensures that feasible\n    bins have positive priority scores.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Penalty for bins with less remaining capacity than the item (can't fit the item)\n    penalties = (bins_remain_cap < item) * -np.inf\n    \n    # Calculate priority: linearly based on remaining capacity\n    priority = bins_remain_cap\n    \n    # Apply penalties to bins where the item can't be placed\n    priority += penalties\n    \n    return priority",
    "response_id": 9,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 28, in priority_v2\n    priority += penalties\nnumpy._core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'\n5\n1\n"
  }
]