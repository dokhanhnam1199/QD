[
  {
    "stdout_filepath": "problem_iter14_response0.txt_stdout.txt",
    "code_path": "problem_iter14_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Z-score synergy with item-size adaptation and entropy-controlled frag mitigator.\n    \n    Combines normalized tightness/util metrics weighted by item size relative to system\n    state, entropy-adjusted penalty scaling, and residual clustering continuity.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eps = 1e-9\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if not mask.any():\n        return scores\n\n    # Base parameters\n    origin_cap = bins_remain_cap.max()\n    rem_cap = bins_remain_cap[mask]\n    leftover = rem_cap - item\n\n    # Tightness & utilization metrics\n    inv_waste = 1.0 / (leftover + eps)\n    exp_tight = np.exp(-2.0 * leftover / (origin_cap + eps))\n    tight_fit = inv_waste + 2.0 * exp_tight\n    bin_util = (origin_cap - rem_cap) / (origin_cap + eps)\n\n    # Z-score normalization\n    t_mean, t_std = tight_fit.mean(), tight_fit.std(ddof=1)\n    u_mean, u_std = bin_util.mean(), bin_util.std(ddof=1)\n    z_tight = (tight_fit - t_mean) / (t_std + eps)\n    z_util = (bin_util - u_mean) / (u_std + eps)\n\n    # Entropy adaptation\n    active_caps = bins_remain_cap[bins_remain_cap > 0]\n    if active_caps.size > 1:\n        log_caps_std = np.log(active_caps + eps).std()\n        entropic_scale = log_caps_std\n    else:\n        entropic_scale = 0.0\n\n    # Item size context\n    median_cap = np.median(active_caps) if active_caps.size else origin_cap\n    is_small_item = item / (median_cap + eps) < 0.75\n\n    # Dynamic weights\n    entropy_amp = (1.0 + entropic_scale) ** 1.5\n    if is_small_item:\n        tight_weight = 1.8 + 0.5 * entropy_amp\n        util_weight = 0.3\n    else:\n        tight_weight = 1.0 + 0.2 * entropy_amp\n        util_weight = 1.2 + entropy_amp\n\n    # Core synergy\n    synergy = tight_weight * z_tight + util_weight * z_util\n\n    # Fragmentation control\n    leftover_norm = leftover / (origin_cap + eps)\n    base_frag = -np.expm1(-leftover_norm ** 1.25)\n    median_leftover = np.median(leftover) if leftover.size else origin_cap / 2\n    cap_highway = np.exp(-3.0 * ((leftover - median_leftover)**2) / ((origin_cap * 0.4)**2 + eps))\n    frag_penalty = base_frag - cap_highway\n    frag_weight = 0.7 * entropy_amp * (1.0 + bins_remain_cap.size / (origin_cap + eps))\n    scores_masked = synergy - frag_weight * frag_penalty\n\n    # Continuity reinforcement\n    if active_caps.size > 2:\n        leftover_std = active_caps.std() if active_caps.size > 1 else origin_cap\n        dist_to_mean = np.abs(leftover - rem_cap.mean()) / (leftover_std + eps)\n        continuity_bonus = np.exp(-0.5 * dist_to_mean)\n        scores_masked += 0.2 * continuity_bonus\n\n    scores[mask] = scores_masked\n    return scores",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.0885520542481055,
    "SLOC": 53.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response1.txt_stdout.txt",
    "code_path": "problem_iter14_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combined adaptive Z-synergy with entropy-weighted fragmentation and balance control.\n    Uses metric normalization, system variance-adaptive weights, and item-classification dynamics.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Metrics and system descriptors\n    leftover = bins_remain_cap - item\n    origin_cap = np.max(bins_remain_cap)\n    tightness = item / (bins_remain_cap + eps)\n    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)\n    fit_quality = 1.0 / (leftover + eps)\n    \n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + eps)\n    large_item = item > (system_avg + eps)\n    \n    # Z-score normalization across eligible bins\n    eligible_metrics = {\n        'fit': fit_quality[eligible],\n        'tight': tightness[eligible],\n        'cap': bins_remain_cap[eligible]\n    }\n    mean = {k: np.mean(v) for k, v in eligible_metrics.items()}\n    std = {k: np.std(v) for k, v in eligible_metrics.items()}\n    \n    # Z-score computation for key metrics\n    z_fit = (fit_quality - mean['fit']) / (std['fit'] + eps)\n    z_tight = (tightness - mean['tight']) / (std['tight'] + eps)\n    z_cap = (bins_remain_cap - mean['cap']) / (std['cap'] + eps)\n    \n    # Primary synergy term with exponential enhancer\n    enhancer = np.exp(utilization * tightness)\n    primary_score = (z_fit + z_tight + 0.5 * z_cap) * enhancer\n    \n    # Entropy-weighted penalties for fragmentation and balance\n    frag_penalty = 1.0 - np.exp(-leftover / (origin_cap + eps))\n    frag_weight = 0.3 * system_cv\n    \n    balance_term = -np.abs(leftover - system_avg)\n    balance_weight = 0.2 * system_cv * (2.0 if not large_item else 1.0)\n    \n    # Hybrid scoring based on multi-metric analysis\n    priority = primary_score - frag_weight * frag_penalty + balance_weight * balance_term\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 53.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response2.txt_stdout.txt",
    "code_path": "problem_iter14_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Online BPP priority that blends Z-score weighted fit (BF) and tightness (FF) via adaptive variance ratios,\n    with item classification (large/tiny) driving climb envelope modulation and exponentially decayed \n    fit-scoring to balance BF/FF tradeoffs. Stability-enhanced tie-breaking via median proximity sensitivity.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    if item <= 1e-9:\n        return np.where(bins_remain_cap >= 0, 0.01 * bins_remain_cap, -np.inf)\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # Capacity metrics\n    C_est = bins_remain_cap.max() if bins_remain_cap.max() > 0 else item * 2\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    fit_score = 1.0 / (leftover + 1e-9)\n    \n    # Z-score normalization\n    def z_score(x):\n        mean = x[eligible].mean()\n        std = x[eligible].std()\n        return (x - mean) / (std + 1e-9)\n    \n    tight_z = z_score(tightness)\n    fit_z = z_score(fit_score)\n    \n    # Adaptive weighting by variance\n    fit_var, tight_var = fit_z.var(), tight_z.var()\n    var_ratio = fit_var / (fit_var + tight_var + 1e-9)\n    base_score = (var_ratio * fit_z) + ((1 - var_ratio) * tight_z)\n    \n    # Item classification\n    is_large = item > 0.7 * C_est\n    grad_factor = 1.35 if is_large else 1.0\n    utilization = 1.0 - (bins_remain_cap / C_est)\n    \n    # Energy climbing envelope\n    climb_env = grad_factor * np.exp(0.3 * tightness + 0.2 * utilization)\n    \n    # Fragmentation control\n    frag_penalty = 0.2 * np.exp(-leftover / (C_est / 3 + 1e-9))\n    \n    # Stability preservation\n    median_cap = np.median(bins_remain_cap[eligible]) if eligible.any() else bins_remain_cap.median()\n    proximity = np.abs(bins_remain_cap - median_cap) / (C_est + 1e-9)    \n    stability = 0.02 * bins_remain_cap.std() * np.abs(tight_z) * proximity\n    \n    # Priority construction\n    priority = (base_score * climb_env) - frag_penalty + stability\n    \n    # BF bias with soft exponential decay\n    priority *= np.exp(-0.05 * leftover / (C_est + 1e-9))\n    \n    # Reinforcement-style tiebreaker\n    priority += 1e-7 * np.random.normal(0, 1, priority.shape)\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 53.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response3.txt_stdout.txt",
    "code_path": "problem_iter14_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    v2: Adaptive Z-synergy + entropy-aware balancing + gradient-boosted reinforcement gains.\n    Core ideas: Normalized metric blending, fragmentation-aware penalties, and \n    system-state adaptive scoring for optimal bin selection.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = bins_remain_cap.max()\n    eligible = bins_remain_cap >= item + eps  # Strict eligibility check\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    # System statistics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std() + eps\n    system_cv = system_std / (system_avg + eps)\n    \n    # Core eligibility metrics\n    remaining = bins_remain_cap[eligible] + eps\n    leftover = remaining - item\n    tightness = item / remaining  # Tightness of fit\n    \n    # Normalized metric synergy\n    phi = (1.0 / leftover) * tightness  # Fit quality driver\n    phi_z = ((phi - phi.mean()) / phi.std())[eligible]  # Z-normalized synergy\n    cap_z = ((remaining - system_avg) / system_std)[eligible]  # Capacity deviation score\n\n    # Dynamic metric weighting\n    cross_weight = 1.0 / (1.0 + np.emath.logn(2, phi.std() + cap_z.std() + 1))\n    composite = (1 - cross_weight) * phi_z + cross_weight * cap_z  # Adaptive blending\n    \n    # Reinforcement-inspired gradient boost\n    margin_weight = (system_avg - leftover) / system_avg  # Exploitation margin\n    entropy_offset = 0.5 * (tightness + tightness.std())  # Entropy-sensitive boosting\n    enhancer = np.exp((composite * margin_weight) + entropy_offset)  # Multiplier amplification\n    \n    # Entropy-preserving balance term\n    filled_frac = (orig_cap - remaining) / orig_cap  # Current fill level\n    system_var = filled_frac.var() / (filled_frac.std() + eps)  # Entropy coefficient\n    balance_factor = filled_frac * (1 + system_var)  # System state alignment weight\n    balance_z = (balance_factor - balance_factor.mean()) / balance_factor.std()  # Z-normalized cost\n    \n    # Smart fragmentation control\n    capacity_ratio = remaining / orig_cap\n    frag_indicator = leftover < (0.15 * orig_cap)  # Aggressive leftover check\n    fragility_penalty = np.where(\n        frag_indicator,\n        -tightness * 0.05 * system_var,\n        -tightness * 0.01 * system_var\n    )\n    \n    # Final score composition\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    scores[eligible] = (\n        (composite * enhancer) * (1 + system_var)  # Primary driver with variance scaling\n        + (0.1 * balance_z * system_std)  # Adaptive entropy balancing\n        + (0.05 * fragility_penalty)  # Fragmentation-aware penalty\n    )\n\n    return scores",
    "response_id": 3,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 30, in priority_v2\n    z_fit = (1.0 / (leftover + 1e-9) - np.mean(fit_quality)) / (np.std(fit_quality) + 1e-9)\nIndexError: boolean index did not match indexed array along axis 0; size of axis is 4987 but size of corresponding boolean axis is 4988\n53\n4\n"
  },
  {
    "stdout_filepath": "problem_iter14_response4.txt_stdout.txt",
    "code_path": "problem_iter14_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid priority using Z-standardized fit metrics, entropy-driven variance forecasting,\n    and exponential gain based on utilization-tightness synergy for optimal bin selection.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9) - 1e-9 * bins_remain_cap,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Metric calculations\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)  # Tightness of fit\n    utilization = (orig_cap - bins_remain_cap) / orig_cap  # Bin fullness metric\n\n    # Z-normalization of fit quality and capacities\n    fit_quality = 1.0 / (leftover[eligible] + 1e-9)\n    z_fit = (1.0 / (leftover + 1e-9) - np.mean(fit_quality)) / (np.std(fit_quality) + 1e-9)\n    \n    elig_capacities = bins_remain_cap[eligible]\n    z_cap = (bins_remain_cap - np.mean(elig_capacities)) / (np.std(elig_capacities) + 1e-9)\n    \n    # Core hybrid score combining fit tightness and Z-normalized metrics\n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    \n    # Exponential reinforcement based on utilization synergy\n    enhancer = np.exp(utilization * tightness)\n    \n    # Predictive entropy modeling\n    N = len(bins_remain_cap)\n    new_capacities = bins_remain_cap[eligible] - item\n    total_old = np.sum(bins_remain_cap)\n    \n    # Global statistics before any placement\n    mean_old = total_old / N\n    sum_sq_old = np.sum(bins_remain_cap ** 2)\n    var_old = (sum_sq_old / N) - (mean_old ** 2)\n    \n    # Hypothetical new statistics post-placement\n    new_means_i = (total_old - item) / N\n    sum_sq_new_i = sum_sq_old - (bins_remain_cap[eligible] ** 2) + (new_capacities ** 2)\n    var_new_i = (sum_sq_new_i / N) - (new_means_i ** 2)\n    \n    # Entropy sensitivity analysis\n    system_sensitivity = var_old - var_new_i\n    entropy_std = np.std(system_sensitivity) if len(system_sensitivity) > 1 else 1.0\n    normalized_entropy = system_sensitivity / (entropy_std + 1e-9)\n    \n    # Adaptive entropy weighting\n    system_cv = np.std(bins_remain_cap) / (np.mean(bins_remain_cap) + 1e-9)\n    large_item = item > np.mean(bins_remain_cap)\n    entropy_weight = 0.5 * (1.0 + system_cv) * (1.5 if large_item else 0.5)\n    entropy_weight /= (np.log(2 + np.abs(system_sensitivity))).mean() + 1e-9\n    \n    # Residual sensitivity saturation\n    sensitivity = 1.0 / (1.0 + np.abs((new_capacities - new_means_i) / (new_means_i + 1e-9)))\n    \n    # Combined priority score\n    priority = primary_score * enhancer * sensitivity + entropy_weight * normalized_entropy\n    \n    # Deterministic tightness-aware tie-breaking\n    if np.allclose(priority[eligible], priority[eligible][0]):\n        priority += 1e-4 * tightness * (bins_remain_cap ** (-0.5))\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 4,
    "tryHS": false,
    "obj": 7.828081372157958,
    "SLOC": 53.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response5.txt_stdout.txt",
    "code_path": "problem_iter14_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines adaptive Z-score normalization, entropy-driven balance, and reinforcement learning concepts.\n    Prioritizes bins that optimize fit tightness, system-wide capacity balance, and future flexibility.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + 1e-9,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System-wide metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 1e-9 else 0.0\n    \n    # Item classification\n    large_item = item > system_avg\n    \n    # Core metrics\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    utilization = (orig_cap - bins_remain_cap) / (orig_cap + 1e-9)\n    \n    # Z-score normalization for fit and space\n    elig_fit = 1.0 / (leftover + 1e-9)\n    elig_fit_mean, elig_fit_std = np.mean(elig_fit[eligible]), np.std(elig_fit[eligible])\n    z_fit = (elig_fit - elig_fit_mean) / (elig_fit_std + 1e-9)\n    \n    elig_space = bins_remain_cap\n    elig_space_mean, elig_space_std = np.mean(elig_space[eligible]), np.std(elig_space[eligible])\n    z_cap = (elig_space - elig_space_mean) / (elig_space_std + 1e-9)\n    \n    # Primary score: adaptive tightness-weighted Z-combination\n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    \n    # Exponential enhancer for utilization-tightness synergy\n    enhancer = np.exp(utilization * tightness)\n    \n    # Entropy-driven balance term\n    balance_term = -np.abs(leftover - system_avg) / (system_std + 1e-9)\n    balance_weight = 0.5 * system_cv * np.where(large_item, 1.0, 2.0)\n    balance_contrib = balance_term * balance_weight\n    \n    # Reinforcement multiplier\n    eligible_rem = bins_remain_cap[eligible]\n    rel_size = item / (np.median(eligible_rem) + 1e-9)\n    fragility = ((orig_cap - bins_remain_cap) / (orig_cap + 1e-9)).clip(0, 1)\n    rem_rel = bins_remain_cap / (orig_cap + 1e-9)\n    reinforce_factor = (1 - rel_size) ** 2 * rem_rel * fragility\n    reinforcer = 1 + 0.5 * reinforce_factor\n    \n    # Final priority calculation\n    priority = (primary_score * enhancer + balance_contrib) * reinforcer\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 5,
    "tryHS": false,
    "obj": 3.9190267251695206,
    "SLOC": 53.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response6.txt_stdout.txt",
    "code_path": "problem_iter14_code6.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive synergy of Z-normalized metrics with entropy-weighted fragmentation control and predictive balance.\n    Combines tight-fit synergy, exponential enhancer, and system-aware tie-breaking.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Core metrics\n    C = bins_remain_cap.max()\n    remaining_cap = bins_remain_cap[eligible]\n    leftover = remaining_cap - item + eps\n    \n    fit_quality = 1.0 / leftover\n    tightness = item / (remaining_cap + eps)\n    utilization = (C - remaining_cap) / C\n    \n    # Z-score normalization\n    def z_score(x):\n        return (x - x.mean()) / (x.std() + eps)\n    \n    z_fit = z_score(fit_quality)\n    z_tight = z_score(tightness)\n    z_util = z_score(utilization)\n    \n    # Adaptive synergy: variance-weighted tight-fit + utilization coupling\n    var_fit = max(z_fit.var(), 0.1)\n    var_tight = max(z_tight.var(), 0.1)\n    weight_tight = var_fit / (var_fit + var_tight)  # Inverse variance weighting\n    adaptive_synergy = weight_tight * z_tight + (1 - weight_tight) * z_fit\n    synergy = adaptive_synergy * (1 + z_util)  # Utilization-coupled reinforcement\n    \n    # Exponential enhancer with predictive utilization\n    enhancer = np.exp(utilization * tightness)\n    primary_score = synergy * enhancer\n    \n    # Entropy-sensitive fragmentation control\n    sys_entropy = bins_remain_cap.std() / (C + eps)\n    frag_penalty = 1.0 - np.exp(-leftover / (C + eps))\n    frag_weight = 0.2 * (1.0 + sys_entropy)\n    \n    # System balance term (leftover proximity to average)\n    system_avg = bins_remain_cap.mean()\n    balance = -np.abs(leftover - system_avg) * (1.0 + sys_entropy)\n    balance_weight = 0.1 * (1.0 + sys_entropy)\n    \n    # Deterministic tie-breaker (entropy-coupled exponential decay)\n    tie_breaker = 0.05 * np.exp(-leftover) * (1.0 + sys_entropy)\n    \n    # Final score with multi-layered components\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    eligible_scores = (\n        primary_score \n        - frag_weight * frag_penalty \n        + balance * balance_weight \n        + tie_breaker\n    )\n    scores[eligible] = eligible_scores\n    \n    return scores",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 53.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response7.txt_stdout.txt",
    "code_path": "problem_iter14_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Z-score synergy with variance-aware rewards and entropy-modulated penalties.\n    Adaptive normalization balances fit metrics, utilization, and fragmentation control.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Metric computations\n    leftover = bins_remain_cap - item\n    origin_cap = np.max(bins_remain_cap)  # Infer bin capacity from max remaining\n    fit_quality = 1.0 / (leftover + eps)\n    tightness = item / (bins_remain_cap + eps)\n    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)\n    \n    # Z-score normalization across eligible bins\n    elig_fit, elig_tight = fit_quality[eligible], tightness[eligible]\n    elig_util, elig_left = utilization[eligible], leftover[eligible]\n    \n    # Calculate Z-scores for core metrics\n    z_fit = (fit_quality - np.mean(elig_fit)) / (np.std(elig_fit) + eps)\n    z_tight = (tightness - np.mean(elig_tight)) / (np.std(elig_tight) + eps)\n    z_util = (utilization - np.mean(elig_util)) / (np.std(elig_util) + eps)\n    \n    # Adaptive synergy with utilization coupling\n    synergy = z_tight * (1 + z_util)  # Prioritize tight fits in utilized bins\n    \n    # Exponential enhancer for utilization-tightness interaction\n    enhancer = np.exp(0.5 * utilization * tightness)  # Softer exponent than v0\n    \n    # Primary adaptive score\n    primary_score = synergy * enhancer\n    \n    # System entropy metrics\n    sys_std = bins_remain_cap.std()\n    sys_entropy = sys_std / (origin_cap + eps)\n    \n    # Fragmentation penalty with entropy scaling\n    frag_penalty = 1.0 - np.exp(-leftover / (origin_cap + eps))\n    frag_weight = 0.25 * (1.0 + sys_entropy)  # Stronger modulation than v0\n    \n    # Variance-aware reward (normalized over eligible bins)\n    sys_avg = np.mean(bins_remain_cap)\n    variance_component = -np.abs(leftover - sys_avg)\n    elig_var = variance_component[eligible]\n    z_variance = (variance_component - np.mean(elig_var)) / (np.std(elig_var) + eps)\n    variance_weight = 0.15 * (1.0 + sys_entropy)  # Adaptive scaling\n    \n    # Tie-breaker with entropy-coupled decay\n    tie_breaker = 0.075 * np.exp(-leftover) * (1.0 + sys_entropy)\n    \n    # Final score with multi-metric integration\n    scores = np.where(\n        eligible,\n        primary_score \n        - frag_weight * frag_penalty \n        + 0.1 * z_variance  # Reinforcement-inspired gain\n        + tie_breaker,\n        -np.inf\n    )\n    \n    return scores",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 53.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response8.txt_stdout.txt",
    "code_path": "problem_iter14_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive hybrid heuristic combining Z-normalized metrics, entropy-aware penalties, \n    and gradient-enhanced fragility control for online bin packing. Balances tight fits \n    with system-wide entropy optimization through dynamic weight allocation.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    # Fast path for negligible items\n    if item < 1e-9:\n        return np.where(bins_remain_cap >= 0, \n                      0.3 / (bins_remain_cap + 1e-4) - 0.1 * bins_remain_cap,\n                      -np.inf)\n    \n    eligible = bins_remain_cap >= item - 1e-9\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # System state analysis\n    sys_avg, sys_std = np.mean(bins_remain_cap), np.std(bins_remain_cap)\n    sys_cv = sys_std / (sys_avg + 1e-9) if sys_avg > 1e-9 else 0.0\n    large_item = item > sys_avg * 0.75 * (1 + 0.3 * sys_cv)\n    \n    # Core metric computation\n    residual = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    fit_power = 1.0 / (residual + 1e-6)\n    frag_indicator = (residual - sys_avg) / (sys_std + 1e-9)\n    \n    # Adaptive Z-normalization\n    def z_normalize(metric):\n        return (metric - np.mean(metric[eligible])) / (np.std(metric[eligible]) + 1e-9)\n    \n    z_fit = z_normalize(fit_power)\n    z_tight = z_normalize(tightness)\n    z_balance = z_normalize(-np.abs(residual - sys_avg))\n    \n    # Dynamic weight calculation\n    def calc_weight(metric):\n        m_var = np.var(metric[eligible])\n        return m_var * (1.5 if large_item else 1.0) * (1 + sys_cv)\n    \n    fit_weight = calc_weight(z_fit)\n    tight_weight = calc_weight(1 - tightness)\n    balance_weight = calc_weight(-np.abs(residual - sys_avg))\n    \n    # Synergy amplification\n    synergy = 1.0 + np.tanh(\n        0.5 * (z_fit * fit_weight - z_tight * tight_weight)\n    ) * (1 - 0.4 * sys_std / (sys_avg + 1e-9))\n    \n    # Entropy-aware penalty tiers\n    penalty = np.select(\n        [\n            frag_indicator > sys_cv + 1.0,\n            frag_indicator < -sys_cv - 0.7,\n            frag_indicator < 0.5\n        ],\n        [\n            0.4 * sys_std**0.7 * (1 + sys_cv**0.5),\n            0.2 * sys_std**0.4 / (sys_avg + 1e-9),\n            0.15 * sys_cv**0.6\n        ],\n        default=0.08 * (1 - sys_cv)\n    )\n    \n    # Gradient-enhanced scoring\n    base_score = (\n        z_fit * fit_weight * synergy * (1.2 if large_item else 0.9) +\n        z_balance * balance_weight * (0.8 + 0.5 * sys_cv) +\n        (1 - z_tight * tight_weight * (0.6 if large_item else 1.0))\n    )\n    \n    # Fragility-adjusted final score\n    final_score = np.where(\n        eligible,\n        base_score - residual * penalty * (1.3 if item < sys_avg else 1.0),\n        -np.inf\n    )\n    \n    return final_score",
    "response_id": 8,
    "tryHS": false,
    "obj": 96.9186278420423,
    "SLOC": 53.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response9.txt_stdout.txt",
    "code_path": "problem_iter14_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Z-normalized synergy metrics with predictive entropy weighting.\n    Uses fit/utilization synergy, variance delta analysis, and adaptive fragmentation control.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    origin_cap = np.max(bins_remain_cap)\n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Core metrics\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + eps)\n    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)\n    fit_quality = 1.0 / (leftover + eps)\n    \n    # Z-score normalization across eligible bins\n    fit_mean = fit_quality[eligible].mean()\n    fit_std = fit_quality[eligible].std()\n    z_fit = (fit_quality - fit_mean) / (fit_std + eps)\n    \n    tight_mean = tightness[eligible].mean()\n    tight_std = tightness[eligible].std()\n    z_tight = (tightness - tight_mean) / (tight_std + eps)\n    \n    util_mean = utilization[eligible].mean()\n    util_std = utilization[eligible].std()\n    z_util = (utilization - util_mean) / (util_std + eps)\n    \n    # Primary synergy and enhancer\n    synergy = z_tight * (1 + z_util)\n    enhancer = np.exp(utilization * tightness)\n    primary_score = synergy * enhancer\n    \n    # Fragmentation penalty with entropy scaling\n    sys_entropy = bins_remain_cap.std() / (origin_cap + eps)\n    frag_penalty = 1.0 - np.exp(-leftover / (origin_cap + eps))\n    frag_component = 0.2 * (1 + sys_entropy) * frag_penalty\n    \n    # Predictive entropy delta calculation\n    sum_cap = bins_remain_cap.sum()\n    sum_sq = (bins_remain_cap ** 2).sum()\n    N = bins_remain_cap.size\n    mean_old = sum_cap / N\n    var_old = (sum_sq / N) - mean_old**2\n    \n    elig_remain = bins_remain_cap[eligible]\n    delta_sq_i = (elig_remain - item)**2 - elig_remain**2\n    new_sum_sq_i = sum_sq + delta_sq_i\n    new_mean_i = (sum_cap - item) / N\n    var_new_i = (new_sum_sq_i / N) - (new_mean_i**2)\n    var_delta_i = var_old - var_new_i\n    \n    # Normalize entropy delta across eligible bins\n    vd_mean = var_delta_i.mean()\n    vd_std = var_delta_i.std()\n    norm_entropy_delta = (var_delta_i - vd_mean) / (vd_std + eps)\n    \n    # Adaptive entropy weighting\n    system_cv = bins_remain_cap.std() / (bins_remain_cap.mean() + eps)\n    large_item = item > mean_old\n    weight_entropy = 0.5 * (1 + system_cv) * (1.5 if large_item else 0.5)\n    \n    entropy_component = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    entropy_component[eligible] = (weight_entropy * norm_entropy_delta).astype(np.float64)\n    \n    # Tie-breaker with sensitivity damping\n    tie_breaker = 0.01 * np.exp(-leftover) * (1 + sys_entropy)\n    \n    # Final score assembly\n    scores = np.where(\n        eligible,\n        primary_score - frag_component + entropy_component + tie_breaker,\n        -np.inf\n    )\n    \n    return scores",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 53.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  }
]