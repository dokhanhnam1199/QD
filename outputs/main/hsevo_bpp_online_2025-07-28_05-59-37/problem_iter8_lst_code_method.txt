{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive hybrid priority combining z-scored fit quality/capacity, exponential utilization enhancement, and layered tightness-weighted metrics.\n    Encourages best-fit in high-utilization bins while dynamically balancing with worst-fit for small items.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    leftover = bins_remain_cap - item\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    # Z-score normalized fit and capacity for fair comparison\n    fit_quality = 1.0 / (leftover + 1e-9)\n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    \n    if elig_fit.size == 0:\n        return np.full_like(fit_quality, -np.inf, dtype=np.float64)\n    \n    # Z-score calculation\n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    \n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    # Dynamic best/worst-fit blend with exponential utilization boost\n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    enhancer = np.exp(utilization * tightness)  # Exponential amplification for utilized bins\n    \n    return np.where(eligible, primary_score * enhancer, -np.inf)\n\n[Heuristics 2nd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive hybrid priority combining z-scored fit quality/capacity, exponential utilization enhancement, and layered tightness-weighted metrics.\n    Encourages best-fit in high-utilization bins while dynamically balancing with worst-fit for small items.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    leftover = bins_remain_cap - item\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    # Z-score normalized fit and capacity for fair comparison\n    fit_quality = 1.0 / (leftover + 1e-9)\n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    \n    if elig_fit.size == 0:\n        return np.full_like(fit_quality, -np.inf, dtype=np.float64)\n    \n    # Z-score calculation\n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    \n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    # Dynamic best/worst-fit blend with exponential utilization boost\n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    enhancer = np.exp(utilization * tightness)  # Exponential amplification for utilized bins\n    \n    return np.where(eligible, primary_score * enhancer, -np.inf)\n\n[Heuristics 3rd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive synergy of inv_leftover*utilization for tight fits + exponential waste decay for fragmentation reduction.\"\"\"\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    if not mask.any():\n        return scores\n    \n    eps = 1e-9\n    remaining = bins_remain_cap[mask]\n    \n    # Core metrics from both heuristics\n    inv_leftover = 1.0 / (remaining - item + eps)  # Maximize for tight fits\n    utilization = item / (remaining + eps)  # Prioritize high space efficiency\n    exp_waste = np.exp(-(remaining - item))  # Penalize large leftover exponentially\n    \n    # Hierarchical synergy: Tight fit + stable baseline + entropy-aware tie-breaking\n    multiplicative_term = inv_leftover * utilization  # Tight-fit dominance\n    additive_term = utilization  # Baseline utilization\n    exp_tie_breaker = 0.1 * exp_waste  # Non-linear tie-breaking\n    \n    # Final score combining hierarchical objectives\n    scores[mask] = multiplicative_term + additive_term + exp_tie_breaker\n    return scores\n\n[Heuristics 4th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive synergy of inv_leftover*utilization for tight fits + exponential waste decay for fragmentation reduction.\"\"\"\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    if not mask.any():\n        return scores\n    \n    eps = 1e-9\n    remaining = bins_remain_cap[mask]\n    \n    # Core metrics from both heuristics\n    inv_leftover = 1.0 / (remaining - item + eps)  # Maximize for tight fits\n    utilization = item / (remaining + eps)  # Prioritize high space efficiency\n    exp_waste = np.exp(-(remaining - item))  # Penalize large leftover exponentially\n    \n    # Hierarchical synergy: Tight fit + stable baseline + entropy-aware tie-breaking\n    multiplicative_term = inv_leftover * utilization  # Tight-fit dominance\n    additive_term = utilization  # Baseline utilization\n    exp_tie_breaker = 0.1 * exp_waste  # Non-linear tie-breaking\n    \n    # Final score combining hierarchical objectives\n    scores[mask] = multiplicative_term + additive_term + exp_tie_breaker\n    return scores\n\n[Heuristics 5th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combine v0's dynamic class with v1's balance using adaptive weights.\n\n    Uses item classification for epsilon scaling and adds system balance scaled by fragmentation level.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    threshold = np.mean(bins_remain_cap)\n    large_item = item > threshold\n\n    valid = bins_remain_cap >= item\n    if not np.any(valid):\n        return -np.inf * np.ones_like(bins_remain_cap)\n    \n    remaining_after = bins_remain_cap - item\n\n    # Base priority from v0\n    epsilon = 1e-6 if large_item else 1e-3\n    base_priority = -remaining_after - epsilon * bins_remain_cap\n\n    # System-wide balance metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    # Adaptive balance weight: higher for fragmentation (system_cv) and small items\n    load_balance_score = -np.abs(remaining_after - system_avg)\n    balance_weight = 0.1 * system_cv * (2 if not large_item else 1)\n\n    # Combine components with dynamic synergy\n    priority = base_priority + balance_weight * load_balance_score\n\n    return np.where(valid, priority, -np.inf)\n\n[Heuristics 6th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combine v0's dynamic class with v1's balance using adaptive weights.\n\n    Uses item classification for epsilon scaling and adds system balance scaled by fragmentation level.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    threshold = np.mean(bins_remain_cap)\n    large_item = item > threshold\n\n    valid = bins_remain_cap >= item\n    if not np.any(valid):\n        return -np.inf * np.ones_like(bins_remain_cap)\n    \n    remaining_after = bins_remain_cap - item\n\n    # Base priority from v0\n    epsilon = 1e-6 if large_item else 1e-3\n    base_priority = -remaining_after - epsilon * bins_remain_cap\n\n    # System-wide balance metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    # Adaptive balance weight: higher for fragmentation (system_cv) and small items\n    load_balance_score = -np.abs(remaining_after - system_avg)\n    balance_weight = 0.1 * system_cv * (2 if not large_item else 1)\n\n    # Combine components with dynamic synergy\n    priority = base_priority + balance_weight * load_balance_score\n\n    return np.where(valid, priority, -np.inf)\n\n[Heuristics 7th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combine v0's dynamic class with v1's balance using adaptive weights.\n\n    Uses item classification for epsilon scaling and adds system balance scaled by fragmentation level.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    threshold = np.mean(bins_remain_cap)\n    large_item = item > threshold\n\n    valid = bins_remain_cap >= item\n    if not np.any(valid):\n        return -np.inf * np.ones_like(bins_remain_cap)\n    \n    remaining_after = bins_remain_cap - item\n\n    # Base priority from v0\n    epsilon = 1e-6 if large_item else 1e-3\n    base_priority = -remaining_after - epsilon * bins_remain_cap\n\n    # System-wide balance metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    # Adaptive balance weight: higher for fragmentation (system_cv) and small items\n    load_balance_score = -np.abs(remaining_after - system_avg)\n    balance_weight = 0.1 * system_cv * (2 if not large_item else 1)\n\n    # Combine components with dynamic synergy\n    priority = base_priority + balance_weight * load_balance_score\n\n    return np.where(valid, priority, -np.inf)\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Priority combining inverse/exp tight fit with bin utilization via multiplicative synergy to prevent fragmentation.\"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = bins_remain_cap.max()\n    eps = 1e-9\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    \n    if not mask.any():\n        return scores\n    \n    remaining = bins_remain_cap[mask]\n    inv_waste = 1.0 / (remaining - item + eps)\n    exp_waste = np.exp(-(remaining - item))\n    tight_fit = inv_waste + exp_waste  # Tightness focus\n    \n    bin_utilization = (orig_cap - remaining) / (orig_cap + eps)  # Current utilization\n    \n    # Multiplicative synergy: amplify tight_fit in utilized bins to guide system towards packing dense clusters\n    scores[mask] = tight_fit * (1 + bin_utilization)\n    \n    return scores\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Priority combining inverse/exp tight fit with bin utilization via multiplicative synergy to prevent fragmentation.\"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = bins_remain_cap.max()\n    eps = 1e-9\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    \n    if not mask.any():\n        return scores\n    \n    remaining = bins_remain_cap[mask]\n    inv_waste = 1.0 / (remaining - item + eps)\n    exp_waste = np.exp(-(remaining - item))\n    tight_fit = inv_waste + exp_waste  # Tightness focus\n    \n    bin_utilization = (orig_cap - remaining) / (orig_cap + eps)  # Current utilization\n    \n    # Multiplicative synergy: amplify tight_fit in utilized bins to guide system towards packing dense clusters\n    scores[mask] = tight_fit * (1 + bin_utilization)\n    \n    return scores\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Multi-objective priority function with dynamic sigmoid blending, adaptive normalization, \n    and contextual reinforcement-inspired adjustments for online BPP.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    # Core metrics\n    leftover = bins_remain_cap - item\n    fit_quality = 1.0 / (leftover + 1e-9)\n    space_quality = bins_remain_cap\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    \n    # Contextual bottleneck analysis\n    eligible_caps = bins_remain_cap[eligible]\n    med_cap = np.median(eligible_caps)\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    # Dynamic best-worst blending via logistic sensitivity\n    x = item / (med_cap + 1e-9)\n    blending = 1.0 / (1 + np.exp(-5 * (x - 0.5)))  # Midpoint at item=0.5*med_cap\n    \n    # Z-score normalization across multiple axes\n    fit_sub = fit_quality[eligible]\n    space_sub = space_quality[eligible]\n    fit_norm = (fit_quality - np.mean(fit_sub)) / (np.std(fit_sub) + 1e-9)\n    space_norm = (space_quality - np.mean(space_sub)) / (np.std(space_sub) + 1e-9)\n    \n    # Primary multi-objective score with gap-aware bonus\n    primary = blending * fit_norm + (1 - blending) * space_norm\n    \n    # Non-linear efficiency modifier for residual space\n    residual_efficiency = -np.log(1 + leftover / (orig_cap + 1e-9))\n    secondary = residual_efficiency * (tightness / (np.sqrt(utilization + 1e-9) + 1))\n    \n    # Utilization enhancer with adaptive regulation\n    fragility = np.abs(orig_cap - bins_remain_cap) / (orig_cap + 1e-9)\n    confidence = np.clip((1 - utilization)**3, 0, 1)\n    enhancer = np.exp(2 * tightness) * confidence\n    \n    # Reinforcement-inspired dynamic calibration\n    reinforcement_gain = 1 + np.clip((1 - x)**2 * (1 - utilization) * fragility, 0, 3)\n    \n    # Hierarchical priority combination\n    return np.where(\n        eligible,\n        (primary + 0.5 * secondary) * enhancer * reinforcement_gain,\n        -np.inf\n    )\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins using hybrid adaptive metrics: combines localized fit quality (tightness, utilization) with system-wide entropy reduction.\n    Dynamic weights adjust via relative item size, bin population regularity (CV), and fragmentation sensitivity.\n    \"\"\"\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    if not mask.any():\n        return scores\n\n    eps = 1e-9\n    remaining = bins_remain_cap[mask].astype(np.float64)\n    \n    # Quantify local bin context (valid bins only)\n    mean_remaining = remaining.mean()\n    std_remaining = remaining.std()\n    relative_size = np.clip(item / (mean_remaining + eps), 0.0, 1.0)\n    \n    # Global bin chaos assessment (entire packing state)\n    system_avg = bins_remain_cap.mean().astype(np.float64)\n    system_std = bins_remain_cap.std().astype(np.float64)\n    system_cv = system_std / (system_avg + eps) if system_avg > 0 else float('inf')\n    \n    # Phase 1: Critical Fit Metrics (weighted by local adaptivity)\n    w_inv = relative_size**2 + std_remaining/(mean_remaining + eps)  # Favors tight fits when item is large\n    w_uti = (1 - relative_size)**2                                  # Rewards high utilization for smaller items\n    w_exp = 0.5 * (1 + relative_size * std_remaining)               # Exponential waste penalty\n    \n    # Phase 2: System Stabilization Forces\n    w_balance = 2.0 * system_cv              # Stronger stabilization needed with higher variance\n    frag_weight = (1.0/system_cv) if system_cv > 1 else 1.0        # Adaptive fragmentation control\n    \n    # Metric Calculations\n    leftover = remaining - item\n    inv_leftover = 1.0 / (leftover + eps)\n    utilization = item / (remaining + eps)\n    exp_waste = np.exp(-leftover)\n    \n    # Bin-level behaviors\n    remaining_after = remaining - item\n    balance_term = -np.abs(remaining_after - system_avg)              # Alignment with system average\n    frag_penalty = np.log1p(1.0 / (leftover + system_cv + eps))       # Penalizes high fragmentation risks\n    \n    # Dynamic base score calculation\n    main_components = [\n        w_inv * inv_leftover,\n        w_uti * utilization,\n        w_exp * exp_waste,\n        w_balance * balance_term,\n        frag_weight * frag_penalty\n    ]\n    main_score = np.sum(main_components, axis=0)\n    \n    # Phase 3: Entropy-aware Final Adjustment (variance minimization)\n    bin_count = bins_remain_cap.size\n    sum_total = bins_remain_cap.sum().astype(np.float64)\n    sum_squares = (bins_remain_cap**2).sum().astype(np.float64)\n    \n    delta_sq = (-2 * item * remaining) + (item**2)\n    new_squares = sum_squares + delta_sq\n    mu_new = (sum_total - item) / bin_count\n    var_new = (new_squares / bin_count) - mu_new**2\n    \n    # Intelligent tie-breaking mechanism with CV-aware scaling\n    var_sensitivity = 0.1 * (1 + np.sqrt(system_cv + eps))\n    tiebreaker = var_sensitivity * (-var_new) / (np.sqrt(np.abs(var_new) + eps) + 1e-5)\n    \n    # Final score with hierarchical optimization layers\n    scores[mask] = main_score + tiebreaker\n    \n    return scores\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins using hybrid adaptive metrics: combines localized fit quality (tightness, utilization) with system-wide entropy reduction.\n    Dynamic weights adjust via relative item size, bin population regularity (CV), and fragmentation sensitivity.\n    \"\"\"\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    if not mask.any():\n        return scores\n\n    eps = 1e-9\n    remaining = bins_remain_cap[mask].astype(np.float64)\n    \n    # Quantify local bin context (valid bins only)\n    mean_remaining = remaining.mean()\n    std_remaining = remaining.std()\n    relative_size = np.clip(item / (mean_remaining + eps), 0.0, 1.0)\n    \n    # Global bin chaos assessment (entire packing state)\n    system_avg = bins_remain_cap.mean().astype(np.float64)\n    system_std = bins_remain_cap.std().astype(np.float64)\n    system_cv = system_std / (system_avg + eps) if system_avg > 0 else float('inf')\n    \n    # Phase 1: Critical Fit Metrics (weighted by local adaptivity)\n    w_inv = relative_size**2 + std_remaining/(mean_remaining + eps)  # Favors tight fits when item is large\n    w_uti = (1 - relative_size)**2                                  # Rewards high utilization for smaller items\n    w_exp = 0.5 * (1 + relative_size * std_remaining)               # Exponential waste penalty\n    \n    # Phase 2: System Stabilization Forces\n    w_balance = 2.0 * system_cv              # Stronger stabilization needed with higher variance\n    frag_weight = (1.0/system_cv) if system_cv > 1 else 1.0        # Adaptive fragmentation control\n    \n    # Metric Calculations\n    leftover = remaining - item\n    inv_leftover = 1.0 / (leftover + eps)\n    utilization = item / (remaining + eps)\n    exp_waste = np.exp(-leftover)\n    \n    # Bin-level behaviors\n    remaining_after = remaining - item\n    balance_term = -np.abs(remaining_after - system_avg)              # Alignment with system average\n    frag_penalty = np.log1p(1.0 / (leftover + system_cv + eps))       # Penalizes high fragmentation risks\n    \n    # Dynamic base score calculation\n    main_components = [\n        w_inv * inv_leftover,\n        w_uti * utilization,\n        w_exp * exp_waste,\n        w_balance * balance_term,\n        frag_weight * frag_penalty\n    ]\n    main_score = np.sum(main_components, axis=0)\n    \n    # Phase 3: Entropy-aware Final Adjustment (variance minimization)\n    bin_count = bins_remain_cap.size\n    sum_total = bins_remain_cap.sum().astype(np.float64)\n    sum_squares = (bins_remain_cap**2).sum().astype(np.float64)\n    \n    delta_sq = (-2 * item * remaining) + (item**2)\n    new_squares = sum_squares + delta_sq\n    mu_new = (sum_total - item) / bin_count\n    var_new = (new_squares / bin_count) - mu_new**2\n    \n    # Intelligent tie-breaking mechanism with CV-aware scaling\n    var_sensitivity = 0.1 * (1 + np.sqrt(system_cv + eps))\n    tiebreaker = var_sensitivity * (-var_new) / (np.sqrt(np.abs(var_new) + eps) + 1e-5)\n    \n    # Final score with hierarchical optimization layers\n    scores[mask] = main_score + tiebreaker\n    \n    return scores\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins using hybrid adaptive metrics: combines localized fit quality (tightness, utilization) with system-wide entropy reduction.\n    Dynamic weights adjust via relative item size, bin population regularity (CV), and fragmentation sensitivity.\n    \"\"\"\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    if not mask.any():\n        return scores\n\n    eps = 1e-9\n    remaining = bins_remain_cap[mask].astype(np.float64)\n    \n    # Quantify local bin context (valid bins only)\n    mean_remaining = remaining.mean()\n    std_remaining = remaining.std()\n    relative_size = np.clip(item / (mean_remaining + eps), 0.0, 1.0)\n    \n    # Global bin chaos assessment (entire packing state)\n    system_avg = bins_remain_cap.mean().astype(np.float64)\n    system_std = bins_remain_cap.std().astype(np.float64)\n    system_cv = system_std / (system_avg + eps) if system_avg > 0 else float('inf')\n    \n    # Phase 1: Critical Fit Metrics (weighted by local adaptivity)\n    w_inv = relative_size**2 + std_remaining/(mean_remaining + eps)  # Favors tight fits when item is large\n    w_uti = (1 - relative_size)**2                                  # Rewards high utilization for smaller items\n    w_exp = 0.5 * (1 + relative_size * std_remaining)               # Exponential waste penalty\n    \n    # Phase 2: System Stabilization Forces\n    w_balance = 2.0 * system_cv              # Stronger stabilization needed with higher variance\n    frag_weight = (1.0/system_cv) if system_cv > 1 else 1.0        # Adaptive fragmentation control\n    \n    # Metric Calculations\n    leftover = remaining - item\n    inv_leftover = 1.0 / (leftover + eps)\n    utilization = item / (remaining + eps)\n    exp_waste = np.exp(-leftover)\n    \n    # Bin-level behaviors\n    remaining_after = remaining - item\n    balance_term = -np.abs(remaining_after - system_avg)              # Alignment with system average\n    frag_penalty = np.log1p(1.0 / (leftover + system_cv + eps))       # Penalizes high fragmentation risks\n    \n    # Dynamic base score calculation\n    main_components = [\n        w_inv * inv_leftover,\n        w_uti * utilization,\n        w_exp * exp_waste,\n        w_balance * balance_term,\n        frag_weight * frag_penalty\n    ]\n    main_score = np.sum(main_components, axis=0)\n    \n    # Phase 3: Entropy-aware Final Adjustment (variance minimization)\n    bin_count = bins_remain_cap.size\n    sum_total = bins_remain_cap.sum().astype(np.float64)\n    sum_squares = (bins_remain_cap**2).sum().astype(np.float64)\n    \n    delta_sq = (-2 * item * remaining) + (item**2)\n    new_squares = sum_squares + delta_sq\n    mu_new = (sum_total - item) / bin_count\n    var_new = (new_squares / bin_count) - mu_new**2\n    \n    # Intelligent tie-breaking mechanism with CV-aware scaling\n    var_sensitivity = 0.1 * (1 + np.sqrt(system_cv + eps))\n    tiebreaker = var_sensitivity * (-var_new) / (np.sqrt(np.abs(var_new) + eps) + 1e-5)\n    \n    # Final score with hierarchical optimization layers\n    scores[mask] = main_score + tiebreaker\n    \n    return scores\n\n[Heuristics 14th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive priority combining tight-fit best-fit/worst-fit + normalized utilization bias + exponential fragmentation penalty.\"\"\"\n    possible = bins_remain_cap >= item\n    leftover = bins_remain_cap - item\n    \n    if not possible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Estimate system state (bin capacity)\n    C_est = bins_remain_cap.max() if bins_remain_cap.size > 0 else 1.0\n    \n    # Dynamic tight-fit classification (>50% of current bin cap)\n    tight = (item > bins_remain_cap / 2) & possible\n    \n    # Best Fit for tight fits, Worst Fit for remaining\n    best_fit = -leftover\n    worst_fit = leftover\n    primary = np.where(tight, best_fit, worst_fit)\n    \n    # Tie-breaker 1: Normalized utilization (higher filled fraction preferred)\n    filled_frac = (C_est - bins_remain_cap) / C_est\n    normalized_util = filled_frac\n    \n    # Tie-breaker 2: Exponential penalty for fragment leftovers (e^-lambda*x)\n    frag_penalty = -np.expm1(-(leftover / C_est))  # Smooths penalty at zero\n    \n    # Weights: Tie-breakers with entropy-aware scaling\n    util_weight = 1e-3 * (1 + normalized_util)  \n    frag_weight = 1e-4\n    \n    # Composite score with synergistic multi metrics\n    priority = primary + util_weight * normalized_util + frag_weight * frag_penalty\n    \n    return np.where(possible, priority, -np.inf)\n\n[Heuristics 15th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive priority combining tight-fit best-fit/worst-fit + normalized utilization bias + exponential fragmentation penalty.\"\"\"\n    possible = bins_remain_cap >= item\n    leftover = bins_remain_cap - item\n    \n    if not possible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Estimate system state (bin capacity)\n    C_est = bins_remain_cap.max() if bins_remain_cap.size > 0 else 1.0\n    \n    # Dynamic tight-fit classification (>50% of current bin cap)\n    tight = (item > bins_remain_cap / 2) & possible\n    \n    # Best Fit for tight fits, Worst Fit for remaining\n    best_fit = -leftover\n    worst_fit = leftover\n    primary = np.where(tight, best_fit, worst_fit)\n    \n    # Tie-breaker 1: Normalized utilization (higher filled fraction preferred)\n    filled_frac = (C_est - bins_remain_cap) / C_est\n    normalized_util = filled_frac\n    \n    # Tie-breaker 2: Exponential penalty for fragment leftovers (e^-lambda*x)\n    frag_penalty = -np.expm1(-(leftover / C_est))  # Smooths penalty at zero\n    \n    # Weights: Tie-breakers with entropy-aware scaling\n    util_weight = 1e-3 * (1 + normalized_util)  \n    frag_weight = 1e-4\n    \n    # Composite score with synergistic multi metrics\n    priority = primary + util_weight * normalized_util + frag_weight * frag_penalty\n    \n    return np.where(possible, priority, -np.inf)\n\n[Heuristics 16th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive priority combining tight-fit best-fit/worst-fit + normalized utilization bias + exponential fragmentation penalty.\"\"\"\n    possible = bins_remain_cap >= item\n    leftover = bins_remain_cap - item\n    \n    if not possible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Estimate system state (bin capacity)\n    C_est = bins_remain_cap.max() if bins_remain_cap.size > 0 else 1.0\n    \n    # Dynamic tight-fit classification (>50% of current bin cap)\n    tight = (item > bins_remain_cap / 2) & possible\n    \n    # Best Fit for tight fits, Worst Fit for remaining\n    best_fit = -leftover\n    worst_fit = leftover\n    primary = np.where(tight, best_fit, worst_fit)\n    \n    # Tie-breaker 1: Normalized utilization (higher filled fraction preferred)\n    filled_frac = (C_est - bins_remain_cap) / C_est\n    normalized_util = filled_frac\n    \n    # Tie-breaker 2: Exponential penalty for fragment leftovers (e^-lambda*x)\n    frag_penalty = -np.expm1(-(leftover / C_est))  # Smooths penalty at zero\n    \n    # Weights: Tie-breakers with entropy-aware scaling\n    util_weight = 1e-3 * (1 + normalized_util)  \n    frag_weight = 1e-4\n    \n    # Composite score with synergistic multi metrics\n    priority = primary + util_weight * normalized_util + frag_weight * frag_penalty\n    \n    return np.where(possible, priority, -np.inf)\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Multi-objective priority combining dynamic BF/WF weights, fragmentation penalty, and system balance.\n    Uses online entropy-aware balancing and exponential waste sensitivity.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    valid = bins_remain_cap >= item\n    if not valid.any():\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    C_est = bins_remain_cap.max()\n    relative_size = item / (C_est + 1e-9)  # Avoid division by zero\n    \n    # Dynamic BF/WF blending with non-linear weighting\n    weight_bf = np.clip(relative_size ** 2 + 0.1 * (C_est - bins_remain_cap.mean()) / C_est, 0, 1)\n    best_fit = -(bins_remain_cap - item)  # Higher score = better BF fit\n    worst_fit = bins_remain_cap           # Higher score = more residual space\n    primary_score = weight_bf * best_fit + (1 - weight_bf) * worst_fit\n    \n    # Fragmentation penalty using sigmoid-decay for marginal spaces\n    leftover = bins_remain_cap - item\n    frag_score = -np.exp(-leftover / (0.2 * C_est + 1e-9))  # Exponential penalty for small leftovers\n    \n    # Entropy-aware balancing using median alignment and variance reward\n    med = np.median(bins_remain_cap)\n    distance = np.abs(bins_remain_cap - med)\n    balance_score = -0.1 * distance / (C_est + 1e-9)  # Encourage utilization convergence\n    \n    # Tie-breaking layer for degenerate cases\n    entropy_tiebreaker = 1e-4 * (np.random.rand(*bins_remain_cap.shape) - 0.5)  # Small stochastic component\n    \n    # Final score with validity masking\n    scores = np.where(valid, primary_score + frag_score + balance_score + entropy_tiebreaker, -np.inf)\n    return scores\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Multi-objective priority combining dynamic BF/WF weights, fragmentation penalty, and system balance.\n    Uses online entropy-aware balancing and exponential waste sensitivity.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    valid = bins_remain_cap >= item\n    if not valid.any():\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    C_est = bins_remain_cap.max()\n    relative_size = item / (C_est + 1e-9)  # Avoid division by zero\n    \n    # Dynamic BF/WF blending with non-linear weighting\n    weight_bf = np.clip(relative_size ** 2 + 0.1 * (C_est - bins_remain_cap.mean()) / C_est, 0, 1)\n    best_fit = -(bins_remain_cap - item)  # Higher score = better BF fit\n    worst_fit = bins_remain_cap           # Higher score = more residual space\n    primary_score = weight_bf * best_fit + (1 - weight_bf) * worst_fit\n    \n    # Fragmentation penalty using sigmoid-decay for marginal spaces\n    leftover = bins_remain_cap - item\n    frag_score = -np.exp(-leftover / (0.2 * C_est + 1e-9))  # Exponential penalty for small leftovers\n    \n    # Entropy-aware balancing using median alignment and variance reward\n    med = np.median(bins_remain_cap)\n    distance = np.abs(bins_remain_cap - med)\n    balance_score = -0.1 * distance / (C_est + 1e-9)  # Encourage utilization convergence\n    \n    # Tie-breaking layer for degenerate cases\n    entropy_tiebreaker = 1e-4 * (np.random.rand(*bins_remain_cap.shape) - 0.5)  # Small stochastic component\n    \n    # Final score with validity masking\n    scores = np.where(valid, primary_score + frag_score + balance_score + entropy_tiebreaker, -np.inf)\n    return scores\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Sigmoid blending of Best and Worst Fit with adaptive utilization tie-breaker and leftover variance scaling.\n    \n    Primary priority combines BFT tightness (negative leftover) and WFT spreading \n    (bin_remaining) through a smooth function of item size relative to system average. \n    Adaptive epsilon scaling preserves utilization dynamics for robust tie-breaking.\n    \"\"\"\n    mask = bins_remain_cap >= item\n    priority = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    if not bins_remain_cap.size or not np.any(mask):\n        return priority\n\n    # Core fit metrics\n    leftover = bins_remain_cap - item\n    bft = -leftover  # Best Fit Term (negative leftover)\n    wft = bins_remain_cap  # Worst Fit candidate bins\n\n    # Dynamic blending factor via item-to-average ratio\n    avg_remaining = np.mean(bins_remain_cap)\n    # Normalize difference to system-scale (prevent saturation at extreme values)\n    sigmoid_input = (item - avg_remaining) / (avg_remaining + 1e-8)  \n    # Higher sigmoid_weight = preferred WFT for smaller items\n    sigmoid_weight = 1.0 / (1.0 + np.exp(sigmoid_input))  \n\n    # Blend BF/WF preferences: WFT dominates for smaller items\n    combined = (1 - sigmoid_weight) * bft + sigmoid_weight * wft\n\n    # Utilization tie-breaker (item / bin_remaining where feasible)\n    utilization = np.zeros_like(combined)\n    np.divide(item, bins_remain_cap, where=mask, out=utilization)\n\n    # Adaptive epsilon scaling from leftover variance\n    valid_leftover = leftover[mask]\n    std_leftover = np.std(valid_leftover) if valid_leftover.size > 1 else 1e-6\n    epsilon = 1e-4 / (std_leftover + 1e-8)\n\n    # Final priority with utility augmentation\n    priority[mask] = combined[mask] + epsilon * utilization[mask]\n    return priority\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Sigmoid blending of Best and Worst Fit with adaptive utilization tie-breaker and leftover variance scaling.\n    \n    Primary priority combines BFT tightness (negative leftover) and WFT spreading \n    (bin_remaining) through a smooth function of item size relative to system average. \n    Adaptive epsilon scaling preserves utilization dynamics for robust tie-breaking.\n    \"\"\"\n    mask = bins_remain_cap >= item\n    priority = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    if not bins_remain_cap.size or not np.any(mask):\n        return priority\n\n    # Core fit metrics\n    leftover = bins_remain_cap - item\n    bft = -leftover  # Best Fit Term (negative leftover)\n    wft = bins_remain_cap  # Worst Fit candidate bins\n\n    # Dynamic blending factor via item-to-average ratio\n    avg_remaining = np.mean(bins_remain_cap)\n    # Normalize difference to system-scale (prevent saturation at extreme values)\n    sigmoid_input = (item - avg_remaining) / (avg_remaining + 1e-8)  \n    # Higher sigmoid_weight = preferred WFT for smaller items\n    sigmoid_weight = 1.0 / (1.0 + np.exp(sigmoid_input))  \n\n    # Blend BF/WF preferences: WFT dominates for smaller items\n    combined = (1 - sigmoid_weight) * bft + sigmoid_weight * wft\n\n    # Utilization tie-breaker (item / bin_remaining where feasible)\n    utilization = np.zeros_like(combined)\n    np.divide(item, bins_remain_cap, where=mask, out=utilization)\n\n    # Adaptive epsilon scaling from leftover variance\n    valid_leftover = leftover[mask]\n    std_leftover = np.std(valid_leftover) if valid_leftover.size > 1 else 1e-6\n    epsilon = 1e-4 / (std_leftover + 1e-8)\n\n    # Final priority with utility augmentation\n    priority[mask] = combined[mask] + epsilon * utilization[mask]\n    return priority\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}