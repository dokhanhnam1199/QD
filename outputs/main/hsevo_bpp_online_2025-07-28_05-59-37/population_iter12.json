[
  {
    "stdout_filepath": "problem_iter11_response0.txt_stdout.txt",
    "code_path": "problem_iter11_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Enhanced Z-synergy with entropy-control and dynamic reinforcement.\n    Combines adaptive fit/space blending (v1), Z-normalized balance terms (v0), \n    and entropy-driven variance penalties with reinforcement multipliers.\"\"\"\n    \n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    valid_mask = bins_remain_cap >= item\n    if not np.any(valid_mask):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    # System characterization\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 1e-9 else 0.0\n    orig_cap = bins_remain_cap.max()  # Fixed-bin assumption\n    \n    # Item classification\n    large_item = item > system_avg\n    leftover = bins_remain_cap - item\n    \n    # Adaptive blending components (v1)\n    eligible_rem = bins_remain_cap[valid_mask]\n    tightness = item / (eligible_rem + 1e-9)\n    rel_size = item / (np.median(eligible_rem) + 1e-9)\n    blending = 1.0 / (1.0 + np.exp(-5 * (rel_size - 0.5)))\n    \n    # Z-score metrics (v0 + v1 synergy)\n    fit_metric = 1.0 / (leftover + 1e-9)\n    space_metric = bins_remain_cap.astype(np.float64)\n    \n    def z_score(x, where_mask):\n        mean = x[where_mask].mean()\n        std = x[where_mask].std() + 1e-9\n        return (x - mean) / std\n\n    fit_z = z_score(fit_metric, valid_mask)\n    space_z = z_score(space_metric, valid_mask)\n    tight_z = z_score(-leftover, valid_mask)  # Natural tightness metric\n    \n    # Utilization boost (dynamic adaptation)\n    boost_factor = 2.5 if rel_size < 0.7 else 1.5\n    util_boost = np.exp(boost_factor * tightness * valid_mask)\n    \n    # Balance dynamics (v0 base + entropy sensitivity)\n    balance_comp = -np.abs(leftover - system_avg) / (system_std + 1e-9)\n    balance_weight = 1.8 * np.sqrt(system_cv) * (1.3 if not large_item else 1.0)\n    \n    # Reinforcement multipliers (v1)\n    bin_util = (orig_cap - bins_remain_cap) / orig_cap\n    fragility = np.abs(orig_cap - bins_remain_cap) / (orig_cap + 1e-9)\n    reinforcer = 1 + np.clip((1 - rel_size)**2 * (1 - bin_util) * fragility, 0, 3)\n    \n    # Entropy tiebreaker with variance control (v1)\n    bin_count = bins_remain_cap.size\n    sum_total = bins_remain_cap.sum()\n    sum_sq = (bins_remain_cap**2).sum()\n    new_sum_sq = sum_sq - 2*item*bins_remain_cap + item**2\n    mu_new = (sum_total - item) / bin_count\n    var_new = (new_sum_sq / bin_count) - mu_new**2\n    var_term = -0.3 * var_new / (np.sqrt(np.abs(var_new) + 1e-9) + 1e-5)\n    \n    # Priority construction with hybrid gains\n    blended_score = blending * fit_z + (1 - blending) * space_z\n    synergy = blended_score * (1 + 0.5 * tight_z + 0.3 * util_boost[valid_mask])\n    \n    priority = np.where(\n        valid_mask,\n        (synergy + balance_weight * balance_comp) * reinforcer + var_term,\n        -np.inf\n    )\n    \n    return priority",
    "response_id": 0,
    "tryHS": false,
    "obj": 49.222177901874765,
    "SLOC": 35.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response1.txt_stdout.txt",
    "code_path": "problem_iter11_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    v2.3: Adaptive multi-layer synergy with variance-controlled Z-scoring, \n    gradient-harnessed exponential enhancer, and entropy-covariance balancing.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    # Edge case: negligible item or bin capacity\n    if item <= 1e-9:\n        return np.where(bins_remain_cap >= 0, bins_remain_cap * 0.0, -np.inf)\n    \n    # Identify eligible bins where item fits\n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # Core metrics for eligible bins\n    e_rc = bins_remain_cap[eligible]\n    e_leftover = e_rc - item\n    e_tightness = item / (e_rc + 1e-9)\n    e_fit_quality = 1.0 / (e_leftover + 1e-9)\n    \n    # System-wide statistics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 0 else 0\n    \n    # Z-score normalization for dynamic metric adaptation\n    if len(e_rc) > 1:\n        z_cap = (e_rc - e_rc.mean()) / (e_rc.std() + 1e-9)\n        z_fit = (e_fit_quality - e_fit_quality.mean()) / (e_fit_quality.std() + 1e-9)\n    else:\n        z_cap = np.zeros_like(e_rc)\n        z_fit = np.zeros_like(e_fit_quality)\n    \n    # Dynamic weighting based on metric variance dominance\n    var_fit = e_fit_quality.var() if len(e_rc) > 1 else 0\n    var_cap = e_rc.var() if len(e_rc) > 1 else 0\n    cross_weight = var_cap / (var_fit + var_cap + 1e-9)\n    \n    # Hybrid metric with continuity-aware blending\n    composite_z = cross_weight * z_fit + (1 - cross_weight) * z_cap\n    alpha = np.clip(1.0 - e_tightness.mean(), 0.3, 0.7)\n    hybrid_metric = alpha * composite_z + (1 - alpha) * e_tightness\n    \n    # Adaptive gradient-driven exponential enhancer\n    item_reso_factor = 1.0 + np.sqrt((item + 1e-9) / (system_avg + 1e-9)) if system_avg > 0 else 1.0\n    oversized_flag = e_leftover < 0.1 * e_rc  # High-urgency marker\n    frag_density = np.clip((e_leftover - system_avg) / (2 * system_std + 1e-9), -1, 1) if system_std > 0 else 0\n    enhancer = np.exp(hybrid_metric * item_reso_factor * (1.5 + 0.3 * frag_density * over)))\n    \n    # Entropy-sensitive balance with covariance control\n    if len(e_rc) > 1:\n        fragility_mask = e_leftover < (system_avg - system_std)\n        balance_term = -np.abs(e_leftover - system_avg) * (1 + fragility_mask)\n        covar_factor = np.cov(e_fit_quality, balance_term)[0, 1] + 1e-9 if len(e_rc) > 1 else 0\n        dynamic_modulation = np.sign(covar_factor) * np.log(1.0 + system_cv / (np.abs(covar_factor) + 1e-9))\n    else:\n        balance_term = -np.abs(e_leftover - system_avg)\n        dynamic_modulation = 1.0\n    balance_weight = 0.05 * system_cv * dynamic_modulation * over)))\n    \n    # Fragility-aware reinforcement with dynamic penalty zones\n    fragility_threshold = system_avg - system_std\n    fragility_reward = np.zeros_like(e_tightness)\n    reward_mask = (e_leftover > fragility_threshold) & (e_leftover < system_avg)\n    fragility_reward[reward_mask] = 0.1 * (e_leftover[reward_mask] - fragility_threshold)\n    fragility_penalty = -1e4 * system_cv * (1.0 - e_tightness) * (e_leftover < fragility_threshold)\n    \n    # Multi-tiered perturbation for degeneracy avoidance\n    v_optimal_tie = 1e-10 * (1.0 / (e_leftover + 1e-9))\n    v_mediation = 1e-8 * np.random.rand(len(e_leftover)) * over.))  # Random stabilization noise\n    \n    # Final priority synthesis\n    final_priority = enhancer + balance_weight * balance_term + fragility_penalty + v_optimal_tie + v_mediation\n    \n    # Matrix assembly and return\n    full_priority = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    full_priority[eligible] = final_priority\n    \n    return full_priority",
    "response_id": 1,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 53\n    enhancer = np.exp(hybrid_metric * item_reso_factor * (1.5 + 0.3 * frag_density * over)))\n                                                                                           ^\nSyntaxError: unmatched ')'\n35\n5\n"
  },
  {
    "stdout_filepath": "problem_iter11_response2.txt_stdout.txt",
    "code_path": "problem_iter11_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines z-score fit/cap synergy with median stabilization and volatility damping \n    with variance-scaled tie-breaking for robust online bin packing dynamics.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap < 1e-9 or item < 1e-9:\n        return np.where(\n            bins_remain_cap >= item, \n            1.0 / (bins_remain_cap - item + 1e-9), \n            -np.inf\n        )\n    \n    valid = bins_remain_cap >= item\n    if not valid.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    leftover = bins_remain_cap - item\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    # Z-score normalization of fit (best-fit) and capacity (worst-fit) metrics\n    elig_fit = 1.0 / (leftover[valid] + 1e-9)\n    mean_fit, std_fit = elig_fit.mean(), elig_fit.std()\n    z_fit = (1.0 / (leftover + 1e-9) - mean_fit) / (std_fit + 1e-9)\n    \n    elig_cap = bins_remain_cap[valid]\n    mean_cap, std_cap = elig_cap.mean(), elig_cap.std()\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    # Dynamic hybrid weighting with exponential utilization enhancer\n    primary = (tightness * z_fit + (1.0 - tightness) * z_cap) * np.exp(utilization * tightness)\n    \n    # Entropy-guided median stabilization to avoid capacity fragmentation\n    med_cap = np.median(bins_remain_cap)\n    balance = -0.05 * np.abs(bins_remain_cap - med_cap) / (orig_cap + 1e-9)\n    \n    # Multi-scale tie-breaking with volatility damping and variance scaling\n    vol_damping = 0.02 * np.exp(-leftover)\n    valid_leftover = leftover[valid]\n    std_leftover = valid_leftover.std() if len(valid_leftover) > 1 else 1e-6\n    epsilon = 1e-4 / (std_leftover + 1e-9)\n    grad_sensitivity = epsilon * (item / (bins_remain_cap + 1e-9))\n    \n    return np.where(\n        valid,\n        primary + balance + vol_damping + grad_sensitivity,\n        -np.inf\n    )",
    "response_id": 2,
    "tryHS": false,
    "obj": 3.9389708815317115,
    "SLOC": 35.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response3.txt_stdout.txt",
    "code_path": "problem_iter11_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid heuristic combining tightness-weighted Z-scores, exponential utilization enhancer,\n    entropy-aware fragmentation control via adaptive-scale penalty, and median-anchored\n    balance stabilization for online Bin Packing prioritization.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    leftover = bins_remain_cap - item\n    \n    # Core metrics\n    utilization = (orig_cap - bins_remain_cap) / (orig_cap + 1e-9)\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    # Z-score normalized fit/capacity synergy\n    fit_quality = 1.0 / (leftover + 1e-9)\n    elig_fit, elig_cap = fit_quality[eligible], bins_remain_cap[eligible]\n    z_fit = (fit_quality - np.mean(elig_fit)) / (np.std(elig_fit) + 1e-9)\n    z_cap = (bins_remain_cap - np.mean(elig_cap)) / (np.std(elig_cap) + 1e-9)\n    primary_score = (tightness * z_fit + (1.0 - tightness) * z_cap) * np.exp(utilization * tightness)\n    \n    # Adaptive fragmentation penalty with entropy-sensitive scaling\n    std_leftover = leftover[eligible].std() + 1e-9\n    frag_scale = (0.05 * orig_cap) + 0.15 * std_leftover\n    frag_penalty = np.exp(-leftover / (frag_scale + 1e-9))\n    \n    # Median-centered balance constraint for distribution stabilization\n    med_cap = np.median(bins_remain_cap)\n    balance_term = 0.05 * np.abs(bins_remain_cap - med_cap) / (orig_cap + 1e-9)\n    \n    # Multi-axis perturbation mechanism for deterministic tie-breaking\n    var_factor = 1e-6 / (std_leftover + 1e-9)\n    perturbation = var_factor * tightness\n    \n    # Score aggregation with hierarchical prioritization\n    return np.where(\n        eligible,\n        primary_score - frag_penalty - balance_term + perturbation,\n        -np.inf\n    )",
    "response_id": 3,
    "tryHS": false,
    "obj": 3.9389708815317115,
    "SLOC": 35.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response4.txt_stdout.txt",
    "code_path": "problem_iter11_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Entropy-adapted Z-synergy with item-size pathing and continuity tiebreakers.\n    \n    Combines tunable tight/util synergy where weights depend on item size relative to \n    bin medians, enhanced fragmentation control with system entropy memory, and\n    continuity-sensitive tiebreakers leveraging leftover clustering predictivity.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eps = 1e-9\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if not mask.any():\n        return scores\n\n    origin_cap = bins_remain_cap.max()\n    rem_cap = bins_remain_cap[mask]\n    leftover = rem_cap - item\n\n    # Use tight_fit with exponential sensitivity adjustment\n    inv_waste = 1.0 / (leftover + eps)\n    exp_tight = np.exp(-2.0 * leftover / (origin_cap + eps))\n    tight_fit = inv_waste + 2.0 * exp_tight\n\n    # Utilization metric with floor regularization\n    bin_util = (origin_cap - rem_cap) / (origin_cap + eps)\n\n    # Z-score normalization with bias correction\n    t_mean, t_std = tight_fit.mean(), tight_fit.std(ddof=1)\n    u_mean, u_std = bin_util.mean(), bin_util.std(ddof=1)\n    z_tight = (tight_fit - t_mean) / (t_std + 1.2 * eps)\n    z_util = (bin_util - u_mean) / (u_std + 1.2 * eps)\n\n    # System entropy from logarithmic cap distribution\n    log_caps = np.log(bins_remain_cap[ bins_remain_cap > 0 ] + eps)\n    if log_caps.size > 1:\n        entropic_scale = log_caps.std()  # Higher sensitivity to distributional shape\n    else:\n        entropic_scale = 0.0\n    \n    # Item sizing heuristic relative to active bins\n    active_caps = bins_remain_cap[ bins_remain_cap > 0 ]\n    median_cap = np.median(active_caps) if active_caps.size else origin_cap\n    is_small_item = item / (median_cap + eps) < 0.75\n\n    # Predictive synergy formulation\n    entropy_amp = (1.0 + entropic_scale) ** 1.5\n    \n    if is_small_item:\n        tight_weight = 1.8 + entropy_amp  # Priority small-item consolidation\n        util_weight = 0.3  # Prevent underutilization penalty\n    else:\n        tight_weight = 1.0 \n        util_weight = 1.2 + entropy_amp  # Reward fuller bins\n    \n    synergy = (tight_weight * z_tight) * (1.0 + (util_weight * z_util))\n\n    # Enhanced frag control with lifetime decay\n    leftover_norm = leftover / (origin_cap + eps)\n    base_frag = -np.expm1(-leftover_norm ** 1.25)\n    cap_highway = np.exp(-3.0 * ((leftover - median_cap)**2) / ((origin_cap * 0.4)**2 + eps))\n    frag_penalty = base_frag - cap_highway  # EPS decay for cluster alignment\n\n    # Dynamic weight with memory effects\n    entropic_activity = bins_remain_cap.shape[0] / (origin_cap + eps)\n    frag_weight = 0.8 * entropy_amp * (1.0 + entropic_activity) \n\n    # Core score calculation\n    scores_masked = synergy - (frag_weight * frag_penalty)\n\n    # Tiebreakers with thermalization\n    if active_caps.size > 2:\n        # Preserve topological continuity through spectral alignment\n        dist_to_mean = np.abs(leftover - rem_cap.mean()) / rem_cap.std()\n        continuity_bonus = np.exp(-0.7 * dist_to_mean)\n        scores_masked += 0.15 * continuity_bonus  # Second-order reinforcement\n\n    scores[mask] = scores_masked\n    \n    return scores",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 35.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response5.txt_stdout.txt",
    "code_path": "problem_iter11_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    v2: Z-normalized synergy + cross-metric variance adaptation + gradient-boosted enhancer \n    + fragility-aware balancing with dynamic entropy-sensitive weights.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System stats\n    C_est = bins_remain_cap.max() if bins_remain_cap.size > 0 else 1.0\n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap) + eps\n    system_cv = system_std / (system_avg + eps)\n    remaining = bins_remain_cap + eps\n    \n    # Core metrics\n    leftover = remaining - item + eps\n    utilization = item / remaining\n    inv_leftover = 1.0 / leftover\n    \n    # Z-synergy (tight-fit * utilization)\n    phi = inv_leftover * utilization\n    phi_z = ((phi[eligible] - phi[eligible].mean()) / phi[eligible].std())[eligible]\n    \n    # Z-cap (remaining capacity adaptation)\n    cap_z = ((remaining - np.mean(remaining)) / np.std(remaining + eps))[eligible]\n    \n    # Cross-metric variance adaptation\n    var_phi = np.var(phi[eligible])\n    var_cap = np.var(remaining[eligible])\n    cross_weight = var_cap / (var_phi + var_cap + eps) if (var_phi + var_cap) > 1e-7 else 0.5\n    \n    # Adaptive curvature factor\n    curvature = 1.0 + np.arctan((phi_z.mean() - cap_z.mean()))\n    \n    # Composite base score\n    composite = cross_weight * phi_z + (1 - cross_weight) * cap_z * curvature\n    \n    # Gradient-driven enhancer (concentration + deviation sensitivity)\n    leftover_norm = (leftover - system_avg) / (3 * system_std + eps)\n    tight_density = np.clip(1.0 - np.abs(leftover_norm), 0, 1)  # Concentration zone control\n    grad_scale = np.where(item > system_avg, \n                         1.0 + 2.0 * utilization * system_std, \n                         1.0 + utilization)  # Dynamic gradient sensitivity\n    enhancer = np.exp(composite * tight_density * grad_scale)\n    \n    # Entropy-sensitive load balance (v0-style system variance coupling)\n    filled_frac = (C_est - remaining) / (C_est + eps)\n    system_var = np.var(bins_remain_cap) / (C_est**2 + eps)\n    balance_factor = filled_frac * (1 + system_var)\n    balance_z = (balance_factor - balance_factor[eligible].mean()) / balance_factor[eligible].std()\n    \n    # Fragility-aware perturbation (exponential decay approach)\n    fragility = 1.0 - np.exp(-leftover / (0.2 * C_est + eps))\n    frag_weight = 0.25 * system_var * np.clip((1 - utilization), 0, 1)\n    frag_term = np.where(leftover < 0.15 * C_est, -frag_weight * fragility, -0.01 * frag_weight * fragility)\n    \n    # Dynamic weight optimization\n    entropy_weight = 0.1 * system_cv \n    fragility_weight = 0.05 * (1 - system_cv) * system_var\n    \n    # Final score with hierarchical reinforcement\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    scores[eligible] = (\n        composite * enhancer \n        + entropy_weight * balance_z \n        + fragility_weight * frag_term[eligible]\n    ) * (1 + system_var)\n    \n    return scores",
    "response_id": 5,
    "tryHS": false,
    "obj": 34.1842840047866,
    "SLOC": 35.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response6.txt_stdout.txt",
    "code_path": "problem_iter11_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines variance-weighted Z-score synergy with decaying entropy-aware penalty fields. \n    Features hybrid BF/WF behavior through adaptive item classification, predictive fragility control, \n    and dynamic multi-metric balance preservation.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    if item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= 0,\n            1e-4 * bins_remain_cap - 1e-7 * bins_remain_cap**2,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # State-aware metric basis\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    C_est = bins_remain_cap.max() if bins_remain_cap.max() > 0 else item * 5\n    \n    # Z-score framework with variance sensitivity\n    def z_metric(metric):\n        subset = metric[eligible]\n        return (metric - subset.mean()) / (subset.std() + 1e-9)\n    \n    tight_z = z_metric(tightness)\n    fit_z = z_metric(1.0 / (leftover + 1e-9))\n    var_ratio = (fit_z.var() + 1e-9) / (fit_z.var() + tight_z.var() + 2e-9)\n    base_priority = var_ratio * fit_z + (1 - var_ratio) * tight_z\n    \n    # Granular item classification macros\n    sys_avg = bins_remain_cap.mean()\n    sys_std = bins_remain_cap.std()\n    is_large = item > 0.75 * C_est  # Hard threshold for guaranteed-waste scenarios\n    is_typical = item < 1.25 * sys_avg  # Empirically derived normality threshold\n    \n    # Nonlinear climb modulation\n    grad_factor = 1.25 if is_large else 0.85\n    utilization = 1 - bins_remain_cap / C_est\n    climb_envelope = grad_factor * np.exp(0.3 * (tightness + utilization))\n    \n    # Fragmentation hazard containment\n    frag_penalty_powers = 0.2 * np.exp(-leftover / (0.3 * C_est + 1e-9))\n    bin_normalization = bins_remain_cap / (C_est + 1e-9)\n    frag_penalty_modes = (1 + 0.25 * bin_normalization) * abs(leftover - 0.5 * sys_avg)\n    \n    # Landscape stability maintenance\n    median_cap = np.median(bins_remain_cap[eligible])\n    median_proximity = np.abs(bins_remain_cap - median_cap) / (C_est + 1e-9)\n    volatility_term = 0.1 * sys_std * np.abs(tight_z) * median_proximity\n    \n    # Final priority composition\n    priority = base_priority * climb_envelope\n    priority -= frag_penalty_powers\n    priority -= frag_penalty_modes\n    priority += volatility_term\n    priority *= np.exp(-0.05 * left_over / C_est if (left_over := bins_remain_cap - item).max() > 0 else 1.0)\n    \n    # Frailty resilience micro-perturbation\n    priority += 1e-7 * np.random.randn(*bins_remain_cap.shape)\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.88631830873554,
    "SLOC": 35.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response7.txt_stdout.txt",
    "code_path": "problem_iter11_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combined heuristic: Z-normalized synergy + entropy-tiered penalty + adaptive \n    item-class control + cross-metric variance reinforcement for resilient \n    online bin packing decisions with predictive entropy modeling\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    # Near-zero item fast path\n    if item < 1e-9:\n        return np.where(bins_remain_cap >= 0, \n                      0.5 / (bins_remain_cap + 1e-4) - 1e-4 * bins_remain_cap,\n                      -np.inf)\n    \n    eligible = bins_remain_cap >= item - 1e-9  # Floating point tolerance\n    \n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # Core system statistics\n    sys_avg, sys_std = np.mean(bins_remain_cap), np.std(bins_remain_cap)\n    sys_cv = sys_std / (sys_avg + 1e-9) if sys_avg > 1e-9 else 0.0\n    \n    # Dynamic item classification enhanced by variance tradeoff\n    large_item = item > sys_avg * 0.85 * (1 + 0.2 * sys_cv)\n    \n    # Metric definitions with gradient sensitivity\n    residual = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    fit_power = 1.0 / (residual + 1e-6)\n    frag_indicator = (residual - sys_avg) / (sys_std + 1e-9)\n    \n    # Gradient-boosted Z-note normalization\n    def calc_zscore(metric):\n        return (metric - metric[eligible].mean()) / (metric[eligible].std() + 1e-9)\n    \n    z_fit = calc_zscore(fit_power)\n    z_tight = calc_zscore(tightness)\n    z_balance = calc_zscore(-np.abs(residual - sys_avg))\n    \n    # Enhanced variance sensitivity calculation\n    def calc_adaptive_weight(metric):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            mags = metric[eligible]\n            var_factor = np.std(mags) / (abs(np.mean(mags)) + 1e-9)\n            depth_factor = 1 + 0.5 * (1 - sys_cv)\n            return var_factor * depth_factor\n    \n    # Metric-specific weight generation\n    fit_weight = calc_adaptive_weight(z_fit) * (1.2 if large_item else 1.0)\n    tight_weight = calc_adaptive_weight(1 - tightness) * (0.8 if large_item else 1.4)\n    balance_weight = calc_adaptive_weight(-np.abs(residual - sys_avg)) * (0.5 + 1.0 * sys_cv)\n    \n    # Entropy-aware synergy amplification\n    synergy_factor = 1.0 + np.tanh(\n        0.3 * (z_fit * fit_weight - z_tight * tight_weight)\n    ) * (1 - 0.3 * sys_std / (sys_avg + 1e-9))\n    \n    # Tiered penalty with predictive entropy modeling\n    penalty_weight = np.select(\n        [\n            frag_indicator > sys_cv + 1.2,\n            frag_indicator < -sys_cv - 0.5,\n            frag_indicator < 0.8\n        ],\n        [\n            0.5 * sys_std**0.8 * (1 + sys_cv**0.5),\n            0.3 * sys_std**0.5 / (sys_avg + 1e-9),\n            0.2 * sys_cv**0.7\n        ],\n        default=0.1 * (1 - sys_cv)\n    )\n    \n    # Priority synthesis with gradient control\n    base_score = (\n        z_fit * fit_weight * synergy_factor +\n        (1 - z_tight * tight_weight * (0.7 if large_item else 1.2)) +\n        z_balance * balance_weight\n    )\n    \n    penalty_score = np.where(\n        eligible,\n        residual * penalty_weight * (1.2 if item < sys_avg else 1.0),\n        0\n    )\n    \n    # Final score quantization with fragility suppression\n    final_score = np.where(eligible, base_score - penalty_score, -np.inf)\n    \n    return final_score",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 35.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response8.txt_stdout.txt",
    "code_path": "problem_iter11_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Dynamic adaptive synergy of Z-normalized tight-fit and utilization with entropy-weighted hybridization via predictive variance modeling and perturbed threshold reinforcement.\"\"\"\n    eps = 1e-9\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    \n    if not mask.any():\n        return scores\n\n    # System state estimation\n    C_est = bins_remain_cap.max() if bins_remain_cap.size > 0 else 1.0\n    remaining_cap = bins_remain_cap[mask]\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + eps)\n    \n    # Core metrics\n    leftover = remaining_cap - item + eps\n    inv_leftover = 1.0 / leftover  # Fit quality\n    tightness = item / (remaining_cap + eps)  # Utilization tightness\n    filled_frac = (C_est - remaining_cap) / C_est  # Relative fill fraction\n    norm_leftover = leftover / C_est  # Normalized waste\n    \n    # Synergy metric: tight-fit \u00d7 utilization (v0 Z-normalized)\n    synergy = inv_leftover * tightness\n    z_synergy = (synergy - synergy.mean()) / (synergy.std() + eps)\n    \n    # Adaptive cross-metric weights (v1's variance-driven logic)\n    tight_z = (tightness - tightness.mean()) / (tightness.std() + eps)\n    fit_z = (inv_leftover - inv_leftover.mean()) / (inv_leftover.std() + eps)\n    tight_var, fit_var = max(0.1, tight_z.var()), max(0.1, fit_z.var())\n    weight_tight = fit_var / (fit_var + tight_var)  # Reciprocal variance weighting\n    adaptive_combo = weight_tight * tight_z + (1 - weight_tight) * fit_z\n    z_adaptive = (adaptive_combo - adaptive_combo.mean()) / (adaptive_combo.std() + eps)\n    \n    # Entropy-regularized balance terms\n    balance_term = -np.abs(leftover - system_avg) * (1.0 + system_cv ** 2)\n    fragility_mask = np.logical_or(\n        remaining_cap > (system_avg + 2 * system_std),\n        remaining_cap < (system_avg - 2 * system_std)\n    )\n    fragility_score = np.where(\n        fragility_mask,\n        system_std ** 2 / (np.abs(remaining_cap - system_avg) + eps),\n        1.0\n    )\n    system_var = np.var(filled_frac)\n    z_balance = ((balance_term * system_var * (1 + system_cv)) - balance_term.mean()) / (balance_term.std() + eps)\n    \n    # Enhancer with predictive variance adaptation\n    item_vol_score = (item - system_avg) / (system_std + eps)\n    dynamic_gamma = np.clip(0.8 * tight_z + 0.2 * (1 - system_cv), 0.5, 1.5)\n    volatile_item = np.abs(item_vol_score) > 1.0\n    large_item = item_vol_score > 0.0\n    base_util_term = np.where(\n        large_item,\n        np.exp(tight_z + filled_frac),\n        np.sqrt((1 - tight_z) * (1 - filled_frac) + 1e-9)\n    )\n    enhancer = (base_util_term ** dynamic_gamma) * (1 + 0.5 * system_var)\n    \n    # Exponential waste decay (v0 style)\n    exp_waste = np.exp(-norm_leftover)\n    z_exp = (exp_waste - exp_waste.mean()) / (exp_waste.std() + eps)\n    \n    # Composite score with hierarchical reinforcement\n    raw_scores = (\n        1.0 * z_synergy +\n        0.4 * (z_adaptive + (system_cv * z_balance)) +\n        0.3 * system_var * z_exp +\n        0.2 * z_balance\n    ) * enhancer\n    \n    # Deterministic perturbation for tie-break (v1)\n    seed = int(np.abs(item * 1e5) % 1e9)\n    np.random.seed(seed)\n    scores[mask] = raw_scores + 1e-9 * np.random.normal(size=raw_scores.shape)\n    return scores",
    "response_id": 8,
    "tryHS": false,
    "obj": 40.75588352612685,
    "SLOC": 35.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response9.txt_stdout.txt",
    "code_path": "problem_iter11_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive synergy prioritization with exponential enhancement and entropy-dynamic frag control.\n    Z-synergy augmented through normalized multi-metric fusion and predictive entropy damping.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Metric computations\n    leftover = bins_remain_cap - item\n    origin_cap = np.max(bins_remain_cap)\n    fit_quality = 1.0 / (leftover + eps)\n    tightness = item / (bins_remain_cap + eps)\n    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)\n    \n    # Z-score normalization across eligible bins\n    elig = {\n        'fit': fit_quality[eligible],\n        'tight': tightness[eligible],\n        'util': utilization[eligible]\n    }\n    mean = {k: np.mean(v) for k, v in elig.items()}\n    std = {k: np.std(v) for k, v in elig.items()}\n    \n    # Calculate normalized fields\n    z_fit = (fit_quality - mean['fit']) / (std['fit'] + eps)\n    z_tight = (tightness - mean['tight']) / (std['tight'] + eps)\n    z_util = (utilization - mean['util']) / (std['util'] + eps)\n    \n    # Adaptive synergy with deep utilization coupling (from v1 synergy)\n    synergy = z_tight * (1 + z_util)  # v1's cross-metric synergy\n    \n    # Exponential enhancer (from v0) with raw metric interaction\n    enhancer = np.exp(utilization * tightness)  # Reinforced BF-like prioritization\n    \n    # Primary adaptive score with layered reinforcement\n    primary_score = synergy * enhancer\n    \n    # Entropy-sensitive fragmentation control\n    sys_entropy = bins_remain_cap.std() / (origin_cap + eps)\n    frag_penalty = 1.0 - np.exp(-leftover / (origin_cap + eps))  # Differentiable frag metric\n    frag_weight = 0.2 * (1.0 + sys_entropy)  # Dynamic entropy scaling\n    \n    # Perturbed threshold tie-breaker (enhanced volatility damping)\n    tie_breaker = 0.075 * np.exp(-leftover) * (1.0 + sys_entropy)  # Entropy-coupled decay\n    \n    # Final score with dynamic entropy mitigation and multi-metric balance\n    scores = np.where(\n        eligible,\n        primary_score - frag_weight * frag_penalty + tie_breaker,\n        -np.inf\n    )\n    \n    return scores",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 35.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response0.txt_stdout.txt",
    "code_path": "problem_iter12_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9) - 1e-9 * bins_remain_cap,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Core fit metrics\n    leftover = bins_remain_cap - item\n    fit_quality = 1.0 / (leftover + 1e-9)\n    tightness = item / (bins_remain_cap + 1e-9)\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    \n    # Z-score normalization\n    elig_fit = fit_quality[eligible]\n    z_fit = (fit_quality - np.mean(elig_fit)) / (np.std(elig_fit) + 1e-9)\n    \n    elig_bins_remain = bins_remain_cap[eligible]\n    z_cap = (bins_remain_cap - np.mean(elig_bins_remain)) / (np.std(elig_bins_remain) + 1e-9)\n    \n    # Primary fit-balanced score\n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    enhancer = np.exp(utilization * tightness)\n    \n    # Predictive entropy modeling\n    N = len(bins_remain_cap)\n    sum_cap = np.sum(bins_remain_cap)\n    mean_old = sum_cap / N\n    sum_sq = np.sum(bins_remain_cap ** 2)\n    var_old = sum_sq / N - mean_old ** 2\n    \n    elig_remain = bins_remain_cap[eligible]\n    new_remain = elig_remain - item\n    \n    # Precompute new statistics\n    new_sum = sum_cap - item\n    new_mean = new_sum / N\n    \n    # Compute new variance for each eligible bin\n    old_sq = elig_remain ** 2\n    new_sq = new_remain ** 2\n    sum_sq_changed = sum_sq - old_sq + new_sq\n    \n    new_var_i = sum_sq_changed / N - new_mean ** 2\n    \n    # Predictive entropy delta\n    entropy_delta = var_old - new_var_i\n    entropy_std = np.std(entropy_delta) if len(entropy_delta) > 1 else 1.0\n    norm_entropy = entropy_delta / (entropy_std + 1e-9)\n    \n    # Adaptive entropy weight\n    system_cv = np.std(bins_remain_cap) / (np.mean(bins_remain_cap) + 1e-9)\n    large_item = item > np.mean(bins_remain_cap)\n    weight_entropy = 0.5 * (1.0 + system_cv) * (1.5 if large_item else 0.5)\n    weight_entropy /= np.log(2 + np.abs(entropy_delta)).mean() + 1e-9  # Self-regulating dampening\n    \n    # Sensitivity-based refinement\n    residual_sensitivity = np.abs((new_remain - new_mean) / (np.abs(new_mean) + 1e-9))\n    sensitivity_factor = 1.0 / (1.0 + residual_sensitivity)\n    \n    # Final priority calculation\n    priority = (\n        primary_score * enhancer * sensitivity_factor + \n        weight_entropy * norm_entropy\n    )\n    \n    # Tie-breaking with dynamic perturbations\n    if np.allclose(priority, priority[0]):\n        priority += 1e-4 * tightness * (bins_remain_cap + 1e-9) ** (-0.5)\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 0,
    "tryHS": false,
    "obj": 7.828081372157958,
    "SLOC": 47.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response1.txt_stdout.txt",
    "code_path": "problem_iter12_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            (1.0 / (bins_remain_cap - item + 1e-9)) - 1e-9 * bins_remain_cap,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    leftover = bins_remain_cap - item\n    utilization = (orig_cap - bins_remain_cap) / (orig_cap + 1e-9)\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    # Base metrics\n    fit_quality = 1.0 / (leftover + 1e-9)\n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    \n    # Z-score normalization\n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    \n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    # Reinforcement-inspired gain w/meta-metric modulation\n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap)\n    system_skew = np.mean((bins_remain_cap - system_avg)**3) / (system_std**3 + 1e-9)\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    large_item = item > np.quantile(bins_remain_cap, 0.75)\n    \n    # Modulated enhancer with entropy-sensitive gradient\n    gradient_energy = (1.0 - system_cv) * utilization + system_cv * tightness\n    rein_enhancer = np.exp(tightness * utilization * gradient_energy + (1.0 - tightness) * (1.0 - utilization) * (1.0 - system_cv))\n    \n    # Multi-spectral entropy balance metrics\n    distance_from_mean = leftover - system_avg\n    quadratic_balance = -(distance_from_mean**2) / (1 + 0.5*system_skew)  # Forward-skew adjustment\n    \n    cap_fluctuations = np.std(bins_remain_cap + 1e-9)\n    fluctuation_term = -np.abs(np.log1p(leftover/(cap_fluctuations + 1e-9)))\n    \n    # Adaptive decision layer weights\n    balance_weight = 0.2 * system_cv * (1 + system_skew*(0.5 if item > system_avg else 2.0)) * np.log(2 + system_avg)\n    fluctuation_weight = 0.05 * (1.0 - system_cv**2) * (1.0 + np.sqrt(tightness))\n    \n    # Spectral sensitivity tie-breaker with perturbations\n    sensitivity_kernel = np.where(\n        bins_remain_cap > system_avg,\n        1.0 / (1e-9 + np.abs(bins_remain_cap - 2*system_avg - item)),\n        1.0 / (1e-9 + np.abs(bins_remain_cap - 0.5*system_avg))\n    )\n    perturbation = 0.001 * sensitivity_kernel * (1.0 + np.random.normal(0, 1, bins_remain_cap.shape) * system_cv)\n    \n    # Core spectral synergy\n    priority_base = tightness * z_fit + (1.0 - tightness) * z_cap\n    dynamic_sensitivity = sensitivity_kernel * (1.0 - 0.5 * system_skew * tightness)\n    \n    # Full integration\n    priority = (\n        priority_base * rein_enhancer \n        + balance_weight * quadratic_balance \n        + fluctuation_weight * fluctuation_term\n        + 0.005 * dynamic_sensitivity\n        + perturbation\n    )\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 1,
    "tryHS": false,
    "obj": 33.82528919026726,
    "SLOC": 47.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response2.txt_stdout.txt",
    "code_path": "problem_iter12_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hyper-adaptive fragmentation-averse heuristic with predictive entropy control and dynamic sensitivity gradients.\n    Integrates reinforcement-inspired system gain, meta-learned weight gradients, and multi-horizon balancing.\n    \"\"\"\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap * (1 + 0.1 * (bins_remain_cap > (orig_cap * 0.7))),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full(n, -np.inf, dtype=np.float64)\n\n    # Core adaptive metrics\n    leftover = bins_remain_cap - item\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    tightness = np.clip(item / (bins_remain_cap + 1e-9), 0, 1e6)\n    \n    # Predictive entropy modeling (MPU: Minimum Predictive Uncertainty)\n    sum_c, sum_sq_c = np.sum(bins_remain_cap), np.sum(bins_remain_cap**2)\n    delta_sq = -2 * item * bins_remain_cap + item**2  # Change in squared capacities\n    new_sum_sq = sum_sq_c + delta_sq\n    new_mean_sq = (sum_c - item) / n\n    predicted_var = (new_sum_sq / n) - new_mean_sq**2\n    system_var, system_std = sum_sq_c / n - (sum_c / n)**2, np.std(bins_remain_cap)\n    variance_hysteresis = predicted_var - system_var  # Lower better: indicates stability\n\n    # Reinforcement-inspired sensitivity fields\n    fit_weight = np.exp(-tightness * (1 + utilization))  # Tight-fit exploration\n    var_penalty_weight = 1 + 2 * np.abs(variance_hysteresis) / (system_var + 1e-9)\n    \n    # Z-score synergy with hyperbolic sensitivity enhancement\n    mu_fit = np.mean(1.0 / (leftover[eligible] + 1e-9))\n    grad_fit = 1/(leftover + 1e-9) - mu_fit  # Local-Global deviation\n    fit_z = grad_fit / (np.std(1.0 / (leftover + 1e-9)) + 1e-9)\n    \n    mu_cap = np.mean(bins_remain_cap[eligible])\n    cap_z = (bins_remain_cap - mu_cap) / (np.std(bins_remain_cap) + 1e-9)\n    \n    # Dynamic gain optimization via system entropy Jaccard\n    system_cv = system_std / (sum_c/n + 1e-9)\n    alpha_gain = 1 + 1.5 * tightness * system_cv  # Tight bins amplify sensitivity\n    decay_rate = tightness * 0.3\n    \n    # Meta-heuristic energy balancing\n    energy_gradient = np.gradient(bins_remain_cap + 1e-9)\n    energy_profile = -grad_fit * np.abs(energy_gradient)\n    ep_weight = 1 - tightness * (1 - system_cv)\n    \n    # Perturbative exploration strategy\n    explore_noise = 1e-6 * np.abs(np.sin(0.1 * left_margin)) * np.random.normal(0, 1, n)\n    \n    # Hierarchical priority synthesis\n    priority = alpha_gain * (fit_z * fit_weight + cap_z * decay_rate) - var_penalty_weight * variance_hysteresis\n    priority += explore_noise + energy_profile * ep_weight\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 2,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 61, in priority_v2\n    )\nNameError: name 'left_margin' is not defined\n47\n7\n"
  },
  {
    "stdout_filepath": "problem_iter12_response3.txt_stdout.txt",
    "code_path": "problem_iter12_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eps = 1e-9\n    if item < eps:\n        return np.where(\n            bins_remain_cap >= 0, \n            -np.log(bins_remain_cap + eps) * (bins_remain_cap > 0) + 1e-3,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System state analytics\n    sys_avg = np.mean(bins_remain_cap)\n    sys_std = np.std(bins_remain_cap)\n    sys_cv = sys_std / (sys_avg + eps)\n    \n    # Placement metrics\n    avail_cap = bins_remain_cap[eligible]\n    remain_cap_post = np.where(eligible, bins_remain_cap - item, bins_remain_cap)\n    tightness = item / (avail_cap + eps)\n    leftover = avail_cap - item\n    \n    # Reinforcement learning-inspired reward structure\n    fit_reward = 1.0 / (leftover + eps) * item\n    tightness_reward = 1.0 / (tightness + 1e-3)\n    variance_reward = -np.abs(leftover - sys_avg)\n    \n    gradient_magnitude = abs(np.gradient(bins_remain_cap)).mean() + eps\n    adaptive_rate = 1.0 / (1.0 + gradient_magnitude)\n    \n    reward = (\n        .5 * fit_reward + \n        .35 * tightness_reward + \n        .15 * variance_reward\n    ) * adaptive_rate\n    \n    # Entropy forecasting matrix\n    entropy_base = -(bins_remain_cap / sys_avg) * np.log(bins_remain_cap / sys_avg + eps)\n    \n    entropy_forecast = -(\n        remain_cap_post / sys_avg\n    ) * np.log(remain_cap_post / sys_avg + eps)\n    \n    entropy_diff = entropy_forecast - entropy_base\n    \n    imbalance_sensitivity = 1.0 / (abs(remain_cap_post - sys_avg) + 1e-4)\n    imbalance_sensitivity /= imbalance_sensitivity.sum() + eps\n    \n    entropy_gain = np.where(\n        eligible, -entropy_diff * tightness, -np.inf\n    )\n    entropy_gain_norm = (entropy_gain - np.min(entropy_gain)) / (np.ptp(entropy_gain) + eps)\n    \n    # Sensitivity-adjusted lookahead policy\n    lookahead_policy = np.exp(-(bins_remain_cap / (sys_avg + eps)) ** 2) * entropy_gain_norm\n    \n    # Multi-scale sensitivity harmonization\n    grad_order1 = np.gradient(bins_remain_cap)\n    grad_order2 = np.gradient(grad_order1)\n    \n    state_complexity = np.clip(abs(grad_order2), 0, 1)\n    complexity_sensitivity = 1.0 - np.tanh(sys_cv)\n    \n    mixed_sensitivity = .4 * tightness + .3 * (leftover / sys_avg) * (1 - complexity_sensitivity) + \\\n                        .3 * abs(grad_order2) * complexity_sensitivity\n    \n    # Adaptive weight matrix with entropy flow dynamics\n    smoothed_state = np.convolve(bins_remain_cap, np.ones(5)/5, mode='same')\n    state_change = abs(remain_cap_post - smoothed_state)\n    entropy_flow = np.where(eligible, 1.0 / (state_change + 1e-4), -np.inf)\n    \n    reward_weight = .3 + .7 * entropy_flow / (entropy_flow.max() + eps)\n    entropy_weight = .5 + .5 * (1 / (1 + np.exp(-sys_cv)))\n    \n    priority = (\n        reward_weight * reward + \n        entropy_weight * lookahead_policy + \n        .2 * entropy_gain_norm + \n        .1 * mixed_sensitivity\n    )\n    \n    large_deviation = abs(bins_remain_cap - sys_avg) > 1.5 * sys_std\n    priority[large_deviation] *= .9\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 3,
    "tryHS": false,
    "obj": 43.28879138412447,
    "SLOC": 47.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response4.txt_stdout.txt",
    "code_path": "problem_iter12_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - 2 * (bins_remain_cap - item),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    leftover = bins_remain_cap - item\n    utilization = 1.0 - bins_remain_cap / orig_cap\n    tightness = item / (bins_remain_cap + 1e-9)\n    fit_quality = 1.0 / (leftover + 1e-9)\n    \n    elig_indices = np.where(eligible)[0]\n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    \n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap)\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 0 else 0\n    \n    n_bins = bins_remain_cap.size\n    s_total = bins_remain_cap.sum()\n    s2_total = (bins_remain_cap ** 2).sum()\n    delta_s2 = (leftover ** 2) - (bins_remain_cap ** 2)\n    new_s2 = s2_total + delta_s2\n    new_s = s_total - item\n    new_mean = new_s / n_bins\n    new_var = new_s2 / n_bins - new_mean ** 2\n    rel_variance = (new_var - np.std(bins_remain_cap) ** 2) / (np.std(bins_remain_cap) ** 2 + 1e-9)\n    entropy_penalty = np.where(eligible, -rel_variance, np.inf)\n    \n    spatial_sensitivity = np.abs(bins_remain_cap - system_avg) / (system_std + 1e-9)\n    sensitivity_weight = 1.0 / (spatial_sensitivity + 1e-9)\n    adaptive_weights = np.sqrt(system_cv ** 2) * sensitivity_weight\n    \n    fit_primary = z_fit * tightness + z_cap * spatial_sensitivity\n    entropy_response = entropy_penalty * np.exp(-utilization)\n    \n    priority = fit_primary * (1 + system_cv) + adaptive_weights * entropy_response\n    \n    # Perturbation mechanism\n    if np.any(eligible):\n        eps = 1e-6 * system_cv * (np.random.rand(n_bins) - 0.5)\n        priority += np.where(eligible, eps, 0)\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 4,
    "tryHS": false,
    "obj": 79.88631830873555,
    "SLOC": 47.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  }
]