```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Adaptive normalization with dynamic weights based on item size to balance
    tight packing, fragmentation, and system variance. Preserves entropy-driven balance.
    """
    eps = 1e-9
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    # Handle negligible items
    if item < eps:
        return np.where(
            bins_remain_cap >= 0,
            0.3 / (bins_remain_cap + 1e-4) - 0.1 * bins_remain_cap,
            -np.inf
        )
    
    origin_cap = np.max(bins_remain_cap)
    eligible = bins_remain_cap >= item - 1e-9

    if not np.any(eligible):
        return np.full_like(bins_remain_cap, -np.inf)

    # Core metrics
    leftover = bins_remain_cap - item
    tightness = item / (bins_remain_cap + eps)
    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)

    # Adaptive normalization function
    def minmax_normalize_full(metric_full, eligible_mask):
        metric = metric_full.copy()
        eligible_metric = metric[eligible_mask]
        if eligible_metric.size == 0:
            return np.full_like(metric, -np.inf)
        same_values = np.allclose(eligible_metric, eligible_metric[0])
        if same_values:
            normalized = np.full_like(metric, 0.5)
            normalized[~eligible_mask] = -np.inf
            return normalized
        min_val = eligible_metric.min()
        max_val = eligible_metric.max()
        range_val = max_val - min_val
        if range_val < eps:
            normalized = np.full_like(metric, 0.5)
            normalized[eligible_mask] = 0.5
        else:
            eligible_normalized = (eligible_metric - min_val) / (range_val + eps)
            eligible_normalized = np.clip(eligible_normalized, 0, 1)
            normalized = np.full_like(metric, -np.inf)
            normalized[eligible_mask] = eligible_normalized
        return normalized

    # Calculate normalized metrics
    norm_tight = minmax_normalize_full(tightness, eligible)
    norm_util = minmax_normalize_full(utilization, eligible)
    norm_leftover = minmax_normalize_full(leftover, eligible)

    # Predictive variance modeling
    N = bins_remain_cap.size
    sum_cap = np.sum(bins_remain_cap)
    sum_sq = np.sum(bins_remain_cap**2)

    elig_remain = bins_remain_cap[eligible]
    delta_sq_i = (elig_remain - item)**2 - elig_remain**2
    new_sum_sq_i = sum_sq + delta_sq_i
    new_mean_i = (sum_cap - item) / N
    var_new_i = new_sum_sq_i / N - new_mean_i**2

    var_old = sum_sq / N - (sum_cap / N)**2
    var_delta_i = var_old - var_new_i

    var_delta_full = np.full_like(bins_remain_cap, -np.inf)
    var_delta_full[eligible] = var_delta_i
    norm_var = minmax_normalize_full(var_delta_full, eligible)

    # Dynamic weight adjustment based on item size
    relative_size = item / origin_cap

    # Weights for larger items: prioritize tightness and utilization
    tight_weight = 1.2 + 0.8 * relative_size
    util_weight = 0.8 * relative_size
    var_weight = 0.5 * (1 - relative_size)
    frag_weight = 0.4 * (1 - relative_size)

    # Component assembly with proper weighting
    tight_component = tight_weight * norm_tight
    util_component = util_weight * norm_util  # encourages filling existing bins
    frag_component = frag_weight * norm_leftover  # subtract as larger leftover is worse
    var_component = var_weight * norm_var  # higher var_delta means variance is reduced

    # Tie-breaker based on remaining capacity
    tie_breaker = 0.01 * (bins_remain_cap / (origin_cap + eps))

    # Combine all elements into priority scores
    scores = np.where(
        eligible,
        tight_component + util_component + var_component - frag_component + tie_breaker,
        -np.inf
    )

    return scores
```
