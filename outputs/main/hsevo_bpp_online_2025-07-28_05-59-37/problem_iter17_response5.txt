```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines min-max synergy metrics with predictive variance modeling.
    Uses adaptive fragmentation control and entropy-agnostic balance.
    """
    eps = 1e-9
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    # Handle negligible items
    if item < eps:
        return np.where(
            bins_remain_cap >= 0,
            0.3 / (bins_remain_cap + 1e-4) - 0.1 * bins_remain_cap,
            -np.inf
        )
    
    origin_cap = np.max(bins_remain_cap)
    eligible = bins_remain_cap >= item - 1e-9
    if not np.any(eligible):
        return np.full_like(bins_remain_cap, -np.inf)
    
    # Core metrics calculation
    leftover = bins_remain_cap - item
    tightness = item / (bins_remain_cap + eps)
    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)
    
    # Min-Max normalization function
    def minmax_normalize_full(metric_full, eligible_mask):
        metric = metric_full.copy()
        eligible_metric = metric[eligible_mask]
        if eligible_metric.size == 0:
            return np.full_like(metric, -np.inf)
        same_values = np.allclose(eligible_metric, eligible_metric[0])
        if same_values:
            normalized = np.full_like(metric, 0.5)
            normalized[~eligible_mask] = -np.inf
            return normalized
        min_val = eligible_metric.min()
        max_val = eligible_metric.max()
        range_val = max_val - min_val
        eligible_normalized = (eligible_metric - min_val) / (range_val + eps)
        eligible_normalized = np.clip(eligible_normalized, 0, 1)
        normalized = np.full_like(metric, -np.inf)
        normalized[eligible_mask] = eligible_normalized
        return normalized
    
    # Normalize metrics
    norm_tight = minmax_normalize_full(tightness, eligible)
    norm_util = minmax_normalize_full(utilization, eligible)
    
    # Primary synergy score
    synergy = norm_tight * (1.0 + norm_util)
    
    # Fragmentation penalty (entropy-agnostic)
    frag_penalty = 1.0 - np.exp(-leftover / (origin_cap + eps))
    frag_component = 0.2 * frag_penalty
    
    # Predictive variance modeling
    N = bins_remain_cap.size
    sum_cap = np.sum(bins_remain_cap)
    sum_sq = np.sum(bins_remain_cap ** 2)
    mean_old = sum_cap / N
    var_old = (sum_sq / N) - (mean_old ** 2)
    
    elig_remain = bins_remain_cap[eligible]
    delta_sq_i = (elig_remain - item) ** 2 - elig_remain ** 2
    new_sum_sq_i = sum_sq + delta_sq_i
    new_mean_i = (sum_cap - item) / N
    var_new_i = (new_sum_sq_i / N) - (new_mean_i ** 2)
    var_delta_i = var_old - var_new_i
    
    # Normalize variance delta
    var_delta_full = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    var_delta_full[eligible] = var_delta_i
    norm_var = minmax_normalize_full(var_delta_full, eligible)
    
    # Final components
    var_component = norm_var
    tie_breaker = 0.01 * (bins_remain_cap / (origin_cap + eps))
    
    # Final score assembly
    scores = np.where(
        eligible,
        synergy - frag_component + 0.5 * var_component + tie_breaker,
        -np.inf
    )
    
    return scores
```
