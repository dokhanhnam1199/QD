```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines Z-score synergy with variance-aware rewards and entropy-modulated penalties.
    Adaptive normalization balances fit metrics, utilization, and fragmentation control.
    """
    eps = 1e-9
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    eligible = bins_remain_cap >= item
    if not eligible.any():
        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    
    # Metric computations
    leftover = bins_remain_cap - item
    origin_cap = np.max(bins_remain_cap)  # Infer bin capacity from max remaining
    fit_quality = 1.0 / (leftover + eps)
    tightness = item / (bins_remain_cap + eps)
    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)
    
    # Z-score normalization across eligible bins
    elig_fit, elig_tight = fit_quality[eligible], tightness[eligible]
    elig_util, elig_left = utilization[eligible], leftover[eligible]
    
    # Calculate Z-scores for core metrics
    z_fit = (fit_quality - np.mean(elig_fit)) / (np.std(elig_fit) + eps)
    z_tight = (tightness - np.mean(elig_tight)) / (np.std(elig_tight) + eps)
    z_util = (utilization - np.mean(elig_util)) / (np.std(elig_util) + eps)
    
    # Adaptive synergy with utilization coupling
    synergy = z_tight * (1 + z_util)  # Prioritize tight fits in utilized bins
    
    # Exponential enhancer for utilization-tightness interaction
    enhancer = np.exp(0.5 * utilization * tightness)  # Softer exponent than v0
    
    # Primary adaptive score
    primary_score = synergy * enhancer
    
    # System entropy metrics
    sys_std = bins_remain_cap.std()
    sys_entropy = sys_std / (origin_cap + eps)
    
    # Fragmentation penalty with entropy scaling
    frag_penalty = 1.0 - np.exp(-leftover / (origin_cap + eps))
    frag_weight = 0.25 * (1.0 + sys_entropy)  # Stronger modulation than v0
    
    # Variance-aware reward (normalized over eligible bins)
    sys_avg = np.mean(bins_remain_cap)
    variance_component = -np.abs(leftover - sys_avg)
    elig_var = variance_component[eligible]
    z_variance = (variance_component - np.mean(elig_var)) / (np.std(elig_var) + eps)
    variance_weight = 0.15 * (1.0 + sys_entropy)  # Adaptive scaling
    
    # Tie-breaker with entropy-coupled decay
    tie_breaker = 0.075 * np.exp(-leftover) * (1.0 + sys_entropy)
    
    # Final score with multi-metric integration
    scores = np.where(
        eligible,
        primary_score 
        - frag_weight * frag_penalty 
        + 0.1 * z_variance  # Reinforcement-inspired gain
        + tie_breaker,
        -np.inf
    )
    
    return scores
```
