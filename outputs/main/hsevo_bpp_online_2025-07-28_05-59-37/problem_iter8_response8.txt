```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Multi-objective priority with adaptive normalization, entropy-aware variance control, 
    and reinforcement-inspired dynamics. Blends z-score metrics, gradient-driven enhancements, 
    and variance-based entropy penalties for robust online bin selection.
    """
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    eligible = bins_remain_cap >= item
    if not np.any(eligible):
        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    
    remaining = bins_remain_cap.astype(np.float64)
    orig_cap = np.max(remaining)
    
    # Dynamic blending parameter
    eligible_remaining = remaining[eligible]
    median_rem = np.median(eligible_remaining)
    relative_size = item / (median_rem + 1e-9)
    blending = 1.0 / (1.0 + np.exp(-5 * (relative_size - 0.5)))
    
    # Core metrics
    leftover = remaining - item
    fit_metric = 1.0 / (leftover + 1e-9)                # Tightest fit preference
    space_metric = remaining                           # Capacity preservation
    tightness = item / (remaining + 1e-9)               # Local fit severity
    bin_utilization = (orig_cap - remaining) / (orig_cap + 1e-9)  # Global bin status
    
    # Z-score normalization
    fit_mean, fit_std = fit_metric[eligible].mean(), fit_metric[eligible].std()
    fit_z = (fit_metric - fit_mean) / (fit_std + 1e-9)
    
    space_mean, space_std = space_metric[eligible].mean(), space_metric[eligible].std()
    space_z = (space_metric - space_mean) / (space_std + 1e-9)
    
    # Primary score with adaptive balancing
    primary_score = blending * fit_z + (1 - blending) * space_z
    
    # Nonlinear modifiers
    residual_mod = -np.log(1 + leftover / (orig_cap + 1e-9))  # Elastic space penalty
    efficiency_bonus = residual_mod * tightness / (np.sqrt(bin_utilization + 1e-9) + 1)
    
    # Dynamic boosting mechanism
    enhancer = np.exp(2 * tightness) * np.clip((1 - bin_utilization)**3, 0, 1)
    
    # System-wide feedback
    sys_stats = remaining.mean(), remaining.std()
    sys_cv = sys_stats[1] / sys_stats[0] if sys_stats[0] > 1e-9 else float('inf')
    
    # Reinforcement learning inspiration
    fragility = np.abs(orig_cap - remaining) / (orig_cap + 1e-9)
    reinforcement = 1 + np.clip((1 - relative_size)**2 * (1 - bin_utilization) * fragility, 0, 3)
    
    # Entropy-aware tiebreaker (variance delta)
    bin_count = remaining.size
    sum_total = remaining.sum()
    sum_sq = (remaining**2).sum()
    new_sum_sq = sum_sq - (2 * item) * remaining + item**2  # Î”(sum squares) per bin
    mu_new = (sum_total - item) / bin_count
    var_new = (new_sum_sq / bin_count) - mu_new**2
    
    var_sensitivity = 0.1 * (1 + np.sqrt(sys_cv + 1e-9))
    var_term = var_sensitivity * (-var_new) / (np.sqrt(np.abs(var_new) + 1e-9) + 1e-5)
    
    # Final priority calculation
    return np.where(
        eligible,
        (primary_score + 0.75 * efficiency_bonus) * enhancer * reinforcement + var_term,
        -np.inf
    )
```
