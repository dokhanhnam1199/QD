```python
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines predictive variance modeling, min-max normalization, and reinforcement-driven prioritization.
    Balances fit quality, system variance reduction, and utilization with static hybrid weights.
    """
    eps = 1e-9
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    origin_cap = np.max(bins_remain_cap)
    # Handle edge cases with zero-sized items or bins
    if origin_cap <= eps or item <= eps:
        return np.where(bins_remain_cap >= item, bins_remain_cap - item + eps, -np.inf)
    
    eligible = bins_remain_cap >= item - eps  # Allow floating-point tolerance
    if not np.any(eligible):
        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    
    # Core metric calculations
    leftover = bins_remain_cap - item
    tightness = item / (bins_remain_cap + eps)
    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)
    fit_quality = 1.0 / (leftover + eps)
    
    # Predictive variance modeling: calculate variance delta for each eligible bin
    n = bins_remain_cap.size
    sum_cap = bins_remain_cap.sum()
    sum_sq = (bins_remain_cap ** 2).sum()
    mean_old = sum_cap / n
    var_old = sum_sq / n - mean_old**2
    
    c_elig, lo_elig = bins_remain_cap[eligible], leftover[eligible]
    delta_sq_i = c_elig**2 - lo_elig**2
    new_sum_sq_i = sum_sq - delta_sq_i
    new_mean_i = (sum_cap - item) / n
    var_new_i = (new_sum_sq_i / n) - (new_mean_i ** 2)
    delta_var_i = var_old - var_new_i  # Higher delta means better variance reduction
    
    # Min-max normalization for metrics across eligible bins
    def min_max_normalize(values, eligible_mask, default=0.5):
        vals = values[eligible_mask]
        min_val, max_val = vals.min(), vals.max()
        if (max_val - min_val) > eps:
            return (values - min_val) / (max_val - min_val + eps)
        return np.full_like(values, default, dtype=np.float64)
    
    norm_fit = min_max_normalize(fit_quality, eligible)
    norm_util = min_max_normalize(utilization, eligible)
    norm_tight = min_max_normalize(tightness, eligible)
    
    # Normalize variance delta
    norm_var = np.zeros_like(bins_remain_cap, dtype=np.float64)
    dv_vals = delta_var_i
    dv_min, dv_max = dv_vals.min(), dv_vals.max()
    if (dv_max - dv_min) > eps:
        norm_var_elig = (dv_vals - dv_min) / (dv_max - dv_min + eps)
    else:
        norm_var_elig = 0.5
    norm_var[eligible] = norm_var_elig
    
    # Primary score combines normalized metrics with equal weights
    primary_score = (norm_fit + norm_util + norm_var) / 3.0
    
    # Reinforcement factor: rewards usable remaining space and filled bins
    rel_size = item / origin_cap
    rem_rel = bins_remain_cap / origin_cap
    reinforcer = (1.0 - rel_size) * rem_rel * (1.0 + utilization)  # Utilization boosts filled bins
    
    # Tie-breaker to prefer larger leftover space
    tie_breaker = 1e-6 * leftover
    
    # Final priority calculation
    priority = primary_score * reinforcer + tie_breaker
    scores = np.where(eligible, priority, -np.inf)
    
    return np.nan_to_num(scores, neginf=-np.inf, posinf=0, nan=0)
```
