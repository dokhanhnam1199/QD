```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines dynamic metric weights, predictive variance control, and entropy-aware reinforcement.
    Uses adaptive normalization, item-size classification, and variance-reduction balance.
    """
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    orig_cap = bins_remain_cap.max()
    eps = 1e-9
    
    # Edge cases: zero-sized item or bins
    if orig_cap <= eps or item <= eps:
        return np.where(
            bins_remain_cap >= item,
            bins_remain_cap - item + eps,
            -np.inf
        )
    
    eligible = bins_remain_cap >= item
    if not np.any(eligible):
        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    
    # Subset eligible bins and calculate basic metrics
    eligible_rem = bins_remain_cap[eligible]
    leftover = eligible_rem - item
    tightness = item / (eligible_rem + eps)
    utilization = (orig_cap - eligible_rem) / (orig_cap + eps)
    
    # Min-Max normalization for metrics (higher value = better)
    def normalize(arr):
        a_max = arr.max()
        a_min = arr.min()
        if a_max > a_min + eps:
            return (arr - a_min) / (a_max - a_min + eps)
        return np.zeros_like(arr)
    
    norm_tight = normalize(tightness)
    norm_util = normalize(utilization)
    
    # Predictive variance calculation (from v1)
    S = bins_remain_cap.sum()
    S2 = (bins_remain_cap**2).sum()
    n = len(bins_remain_cap)
    c_i = bins_remain_cap[eligible]
    variance_i = ((S2 - 2 * c_i * item + item**2) / n) - ((S - item)**2) / (n**2)
    # Normalize variance score: higher = better (inverse variance)
    var_score = -variance_i  # Negative to make higher better
    norm_var = normalize(var_score)
    
    # Dynamic weights based on item size
    active_caps = bins_remain_cap[bins_remain_cap > 0]
    median_cap = np.median(active_caps) if active_caps.size else orig_cap
    rel_size = item / (median_cap + eps)
    small_item = rel_size < 0.75
    
    tight_weight = 1.0 if small_item else 1.8
    util_weight = 1.5 if small_item else 0.5
    var_weight = 0.7  # Static weight for predictive variance contribution
    
    # Primary score combines normalized metrics
    primary = tight_weight * norm_tight + util_weight * norm_util + var_weight * norm_var
    
    # Balance contribution: leftover variance reduction (from v0)
    if leftover.size > 1:
        median_left = np.median(leftover)
        balance_term = -np.abs(leftover - median_left) / (leftover.std() + eps)
        system_avg = bins_remain_cap.mean()
        system_std = bins_remain_cap.std()
        system_cv = system_std / (system_avg + eps) if system_avg > eps else 0.0
        balance_contrib = balance_term * 0.3 * system_cv
    else:
        balance_contrib = np.zeros_like(leftover)
    
    # Reinforcement: capacity clustering similarity (from v0)
    if active_caps.size > 1:
        active_median = np.median(active_caps)
        active_iqr = np.percentile(active_caps, 75) - np.percentile(active_caps, 25)
        similarity = 1.0 / (np.abs(leftover - active_median) / (active_iqr + eps) + 1.0)
    else:
        similarity = np.ones_like(leftover)
    reinforcer = 0.5 + 0.5 * similarity  # Scale to 0.5-1.0
    
    # Final priority calculation
    priority = (primary + balance_contrib) * reinforcer
    
    # Fill scores array
    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    scores[eligible] = priority
    return scores
```
