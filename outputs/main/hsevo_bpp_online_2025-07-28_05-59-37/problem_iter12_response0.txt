```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    orig_cap = np.max(bins_remain_cap)
    if orig_cap <= 1e-9 or item <= 1e-9:
        return np.where(
            bins_remain_cap >= item,
            1.0 / (bins_remain_cap - item + 1e-9) - 1e-9 * bins_remain_cap,
            -np.inf
        )
    
    eligible = bins_remain_cap >= item
    if not np.any(eligible):
        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    
    # Core fit metrics
    leftover = bins_remain_cap - item
    fit_quality = 1.0 / (leftover + 1e-9)
    tightness = item / (bins_remain_cap + 1e-9)
    utilization = (orig_cap - bins_remain_cap) / orig_cap
    
    # Z-score normalization
    elig_fit = fit_quality[eligible]
    z_fit = (fit_quality - np.mean(elig_fit)) / (np.std(elig_fit) + 1e-9)
    
    elig_bins_remain = bins_remain_cap[eligible]
    z_cap = (bins_remain_cap - np.mean(elig_bins_remain)) / (np.std(elig_bins_remain) + 1e-9)
    
    # Primary fit-balanced score
    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap
    enhancer = np.exp(utilization * tightness)
    
    # Predictive entropy modeling
    N = len(bins_remain_cap)
    sum_cap = np.sum(bins_remain_cap)
    mean_old = sum_cap / N
    sum_sq = np.sum(bins_remain_cap ** 2)
    var_old = sum_sq / N - mean_old ** 2
    
    elig_remain = bins_remain_cap[eligible]
    new_remain = elig_remain - item
    
    # Precompute new statistics
    new_sum = sum_cap - item
    new_mean = new_sum / N
    
    # Compute new variance for each eligible bin
    old_sq = elig_remain ** 2
    new_sq = new_remain ** 2
    sum_sq_changed = sum_sq - old_sq + new_sq
    
    new_var_i = sum_sq_changed / N - new_mean ** 2
    
    # Predictive entropy delta
    entropy_delta = var_old - new_var_i
    entropy_std = np.std(entropy_delta) if len(entropy_delta) > 1 else 1.0
    norm_entropy = entropy_delta / (entropy_std + 1e-9)
    
    # Adaptive entropy weight
    system_cv = np.std(bins_remain_cap) / (np.mean(bins_remain_cap) + 1e-9)
    large_item = item > np.mean(bins_remain_cap)
    weight_entropy = 0.5 * (1.0 + system_cv) * (1.5 if large_item else 0.5)
    weight_entropy /= np.log(2 + np.abs(entropy_delta)).mean() + 1e-9  # Self-regulating dampening
    
    # Sensitivity-based refinement
    residual_sensitivity = np.abs((new_remain - new_mean) / (np.abs(new_mean) + 1e-9))
    sensitivity_factor = 1.0 / (1.0 + residual_sensitivity)
    
    # Final priority calculation
    priority = (
        primary_score * enhancer * sensitivity_factor + 
        weight_entropy * norm_entropy
    )
    
    # Tie-breaking with dynamic perturbations
    if np.allclose(priority, priority[0]):
        priority += 1e-4 * tightness * (bins_remain_cap + 1e-9) ** (-0.5)
    
    return np.where(eligible, priority, -np.inf)
```
