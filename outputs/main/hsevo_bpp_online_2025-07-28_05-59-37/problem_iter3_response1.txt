```python
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    mask = bins_remain_cap >= item
    scores = np.full_like(bins_remain_cap, -np.inf)
    if not mask.any():
        return scores
    
    eps = 1e-9
    remaining = bins_remain_cap[mask]
    num_candidates = remaining.shape[0]
    
    # 1. Basic Metrics
    inv_leftover = 1.0 / (remaining - item + eps)
    utilization = item / (remaining + eps)
    exp_waste = np.exp(-(remaining - item))
    
    # 2. Adaptive Struct Ratio: item vs average available capacity
    avg_remaining = np.mean(remaining)
    struct_ratio = item / (avg_remaining + eps)
    weight_inv = 1.0 + struct_ratio  # Prioritize minimal waste for large items
    weight_util = 2.0 / (struct_ratio + eps)  # Adapt utilization sensitivity for small items

    # 3. Normalize metrics to shared dynamic range for fair weighting
    inv_norm = (inv_leftover - np.min(inv_leftover)) / (np.ptp(inv_leftover) + eps)
    util_norm = (utilization - np.min(utilization)) / (np.ptp(utilization) + eps)
    
    # 4. Fragmentation-preventive load balancing
    if num_candidates > 1:
        sum_remaining_full = np.sum(remaining)
        sum_remaining_else = sum_remaining_full - remaining  # Excl. current bin
        avg_other = sum_remaining_else / (num_candidates - 1 + eps)
        post_remaining = remaining - item
        delta = np.abs(post_remaining - avg_other)
        load_balance = 1.0 / (np.log1p(delta))  # Heavy sensitivity at small deltas
    else:
        load_balance = np.ones_like(inv_leftover)
    
    # 5. Variance-specific adaptivity
    curr_variance = np.var(remaining)
    variance_sensitivity = 1.0 + np.tanh(curr_variance / (avg_remaining + eps))
    
    # 6. Composite score with adaptive synergies
    dynamic_core = weight_inv * inv_norm + weight_util * util_norm
    adaptive_term = 0.7 * load_balance * variance_sensitivity
    scores[mask] = dynamic_core + adaptive_term + 0.3 * exp_waste  # Blend with waste-avoidance
    
    return scores
```
