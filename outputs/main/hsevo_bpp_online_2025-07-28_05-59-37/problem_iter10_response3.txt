import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray, 
                threshold_negligible_capacity: float = 0.008721945263091162,
                threshold_negligible_item: float = 0.0030983478844887344,
                epsilon_edge_denominator: float = 0.009040525212567617,
                penalty_factor_edge: float = 0.004958449401169328,
                epsilon_tightness_denominator: float = 0.0031651151747499997,
                epsilon_zscores_denominator: float = 0.009441922585832582,
                epsilon_system_mean: float = 0.008958329900376519,
                balance_weight_base: float = 3.984000287226551,
                small_item_multiplier: float = 1.0002070705779067,
                large_item_multiplier: float = 1.4428191481344597) -> np.ndarray:
    """
    Z-optimized fit combining exp-utilized tightness with system-wide entropy scaling (higher priority to bins that reduce overall fragmentation).
    Hybridizes v0 adaptive normalization and v1 entropy-aware balance.
    """
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    orig_cap = np.max(bins_remain_cap)
    if orig_cap <= threshold_negligible_capacity or item <= threshold_negligible_item:
        # Edge-case: negligible item, prefer minimal leftover while slightly favoring large-capacity bins
        return np.where(
            bins_remain_cap >= item,
            1.0 / (bins_remain_cap - item + epsilon_edge_denominator) - penalty_factor_edge * bins_remain_cap,
            -np.inf
        )
    
    eligible = bins_remain_cap >= item
    if not np.any(eligible):
        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    
    # Core v0 metrics: z-score fit/capacity + exponential enhancer
    leftover = bins_remain_cap - item
    utilization = (orig_cap - bins_remain_cap) / orig_cap
    tightness = item / (bins_remain_cap + epsilon_tightness_denominator)
    
    fit_quality = 1.0 / (leftover + epsilon_zscores_denominator)
    elig_fit = fit_quality[eligible]
    elig_cap = bins_remain_cap[eligible]
    
    # Z-score normalization with perturbed thresholds
    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)
    z_fit = (fit_quality - mean_fit) / (std_fit + epsilon_zscores_denominator)
    
    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)
    z_cap = (bins_remain_cap - mean_cap) / (std_cap + epsilon_zscores_denominator)
    
    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap
    enhancer = np.exp(utilization * tightness)  # Gradient-aware exponential boosting
    
    # v1-inspired entropy control with adaptive weight scaling
    system_avg = np.mean(bins_remain_cap)
    system_std = np.std(bins_remain_cap)
    system_cv = system_std / (system_avg + epsilon_system_mean)
    
    # Item classification for epsilon scaling
    threshold = np.mean(bins_remain_cap)
    large_item = item > threshold
    
    # System-aware fragmentation penalty
    balance_term = -np.abs(leftover - system_avg)  # Favours bins that reduce global variance
    balance_weight = balance_weight_base * system_cv * (small_item_multiplier if not large_item else large_item_multiplier)
    
    # Multi-layer synergy with cross-metric variance analysis
    priority = primary_score * enhancer + balance_weight * balance_term
    
    return np.where(eligible, priority, -np.inf)
