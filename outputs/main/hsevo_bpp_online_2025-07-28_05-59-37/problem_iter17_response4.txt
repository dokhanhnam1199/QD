```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines min-max normalization, variance-aware balance, and predictive reinforcement.
    Prioritizes bins based on adaptive metric weights, system variance reduction, and 
    future capacity clustering.
    """
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    orig_cap = bins_remain_cap.max()
    eps = 1e-9
    
    # Handle edge cases with zero-sized item or bins
    if orig_cap <= eps or item <= eps:
        return np.where(
            bins_remain_cap >= item,
            bins_remain_cap - item + eps,
            -np.inf
        )
    
    eligible = bins_remain_cap >= item
    if not np.any(eligible):
        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    
    eligible_rem = bins_remain_cap[eligible]
    leftover = eligible_rem - item
    tightness = item / (eligible_rem + eps)
    utilization = (orig_cap - eligible_rem) / (orig_cap + eps)
    
    # Min-Max normalization for metrics (higher value = better)
    max_left = leftover.max()
    min_left = leftover.min()
    if max_left > min_left + eps:
        norm_left = (max_left - leftover) / (max_left - min_left + eps)
    else:
        norm_left = np.zeros_like(leftover)
    
    max_tight = tightness.max()
    min_tight = tightness.min()
    if max_tight > min_tight + eps:
        norm_tight = (tightness - min_tight) / (max_tight - min_tight + eps)
    else:
        norm_tight = np.zeros_like(tightness)
    
    max_util = utilization.max()
    min_util = utilization.min()
    if max_util > min_util + eps:
        norm_util = (utilization - min_util) / (max_util - min_util + eps)
    else:
        norm_util = np.zeros_like(utilization)
    
    # Item classification
    active_caps = bins_remain_cap[bins_remain_cap > 0]
    median_cap = np.median(active_caps) if active_caps.size else orig_cap
    rel_size = item / (median_cap + eps)
    small_item = rel_size < 0.75
    
    # Dynamic weights
    tight_weight = 1.0 if small_item else 1.8
    util_weight = 1.5 if small_item else 0.5
    
    # Primary score
    primary = tight_weight * norm_tight + util_weight * norm_util
    
    # Balance contribution (variance reduction)
    system_avg = bins_remain_cap.mean()
    system_std = bins_remain_cap.std()
    system_cv = system_std / (system_avg + eps) if system_avg > eps else 0.0
    
    if leftover.size > 1:
        median_leftover = np.median(leftover)
        balance_term = -np.abs(leftover - median_leftover) / (leftover.std() + eps)
        balance_contrib = balance_term * 0.3 * system_cv
    else:
        balance_contrib = np.zeros_like(leftover)
    
    # Reinforcement: capacity clustering
    if active_caps.size > 1:
        active_median = np.median(active_caps)
        active_iqr = np.percentile(active_caps, 75) - np.percentile(active_caps, 25)
        similarity = 1.0 / (np.abs(leftover - active_median) / (active_iqr + eps) + 1.0)
    else:
        similarity = np.ones_like(leftover)
    
    reinforcer = 0.5 + 0.5 * similarity  # Scale between 0.5-1.0
    
    # Final priority
    priority = (primary + balance_contrib) * reinforcer
    
    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    scores[eligible] = priority
    return scores
```
