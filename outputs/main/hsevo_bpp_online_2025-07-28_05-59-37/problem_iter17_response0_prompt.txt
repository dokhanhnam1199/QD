{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines adaptive Z-score normalization, entropy-driven balance, and reinforcement learning concepts.\n    Prioritizes bins that optimize fit tightness, system-wide capacity balance, and future flexibility.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + 1e-9,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System-wide metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 1e-9 else 0.0\n    \n    # Item classification\n    large_item = item > system_avg\n    \n    # Core metrics\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    utilization = (orig_cap - bins_remain_cap) / (orig_cap + 1e-9)\n    \n    # Z-score normalization for fit and space\n    elig_fit = 1.0 / (leftover + 1e-9)\n    elig_fit_mean, elig_fit_std = np.mean(elig_fit[eligible]), np.std(elig_fit[eligible])\n    z_fit = (elig_fit - elig_fit_mean) / (elig_fit_std + 1e-9)\n    \n    elig_space = bins_remain_cap\n    elig_space_mean, elig_space_std = np.mean(elig_space[eligible]), np.std(elig_space[eligible])\n    z_cap = (elig_space - elig_space_mean) / (elig_space_std + 1e-9)\n    \n    # Primary score: adaptive tightness-weighted Z-combination\n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    \n    # Exponential enhancer for utilization-tightness synergy\n    enhancer = np.exp(utilization * tightness)\n    \n    # Entropy-driven balance term\n    balance_term = -np.abs(leftover - system_avg) / (system_std + 1e-9)\n    balance_weight = 0.5 * system_cv * np.where(large_item, 1.0, 2.0)\n    balance_contrib = balance_term * balance_weight\n    \n    # Reinforcement multiplier\n    eligible_rem = bins_remain_cap[eligible]\n    rel_size = item / (np.median(eligible_rem) + 1e-9)\n    fragility = ((orig_cap - bins_remain_cap) / (orig_cap + 1e-9)).clip(0, 1)\n    rem_rel = bins_remain_cap / (orig_cap + 1e-9)\n    reinforce_factor = (1 - rel_size) ** 2 * rem_rel * fragility\n    reinforcer = 1 + 0.5 * reinforce_factor\n    \n    # Final priority calculation\n    priority = (primary_score * enhancer + balance_contrib) * reinforcer\n    \n    return np.where(eligible, priority, -np.inf)\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Z-normalized synergy metrics with predictive entropy weighting.\n    Uses fit/utilization synergy, variance delta analysis, and adaptive fragmentation control.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    origin_cap = np.max(bins_remain_cap)\n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Core metrics\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + eps)\n    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)\n    fit_quality = 1.0 / (leftover + eps)\n    \n    # Z-score normalization across eligible bins\n    fit_mean = fit_quality[eligible].mean()\n    fit_std = fit_quality[eligible].std()\n    z_fit = (fit_quality - fit_mean) / (fit_std + eps)\n    \n    tight_mean = tightness[eligible].mean()\n    tight_std = tightness[eligible].std()\n    z_tight = (tightness - tight_mean) / (tight_std + eps)\n    \n    util_mean = utilization[eligible].mean()\n    util_std = utilization[eligible].std()\n    z_util = (utilization - util_mean) / (util_std + eps)\n    \n    # Primary synergy and enhancer\n    synergy = z_tight * (1 + z_util)\n    enhancer = np.exp(utilization * tightness)\n    primary_score = synergy * enhancer\n    \n    # Fragmentation penalty with entropy scaling\n    sys_entropy = bins_remain_cap.std() / (origin_cap + eps)\n    frag_penalty = 1.0 - np.exp(-leftover / (origin_cap + eps))\n    frag_component = 0.2 * (1 + sys_entropy) * frag_penalty\n    \n    # Predictive entropy delta calculation\n    sum_cap = bins_remain_cap.sum()\n    sum_sq = (bins_remain_cap ** 2).sum()\n    N = bins_remain_cap.size\n    mean_old = sum_cap / N\n    var_old = (sum_sq / N) - mean_old**2\n    \n    elig_remain = bins_remain_cap[eligible]\n    delta_sq_i = (elig_remain - item)**2 - elig_remain**2\n    new_sum_sq_i = sum_sq + delta_sq_i\n    new_mean_i = (sum_cap - item) / N\n    var_new_i = (new_sum_sq_i / N) - (new_mean_i**2)\n    var_delta_i = var_old - var_new_i\n    \n    # Normalize entropy delta across eligible bins\n    vd_mean = var_delta_i.mean()\n    vd_std = var_delta_i.std()\n    norm_entropy_delta = (var_delta_i - vd_mean) / (vd_std + eps)\n    \n    # Adaptive entropy weighting\n    system_cv = bins_remain_cap.std() / (bins_remain_cap.mean() + eps)\n    large_item = item > mean_old\n    weight_entropy = 0.5 * (1 + system_cv) * (1.5 if large_item else 0.5)\n    \n    entropy_component = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    entropy_component[eligible] = (weight_entropy * norm_entropy_delta).astype(np.float64)\n    \n    # Tie-breaker with sensitivity damping\n    tie_breaker = 0.01 * np.exp(-leftover) * (1 + sys_entropy)\n    \n    # Final score assembly\n    scores = np.where(\n        eligible,\n        primary_score - frag_component + entropy_component + tie_breaker,\n        -np.inf\n    )\n    \n    return scores\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the top heuristics use Z-score normalization across multiple metrics (fit, capacity, utilization), entropy-driven balance, and reinforcement learning concepts, while the worst rely on simpler adaptive weights without comprehensive normalization. (4th) vs (16th) shows that predictive entropy delta analysis and variance forecasting improve performance over basic entropy factors. (8th) vs (19th) highlights that gradient-enhanced scoring with dynamic item classification outperforms static penalty tiers. Overall, superior heuristics integrate **multi-metric Z-synergy**, **entropy-weighted fragmentation control**, **system variance adaptation**, and **reinforcement-style future-state prediction** to balance tight fits with long-term flexibility.\n- \nKeywords: predictive variance modeling, reinforcement learning, non-Z-score normalization, entropy-agnostic balance  \nAdvice: Use **predictive variance modeling** to anticipate fragmentation risks, **reinforcement learning** for long-term flexibility without entropy penalties, **min-max normalization** over Z-scores to reduce sensitivity to outliers, and **entropy-agnostic balance** via static-dynamic hybrid weights.  \nAvoid: Z-score synergy, entropy penalties modulated by system metrics, perturbation-based tie-breaking, exponential utilization boosting.  \nExplanation: Avoiding variance-ratio dependency (Z-scores) and system-metric-modulated entropy penalties prevents instability. Predictive variance modeling and reinforcement learning prioritize long-term gains over short-term metrics, while min-max normalization ensures robustness. Epsilon-based tie-breaking (not perturbation) maintains simplicity without introducing noise.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}