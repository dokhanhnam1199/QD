{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    \n    # Edge cases: negligible item or bin capacity\n    if item <= 1e-9 or orig_cap <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            0.1 / (bins_remain_cap + 1e-9) - 0.01 * bins_remain_cap,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # Core bin metrics for eligible bins\n    e_rc = bins_remain_cap[eligible]\n    e_leftover = e_rc - item\n    e_tightness = item / e_rc\n    e_utilization = (orig_cap - e_rc) / orig_cap\n    \n    # System-wide statistics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    # Normalized metric layers\n    fit_quality = 1.0 / (e_leftover + 1e-9)\n    z_fit = (fit_quality - fit_quality.mean()) / (fit_quality.std() + 1e-9)\n    \n    z_rc = (e_rc - e_rc.mean()) / (e_rc.std() + 1e-9)\n    \n    # Dynamic hybridization based on item tightness distribution\n    alpha = np.clip(1.0 - e_tightness.mean(), 0.3, 0.7)\n    hybrid_score = alpha * z_fit + (1 - alpha) * z_rc\n    \n    # Gradient-targeted exponential modulation\n    item_reso_factor = 1.0 + np.sqrt((item / system_avg))\n    enhancer = np.exp(e_utilization * e_tightness * item_reso_factor)\n    \n    # Entrophy-aware balance with covariance-based weighting\n    balance_term = -np.abs(e_leftover - system_avg)\n    \n    if len(e_leftover) > 1:\n        covar_factor = np.cov(fit_quality, balance_term)[0, 1] + 1e-9\n        weight_modulation = np.sign(covar_factor) * np.log(1.0 + system_cv / np.abs(covar_factor))\n    else:\n        weight_modulation = 1.0\n    \n    balance_weight = 0.1 * system_cv * weight_modulation\n    \n    # Fragility mitigation with dynamic thresholding\n    fragility_mask = (e_leftover < (system_avg - system_std))\n    fragility_penalty = np.zeros_like(balance_term)\n    fragility_penalty[fragility_mask] = -1e4 * system_cv\n    \n    # Final priority assembly with epsilon-perturbed tie-breaking\n    priority_core = (hybrid_score * enhancer + balance_weight * balance_term + fragility_penalty)\n    tiebreaker = 1e-10 * (1.0 / (e_leftover + 1e-9))\n    final_priority = priority_core + tiebreaker\n    \n    full_array = np.full_like(bins_remain_cap, -np.inf)\n    full_array[eligible] = final_priority\n    \n    return full_array\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    v2: State-aware normalization with cross-metric variance adaptation, gradient-boosted enhancer, \n    and multi-layer entropy-sensitive scoring with fragility control and perturbed thresholds.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            0.1 * bins_remain_cap / (orig_cap + 1e-5) - (bins_remain_cap - item),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Metric initialization\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    fit_quality = 1.0 / (leftover + 1e-9)\n    \n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    elig_tight = tightness[eligible]\n    \n    # State-aware Z-score with variance-controlled calibration\n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    \n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    # Cross-metric variance weighting (higher variance metric dominates)\n    var_fit = np.var(elig_fit)\n    var_cap = np.var(elig_cap)\n    cross_weight = var_cap / (var_fit + var_cap + 1e-9) if (var_fit + var_cap) > 1e-7 else 0.5\n    \n    # Composite score with adaptive curvature\n    curvature_factor = 1.0 + np.arctan(np.mean(z_fit) - np.mean(z_cap))\n    composite_z = cross_weight * z_fit + (1 - cross_weight) * z_cap * curvature_factor\n    \n    # Gradient-driven dynamic enhancer\n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap)\n    tight_density = np.clip(1.0 - (leftover - system_avg) / (3 * system_std + 1e-9), 0, 1)\n    \n    grad_scale = np.where(item > system_avg,\n                          1.0 + 2.0 * tightness * system_std,\n                          1.0 + tightness)\n    \n    enhancer = np.exp(composite_z * tight_density * grad_scale)\n    \n    # Entropy-sensitive load balance with perturbed thresholds\n    balance_term = -(np.abs(leftover - system_avg) + 1e-5 * (leftover - system_avg))\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    # Fragility-sensitive reinforcement\n    fragility_factor = 1.0 - np.exp(-leftover / (0.25 * orig_cap + 1e-9))\n    fragility_term = np.where(leftover > 0.1 * orig_cap,\n                              -0.1 * fragility_factor,\n                              -0.3 * fragility_factor)\n    \n    # Dynamic weight cross-metric adaptation\n    var_zfit = np.var(z_fit[eligible])\n    var_zcap = np.var(z_cap[eligible])\n    weight_balance = 0.1 * system_cv * var_zfit / (var_zfit + var_zcap + 1e-9)\n    weight_fragility = 0.05 * system_cv * (1 - system_cv) * (var_zcap / (var_zfit + var_zcap + 1e-9))\n    \n    # Final priority with hierarchical reinforcement\n    priority = composite_z * enhancer + weight_balance * balance_term + weight_fragility * fragility_term\n    \n    return np.where(eligible, priority, -np.inf)\n\n### Analyze & experience\n- Comparing (1st) vs (18th), we see the top heuristic integrates **Z-normalized multiscale synergy** (tight-fit/utilization) with entropy-driven fragmentation control vs basic BF/WF blending. The 1st uses adaptive frag weight (sys_entropy), while 18th uses fixed exponential penalty.  \n(4th) vs (19th): 4th implements **cross-metric variance adaptation** (synergy = z_tight * (1 + z_util)) and adaptive frag weights, whereas 19th uses simpler median alignment without dynamic entropy scaling.  \n(11th) vs (20th): Both lack unique features of top heuristics like perturbed thresholds and gradient-boosted enhancer. 11th's entropy-regularized tiebreakers are less nuanced than top-ranked heuristics.  \nComparing (1st) vs (4th): The former uses layered priortization with perturbed thresholds and exponential utilization enhancer, while the latter emphasizes entropy-adjusted frag penalty. Top heuristics excel in **hierarchical reinforcement** and **dynamic hybrid weighting**.\n- \n**Keywords**: Z-score synergy, entropy-sensitive penalties, perturbation mechanisms, dynamic weight optimization  \n**Advice**: Embrace reinforcement-inspired gain adaptation for hierarchical metrics, integrate predictive entropy modeling to anticipate system imbalance, refine tie-breaking via multi-dimensional sensitivity (e.g., combining variance and gradient metrics), and leverage meta-heuristics for self-adjusting weight gradients.  \n**Avoid**: Precomputed weight distributions, rigid penalization structures, low-resolution thresholds, or scale-agnostic normalization.  \n**Explanation**: These strategies enhance adaptability and foresight in dynamic environments, balancing short-term fit with long-term system resilience while mitigating overfitting to transient states.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}