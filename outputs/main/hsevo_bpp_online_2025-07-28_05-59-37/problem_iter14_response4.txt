```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Hybrid priority using Z-standardized fit metrics, entropy-driven variance forecasting,
    and exponential gain based on utilization-tightness synergy for optimal bin selection.
    """
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    orig_cap = np.max(bins_remain_cap)
    if orig_cap <= 1e-9 or item <= 1e-9:
        return np.where(
            bins_remain_cap >= item,
            1.0 / (bins_remain_cap - item + 1e-9) - 1e-9 * bins_remain_cap,
            -np.inf
        )
    
    eligible = bins_remain_cap >= item
    if not np.any(eligible):
        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    
    # Metric calculations
    leftover = bins_remain_cap - item
    tightness = item / (bins_remain_cap + 1e-9)  # Tightness of fit
    utilization = (orig_cap - bins_remain_cap) / orig_cap  # Bin fullness metric

    # Z-normalization of fit quality and capacities
    fit_quality = 1.0 / (leftover[eligible] + 1e-9)
    z_fit = (1.0 / (leftover + 1e-9) - np.mean(fit_quality)) / (np.std(fit_quality) + 1e-9)
    
    elig_capacities = bins_remain_cap[eligible]
    z_cap = (bins_remain_cap - np.mean(elig_capacities)) / (np.std(elig_capacities) + 1e-9)
    
    # Core hybrid score combining fit tightness and Z-normalized metrics
    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap
    
    # Exponential reinforcement based on utilization synergy
    enhancer = np.exp(utilization * tightness)
    
    # Predictive entropy modeling
    N = len(bins_remain_cap)
    new_capacities = bins_remain_cap[eligible] - item
    total_old = np.sum(bins_remain_cap)
    
    # Global statistics before any placement
    mean_old = total_old / N
    sum_sq_old = np.sum(bins_remain_cap ** 2)
    var_old = (sum_sq_old / N) - (mean_old ** 2)
    
    # Hypothetical new statistics post-placement
    new_means_i = (total_old - item) / N
    sum_sq_new_i = sum_sq_old - (bins_remain_cap[eligible] ** 2) + (new_capacities ** 2)
    var_new_i = (sum_sq_new_i / N) - (new_means_i ** 2)
    
    # Entropy sensitivity analysis
    system_sensitivity = var_old - var_new_i
    entropy_std = np.std(system_sensitivity) if len(system_sensitivity) > 1 else 1.0
    normalized_entropy = system_sensitivity / (entropy_std + 1e-9)
    
    # Adaptive entropy weighting
    system_cv = np.std(bins_remain_cap) / (np.mean(bins_remain_cap) + 1e-9)
    large_item = item > np.mean(bins_remain_cap)
    entropy_weight = 0.5 * (1.0 + system_cv) * (1.5 if large_item else 0.5)
    entropy_weight /= (np.log(2 + np.abs(system_sensitivity))).mean() + 1e-9
    
    # Residual sensitivity saturation
    sensitivity = 1.0 / (1.0 + np.abs((new_capacities - new_means_i) / (new_means_i + 1e-9)))
    
    # Combined priority score
    priority = primary_score * enhancer * sensitivity + entropy_weight * normalized_entropy
    
    # Deterministic tightness-aware tie-breaking
    if np.allclose(priority[eligible], priority[eligible][0]):
        priority += 1e-4 * tightness * (bins_remain_cap ** (-0.5))
    
    return np.where(eligible, priority, -np.inf)
```
