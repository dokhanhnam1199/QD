[
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Priority combining inv_waste, utilization, and exp_waste with adaptive tie-breaking for real-time opt.\"\"\"\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    if not mask.any():\n        return scores\n    \n    eps = 1e-9\n    remaining = bins_remain_cap[mask]\n    \n    # v0-derived factors: inv_leftover + utilization\n    inv_leftover = 1.0 / (remaining - item + eps)\n    utilization = item / (remaining + eps)\n    \n    # v1-derived exponential sensitivity to waste\n    exp_waste = np.exp(-(remaining - item))\n    \n    # Dynamic synergy: Combine v0's stability with v1's waste-awareness\n    # Weights chosen to preserve metric dominance while allowing adaptive tie-breaking\n    scores[mask] = inv_leftover + utilization + 0.1 * exp_waste\n    return scores",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.01874750698045,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response1.txt_stdout.txt",
    "code_path": "problem_iter2_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit for large items and fit+utilization for small items using dynamic thresholding and adaptive tie-breaking.\"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    C_est = bins_remain_cap.max()\n    valid = bins_remain_cap >= item\n    \n    if not np.any(valid):\n        return -np.inf * np.ones_like(bins_remain_cap)\n    \n    # Dynamic thresholds and metrics\n    avg_remain = bins_remain_cap.mean()\n    is_large = item > avg_remain\n    \n    fit_score = item - bins_remain_cap  # Best Fit priority\n    utilization = (C_est - bins_remain_cap + item) / C_est  # Utilization ratio after placement\n    \n    # Adaptive tie-breaker weight scaled by bin capacity\n    epsilon = 1e-6 * C_est  # Small dynamic weight to prevent dominance\n    \n    # Combine strategies based on item size\n    priority = np.where(\n        valid,\n        fit_score + (~is_large) * epsilon * utilization,\n        -np.inf\n    )\n    \n    return priority",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response2.txt_stdout.txt",
    "code_path": "problem_iter2_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Priority combining dynamic item size classification and adaptive tie-breaking.\n    \n    Uses current mean remaining capacity to classify item as large/small.\n    Adjusts tie-breaking weight (epsilon) to prioritize utilization for small items.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    # Dynamic threshold based on current average remaining capacity\n    threshold = np.mean(bins_remain_cap)\n    large_item = item > threshold\n    \n    valid = bins_remain_cap >= item\n    if not np.any(valid):\n        return -np.inf * np.ones_like(bins_remain_cap)\n    \n    remaining_after = bins_remain_cap - item\n    \n    # Adaptive epsilon weights based on item size classification\n    if large_item:\n        # Prioritize minimal leftover with small tie-breaker (v0-like)\n        epsilon = 1e-6\n    else:\n        # Prioritize tighter packing with larger tie-breaker for small items\n        epsilon = 1e-3\n    \n    priority = -remaining_after - epsilon * bins_remain_cap\n    return np.where(valid, priority, -np.inf)",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with dynamic tie-breaking by utilization ratio scaled adaptively.\n    \n    Uses Best Fit's negative leftover space as primary priority, augmented with a \n    dynamic epsilon-weighted utilization ratio (item/bin_remaining) for tie-breaking.\n    Epsilon scales inversely with leftover standard deviation to balance adaptability \n    and stability.\n    \"\"\"\n    mask = bins_remain_cap >= item\n    leftover = bins_remain_cap - item\n    best_fit = -leftover\n    \n    # Dynamic epsilon based on leftover variance (less variance => higher epsilon)\n    valid_leftover = leftover[mask]\n    std = np.std(valid_leftover) if len(valid_leftover) > 1 else 1e-6\n    epsilon = 1e-4 / (std + 1e-9)\n    \n    # Utilization ratio (item / remaining_cap) as adaptive tie-breaker\n    utilization = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    np.divide(item, bins_remain_cap, where=mask, out=utilization)\n    \n    # Combined priority with dynamic scaling\n    priority = np.where(mask, best_fit + epsilon * utilization, -np.inf)\n    return priority",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response4.txt_stdout.txt",
    "code_path": "problem_iter2_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Hybrid priority using inverse leftover space and utilization ratio with dynamic weight adjustment.\n    Inspired by synergy of v0's best-fit focus and v1's threshold adaptability, combining:\n        1. Inverse leftover minimization for large items (exponential decay effect)\n        2. Utilization ratio tie-breaking for small items\n        3. Dynamic weights based on item-to-mean-capacity ratio\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n        \n    eligible = bins_remain_cap >= item\n    mean_remain = np.mean(bins_remain_cap)\n    \n    # Base metrics for eligible bins\n    cap_elig = bins_remain_cap[eligible]\n    leftover = cap_elig - item\n    \n    # Inverse leftover (epsilon prevents division by zero, dominates for large items)\n    inv_leftover = 1.0 / (leftover + 1e-9)\n    # Bin utilization (higher = more filled, better for small items)\n    utilization = 1.0 - cap_elig\n    \n    # Dynamic weight adjustment: emphasizes leftover for large items, utilization for small items\n    leftover_weight = 1.0 + np.clip((item - mean_remain) / max(mean_remain, 0.1), 0, 1)\n    util_weight = 1.0 + np.clip((mean_remain - item) / max(mean_remain, 0.1), 0, 1)\n    \n    # Combined priority (product synergy with weighted components)\n    priority_value = (inv_leftover * leftover_weight) * (utilization * util_weight)\n    \n    # Initialize result with -inf for non-eligible\n    priority = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    priority[eligible] = priority_value\n    return priority",
    "response_id": 4,
    "tryHS": false,
    "obj": 149.30195452732352,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response5.txt_stdout.txt",
    "code_path": "problem_iter2_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Priority combining fit quality and dynamic utilization adjustment.\n    \n    Prioritizes bins with:\n    1. Sufficient capacity to fit the item (ineligible bins get -inf)\n    2. A score combining (1) perfect fit bias using inverse leftover, and (2) \n       dynamic weighting by bin utilization ratio measured via remaining capacity.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        # Fallback to v0 when capacity not determined or item negligible\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    \n    leftover = bins_remain_cap - item\n    fit_quality = 1.0 / (leftover + 1e-9)\n    \n    # Dynamic synergy: amplify fit quality in more utilized bins\n    scores = fit_quality * (1 + utilization)\n    return np.where(eligible, scores, -np.inf)",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response6.txt_stdout.txt",
    "code_path": "problem_iter2_code6.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with dynamic hybrid Worst Fit using real-time average to reduce fragility and improve bin usage.\"\"\"\n    can_fit = bins_remain_cap >= item\n    \n    # Calculate dynamic threshold based on average remaining capacity\n    avg_remaining = np.mean(bins_remain_cap) if bins_remain_cap.size else 1.0\n    \n    # Switch between best-fit and worst-fit based on item size relative to current system state\n    if item > avg_remaining:\n        # Best Fit for larger items: minimize leftover space (directly from v0)\n        priority = np.where(can_fit, -(bins_remain_cap - item), -np.inf)\n    else:\n        # Worst Fit for smaller items: preserve larger gaps (enhanced by v1's non-tight logic)\n        priority = np.where(can_fit, bins_remain_cap - item, -np.inf)\n    \n    return priority",
    "response_id": 6,
    "tryHS": false,
    "obj": 149.30195452732352,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response7.txt_stdout.txt",
    "code_path": "problem_iter2_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive priority combining tight-fit best-fit & small-item worst-fit with utilization tie-breaker.\"\"\"\n    possible = bins_remain_cap >= item\n    leftover = bins_remain_cap - item\n    \n    # Dynamic tightness threshold per bin (item > 50% of current bin capacity)\n    tight = (item > bins_remain_cap / 2) & possible\n    \n    # Primary heuristics: Best Fit for tight fits, Worst Fit for others\n    primary = np.where(tight, -leftover, leftover)\n    \n    # Tie-breaker: Bin utilization (normalized current occupancy) with epsilon weight\n    utilization = 1 - bins_remain_cap\n    tie_breaker = 1e-6 * utilization  # Small weight to avoid overriding primary rules\n    \n    # Composite priority: synergized primary decision + adaptive utilization bias\n    priority = primary + tie_breaker\n    \n    return np.where(possible, priority, -np.inf)",
    "response_id": 7,
    "tryHS": false,
    "obj": 88.67171918627844,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response8.txt_stdout.txt",
    "code_path": "problem_iter2_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Priority combines Best Fit (leftover minimization) and Worst Fit (remaining space maximization) dynamically.\n    Weighting adjusts based on item's relative size to current maximum bin capacity estimate.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    C_est = bins_remain_cap.max()\n    valid = bins_remain_cap >= item\n    \n    if not np.any(valid):\n        return -np.inf * np.ones_like(bins_remain_cap)\n    \n    relative_size = item / C_est\n    weight = relative_size ** 2  # Dynamic weight emphasizes Best Fit for larger items\n    \n    # Blend Best Fit and Worst Fit: [0, 1] weight on Best Fit, [1, 0] on Worst Fit\n    priorities = np.where(valid,\n                          weight * (item - bins_remain_cap) + (1 - weight) * bins_remain_cap,\n                          -np.inf)\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 144.31591543677703,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority scores combining inverse leftover space, utilization ratio, \n    and exponential decay of leftover space to prioritize tight fits and efficient packing.\n    Adapts synergy across multi-metric factors for better real-time bin selection.\n    \"\"\"\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    if not mask.any():\n        return scores\n    \n    eps = 1e-9\n    s_j = bins_remain_cap[mask]\n    # Inverse of leftover space (heavily favors tight fits)\n    inv_leftover = 1.0 / (s_j - item + eps)\n    # Item-to-remaining capacity utilization (promotes filling available space)\n    utilization = item / (s_j + eps)\n    # Exponential decay penalty for non-perfect fits (sharp disincentive for large leftover)\n    leftover = s_j - item\n    exp_decay = np.exp(-leftover)  # Normalize by input size for stability if needed\n    \n    # Dynamic synergy: sum of complementary metrics\n    combined = inv_leftover * (1 + utilization) + exp_decay\n    scores[mask] = combined\n    return scores",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.108496210610296,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response0.txt_stdout.txt",
    "code_path": "problem_iter3_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    if not mask.any():\n        return scores\n\n    eps = 1e-9\n    remaining = bins_remain_cap[mask]\n    \n    # Global state monitoring\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + eps)  # Coefficient of variation\n    \n    # Core heuristic components\n    leftover = remaining - item\n    inv_leftover = 1.0 / (leftover + eps)\n    utilization = item / (remaining + eps)\n    exp_waste = np.exp(-leftover)\n    \n    # Adaptive balancing logic\n    remaining_after = remaining - item\n    load_balance_factor = np.abs(remaining_after - system_avg)\n    load_balance_score = -load_balance_factor  # Prioritize bins reducing systemic imbalance\n    \n    # Fragmentation mitigation term (entropy-aware)\n    fragmentation_penalty = np.log1p(1.0 / (leftover + system_cv))\n    \n    # Dynamic weighting strategy\n    w_inv = 1.0 + 0.7 * system_cv  # Inverse left weight increases with system variance\n    w_util = 0.9 / (1.0 + np.exp(-utilization * 2))  # S-shaped activation based on local fit\n    w_balance = 2.0 * system_cv  # Balance importance scales with fragmentation level\n    \n    # Real-time validation layer - self-consistency check\n    fit_consistency = 1.0 / (1e-6 + np.abs(remaining_after - (1.0 - system_avg)))\n    \n    # Multimodal synergy calculation\n    scores[mask] = (\n        w_inv * inv_leftover + \n        w_util * utilization + \n        w_balance * load_balance_score + \n        0.5 * exp_waste * (1 + system_cv) +\n        fragmentation_penalty * (1/system_cv if system_cv > 1 else 1) +\n        fit_consistency\n    )\n    \n    return scores",
    "response_id": 0,
    "tryHS": false,
    "obj": 73.91304347826087,
    "SLOC": 22.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response1.txt_stdout.txt",
    "code_path": "problem_iter3_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    if not mask.any():\n        return scores\n    \n    eps = 1e-9\n    remaining = bins_remain_cap[mask]\n    num_candidates = remaining.shape[0]\n    \n    # 1. Basic Metrics\n    inv_leftover = 1.0 / (remaining - item + eps)\n    utilization = item / (remaining + eps)\n    exp_waste = np.exp(-(remaining - item))\n    \n    # 2. Adaptive Struct Ratio: item vs average available capacity\n    avg_remaining = np.mean(remaining)\n    struct_ratio = item / (avg_remaining + eps)\n    weight_inv = 1.0 + struct_ratio  # Prioritize minimal waste for large items\n    weight_util = 2.0 / (struct_ratio + eps)  # Adapt utilization sensitivity for small items\n\n    # 3. Normalize metrics to shared dynamic range for fair weighting\n    inv_norm = (inv_leftover - np.min(inv_leftover)) / (np.ptp(inv_leftover) + eps)\n    util_norm = (utilization - np.min(utilization)) / (np.ptp(utilization) + eps)\n    \n    # 4. Fragmentation-preventive load balancing\n    if num_candidates > 1:\n        sum_remaining_full = np.sum(remaining)\n        sum_remaining_else = sum_remaining_full - remaining  # Excl. current bin\n        avg_other = sum_remaining_else / (num_candidates - 1 + eps)\n        post_remaining = remaining - item\n        delta = np.abs(post_remaining - avg_other)\n        load_balance = 1.0 / (np.log1p(delta))  # Heavy sensitivity at small deltas\n    else:\n        load_balance = np.ones_like(inv_leftover)\n    \n    # 5. Variance-specific adaptivity\n    curr_variance = np.var(remaining)\n    variance_sensitivity = 1.0 + np.tanh(curr_variance / (avg_remaining + eps))\n    \n    # 6. Composite score with adaptive synergies\n    dynamic_core = weight_inv * inv_norm + weight_util * util_norm\n    adaptive_term = 0.7 * load_balance * variance_sensitivity\n    scores[mask] = dynamic_core + adaptive_term + 0.3 * exp_waste  # Blend with waste-avoidance\n    \n    return scores",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.098524132429212,
    "SLOC": 22.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response2.txt_stdout.txt",
    "code_path": "problem_iter3_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    if not mask.any():\n        return scores\n\n    eps = 1e-9\n    remaining = bins_remain_cap[mask].astype(np.float64)\n    \n    # Base factors from v1\n    leftover = remaining - item\n    inv_leftover = 1.0 / (leftover + eps)\n    utilization = item / (remaining + eps)\n    exp_waste = np.exp(-leftover)\n    \n    # Dynamic weight adjustment based on relative item size and distribution entropy\n    mean_remaining = remaining.mean()\n    std_remaining = remaining.std()\n    relative_size = np.clip(item / (mean_remaining + eps), 0, 1)  # normalize to [0,1]\n    \n    # Adaptive weights via piecewise functions inspired by real-time bin status\n    w_inv = (relative_size**2 + (std_remaining/(mean_remaining + eps))) \n    w_uti = (1 - relative_size)**2\n    w_exp = 0.5 * (1 + relative_size * std_remaining)\n    \n    # Fragmentation/entropy-based tiebreaker: variance normalization\n    bin_count = bins_remain_cap.size\n    sum_old = bins_remain_cap.sum()\n    sum_squares_old = (bins_remain_cap**2).sum()\n    delta_sum_squares = (-2 * item * remaining + item**2)\n    sum_squares_new = sum_squares_old + delta_sum_squares\n    mu_new = (sum_old - item) / bin_count\n    variance_new = (sum_squares_new / bin_count) - mu_new**2\n    \n    # Adaptation tweak: calculate sensitivity of variance contribution\n    moment_ratio = np.sqrt(np.abs(variance_new + eps)) + 1e-5\n    \n    # Dynamic score composition: combine weighted base factors and normalized variance difference\n    main_score = (w_inv * inv_leftover) + (w_uti * utilization) + (w_exp * exp_waste)\n    tie_breaker = -variance_new / moment_ratio  # Prefer lower variance outcomes\n    \n    # Final score with auto-scaled tiebreaker using magic (could be further optimized)\n    scores[mask] = main_score + 1e-4 * tie_breaker\n    \n    return scores",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 22.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response3.txt_stdout.txt",
    "code_path": "problem_iter3_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    if not mask.any():\n        return scores\n    \n    eps = 1e-9\n    remaining = bins_remain_cap[mask]\n    \n    # Adaptive statistics tracking\n    item_rel = item / (remaining.mean() + eps)\n    sigma = remaining.std()\n    skew = np.abs((remaining - remaining.mean()) / (sigma + eps))\n    entropy = -((remaining / (remaining.sum() + eps)) * np.log(remaining / (remaining.sum() + eps) + eps)).sum()\n    \n    # Dynamic component weights\n    best_weight = np.abs(np.log(item_rel + eps)) + entropy  # Stronger best-fit when skewed\n    balance_weight = 1 - item_rel + entropy  # Adapt to uniformity needs\n    frag_weight = item_rel  # Fragmentation penalty scales with item size\n    \n    # Core components (v1-inspired)\n    inv_leftover = 1.0 / (remaining - item + eps)\n    utilization = item / (remaining + eps)\n    exp_waste = np.exp(-(remaining - item))\n    \n    # Entropy-aware fragmentation metric\n    frag_mask = (remaining - item) < (remaining.mean() / (1 + skew.mean() + eps))\n    frag_penalty = np.where(frag_mask, 0.1, -0.1)  # Reward meaningful fragmentations\n    \n    # Adaptive bin utilization balance\n    balance_score = 1 / (1 + np.exp(2 * (remaining - (remaining.mean() + sigma))))\n    \n    # Synergistic score calculation\n    synergy_score = (\n        best_weight * inv_leftover +\n        balance_weight * balance_score +\n        0.5 * exp_waste +\n        frag_weight * frag_penalty\n    )\n    \n    # Adaptive tie-breaking via bin heterogeneity metrics\n    secondary_tiebreaker = -skew if entropy > 0.7 else skew\n    \n    scores[mask] = synergy_score + 0.01 * secondary_tiebreaker  # Small tilt for real-time adaptability\n    return scores",
    "response_id": 3,
    "tryHS": false,
    "obj": 3.9888312724371757,
    "SLOC": 22.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response4.txt_stdout.txt",
    "code_path": "problem_iter3_code4.py",
    "code": "import numpy as np\nfrom typing import Optional\nimport collections\nimport threading\n\n# Global state manager with adaptive learning capabilities\nclass AdaptivePackingState:\n    _instance_lock = threading.Lock()\n    _instance = None\n\n    def __new__(cls, *args, **kwargs):\n        if not cls._instance:\n            with cls._instance_lock:\n                if not cls._instance:\n                    cls._instance = super(AdaptivePackingState, cls).__new__(cls)\n        return cls._instance\n\n    def __init__(self, window_size=1000):\n        if hasattr(self, 'initialized'):\n            return\n            \n        self.utilization_weight = 0.4\n        self.waste_weight = 0.5\n        self.entropy_weight = 0.1\n        self.capacity_history = collections.deque(maxlen=window_size)\n        self.size_history = collections.deque(maxlen=window_size)\n        self.efficiency_weights = collections.deque(maxlen=100)\n        self.preference_trend = collections.deque(maxlen=100)\n        self.lock = threading.Lock()\n        self.initialized = True\n\n    def adapt_weights(self, item: float, valid_caps: np.ndarray, scores: np.ndarray):\n        \"\"\"Dynamically adjust decision weights based based on item statistics and packing patterns.\"\"\"\n        with self.lock:\n            # Update historical records\n            self.capacity_history.append(np.mean(valid_caps) if valid_caps.size > 0 else 0)\n            self.size_history.append(item)\n            \n            # Analyze distribution patterns\n            small_items = sum(sz <= 0.1 for sz in self.size_history)\n            large_items = sum(sz >= 0.7 for sz in self.size_history)\n            \n            # Basic decision making based on item statistics\n            if len(self.size_history) >= 10:\n                size_ratio = small_items / (large_items + 1)\n                \n                # Adjust weights in favor of entropy preservation when many small items\n                if size_ratio > 2 and self.entropy_weight < 0.35:\n                    self.entropy_weight += 0.025\n                elif size_ratio < 0.5 and self.entropy_weight > 0.05:\n                    self.entropy_weight -= 0.01\n                \n                # Adjust utilization weight based on score dynamics\n                if scores.size > 1:\n                    max_score = np.max(scores)\n                    second_best = np.partition(scores, -2)[-2] if scores.size > 1 else -np.inf\n                    \n                    # When choices are ambiguous, increase entropy weight\n                    if np.isfinite(second_best) and max_score < second_best * 1.2:\n                        self.entropy_weight = min(0.35, self.entropy_weight * 1.05)\n                        \n                # Recalibrate weights to sum to 1\n                total = self.utilization_weight + self.waste_weight + self.entropy_weight\n                self.utilization_weight = max(0.05, self.utilization_weight / (total + 1e-9))\n                self.waste_weight = max(0.05, self.waste_weight / (total + 1e-9))\n                self.entropy_weight = max(0.05, self.entropy_weight / (total + 1e-9))\n                \n            # Store current preference\n            if scores.size > 0 and np.isfinite(scores).any():\n                self.preference_trend.append(np.argmin(valid_caps) if valid_caps.size > 0 else 0)\n                \nstate = AdaptivePackingState()\n\ndef is_viable(item: float, capacity: np.ndarray) -> np.ndarray:\n    \"\"\"Determine which bins can accept the current item.\"\"\"\n    return capacity >= item\n\ndef calculate_utilization(item: float, capacity: np.ndarray) -> np.ndarray:\n    \"\"\"Compute utilization efficiency on a per-bin basis.\"\"\"\n    return item / (capacity + 1e-9)\n\ndef calculate_inverse_waste(item: float, capacity: np.ndarray) -> np.ndarray:\n    \"\"\"Quantify wasted space in a inverse way - smaller waste = higher score.\"\"\"\n    return 1.0 / (capacity - item + 1e-9)\n\ndef calculate_fragility_impact(item: float, capacity: np.ndarray) -> np.ndarray:\n    \"\"\"Calculate the potential fragility of each bin after accepting this item.\"\"\"\n    mask = is_viable(item, capacity)\n    fragility = np.zeros_like(capacity)\n    \n    if mask.any():\n        used = capacity[mask] - item\n        fragility[mask] = 1.0 / (np.log2(used/item + 2) + 1e-9)\n    return fragility\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive packing heuristic that evolves based on item stream characteristics.\n    \n    Incorporates:\n    - Dynamic weight adjustment for different packing patterns\n    - Entropy-aware tie-breaking strategy\n    - Fragility analysis to prevent capacity traps\n    - Self-optimization based on historical feedback\n    \n    Objectives:\n    1. Maximize capacity utilization\n    2. Minimize wasted space\n    3. Preserve packing flexibility\n    4. Adapt to item distributions in real-time\n    \n    Args:\n        item: Size of item to pack\n        bins_remain_cap: Array of remaining capacities for each bin\n    \n    Returns:\n        np.ndarray: Priority scores for each bin\n    \"\"\"\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Filter viable bins\n    viable = is_viable(item, bins_remain_cap)\n    if not viable.any():\n        return scores\n    \n    # Calculate core metrics\n    cap_viable = bins_remain_cap[viable]\n    utilization = calculate_utilization(item, cap_viable)\n    inverse_waste = calculate_inverse_waste(item, cap_viable)\n    fragility = calculate_fragility_impact(item, cap_viable)\n    \n    # Combine components with adaptive weights\n    core_array = (\n        state.utilization_weight * utilization +\n        state.waste_weight * inverse_waste +\n        state.entropy_weight * fragility\n    )\n    \n    # Tie-breaking mechanism\n    # When scores are too close, prefer bins slightly larger than item size\n    if np.count_nonzero(viable) > 1 and np.any(np.isclose(core_array, np.max(core_array), atol=1e-9)):\n        delta = (cap_viable - item) / (cap_viable + 1e-9)\n        secondary_key = -delta  # Prefer smaller deltas (better fits)\n        \n        # Add a small boost to differentiate near-equal scores\n        core_array += 0.001 * secondary_key\n    \n    # Update scores array\n    scores[viable] = core_array\n    \n    # Update adaptive state for next iteration\n    cap_used = cap_viable if viable.any() else np.array([])\n    state.adapt_weights(item, cap_used, scores[scores != -np.inf])\n    \n    return scores",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 22.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  }
]