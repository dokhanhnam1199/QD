```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines Z-normalized synergy metrics with predictive entropy weighting.
    Uses fit/utilization synergy, variance delta analysis, and adaptive fragmentation control.
    """
    eps = 1e-9
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    origin_cap = np.max(bins_remain_cap)
    eligible = bins_remain_cap >= item
    if not eligible.any():
        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    
    # Core metrics
    leftover = bins_remain_cap - item
    tightness = item / (bins_remain_cap + eps)
    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)
    fit_quality = 1.0 / (leftover + eps)
    
    # Z-score normalization across eligible bins
    fit_mean = fit_quality[eligible].mean()
    fit_std = fit_quality[eligible].std()
    z_fit = (fit_quality - fit_mean) / (fit_std + eps)
    
    tight_mean = tightness[eligible].mean()
    tight_std = tightness[eligible].std()
    z_tight = (tightness - tight_mean) / (tight_std + eps)
    
    util_mean = utilization[eligible].mean()
    util_std = utilization[eligible].std()
    z_util = (utilization - util_mean) / (util_std + eps)
    
    # Primary synergy and enhancer
    synergy = z_tight * (1 + z_util)
    enhancer = np.exp(utilization * tightness)
    primary_score = synergy * enhancer
    
    # Fragmentation penalty with entropy scaling
    sys_entropy = bins_remain_cap.std() / (origin_cap + eps)
    frag_penalty = 1.0 - np.exp(-leftover / (origin_cap + eps))
    frag_component = 0.2 * (1 + sys_entropy) * frag_penalty
    
    # Predictive entropy delta calculation
    sum_cap = bins_remain_cap.sum()
    sum_sq = (bins_remain_cap ** 2).sum()
    N = bins_remain_cap.size
    mean_old = sum_cap / N
    var_old = (sum_sq / N) - mean_old**2
    
    elig_remain = bins_remain_cap[eligible]
    delta_sq_i = (elig_remain - item)**2 - elig_remain**2
    new_sum_sq_i = sum_sq + delta_sq_i
    new_mean_i = (sum_cap - item) / N
    var_new_i = (new_sum_sq_i / N) - (new_mean_i**2)
    var_delta_i = var_old - var_new_i
    
    # Normalize entropy delta across eligible bins
    vd_mean = var_delta_i.mean()
    vd_std = var_delta_i.std()
    norm_entropy_delta = (var_delta_i - vd_mean) / (vd_std + eps)
    
    # Adaptive entropy weighting
    system_cv = bins_remain_cap.std() / (bins_remain_cap.mean() + eps)
    large_item = item > mean_old
    weight_entropy = 0.5 * (1 + system_cv) * (1.5 if large_item else 0.5)
    
    entropy_component = np.zeros_like(bins_remain_cap, dtype=np.float64)
    entropy_component[eligible] = (weight_entropy * norm_entropy_delta).astype(np.float64)
    
    # Tie-breaker with sensitivity damping
    tie_breaker = 0.01 * np.exp(-leftover) * (1 + sys_entropy)
    
    # Final score assembly
    scores = np.where(
        eligible,
        primary_score - frag_component + entropy_component + tie_breaker,
        -np.inf
    )
    
    return scores
```
