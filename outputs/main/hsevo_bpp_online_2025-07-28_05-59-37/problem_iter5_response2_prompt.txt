{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n# Global state manager with adaptive learning capabilities\nclass AdaptivePackingState:\n    _instance_lock = threading.Lock()\n    _instance = None\n\n    def __new__(cls, *args, **kwargs):\n        if not cls._instance:\n            with cls._instance_lock:\n                if not cls._instance:\n                    cls._instance = super(AdaptivePackingState, cls).__new__(cls)\n        return cls._instance\n\n    def __init__(self, window_size=1000):\n        if hasattr(self, 'initialized'):\n            return\n            \n        self.utilization_weight = 0.4\n        self.waste_weight = 0.5\n        self.entropy_weight = 0.1\n        self.capacity_history = collections.deque(maxlen=window_size)\n        self.size_history = collections.deque(maxlen=window_size)\n        self.efficiency_weights = collections.deque(maxlen=100)\n        self.preference_trend = collections.deque(maxlen=100)\n        self.lock = threading.Lock()\n        self.initialized = True\n\n    def adapt_weights(self, item: float, valid_caps: np.ndarray, scores: np.ndarray):\n        \"\"\"Dynamically adjust decision weights based based on item statistics and packing patterns.\"\"\"\n        with self.lock:\n            # Update historical records\n            self.capacity_history.append(np.mean(valid_caps) if valid_caps.size > 0 else 0)\n            self.size_history.append(item)\n            \n            # Analyze distribution patterns\n            small_items = sum(sz <= 0.1 for sz in self.size_history)\n            large_items = sum(sz >= 0.7 for sz in self.size_history)\n            \n            # Basic decision making based on item statistics\n            if len(self.size_history) >= 10:\n                size_ratio = small_items / (large_items + 1)\n                \n                # Adjust weights in favor of entropy preservation when many small items\n                if size_ratio > 2 and self.entropy_weight < 0.35:\n                    self.entropy_weight += 0.025\n                elif size_ratio < 0.5 and self.entropy_weight > 0.05:\n                    self.entropy_weight -= 0.01\n                \n                # Adjust utilization weight based on score dynamics\n                if scores.size > 1:\n                    max_score = np.max(scores)\n                    second_best = np.partition(scores, -2)[-2] if scores.size > 1 else -np.inf\n                    \n                    # When choices are ambiguous, increase entropy weight\n                    if np.isfinite(second_best) and max_score < second_best * 1.2:\n                        self.entropy_weight = min(0.35, self.entropy_weight * 1.05)\n                        \n                # Recalibrate weights to sum to 1\n                total = self.utilization_weight + self.waste_weight + self.entropy_weight\n                self.utilization_weight = max(0.05, self.utilization_weight / (total + 1e-9))\n                self.waste_weight = max(0.05, self.waste_weight / (total + 1e-9))\n                self.entropy_weight = max(0.05, self.entropy_weight / (total + 1e-9))\n                \n            # Store current preference\n            if scores.size > 0 and np.isfinite(scores).any():\n                self.preference_trend.append(np.argmin(valid_caps) if valid_caps.size > 0 else 0)\n                \nstate = AdaptivePackingState()\n\n    \"\"\"Determine which bins can accept the current item.\"\"\"\n    return capacity >= item\n\n    \"\"\"Compute utilization efficiency on a per-bin basis.\"\"\"\n    return item / (capacity + 1e-9)\n\n    \"\"\"Quantify wasted space in a inverse way - smaller waste = higher score.\"\"\"\n    return 1.0 / (capacity - item + 1e-9)\n\n    \"\"\"Calculate the potential fragility of each bin after accepting this item.\"\"\"\n    mask = is_viable(item, capacity)\n    fragility = np.zeros_like(capacity)\n    \n    if mask.any():\n        used = capacity[mask] - item\n        fragility[mask] = 1.0 / (np.log2(used/item + 2) + 1e-9)\n    return fragility\n\n    \"\"\"\n    Adaptive packing heuristic that evolves based on item stream characteristics.\n    \n    Incorporates:\n    - Dynamic weight adjustment for different packing patterns\n    - Entropy-aware tie-breaking strategy\n    - Fragility analysis to prevent capacity traps\n    - Self-optimization based on historical feedback\n    \n    Objectives:\n    1. Maximize capacity utilization\n    2. Minimize wasted space\n    3. Preserve packing flexibility\n    4. Adapt to item distributions in real-time\n    \n    Args:\n        item: Size of item to pack\n        bins_remain_cap: Array of remaining capacities for each bin\n    \n    Returns:\n        np.ndarray: Priority scores for each bin\n    \"\"\"\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Filter viable bins\n    viable = is_viable(item, bins_remain_cap)\n    if not viable.any():\n        return scores\n    \n    # Calculate core metrics\n    cap_viable = bins_remain_cap[viable]\n    utilization = calculate_utilization(item, cap_viable)\n    inverse_waste = calculate_inverse_waste(item, cap_viable)\n    fragility = calculate_fragility_impact(item, cap_viable)\n    \n    # Combine components with adaptive weights\n    core_array = (\n        state.utilization_weight * utilization +\n        state.waste_weight * inverse_waste +\n        state.entropy_weight * fragility\n    )\n    \n    # Tie-breaking mechanism\n    # When scores are too close, prefer bins slightly larger than item size\n    if np.count_nonzero(viable) > 1 and np.any(np.isclose(core_array, np.max(core_array), atol=1e-9)):\n        delta = (cap_viable - item) / (cap_viable + 1e-9)\n        secondary_key = -delta  # Prefer smaller deltas (better fits)\n        \n        # Add a small boost to differentiate near-equal scores\n        core_array += 0.001 * secondary_key\n    \n    # Update scores array\n    scores[viable] = core_array\n    \n    # Update adaptive state for next iteration\n    cap_used = cap_viable if viable.any() else np.array([])\n    state.adapt_weights(item, cap_used, scores[scores != -np.inf])\n    \n    return scores\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Priority combines Best Fit (leftover minimization) and Worst Fit (remaining space maximization) dynamically.\n    Weighting adjusts based on item's relative size to current maximum bin capacity estimate.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    C_est = bins_remain_cap.max()\n    valid = bins_remain_cap >= item\n    \n    if not np.any(valid):\n        return -np.inf * np.ones_like(bins_remain_cap)\n    \n    relative_size = item / C_est\n    weight = relative_size ** 2  # Dynamic weight emphasizes Best Fit for larger items\n    \n    # Blend Best Fit and Worst Fit: [0, 1] weight on Best Fit, [1, 0] on Worst Fit\n    priorities = np.where(valid,\n                          weight * (item - bins_remain_cap) + (1 - weight) * bins_remain_cap,\n                          -np.inf)\n    return priorities\n\n### Analyze & experience\n- Comparing 1st vs 20th: Best uses multiple adaptive metrics (inv_waste + utilization + exp_waste) vs Worst applies static Best Fit/Worst Fit switching. 2nd vs 19th: Both blend metrics, but 2nd's exponential penalty prevents fragmentation vs 19th's rigid threshold. 3rd vs 18th: Utilization-weighted fit quality outperforms size-based switching by balancing utilization dynamics. 4th vs 17th: Dynamic epsilon adjustment (item size vs average) improves flexibility vs static linear blending. 5th vs 16th: Global state tracking with entropy-aware tie-breaking adapts better than fixed 50% tightness threshold.  \n\nTop heuristics excel through multi-metric synergy (tight fit + system-wide balance + utilization), real-time normalization (dynamic weights via variance/mean), and layered tie-breaking (fragility, entropies). Struggling heuristics oversimplify with rigid thresholds (fixed average/item-based cuts) or suffer from overcomplexity (stateful systems requiring fine-tuned decay windows). Key losers rely on single heuristic hybrids (Best/Worst Fit mixing) without contextual adaptation.\n- \n- **Keywords**: Multi-objective optimization, hybrid metrics, adaptive normalization, contextual dynamics.  \n- **Advice**: Prioritize hierarchical objectives with non-linear sensitivity analysis (e.g., sigmoid or logarithmic decay) and integrate reinforcement learning for parameter calibration.  \n- **Avoid**: Static blending ratios, post-hoc rationalization, and over-reliance on single-state snapshots.  \n- **Explanation**: Non-linear sensitivity ensures scalable trade-offs; machine learning adapts to systemic unpredictability. Contextual tie-breakers maintain robustness without brittle thresholds.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}