```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Z-synergy tight-fit/util + entropy-adjusted frag penalty for adaptive packing.
    
    Combines z-score normalized tight-fit/utilization synergy with entropy-driven
    fragmentation control. Dynamically balances packing density against bin state 
    variance to minimize long-tail fragmentation.
    """
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    eps = 1e-9
    mask = bins_remain_cap >= item
    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    
    if not mask.any():
        return scores

    origin_cap = bins_remain_cap.max()
    rem_cap = bins_remain_cap[mask]
    leftover = rem_cap - item

    # Tight-fit metric (v0)
    inv_waste = 1.0 / (leftover + eps)
    exp_tight = np.exp(-(leftover))
    tight_fit = inv_waste + exp_tight
    
    # Utilization metric
    bin_util = (origin_cap - rem_cap) / (origin_cap + eps)

    # Z-score normalization across candidate bins
    t_mean, t_std = tight_fit.mean(), tight_fit.std()
    u_mean, u_std = bin_util.mean(), bin_util.std()
    z_tight = (tight_fit - t_mean) / (t_std + eps)
    z_util = (bin_util - u_mean) / (u_std + eps)

    # Synergy (multiplicative interaction with normalized amplification)
    synergy = z_tight * (1 + z_util)  # Capture utilization-boosted tightness
    
    # System entropy measurement (capacity spread)
    sys_entropy = bins_remain_cap.std() / (origin_cap + eps)
    
    # Fragmentation penalty (smoothly scales with small leftover spaces)
    frag_penalty = -np.expm1(-leftover / (origin_cap + eps))
    
    # Adaptive frag weight: higher entropy â†’ heavier penalty for fragmentation
    frag_weight = 0.15 * (1.0 + sys_entropy) 
    
    # Final score computation
    scores_masked = synergy - frag_weight * frag_penalty
    scores[mask] = scores_masked
    
    return scores
```
