```python
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Hybridized priority function combining multi-axis Z-normalization, 
    gradient-aware boosting, and perturbed variance entropy control.
    - Implements cross-metric variance weighting
    - Embeds fragility-aware tie-breaking heuristics
    - Uses adaptive item classification based on percentile thresholds
    """
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    # Zero-mass item handling with bias toward future utilization
    if item <= 1e-9:
        return np.where(
            bins_remain_cap >= 0,
            1e-4 * bins_remain_cap - 1e-7 * bins_remain_cap**2,
            -np.inf
        )
    
    eligible = bins_remain_cap >= item
    if not np.any(eligible):
        return np.full_like(bins_remain_cap, -np.inf)
    
    # Metric primitives for bin assessment
    leftover = bins_remain_cap - item
    fit_quality = 1.0 / (leftover + 1e-9)
    tightness = item / (bins_remain_cap + 1e-9)
    utilization = (1.0 - bins_remain_cap / bins_remain_cap.max()) if bins_remain_cap.max() > 0 else 0.0
    
    # Dynamic normalization core (state-aware Z-transformation)
    def z_scores(vec, mask):
        subset = vec[mask] if mask.any() else vec
        return (vec - subset.mean()) / (subset.std() + 1e-9) if subset.size > 1 else 0.0
    
    # Fundamental scoring layers
    z_fit = z_scores(fit_quality, eligible)
    z_tight = z_scores(tightness, eligible)
    z_left = z_scores(leftover, eligible)
    
    # Metric variance analysis for adaptive fusion
    var_fit = 1e-9 + (fit_quality[eligible]).var()
    var_tight = 1e-9 + (tightness[eligible]).var()
    
    # Primary scorer: variance-weighted Z-transformation fusion
    var_ratio = var_fit / (var_fit + var_tight)
    primary = var_ratio * z_fit + (1 - var_ratio) * z_tight
    
    # Gradient-aware multiplicative boosting
    sys_skew = z_scores(bins_remain_cap, np.ones_like(eligible, dtype=bool))  # System-wide Z
    # Dynamic item classification (median as threshold)
    median_cap = np.median(bins_remain_cap[eligible]) if eligible.sum() > 0 else item
    boost_magnitude = (1.25 if item > median_cap else 0.85)
    gradient_boost = boost_magnitude * np.exp(0.5 * (z_tight + utilization))
    
    # Multi-scale balance control with entropy decay
    system_avg, system_std = bins_remain_cap.mean(), bins_remain_cap.std()
    balance_term = -abs((leftover - system_avg) / (system_std + 1e-9))
    
    # Fragility layer (leftover usability estimation)
    fragility_metric = abs(leftover - 0.5 * system_avg) * (1 + abs(sys_skew))
    fragility_penalty = 0.1 * fragility_metric * z_left
    
    # Cross-metric decay envelope      
    decay_envelope = np.exp(-0.1 * (leftover / (bins_remain_cap.max() + 1e-9)))
    
    # Final priority aggregation - multi-phase improvement cascade
    priority = primary * gradient_boost
    priority += 0.05 * system_std * balance_term
    priority -= fragility_penalty
    priority *= decay_envelope  # Add contextual scaling

    # Epsilon perturbation to break degeneracy
    priority += 1e-8 * np.random.randn(*bins_remain_cap.shape)

    return np.where(eligible, priority, -np.inf)
```
