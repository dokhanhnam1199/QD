[
  {
    "stdout_filepath": "problem_iter20_response0.txt_stdout.txt",
    "code_path": "problem_iter20_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines adaptive normalization, item-size classification, predictive variance control,\n    and layered entropy-aware penalties for online BPP.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    if item <= 1e-9:\n        return np.where(bins_remain_cap >= 0, 0.01 * bins_remain_cap, -np.inf)\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    C_est = bins_remain_cap.max() if bins_remain_cap.max() > 0 else item * 2\n    \n    # Metric calculations\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    fit_score = 1.0 / (leftover + 1e-9)\n    \n    # Min-Max normalization\n    tight_mask = tightness[eligible]\n    fit_mask = fit_score[eligible]\n    tight_min, tight_max = tight_mask.min(), tight_mask.max()\n    fit_min, fit_max = fit_mask.min(), fit_mask.max()\n    \n    norm_tight = (tightness - tight_min) / (tight_max - tight_min + 1e-9)\n    norm_fit = (fit_score - fit_min) / (fit_max - fit_min + 1e-9)\n    \n    # Item classification and adaptive weights\n    is_large = item > 0.7 * C_est\n    fit_weight = 0.2 if is_large else 0.7\n    tight_weight = 0.8 if is_large else 0.3\n    base_score = fit_weight * norm_fit + tight_weight * norm_tight\n    \n    # Predictive variance modeling (without variance ratio division)\n    n = len(bins_remain_cap)\n    current_sum = bins_remain_cap.sum()\n    current_sum_sq = (bins_remain_cap ** 2).sum()\n    \n    new_cap_elig = bins_remain_cap[eligible] - item\n    delta_sq_elig = new_cap_elig**2 - bins_remain_cap[eligible]**2\n    new_sum_sq_elig = current_sum_sq + delta_sq_elig\n    new_mean_elig = (current_sum - item) / n\n    new_var_elig = (new_sum_sq_elig / n) - new_mean_elig**2\n    current_var = (current_sum_sq / n) - (current_sum / n)**2\n    delta_var_elig = new_var_elig - current_var\n    \n    variance_term = np.zeros_like(bins_remain_cap)\n    variance_term[eligible] = -delta_var_elig  # Reward variance reduction\n    \n    # Fragmentation anticipation\n    frag_term = np.zeros_like(bins_remain_cap)\n    frag_term[eligible] = np.exp(-leftover[eligible] / (C_est / 3 + 1e-9))\n    \n    # Stability preservation\n    median_cap = np.median(bins_remain_cap[eligible]) if eligible.any() else C_est\n    proximity = np.abs(bins_remain_cap - median_cap) / (C_est + 1e-9)\n    \n    # Priority assembly\n    priority = (\n        base_score\n        + 0.001 * variance_term  # Small weight for absolute variance changes\n        - 0.1 * frag_term\n        - 0.05 * proximity\n    )\n    \n    # Reinforcement decay (avoid exponential boosting)\n    priority *= (1.0 / (1.0 + 0.05 * leftover))  # Inverse scaling instead of exp\n    \n    # Deterministic tie-breaking (layered entropy-aware)\n    tie_breaker = (1.0 / (leftover + 1e-9)) * (1.0 / (proximity + 1e-9))\n    priority += 1e-7 * tie_breaker\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 0,
    "tryHS": false,
    "obj": 59.96210610291185,
    "SLOC": 71.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response1.txt_stdout.txt",
    "code_path": "problem_iter20_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Priority V2 heuristic combining tight-fit and future flexibility.\n    Adaptive normalization of fit tightness and variance penalty (V0) combined with\n    dynamic weight-adjusted reinforcement learning-inspired flexibility gains (V1)\n    to balance immediate and future packing efficiency.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item, \n            bins_remain_cap - item + 1e-9, \n            -np.inf\n        )\n\n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    el_remain = bins_remain_cap[eligible]\n    el_leftover = el_remain - item\n    median_L = np.median(el_leftover)\n    range_L = el_leftover.max() - el_leftover.min()\n    \n    if range_L > 0:\n        var_pen = np.abs(el_leftover - median_L) / range_L\n    else:\n        var_pen = np.zeros_like(el_leftover, dtype=np.float64)\n    \n    median_remain = np.median(el_remain)\n    weight_t = item / (median_remain + 1e-9)\n    weight_v = 1.0 - weight_t\n    \n    tightness = item / (el_remain + 1e-9)\n    base_priority = weight_t * tightness - weight_v * var_pen\n\n    # Reinforce score for future flexibility\n    rem_ratio = (el_remain - item) / (orig_cap + 1e-9)\n    min_rr, max_rr = np.min(rem_ratio), np.max(rem_ratio)\n    range_rr = max_rr - min_rr\n    reinforce_score = np.zeros_like(rem_ratio, dtype=np.float64)\n    if range_rr > 1e-9:\n        reinforce_score = (rem_ratio - min_rr) / (range_rr + 1e-9)\n\n    # Dynamic fusion of base priority and predictive flexibility\n    adjusted_priority = base_priority + weight_v * reinforce_score\n    \n    # Layered tie-breaker preserving small-space robustness\n    tie_breaker = -1e-9 * el_leftover\n    final_priority = adjusted_priority + tie_breaker\n    \n    priority = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    priority[eligible] = final_priority\n    return priority",
    "response_id": 1,
    "tryHS": false,
    "obj": 73.20502592740328,
    "SLOC": 71.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response2.txt_stdout.txt",
    "code_path": "problem_iter20_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive priority combining dynamic weights, min-max normalization,\n    proximity penalties, and fragility-aware reinforcement for online BPP.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    if item < eps:\n        return np.where(bins_remain_cap >= 0, 0.01 * bins_remain_cap, -np.inf)\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    orig_cap = bins_remain_cap.max() if bins_remain_cap.max() > 0 else item * 2\n    remaining_cap = bins_remain_cap[eligible]\n    leftover = remaining_cap - item + eps\n    \n    # Metric calculations\n    fit_tightness = item / (remaining_cap + eps)\n    util_after = (orig_cap - remaining_cap + item) / (orig_cap + eps)\n    \n    # Min-max normalization\n    def minmax_normalize(x):\n        xmin, xmax = x.min(), x.max()\n        if xmax > xmin:\n            return (x - xmin) / (xmax - xmin + eps)\n        return np.full_like(x, 0.5)\n    \n    norm_fit = minmax_normalize(fit_tightness)\n    norm_util = minmax_normalize(util_after)\n    \n    # Dynamic weights based on item size\n    is_large = item > 0.7 * orig_cap\n    weight_fit = 0.7 if is_large else 0.4\n    weight_util = 0.3 if is_large else 0.6\n    base_score = weight_fit * norm_fit + weight_util * norm_util\n    \n    # System metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std() + eps\n    \n    # Proximity penalty\n    proximity = np.abs(leftover - system_avg) / system_std\n    proximity_penalty = np.exp(-proximity)\n    \n    # Reinforcement term\n    usability = np.clip((system_avg - leftover) / system_std, -1.0, 1.0)\n    reinforcer = util_after * np.abs(usability)\n    \n    # Tie-breaker\n    tie_breaker = 0.01 * (leftover / (orig_cap + eps))\n    \n    # Final score assembly\n    eligible_scores = (\n        base_score * proximity_penalty \n        + reinforcer \n        + tie_breaker\n    )\n    \n    scores = np.full_like(bins_remain_cap, -np.inf)\n    scores[eligible] = eligible_scores\n    \n    return scores",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.397686477862,
    "SLOC": 71.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response3.txt_stdout.txt",
    "code_path": "problem_iter20_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive heuristic combining Z-score normalization, entropy-aware balance,\n    and fragility-aware reinforcement. Prioritizes tight fits for large items\n    and capacity balancing for small items while promoting flexible remaining capacity.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + 1e-9,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System state metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 1e-9 else 0.0\n    \n    # Item classification\n    strong_item = item > system_avg\n    \n    # Core metrics per bin\n    leftover = np.where(eligible, bins_remain_cap - item, np.inf)\n    tightness = item / (bins_remain_cap + 1e-9)\n    fragility = (orig_cap - bins_remain_cap) / (orig_cap + 1e-9)  # Utilization proxy\n    \n    # Adaptive Z-score normalization\n    elig_fit = np.where(eligible, 1.0 / (leftover + 1e-9), np.nan)\n    z_fit = (elig_fit - np.nanmean(elig_fit)) / (np.nanstd(elig_fit) + 1e-9)\n    \n    elig_space = np.where(eligible, bins_remain_cap, np.nan)\n    z_cap = (elig_space - np.nanmean(elig_space)) / (np.nanstd(elig_space) + 1e-9)\n    \n    # Primary score: dynamic fit/capacity weighting\n    primary = tightness * z_fit + (1 - tightness) * z_cap\n    \n    # Entropy-aware balance component\n    balance_term = -abs(leftover - system_avg) / (system_std + 1e-9)\n    balance_weight = 0.5 * system_cv * (1.0 if strong_item else 2.0)\n    balance_contrib = balance_term * balance_weight\n    \n    # Reinforcement for flexible futures\n    rel_size = item / (np.median(bins_remain_cap[eligible]) + 1e-9)\n    rem_rel = bins_remain_cap / (orig_cap + 1e-9)\n    reinforcer = 1.0 + 0.5 * (1 - rel_size) ** 2 * rem_rel * (fragility + 1e-9)\n    \n    # Final priority calculation\n    priority = (primary + balance_contrib) * reinforcer\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 3,
    "tryHS": false,
    "obj": 3.9190267251695206,
    "SLOC": 71.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response4.txt_stdout.txt",
    "code_path": "problem_iter20_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines adaptive normalization, dynamic item-size-aware weights, and reinforcement-based \n    fragility control. Prioritizes tight fits for large items and utilization-aware placement \n    for small items, with proximity to system average to reduce fragmentation.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    if item < 1e-9:\n        return np.where(bins_remain_cap >= 0, bins_remain_cap, -np.inf)\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System metrics\n    orig_cap = np.max(bins_remain_cap)\n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap)\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    # Item classification\n    large_item = item > system_avg * (0.75 * (1 + 0.3 * system_cv))\n    \n    # Metric extraction\n    bin_remain = bins_remain_cap[eligible]\n    leftover = bin_remain - item\n    tightness = item / (bin_remain + 1e-9)\n    used_after = orig_cap - bin_remain + item\n    util_after = used_after / (orig_cap + 1e-9)\n    \n    # Normalization function\n    def minmax_normalize(arr):\n        min_val, max_val = np.min(arr), np.max(arr)\n        if max_val - min_val < 1e-9:\n            return np.full_like(arr, 0.5)\n        return (arr - min_val) / (max_val - min_val + 1e-9)\n    \n    # Normalized metrics\n    tight_norm = minmax_normalize(tightness)\n    proximity_norm = minmax_normalize(-np.abs(leftover - system_avg))  # Higher when closer to avg\n    util_norm = minmax_normalize(util_after)  # Higher utilization penalized less for small items\n    \n    # Dynamic weighting\n    if large_item:\n        w_tight, w_prox, w_util = 0.6, 0.3, 0.1\n    else:\n        w_tight, w_prox, w_util = 0.3, 0.4, 0.3\n    \n    # Base score calculation\n    base_score = w_tight * tight_norm + w_prox * proximity_norm + w_util * util_norm\n    \n    # Reinforcement fragility control\n    usability = np.clip((system_avg - leftover) / (system_std + 1e-9), 0, None)\n    fragility_factor = usability * proximity_norm\n    reinforcer = 1.0 + (0.2 * fragility_factor)  # 20% max boost for usable leftovers\n    \n    # Final score\n    score = base_score * reinforcer\n    \n    # Build result\n    result = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    result[eligible] = score\n    return result",
    "response_id": 4,
    "tryHS": false,
    "obj": 12.554846429996022,
    "SLOC": 71.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response5.txt_stdout.txt",
    "code_path": "problem_iter20_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines adaptive normalization with variance impact modeling. Uses item-size-aware dynamic weights and reinforcement-inspired usability boosting. Balances tight packing, variance reduction, and system stability through hybrid scoring.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System metrics\n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap)\n    system_cv = system_std / (system_avg + 1e-9)\n    n = bins_remain_cap.size\n    \n    # Item classification\n    large_item_threshold = system_avg * (0.75 * (1 + 0.3 * system_cv))\n    large_item = item > large_item_threshold\n    \n    # Metrics for eligible bins\n    bin_remain = bins_remain_cap[eligible]\n    tightness = item / (bin_remain + 1e-9)\n    leftover = bin_remain - item\n    proximity = np.abs(leftover - system_avg)\n    usability = (system_avg - leftover) / (system_std + 1e-9)\n    \n    # Variance impact calculation\n    sum_remain = bins_remain_cap.sum()\n    sum_remain_sq = (bins_remain_cap ** 2).sum()\n    sum_remain_new = sum_remain - item\n    sum_remain_sq_new = sum_remain_sq - bins_remain_cap**2 + (bins_remain_cap - item)**2\n    var_new = (sum_remain_sq_new / n) - (sum_remain_new / n)**2\n    \n    # Normalization of metrics\n    var_score = np.zeros_like(var_new)\n    eligible_var = var_new[eligible]\n    var_min, var_max = eligible_var.min(), eligible_var.max()\n    if var_max > var_min:\n        var_score[eligible] = (var_max - var_new[eligible]) / (var_max - var_min + 1e-9)\n    else:\n        var_score[eligible] = 0.5\n    \n    tight_norm = np.zeros_like(tightness)\n    tight_min, tight_max = tightness.min(), tightness.max()\n    if tight_max > tight_min:\n        tight_norm = (tightness - tight_min) / (tight_max - tight_min + 1e-9)\n    else:\n        tight_norm.fill(0.5)\n    \n    proximity_norm = np.zeros_like(proximity)\n    prox_min, prox_max = proximity.min(), proximity.max()\n    if prox_max > prox_min:\n        proximity_norm = 1.0 - (proximity - prox_min) / (prox_max - prox_min + 1e-9)\n    else:\n        proximity_norm.fill(0.5)\n    \n    # Reinforcement-inspired usability factor\n    fragility_factor = np.clip(usability, 0, None)\n    reinforcer = 1.0 + 0.3 * fragility_factor\n    \n    # Dynamic weight allocation\n    if large_item:\n        w_tight, w_var, w_prox = 0.5, 0.4, 0.1\n    else:\n        w_tight, w_var, w_prox = 0.3, 0.3, 0.4\n    \n    # Final priority calculation\n    priority_components = (\n        w_tight * tight_norm +\n        w_var * var_score[eligible] +\n        w_prox * proximity_norm\n    ) * reinforcer\n    \n    result = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    result[eligible] = priority_components\n    \n    return result",
    "response_id": 5,
    "tryHS": false,
    "obj": 117.5109692859992,
    "SLOC": 71.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response6.txt_stdout.txt",
    "code_path": "problem_iter20_code6.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines adaptive metric normalization, item-size-aware weights, variance-based balance,\n    and capacity clustering to optimize bin selection. Dynamically balances tight fit requirements\n    with future flexibility preservation using real-time system characterization.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = bins_remain_cap.max()\n    eps = 1e-9\n    \n    if orig_cap <= eps or item <= eps:\n        return np.where(bins_remain_cap >= item, bins_remain_cap - item + eps, -np.inf)\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Active capacity metrics\n    active_caps = bins_remain_cap[bins_remain_cap > 0]\n    median_cap = np.median(active_caps) if active_caps.size > 0 else orig_cap\n    rel_size = item / (median_cap + eps)\n    small_item = rel_size < 0.65  # Intermediate item sizing\n    \n    # Metric calculations\n    e_rem = bins_remain_cap[eligible]\n    leftover = e_rem - item\n    tightness = item / (e_rem + eps)\n    prev_util = (orig_cap - e_rem) / orig_cap  # Previous occupancy level\n    \n    # Metric normalization (higher = better)\n    norm_t = (tightness - tightness.min()) / (tightness.ptp() + 1e-9) if tightness.ptp() > 1e-9 else np.zeros_like(tightness)\n    norm_u = (prev_util - prev_util.min()) / (prev_util.ptp() + 1e-9) if prev_util.ptp() > 1e-9 else np.zeros_like(prev_util)\n    \n    # Dynamic weighting\n    t_weight, u_weight = (1.0, 1.5) if small_item else (1.8, 0.5)\n    primary_score = t_weight * norm_t + u_weight * norm_u\n    \n    # Variance reduction component\n    system_std = bins_remain_cap.std()\n    system_avg = bins_remain_cap.mean() + eps\n    sys_cv = system_std / system_avg\n    \n    if leftover.size > 1 and sys_cv > 0.2:\n        left_std = leftover.std()\n        left_med = np.median(leftover)\n        # Reward bins closer to median leftover\n        balance = 1.0 / (np.abs(leftover - left_med) / (left_std + eps) + 1.0)\n    else:\n        balance = np.ones_like(leftover)\n    \n    # Capacity clustering reinforcement\n    if active_caps.size > 1:\n        act_iqr = np.percentile(active_caps, 75) - np.percentile(active_caps, 25)\n        act_iqr = max(act_iqr, 1e-9)\n        # Higher similarity for leftover close to active cluster\n        sim_score = 1.0 / (np.abs(leftover - np.median(active_caps)) / act_iqr + 1.0)\n    else:\n        sim_score = np.ones_like(leftover)\n    \n    reinforcer = 0.6 + 0.4 * sim_score  # Reinforcement amplification\n    \n    # Final priority with strategic weighting\n    priority = (primary_score + 0.4 * balance) * reinforcer\n    \n    # Construct scores array\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    scores[eligible] = priority\n    return scores",
    "response_id": 6,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 38, in priority_v2\n    return np.zeros_like(arr)\nAttributeError: `ptp` was removed from the ndarray class in NumPy 2.0. Use np.ptp(arr, ...) instead.\n71\n4\n"
  },
  {
    "stdout_filepath": "problem_iter20_response7.txt_stdout.txt",
    "code_path": "problem_iter20_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines dynamic metric weights, predictive variance control, and entropy-aware reinforcement.\n    Uses adaptive normalization, item-size classification, and variance-reduction balance.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = bins_remain_cap.max()\n    eps = 1e-9\n    \n    # Edge cases: zero-sized item or bins\n    if orig_cap <= eps or item <= eps:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + eps,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Subset eligible bins and calculate basic metrics\n    eligible_rem = bins_remain_cap[eligible]\n    leftover = eligible_rem - item\n    tightness = item / (eligible_rem + eps)\n    utilization = (orig_cap - eligible_rem) / (orig_cap + eps)\n    \n    # Min-Max normalization for metrics (higher value = better)\n    def normalize(arr):\n        a_max = arr.max()\n        a_min = arr.min()\n        if a_max > a_min + eps:\n            return (arr - a_min) / (a_max - a_min + eps)\n        return np.zeros_like(arr)\n    \n    norm_tight = normalize(tightness)\n    norm_util = normalize(utilization)\n    \n    # Predictive variance calculation (from v1)\n    S = bins_remain_cap.sum()\n    S2 = (bins_remain_cap**2).sum()\n    n = len(bins_remain_cap)\n    c_i = bins_remain_cap[eligible]\n    variance_i = ((S2 - 2 * c_i * item + item**2) / n) - ((S - item)**2) / (n**2)\n    # Normalize variance score: higher = better (inverse variance)\n    var_score = -variance_i  # Negative to make higher better\n    norm_var = normalize(var_score)\n    \n    # Dynamic weights based on item size\n    active_caps = bins_remain_cap[bins_remain_cap > 0]\n    median_cap = np.median(active_caps) if active_caps.size else orig_cap\n    rel_size = item / (median_cap + eps)\n    small_item = rel_size < 0.75\n    \n    tight_weight = 1.0 if small_item else 1.8\n    util_weight = 1.5 if small_item else 0.5\n    var_weight = 0.7  # Static weight for predictive variance contribution\n    \n    # Primary score combines normalized metrics\n    primary = tight_weight * norm_tight + util_weight * norm_util + var_weight * norm_var\n    \n    # Balance contribution: leftover variance reduction (from v0)\n    if leftover.size > 1:\n        median_left = np.median(leftover)\n        balance_term = -np.abs(leftover - median_left) / (leftover.std() + eps)\n        system_avg = bins_remain_cap.mean()\n        system_std = bins_remain_cap.std()\n        system_cv = system_std / (system_avg + eps) if system_avg > eps else 0.0\n        balance_contrib = balance_term * 0.3 * system_cv\n    else:\n        balance_contrib = np.zeros_like(leftover)\n    \n    # Reinforcement: capacity clustering similarity (from v0)\n    if active_caps.size > 1:\n        active_median = np.median(active_caps)\n        active_iqr = np.percentile(active_caps, 75) - np.percentile(active_caps, 25)\n        similarity = 1.0 / (np.abs(leftover - active_median) / (active_iqr + eps) + 1.0)\n    else:\n        similarity = np.ones_like(leftover)\n    reinforcer = 0.5 + 0.5 * similarity  # Scale to 0.5-1.0\n    \n    # Final priority calculation\n    priority = (primary + balance_contrib) * reinforcer\n    \n    # Fill scores array\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    scores[eligible] = priority\n    return scores",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 71.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response8.txt_stdout.txt",
    "code_path": "problem_iter20_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines dynamic tightness-proximity weighting with adaptive normalization. \n    Large items favor tight fits, small items balance proximity to system average, \n    and tie-breaker minimizes leftover variance, optimizing entropy reduction.\n    \"\"\"\n    \n    def minmax_normalize(arr):\n        min_val = np.min(arr)\n        max_val = np.max(arr)\n        if max_val - min_val < 1e-9:\n            return np.zeros_like(arr, dtype=np.float64)\n        return (arr - min_val) / (max_val - min_val + 1e-9)\n    \n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    # Edge case: negligible item\n    if item < 1e-9:\n        return np.where(bins_remain_cap >= 0, bins_remain_cap, -np.inf)\n    \n    # Identify eligible bins\n    eligible = bins_remain_cap >= (item - 1e-9)\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System-wide statistics\n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap)\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 0 else 0.0\n    \n    # Item classification based on adaptive threshold\n    large_item_threshold = system_avg * 0.7 * (1 + 0.3 * system_cv)\n    is_large_item = item > large_item_threshold\n    \n    # Extract eligible bin data\n    el_remain = bins_remain_cap[eligible]\n    leftover = el_remain - item\n    \n    # Metric calculations\n    tightness = item / (el_remain + 1e-9)  # Higher for tighter fits\n    proximity = np.abs(leftover - system_avg)  # Closer to system average\n    \n    # Metric normalization\n    tight_norm = minmax_normalize(tightness)\n    proximity_norm = minmax_normalize(-proximity)  # Higher when proximity is lower\n    \n    # Dynamic weight allocation\n    w_tight = 0.55 if is_large_item else 0.4\n    w_prox = 0.45 if is_large_item else 0.6\n    \n    # Base score with bias toward classification\n    base_score = w_tight * tight_norm + w_prox * proximity_norm\n    \n    # Tie-breaker for fine-grained entropy reduction\n    tie_breaker = -1e-9 * (leftover - 0.5 * system_avg)  # Favor moderate leftovers\n    \n    # Final priority with layered tie-breaker\n    final_score = base_score + tie_breaker\n    \n    # Build result array\n    result = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    result[eligible] = final_score\n    return result",
    "response_id": 8,
    "tryHS": false,
    "obj": 120.28320702034306,
    "SLOC": 71.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response9.txt_stdout.txt",
    "code_path": "problem_iter20_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive normalization with dynamic weights based on item size to balance\n    tight packing, fragmentation, and system variance. Preserves entropy-driven balance.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    # Handle negligible items\n    if item < eps:\n        return np.where(\n            bins_remain_cap >= 0,\n            0.3 / (bins_remain_cap + 1e-4) - 0.1 * bins_remain_cap,\n            -np.inf\n        )\n    \n    origin_cap = np.max(bins_remain_cap)\n    eligible = bins_remain_cap >= item - 1e-9\n\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n\n    # Core metrics\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + eps)\n    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)\n\n    # Adaptive normalization function\n    def minmax_normalize_full(metric_full, eligible_mask):\n        metric = metric_full.copy()\n        eligible_metric = metric[eligible_mask]\n        if eligible_metric.size == 0:\n            return np.full_like(metric, -np.inf)\n        same_values = np.allclose(eligible_metric, eligible_metric[0])\n        if same_values:\n            normalized = np.full_like(metric, 0.5)\n            normalized[~eligible_mask] = -np.inf\n            return normalized\n        min_val = eligible_metric.min()\n        max_val = eligible_metric.max()\n        range_val = max_val - min_val\n        if range_val < eps:\n            normalized = np.full_like(metric, 0.5)\n            normalized[eligible_mask] = 0.5\n        else:\n            eligible_normalized = (eligible_metric - min_val) / (range_val + eps)\n            eligible_normalized = np.clip(eligible_normalized, 0, 1)\n            normalized = np.full_like(metric, -np.inf)\n            normalized[eligible_mask] = eligible_normalized\n        return normalized\n\n    # Calculate normalized metrics\n    norm_tight = minmax_normalize_full(tightness, eligible)\n    norm_util = minmax_normalize_full(utilization, eligible)\n    norm_leftover = minmax_normalize_full(leftover, eligible)\n\n    # Predictive variance modeling\n    N = bins_remain_cap.size\n    sum_cap = np.sum(bins_remain_cap)\n    sum_sq = np.sum(bins_remain_cap**2)\n\n    elig_remain = bins_remain_cap[eligible]\n    delta_sq_i = (elig_remain - item)**2 - elig_remain**2\n    new_sum_sq_i = sum_sq + delta_sq_i\n    new_mean_i = (sum_cap - item) / N\n    var_new_i = new_sum_sq_i / N - new_mean_i**2\n\n    var_old = sum_sq / N - (sum_cap / N)**2\n    var_delta_i = var_old - var_new_i\n\n    var_delta_full = np.full_like(bins_remain_cap, -np.inf)\n    var_delta_full[eligible] = var_delta_i\n    norm_var = minmax_normalize_full(var_delta_full, eligible)\n\n    # Dynamic weight adjustment based on item size\n    relative_size = item / origin_cap\n\n    # Weights for larger items: prioritize tightness and utilization\n    tight_weight = 1.2 + 0.8 * relative_size\n    util_weight = 0.8 * relative_size\n    var_weight = 0.5 * (1 - relative_size)\n    frag_weight = 0.4 * (1 - relative_size)\n\n    # Component assembly with proper weighting\n    tight_component = tight_weight * norm_tight\n    util_component = util_weight * norm_util  # encourages filling existing bins\n    frag_component = frag_weight * norm_leftover  # subtract as larger leftover is worse\n    var_component = var_weight * norm_var  # higher var_delta means variance is reduced\n\n    # Tie-breaker based on remaining capacity\n    tie_breaker = 0.01 * (bins_remain_cap / (origin_cap + eps))\n\n    # Combine all elements into priority scores\n    scores = np.where(\n        eligible,\n        tight_component + util_component + var_component - frag_component + tie_breaker,\n        -np.inf\n    )\n\n    return scores",
    "response_id": 9,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 76, in priority_v2\n    norm_var = minmax_normalize_full(var_delta_full, eligible)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 40, in minmax_normalize_full\n    normalized[~eligible_mask] = -np.inf\nOverflowError: cannot convert float infinity to integer\n71\n4\n"
  }
]