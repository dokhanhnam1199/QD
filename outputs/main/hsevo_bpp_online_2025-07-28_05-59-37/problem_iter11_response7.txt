```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combined heuristic: Z-normalized synergy + entropy-tiered penalty + adaptive 
    item-class control + cross-metric variance reinforcement for resilient 
    online bin packing decisions with predictive entropy modeling
    """
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    # Near-zero item fast path
    if item < 1e-9:
        return np.where(bins_remain_cap >= 0, 
                      0.5 / (bins_remain_cap + 1e-4) - 1e-4 * bins_remain_cap,
                      -np.inf)
    
    eligible = bins_remain_cap >= item - 1e-9  # Floating point tolerance
    
    if not np.any(eligible):
        return np.full_like(bins_remain_cap, -np.inf)
    
    # Core system statistics
    sys_avg, sys_std = np.mean(bins_remain_cap), np.std(bins_remain_cap)
    sys_cv = sys_std / (sys_avg + 1e-9) if sys_avg > 1e-9 else 0.0
    
    # Dynamic item classification enhanced by variance tradeoff
    large_item = item > sys_avg * 0.85 * (1 + 0.2 * sys_cv)
    
    # Metric definitions with gradient sensitivity
    residual = bins_remain_cap - item
    tightness = item / (bins_remain_cap + 1e-9)
    fit_power = 1.0 / (residual + 1e-6)
    frag_indicator = (residual - sys_avg) / (sys_std + 1e-9)
    
    # Gradient-boosted Z-note normalization
    def calc_zscore(metric):
        return (metric - metric[eligible].mean()) / (metric[eligible].std() + 1e-9)
    
    z_fit = calc_zscore(fit_power)
    z_tight = calc_zscore(tightness)
    z_balance = calc_zscore(-np.abs(residual - sys_avg))
    
    # Enhanced variance sensitivity calculation
    def calc_adaptive_weight(metric):
        with np.errstate(divide='ignore', invalid='ignore'):
            mags = metric[eligible]
            var_factor = np.std(mags) / (abs(np.mean(mags)) + 1e-9)
            depth_factor = 1 + 0.5 * (1 - sys_cv)
            return var_factor * depth_factor
    
    # Metric-specific weight generation
    fit_weight = calc_adaptive_weight(z_fit) * (1.2 if large_item else 1.0)
    tight_weight = calc_adaptive_weight(1 - tightness) * (0.8 if large_item else 1.4)
    balance_weight = calc_adaptive_weight(-np.abs(residual - sys_avg)) * (0.5 + 1.0 * sys_cv)
    
    # Entropy-aware synergy amplification
    synergy_factor = 1.0 + np.tanh(
        0.3 * (z_fit * fit_weight - z_tight * tight_weight)
    ) * (1 - 0.3 * sys_std / (sys_avg + 1e-9))
    
    # Tiered penalty with predictive entropy modeling
    penalty_weight = np.select(
        [
            frag_indicator > sys_cv + 1.2,
            frag_indicator < -sys_cv - 0.5,
            frag_indicator < 0.8
        ],
        [
            0.5 * sys_std**0.8 * (1 + sys_cv**0.5),
            0.3 * sys_std**0.5 / (sys_avg + 1e-9),
            0.2 * sys_cv**0.7
        ],
        default=0.1 * (1 - sys_cv)
    )
    
    # Priority synthesis with gradient control
    base_score = (
        z_fit * fit_weight * synergy_factor +
        (1 - z_tight * tight_weight * (0.7 if large_item else 1.2)) +
        z_balance * balance_weight
    )
    
    penalty_score = np.where(
        eligible,
        residual * penalty_weight * (1.2 if item < sys_avg else 1.0),
        0
    )
    
    # Final score quantization with fragility suppression
    final_score = np.where(eligible, base_score - penalty_score, -np.inf)
    
    return final_score
```
