[
  {
    "stdout_filepath": "problem_iter17_response0.txt_stdout.txt",
    "code_path": "problem_iter17_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines predictive variance modeling, min-max normalization, and reinforcement-driven prioritization.\n    Balances fit quality, system variance reduction, and utilization with static hybrid weights.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    origin_cap = np.max(bins_remain_cap)\n    # Handle edge cases with zero-sized items or bins\n    if origin_cap <= eps or item <= eps:\n        return np.where(bins_remain_cap >= item, bins_remain_cap - item + eps, -np.inf)\n    \n    eligible = bins_remain_cap >= item - eps  # Allow floating-point tolerance\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Core metric calculations\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + eps)\n    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)\n    fit_quality = 1.0 / (leftover + eps)\n    \n    # Predictive variance modeling: calculate variance delta for each eligible bin\n    n = bins_remain_cap.size\n    sum_cap = bins_remain_cap.sum()\n    sum_sq = (bins_remain_cap ** 2).sum()\n    mean_old = sum_cap / n\n    var_old = sum_sq / n - mean_old**2\n    \n    c_elig, lo_elig = bins_remain_cap[eligible], leftover[eligible]\n    delta_sq_i = c_elig**2 - lo_elig**2\n    new_sum_sq_i = sum_sq - delta_sq_i\n    new_mean_i = (sum_cap - item) / n\n    var_new_i = (new_sum_sq_i / n) - (new_mean_i ** 2)\n    delta_var_i = var_old - var_new_i  # Higher delta means better variance reduction\n    \n    # Min-max normalization for metrics across eligible bins\n    def min_max_normalize(values, eligible_mask, default=0.5):\n        vals = values[eligible_mask]\n        min_val, max_val = vals.min(), vals.max()\n        if (max_val - min_val) > eps:\n            return (values - min_val) / (max_val - min_val + eps)\n        return np.full_like(values, default, dtype=np.float64)\n    \n    norm_fit = min_max_normalize(fit_quality, eligible)\n    norm_util = min_max_normalize(utilization, eligible)\n    norm_tight = min_max_normalize(tightness, eligible)\n    \n    # Normalize variance delta\n    norm_var = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    dv_vals = delta_var_i\n    dv_min, dv_max = dv_vals.min(), dv_vals.max()\n    if (dv_max - dv_min) > eps:\n        norm_var_elig = (dv_vals - dv_min) / (dv_max - dv_min + eps)\n    else:\n        norm_var_elig = 0.5\n    norm_var[eligible] = norm_var_elig\n    \n    # Primary score combines normalized metrics with equal weights\n    primary_score = (norm_fit + norm_util + norm_var) / 3.0\n    \n    # Reinforcement factor: rewards usable remaining space and filled bins\n    rel_size = item / origin_cap\n    rem_rel = bins_remain_cap / origin_cap\n    reinforcer = (1.0 - rel_size) * rem_rel * (1.0 + utilization)  # Utilization boosts filled bins\n    \n    # Tie-breaker to prefer larger leftover space\n    tie_breaker = 1e-6 * leftover\n    \n    # Final priority calculation\n    priority = primary_score * reinforcer + tie_breaker\n    scores = np.where(eligible, priority, -np.inf)\n    \n    return np.nan_to_num(scores, neginf=-np.inf, posinf=0, nan=0)",
    "response_id": 0,
    "tryHS": false,
    "obj": 11.24850418827283,
    "SLOC": 40.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response1.txt_stdout.txt",
    "code_path": "problem_iter17_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines min-max-normalized fit, variance-driven balance, and reinforcement learning.\n    Uses predictive variance modeling and entropy-agnostic weights to balance tight fits\n    with system-wide flexibility.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + 1e-9,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System metrics\n    system_mean = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_var = system_std ** 2\n    \n    # Variance ratio for adaptive control\n    var_rel = system_var / (orig_cap ** 2 + 1e-9)\n    var_rel = np.clip(var_rel, 0.0, 1.0)\n    \n    # Eligible bin metrics\n    eligible_remain = bins_remain_cap[eligible]\n    leftover = eligible_remain - item\n    tightness = item / (eligible_remain + 1e-9)\n    \n    # Adaptive weights\n    fit_weight = 1.0 - var_rel\n    balance_weight = np.sqrt(var_rel)\n    \n    # Balance component: minimize deviation from mean\n    balance_component = -np.abs(leftover - system_mean) / (orig_cap + 1e-9)\n    \n    # Flexibility component: promote median-close leftover\n    med_remain = np.median(eligible_remain)\n    flex_component = -((leftover - med_remain) ** 2) / (orig_cap ** 2 + 1e-9)\n    \n    # Item size adaptation\n    item_rel_size = item / (system_mean + 1e-9)\n    fit_decay = np.exp(-item_rel_size * tightness)\n    \n    # Hybrid score assembly\n    hybrid = (\n        tightness * fit_weight +\n        balance_component * balance_weight +\n        flex_component * fit_decay\n    )\n    \n    # Reinforcement-style future-promise multiplier\n    rel_size = item / (np.median(eligible_remain) + 1e-9)\n    rem_rel = bins_remain_cap / (orig_cap + 1e-9)\n    fragility = (orig_cap - bins_remain_cap) / (orig_cap + 1e-9)\n    reinforcer = 1.0 + 0.5 * ((1.0 - rel_size) ** 2) * rem_rel[eligible] * fragility[eligible]\n    \n    # Final priority with reinforcement scaling\n    final_score = hybrid * reinforcer\n    \n    # Build return array with -inf for ineligible bins\n    result = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    result[eligible] = final_score\n    return result",
    "response_id": 1,
    "tryHS": false,
    "obj": 75.63821300358995,
    "SLOC": 40.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response2.txt_stdout.txt",
    "code_path": "problem_iter17_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Priority function combining min-max normalization, adaptive weights based on item size,\n    predictive variance modeling via reinforcement term, and system balance factor.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    # Edge case: zero-sized item or bins\n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + 1e-9,\n            -np.inf\n        )\n    \n    # Identify eligible bins that can fit the item\n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    elig_remaining = bins_remain_cap[eligible]\n    remaining_after = elig_remaining - item\n    \n    # System-wide metrics\n    system_median = np.median(bins_remain_cap)\n    system_iqr = np.percentile(bins_remain_cap, 75) - np.percentile(bins_remain_cap, 25)\n    \n    # Metric calculations for eligible bins\n    tightness = item / elig_remaining  # Tightness of fit\n    utilization = (orig_cap - elig_remaining) / orig_cap  # Current bin utilization\n    \n    # Min-max normalization for each metric\n    # Tightness normalization\n    t_min, t_max = tightness.min(), tightness.max()\n    tightness_norm = (tightness - t_min) / (t_max - t_min + 1e-9) if t_max > t_min else np.full_like(tightness, 0.5)\n    \n    # Remaining space normalization (higher = less space left)\n    ra_min, ra_max = remaining_after.min(), remaining_after.max()\n    space_norm = (ra_max - remaining_after) / (ra_max - ra_min + 1e-9) if ra_max > ra_min else np.full_like(remaining_after, 0.5)\n    \n    # Utilization normalization (higher = more filled)\n    u_min, u_max = utilization.min(), utilization.max()\n    util_norm = (utilization - u_min) / (u_max - u_min + 1e-9) if u_max > u_min else np.full_like(utilization, 0.5)\n    \n    # Adaptive weight calculation based on item size relative to system median\n    item_rel_size = item / (system_median + 1e-9)\n    tight_weight = 0.4 + 0.5 * item_rel_size  # Prioritize tight fit for large items\n    space_weight = 0.4 + 0.5 * (1 - item_rel_size)  # Prioritize space preservation for small items\n    util_weight = 0.2  # Fixed weight for utilization\n    \n    # Hybrid score combining normalized metrics\n    hybrid_score = (\n        (tightness_norm + 1e-9) ** tight_weight *\n        (space_norm + 1e-9) ** space_weight *\n        (util_norm + 1e-9) ** util_weight\n    )\n    \n    # Reinforcement term: proximity to median remaining capacity after placement\n    median_after = np.median(remaining_after)\n    mad_after = np.median(np.abs(remaining_after - median_after))\n    reinforce_score = np.exp(-np.abs(remaining_after - median_after) / (mad_after + 1e-9))\n    \n    # Final score with reinforcement and balance factor\n    final_score = hybrid_score * (0.5 + 0.5 * reinforce_score)\n    final_score *= 1.0 / (1.0 + system_iqr)  # System balance adjustment\n    \n    # Build final priority array\n    priority = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    priority[eligible] = final_score\n    \n    return priority",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 40.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response3.txt_stdout.txt",
    "code_path": "problem_iter17_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Predictive variance-aware binning with min-max synergy and reinforcement-inspired balance.\n    Combines normalized fit metrics with variance delta prediction and static fragmentation weights.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Metric definitions\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + eps)\n    fit_quality = 1.0 / (leftover + eps)\n    \n    # Min-max normalization helper\n    def normalize(arr):\n        min_val = arr.min()\n        max_val = arr.max()\n        denominator = (max_val - min_val) if (max_val > min_val) else 1.0\n        return (arr - min_val) / (denominator + eps)\n    \n    # Normalized metrics (higher = better)\n    fit_norm = normalize(fit_quality)\n    tight_norm = normalize(tightness)\n    \n    # Fragmentation penalty (static weight, higher penalty for smaller leftovers)\n    leftover_minmax = (leftover - leftover.min()) / (leftover.ptp() + eps)\n    frag_penalty = 1.0 - leftover_minmax  # Higher penalty for smaller leftovers\n    \n    # Predictive variance modeling: calculate variance delta for each bin\n    n = bins_remain_cap.size\n    current_sum = bins_remain_cap.sum()\n    current_sum_sq = (bins_remain_cap**2).sum()\n    \n    # Vectorized variance delta calculation\n    old_val = bins_remain_cap\n    new_val = old_val - item\n    new_sum = current_sum - item\n    new_sum_sq = current_sum_sq - old_val**2 + new_val**2\n    \n    new_mean = new_sum / n\n    new_var = (new_sum_sq / n) - new_mean**2\n    current_var = (current_sum_sq / n) - (current_sum / n)**2\n    \n    delta_var = new_var - current_var  # Negative delta improves balance\n    predictive_balance = -delta_var    # Higher = better balance impact\n    \n    # Core synergy with balance reinforcement\n    synergy = fit_norm + tight_norm + 0.5 * predictive_balance\n    \n    # Final priority with static fragmentation control\n    frag_weight = 0.3\n    priority = synergy - frag_weight * frag_penalty\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 3,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 33, in priority_v2\n    max_left = leftover.max()\nAttributeError: `ptp` was removed from the ndarray class in NumPy 2.0. Use np.ptp(arr, ...) instead.\n40\n7\n"
  },
  {
    "stdout_filepath": "problem_iter17_response4.txt_stdout.txt",
    "code_path": "problem_iter17_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines min-max normalization, variance-aware balance, and predictive reinforcement.\n    Prioritizes bins based on adaptive metric weights, system variance reduction, and \n    future capacity clustering.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = bins_remain_cap.max()\n    eps = 1e-9\n    \n    # Handle edge cases with zero-sized item or bins\n    if orig_cap <= eps or item <= eps:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + eps,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    eligible_rem = bins_remain_cap[eligible]\n    leftover = eligible_rem - item\n    tightness = item / (eligible_rem + eps)\n    utilization = (orig_cap - eligible_rem) / (orig_cap + eps)\n    \n    # Min-Max normalization for metrics (higher value = better)\n    max_left = leftover.max()\n    min_left = leftover.min()\n    if max_left > min_left + eps:\n        norm_left = (max_left - leftover) / (max_left - min_left + eps)\n    else:\n        norm_left = np.zeros_like(leftover)\n    \n    max_tight = tightness.max()\n    min_tight = tightness.min()\n    if max_tight > min_tight + eps:\n        norm_tight = (tightness - min_tight) / (max_tight - min_tight + eps)\n    else:\n        norm_tight = np.zeros_like(tightness)\n    \n    max_util = utilization.max()\n    min_util = utilization.min()\n    if max_util > min_util + eps:\n        norm_util = (utilization - min_util) / (max_util - min_util + eps)\n    else:\n        norm_util = np.zeros_like(utilization)\n    \n    # Item classification\n    active_caps = bins_remain_cap[bins_remain_cap > 0]\n    median_cap = np.median(active_caps) if active_caps.size else orig_cap\n    rel_size = item / (median_cap + eps)\n    small_item = rel_size < 0.75\n    \n    # Dynamic weights\n    tight_weight = 1.0 if small_item else 1.8\n    util_weight = 1.5 if small_item else 0.5\n    \n    # Primary score\n    primary = tight_weight * norm_tight + util_weight * norm_util\n    \n    # Balance contribution (variance reduction)\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + eps) if system_avg > eps else 0.0\n    \n    if leftover.size > 1:\n        median_leftover = np.median(leftover)\n        balance_term = -np.abs(leftover - median_leftover) / (leftover.std() + eps)\n        balance_contrib = balance_term * 0.3 * system_cv\n    else:\n        balance_contrib = np.zeros_like(leftover)\n    \n    # Reinforcement: capacity clustering\n    if active_caps.size > 1:\n        active_median = np.median(active_caps)\n        active_iqr = np.percentile(active_caps, 75) - np.percentile(active_caps, 25)\n        similarity = 1.0 / (np.abs(leftover - active_median) / (active_iqr + eps) + 1.0)\n    else:\n        similarity = np.ones_like(leftover)\n    \n    reinforcer = 0.5 + 0.5 * similarity  # Scale between 0.5-1.0\n    \n    # Final priority\n    priority = (primary + balance_contrib) * reinforcer\n    \n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    scores[eligible] = priority\n    return scores",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 40.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response5.txt_stdout.txt",
    "code_path": "problem_iter17_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines min-max synergy metrics with predictive variance modeling.\n    Uses adaptive fragmentation control and entropy-agnostic balance.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    # Handle negligible items\n    if item < eps:\n        return np.where(\n            bins_remain_cap >= 0,\n            0.3 / (bins_remain_cap + 1e-4) - 0.1 * bins_remain_cap,\n            -np.inf\n        )\n    \n    origin_cap = np.max(bins_remain_cap)\n    eligible = bins_remain_cap >= item - 1e-9\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # Core metrics calculation\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + eps)\n    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)\n    \n    # Min-Max normalization function\n    def minmax_normalize_full(metric_full, eligible_mask):\n        metric = metric_full.copy()\n        eligible_metric = metric[eligible_mask]\n        if eligible_metric.size == 0:\n            return np.full_like(metric, -np.inf)\n        same_values = np.allclose(eligible_metric, eligible_metric[0])\n        if same_values:\n            normalized = np.full_like(metric, 0.5)\n            normalized[~eligible_mask] = -np.inf\n            return normalized\n        min_val = eligible_metric.min()\n        max_val = eligible_metric.max()\n        range_val = max_val - min_val\n        eligible_normalized = (eligible_metric - min_val) / (range_val + eps)\n        eligible_normalized = np.clip(eligible_normalized, 0, 1)\n        normalized = np.full_like(metric, -np.inf)\n        normalized[eligible_mask] = eligible_normalized\n        return normalized\n    \n    # Normalize metrics\n    norm_tight = minmax_normalize_full(tightness, eligible)\n    norm_util = minmax_normalize_full(utilization, eligible)\n    \n    # Primary synergy score\n    synergy = norm_tight * (1.0 + norm_util)\n    \n    # Fragmentation penalty (entropy-agnostic)\n    frag_penalty = 1.0 - np.exp(-leftover / (origin_cap + eps))\n    frag_component = 0.2 * frag_penalty\n    \n    # Predictive variance modeling\n    N = bins_remain_cap.size\n    sum_cap = np.sum(bins_remain_cap)\n    sum_sq = np.sum(bins_remain_cap ** 2)\n    mean_old = sum_cap / N\n    var_old = (sum_sq / N) - (mean_old ** 2)\n    \n    elig_remain = bins_remain_cap[eligible]\n    delta_sq_i = (elig_remain - item) ** 2 - elig_remain ** 2\n    new_sum_sq_i = sum_sq + delta_sq_i\n    new_mean_i = (sum_cap - item) / N\n    var_new_i = (new_sum_sq_i / N) - (new_mean_i ** 2)\n    var_delta_i = var_old - var_new_i\n    \n    # Normalize variance delta\n    var_delta_full = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    var_delta_full[eligible] = var_delta_i\n    norm_var = minmax_normalize_full(var_delta_full, eligible)\n    \n    # Final components\n    var_component = norm_var\n    tie_breaker = 0.01 * (bins_remain_cap / (origin_cap + eps))\n    \n    # Final score assembly\n    scores = np.where(\n        eligible,\n        synergy - frag_component + 0.5 * var_component + tie_breaker,\n        -np.inf\n    )\n    \n    return scores",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 40.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response6.txt_stdout.txt",
    "code_path": "problem_iter17_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid priority combining min-max synergy, predictive variance analysis,\n    and stability-aware reinforcement for online BPP. Uses item classification\n    and fragmentation-anticipation penalties.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    if item <= 1e-9:\n        return np.where(bins_remain_cap >= 0, 0.01 * bins_remain_cap, -np.inf)\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    C_est = bins_remain_cap.max() if bins_remain_cap.max() > 0 else item * 2\n    \n    # Metric calculations\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    fit_score = 1.0 / (leftover + 1e-9)\n    \n    # Min-Max normalization\n    tight_min, tight_max = tightness[eligible].min(), tightness[eligible].max()\n    fit_min, fit_max = fit_score[eligible].min(), fit_score[eligible].max()\n    norm_tight = (tightness - tight_min) / (tight_max - tight_min + 1e-9)\n    norm_fit = (fit_score - fit_min) / (fit_max - fit_min + 1e-9)\n    \n    # Item classification and adaptive weights\n    is_large = item > 0.7 * C_est\n    fit_weight = 0.8 if is_large else 0.5\n    tight_weight = 0.2 if is_large else 0.5\n    base_score = fit_weight * norm_fit + tight_weight * norm_tight\n    \n    # Predictive variance modeling\n    n = len(bins_remain_cap)\n    current_sum = bins_remain_cap.sum()\n    current_sum_sq = (bins_remain_cap ** 2).sum()\n    current_var = (current_sum_sq / n) - (current_sum / n) ** 2\n    \n    new_cap_elig = bins_remain_cap[eligible] - item\n    delta_sq_elig = new_cap_elig**2 - bins_remain_cap[eligible]**2\n    new_sum_sq_elig = current_sum_sq + delta_sq_elig\n    new_mean_elig = (current_sum - item) / n\n    new_var_elig = (new_sum_sq_elig / n) - new_mean_elig**2\n    delta_var_elig = new_var_elig - current_var\n    \n    variance_term = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    variance_term[eligible] = -delta_var_elig / (np.abs(current_var) + 1e-9)\n    \n    # Fragmentation anticipation\n    frag_term = np.zeros_like(bins_remain_cap)\n    frag_term[eligible] = np.exp(-leftover[eligible] / (C_est / 3 + 1e-9))\n    \n    # Stability preservation\n    median_cap = np.median(bins_remain_cap[eligible]) if eligible.any() else C_est\n    proximity = np.abs(bins_remain_cap - median_cap) / (C_est + 1e-9)\n    \n    # Priority assembly\n    priority = (\n        base_score\n        + 0.2 * variance_term\n        - 0.1 * frag_term\n        - 0.05 * proximity\n    )\n    \n    # Reinforcement decay\n    priority *= np.exp(-0.05 * leftover / (C_est + 1e-9))\n    \n    # Deterministic tie-breaking\n    priority += 1e-7 * (1.0 / (tightness + 1e-9)) * (1.0 / (proximity + 1e-9))\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 6,
    "tryHS": false,
    "obj": 72.78619864379738,
    "SLOC": 40.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response7.txt_stdout.txt",
    "code_path": "problem_iter17_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines min-max normalized fit/util metrics with adaptive weights,\n    predictive leftover clustering, and fragmentation-agnostic balance.\n    Uses item-relative weighting and variance-driven penalty scaling.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eps = 1e-9\n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + eps)\n    origin_cap = np.max(bins_remain_cap)\n    \n    # Item classification\n    relative_size = item / (system_avg + eps)\n    is_large = relative_size > 1.0\n    \n    # Per-bin metrics\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + eps)\n    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)\n    \n    # Adaptive weight calculation using sigmoid\n    fit_weight = 1.0 / (1.0 + np.exp(-2.0 * (relative_size - 0.5)))\n    fit_weight = np.clip(fit_weight, 0.3, 0.7)\n    util_weight = 1.0 - fit_weight\n    \n    # Min-Max normalization for metrics within eligible bins\n    tight_masked = tightness[eligible]\n    util_masked = utilization[eligible]\n    left_masked = leftover[eligible]\n    \n    t_min, t_max = tight_masked.min(), tight_masked.max()\n    u_min, u_max = util_masked.min(), util_masked.max()\n    l_min, l_max = left_masked.min(), left_masked.max()\n    \n    tight_norm = (tightness - t_min) / (t_max - t_min + eps)\n    util_norm = (utilization - u_min) / (u_max - u_min + eps)\n    left_norm = (leftover - l_min) / (l_max - l_min + eps)\n    \n    # Hybrid score with adaptive weights\n    hybrid = fit_weight * tight_norm + util_weight * util_norm\n    \n    # Balance penalty based on system variance\n    balance_penalty = np.abs(leftover - system_avg) / (system_std + eps)\n    balanced_score = hybrid - system_cv * balance_penalty\n    \n    # Fragmentation penalty (static weight, entropy-agnostic)\n    frag_component = np.exp(- (left_norm ** 0.5) * 2)\n    frag_weight = 0.3 * (1.0 + bins_remain_cap.size / (origin_cap + eps))\n    frag_score = balanced_score - frag_weight * frag_component\n    \n    # Predictive leftover clustering (reinforcement-inspired)\n    if left_masked.size > 0:\n        median_left = np.median(left_masked)\n        spread = np.std(left_masked) if left_masked.size > 1 else origin_cap\n    else:\n        median_left = origin_cap / 2\n        spread = origin_cap\n    \n    rem_gap = np.abs(leftover - median_left)\n    reinforcer = np.exp(- (rem_gap / (spread + eps)) ** 2)\n    reinforcer_weight = 0.5 * (1.0 + system_cv * 0.5)\n    reinforced_score = frag_score + reinforcer * reinforcer_weight\n    \n    # Utilization boost (linear instead of sqrt)\n    util_boost = utilization * 0.1 * (1.0 + is_large * 0.5)\n    final_score = reinforced_score + util_boost\n    \n    # Final scores\n    scores = np.where(eligible, final_score, -np.inf)\n    return scores",
    "response_id": 7,
    "tryHS": false,
    "obj": 137.554846429996,
    "SLOC": 40.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response8.txt_stdout.txt",
    "code_path": "problem_iter17_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Predictive variance-aware packing using min-max synergy and entropy-agnostic reinforcement.\n    Combines normalized fit-tightness/utilization with proximity-based penalties and fragility-aware reinforcer.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    orig_cap = bins_remain_cap.max()\n    remaining_cap = bins_remain_cap[eligible]\n    leftover = remaining_cap - item + eps\n    \n    # Metric calculation\n    fit_tightness = item / (remaining_cap + eps)\n    util_after = (orig_cap - remaining_cap + item) / (orig_cap + eps)\n    \n    # System metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std() + eps\n    \n    # Min-max normalization\n    def minmax_normalize(x):\n        xmin, xmax = x.min(), x.max()\n        if xmax > xmin:\n            return (x - xmin) / (xmax - xmin)\n        return np.full_like(x, 0.5)\n    \n    norm_fit = minmax_normalize(fit_tightness)\n    norm_util = minmax_normalize(util_after)\n    \n    # Hybrid score with static weights\n    hybrid = 0.6 * norm_fit + 0.4 * norm_util\n    boosted = hybrid ** 2  # Nonlinear amplification\n    \n    # Proximity-based penalty (distance from average leftover)\n    proximity = np.abs(leftover - system_avg) / system_std\n    proximity_penalty = np.exp(-proximity)\n    \n    # Reinforcement term: fragility * usability\n    fragility = util_after\n    usability = np.clip((system_avg - leftover) / system_std, -1.0, 1.0)\n    reinforcer = fragility * np.abs(usability)\n    \n    # Tie-breaker (epsilon-greedy preference for larger leftover)\n    tie_breaker = 0.01 * (leftover / (orig_cap + eps))\n    \n    # Final score assembly\n    eligible_scores = (\n        boosted * proximity_penalty \n        + reinforcer \n        + tie_breaker\n    )\n    \n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    scores[eligible] = eligible_scores\n    \n    return scores",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 40.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response9.txt_stdout.txt",
    "code_path": "problem_iter17_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines min-max normalization, variance-aware proximity, and dynamic weighting.\n    Uses predictive proximity to reduce fragmentation, reinforcement fragility control,\n    and item-size-adaptive metrics for balanced bin selection.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    # Edge case: negligible item\n    if item < 1e-9:\n        return np.where(bins_remain_cap >= 0, bins_remain_cap, -np.inf)\n    \n    eligible = bins_remain_cap >= (item - 1e-9)\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System metrics\n    orig_cap = np.max(bins_remain_cap)\n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap)\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    # Item classification\n    large_item = item > system_avg * (0.75 * (1 + 0.3 * system_cv))\n    \n    # Eligible bin metrics\n    bin_remain = bins_remain_cap[eligible]\n    leftover = bin_remain - item\n    tightness = item / (bin_remain + 1e-9)\n    util_after = (orig_cap - bin_remain + item) / (orig_cap + 1e-9)\n    proximity = np.abs(leftover - system_avg)\n    \n    # Min-max normalization of metrics\n    def minmax_normalize(arr):\n        min_val = np.min(arr)\n        max_val = np.max(arr)\n        if max_val - min_val < 1e-9:\n            return np.zeros_like(arr) + 0.5\n        return (arr - min_val) / (max_val - min_val + 1e-9)\n    \n    tight_norm = minmax_normalize(tightness)\n    proximity_norm = minmax_normalize(-proximity)  # Higher when closer to avg\n    util_norm = 1.0 - minmax_normalize(util_after)  # Higher when util is lower\n    \n    # Dynamic weight allocation\n    w_tight = 0.6 if large_item else 0.3\n    w_prox = 0.3 if large_item else 0.5\n    w_util = 0.1 if large_item else 0.2\n    \n    # Hybrid score with nonlinear boost\n    hybrid = (w_tight * tight_norm) + (w_prox * proximity_norm) + (w_util * util_norm)\n    boosted = hybrid ** 1.5  # Stronger nonlinear enhancement\n    \n    # Reinforcement-inspired fragility control\n    usability = (system_avg - leftover) / (system_std + 1e-9)\n    fragility_factor = util_after * np.clip(usability, 0, None) * proximity_norm\n    reinforcer = 1.0 + (0.3 * fragility_factor)  # Boosts fragility when usable\n    \n    # Predictive variance adjustment\n    variance_factor = np.exp(-proximity / (system_std + 1e-9))  # Favors proximity\n    \n    # Final score calculation\n    score = boosted * reinforcer * variance_factor\n    \n    # Build result array\n    result = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    result[eligible] = score\n    \n    return result",
    "response_id": 9,
    "tryHS": false,
    "obj": 133.3366573593937,
    "SLOC": 40.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter18_response0.txt_stdout.txt",
    "code_path": "problem_iter18_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins using predictive variance modeling, min-max normalization, \n    and reinforcement via utilization-aware balancing. Avoids Z-scores, entropy penalties, \n    and exponential boosting while emphasizing long-term flexibility and robustness.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + 1e-9,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Vectorized predictive variance calculation\n    r_i = bins_remain_cap[eligible]\n    l = r_i - item\n    n = len(bins_remain_cap)\n    sum_r_initial = np.sum(bins_remain_cap)\n    sum_sq_initial = np.sum(bins_remain_cap ** 2)\n    \n    sum_r_new = sum_r_initial - item\n    sum_sq_new = sum_sq_initial - r_i**2 + l**2\n    new_mean = sum_r_new / n\n    new_var = (sum_sq_new / n) - (new_mean ** 2)\n    \n    # Min-max normalization components\n    tightness = item / (r_i + 1e-9)\n    tight_norm = (tightness - tightness.min()) / (tightness.ptp() + 1e-9)\n    \n    var_contrib = 1.0 - (new_var - new_var.min()) / (new_var.ptp() + 1e-9)\n    \n    utilization = (orig_cap - r_i) / (orig_cap + 1e-9)\n    util_norm = (utilization - utilization.min()) / (utilization.ptp() + 1e-9)\n    \n    # Hybrid static-dynamic weighting\n    item_ratio = item / (orig_cap + 1e-9)\n    tight_weight = 0.4 + 0.4 * item_ratio\n    var_weight = 0.4 - 0.2 * item_ratio\n    util_weight = 0.2 + 0.2 * (1 - item_ratio)\n    \n    # Final priority calculation\n    total = (\n        tight_norm * tight_weight +\n        var_contrib * var_weight +\n        util_norm * util_weight\n    )\n    \n    priority = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    priority[eligible] = total\n    \n    return priority",
    "response_id": 0,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 38, in priority_v2\n    leftover = bins_remain_cap - delta\nAttributeError: `ptp` was removed from the ndarray class in NumPy 2.0. Use np.ptp(arr, ...) instead.\n31\n6\n"
  },
  {
    "stdout_filepath": "problem_iter18_response1.txt_stdout.txt",
    "code_path": "problem_iter18_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + 1e-9,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    n = bins_remain_cap.shape[0]\n    delta = item\n    \n    # Predictive variance modeling\n    S = bins_remain_cap.sum()\n    Q = (bins_remain_cap ** 2).sum()\n    mean_new = (S - delta) / n\n    sum_sq_new = Q - 2 * delta * bins_remain_cap + delta ** 2\n    var_new = (sum_sq_new / n) - (mean_new ** 2)\n    \n    # Variance score normalization (lower variance is better)\n    eligible_var = var_new[eligible]\n    var_min, var_max = eligible_var.min(), eligible_var.max()\n    if var_max > var_min:\n        norm_var = (var_max - var_new) / (var_max - var_min + 1e-9)\n    else:\n        norm_var = np.zeros_like(var_new)\n    \n    # Leftover space normalization\n    leftover = bins_remain_cap - delta\n    eligible_left = leftover[eligible]\n    left_min, left_max = eligible_left.min(), eligible_left.max()\n    if left_max > left_min:\n        norm_left = (leftover - left_min) / (left_max - left_min + 1e-9)\n    else:\n        norm_left = np.zeros_like(leftover)\n    \n    # Tightness normalization\n    tightness = delta / (bins_remain_cap + 1e-9)\n    eligible_tight = tightness[eligible]\n    tight_min, tight_max = eligible_tight.min(), eligible_tight.max()\n    if tight_max > tight_min:\n        norm_tight = (tightness - tight_min) / (tight_max - tight_min + 1e-9)\n    else:\n        norm_tight = np.zeros_like(tightness)\n    \n    # Item classification\n    system_avg = bins_remain_cap.mean()\n    large_item = delta > system_avg\n    \n    # Static-dynamic weighting\n    tight_weight = 0.7 if large_item else 0.3\n    var_weight = 0.2 if large_item else 0.5\n    left_weight = 0.1 if large_item else 0.2\n    \n    # Priority components\n    priority = (\n        norm_tight * tight_weight +\n        norm_var * var_weight +\n        norm_left * left_weight\n    )\n    \n    # Reinforcement adjustment for future flexibility\n    used_cap = orig_cap - bins_remain_cap\n    utilization = used_cap / (orig_cap + 1e-9)\n    if not large_item:\n        priority += 0.1 * utilization  # Prefer filling partially used bins\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 1,
    "tryHS": false,
    "obj": 149.30195452732352,
    "SLOC": 31.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter18_response2.txt_stdout.txt",
    "code_path": "problem_iter18_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + 1e-9,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # 1. Fit score: Min-max normalized tightness (higher = tighter fit)\n    leftover = np.where(eligible, bins_remain_cap - item, np.inf)\n    min_left, max_left = np.min(leftover), np.max(leftover)\n    fit_score = (max_left - leftover) / (max_left - min_left + 1e-9)\n    \n    # 2. Predictive variance score: Variance after placement (lower is better)\n    S, S2, n = bins_remain_cap.sum(), (bins_remain_cap**2).sum(), len(bins_remain_cap)\n    c_i = bins_remain_cap\n    variance_i = (S2 - 2*c_i*item + item**2)/n - ((S - item)**2)/(n**2)\n    variance_i = np.where(eligible, variance_i, np.inf)\n    \n    min_var, max_var = np.min(variance_i), np.max(variance_i)\n    var_score = (max_var - variance_i) / (max_var - min_var + 1e-9)\n    \n    # 3. Reinforcement score: Remaining capacity flexibility\n    rem_after = bins_remain_cap - item\n    rem_ratio = rem_after / (orig_cap + 1e-9)\n    min_rem, max_rem = np.min(rem_ratio[eligible]), np.max(rem_ratio[eligible])\n    reinforce_score = (rem_ratio - min_rem) / (max_rem - min_rem + 1e-9)\n    \n    # 4. Hybrid priority calculation (entropy-agnostic balance)\n    tightness_weight = 0.6  # Static weight for immediate fit\n    future_weight = 0.4      # Static weight for future flexibility\n    \n    tightness_component = fit_score * var_score\n    future_component = reinforce_score * (1.0 + var_score)  # Amplify by variance quality\n    \n    priority = tightness_weight * tightness_component + future_weight * future_component\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 2,
    "tryHS": false,
    "obj": 149.30195452732352,
    "SLOC": 31.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter18_response3.txt_stdout.txt",
    "code_path": "problem_iter18_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Core metrics\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    # Predictive variance modeling\n    n = bins_remain_cap.size\n    sum_remain = bins_remain_cap.sum()\n    sum_remain_sq = (bins_remain_cap ** 2).sum()\n    \n    sum_remain_new = sum_remain - item\n    sum_remain_sq_new = sum_remain_sq - bins_remain_cap**2 + (bins_remain_cap - item)**2\n    \n    var_new = (sum_remain_sq_new / n) - (sum_remain_new / n)**2\n    \n    # Min-max normalization for variance score\n    var_score = np.zeros_like(var_new)\n    eligible_var = var_new[eligible]\n    var_min, var_max = eligible_var.min(), eligible_var.max()\n    if var_max > var_min:\n        var_score = (var_max - var_new) / (var_max - var_min + 1e-9)\n    \n    # Min-max normalization for tightness score\n    t_score = np.zeros_like(tightness)\n    eligible_tight = tightness[eligible]\n    t_min, t_max = eligible_tight.min(), eligible_tight.max()\n    if t_max > t_min:\n        t_score = (tightness - t_min) / (t_max - t_min + 1e-9)\n    \n    # Reinforcement similarity score\n    median_remain = np.median(bins_remain_cap)\n    diff = np.abs(leftover - median_remain)\n    sim_score = np.zeros_like(diff)\n    eligible_diff = diff[eligible]\n    d_min, d_max = eligible_diff.min(), eligible_diff.max()\n    if d_max > d_min:\n        sim_score = 1.0 - (diff - d_min) / (d_max - d_min + 1e-9)\n    \n    # Static-dynamic hybrid balance weights\n    tightness_weight = 0.5\n    variance_weight = 0.3\n    similarity_weight = 0.2\n    \n    # Final priority calculation\n    priority = (\n        tightness_weight * t_score + \n        variance_weight * var_score + \n        similarity_weight * sim_score\n    )\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 3,
    "tryHS": false,
    "obj": 119.14639010769845,
    "SLOC": 31.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter18_response4.txt_stdout.txt",
    "code_path": "problem_iter18_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + 1e-9,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    el_remain = bins_remain_cap[eligible]\n    el_leftover = el_remain - item\n    \n    median_L = np.median(el_leftover)\n    range_L = el_leftover.max() - el_leftover.min()\n    if range_L > 0:\n        var_pen = np.abs(el_leftover - median_L) / range_L\n    else:\n        var_pen = np.zeros_like(el_leftover, dtype=np.float64)\n    \n    median_remain = np.median(el_remain)\n    weight_t = item / (median_remain + 1e-9)\n    weight_v = 1.0 - weight_t\n    \n    tightness = item / (el_remain + 1e-9)\n    base_priority = weight_t * tightness - weight_v * var_pen\n    \n    tie_breaker = -1e-9 * el_leftover\n    final_priority = base_priority + tie_breaker\n    \n    priority = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    priority[eligible] = final_priority\n    \n    return priority",
    "response_id": 4,
    "tryHS": false,
    "obj": 68.9469485440766,
    "SLOC": 31.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  }
]