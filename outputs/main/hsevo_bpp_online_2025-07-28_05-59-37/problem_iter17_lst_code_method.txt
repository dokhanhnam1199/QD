{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines adaptive Z-score normalization, entropy-driven balance, and reinforcement learning concepts.\n    Prioritizes bins that optimize fit tightness, system-wide capacity balance, and future flexibility.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + 1e-9,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System-wide metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 1e-9 else 0.0\n    \n    # Item classification\n    large_item = item > system_avg\n    \n    # Core metrics\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    utilization = (orig_cap - bins_remain_cap) / (orig_cap + 1e-9)\n    \n    # Z-score normalization for fit and space\n    elig_fit = 1.0 / (leftover + 1e-9)\n    elig_fit_mean, elig_fit_std = np.mean(elig_fit[eligible]), np.std(elig_fit[eligible])\n    z_fit = (elig_fit - elig_fit_mean) / (elig_fit_std + 1e-9)\n    \n    elig_space = bins_remain_cap\n    elig_space_mean, elig_space_std = np.mean(elig_space[eligible]), np.std(elig_space[eligible])\n    z_cap = (elig_space - elig_space_mean) / (elig_space_std + 1e-9)\n    \n    # Primary score: adaptive tightness-weighted Z-combination\n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    \n    # Exponential enhancer for utilization-tightness synergy\n    enhancer = np.exp(utilization * tightness)\n    \n    # Entropy-driven balance term\n    balance_term = -np.abs(leftover - system_avg) / (system_std + 1e-9)\n    balance_weight = 0.5 * system_cv * np.where(large_item, 1.0, 2.0)\n    balance_contrib = balance_term * balance_weight\n    \n    # Reinforcement multiplier\n    eligible_rem = bins_remain_cap[eligible]\n    rel_size = item / (np.median(eligible_rem) + 1e-9)\n    fragility = ((orig_cap - bins_remain_cap) / (orig_cap + 1e-9)).clip(0, 1)\n    rem_rel = bins_remain_cap / (orig_cap + 1e-9)\n    reinforce_factor = (1 - rel_size) ** 2 * rem_rel * fragility\n    reinforcer = 1 + 0.5 * reinforce_factor\n    \n    # Final priority calculation\n    priority = (primary_score * enhancer + balance_contrib) * reinforcer\n    \n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 2nd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines adaptive Z-score normalization, entropy-driven balance, and reinforcement learning concepts.\n    Prioritizes bins that optimize fit tightness, system-wide capacity balance, and future flexibility.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + 1e-9,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System-wide metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 1e-9 else 0.0\n    \n    # Item classification\n    large_item = item > system_avg\n    \n    # Core metrics\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    utilization = (orig_cap - bins_remain_cap) / (orig_cap + 1e-9)\n    \n    # Z-score normalization for fit and space\n    elig_fit = 1.0 / (leftover + 1e-9)\n    elig_fit_mean, elig_fit_std = np.mean(elig_fit[eligible]), np.std(elig_fit[eligible])\n    z_fit = (elig_fit - elig_fit_mean) / (elig_fit_std + 1e-9)\n    \n    elig_space = bins_remain_cap\n    elig_space_mean, elig_space_std = np.mean(elig_space[eligible]), np.std(elig_space[eligible])\n    z_cap = (elig_space - elig_space_mean) / (elig_space_std + 1e-9)\n    \n    # Primary score: adaptive tightness-weighted Z-combination\n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    \n    # Exponential enhancer for utilization-tightness synergy\n    enhancer = np.exp(utilization * tightness)\n    \n    # Entropy-driven balance term\n    balance_term = -np.abs(leftover - system_avg) / (system_std + 1e-9)\n    balance_weight = 0.5 * system_cv * np.where(large_item, 1.0, 2.0)\n    balance_contrib = balance_term * balance_weight\n    \n    # Reinforcement multiplier\n    eligible_rem = bins_remain_cap[eligible]\n    rel_size = item / (np.median(eligible_rem) + 1e-9)\n    fragility = ((orig_cap - bins_remain_cap) / (orig_cap + 1e-9)).clip(0, 1)\n    rem_rel = bins_remain_cap / (orig_cap + 1e-9)\n    reinforce_factor = (1 - rel_size) ** 2 * rem_rel * fragility\n    reinforcer = 1 + 0.5 * reinforce_factor\n    \n    # Final priority calculation\n    priority = (primary_score * enhancer + balance_contrib) * reinforcer\n    \n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 3rd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines adaptive Z-score normalization, entropy-driven balance, and reinforcement learning concepts.\n    Prioritizes bins that optimize fit tightness, system-wide capacity balance, and future flexibility.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + 1e-9,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System-wide metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 1e-9 else 0.0\n    \n    # Item classification\n    large_item = item > system_avg\n    \n    # Core metrics\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    utilization = (orig_cap - bins_remain_cap) / (orig_cap + 1e-9)\n    \n    # Z-score normalization for fit and space\n    elig_fit = 1.0 / (leftover + 1e-9)\n    elig_fit_mean, elig_fit_std = np.mean(elig_fit[eligible]), np.std(elig_fit[eligible])\n    z_fit = (elig_fit - elig_fit_mean) / (elig_fit_std + 1e-9)\n    \n    elig_space = bins_remain_cap\n    elig_space_mean, elig_space_std = np.mean(elig_space[eligible]), np.std(elig_space[eligible])\n    z_cap = (elig_space - elig_space_mean) / (elig_space_std + 1e-9)\n    \n    # Primary score: adaptive tightness-weighted Z-combination\n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    \n    # Exponential enhancer for utilization-tightness synergy\n    enhancer = np.exp(utilization * tightness)\n    \n    # Entropy-driven balance term\n    balance_term = -np.abs(leftover - system_avg) / (system_std + 1e-9)\n    balance_weight = 0.5 * system_cv * np.where(large_item, 1.0, 2.0)\n    balance_contrib = balance_term * balance_weight\n    \n    # Reinforcement multiplier\n    eligible_rem = bins_remain_cap[eligible]\n    rel_size = item / (np.median(eligible_rem) + 1e-9)\n    fragility = ((orig_cap - bins_remain_cap) / (orig_cap + 1e-9)).clip(0, 1)\n    rem_rel = bins_remain_cap / (orig_cap + 1e-9)\n    reinforce_factor = (1 - rel_size) ** 2 * rem_rel * fragility\n    reinforcer = 1 + 0.5 * reinforce_factor\n    \n    # Final priority calculation\n    priority = (primary_score * enhancer + balance_contrib) * reinforcer\n    \n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Z-normalized synergy metrics with predictive entropy weighting.\n    Uses fit/utilization synergy, variance delta analysis, and adaptive fragmentation control.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    origin_cap = np.max(bins_remain_cap)\n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Core metrics\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + eps)\n    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)\n    fit_quality = 1.0 / (leftover + eps)\n    \n    # Z-score normalization across eligible bins\n    fit_mean = fit_quality[eligible].mean()\n    fit_std = fit_quality[eligible].std()\n    z_fit = (fit_quality - fit_mean) / (fit_std + eps)\n    \n    tight_mean = tightness[eligible].mean()\n    tight_std = tightness[eligible].std()\n    z_tight = (tightness - tight_mean) / (tight_std + eps)\n    \n    util_mean = utilization[eligible].mean()\n    util_std = utilization[eligible].std()\n    z_util = (utilization - util_mean) / (util_std + eps)\n    \n    # Primary synergy and enhancer\n    synergy = z_tight * (1 + z_util)\n    enhancer = np.exp(utilization * tightness)\n    primary_score = synergy * enhancer\n    \n    # Fragmentation penalty with entropy scaling\n    sys_entropy = bins_remain_cap.std() / (origin_cap + eps)\n    frag_penalty = 1.0 - np.exp(-leftover / (origin_cap + eps))\n    frag_component = 0.2 * (1 + sys_entropy) * frag_penalty\n    \n    # Predictive entropy delta calculation\n    sum_cap = bins_remain_cap.sum()\n    sum_sq = (bins_remain_cap ** 2).sum()\n    N = bins_remain_cap.size\n    mean_old = sum_cap / N\n    var_old = (sum_sq / N) - mean_old**2\n    \n    elig_remain = bins_remain_cap[eligible]\n    delta_sq_i = (elig_remain - item)**2 - elig_remain**2\n    new_sum_sq_i = sum_sq + delta_sq_i\n    new_mean_i = (sum_cap - item) / N\n    var_new_i = (new_sum_sq_i / N) - (new_mean_i**2)\n    var_delta_i = var_old - var_new_i\n    \n    # Normalize entropy delta across eligible bins\n    vd_mean = var_delta_i.mean()\n    vd_std = var_delta_i.std()\n    norm_entropy_delta = (var_delta_i - vd_mean) / (vd_std + eps)\n    \n    # Adaptive entropy weighting\n    system_cv = bins_remain_cap.std() / (bins_remain_cap.mean() + eps)\n    large_item = item > mean_old\n    weight_entropy = 0.5 * (1 + system_cv) * (1.5 if large_item else 0.5)\n    \n    entropy_component = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    entropy_component[eligible] = (weight_entropy * norm_entropy_delta).astype(np.float64)\n    \n    # Tie-breaker with sensitivity damping\n    tie_breaker = 0.01 * np.exp(-leftover) * (1 + sys_entropy)\n    \n    # Final score assembly\n    scores = np.where(\n        eligible,\n        primary_score - frag_component + entropy_component + tie_breaker,\n        -np.inf\n    )\n    \n    return scores\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + 1e-9,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System metrics\n    system_median = np.median(bins_remain_cap)\n    system_iqr = np.percentile(bins_remain_cap, 75) - np.percentile(bins_remain_cap, 25)\n    \n    # Entropy approximation via discretization\n    bin_edges = np.arange(0, orig_cap + 1e-9, 0.1 * orig_cap)\n    hist, _ = np.histogram(bins_remain_cap, bins=bin_edges)\n    probs = hist / len(bins_remain_cap)\n    entropy = -np.sum(p * np.log(p + 1e-9) for p in probs if p > 0)\n    \n    # Eligible bin metrics\n    elig_remaining = bins_remain_cap[eligible]\n    elig_after = elig_remaining - item\n    tightness = item / elig_remaining\n    \n    # Metric normalization\n    tightness_min, tightness_max = tightness.min(), tightness.max()\n    tightness_norm = (tightness - tightness_min) / (tightness_max - tightness_min + 1e-9)\n    \n    rem_min, rem_max = elig_after.min(), elig_after.max()\n    space_norm = 1.0 - ((elig_after - rem_min) / (rem_max - rem_min + 1e-9)) if rem_max > rem_min else np.zeros_like(elig_after)\n    \n    utilization = (orig_cap - elig_remaining) / orig_cap\n    util_min, util_max = utilization.min(), utilization.max()\n    util_norm = (utilization - util_min) / (util_max - util_min + 1e-9) if util_max > util_min else np.zeros_like(utilization)\n    \n    # Adaptive item classification\n    item_rel_size = item / (system_median + 1e-9)\n    entropy_weight = np.clip(entropy / (np.log(len(bin_edges)) + 1e-9), 0, 1)\n    \n    # Hybrid metric weights based on item size and entropy\n    tight_weight = 0.4 + 0.5 * entropy_weight * item_rel_size\n    space_weight = 0.4 + 0.5 * entropy_weight * (1 - item_rel_size)\n    util_weight = 0.2 * (1 - entropy_weight)\n    \n    # Nonlinear hybrid score\n    hybrid_score = (\n        (tightness_norm + 1e-9) ** tight_weight *\n        (space_norm + 1e-9) ** space_weight *\n        (util_norm + 1e-9) ** util_weight\n    )\n    \n    # Reinforcement term: proximity to median remaining after\n    rem_after_median = np.median(elig_after)\n    rem_after_mad = np.median(np.abs(elig_after - rem_after_median))\n    reinforce_score = np.exp(-np.abs(elig_after - rem_after_median) / (rem_after_mad + 1e-9))\n    \n    # Final priority calculation\n    final_score = hybrid_score * (0.5 + 0.5 * entropy_weight * reinforce_score)\n    \n    # Apply system-wide balance factor\n    balance_factor = 1.0 / (1.0 + system_iqr)\n    final_score *= balance_factor\n    \n    # Construct output\n    priority = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    priority[eligible] = final_score\n    \n    return priority\n\n[Heuristics 6th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combined adaptive Z-synergy with entropy-weighted fragmentation and balance control.\n    Uses metric normalization, system variance-adaptive weights, and item-classification dynamics.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Metrics and system descriptors\n    leftover = bins_remain_cap - item\n    origin_cap = np.max(bins_remain_cap)\n    tightness = item / (bins_remain_cap + eps)\n    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)\n    fit_quality = 1.0 / (leftover + eps)\n    \n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + eps)\n    large_item = item > (system_avg + eps)\n    \n    # Z-score normalization across eligible bins\n    eligible_metrics = {\n        'fit': fit_quality[eligible],\n        'tight': tightness[eligible],\n        'cap': bins_remain_cap[eligible]\n    }\n    mean = {k: np.mean(v) for k, v in eligible_metrics.items()}\n    std = {k: np.std(v) for k, v in eligible_metrics.items()}\n    \n    # Z-score computation for key metrics\n    z_fit = (fit_quality - mean['fit']) / (std['fit'] + eps)\n    z_tight = (tightness - mean['tight']) / (std['tight'] + eps)\n    z_cap = (bins_remain_cap - mean['cap']) / (std['cap'] + eps)\n    \n    # Primary synergy term with exponential enhancer\n    enhancer = np.exp(utilization * tightness)\n    primary_score = (z_fit + z_tight + 0.5 * z_cap) * enhancer\n    \n    # Entropy-weighted penalties for fragmentation and balance\n    frag_penalty = 1.0 - np.exp(-leftover / (origin_cap + eps))\n    frag_weight = 0.3 * system_cv\n    \n    balance_term = -np.abs(leftover - system_avg)\n    balance_weight = 0.2 * system_cv * (2.0 if not large_item else 1.0)\n    \n    # Hybrid scoring based on multi-metric analysis\n    priority = primary_score - frag_weight * frag_penalty + balance_weight * balance_term\n    \n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Z-normalized synergy metrics with predictive entropy weighting.\n    Uses fit/utilization synergy, variance delta analysis, and adaptive fragmentation control.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    origin_cap = np.max(bins_remain_cap)\n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Core metrics\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + eps)\n    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)\n    fit_quality = 1.0 / (leftover + eps)\n    \n    # Z-score normalization across eligible bins\n    fit_mean = fit_quality[eligible].mean()\n    fit_std = fit_quality[eligible].std()\n    z_fit = (fit_quality - fit_mean) / (fit_std + eps)\n    \n    tight_mean = tightness[eligible].mean()\n    tight_std = tightness[eligible].std()\n    z_tight = (tightness - tight_mean) / (tight_std + eps)\n    \n    util_mean = utilization[eligible].mean()\n    util_std = utilization[eligible].std()\n    z_util = (utilization - util_mean) / (util_std + eps)\n    \n    # Primary synergy and enhancer\n    synergy = z_tight * (1 + z_util)\n    enhancer = np.exp(utilization * tightness)\n    primary_score = synergy * enhancer\n    \n    # Fragmentation penalty with entropy scaling\n    sys_entropy = bins_remain_cap.std() / (origin_cap + eps)\n    frag_penalty = 1.0 - np.exp(-leftover / (origin_cap + eps))\n    frag_component = 0.2 * (1 + sys_entropy) * frag_penalty\n    \n    # Predictive entropy delta calculation\n    sum_cap = bins_remain_cap.sum()\n    sum_sq = (bins_remain_cap ** 2).sum()\n    N = bins_remain_cap.size\n    mean_old = sum_cap / N\n    var_old = (sum_sq / N) - mean_old**2\n    \n    elig_remain = bins_remain_cap[eligible]\n    delta_sq_i = (elig_remain - item)**2 - elig_remain**2\n    new_sum_sq_i = sum_sq + delta_sq_i\n    new_mean_i = (sum_cap - item) / N\n    var_new_i = (new_sum_sq_i / N) - (new_mean_i**2)\n    var_delta_i = var_old - var_new_i\n    \n    # Normalize entropy delta across eligible bins\n    vd_mean = var_delta_i.mean()\n    vd_std = var_delta_i.std()\n    norm_entropy_delta = (var_delta_i - vd_mean) / (vd_std + eps)\n    \n    # Adaptive entropy weighting\n    system_cv = bins_remain_cap.std() / (bins_remain_cap.mean() + eps)\n    large_item = item > mean_old\n    weight_entropy = 0.5 * (1 + system_cv) * (1.5 if large_item else 0.5)\n    \n    entropy_component = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    entropy_component[eligible] = (weight_entropy * norm_entropy_delta).astype(np.float64)\n    \n    # Tie-breaker with sensitivity damping\n    tie_breaker = 0.01 * np.exp(-leftover) * (1 + sys_entropy)\n    \n    # Final score assembly\n    scores = np.where(\n        eligible,\n        primary_score - frag_component + entropy_component + tie_breaker,\n        -np.inf\n    )\n    \n    return scores\n\n[Heuristics 8th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Online BPP priority that blends Z-score weighted fit (BF) and tightness (FF) via adaptive variance ratios,\n    with item classification (large/tiny) driving climb envelope modulation and exponentially decayed \n    fit-scoring to balance BF/FF tradeoffs. Stability-enhanced tie-breaking via median proximity sensitivity.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    if item <= 1e-9:\n        return np.where(bins_remain_cap >= 0, 0.01 * bins_remain_cap, -np.inf)\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # Capacity metrics\n    C_est = bins_remain_cap.max() if bins_remain_cap.max() > 0 else item * 2\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    fit_score = 1.0 / (leftover + 1e-9)\n    \n    # Z-score normalization\n    def z_score(x):\n        mean = x[eligible].mean()\n        std = x[eligible].std()\n        return (x - mean) / (std + 1e-9)\n    \n    tight_z = z_score(tightness)\n    fit_z = z_score(fit_score)\n    \n    # Adaptive weighting by variance\n    fit_var, tight_var = fit_z.var(), tight_z.var()\n    var_ratio = fit_var / (fit_var + tight_var + 1e-9)\n    base_score = (var_ratio * fit_z) + ((1 - var_ratio) * tight_z)\n    \n    # Item classification\n    is_large = item > 0.7 * C_est\n    grad_factor = 1.35 if is_large else 1.0\n    utilization = 1.0 - (bins_remain_cap / C_est)\n    \n    # Energy climbing envelope\n    climb_env = grad_factor * np.exp(0.3 * tightness + 0.2 * utilization)\n    \n    # Fragmentation control\n    frag_penalty = 0.2 * np.exp(-leftover / (C_est / 3 + 1e-9))\n    \n    # Stability preservation\n    median_cap = np.median(bins_remain_cap[eligible]) if eligible.any() else bins_remain_cap.median()\n    proximity = np.abs(bins_remain_cap - median_cap) / (C_est + 1e-9)    \n    stability = 0.02 * bins_remain_cap.std() * np.abs(tight_z) * proximity\n    \n    # Priority construction\n    priority = (base_score * climb_env) - frag_penalty + stability\n    \n    # BF bias with soft exponential decay\n    priority *= np.exp(-0.05 * leftover / (C_est + 1e-9))\n    \n    # Reinforcement-style tiebreaker\n    priority += 1e-7 * np.random.normal(0, 1, priority.shape)\n    \n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(bins_remain_cap >= item, bins_remain_cap - item + 1e-9, -np.inf)\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    # Item classification\n    relative_size = item / (system_avg + 1e-9)\n    large_item = relative_size > 1.0\n    \n    # Per-bin metrics\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    utilization = (orig_cap - bins_remain_cap) / (orig_cap + 1e-9)\n    \n    # Adaptive weight calculation using smooth transition\n    fit_weight = 1.0 / (1.0 + np.exp(-2.0 * (relative_size - 0.5)))\n    fit_weight = np.clip(fit_weight, 0.3, 0.8)\n    \n    # Normalized fit and utilization components\n    fit_component = -leftover\n    fit_mean = fit_component[eligible].mean()\n    fit_std = fit_component[eligible].std()\n    z_fit = (fit_component - fit_mean) / (fit_std + 1e-9)\n    \n    util_component = utilization\n    util_mean = util_component[eligible].mean()\n    util_std = util_component[eligible].std()\n    z_util = (util_component - util_mean) / (util_std + 1e-9)\n    \n    # Hybrid score with adaptive weights\n    hybrid = fit_weight * z_fit + (1.0 - fit_weight) * z_util\n    \n    # Entropy-driven balance adjustment\n    balance_factor = system_cv * np.where(large_item, 1.5, 1.0)\n    balance_term = -np.abs(leftover - system_avg) / (system_std + 1e-9)\n    balanced_hybrid = hybrid + balance_factor * balance_term\n    \n    # Reinforcement-inspired term: promote leftover near median\n    median_remain = np.median(bins_remain_cap[eligible])\n    rem_gap = np.abs(leftover - median_remain)\n    reinforcer = np.exp(-rem_gap / (orig_cap + 1e-9))\n    \n    # Adaptive reinforcement weight\n    reinforcer_weight = 0.5 * (1.0 - system_cv + relative_size)\n    final_score = balanced_hybrid + reinforcer * reinforcer_weight\n    \n    # Utilization boost for high-util bins\n    util_boost = np.sqrt(utilization.clip(0, 1)) * 0.2 * system_cv\n    final_score += util_boost\n    \n    return np.where(eligible, final_score, -np.inf)\n\n[Heuristics 10th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive synergy of Z-normalized metrics with entropy-weighted fragmentation control and predictive balance.\n    Combines tight-fit synergy, exponential enhancer, and system-aware tie-breaking.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Core metrics\n    C = bins_remain_cap.max()\n    remaining_cap = bins_remain_cap[eligible]\n    leftover = remaining_cap - item + eps\n    \n    fit_quality = 1.0 / leftover\n    tightness = item / (remaining_cap + eps)\n    utilization = (C - remaining_cap) / C\n    \n    # Z-score normalization\n    def z_score(x):\n        return (x - x.mean()) / (x.std() + eps)\n    \n    z_fit = z_score(fit_quality)\n    z_tight = z_score(tightness)\n    z_util = z_score(utilization)\n    \n    # Adaptive synergy: variance-weighted tight-fit + utilization coupling\n    var_fit = max(z_fit.var(), 0.1)\n    var_tight = max(z_tight.var(), 0.1)\n    weight_tight = var_fit / (var_fit + var_tight)  # Inverse variance weighting\n    adaptive_synergy = weight_tight * z_tight + (1 - weight_tight) * z_fit\n    synergy = adaptive_synergy * (1 + z_util)  # Utilization-coupled reinforcement\n    \n    # Exponential enhancer with predictive utilization\n    enhancer = np.exp(utilization * tightness)\n    primary_score = synergy * enhancer\n    \n    # Entropy-sensitive fragmentation control\n    sys_entropy = bins_remain_cap.std() / (C + eps)\n    frag_penalty = 1.0 - np.exp(-leftover / (C + eps))\n    frag_weight = 0.2 * (1.0 + sys_entropy)\n    \n    # System balance term (leftover proximity to average)\n    system_avg = bins_remain_cap.mean()\n    balance = -np.abs(leftover - system_avg) * (1.0 + sys_entropy)\n    balance_weight = 0.1 * (1.0 + sys_entropy)\n    \n    # Deterministic tie-breaker (entropy-coupled exponential decay)\n    tie_breaker = 0.05 * np.exp(-leftover) * (1.0 + sys_entropy)\n    \n    # Final score with multi-layered components\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    eligible_scores = (\n        primary_score \n        - frag_weight * frag_penalty \n        + balance * balance_weight \n        + tie_breaker\n    )\n    scores[eligible] = eligible_scores\n    \n    return scores\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Z-score synergy with item-size adaptation and entropy-controlled frag mitigator.\n    \n    Combines normalized tightness/util metrics weighted by item size relative to system\n    state, entropy-adjusted penalty scaling, and residual clustering continuity.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eps = 1e-9\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if not mask.any():\n        return scores\n\n    # Base parameters\n    origin_cap = bins_remain_cap.max()\n    rem_cap = bins_remain_cap[mask]\n    leftover = rem_cap - item\n\n    # Tightness & utilization metrics\n    inv_waste = 1.0 / (leftover + eps)\n    exp_tight = np.exp(-2.0 * leftover / (origin_cap + eps))\n    tight_fit = inv_waste + 2.0 * exp_tight\n    bin_util = (origin_cap - rem_cap) / (origin_cap + eps)\n\n    # Z-score normalization\n    t_mean, t_std = tight_fit.mean(), tight_fit.std(ddof=1)\n    u_mean, u_std = bin_util.mean(), bin_util.std(ddof=1)\n    z_tight = (tight_fit - t_mean) / (t_std + eps)\n    z_util = (bin_util - u_mean) / (u_std + eps)\n\n    # Entropy adaptation\n    active_caps = bins_remain_cap[bins_remain_cap > 0]\n    if active_caps.size > 1:\n        log_caps_std = np.log(active_caps + eps).std()\n        entropic_scale = log_caps_std\n    else:\n        entropic_scale = 0.0\n\n    # Item size context\n    median_cap = np.median(active_caps) if active_caps.size else origin_cap\n    is_small_item = item / (median_cap + eps) < 0.75\n\n    # Dynamic weights\n    entropy_amp = (1.0 + entropic_scale) ** 1.5\n    if is_small_item:\n        tight_weight = 1.8 + 0.5 * entropy_amp\n        util_weight = 0.3\n    else:\n        tight_weight = 1.0 + 0.2 * entropy_amp\n        util_weight = 1.2 + entropy_amp\n\n    # Core synergy\n    synergy = tight_weight * z_tight + util_weight * z_util\n\n    # Fragmentation control\n    leftover_norm = leftover / (origin_cap + eps)\n    base_frag = -np.expm1(-leftover_norm ** 1.25)\n    median_leftover = np.median(leftover) if leftover.size else origin_cap / 2\n    cap_highway = np.exp(-3.0 * ((leftover - median_leftover)**2) / ((origin_cap * 0.4)**2 + eps))\n    frag_penalty = base_frag - cap_highway\n    frag_weight = 0.7 * entropy_amp * (1.0 + bins_remain_cap.size / (origin_cap + eps))\n    scores_masked = synergy - frag_weight * frag_penalty\n\n    # Continuity reinforcement\n    if active_caps.size > 2:\n        leftover_std = active_caps.std() if active_caps.size > 1 else origin_cap\n        dist_to_mean = np.abs(leftover - rem_cap.mean()) / (leftover_std + eps)\n        continuity_bonus = np.exp(-0.5 * dist_to_mean)\n        scores_masked += 0.2 * continuity_bonus\n\n    scores[mask] = scores_masked\n    return scores\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Z-score synergy with item-size adaptation and entropy-controlled frag mitigator.\n    \n    Combines normalized tightness/util metrics weighted by item size relative to system\n    state, entropy-adjusted penalty scaling, and residual clustering continuity.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eps = 1e-9\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if not mask.any():\n        return scores\n\n    # Base parameters\n    origin_cap = bins_remain_cap.max()\n    rem_cap = bins_remain_cap[mask]\n    leftover = rem_cap - item\n\n    # Tightness & utilization metrics\n    inv_waste = 1.0 / (leftover + eps)\n    exp_tight = np.exp(-2.0 * leftover / (origin_cap + eps))\n    tight_fit = inv_waste + 2.0 * exp_tight\n    bin_util = (origin_cap - rem_cap) / (origin_cap + eps)\n\n    # Z-score normalization\n    t_mean, t_std = tight_fit.mean(), tight_fit.std(ddof=1)\n    u_mean, u_std = bin_util.mean(), bin_util.std(ddof=1)\n    z_tight = (tight_fit - t_mean) / (t_std + eps)\n    z_util = (bin_util - u_mean) / (u_std + eps)\n\n    # Entropy adaptation\n    active_caps = bins_remain_cap[bins_remain_cap > 0]\n    if active_caps.size > 1:\n        log_caps_std = np.log(active_caps + eps).std()\n        entropic_scale = log_caps_std\n    else:\n        entropic_scale = 0.0\n\n    # Item size context\n    median_cap = np.median(active_caps) if active_caps.size else origin_cap\n    is_small_item = item / (median_cap + eps) < 0.75\n\n    # Dynamic weights\n    entropy_amp = (1.0 + entropic_scale) ** 1.5\n    if is_small_item:\n        tight_weight = 1.8 + 0.5 * entropy_amp\n        util_weight = 0.3\n    else:\n        tight_weight = 1.0 + 0.2 * entropy_amp\n        util_weight = 1.2 + entropy_amp\n\n    # Core synergy\n    synergy = tight_weight * z_tight + util_weight * z_util\n\n    # Fragmentation control\n    leftover_norm = leftover / (origin_cap + eps)\n    base_frag = -np.expm1(-leftover_norm ** 1.25)\n    median_leftover = np.median(leftover) if leftover.size else origin_cap / 2\n    cap_highway = np.exp(-3.0 * ((leftover - median_leftover)**2) / ((origin_cap * 0.4)**2 + eps))\n    frag_penalty = base_frag - cap_highway\n    frag_weight = 0.7 * entropy_amp * (1.0 + bins_remain_cap.size / (origin_cap + eps))\n    scores_masked = synergy - frag_weight * frag_penalty\n\n    # Continuity reinforcement\n    if active_caps.size > 2:\n        leftover_std = active_caps.std() if active_caps.size > 1 else origin_cap\n        dist_to_mean = np.abs(leftover - rem_cap.mean()) / (leftover_std + eps)\n        continuity_bonus = np.exp(-0.5 * dist_to_mean)\n        scores_masked += 0.2 * continuity_bonus\n\n    scores[mask] = scores_masked\n    return scores\n\n[Heuristics 13th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, \n                eps= 2.0891687420063354e-08, \n                z_cap_weight= 0.7968322271186733, \n                frag_weight_factor= 0.06743678360822358, \n                balance_weight_factor= 0.1422737624920359, \n                balance_multiplier_normal= 1.670035684998664, \n                balance_multiplier_large= 1.89673002375526) -> np.ndarray:\n    \"\"\"\n    Combined adaptive Z-synergy with entropy-weighted fragmentation and balance control.\n    Uses metric normalization, system variance-adaptive weights, and item-classification dynamics.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n\n[Heuristics 14th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, \n                eps= 2.0891687420063354e-08, \n                z_cap_weight= 0.7968322271186733, \n                frag_weight_factor= 0.06743678360822358, \n                balance_weight_factor= 0.1422737624920359, \n                balance_multiplier_normal= 1.670035684998664, \n                balance_multiplier_large= 1.89673002375526) -> np.ndarray:\n    \"\"\"\n    Combined adaptive Z-synergy with entropy-weighted fragmentation and balance control.\n    Uses metric normalization, system variance-adaptive weights, and item-classification dynamics.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid priority using Z-standardized fit metrics, entropy-driven variance forecasting,\n    and exponential gain based on utilization-tightness synergy for optimal bin selection.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9) - 1e-9 * bins_remain_cap,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Metric calculations\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)  # Tightness of fit\n    utilization = (orig_cap - bins_remain_cap) / orig_cap  # Bin fullness metric\n\n    # Z-normalization of fit quality and capacities\n    fit_quality = 1.0 / (leftover[eligible] + 1e-9)\n    z_fit = (1.0 / (leftover + 1e-9) - np.mean(fit_quality)) / (np.std(fit_quality) + 1e-9)\n    \n    elig_capacities = bins_remain_cap[eligible]\n    z_cap = (bins_remain_cap - np.mean(elig_capacities)) / (np.std(elig_capacities) + 1e-9)\n    \n    # Core hybrid score combining fit tightness and Z-normalized metrics\n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    \n    # Exponential reinforcement based on utilization synergy\n    enhancer = np.exp(utilization * tightness)\n    \n    # Predictive entropy modeling\n    N = len(bins_remain_cap)\n    new_capacities = bins_remain_cap[eligible] - item\n    total_old = np.sum(bins_remain_cap)\n    \n    # Global statistics before any placement\n    mean_old = total_old / N\n    sum_sq_old = np.sum(bins_remain_cap ** 2)\n    var_old = (sum_sq_old / N) - (mean_old ** 2)\n    \n    # Hypothetical new statistics post-placement\n    new_means_i = (total_old - item) / N\n    sum_sq_new_i = sum_sq_old - (bins_remain_cap[eligible] ** 2) + (new_capacities ** 2)\n    var_new_i = (sum_sq_new_i / N) - (new_means_i ** 2)\n    \n    # Entropy sensitivity analysis\n    system_sensitivity = var_old - var_new_i\n    entropy_std = np.std(system_sensitivity) if len(system_sensitivity) > 1 else 1.0\n    normalized_entropy = system_sensitivity / (entropy_std + 1e-9)\n    \n    # Adaptive entropy weighting\n    system_cv = np.std(bins_remain_cap) / (np.mean(bins_remain_cap) + 1e-9)\n    large_item = item > np.mean(bins_remain_cap)\n    entropy_weight = 0.5 * (1.0 + system_cv) * (1.5 if large_item else 0.5)\n    entropy_weight /= (np.log(2 + np.abs(system_sensitivity))).mean() + 1e-9\n    \n    # Residual sensitivity saturation\n    sensitivity = 1.0 / (1.0 + np.abs((new_capacities - new_means_i) / (new_means_i + 1e-9)))\n    \n    # Combined priority score\n    priority = primary_score * enhancer * sensitivity + entropy_weight * normalized_entropy\n    \n    # Deterministic tightness-aware tie-breaking\n    if np.allclose(priority[eligible], priority[eligible][0]):\n        priority += 1e-4 * tightness * (bins_remain_cap ** (-0.5))\n    \n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 16th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + 1e-9,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    # Entropy calculation with discretization\n    rounded_caps = np.round(bins_remain_cap, 3)\n    _, counts = np.unique(rounded_caps, return_counts=True)\n    probs = counts / counts.sum()\n    system_entropy = -np.sum(probs * np.log(probs + 1e-9))\n    max_entropy = np.log(len(bins_remain_cap)) if len(bins_remain_cap) > 1 else 1.0\n    entropy_factor = system_entropy / (max_entropy + 1e-9)\n    \n    # Eligible bin metrics\n    bin_remain = bins_remain_cap[eligible]\n    fit_tightness = item / (bin_remain + 1e-9)\n    util_after = (orig_cap - bin_remain + item) / (orig_cap + 1e-9)\n    leftover = bin_remain - item\n    \n    # Adaptive normalization\n    ft_min, ft_max = fit_tightness.min(), fit_tightness.max()\n    fit_norm = 0.5 if ft_max <= ft_min else (fit_tightness - ft_min) / (ft_max - ft_min)\n    \n    ua_min, ua_max = util_after.min(), util_after.max()\n    util_norm = 0.5 if ua_max <= ua_min else (util_after - ua_min) / (ua_max - ua_min)\n    \n    # Hybrid score with entropy-driven weights\n    weight_fit = np.clip(1.0 - entropy_factor, 0.1, 0.9)\n    weight_util = 1.0 - weight_fit\n    hybrid = (weight_fit * fit_norm) + (weight_util * util_norm)\n    \n    # Nonlinear boost\n    boosted = hybrid ** 2\n    \n    # Entropy-driven proximity term\n    proximity = np.abs(leftover - system_avg) / (system_std + 1e-9)\n    proximity_penalty = np.exp(-proximity)\n    \n    # Shape factor using z-score\n    ls_z = (leftover - system_avg) / (system_std + 1e-9)\n    shape_factor = np.exp(-np.abs(ls_z))\n    \n    # Reinforcement-inspired terms\n    fragility = util_after  # High fragility = nearly full bin\n    usability = np.clip((system_avg - leftover) / (system_std + 1e-9), -1.0, 1.0)\n    closeness_weight = np.clip(1.0 - entropy_factor, 0.2, 1.0)\n    \n    reinforcer = 1.0 + (fragility * np.abs(usability) * closeness_weight)\n    \n    # Final priority calculation\n    priority = boosted * proximity_penalty * shape_factor * reinforcer\n    \n    # Build result array\n    result = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    result[eligible] = priority\n    \n    return result\n\n[Heuristics 17th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + 1e-9,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    # Entropy calculation with discretization\n    rounded_caps = np.round(bins_remain_cap, 3)\n    _, counts = np.unique(rounded_caps, return_counts=True)\n    probs = counts / counts.sum()\n    system_entropy = -np.sum(probs * np.log(probs + 1e-9))\n    max_entropy = np.log(len(bins_remain_cap)) if len(bins_remain_cap) > 1 else 1.0\n    entropy_factor = system_entropy / (max_entropy + 1e-9)\n    \n    # Eligible bin metrics\n    bin_remain = bins_remain_cap[eligible]\n    fit_tightness = item / (bin_remain + 1e-9)\n    util_after = (orig_cap - bin_remain + item) / (orig_cap + 1e-9)\n    leftover = bin_remain - item\n    \n    # Adaptive normalization\n    ft_min, ft_max = fit_tightness.min(), fit_tightness.max()\n    fit_norm = 0.5 if ft_max <= ft_min else (fit_tightness - ft_min) / (ft_max - ft_min)\n    \n    ua_min, ua_max = util_after.min(), util_after.max()\n    util_norm = 0.5 if ua_max <= ua_min else (util_after - ua_min) / (ua_max - ua_min)\n    \n    # Hybrid score with entropy-driven weights\n    weight_fit = np.clip(1.0 - entropy_factor, 0.1, 0.9)\n    weight_util = 1.0 - weight_fit\n    hybrid = (weight_fit * fit_norm) + (weight_util * util_norm)\n    \n    # Nonlinear boost\n    boosted = hybrid ** 2\n    \n    # Entropy-driven proximity term\n    proximity = np.abs(leftover - system_avg) / (system_std + 1e-9)\n    proximity_penalty = np.exp(-proximity)\n    \n    # Shape factor using z-score\n    ls_z = (leftover - system_avg) / (system_std + 1e-9)\n    shape_factor = np.exp(-np.abs(ls_z))\n    \n    # Reinforcement-inspired terms\n    fragility = util_after  # High fragility = nearly full bin\n    usability = np.clip((system_avg - leftover) / (system_std + 1e-9), -1.0, 1.0)\n    closeness_weight = np.clip(1.0 - entropy_factor, 0.2, 1.0)\n    \n    reinforcer = 1.0 + (fragility * np.abs(usability) * closeness_weight)\n    \n    # Final priority calculation\n    priority = boosted * proximity_penalty * shape_factor * reinforcer\n    \n    # Build result array\n    result = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    result[eligible] = priority\n    \n    return result\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive hybrid heuristic combining Z-normalized metrics, entropy-aware penalties, \n    and gradient-enhanced fragility control for online bin packing. Balances tight fits \n    with system-wide entropy optimization through dynamic weight allocation.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    # Fast path for negligible items\n    if item < 1e-9:\n        return np.where(bins_remain_cap >= 0, \n                      0.3 / (bins_remain_cap + 1e-4) - 0.1 * bins_remain_cap,\n                      -np.inf)\n    \n    eligible = bins_remain_cap >= item - 1e-9\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # System state analysis\n    sys_avg, sys_std = np.mean(bins_remain_cap), np.std(bins_remain_cap)\n    sys_cv = sys_std / (sys_avg + 1e-9) if sys_avg > 1e-9 else 0.0\n    large_item = item > sys_avg * 0.75 * (1 + 0.3 * sys_cv)\n    \n    # Core metric computation\n    residual = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    fit_power = 1.0 / (residual + 1e-6)\n    frag_indicator = (residual - sys_avg) / (sys_std + 1e-9)\n    \n    # Adaptive Z-normalization\n    def z_normalize(metric):\n        return (metric - np.mean(metric[eligible])) / (np.std(metric[eligible]) + 1e-9)\n    \n    z_fit = z_normalize(fit_power)\n    z_tight = z_normalize(tightness)\n    z_balance = z_normalize(-np.abs(residual - sys_avg))\n    \n    # Dynamic weight calculation\n    def calc_weight(metric):\n        m_var = np.var(metric[eligible])\n        return m_var * (1.5 if large_item else 1.0) * (1 + sys_cv)\n    \n    fit_weight = calc_weight(z_fit)\n    tight_weight = calc_weight(1 - tightness)\n    balance_weight = calc_weight(-np.abs(residual - sys_avg))\n    \n    # Synergy amplification\n    synergy = 1.0 + np.tanh(\n        0.5 * (z_fit * fit_weight - z_tight * tight_weight)\n    ) * (1 - 0.4 * sys_std / (sys_avg + 1e-9))\n    \n    # Entropy-aware penalty tiers\n    penalty = np.select(\n        [\n            frag_indicator > sys_cv + 1.0,\n            frag_indicator < -sys_cv - 0.7,\n            frag_indicator < 0.5\n        ],\n        [\n            0.4 * sys_std**0.7 * (1 + sys_cv**0.5),\n            0.2 * sys_std**0.4 / (sys_avg + 1e-9),\n            0.15 * sys_cv**0.6\n        ],\n        default=0.08 * (1 - sys_cv)\n    )\n    \n    # Gradient-enhanced scoring\n    base_score = (\n        z_fit * fit_weight * synergy * (1.2 if large_item else 0.9) +\n        z_balance * balance_weight * (0.8 + 0.5 * sys_cv) +\n        (1 - z_tight * tight_weight * (0.6 if large_item else 1.0))\n    )\n    \n    # Fragility-adjusted final score\n    final_score = np.where(\n        eligible,\n        base_score - residual * penalty * (1.3 if item < sys_avg else 1.0),\n        -np.inf\n    )\n    \n    return final_score\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive hybrid heuristic combining Z-normalized metrics, entropy-aware penalties, \n    and gradient-enhanced fragility control for online bin packing. Balances tight fits \n    with system-wide entropy optimization through dynamic weight allocation.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    # Fast path for negligible items\n    if item < 1e-9:\n        return np.where(bins_remain_cap >= 0, \n                      0.3 / (bins_remain_cap + 1e-4) - 0.1 * bins_remain_cap,\n                      -np.inf)\n    \n    eligible = bins_remain_cap >= item - 1e-9\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # System state analysis\n    sys_avg, sys_std = np.mean(bins_remain_cap), np.std(bins_remain_cap)\n    sys_cv = sys_std / (sys_avg + 1e-9) if sys_avg > 1e-9 else 0.0\n    large_item = item > sys_avg * 0.75 * (1 + 0.3 * sys_cv)\n    \n    # Core metric computation\n    residual = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    fit_power = 1.0 / (residual + 1e-6)\n    frag_indicator = (residual - sys_avg) / (sys_std + 1e-9)\n    \n    # Adaptive Z-normalization\n    def z_normalize(metric):\n        return (metric - np.mean(metric[eligible])) / (np.std(metric[eligible]) + 1e-9)\n    \n    z_fit = z_normalize(fit_power)\n    z_tight = z_normalize(tightness)\n    z_balance = z_normalize(-np.abs(residual - sys_avg))\n    \n    # Dynamic weight calculation\n    def calc_weight(metric):\n        m_var = np.var(metric[eligible])\n        return m_var * (1.5 if large_item else 1.0) * (1 + sys_cv)\n    \n    fit_weight = calc_weight(z_fit)\n    tight_weight = calc_weight(1 - tightness)\n    balance_weight = calc_weight(-np.abs(residual - sys_avg))\n    \n    # Synergy amplification\n    synergy = 1.0 + np.tanh(\n        0.5 * (z_fit * fit_weight - z_tight * tight_weight)\n    ) * (1 - 0.4 * sys_std / (sys_avg + 1e-9))\n    \n    # Entropy-aware penalty tiers\n    penalty = np.select(\n        [\n            frag_indicator > sys_cv + 1.0,\n            frag_indicator < -sys_cv - 0.7,\n            frag_indicator < 0.5\n        ],\n        [\n            0.4 * sys_std**0.7 * (1 + sys_cv**0.5),\n            0.2 * sys_std**0.4 / (sys_avg + 1e-9),\n            0.15 * sys_cv**0.6\n        ],\n        default=0.08 * (1 - sys_cv)\n    )\n    \n    # Gradient-enhanced scoring\n    base_score = (\n        z_fit * fit_weight * synergy * (1.2 if large_item else 0.9) +\n        z_balance * balance_weight * (0.8 + 0.5 * sys_cv) +\n        (1 - z_tight * tight_weight * (0.6 if large_item else 1.0))\n    )\n    \n    # Fragility-adjusted final score\n    final_score = np.where(\n        eligible,\n        base_score - residual * penalty * (1.3 if item < sys_avg else 1.0),\n        -np.inf\n    )\n    \n    return final_score\n\n[Heuristics 20th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - item + 1e-9,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System metrics\n    system_mean = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_var = system_std ** 2\n    \n    # Relative variance for adaptive control\n    var_rel = system_var / (orig_cap ** 2 + 1e-9)\n    \n    # Core metrics for eligible bins\n    leftover = bins_remain_cap - item\n    tightness = np.clip(item / (bins_remain_cap + 1e-9), 0, 1)\n    \n    # Fit quality (tighter fit = better)\n    fit_quality = tightness\n    \n    # Balance quality (leftover near system mean)\n    z_leftover = (leftover - system_mean) / (system_std + 1e-9)\n    balance_quality = -np.abs(z_leftover)\n    \n    # Flexibility quality (promote consolidation around median)\n    med_remain = np.median(bins_remain_cap[eligible])\n    flex_quality = -((leftover - med_remain) ** 2) / (orig_cap ** 2 + 1e-9)\n    \n    # Adaptive weights\n    balance_weight = np.sqrt(np.clip(var_rel, 0, 1))  # Stronger balance priority when imbalanced\n    fit_weight = 1 - var_rel\n    \n    # Hybrid base score with adaptive weights\n    hybrid_base = fit_quality * fit_weight + balance_quality * balance_weight\n    \n    # Reinforcement-inspired flexibility component\n    flex_component = flex_quality * (balance_weight ** 2)\n    \n    # Nonlinear item size adaptation\n    item_rel_size = item / (system_mean + 1e-9)\n    fit_decay = np.exp(-item_rel_size * tightness)  # Stronger fit penalty for large items\n    \n    # Final priority score\n    final_score = hybrid_base + flex_component * fit_decay\n    \n    return np.where(eligible, final_score, -np.inf)\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}