```python
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Adaptive hybrid priority function combining tight-fit incentives with entropy-aware balancing.
    Uses item-relative weighting and deviation penalties to optimize both immediate fit and long-term flexibility.
    """
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    orig_cap = np.max(bins_remain_cap)
    if item <= 1e-9:
        return np.where(bins_remain_cap >= item, bins_remain_cap + 1e-9, -np.inf)
    
    eligible = bins_remain_cap >= item
    if not np.any(eligible):
        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    
    # Focus on eligible bin statistics
    eligible_remaining = bins_remain_cap[eligible]
    avg_eligible = eligible_remaining.mean()
    std_eligible = eligible_remaining.std()
    
    # Adaptive strategy weight based on item size relative to system
    strategy_weight = np.clip(item / (avg_eligible + 1e-9), 0.0, 2.0)
    
    # Core components with nonlinear scaling
    tightness = item / (bins_remain_cap + 1e-9)
    leftover = bins_remain_cap - item
    
    # Hybrid metric: weighted combination of fit quality and space preservation
    fit_score = tightness * strategy_weight
    space_score = (leftover / (orig_cap + 1e-9)) * (1.0 - strategy_weight)
    hybrid_metric = fit_score + space_score
    
    # Entropy-aware deviation penalty
    med_eligible = np.median(eligible_remaining)
    leftover_after = leftover[eligible]
    deviation_from_med = np.abs(leftover_after - med_eligible)
    if len(eligible_remaining) > 1:
        med_penalty = deviation_from_med / (eligible_remaining.ptp() + 1e-9)
    else:
        med_penalty = 0.0
    
    # Apply deviation penalties to eligible bins
    adjusted_metric = hybrid_metric.copy()
    adjusted_metric[eligible] -= 0.3 * med_penalty  # Penalty scaled by empirical factor
    
    # System CV-driven reinforcement
    system_cv = std_eligible / (avg_eligible + 1e-9) if avg_eligible > 1e-9 else 0.0
    utilization_gradient = (orig_cap - bins_remain_cap) / (orig_cap + 1e-9)
    
    # Final priority with CV-modulated reinforcement
    priority = adjusted_metric * (1.0 + system_cv * utilization_gradient)
    
    return np.where(eligible, priority, -np.inf)
```
