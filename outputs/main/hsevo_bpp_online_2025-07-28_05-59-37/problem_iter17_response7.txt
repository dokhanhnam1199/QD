```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines min-max normalized fit/util metrics with adaptive weights,
    predictive leftover clustering, and fragmentation-agnostic balance.
    Uses item-relative weighting and variance-driven penalty scaling.
    """
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    eps = 1e-9
    eligible = bins_remain_cap >= item
    if not eligible.any():
        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    
    # System metrics
    system_avg = bins_remain_cap.mean()
    system_std = bins_remain_cap.std()
    system_cv = system_std / (system_avg + eps)
    origin_cap = np.max(bins_remain_cap)
    
    # Item classification
    relative_size = item / (system_avg + eps)
    is_large = relative_size > 1.0
    
    # Per-bin metrics
    leftover = bins_remain_cap - item
    tightness = item / (bins_remain_cap + eps)
    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)
    
    # Adaptive weight calculation using sigmoid
    fit_weight = 1.0 / (1.0 + np.exp(-2.0 * (relative_size - 0.5)))
    fit_weight = np.clip(fit_weight, 0.3, 0.7)
    util_weight = 1.0 - fit_weight
    
    # Min-Max normalization for metrics within eligible bins
    tight_masked = tightness[eligible]
    util_masked = utilization[eligible]
    left_masked = leftover[eligible]
    
    t_min, t_max = tight_masked.min(), tight_masked.max()
    u_min, u_max = util_masked.min(), util_masked.max()
    l_min, l_max = left_masked.min(), left_masked.max()
    
    tight_norm = (tightness - t_min) / (t_max - t_min + eps)
    util_norm = (utilization - u_min) / (u_max - u_min + eps)
    left_norm = (leftover - l_min) / (l_max - l_min + eps)
    
    # Hybrid score with adaptive weights
    hybrid = fit_weight * tight_norm + util_weight * util_norm
    
    # Balance penalty based on system variance
    balance_penalty = np.abs(leftover - system_avg) / (system_std + eps)
    balanced_score = hybrid - system_cv * balance_penalty
    
    # Fragmentation penalty (static weight, entropy-agnostic)
    frag_component = np.exp(- (left_norm ** 0.5) * 2)
    frag_weight = 0.3 * (1.0 + bins_remain_cap.size / (origin_cap + eps))
    frag_score = balanced_score - frag_weight * frag_component
    
    # Predictive leftover clustering (reinforcement-inspired)
    if left_masked.size > 0:
        median_left = np.median(left_masked)
        spread = np.std(left_masked) if left_masked.size > 1 else origin_cap
    else:
        median_left = origin_cap / 2
        spread = origin_cap
    
    rem_gap = np.abs(leftover - median_left)
    reinforcer = np.exp(- (rem_gap / (spread + eps)) ** 2)
    reinforcer_weight = 0.5 * (1.0 + system_cv * 0.5)
    reinforced_score = frag_score + reinforcer * reinforcer_weight
    
    # Utilization boost (linear instead of sqrt)
    util_boost = utilization * 0.1 * (1.0 + is_large * 0.5)
    final_score = reinforced_score + util_boost
    
    # Final scores
    scores = np.where(eligible, final_score, -np.inf)
    return scores
```
