```python
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines adaptive metric normalization, item-size-aware weights, variance-based balance,
    and capacity clustering to optimize bin selection. Dynamically balances tight fit requirements
    with future flexibility preservation using real-time system characterization.
    """
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    orig_cap = bins_remain_cap.max()
    eps = 1e-9
    
    if orig_cap <= eps or item <= eps:
        return np.where(bins_remain_cap >= item, bins_remain_cap - item + eps, -np.inf)
    
    eligible = bins_remain_cap >= item
    if not np.any(eligible):
        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    
    # Active capacity metrics
    active_caps = bins_remain_cap[bins_remain_cap > 0]
    median_cap = np.median(active_caps) if active_caps.size > 0 else orig_cap
    rel_size = item / (median_cap + eps)
    small_item = rel_size < 0.65  # Intermediate item sizing
    
    # Metric calculations
    e_rem = bins_remain_cap[eligible]
    leftover = e_rem - item
    tightness = item / (e_rem + eps)
    prev_util = (orig_cap - e_rem) / orig_cap  # Previous occupancy level
    
    # Metric normalization (higher = better)
    norm_t = (tightness - tightness.min()) / (tightness.ptp() + 1e-9) if tightness.ptp() > 1e-9 else np.zeros_like(tightness)
    norm_u = (prev_util - prev_util.min()) / (prev_util.ptp() + 1e-9) if prev_util.ptp() > 1e-9 else np.zeros_like(prev_util)
    
    # Dynamic weighting
    t_weight, u_weight = (1.0, 1.5) if small_item else (1.8, 0.5)
    primary_score = t_weight * norm_t + u_weight * norm_u
    
    # Variance reduction component
    system_std = bins_remain_cap.std()
    system_avg = bins_remain_cap.mean() + eps
    sys_cv = system_std / system_avg
    
    if leftover.size > 1 and sys_cv > 0.2:
        left_std = leftover.std()
        left_med = np.median(leftover)
        # Reward bins closer to median leftover
        balance = 1.0 / (np.abs(leftover - left_med) / (left_std + eps) + 1.0)
    else:
        balance = np.ones_like(leftover)
    
    # Capacity clustering reinforcement
    if active_caps.size > 1:
        act_iqr = np.percentile(active_caps, 75) - np.percentile(active_caps, 25)
        act_iqr = max(act_iqr, 1e-9)
        # Higher similarity for leftover close to active cluster
        sim_score = 1.0 / (np.abs(leftover - np.median(active_caps)) / act_iqr + 1.0)
    else:
        sim_score = np.ones_like(leftover)
    
    reinforcer = 0.6 + 0.4 * sim_score  # Reinforcement amplification
    
    # Final priority with strategic weighting
    priority = (primary_score + 0.4 * balance) * reinforcer
    
    # Construct scores array
    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    scores[eligible] = priority
    return scores
```
