{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combined heuristic: Z-normalized synergy + entropy-tiered penalty + adaptive \n    item-class control + cross-metric variance reinforcement for resilient \n    online bin packing decisions with predictive entropy modeling\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    # Near-zero item fast path\n    if item < 1e-9:\n        return np.where(bins_remain_cap >= 0, \n                      0.5 / (bins_remain_cap + 1e-4) - 1e-4 * bins_remain_cap,\n                      -np.inf)\n    \n    eligible = bins_remain_cap >= item - 1e-9  # Floating point tolerance\n    \n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # Core system statistics\n    sys_avg, sys_std = np.mean(bins_remain_cap), np.std(bins_remain_cap)\n    sys_cv = sys_std / (sys_avg + 1e-9) if sys_avg > 1e-9 else 0.0\n    \n    # Dynamic item classification enhanced by variance tradeoff\n    large_item = item > sys_avg * 0.85 * (1 + 0.2 * sys_cv)\n    \n    # Metric definitions with gradient sensitivity\n    residual = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    fit_power = 1.0 / (residual + 1e-6)\n    frag_indicator = (residual - sys_avg) / (sys_std + 1e-9)\n    \n    # Gradient-boosted Z-note normalization\n    def calc_zscore(metric):\n        return (metric - metric[eligible].mean()) / (metric[eligible].std() + 1e-9)\n    \n    z_fit = calc_zscore(fit_power)\n    z_tight = calc_zscore(tightness)\n    z_balance = calc_zscore(-np.abs(residual - sys_avg))\n    \n    # Enhanced variance sensitivity calculation\n    def calc_adaptive_weight(metric):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            mags = metric[eligible]\n            var_factor = np.std(mags) / (abs(np.mean(mags)) + 1e-9)\n            depth_factor = 1 + 0.5 * (1 - sys_cv)\n            return var_factor * depth_factor\n    \n    # Metric-specific weight generation\n    fit_weight = calc_adaptive_weight(z_fit) * (1.2 if large_item else 1.0)\n    tight_weight = calc_adaptive_weight(1 - tightness) * (0.8 if large_item else 1.4)\n    balance_weight = calc_adaptive_weight(-np.abs(residual - sys_avg)) * (0.5 + 1.0 * sys_cv)\n    \n    # Entropy-aware synergy amplification\n    synergy_factor = 1.0 + np.tanh(\n        0.3 * (z_fit * fit_weight - z_tight * tight_weight)\n    ) * (1 - 0.3 * sys_std / (sys_avg + 1e-9))\n    \n    # Tiered penalty with predictive entropy modeling\n    penalty_weight = np.select(\n        [\n            frag_indicator > sys_cv + 1.2,\n            frag_indicator < -sys_cv - 0.5,\n            frag_indicator < 0.8\n        ],\n        [\n            0.5 * sys_std**0.8 * (1 + sys_cv**0.5),\n            0.3 * sys_std**0.5 / (sys_avg + 1e-9),\n            0.2 * sys_cv**0.7\n        ],\n        default=0.1 * (1 - sys_cv)\n    )\n    \n    # Priority synthesis with gradient control\n    base_score = (\n        z_fit * fit_weight * synergy_factor +\n        (1 - z_tight * tight_weight * (0.7 if large_item else 1.2)) +\n        z_balance * balance_weight\n    )\n    \n    penalty_score = np.where(\n        eligible,\n        residual * penalty_weight * (1.2 if item < sys_avg else 1.0),\n        0\n    )\n    \n    # Final score quantization with fragility suppression\n    final_score = np.where(eligible, base_score - penalty_score, -np.inf)\n    \n    return final_score\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    v2: Z-normalized synergy + cross-metric variance adaptation + gradient-boosted enhancer \n    + fragility-aware balancing with dynamic entropy-sensitive weights.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System stats\n    C_est = bins_remain_cap.max() if bins_remain_cap.size > 0 else 1.0\n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap) + eps\n    system_cv = system_std / (system_avg + eps)\n    remaining = bins_remain_cap + eps\n    \n    # Core metrics\n    leftover = remaining - item + eps\n    utilization = item / remaining\n    inv_leftover = 1.0 / leftover\n    \n    # Z-synergy (tight-fit * utilization)\n    phi = inv_leftover * utilization\n    phi_z = ((phi[eligible] - phi[eligible].mean()) / phi[eligible].std())[eligible]\n    \n    # Z-cap (remaining capacity adaptation)\n    cap_z = ((remaining - np.mean(remaining)) / np.std(remaining + eps))[eligible]\n    \n    # Cross-metric variance adaptation\n    var_phi = np.var(phi[eligible])\n    var_cap = np.var(remaining[eligible])\n    cross_weight = var_cap / (var_phi + var_cap + eps) if (var_phi + var_cap) > 1e-7 else 0.5\n    \n    # Adaptive curvature factor\n    curvature = 1.0 + np.arctan((phi_z.mean() - cap_z.mean()))\n    \n    # Composite base score\n    composite = cross_weight * phi_z + (1 - cross_weight) * cap_z * curvature\n    \n    # Gradient-driven enhancer (concentration + deviation sensitivity)\n    leftover_norm = (leftover - system_avg) / (3 * system_std + eps)\n    tight_density = np.clip(1.0 - np.abs(leftover_norm), 0, 1)  # Concentration zone control\n    grad_scale = np.where(item > system_avg, \n                         1.0 + 2.0 * utilization * system_std, \n                         1.0 + utilization)  # Dynamic gradient sensitivity\n    enhancer = np.exp(composite * tight_density * grad_scale)\n    \n    # Entropy-sensitive load balance (v0-style system variance coupling)\n    filled_frac = (C_est - remaining) / (C_est + eps)\n    system_var = np.var(bins_remain_cap) / (C_est**2 + eps)\n    balance_factor = filled_frac * (1 + system_var)\n    balance_z = (balance_factor - balance_factor[eligible].mean()) / balance_factor[eligible].std()\n    \n    # Fragility-aware perturbation (exponential decay approach)\n    fragility = 1.0 - np.exp(-leftover / (0.2 * C_est + eps))\n    frag_weight = 0.25 * system_var * np.clip((1 - utilization), 0, 1)\n    frag_term = np.where(leftover < 0.15 * C_est, -frag_weight * fragility, -0.01 * frag_weight * fragility)\n    \n    # Dynamic weight optimization\n    entropy_weight = 0.1 * system_cv \n    fragility_weight = 0.05 * (1 - system_cv) * system_var\n    \n    # Final score with hierarchical reinforcement\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    scores[eligible] = (\n        composite * enhancer \n        + entropy_weight * balance_z \n        + fragility_weight * frag_term[eligible]\n    ) * (1 + system_var)\n    \n    return scores\n\n### Analyze & experience\n- Comparing (best) vs (worst), we see the top heuristics use advanced Z-score synergy, item-class adaptation, and predictive entropy modeling. (second best) vs (second worst) reveals superior fragmentation control through multi-metric variance analysis. Comparing (1st) vs (2nd), they are identical (possible duplication). (3rd) vs (4th) shows more sophisticated entropy scaling in higher-ranked functions. Comparing (second worst) vs (worst), both lack dynamic weight optimization found in top heuristics. Overall: superior heuristics balance adaptive normalization, context-aware penalties, and multi-layered sensitivity to system entropy.\n- \n**Keywords**: Adaptive normalization, hybrid metrics, entropy control, reinforcement-inspired gains  \n**Advice**: Prioritize adaptive normalization to balance metric scales. Design hybrid metrics blending fit/utilization with nonlinear boosts. Use system entropy to adjust penalty weights. Prefer reinforcement-inspired tie-breakers (e.g., entropy gradients) over random perturbations.  \n**Avoid**: Static thresholds, entropy penalties without context, vague dynamic weight algorithms, perturbation mechanisms as primary tie-breakers.  \n**Explanation**: Avoiding buzzword-heavy constructs (e.g., \"Z-score synergy\") clarifies implementation. Directly linking entropy control to adaptive weights improves stability, while reinforcement-inspired gains ensure exploitable heuristics. Overcomplicated metrics dilute practicality\u2014prioritize validated, context-aware logic for scalability.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}