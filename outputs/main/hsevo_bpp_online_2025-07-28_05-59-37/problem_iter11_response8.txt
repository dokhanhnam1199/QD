```python
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Dynamic adaptive synergy of Z-normalized tight-fit and utilization with entropy-weighted hybridization via predictive variance modeling and perturbed threshold reinforcement."""
    eps = 1e-9
    mask = bins_remain_cap >= item
    scores = np.full_like(bins_remain_cap, -np.inf)
    
    if not mask.any():
        return scores

    # System state estimation
    C_est = bins_remain_cap.max() if bins_remain_cap.size > 0 else 1.0
    remaining_cap = bins_remain_cap[mask]
    system_avg = bins_remain_cap.mean()
    system_std = bins_remain_cap.std()
    system_cv = system_std / (system_avg + eps)
    
    # Core metrics
    leftover = remaining_cap - item + eps
    inv_leftover = 1.0 / leftover  # Fit quality
    tightness = item / (remaining_cap + eps)  # Utilization tightness
    filled_frac = (C_est - remaining_cap) / C_est  # Relative fill fraction
    norm_leftover = leftover / C_est  # Normalized waste
    
    # Synergy metric: tight-fit Ã— utilization (v0 Z-normalized)
    synergy = inv_leftover * tightness
    z_synergy = (synergy - synergy.mean()) / (synergy.std() + eps)
    
    # Adaptive cross-metric weights (v1's variance-driven logic)
    tight_z = (tightness - tightness.mean()) / (tightness.std() + eps)
    fit_z = (inv_leftover - inv_leftover.mean()) / (inv_leftover.std() + eps)
    tight_var, fit_var = max(0.1, tight_z.var()), max(0.1, fit_z.var())
    weight_tight = fit_var / (fit_var + tight_var)  # Reciprocal variance weighting
    adaptive_combo = weight_tight * tight_z + (1 - weight_tight) * fit_z
    z_adaptive = (adaptive_combo - adaptive_combo.mean()) / (adaptive_combo.std() + eps)
    
    # Entropy-regularized balance terms
    balance_term = -np.abs(leftover - system_avg) * (1.0 + system_cv ** 2)
    fragility_mask = np.logical_or(
        remaining_cap > (system_avg + 2 * system_std),
        remaining_cap < (system_avg - 2 * system_std)
    )
    fragility_score = np.where(
        fragility_mask,
        system_std ** 2 / (np.abs(remaining_cap - system_avg) + eps),
        1.0
    )
    system_var = np.var(filled_frac)
    z_balance = ((balance_term * system_var * (1 + system_cv)) - balance_term.mean()) / (balance_term.std() + eps)
    
    # Enhancer with predictive variance adaptation
    item_vol_score = (item - system_avg) / (system_std + eps)
    dynamic_gamma = np.clip(0.8 * tight_z + 0.2 * (1 - system_cv), 0.5, 1.5)
    volatile_item = np.abs(item_vol_score) > 1.0
    large_item = item_vol_score > 0.0
    base_util_term = np.where(
        large_item,
        np.exp(tight_z + filled_frac),
        np.sqrt((1 - tight_z) * (1 - filled_frac) + 1e-9)
    )
    enhancer = (base_util_term ** dynamic_gamma) * (1 + 0.5 * system_var)
    
    # Exponential waste decay (v0 style)
    exp_waste = np.exp(-norm_leftover)
    z_exp = (exp_waste - exp_waste.mean()) / (exp_waste.std() + eps)
    
    # Composite score with hierarchical reinforcement
    raw_scores = (
        1.0 * z_synergy +
        0.4 * (z_adaptive + (system_cv * z_balance)) +
        0.3 * system_var * z_exp +
        0.2 * z_balance
    ) * enhancer
    
    # Deterministic perturbation for tie-break (v1)
    seed = int(np.abs(item * 1e5) % 1e9)
    np.random.seed(seed)
    scores[mask] = raw_scores + 1e-9 * np.random.normal(size=raw_scores.shape)
    return scores
```
