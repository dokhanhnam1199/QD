[
  {
    "stdout_filepath": "problem_iter5_response0.txt_stdout.txt",
    "code_path": "problem_iter5_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Priority combining inverse/exp tight fit with bin utilization via multiplicative synergy to prevent fragmentation.\"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = bins_remain_cap.max()\n    eps = 1e-9\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    \n    if not mask.any():\n        return scores\n    \n    remaining = bins_remain_cap[mask]\n    inv_waste = 1.0 / (remaining - item + eps)\n    exp_waste = np.exp(-(remaining - item))\n    tight_fit = inv_waste + exp_waste  # Tightness focus\n    \n    bin_utilization = (orig_cap - remaining) / (orig_cap + eps)  # Current utilization\n    \n    # Multiplicative synergy: amplify tight_fit in utilized bins to guide system towards packing dense clusters\n    scores[mask] = tight_fit * (1 + bin_utilization)\n    \n    return scores",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.108496210610296,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response1.txt_stdout.txt",
    "code_path": "problem_iter5_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combine v0's dynamic class with v1's balance using adaptive weights.\n\n    Uses item classification for epsilon scaling and adds system balance scaled by fragmentation level.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    threshold = np.mean(bins_remain_cap)\n    large_item = item > threshold\n\n    valid = bins_remain_cap >= item\n    if not np.any(valid):\n        return -np.inf * np.ones_like(bins_remain_cap)\n    \n    remaining_after = bins_remain_cap - item\n\n    # Base priority from v0\n    epsilon = 1e-6 if large_item else 1e-3\n    base_priority = -remaining_after - epsilon * bins_remain_cap\n\n    # System-wide balance metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    # Adaptive balance weight: higher for fragmentation (system_cv) and small items\n    load_balance_score = -np.abs(remaining_after - system_avg)\n    balance_weight = 0.1 * system_cv * (2 if not large_item else 1)\n\n    # Combine components with dynamic synergy\n    priority = base_priority + balance_weight * load_balance_score\n\n    return np.where(valid, priority, -np.inf)",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response2.txt_stdout.txt",
    "code_path": "problem_iter5_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Priority combining utilization efficiency, inverse waste control, and \n    fragility analysis with system load adaptive weighting.\n    \n    Key enhancements:\n    1. Multi-metric synergy: Captures capacity usage (utilization), waste penalty (inv_waste), and future flexibility (fragility)\n    2. Non-linear sensitivity: Sigmoid-based tie-breaking rewards near-perfect fits\n    3. System awareness: Weight modulation based on bin saturation levels\n    \n    Args:\n        item: Size of item to pack\n        bins_remain_cap: Array of remaining capacities for each bin\n    \n    Returns:\n        np.ndarray: Prioritized scoring matrix for bin assignment\n    \"\"\"\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    viable = bins_remain_cap >= item\n    \n    if not viable.any():\n        return scores\n\n    cap_viable = bins_remain_cap[viable]\n    system_fullness = 1.0 - (np.mean(bins_remain_cap) / bins_remain_cap.max())\n\n    # Core fitness metrics\n    utilization = item / (cap_viable + 1e-9)           # Maximize usage\n    inv_waste = 1.0 / (cap_viable - item + 1e-9)       # Minimize critical space traps\n    fragility = np.log2(cap_viable - item + 2)         # Prevent unusable pockets\n    \n    # Dynamic weight initialization\n    weight_util = 0.35 * (1 + system_fullness)  # Higher when system is dense\n    weight_invw = 0.35                          # Base waste control\n    weight_frag = 0.3 * (1 - system_fullness)   # More conservation when sparse\n    \n    # Prioritize dense packing while keeping viability\n    core_metric = (weight_util * utilization) + (weight_invw * inv_waste) + (weight_frag / fragility)\n    \n    # Sigmoid-penalized tie-breaking for near-similar choices\n    if len(cap_viable) > 1 and np.ptp(core_metric) < 0.1:\n        waste_ratio = (cap_viable - item) / cap_viable\n        penalty = 0.0125 * (1 - (1 / (1 + np.exp(-6 * waste_ratio + 2))))\n\n        core_metric += penalty  # Favor slight overfill margins\n        \n    scores[viable] = core_metric\n    return scores",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response3.txt_stdout.txt",
    "code_path": "problem_iter5_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive hybrid priority combining z-scored fit quality/capacity, exponential utilization enhancement, and layered tightness-weighted metrics.\n    Encourages best-fit in high-utilization bins while dynamically balancing with worst-fit for small items.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    leftover = bins_remain_cap - item\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    # Z-score normalized fit and capacity for fair comparison\n    fit_quality = 1.0 / (leftover + 1e-9)\n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    \n    if elig_fit.size == 0:\n        return np.full_like(fit_quality, -np.inf, dtype=np.float64)\n    \n    # Z-score calculation\n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    \n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    # Dynamic best/worst-fit blend with exponential utilization boost\n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    enhancer = np.exp(utilization * tightness)  # Exponential amplification for utilized bins\n    \n    return np.where(eligible, primary_score * enhancer, -np.inf)",
    "response_id": 3,
    "tryHS": false,
    "obj": 3.9389708815317115,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response4.txt_stdout.txt",
    "code_path": "problem_iter5_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive priority combining tight-fit best-fit/worst-fit + normalized utilization bias + exponential fragmentation penalty.\"\"\"\n    possible = bins_remain_cap >= item\n    leftover = bins_remain_cap - item\n    \n    if not possible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Estimate system state (bin capacity)\n    C_est = bins_remain_cap.max() if bins_remain_cap.size > 0 else 1.0\n    \n    # Dynamic tight-fit classification (>50% of current bin cap)\n    tight = (item > bins_remain_cap / 2) & possible\n    \n    # Best Fit for tight fits, Worst Fit for remaining\n    best_fit = -leftover\n    worst_fit = leftover\n    primary = np.where(tight, best_fit, worst_fit)\n    \n    # Tie-breaker 1: Normalized utilization (higher filled fraction preferred)\n    filled_frac = (C_est - bins_remain_cap) / C_est\n    normalized_util = filled_frac\n    \n    # Tie-breaker 2: Exponential penalty for fragment leftovers (e^-lambda*x)\n    frag_penalty = -np.expm1(-(leftover / C_est))  # Smooths penalty at zero\n    \n    # Weights: Tie-breakers with entropy-aware scaling\n    util_weight = 1e-3 * (1 + normalized_util)  \n    frag_weight = 1e-4\n    \n    # Composite score with synergistic multi metrics\n    priority = primary + util_weight * normalized_util + frag_weight * frag_penalty\n    \n    return np.where(possible, priority, -np.inf)",
    "response_id": 4,
    "tryHS": false,
    "obj": 88.67171918627844,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response5.txt_stdout.txt",
    "code_path": "problem_iter5_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritize bins using hybrid adaptive metrics: combines localized fit quality (tightness, utilization) with system-wide entropy reduction.\n    Dynamic weights adjust via relative item size, bin population regularity (CV), and fragmentation sensitivity.\n    \"\"\"\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    if not mask.any():\n        return scores\n\n    eps = 1e-9\n    remaining = bins_remain_cap[mask].astype(np.float64)\n    \n    # Quantify local bin context (valid bins only)\n    mean_remaining = remaining.mean()\n    std_remaining = remaining.std()\n    relative_size = np.clip(item / (mean_remaining + eps), 0.0, 1.0)\n    \n    # Global bin chaos assessment (entire packing state)\n    system_avg = bins_remain_cap.mean().astype(np.float64)\n    system_std = bins_remain_cap.std().astype(np.float64)\n    system_cv = system_std / (system_avg + eps) if system_avg > 0 else float('inf')\n    \n    # Phase 1: Critical Fit Metrics (weighted by local adaptivity)\n    w_inv = relative_size**2 + std_remaining/(mean_remaining + eps)  # Favors tight fits when item is large\n    w_uti = (1 - relative_size)**2                                  # Rewards high utilization for smaller items\n    w_exp = 0.5 * (1 + relative_size * std_remaining)               # Exponential waste penalty\n    \n    # Phase 2: System Stabilization Forces\n    w_balance = 2.0 * system_cv              # Stronger stabilization needed with higher variance\n    frag_weight = (1.0/system_cv) if system_cv > 1 else 1.0        # Adaptive fragmentation control\n    \n    # Metric Calculations\n    leftover = remaining - item\n    inv_leftover = 1.0 / (leftover + eps)\n    utilization = item / (remaining + eps)\n    exp_waste = np.exp(-leftover)\n    \n    # Bin-level behaviors\n    remaining_after = remaining - item\n    balance_term = -np.abs(remaining_after - system_avg)              # Alignment with system average\n    frag_penalty = np.log1p(1.0 / (leftover + system_cv + eps))       # Penalizes high fragmentation risks\n    \n    # Dynamic base score calculation\n    main_components = [\n        w_inv * inv_leftover,\n        w_uti * utilization,\n        w_exp * exp_waste,\n        w_balance * balance_term,\n        frag_weight * frag_penalty\n    ]\n    main_score = np.sum(main_components, axis=0)\n    \n    # Phase 3: Entropy-aware Final Adjustment (variance minimization)\n    bin_count = bins_remain_cap.size\n    sum_total = bins_remain_cap.sum().astype(np.float64)\n    sum_squares = (bins_remain_cap**2).sum().astype(np.float64)\n    \n    delta_sq = (-2 * item * remaining) + (item**2)\n    new_squares = sum_squares + delta_sq\n    mu_new = (sum_total - item) / bin_count\n    var_new = (new_squares / bin_count) - mu_new**2\n    \n    # Intelligent tie-breaking mechanism with CV-aware scaling\n    var_sensitivity = 0.1 * (1 + np.sqrt(system_cv + eps))\n    tiebreaker = var_sensitivity * (-var_new) / (np.sqrt(np.abs(var_new) + eps) + 1e-5)\n    \n    # Final score with hierarchical optimization layers\n    scores[mask] = main_score + tiebreaker\n    \n    return scores",
    "response_id": 5,
    "tryHS": false,
    "obj": 84.58316713203033,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response6.txt_stdout.txt",
    "code_path": "problem_iter5_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Multi-objective priority combining dynamic BF/WF weights, fragmentation penalty, and system balance.\n    Uses online entropy-aware balancing and exponential waste sensitivity.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    valid = bins_remain_cap >= item\n    if not valid.any():\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    C_est = bins_remain_cap.max()\n    relative_size = item / (C_est + 1e-9)  # Avoid division by zero\n    \n    # Dynamic BF/WF blending with non-linear weighting\n    weight_bf = np.clip(relative_size ** 2 + 0.1 * (C_est - bins_remain_cap.mean()) / C_est, 0, 1)\n    best_fit = -(bins_remain_cap - item)  # Higher score = better BF fit\n    worst_fit = bins_remain_cap           # Higher score = more residual space\n    primary_score = weight_bf * best_fit + (1 - weight_bf) * worst_fit\n    \n    # Fragmentation penalty using sigmoid-decay for marginal spaces\n    leftover = bins_remain_cap - item\n    frag_score = -np.exp(-leftover / (0.2 * C_est + 1e-9))  # Exponential penalty for small leftovers\n    \n    # Entropy-aware balancing using median alignment and variance reward\n    med = np.median(bins_remain_cap)\n    distance = np.abs(bins_remain_cap - med)\n    balance_score = -0.1 * distance / (C_est + 1e-9)  # Encourage utilization convergence\n    \n    # Tie-breaking layer for degenerate cases\n    entropy_tiebreaker = 1e-4 * (np.random.rand(*bins_remain_cap.shape) - 0.5)  # Small stochastic component\n    \n    # Final score with validity masking\n    scores = np.where(valid, primary_score + frag_score + balance_score + entropy_tiebreaker, -np.inf)\n    return scores",
    "response_id": 6,
    "tryHS": false,
    "obj": 144.56521739130437,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response7.txt_stdout.txt",
    "code_path": "problem_iter5_code7.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive synergy of inv_leftover*utilization for tight fits + exponential waste decay for fragmentation reduction.\"\"\"\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    if not mask.any():\n        return scores\n    \n    eps = 1e-9\n    remaining = bins_remain_cap[mask]\n    \n    # Core metrics from both heuristics\n    inv_leftover = 1.0 / (remaining - item + eps)  # Maximize for tight fits\n    utilization = item / (remaining + eps)  # Prioritize high space efficiency\n    exp_waste = np.exp(-(remaining - item))  # Penalize large leftover exponentially\n    \n    # Hierarchical synergy: Tight fit + stable baseline + entropy-aware tie-breaking\n    multiplicative_term = inv_leftover * utilization  # Tight-fit dominance\n    additive_term = utilization  # Baseline utilization\n    exp_tie_breaker = 0.1 * exp_waste  # Non-linear tie-breaking\n    \n    # Final score combining hierarchical objectives\n    scores[mask] = multiplicative_term + additive_term + exp_tie_breaker\n    return scores",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.01874750698045,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response8.txt_stdout.txt",
    "code_path": "problem_iter5_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid priority function combining local/global balance, dynamic metric normalization,\n    and variance-sensitive fragmentation control to optimize bin packing decisions.\n    \n    Key ideas:\n    - Multi-metric synergy (inv. leftover, utilization, exponential waste)\n    - Combined local-of-fitting/global system awareness via balance terms\n    - Variance-driven sensitivity scaling\n    - Normalized metric ranges for fair weighting\n    \"\"\"\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    if not mask.any():\n        return scores\n    \n    eps = 1e-9\n    remaining = bins_remain_cap[mask]\n    n_candidates = remaining.shape[0]\n    \n    # Global vs Local context tracking\n    system_avg = np.mean(bins_remain_cap)\n    system_cv = np.std(bins_remain_cap) / (system_avg + eps)  # System-wide fragmentation metric\n    local_avg = remaining.mean()\n    \n    # Base metrics\n    post_insert_remaining = remaining - item\n    inv_leftover = 1.0 / (post_insert_remaining + eps)\n    utilization = item / (remaining + eps)\n    exp_waste = np.exp(-post_insert_remaining)  # Prefer minimal leftover\n    \n    # Adaptive metric normalization (v0-inspired)\n    inv_norm = (inv_leftover - inv_leftover.min()) / (inv_leftover.ptp() + eps)\n    util_norm = (utilization - utilization.min()) / (utilization.ptp() + eps)\n    \n    # Structural ratio with dual-awareness\n    struct_ratio = item / ((local_avg + system_avg) / 2 + eps)  # Balanced size perception\n    weight_inv = 1.0 + struct_ratio * (1 + system_cv)  # Size + fragmentation sensitivity\n    weight_util = 3.0 / (struct_ratio + eps)  # Enhanced small-item focus\n    \n    # Combined balance metric (v0 local + v1 global)\n    if n_candidates > 1:\n        sum_other_candidates = (remaining.sum() - remaining) / (n_candidates - 1e-12)\n        delta_local = np.abs(post_insert_remaining - sum_other_candidates)\n        load_balance_local = 1.0 / (np.log1p(delta_local) + eps)\n    else:\n        load_balance_local = np.ones_like(inv_leftover)\n    \n    delta_global = np.abs(post_insert_remaining - system_avg)\n    load_balance_global = 1.0 / (np.log1p(delta_global) + eps)\n    load_balance = 0.6 * load_balance_local + 0.4 * load_balance_global  # Prioritize fitting context\n    \n    # Variance-aware adaptation\n    local_variance = np.var(remaining)\n    variance_sensitivity = 1.0 + np.tanh(local_variance / (local_avg + eps)) * (1 + system_cv)\n    \n    # Core synergy calculation\n    dynamic_core = weight_inv * inv_norm + weight_util * util_norm\n    adaptive_term = 0.7 * load_balance * variance_sensitivity\n    waste_term = 0.3 * exp_waste * (1 + system_cv)  # System-aware waste scaling\n    \n    # Final score assembly\n    scores[mask] = dynamic_core + adaptive_term + waste_term\n    \n    return scores",
    "response_id": 8,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 38, in priority_v2\n    epsilon = 1e-4 / (std_leftover + 1e-8)\nAttributeError: `ptp` was removed from the ndarray class in NumPy 2.0. Use np.ptp(arr, ...) instead.\n19\n4\n"
  },
  {
    "stdout_filepath": "problem_iter5_response9.txt_stdout.txt",
    "code_path": "problem_iter5_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Sigmoid blending of Best and Worst Fit with adaptive utilization tie-breaker and leftover variance scaling.\n    \n    Primary priority combines BFT tightness (negative leftover) and WFT spreading \n    (bin_remaining) through a smooth function of item size relative to system average. \n    Adaptive epsilon scaling preserves utilization dynamics for robust tie-breaking.\n    \"\"\"\n    mask = bins_remain_cap >= item\n    priority = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    if not bins_remain_cap.size or not np.any(mask):\n        return priority\n\n    # Core fit metrics\n    leftover = bins_remain_cap - item\n    bft = -leftover  # Best Fit Term (negative leftover)\n    wft = bins_remain_cap  # Worst Fit candidate bins\n\n    # Dynamic blending factor via item-to-average ratio\n    avg_remaining = np.mean(bins_remain_cap)\n    # Normalize difference to system-scale (prevent saturation at extreme values)\n    sigmoid_input = (item - avg_remaining) / (avg_remaining + 1e-8)  \n    # Higher sigmoid_weight = preferred WFT for smaller items\n    sigmoid_weight = 1.0 / (1.0 + np.exp(sigmoid_input))  \n\n    # Blend BF/WF preferences: WFT dominates for smaller items\n    combined = (1 - sigmoid_weight) * bft + sigmoid_weight * wft\n\n    # Utilization tie-breaker (item / bin_remaining where feasible)\n    utilization = np.zeros_like(combined)\n    np.divide(item, bins_remain_cap, where=mask, out=utilization)\n\n    # Adaptive epsilon scaling from leftover variance\n    valid_leftover = leftover[mask]\n    std_leftover = np.std(valid_leftover) if valid_leftover.size > 1 else 1e-6\n    epsilon = 1e-4 / (std_leftover + 1e-8)\n\n    # Final priority with utility augmentation\n    priority[mask] = combined[mask] + epsilon * utilization[mask]\n    return priority",
    "response_id": 9,
    "tryHS": false,
    "obj": 149.30195452732352,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response0.txt_stdout.txt",
    "code_path": "problem_iter6_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Multi-objective hybrid priority function with adaptive fit blending and synergy-enhanced utilization metrics.\n    Incorporates non-linear sensitivity analysis through sigmoid-controlled fit preferences and hierarchical synergy boosting.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    epsilon = 1e-9\n    \n    if orig_cap <= epsilon or item <= epsilon:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + epsilon),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item - epsilon\n    eligible_indices = np.where(eligible)[0]\n    if not eligible_indices.size:\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    remaining = bins_remain_cap[eligible_indices]\n    n_eligible = eligible_indices.size\n    \n    # Calculate fit components\n    tightness = item / (remaining + epsilon)\n    utilization = (orig_cap - remaining) / orig_cap\n    leftover = remaining - item\n    \n    # Dynamic fit preference based on item size\n    item_norm = item / orig_cap\n    fit_weight = 1 / (1 + np.exp(-5 * (item_norm - 0.4)))\n    \n    # Best fit component with softmax normalization\n    best_fit_score = 1.0 / (leftover + epsilon)\n    best_fit_exp = np.exp(best_fit_score - best_fit_score.max())\n    best_fit_norm = best_fit_exp / (best_fit_exp.sum() + epsilon)\n    \n    # Worst fit component with min-max normalization\n    worst_fit_norm = (leftover - leftover.min()) / (leftover.ptp() + epsilon) + 1\n    \n    # Adaptive blending of fit preferences\n    tight_weight = fit_weight\n    loose_weight = 1 - fit_weight\n    blended_fit = tight_weight * best_fit_norm + loose_weight * worst_fit_norm\n    \n    # Synergy enhancement based on cross-component analysis\n    avg_util = utilization.mean()\n    avg_tight = tightness.mean()\n    synergy_factor = 1.0 + 0.5 * (\n        (tightness > avg_tight).astype(float) + \n        (utilization > avg_util).astype(float)\n    )\n    \n    # Dynamic synergy weighting based on eligible bin count\n    synergy_weight = max(0.2, min(0.7, n_eligible / (n_eligible + 1)))\n    \n    # Primary scoring with synergy integration\n    primary_score = blended_fit * synergy_factor\n    \n    # Exponential enhancer with Utilization-Tightness coupling\n    enhancer = np.exp(utilization + tightness)\n    \n    # Final priority score assembly\n    final_score = primary_score * enhancer\n    \n    # Compose results for output\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    scores[eligible_indices] = final_score\n    \n    return scores",
    "response_id": 0,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 47, in priority_v2\n    utilization > np.quantile(utilization, 0.5),\nAttributeError: `ptp` was removed from the ndarray class in NumPy 2.0. Use np.ptp(arr, ...) instead.\n38\n5\n"
  },
  {
    "stdout_filepath": "problem_iter6_response1.txt_stdout.txt",
    "code_path": "problem_iter6_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Advanced contextual priority combining hierarchical non-linear dynamics and adaptive entropic sensitivity.\n    Operates with asymptotic efficiency patterns and self-adaptive entropy minimization.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9),\n            -np.inf\n        )\n    \n    eligible = (bins_remain_cap >= item)\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Base metrics\n    remain = bins_remain_cap[eligible]\n    leftover = remain - item\n    tightness = item / (remain + 1e-9)\n    utilization = 1.0 - (remain / orig_cap)\n    \n    # Hierarchical metric components\n    fit_quality = 1.0 / (leftover + 1e-9)                          # Best-fit tightness amplifier\n    entropic_score = -tightness * np.log(tightness + 1e-9)         # Information entropy minimization\n    momentum_factor = np.exp(3.0 * utilization) * (1.0 - tightness) # Utilization-driven reinforcement learning proxy\n    \n    # Adaptive normalization context\n    context_variability = bins_remain_cap.std() / (orig_cap + 1e-9) + 0.1  # Dynamic normalization coefficient\n    \n    # Non-linear scoring with context-switching logic\n    tightness_rescale = np.clip(tightness, 0.25, 0.75)\n    swing_factor = (np.log(orig_cap / (leftover + 1e-9)) *  \n                   np.abs(np.log1p(orig_cap - remain)) + 1e-9)\n    \n    # Custom sensitivity analysis\n    sensitivity_mask = np.where(\n        utilization > np.quantile(utilization, 0.5),\n        1.0 + 0.5 * (utilization - np.min(utilization)),\n        1.0 + 0.2 * (np.max(tightness) - tightness)\n    )\n    \n    # Final score assembly\n    priority = (\n        fit_quality ** (tightness_rescale * sensitivity_mask) +\n        entropic_score * 0.3 +\n        momentum_factor * tightness +\n        np.where(leftover < 1e-3, float('inf'), \n                 np.log(swing_factor) * context_variability)\n    )\n    \n    # Contextually-aware tiebreakers\n    tiebreaker = ((tightness * 0.4 + utilization * 0.6) * \n                 (1.0 - context_variability * np.log(1 + leftover)))\n    \n    final_scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    final_scores[eligible] = (priority + tiebreaker) * (1 + context_variability)\n    \n    return final_scores",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.417630634224167,
    "SLOC": 38.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response2.txt_stdout.txt",
    "code_path": "problem_iter6_code2.py",
    "code": "import numpy as np\nfrom numpy.typing import ArrayLike\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins through multi-factorial hybridization, integrating:\n    - Sigmoid transition between best/worst-fit strategies (contextual size adaptation)\n    - Utilization elevation with tunable penalty for under-filled bins\n    - Space erosion analysis via wavelength-weighted critical zone detection \n    - Sparsity-aware exploration-exploitation compensation\n    Returns prioritized scores identifying optimal bin placement considering systemic objectives.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    n_elig = np.sum(eligible)\n    \n    if n_elig == 0:\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    rem_cap = bins_remain_cap[eligible]\n    leftover = rem_cap - item\n    \n    # Adaptive blending coefficient using logistic sigmoid\n    item_norm = item / orig_cap\n    fit_preference = 1.0 / (1 + np.exp(-5.0 * (item_norm - 0.3)))\n    \n    # Best-fit z-score (normalized inverse leftover)\n    inv_leftover = 1.0 / (leftover + 1e-9)\n    inv_mean, inv_std = np.mean(inv_leftover), np.std(inv_leftover)\n    best_score = ((inv_leftover - inv_mean) / (inv_std + 1e-9)) if inv_std > 0 else np.zeros_like(inv_leftover)\n    \n    # Worst-fit z-score (normalized rem_cap)\n    worst_mean, worst_std = np.mean(rem_cap), np.std(rem_cap)\n    worst_score = ((rem_cap - worst_mean) / (worst_std + 1e-9)) if worst_std > 0 else np.zeros_like(rem_cap)\n    \n    # Blending best/worst scores\n    fit_merit = fit_preference * best_score + (1 - fit_preference) * worst_score\n    \n    # Utilization: calculate enhancement with sigmoidal compression\n    bin_util = (orig_cap - rem_cap) / orig_cap\n    util_threshold = np.maximum(0.25, 0.9 * item_norm)\n    adjusted_util = np.clip((bin_util - util_threshold) * (1 + 3 * item_norm), -3, 5)\n    util_gain = 1.0 / (1 + np.exp(-adjusted_util + 0.8))\n    \n    # Erosion penalty for critical leftover zones\n    leftover_ratio = leftover / orig_cap + 1e-9\n    erosion_flag = (leftover_ratio >= 0.1) & (leftover_ratio <= 0.5)\n    erosion_penalty = erosion_flag * np.sin(np.pi * ((leftover_ratio - 0.1)/0.4 + 1e-9))\n    \n    # Sparsity adjustment\n    if n_elig > 1:\n        bin_edges = np.linspace(rem_cap.min(), rem_cap.max(), int(np.sqrt(n_elig)) + 2)\n        digitized = np.digitize(rem_cap, bin_edges)\n        hist, _ = np.histogram(rem_cap, bin_edges)\n        inv_freq = 1.0 / (hist[digitized - 1] + 1e-9)\n        sparsity_adjust = ((inv_freq - inv_freq.mean()) / (inv_freq.std() + 1e-9))\n        fit_merit += 0.4 * sparsity_adjust\n    \n    # Final composite score\n    prioritization = (fit_merit + util_gain) * (1 + 2 * np.sqrt(util_gain)) - 1.2 * erosion_penalty\n    \n    # Output population\n    output = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    output[eligible] = prioritization.astype(np.float64)\n    \n    return output",
    "response_id": 2,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 65, in priority_v2\nIndexError: index 71 is out of bounds for axis 0 with size 71\n38\n5\n"
  },
  {
    "stdout_filepath": "problem_iter6_response3.txt_stdout.txt",
    "code_path": "problem_iter6_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    \n    # Contextual blending of fit and capacity metrics\n    rel_item = item / orig_cap\n    urgency_weight = 1.0 / (1.0 + np.exp(-10 * (rel_item - 0.4)))  # Sigmoid blend control\n    \n    # Advanced normalization subsets\n    elig_slices = [eligible, np.invert(eligible)]\n    norm_funcs = lambda x: np.clip((x - np.nanmean(x[eligible])) / (np.nanstd(x[eligible]) + 1e-9), -5, 5)\n    \n    # Fit/tightness metrics with hierarchical protection\n    fit_quality = 1.0 / (leftover + 1e-9)\n    fit_norm = np.where(eligible, norm_funcs(fit_quality), -np.inf)\n    \n    # Sensitivity-weighted bin equilibrium\n    cap_sensitivity = bins_remain_cap / orig_cap\n    cap_norm = np.where(eligible, norm_funcs(cap_sensitivity), -np.inf)\n    \n    # Multi-stage reinforcement objective\n    tight_weight = tightness * urgency_weight\n    fit_weight = urgency_weight * norm_funcs(utilization)\n    \n    # Hybrid metric synthesis with threshold avoidance\n    base_score = tight_weight * fit_norm + (1 - tight_weight) * cap_norm\n    util_derivative = np.power(utilization + 1e-3, 1.5)\n    \n    # Recurrent feedback mechanism for context propagation\n    dynamic_factor = (\n        util_derivative * \n        np.exp(-0.5 * (1 - tightness) * (1 - rel_item)) * \n        (0.5 + tightness * urgency_weight)\n    )\n    \n    return np.where(eligible, base_score * dynamic_factor + fit_weight, -np.inf)",
    "response_id": 3,
    "tryHS": false,
    "obj": 10.31112883925011,
    "SLOC": 38.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response4.txt_stdout.txt",
    "code_path": "problem_iter6_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Multi-objective priority function with dynamic sigmoid blending, adaptive normalization, \n    and contextual reinforcement-inspired adjustments for online BPP.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    # Core metrics\n    leftover = bins_remain_cap - item\n    fit_quality = 1.0 / (leftover + 1e-9)\n    space_quality = bins_remain_cap\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    \n    # Contextual bottleneck analysis\n    eligible_caps = bins_remain_cap[eligible]\n    med_cap = np.median(eligible_caps)\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    # Dynamic best-worst blending via logistic sensitivity\n    x = item / (med_cap + 1e-9)\n    blending = 1.0 / (1 + np.exp(-5 * (x - 0.5)))  # Midpoint at item=0.5*med_cap\n    \n    # Z-score normalization across multiple axes\n    fit_sub = fit_quality[eligible]\n    space_sub = space_quality[eligible]\n    fit_norm = (fit_quality - np.mean(fit_sub)) / (np.std(fit_sub) + 1e-9)\n    space_norm = (space_quality - np.mean(space_sub)) / (np.std(space_sub) + 1e-9)\n    \n    # Primary multi-objective score with gap-aware bonus\n    primary = blending * fit_norm + (1 - blending) * space_norm\n    \n    # Non-linear efficiency modifier for residual space\n    residual_efficiency = -np.log(1 + leftover / (orig_cap + 1e-9))\n    secondary = residual_efficiency * (tightness / (np.sqrt(utilization + 1e-9) + 1))\n    \n    # Utilization enhancer with adaptive regulation\n    fragility = np.abs(orig_cap - bins_remain_cap) / (orig_cap + 1e-9)\n    confidence = np.clip((1 - utilization)**3, 0, 1)\n    enhancer = np.exp(2 * tightness) * confidence\n    \n    # Reinforcement-inspired dynamic calibration\n    reinforcement_gain = 1 + np.clip((1 - x)**2 * (1 - utilization) * fragility, 0, 3)\n    \n    # Hierarchical priority combination\n    return np.where(\n        eligible,\n        (primary + 0.5 * secondary) * enhancer * reinforcement_gain,\n        -np.inf\n    )",
    "response_id": 4,
    "tryHS": false,
    "obj": 36.32828081372158,
    "SLOC": 38.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  }
]