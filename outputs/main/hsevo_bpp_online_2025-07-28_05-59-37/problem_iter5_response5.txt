```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Prioritize bins using hybrid adaptive metrics: combines localized fit quality (tightness, utilization) with system-wide entropy reduction.
    Dynamic weights adjust via relative item size, bin population regularity (CV), and fragmentation sensitivity.
    """
    mask = bins_remain_cap >= item
    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    if not mask.any():
        return scores

    eps = 1e-9
    remaining = bins_remain_cap[mask].astype(np.float64)
    
    # Quantify local bin context (valid bins only)
    mean_remaining = remaining.mean()
    std_remaining = remaining.std()
    relative_size = np.clip(item / (mean_remaining + eps), 0.0, 1.0)
    
    # Global bin chaos assessment (entire packing state)
    system_avg = bins_remain_cap.mean().astype(np.float64)
    system_std = bins_remain_cap.std().astype(np.float64)
    system_cv = system_std / (system_avg + eps) if system_avg > 0 else float('inf')
    
    # Phase 1: Critical Fit Metrics (weighted by local adaptivity)
    w_inv = relative_size**2 + std_remaining/(mean_remaining + eps)  # Favors tight fits when item is large
    w_uti = (1 - relative_size)**2                                  # Rewards high utilization for smaller items
    w_exp = 0.5 * (1 + relative_size * std_remaining)               # Exponential waste penalty
    
    # Phase 2: System Stabilization Forces
    w_balance = 2.0 * system_cv              # Stronger stabilization needed with higher variance
    frag_weight = (1.0/system_cv) if system_cv > 1 else 1.0        # Adaptive fragmentation control
    
    # Metric Calculations
    leftover = remaining - item
    inv_leftover = 1.0 / (leftover + eps)
    utilization = item / (remaining + eps)
    exp_waste = np.exp(-leftover)
    
    # Bin-level behaviors
    remaining_after = remaining - item
    balance_term = -np.abs(remaining_after - system_avg)              # Alignment with system average
    frag_penalty = np.log1p(1.0 / (leftover + system_cv + eps))       # Penalizes high fragmentation risks
    
    # Dynamic base score calculation
    main_components = [
        w_inv * inv_leftover,
        w_uti * utilization,
        w_exp * exp_waste,
        w_balance * balance_term,
        frag_weight * frag_penalty
    ]
    main_score = np.sum(main_components, axis=0)
    
    # Phase 3: Entropy-aware Final Adjustment (variance minimization)
    bin_count = bins_remain_cap.size
    sum_total = bins_remain_cap.sum().astype(np.float64)
    sum_squares = (bins_remain_cap**2).sum().astype(np.float64)
    
    delta_sq = (-2 * item * remaining) + (item**2)
    new_squares = sum_squares + delta_sq
    mu_new = (sum_total - item) / bin_count
    var_new = (new_squares / bin_count) - mu_new**2
    
    # Intelligent tie-breaking mechanism with CV-aware scaling
    var_sensitivity = 0.1 * (1 + np.sqrt(system_cv + eps))
    tiebreaker = var_sensitivity * (-var_new) / (np.sqrt(np.abs(var_new) + eps) + 1e-5)
    
    # Final score with hierarchical optimization layers
    scores[mask] = main_score + tiebreaker
    
    return scores
```
