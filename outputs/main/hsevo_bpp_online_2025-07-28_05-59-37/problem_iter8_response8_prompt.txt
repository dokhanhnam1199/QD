{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Multi-objective priority function with dynamic sigmoid blending, adaptive normalization, \n    and contextual reinforcement-inspired adjustments for online BPP.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    # Core metrics\n    leftover = bins_remain_cap - item\n    fit_quality = 1.0 / (leftover + 1e-9)\n    space_quality = bins_remain_cap\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    \n    # Contextual bottleneck analysis\n    eligible_caps = bins_remain_cap[eligible]\n    med_cap = np.median(eligible_caps)\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    # Dynamic best-worst blending via logistic sensitivity\n    x = item / (med_cap + 1e-9)\n    blending = 1.0 / (1 + np.exp(-5 * (x - 0.5)))  # Midpoint at item=0.5*med_cap\n    \n    # Z-score normalization across multiple axes\n    fit_sub = fit_quality[eligible]\n    space_sub = space_quality[eligible]\n    fit_norm = (fit_quality - np.mean(fit_sub)) / (np.std(fit_sub) + 1e-9)\n    space_norm = (space_quality - np.mean(space_sub)) / (np.std(space_sub) + 1e-9)\n    \n    # Primary multi-objective score with gap-aware bonus\n    primary = blending * fit_norm + (1 - blending) * space_norm\n    \n    # Non-linear efficiency modifier for residual space\n    residual_efficiency = -np.log(1 + leftover / (orig_cap + 1e-9))\n    secondary = residual_efficiency * (tightness / (np.sqrt(utilization + 1e-9) + 1))\n    \n    # Utilization enhancer with adaptive regulation\n    fragility = np.abs(orig_cap - bins_remain_cap) / (orig_cap + 1e-9)\n    confidence = np.clip((1 - utilization)**3, 0, 1)\n    enhancer = np.exp(2 * tightness) * confidence\n    \n    # Reinforcement-inspired dynamic calibration\n    reinforcement_gain = 1 + np.clip((1 - x)**2 * (1 - utilization) * fragility, 0, 3)\n    \n    # Hierarchical priority combination\n    return np.where(\n        eligible,\n        (primary + 0.5 * secondary) * enhancer * reinforcement_gain,\n        -np.inf\n    )\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritize bins using hybrid adaptive metrics: combines localized fit quality (tightness, utilization) with system-wide entropy reduction.\n    Dynamic weights adjust via relative item size, bin population regularity (CV), and fragmentation sensitivity.\n    \"\"\"\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    if not mask.any():\n        return scores\n\n    eps = 1e-9\n    remaining = bins_remain_cap[mask].astype(np.float64)\n    \n    # Quantify local bin context (valid bins only)\n    mean_remaining = remaining.mean()\n    std_remaining = remaining.std()\n    relative_size = np.clip(item / (mean_remaining + eps), 0.0, 1.0)\n    \n    # Global bin chaos assessment (entire packing state)\n    system_avg = bins_remain_cap.mean().astype(np.float64)\n    system_std = bins_remain_cap.std().astype(np.float64)\n    system_cv = system_std / (system_avg + eps) if system_avg > 0 else float('inf')\n    \n    # Phase 1: Critical Fit Metrics (weighted by local adaptivity)\n    w_inv = relative_size**2 + std_remaining/(mean_remaining + eps)  # Favors tight fits when item is large\n    w_uti = (1 - relative_size)**2                                  # Rewards high utilization for smaller items\n    w_exp = 0.5 * (1 + relative_size * std_remaining)               # Exponential waste penalty\n    \n    # Phase 2: System Stabilization Forces\n    w_balance = 2.0 * system_cv              # Stronger stabilization needed with higher variance\n    frag_weight = (1.0/system_cv) if system_cv > 1 else 1.0        # Adaptive fragmentation control\n    \n    # Metric Calculations\n    leftover = remaining - item\n    inv_leftover = 1.0 / (leftover + eps)\n    utilization = item / (remaining + eps)\n    exp_waste = np.exp(-leftover)\n    \n    # Bin-level behaviors\n    remaining_after = remaining - item\n    balance_term = -np.abs(remaining_after - system_avg)              # Alignment with system average\n    frag_penalty = np.log1p(1.0 / (leftover + system_cv + eps))       # Penalizes high fragmentation risks\n    \n    # Dynamic base score calculation\n    main_components = [\n        w_inv * inv_leftover,\n        w_uti * utilization,\n        w_exp * exp_waste,\n        w_balance * balance_term,\n        frag_weight * frag_penalty\n    ]\n    main_score = np.sum(main_components, axis=0)\n    \n    # Phase 3: Entropy-aware Final Adjustment (variance minimization)\n    bin_count = bins_remain_cap.size\n    sum_total = bins_remain_cap.sum().astype(np.float64)\n    sum_squares = (bins_remain_cap**2).sum().astype(np.float64)\n    \n    delta_sq = (-2 * item * remaining) + (item**2)\n    new_squares = sum_squares + delta_sq\n    mu_new = (sum_total - item) / bin_count\n    var_new = (new_squares / bin_count) - mu_new**2\n    \n    # Intelligent tie-breaking mechanism with CV-aware scaling\n    var_sensitivity = 0.1 * (1 + np.sqrt(system_cv + eps))\n    tiebreaker = var_sensitivity * (-var_new) / (np.sqrt(np.abs(var_new) + eps) + 1e-5)\n    \n    # Final score with hierarchical optimization layers\n    scores[mask] = main_score + tiebreaker\n    \n    return scores\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see top heuristics use **z-score normalization** and **exponential utilization enhancement** to dynamically adjust priorities, while simpler sigmoid blending in worst heuristics lacks depth. (2nd) vs (3rd) reveals the criticality of hybrid metrics (e.g., z-fit + z-cap) over basic inverse/exp combinations. (5th-7th) incorporate **system-wide entropy and fragmentation control** via adaptive balance weights, which are absent in mid-tier heuristics. Top performers prioritize **multi-layered synergy** (e.g., tightness \u00d7 utilization \u00d7 exponential enhancers) and **reinforcement-inspired gains**, while lower ranks rely on basic BF/WF blending. Worst heuristics (e.g., 19th/20th) lack nuanced tie-breaking (e.g., variance minimization) and system-aware adjustments.\n- \n**Keywords:** Z-score normalization, dynamic exponential boosting, entropy control, hierarchical scoring  \n**Advice:** Embed **state-aware normalization** to balance metric scales, pair **gradient-driven boosting** with entropy-based decay for bin prioritization, and apply **multi-layer tie-breakers** (e.g., fragility + load balance) with epsilon-perturbed thresholds. Use **cross-metric variance analysis** to dynamically adjust weights.  \n**Avoid:** Static thresholds, single-axis tie-breakers, unbounded normalization ranges.  \n**Explanation:** Combining adaptive normalization, gradient-aware dynamics, and entropy-sensitive hierarchies ensures context-specific responsiveness while avoiding brittle rules. Perturbed thresholds and cross-metric variance mitigate overfitting, validating gains without overcomplicating logic.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}