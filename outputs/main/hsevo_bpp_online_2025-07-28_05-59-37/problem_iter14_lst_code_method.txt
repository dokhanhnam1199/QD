{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Z-optimized fit combining exp-utilized tightness with system-wide entropy scaling (higher priority to bins that reduce overall fragmentation).\n    Hybridizes v0 adaptive normalization and v1 entropy-aware balance.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        # Edge-case: negligible item, prefer minimal leftover while slightly favoring large-capacity bins\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9) - 1e-9 * bins_remain_cap,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Core v0 metrics: z-score fit/capacity + exponential enhancer\n    leftover = bins_remain_cap - item\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    fit_quality = 1.0 / (leftover + 1e-9)\n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    \n    # Z-score normalization with perturbed thresholds\n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    \n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    enhancer = np.exp(utilization * tightness)  # Gradient-aware exponential boosting\n    \n    # v1-inspired entropy control with adaptive weight scaling\n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap)\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    # Item classification for epsilon scaling\n    threshold = np.mean(bins_remain_cap)\n    large_item = item > threshold\n    \n    # System-aware fragmentation penalty\n    balance_term = -np.abs(leftover - system_avg)  # Favours bins that reduce global variance\n    balance_weight = 0.1 * system_cv * (2 if not large_item else 1)  # Reinforces entropy control\n    \n    # Multi-layer synergy with cross-metric variance analysis\n    priority = primary_score * enhancer + balance_weight * balance_term\n    \n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Z-optimized fit combining exp-utilized tightness with system-wide entropy scaling (higher priority to bins that reduce overall fragmentation).\n    Hybridizes v0 adaptive normalization and v1 entropy-aware balance.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        # Edge-case: negligible item, prefer minimal leftover while slightly favoring large-capacity bins\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9) - 1e-9 * bins_remain_cap,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Core v0 metrics: z-score fit/capacity + exponential enhancer\n    leftover = bins_remain_cap - item\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    fit_quality = 1.0 / (leftover + 1e-9)\n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    \n    # Z-score normalization with perturbed thresholds\n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    \n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    enhancer = np.exp(utilization * tightness)  # Gradient-aware exponential boosting\n    \n    # v1-inspired entropy control with adaptive weight scaling\n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap)\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    # Item classification for epsilon scaling\n    threshold = np.mean(bins_remain_cap)\n    large_item = item > threshold\n    \n    # System-aware fragmentation penalty\n    balance_term = -np.abs(leftover - system_avg)  # Favours bins that reduce global variance\n    balance_weight = 0.1 * system_cv * (2 if not large_item else 1)  # Reinforces entropy control\n    \n    # Multi-layer synergy with cross-metric variance analysis\n    priority = primary_score * enhancer + balance_weight * balance_term\n    \n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 3rd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Entropy-adapted Z-synergy with item-size pathing and continuity tiebreakers.\n    \n    Combines tunable tight/util synergy where weights depend on item size relative to \n    bin medians, enhanced fragmentation control with system entropy memory, and\n    continuity-sensitive tiebreakers leveraging leftover clustering predictivity.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eps = 1e-9\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if not mask.any():\n        return scores\n\n    origin_cap = bins_remain_cap.max()\n    rem_cap = bins_remain_cap[mask]\n    leftover = rem_cap - item\n\n    # Use tight_fit with exponential sensitivity adjustment\n    inv_waste = 1.0 / (leftover + eps)\n    exp_tight = np.exp(-2.0 * leftover / (origin_cap + eps))\n    tight_fit = inv_waste + 2.0 * exp_tight\n\n    # Utilization metric with floor regularization\n    bin_util = (origin_cap - rem_cap) / (origin_cap + eps)\n\n    # Z-score normalization with bias correction\n    t_mean, t_std = tight_fit.mean(), tight_fit.std(ddof=1)\n    u_mean, u_std = bin_util.mean(), bin_util.std(ddof=1)\n    z_tight = (tight_fit - t_mean) / (t_std + 1.2 * eps)\n    z_util = (bin_util - u_mean) / (u_std + 1.2 * eps)\n\n    # System entropy from logarithmic cap distribution\n    log_caps = np.log(bins_remain_cap[ bins_remain_cap > 0 ] + eps)\n    if log_caps.size > 1:\n        entropic_scale = log_caps.std()  # Higher sensitivity to distributional shape\n    else:\n        entropic_scale = 0.0\n    \n    # Item sizing heuristic relative to active bins\n    active_caps = bins_remain_cap[ bins_remain_cap > 0 ]\n    median_cap = np.median(active_caps) if active_caps.size else origin_cap\n    is_small_item = item / (median_cap + eps) < 0.75\n\n    # Predictive synergy formulation\n    entropy_amp = (1.0 + entropic_scale) ** 1.5\n    \n    if is_small_item:\n        tight_weight = 1.8 + entropy_amp  # Priority small-item consolidation\n        util_weight = 0.3  # Prevent underutilization penalty\n    else:\n        tight_weight = 1.0 \n        util_weight = 1.2 + entropy_amp  # Reward fuller bins\n    \n    synergy = (tight_weight * z_tight) * (1.0 + (util_weight * z_util))\n\n    # Enhanced frag control with lifetime decay\n    leftover_norm = leftover / (origin_cap + eps)\n    base_frag = -np.expm1(-leftover_norm ** 1.25)\n    cap_highway = np.exp(-3.0 * ((leftover - median_cap)**2) / ((origin_cap * 0.4)**2 + eps))\n    frag_penalty = base_frag - cap_highway  # EPS decay for cluster alignment\n\n    # Dynamic weight with memory effects\n    entropic_activity = bins_remain_cap.shape[0] / (origin_cap + eps)\n    frag_weight = 0.8 * entropy_amp * (1.0 + entropic_activity) \n\n    # Core score calculation\n    scores_masked = synergy - (frag_weight * frag_penalty)\n\n    # Tiebreakers with thermalization\n    if active_caps.size > 2:\n        # Preserve topological continuity through spectral alignment\n        dist_to_mean = np.abs(leftover - rem_cap.mean()) / rem_cap.std()\n        continuity_bonus = np.exp(-0.7 * dist_to_mean)\n        scores_masked += 0.15 * continuity_bonus  # Second-order reinforcement\n\n    scores[mask] = scores_masked\n    \n    return scores\n\n[Heuristics 4th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive synergy prioritization with exponential enhancement and entropy-dynamic frag control.\n    Z-synergy augmented through normalized multi-metric fusion and predictive entropy damping.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Metric computations\n    leftover = bins_remain_cap - item\n    origin_cap = np.max(bins_remain_cap)\n    fit_quality = 1.0 / (leftover + eps)\n    tightness = item / (bins_remain_cap + eps)\n    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)\n    \n    # Z-score normalization across eligible bins\n    elig = {\n        'fit': fit_quality[eligible],\n        'tight': tightness[eligible],\n        'util': utilization[eligible]\n    }\n    mean = {k: np.mean(v) for k, v in elig.items()}\n    std = {k: np.std(v) for k, v in elig.items()}\n    \n    # Calculate normalized fields\n    z_fit = (fit_quality - mean['fit']) / (std['fit'] + eps)\n    z_tight = (tightness - mean['tight']) / (std['tight'] + eps)\n    z_util = (utilization - mean['util']) / (std['util'] + eps)\n    \n    # Adaptive synergy with deep utilization coupling (from v1 synergy)\n    synergy = z_tight * (1 + z_util)  # v1's cross-metric synergy\n    \n    # Exponential enhancer (from v0) with raw metric interaction\n    enhancer = np.exp(utilization * tightness)  # Reinforced BF-like prioritization\n    \n    # Primary adaptive score with layered reinforcement\n    primary_score = synergy * enhancer\n    \n    # Entropy-sensitive fragmentation control\n    sys_entropy = bins_remain_cap.std() / (origin_cap + eps)\n    frag_penalty = 1.0 - np.exp(-leftover / (origin_cap + eps))  # Differentiable frag metric\n    frag_weight = 0.2 * (1.0 + sys_entropy)  # Dynamic entropy scaling\n    \n    # Perturbed threshold tie-breaker (enhanced volatility damping)\n    tie_breaker = 0.075 * np.exp(-leftover) * (1.0 + sys_entropy)  # Entropy-coupled decay\n    \n    # Final score with dynamic entropy mitigation and multi-metric balance\n    scores = np.where(\n        eligible,\n        primary_score - frag_weight * frag_penalty + tie_breaker,\n        -np.inf\n    )\n    \n    return scores\n\n[Heuristics 5th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive synergy prioritization with exponential enhancement and entropy-dynamic frag control.\n    Z-synergy augmented through normalized multi-metric fusion and predictive entropy damping.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Metric computations\n    leftover = bins_remain_cap - item\n    origin_cap = np.max(bins_remain_cap)\n    fit_quality = 1.0 / (leftover + eps)\n    tightness = item / (bins_remain_cap + eps)\n    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)\n    \n    # Z-score normalization across eligible bins\n    elig = {\n        'fit': fit_quality[eligible],\n        'tight': tightness[eligible],\n        'util': utilization[eligible]\n    }\n    mean = {k: np.mean(v) for k, v in elig.items()}\n    std = {k: np.std(v) for k, v in elig.items()}\n    \n    # Calculate normalized fields\n    z_fit = (fit_quality - mean['fit']) / (std['fit'] + eps)\n    z_tight = (tightness - mean['tight']) / (std['tight'] + eps)\n    z_util = (utilization - mean['util']) / (std['util'] + eps)\n    \n    # Adaptive synergy with deep utilization coupling (from v1 synergy)\n    synergy = z_tight * (1 + z_util)  # v1's cross-metric synergy\n    \n    # Exponential enhancer (from v0) with raw metric interaction\n    enhancer = np.exp(utilization * tightness)  # Reinforced BF-like prioritization\n    \n    # Primary adaptive score with layered reinforcement\n    primary_score = synergy * enhancer\n    \n    # Entropy-sensitive fragmentation control\n    sys_entropy = bins_remain_cap.std() / (origin_cap + eps)\n    frag_penalty = 1.0 - np.exp(-leftover / (origin_cap + eps))  # Differentiable frag metric\n    frag_weight = 0.2 * (1.0 + sys_entropy)  # Dynamic entropy scaling\n    \n    # Perturbed threshold tie-breaker (enhanced volatility damping)\n    tie_breaker = 0.075 * np.exp(-leftover) * (1.0 + sys_entropy)  # Entropy-coupled decay\n    \n    # Final score with dynamic entropy mitigation and multi-metric balance\n    scores = np.where(\n        eligible,\n        primary_score - frag_weight * frag_penalty + tie_breaker,\n        -np.inf\n    )\n    \n    return scores\n\n[Heuristics 6th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive synergy prioritization with exponential enhancement and entropy-dynamic frag control.\n    Z-synergy augmented through normalized multi-metric fusion and predictive entropy damping.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Metric computations\n    leftover = bins_remain_cap - item\n    origin_cap = np.max(bins_remain_cap)\n    fit_quality = 1.0 / (leftover + eps)\n    tightness = item / (bins_remain_cap + eps)\n    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)\n    \n    # Z-score normalization across eligible bins\n    elig = {\n        'fit': fit_quality[eligible],\n        'tight': tightness[eligible],\n        'util': utilization[eligible]\n    }\n    mean = {k: np.mean(v) for k, v in elig.items()}\n    std = {k: np.std(v) for k, v in elig.items()}\n    \n    # Calculate normalized fields\n    z_fit = (fit_quality - mean['fit']) / (std['fit'] + eps)\n    z_tight = (tightness - mean['tight']) / (std['tight'] + eps)\n    z_util = (utilization - mean['util']) / (std['util'] + eps)\n    \n    # Adaptive synergy with deep utilization coupling (from v1 synergy)\n    synergy = z_tight * (1 + z_util)  # v1's cross-metric synergy\n    \n    # Exponential enhancer (from v0) with raw metric interaction\n    enhancer = np.exp(utilization * tightness)  # Reinforced BF-like prioritization\n    \n    # Primary adaptive score with layered reinforcement\n    primary_score = synergy * enhancer\n    \n    # Entropy-sensitive fragmentation control\n    sys_entropy = bins_remain_cap.std() / (origin_cap + eps)\n    frag_penalty = 1.0 - np.exp(-leftover / (origin_cap + eps))  # Differentiable frag metric\n    frag_weight = 0.2 * (1.0 + sys_entropy)  # Dynamic entropy scaling\n    \n    # Perturbed threshold tie-breaker (enhanced volatility damping)\n    tie_breaker = 0.075 * np.exp(-leftover) * (1.0 + sys_entropy)  # Entropy-coupled decay\n    \n    # Final score with dynamic entropy mitigation and multi-metric balance\n    scores = np.where(\n        eligible,\n        primary_score - frag_weight * frag_penalty + tie_breaker,\n        -np.inf\n    )\n    \n    return scores\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combined heuristic: Z-normalized synergy + entropy-tiered penalty + adaptive \n    item-class control + cross-metric variance reinforcement for resilient \n    online bin packing decisions with predictive entropy modeling\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    # Near-zero item fast path\n    if item < 1e-9:\n        return np.where(bins_remain_cap >= 0, \n                      0.5 / (bins_remain_cap + 1e-4) - 1e-4 * bins_remain_cap,\n                      -np.inf)\n    \n    eligible = bins_remain_cap >= item - 1e-9  # Floating point tolerance\n    \n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # Core system statistics\n    sys_avg, sys_std = np.mean(bins_remain_cap), np.std(bins_remain_cap)\n    sys_cv = sys_std / (sys_avg + 1e-9) if sys_avg > 1e-9 else 0.0\n    \n    # Dynamic item classification enhanced by variance tradeoff\n    large_item = item > sys_avg * 0.85 * (1 + 0.2 * sys_cv)\n    \n    # Metric definitions with gradient sensitivity\n    residual = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    fit_power = 1.0 / (residual + 1e-6)\n    frag_indicator = (residual - sys_avg) / (sys_std + 1e-9)\n    \n    # Gradient-boosted Z-note normalization\n    def calc_zscore(metric):\n        return (metric - metric[eligible].mean()) / (metric[eligible].std() + 1e-9)\n    \n    z_fit = calc_zscore(fit_power)\n    z_tight = calc_zscore(tightness)\n    z_balance = calc_zscore(-np.abs(residual - sys_avg))\n    \n    # Enhanced variance sensitivity calculation\n    def calc_adaptive_weight(metric):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            mags = metric[eligible]\n            var_factor = np.std(mags) / (abs(np.mean(mags)) + 1e-9)\n            depth_factor = 1 + 0.5 * (1 - sys_cv)\n            return var_factor * depth_factor\n    \n    # Metric-specific weight generation\n    fit_weight = calc_adaptive_weight(z_fit) * (1.2 if large_item else 1.0)\n    tight_weight = calc_adaptive_weight(1 - tightness) * (0.8 if large_item else 1.4)\n    balance_weight = calc_adaptive_weight(-np.abs(residual - sys_avg)) * (0.5 + 1.0 * sys_cv)\n    \n    # Entropy-aware synergy amplification\n    synergy_factor = 1.0 + np.tanh(\n        0.3 * (z_fit * fit_weight - z_tight * tight_weight)\n    ) * (1 - 0.3 * sys_std / (sys_avg + 1e-9))\n    \n    # Tiered penalty with predictive entropy modeling\n    penalty_weight = np.select(\n        [\n            frag_indicator > sys_cv + 1.2,\n            frag_indicator < -sys_cv - 0.5,\n            frag_indicator < 0.8\n        ],\n        [\n            0.5 * sys_std**0.8 * (1 + sys_cv**0.5),\n            0.3 * sys_std**0.5 / (sys_avg + 1e-9),\n            0.2 * sys_cv**0.7\n        ],\n        default=0.1 * (1 - sys_cv)\n    )\n    \n    # Priority synthesis with gradient control\n    base_score = (\n        z_fit * fit_weight * synergy_factor +\n        (1 - z_tight * tight_weight * (0.7 if large_item else 1.2)) +\n        z_balance * balance_weight\n    )\n    \n    penalty_score = np.where(\n        eligible,\n        residual * penalty_weight * (1.2 if item < sys_avg else 1.0),\n        0\n    )\n    \n    # Final score quantization with fragility suppression\n    final_score = np.where(eligible, base_score - penalty_score, -np.inf)\n    \n    return final_score\n\n[Heuristics 8th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive synergy prioritization with exponential enhancement and entropy-dynamic frag control.\n    Z-synergy augmented through normalized multi-metric fusion and predictive entropy damping.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Metric computations\n    leftover = bins_remain_cap - item\n    origin_cap = np.max(bins_remain_cap)\n    fit_quality = 1.0 / (leftover + eps)\n    tightness = item / (bins_remain_cap + eps)\n    utilization = (origin_cap - bins_remain_cap) / (origin_cap + eps)\n    \n    # Z-score normalization across eligible bins\n    elig = {\n        'fit': fit_quality[eligible],\n        'tight': tightness[eligible],\n        'util': utilization[eligible]\n    }\n    mean = {k: np.mean(v) for k, v in elig.items()}\n    std = {k: np.std(v) for k, v in elig.items()}\n    \n    # Calculate normalized fields\n    z_fit = (fit_quality - mean['fit']) / (std['fit'] + eps)\n    z_tight = (tightness - mean['tight']) / (std['tight'] + eps)\n    z_util = (utilization - mean['util']) / (std['util'] + eps)\n    \n    # Adaptive synergy with deep utilization coupling (from v1 synergy)\n    synergy = z_tight * (1 + z_util)  # v1's cross-metric synergy\n    \n    # Exponential enhancer (from v0) with raw metric interaction\n    enhancer = np.exp(utilization * tightness)  # Reinforced BF-like prioritization\n    \n    # Primary adaptive score with layered reinforcement\n    primary_score = synergy * enhancer\n    \n    # Entropy-sensitive fragmentation control\n    sys_entropy = bins_remain_cap.std() / (origin_cap + eps)\n    frag_penalty = 1.0 - np.exp(-leftover / (origin_cap + eps))  # Differentiable frag metric\n    frag_weight = 0.2 * (1.0 + sys_entropy)  # Dynamic entropy scaling\n    \n    # Perturbed threshold tie-breaker (enhanced volatility damping)\n    tie_breaker = 0.075 * np.exp(-leftover) * (1.0 + sys_entropy)  # Entropy-coupled decay\n    \n    # Final score with dynamic entropy mitigation and multi-metric balance\n    scores = np.where(\n        eligible,\n        primary_score - frag_weight * frag_penalty + tie_breaker,\n        -np.inf\n    )\n    \n    return scores\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines variance-weighted Z-score synergy with decaying entropy-aware penalty fields. \n    Features hybrid BF/WF behavior through adaptive item classification, predictive fragility control, \n    and dynamic multi-metric balance preservation.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    if item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= 0,\n            1e-4 * bins_remain_cap - 1e-7 * bins_remain_cap**2,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # State-aware metric basis\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    C_est = bins_remain_cap.max() if bins_remain_cap.max() > 0 else item * 5\n    \n    # Z-score framework with variance sensitivity\n    def z_metric(metric):\n        subset = metric[eligible]\n        return (metric - subset.mean()) / (subset.std() + 1e-9)\n    \n    tight_z = z_metric(tightness)\n    fit_z = z_metric(1.0 / (leftover + 1e-9))\n    var_ratio = (fit_z.var() + 1e-9) / (fit_z.var() + tight_z.var() + 2e-9)\n    base_priority = var_ratio * fit_z + (1 - var_ratio) * tight_z\n    \n    # Granular item classification macros\n    sys_avg = bins_remain_cap.mean()\n    sys_std = bins_remain_cap.std()\n    is_large = item > 0.75 * C_est  # Hard threshold for guaranteed-waste scenarios\n    is_typical = item < 1.25 * sys_avg  # Empirically derived normality threshold\n    \n    # Nonlinear climb modulation\n    grad_factor = 1.25 if is_large else 0.85\n    utilization = 1 - bins_remain_cap / C_est\n    climb_envelope = grad_factor * np.exp(0.3 * (tightness + utilization))\n    \n    # Fragmentation hazard containment\n    frag_penalty_powers = 0.2 * np.exp(-leftover / (0.3 * C_est + 1e-9))\n    bin_normalization = bins_remain_cap / (C_est + 1e-9)\n    frag_penalty_modes = (1 + 0.25 * bin_normalization) * abs(leftover - 0.5 * sys_avg)\n    \n    # Landscape stability maintenance\n    median_cap = np.median(bins_remain_cap[eligible])\n    median_proximity = np.abs(bins_remain_cap - median_cap) / (C_est + 1e-9)\n    volatility_term = 0.1 * sys_std * np.abs(tight_z) * median_proximity\n    \n    # Final priority composition\n    priority = base_priority * climb_envelope\n    priority -= frag_penalty_powers\n    priority -= frag_penalty_modes\n    priority += volatility_term\n    priority *= np.exp(-0.05 * left_over / C_est if (left_over := bins_remain_cap - item).max() > 0 else 1.0)\n    \n    # Frailty resilience micro-perturbation\n    priority += 1e-7 * np.random.randn(*bins_remain_cap.shape)\n    \n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9) - 1e-9 * bins_remain_cap,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Core fit metrics\n    leftover = bins_remain_cap - item\n    fit_quality = 1.0 / (leftover + 1e-9)\n    tightness = item / (bins_remain_cap + 1e-9)\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    \n    # Z-score normalization\n    elig_fit = fit_quality[eligible]\n    z_fit = (fit_quality - np.mean(elig_fit)) / (np.std(elig_fit) + 1e-9)\n    \n    elig_bins_remain = bins_remain_cap[eligible]\n    z_cap = (bins_remain_cap - np.mean(elig_bins_remain)) / (np.std(elig_bins_remain) + 1e-9)\n    \n    # Primary fit-balanced score\n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    enhancer = np.exp(utilization * tightness)\n    \n    # Predictive entropy modeling\n    N = len(bins_remain_cap)\n    sum_cap = np.sum(bins_remain_cap)\n    mean_old = sum_cap / N\n    sum_sq = np.sum(bins_remain_cap ** 2)\n    var_old = sum_sq / N - mean_old ** 2\n    \n    elig_remain = bins_remain_cap[eligible]\n    new_remain = elig_remain - item\n    \n    # Precompute new statistics\n    new_sum = sum_cap - item\n    new_mean = new_sum / N\n    \n    # Compute new variance for each eligible bin\n    old_sq = elig_remain ** 2\n    new_sq = new_remain ** 2\n    sum_sq_changed = sum_sq - old_sq + new_sq\n    \n    new_var_i = sum_sq_changed / N - new_mean ** 2\n    \n    # Predictive entropy delta\n    entropy_delta = var_old - new_var_i\n    entropy_std = np.std(entropy_delta) if len(entropy_delta) > 1 else 1.0\n    norm_entropy = entropy_delta / (entropy_std + 1e-9)\n    \n    # Adaptive entropy weight\n    system_cv = np.std(bins_remain_cap) / (np.mean(bins_remain_cap) + 1e-9)\n    large_item = item > np.mean(bins_remain_cap)\n    weight_entropy = 0.5 * (1.0 + system_cv) * (1.5 if large_item else 0.5)\n    weight_entropy /= np.log(2 + np.abs(entropy_delta)).mean() + 1e-9  # Self-regulating dampening\n    \n    # Sensitivity-based refinement\n    residual_sensitivity = np.abs((new_remain - new_mean) / (np.abs(new_mean) + 1e-9))\n    sensitivity_factor = 1.0 / (1.0 + residual_sensitivity)\n    \n    # Final priority calculation\n    priority = (\n        primary_score * enhancer * sensitivity_factor + \n        weight_entropy * norm_entropy\n    )\n    \n    # Tie-breaking with dynamic perturbations\n    if np.allclose(priority, priority[0]):\n        priority += 1e-4 * tightness * (bins_remain_cap + 1e-9) ** (-0.5)\n    \n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9) - 1e-9 * bins_remain_cap,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Core fit metrics\n    leftover = bins_remain_cap - item\n    fit_quality = 1.0 / (leftover + 1e-9)\n    tightness = item / (bins_remain_cap + 1e-9)\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    \n    # Z-score normalization\n    elig_fit = fit_quality[eligible]\n    z_fit = (fit_quality - np.mean(elig_fit)) / (np.std(elig_fit) + 1e-9)\n    \n    elig_bins_remain = bins_remain_cap[eligible]\n    z_cap = (bins_remain_cap - np.mean(elig_bins_remain)) / (np.std(elig_bins_remain) + 1e-9)\n    \n    # Primary fit-balanced score\n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    enhancer = np.exp(utilization * tightness)\n    \n    # Predictive entropy modeling\n    N = len(bins_remain_cap)\n    sum_cap = np.sum(bins_remain_cap)\n    mean_old = sum_cap / N\n    sum_sq = np.sum(bins_remain_cap ** 2)\n    var_old = sum_sq / N - mean_old ** 2\n    \n    elig_remain = bins_remain_cap[eligible]\n    new_remain = elig_remain - item\n    \n    # Precompute new statistics\n    new_sum = sum_cap - item\n    new_mean = new_sum / N\n    \n    # Compute new variance for each eligible bin\n    old_sq = elig_remain ** 2\n    new_sq = new_remain ** 2\n    sum_sq_changed = sum_sq - old_sq + new_sq\n    \n    new_var_i = sum_sq_changed / N - new_mean ** 2\n    \n    # Predictive entropy delta\n    entropy_delta = var_old - new_var_i\n    entropy_std = np.std(entropy_delta) if len(entropy_delta) > 1 else 1.0\n    norm_entropy = entropy_delta / (entropy_std + 1e-9)\n    \n    # Adaptive entropy weight\n    system_cv = np.std(bins_remain_cap) / (np.mean(bins_remain_cap) + 1e-9)\n    large_item = item > np.mean(bins_remain_cap)\n    weight_entropy = 0.5 * (1.0 + system_cv) * (1.5 if large_item else 0.5)\n    weight_entropy /= np.log(2 + np.abs(entropy_delta)).mean() + 1e-9  # Self-regulating dampening\n    \n    # Sensitivity-based refinement\n    residual_sensitivity = np.abs((new_remain - new_mean) / (np.abs(new_mean) + 1e-9))\n    sensitivity_factor = 1.0 / (1.0 + residual_sensitivity)\n    \n    # Final priority calculation\n    priority = (\n        primary_score * enhancer * sensitivity_factor + \n        weight_entropy * norm_entropy\n    )\n    \n    # Tie-breaking with dynamic perturbations\n    if np.allclose(priority, priority[0]):\n        priority += 1e-4 * tightness * (bins_remain_cap + 1e-9) ** (-0.5)\n    \n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 12th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            (1.0 / (bins_remain_cap - item + 1e-9)) - 1e-9 * bins_remain_cap,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    leftover = bins_remain_cap - item\n    utilization = (orig_cap - bins_remain_cap) / (orig_cap + 1e-9)\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    # Base metrics\n    fit_quality = 1.0 / (leftover + 1e-9)\n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    \n    # Z-score normalization\n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    \n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    # Reinforcement-inspired gain w/meta-metric modulation\n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap)\n    system_skew = np.mean((bins_remain_cap - system_avg)**3) / (system_std**3 + 1e-9)\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    large_item = item > np.quantile(bins_remain_cap, 0.75)\n    \n    # Modulated enhancer with entropy-sensitive gradient\n    gradient_energy = (1.0 - system_cv) * utilization + system_cv * tightness\n    rein_enhancer = np.exp(tightness * utilization * gradient_energy + (1.0 - tightness) * (1.0 - utilization) * (1.0 - system_cv))\n    \n    # Multi-spectral entropy balance metrics\n    distance_from_mean = leftover - system_avg\n    quadratic_balance = -(distance_from_mean**2) / (1 + 0.5*system_skew)  # Forward-skew adjustment\n    \n    cap_fluctuations = np.std(bins_remain_cap + 1e-9)\n    fluctuation_term = -np.abs(np.log1p(leftover/(cap_fluctuations + 1e-9)))\n    \n    # Adaptive decision layer weights\n    balance_weight = 0.2 * system_cv * (1 + system_skew*(0.5 if item > system_avg else 2.0)) * np.log(2 + system_avg)\n    fluctuation_weight = 0.05 * (1.0 - system_cv**2) * (1.0 + np.sqrt(tightness))\n    \n    # Spectral sensitivity tie-breaker with perturbations\n    sensitivity_kernel = np.where(\n        bins_remain_cap > system_avg,\n        1.0 / (1e-9 + np.abs(bins_remain_cap - 2*system_avg - item)),\n        1.0 / (1e-9 + np.abs(bins_remain_cap - 0.5*system_avg))\n    )\n    perturbation = 0.001 * sensitivity_kernel * (1.0 + np.random.normal(0, 1, bins_remain_cap.shape) * system_cv)\n    \n    # Core spectral synergy\n    priority_base = tightness * z_fit + (1.0 - tightness) * z_cap\n    dynamic_sensitivity = sensitivity_kernel * (1.0 - 0.5 * system_skew * tightness)\n    \n    # Full integration\n    priority = (\n        priority_base * rein_enhancer \n        + balance_weight * quadratic_balance \n        + fluctuation_weight * fluctuation_term\n        + 0.005 * dynamic_sensitivity\n        + perturbation\n    )\n    \n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 13th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    v2: Z-normalized synergy + cross-metric variance adaptation + gradient-boosted enhancer \n    + fragility-aware balancing with dynamic entropy-sensitive weights.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System stats\n    C_est = bins_remain_cap.max() if bins_remain_cap.size > 0 else 1.0\n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap) + eps\n    system_cv = system_std / (system_avg + eps)\n    remaining = bins_remain_cap + eps\n    \n    # Core metrics\n    leftover = remaining - item + eps\n    utilization = item / remaining\n    inv_leftover = 1.0 / leftover\n    \n    # Z-synergy (tight-fit * utilization)\n    phi = inv_leftover * utilization\n    phi_z = ((phi[eligible] - phi[eligible].mean()) / phi[eligible].std())[eligible]\n    \n    # Z-cap (remaining capacity adaptation)\n    cap_z = ((remaining - np.mean(remaining)) / np.std(remaining + eps))[eligible]\n    \n    # Cross-metric variance adaptation\n    var_phi = np.var(phi[eligible])\n    var_cap = np.var(remaining[eligible])\n    cross_weight = var_cap / (var_phi + var_cap + eps) if (var_phi + var_cap) > 1e-7 else 0.5\n    \n    # Adaptive curvature factor\n    curvature = 1.0 + np.arctan((phi_z.mean() - cap_z.mean()))\n    \n    # Composite base score\n    composite = cross_weight * phi_z + (1 - cross_weight) * cap_z * curvature\n    \n    # Gradient-driven enhancer (concentration + deviation sensitivity)\n    leftover_norm = (leftover - system_avg) / (3 * system_std + eps)\n    tight_density = np.clip(1.0 - np.abs(leftover_norm), 0, 1)  # Concentration zone control\n    grad_scale = np.where(item > system_avg, \n                         1.0 + 2.0 * utilization * system_std, \n                         1.0 + utilization)  # Dynamic gradient sensitivity\n    enhancer = np.exp(composite * tight_density * grad_scale)\n    \n    # Entropy-sensitive load balance (v0-style system variance coupling)\n    filled_frac = (C_est - remaining) / (C_est + eps)\n    system_var = np.var(bins_remain_cap) / (C_est**2 + eps)\n    balance_factor = filled_frac * (1 + system_var)\n    balance_z = (balance_factor - balance_factor[eligible].mean()) / balance_factor[eligible].std()\n    \n    # Fragility-aware perturbation (exponential decay approach)\n    fragility = 1.0 - np.exp(-leftover / (0.2 * C_est + eps))\n    frag_weight = 0.25 * system_var * np.clip((1 - utilization), 0, 1)\n    frag_term = np.where(leftover < 0.15 * C_est, -frag_weight * fragility, -0.01 * frag_weight * fragility)\n    \n    # Dynamic weight optimization\n    entropy_weight = 0.1 * system_cv \n    fragility_weight = 0.05 * (1 - system_cv) * system_var\n    \n    # Final score with hierarchical reinforcement\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    scores[eligible] = (\n        composite * enhancer \n        + entropy_weight * balance_z \n        + fragility_weight * frag_term[eligible]\n    ) * (1 + system_var)\n    \n    return scores\n\n[Heuristics 14th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    v2: Z-normalized synergy + cross-metric variance adaptation + gradient-boosted enhancer \n    + fragility-aware balancing with dynamic entropy-sensitive weights.\n    \"\"\"\n    eps = 1e-9\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System stats\n    C_est = bins_remain_cap.max() if bins_remain_cap.size > 0 else 1.0\n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap) + eps\n    system_cv = system_std / (system_avg + eps)\n    remaining = bins_remain_cap + eps\n    \n    # Core metrics\n    leftover = remaining - item + eps\n    utilization = item / remaining\n    inv_leftover = 1.0 / leftover\n    \n    # Z-synergy (tight-fit * utilization)\n    phi = inv_leftover * utilization\n    phi_z = ((phi[eligible] - phi[eligible].mean()) / phi[eligible].std())[eligible]\n    \n    # Z-cap (remaining capacity adaptation)\n    cap_z = ((remaining - np.mean(remaining)) / np.std(remaining + eps))[eligible]\n    \n    # Cross-metric variance adaptation\n    var_phi = np.var(phi[eligible])\n    var_cap = np.var(remaining[eligible])\n    cross_weight = var_cap / (var_phi + var_cap + eps) if (var_phi + var_cap) > 1e-7 else 0.5\n    \n    # Adaptive curvature factor\n    curvature = 1.0 + np.arctan((phi_z.mean() - cap_z.mean()))\n    \n    # Composite base score\n    composite = cross_weight * phi_z + (1 - cross_weight) * cap_z * curvature\n    \n    # Gradient-driven enhancer (concentration + deviation sensitivity)\n    leftover_norm = (leftover - system_avg) / (3 * system_std + eps)\n    tight_density = np.clip(1.0 - np.abs(leftover_norm), 0, 1)  # Concentration zone control\n    grad_scale = np.where(item > system_avg, \n                         1.0 + 2.0 * utilization * system_std, \n                         1.0 + utilization)  # Dynamic gradient sensitivity\n    enhancer = np.exp(composite * tight_density * grad_scale)\n    \n    # Entropy-sensitive load balance (v0-style system variance coupling)\n    filled_frac = (C_est - remaining) / (C_est + eps)\n    system_var = np.var(bins_remain_cap) / (C_est**2 + eps)\n    balance_factor = filled_frac * (1 + system_var)\n    balance_z = (balance_factor - balance_factor[eligible].mean()) / balance_factor[eligible].std()\n    \n    # Fragility-aware perturbation (exponential decay approach)\n    fragility = 1.0 - np.exp(-leftover / (0.2 * C_est + eps))\n    frag_weight = 0.25 * system_var * np.clip((1 - utilization), 0, 1)\n    frag_term = np.where(leftover < 0.15 * C_est, -frag_weight * fragility, -0.01 * frag_weight * fragility)\n    \n    # Dynamic weight optimization\n    entropy_weight = 0.1 * system_cv \n    fragility_weight = 0.05 * (1 - system_cv) * system_var\n    \n    # Final score with hierarchical reinforcement\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    scores[eligible] = (\n        composite * enhancer \n        + entropy_weight * balance_z \n        + fragility_weight * frag_term[eligible]\n    ) * (1 + system_var)\n    \n    return scores\n\n[Heuristics 15th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Dynamic adaptive synergy of Z-normalized tight-fit and utilization with entropy-weighted hybridization via predictive variance modeling and perturbed threshold reinforcement.\"\"\"\n    eps = 1e-9\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    \n    if not mask.any():\n        return scores\n\n    # System state estimation\n    C_est = bins_remain_cap.max() if bins_remain_cap.size > 0 else 1.0\n    remaining_cap = bins_remain_cap[mask]\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + eps)\n    \n    # Core metrics\n    leftover = remaining_cap - item + eps\n    inv_leftover = 1.0 / leftover  # Fit quality\n    tightness = item / (remaining_cap + eps)  # Utilization tightness\n    filled_frac = (C_est - remaining_cap) / C_est  # Relative fill fraction\n    norm_leftover = leftover / C_est  # Normalized waste\n    \n    # Synergy metric: tight-fit \u00d7 utilization (v0 Z-normalized)\n    synergy = inv_leftover * tightness\n    z_synergy = (synergy - synergy.mean()) / (synergy.std() + eps)\n    \n    # Adaptive cross-metric weights (v1's variance-driven logic)\n    tight_z = (tightness - tightness.mean()) / (tightness.std() + eps)\n    fit_z = (inv_leftover - inv_leftover.mean()) / (inv_leftover.std() + eps)\n    tight_var, fit_var = max(0.1, tight_z.var()), max(0.1, fit_z.var())\n    weight_tight = fit_var / (fit_var + tight_var)  # Reciprocal variance weighting\n    adaptive_combo = weight_tight * tight_z + (1 - weight_tight) * fit_z\n    z_adaptive = (adaptive_combo - adaptive_combo.mean()) / (adaptive_combo.std() + eps)\n    \n    # Entropy-regularized balance terms\n    balance_term = -np.abs(leftover - system_avg) * (1.0 + system_cv ** 2)\n    fragility_mask = np.logical_or(\n        remaining_cap > (system_avg + 2 * system_std),\n        remaining_cap < (system_avg - 2 * system_std)\n    )\n    fragility_score = np.where(\n        fragility_mask,\n        system_std ** 2 / (np.abs(remaining_cap - system_avg) + eps),\n        1.0\n    )\n    system_var = np.var(filled_frac)\n    z_balance = ((balance_term * system_var * (1 + system_cv)) - balance_term.mean()) / (balance_term.std() + eps)\n    \n    # Enhancer with predictive variance adaptation\n    item_vol_score = (item - system_avg) / (system_std + eps)\n    dynamic_gamma = np.clip(0.8 * tight_z + 0.2 * (1 - system_cv), 0.5, 1.5)\n    volatile_item = np.abs(item_vol_score) > 1.0\n    large_item = item_vol_score > 0.0\n    base_util_term = np.where(\n        large_item,\n        np.exp(tight_z + filled_frac),\n        np.sqrt((1 - tight_z) * (1 - filled_frac) + 1e-9)\n    )\n    enhancer = (base_util_term ** dynamic_gamma) * (1 + 0.5 * system_var)\n    \n    # Exponential waste decay (v0 style)\n    exp_waste = np.exp(-norm_leftover)\n    z_exp = (exp_waste - exp_waste.mean()) / (exp_waste.std() + eps)\n    \n    # Composite score with hierarchical reinforcement\n    raw_scores = (\n        1.0 * z_synergy +\n        0.4 * (z_adaptive + (system_cv * z_balance)) +\n        0.3 * system_var * z_exp +\n        0.2 * z_balance\n    ) * enhancer\n    \n    # Deterministic perturbation for tie-break (v1)\n    seed = int(np.abs(item * 1e5) % 1e9)\n    np.random.seed(seed)\n    scores[mask] = raw_scores + 1e-9 * np.random.normal(size=raw_scores.shape)\n    return scores\n\n[Heuristics 16th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eps = 1e-9\n    if item < eps:\n        return np.where(\n            bins_remain_cap >= 0, \n            -np.log(bins_remain_cap + eps) * (bins_remain_cap > 0) + 1e-3,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System state analytics\n    sys_avg = np.mean(bins_remain_cap)\n    sys_std = np.std(bins_remain_cap)\n    sys_cv = sys_std / (sys_avg + eps)\n    \n    # Placement metrics\n    avail_cap = bins_remain_cap[eligible]\n    remain_cap_post = np.where(eligible, bins_remain_cap - item, bins_remain_cap)\n    tightness = item / (avail_cap + eps)\n    leftover = avail_cap - item\n    \n    # Reinforcement learning-inspired reward structure\n    fit_reward = 1.0 / (leftover + eps) * item\n    tightness_reward = 1.0 / (tightness + 1e-3)\n    variance_reward = -np.abs(leftover - sys_avg)\n    \n    gradient_magnitude = abs(np.gradient(bins_remain_cap)).mean() + eps\n    adaptive_rate = 1.0 / (1.0 + gradient_magnitude)\n    \n    reward = (\n        .5 * fit_reward + \n        .35 * tightness_reward + \n        .15 * variance_reward\n    ) * adaptive_rate\n    \n    # Entropy forecasting matrix\n    entropy_base = -(bins_remain_cap / sys_avg) * np.log(bins_remain_cap / sys_avg + eps)\n    \n    entropy_forecast = -(\n        remain_cap_post / sys_avg\n    ) * np.log(remain_cap_post / sys_avg + eps)\n    \n    entropy_diff = entropy_forecast - entropy_base\n    \n    imbalance_sensitivity = 1.0 / (abs(remain_cap_post - sys_avg) + 1e-4)\n    imbalance_sensitivity /= imbalance_sensitivity.sum() + eps\n    \n    entropy_gain = np.where(\n        eligible, -entropy_diff * tightness, -np.inf\n    )\n    entropy_gain_norm = (entropy_gain - np.min(entropy_gain)) / (np.ptp(entropy_gain) + eps)\n    \n    # Sensitivity-adjusted lookahead policy\n    lookahead_policy = np.exp(-(bins_remain_cap / (sys_avg + eps)) ** 2) * entropy_gain_norm\n    \n    # Multi-scale sensitivity harmonization\n    grad_order1 = np.gradient(bins_remain_cap)\n    grad_order2 = np.gradient(grad_order1)\n    \n    state_complexity = np.clip(abs(grad_order2), 0, 1)\n    complexity_sensitivity = 1.0 - np.tanh(sys_cv)\n    \n    mixed_sensitivity = .4 * tightness + .3 * (leftover / sys_avg) * (1 - complexity_sensitivity) + \\\n                        .3 * abs(grad_order2) * complexity_sensitivity\n    \n    # Adaptive weight matrix with entropy flow dynamics\n    smoothed_state = np.convolve(bins_remain_cap, np.ones(5)/5, mode='same')\n    state_change = abs(remain_cap_post - smoothed_state)\n    entropy_flow = np.where(eligible, 1.0 / (state_change + 1e-4), -np.inf)\n    \n    reward_weight = .3 + .7 * entropy_flow / (entropy_flow.max() + eps)\n    entropy_weight = .5 + .5 * (1 / (1 + np.exp(-sys_cv)))\n    \n    priority = (\n        reward_weight * reward + \n        entropy_weight * lookahead_policy + \n        .2 * entropy_gain_norm + \n        .1 * mixed_sensitivity\n    )\n    \n    large_deviation = abs(bins_remain_cap - sys_avg) > 1.5 * sys_std\n    priority[large_deviation] *= .9\n    \n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 17th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Enhanced Z-synergy with entropy-control and dynamic reinforcement.\n    Combines adaptive fit/space blending (v1), Z-normalized balance terms (v0), \n    and entropy-driven variance penalties with reinforcement multipliers.\"\"\"\n    \n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    valid_mask = bins_remain_cap >= item\n    if not np.any(valid_mask):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    # System characterization\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 1e-9 else 0.0\n    orig_cap = bins_remain_cap.max()  # Fixed-bin assumption\n    \n    # Item classification\n    large_item = item > system_avg\n    leftover = bins_remain_cap - item\n    \n    # Adaptive blending components (v1)\n    eligible_rem = bins_remain_cap[valid_mask]\n    tightness = item / (eligible_rem + 1e-9)\n    rel_size = item / (np.median(eligible_rem) + 1e-9)\n    blending = 1.0 / (1.0 + np.exp(-5 * (rel_size - 0.5)))\n    \n    # Z-score metrics (v0 + v1 synergy)\n    fit_metric = 1.0 / (leftover + 1e-9)\n    space_metric = bins_remain_cap.astype(np.float64)\n    \n    def z_score(x, where_mask):\n        mean = x[where_mask].mean()\n        std = x[where_mask].std() + 1e-9\n        return (x - mean) / std\n\n    fit_z = z_score(fit_metric, valid_mask)\n    space_z = z_score(space_metric, valid_mask)\n    tight_z = z_score(-leftover, valid_mask)  # Natural tightness metric\n    \n    # Utilization boost (dynamic adaptation)\n    boost_factor = 2.5 if rel_size < 0.7 else 1.5\n    util_boost = np.exp(boost_factor * tightness * valid_mask)\n    \n    # Balance dynamics (v0 base + entropy sensitivity)\n    balance_comp = -np.abs(leftover - system_avg) / (system_std + 1e-9)\n    balance_weight = 1.8 * np.sqrt(system_cv) * (1.3 if not large_item else 1.0)\n    \n    # Reinforcement multipliers (v1)\n    bin_util = (orig_cap - bins_remain_cap) / orig_cap\n    fragility = np.abs(orig_cap - bins_remain_cap) / (orig_cap + 1e-9)\n    reinforcer = 1 + np.clip((1 - rel_size)**2 * (1 - bin_util) * fragility, 0, 3)\n    \n    # Entropy tiebreaker with variance control (v1)\n    bin_count = bins_remain_cap.size\n    sum_total = bins_remain_cap.sum()\n    sum_sq = (bins_remain_cap**2).sum()\n    new_sum_sq = sum_sq - 2*item*bins_remain_cap + item**2\n    mu_new = (sum_total - item) / bin_count\n    var_new = (new_sum_sq / bin_count) - mu_new**2\n    var_term = -0.3 * var_new / (np.sqrt(np.abs(var_new) + 1e-9) + 1e-5)\n    \n    # Priority construction with hybrid gains\n    blended_score = blending * fit_z + (1 - blending) * space_z\n    synergy = blended_score * (1 + 0.5 * tight_z + 0.3 * util_boost[valid_mask])\n    \n    priority = np.where(\n        valid_mask,\n        (synergy + balance_weight * balance_comp) * reinforcer + var_term,\n        -np.inf\n    )\n    \n    return priority\n\n[Heuristics 18th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - 2 * (bins_remain_cap - item),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    leftover = bins_remain_cap - item\n    utilization = 1.0 - bins_remain_cap / orig_cap\n    tightness = item / (bins_remain_cap + 1e-9)\n    fit_quality = 1.0 / (leftover + 1e-9)\n    \n    elig_indices = np.where(eligible)[0]\n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    \n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap)\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 0 else 0\n    \n    n_bins = bins_remain_cap.size\n    s_total = bins_remain_cap.sum()\n    s2_total = (bins_remain_cap ** 2).sum()\n    delta_s2 = (leftover ** 2) - (bins_remain_cap ** 2)\n    new_s2 = s2_total + delta_s2\n    new_s = s_total - item\n    new_mean = new_s / n_bins\n    new_var = new_s2 / n_bins - new_mean ** 2\n    rel_variance = (new_var - np.std(bins_remain_cap) ** 2) / (np.std(bins_remain_cap) ** 2 + 1e-9)\n    entropy_penalty = np.where(eligible, -rel_variance, np.inf)\n    \n    spatial_sensitivity = np.abs(bins_remain_cap - system_avg) / (system_std + 1e-9)\n    sensitivity_weight = 1.0 / (spatial_sensitivity + 1e-9)\n    adaptive_weights = np.sqrt(system_cv ** 2) * sensitivity_weight\n    \n    fit_primary = z_fit * tightness + z_cap * spatial_sensitivity\n    entropy_response = entropy_penalty * np.exp(-utilization)\n    \n    priority = fit_primary * (1 + system_cv) + adaptive_weights * entropy_response\n    \n    # Perturbation mechanism\n    if np.any(eligible):\n        eps = 1e-6 * system_cv * (np.random.rand(n_bins) - 0.5)\n        priority += np.where(eligible, eps, 0)\n    \n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 19th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - 2 * (bins_remain_cap - item),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    leftover = bins_remain_cap - item\n    utilization = 1.0 - bins_remain_cap / orig_cap\n    tightness = item / (bins_remain_cap + 1e-9)\n    fit_quality = 1.0 / (leftover + 1e-9)\n    \n    elig_indices = np.where(eligible)[0]\n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    \n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap)\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 0 else 0\n    \n    n_bins = bins_remain_cap.size\n    s_total = bins_remain_cap.sum()\n    s2_total = (bins_remain_cap ** 2).sum()\n    delta_s2 = (leftover ** 2) - (bins_remain_cap ** 2)\n    new_s2 = s2_total + delta_s2\n    new_s = s_total - item\n    new_mean = new_s / n_bins\n    new_var = new_s2 / n_bins - new_mean ** 2\n    rel_variance = (new_var - np.std(bins_remain_cap) ** 2) / (np.std(bins_remain_cap) ** 2 + 1e-9)\n    entropy_penalty = np.where(eligible, -rel_variance, np.inf)\n    \n    spatial_sensitivity = np.abs(bins_remain_cap - system_avg) / (system_std + 1e-9)\n    sensitivity_weight = 1.0 / (spatial_sensitivity + 1e-9)\n    adaptive_weights = np.sqrt(system_cv ** 2) * sensitivity_weight\n    \n    fit_primary = z_fit * tightness + z_cap * spatial_sensitivity\n    entropy_response = entropy_penalty * np.exp(-utilization)\n    \n    priority = fit_primary * (1 + system_cv) + adaptive_weights * entropy_response\n    \n    # Perturbation mechanism\n    if np.any(eligible):\n        eps = 1e-6 * system_cv * (np.random.rand(n_bins) - 0.5)\n        priority += np.where(eligible, eps, 0)\n    \n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 20th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            bins_remain_cap - 2 * (bins_remain_cap - item),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    leftover = bins_remain_cap - item\n    utilization = 1.0 - bins_remain_cap / orig_cap\n    tightness = item / (bins_remain_cap + 1e-9)\n    fit_quality = 1.0 / (leftover + 1e-9)\n    \n    elig_indices = np.where(eligible)[0]\n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    \n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap)\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 0 else 0\n    \n    n_bins = bins_remain_cap.size\n    s_total = bins_remain_cap.sum()\n    s2_total = (bins_remain_cap ** 2).sum()\n    delta_s2 = (leftover ** 2) - (bins_remain_cap ** 2)\n    new_s2 = s2_total + delta_s2\n    new_s = s_total - item\n    new_mean = new_s / n_bins\n    new_var = new_s2 / n_bins - new_mean ** 2\n    rel_variance = (new_var - np.std(bins_remain_cap) ** 2) / (np.std(bins_remain_cap) ** 2 + 1e-9)\n    entropy_penalty = np.where(eligible, -rel_variance, np.inf)\n    \n    spatial_sensitivity = np.abs(bins_remain_cap - system_avg) / (system_std + 1e-9)\n    sensitivity_weight = 1.0 / (spatial_sensitivity + 1e-9)\n    adaptive_weights = np.sqrt(system_cv ** 2) * sensitivity_weight\n    \n    fit_primary = z_fit * tightness + z_cap * spatial_sensitivity\n    entropy_response = entropy_penalty * np.exp(-utilization)\n    \n    priority = fit_primary * (1 + system_cv) + adaptive_weights * entropy_response\n    \n    # Perturbation mechanism\n    if np.any(eligible):\n        eps = 1e-6 * system_cv * (np.random.rand(n_bins) - 0.5)\n        priority += np.where(eligible, eps, 0)\n    \n    return np.where(eligible, priority, -np.inf)\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}