[
  {
    "stdout_filepath": "problem_iter8_response0.txt_stdout.txt",
    "code_path": "problem_iter8_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Z-score normalized balance with exponential utilization boosting and adaptive component weights.\n    \n    Combines dynamic BF/WF tendencies separated by item class with entropy-driven balance control \n    and hierarchical tie-breakers using:\n    - Adaptive weights based on system CV and leftover variance\n    - Exponential utilization enhancement for dynamic differentiation\n    - Multi-metric synergy (tightness + balance + utilization)\n    - Z-score normalization of key components\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    valid_mask = bins_remain_cap >= item\n    priority = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if not np.any(valid_mask):\n        return priority\n    \n    # System metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 1e-9 else 0.0\n    \n    # Item classification\n    large_item = item > system_avg  # Threshold at system mean\n    \n    # Primary metrics\n    leftover = bins_remain_cap - item\n    valid_leftover = leftover[valid_mask]\n    \n    # Exponential utilization enhancement\n    utilization = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    np.divide(item, bins_remain_cap, where=valid_mask, out=utilization)\n    exp_util = np.exp(utilization * (1.5 if large_item else 2.5))  # Dynamic scaling\n    \n    # Z-score normalized components\n    tightness_z = (-leftover - system_avg) / (system_std + 1e-9)  # Negative space\n    bin_cap_z = (bins_remain_cap - system_avg) / (system_std + 1e-9)  # Fullness\n    \n    # Core terms\n    tightness_component = tightness_z - 1e-5 * bin_cap_z  # Encourage tighter fits\n    \n    # System balance term\n    load_balance = -np.abs(leftover - system_avg) / (system_std + 1e-9)\n    balance_weight = 1.5 * np.sqrt(system_cv) * (1.5 if not large_item else 1.0)\n    \n    # Entropy-regularized tiebreaker\n    leftover_std = np.std(valid_leftover) if len(valid_leftover) > 1 else 1e-6\n    util_weight = 0.8 / (leftover_std + 1e-9)\n    \n    # Synergistic combination\n    priority[valid_mask] = (\n        tightness_component[valid_mask] * (1.2 if large_item else 1.0)\n        + balance_weight * load_balance[valid_mask]\n        + util_weight * exp_util[valid_mask]\n    )\n    \n    return priority",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.11846828879138,
    "SLOC": 31.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response1.txt_stdout.txt",
    "code_path": "problem_iter8_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines z-score-normalized fit/cap metrics with exponential utilization enhancer \n    and entropy-aware tie-breaking via leftover volatility damping.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    leftover = bins_remain_cap - item\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    # Z-score normalization of fit and capacity\n    fit_quality = 1.0 / (leftover + 1e-9)\n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    \n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    \n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    # Dynamic hybrid weighting and exponential enhancement\n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    enhancer = np.exp(utilization * tightness)\n    \n    # Entropy-aware tie-breaking: small term to penalize fragmentation volatility\n    tie_breaker = 0.05 * np.exp(-leftover)  # Weak exponential damping of leftover space\n    \n    # Final layered prioritization with perturbed thresholds\n    return np.where(\n        eligible, \n        (primary_score * enhancer) + tie_breaker, \n        -np.inf\n    )",
    "response_id": 1,
    "tryHS": false,
    "obj": 3.9389708815317115,
    "SLOC": 31.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response2.txt_stdout.txt",
    "code_path": "problem_iter8_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid priority function combining z-score-normalized metrics, dynamic adaptive weights, \n    and entropy-aware tie-breaking. Balances tight fit, utilization, and system variance.\n    \"\"\"\n    mask = bins_remain_cap >= item\n    if not mask.any():\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    eps = 1e-9\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    remaining = bins_remain_cap[mask].astype(np.float64)\n    \n    # Local/system statistics\n    local_mean, local_std = remaining.mean(), remaining.std()\n    relative_size = np.clip(item / (local_mean + eps), 0, 1)\n    system_mean, system_std = bins_remain_cap.mean(), bins_remain_cap.std()\n    system_cv = system_std / (system_mean + eps) if system_mean > eps else np.inf\n    \n    # Dynamic weight calculation\n    w_inv = relative_size**2 + local_std / (local_mean + eps)\n    w_uti = (1 - relative_size)**2\n    w_exp = 0.5 * (1 + relative_size * local_std)\n    w_balance = 2.0 * system_cv\n    frag_weight = (1.0 / system_cv) if system_cv > 1 else 1.0\n    \n    # Core metric calculation\n    leftover = remaining - item\n    inv_leftover = 1 / (leftover + eps)\n    utilization = item / (remaining + eps)\n    exp_waste = np.exp(-leftover)\n    balance_term = -np.abs((remaining - item) - system_mean)\n    frag_penalty = np.log1p(1.0 / (leftover + system_cv + eps))\n    \n    def zscore(x):\n        std = x.std()\n        return (x - x.mean()) / (std + eps if std > 1e-9 else 1.0)\n    \n    # Normalized components\n    normalized = [\n        zscore(inv_leftover) * w_inv,\n        zscore(utilization) * w_uti,\n        zscore(exp_waste) * w_exp,\n        zscore(balance_term) * w_balance,\n        zscore(frag_penalty) * frag_weight\n    ]\n    \n    # Variance-sensitive tiebreaker\n    N = len(bins_remain_cap)\n    total = bins_remain_cap.sum()\n    squares = (bins_remain_cap**2).sum()\n    delta_sq = -2 * item * remaining + item**2\n    mu_new = (total - item) / N\n    \n    tiebreaker = 0.1 * (1 + np.sqrt(system_cv)) * (\n        -((squares + delta_sq) / N - mu_new**2)\n    ) / (np.sqrt(np.abs(delta_sq) + eps) + 1e-5)\n    \n    scores[mask] = np.sum(normalized, axis=0) + tiebreaker\n    return scores",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.0885520542481055,
    "SLOC": 31.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response3.txt_stdout.txt",
    "code_path": "problem_iter8_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid priority using z-score BF/WF blending, exponential fragmentation control, entropy-based median alignment, \n    and variance-scaled utilization tie-breakers for online bin stability.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    valid = bins_remain_cap >= item\n    if not valid.any():\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # System stats\n    C_est = bins_remain_cap.max()\n    avg_cap = bins_remain_cap.mean()\n    std_cap = bins_remain_cap.std()\n    med_cap = np.median(bins_remain_cap)\n    leftover = bins_remain_cap - item\n    \n    # Z-score driven BF/WF blending\n    z = (item - avg_cap) / (std_cap + 1e-9)\n    blend_weight = 1.0 / (1.0 + np.exp(-z))  # Blend to BF when item > avg\n    primary_score = blend_weight * (-leftover) + (1 - blend_weight) * bins_remain_cap\n    \n    # Nonlinear fragmentation penalty\n    frag_score = -np.exp(-leftover / (0.2 * C_est + 1e-9))\n    \n    # Entropy control via median alignment\n    balance_score = -0.1 * np.abs(bins_remain_cap - med_cap) / (C_est + 1e-9)\n    \n    # Variance-adaptive tie-breaker\n    valid_leftover = leftover[valid]\n    std_leftover = valid_leftover.std() if len(valid_leftover) > 1 else 1e-6\n    epsilon = 1e-4 / (std_leftover + 1e-9)\n    \n    # Utilization gradient per bin\n    utilization = np.zeros_like(leftover, dtype=np.float64)\n    np.divide(item, bins_remain_cap, where=valid, out=utilization)\n    \n    # Score aggregation\n    scores = (primary_score + frag_score + balance_score + epsilon * utilization)\n    scores[~valid] = -np.inf  # Invalid bins\n    return scores",
    "response_id": 3,
    "tryHS": false,
    "obj": 149.30195452732352,
    "SLOC": 31.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response4.txt_stdout.txt",
    "code_path": "problem_iter8_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Z-score normalized synergy of tight-fit and utilization + entropy-aware balancing via system variance.\"\"\"\n    eps = 1e-9\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    \n    if not mask.any():\n        return scores\n    \n    C_est = bins_remain_cap.max() if bins_remain_cap.size > 0 else 1.0\n    remaining = bins_remain_cap[mask]\n    \n    # Core metrics\n    leftover = remaining - item + eps\n    inv_leftover = 1.0 / leftover\n    utilization = item / (remaining + eps)\n    \n    # Multi-metric synergy normalized per-z-score\n    multiplicative_term = inv_leftover * utilization\n    z_ilu = (multiplicative_term - multiplicative_term.mean()) / (multiplicative_term.std() + eps)\n    \n    # Adaptive fragmentation penalty with exponential waste decay\n    norm_leftover = leftover / C_est\n    exp_waste = np.exp(-norm_leftover)\n    z_exp = (exp_waste - exp_waste.mean()) / (exp_waste.std() + eps)\n    \n    # Entropy control via load balance (normalized filled fraction)\n    filled_frac = (C_est - remaining) / C_est\n    system_var = np.var(bins_remain_cap) / (C_est**2 + eps)\n    load_balance_term = filled_frac * (1 + system_var)\n    z_balance = (load_balance_term - load_balance_term.mean()) / (load_balance_term.std() + eps)\n    \n    # Hierarchical score with entropy-weighted tie-breaking\n    scores[mask] = z_ilu + 0.1 * z_exp + 0.05 * system_var * z_balance\n    return scores",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 31.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response5.txt_stdout.txt",
    "code_path": "problem_iter8_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Hybrid heuristic combining z-score balance, exponential utilization boosting, and tight-fit frag control.\n    \n    Adaptive weights driven by system-wide CV and item size class. Balances between tight-fits and entropy-minimized placement.\n    \"\"\"\n    if not bins_remain_cap.size:\n        return np.array([], dtype=np.float64)\n    \n    possible = bins_remain_cap >= item\n    if not possible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # System-wide parameter estimation\n    C_est = bins_remain_cap.max() if bins_remain_cap.size > 0 else 1.0\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 0 else 0.0\n\n    # Item classification for adaptive scaling\n    large_item = item > system_avg\n\n    # Key metrics calculation\n    leftover = bins_remain_cap - item\n    filled_frac = 1.0 - (bins_remain_cap / C_est)  # fraction of bin used\n\n    # Tight-fit classification\n    tight = (item > bins_remain_cap / 2) & possible\n\n    # Base priority logic\n    base_priority = np.where(tight, -leftover, leftover)  # best/worst-fit switching\n\n    # Normalized balance metric using Z-score\n    z_balance = (leftover - system_avg) / (system_std + 1e-9)\n    balance_weight = 0.1 * system_cv * (2 if large_item else 1)\n    balance_score = -balance_weight * np.abs(z_balance)  # Favor bins close to avg remaining\n\n    # Exponential utilization enhancement\n    util_exp = np.exp(2 * filled_frac)  # Super-linear boost for higher filled bins\n    util_weight = 1e-3 * (1 + system_cv)\n    util_score = util_weight * util_exp\n\n    # Fragmentation penalty for small leftover gaps\n    x = leftover / C_est\n    frag_score = np.exp(-x)  # Higher penalty for smaller leftover\n    frag_weight = 1e-4 * (1 + filled_frac)  # Adaptive scaling by bin occupancy\n    frag_term = frag_weight * frag_score\n\n    # Composite priority combining all factors\n    priority = base_priority + balance_score + util_score - frag_term\n    \n    return np.where(possible, priority, -np.inf)",
    "response_id": 5,
    "tryHS": false,
    "obj": 88.67171918627844,
    "SLOC": 31.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response6.txt_stdout.txt",
    "code_path": "problem_iter8_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Z-optimized fit combining exp-utilized tightness with system-wide entropy scaling (higher priority to bins that reduce overall fragmentation).\n    Hybridizes v0 adaptive normalization and v1 entropy-aware balance.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        # Edge-case: negligible item, prefer minimal leftover while slightly favoring large-capacity bins\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9) - 1e-9 * bins_remain_cap,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Core v0 metrics: z-score fit/capacity + exponential enhancer\n    leftover = bins_remain_cap - item\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    fit_quality = 1.0 / (leftover + 1e-9)\n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    \n    # Z-score normalization with perturbed thresholds\n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    \n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    enhancer = np.exp(utilization * tightness)  # Gradient-aware exponential boosting\n    \n    # v1-inspired entropy control with adaptive weight scaling\n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap)\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    # Item classification for epsilon scaling\n    threshold = np.mean(bins_remain_cap)\n    large_item = item > threshold\n    \n    # System-aware fragmentation penalty\n    balance_term = -np.abs(leftover - system_avg)  # Favours bins that reduce global variance\n    balance_weight = 0.1 * system_cv * (2 if not large_item else 1)  # Reinforces entropy control\n    \n    # Multi-layer synergy with cross-metric variance analysis\n    priority = primary_score * enhancer + balance_weight * balance_term\n    \n    return np.where(eligible, priority, -np.inf)",
    "response_id": 6,
    "tryHS": false,
    "obj": 3.9289988033506273,
    "SLOC": 31.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response7.txt_stdout.txt",
    "code_path": "problem_iter8_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Z-synergy tight-fit/util + entropy-adjusted frag penalty for adaptive packing.\n    \n    Combines z-score normalized tight-fit/utilization synergy with entropy-driven\n    fragmentation control. Dynamically balances packing density against bin state \n    variance to minimize long-tail fragmentation.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eps = 1e-9\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if not mask.any():\n        return scores\n\n    origin_cap = bins_remain_cap.max()\n    rem_cap = bins_remain_cap[mask]\n    leftover = rem_cap - item\n\n    # Tight-fit metric (v0)\n    inv_waste = 1.0 / (leftover + eps)\n    exp_tight = np.exp(-(leftover))\n    tight_fit = inv_waste + exp_tight\n    \n    # Utilization metric\n    bin_util = (origin_cap - rem_cap) / (origin_cap + eps)\n\n    # Z-score normalization across candidate bins\n    t_mean, t_std = tight_fit.mean(), tight_fit.std()\n    u_mean, u_std = bin_util.mean(), bin_util.std()\n    z_tight = (tight_fit - t_mean) / (t_std + eps)\n    z_util = (bin_util - u_mean) / (u_std + eps)\n\n    # Synergy (multiplicative interaction with normalized amplification)\n    synergy = z_tight * (1 + z_util)  # Capture utilization-boosted tightness\n    \n    # System entropy measurement (capacity spread)\n    sys_entropy = bins_remain_cap.std() / (origin_cap + eps)\n    \n    # Fragmentation penalty (smoothly scales with small leftover spaces)\n    frag_penalty = -np.expm1(-leftover / (origin_cap + eps))\n    \n    # Adaptive frag weight: higher entropy \u2192 heavier penalty for fragmentation\n    frag_weight = 0.15 * (1.0 + sys_entropy) \n    \n    # Final score computation\n    scores_masked = synergy - frag_weight * frag_penalty\n    scores[mask] = scores_masked\n    \n    return scores",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 31.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response8.txt_stdout.txt",
    "code_path": "problem_iter8_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Multi-objective priority with adaptive normalization, entropy-aware variance control, \n    and reinforcement-inspired dynamics. Blends z-score metrics, gradient-driven enhancements, \n    and variance-based entropy penalties for robust online bin selection.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    remaining = bins_remain_cap.astype(np.float64)\n    orig_cap = np.max(remaining)\n    \n    # Dynamic blending parameter\n    eligible_remaining = remaining[eligible]\n    median_rem = np.median(eligible_remaining)\n    relative_size = item / (median_rem + 1e-9)\n    blending = 1.0 / (1.0 + np.exp(-5 * (relative_size - 0.5)))\n    \n    # Core metrics\n    leftover = remaining - item\n    fit_metric = 1.0 / (leftover + 1e-9)                # Tightest fit preference\n    space_metric = remaining                           # Capacity preservation\n    tightness = item / (remaining + 1e-9)               # Local fit severity\n    bin_utilization = (orig_cap - remaining) / (orig_cap + 1e-9)  # Global bin status\n    \n    # Z-score normalization\n    fit_mean, fit_std = fit_metric[eligible].mean(), fit_metric[eligible].std()\n    fit_z = (fit_metric - fit_mean) / (fit_std + 1e-9)\n    \n    space_mean, space_std = space_metric[eligible].mean(), space_metric[eligible].std()\n    space_z = (space_metric - space_mean) / (space_std + 1e-9)\n    \n    # Primary score with adaptive balancing\n    primary_score = blending * fit_z + (1 - blending) * space_z\n    \n    # Nonlinear modifiers\n    residual_mod = -np.log(1 + leftover / (orig_cap + 1e-9))  # Elastic space penalty\n    efficiency_bonus = residual_mod * tightness / (np.sqrt(bin_utilization + 1e-9) + 1)\n    \n    # Dynamic boosting mechanism\n    enhancer = np.exp(2 * tightness) * np.clip((1 - bin_utilization)**3, 0, 1)\n    \n    # System-wide feedback\n    sys_stats = remaining.mean(), remaining.std()\n    sys_cv = sys_stats[1] / sys_stats[0] if sys_stats[0] > 1e-9 else float('inf')\n    \n    # Reinforcement learning inspiration\n    fragility = np.abs(orig_cap - remaining) / (orig_cap + 1e-9)\n    reinforcement = 1 + np.clip((1 - relative_size)**2 * (1 - bin_utilization) * fragility, 0, 3)\n    \n    # Entropy-aware tiebreaker (variance delta)\n    bin_count = remaining.size\n    sum_total = remaining.sum()\n    sum_sq = (remaining**2).sum()\n    new_sum_sq = sum_sq - (2 * item) * remaining + item**2  # \u0394(sum squares) per bin\n    mu_new = (sum_total - item) / bin_count\n    var_new = (new_sum_sq / bin_count) - mu_new**2\n    \n    var_sensitivity = 0.1 * (1 + np.sqrt(sys_cv + 1e-9))\n    var_term = var_sensitivity * (-var_new) / (np.sqrt(np.abs(var_new) + 1e-9) + 1e-5)\n    \n    # Final priority calculation\n    return np.where(\n        eligible,\n        (primary_score + 0.75 * efficiency_bonus) * enhancer * reinforcement + var_term,\n        -np.inf\n    )",
    "response_id": 8,
    "tryHS": false,
    "obj": 35.50059832469087,
    "SLOC": 31.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response9.txt_stdout.txt",
    "code_path": "problem_iter8_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid heuristic combining z-normalized fit alignment, exponential synergy, and entropy-sensitive frag control.\n    Dynamic weights adapt to item size and system stability via cross-metric variance analysis.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = bins_remain_cap.max()\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    if not mask.any():\n        return scores\n\n    eps = 1e-9\n    remaining = bins_remain_cap[mask]\n    leftover = remaining - item\n    orig_utilization = 1.0 - (remaining / orig_cap)\n    \n    # Core tightness metrics (v0's foundation)\n    inv_fit = 1.0 / (leftover + eps)\n    exp_tight = np.exp(-(leftover))\n    tight_base = inv_fit + exp_tight\n    \n    # Z-score normalization across all bins (state-aware alignment)\n    sys_avg = bins_remain_cap.mean()\n    sys_std = bins_remain_cap.std()\n    zscore_system = (sys_avg - leftover) / (sys_std + eps)  # Prefer fit with system average\n    fit_alignment = np.clip(zscore_system, -2, 2)\n    \n    # Fragmentation control (entropy-sensitive from v1)\n    system_cv = sys_std / (sys_avg + eps)\n    frag_scale = np.where(leftover < sys_avg*0.05,  # Check for critical fragmentation\n                         1.0 + (1.0 / (leftover + eps)), \n                         0.0)\n    \n    # Dynamic weights via hybrid adaptability\n    item_ratio = item / (orig_cap + eps)\n    tight_weight = 0.6 + 0.4*(item_ratio**1.5)  # Prioritize tightness for large items\n    align_weight = 0.2 + 0.5*system_cv          # Higher weight when system unstable\n    frag_weight = 0.8 * np.log1p(system_cv*4)  # Intensify frag penalty with entropy\n    \n    # Exponential synergy boost (analysis recommendation)\n    util_exponent = np.exp(orig_utilization)  # Aggressive enhancement for used bins\n    frag_penalty = frag_scale * frag_weight\n    \n    # Score assembly with hierarchical components\n    scores[mask] = ((tight_base * tight_weight + fit_alignment * align_weight) * util_exponent) - frag_penalty\n    \n    return scores",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 31.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  }
]