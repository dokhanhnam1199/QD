```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Multi-objective priority function with dynamic sigmoid blending, adaptive normalization, 
    and contextual reinforcement-inspired adjustments for online BPP.
    """
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    orig_cap = np.max(bins_remain_cap)
    if orig_cap <= 1e-9 or item <= 1e-9:
        return np.where(
            bins_remain_cap >= item,
            1.0 / (bins_remain_cap - item + 1e-9),
            -np.inf
        )
    
    eligible = bins_remain_cap >= item
    if not np.any(eligible):
        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)

    # Core metrics
    leftover = bins_remain_cap - item
    fit_quality = 1.0 / (leftover + 1e-9)
    space_quality = bins_remain_cap
    utilization = (orig_cap - bins_remain_cap) / orig_cap
    
    # Contextual bottleneck analysis
    eligible_caps = bins_remain_cap[eligible]
    med_cap = np.median(eligible_caps)
    tightness = item / (bins_remain_cap + 1e-9)
    
    # Dynamic best-worst blending via logistic sensitivity
    x = item / (med_cap + 1e-9)
    blending = 1.0 / (1 + np.exp(-5 * (x - 0.5)))  # Midpoint at item=0.5*med_cap
    
    # Z-score normalization across multiple axes
    fit_sub = fit_quality[eligible]
    space_sub = space_quality[eligible]
    fit_norm = (fit_quality - np.mean(fit_sub)) / (np.std(fit_sub) + 1e-9)
    space_norm = (space_quality - np.mean(space_sub)) / (np.std(space_sub) + 1e-9)
    
    # Primary multi-objective score with gap-aware bonus
    primary = blending * fit_norm + (1 - blending) * space_norm
    
    # Non-linear efficiency modifier for residual space
    residual_efficiency = -np.log(1 + leftover / (orig_cap + 1e-9))
    secondary = residual_efficiency * (tightness / (np.sqrt(utilization + 1e-9) + 1))
    
    # Utilization enhancer with adaptive regulation
    fragility = np.abs(orig_cap - bins_remain_cap) / (orig_cap + 1e-9)
    confidence = np.clip((1 - utilization)**3, 0, 1)
    enhancer = np.exp(2 * tightness) * confidence
    
    # Reinforcement-inspired dynamic calibration
    reinforcement_gain = 1 + np.clip((1 - x)**2 * (1 - utilization) * fragility, 0, 3)
    
    # Hierarchical priority combination
    return np.where(
        eligible,
        (primary + 0.5 * secondary) * enhancer * reinforcement_gain,
        -np.inf
    )
```
