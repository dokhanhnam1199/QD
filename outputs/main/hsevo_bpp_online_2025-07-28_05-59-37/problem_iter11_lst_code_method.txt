{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines z-score-normalized fit/cap metrics with exponential utilization enhancer \n    and entropy-aware tie-breaking via leftover volatility damping.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    leftover = bins_remain_cap - item\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    # Z-score normalization of fit and capacity\n    fit_quality = 1.0 / (leftover + 1e-9)\n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    \n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    \n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    # Dynamic hybrid weighting and exponential enhancement\n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    enhancer = np.exp(utilization * tightness)\n    \n    # Entropy-aware tie-breaking: small term to penalize fragmentation volatility\n    tie_breaker = 0.05 * np.exp(-leftover)  # Weak exponential damping of leftover space\n    \n    # Final layered prioritization with perturbed thresholds\n    return np.where(\n        eligible, \n        (primary_score * enhancer) + tie_breaker, \n        -np.inf\n    )\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines z-score-normalized fit/cap metrics with exponential utilization enhancer \n    and entropy-aware tie-breaking via leftover volatility damping.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    leftover = bins_remain_cap - item\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    # Z-score normalization of fit and capacity\n    fit_quality = 1.0 / (leftover + 1e-9)\n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    \n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    \n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    # Dynamic hybrid weighting and exponential enhancement\n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    enhancer = np.exp(utilization * tightness)\n    \n    # Entropy-aware tie-breaking: small term to penalize fragmentation volatility\n    tie_breaker = 0.05 * np.exp(-leftover)  # Weak exponential damping of leftover space\n    \n    # Final layered prioritization with perturbed thresholds\n    return np.where(\n        eligible, \n        (primary_score * enhancer) + tie_breaker, \n        -np.inf\n    )\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines z-score-normalized fit/cap metrics with exponential utilization enhancer \n    and entropy-aware tie-breaking via leftover volatility damping.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            1.0 / (bins_remain_cap - item + 1e-9),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not eligible.any():\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    leftover = bins_remain_cap - item\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    tightness = item / (bins_remain_cap + 1e-9)\n    \n    # Z-score normalization of fit and capacity\n    fit_quality = 1.0 / (leftover + 1e-9)\n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    \n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    \n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    # Dynamic hybrid weighting and exponential enhancement\n    primary_score = tightness * z_fit + (1.0 - tightness) * z_cap\n    enhancer = np.exp(utilization * tightness)\n    \n    # Entropy-aware tie-breaking: small term to penalize fragmentation volatility\n    tie_breaker = 0.05 * np.exp(-leftover)  # Weak exponential damping of leftover space\n    \n    # Final layered prioritization with perturbed thresholds\n    return np.where(\n        eligible, \n        (primary_score * enhancer) + tie_breaker, \n        -np.inf\n    )\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Z-synergy tight-fit/util + entropy-adjusted frag penalty for adaptive packing.\n    \n    Combines z-score normalized tight-fit/utilization synergy with entropy-driven\n    fragmentation control. Dynamically balances packing density against bin state \n    variance to minimize long-tail fragmentation.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eps = 1e-9\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if not mask.any():\n        return scores\n\n    origin_cap = bins_remain_cap.max()\n    rem_cap = bins_remain_cap[mask]\n    leftover = rem_cap - item\n\n    # Tight-fit metric (v0)\n    inv_waste = 1.0 / (leftover + eps)\n    exp_tight = np.exp(-(leftover))\n    tight_fit = inv_waste + exp_tight\n    \n    # Utilization metric\n    bin_util = (origin_cap - rem_cap) / (origin_cap + eps)\n\n    # Z-score normalization across candidate bins\n    t_mean, t_std = tight_fit.mean(), tight_fit.std()\n    u_mean, u_std = bin_util.mean(), bin_util.std()\n    z_tight = (tight_fit - t_mean) / (t_std + eps)\n    z_util = (bin_util - u_mean) / (u_std + eps)\n\n    # Synergy (multiplicative interaction with normalized amplification)\n    synergy = z_tight * (1 + z_util)  # Capture utilization-boosted tightness\n    \n    # System entropy measurement (capacity spread)\n    sys_entropy = bins_remain_cap.std() / (origin_cap + eps)\n    \n    # Fragmentation penalty (smoothly scales with small leftover spaces)\n    frag_penalty = -np.expm1(-leftover / (origin_cap + eps))\n    \n    # Adaptive frag weight: higher entropy \u2192 heavier penalty for fragmentation\n    frag_weight = 0.15 * (1.0 + sys_entropy) \n    \n    # Final score computation\n    scores_masked = synergy - frag_weight * frag_penalty\n    scores[mask] = scores_masked\n    \n    return scores\n\n[Heuristics 5th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Z-score normalized synergy of tight-fit and utilization + entropy-aware balancing via system variance.\"\"\"\n    eps = 1e-9\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    \n    if not mask.any():\n        return scores\n    \n    C_est = bins_remain_cap.max() if bins_remain_cap.size > 0 else 1.0\n    remaining = bins_remain_cap[mask]\n    \n    # Core metrics\n    leftover = remaining - item + eps\n    inv_leftover = 1.0 / leftover\n    utilization = item / (remaining + eps)\n    \n    # Multi-metric synergy normalized per-z-score\n    multiplicative_term = inv_leftover * utilization\n    z_ilu = (multiplicative_term - multiplicative_term.mean()) / (multiplicative_term.std() + eps)\n    \n    # Adaptive fragmentation penalty with exponential waste decay\n    norm_leftover = leftover / C_est\n    exp_waste = np.exp(-norm_leftover)\n    z_exp = (exp_waste - exp_waste.mean()) / (exp_waste.std() + eps)\n    \n    # Entropy control via load balance (normalized filled fraction)\n    filled_frac = (C_est - remaining) / C_est\n    system_var = np.var(bins_remain_cap) / (C_est**2 + eps)\n    load_balance_term = filled_frac * (1 + system_var)\n    z_balance = (load_balance_term - load_balance_term.mean()) / (load_balance_term.std() + eps)\n    \n    # Hierarchical score with entropy-weighted tie-breaking\n    scores[mask] = z_ilu + 0.1 * z_exp + 0.05 * system_var * z_balance\n    return scores\n\n[Heuristics 6th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybridized priority function combining multi-axis Z-normalization, \n    gradient-aware boosting, and perturbed variance entropy control.\n    - Implements cross-metric variance weighting\n    - Embeds fragility-aware tie-breaking heuristics\n    - Uses adaptive item classification based on percentile thresholds\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    # Zero-mass item handling with bias toward future utilization\n    if item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= 0,\n            1e-4 * bins_remain_cap - 1e-7 * bins_remain_cap**2,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # Metric primitives for bin assessment\n    leftover = bins_remain_cap - item\n    fit_quality = 1.0 / (leftover + 1e-9)\n    tightness = item / (bins_remain_cap + 1e-9)\n    utilization = (1.0 - bins_remain_cap / bins_remain_cap.max()) if bins_remain_cap.max() > 0 else 0.0\n    \n    # Dynamic normalization core (state-aware Z-transformation)\n    def z_scores(vec, mask):\n        subset = vec[mask] if mask.any() else vec\n        return (vec - subset.mean()) / (subset.std() + 1e-9) if subset.size > 1 else 0.0\n    \n    # Fundamental scoring layers\n    z_fit = z_scores(fit_quality, eligible)\n    z_tight = z_scores(tightness, eligible)\n    z_left = z_scores(leftover, eligible)\n    \n    # Metric variance analysis for adaptive fusion\n    var_fit = 1e-9 + (fit_quality[eligible]).var()\n    var_tight = 1e-9 + (tightness[eligible]).var()\n    \n    # Primary scorer: variance-weighted Z-transformation fusion\n    var_ratio = var_fit / (var_fit + var_tight)\n    primary = var_ratio * z_fit + (1 - var_ratio) * z_tight\n    \n    # Gradient-aware multiplicative boosting\n    sys_skew = z_scores(bins_remain_cap, np.ones_like(eligible, dtype=bool))  # System-wide Z\n    # Dynamic item classification (median as threshold)\n    median_cap = np.median(bins_remain_cap[eligible]) if eligible.sum() > 0 else item\n    boost_magnitude = (1.25 if item > median_cap else 0.85)\n    gradient_boost = boost_magnitude * np.exp(0.5 * (z_tight + utilization))\n    \n    # Multi-scale balance control with entropy decay\n    system_avg, system_std = bins_remain_cap.mean(), bins_remain_cap.std()\n    balance_term = -abs((leftover - system_avg) / (system_std + 1e-9))\n    \n    # Fragility layer (leftover usability estimation)\n    fragility_metric = abs(leftover - 0.5 * system_avg) * (1 + abs(sys_skew))\n    fragility_penalty = 0.1 * fragility_metric * z_left\n    \n    # Cross-metric decay envelope      \n    decay_envelope = np.exp(-0.1 * (leftover / (bins_remain_cap.max() + 1e-9)))\n    \n    # Final priority aggregation - multi-phase improvement cascade\n    priority = primary * gradient_boost\n    priority += 0.05 * system_std * balance_term\n    priority -= fragility_penalty\n    priority *= decay_envelope  # Add contextual scaling\n\n    # Epsilon perturbation to break degeneracy\n    priority += 1e-8 * np.random.randn(*bins_remain_cap.shape)\n\n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 7th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    State-aware, entropy-sensitive priority with gradient-enhanced scoring and multi-modal tiebreakers.\n    Combines dynamic Z-scores, entropy-regulated fill dynamics, and perturbed balance hierarchies.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n\n    orig_max = np.max(bins_remain_cap)\n    if orig_max < 1e-9 or item < 1e-9:\n        return np.where(bins_remain_cap >= item, \n                       1.0 / (bins_remain_cap - item + 1e-4) - 1e-6 * bins_remain_cap,\n                       -np.inf)\n\n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n\n    # System context tracking for adaptive normalization\n    sys_mean, sys_std = np.mean(bins_remain_cap), np.std(bins_remain_cap)\n    elig_caps = bins_remain_cap[eligible]\n    elig_centers = bins_remain_cap - (orig_max * 0.5)\n\n    # Core allocation metrics\n    residual = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-6)\n    fit_power = 1.0 / (residual + 1e-6)\n    \n    # Normalized scoring layers\n    z_score = (fit_power - np.mean(fit_power)) / (np.std(fit_power) + 1e-6)  # Dynamic Z-score\n    \n    # Entropy-sensitive decay weight\n    fill_entropy = np.abs(residual - sys_mean + item / 2) * np.sqrt(sys_std + 1e-6)\n    entropy_weight = 0.05 * (sys_std / (sys_mean + 1e-6)**0.75)\n\n    # Gradient-enhanced utilization tracking\n    utilization = 1.0 - (bins_remain_cap / orig_max)\n    utilization_curve = np.log(orig_max - utilization * orig_max + 1.0)  # Smooth nonlinearity\n    \n    # Cross-metric variance adaptive weighting\n    def calc_variance_factor(metric):\n        return np.std(metric) / (np.abs(np.mean(metric)) + 1e-6)\n\n    z_weight = calc_variance_factor(z_score) * 0.6\n    tightness_weight = calc_variance_factor(1-tightness) * 0.4\n    \n    # Hierarchical scoring with exponential decay\n    base_score = z_score * z_weight * utilization_curve\n    efficiency_term = tightness * tightness_weight * (orig_max - bins_remain_cap)\n    \n    # Tiered entropy penalty with state conditioning\n    tiered_entropy = np.where(\n        residual > sys_std * 1.5,  # High-margin bins\n        fill_entropy * 0.1,\n        np.where(\n            residual < -sys_std * 0.5,  # Already overflowed bins\n            fill_entropy * 1.5,\n            fill_entropy * 0.5\n        )\n    )\n\n    # Final composition with fragility tiebreaker\n    final_score = np.where(\n        eligible,\n        base_score + efficiency_term - tiered_entropy * entropy_weight,\n        -np.inf\n    )\n    \n    return final_score\n\n[Heuristics 8th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Z-score normalized synergy of tight-fit and utilization + entropy-aware balancing via system variance.\"\"\"\n    eps = 1e-9\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf)\n    \n    if not mask.any():\n        return scores\n    \n    C_est = bins_remain_cap.max() if bins_remain_cap.size > 0 else 1.0\n    remaining = bins_remain_cap[mask]\n    \n    # Core metrics\n    leftover = remaining - item + eps\n    inv_leftover = 1.0 / leftover\n    utilization = item / (remaining + eps)\n    \n    # Multi-metric synergy normalized per-z-score\n    multiplicative_term = inv_leftover * utilization\n    z_ilu = (multiplicative_term - multiplicative_term.mean()) / (multiplicative_term.std() + eps)\n    \n    # Adaptive fragmentation penalty with exponential waste decay\n    norm_leftover = leftover / C_est\n    exp_waste = np.exp(-norm_leftover)\n    z_exp = (exp_waste - exp_waste.mean()) / (exp_waste.std() + eps)\n    \n    # Entropy control via load balance (normalized filled fraction)\n    filled_frac = (C_est - remaining) / C_est\n    system_var = np.var(bins_remain_cap) / (C_est**2 + eps)\n    load_balance_term = filled_frac * (1 + system_var)\n    z_balance = (load_balance_term - load_balance_term.mean()) / (load_balance_term.std() + eps)\n    \n    # Hierarchical score with entropy-weighted tie-breaking\n    scores[mask] = z_ilu + 0.1 * z_exp + 0.05 * system_var * z_balance\n    return scores\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Z-synergy tight-fit/util + entropy-adjusted frag penalty for adaptive packing.\n    \n    Combines z-score normalized tight-fit/utilization synergy with entropy-driven\n    fragmentation control. Dynamically balances packing density against bin state \n    variance to minimize long-tail fragmentation.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eps = 1e-9\n    mask = bins_remain_cap >= item\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if not mask.any():\n        return scores\n\n    origin_cap = bins_remain_cap.max()\n    rem_cap = bins_remain_cap[mask]\n    leftover = rem_cap - item\n\n    # Tight-fit metric (v0)\n    inv_waste = 1.0 / (leftover + eps)\n    exp_tight = np.exp(-(leftover))\n    tight_fit = inv_waste + exp_tight\n    \n    # Utilization metric\n    bin_util = (origin_cap - rem_cap) / (origin_cap + eps)\n\n    # Z-score normalization across candidate bins\n    t_mean, t_std = tight_fit.mean(), tight_fit.std()\n    u_mean, u_std = bin_util.mean(), bin_util.std()\n    z_tight = (tight_fit - t_mean) / (t_std + eps)\n    z_util = (bin_util - u_mean) / (u_std + eps)\n\n    # Synergy (multiplicative interaction with normalized amplification)\n    synergy = z_tight * (1 + z_util)  # Capture utilization-boosted tightness\n    \n    # System entropy measurement (capacity spread)\n    sys_entropy = bins_remain_cap.std() / (origin_cap + eps)\n    \n    # Fragmentation penalty (smoothly scales with small leftover spaces)\n    frag_penalty = -np.expm1(-leftover / (origin_cap + eps))\n    \n    # Adaptive frag weight: higher entropy \u2192 heavier penalty for fragmentation\n    frag_weight = 0.15 * (1.0 + sys_entropy) \n    \n    # Final score computation\n    scores_masked = synergy - frag_weight * frag_penalty\n    scores[mask] = scores_masked\n    \n    return scores\n\n[Heuristics 10th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Z-score normalized balance with exponential utilization boosting and adaptive component weights.\n    \n    Combines dynamic BF/WF tendencies separated by item class with entropy-driven balance control \n    and hierarchical tie-breakers using:\n    - Adaptive weights based on system CV and leftover variance\n    - Exponential utilization enhancement for dynamic differentiation\n    - Multi-metric synergy (tightness + balance + utilization)\n    - Z-score normalization of key components\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    valid_mask = bins_remain_cap >= item\n    priority = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if not np.any(valid_mask):\n        return priority\n    \n    # System metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 1e-9 else 0.0\n    \n    # Item classification\n    large_item = item > system_avg  # Threshold at system mean\n    \n    # Primary metrics\n    leftover = bins_remain_cap - item\n    valid_leftover = leftover[valid_mask]\n    \n    # Exponential utilization enhancement\n    utilization = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    np.divide(item, bins_remain_cap, where=valid_mask, out=utilization)\n    exp_util = np.exp(utilization * (1.5 if large_item else 2.5))  # Dynamic scaling\n    \n    # Z-score normalized components\n    tightness_z = (-leftover - system_avg) / (system_std + 1e-9)  # Negative space\n    bin_cap_z = (bins_remain_cap - system_avg) / (system_std + 1e-9)  # Fullness\n    \n    # Core terms\n    tightness_component = tightness_z - 1e-5 * bin_cap_z  # Encourage tighter fits\n    \n    # System balance term\n    load_balance = -np.abs(leftover - system_avg) / (system_std + 1e-9)\n    balance_weight = 1.5 * np.sqrt(system_cv) * (1.5 if not large_item else 1.0)\n    \n    # Entropy-regularized tiebreaker\n    leftover_std = np.std(valid_leftover) if len(valid_leftover) > 1 else 1e-6\n    util_weight = 0.8 / (leftover_std + 1e-9)\n    \n    # Synergistic combination\n    priority[valid_mask] = (\n        tightness_component[valid_mask] * (1.2 if large_item else 1.0)\n        + balance_weight * load_balance[valid_mask]\n        + util_weight * exp_util[valid_mask]\n    )\n    \n    return priority\n\n[Heuristics 11th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Z-score normalized balance with exponential utilization boosting and adaptive component weights.\n    \n    Combines dynamic BF/WF tendencies separated by item class with entropy-driven balance control \n    and hierarchical tie-breakers using:\n    - Adaptive weights based on system CV and leftover variance\n    - Exponential utilization enhancement for dynamic differentiation\n    - Multi-metric synergy (tightness + balance + utilization)\n    - Z-score normalization of key components\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    valid_mask = bins_remain_cap >= item\n    priority = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    if not np.any(valid_mask):\n        return priority\n    \n    # System metrics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9) if system_avg > 1e-9 else 0.0\n    \n    # Item classification\n    large_item = item > system_avg  # Threshold at system mean\n    \n    # Primary metrics\n    leftover = bins_remain_cap - item\n    valid_leftover = leftover[valid_mask]\n    \n    # Exponential utilization enhancement\n    utilization = np.zeros_like(bins_remain_cap, dtype=np.float64)\n    np.divide(item, bins_remain_cap, where=valid_mask, out=utilization)\n    exp_util = np.exp(utilization * (1.5 if large_item else 2.5))  # Dynamic scaling\n    \n    # Z-score normalized components\n    tightness_z = (-leftover - system_avg) / (system_std + 1e-9)  # Negative space\n    bin_cap_z = (bins_remain_cap - system_avg) / (system_std + 1e-9)  # Fullness\n    \n    # Core terms\n    tightness_component = tightness_z - 1e-5 * bin_cap_z  # Encourage tighter fits\n    \n    # System balance term\n    load_balance = -np.abs(leftover - system_avg) / (system_std + 1e-9)\n    balance_weight = 1.5 * np.sqrt(system_cv) * (1.5 if not large_item else 1.0)\n    \n    # Entropy-regularized tiebreaker\n    leftover_std = np.std(valid_leftover) if len(valid_leftover) > 1 else 1e-6\n    util_weight = 0.8 / (leftover_std + 1e-9)\n    \n    # Synergistic combination\n    priority[valid_mask] = (\n        tightness_component[valid_mask] * (1.2 if large_item else 1.0)\n        + balance_weight * load_balance[valid_mask]\n        + util_weight * exp_util[valid_mask]\n    )\n    \n    return priority\n\n[Heuristics 12th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, \n                threshold_negligible_capacity: float = 0.006093226508804939,\n                threshold_negligible_item: float = 0.008865879071201838,\n                epsilon_edge_denominator: float = 0.005115823931161712,\n                penalty_factor_edge: float = 0.00993031308328483,\n                epsilon_tightness_denominator: float = 0.0010791111008370765,\n                epsilon_zscores_denominator: float = 0.0017621098539563904,\n                epsilon_system_mean: float = 0.007376300906537722,\n                balance_weight_base: float = 5.229103652038248,\n                small_item_multiplier: float = 3.1515700727321097,\n                large_item_multiplier: float = 1.9734277003678775) -> np.ndarray:\n    \"\"\"\n    Z-optimized fit combining exp-utilized tightness with system-wide entropy scaling (higher priority to bins that reduce overall fragmentation).\n    Hybridizes v0 adaptive normalization and v1 entropy-aware balance.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n\n[Heuristics 13th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n\n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        base_score = np.where(bins_remain_cap >= item,\n            (bins_remain_cap - item) * -1e-5 + (bins_remain_cap * 1e-7),\n            -np.inf)\n        return base_score\n\n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n\n    leftover = bins_remain_cap - item\n    utilization = (orig_cap - bins_remain_cap) / orig_cap\n    tightness = item / (bins_remain_cap + 1e-9)\n    elig_tight = tightness[eligible]\n    elig_left = leftover[eligible]\n    elig_util = utilization[eligible]\n    elig_rc = bins_remain_cap[eligible]\n\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9)\n\n    volatilify = lambda x: (x - system_avg) / (system_std + 1e-9)\n    item_vol_score = volatilify(item)\n    volatile_item = abs(item_vol_score) > 1.0\n    large_item = item_vol_score > 0.0\n\n    perturbed_z_vol = lambda x: (x - x.mean()) / (x.std(ddof=1) + 1e-9) * (1 + 1e-5 * x.std())\n\n    fit_quality = 1.0 / (leftover + 1e-9)\n    elig_fit = fit_quality[eligible]\n\n    tight_z = perturbed_z_vol(elig_tight)\n    fit_z = perturbed_z_vol(elig_fit)\n    util_z = perturbed_z_vol(elig_util)\n    cap_z = perturbed_z_vol(elig_rc)\n\n    fit_var_score = max(0.1, fit_z.var(ddof=1, dtype=np.float64))\n    tight_var_score = max(0.1, tight_z.var(ddof=1, dtype=np.float64))\n    weight_fit = tight_var_score / (fit_var_score + tight_var_score)\n    weight_tight = fit_var_score / (fit_var_score + tight_var_score)\n\n    primary_layers = (\n        weight_tight * (1 + util_z) ** 2 * elig_tight +\n        weight_fit * (1 - elig_tight) * elig_fit\n    )\n    primary_score = (primary_layers - np.min(primary_layers))/(np.ptp(primary_layers) + 1e-9)\n\n    base_util_term = np.where(\n        large_item,\n        np.exp(util_z + tight_z),\n        np.sqrt((1.0 - util_z) * (1.0 - tight_z) + 1e-9)\n    )\n    dynamic_gamma = np.clip(0.8 * tight_z + 0.2 * (1 - system_cv), 0.5, 1.5)\n    enhancer = base_util_term ** dynamic_gamma\n\n    fragility_mask = np.logical_or(bins_remain_cap > (system_avg + 2 * system_std), bins_remain_cap < (system_avg - 2 * system_std))\n    fragility_score = np.where(\n        fragility_mask,\n        system_std ** 2 / (np.abs(bins_remain_cap - system_avg) + 1e-9),\n        1.0\n    )[eligible]\n\n    balance_term = -np.abs(leftover - system_avg) * (1.0 + system_cv ** 2)\n    balance_decay = np.exp(-0.2 * (system_cv * tight_z + system_std / (system_avg - item + 1e-9)))\n\n    entropy_component = balance_term * balance_decay * fragility_score\n    unnormalized_score = (primary_score * enhancer) + 0.1 * entropy_component\n\n    seed = int(np.abs(item * 1e5) % 1e9)\n    np.random.seed(seed)\n    perturbation = 1e-8 * np.random.normal(size=leftover.shape)\n\n    priority = unnormalized_score + perturbation[eligible]\n\n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 14th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    \n    # Edge cases: negligible item or bin capacity\n    if item <= 1e-9 or orig_cap <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            0.1 / (bins_remain_cap + 1e-9) - 0.01 * bins_remain_cap,\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # Core bin metrics for eligible bins\n    e_rc = bins_remain_cap[eligible]\n    e_leftover = e_rc - item\n    e_tightness = item / e_rc\n    e_utilization = (orig_cap - e_rc) / orig_cap\n    \n    # System-wide statistics\n    system_avg = bins_remain_cap.mean()\n    system_std = bins_remain_cap.std()\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    # Normalized metric layers\n    fit_quality = 1.0 / (e_leftover + 1e-9)\n    z_fit = (fit_quality - fit_quality.mean()) / (fit_quality.std() + 1e-9)\n    \n    z_rc = (e_rc - e_rc.mean()) / (e_rc.std() + 1e-9)\n    \n    # Dynamic hybridization based on item tightness distribution\n    alpha = np.clip(1.0 - e_tightness.mean(), 0.3, 0.7)\n    hybrid_score = alpha * z_fit + (1 - alpha) * z_rc\n    \n    # Gradient-targeted exponential modulation\n    item_reso_factor = 1.0 + np.sqrt((item / system_avg))\n    enhancer = np.exp(e_utilization * e_tightness * item_reso_factor)\n    \n    # Entrophy-aware balance with covariance-based weighting\n    balance_term = -np.abs(e_leftover - system_avg)\n    \n    if len(e_leftover) > 1:\n        covar_factor = np.cov(fit_quality, balance_term)[0, 1] + 1e-9\n        weight_modulation = np.sign(covar_factor) * np.log(1.0 + system_cv / np.abs(covar_factor))\n    else:\n        weight_modulation = 1.0\n    \n    balance_weight = 0.1 * system_cv * weight_modulation\n    \n    # Fragility mitigation with dynamic thresholding\n    fragility_mask = (e_leftover < (system_avg - system_std))\n    fragility_penalty = np.zeros_like(balance_term)\n    fragility_penalty[fragility_mask] = -1e4 * system_cv\n    \n    # Final priority assembly with epsilon-perturbed tie-breaking\n    priority_core = (hybrid_score * enhancer + balance_weight * balance_term + fragility_penalty)\n    tiebreaker = 1e-10 * (1.0 / (e_leftover + 1e-9))\n    final_priority = priority_core + tiebreaker\n    \n    full_array = np.full_like(bins_remain_cap, -np.inf)\n    full_array[eligible] = final_priority\n    \n    return full_array\n\n[Heuristics 15th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    v2: State-aware normalization with cross-metric variance adaptation, gradient-boosted enhancer, \n    and multi-layer entropy-sensitive scoring with fragility control and perturbed thresholds.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            0.1 * bins_remain_cap / (orig_cap + 1e-5) - (bins_remain_cap - item),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Metric initialization\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    fit_quality = 1.0 / (leftover + 1e-9)\n    \n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    elig_tight = tightness[eligible]\n    \n    # State-aware Z-score with variance-controlled calibration\n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    \n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    # Cross-metric variance weighting (higher variance metric dominates)\n    var_fit = np.var(elig_fit)\n    var_cap = np.var(elig_cap)\n    cross_weight = var_cap / (var_fit + var_cap + 1e-9) if (var_fit + var_cap) > 1e-7 else 0.5\n    \n    # Composite score with adaptive curvature\n    curvature_factor = 1.0 + np.arctan(np.mean(z_fit) - np.mean(z_cap))\n    composite_z = cross_weight * z_fit + (1 - cross_weight) * z_cap * curvature_factor\n    \n    # Gradient-driven dynamic enhancer\n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap)\n    tight_density = np.clip(1.0 - (leftover - system_avg) / (3 * system_std + 1e-9), 0, 1)\n    \n    grad_scale = np.where(item > system_avg,\n                          1.0 + 2.0 * tightness * system_std,\n                          1.0 + tightness)\n    \n    enhancer = np.exp(composite_z * tight_density * grad_scale)\n    \n    # Entropy-sensitive load balance with perturbed thresholds\n    balance_term = -(np.abs(leftover - system_avg) + 1e-5 * (leftover - system_avg))\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    # Fragility-sensitive reinforcement\n    fragility_factor = 1.0 - np.exp(-leftover / (0.25 * orig_cap + 1e-9))\n    fragility_term = np.where(leftover > 0.1 * orig_cap,\n                              -0.1 * fragility_factor,\n                              -0.3 * fragility_factor)\n    \n    # Dynamic weight cross-metric adaptation\n    var_zfit = np.var(z_fit[eligible])\n    var_zcap = np.var(z_cap[eligible])\n    weight_balance = 0.1 * system_cv * var_zfit / (var_zfit + var_zcap + 1e-9)\n    weight_fragility = 0.05 * system_cv * (1 - system_cv) * (var_zcap / (var_zfit + var_zcap + 1e-9))\n    \n    # Final priority with hierarchical reinforcement\n    priority = composite_z * enhancer + weight_balance * balance_term + weight_fragility * fragility_term\n    \n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 16th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    v2: State-aware normalization with cross-metric variance adaptation, gradient-boosted enhancer, \n    and multi-layer entropy-sensitive scoring with fragility control and perturbed thresholds.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    orig_cap = np.max(bins_remain_cap)\n    if orig_cap <= 1e-9 or item <= 1e-9:\n        return np.where(\n            bins_remain_cap >= item,\n            0.1 * bins_remain_cap / (orig_cap + 1e-5) - (bins_remain_cap - item),\n            -np.inf\n        )\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    # Metric initialization\n    leftover = bins_remain_cap - item\n    tightness = item / (bins_remain_cap + 1e-9)\n    fit_quality = 1.0 / (leftover + 1e-9)\n    \n    elig_fit = fit_quality[eligible]\n    elig_cap = bins_remain_cap[eligible]\n    elig_tight = tightness[eligible]\n    \n    # State-aware Z-score with variance-controlled calibration\n    mean_fit, std_fit = np.mean(elig_fit), np.std(elig_fit)\n    z_fit = (fit_quality - mean_fit) / (std_fit + 1e-9)\n    \n    mean_cap, std_cap = np.mean(elig_cap), np.std(elig_cap)\n    z_cap = (bins_remain_cap - mean_cap) / (std_cap + 1e-9)\n    \n    # Cross-metric variance weighting (higher variance metric dominates)\n    var_fit = np.var(elig_fit)\n    var_cap = np.var(elig_cap)\n    cross_weight = var_cap / (var_fit + var_cap + 1e-9) if (var_fit + var_cap) > 1e-7 else 0.5\n    \n    # Composite score with adaptive curvature\n    curvature_factor = 1.0 + np.arctan(np.mean(z_fit) - np.mean(z_cap))\n    composite_z = cross_weight * z_fit + (1 - cross_weight) * z_cap * curvature_factor\n    \n    # Gradient-driven dynamic enhancer\n    system_avg = np.mean(bins_remain_cap)\n    system_std = np.std(bins_remain_cap)\n    tight_density = np.clip(1.0 - (leftover - system_avg) / (3 * system_std + 1e-9), 0, 1)\n    \n    grad_scale = np.where(item > system_avg,\n                          1.0 + 2.0 * tightness * system_std,\n                          1.0 + tightness)\n    \n    enhancer = np.exp(composite_z * tight_density * grad_scale)\n    \n    # Entropy-sensitive load balance with perturbed thresholds\n    balance_term = -(np.abs(leftover - system_avg) + 1e-5 * (leftover - system_avg))\n    system_cv = system_std / (system_avg + 1e-9)\n    \n    # Fragility-sensitive reinforcement\n    fragility_factor = 1.0 - np.exp(-leftover / (0.25 * orig_cap + 1e-9))\n    fragility_term = np.where(leftover > 0.1 * orig_cap,\n                              -0.1 * fragility_factor,\n                              -0.3 * fragility_factor)\n    \n    # Dynamic weight cross-metric adaptation\n    var_zfit = np.var(z_fit[eligible])\n    var_zcap = np.var(z_cap[eligible])\n    weight_balance = 0.1 * system_cv * var_zfit / (var_zfit + var_zcap + 1e-9)\n    weight_fragility = 0.05 * system_cv * (1 - system_cv) * (var_zcap / (var_zfit + var_zcap + 1e-9))\n    \n    # Final priority with hierarchical reinforcement\n    priority = composite_z * enhancer + weight_balance * balance_term + weight_fragility * fragility_term\n    \n    return np.where(eligible, priority, -np.inf)\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Multi-objective priority with adaptive normalization, entropy-aware variance control, \n    and reinforcement-inspired dynamics. Blends z-score metrics, gradient-driven enhancements, \n    and variance-based entropy penalties for robust online bin selection.\n    \"\"\"\n    if bins_remain_cap.size == 0:\n        return np.array([], dtype=np.float64)\n    \n    eligible = bins_remain_cap >= item\n    if not np.any(eligible):\n        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)\n    \n    remaining = bins_remain_cap.astype(np.float64)\n    orig_cap = np.max(remaining)\n    \n    # Dynamic blending parameter\n    eligible_remaining = remaining[eligible]\n    median_rem = np.median(eligible_remaining)\n    relative_size = item / (median_rem + 1e-9)\n    blending = 1.0 / (1.0 + np.exp(-5 * (relative_size - 0.5)))\n    \n    # Core metrics\n    leftover = remaining - item\n    fit_metric = 1.0 / (leftover + 1e-9)                # Tightest fit preference\n    space_metric = remaining                           # Capacity preservation\n    tightness = item / (remaining + 1e-9)               # Local fit severity\n    bin_utilization = (orig_cap - remaining) / (orig_cap + 1e-9)  # Global bin status\n    \n    # Z-score normalization\n    fit_mean, fit_std = fit_metric[eligible].mean(), fit_metric[eligible].std()\n    fit_z = (fit_metric - fit_mean) / (fit_std + 1e-9)\n    \n    space_mean, space_std = space_metric[eligible].mean(), space_metric[eligible].std()\n    space_z = (space_metric - space_mean) / (space_std + 1e-9)\n    \n    # Primary score with adaptive balancing\n    primary_score = blending * fit_z + (1 - blending) * space_z\n    \n    # Nonlinear modifiers\n    residual_mod = -np.log(1 + leftover / (orig_cap + 1e-9))  # Elastic space penalty\n    efficiency_bonus = residual_mod * tightness / (np.sqrt(bin_utilization + 1e-9) + 1)\n    \n    # Dynamic boosting mechanism\n    enhancer = np.exp(2 * tightness) * np.clip((1 - bin_utilization)**3, 0, 1)\n    \n    # System-wide feedback\n    sys_stats = remaining.mean(), remaining.std()\n    sys_cv = sys_stats[1] / sys_stats[0] if sys_stats[0] > 1e-9 else float('inf')\n    \n    # Reinforcement learning inspiration\n    fragility = np.abs(orig_cap - remaining) / (orig_cap + 1e-9)\n    reinforcement = 1 + np.clip((1 - relative_size)**2 * (1 - bin_utilization) * fragility, 0, 3)\n    \n    # Entropy-aware tiebreaker (variance delta)\n    bin_count = remaining.size\n    sum_total = remaining.sum()\n    sum_sq = (remaining**2).sum()\n    new_sum_sq = sum_sq - (2 * item) * remaining + item**2  # \u0394(sum squares) per bin\n    mu_new = (sum_total - item) / bin_count\n    var_new = (new_sum_sq / bin_count) - mu_new**2\n    \n    var_sensitivity = 0.1 * (1 + np.sqrt(sys_cv + 1e-9))\n    var_term = var_sensitivity * (-var_new) / (np.sqrt(np.abs(var_new) + 1e-9) + 1e-5)\n    \n    # Final priority calculation\n    return np.where(\n        eligible,\n        (primary_score + 0.75 * efficiency_bonus) * enhancer * reinforcement + var_term,\n        -np.inf\n    )\n\n[Heuristics 18th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid priority using z-score BF/WF blending, exponential fragmentation control, entropy-based median alignment, \n    and variance-scaled utilization tie-breakers for online bin stability.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    valid = bins_remain_cap >= item\n    if not valid.any():\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # System stats\n    C_est = bins_remain_cap.max()\n    avg_cap = bins_remain_cap.mean()\n    std_cap = bins_remain_cap.std()\n    med_cap = np.median(bins_remain_cap)\n    leftover = bins_remain_cap - item\n    \n    # Z-score driven BF/WF blending\n    z = (item - avg_cap) / (std_cap + 1e-9)\n    blend_weight = 1.0 / (1.0 + np.exp(-z))  # Blend to BF when item > avg\n    primary_score = blend_weight * (-leftover) + (1 - blend_weight) * bins_remain_cap\n    \n    # Nonlinear fragmentation penalty\n    frag_score = -np.exp(-leftover / (0.2 * C_est + 1e-9))\n    \n    # Entropy control via median alignment\n    balance_score = -0.1 * np.abs(bins_remain_cap - med_cap) / (C_est + 1e-9)\n    \n    # Variance-adaptive tie-breaker\n    valid_leftover = leftover[valid]\n    std_leftover = valid_leftover.std() if len(valid_leftover) > 1 else 1e-6\n    epsilon = 1e-4 / (std_leftover + 1e-9)\n    \n    # Utilization gradient per bin\n    utilization = np.zeros_like(leftover, dtype=np.float64)\n    np.divide(item, bins_remain_cap, where=valid, out=utilization)\n    \n    # Score aggregation\n    scores = (primary_score + frag_score + balance_score + epsilon * utilization)\n    scores[~valid] = -np.inf  # Invalid bins\n    return scores\n\n[Heuristics 19th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid priority using z-score BF/WF blending, exponential fragmentation control, entropy-based median alignment, \n    and variance-scaled utilization tie-breakers for online bin stability.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    valid = bins_remain_cap >= item\n    if not valid.any():\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # System stats\n    C_est = bins_remain_cap.max()\n    avg_cap = bins_remain_cap.mean()\n    std_cap = bins_remain_cap.std()\n    med_cap = np.median(bins_remain_cap)\n    leftover = bins_remain_cap - item\n    \n    # Z-score driven BF/WF blending\n    z = (item - avg_cap) / (std_cap + 1e-9)\n    blend_weight = 1.0 / (1.0 + np.exp(-z))  # Blend to BF when item > avg\n    primary_score = blend_weight * (-leftover) + (1 - blend_weight) * bins_remain_cap\n    \n    # Nonlinear fragmentation penalty\n    frag_score = -np.exp(-leftover / (0.2 * C_est + 1e-9))\n    \n    # Entropy control via median alignment\n    balance_score = -0.1 * np.abs(bins_remain_cap - med_cap) / (C_est + 1e-9)\n    \n    # Variance-adaptive tie-breaker\n    valid_leftover = leftover[valid]\n    std_leftover = valid_leftover.std() if len(valid_leftover) > 1 else 1e-6\n    epsilon = 1e-4 / (std_leftover + 1e-9)\n    \n    # Utilization gradient per bin\n    utilization = np.zeros_like(leftover, dtype=np.float64)\n    np.divide(item, bins_remain_cap, where=valid, out=utilization)\n    \n    # Score aggregation\n    scores = (primary_score + frag_score + balance_score + epsilon * utilization)\n    scores[~valid] = -np.inf  # Invalid bins\n    return scores\n\n[Heuristics 20th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid priority using z-score BF/WF blending, exponential fragmentation control, entropy-based median alignment, \n    and variance-scaled utilization tie-breakers for online bin stability.\n    \"\"\"\n    if len(bins_remain_cap) == 0:\n        return np.array([], dtype=np.float64)\n    \n    valid = bins_remain_cap >= item\n    if not valid.any():\n        return np.full_like(bins_remain_cap, -np.inf)\n    \n    # System stats\n    C_est = bins_remain_cap.max()\n    avg_cap = bins_remain_cap.mean()\n    std_cap = bins_remain_cap.std()\n    med_cap = np.median(bins_remain_cap)\n    leftover = bins_remain_cap - item\n    \n    # Z-score driven BF/WF blending\n    z = (item - avg_cap) / (std_cap + 1e-9)\n    blend_weight = 1.0 / (1.0 + np.exp(-z))  # Blend to BF when item > avg\n    primary_score = blend_weight * (-leftover) + (1 - blend_weight) * bins_remain_cap\n    \n    # Nonlinear fragmentation penalty\n    frag_score = -np.exp(-leftover / (0.2 * C_est + 1e-9))\n    \n    # Entropy control via median alignment\n    balance_score = -0.1 * np.abs(bins_remain_cap - med_cap) / (C_est + 1e-9)\n    \n    # Variance-adaptive tie-breaker\n    valid_leftover = leftover[valid]\n    std_leftover = valid_leftover.std() if len(valid_leftover) > 1 else 1e-6\n    epsilon = 1e-4 / (std_leftover + 1e-9)\n    \n    # Utilization gradient per bin\n    utilization = np.zeros_like(leftover, dtype=np.float64)\n    np.divide(item, bins_remain_cap, where=valid, out=utilization)\n    \n    # Score aggregation\n    scores = (primary_score + frag_score + balance_score + epsilon * utilization)\n    scores[~valid] = -np.inf  # Invalid bins\n    return scores\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}