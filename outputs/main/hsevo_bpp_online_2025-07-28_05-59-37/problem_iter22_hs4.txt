import numpy as np
import random
import math
import scipy
import torch
def priority_v2(item: float, bins_remain_cap: np.ndarray, large_item_base_factor= 1.1891365865431465, large_item_cv_factor= 0.4847602828860009, normalization_fallback= 0.48600146369977515, usability_boost_weight= -0.04591204748133357, large_tight= 0.0701359747383904, large_var= 0.43076973486852954, large_prox= 0.2192541978611534, small_tight= 0.6421122192849282, small_var= 0.11701589975580617, small_prox= 0.8884767176963057, epsilon= 2.704783760439713e-07) -> np.ndarray:
    """
    Combines adaptive normalization with variance impact modeling. Uses item-size-aware dynamic weights and reinforcement-inspired usability boosting. Balances tight packing, variance reduction, and system stability through hybrid scoring.
    """
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    eligible = bins_remain_cap >= item
    if not np.any(eligible):
        return np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    
    # System metrics
    system_avg = np.mean(bins_remain_cap)
    system_std = np.std(bins_remain_cap)
    system_cv = system_std / (system_avg + epsilon)
    n = bins_remain_cap.size
    
    # Item classification
    large_item_threshold = system_avg * (large_item_base_factor * (1 + large_item_cv_factor * system_cv))
    large_item = item > large_item_threshold
    
    # Metrics for eligible bins
    bin_remain = bins_remain_cap[eligible]
    tightness = item / (bin_remain + epsilon)
    leftover = bin_remain - item
    proximity = np.abs(leftover - system_avg)
    usability = (system_avg - leftover) / (system_std + epsilon)
    
    # Variance impact calculation
    sum_remain = bins_remain_cap.sum()
    sum_remain_sq = (bins_remain_cap ** 2).sum()
    sum_remain_new = sum_remain - item
    sum_remain_sq_new = sum_remain_sq - bins_remain_cap**2 + (bins_remain_cap - item)**2
    var_new = (sum_remain_sq_new / n) - (sum_remain_new / n)**2
    
    # Normalization of metrics
    var_score = np.zeros_like(var_new)
    eligible_var = var_new[eligible]
    var_min, var_max = eligible_var.min(), eligible_var.max()
    if var_max > var_min:
        var_score[eligible] = (var_max - var_new[eligible]) / (var_max - var_min + epsilon)
    else:
        var_score[eligible] = normalization_fallback
    
    tight_norm = np.zeros_like(tightness)
    tight_min, tight_max = tightness.min(), tightness.max()
    if tight_max > tight_min:
        tight_norm = (tightness - tight_min) / (tight_max - tight_min + epsilon)
    else:
        tight_norm.fill(normalization_fallback)
    
    proximity_norm = np.zeros_like(proximity)
    prox_min, prox_max = proximity.min(), proximity.max()
    if prox_max > prox_min:
        proximity_norm = 1.0 - (proximity - prox_min) / (prox_max - prox_min + epsilon)
    else:
        proximity_norm.fill(normalization_fallback)
    
    # Reinforcement-inspired usability factor
    fragility_factor = np.clip(usability, 0, None)
    reinforcer = 1.0 + usability_boost_weight * fragility_factor
    
    # Dynamic weight allocation
    if large_item:
        w_tight, w_var, w_prox = large_tight, large_var, large_prox
    else:
        w_tight, w_var, w_prox = small_tight, small_var, small_prox
    
    # Final priority calculation
    priority_components = (
        w_tight * tight_norm +
        w_var * var_score[eligible] +
        w_prox * proximity_norm
    ) * reinforcer
    
    result = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    result[eligible] = priority_components
    
    return result
