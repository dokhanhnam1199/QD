```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Z-score synergy with item-size adaptation and entropy-controlled frag mitigator.
    
    Combines normalized tightness/util metrics weighted by item size relative to system
    state, entropy-adjusted penalty scaling, and residual clustering continuity.
    """
    if bins_remain_cap.size == 0:
        return np.array([], dtype=np.float64)
    
    eps = 1e-9
    mask = bins_remain_cap >= item
    scores = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)
    
    if not mask.any():
        return scores

    # Base parameters
    origin_cap = bins_remain_cap.max()
    rem_cap = bins_remain_cap[mask]
    leftover = rem_cap - item

    # Tightness & utilization metrics
    inv_waste = 1.0 / (leftover + eps)
    exp_tight = np.exp(-2.0 * leftover / (origin_cap + eps))
    tight_fit = inv_waste + 2.0 * exp_tight
    bin_util = (origin_cap - rem_cap) / (origin_cap + eps)

    # Z-score normalization
    t_mean, t_std = tight_fit.mean(), tight_fit.std(ddof=1)
    u_mean, u_std = bin_util.mean(), bin_util.std(ddof=1)
    z_tight = (tight_fit - t_mean) / (t_std + eps)
    z_util = (bin_util - u_mean) / (u_std + eps)

    # Entropy adaptation
    active_caps = bins_remain_cap[bins_remain_cap > 0]
    if active_caps.size > 1:
        log_caps_std = np.log(active_caps + eps).std()
        entropic_scale = log_caps_std
    else:
        entropic_scale = 0.0

    # Item size context
    median_cap = np.median(active_caps) if active_caps.size else origin_cap
    is_small_item = item / (median_cap + eps) < 0.75

    # Dynamic weights
    entropy_amp = (1.0 + entropic_scale) ** 1.5
    if is_small_item:
        tight_weight = 1.8 + 0.5 * entropy_amp
        util_weight = 0.3
    else:
        tight_weight = 1.0 + 0.2 * entropy_amp
        util_weight = 1.2 + entropy_amp

    # Core synergy
    synergy = tight_weight * z_tight + util_weight * z_util

    # Fragmentation control
    leftover_norm = leftover / (origin_cap + eps)
    base_frag = -np.expm1(-leftover_norm ** 1.25)
    median_leftover = np.median(leftover) if leftover.size else origin_cap / 2
    cap_highway = np.exp(-3.0 * ((leftover - median_leftover)**2) / ((origin_cap * 0.4)**2 + eps))
    frag_penalty = base_frag - cap_highway
    frag_weight = 0.7 * entropy_amp * (1.0 + bins_remain_cap.size / (origin_cap + eps))
    scores_masked = synergy - frag_weight * frag_penalty

    # Continuity reinforcement
    if active_caps.size > 2:
        leftover_std = active_caps.std() if active_caps.size > 1 else origin_cap
        dist_to_mean = np.abs(leftover - rem_cap.mean()) / (leftover_std + eps)
        continuity_bonus = np.exp(-0.5 * dist_to_mean)
        scores_masked += 0.2 * continuity_bonus

    scores[mask] = scores_masked
    return scores
```
