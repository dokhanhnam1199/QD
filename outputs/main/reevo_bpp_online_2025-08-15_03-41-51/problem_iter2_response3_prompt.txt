{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Implements an epsilon-greedy priority function for online Bin Packing.\n    The function balances exploration (choosing a random bin) with\n    exploitation (choosing the best-fitting bin).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploring a random bin\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate the 'best fit' priority for each bin\n    # Bins that can fit the item are prioritized.\n    # Among those that can fit, we prefer bins where the remaining capacity\n    # is closest to the item size (to minimize waste).\n    for i in range(num_bins):\n        if bins_remain_cap[i] >= item:\n            # A good heuristic is to maximize the \"tightness\" of the fit.\n            # This means minimizing bins_remain_cap[i] - item.\n            # However, since we are using this as a priority for selection,\n            # a larger value should mean higher priority. So, we can use\n            # a score inversely related to the remaining capacity after packing.\n            # A simple way is to use the inverse of the remaining capacity.\n            # Or, to encourage tighter fits, we can use the capacity minus item size,\n            # but inverted, so smaller (better) differences get higher scores.\n            # Let's use (bins_remain_cap[i] - item) as the \"waste\" and\n            # invert it (or subtract from a large number) to get a priority.\n            # Using a large constant minus waste ensures positive scores and\n            # higher priority for smaller waste.\n            waste = bins_remain_cap[i] - item\n            # We want smaller waste to have higher priority.\n            # If waste is 0, it's a perfect fit, highest priority.\n            # If waste is large, priority is low.\n            priorities[i] = 1.0 / (1.0 + waste) # This is a common approach for \"best fit\" score\n            # Alternative: priorities[i] = 1.0 / (bins_remain_cap[i] - item + 1e-9)\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Assign a random, non-zero priority to all bins\n        # to give them a chance to be picked.\n        # For exploration, we want to avoid picking bins that cannot fit.\n        # So we still use the existing `priorities` as a base, but add\n        # a small random noise to all potentially viable bins.\n        # A simple exploration is to give a small uniform boost to all bins.\n        # Or, pick a random bin that can fit and assign it a high priority.\n        # Let's try assigning a slightly randomized priority to all bins that *can* fit.\n        explorable_indices = np.where(bins_remain_cap >= item)[0]\n        if len(explorable_indices) > 0:\n            random_boost = np.random.uniform(0.5, 1.5, size=len(explorable_indices))\n            priorities[explorable_indices] *= random_boost\n        else:\n            # If no bin can fit, we might still want to \"explore\" by just picking one,\n            # but in BPP, this means failing. So, we stick to prioritizing valid bins.\n            pass\n\n    # Normalize priorities to prevent extremely large or small values from dominating\n    # and to make the exploration/exploitation balance more robust.\n    # We only normalize the priorities of bins that can actually fit the item.\n    eligible_priorities = priorities[bins_remain_cap >= item]\n    if len(eligible_priorities) > 0 and np.max(eligible_priorities) > 0:\n        # Find the indices of bins that can fit the item\n        eligible_indices = np.where(bins_remain_cap >= item)[0]\n        # Normalize only the priorities of eligible bins\n        max_priority = np.max(priorities[eligible_indices])\n        if max_priority > 0:\n            priorities[eligible_indices] /= max_priority\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score prioritizes bins that are closer to fitting the item perfectly,\n    using a sigmoid function to map the \"tightness\" of the fit to a priority score.\n    Bins that can fit the item (remaining capacity >= item size) are considered.\n    The score is higher for bins where (remaining_capacity - item_size) is closer to zero.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    possible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(possible_bins_mask):\n        # No bin can fit the item, return all zeros (or handle as an error/special case)\n        return priorities\n\n    # Calculate the \"gap\" for possible bins (how much space is left after placing the item)\n    # We want to minimize this gap, so a smaller gap is better.\n    gaps = bins_remain_cap[possible_bins_mask] - item\n\n    # We want to use a sigmoid function that maps smaller gaps to higher priorities.\n    # The standard sigmoid function outputs values between 0 and 1.\n    # sigmoid(x) = 1 / (1 + exp(-x))\n    # If x is large positive, sigmoid(x) is close to 1.\n    # If x is large negative, sigmoid(x) is close to 0.\n    # We want a higher score when the gap is small (close to 0).\n    # Let's transform the gap: a smaller gap should result in a larger positive value\n    # fed into the sigmoid.\n    # Consider `scaled_gap = -gaps / scale_factor`. As gap approaches 0, scaled_gap approaches 0.\n    # sigmoid(0) = 0.5. This doesn't quite give us the highest priority for a perfect fit.\n    #\n    # Let's reconsider the goal: prioritize bins that fit the item *tightly*.\n    # This means the remaining capacity is just slightly larger than the item size.\n    # So, (bins_remain_cap[i] - item) should be small and positive.\n    #\n    # A common way to use sigmoid for prioritization is to map the \"goodness\" of a\n    # characteristic to a score.\n    # Let's define \"tightness\" as how close `bins_remain_cap[i]` is to `item`.\n    # Specifically, for bins that can fit the item, we are interested in\n    # `bins_remain_cap[i] - item`.\n    #\n    # We want a function f(diff) where `diff = bins_remain_cap[i] - item` such that:\n    # - f(diff) is high when `diff` is small and positive.\n    # - f(diff) is lower when `diff` is large and positive.\n    # - f(diff) is 0 or very low when `diff` is negative (or item doesn't fit).\n    #\n    # Let's try `sigmoid(k * (max_capacity - bins_remain_cap[i]))`.\n    # If `bins_remain_cap[i]` is close to `item`, then `bins_remain_cap[i] - item` is small.\n    #\n    # Alternative approach: Model the \"cost\" of fitting.\n    # A perfect fit has zero cost. A loose fit has a cost.\n    # Consider the metric `bins_remain_cap[i] - item`.\n    # We want a high priority when this value is close to 0 (and positive).\n    #\n    # Let's use a sigmoid on the *inverse* of the gap.\n    # A small gap is a large inverse gap.\n    #\n    # Consider the difference `diff = bins_remain_cap[i] - item`.\n    # We want a high score when `diff` is small and positive.\n    #\n    # Let's map `diff` to a score using sigmoid:\n    # `sigmoid(A - B * diff)`:\n    # - If `diff` is small positive, `B * diff` is small positive. `A - B * diff` is large positive. Sigmoid is close to 1.\n    # - If `diff` is large positive, `B * diff` is large positive. `A - B * diff` is large negative. Sigmoid is close to 0.\n    #\n    # We need to choose parameters A and B appropriately.\n    # Let's set A to control the center of the sigmoid and B to control the steepness.\n    # A common approach is to center the sigmoid around 0.\n    #\n    # Let's define a score that peaks at 0 difference.\n    # We can use `sigmoid(slope * (optimal_diff - current_diff))`.\n    # `optimal_diff = 0`. So, `sigmoid(slope * (0 - (bins_remain_cap[i] - item)))`\n    # = `sigmoid(slope * (item - bins_remain_cap[i]))`\n    # = `sigmoid(slope * -(bins_remain_cap[i] - item))`\n    #\n    # For bins where `bins_remain_cap[i] < item`, this calculation is not directly applicable.\n    # We've already filtered these out.\n    # For bins where `bins_remain_cap[i] >= item`:\n    # Let `gap = bins_remain_cap[i] - item`. `gap >= 0`.\n    # The score is `sigmoid(-slope * gap)`.\n    # - If `gap = 0` (perfect fit), score = `sigmoid(0)` = 0.5.\n    # - If `gap` is small positive, `sigmoid` is slightly less than 0.5.\n    # - If `gap` is large positive, `sigmoid` is close to 0.\n    # This gives higher priority to bins with larger gaps, which is the opposite of what we want.\n    #\n    # We need a function that *decreases* as `gap` increases.\n    # So, let's use `1 - sigmoid(slope * gap)` or `sigmoid(-slope * gap)`.\n    #\n    # Let's rethink: we want to prioritize bins where `bins_remain_cap[i]` is CLOSEST to `item`.\n    # The difference `d = bins_remain_cap[i] - item`. We want `d` to be small and positive.\n    #\n    # Consider `1 / (1 + exp(-k * (value)))`\n    # If we want the score to be high when `bins_remain_cap[i]` is just above `item`.\n    #\n    # Let `ratio = item / bins_remain_cap[i]`. This is relevant if bin capacity is variable.\n    # Here, bin capacity is fixed, but remaining capacity changes.\n    #\n    # The \"Sigmoid Fit Score\" usually implies fitting the item as snugly as possible.\n    # This means minimizing `bins_remain_cap[i] - item` for `bins_remain_cap[i] >= item`.\n    #\n    # Let's use the negative of the gap as the input to the sigmoid, which will give\n    # values closer to 1 for smaller gaps.\n    # `score = sigmoid(k * (item - bins_remain_cap[i]))`\n    # This is equivalent to `sigmoid(k * -(bins_remain_cap[i] - item))`.\n    #\n    # Let's use `k = 1.0` for simplicity for now, and `sigmoid(x) = 1 / (1 + exp(-x))`.\n    #\n    # We want the score to be high for `bins_remain_cap[i]` just above `item`.\n    #\n    # Let's scale the gap to avoid numerical issues and control sensitivity.\n    # `scaled_gap = (bins_remain_cap[possible_bins_mask] - item) / max(1, bins_remain_cap[possible_bins_mask].max())`\n    # This makes the gap a value between 0 and 1 (if max_cap is 1).\n    #\n    # Consider `score = sigmoid(A - B * (bins_remain_cap[i] - item))`\n    # Let's center the sigmoid such that a difference of `delta` results in a score of 0.5.\n    # If we want a difference of 0 to be ideal, then `sigmoid(0) = 0.5`.\n    #\n    # A common sigmoid fit score in BPP aims to place the item in the bin\n    # where the remaining capacity is the smallest that can still fit the item.\n    # This is equivalent to minimizing `bins_remain_cap[i] - item` for `bins_remain_cap[i] >= item`.\n    #\n    # So, we want a function `f(gap)` where `f(gap)` is high for small `gap >= 0`.\n    #\n    # Let's use `f(gap) = 1 - sigmoid(k * gap)` or `f(gap) = sigmoid(-k * gap)`.\n    # With `sigmoid(x) = 1 / (1 + exp(-x))`:\n    # - `gap = 0`: `f(0) = sigmoid(0) = 0.5`. This is the peak value.\n    # - `gap = small_positive`: `f(small_positive) = sigmoid(-k * small_positive)` is slightly less than 0.5.\n    # - `gap = large_positive`: `f(large_positive) = sigmoid(-k * large_positive)` is close to 0.\n    #\n    # This means `sigmoid(-k * gap)` assigns a score that decreases as the gap increases.\n    # The highest score is 0.5 for a perfect fit.\n\n    # Let's define a \"steepness\" parameter for the sigmoid.\n    # A larger `steepness` will make the score drop faster as the gap increases.\n    steepness = 5.0  # Tune this parameter\n\n    # Calculate the argument for the sigmoid function\n    # We want to prioritize smaller gaps. So, `item - bins_remain_cap[i]` should be maximized (least negative).\n    # Or equivalently, `bins_remain_cap[i] - item` should be minimized.\n    # Using `sigmoid(-steepness * gap)` makes scores higher for smaller gaps.\n    sigmoid_arg = -steepness * gaps\n\n    # Apply the sigmoid function to get priorities\n    # We are only calculating for possible bins, so we need to put these values back.\n    priorities[possible_bins_mask] = 1 / (1 + np.exp(-sigmoid_arg))\n\n    # If `item` is very small, `bins_remain_cap[i] - item` is close to `bins_remain_cap[i]`.\n    # The sigmoid still works, prioritizing bins with less remaining capacity.\n\n    return priorities\n\n[Reflection]\nPrioritize tight fits, use smooth transitions like sigmoid, and avoid abrupt changes or randomness in deterministic scenarios.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}