```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Softmax-Based Fit.

    The priority is calculated based on how well an item fits into a bin,
    favoring bins that leave minimal remaining capacity after packing the item,
    while also considering bins that have more remaining capacity. The softmax
    function is used to convert these 'fitness' scores into probabilities,
    where bins with higher 'fitness' (i.e., better fits) get higher priorities.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Calculate the 'fitness' of each bin for the current item.
    # A good fit is one where the remaining capacity is small after adding the item.
    # We only consider bins that can actually fit the item.
    # For bins that cannot fit the item, we assign a very low 'fitness' (-infinity)
    # so they get a priority of 0 after softmax.
    fitness_scores = np.full_like(bins_remain_cap, -np.inf)
    can_fit_mask = bins_remain_cap >= item
    
    # For bins that can fit, a higher priority is given to bins where
    # the remaining capacity after adding the item is smaller.
    # We add a small epsilon to avoid division by zero if remaining capacity is 0.
    # The score is inversely proportional to the remaining capacity after packing.
    # Adding the item size to the capacity and then taking the reciprocal might
    # be a simple way to represent "how much space is left". A smaller value
    # is better. We can also consider the proportion of space used.
    
    # Let's try a simple approach: higher priority for bins that leave *less*
    # remaining capacity. This is the "best fit" idea.
    # remaining_after_packing = bins_remain_cap[can_fit_mask] - item
    # To convert to a score where larger is better, we can take the negative of
    # remaining capacity, or a function like exp(alpha * (-remaining_capacity)).
    
    # A common strategy for softmax-based selection is to have scores that
    # represent desirability. For bin packing, fitting an item into a bin
    # without leaving too much wasted space is desirable.
    
    # Let's define a score that is high for bins that have just enough space
    # and lower for bins with much more space.
    # A simple score could be 1 / (bins_remain_cap - item + epsilon)
    # or simply the negative of the remaining capacity: -(bins_remain_cap - item)
    # Let's try a score that represents how much capacity is *used* relative to available space.
    # For bins that can fit: score = item / bins_remain_cap (if we want to fill bins)
    # Or, we want to minimize wasted space, so minimize (bins_remain_cap - item).
    # For softmax, we want higher values for better bins.
    # So, we can use - (bins_remain_cap - item) or (item - bins_remain_cap)
    # Let's use a score that is the negative of the leftover space.
    # Lower (more negative) values mean more leftover space, so less desirable.
    # Thus, a bin with *less* leftover space is more desirable.
    # For example, if item=3, bins_remain_cap=[10, 5, 4].
    # Bin 1: remaining=7, score=-7
    # Bin 2: remaining=2, score=-2
    # Bin 3: remaining=1, score=-1
    # Softmax of [-7, -2, -1] will give higher probability to -1.
    
    # To avoid issues with exactly zero remaining capacity if we were to use reciprocal,
    # and to ensure that bins that are nearly full get higher scores, we can use
    # the negative of the remaining capacity after packing.
    
    # If remaining capacity is R, and item is I, new remaining capacity is R-I.
    # We want to prioritize bins where R-I is small.
    # So, we can assign a score of -(R-I) or equivalently I-R.
    
    # For bins that can fit:
    effective_remaining_caps = bins_remain_cap[can_fit_mask] - item
    # We want smaller effective_remaining_caps to be better.
    # For softmax, higher values are better. So, we invert the desirability.
    # A simple inversion is 1 / (effective_remaining_caps + epsilon) or
    # -(effective_remaining_caps)
    
    # Let's use -(effective_remaining_caps). This means bins with less remaining
    # space (after packing) get higher scores.
    # Example: bins_remain_cap = [10, 5, 4], item = 3
    # Bin 1: can fit, 10-3=7, score = -7
    # Bin 2: can fit, 5-3=2, score = -2
    # Bin 3: can fit, 4-3=1, score = -1
    # Softmax of [-7, -2, -1] would give higher weights to the bins with smaller remaining capacity.
    
    scores = -(bins_remain_cap[can_fit_mask] - item)
    fitness_scores[can_fit_mask] = scores

    # Apply softmax function to get probabilities (priorities)
    # Ensure numerical stability for softmax by subtracting the maximum score
    if np.all(fitness_scores == -np.inf): # No bins can fit the item
        return np.zeros_like(bins_remain_cap)
        
    # Subtracting the max score before exponentiation for numerical stability.
    # The relative differences in scores are preserved.
    max_score = np.max(fitness_scores)
    exp_scores = np.exp(fitness_scores - max_score)
    
    # Avoid division by zero if all exp_scores are effectively zero
    sum_exp_scores = np.sum(exp_scores)
    if sum_exp_scores == 0:
        # If all scores are very negative, resulting in near-zero exponentials,
        # distribute probability equally among bins that *can* fit.
        # This case is unlikely with typical capacities and item sizes.
        # However, for robustness, if sum is zero, we can return uniform probabilities
        # for fitting bins, and zero for non-fitting bins.
        priorities = np.zeros_like(bins_remain_cap)
        num_fitting_bins = np.sum(can_fit_mask)
        if num_fitting_bins > 0:
            priorities[can_fit_mask] = 1.0 / num_fitting_bins
        return priorities
        
    priorities = exp_scores / sum_exp_scores

    return priorities
```
