{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-based Best Fit strategy.\n\n    This strategy aims to mimic the \"Best Fit\" heuristic within a softmax framework.\n    It prioritizes bins where placing the item results in the least remaining capacity\n    (i.e., the \"tightest\" fit). Bins that cannot accommodate the item receive zero priority.\n    The priorities are generated using a softmax function on desirability scores,\n    where a higher score indicates a better fit (less remaining capacity after packing).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n        For fitting bins, scores are derived from the resulting remaining capacity,\n        and transformed by softmax to represent probabilities or preferences.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        # Calculate the remaining capacity *after* placing the item for eligible bins\n        valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        resulting_remain_cap = valid_bins_remain_cap - item\n\n        # Define a desirability score for each fitting bin.\n        # For \"Best Fit\", we want to minimize `resulting_remain_cap`.\n        # A higher desirability score should correspond to a lower `resulting_remain_cap`.\n        # We can use a score that is inversely proportional to `resulting_remain_cap`.\n        # To avoid division by zero and ensure positive scores, we can add a small constant\n        # or use a transformation like `1 / (slack + 1)`.\n        # A simple approach is to use `max_possible_slack - slack`, where `max_possible_slack`\n        # is the maximum possible remaining capacity among fitting bins.\n        # Or, more directly, we can use a desirability score that is higher when `resulting_remain_cap` is smaller.\n        # Let's use `-resulting_remain_cap` as a raw desirability, so smaller slack (closer to 0) is better.\n        # For softmax, we want scores that are generally positive for exponentiation.\n        # A good score is `1.0 / (resulting_remain_cap + 1.0)` which is high when `resulting_remain_cap` is small.\n\n        # Let's use a score that emphasizes bins with very little slack.\n        # A slightly larger slack should yield a significantly lower score.\n        # We can use an exponential decay, or a simple inverse relationship.\n        # `score = 1.0 / (resulting_remain_cap + epsilon)` where epsilon is a small positive number.\n        # Let's use `1.0 / (resulting_remain_cap + 1e-6)` to ensure stability and positive values.\n\n        desirability_scores = 1.0 / (resulting_remain_cap + 1e-6)\n\n        # Apply softmax to convert desirability scores into a probability-like distribution.\n        # A temperature parameter can control the \"softness\" of the distribution.\n        # A lower temperature makes the distribution sharper, favoring the best bins more.\n        # A higher temperature makes it flatter, giving more similar probabilities.\n        temperature = 1.0  # Tunable parameter\n\n        # Avoid numerical instability with very large desirability scores by clipping or normalizing first,\n        # or by using a stable softmax implementation if available.\n        # Here, we use `exp(score / temperature)`.\n\n        try:\n            exp_scores = np.exp(desirability_scores / temperature)\n            sum_exp_scores = np.sum(exp_scores)\n            if sum_exp_scores > 0:\n                softmax_probabilities = exp_scores / sum_exp_scores\n            else:\n                # If all desirability scores are extremely small (or negative if we didn't ensure positivity),\n                # softmax might result in zeros. Assign uniform probability in such edge cases.\n                softmax_probabilities = np.ones_like(desirability_scores) / len(desirability_scores)\n        except OverflowError:\n            # Handle potential overflow if desirability_scores are too large\n            # A common approach is to subtract the max score before exponentiation.\n            max_score = np.max(desirability_scores)\n            stable_scores = desirability_scores - max_score\n            exp_scores = np.exp(stable_scores / temperature)\n            sum_exp_scores = np.sum(exp_scores)\n            if sum_exp_scores > 0:\n                softmax_probabilities = exp_scores / sum_exp_scores\n            else:\n                softmax_probabilities = np.ones_like(desirability_scores) / len(desirability_scores)\n\n\n        # Place the calculated probabilities back into the original priorities array\n        priorities[can_fit_mask] = softmax_probabilities\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Tunable Sigmoid Fit Score.\n\n    This version introduces a tunable parameter `steepness` to control how quickly the\n    priority score drops as the gap (remaining capacity - item size) increases.\n    It also adds a small offset to the sigmoid argument to slightly favor bins that\n    are not perfectly filled, potentially leaving more room for future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to a very low value for bins that cannot fit the item.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can accommodate the item\n    possible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(possible_bins_mask):\n        # No bin can fit the item, return all negative infinities.\n        return priorities\n\n    # Calculate the \"gap\" for possible bins (how much space is left after placing the item)\n    gaps = bins_remain_cap[possible_bins_mask] - item\n\n    # Tunable parameter to control the steepness of the sigmoid.\n    # A higher value means the priority drops more sharply as the gap increases.\n    # A value of 0 would make all fitting bins have a score of 0.5 (if offset is 0).\n    steepness = 10.0  # Increased steepness for more distinct prioritization\n\n    # Introduce a small negative offset to the argument.\n    # This means that a gap of 0 (perfect fit) will result in sigmoid(-offset),\n    # which is slightly less than 0.5. Larger gaps will result in scores even further below 0.5.\n    # This can encourage using bins that have a small positive gap, potentially\n    # leaving perfectly filled bins for smaller items if available.\n    # Experimentation is needed to find the optimal offset.\n    # Let's try to make the \"ideal\" gap slightly positive, e.g., `optimal_gap = 0.1 * item`.\n    # We want `steepness * (optimal_gap - gap)` to be around 0.\n    # `steepness * (0.1 * item - gap)`\n    # This means we want `gap` to be slightly less than `0.1 * item` for highest priority.\n    # Or, let's consider the argument `steepness * (item - bins_remain_cap[i])`.\n    # We want this to be high when `bins_remain_cap[i]` is slightly larger than `item`.\n    # Let's scale the negative gap by `steepness`.\n    # `sigmoid_arg = steepness * (item - bins_remain_cap[possible_bins_mask])`\n    # `sigmoid_arg = -steepness * (bins_remain_cap[possible_bins_mask] - item)`\n    # This assigns 0.5 to a perfect fit, lower to larger gaps, and higher to negative gaps (not possible here).\n    #\n    # To slightly prefer bins that are NOT perfectly filled, we can shift the sigmoid.\n    # Let's aim for the peak priority to be at a small positive gap.\n    # Consider `sigmoid(k * (ideal_gap - current_gap))`.\n    # If we want `ideal_gap = 0`, this is `sigmoid(k * -gap)`.\n    #\n    # Let's consider a function that rewards tightness: `f(gap)`.\n    # `f(gap) = exp(-steepness * gap)` could be an alternative, but it's not bounded between 0 and 1.\n    #\n    # Let's stick to the sigmoid but adjust the argument.\n    # We want to prioritize smaller gaps. `sigmoid(-steepness * gap)` does this, with peak at 0.5.\n    # To shift the peak to a small positive gap `g_ideal`, we can use `sigmoid(steepness * (g_ideal - gap))`.\n    # Let's choose `g_ideal` to be a small fraction of the item size, e.g., 10% of item size.\n    # This introduces a dependency on the item size, which might be complex.\n    #\n    # A simpler approach: use `sigmoid(-steepness * gap)` and then scale or offset the output.\n    # Or, slightly modify the input:\n    # Let's aim for the highest priority when `bins_remain_cap[i] - item` is small and positive.\n    # We can use `sigmoid(A - B * gap)` where `A` shifts the curve and `B` controls steepness.\n    # If we want the peak at `gap = 0`, then `A = 0` works (for `sigmoid(B * (-gap))`).\n    #\n    # To reward slightly less full bins, let's try to make the score decrease faster.\n    # Let's modify the sigmoid argument:\n    # Instead of just `-steepness * gap`, consider `-steepness * gap - offset`.\n    # This shifts the entire curve to the left.\n    # For `gap = 0`, the argument becomes `-offset`. `sigmoid(-offset)` is < 0.5.\n    # This means perfect fits get lower scores than before.\n    #\n    # The goal is to rank bins. The absolute values don't matter as much as the relative order.\n    # `sigmoid(-steepness * gap)` already provides a good ranking for \"tightest fit\".\n    #\n    # Let's introduce a small constant to the sigmoid argument to shift the peak.\n    # A small positive constant in `sigmoid(-steepness * gap + const)` will increase scores for all gaps.\n    # A small negative constant `const` will decrease scores.\n    #\n    # Let's reconsider the goal: prioritize bins that are \"almost full\" but can still fit.\n    # This means `bins_remain_cap[i]` should be minimized, subject to `bins_remain_cap[i] >= item`.\n    #\n    # Consider the inverse of the gap, but normalized.\n    # Let `normalized_gap = gap / max_possible_gap`. This is still problematic if max_possible_gap is 0.\n    #\n    # Let's try a simple scaling of the gap and an offset.\n    # `sigmoid_input = steepness * (item - bins_remain_cap[possible_bins_mask])`\n    # `sigmoid_input = -steepness * gaps`\n    #\n    # Let's add a small bias to favor bins that are not completely full.\n    # This is to avoid situations where a bin is filled to absolute capacity, potentially\n    # making it unusable for even the smallest future items.\n    # A small positive value `epsilon` added to the `item` size might simulate this.\n    # `effective_item_size = item + epsilon`\n    # `gaps_adjusted = bins_remain_cap[possible_bins_mask] - effective_item_size`\n    # `sigmoid_arg = -steepness * gaps_adjusted`\n    #\n    # Let's try a simple adjustment to the input of the sigmoid:\n    # Instead of `sigmoid(-steepness * gap)`, let's use `sigmoid(-steepness * gap + adjustment)`.\n    # If `adjustment` is positive, it pushes scores higher for all gaps.\n    # If `adjustment` is negative, it pushes scores lower.\n    #\n    # Let's try to favor bins with a small positive gap, say up to `item * 0.1`.\n    # We want the score to be high in this range.\n    #\n    # Let's use `sigmoid(k * (target_capacity - current_remaining_capacity))`\n    # Target capacity should be `item`. So, `sigmoid(k * (item - bins_remain_cap[i]))`.\n    # This is `sigmoid(-k * gap)`.\n    #\n    # To slightly favor bins with a small positive gap (e.g., `gap = 0.1 * item`),\n    # we want the input to sigmoid to be slightly positive.\n    # `sigmoid(k * (ideal_gap - gap))`. If `ideal_gap = 0.1 * item`, and `gap` is a bit smaller.\n    #\n    # Let's use a simpler approach that's common: scale the gap.\n    # `scaled_gap = gap / item` (if item > 0). This makes it relative.\n    # Then `sigmoid(-steepness * scaled_gap)`.\n    # This way, the \"tightness\" is relative to the item size.\n    #\n    # Let's try to slightly penalize perfect fits (gap=0) to encourage slight overflow.\n    # This means the peak of our priority function should be at a small positive gap.\n    # We can achieve this by shifting the sigmoid input.\n    # `sigmoid(steepness * (ideal_gap - gap))`\n    # Let `ideal_gap = 0.05 * item` (a small fraction of the item size).\n    #\n    # If item is 0, this formula would be problematic.\n    # Let's assume item > 0.\n    #\n    # `ideal_gap = 0.05 * item`\n    # `sigmoid_arg = steepness * (ideal_gap - gaps)`\n    #\n    # Example: item = 10. steepness = 10. ideal_gap = 0.5.\n    # If gap = 0.1 (bin cap = 10.1): sigmoid_arg = 10 * (0.5 - 0.1) = 4.0. Score = sigmoid(4.0) ~ 0.98\n    # If gap = 0.5 (bin cap = 10.5): sigmoid_arg = 10 * (0.5 - 0.5) = 0.0. Score = sigmoid(0.0) = 0.5\n    # If gap = 1.0 (bin cap = 11.0): sigmoid_arg = 10 * (0.5 - 1.0) = -5.0. Score = sigmoid(-5.0) ~ 0.007\n    # If gap = 0.0 (bin cap = 10.0): sigmoid_arg = 10 * (0.5 - 0.0) = 5.0. Score = sigmoid(5.0) ~ 0.993 (Oops, perfect fit is highest!)\n\n    # The goal is usually to fill bins as much as possible. So tightest fit is preferred.\n    # Let's go back to prioritizing the minimum non-negative gap.\n    # `sigmoid(-steepness * gap)` where peak is at gap=0.\n    #\n    # To make `priority_v2` distinct and tunable, let's introduce a parameter\n    # that influences the \"ideal\" tightness.\n    #\n    # `fit_preference`:\n    # - `fit_preference = 0`: Prioritize bins that are exactly full (gap = 0).\n    # - `fit_preference = 1`: Prioritize bins that are somewhat full, but leave a small buffer (e.g., gap = 5% of item size).\n    # - `fit_preference = -1`: Prioritize bins that are less full (larger gaps). This is generally not good.\n\n    # Let's aim for `fit_preference` to control the \"ideal gap\".\n    # `ideal_gap = fit_preference * item * 0.1` (0.1 is a scaling factor for preference strength)\n    # If `fit_preference = 0`, `ideal_gap = 0`.\n    # If `fit_preference = 1`, `ideal_gap = 0.1 * item`.\n    # If `fit_preference = -1`, `ideal_gap = -0.1 * item` (problematic, means item must be smaller than capacity).\n\n    # Let's simplify: use `steepness` and a fixed adjustment that shifts the peak slightly.\n    # `sigmoid(-steepness * gap)` has peak at `gap = 0`.\n    # To shift the peak to `gap = ideal_gap`, use `sigmoid(steepness * (ideal_gap - gap))`.\n    #\n    # Let's define `ideal_gap` as a fraction of the item size.\n    # `ideal_gap_fraction = 0.05`  # Aim for a gap that's 5% of the item's size\n    # `ideal_gap = ideal_gap_fraction * item`\n    #\n    # Ensure `ideal_gap` is not negative and not larger than typical gaps.\n    # `ideal_gap = max(0.0, ideal_gap)` # Ensure non-negative\n\n    # The core idea of Sigmoid Fit Score is to rank by tightness.\n    # `sigmoid(-steepness * gap)` achieves this with peak at `gap=0`.\n    # The \"improvement\" can be in the tuning of `steepness` and potentially\n    # how we normalize or bound the `gap` for the sigmoid input.\n\n    # Let's consider a robust scaling of the gap:\n    # `scaled_gap = gap / max(1, item)` # Avoid division by zero and scale by item size.\n    # This makes the priority sensitive to the relative tightness.\n    # Then `sigmoid(-steepness * scaled_gap)`.\n    #\n    # Let's try a combination: `steepness` for sensitivity and a small `gap_offset`\n    # to slightly shift the priority away from perfect fits.\n\n    # Parameter to control how sensitive the priority is to the gap.\n    # Higher value means smaller gaps are much more preferred than larger gaps.\n    steepness = 15.0\n\n    # Parameter to slightly offset the ideal fit.\n    # A positive `gap_preference_offset` means we slightly prefer bins that are NOT perfectly filled.\n    # A negative offset would strongly prefer perfectly filled bins.\n    # Let's try to prefer bins where the remaining capacity is just slightly larger than the item.\n    # This corresponds to a small positive gap.\n    # If `gap = 0`, we want the input to sigmoid to be low.\n    # If `gap = ideal_gap > 0`, we want the input to sigmoid to be close to 0.\n    # So, `k * (ideal_gap - gap)`.\n    #\n    # Let's use `gap` directly, but adjust the sigmoid mapping.\n    # `sigmoid_arg = -steepness * gaps` -> peak at gap = 0.\n    #\n    # Alternative: `1 - sigmoid(steepness * gaps)` -> peak at gap = 0.\n    #\n    # Consider `max(0, 1 - steepness * gaps)` -- not smooth.\n\n    # Let's stick to sigmoid and tune its input:\n    # `sigmoid(steepness * (B - A * gap))` where B is offset and A is steepness multiplier.\n    # `sigmoid(steepness * (0.1 - gap))` -- this is very dependent on scale.\n    #\n    # A common approach is to normalize the gap:\n    # `normalized_gap = gap / max_capacity_of_bin` (but capacity is fixed, remaining varies)\n    #\n    # Let's use a simple sigmoid where `steepness` controls the curve.\n    # And we can add a small constant to the sigmoid argument to nudge it.\n    # `sigmoid_arg = -steepness * gaps + bias`\n    # `bias = 0.0` means peak at `gap = 0`.\n    # `bias = 1.0` means `sigmoid(-steepness * gap + 1.0)`.\n    # If `gap=0`, arg = 1.0, score = sigmoid(1.0) > 0.5.\n    # If `gap=0.1`, arg = -steepness*0.1 + 1.0. If steepness=10, arg=0.0, score=0.5.\n    # So, `bias` shifts the point where score is 0.5.\n    #\n    # Let's make `steepness` and `bias` tunable.\n    steepness = 20.0  # Higher steepness for better discrimination\n    bias = 0.2        # Shift the sigmoid to favor slightly larger gaps (less than perfect fits)\n\n    # Calculate the argument for the sigmoid function\n    # We want to penalize larger gaps, so `gaps` should be subtracted from something.\n    # `bias - steepness * gaps`\n    # If gap is small positive, `bias - steepness * gap` is positive and large (if gap is much smaller than bias/steepness).\n    # If gap is 0, arg = bias.\n    # If gap is larger than `bias/steepness`, arg becomes negative.\n    sigmoid_arg = bias - steepness * gaps\n\n    # Apply the sigmoid function to get priorities\n    # `1 / (1 + exp(-x))` maps values to [0, 1].\n    # For `gap = 0`, score = `sigmoid(bias)`.\n    # For `gap = bias / steepness`, score = `sigmoid(0) = 0.5`.\n    # For `gap > bias / steepness`, score < 0.5.\n    # This means bins with gaps larger than `bias / steepness` get lower scores.\n    # Higher `bias` means higher scores for all gaps, and a higher gap is needed for score 0.5.\n    priorities[possible_bins_mask] = 1 / (1 + np.exp(-sigmoid_arg))\n\n    return priorities\n\n[Reflection]\nTune sigmoid parameters; prioritize slight positive gaps for future packing flexibility.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}