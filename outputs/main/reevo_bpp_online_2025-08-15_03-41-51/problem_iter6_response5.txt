```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority with which we want to add item to each bin,
    prioritizing tight fits using a hyperbolic tangent scaling for smoother
    distribution, and adding a small bonus for larger capacities.

    The strategy aims to:
    1. Prioritize bins that offer a "tight fit" (minimal remaining capacity after packing).
       This is modeled using a scaled inverse of the "waste" (remaining_capacity - item).
    2. Give a small bonus to bins with larger remaining capacities to encourage
       their use for potentially larger future items.
    3. Use a tanh activation to map scores to a more stable [0, 1] range,
       effectively balancing the two objectives.
    4. Handle un-fittable bins explicitly with zero priority.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
        Higher scores indicate higher priority.
    """
    num_bins = len(bins_remain_cap)
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Determine which bins can fit the item
    can_fit_mask = bins_remain_cap >= item

    # If no bins can fit the item, return all zeros
    if not np.any(can_fit_mask):
        return priorities

    # Filter to only consider bins that can fit the item
    fitting_bins_cap = bins_remain_cap[can_fit_mask]
    fitting_indices = np.where(can_fit_mask)[0]

    # --- Scoring components ---

    # 1. Tight Fit Score: Prioritize bins with minimal remaining capacity after packing.
    #    We want to minimize (remaining_capacity - item), which is the "waste".
    #    To map this to a priority (higher is better), we'll use an inverse relationship.
    #    A smaller waste should yield a higher score.
    waste = fitting_bins_cap - item

    # Normalize waste to a [0, K] range for scaling. We want smaller waste to be higher priority.
    # A common strategy is to use a function that maps [0, max_waste] to a range where 0 maps to high.
    # Let's map waste to a negative value for tanh: -waste / scale_factor.
    # The scale_factor determines the steepness of the priority curve. A larger scale_factor
    # means more sensitivity to small differences in waste.
    # Max waste can be large, so we need a robust scaling.
    # Let's scale by a factor related to the maximum possible waste or average waste.
    # A simpler approach: map waste to [0, 1] and then invert.
    epsilon = 1e-9 # To avoid division by zero or log(0)
    max_waste = np.max(waste) if len(waste) > 0 else 0
    
    # If max_waste is 0, all fitting bins are perfect fits, give them highest priority.
    if max_waste < epsilon:
        tight_fit_scores = np.ones_like(waste)
    else:
        # Map waste to a range that favors smaller values.
        # Using a function like 1 / (1 + waste/scale) or exp(-waste/scale)
        # or directly feeding negative scaled waste into tanh.
        # Let's use a scale factor that is roughly the average waste or a fraction of max_cap.
        # A simple scaling factor can be the maximum *remaining* capacity across all bins (before fitting).
        max_original_cap = np.max(bins_remain_cap) if num_bins > 0 else 1
        scale_factor_tight = max_original_cap * 0.5 # Tunable parameter for tightness sensitivity

        # Negative scaled waste for tanh: closer to 0 for smaller waste.
        # We want smaller waste to map to larger positive tanh output.
        # tanh(x) is 0 at x=0, approaches 1 for large positive x, -1 for large negative x.
        # So, we want to map small waste to large positive arguments for tanh.
        # This means mapping small waste to large negative arguments of -waste/scale_factor.
        # So, small waste -> large positive arg for -waste/scale_factor
        # Example: waste=0.1, scale=1 -> -0.1. waste=1, scale=1 -> -1.
        # This seems counter-intuitive. Let's rethink.
        # We want to prioritize bins where (fitting_bins_cap - item) is small.
        # Let's use (max_possible_prior_capacity - fitting_bins_cap) and add item.
        # Or, focus on (remaining_capacity - item). We want this value to be small.
        # Let's map (remaining_capacity - item) to a range, and then apply a function that
        # results in higher scores for smaller values.
        # Consider the "closeness" to fitting: `fitting_bins_cap - item`.
        # We want `fitting_bins_cap - item` to be small.
        # Map `fitting_bins_cap - item` to a range `[-S, S]` where `S` is some scaling factor.
        # Then apply tanh.
        # The scale factor `S` should relate to typical waste values.
        scale_tightness = np.max(bins_remain_cap[can_fit_mask]) if len(fitting_bins_cap) > 0 else 1
        # Ensure scale_tightness is not zero and not too small
        scale_tightness = max(scale_tightness * 0.5, epsilon) 
        
        # Negative waste scaled: smaller waste -> larger negative value -> higher tanh output
        scaled_negative_waste = -(waste / scale_tightness)
        tight_fit_scores = np.tanh(scaled_negative_waste) # tanh outputs [-1, 1]

    # 2. Capacity Bonus Score: Give a small bonus to bins with larger remaining capacities.
    #    Normalize remaining capacity to a [0, 1] range.
    max_original_cap = np.max(bins_remain_cap) if num_bins > 0 else 1
    max_original_cap = max(max_original_cap, epsilon) # Avoid division by zero
    
    # Consider original remaining capacity for the bonus.
    # We want larger capacities to get a higher bonus.
    # Normalize capacity and then apply a multiplier.
    capacity_bonus_weight = 0.15 # Tunable parameter
    # Use original bins_remain_cap for bonus calculation, not just fitting ones
    normalized_original_caps = bins_remain_cap / max_original_cap
    # Apply bonus only to fitting bins
    capacity_bonus_scores = capacity_bonus_weight * normalized_original_caps[can_fit_mask]

    # --- Combine scores ---
    # Add the capacity bonus to the tight fit score.
    # The tight_fit_scores from tanh are in [-1, 1].
    # We want to shift these to be non-negative and ensure tight fits are generally higher.
    # Add `1` to tanh scores to shift range to [0, 2].
    # Then, we can add the capacity bonus.
    combined_scores_raw = (1 + tight_fit_scores) + capacity_bonus_scores

    # Apply a final scaling or activation to ensure scores are in a reasonable range,
    # typically [0, 1], and to emphasize higher scores.
    # A simple way is to normalize by the max possible value or apply another tanh.
    # Let's normalize by the maximum possible combined score (theoretically) or empirically.
    # The maximum value of (1 + tanh) is 2. The maximum capacity bonus is capacity_bonus_weight.
    # So, max raw score is roughly 2 + capacity_bonus_weight.
    
    # Scale combined_scores_raw to [0, 1]
    # The range of (1 + tight_fit_scores) is [0, 2].
    # The range of capacity_bonus_scores is [0, capacity_bonus_weight].
    # Max raw score ~ 2 + capacity_bonus_weight.
    # Min raw score ~ 0 + 0 = 0 (if tight_fit = -1 and bonus = 0).

    # A simple scaling by a factor slightly larger than the max possible raw score.
    # Or, use tanh again to compress and normalize.
    # Let's use tanh to re-normalize, it provides a smooth distribution and maps to [0, 1].
    # We need to scale the input to tanh appropriately.
    # Let's scale `combined_scores_raw` such that a typical 'good' score is mapped to positive tanh input.
    # Consider a typical tight fit with small waste (e.g., waste=0, tanh=1) plus bonus.
    # Raw score around 2 + bonus.
    # A scaling factor of ~1/(2 + capacity_bonus_weight) could work.
    
    # Let's normalize by the maximum achievable raw score to get values roughly in [0, 1].
    # Max value of (1 + tanh) is 2. Max value of capacity_bonus_scores is capacity_bonus_weight.
    # So theoretical max raw score is approx 2 + capacity_bonus_weight.
    # A safer normalization is by the actual max observed raw score.
    
    max_combined_raw = np.max(combined_scores_raw) if len(combined_scores_raw) > 0 else 1
    if max_combined_raw > epsilon:
        final_fitting_scores = combined_scores_raw / max_combined_raw
    else:
        # If all scores are zero (e.g., no fitting bins, or all calculations resulted in zero),
        # this means something went wrong or no bins are viable.
        final_fitting_scores = np.zeros_like(combined_scores_raw)

    # Add a small random noise for exploration, ensuring it doesn't drastically change rankings.
    # Noise should be small relative to the score differences.
    exploration_noise_magnitude = 0.02 # Small value
    noise = np.random.uniform(-exploration_noise_magnitude, exploration_noise_magnitude, size=len(final_fitting_scores))
    final_fitting_scores += noise

    # Ensure scores stay within a reasonable range after adding noise, e.g., [0, 1].
    final_fitting_scores = np.clip(final_fitting_scores, 0.0, 1.0)

    # Assign these final scores to the appropriate bins
    priorities[fitting_indices] = final_fitting_scores

    # Bins that cannot fit the item will retain their initial priority of 0.0

    return priorities
```
