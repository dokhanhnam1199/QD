{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-based Best Fit strategy.\n\n    This strategy aims to mimic the \"Best Fit\" heuristic within a softmax framework.\n    It prioritizes bins where placing the item results in the least remaining capacity\n    (i.e., the \"tightest\" fit). Bins that cannot accommodate the item receive zero priority.\n    The priorities are generated using a softmax function on desirability scores,\n    where a higher score indicates a better fit (less remaining capacity after packing).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n        For fitting bins, scores are derived from the resulting remaining capacity,\n        and transformed by softmax to represent probabilities or preferences.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        # Calculate the remaining capacity *after* placing the item for eligible bins\n        valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        resulting_remain_cap = valid_bins_remain_cap - item\n\n        # Define a desirability score for each fitting bin.\n        # For \"Best Fit\", we want to minimize `resulting_remain_cap`.\n        # A higher desirability score should correspond to a lower `resulting_remain_cap`.\n        # We can use a score that is inversely proportional to `resulting_remain_cap`.\n        # To avoid division by zero and ensure positive scores, we can add a small constant\n        # or use a transformation like `1 / (slack + 1)`.\n        # A simple approach is to use `max_possible_slack - slack`, where `max_possible_slack`\n        # is the maximum possible remaining capacity among fitting bins.\n        # Or, more directly, we can use a desirability score that is higher when `resulting_remain_cap` is smaller.\n        # Let's use `-resulting_remain_cap` as a raw desirability, so smaller slack (closer to 0) is better.\n        # For softmax, we want scores that are generally positive for exponentiation.\n        # A good score is `1.0 / (resulting_remain_cap + 1.0)` which is high when `resulting_remain_cap` is small.\n\n        # Let's use a score that emphasizes bins with very little slack.\n        # A slightly larger slack should yield a significantly lower score.\n        # We can use an exponential decay, or a simple inverse relationship.\n        # `score = 1.0 / (resulting_remain_cap + epsilon)` where epsilon is a small positive number.\n        # Let's use `1.0 / (resulting_remain_cap + 1e-6)` to ensure stability and positive values.\n\n        desirability_scores = 1.0 / (resulting_remain_cap + 1e-6)\n\n        # Apply softmax to convert desirability scores into a probability-like distribution.\n        # A temperature parameter can control the \"softness\" of the distribution.\n        # A lower temperature makes the distribution sharper, favoring the best bins more.\n        # A higher temperature makes it flatter, giving more similar probabilities.\n        temperature = 1.0  # Tunable parameter\n\n        # Avoid numerical instability with very large desirability scores by clipping or normalizing first,\n        # or by using a stable softmax implementation if available.\n        # Here, we use `exp(score / temperature)`.\n\n        try:\n            exp_scores = np.exp(desirability_scores / temperature)\n            sum_exp_scores = np.sum(exp_scores)\n            if sum_exp_scores > 0:\n                softmax_probabilities = exp_scores / sum_exp_scores\n            else:\n                # If all desirability scores are extremely small (or negative if we didn't ensure positivity),\n                # softmax might result in zeros. Assign uniform probability in such edge cases.\n                softmax_probabilities = np.ones_like(desirability_scores) / len(desirability_scores)\n        except OverflowError:\n            # Handle potential overflow if desirability_scores are too large\n            # A common approach is to subtract the max score before exponentiation.\n            max_score = np.max(desirability_scores)\n            stable_scores = desirability_scores - max_score\n            exp_scores = np.exp(stable_scores / temperature)\n            sum_exp_scores = np.sum(exp_scores)\n            if sum_exp_scores > 0:\n                softmax_probabilities = exp_scores / sum_exp_scores\n            else:\n                softmax_probabilities = np.ones_like(desirability_scores) / len(desirability_scores)\n\n\n        # Place the calculated probabilities back into the original priorities array\n        priorities[can_fit_mask] = softmax_probabilities\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Softmax-Based Fit.\n\n    This version refines the priority calculation by directly penalizing remaining\n    capacity. Bins that leave less remaining capacity after packing the item are\n    given higher priority. This encourages tighter fits and aims to minimize\n    wasted space. The scores are transformed using an exponential function\n    (similar to softmax) to ensure that bins with better fits (less remaining capacity)\n    have significantly higher probabilities, while still allowing some probability\n    for less optimal fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        # No bin can fit the item, return all zeros\n        return priorities\n\n    # Calculate the \"desirability\" score for bins that can fit the item.\n    # A higher score means less remaining capacity after packing.\n    # We want to minimize (bins_remain_cap - item).\n    # For a softmax-like score where higher is better, we use the negative\n    # of the remaining capacity, i.e., -(bins_remain_cap[can_fit_mask] - item).\n    # This is equivalent to item - bins_remain_cap[can_fit_mask].\n    # Adding a small constant could help if item sizes are very close to capacities,\n    # but the current formulation directly favors less leftover space.\n    remaining_capacity_after_packing = bins_remain_cap[can_fit_mask] - item\n    \n    # To ensure scores are positive and higher for better fits, we can use:\n    # score = 1.0 / (remaining_capacity_after_packing + 1e-9)  # Small epsilon for stability\n    # Alternatively, and perhaps more robustly for softmax, we can use the negative\n    # of the remaining capacity, as implemented in v1, which favors less leftover space.\n    # Let's stick to the logic of favoring bins with less leftover space:\n    # score = -(remaining_capacity_after_packing)\n    \n    # For softmax, it's often beneficial to have scores that are not too extreme,\n    # or to scale them. A simple approach is to use the negative of the remaining\n    # capacity. Let's use this: higher score means less remaining capacity.\n    scores_for_fitting_bins = -(remaining_capacity_after_packing)\n    \n    # Apply softmax-like transformation.\n    # Subtracting the maximum score before exponentiation for numerical stability.\n    max_score = np.max(scores_for_fitting_bins)\n    exp_scores = np.exp(scores_for_fitting_bins - max_score)\n\n    sum_exp_scores = np.sum(exp_scores)\n\n    if sum_exp_scores > 1e-9:  # Check for numerical stability\n        priorities[can_fit_mask] = exp_scores / sum_exp_scores\n    else:\n        # If all scores are very negative, resulting in near-zero exponentials,\n        # distribute probability equally among bins that *can* fit.\n        num_fitting_bins = np.sum(can_fit_mask)\n        if num_fitting_bins > 0:\n            priorities[can_fit_mask] = 1.0 / num_fitting_bins\n\n    return priorities\n\n[Reflection]\nPrioritize minimal remaining capacity. Stabilize softmax with score subtraction.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}