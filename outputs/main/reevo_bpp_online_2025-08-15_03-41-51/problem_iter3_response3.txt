```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin.

    This version prioritizes bins with minimal remaining capacity (tightest fits)
    using a smooth ranking function (sigmoid-like). It also adds a small bonus
    for larger remaining capacities to balance immediate tightness with future
    flexibility.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
        Higher scores indicate higher priority.
    """
    num_bins = len(bins_remain_cap)
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Determine which bins can fit the item
    can_fit_mask = bins_remain_cap >= item

    # Calculate scores only for bins that can fit the item
    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]

    if len(fitting_bins_remain_cap) == 0:
        return priorities  # No bin can fit the item

    # --- Heuristic 1: Prioritize tightest fits (minimal remaining capacity) ---
    # Use a sigmoid-like function to rank the remaining capacities.
    # A bin with capacity `c` will have a score related to `c - item`.
    # We want smaller `c - item` to have higher priority.
    # Let's transform `c` to `1 / (c - item + epsilon)` for tight fit, then scale.
    # A simpler approach is to directly use the inverse of the difference,
    # and apply a sigmoid-like transformation to smooth it.

    # Calculate the "tightness" score: smaller is better (tighter fit)
    # Add a small epsilon to avoid division by zero or extremely large values.
    epsilon_tightness = 1e-6
    tightness_scores = 1.0 / (fitting_bins_remain_cap - item + epsilon_tightness)

    # Smooth the tightness scores using a sigmoid-like function.
    # We want to map smaller `tightness_scores` (closer to 0 for tightest fits)
    # to higher priority values.
    # A function like `1 / (1 + exp(-k * (x - offset)))` would map small x to high values.
    # Here, x = tightness_scores. Let's aim for a concave shape where scores
    # are high for small tightness_scores.
    # Using a simple inverse scaling and then sigmoid can be complex.
    # A more direct approach: map the differences `fitting_bins_remain_cap - item`
    # to priorities. Small difference -> high priority.
    # Let's use `exp(-k * diff)` where `k` is a scaling factor.
    # A larger `k` makes the function steeper, focusing on very tight fits.
    k_tightness = 5.0  # Sensitivity parameter for tightness
    tightness_priority = np.exp(-k_tightness * (fitting_bins_remain_cap - item))

    # --- Heuristic 2: Small bonus for larger remaining capacities (future flexibility) ---
    # This encourages not packing too tightly if there's a slightly less tight fit
    # that still leaves significant space.
    # We can add a small bonus based on the `fitting_bins_remain_cap`.
    # A simple linear bonus or a scaled sigmoid might work.
    # Let's use a scaled version of the remaining capacity, perhaps with a saturation.
    bonus_scale = 0.05  # How much bonus to give for remaining capacity
    flexibility_bonus = bonus_scale * fitting_bins_remain_cap

    # Combine the heuristics
    # Simple addition: weighted sum of tightness priority and flexibility bonus.
    # Adjust weights as needed. Let's give more weight to tightness.
    w_tightness = 1.0
    w_flexibility = 0.5

    combined_scores = w_tightness * tightness_priority + w_flexibility * flexibility_bonus

    # Normalize the combined scores for bins that can fit the item
    if np.max(combined_scores) > 0:
        normalized_scores = combined_scores / np.max(combined_scores)
    else:
        normalized_scores = np.zeros_like(combined_scores)

    # Assign the calculated normalized scores to the original positions
    priorities[can_fit_mask] = normalized_scores

    return priorities
```
