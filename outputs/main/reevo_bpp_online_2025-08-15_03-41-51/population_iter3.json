[
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-based Best Fit strategy.\n\n    This strategy aims to mimic the \"Best Fit\" heuristic within a softmax framework.\n    It prioritizes bins where placing the item results in the least remaining capacity\n    (i.e., the \"tightest\" fit). Bins that cannot accommodate the item receive zero priority.\n    The priorities are generated using a softmax function on desirability scores,\n    where a higher score indicates a better fit (less remaining capacity after packing).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n        For fitting bins, scores are derived from the resulting remaining capacity,\n        and transformed by softmax to represent probabilities or preferences.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        # Calculate the remaining capacity *after* placing the item for eligible bins\n        valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        resulting_remain_cap = valid_bins_remain_cap - item\n\n        # Define a desirability score for each fitting bin.\n        # For \"Best Fit\", we want to minimize `resulting_remain_cap`.\n        # A higher desirability score should correspond to a lower `resulting_remain_cap`.\n        # We can use a score that is inversely proportional to `resulting_remain_cap`.\n        # To avoid division by zero and ensure positive scores, we can add a small constant\n        # or use a transformation like `1 / (slack + 1)`.\n        # A simple approach is to use `max_possible_slack - slack`, where `max_possible_slack`\n        # is the maximum possible remaining capacity among fitting bins.\n        # Or, more directly, we can use a desirability score that is higher when `resulting_remain_cap` is smaller.\n        # Let's use `-resulting_remain_cap` as a raw desirability, so smaller slack (closer to 0) is better.\n        # For softmax, we want scores that are generally positive for exponentiation.\n        # A good score is `1.0 / (resulting_remain_cap + 1.0)` which is high when `resulting_remain_cap` is small.\n\n        # Let's use a score that emphasizes bins with very little slack.\n        # A slightly larger slack should yield a significantly lower score.\n        # We can use an exponential decay, or a simple inverse relationship.\n        # `score = 1.0 / (resulting_remain_cap + epsilon)` where epsilon is a small positive number.\n        # Let's use `1.0 / (resulting_remain_cap + 1e-6)` to ensure stability and positive values.\n\n        desirability_scores = 1.0 / (resulting_remain_cap + 1e-6)\n\n        # Apply softmax to convert desirability scores into a probability-like distribution.\n        # A temperature parameter can control the \"softness\" of the distribution.\n        # A lower temperature makes the distribution sharper, favoring the best bins more.\n        # A higher temperature makes it flatter, giving more similar probabilities.\n        temperature = 1.0  # Tunable parameter\n\n        # Avoid numerical instability with very large desirability scores by clipping or normalizing first,\n        # or by using a stable softmax implementation if available.\n        # Here, we use `exp(score / temperature)`.\n\n        try:\n            exp_scores = np.exp(desirability_scores / temperature)\n            sum_exp_scores = np.sum(exp_scores)\n            if sum_exp_scores > 0:\n                softmax_probabilities = exp_scores / sum_exp_scores\n            else:\n                # If all desirability scores are extremely small (or negative if we didn't ensure positivity),\n                # softmax might result in zeros. Assign uniform probability in such edge cases.\n                softmax_probabilities = np.ones_like(desirability_scores) / len(desirability_scores)\n        except OverflowError:\n            # Handle potential overflow if desirability_scores are too large\n            # A common approach is to subtract the max score before exponentiation.\n            max_score = np.max(desirability_scores)\n            stable_scores = desirability_scores - max_score\n            exp_scores = np.exp(stable_scores / temperature)\n            sum_exp_scores = np.sum(exp_scores)\n            if sum_exp_scores > 0:\n                softmax_probabilities = exp_scores / sum_exp_scores\n            else:\n                softmax_probabilities = np.ones_like(desirability_scores) / len(desirability_scores)\n\n\n        # Place the calculated probabilities back into the original priorities array\n        priorities[can_fit_mask] = softmax_probabilities\n\n    return priorities",
    "response_id": 0,
    "obj": 5.534503390506582,
    "cyclomatic_complexity": 5.0,
    "halstead": 176.41891628622352,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response1.txt_stdout.txt",
    "code_path": "problem_iter2_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This priority function implements a modified First Fit strategy,\n    often referred to as \"Almost Full Fit\" or \"Best Fit\". It prioritizes\n    bins that can accommodate the item with the least amount of remaining\n    capacity (i.e., the tightest fit). This is because packing an item\n    into a bin that is nearly full leaves less \"wasted\" space in that bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority. Bins that cannot fit the item\n        will have a very low priority.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # We only consider bins that have enough remaining capacity for the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, we want to prioritize those that leave\n    # the least amount of remaining space after the item is placed.\n    # This means minimizing (bins_remain_cap - item).\n    # A common way to turn minimization into maximization for priority is to\n    # use the negative of the difference or an inverse.\n    # Using the negative of the difference directly works well:\n    # A smaller (bins_remain_cap - item) results in a less negative (higher) score.\n    # E.g., if item=0.3, bin_caps=[0.35, 0.4, 0.5]\n    # diffs: [0.05, 0.1, 0.2]\n    # scores: [-0.05, -0.1, -0.2] -> bin_cap=0.35 is prioritized.\n    \n    # To ensure higher priority for tighter fits, we can assign a score that is\n    # inversely proportional to the remaining capacity *after* placing the item.\n    # Specifically, we want to maximize `1 / (bins_remain_cap - item + epsilon)`\n    # or, equivalently, minimize `bins_remain_cap - item`.\n    \n    # A simple and effective way is to use the negative of the slack space:\n    # `slack = bins_remain_cap - item`\n    # Priority = -slack\n    # This means smaller positive slacks (tighter fits) get higher priority.\n\n    # To avoid issues with floating point precision or bins that are exactly full,\n    # we can also consider a score that peaks when bins_remain_cap is exactly item.\n    # A common approach is to use a function that is maximized at 0 for `bins_remain_cap - item`.\n    # For example, `-abs(bins_remain_cap - item)` or `1 / (1 + abs(bins_remain_cap - item))`.\n    # The former `-(bins_remain_cap - item)` is simpler and achieves the goal for\n    # bins that can fit the item.\n\n    # Let's use the negative of the slack space.\n    # We add a small epsilon to the `bins_remain_cap` before calculating slack\n    # to ensure that even if `bins_remain_cap` is exactly `item`, the slack\n    # is a small positive number, leading to a priority close to 0, and\n    # preventing any zero or negative slack values from causing issues if\n    # they were to be inverted directly without care.\n    # However, simply taking `- (bins_remain_cap - item)` is generally sufficient\n    # and more direct for \"Best Fit\" / \"Almost Full Fit\".\n\n    # Prioritize bins where `bins_remain_cap - item` is minimized.\n    # This translates to a score that is maximized for these bins.\n    # We can use `-(bins_remain_cap - item)` which means smaller positive values of `bins_remain_cap - item`\n    # result in higher (less negative) scores.\n    \n    # A small adjustment for \"almost full fit\" could be to slightly penalize bins\n    # that are *too* full (i.e., `bins_remain_cap` is very large), even if they can fit.\n    # However, standard \"Almost Full Fit\" or \"Best Fit\" primarily focuses on minimizing slack.\n\n    # Let's refine the score to be `- (bins_remain_cap - item)` for bins that fit.\n    # This score directly reflects the remaining space after packing.\n    # Higher priority for smaller remaining space.\n\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # For bins that cannot fit the item, their priority should be very low so they are never chosen.\n    # Setting them to negative infinity ensures this.\n    priorities[~can_fit_mask] = -np.inf\n\n    # Note: This heuristic is essentially \"Best Fit\". It prioritizes the bin\n    # that will have the least remaining capacity after the item is placed.\n    # This is often a good strategy for \"Almost Full Fit\" as it aims to\n    # utilize space efficiently in bins that are already somewhat full.\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 39.863137138648355,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response2.txt_stdout.txt",
    "code_path": "problem_iter2_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Minimal Positive Remaining Capacity strategy.\n\n    This strategy prioritizes bins that, after placing the item, will have the smallest\n    positive remaining capacity. This encourages a \"tight fit\" and aims to minimize\n    wasted space in bins that are nearly full.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity if the item is placed in each bin\n    remaining_capacities_after_placement = bins_remain_cap - item\n\n    # Filter out bins where the item cannot fit (remaining capacity would be negative)\n    valid_bins_mask = remaining_capacities_after_placement >= 0\n    \n    # If no bin can fit the item, return zeros (no priority)\n    if not np.any(valid_bins_mask):\n        return np.zeros_like(bins_remain_cap)\n        \n    # Get the remaining capacities for the bins where the item fits\n    valid_remaining_capacities = remaining_capacities_after_placement[valid_bins_mask]\n    \n    # We want to prioritize bins with the smallest positive remaining capacity.\n    # A simple way to do this is to assign a priority score that is inversely\n    # proportional to the remaining capacity. A smaller remaining capacity should\n    # result in a higher priority score.\n    #\n    # To avoid division by zero (if remaining capacity is exactly 0), we add a small epsilon.\n    # The inverse of a small positive number is a large number, effectively giving\n    # the highest priority to bins with the tightest fit.\n    \n    epsilon = 1e-9  # A small value to avoid division by zero\n    \n    # Calculate priorities for the valid bins. Higher value means higher priority.\n    # The smaller the `valid_remaining_capacities`, the larger the priority score.\n    priorities_for_valid_bins = 1.0 / (valid_remaining_capacities + epsilon)\n    \n    # Initialize the full priorities array with zeros\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Assign the calculated priorities to the valid bins\n    priorities[valid_bins_mask] = priorities_for_valid_bins\n    \n    return priorities",
    "response_id": 2,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 53.30296890880645,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements a Sigmoid-based priority function for online Bin Packing.\n\n    This function prioritizes bins that offer the \"tightest fit\" for the item.\n    A tight fit is defined as a bin where the remaining capacity is just\n    slightly larger than the item's size. The priority is calculated using\n    a sigmoid function, which provides a smooth score that peaks when the\n    difference between remaining capacity and item size is minimal (and positive).\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can potentially fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit the item, return all zeros.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the \"gap\" for bins that can fit the item.\n    # The gap is the difference between the bin's remaining capacity and the item's size.\n    # A smaller gap indicates a tighter fit.\n    gaps = bins_remain_cap[can_fit_mask] - item\n\n    # Use a sigmoid function to map the gap to a priority score.\n    # We want to assign higher scores to smaller gaps.\n    # The sigmoid function `1 / (1 + exp(-x))` maps `x` to `(0, 1)`.\n    # To prioritize smaller gaps, we can use `sigmoid(-k * gap)`, where `k` is a steepness factor.\n    # A smaller `gap` results in a less negative argument to the sigmoid, yielding a higher score.\n    # For a gap of 0 (perfect fit), the sigmoid argument is 0, and the score is 0.5.\n    # For larger gaps, the sigmoid argument becomes more negative, and the score approaches 0.\n\n    steepness = 10.0  # This parameter controls how quickly the priority drops as the gap increases.\n                     # A higher value means a stronger preference for very tight fits.\n                     # It can be tuned based on experimental results or problem specifics.\n\n    # Calculate the sigmoid argument: -steepness * gap\n    sigmoid_input = -steepness * gaps\n\n    # Apply the sigmoid function. The output will be in the range (0, 1).\n    # These are the priority scores for the bins that can fit the item.\n    priority_scores = 1 / (1 + np.exp(-sigmoid_input))\n\n    # Place these calculated priority scores back into the full priorities array.\n    priorities[can_fit_mask] = priority_scores\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 87.56842503028855,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response4.txt_stdout.txt",
    "code_path": "problem_iter2_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Best Fit strategy.\n\n    This strategy prioritizes bins that have the smallest remaining capacity *after*\n    packing the item, effectively minimizing wasted space. Bins where the item\n    does not fit are given a very low priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate a preferred bin.\n    \"\"\"\n    # Initialize priorities to a very low value (negative infinity) for all bins.\n    # This ensures that only bins where the item can fit will receive a positive or zero score.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity after placement.\n    # The goal is to minimize this value.\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # To prioritize the smallest remaining capacity, we can assign a priority based on the\n    # negative of this value. This makes smaller positive remaining capacities result in\n    # higher (less negative) priority scores.\n    # For example, if remaining capacity after placement is 0, priority is 0.\n    # If remaining capacity after placement is 1, priority is -1.\n    # If remaining capacity after placement is 5, priority is -5.\n    # Thus, 0 > -1 > -5, correctly prioritizing the tightest fit.\n    priorities[can_fit_mask] = -remaining_after_placement\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 30.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response5.txt_stdout.txt",
    "code_path": "problem_iter2_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, prioritizing tighter fits.\n\n    This strategy aims to fill bins as completely as possible. Bins that have just\n    enough remaining capacity to fit the item (i.e., the smallest positive difference\n    between remaining capacity and item size) will receive the highest priority.\n    For bins where the item does not fit, the priority is set to a very low value.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities with a very low value to represent infeasible bins\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    # For bins where the item fits, calculate the \"gap\" or excess capacity.\n    # A smaller gap means a tighter fit.\n    # We want to prioritize smaller gaps, so we can use the negative of the gap\n    # or an inverse function of the gap. Using negative of the gap directly\n    # makes higher values correspond to tighter fits.\n    gaps = bins_remain_cap[fit_mask] - item\n    priorities[fit_mask] = -gaps\n\n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 30.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response6.txt_stdout.txt",
    "code_path": "problem_iter2_code6.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Tunable Sigmoid Fit Score.\n\n    This version introduces a tunable parameter `steepness` to control how quickly the\n    priority score drops as the gap (remaining capacity - item size) increases.\n    It also adds a small offset to the sigmoid argument to slightly favor bins that\n    are not perfectly filled, potentially leaving more room for future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to a very low value for bins that cannot fit the item.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can accommodate the item\n    possible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(possible_bins_mask):\n        # No bin can fit the item, return all negative infinities.\n        return priorities\n\n    # Calculate the \"gap\" for possible bins (how much space is left after placing the item)\n    gaps = bins_remain_cap[possible_bins_mask] - item\n\n    # Tunable parameter to control the steepness of the sigmoid.\n    # A higher value means the priority drops more sharply as the gap increases.\n    # A value of 0 would make all fitting bins have a score of 0.5 (if offset is 0).\n    steepness = 10.0  # Increased steepness for more distinct prioritization\n\n    # Introduce a small negative offset to the argument.\n    # This means that a gap of 0 (perfect fit) will result in sigmoid(-offset),\n    # which is slightly less than 0.5. Larger gaps will result in scores even further below 0.5.\n    # This can encourage using bins that have a small positive gap, potentially\n    # leaving perfectly filled bins for smaller items if available.\n    # Experimentation is needed to find the optimal offset.\n    # Let's try to make the \"ideal\" gap slightly positive, e.g., `optimal_gap = 0.1 * item`.\n    # We want `steepness * (optimal_gap - gap)` to be around 0.\n    # `steepness * (0.1 * item - gap)`\n    # This means we want `gap` to be slightly less than `0.1 * item` for highest priority.\n    # Or, let's consider the argument `steepness * (item - bins_remain_cap[i])`.\n    # We want this to be high when `bins_remain_cap[i]` is slightly larger than `item`.\n    # Let's scale the negative gap by `steepness`.\n    # `sigmoid_arg = steepness * (item - bins_remain_cap[possible_bins_mask])`\n    # `sigmoid_arg = -steepness * (bins_remain_cap[possible_bins_mask] - item)`\n    # This assigns 0.5 to a perfect fit, lower to larger gaps, and higher to negative gaps (not possible here).\n    #\n    # To slightly prefer bins that are NOT perfectly filled, we can shift the sigmoid.\n    # Let's aim for the peak priority to be at a small positive gap.\n    # Consider `sigmoid(k * (ideal_gap - current_gap))`.\n    # If we want `ideal_gap = 0`, this is `sigmoid(k * -gap)`.\n    #\n    # Let's consider a function that rewards tightness: `f(gap)`.\n    # `f(gap) = exp(-steepness * gap)` could be an alternative, but it's not bounded between 0 and 1.\n    #\n    # Let's stick to the sigmoid but adjust the argument.\n    # We want to prioritize smaller gaps. `sigmoid(-steepness * gap)` does this, with peak at 0.5.\n    # To shift the peak to a small positive gap `g_ideal`, we can use `sigmoid(steepness * (g_ideal - gap))`.\n    # Let's choose `g_ideal` to be a small fraction of the item size, e.g., 10% of item size.\n    # This introduces a dependency on the item size, which might be complex.\n    #\n    # A simpler approach: use `sigmoid(-steepness * gap)` and then scale or offset the output.\n    # Or, slightly modify the input:\n    # Let's aim for the highest priority when `bins_remain_cap[i] - item` is small and positive.\n    # We can use `sigmoid(A - B * gap)` where `A` shifts the curve and `B` controls steepness.\n    # If we want the peak at `gap = 0`, then `A = 0` works (for `sigmoid(B * (-gap))`).\n    #\n    # To reward slightly less full bins, let's try to make the score decrease faster.\n    # Let's modify the sigmoid argument:\n    # Instead of just `-steepness * gap`, consider `-steepness * gap - offset`.\n    # This shifts the entire curve to the left.\n    # For `gap = 0`, the argument becomes `-offset`. `sigmoid(-offset)` is < 0.5.\n    # This means perfect fits get lower scores than before.\n    #\n    # The goal is to rank bins. The absolute values don't matter as much as the relative order.\n    # `sigmoid(-steepness * gap)` already provides a good ranking for \"tightest fit\".\n    #\n    # Let's introduce a small constant to the sigmoid argument to shift the peak.\n    # A small positive constant in `sigmoid(-steepness * gap + const)` will increase scores for all gaps.\n    # A small negative constant `const` will decrease scores.\n    #\n    # Let's reconsider the goal: prioritize bins that are \"almost full\" but can still fit.\n    # This means `bins_remain_cap[i]` should be minimized, subject to `bins_remain_cap[i] >= item`.\n    #\n    # Consider the inverse of the gap, but normalized.\n    # Let `normalized_gap = gap / max_possible_gap`. This is still problematic if max_possible_gap is 0.\n    #\n    # Let's try a simple scaling of the gap and an offset.\n    # `sigmoid_input = steepness * (item - bins_remain_cap[possible_bins_mask])`\n    # `sigmoid_input = -steepness * gaps`\n    #\n    # Let's add a small bias to favor bins that are not completely full.\n    # This is to avoid situations where a bin is filled to absolute capacity, potentially\n    # making it unusable for even the smallest future items.\n    # A small positive value `epsilon` added to the `item` size might simulate this.\n    # `effective_item_size = item + epsilon`\n    # `gaps_adjusted = bins_remain_cap[possible_bins_mask] - effective_item_size`\n    # `sigmoid_arg = -steepness * gaps_adjusted`\n    #\n    # Let's try a simple adjustment to the input of the sigmoid:\n    # Instead of `sigmoid(-steepness * gap)`, let's use `sigmoid(-steepness * gap + adjustment)`.\n    # If `adjustment` is positive, it pushes scores higher for all gaps.\n    # If `adjustment` is negative, it pushes scores lower.\n    #\n    # Let's try to favor bins with a small positive gap, say up to `item * 0.1`.\n    # We want the score to be high in this range.\n    #\n    # Let's use `sigmoid(k * (target_capacity - current_remaining_capacity))`\n    # Target capacity should be `item`. So, `sigmoid(k * (item - bins_remain_cap[i]))`.\n    # This is `sigmoid(-k * gap)`.\n    #\n    # To slightly favor bins with a small positive gap (e.g., `gap = 0.1 * item`),\n    # we want the input to sigmoid to be slightly positive.\n    # `sigmoid(k * (ideal_gap - gap))`. If `ideal_gap = 0.1 * item`, and `gap` is a bit smaller.\n    #\n    # Let's use a simpler approach that's common: scale the gap.\n    # `scaled_gap = gap / item` (if item > 0). This makes it relative.\n    # Then `sigmoid(-steepness * scaled_gap)`.\n    # This way, the \"tightness\" is relative to the item size.\n    #\n    # Let's try to slightly penalize perfect fits (gap=0) to encourage slight overflow.\n    # This means the peak of our priority function should be at a small positive gap.\n    # We can achieve this by shifting the sigmoid input.\n    # `sigmoid(steepness * (ideal_gap - gap))`\n    # Let `ideal_gap = 0.05 * item` (a small fraction of the item size).\n    #\n    # If item is 0, this formula would be problematic.\n    # Let's assume item > 0.\n    #\n    # `ideal_gap = 0.05 * item`\n    # `sigmoid_arg = steepness * (ideal_gap - gaps)`\n    #\n    # Example: item = 10. steepness = 10. ideal_gap = 0.5.\n    # If gap = 0.1 (bin cap = 10.1): sigmoid_arg = 10 * (0.5 - 0.1) = 4.0. Score = sigmoid(4.0) ~ 0.98\n    # If gap = 0.5 (bin cap = 10.5): sigmoid_arg = 10 * (0.5 - 0.5) = 0.0. Score = sigmoid(0.0) = 0.5\n    # If gap = 1.0 (bin cap = 11.0): sigmoid_arg = 10 * (0.5 - 1.0) = -5.0. Score = sigmoid(-5.0) ~ 0.007\n    # If gap = 0.0 (bin cap = 10.0): sigmoid_arg = 10 * (0.5 - 0.0) = 5.0. Score = sigmoid(5.0) ~ 0.993 (Oops, perfect fit is highest!)\n\n    # The goal is usually to fill bins as much as possible. So tightest fit is preferred.\n    # Let's go back to prioritizing the minimum non-negative gap.\n    # `sigmoid(-steepness * gap)` where peak is at gap=0.\n    #\n    # To make `priority_v2` distinct and tunable, let's introduce a parameter\n    # that influences the \"ideal\" tightness.\n    #\n    # `fit_preference`:\n    # - `fit_preference = 0`: Prioritize bins that are exactly full (gap = 0).\n    # - `fit_preference = 1`: Prioritize bins that are somewhat full, but leave a small buffer (e.g., gap = 5% of item size).\n    # - `fit_preference = -1`: Prioritize bins that are less full (larger gaps). This is generally not good.\n\n    # Let's aim for `fit_preference` to control the \"ideal gap\".\n    # `ideal_gap = fit_preference * item * 0.1` (0.1 is a scaling factor for preference strength)\n    # If `fit_preference = 0`, `ideal_gap = 0`.\n    # If `fit_preference = 1`, `ideal_gap = 0.1 * item`.\n    # If `fit_preference = -1`, `ideal_gap = -0.1 * item` (problematic, means item must be smaller than capacity).\n\n    # Let's simplify: use `steepness` and a fixed adjustment that shifts the peak slightly.\n    # `sigmoid(-steepness * gap)` has peak at `gap = 0`.\n    # To shift the peak to `gap = ideal_gap`, use `sigmoid(steepness * (ideal_gap - gap))`.\n    #\n    # Let's define `ideal_gap` as a fraction of the item size.\n    # `ideal_gap_fraction = 0.05`  # Aim for a gap that's 5% of the item's size\n    # `ideal_gap = ideal_gap_fraction * item`\n    #\n    # Ensure `ideal_gap` is not negative and not larger than typical gaps.\n    # `ideal_gap = max(0.0, ideal_gap)` # Ensure non-negative\n\n    # The core idea of Sigmoid Fit Score is to rank by tightness.\n    # `sigmoid(-steepness * gap)` achieves this with peak at `gap=0`.\n    # The \"improvement\" can be in the tuning of `steepness` and potentially\n    # how we normalize or bound the `gap` for the sigmoid input.\n\n    # Let's consider a robust scaling of the gap:\n    # `scaled_gap = gap / max(1, item)` # Avoid division by zero and scale by item size.\n    # This makes the priority sensitive to the relative tightness.\n    # Then `sigmoid(-steepness * scaled_gap)`.\n    #\n    # Let's try a combination: `steepness` for sensitivity and a small `gap_offset`\n    # to slightly shift the priority away from perfect fits.\n\n    # Parameter to control how sensitive the priority is to the gap.\n    # Higher value means smaller gaps are much more preferred than larger gaps.\n    steepness = 15.0\n\n    # Parameter to slightly offset the ideal fit.\n    # A positive `gap_preference_offset` means we slightly prefer bins that are NOT perfectly filled.\n    # A negative offset would strongly prefer perfectly filled bins.\n    # Let's try to prefer bins where the remaining capacity is just slightly larger than the item.\n    # This corresponds to a small positive gap.\n    # If `gap = 0`, we want the input to sigmoid to be low.\n    # If `gap = ideal_gap > 0`, we want the input to sigmoid to be close to 0.\n    # So, `k * (ideal_gap - gap)`.\n    #\n    # Let's use `gap` directly, but adjust the sigmoid mapping.\n    # `sigmoid_arg = -steepness * gaps` -> peak at gap = 0.\n    #\n    # Alternative: `1 - sigmoid(steepness * gaps)` -> peak at gap = 0.\n    #\n    # Consider `max(0, 1 - steepness * gaps)` -- not smooth.\n\n    # Let's stick to sigmoid and tune its input:\n    # `sigmoid(steepness * (B - A * gap))` where B is offset and A is steepness multiplier.\n    # `sigmoid(steepness * (0.1 - gap))` -- this is very dependent on scale.\n    #\n    # A common approach is to normalize the gap:\n    # `normalized_gap = gap / max_capacity_of_bin` (but capacity is fixed, remaining varies)\n    #\n    # Let's use a simple sigmoid where `steepness` controls the curve.\n    # And we can add a small constant to the sigmoid argument to nudge it.\n    # `sigmoid_arg = -steepness * gaps + bias`\n    # `bias = 0.0` means peak at `gap = 0`.\n    # `bias = 1.0` means `sigmoid(-steepness * gap + 1.0)`.\n    # If `gap=0`, arg = 1.0, score = sigmoid(1.0) > 0.5.\n    # If `gap=0.1`, arg = -steepness*0.1 + 1.0. If steepness=10, arg=0.0, score=0.5.\n    # So, `bias` shifts the point where score is 0.5.\n    #\n    # Let's make `steepness` and `bias` tunable.\n    steepness = 20.0  # Higher steepness for better discrimination\n    bias = 0.2        # Shift the sigmoid to favor slightly larger gaps (less than perfect fits)\n\n    # Calculate the argument for the sigmoid function\n    # We want to penalize larger gaps, so `gaps` should be subtracted from something.\n    # `bias - steepness * gaps`\n    # If gap is small positive, `bias - steepness * gap` is positive and large (if gap is much smaller than bias/steepness).\n    # If gap is 0, arg = bias.\n    # If gap is larger than `bias/steepness`, arg becomes negative.\n    sigmoid_arg = bias - steepness * gaps\n\n    # Apply the sigmoid function to get priorities\n    # `1 / (1 + exp(-x))` maps values to [0, 1].\n    # For `gap = 0`, score = `sigmoid(bias)`.\n    # For `gap = bias / steepness`, score = `sigmoid(0) = 0.5`.\n    # For `gap > bias / steepness`, score < 0.5.\n    # This means bins with gaps larger than `bias / steepness` get lower scores.\n    # Higher `bias` means higher scores for all gaps, and a higher gap is needed for score 0.5.\n    priorities[possible_bins_mask] = 1 / (1 + np.exp(-sigmoid_arg))\n\n    return priorities",
    "response_id": 6,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 103.72627427729671,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response7.txt_stdout.txt",
    "code_path": "problem_iter2_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements the 'Almost Full Fit' priority function for online Bin Packing.\n\n    This strategy prioritizes bins that will have the least remaining capacity\n    after the item is packed, provided they can fit the item. This aims to\n    minimize wasted space in each bin.\n\n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: A NumPy array containing the remaining capacities of each bin.\n\n    Returns:\n        A NumPy array of priority scores for each bin. Bins that can fit the item\n        will have higher scores (closer to 0) if their remaining capacity after\n        packing is smaller. Bins that cannot fit the item will have a score of -inf.\n    \"\"\"\n    # Initialize priorities to negative infinity, as bins that cannot fit the item\n    # should have the lowest possible priority.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that have enough remaining capacity to accommodate the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the 'slack' (remaining capacity\n    # after the item is placed). The goal of 'Almost Full Fit' is to minimize this slack.\n    # We want bins with the smallest non-negative slack to have the highest priority.\n    if np.any(can_fit_mask):\n        # Calculate the remaining capacity in the bins *after* the item is placed.\n        remaining_after_packing = bins_remain_cap[can_fit_mask] - item\n\n        # To prioritize bins with the smallest remaining capacity after packing,\n        # we can use the negative of this remaining capacity as the priority.\n        # A smaller remaining_after_packing (e.g., 0) will result in a higher priority (-0).\n        # A larger remaining_after_packing (e.g., 5) will result in a lower priority (-5).\n        priorities[can_fit_mask] = -remaining_after_packing\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 30.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response8.txt_stdout.txt",
    "code_path": "problem_iter2_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using an optimized Sigmoid Fit Score strategy.\n\n    This strategy prioritizes bins that, after adding the item, would have a\n    remaining capacity that is \"close\" to zero. This is achieved by mapping\n    the remaining capacity to a sigmoid function that outputs higher scores\n    for smaller non-negative remaining capacities. A scaling factor `k` is used\n    to control the steepness of the sigmoid curve, making the preference for\n    minimal remaining capacity more pronounced.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins where the item does not fit will have a priority of -np.inf.\n        For bins where the item fits, the priority is calculated using a sigmoid\n        function to favor minimal remaining capacity.\n    \"\"\"\n    # Calculate the remaining capacity if the item is placed in each bin\n    remaining_capacities_after_placement = bins_remain_cap - item\n\n    # Filter out bins where the item cannot fit. Assign them the lowest possible priority.\n    valid_bins_mask = remaining_capacities_after_placement >= 0\n    \n    # Initialize priorities to a very low value for bins where the item doesn't fit.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float) \n    \n    # For valid bins, calculate the priority using a sigmoid function.\n    # We want to maximize priority as remaining_capacities_after_placement approaches 0.\n    # A sigmoid function that achieves this is `1 / (1 + exp(k * r))`, where `r` is remaining capacity and `k > 0`.\n    # This maps r=0 to 0.5, and larger r to values less than 0.5.\n    # To make the priority higher for smaller remaining capacities, we want the sigmoid argument to be more negative\n    # for smaller `r`. This is achieved by mapping `r` to `-k * r`.\n    # The sigmoid form becomes `1 / (1 + exp(-(-k * r))) = 1 / (1 + exp(k * r))`.\n    #\n    # Alternatively, and perhaps more standard for \"minimizing a value\", we can use the sigmoid function\n    # `sigmoid(x) = 1 / (1 + exp(-x))`. To prioritize small `r`, we need the argument `x` to be large\n    # when `r` is small. So, `x = C - k * r` for constants `C` and `k > 0`.\n    # Let's choose `x = -k * r` for simplicity, which means `C=0`.\n    # Priority = `sigmoid(-k * r) = 1 / (1 + exp(-(-k * r))) = 1 / (1 + exp(k * r))`.\n    #\n    # This formulation maps:\n    # r = 0     -> exp(0) = 1   -> priority = 1 / (1 + 1) = 0.5\n    # r = small > 0 -> exp(small positive) > 1 -> priority < 0.5\n    # r = large > 0 -> exp(large positive) >> 1 -> priority near 0\n    #\n    # This correctly assigns the highest priority (0.5) to bins with exactly zero remaining capacity,\n    # and decreasing priorities for bins with larger remaining capacities.\n\n    k = 5.0  # Scaling factor: higher k means a stronger preference for minimal remaining capacity.\n             # This value can be tuned. A higher k makes the priority drop off more sharply as remaining capacity increases.\n\n    r_valid = remaining_capacities_after_placement[valid_bins_mask]\n\n    # Calculate the argument for the sigmoid function: `k * r_valid`.\n    # For numerical stability with `np.exp`, we can bound the argument.\n    # `exp(x)` overflows for `x > ~700`. If `k * r_valid` exceeds this, the priority should be near zero,\n    # which `1 / (1 + infinity)` correctly yields.\n    # If `k * r_valid` is very small (large negative), `exp` underflows to 0, giving priority 1.\n    # Since `r_valid >= 0` and `k > 0`, `k * r_valid >= 0`.\n\n    # We use `np.clip` to prevent potential issues if `k * r_valid` becomes extremely large,\n    # ensuring `exp` doesn't overflow or produce NaNs. The value 700 is a common threshold for `exp(x)`.\n    # If `k * r_valid` is greater than this, `exp` would be very large, and the priority would be close to 0.\n    # If `k * r_valid` is very small (negative), which won't happen here since `r_valid >= 0`,\n    # `exp` would be near 0, giving priority near 1.\n    \n    # The argument for the sigmoid is `k * r_valid`.\n    # To ensure stability, we cap the argument to `exp`.\n    # `bounded_arg = np.clip(k * r_valid, -700.0, 700.0)` is not strictly needed for `1 / (1 + exp(k * r_valid))`\n    # because if `k * r_valid` is very large, `exp` will overflow to infinity, and `1 / (1 + inf)` is 0.\n    # This is the desired behavior (very low priority for large remaining capacity).\n\n    # Directly compute the sigmoid for valid bins.\n    # `np.exp(k * r_valid)` can be very large, causing `1 / (1 + ...)` to be very small.\n    # This is desired for large `r_valid`.\n    priorities[valid_bins_mask] = 1.0 / (1.0 + np.exp(k * r_valid))\n\n    return priorities",
    "response_id": 8,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 68.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Softmax-Based Fit.\n\n    This version refines the priority calculation by directly penalizing remaining\n    capacity. Bins that leave less remaining capacity after packing the item are\n    given higher priority. This encourages tighter fits and aims to minimize\n    wasted space. The scores are transformed using an exponential function\n    (similar to softmax) to ensure that bins with better fits (less remaining capacity)\n    have significantly higher probabilities, while still allowing some probability\n    for less optimal fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        # No bin can fit the item, return all zeros\n        return priorities\n\n    # Calculate the \"desirability\" score for bins that can fit the item.\n    # A higher score means less remaining capacity after packing.\n    # We want to minimize (bins_remain_cap - item).\n    # For a softmax-like score where higher is better, we use the negative\n    # of the remaining capacity, i.e., -(bins_remain_cap[can_fit_mask] - item).\n    # This is equivalent to item - bins_remain_cap[can_fit_mask].\n    # Adding a small constant could help if item sizes are very close to capacities,\n    # but the current formulation directly favors less leftover space.\n    remaining_capacity_after_packing = bins_remain_cap[can_fit_mask] - item\n    \n    # To ensure scores are positive and higher for better fits, we can use:\n    # score = 1.0 / (remaining_capacity_after_packing + 1e-9)  # Small epsilon for stability\n    # Alternatively, and perhaps more robustly for softmax, we can use the negative\n    # of the remaining capacity, as implemented in v1, which favors less leftover space.\n    # Let's stick to the logic of favoring bins with less leftover space:\n    # score = -(remaining_capacity_after_packing)\n    \n    # For softmax, it's often beneficial to have scores that are not too extreme,\n    # or to scale them. A simple approach is to use the negative of the remaining\n    # capacity. Let's use this: higher score means less remaining capacity.\n    scores_for_fitting_bins = -(remaining_capacity_after_packing)\n    \n    # Apply softmax-like transformation.\n    # Subtracting the maximum score before exponentiation for numerical stability.\n    max_score = np.max(scores_for_fitting_bins)\n    exp_scores = np.exp(scores_for_fitting_bins - max_score)\n\n    sum_exp_scores = np.sum(exp_scores)\n\n    if sum_exp_scores > 1e-9:  # Check for numerical stability\n        priorities[can_fit_mask] = exp_scores / sum_exp_scores\n    else:\n        # If all scores are very negative, resulting in near-zero exponentials,\n        # distribute probability equally among bins that *can* fit.\n        num_fitting_bins = np.sum(can_fit_mask)\n        if num_fitting_bins > 0:\n            priorities[can_fit_mask] = 1.0 / num_fitting_bins\n\n    return priorities",
    "response_id": 9,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 106.19818783608963,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response0.txt_stdout.txt",
    "code_path": "problem_iter3_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority for adding an item to each bin, prioritizing\n    bins with remaining capacity that is slightly larger than the item size.\n\n    This heuristic balances the 'best fit' (tightest fit) with 'flexibility'\n    by favoring bins that have a comfortable amount of space left after packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Apply priority function only to bins that can fit the item\n    if np.any(can_fit_mask):\n        fitting_capacities = bins_remain_cap[can_fit_mask]\n\n        # Parameters for the Gaussian-like priority function\n        # C: a small buffer, meaning we prefer bins with capacity item + C\n        # sigma: controls the 'spread' or sensitivity of the preference.\n        # A smaller sigma means the priority drops off more quickly as capacity deviates from item + C.\n        buffer = 1.0  # Prefer bins with remaining capacity item + 1.0\n        sigma = 2.0   # Sensitivity parameter\n\n        # Calculate the difference from the ideal capacity (item + buffer)\n        # We use a smooth function like a Gaussian kernel.\n        # The function peaks when remaining_capacity == item + buffer.\n        # It penalizes capacities that are too small (already handled by can_fit_mask)\n        # and capacities that are very large (less focused fit).\n        differences = fitting_capacities - (item + buffer)\n        \n        # Calculate priority using a Gaussian-like function\n        # The form exp(-(x^2)/(2*sigma^2)) peaks at x=0.\n        # We want to peak when fitting_capacities - (item + buffer) = 0\n        # So, x = fitting_capacities - (item + buffer)\n        gaussian_priority = np.exp(-(differences**2) / (2 * sigma**2))\n        \n        # Assign these priorities to the bins that can fit the item\n        priorities[can_fit_mask] = gaussian_priority\n\n        # Optional: Add a small 'tightness' bonus for bins that are very close fits\n        # This can be seen as a slight leaning towards 'best fit' from the peak.\n        # We can add a term that is inversely proportional to (r - item + epsilon)\n        # Let's add a small value for the tightest fits.\n        # Calculate tightness score: 1 / (remaining_capacity - item + 1)\n        tightness_scores = 1.0 / (fitting_capacities - item + 1.0)\n        \n        # Combine the Gaussian priority with a scaled tightness score.\n        # We need to scale tightness_scores so they don't overwhelm gaussian_priority.\n        # Let's scale them by a small factor, e.g., 0.1.\n        # The goal is to slightly boost the very tight fits without dominating.\n        \n        # Normalize tightness scores relative to themselves to avoid huge values\n        if np.max(tightness_scores) > 0:\n            scaled_tightness = 0.1 * (tightness_scores / np.max(tightness_scores))\n            priorities[can_fit_mask] += scaled_tightness\n        \n        # Ensure priorities are not negative (though gaussian_priority is always non-negative)\n        priorities = np.maximum(priorities, 0.0)\n\n        # Normalize priorities to be between 0 and 1 for consistency\n        max_priority = np.max(priorities)\n        if max_priority > 0:\n            priorities = priorities / max_priority\n        else:\n            # If all priorities are 0 (e.g., no bins fit and can_fit_mask was empty),\n            # this case should ideally not be reached if can_fit_mask is handled correctly.\n            # But as a fallback, ensure it's a zero array.\n            priorities = np.zeros_like(bins_remain_cap, dtype=float)\n            \n    return priorities",
    "response_id": 0,
    "obj": 4.427602712405275,
    "cyclomatic_complexity": 4.0,
    "halstead": 252.21970596792266,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response1.txt_stdout.txt",
    "code_path": "problem_iter3_code1.py",
    "code": "import numpy as np\nfrom scipy.special import expit\n\ndef sigmoid(x):\n    return expit(x)\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin,\n    prioritizing tightest fits with a bonus for flexibility.\n\n    The priority is a composite score based on:\n    1. Tightness: Prioritizes bins with minimal remaining capacity that can fit the item.\n       This is modeled using a sigmoid function where smaller positive gaps\n       (remaining_capacity - item) yield higher scores.\n    2. Flexibility: Prioritizes bins with larger remaining capacity.\n       This is modeled using a sigmoid function of the remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins, dtype=float)\n\n    # Parameters for the scoring functions\n    # k_tight: Controls the steepness of the tightness score. Higher k_tight means\n    #          smaller gaps are much more favored.\n    # k_flex: Controls the steepness of the flexibility score. Higher k_flex means\n    #         larger capacities reach saturation faster.\n    # w_tight: Weight for the tightness component.\n    # w_flex: Weight for the flexibility component.\n    k_tight = 3.0\n    k_flex = 0.5\n    w_tight = 0.7\n    w_flex = 0.3\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Process only bins that can fit the item\n    eligible_bins_indices = np.where(can_fit_mask)[0]\n\n    if len(eligible_bins_indices) > 0:\n        eligible_rem_cap = bins_remain_cap[eligible_bins_indices]\n        gaps = eligible_rem_cap - item\n\n        # Calculate tightness score:\n        # We want small positive gaps to have high scores.\n        # Use 1 - sigmoid(k * gap)\n        # If gap = 0, score = 1 - sigmoid(0) = 0.5\n        # If gap = small positive, score < 0.5\n        # If gap = large positive, score -> 0\n        # This is slightly counter-intuitive. Let's use sigmoid(k * (1 - gap))\n        # Or sigmoid(k * (-gap)) ... this is low for small gaps.\n\n        # Let's use sigmoid(k * (score_input)) where score_input is high for tight fits.\n        # Input to sigmoid should be high for small `gap`.\n        # e.g., `k * exp(-gap)` or `k * (1 / (1 + gap))`\n        # Let's try a simple linear mapping that we then feed to sigmoid.\n        # Map gap from [0, max_gap] to [-sensitivity, sensitivity] or similar.\n\n        # Alternative: Use the reciprocal of gap + epsilon, then normalize/sigmoid.\n        # `tightness_factor = 1.0 / (gaps + 1e-6)`\n        # `tightness_score = sigmoid(k_tight * (normalized_tightness_factor))`\n\n        # Let's stick to the reflection's idea of sigmoid on a measure.\n        # Measure of \"how tight is it\": higher value for smaller positive gap.\n        # Let `tightness_measure = -gaps`.\n        # `tightness_score = sigmoid(k_tight * tightness_measure)`\n        # If gap = 0, measure = 0, score = 0.5\n        # If gap = small positive, measure = small neg, score < 0.5\n        # If gap = large positive, measure = large neg, score -> 0.\n\n        # This is inverted again. The mapping needs to be correct.\n        # Let's re-evaluate sigmoid(X) for desired mapping:\n        # We want `f(gap)` such that `f(0)` is high, `f(large)` is low.\n\n        # Option 1: `sigmoid(k * (-gap))` -> Low for small gaps.\n        # Option 2: `1 - sigmoid(k * gap)` -> High for small gaps.\n        #   If gap=0, 1-0.5 = 0.5\n        #   If gap=small pos, 1-sigmoid(pos) = 1 - ( >0.5 ) = <0.5. This is inverted!\n\n        # Let's consider the quantity `1 / (1 + exp(-k * x))` for `x`.\n        # If we want high score for small `x`, we can use `1 / (1 + exp(k * x))`.\n        # Let `x = gaps`.\n        # `tightness_score = 1 / (1 + np.exp(k_tight * gaps))`\n        #   If gap = 0, score = 1 / (1 + 1) = 0.5\n        #   If gap = small positive, score < 0.5\n        #   If gap = large positive, score -> 0. This is still inverted.\n\n        # How about `sigmoid(k * (C - gap))`? Let C be the max possible gap or just a constant.\n        # Let C = 10 (assume max gap won't exceed 10 significantly).\n        # `tightness_score = sigmoid(k_tight * (10 - gaps))`\n        #   If gap = 0, input = 10*k_tight. Score ~ 1.\n        #   If gap = 1, input = k_tight*(10-1). Score is lower.\n        #   If gap = 10, input = 0. Score = 0.5.\n        #   If gap = 11, input = k_tight*(10-11) = -k_tight. Score < 0.5.\n\n        # This seems to work! Higher score for smaller gaps.\n        # However, we must ensure that unfit bins (negative gap if we didn't mask) are handled.\n        # The `can_fit_mask` already handles this by only processing eligible bins.\n        # So, `gaps` are always non-negative here.\n\n        # Let's normalize gaps to be more robust, or choose C carefully.\n        # Max possible gap could be `bin_capacity_limit - min_item_size`.\n        # For simplicity, let's just use `k_tight` to control sensitivity.\n        # `tightness_score = sigmoid(k_tight * (item - gaps))` --> this is `sigmoid(k_tight * (2*item - eligible_rem_cap))`\n        # Let's use `sigmoid(k_tight * (target_gap - gaps))` where `target_gap` is near 0.\n        # Let `target_gap = 0`.\n        # `tightness_score = sigmoid(k_tight * (0 - gaps))` is wrong.\n\n        # Let's define tightness as `(remaining_capacity - item)`. Smaller is better.\n        # We want a function `f(x)` where `f(0)` is high, `f(large)` is low.\n        # `f(x) = 1 / (1 + x)` is decreasing. `f(0)=1, f(large)->0`.\n        # `f(x) = exp(-k * x)` is decreasing. `f(0)=1, f(large)->0`.\n        # We can use these directly or pass them through a sigmoid.\n        # Using `exp(-k * x)` for tightness:\n        tightness_score = np.exp(-k_tight * gaps)\n\n        # Using sigmoid for flexibility, to keep scores bounded and smooth.\n        flexibility_score = sigmoid(k_flex * eligible_rem_cap)\n\n        # Combine scores\n        composite_priorities = w_tight * tightness_score + w_flex * flexibility_score\n        priorities[eligible_bins_indices] = composite_priorities\n\n    # Normalize priorities so the max is 1, for easier comparison if needed.\n    # This is optional, as relative values matter for selection.\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n    # If all priorities are 0, then priorities remains all zeros.\n\n    return priorities",
    "response_id": 1,
    "obj": 19.335859593139222,
    "cyclomatic_complexity": 3.0,
    "halstead": 146.71880002307702,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response2.txt_stdout.txt",
    "code_path": "problem_iter3_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin.\n    This version prioritizes tighter fits using a sigmoid function and\n    adds a small bonus for bins with more remaining capacity to encourage\n    future flexibility.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate a priority score\n    if np.any(can_fit_mask):\n        valid_bins_cap = bins_remain_cap[can_fit_mask]\n        valid_bins_indices = np.where(can_fit_mask)[0]\n\n        # --- Core Priority Calculation ---\n        # 1. Tightest Fit (Exploitation):\n        #    We want bins with remaining capacity just slightly larger than the item.\n        #    A good measure is `remaining_capacity - item`. Smaller values are better.\n        #    We can use a sigmoid-like function where small positive values (tight fit)\n        #    result in high scores, and large positive values (loose fit) result in lower scores.\n        #    Let's use `1 / (1 + exp(k * (capacity - item)))` or similar.\n        #    A simpler approach is to rank based on (capacity - item) and invert.\n        #    Let's transform `capacity - item` to prioritize smaller values.\n        #    We can use `- (capacity - item)` and then apply sigmoid, or scale and invert.\n        #    A good transformation might be `1 / (1 + (capacity - item))` or\n        #    `sigmoid(negative_difference)`.\n        #    Let's try: `sigmoid(alpha * (item - remaining_capacity))`\n        #    where alpha is a steepness parameter. A larger alpha makes the \"best fit\"\n        #    more pronounced.\n        #    Alternatively, `remaining_capacity` itself can be used, but we want smaller\n        #    remaining capacities for tight fits. So, we'll invert it or use a negative\n        #    dependency.\n\n        # Let's use a function that maps (remaining_capacity - item) to a priority.\n        # We want small (remaining_capacity - item) to have high priority.\n        # A simple transformation is `1 / (1 + (remaining_capacity - item))`\n        # For numerical stability and better control, let's scale and then apply sigmoid.\n        # Consider `sigmoid(k * (item - remaining_capacity))`.\n        # `k` controls sensitivity. A positive `k` will mean smaller `remaining_capacity` (for a given `item`) has higher priority.\n        # Let `diff = remaining_capacity - item`. We want to prioritize small `diff`.\n        # Sigmoid of `-k * diff` is good. `1 / (1 + exp(k * diff))`\n        # To avoid numerical issues, let's normalize `diff` first.\n        # `normalized_diff = (diff - min_diff) / (max_diff - min_diff)`\n        # Then `sigmoid(k * (1 - normalized_diff))`\n\n        # A more direct approach for \"tightest fit\":\n        # Prioritize bins where `bins_remain_cap` is just above `item`.\n        # `tightness_score = 1.0 / (1.0 + bins_remain_cap[valid_bins_indices] - item)` # higher is better fit\n        # This can be unstable if `bins_remain_cap - item` is very large.\n        # A sigmoid is better: `sigmoid(k * (item - bins_remain_cap[valid_bins_indices]))`\n        # Let's use `k=1.0` for now.\n        k_tightness = 1.0  # Sensitivity for tight fit\n        tightness_scores = 1.0 / (1.0 + np.exp(k_tightness * (item - valid_bins_cap)))\n\n        # 2. Future Flexibility Bonus (Exploration/Balancing):\n        #    Consider bins with larger remaining capacity as having a small bonus.\n        #    This is like a small incentive to keep some space.\n        #    We can use `sigmoid(k_flex * (bins_remain_cap[valid_bins_indices] - threshold))`\n        #    or simply a linear scaling.\n        #    Let's make it a smaller bonus, so we scale `valid_bins_cap` and add it.\n        #    To prevent very large bins from dominating, we can normalize `valid_bins_cap`.\n        #    Max remaining capacity among valid bins:\n        max_cap_valid = np.max(valid_bins_cap)\n        min_cap_valid = np.min(valid_bins_cap)\n\n        if max_cap_valid > min_cap_valid:\n            normalized_caps = (valid_bins_cap - min_cap_valid) / (max_cap_valid - min_cap_valid)\n        else: # All valid bins have same remaining capacity\n            normalized_caps = np.ones_like(valid_bins_cap) * 0.5 # Neutral value\n\n        # The flexibility bonus should be smaller than the tightness score.\n        # So, we can scale `normalized_caps` by a small factor, say `0.2`.\n        flexibility_bonus_scale = 0.2\n        flexibility_bonus = flexibility_bonus_scale * normalized_caps\n\n        # Combine scores\n        combined_scores = tightness_scores + flexibility_bonus\n\n        # Assign scores to the original bins_remain_cap array\n        priorities[valid_bins_indices] = combined_scores\n\n        # --- Normalization ---\n        # Normalize priorities to a 0-1 range. This makes the scale consistent\n        # and avoids extremely large or small numbers if the sigmoid or bonus\n        # values get too extreme.\n        max_priority = np.max(priorities)\n        if max_priority > 0:\n            priorities = priorities / max_priority\n        else:\n            # If all valid bins resulted in 0 priority (unlikely with sigmoid),\n            # or if there were no valid bins, handle it.\n            pass # Priorities remain 0\n\n    # Bins that cannot fit the item will have a priority of 0, which is already set.\n\n    # Add a small random perturbation to break ties and encourage slight exploration\n    # among equally good options. This is a subtle form of exploration.\n    # Apply to bins that can fit the item.\n    if np.any(can_fit_mask):\n        perturbation_scale = 0.05 # Small random boost\n        eligible_indices = np.where(can_fit_mask)[0]\n        # Ensure we don't add perturbation to bins with 0 priority initially\n        # (though in this logic, all can_fit bins have positive priority)\n        priorities[eligible_indices] += np.random.rand(len(eligible_indices)) * perturbation_scale\n\n        # Re-normalize after adding perturbation to keep priorities in a reasonable range\n        max_priority = np.max(priorities)\n        if max_priority > 0:\n            priorities = priorities / max_priority\n\n    return priorities",
    "response_id": 2,
    "obj": 149.19226166733148,
    "cyclomatic_complexity": 6.0,
    "halstead": 267.5266007608913,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response3.txt_stdout.txt",
    "code_path": "problem_iter3_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version prioritizes bins with minimal remaining capacity (tightest fits)\n    using a smooth ranking function (sigmoid-like). It also adds a small bonus\n    for larger remaining capacities to balance immediate tightness with future\n    flexibility.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Determine which bins can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate scores only for bins that can fit the item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if len(fitting_bins_remain_cap) == 0:\n        return priorities  # No bin can fit the item\n\n    # --- Heuristic 1: Prioritize tightest fits (minimal remaining capacity) ---\n    # Use a sigmoid-like function to rank the remaining capacities.\n    # A bin with capacity `c` will have a score related to `c - item`.\n    # We want smaller `c - item` to have higher priority.\n    # Let's transform `c` to `1 / (c - item + epsilon)` for tight fit, then scale.\n    # A simpler approach is to directly use the inverse of the difference,\n    # and apply a sigmoid-like transformation to smooth it.\n\n    # Calculate the \"tightness\" score: smaller is better (tighter fit)\n    # Add a small epsilon to avoid division by zero or extremely large values.\n    epsilon_tightness = 1e-6\n    tightness_scores = 1.0 / (fitting_bins_remain_cap - item + epsilon_tightness)\n\n    # Smooth the tightness scores using a sigmoid-like function.\n    # We want to map smaller `tightness_scores` (closer to 0 for tightest fits)\n    # to higher priority values.\n    # A function like `1 / (1 + exp(-k * (x - offset)))` would map small x to high values.\n    # Here, x = tightness_scores. Let's aim for a concave shape where scores\n    # are high for small tightness_scores.\n    # Using a simple inverse scaling and then sigmoid can be complex.\n    # A more direct approach: map the differences `fitting_bins_remain_cap - item`\n    # to priorities. Small difference -> high priority.\n    # Let's use `exp(-k * diff)` where `k` is a scaling factor.\n    # A larger `k` makes the function steeper, focusing on very tight fits.\n    k_tightness = 5.0  # Sensitivity parameter for tightness\n    tightness_priority = np.exp(-k_tightness * (fitting_bins_remain_cap - item))\n\n    # --- Heuristic 2: Small bonus for larger remaining capacities (future flexibility) ---\n    # This encourages not packing too tightly if there's a slightly less tight fit\n    # that still leaves significant space.\n    # We can add a small bonus based on the `fitting_bins_remain_cap`.\n    # A simple linear bonus or a scaled sigmoid might work.\n    # Let's use a scaled version of the remaining capacity, perhaps with a saturation.\n    bonus_scale = 0.05  # How much bonus to give for remaining capacity\n    flexibility_bonus = bonus_scale * fitting_bins_remain_cap\n\n    # Combine the heuristics\n    # Simple addition: weighted sum of tightness priority and flexibility bonus.\n    # Adjust weights as needed. Let's give more weight to tightness.\n    w_tightness = 1.0\n    w_flexibility = 0.5\n\n    combined_scores = w_tightness * tightness_priority + w_flexibility * flexibility_bonus\n\n    # Normalize the combined scores for bins that can fit the item\n    if np.max(combined_scores) > 0:\n        normalized_scores = combined_scores / np.max(combined_scores)\n    else:\n        normalized_scores = np.zeros_like(combined_scores)\n\n    # Assign the calculated normalized scores to the original positions\n    priorities[can_fit_mask] = normalized_scores\n\n    return priorities",
    "response_id": 3,
    "obj": 124.95013960909456,
    "cyclomatic_complexity": 3.0,
    "halstead": 201.18251441994926,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response4.txt_stdout.txt",
    "code_path": "problem_iter3_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority scores for each bin to pack an item, prioritizing tighter fits\n    with a slight bonus for larger remaining capacities for future flexibility.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element is the remaining capacity\n                         of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, containing priority\n        scores for each bin. Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, all priorities remain 0.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Filter to consider only bins that can fit the item\n    fitting_bins_caps = bins_remain_cap[can_fit_mask]\n    fitting_bin_indices = np.where(can_fit_mask)[0]\n\n    # --- Heuristic Calculation ---\n    # Prioritize bins with minimal remaining capacity (tightest fits)\n    # Use a sigmoid-like function to give higher priority to tighter fits,\n    # but with a gradual decrease rather than a sharp cutoff.\n    # We use 1 / (1 + exp(-(x - center) / scale)))\n    # Here, 'x' is related to how much capacity is left *after* placing the item.\n    # Smaller remaining capacity after placement (i.e., tighter fit) should have higher priority.\n    # So, we want to map small (fitting_bins_caps - item) values to high priority.\n\n    # Calculate remaining capacity after placing the item\n    remaining_after_placement = fitting_bins_caps - item\n\n    # Define parameters for the sigmoid-like function for tightness\n    # 'center' can be set around the median of the 'remaining_after_placement' values\n    # or a small value to emphasize very tight fits.\n    # 'scale' controls the steepness of the transition.\n    tightness_scores = np.zeros_like(fitting_bins_caps)\n    if len(remaining_after_placement) > 0:\n        # A simple approach: map 0 remaining capacity to highest priority.\n        # A slightly more sophisticated approach could use a sigmoid.\n        # Let's use a simple inverse relationship with a small offset to avoid division by zero.\n        # And add a small bonus for larger capacities to balance.\n\n        # Calculate a 'tightness' score: higher is tighter fit (closer to 0)\n        # Normalize remaining_after_placement to be between 0 and 1 (relative to max possible remaining)\n        # or use a fixed scale. A simpler approach:\n        # Higher priority for smaller remaining_after_placement.\n        # Adding a small constant to avoid division by zero or extremely high values for 0 remaining.\n        tightness_factor = 1.0 / (remaining_after_placement + 0.1)\n\n        # Bonus for larger remaining capacities (exploration/flexibility):\n        # This gives a slight boost to bins that still have significant space left,\n        # even if they aren't the tightest fit. This can help avoid packing\n        # items in a way that leaves very little space in many bins.\n        # Normalize fitting_bins_caps to get a relative sense of large capacity.\n        # We can add a scaled version of the remaining capacity itself.\n        # Ensure we don't add too much to the tightest fits.\n        flexibility_bonus_scale = 0.1 # Controls how much this bonus influences the priority\n        flexibility_factor = fitting_bins_caps / np.max(fitting_bins_caps) * flexibility_bonus_scale if np.max(fitting_bins_caps) > 0 else 0\n\n        # Combine tightness and flexibility.\n        # Tightness is the primary driver.\n        combined_priority_values = tightness_factor + flexibility_factor\n        priorities[fitting_bin_indices] = combined_priority_values\n\n        # Normalize priorities so the max is 1.0 for easier interpretation and selection.\n        if np.max(priorities) > 0:\n            priorities = priorities / np.max(priorities)\n\n    return priorities",
    "response_id": 4,
    "obj": 35.63023534104507,
    "cyclomatic_complexity": 5.0,
    "halstead": 164.51539013493823,
    "exec_success": true
  }
]