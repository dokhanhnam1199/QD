{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin.\n    This version prioritizes tighter fits using a sigmoid function and\n    adds a small bonus for bins with more remaining capacity to encourage\n    future flexibility.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate a priority score\n    if np.any(can_fit_mask):\n        valid_bins_cap = bins_remain_cap[can_fit_mask]\n        valid_bins_indices = np.where(can_fit_mask)[0]\n\n        # --- Core Priority Calculation ---\n        # 1. Tightest Fit (Exploitation):\n        #    We want bins with remaining capacity just slightly larger than the item.\n        #    A good measure is `remaining_capacity - item`. Smaller values are better.\n        #    We can use a sigmoid-like function where small positive values (tight fit)\n        #    result in high scores, and large positive values (loose fit) result in lower scores.\n        #    Let's use `1 / (1 + exp(k * (capacity - item)))` or similar.\n        #    A simpler approach is to rank based on (capacity - item) and invert.\n        #    Let's transform `capacity - item` to prioritize smaller values.\n        #    We can use `- (capacity - item)` and then apply sigmoid, or scale and invert.\n        #    A good transformation might be `1 / (1 + (capacity - item))` or\n        #    `sigmoid(negative_difference)`.\n        #    Let's try: `sigmoid(alpha * (item - remaining_capacity))`\n        #    where alpha is a steepness parameter. A larger alpha makes the \"best fit\"\n        #    more pronounced.\n        #    Alternatively, `remaining_capacity` itself can be used, but we want smaller\n        #    remaining capacities for tight fits. So, we'll invert it or use a negative\n        #    dependency.\n\n        # Let's use a function that maps (remaining_capacity - item) to a priority.\n        # We want small (remaining_capacity - item) to have high priority.\n        # A simple transformation is `1 / (1 + (remaining_capacity - item))`\n        # For numerical stability and better control, let's scale and then apply sigmoid.\n        # Consider `sigmoid(k * (item - remaining_capacity))`.\n        # `k` controls sensitivity. A positive `k` will mean smaller `remaining_capacity` (for a given `item`) has higher priority.\n        # Let `diff = remaining_capacity - item`. We want to prioritize small `diff`.\n        # Sigmoid of `-k * diff` is good. `1 / (1 + exp(k * diff))`\n        # To avoid numerical issues, let's normalize `diff` first.\n        # `normalized_diff = (diff - min_diff) / (max_diff - min_diff)`\n        # Then `sigmoid(k * (1 - normalized_diff))`\n\n        # A more direct approach for \"tightest fit\":\n        # Prioritize bins where `bins_remain_cap` is just above `item`.\n        # `tightness_score = 1.0 / (1.0 + bins_remain_cap[valid_bins_indices] - item)` # higher is better fit\n        # This can be unstable if `bins_remain_cap - item` is very large.\n        # A sigmoid is better: `sigmoid(k * (item - bins_remain_cap[valid_bins_indices]))`\n        # Let's use `k=1.0` for now.\n        k_tightness = 1.0  # Sensitivity for tight fit\n        tightness_scores = 1.0 / (1.0 + np.exp(k_tightness * (item - valid_bins_cap)))\n\n        # 2. Future Flexibility Bonus (Exploration/Balancing):\n        #    Consider bins with larger remaining capacity as having a small bonus.\n        #    This is like a small incentive to keep some space.\n        #    We can use `sigmoid(k_flex * (bins_remain_cap[valid_bins_indices] - threshold))`\n        #    or simply a linear scaling.\n        #    Let's make it a smaller bonus, so we scale `valid_bins_cap` and add it.\n        #    To prevent very large bins from dominating, we can normalize `valid_bins_cap`.\n        #    Max remaining capacity among valid bins:\n        max_cap_valid = np.max(valid_bins_cap)\n        min_cap_valid = np.min(valid_bins_cap)\n\n        if max_cap_valid > min_cap_valid:\n            normalized_caps = (valid_bins_cap - min_cap_valid) / (max_cap_valid - min_cap_valid)\n        else: # All valid bins have same remaining capacity\n            normalized_caps = np.ones_like(valid_bins_cap) * 0.5 # Neutral value\n\n        # The flexibility bonus should be smaller than the tightness score.\n        # So, we can scale `normalized_caps` by a small factor, say `0.2`.\n        flexibility_bonus_scale = 0.2\n        flexibility_bonus = flexibility_bonus_scale * normalized_caps\n\n        # Combine scores\n        combined_scores = tightness_scores + flexibility_bonus\n\n        # Assign scores to the original bins_remain_cap array\n        priorities[valid_bins_indices] = combined_scores\n\n        # --- Normalization ---\n        # Normalize priorities to a 0-1 range. This makes the scale consistent\n        # and avoids extremely large or small numbers if the sigmoid or bonus\n        # values get too extreme.\n        max_priority = np.max(priorities)\n        if max_priority > 0:\n            priorities = priorities / max_priority\n        else:\n            # If all valid bins resulted in 0 priority (unlikely with sigmoid),\n            # or if there were no valid bins, handle it.\n            pass # Priorities remain 0\n\n    # Bins that cannot fit the item will have a priority of 0, which is already set.\n\n    # Add a small random perturbation to break ties and encourage slight exploration\n    # among equally good options. This is a subtle form of exploration.\n    # Apply to bins that can fit the item.\n    if np.any(can_fit_mask):\n        perturbation_scale = 0.05 # Small random boost\n        eligible_indices = np.where(can_fit_mask)[0]\n        # Ensure we don't add perturbation to bins with 0 priority initially\n        # (though in this logic, all can_fit bins have positive priority)\n        priorities[eligible_indices] += np.random.rand(len(eligible_indices)) * perturbation_scale\n\n        # Re-normalize after adding perturbation to keep priorities in a reasonable range\n        max_priority = np.max(priorities)\n        if max_priority > 0:\n            priorities = priorities / max_priority\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Softmax-Based Fit.\n\n    This version refines the priority calculation by directly penalizing remaining\n    capacity. Bins that leave less remaining capacity after packing the item are\n    given higher priority. This encourages tighter fits and aims to minimize\n    wasted space. The scores are transformed using an exponential function\n    (similar to softmax) to ensure that bins with better fits (less remaining capacity)\n    have significantly higher probabilities, while still allowing some probability\n    for less optimal fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        # No bin can fit the item, return all zeros\n        return priorities\n\n    # Calculate the \"desirability\" score for bins that can fit the item.\n    # A higher score means less remaining capacity after packing.\n    # We want to minimize (bins_remain_cap - item).\n    # For a softmax-like score where higher is better, we use the negative\n    # of the remaining capacity, i.e., -(bins_remain_cap[can_fit_mask] - item).\n    # This is equivalent to item - bins_remain_cap[can_fit_mask].\n    # Adding a small constant could help if item sizes are very close to capacities,\n    # but the current formulation directly favors less leftover space.\n    remaining_capacity_after_packing = bins_remain_cap[can_fit_mask] - item\n    \n    # To ensure scores are positive and higher for better fits, we can use:\n    # score = 1.0 / (remaining_capacity_after_packing + 1e-9)  # Small epsilon for stability\n    # Alternatively, and perhaps more robustly for softmax, we can use the negative\n    # of the remaining capacity, as implemented in v1, which favors less leftover space.\n    # Let's stick to the logic of favoring bins with less leftover space:\n    # score = -(remaining_capacity_after_packing)\n    \n    # For softmax, it's often beneficial to have scores that are not too extreme,\n    # or to scale them. A simple approach is to use the negative of the remaining\n    # capacity. Let's use this: higher score means less remaining capacity.\n    scores_for_fitting_bins = -(remaining_capacity_after_packing)\n    \n    # Apply softmax-like transformation.\n    # Subtracting the maximum score before exponentiation for numerical stability.\n    max_score = np.max(scores_for_fitting_bins)\n    exp_scores = np.exp(scores_for_fitting_bins - max_score)\n\n    sum_exp_scores = np.sum(exp_scores)\n\n    if sum_exp_scores > 1e-9:  # Check for numerical stability\n        priorities[can_fit_mask] = exp_scores / sum_exp_scores\n    else:\n        # If all scores are very negative, resulting in near-zero exponentials,\n        # distribute probability equally among bins that *can* fit.\n        num_fitting_bins = np.sum(can_fit_mask)\n        if num_fitting_bins > 0:\n            priorities[can_fit_mask] = 1.0 / num_fitting_bins\n\n    return priorities\n\n[Reflection]\nPrioritize tight fits. Softmax-like scoring encourages better fits without absolute reliance on small margins.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}