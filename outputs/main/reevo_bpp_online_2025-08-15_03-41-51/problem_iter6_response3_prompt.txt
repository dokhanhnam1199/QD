{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin.\n\n    This heuristic prioritizes bins based on a combination of:\n    1. Tightness of fit: Bins that leave minimal remaining space after packing.\n    2. Flexibility: A small bonus for bins with larger remaining capacities.\n    3. Stability: Using a sigmoid function to smoothly differentiate priorities.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Determine which bins can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate scores for bins that can fit the item\n    fittable_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    fittable_indices = np.where(can_fit_mask)[0]\n\n    # Score 1: Tightness (prioritize smaller remaining capacity after fit)\n    # We want to minimize (remaining_capacity - item).\n    # A smaller difference is better. We can map this to a higher score.\n    # Using 1 / (difference + small_epsilon) for a steep increase as difference approaches 0.\n    # Or using a negative of the difference for direct correlation with closeness.\n    tightness_scores = fittable_bins_remain_cap - item\n    # Small epsilon to avoid division by zero if remaining capacity is exactly item size\n    epsilon_small = 1e-6\n    tightness_priority = 1.0 / (tightness_scores + epsilon_small)\n\n\n    # Score 2: Flexibility bonus (prioritize larger remaining capacity)\n    # This encourages keeping some large bins open for potentially larger future items.\n    # We can use a small boost proportional to the remaining capacity.\n    flexibility_bonus = 0.1 * fittable_bins_remain_cap / np.max(fittable_bins_remain_cap + epsilon_small)\n\n    # Combined score before sigmoid\n    # Higher tightness_priority is good, higher flexibility_bonus is good\n    combined_raw_score = tightness_priority + flexibility_bonus\n\n    # Use sigmoid to smooth and bound the scores between 0 and 1.\n    # Sigmoid(x) = 1 / (1 + exp(-x))\n    # We need to scale the input to sigmoid to control its steepness.\n    # Let's map a typical range of combined_raw_score to the sigmoid's sensitive region.\n    # A simple scaling can be done by dividing by an estimate of the typical score.\n    # Or, more directly, tune a parameter 'k' for sigmoid(k * x).\n    # For simplicity, we'll use a direct sigmoid, assuming raw scores are somewhat reasonable.\n    # A higher raw score maps to a higher sigmoid output.\n    k_sigmoid = 0.5 # Steepness parameter for sigmoid\n    sigmoid_scores = 1 / (1 + np.exp(-k_sigmoid * (combined_raw_score - np.median(combined_raw_score))))\n    \n    # Assign these sigmoid scores to the priorities array\n    priorities[fittable_indices] = sigmoid_scores\n\n    # Ensure that bins that cannot fit the item have a priority of 0\n    priorities[~can_fit_mask] = 0.0\n\n    # Normalize priorities to be between 0 and 1 (optional, but good for consistency)\n    if np.max(priorities) > 0:\n        priorities = priorities / np.max(priorities)\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin,\n    prioritizing tight fits and adding small bonuses for larger capacities.\n\n    The strategy aims to:\n    1. Prioritize bins that offer a \"tight fit\" (minimal remaining capacity after packing).\n    2. Give a small bonus to bins with larger remaining capacities, encouraging their\n       use for potentially larger future items.\n    3. Stabilize scoring and handle un-fittable bins explicitly.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Determine which bins can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Filter to only consider bins that can fit the item\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    fitting_indices = np.where(can_fit_mask)[0]\n\n    # --- Scoring components ---\n\n    # 1. Tight Fit Score: Prioritize bins with minimal remaining capacity after packing.\n    #    We want to minimize (remaining_capacity - item).\n    #    To turn this into a priority (higher is better), we can invert and scale.\n    #    A small value for (remaining_capacity - item) should result in a high score.\n    #    Using (max_possible_difference - diff) or similar.\n    #    Let's define a \"waste\" score: waste = remaining_capacity - item. Lower waste is better.\n    waste = fitting_bins_cap - item\n\n    # Normalize waste to [0, 1] range for consistent scoring.\n    # If all waste is 0, max_waste will be 0, avoid division by zero.\n    max_waste = np.max(waste) if len(waste) > 0 else 0\n    if max_waste > 0:\n        normalized_waste = waste / max_waste\n    else:\n        normalized_waste = np.zeros_like(waste) # All bins perfectly fit or no bins fit\n\n    # Tight fit priority: Higher when normalized_waste is lower (closer to 0).\n    # We can use a function like 1 - normalized_waste or apply a sigmoid-like shape.\n    # Let's try a simple inverted linear score: 1 - normalized_waste.\n    tight_fit_scores = 1.0 - normalized_waste\n\n    # 2. Capacity Bonus Score: Give a small bonus to bins with larger remaining capacities.\n    #    This encourages using bins that might be able to fit larger items later.\n    #    Normalize remaining capacity to [0, 1].\n    max_cap = np.max(bins_remain_cap) if num_bins > 0 else 1 # Avoid division by zero if no bins\n    if max_cap > 0:\n        normalized_caps = bins_remain_cap / max_cap\n    else:\n        normalized_caps = np.zeros_like(bins_remain_cap)\n\n    # Capacity bonus: Add a fraction of the normalized capacity.\n    capacity_bonus_weight = 0.1 # Tunable parameter\n    capacity_bonus_scores = capacity_bonus_weight * normalized_caps[can_fit_mask]\n\n    # --- Combine scores ---\n    # Total score for fitting bins is a weighted sum of tight fit and capacity bonus.\n    # We can use a sigmoid-like transformation to map scores to a [0, 1] range,\n    # ensuring that tight fits dominate but capacity bonus provides a nudge.\n    # Let's combine them linearly first and then apply a scaling/transformation.\n\n    combined_raw_scores = tight_fit_scores + capacity_bonus_scores\n\n    # Apply a sigmoid-like function to map scores to [0, 1] and create a smoother distribution.\n    # A simple approach is to scale and shift, or use np.tanh.\n    # Let's map the combined_raw_scores to a range and then use a function that\n    # emphasizes higher values. For simplicity, let's use a soft ranking.\n    # A softmax-like approach can also work to create relative priorities.\n\n    # Let's use a simple scaling and add noise for exploration.\n    # We want tight fits to be generally higher.\n    # A simple approach: score = tight_fit_score + bonus_for_large_capacity\n    # Let's rescale the tight_fit_scores to be in a higher range, e.g., [0.5, 1]\n    # and bonuses in [0, 0.1].\n\n    # Re-scaling tight fit scores to [0.5, 1.0]\n    scaled_tight_fit = 0.5 + 0.5 * tight_fit_scores\n    # Adding capacity bonus (scaled down)\n    final_fitting_scores = scaled_tight_fit + capacity_bonus_scores * 0.5 # Lower weight for bonus\n\n    # Add a small random component for exploration/stochasticity\n    exploration_noise = np.random.uniform(0, 0.05, size=len(final_fitting_scores))\n    final_fitting_scores += exploration_noise\n\n    # Assign these scores to the appropriate bins\n    priorities[fitting_indices] = final_fitting_scores\n\n    # Ensure non-fitting bins have 0 priority\n    priorities[~can_fit_mask] = 0.0\n\n    # Normalize priorities to [0, 1] for consistency if needed for specific algorithms,\n    # but for selection, relative values are what matter.\n    # If all fitting scores are 0 (which shouldn't happen if can_fit_mask is true and item fits),\n    # avoid division by zero.\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n    else:\n        # This case should ideally not be reached if can_fit_mask has true values.\n        pass\n\n    return priorities\n\n[Reflection]\nPrioritize tight fits, but consider flexibility. Normalize scores and tune weights.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}