```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority with which we want to add item to each bin using a
    balanced Softmax-Fit heuristic with tunable parameters for tightness and flexibility.

    This heuristic aims to:
    1. Prioritize bins that result in the least remaining capacity after packing
       the item (tightest fit). This is the primary driver for packing efficiency.
    2. Provide a secondary "flexibility bonus" for bins that, after packing, still
       retain significant remaining capacity. This aims to keep larger bins available
       for potentially larger future items.

    The scores are combined and then transformed using a softmax function to
    produce probability-like priorities, encouraging exploration while favoring
    better-fitting bins. Tunable parameters control the relative importance of
    tightness versus flexibility.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Returns:
        Array of same size as bins_remain_cap with priority score of each bin.
        Higher scores indicate higher priority. Scores sum to 1 if at least one
        bin can fit the item.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        # No bin can fit the item, return all zeros
        return priorities

    # --- Heuristic Components ---

    # 1. Tightness Score: Prioritize bins with minimal remaining capacity after packing.
    #    We want to minimize (remaining_capacity - item).
    #    For softmax (higher input = higher probability), we use a negative score
    #    where smaller remaining capacity results in a larger (less negative) score.
    #    The goal is to minimize `remaining_capacity_after_packing`.
    #    A small positive offset can help differentiate bins that perfectly fit.
    fitting_bins_caps = bins_remain_cap[can_fit_mask]
    remaining_capacity_after_packing = fitting_bins_caps - item

    # We want to maximize the "tightness", which is inversely related to
    # `remaining_capacity_after_packing`. A common transformation is
    # `max_possible_remaining - actual_remaining`.
    # Let's use `1 / (1 + remaining_capacity_after_packing)` or similar.
    # A simple approach is to use the negative of the remaining capacity,
    # potentially scaled and shifted.
    # `tightness = -remaining_capacity_after_packing`
    # Adding a constant and scaling can help normalize the range before softmax.
    # Let's scale `remaining_capacity_after_packing` so that tighter fits
    # produce higher scores.
    # Consider `max_rem_cap_after_packing - remaining_capacity_after_packing`.
    # Let's use `1.0 / (1.0 + remaining_capacity_after_packing)` and scale it.
    # A larger value here means a tighter fit.
    # `tightness_score_base = 1.0 / (1.0 + remaining_capacity_after_packing)`
    # Maximize tightness means minimize remaining_capacity_after_packing
    # For softmax, we want a higher value for better bins.
    # So, `tightness_score = -remaining_capacity_after_packing`

    # Let's try a simpler linear transformation for tightness that maps
    # lower remaining capacity to higher scores.
    # We can scale `remaining_capacity_after_packing` such that the minimum is
    # mapped to a higher value.
    # Example: If remaining capacities are [0.1, 0.3, 0.05], we want scores like [0.9, 0.7, 1.0].
    # A good transformation is `max_val - current_val`.
    # `max_rem = np.max(remaining_capacity_after_packing)`
    # `tightness_scores = max_rem - remaining_capacity_after_packing`
    # However, `max_rem` can be 0 if all bins are perfectly filled.
    # Let's use `1.0 / (1.0 + remaining_capacity_after_packing)` for robustness.
    # This maps 0 to 1, 0.1 to ~0.909, 0.3 to ~0.769.
    # To make it more pronounced for tighter fits, we can use `1.0 / (epsilon + remaining_capacity_after_packing)`
    # or `exp(-k * remaining_capacity_after_packing)`.
    # Let's stick to a simple inverse linear for now, potentially scaled.
    # `tightness_scores = 1.0 - (remaining_capacity_after_packing / bin_capacity)`
    # No, that's not quite right. We want to minimize remaining capacity *after* packing.
    # The scores for softmax should be higher for more desirable bins.
    # Desirable = smaller `remaining_capacity_after_packing`.
    # So, `score = C - remaining_capacity_after_packing` where C is a constant.
    # Let's use `tightness_score_base = -remaining_capacity_after_packing`
    # and scale it to give it more weight.
    tightness_weight = 10.0
    tightness_scores = tightness_weight * (-remaining_capacity_after_packing)

    # 2. Flexibility Bonus: Add a bonus for bins that still have substantial capacity.
    #    This bonus should be positive and increase with remaining capacity.
    #    We use the *original* `fitting_bins_caps` to represent the capacity
    #    available *before* fitting the item, or rather, the remaining capacity
    #    after packing the current item for the "flexibility" aspect.
    #    It's `fitting_bins_caps` that we are interested in for future flexibility.
    #    Let's consider the capacity *after* the item is packed.
    #    `remaining_after_item = fitting_bins_caps - item`
    #    We want to reward larger `remaining_after_item`.
    #    A simple approach is to scale these remaining capacities.
    #    We need to ensure this bonus doesn't overpower tightness.
    flexibility_scale_factor = 0.5  # Tunable: how much flexibility matters relative to tightness
    flexibility_bonus = flexibility_scale_factor * (fitting_bins_caps - item)

    # Combine scores: Tightness is the primary factor, flexibility adds a bonus.
    # The combined score directly influences the softmax probability.
    combined_scores = tightness_scores + flexibility_bonus

    # Apply softmax-like transformation for probabilities.
    # To prevent numerical overflow/underflow with exp(), subtract the maximum value.
    # Add a small epsilon to the scores to ensure that even if all `combined_scores`
    # are identical and negative, they don't collapse to a single softmax output.
    # Or, ensure the scores are not all identical and negative in a way that
    # exp underflows. Adding a small positive value to `tightness_scores` might help.
    # Let's add a small baseline positive value to ensure scores are not all negative.
    # baseline_positive = 1.0
    # combined_scores = baseline_positive + tightness_scores + flexibility_bonus

    # Softmax calculation:
    # Avoid numerical issues by shifting scores.
    try:
        max_score = np.max(combined_scores)
        # Ensure we don't have all zeros or very small numbers that result in zero exp
        # Add a small constant to ensure exp arguments are not too negative.
        # Adding a small positive value to `tightness_scores` before combining might be better.
        # Let's re-evaluate `tightness_scores`. It should be positive for tighter fits.
        # Let's map `remaining_capacity_after_packing` to a positive score.
        # E.g., `1 / (1 + rem_cap)`. Max value is 1 when rem_cap=0.
        # `tightness_scores_positive = 1.0 / (1.0 + remaining_capacity_after_packing)`
        # `tightness_scores_positive = np.clip(tightness_scores_positive, 1e-9, 1.0)` # Clip for stability
        # Then `combined_scores = tightness_weight * tightness_scores_positive + flexibility_bonus`
        # This seems more aligned with softmax inputs typically being positive.
        # Let's try this approach.

        # Recalculate tightness scores to be positive and higher for tighter fits.
        # Map `remaining_capacity_after_packing` [0, max_val] to [1, ~0] or similar.
        # Use `1.0 / (1.0 + remaining_capacity_after_packing)`.
        # Max value is 1 (when remaining_capacity_after_packing is 0).
        # Smallest value is `1.0 / (1.0 + max(remaining_capacity_after_packing))`.
        # This naturally gives higher scores to tighter fits.
        tightness_scores_positive = 1.0 / (1.0 + remaining_capacity_after_packing)

        # Re-combine with flexibility bonus. The flexibility bonus is directly
        # proportional to the remaining capacity after packing.
        # `flexibility_bonus = flexibility_scale_factor * (fitting_bins_caps - item)`
        # We want to add this bonus to the positive tightness score.
        # The `flexibility_bonus` can be negative if `fitting_bins_caps - item` is negative,
        # which shouldn't happen due to `can_fit_mask`.
        # The bonus should be positive, so if `fitting_bins_caps - item` is small,
        # the bonus is small.

        # Let's adjust `flexibility_bonus` to be always positive and scaled from
        # the remaining capacity.
        # `flexibility_bonus_positive = flexibility_scale_factor * (fitting_bins_caps - item)`
        # This still works as intended since `fitting_bins_caps - item` is the remaining capacity.

        # The combined score for softmax:
        # `tightness_weight` should be larger if we want tightness to dominate.
        # `flexibility_scale_factor` influences how much larger residual capacities matter.
        combined_scores = tightness_weight * tightness_scores_positive + flexibility_bonus

        # Softmax calculation
        max_combined_score = np.max(combined_scores)
        exp_scores = np.exp(combined_scores - max_combined_score)
        sum_exp_scores = np.sum(exp_scores)

        if sum_exp_scores > 1e-9:  # Prevent division by near-zero
            priorities[can_fit_mask] = exp_scores / sum_exp_scores
        else:
            # Fallback: if all scores are extremely low/equal, distribute probability evenly.
            num_fitting_bins = np.sum(can_fit_mask)
            if num_fitting_bins > 0:
                priorities[can_fit_mask] = 1.0 / num_fitting_bins

    except Exception as e:
        # Fallback for any unexpected numerical issues
        print(f"Numerical stability issue in priority_v2: {e}. Falling back to equal distribution.")
        num_fitting_bins = np.sum(can_fit_mask)
        if num_fitting_bins > 0:
            priorities[can_fit_mask] = 1.0 / num_fitting_bins

    return priorities
```
