```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority with which we want to add item to each bin,
    prioritizing tight fits and adding small bonuses for larger capacities.

    The strategy aims to:
    1. Prioritize bins that offer a "tight fit" (minimal remaining capacity after packing).
    2. Give a small bonus to bins with larger remaining capacities, encouraging their
       use for potentially larger future items.
    3. Stabilize scoring and handle un-fittable bins explicitly.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
        Higher scores indicate higher priority.
    """
    num_bins = len(bins_remain_cap)
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Determine which bins can fit the item
    can_fit_mask = bins_remain_cap >= item

    # If no bins can fit the item, return all zeros
    if not np.any(can_fit_mask):
        return priorities

    # Filter to only consider bins that can fit the item
    fitting_bins_cap = bins_remain_cap[can_fit_mask]
    fitting_indices = np.where(can_fit_mask)[0]

    # --- Scoring components ---

    # 1. Tight Fit Score: Prioritize bins with minimal remaining capacity after packing.
    #    We want to minimize (remaining_capacity - item).
    #    To turn this into a priority (higher is better), we can invert and scale.
    #    A small value for (remaining_capacity - item) should result in a high score.
    #    Using (max_possible_difference - diff) or similar.
    #    Let's define a "waste" score: waste = remaining_capacity - item. Lower waste is better.
    waste = fitting_bins_cap - item

    # Normalize waste to [0, 1] range for consistent scoring.
    # If all waste is 0, max_waste will be 0, avoid division by zero.
    max_waste = np.max(waste) if len(waste) > 0 else 0
    if max_waste > 0:
        normalized_waste = waste / max_waste
    else:
        normalized_waste = np.zeros_like(waste) # All bins perfectly fit or no bins fit

    # Tight fit priority: Higher when normalized_waste is lower (closer to 0).
    # We can use a function like 1 - normalized_waste or apply a sigmoid-like shape.
    # Let's try a simple inverted linear score: 1 - normalized_waste.
    tight_fit_scores = 1.0 - normalized_waste

    # 2. Capacity Bonus Score: Give a small bonus to bins with larger remaining capacities.
    #    This encourages using bins that might be able to fit larger items later.
    #    Normalize remaining capacity to [0, 1].
    max_cap = np.max(bins_remain_cap) if num_bins > 0 else 1 # Avoid division by zero if no bins
    if max_cap > 0:
        normalized_caps = bins_remain_cap / max_cap
    else:
        normalized_caps = np.zeros_like(bins_remain_cap)

    # Capacity bonus: Add a fraction of the normalized capacity.
    capacity_bonus_weight = 0.1 # Tunable parameter
    capacity_bonus_scores = capacity_bonus_weight * normalized_caps[can_fit_mask]

    # --- Combine scores ---
    # Total score for fitting bins is a weighted sum of tight fit and capacity bonus.
    # We can use a sigmoid-like transformation to map scores to a [0, 1] range,
    # ensuring that tight fits dominate but capacity bonus provides a nudge.
    # Let's combine them linearly first and then apply a scaling/transformation.

    combined_raw_scores = tight_fit_scores + capacity_bonus_scores

    # Apply a sigmoid-like function to map scores to [0, 1] and create a smoother distribution.
    # A simple approach is to scale and shift, or use np.tanh.
    # Let's map the combined_raw_scores to a range and then use a function that
    # emphasizes higher values. For simplicity, let's use a soft ranking.
    # A softmax-like approach can also work to create relative priorities.

    # Let's use a simple scaling and add noise for exploration.
    # We want tight fits to be generally higher.
    # A simple approach: score = tight_fit_score + bonus_for_large_capacity
    # Let's rescale the tight_fit_scores to be in a higher range, e.g., [0.5, 1]
    # and bonuses in [0, 0.1].

    # Re-scaling tight fit scores to [0.5, 1.0]
    scaled_tight_fit = 0.5 + 0.5 * tight_fit_scores
    # Adding capacity bonus (scaled down)
    final_fitting_scores = scaled_tight_fit + capacity_bonus_scores * 0.5 # Lower weight for bonus

    # Add a small random component for exploration/stochasticity
    exploration_noise = np.random.uniform(0, 0.05, size=len(final_fitting_scores))
    final_fitting_scores += exploration_noise

    # Assign these scores to the appropriate bins
    priorities[fitting_indices] = final_fitting_scores

    # Ensure non-fitting bins have 0 priority
    priorities[~can_fit_mask] = 0.0

    # Normalize priorities to [0, 1] for consistency if needed for specific algorithms,
    # but for selection, relative values are what matter.
    # If all fitting scores are 0 (which shouldn't happen if can_fit_mask is true and item fits),
    # avoid division by zero.
    max_priority = np.max(priorities)
    if max_priority > 0:
        priorities = priorities / max_priority
    else:
        # This case should ideally not be reached if can_fit_mask has true values.
        pass

    return priorities
```
