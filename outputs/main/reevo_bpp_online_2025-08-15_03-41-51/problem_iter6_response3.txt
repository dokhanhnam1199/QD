```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority with which we want to add item to each bin,
    prioritizing tight fits and adding bonuses for larger capacities,
    with tunable parameters.

    The strategy aims to:
    1. Prioritize bins that offer a "tight fit" (minimal remaining capacity after packing).
    2. Give a bonus to bins with larger remaining capacities, encouraging their
       use for potentially larger future items.
    3. Use a soft ranking mechanism to combine these factors and ensure smooth prioritization.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
        Higher scores indicate higher priority.
    """
    num_bins = len(bins_remain_cap)
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Determine which bins can fit the item
    can_fit_mask = bins_remain_cap >= item

    # If no bins can fit the item, return all zeros
    if not np.any(can_fit_mask):
        return priorities

    # Filter to only consider bins that can fit the item
    fitting_bins_cap = bins_remain_cap[can_fit_mask]
    fitting_indices = np.where(can_fit_mask)[0]

    # --- Scoring components ---

    # 1. Tight Fit Score: Prioritize bins with minimal remaining capacity after packing.
    #    Calculate the "slack" or "waste": slack = remaining_capacity - item.
    #    Lower slack is better. We want a score that increases as slack decreases.
    slack = fitting_bins_cap - item

    # To convert "low slack is good" to "high score is good", we can use a transformation.
    # A simple inverse relationship or a function that maps [min_slack, max_slack] to [high_score, low_score].
    # Let's use a function that is high for small slack and low for large slack.
    # Example: score = 1 / (slack + epsilon). Or, normalized_slack = slack / max_slack, then score = 1 - normalized_slack.
    # We'll use a variation of the latter: higher score for lower slack.
    # Normalize slack to [0, 1] range.
    max_slack = np.max(slack) if len(slack) > 0 else 0
    if max_slack > 0:
        normalized_slack = slack / max_slack
    else:
        normalized_slack = np.zeros_like(slack) # All bins perfectly fit or no bins fit

    # Tight fit priority: higher when normalized_slack is closer to 0.
    # We can use `1 - normalized_slack` or `np.exp(-k_tight * normalized_slack)`.
    # Let's use `1 - normalized_slack` for simplicity, scaled to a higher range.
    # A parameter `tight_fit_weight` can control its influence.
    tight_fit_weight = 1.0
    tight_fit_scores = tight_fit_weight * (1.0 - normalized_slack)

    # 2. Capacity Bonus Score: Give a bonus to bins with larger remaining capacities.
    #    This encourages using bins that might be able to fit larger items later.
    #    Normalize remaining capacity to [0, 1].
    max_overall_cap = np.max(bins_remain_cap) if num_bins > 0 else 1 # Avoid division by zero if no bins
    if max_overall_cap > 0:
        normalized_caps_fitting = fitting_bins_cap / max_overall_cap
    else:
        normalized_caps_fitting = np.zeros_like(fitting_bins_cap)

    # Capacity bonus: add a fraction of the normalized capacity.
    # A parameter `capacity_bonus_weight` controls its influence.
    capacity_bonus_weight = 0.2 # Tunable parameter
    capacity_bonus_scores = capacity_bonus_weight * normalized_caps_fitting

    # --- Combine scores ---
    # Combine the scores. We want tight fits to have a stronger say, but capacity bonus
    # provides a secondary preference.
    # A simple linear combination:
    combined_raw_scores = tight_fit_scores + capacity_bonus_scores

    # Apply a non-linear transformation to smooth scores and create a clearer preference.
    # Using a sigmoid-like function or tanh can help normalize and emphasize differences.
    # Let's use a scaled tanh function: `tanh(k * x)` maps to [-1, 1].
    # We want scores to be positive, so we can shift it or use a different function.
    # A simple exponential transformation `np.exp(x)` can also amplify differences.
    # Let's try an exponential to push higher scores further up.
    # Scaling factor `k_transform` can tune the steepness.
    k_transform = 1.0
    transformed_scores = np.exp(k_transform * combined_raw_scores)

    # Assign these transformed scores to the appropriate bins
    priorities[fitting_indices] = transformed_scores

    # Ensure non-fitting bins have 0 priority
    priorities[~can_fit_mask] = 0.0

    # Normalize priorities to [0, 1] for consistency if needed.
    # This step is crucial if the selection mechanism expects probabilities or normalized scores.
    max_priority = np.max(priorities)
    if max_priority > 0:
        priorities = priorities / max_priority
    else:
        # This case should ideally not be reached if can_fit_mask has true values
        # and transformed_scores are non-zero.
        pass

    return priorities
```
