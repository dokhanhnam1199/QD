{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-based Best Fit strategy.\n\n    This strategy aims to mimic the \"Best Fit\" heuristic within a softmax framework.\n    It prioritizes bins where placing the item results in the least remaining capacity\n    (i.e., the \"tightest\" fit). Bins that cannot accommodate the item receive zero priority.\n    The priorities are generated using a softmax function on desirability scores,\n    where a higher score indicates a better fit (less remaining capacity after packing).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n        For fitting bins, scores are derived from the resulting remaining capacity,\n        and transformed by softmax to represent probabilities or preferences.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        # Calculate the remaining capacity *after* placing the item for eligible bins\n        valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        resulting_remain_cap = valid_bins_remain_cap - item\n\n        # Define a desirability score for each fitting bin.\n        # For \"Best Fit\", we want to minimize `resulting_remain_cap`.\n        # A higher desirability score should correspond to a lower `resulting_remain_cap`.\n        # We can use a score that is inversely proportional to `resulting_remain_cap`.\n        # To avoid division by zero and ensure positive scores, we can add a small constant\n        # or use a transformation like `1 / (slack + 1)`.\n        # A simple approach is to use `max_possible_slack - slack`, where `max_possible_slack`\n        # is the maximum possible remaining capacity among fitting bins.\n        # Or, more directly, we can use a desirability score that is higher when `resulting_remain_cap` is smaller.\n        # Let's use `-resulting_remain_cap` as a raw desirability, so smaller slack (closer to 0) is better.\n        # For softmax, we want scores that are generally positive for exponentiation.\n        # A good score is `1.0 / (resulting_remain_cap + 1.0)` which is high when `resulting_remain_cap` is small.\n\n        # Let's use a score that emphasizes bins with very little slack.\n        # A slightly larger slack should yield a significantly lower score.\n        # We can use an exponential decay, or a simple inverse relationship.\n        # `score = 1.0 / (resulting_remain_cap + epsilon)` where epsilon is a small positive number.\n        # Let's use `1.0 / (resulting_remain_cap + 1e-6)` to ensure stability and positive values.\n\n        desirability_scores = 1.0 / (resulting_remain_cap + 1e-6)\n\n        # Apply softmax to convert desirability scores into a probability-like distribution.\n        # A temperature parameter can control the \"softness\" of the distribution.\n        # A lower temperature makes the distribution sharper, favoring the best bins more.\n        # A higher temperature makes it flatter, giving more similar probabilities.\n        temperature = 1.0  # Tunable parameter\n\n        # Avoid numerical instability with very large desirability scores by clipping or normalizing first,\n        # or by using a stable softmax implementation if available.\n        # Here, we use `exp(score / temperature)`.\n\n        try:\n            exp_scores = np.exp(desirability_scores / temperature)\n            sum_exp_scores = np.sum(exp_scores)\n            if sum_exp_scores > 0:\n                softmax_probabilities = exp_scores / sum_exp_scores\n            else:\n                # If all desirability scores are extremely small (or negative if we didn't ensure positivity),\n                # softmax might result in zeros. Assign uniform probability in such edge cases.\n                softmax_probabilities = np.ones_like(desirability_scores) / len(desirability_scores)\n        except OverflowError:\n            # Handle potential overflow if desirability_scores are too large\n            # A common approach is to subtract the max score before exponentiation.\n            max_score = np.max(desirability_scores)\n            stable_scores = desirability_scores - max_score\n            exp_scores = np.exp(stable_scores / temperature)\n            sum_exp_scores = np.sum(exp_scores)\n            if sum_exp_scores > 0:\n                softmax_probabilities = exp_scores / sum_exp_scores\n            else:\n                softmax_probabilities = np.ones_like(desirability_scores) / len(desirability_scores)\n\n\n        # Place the calculated probabilities back into the original priorities array\n        priorities[can_fit_mask] = softmax_probabilities\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    The Epsilon-Greedy strategy balances exploration and exploitation.\n    - With probability epsilon, it explores by choosing a random bin.\n    - With probability 1-epsilon, it exploits by choosing the bin that offers the\n      best \"fit\" for the current item.\n\n    Here, \"best fit\" is defined as the bin with the smallest remaining capacity\n    that can still accommodate the item. This is a common heuristic for bin packing\n    (e.g., First Fit Decreasing, Best Fit).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    epsilon = 0.2  # Exploration rate\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Determine which bins can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, assign a very low priority to all\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Exploitation: Find the best fit bin(s)\n    # Best fit means the bin with the smallest remaining capacity that still fits the item.\n    # We calculate (remaining_capacity - item) and find the minimum of this difference\n    # among the bins that can fit the item.\n    potential_bins_remaining_cap = bins_remain_cap[can_fit_mask]\n    if len(potential_bins_remaining_cap) > 0:\n        differences = potential_bins_remaining_cap - item\n        min_diff = np.min(differences)\n\n        # Bins with the minimum difference get a high priority (exploitation)\n        # We can assign a base high priority, e.g., 1.0\n        best_fit_indices_in_mask = np.where(differences == min_diff)[0]\n        original_indices_of_best_fit = np.where(can_fit_mask)[0][best_fit_indices_in_mask]\n        priorities[original_indices_of_best_fit] = 1.0\n\n    # Exploration: Assign a smaller priority to some bins randomly\n    # Identify bins that are candidates for exploration (can fit the item)\n    candidate_indices_for_exploration = np.where(can_fit_mask)[0]\n\n    # If there are candidate bins, randomly pick some to give a slightly lower\n    # exploration priority. This ensures exploration doesn't always pick the best.\n    if len(candidate_indices_for_exploration) > 0:\n        num_to_explore = int(np.floor(epsilon * len(candidate_indices_for_exploration)))\n        if num_to_explore > 0:\n            # Choose which of the candidate bins to give an exploration priority\n            explore_indices = np.random.choice(candidate_indices_for_exploration, size=num_to_explore, replace=False)\n            # Assign a priority lower than the best fit, but still positive\n            # This exploration priority should be lower than the exploitation priority (1.0)\n            exploration_priority_value = 0.5\n            priorities[explore_indices] = exploration_priority_value\n\n    # Ensure that bins that cannot fit the item have a priority of 0 (or negative if preferred)\n    priorities[~can_fit_mask] = 0.0\n\n    # Normalize priorities to avoid issues with very large/small numbers,\n    # though for selection, relative values are more important.\n    # This step is optional but can be good practice if priorities are used in other contexts.\n    # If all priorities are 0 (no bin can fit), this will result in NaNs, so handle that.\n    if np.max(priorities) > 0:\n        priorities = priorities / np.max(priorities)\n    else:\n        # If no bin could fit, and priorities are all 0, this is handled.\n        pass\n\n    # Further refinement: Add a small random component to the \"best fit\" bins\n    # to make the greedy choice less deterministic if multiple bins are tied for best fit.\n    # This can be considered a micro-exploration within the exploitation phase.\n    best_fit_indices_refined = np.where(priorities == 1.0)[0]\n    if len(best_fit_indices_refined) > 0:\n        random_boost = np.random.rand(len(best_fit_indices_refined)) * 0.1 # Small boost\n        priorities[best_fit_indices_refined] += random_boost\n\n    # Re-normalize after the boost if needed\n    if np.max(priorities) > 0:\n        priorities = priorities / np.max(priorities)\n\n\n    return priorities\n\n[Reflection]\nPrioritize strict fits, balance exploration/exploitation, and consider adaptive temperature.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}