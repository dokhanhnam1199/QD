```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Softmax-Based Fit.

    The priority is calculated by considering how well the item fits into the bin,
    penalizing bins that are too full or too empty relative to the item's size.
    A temperature parameter can be used to control the "softness" of the softmax.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Filter out bins that cannot accommodate the item
    available_bins_mask = bins_remain_cap >= item
    priorities = np.zeros_like(bins_remain_cap)

    if np.any(available_bins_mask):
        valid_bins_remain_cap = bins_remain_cap[available_bins_mask]

        # Calculate a "fit score" for each available bin.
        # A good fit is when the remaining capacity is just enough or slightly more than the item.
        # We can use the inverse of (remaining_capacity - item) as a measure,
        # but this can lead to division by zero or very large values if remaining_capacity is very close to item.
        # A better approach might be to look at the ratio or a penalized version.

        # Strategy: Prioritize bins where the remaining capacity is "close" to the item size.
        # This encourages filling bins more completely.
        # Let's define a "fitness" as the proportion of the item that fits into the remaining capacity.
        # Or, a score that is high when remaining_capacity is close to item, and decreases
        # as remaining_capacity deviates from item.

        # Consider a score based on the negative difference between remaining capacity and item size,
        # but this penalizes overfilling significantly and underfilling somewhat.
        # fitness_scores = -(valid_bins_remain_cap - item)

        # Alternative: Consider the "tightness" of the fit.
        # A tighter fit is when remaining_capacity is slightly larger than item.
        # Let's use a score that is high when remaining_capacity is close to `item` and decreases as it gets larger.
        # `1 / (valid_bins_remain_cap - item + epsilon)` can be unstable.

        # Let's use a score that rewards bins that can *just* fit the item.
        # This could be related to the "waste" or the "slack" remaining.
        # We want bins where `bins_remain_cap - item` is small and non-negative.
        # So, a high score for small non-negative `bins_remain_cap - item`.

        # Let's define a "proximity" score: a function that is maximized when `bins_remain_cap - item` is close to 0.
        # For example, a Gaussian-like function or an inverse quadratic.

        # Let's try a simple approach: the larger the remaining capacity is *relative* to the item,
        # the less preferred it might be, up to a point.
        # However, Softmax-Based Fit usually means mapping a desirability score to a probability-like distribution.
        # A common desirability for "Best Fit" is simply `remaining_capacity - item`.
        # If we want to use Softmax, we need a score that can be exponentiated.

        # Let's try to score bins based on how "full" they would become after adding the item.
        # A more full bin is often preferred to keep smaller bins open.
        # So, let's consider `item / original_bin_capacity`. But we don't have original capacity here.
        # We only have `bins_remain_cap`.

        # Let's consider a score where we want to leave as little space as possible after packing.
        # The space left would be `bins_remain_cap - item`. We want this to be small.
        # So, we want `bins_remain_cap - item` to be minimized.
        # However, Softmax usually picks the *best* option, so a larger score is better.
        # Let's define a score as `-(bins_remain_cap - item)` which is `item - bins_remain_cap`.
        # This means we prefer bins where the remaining capacity is smaller (closer to the item size).
        # This is essentially a variation of "Worst Fit Decreasing" if used with a greedy strategy.
        # For Softmax, we need a score.

        # Let's try this: a score that is high when `bins_remain_cap` is just slightly larger than `item`.
        # Consider `1 / (bins_remain_cap - item + 1e-6)` might work but is prone to large values.

        # A more robust approach for Softmax is to define a desirability.
        # For example, a desirability score related to how "good" the fit is.
        # A good fit means `bins_remain_cap` is close to `item`.
        # Let's use a score based on the *proportion* of capacity used by the item.
        # For available bins, let's calculate `item / (item + bins_remain_cap)`. This is like a "fill ratio" if it were empty.
        # This would prefer bins that are already somewhat full.
        # However, that's not directly related to `bins_remain_cap` alone.

        # Let's re-evaluate the "Softmax-Based Fit" idea. The core is to convert
        # desirability scores into a probability distribution using softmax.
        # The desirability should reflect how good it is to place the item in a bin.
        # For BPP, a good fit is often considered one that leaves minimal waste.
        # Waste for a bin is `bins_remain_cap - item`. We want to minimize this.
        # A higher score should correspond to lower waste.
        # So, let's try a score that is inversely related to the waste, or directly related to how close `bins_remain_cap` is to `item`.

        # Let's try a score that prioritizes bins where `bins_remain_cap` is slightly larger than `item`.
        # `score = 1.0 / (bins_remain_cap - item + 1.0)` could work, as it's high when the denominator is small.
        # Adding 1.0 makes it more numerically stable and ensures the denominator is at least 1.

        # More formally for Softmax, we want a utility function u(state).
        # Here, state could be (item, bin_remaining_capacity).
        # A possible utility function for placing `item` in `bin` with `remaining_cap`:
        # utility = -(remaining_cap - item)^2  (penalizes deviation from perfect fit)
        # utility = -(remaining_cap - item) (prefers bins with less slack)
        # utility = item / remaining_cap (prefers fuller bins)

        # Let's try a utility that favors bins where `bins_remain_cap` is close to `item`.
        # We can use a Gaussian-like function centered around `item`.
        # `score = exp(-alpha * (bins_remain_cap - item)^2)`
        # Or, for simplicity, a score inversely proportional to the slack:
        # `score = 1.0 / (bins_remain_cap - item + epsilon)`

        # Let's use a desirability function that's high when remaining_capacity is just slightly larger than item.
        # This encourages finding a "tight" fit.
        # `desirability = 1.0 / (bins_remain_cap - item + 1.0)`

        # Alternative idea for Softmax: Focus on the *resulting* occupancy.
        # If the item fits, the new remaining capacity will be `bins_remain_cap - item`.
        # We might want bins that are "nearly full" after packing.
        # So, we want `bins_remain_cap - item` to be small, but not negative.
        # The "fullness" is `1 - (bins_remain_cap - item) / original_capacity`. We don't have original capacity.

        # Let's simplify: Use a desirability score that's directly related to the quality of the fit.
        # A good fit means `bins_remain_cap - item` is small.
        # Let's define a desirability that is high for small non-negative differences.
        # `desirability = 1.0 / (max(0, bins_remain_cap - item) + 1.0)`
        # This will give a score of 1.0 for bins where `bins_remain_cap == item`,
        # and a score close to 1.0 for bins where `bins_remain_cap` is slightly larger than `item`.
        # For bins where `bins_remain_cap < item`, they are already excluded.

        # Let's try a slightly different approach: penalize bins that leave *too much* space.
        # A desirable bin is one where `bins_remain_cap` is close to `item`.
        # Let's define a score based on the "slack" or "waste". Lower waste is better.
        # For softmax, higher score is better. So, a score inversely proportional to waste.
        # Waste = `bins_remain_cap - item`.
        # Score = `1.0 / (bins_remain_cap - item + epsilon)`

        # Let's use a score that prefers bins that become "most full" after packing the item.
        # This means minimizing `bins_remain_cap - item`.
        # Let's map this to a positive score for softmax.
        # Consider `score = exp(-alpha * (bins_remain_cap - item))`
        # Or, more simply, `score = 1.0 / (bins_remain_cap - item + C)` where C is a small constant.
        # If `bins_remain_cap` is much larger than `item`, this score becomes small.
        # If `bins_remain_cap` is just slightly larger than `item`, this score is large.
        # This is similar to "Best Fit" in terms of the ordering, but the softmax transforms it.

        # Let's try `score = 1.0 / (bins_remain_cap - item + 1.0)` for available bins.
        # This rewards bins that have a small positive slack.

        desirability_scores = 1.0 / (valid_bins_remain_cap - item + 1.0)

        # Apply softmax to the desirability scores
        # Softmax function: exp(x_i) / sum(exp(x_j))
        # A temperature parameter can be added to control the sharpness of the distribution.
        temperature = 1.0  # Can be tuned
        exp_scores = np.exp(desirability_scores / temperature)
        softmax_probabilities = exp_scores / np.sum(exp_scores)

        # Assign these probabilities back to the original priority array
        priorities[available_bins_mask] = softmax_probabilities

    return priorities
```
