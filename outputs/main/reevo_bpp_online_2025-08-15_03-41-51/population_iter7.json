[
  {
    "stdout_filepath": "problem_iter6_response0.txt_stdout.txt",
    "code_path": "problem_iter6_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This priority function implements a \"Best Fit\" strategy. It prioritizes\n    bins that can accommodate the item with the least amount of remaining\n    capacity (i.e., the tightest fit). This is achieved by assigning a priority\n    equal to the negative of the remaining capacity after packing the item.\n    Bins that cannot accommodate the item are given a very low priority (negative infinity).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority. Bins that cannot fit the item\n        will have a very low priority (negative infinity).\n    \"\"\"\n    # Initialize priorities to a very low value, effectively excluding bins\n    # that cannot fit the item from being selected by default.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that have enough remaining capacity for the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the priority.\n    # The goal is to find the \"tightest fit,\" meaning the bin where\n    # `bins_remain_cap - item` is minimized.\n    # To convert this minimization problem into a maximization problem for\n    # priority scores, we use the negative of the remaining capacity after\n    # placing the item.\n    # A smaller positive value for `bins_remain_cap - item` will result in\n    # a larger (less negative) priority score.\n    # E.g., if item=0.3 and capacities are [0.35, 0.4, 0.5]:\n    # Remaining capacities after packing: [0.05, 0.1, 0.2]\n    # Priorities: [-0.05, -0.1, -0.2]\n    # The highest priority (-0.05) corresponds to the tightest fit.\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 30.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response1.txt_stdout.txt",
    "code_path": "problem_iter6_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin,\n    prioritizing tight fits and encouraging the use of fuller bins.\n\n    This heuristic aims to:\n    1. Strongly prioritize bins where the remaining capacity is close to the item size (tight fit).\n    2. Provide a secondary preference for bins that are already relatively full,\n       as this suggests they are being utilized effectively and might be candidates\n       for being closed sooner.\n    3. Explicitly handle bins that cannot fit the item by assigning them zero priority.\n    4. Use a non-linear transformation (e.g., exponential decay) for tightness to\n       create a smoother priority gradient.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Get capacities of bins that can fit the item\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Scoring Component 1: Tight Fit ---\n    # Calculate the \"waste\" or leftover space after fitting the item.\n    # Smaller waste means a tighter fit.\n    waste = fitting_bins_cap - item\n\n    # To prioritize smaller waste, we can use an inverse relationship.\n    # A common way is to use an exponential decay: exp(-k * waste).\n    # A smaller `waste` results in a value closer to 1.\n    # A parameter `k` can control the steepness of this preference.\n    # Let's use k=2 for moderate emphasis on tightness.\n    tightness_score = np.exp(-2 * waste)\n\n    # --- Scoring Component 2: Bin Fullness (Secondary Preference) ---\n    # We want to prioritize bins that are already fuller (less remaining capacity).\n    # This encourages packing items into partially filled bins rather than\n    # spreading them out too thinly.\n    # Normalize the remaining capacity to get a sense of how full each bin is.\n    # For bins that can fit the item, the original capacity matters.\n    # Normalize original remaining capacities of fitting bins.\n    min_orig_cap = np.min(fitting_bins_cap)\n    max_orig_cap = np.max(fitting_bins_cap)\n\n    if max_orig_cap == min_orig_cap:\n        # If all fitting bins have the same capacity, this component is uniform.\n        # Assign a base score or 0, as it doesn't differentiate.\n        fullness_score = np.zeros_like(fitting_bins_cap)\n    else:\n        # Higher remaining capacity means less full. We want the opposite.\n        # So, we use (max_orig_cap - fitting_bins_cap) and normalize it.\n        # Or, simpler, normalize (min_orig_cap to max_orig_cap) and then invert.\n        # Let's normalize the original capacities:\n        normalized_original_cap = (fitting_bins_cap - min_orig_cap) / (max_orig_cap - min_orig_cap)\n        # We want to prioritize *smaller* original capacities, so we invert this.\n        fullness_score = 1.0 - normalized_original_cap\n\n    # --- Combine Scores ---\n    # Combine tightness and fullness. Tightness should be the primary driver.\n    # We can add the fullness score as a bonus, scaled down.\n    # A small bonus weight (e.g., 0.2) ensures tightness remains dominant.\n    combined_scores = tightness_score + 0.2 * fullness_score\n\n    # Assign the computed scores to the correct bins\n    priorities[can_fit_mask] = combined_scores\n\n    # Ensure no NaNs or infinities result from calculations\n    priorities = np.nan_to_num(priorities, nan=0.0, posinf=0.0, neginf=0.0)\n\n    # Normalize priorities to a [0, 1] range for consistent comparison across different item/bin states.\n    # This step helps if the priority values are used in contexts that expect a bounded range.\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n    else:\n        # If all calculated priorities are zero (e.g., item size is larger than bin capacity),\n        # ensure we return an array of zeros.\n        priorities = np.zeros_like(priorities)\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 194.3192398051029,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response2.txt_stdout.txt",
    "code_path": "problem_iter6_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a tunable Sigmoid Fit Score.\n\n    This version further refines the Sigmoid Fit Score by providing an explicit mechanism\n    to tune both the `steepness` of the sigmoid and the `ideal_gap_fraction`.\n    The `steepness` parameter controls the sensitivity of the priority to deviations from the ideal gap,\n    while `ideal_gap_fraction` dictates the preferred slack after packing.\n    A higher `steepness` means the heuristic is pickier about bins that are not close to the ideal gap.\n    A higher `ideal_gap_fraction` means the heuristic prefers leaving more space.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of -np.inf.\n    \"\"\"\n    # Initialize priorities to negative infinity for bins that cannot fit the item.\n    # This ensures they are never selected.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can accommodate the item\n    possible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(possible_bins_mask):\n        # If no bin can fit the item, return the initialized -np.inf priorities.\n        return priorities\n\n    # Calculate the \"gap\" for bins that can fit the item.\n    # Gap is the remaining capacity after placing the item.\n    gaps = bins_remain_cap[possible_bins_mask] - item\n\n    # --- Tunable Parameters ---\n    # `steepness`: Controls how quickly the priority score changes around the ideal gap.\n    # Higher values make the heuristic more sensitive to deviations from the ideal gap.\n    # A value of 0 would make all fitting bins have a priority of 0.5 (if ideal_gap is the same for all).\n    # Generally, values between 10 and 50 are good starting points.\n    steepness = 30.0\n\n    # `ideal_gap_fraction`: Defines the target gap as a fraction of the item's size.\n    # This parameter influences the 'bias' of the sigmoid, determining what gap is considered 'optimal'.\n    # 0.0 means prefer a perfect fit.\n    # 0.1 means prefer a gap of 10% of the item size.\n    # This encourages leaving some buffer space for potentially larger items later.\n    ideal_gap_fraction = 0.10  # Target for a gap of 10% of item size\n\n    # Calculate the ideal gap. Ensure it's non-negative.\n    ideal_gap = max(0.0, ideal_gap_fraction * item)\n\n    # --- Sigmoid Argument Calculation ---\n    # The sigmoid function: S(x) = 1 / (1 + exp(-x)) maps x to [0, 1].\n    # We want to center the most favorable gap (ideal_gap) at the sigmoid's midpoint (where x=0).\n    # The argument to the sigmoid will be `steepness * (ideal_gap - current_gap)`.\n    #\n    # - If `current_gap < ideal_gap`: (ideal_gap - current_gap) > 0, sigmoid_arg > 0, S(x) > 0.5.\n    #   Smaller gaps (closer to 0) get higher scores, up to ideal_gap.\n    # - If `current_gap = ideal_gap`: sigmoid_arg = 0, S(x) = 0.5. This is the pivot.\n    # - If `current_gap > ideal_gap`: (ideal_gap - current_gap) < 0, sigmoid_arg < 0, S(x) < 0.5.\n    #   Larger gaps get lower scores.\n    sigmoid_arg = steepness * (ideal_gap - gaps)\n\n    # Apply the sigmoid function to compute the priority scores for fitting bins.\n    priorities[possible_bins_mask] = 1 / (1 + np.exp(-sigmoid_arg))\n\n    return priorities",
    "response_id": 2,
    "obj": 4.058635819704831,
    "cyclomatic_complexity": 2.0,
    "halstead": 118.59257041502654,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response3.txt_stdout.txt",
    "code_path": "problem_iter6_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin,\n    prioritizing tight fits and adding bonuses for larger capacities,\n    with tunable parameters.\n\n    The strategy aims to:\n    1. Prioritize bins that offer a \"tight fit\" (minimal remaining capacity after packing).\n    2. Give a bonus to bins with larger remaining capacities, encouraging their\n       use for potentially larger future items.\n    3. Use a soft ranking mechanism to combine these factors and ensure smooth prioritization.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Determine which bins can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Filter to only consider bins that can fit the item\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    fitting_indices = np.where(can_fit_mask)[0]\n\n    # --- Scoring components ---\n\n    # 1. Tight Fit Score: Prioritize bins with minimal remaining capacity after packing.\n    #    Calculate the \"slack\" or \"waste\": slack = remaining_capacity - item.\n    #    Lower slack is better. We want a score that increases as slack decreases.\n    slack = fitting_bins_cap - item\n\n    # To convert \"low slack is good\" to \"high score is good\", we can use a transformation.\n    # A simple inverse relationship or a function that maps [min_slack, max_slack] to [high_score, low_score].\n    # Let's use a function that is high for small slack and low for large slack.\n    # Example: score = 1 / (slack + epsilon). Or, normalized_slack = slack / max_slack, then score = 1 - normalized_slack.\n    # We'll use a variation of the latter: higher score for lower slack.\n    # Normalize slack to [0, 1] range.\n    max_slack = np.max(slack) if len(slack) > 0 else 0\n    if max_slack > 0:\n        normalized_slack = slack / max_slack\n    else:\n        normalized_slack = np.zeros_like(slack) # All bins perfectly fit or no bins fit\n\n    # Tight fit priority: higher when normalized_slack is closer to 0.\n    # We can use `1 - normalized_slack` or `np.exp(-k_tight * normalized_slack)`.\n    # Let's use `1 - normalized_slack` for simplicity, scaled to a higher range.\n    # A parameter `tight_fit_weight` can control its influence.\n    tight_fit_weight = 1.0\n    tight_fit_scores = tight_fit_weight * (1.0 - normalized_slack)\n\n    # 2. Capacity Bonus Score: Give a bonus to bins with larger remaining capacities.\n    #    This encourages using bins that might be able to fit larger items later.\n    #    Normalize remaining capacity to [0, 1].\n    max_overall_cap = np.max(bins_remain_cap) if num_bins > 0 else 1 # Avoid division by zero if no bins\n    if max_overall_cap > 0:\n        normalized_caps_fitting = fitting_bins_cap / max_overall_cap\n    else:\n        normalized_caps_fitting = np.zeros_like(fitting_bins_cap)\n\n    # Capacity bonus: add a fraction of the normalized capacity.\n    # A parameter `capacity_bonus_weight` controls its influence.\n    capacity_bonus_weight = 0.2 # Tunable parameter\n    capacity_bonus_scores = capacity_bonus_weight * normalized_caps_fitting\n\n    # --- Combine scores ---\n    # Combine the scores. We want tight fits to have a stronger say, but capacity bonus\n    # provides a secondary preference.\n    # A simple linear combination:\n    combined_raw_scores = tight_fit_scores + capacity_bonus_scores\n\n    # Apply a non-linear transformation to smooth scores and create a clearer preference.\n    # Using a sigmoid-like function or tanh can help normalize and emphasize differences.\n    # Let's use a scaled tanh function: `tanh(k * x)` maps to [-1, 1].\n    # We want scores to be positive, so we can shift it or use a different function.\n    # A simple exponential transformation `np.exp(x)` can also amplify differences.\n    # Let's try an exponential to push higher scores further up.\n    # Scaling factor `k_transform` can tune the steepness.\n    k_transform = 1.0\n    transformed_scores = np.exp(k_transform * combined_raw_scores)\n\n    # Assign these transformed scores to the appropriate bins\n    priorities[fitting_indices] = transformed_scores\n\n    # Ensure non-fitting bins have 0 priority\n    priorities[~can_fit_mask] = 0.0\n\n    # Normalize priorities to [0, 1] for consistency if needed.\n    # This step is crucial if the selection mechanism expects probabilities or normalized scores.\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n    else:\n        # This case should ideally not be reached if can_fit_mask has true values\n        # and transformed_scores are non-zero.\n        pass\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 7.0,
    "halstead": 242.75561920895692,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response4.txt_stdout.txt",
    "code_path": "problem_iter6_code4.py",
    "code": "import numpy as np\nfrom scipy.special import expit\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin,\n    prioritizing tightest fits with a bonus for flexibility, using normalized values\n    and sigmoid for smooth prioritization.\n\n    This version refines the normalization and combination of scores.\n\n    The priority is a composite score based on:\n    1. Tightness: Prioritizes bins with minimal non-negative remaining capacity\n       that can fit the item. Modeled by a sigmoid function of the negative gap.\n    2. Flexibility: Prioritizes bins with larger remaining capacity.\n       Modeled by a sigmoid function of the normalized remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins, dtype=float)\n\n    # Parameters for tuning:\n    # k_tight: Controls the steepness of the tightness score's sigmoid. Higher k_tight\n    #          means smaller gaps are much more favored.\n    # k_flex: Controls the steepness of the flexibility score's sigmoid. Higher k_flex means\n    #         larger capacities reach saturation faster.\n    # w_tight: Weight for the tightness component.\n    # w_flex: Weight for the flexibility component.\n    # norm_clip_val: Value to clip normalized capacities at to prevent extreme flexibility scores,\n    #                especially when capacities are very close.\n    k_tight = 7.0\n    k_flex = 3.0\n    w_tight = 0.7\n    w_flex = 0.3\n    norm_clip_val = 1.0\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Process only bins that can fit the item\n    eligible_bins_indices = np.where(can_fit_mask)[0]\n\n    if len(eligible_bins_indices) > 0:\n        eligible_rem_cap = bins_remain_cap[eligible_bins_indices]\n        gaps = eligible_rem_cap - item  # These are guaranteed non-negative\n\n        # --- Calculate Normalized Tightness Score ---\n        # Normalize gaps to be between 0 and 1.\n        # A smaller gap should result in a higher score.\n        # We want to map `gaps` to an input for sigmoid such that small positive gaps are mapped to high values.\n        # Using `expit(k * (1 - normalized_gap))`.\n        max_gap = np.max(gaps) if len(gaps) > 0 else 0\n        min_gap = np.min(gaps) if len(gaps) > 0 else 0\n\n        if max_gap > min_gap:\n            # Normalize gaps to [0, 1] where 0 is the tightest fit.\n            normalized_gaps = (gaps - min_gap) / (max_gap - min_gap)\n        else:\n            # All gaps are the same, so normalized gap is 0 for all.\n            normalized_gaps = np.zeros_like(gaps)\n\n        # Apply sigmoid. We want higher score for smaller normalized_gaps (closer to 0).\n        # Input to sigmoid should be high for low normalized_gaps.\n        # `k_tight * (1.0 - normalized_gaps)`: when normalized_gaps=0 (tightest), input=k_tight (high).\n        #                                    when normalized_gaps=1 (loosest), input=0 (mid).\n        tightness_score = expit(k_tight * (1.0 - normalized_gaps))\n\n        # --- Calculate Normalized Flexibility Score ---\n        # Normalize remaining capacities to be between 0 and 1.\n        # Larger remaining capacity should result in a higher score.\n        # Using `expit(k * normalized_cap)`.\n        max_cap = np.max(bins_remain_cap) if num_bins > 0 else 0\n        min_cap = np.min(bins_remain_cap) if num_bins > 0 else 0\n\n        if max_cap > min_cap:\n            # Normalize capacities to [0, 1] where 0 is the smallest capacity overall,\n            # and 1 is the largest capacity overall.\n            normalized_caps = (eligible_rem_cap - min_cap) / (max_cap - min_cap)\n            # Clip to prevent extreme sigmoid values if max_cap is very close to min_cap,\n            # or if there's only one bin.\n            normalized_caps = np.clip(normalized_caps, 0.0, norm_clip_val)\n        else:\n            # If all bins have the same capacity (or only one bin exists),\n            # assign a neutral normalized capacity (0.5) for flexibility.\n            normalized_caps = np.full_like(eligible_rem_cap, 0.5)\n\n        # Apply sigmoid. Higher normalized_caps (closer to 1) should yield higher score.\n        flexibility_score = expit(k_flex * normalized_caps)\n\n        # --- Combine Scores ---\n        # Weighted sum of normalized tightness and flexibility scores.\n        # Both scores are already in [0, 1].\n        composite_priorities = w_tight * tightness_score + w_flex * flexibility_score\n\n        # Assign the calculated composite priorities back to the main priorities array\n        priorities[eligible_bins_indices] = composite_priorities\n\n        # --- Final Normalization ---\n        # Normalize all priorities to ensure the maximum priority is 1.0.\n        # This helps in making the choice of the best bin unambiguous.\n        max_priority_value = np.max(priorities)\n        if max_priority_value > 0:\n            priorities = priorities / max_priority_value\n        # If max_priority_value is 0 (e.g., no eligible bins or all calculations resulted in 0),\n        # the priorities array remains all zeros.\n\n    return priorities",
    "response_id": 4,
    "obj": 12.963701635420822,
    "cyclomatic_complexity": 9.0,
    "halstead": 359.45228222839756,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response5.txt_stdout.txt",
    "code_path": "problem_iter6_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin,\n    prioritizing tight fits using a hyperbolic tangent scaling for smoother\n    distribution, and adding a small bonus for larger capacities.\n\n    The strategy aims to:\n    1. Prioritize bins that offer a \"tight fit\" (minimal remaining capacity after packing).\n       This is modeled using a scaled inverse of the \"waste\" (remaining_capacity - item).\n    2. Give a small bonus to bins with larger remaining capacities to encourage\n       their use for potentially larger future items.\n    3. Use a tanh activation to map scores to a more stable [0, 1] range,\n       effectively balancing the two objectives.\n    4. Handle un-fittable bins explicitly with zero priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Determine which bins can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Filter to only consider bins that can fit the item\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    fitting_indices = np.where(can_fit_mask)[0]\n\n    # --- Scoring components ---\n\n    # 1. Tight Fit Score: Prioritize bins with minimal remaining capacity after packing.\n    #    We want to minimize (remaining_capacity - item), which is the \"waste\".\n    #    To map this to a priority (higher is better), we'll use an inverse relationship.\n    #    A smaller waste should yield a higher score.\n    waste = fitting_bins_cap - item\n\n    # Normalize waste to a [0, K] range for scaling. We want smaller waste to be higher priority.\n    # A common strategy is to use a function that maps [0, max_waste] to a range where 0 maps to high.\n    # Let's map waste to a negative value for tanh: -waste / scale_factor.\n    # The scale_factor determines the steepness of the priority curve. A larger scale_factor\n    # means more sensitivity to small differences in waste.\n    # Max waste can be large, so we need a robust scaling.\n    # Let's scale by a factor related to the maximum possible waste or average waste.\n    # A simpler approach: map waste to [0, 1] and then invert.\n    epsilon = 1e-9 # To avoid division by zero or log(0)\n    max_waste = np.max(waste) if len(waste) > 0 else 0\n    \n    # If max_waste is 0, all fitting bins are perfect fits, give them highest priority.\n    if max_waste < epsilon:\n        tight_fit_scores = np.ones_like(waste)\n    else:\n        # Map waste to a range that favors smaller values.\n        # Using a function like 1 / (1 + waste/scale) or exp(-waste/scale)\n        # or directly feeding negative scaled waste into tanh.\n        # Let's use a scale factor that is roughly the average waste or a fraction of max_cap.\n        # A simple scaling factor can be the maximum *remaining* capacity across all bins (before fitting).\n        max_original_cap = np.max(bins_remain_cap) if num_bins > 0 else 1\n        scale_factor_tight = max_original_cap * 0.5 # Tunable parameter for tightness sensitivity\n\n        # Negative scaled waste for tanh: closer to 0 for smaller waste.\n        # We want smaller waste to map to larger positive tanh output.\n        # tanh(x) is 0 at x=0, approaches 1 for large positive x, -1 for large negative x.\n        # So, we want to map small waste to large positive arguments for tanh.\n        # This means mapping small waste to large negative arguments of -waste/scale_factor.\n        # So, small waste -> large positive arg for -waste/scale_factor\n        # Example: waste=0.1, scale=1 -> -0.1. waste=1, scale=1 -> -1.\n        # This seems counter-intuitive. Let's rethink.\n        # We want to prioritize bins where (fitting_bins_cap - item) is small.\n        # Let's use (max_possible_prior_capacity - fitting_bins_cap) and add item.\n        # Or, focus on (remaining_capacity - item). We want this value to be small.\n        # Let's map (remaining_capacity - item) to a range, and then apply a function that\n        # results in higher scores for smaller values.\n        # Consider the \"closeness\" to fitting: `fitting_bins_cap - item`.\n        # We want `fitting_bins_cap - item` to be small.\n        # Map `fitting_bins_cap - item` to a range `[-S, S]` where `S` is some scaling factor.\n        # Then apply tanh.\n        # The scale factor `S` should relate to typical waste values.\n        scale_tightness = np.max(bins_remain_cap[can_fit_mask]) if len(fitting_bins_cap) > 0 else 1\n        # Ensure scale_tightness is not zero and not too small\n        scale_tightness = max(scale_tightness * 0.5, epsilon) \n        \n        # Negative waste scaled: smaller waste -> larger negative value -> higher tanh output\n        scaled_negative_waste = -(waste / scale_tightness)\n        tight_fit_scores = np.tanh(scaled_negative_waste) # tanh outputs [-1, 1]\n\n    # 2. Capacity Bonus Score: Give a small bonus to bins with larger remaining capacities.\n    #    Normalize remaining capacity to a [0, 1] range.\n    max_original_cap = np.max(bins_remain_cap) if num_bins > 0 else 1\n    max_original_cap = max(max_original_cap, epsilon) # Avoid division by zero\n    \n    # Consider original remaining capacity for the bonus.\n    # We want larger capacities to get a higher bonus.\n    # Normalize capacity and then apply a multiplier.\n    capacity_bonus_weight = 0.15 # Tunable parameter\n    # Use original bins_remain_cap for bonus calculation, not just fitting ones\n    normalized_original_caps = bins_remain_cap / max_original_cap\n    # Apply bonus only to fitting bins\n    capacity_bonus_scores = capacity_bonus_weight * normalized_original_caps[can_fit_mask]\n\n    # --- Combine scores ---\n    # Add the capacity bonus to the tight fit score.\n    # The tight_fit_scores from tanh are in [-1, 1].\n    # We want to shift these to be non-negative and ensure tight fits are generally higher.\n    # Add `1` to tanh scores to shift range to [0, 2].\n    # Then, we can add the capacity bonus.\n    combined_scores_raw = (1 + tight_fit_scores) + capacity_bonus_scores\n\n    # Apply a final scaling or activation to ensure scores are in a reasonable range,\n    # typically [0, 1], and to emphasize higher scores.\n    # A simple way is to normalize by the max possible value or apply another tanh.\n    # Let's normalize by the maximum possible combined score (theoretically) or empirically.\n    # The maximum value of (1 + tanh) is 2. The maximum capacity bonus is capacity_bonus_weight.\n    # So, max raw score is roughly 2 + capacity_bonus_weight.\n    \n    # Scale combined_scores_raw to [0, 1]\n    # The range of (1 + tight_fit_scores) is [0, 2].\n    # The range of capacity_bonus_scores is [0, capacity_bonus_weight].\n    # Max raw score ~ 2 + capacity_bonus_weight.\n    # Min raw score ~ 0 + 0 = 0 (if tight_fit = -1 and bonus = 0).\n\n    # A simple scaling by a factor slightly larger than the max possible raw score.\n    # Or, use tanh again to compress and normalize.\n    # Let's use tanh to re-normalize, it provides a smooth distribution and maps to [0, 1].\n    # We need to scale the input to tanh appropriately.\n    # Let's scale `combined_scores_raw` such that a typical 'good' score is mapped to positive tanh input.\n    # Consider a typical tight fit with small waste (e.g., waste=0, tanh=1) plus bonus.\n    # Raw score around 2 + bonus.\n    # A scaling factor of ~1/(2 + capacity_bonus_weight) could work.\n    \n    # Let's normalize by the maximum achievable raw score to get values roughly in [0, 1].\n    # Max value of (1 + tanh) is 2. Max value of capacity_bonus_scores is capacity_bonus_weight.\n    # So theoretical max raw score is approx 2 + capacity_bonus_weight.\n    # A safer normalization is by the actual max observed raw score.\n    \n    max_combined_raw = np.max(combined_scores_raw) if len(combined_scores_raw) > 0 else 1\n    if max_combined_raw > epsilon:\n        final_fitting_scores = combined_scores_raw / max_combined_raw\n    else:\n        # If all scores are zero (e.g., no fitting bins, or all calculations resulted in zero),\n        # this means something went wrong or no bins are viable.\n        final_fitting_scores = np.zeros_like(combined_scores_raw)\n\n    # Add a small random noise for exploration, ensuring it doesn't drastically change rankings.\n    # Noise should be small relative to the score differences.\n    exploration_noise_magnitude = 0.02 # Small value\n    noise = np.random.uniform(-exploration_noise_magnitude, exploration_noise_magnitude, size=len(final_fitting_scores))\n    final_fitting_scores += noise\n\n    # Ensure scores stay within a reasonable range after adding noise, e.g., [0, 1].\n    final_fitting_scores = np.clip(final_fitting_scores, 0.0, 1.0)\n\n    # Assign these final scores to the appropriate bins\n    priorities[fitting_indices] = final_fitting_scores\n\n    # Bins that cannot fit the item will retain their initial priority of 0.0\n\n    return priorities",
    "response_id": 5,
    "obj": 4.028719585161557,
    "cyclomatic_complexity": 9.0,
    "halstead": 310.19550008653874,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response6.txt_stdout.txt",
    "code_path": "problem_iter6_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a\n    balanced Softmax-Fit heuristic with tunable parameters for tightness and flexibility.\n\n    This heuristic aims to:\n    1. Prioritize bins that result in the least remaining capacity after packing\n       the item (tightest fit). This is the primary driver for packing efficiency.\n    2. Provide a secondary \"flexibility bonus\" for bins that, after packing, still\n       retain significant remaining capacity. This aims to keep larger bins available\n       for potentially larger future items.\n\n    The scores are combined and then transformed using a softmax function to\n    produce probability-like priorities, encouraging exploration while favoring\n    better-fitting bins. Tunable parameters control the relative importance of\n    tightness versus flexibility.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority. Scores sum to 1 if at least one\n        bin can fit the item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        # No bin can fit the item, return all zeros\n        return priorities\n\n    # --- Heuristic Components ---\n\n    # 1. Tightness Score: Prioritize bins with minimal remaining capacity after packing.\n    #    We want to minimize (remaining_capacity - item).\n    #    For softmax (higher input = higher probability), we use a negative score\n    #    where smaller remaining capacity results in a larger (less negative) score.\n    #    The goal is to minimize `remaining_capacity_after_packing`.\n    #    A small positive offset can help differentiate bins that perfectly fit.\n    fitting_bins_caps = bins_remain_cap[can_fit_mask]\n    remaining_capacity_after_packing = fitting_bins_caps - item\n\n    # We want to maximize the \"tightness\", which is inversely related to\n    # `remaining_capacity_after_packing`. A common transformation is\n    # `max_possible_remaining - actual_remaining`.\n    # Let's use `1 / (1 + remaining_capacity_after_packing)` or similar.\n    # A simple approach is to use the negative of the remaining capacity,\n    # potentially scaled and shifted.\n    # `tightness = -remaining_capacity_after_packing`\n    # Adding a constant and scaling can help normalize the range before softmax.\n    # Let's scale `remaining_capacity_after_packing` so that tighter fits\n    # produce higher scores.\n    # Consider `max_rem_cap_after_packing - remaining_capacity_after_packing`.\n    # Let's use `1.0 / (1.0 + remaining_capacity_after_packing)` and scale it.\n    # A larger value here means a tighter fit.\n    # `tightness_score_base = 1.0 / (1.0 + remaining_capacity_after_packing)`\n    # Maximize tightness means minimize remaining_capacity_after_packing\n    # For softmax, we want a higher value for better bins.\n    # So, `tightness_score = -remaining_capacity_after_packing`\n\n    # Let's try a simpler linear transformation for tightness that maps\n    # lower remaining capacity to higher scores.\n    # We can scale `remaining_capacity_after_packing` such that the minimum is\n    # mapped to a higher value.\n    # Example: If remaining capacities are [0.1, 0.3, 0.05], we want scores like [0.9, 0.7, 1.0].\n    # A good transformation is `max_val - current_val`.\n    # `max_rem = np.max(remaining_capacity_after_packing)`\n    # `tightness_scores = max_rem - remaining_capacity_after_packing`\n    # However, `max_rem` can be 0 if all bins are perfectly filled.\n    # Let's use `1.0 / (1.0 + remaining_capacity_after_packing)` for robustness.\n    # This maps 0 to 1, 0.1 to ~0.909, 0.3 to ~0.769.\n    # To make it more pronounced for tighter fits, we can use `1.0 / (epsilon + remaining_capacity_after_packing)`\n    # or `exp(-k * remaining_capacity_after_packing)`.\n    # Let's stick to a simple inverse linear for now, potentially scaled.\n    # `tightness_scores = 1.0 - (remaining_capacity_after_packing / bin_capacity)`\n    # No, that's not quite right. We want to minimize remaining capacity *after* packing.\n    # The scores for softmax should be higher for more desirable bins.\n    # Desirable = smaller `remaining_capacity_after_packing`.\n    # So, `score = C - remaining_capacity_after_packing` where C is a constant.\n    # Let's use `tightness_score_base = -remaining_capacity_after_packing`\n    # and scale it to give it more weight.\n    tightness_weight = 10.0\n    tightness_scores = tightness_weight * (-remaining_capacity_after_packing)\n\n    # 2. Flexibility Bonus: Add a bonus for bins that still have substantial capacity.\n    #    This bonus should be positive and increase with remaining capacity.\n    #    We use the *original* `fitting_bins_caps` to represent the capacity\n    #    available *before* fitting the item, or rather, the remaining capacity\n    #    after packing the current item for the \"flexibility\" aspect.\n    #    It's `fitting_bins_caps` that we are interested in for future flexibility.\n    #    Let's consider the capacity *after* the item is packed.\n    #    `remaining_after_item = fitting_bins_caps - item`\n    #    We want to reward larger `remaining_after_item`.\n    #    A simple approach is to scale these remaining capacities.\n    #    We need to ensure this bonus doesn't overpower tightness.\n    flexibility_scale_factor = 0.5  # Tunable: how much flexibility matters relative to tightness\n    flexibility_bonus = flexibility_scale_factor * (fitting_bins_caps - item)\n\n    # Combine scores: Tightness is the primary factor, flexibility adds a bonus.\n    # The combined score directly influences the softmax probability.\n    combined_scores = tightness_scores + flexibility_bonus\n\n    # Apply softmax-like transformation for probabilities.\n    # To prevent numerical overflow/underflow with exp(), subtract the maximum value.\n    # Add a small epsilon to the scores to ensure that even if all `combined_scores`\n    # are identical and negative, they don't collapse to a single softmax output.\n    # Or, ensure the scores are not all identical and negative in a way that\n    # exp underflows. Adding a small positive value to `tightness_scores` might help.\n    # Let's add a small baseline positive value to ensure scores are not all negative.\n    # baseline_positive = 1.0\n    # combined_scores = baseline_positive + tightness_scores + flexibility_bonus\n\n    # Softmax calculation:\n    # Avoid numerical issues by shifting scores.\n    try:\n        max_score = np.max(combined_scores)\n        # Ensure we don't have all zeros or very small numbers that result in zero exp\n        # Add a small constant to ensure exp arguments are not too negative.\n        # Adding a small positive value to `tightness_scores` before combining might be better.\n        # Let's re-evaluate `tightness_scores`. It should be positive for tighter fits.\n        # Let's map `remaining_capacity_after_packing` to a positive score.\n        # E.g., `1 / (1 + rem_cap)`. Max value is 1 when rem_cap=0.\n        # `tightness_scores_positive = 1.0 / (1.0 + remaining_capacity_after_packing)`\n        # `tightness_scores_positive = np.clip(tightness_scores_positive, 1e-9, 1.0)` # Clip for stability\n        # Then `combined_scores = tightness_weight * tightness_scores_positive + flexibility_bonus`\n        # This seems more aligned with softmax inputs typically being positive.\n        # Let's try this approach.\n\n        # Recalculate tightness scores to be positive and higher for tighter fits.\n        # Map `remaining_capacity_after_packing` [0, max_val] to [1, ~0] or similar.\n        # Use `1.0 / (1.0 + remaining_capacity_after_packing)`.\n        # Max value is 1 (when remaining_capacity_after_packing is 0).\n        # Smallest value is `1.0 / (1.0 + max(remaining_capacity_after_packing))`.\n        # This naturally gives higher scores to tighter fits.\n        tightness_scores_positive = 1.0 / (1.0 + remaining_capacity_after_packing)\n\n        # Re-combine with flexibility bonus. The flexibility bonus is directly\n        # proportional to the remaining capacity after packing.\n        # `flexibility_bonus = flexibility_scale_factor * (fitting_bins_caps - item)`\n        # We want to add this bonus to the positive tightness score.\n        # The `flexibility_bonus` can be negative if `fitting_bins_caps - item` is negative,\n        # which shouldn't happen due to `can_fit_mask`.\n        # The bonus should be positive, so if `fitting_bins_caps - item` is small,\n        # the bonus is small.\n\n        # Let's adjust `flexibility_bonus` to be always positive and scaled from\n        # the remaining capacity.\n        # `flexibility_bonus_positive = flexibility_scale_factor * (fitting_bins_caps - item)`\n        # This still works as intended since `fitting_bins_caps - item` is the remaining capacity.\n\n        # The combined score for softmax:\n        # `tightness_weight` should be larger if we want tightness to dominate.\n        # `flexibility_scale_factor` influences how much larger residual capacities matter.\n        combined_scores = tightness_weight * tightness_scores_positive + flexibility_bonus\n\n        # Softmax calculation\n        max_combined_score = np.max(combined_scores)\n        exp_scores = np.exp(combined_scores - max_combined_score)\n        sum_exp_scores = np.sum(exp_scores)\n\n        if sum_exp_scores > 1e-9:  # Prevent division by near-zero\n            priorities[can_fit_mask] = exp_scores / sum_exp_scores\n        else:\n            # Fallback: if all scores are extremely low/equal, distribute probability evenly.\n            num_fitting_bins = np.sum(can_fit_mask)\n            if num_fitting_bins > 0:\n                priorities[can_fit_mask] = 1.0 / num_fitting_bins\n\n    except Exception as e:\n        # Fallback for any unexpected numerical issues\n        print(f\"Numerical stability issue in priority_v2: {e}. Falling back to equal distribution.\")\n        num_fitting_bins = np.sum(can_fit_mask)\n        if num_fitting_bins > 0:\n            priorities[can_fit_mask] = 1.0 / num_fitting_bins\n\n    return priorities",
    "response_id": 6,
    "obj": 148.53410450737937,
    "cyclomatic_complexity": 6.0,
    "halstead": 269.8789827584685,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response7.txt_stdout.txt",
    "code_path": "problem_iter6_code7.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This priority function implements a \"Worst Fit\" strategy with a tunable\n    parameter to balance between \"Worst Fit\" and \"Best Fit\". It prioritizes\n    bins that can accommodate the item with the most remaining capacity (Worst Fit),\n    but with a bias towards tighter fits controlled by `worst_fit_bias`.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority. Bins that cannot fit the item\n        will have a very low priority (negative infinity).\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that have enough remaining capacity for the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item:\n    # Calculate the remaining capacity after placing the item.\n    remaining_caps_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # --- Tunable Parameter ---\n    # `worst_fit_bias`: Controls the preference for the worst fit.\n    # A value of 0.0 makes it purely \"Worst Fit\" (prioritize largest remaining capacity).\n    # A positive value increases the preference for tighter fits (closer to Best Fit).\n    # A value of 1.0 would effectively negate the remaining capacity, making it\n    # closer to a Best Fit strategy based on the *resulting* space.\n    worst_fit_bias = 0.5  # Example: Moderate bias towards tighter fits\n\n    # The priority is based on the remaining capacity after fitting.\n    # For \"Worst Fit\", we want to maximize `remaining_caps_after_fit`.\n    # To incorporate a bias towards tighter fits, we can modify this.\n    # A simple way is to subtract a term that penalizes larger remaining capacities\n    # or rewards smaller remaining capacities.\n    # Let's try to combine the \"Worst Fit\" idea (maximize `remaining_caps_after_fit`)\n    # with the \"Best Fit\" idea (minimize `remaining_caps_after_fit`).\n    # We can use `remaining_caps_after_fit` as the base score and add a term\n    # that reduces the score as `remaining_caps_after_fit` increases, but less\n    # aggressively than pure Best Fit.\n\n    # Strategy: Maximize `remaining_caps_after_fit` but penalize it slightly.\n    # A score of `remaining_caps_after_fit - worst_fit_bias * remaining_caps_after_fit`\n    # simplifies to `remaining_caps_after_fit * (1 - worst_fit_bias)`.\n    # If `worst_fit_bias` is 0, it's pure Worst Fit.\n    # If `worst_fit_bias` is 1, it's `0`, which doesn't distinguish.\n\n    # Alternative: Maximize `remaining_caps_after_fit` plus a bonus for being *closer* to zero.\n    # This is like `remaining_caps_after_fit - slack_penalty` where `slack_penalty`\n    # is small for small `remaining_caps_after_fit`.\n    # A simple linear penalty: `remaining_caps_after_fit - worst_fit_bias * remaining_caps_after_fit`.\n\n    # Let's try a scoring function that is a mix:\n    # Prioritize bins with larger `bins_remain_cap[can_fit_mask]` (Worst Fit)\n    # BUT also prioritize bins that result in smaller `remaining_caps_after_fit` (Best Fit)\n    # We can achieve this by having a score that is a weighted sum.\n    # For example, prioritize bins with `bins_remain_cap[can_fit_mask]` that are large,\n    # but if two bins have similar large remaining capacity, pick the one that fits tighter.\n\n    # Let's try a simpler approach: we want to maximize the remaining capacity,\n    # but \"discount\" larger remaining capacities.\n    # Score = `bins_remain_cap[can_fit_mask]` - `penalty_for_large_remaining_cap`\n    # The penalty should increase with `remaining_caps_after_fit`.\n    # A linear penalty: `remaining_caps_after_fit * worst_fit_bias`.\n    # Score = `remaining_caps_after_fit - worst_fit_bias * remaining_caps_after_fit`\n    # Score = `remaining_caps_after_fit * (1 - worst_fit_bias)`\n\n    # This formulation means if `worst_fit_bias = 0.5`, we prioritize bins with\n    # the largest `remaining_caps_after_fit`, but the score is halved. This doesn't\n    # seem to blend strategies effectively.\n\n    # Let's consider the \"slack\" or `remaining_caps_after_fit`.\n    # For pure Best Fit, we minimize slack. Priority = -slack.\n    # For pure Worst Fit, we maximize slack. Priority = slack.\n\n    # To blend: Maximize `slack * (1 - bias) + (-slack) * bias`\n    # = `slack - slack*bias - slack*bias`\n    # = `slack * (1 - 2*bias)` -> this is not right.\n\n    # Let's think about what we want to maximize:\n    # We want to maximize the remaining capacity (Worst Fit), BUT\n    # we prefer smaller remaining capacity when other factors are equal, or when\n    # the difference in remaining capacity is not too large.\n\n    # Consider the \"gap\" for Best Fit: `bins_remain_cap[can_fit_mask] - item`.\n    # For Worst Fit, we want to maximize `bins_remain_cap[can_fit_mask]`.\n    # This is equivalent to maximizing `(bins_remain_cap[can_fit_mask] - item) + item`.\n    # So, Worst Fit prioritizes maximizing `remaining_caps_after_fit`.\n\n    # Let's create a score that combines these two:\n    # Score = `alpha * remaining_caps_after_fit` + `beta * (-remaining_caps_after_fit)`\n    # We want to favor larger `remaining_caps_after_fit` (Worst Fit), so `alpha` should be positive.\n    # We want to penalize larger `remaining_caps_after_fit` when `bias` is applied,\n    # meaning we prefer smaller `remaining_caps_after_fit` to some degree.\n\n    # A common way to blend \"maximize X\" and \"minimize X\" is to consider them as\n    # two different objectives and find a compromise.\n    # If we want to maximize `R` (remaining capacity) but also prefer smaller `R`,\n    # we can maximize `R * (1-w) + (max_R - R) * w` where `w` is a weight for preferring smaller `R`.\n    # This becomes `R - R*w + max_R*w - R*w` = `R*(1-2w) + max_R*w`.\n    # This is complex as it depends on `max_R`.\n\n    # Simpler approach: combine `bins_remain_cap[can_fit_mask]` (for Worst Fit)\n    # and `-remaining_caps_after_fit` (for Best Fit).\n    # Let `wf_score = bins_remain_cap[can_fit_mask]`\n    # Let `bf_score = -remaining_caps_after_fit`\n    # Combined Score = `(1-bias) * wf_score + bias * bf_score`\n    # Combined Score = `(1-bias) * bins_remain_cap[can_fit_mask]` + `bias * -(bins_remain_cap[can_fit_mask] - item)`\n    # Combined Score = `bins_remain_cap[can_fit_mask] * (1 - bias)` + `bias * (item - bins_remain_cap[can_fit_mask])`\n    # Combined Score = `bins_remain_cap[can_fit_mask] * (1 - bias - bias)` + `bias * item`\n    # Combined Score = `bins_remain_cap[can_fit_mask] * (1 - 2*bias)` + `bias * item`\n\n    # Let's test this:\n    # If `bias = 0`: Score = `bins_remain_cap[can_fit_mask]`. Pure Worst Fit. Maximizes remaining capacity. Correct.\n    # If `bias = 0.5`: Score = `bins_remain_cap[can_fit_mask] * 0` + `0.5 * item` = `0.5 * item`.\n    # This means all bins that can fit get the same priority (a constant). This is not good.\n\n    # The issue is that `bins_remain_cap[can_fit_mask]` and `remaining_caps_after_fit` are directly related.\n    # `bins_remain_cap[can_fit_mask] = remaining_caps_after_fit + item`.\n\n    # Let's try a different blend: prioritize large `bins_remain_cap[can_fit_mask]`,\n    # but if two bins have very similar large `bins_remain_cap`, pick the one with\n    # smaller `remaining_caps_after_fit`.\n\n    # A simpler formulation: Use `bins_remain_cap[can_fit_mask]` as the primary sorting key (Worst Fit).\n    # For tie-breaking, use `-remaining_caps_after_fit` (Best Fit).\n    # This can be done by creating a composite score.\n    # Prioritize maximizing `bins_remain_cap[can_fit_mask]`.\n    # To incorporate bias, we can slightly penalize larger `bins_remain_cap[can_fit_mask]`.\n    # Score = `bins_remain_cap[can_fit_mask] - penalty * remaining_caps_after_fit`\n    # Penalty should be small for Worst Fit bias.\n    # Let `penalty = worst_fit_bias`. We want to maximize this.\n    # Score = `bins_remain_cap[can_fit_mask] - worst_fit_bias * remaining_caps_after_fit`\n    # Score = `(remaining_caps_after_fit + item) - worst_fit_bias * remaining_caps_after_fit`\n    # Score = `remaining_caps_after_fit * (1 - worst_fit_bias) + item`\n\n    # Let's test this:\n    # If `worst_fit_bias = 0`: Score = `remaining_caps_after_fit + item`. Maximizes `bins_remain_cap[can_fit_mask]`. Correct.\n    # If `worst_fit_bias = 1`: Score = `remaining_caps_after_fit * 0 + item` = `item`. All bins have same priority. Not good.\n\n    # The issue is that `remaining_caps_after_fit` is what we're trying to optimize.\n    # We want to maximize it (Worst Fit) but also penalize large values.\n\n    # Consider the function `f(x) = x * (1 - bias)`.\n    # If bias = 0, f(x) = x (maximizes x)\n    # If bias = 1, f(x) = 0 (no distinction)\n\n    # Let's go back to blending Best Fit and Worst Fit criteria.\n    # Best Fit prioritizes minimizing `bins_remain_cap[can_fit_mask] - item`.\n    # Worst Fit prioritizes maximizing `bins_remain_cap[can_fit_mask]`.\n\n    # Let's use the example: item = 0.3, bins_remain_cap = [0.35, 0.4, 0.5, 0.2]\n    # can_fit_mask = [True, True, True, False]\n    # For fitting bins:\n    # bins_remain_cap[can_fit_mask] = [0.35, 0.4, 0.5]\n    # remaining_caps_after_fit = [0.05, 0.1, 0.2]\n\n    # Pure Best Fit priority: [-0.05, -0.1, -0.2] (Bin 0.35 is best)\n    # Pure Worst Fit priority: [0.35, 0.4, 0.5] (Bin 0.5 is best)\n\n    # We want to blend these. Let `bias` be the weight for Best Fit.\n    # Priority = `(1-bias) * (WorstFitScore) + bias * (BestFitScore)`\n    # Priority = `(1-bias) * bins_remain_cap[can_fit_mask] + bias * (-remaining_caps_after_fit)`\n    # Priority = `(1-bias) * (remaining_caps_after_fit + item) + bias * (-remaining_caps_after_fit)`\n    # Priority = `(1-bias) * remaining_caps_after_fit + (1-bias) * item - bias * remaining_caps_after_fit`\n    # Priority = `remaining_caps_after_fit * (1 - bias - bias) + (1-bias) * item`\n    # Priority = `remaining_caps_after_fit * (1 - 2*bias) + (1-bias) * item`\n\n    # Test again:\n    # If `bias = 0` (pure Worst Fit): Priority = `remaining_caps_after_fit * 1 + item`. Maximizes `remaining_caps_after_fit + item`, which is `bins_remain_cap[can_fit_mask]`. Correct.\n    # If `bias = 0.5` (equal blend): Priority = `remaining_caps_after_fit * 0 + (1-0.5) * item` = `0.5 * item`. Constant. Still not good.\n\n    # The problem is that the raw values of `bins_remain_cap` and `remaining_caps_after_fit` are highly correlated.\n\n    # Let's re-frame the \"Worst Fit\" criteria. It's about maximizing the *unused* capacity.\n    # We want to maximize `bins_remain_cap[can_fit_mask]`.\n\n    # For blending, we can use a form that prioritizes the primary goal (Worst Fit)\n    # and uses the secondary goal (Best Fit) to break ties or modify slightly.\n\n    # Let's try prioritizing `bins_remain_cap[can_fit_mask]` directly, but scale it\n    # down by a factor that depends on the remaining capacity.\n    # Score = `bins_remain_cap[can_fit_mask] / (1 + worst_fit_bias * remaining_caps_after_fit)`\n\n    # Test this:\n    # If `worst_fit_bias = 0`: Score = `bins_remain_cap[can_fit_mask]`. Pure Worst Fit. Correct.\n    # If `worst_fit_bias` is large: The denominator increases for bins with large remaining capacity,\n    # thus reducing their score. This makes bins with small remaining capacity (tighter fits) more\n    # likely to be chosen. This is the desired behavior.\n\n    # Example: item = 0.3, bins_remain_cap = [0.35, 0.4, 0.5, 0.2]\n    # `bins_remain_cap[can_fit_mask]` = [0.35, 0.4, 0.5]\n    # `remaining_caps_after_fit` = [0.05, 0.1, 0.2]\n    # `worst_fit_bias` = 0.5\n\n    # Bin 0.35: `bins_remain_cap`=0.35, `remaining_caps_after_fit`=0.05\n    # Score = 0.35 / (1 + 0.5 * 0.05) = 0.35 / (1 + 0.025) = 0.35 / 1.025 \u2248 0.34146\n\n    # Bin 0.4: `bins_remain_cap`=0.4, `remaining_caps_after_fit`=0.1\n    # Score = 0.4 / (1 + 0.5 * 0.1) = 0.4 / (1 + 0.05) = 0.4 / 1.05 \u2248 0.38095\n\n    # Bin 0.5: `bins_remain_cap`=0.5, `remaining_caps_after_fit`=0.2\n    # Score = 0.5 / (1 + 0.5 * 0.2) = 0.5 / (1 + 0.1) = 0.5 / 1.1 \u2248 0.45454\n\n    # With `worst_fit_bias = 0.5`, the ranking is [0.35, 0.4, 0.5] -> [0.34146, 0.38095, 0.45454].\n    # The highest priority is still for the bin with the largest remaining capacity (0.5).\n    # This seems to be leaning towards Worst Fit, which is what `worst_fit_bias` implies.\n\n    # If we want `worst_fit_bias` to control the *degree* to which we deviate from Worst Fit towards Best Fit,\n    # a higher `worst_fit_bias` should mean more preference for tighter fits.\n\n    # Let's try a score that is a sum:\n    # Score = `bins_remain_cap[can_fit_mask]` (Worst Fit component)\n    #       - `worst_fit_bias * remaining_caps_after_fit` (Penalty for slack, i.e. favouring Best Fit)\n\n    # Score = `(remaining_caps_after_fit + item) - worst_fit_bias * remaining_caps_after_fit`\n    # Score = `remaining_caps_after_fit * (1 - worst_fit_bias) + item`\n\n    # Test again with `worst_fit_bias = 0.5`:\n    # Bin 0.35: `remaining_caps_after_fit`=0.05, `item`=0.3\n    # Score = 0.05 * (1 - 0.5) + 0.3 = 0.05 * 0.5 + 0.3 = 0.025 + 0.3 = 0.325\n\n    # Bin 0.4: `remaining_caps_after_fit`=0.1, `item`=0.3\n    # Score = 0.1 * (1 - 0.5) + 0.3 = 0.1 * 0.5 + 0.3 = 0.05 + 0.3 = 0.35\n\n    # Bin 0.5: `remaining_caps_after_fit`=0.2, `item`=0.3\n    # Score = 0.2 * (1 - 0.5) + 0.3 = 0.2 * 0.5 + 0.3 = 0.1 + 0.3 = 0.4\n\n    # Ranking: [0.325, 0.35, 0.4]. Still prioritizing the largest remaining capacity (0.5).\n    # This formula seems to be a scaled Worst Fit.\n\n    # Let's define `worst_fit_bias` as the weight for the Best Fit component.\n    # Priority = `(1 - bias) * (WorstFitMetric) + bias * (BestFitMetric)`\n    # WorstFitMetric = `bins_remain_cap[can_fit_mask]`\n    # BestFitMetric = `-remaining_caps_after_fit`\n\n    # Priority = `(1 - bias) * bins_remain_cap[can_fit_mask] + bias * (-remaining_caps_after_fit)`\n    # Priority = `(1 - bias) * (remaining_caps_after_fit + item) - bias * remaining_caps_after_fit`\n    # Priority = `(1 - bias) * remaining_caps_after_fit + (1 - bias) * item - bias * remaining_caps_after_fit`\n    # Priority = `remaining_caps_after_fit * (1 - bias - bias) + (1 - bias) * item`\n    # Priority = `remaining_caps_after_fit * (1 - 2*bias) + (1 - bias) * item`\n\n    # The issue persists with this linear combination if `1 - 2*bias` is positive.\n\n    # Let's think about what happens when `bins_remain_cap` are similar.\n    # If `bins_remain_cap = [0.5, 0.51]`, item = 0.3.\n    # remaining_caps_after_fit = [0.2, 0.21]\n    # Pure WF: [0.5, 0.51] -> 0.51 is better\n    # Pure BF: [-0.2, -0.21] -> -0.2 is better\n\n    # If we use `bins_remain_cap[can_fit_mask] - penalty * remaining_caps_after_fit`\n    # penalty = `worst_fit_bias`\n    # Score = `bins_remain_cap[can_fit_mask] - worst_fit_bias * (bins_remain_cap[can_fit_mask] - item)`\n    # Score = `bins_remain_cap[can_fit_mask] * (1 - worst_fit_bias) + item * worst_fit_bias`\n\n    # Test again with `worst_fit_bias = 0.5`:\n    # Bin 0.35: Score = 0.35 * (1 - 0.5) + 0.3 * 0.5 = 0.35 * 0.5 + 0.15 = 0.175 + 0.15 = 0.325\n    # Bin 0.4: Score = 0.4 * (1 - 0.5) + 0.3 * 0.5 = 0.4 * 0.5 + 0.15 = 0.2 + 0.15 = 0.35\n    # Bin 0.5: Score = 0.5 * (1 - 0.5) + 0.3 * 0.5 = 0.5 * 0.5 + 0.15 = 0.25 + 0.15 = 0.4\n\n    # This is the same result as before. The `item` term is constant for all bins.\n    # The ranking is still determined by `bins_remain_cap[can_fit_mask] * (1 - worst_fit_bias)`.\n    # If `worst_fit_bias < 1`, this is maximized by maximizing `bins_remain_cap[can_fit_mask]`.\n\n    # To truly blend, we need a term that prioritizes smaller `remaining_caps_after_fit` when `bins_remain_cap` is similar.\n\n    # Let's try: maximize `bins_remain_cap[can_fit_mask]` but penalize it based on its \"slackness\".\n    # Score = `bins_remain_cap[can_fit_mask]` (base WF score)\n    # A bin is \"bad\" if its remaining capacity is much larger than the item.\n    # A bin is \"good\" if its remaining capacity is just enough for the item.\n\n    # Consider the quantity we want to maximize for WF: `bins_remain_cap[can_fit_mask]`\n    # Consider the quantity we want to minimize for BF: `remaining_caps_after_fit`\n\n    # Let's construct a score that uses `bins_remain_cap[can_fit_mask]` but shifts it\n    # based on `remaining_caps_after_fit`.\n    # Score = `bins_remain_cap[can_fit_mask] - worst_fit_bias * remaining_caps_after_fit` is still the most direct blend.\n\n    # If the problem is that `worst_fit_bias` is not directly controlling the *trade-off*,\n    # maybe we should use a different formulation.\n\n    # A different interpretation:\n    # \"Worst Fit\" means selecting the bin that leaves the largest capacity.\n    # If we want to temper this by preferring tighter fits, we can subtract a portion of the \"tightness\".\n    # The \"tightness\" is `bins_remain_cap[can_fit_mask] - item`.\n    # Score = `bins_remain_cap[can_fit_mask]` - `bias * (bins_remain_cap[can_fit_mask] - item)`\n    # Score = `bins_remain_cap[can_fit_mask] * (1 - bias)` + `item * bias`\n\n    # This leads back to the same formula. The issue might be in how `bias` is interpreted.\n\n    # Let's try to maximize `bins_remain_cap[can_fit_mask]` but with a discount applied to larger values.\n    # Score = `bins_remain_cap[can_fit_mask] / (1 + worst_fit_bias * bins_remain_cap[can_fit_mask])`\n    # This would also penalize larger bins.\n\n    # Let's consider the function `f(x) = x - bias * x`.\n    # If `bias` is small, we maximize `x`.\n    # If `bias` is large, we minimize `x`.\n\n    # Let's try a score that combines the *absolute remaining capacity* with the *slack*.\n    # We want to maximize `bins_remain_cap[can_fit_mask]` (Worst Fit).\n    # We also want to minimize `remaining_caps_after_fit` (Best Fit).\n\n    # Consider the value `bins_remain_cap[can_fit_mask]`.\n    # If `worst_fit_bias = 0.0`, we want to maximize this.\n    # If `worst_fit_bias = 1.0`, we want to minimize `remaining_caps_after_fit`.\n    # This implies a transition.\n\n    # Let's try to maximize `bins_remain_cap[can_fit_mask]` but subtract a term that\n    # is related to how \"over-filled\" the bin would become if we were aiming for a perfect fit.\n    # This is getting complicated.\n\n    # Simpler approach:\n    # Prioritize bins with larger `bins_remain_cap`.\n    # If multiple bins have the same (or very similar) `bins_remain_cap`, then prefer the one with less slack.\n    # This suggests a compound score: `(bins_remain_cap[can_fit_mask], -remaining_caps_after_fit)`.\n    # To combine into a single number: `bins_remain_cap[can_fit_mask] + epsilon * (-remaining_caps_after_fit)`\n    # where `epsilon` is a small value.\n\n    # Let `epsilon = worst_fit_bias`.\n    # Score = `bins_remain_cap[can_fit_mask] - worst_fit_bias * remaining_caps_after_fit`\n    # This is the same formula we've been testing.\n\n    # Let's reconsider the interpretation of `worst_fit_bias`.\n    # If `worst_fit_bias` is the *strength* of the \"Worst Fit\" preference.\n    # And we want to temper it with \"Best Fit\".\n\n    # Let's try to maximize `bins_remain_cap[can_fit_mask]` directly, and use `worst_fit_bias`\n    # as a divisor that reduces the score for bins that leave a lot of slack.\n    # Score = `bins_remain_cap[can_fit_mask] / (1 + worst_fit_bias * remaining_caps_after_fit)`\n    # This still leads to Worst Fit preference being dominant if `worst_fit_bias` is small.\n\n    # What if `worst_fit_bias` controls how much we penalize slack?\n    # Score = `bins_remain_cap[can_fit_mask] - worst_fit_bias * remaining_caps_after_fit`\n    # This is `bins_remain_cap[can_fit_mask] * (1 - worst_fit_bias) + item * worst_fit_bias`.\n\n    # The issue might be that `item` is added, creating a constant shift.\n    # Let's remove the `item` term and focus on the relative ordering:\n    # Score_relative = `bins_remain_cap[can_fit_mask] * (1 - worst_fit_bias)`\n\n    # If `worst_fit_bias = 0`: maximize `bins_remain_cap`. WF.\n    # If `worst_fit_bias = 1`: maximize `0`. All equal. Bad.\n\n    # Let's try to maximize a quantity that is *inversely* related to slack, but scaled by WF.\n    # Maximize `bins_remain_cap[can_fit_mask]`\n    # and also maximize `1 / (1 + remaining_caps_after_fit)` (which favors smaller slack).\n\n    # Score = `bins_remain_cap[can_fit_mask] * (1 - bias) + (1 / (1 + remaining_caps_after_fit)) * bias`\n\n    # Test with `worst_fit_bias = 0.5`:\n    # Bin 0.35: `bins_remain_cap`=0.35, `rem_cap`=0.05\n    # Score = 0.35 * 0.5 + (1 / (1 + 0.05)) * 0.5 = 0.175 + (1 / 1.05) * 0.5 = 0.175 + 0.95238 * 0.5 = 0.175 + 0.47619 = 0.65119\n\n    # Bin 0.4: `bins_remain_cap`=0.4, `rem_cap`=0.1\n    # Score = 0.4 * 0.5 + (1 / (1 + 0.1)) * 0.5 = 0.2 + (1 / 1.1) * 0.5 = 0.2 + 0.90909 * 0.5 = 0.2 + 0.45454 = 0.65454\n\n    # Bin 0.5: `bins_remain_cap`=0.5, `rem_cap`=0.2\n    # Score = 0.5 * 0.5 + (1 / (1 + 0.2)) * 0.5 = 0.25 + (1 / 1.2) * 0.5 = 0.25 + 0.83333 * 0.5 = 0.25 + 0.41666 = 0.66666\n\n    # Ranking: [0.65119, 0.65454, 0.66666]. This still prioritizes the largest remaining capacity (0.5).\n\n    # The interpretation of `worst_fit_bias` is crucial. Let's assume a higher value means\n    # we are \"more Worst Fit\".\n\n    # Final attempt at a sensible blending formula:\n    # We want to maximize `bins_remain_cap[can_fit_mask]`.\n    # Let `ideal_gap` be the target slack for Best Fit (e.g., 0).\n    # The \"goodness\" of a bin for Best Fit is related to how close `remaining_caps_after_fit` is to 0.\n    # We can use the sigmoid from `priority_v0` as a measure of \"Best Fit desirability\".\n    # `bf_desirability = 1 / (1 + exp(-steepness * (ideal_gap - remaining_caps_after_fit)))`\n\n    # Then blend Worst Fit metric with this Best Fit desirability.\n    # Score = `bins_remain_cap[can_fit_mask]` (WF metric)\n    #        + `blend_factor * bf_desirability`\n\n    # Let `worst_fit_bias` control the `blend_factor`.\n    # Higher `worst_fit_bias` means more contribution from BF desirability.\n    # Score = `bins_remain_cap[can_fit_mask]` + `worst_fit_bias * bf_desirability`\n\n    # Let's use `steepness = 10` and `ideal_gap = 0`.\n    # bf_desirability = 1 / (1 + exp(10 * remaining_caps_after_fit))\n\n    # Test with `worst_fit_bias = 0.5`:\n    # Bin 0.35: `bins_remain_cap`=0.35, `rem_cap`=0.05\n    # bf_desirability = 1 / (1 + exp(10 * 0.05)) = 1 / (1 + exp(0.5)) = 1 / (1 + 1.6487) = 1 / 2.6487 \u2248 0.37754\n    # Score = 0.35 + 0.5 * 0.37754 = 0.35 + 0.18877 = 0.53877\n\n    # Bin 0.4: `bins_remain_cap`=0.4, `rem_cap`=0.1\n    # bf_desirability = 1 / (1 + exp(10 * 0.1)) = 1 / (1 + exp(1)) = 1 / (1 + 2.71828) = 1 / 3.71828 \u2248 0.26894\n    # Score = 0.4 + 0.5 * 0.26894 = 0.4 + 0.13447 = 0.53447\n\n    # Bin 0.5: `bins_remain_cap`=0.5, `rem_cap`=0.2\n    # bf_desirability = 1 / (1 + exp(10 * 0.2)) = 1 / (1 + exp(2)) = 1 / (1 + 7.38905) = 1 / 8.38905 \u2248 0.11919\n    # Score = 0.5 + 0.5 * 0.11919 = 0.5 + 0.05959 = 0.55959\n\n    # Ranking: [0.53877, 0.53447, 0.55959].\n    # Here, bin 0.5 still has the highest score. The behavior is that WF is primary,\n    # and BF adds a bonus.\n\n    # If we want `worst_fit_bias` to control how much we *shift* from WF to BF.\n    # Let's make `worst_fit_bias` control the weight of the BF term.\n    # Score = `(1 - worst_fit_bias) * bins_remain_cap[can_fit_mask] + worst_fit_bias * (some BF metric)`\n\n    # Let's retry: `bins_remain_cap[can_fit_mask] * (1 - worst_fit_bias) + item * worst_fit_bias`\n    # This form means if `worst_fit_bias` is high, `item` becomes a dominant factor.\n\n    # What if we directly blend the criteria?\n    # We want to maximize `bins_remain_cap[can_fit_mask]`.\n    # And we want to minimize `remaining_caps_after_fit`.\n\n    # Consider the function: `f(rem_cap, bias) = rem_cap * (1-bias) - rem_cap * bias` -- doesn't make sense.\n\n    # Let's use `bins_remain_cap` as the primary sorting key (WF) and `remaining_caps_after_fit` (BF) as secondary.\n    # A common way to achieve this is:\n    # Score = `bins_remain_cap[can_fit_mask] * M - remaining_caps_after_fit`\n    # where `M` is a large multiplier to ensure `bins_remain_cap` dominates.\n    # However, we want to tune the *blend*.\n\n    # The reflection says \"balance between 'Worst Fit' and 'Best Fit'\".\n    # This implies a continuous transition.\n\n    # Let's go with the formulation:\n    # Score = `bins_remain_cap[can_fit_mask] * (1 - worst_fit_bias)`\n    # This makes `worst_fit_bias` control how much the WF score is \"dampened\".\n    # If `worst_fit_bias = 0`, we maximize `bins_remain_cap`.\n    # If `worst_fit_bias = 1`, the score is 0.\n\n    # What if we want `worst_fit_bias = 1.0` to mean pure Best Fit?\n    # Score = `(1 - worst_fit_bias) * bins_remain_cap[can_fit_mask] + worst_fit_bias * (-remaining_caps_after_fit)`\n    # If `worst_fit_bias = 1.0`, Score = `-remaining_caps_after_fit`. Pure BF.\n    # If `worst_fit_bias = 0.0`, Score = `bins_remain_cap[can_fit_mask]`. Pure WF.\n\n    # Let's re-test this formulation with `worst_fit_bias` as the weight for BF.\n    # Score = `(1 - worst_fit_bias) * bins_remain_cap[can_fit_mask] + worst_fit_bias * (-remaining_caps_after_fit)`\n\n    # Example: item = 0.3, bins_remain_cap = [0.35, 0.4, 0.5, 0.2]\n    # `bins_remain_cap[can_fit_mask]` = [0.35, 0.4, 0.5]\n    # `remaining_caps_after_fit` = [0.05, 0.1, 0.2]\n\n    # Let `worst_fit_bias = 0.5` (50% BF, 50% WF)\n    # Bin 0.35: `bins_remain_cap`=0.35, `rem_cap`=0.05\n    # Score = (1 - 0.5) * 0.35 + 0.5 * (-0.05) = 0.5 * 0.35 - 0.5 * 0.05 = 0.175 - 0.025 = 0.15\n\n    # Bin 0.4: `bins_remain_cap`=0.4, `rem_cap`=0.1\n    # Score = (1 - 0.5) * 0.4 + 0.5 * (-0.1) = 0.5 * 0.4 - 0.5 * 0.1 = 0.2 - 0.05 = 0.15\n\n    # Bin 0.5: `bins_remain_cap`=0.5, `rem_cap`=0.2\n    # Score = (1 - 0.5) * 0.5 + 0.5 * (-0.2) = 0.5 * 0.5 - 0.5 * 0.2 = 0.25 - 0.10 = 0.15\n\n    # This formulation results in constant scores when the bias is 0.5. This is because\n    # `0.5 * A + 0.5 * (-B)` where `A = B + C` leads to `0.5 * (B+C) - 0.5 * B = 0.5*C`.\n    # Here `A = bins_remain_cap`, `B = remaining_caps_after_fit`, `C = item`.\n    # `bins_remain_cap = remaining_caps_after_fit + item`.\n    # So, `0.5 * (rem_cap + item) + 0.5 * (-rem_cap)` = `0.5 * item`. Constant.\n\n    # The reflection implies that we should be able to \"tune\" the preference.\n    # The current issue is that WF and BF are opposing objectives that are linearly dependent.\n\n    # Let's try to maximize `bins_remain_cap[can_fit_mask]` and use `worst_fit_bias` to *reduce* the score\n    # for bins with large remaining capacity.\n    # Score = `bins_remain_cap[can_fit_mask] - worst_fit_bias * bins_remain_cap[can_fit_mask]`\n    # Score = `bins_remain_cap[can_fit_mask] * (1 - worst_fit_bias)`\n\n    # Test this:\n    # If `worst_fit_bias = 0`: Score = `bins_remain_cap`. Pure WF.\n    # If `worst_fit_bias = 1`: Score = `0`. Equal.\n    # If `worst_fit_bias = 0.5`: Score = `0.5 * bins_remain_cap`. Still prioritizes largest.\n\n    # The key is to make the *relative* priority between bins change.\n\n    # Consider the metric: `bins_remain_cap[can_fit_mask]`.\n    # For WF, we want to maximize this.\n    # For BF, we want to minimize `bins_remain_cap[can_fit_mask] - item`.\n\n    # Let's use a score that is `bins_remain_cap[can_fit_mask]`, but we \"shift\" it\n    # based on the bias towards BF.\n    # If `worst_fit_bias = 0`, score is `bins_remain_cap`.\n    # If `worst_fit_bias = 1`, score is `-remaining_caps_after_fit`.\n\n    # Consider a score `S = A * x + B * y` where we want to vary `A` and `B`.\n    # Let `x = bins_remain_cap[can_fit_mask]`\n    # Let `y = -remaining_caps_after_fit`\n\n    # If `worst_fit_bias` is the weight for the BF component:\n    # Score = `(1 - worst_fit_bias) * x + worst_fit_bias * y`\n    # Score = `(1 - worst_fit_bias) * bins_remain_cap[can_fit_mask] + worst_fit_bias * (-remaining_caps_after_fit)`\n\n    # As shown earlier, this leads to a constant score if `bias = 0.5`.\n    # The issue is the linear dependency of x and y.\n\n    # Let's try this:\n    # We want to prioritize bins with large `bins_remain_cap`.\n    # But, we want to penalize bins that leave a *large* gap when the `bins_remain_cap` are similar.\n\n    # A different approach:\n    # Prioritize bins by `bins_remain_cap`.\n    # Then, for bins that are \"close\" in `bins_remain_cap`, use the slack as a tie-breaker.\n    # \"Close\" can be defined by `worst_fit_bias`.\n\n    # Score = `bins_remain_cap[can_fit_mask] + worst_fit_bias * (-remaining_caps_after_fit)`\n    # This is the formula that gave WF dominance.\n\n    # Let's try:\n    # Score = `bins_remain_cap[can_fit_mask]`  # Primary WF objective\n    # We want to reduce this score for bins that leave too much slack.\n    # Reduction = `worst_fit_bias * remaining_caps_after_fit`\n    # Score = `bins_remain_cap[can_fit_mask] - worst_fit_bias * remaining_caps_after_fit`\n    # This is the same one that yielded `bins_remain_cap * (1 - worst_fit_bias) + item * worst_fit_bias`.\n\n    # The formulation that blends WF and BF should allow a tunable parameter to move\n    # from pure WF to pure BF.\n\n    # Consider the \"slack\" `s = bins_remain_cap - item`.\n    # WF prioritizes maximizing `s + item`.\n    # BF prioritizes minimizing `s`.\n\n    # Let `w` be `worst_fit_bias`.\n    # We want to transition from maximizing `s + item` (w=0) to minimizing `s` (w=1).\n\n    # Score = `(1-w) * (s + item) + w * (-s)`\n    # This yielded the constant score issue.\n\n    # Alternative blend:\n    # Score = `(1-w) * (s + item) + w * (something else related to BF)`\n\n    # What if we consider the *ratio*?\n    # Maximize `bins_remain_cap / (bins_remain_cap - item)`? No.\n\n    # Let's reconsider the original `priority_v0` with its sigmoid.\n    # It controlled the \"ideal gap\". This is a different dimension.\n\n    # The reflection says: \"balance between 'Worst Fit' and 'Best Fit'\".\n    # This suggests a linear interpolation between the \"ideal\" WF score and the \"ideal\" BF score.\n\n    # WF Score: `bins_remain_cap[can_fit_mask]`\n    # BF Score: `-remaining_caps_after_fit`\n\n    # Blend Score = `(1 - bias) * WF_Score + bias * BF_Score`\n    # where `bias` is the weight for BF. Let's use `best_fit_weight` for clarity.\n    # Score = `(1 - best_fit_weight) * bins_remain_cap[can_fit_mask] + best_fit_weight * (-remaining_caps_after_fit)`\n\n    # This formulation, as proven, fails to provide a useful blend when the input variables are linearly dependent.\n    # `bins_remain_cap[can_fit_mask] = remaining_caps_after_fit + item`.\n    # Let `R = remaining_caps_after_fit`.\n    # Score = `(1 - w) * (R + item) + w * (-R)`\n    # Score = `(1 - w) * R + (1 - w) * item - w * R`\n    # Score = `R * (1 - w - w) + (1 - w) * item`\n    # Score = `R * (1 - 2w) + (1 - w) * item`\n\n    # This score is linear in `R`.\n    # If `1 - 2w > 0` (i.e., `w < 0.5`), the score increases with `R`. This means it still favors WF.\n    # If `1 - 2w < 0` (i.e., `w > 0.5`), the score decreases with `R`. This means it favors BF.\n    # If `1 - 2w = 0` (i.e., `w = 0.5`), the score is constant (`0.5 * item`).\n\n    # This formula *does* provide a tunable transition:\n    # - If `w < 0.5`: It's effectively a weighted WF. Higher `w` in this range means less WF preference.\n    # - If `w > 0.5`: It's effectively a weighted BF (reversed). Higher `w` means more BF preference.\n    # - At `w = 0.5`: All bins are equal.\n\n    # This is a valid blending strategy, though the `w=0.5` point is degenerate.\n    # Let's use `worst_fit_bias` to control the weight for WF, and `best_fit_bias` for BF.\n    # Or, let `worst_fit_bias` be the weight for WF, and we derive the weight for BF.\n\n    # Let's assume `worst_fit_bias` directly controls the blend:\n    # `worst_fit_bias = 0.0` -> Pure WF\n    # `worst_fit_bias = 1.0` -> Pure BF\n\n    # Formula: `Score = (1 - w) * WF_Score + w * BF_Score`\n    # where `w` is `worst_fit_bias`.\n    # WF_Score = `bins_remain_cap[can_fit_mask]`\n    # BF_Score = `-remaining_caps_after_fit`\n\n    # Score = `(1 - worst_fit_bias) * bins_remain_cap[can_fit_mask] + worst_fit_bias * (-remaining_caps_after_fit)`\n    # This is the formula `R * (1 - 2w) + item * (1 - w)`.\n\n    # Let's refine the parameter meaning.\n    # Let `blend_weight` be the parameter, from 0 to 1.\n    # `blend_weight = 0`: Pure WF. Priority = `bins_remain_cap`.\n    # `blend_weight = 1`: Pure BF. Priority = `-remaining_caps_after_fit`.\n\n    # Score = `(1 - blend_weight) * bins_remain_cap[can_fit_mask] + blend_weight * (-remaining_caps_after_fit)`\n\n    # Example: item = 0.3, bins_remain_cap = [0.35, 0.4, 0.5]\n    # `bins_remain_cap` = [0.35, 0.4, 0.5]\n    # `rem_cap_after` = [0.05, 0.1, 0.2]\n\n    # Case `blend_weight = 0` (Pure WF):\n    # Scores = [0.35, 0.4, 0.5]. Max is 0.5.\n\n    # Case `blend_weight = 1` (Pure BF):\n    # Scores = [-0.05, -0.1, -0.2]. Max is -0.05.\n\n    # Case `blend_weight = 0.2` (20% BF):\n    # Score = 0.8 * bins_remain_cap + 0.2 * (-rem_cap_after)\n    # Bin 0.35: 0.8 * 0.35 + 0.2 * (-0.05) = 0.28 - 0.01 = 0.27\n    # Bin 0.4:  0.8 * 0.4  + 0.2 * (-0.1)  = 0.32 - 0.02 = 0.30\n    # Bin 0.5:  0.8 * 0.5  + 0.2 * (-0.2)  = 0.40 - 0.04 = 0.36\n    # Ranking: [0.27, 0.30, 0.36]. Max is 0.36 (from bin 0.5). Still WF dominant.\n\n    # Case `blend_weight = 0.8` (80% BF):\n    # Score = 0.2 * bins_remain_cap + 0.8 * (-rem_cap_after)\n    # Bin 0.35: 0.2 * 0.35 + 0.8 * (-0.05) = 0.07 - 0.04 = 0.03\n    # Bin 0.4:  0.2 * 0.4  + 0.8 * (-0.1)  = 0.08 - 0.08 = 0.00\n    # Bin 0.5:  0.2 * 0.5  + 0.8 * (-0.2)  = 0.10 - 0.16 = -0.06\n    # Ranking: [0.03, 0.00, -0.06]. Max is 0.03 (from bin 0.35). Now BF dominant.\n\n    # This formula correctly transitions the preference.\n    # The name `worst_fit_bias` is misleading if it represents the weight for BF.\n    # Let's rename the tunable parameter to `best_fit_weight`.\n\n    # --- Tunable Parameter ---\n    # `best_fit_weight`: Controls the preference for \"Best Fit\".\n    # A value of 0.0 corresponds to pure \"Worst Fit\" (prioritizing largest remaining capacity).\n    # A value of 1.0 corresponds to pure \"Best Fit\" (prioritizing smallest remaining capacity).\n    # Values in between blend the two strategies.\n    best_fit_weight = 0.5 # Example: Equal blend, which we found to be degenerate.\n                          # Let's choose a non-degenerate example.\n\n    # Let's try `best_fit_weight = 0.3`. This should still be WF dominant.\n    # Let's try `best_fit_bias` for clarity.\n\n    best_fit_bias = 0.3 # Example: 30% preference for Best Fit, 70% for Worst Fit.\n\n    # Calculate the score for bins that can fit the item.\n    # Score = (1 - best_fit_bias) * (Worst Fit metric) + best_fit_bias * (Best Fit metric)\n    # Worst Fit metric: bins_remain_cap[can_fit_mask] (higher is better)\n    # Best Fit metric: -remaining_caps_after_fit (higher is better, i.e., smaller positive remaining_caps_after_fit)\n\n    # The formula `R * (1 - 2w) + item * (1 - w)` works.\n    # Here, `w` is `best_fit_bias`.\n\n    # Let's use the direct formula for clarity.\n    # `bins_remain_cap_fit = bins_remain_cap[can_fit_mask]`\n    # `remaining_caps_fit = bins_remain_cap_fit - item`\n\n    # Score = `(1 - best_fit_bias) * bins_remain_cap_fit + best_fit_bias * (-remaining_caps_fit)`\n    priorities[can_fit_mask] = (1 - best_fit_bias) * bins_remain_cap[can_fit_mask] + \\\n                               best_fit_bias * -(bins_remain_cap[can_fit_mask] - item)\n\n    return priorities",
    "response_id": 7,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 1.0,
    "halstead": 104.2481250360578,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response8.txt_stdout.txt",
    "code_path": "problem_iter6_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a balanced approach.\n\n    This version aims to prioritize bins that offer a \"good fit\" (not too much wasted space)\n    while also considering bins that might be more flexible for future items. It uses\n    a two-component scoring:\n    1. Tightness Score: Penalizes bins that leave excessive remaining capacity after\n       packing the item. It's inverse to the remaining capacity after packing.\n    2. Flexibility Score: Rewards bins that have a larger initial remaining capacity,\n       implying they could accommodate larger future items.\n\n    The scores are then combined and transformed using a stable softmax-like\n    approach to produce probabilities.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        # No bin can fit the item, return all zeros\n        return priorities\n\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Component 1: Tightness Score ---\n    # We want to prioritize bins where the remaining capacity *after* packing\n    # is minimized. A simple approach is to use the inverse of the remaining capacity\n    # after packing, or a transformation that maps smaller remaining capacity to larger scores.\n    # Using `-(remaining_capacity_after_packing)` achieves this.\n    # Adding a small constant to the denominator can prevent division by zero and\n    # slightly favor bins that are not perfectly filled if multiple bins are perfectly filled.\n    remaining_after_packing = fitting_bins_cap - item\n    # Use a small epsilon to avoid zero in denominator and to slightly penalize perfect fits\n    # for tightest bins, ensuring that a bin with a tiny remaining capacity (e.g., 0.001)\n    # gets a slightly higher tightness score than a perfectly fitting bin.\n    epsilon_tightness = 1e-5\n    tightness_scores = 1.0 / (remaining_after_packing + epsilon_tightness)\n\n    # --- Component 2: Flexibility Score ---\n    # We want to reward bins that have a large *original* remaining capacity.\n    # This suggests they are more likely to accommodate future items.\n    # A simple linear scaling works here. The scaling factor controls its influence.\n    flexibility_scale = 0.05  # Tunable parameter to balance tightness vs flexibility\n    flexibility_scores = flexibility_scale * fitting_bins_cap\n\n    # --- Combine Scores ---\n    # Add the flexibility score to the tightness score.\n    combined_scores = tightness_scores + flexibility_scores\n\n    # --- Softmax-like Transformation ---\n    # Convert combined scores into probabilities. Using a stable softmax implementation.\n    # Subtract max for numerical stability before exponentiation.\n    max_combined_score = np.max(combined_scores)\n    exp_scores = np.exp(combined_scores - max_combined_score)\n    sum_exp_scores = np.sum(exp_scores)\n\n    if sum_exp_scores > 1e-9:  # Handle cases where all scores might be extremely small\n        priorities[can_fit_mask] = exp_scores / sum_exp_scores\n    else:\n        # If all scores are very small, distribute probability equally among fitting bins.\n        num_fitting_bins = np.sum(can_fit_mask)\n        if num_fitting_bins > 0:\n            priorities[can_fit_mask] = 1.0 / num_fitting_bins\n\n    return priorities",
    "response_id": 8,
    "obj": 86.58755484643,
    "cyclomatic_complexity": 4.0,
    "halstead": 162.53496664211536,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response9.txt_stdout.txt",
    "code_path": "problem_iter6_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a tunable Sigmoid Fit Score.\n\n    This version refines the Sigmoid Fit Score by introducing `steepness` to control the\n    sigmoid's slope and `ideal_gap_fraction` to specify the desired positive gap as a\n    fraction of the item size. This allows for fine-tuning the preference for bins\n    that are not perfectly filled, potentially leaving more room for future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of -inf.\n    \"\"\"\n    # Initialize priorities to negative infinity for bins that cannot fit the item.\n    # This ensures they are never selected.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can accommodate the item\n    possible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(possible_bins_mask):\n        # If no bin can fit the item, return the initialized -inf priorities.\n        return priorities\n\n    # Calculate the \"gap\" for bins that can fit the item.\n    # Gap is the remaining capacity after placing the item.\n    gaps = bins_remain_cap[possible_bins_mask] - item\n\n    # --- Tunable Parameters ---\n    # `steepness`: Controls how quickly the priority score decreases as the gap increases.\n    # A higher value leads to a sharper drop-off, making the choice more sensitive to small gaps.\n    # Tuned to be responsive to variations around the ideal gap.\n    steepness = 50.0\n\n    # `ideal_gap_fraction`: Defines the target gap as a fraction of the item's size.\n    # A value of 0.0 means we ideally want a perfect fit (gap=0).\n    # A value of 0.1 means we ideally want a gap of 10% of the item size.\n    # This encourages leaving some buffer space.\n    # Tuned to favor a small but positive gap, encouraging efficient packing without\n    # sacrificing too much future flexibility.\n    ideal_gap_fraction = 0.05\n\n    # Calculate the ideal gap based on the item size.\n    # Ensure the ideal gap is non-negative. If item size is 0, ideal_gap is 0.\n    ideal_gap = max(0.0, ideal_gap_fraction * item)\n\n    # --- Sigmoid Argument Calculation ---\n    # We use the sigmoid function `1 / (1 + exp(-x))` which maps x to [0, 1].\n    # To prioritize bins with gaps close to `ideal_gap`, we can map `ideal_gap` to the center\n    # of the sigmoid's steep slope (input close to 0).\n    # The argument `x` for the sigmoid will be `steepness * (ideal_gap - gap)`.\n    #\n    # If `gap < ideal_gap`: The argument is positive, sigmoid output > 0.5. Higher scores for smaller gaps.\n    # If `gap = ideal_gap`: The argument is 0, sigmoid output = 0.5. This is the pivot point.\n    # If `gap > ideal_gap`: The argument is negative, sigmoid output < 0.5. Lower scores for larger gaps.\n    #\n    # By choosing `ideal_gap_fraction`, we are effectively tuning the bias of the sigmoid.\n    # The argument can be seen as: `steepness * ideal_gap - steepness * gap`.\n    # The `steepness * ideal_gap` term acts as an additive bias.\n    sigmoid_arg = steepness * (ideal_gap - gaps)\n\n    # Apply the sigmoid function to compute the priority scores for fitting bins.\n    # Higher values indicate a more desirable bin.\n    priorities[possible_bins_mask] = 1 / (1 + np.exp(-sigmoid_arg))\n\n    return priorities",
    "response_id": 9,
    "obj": 4.01874750698045,
    "cyclomatic_complexity": 2.0,
    "halstead": 118.59257041502654,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response0.txt_stdout.txt",
    "code_path": "problem_iter7_code0.py",
    "code": "import numpy as np\nimport scipy.stats as stats\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins for online Bin Packing Problem (BPP) using a refined heuristic.\n\n    This function aims to balance the immediate \"tightness\" of a fit with future\n    flexibility by considering the remaining capacity in a more nuanced way.\n    It uses a sigmoid function to map the \"gap\" (remaining capacity - item)\n    to a priority score, favoring bins with small positive gaps.\n\n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: A numpy array where each element is the remaining\n                         capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as bins_remain_cap, where each element\n        is the priority score for the corresponding bin. Higher scores indicate\n        higher priority. Bins that cannot fit the item will have a score of 0.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the \"gap\" for bins that can fit the item\n    # Gap is defined as remaining_capacity - item\n    gaps = bins_remain_cap[can_fit_mask] - item\n\n    # Parameters for the sigmoid function\n    # Steepness: Controls how quickly the priority drops off as the gap increases.\n    # A higher steepness means only very tight fits get high priority.\n    steepness = 10.0\n    # Ideal Gap: The gap size that yields the maximum priority.\n    # We aim for a small positive gap, as this implies a good fit without\n    # leaving excessive wasted space. Setting it slightly above 0 encourages\n    # slightly less \"perfect\" fits that might leave more room for future items.\n    ideal_gap = 0.1  # Tuned for a small positive gap\n\n    # Apply sigmoid function to the gaps.\n    # The sigmoid function maps any real number to a value between 0 and 1.\n    # We shift the gaps so that 'ideal_gap' corresponds to the center of the sigmoid,\n    # and then scale by steepness.\n    # sigmoid(x) = 1 / (1 + exp(-x))\n    # We want to transform the gap to have a peak priority at ideal_gap.\n    # A common approach is to use 1 / (1 + exp(-steepness * (gap - ideal_gap)))\n    # This results in priority peaking at ideal_gap.\n    if len(gaps) > 0:\n        sigmoid_scores = 1 / (1 + np.exp(-steepness * (gaps - ideal_gap)))\n        priorities[can_fit_mask] = sigmoid_scores\n\n    # Add a small stabilization factor to prevent extremely low priorities\n    # from being zero and to slightly boost bins that are not the absolute best fit.\n    # This is a form of soft exploration/stabilization.\n    stabilization_factor = 0.05\n    priorities[can_fit_mask] += stabilization_factor\n\n    # Normalize priorities to be between 0 and 1.\n    # This ensures that scores are comparable across different item/bin states.\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n    else:\n        # If no bin can fit the item, all priorities remain 0.\n        pass\n        \n    # Further refinement: Add a small random noise to break ties and encourage exploration\n    # This is applied to all bins that can fit the item.\n    noise_level = 0.02\n    noise = np.random.rand(num_bins) * noise_level\n    priorities[can_fit_mask] += noise\n\n    # Re-normalize after adding noise\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n        \n    # Ensure no negative priorities (though our logic shouldn't produce them)\n    priorities[priorities < 0] = 0\n\n    return priorities",
    "response_id": 0,
    "obj": 74.55125648185084,
    "cyclomatic_complexity": 4.0,
    "halstead": 228.32510677099592,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response1.txt_stdout.txt",
    "code_path": "problem_iter7_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin, prioritizing tight fits\n    with normalized features and a tunable steepness.\n\n    This heuristic aims to balance fitting an item tightly into a bin (to leave larger\n    gaps in other bins for potentially larger future items) while also allowing some\n    flexibility.\n\n    The priority for a bin is calculated based on how well it fits the item,\n    penalizing bins that are \"too large\" for the item, but not so much that\n    very small items can't find a home.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Filter to only consider bins that can fit the item\n    potential_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if len(potential_bins_remain_cap) == 0:\n        return priorities  # No bin can fit the item\n\n    # --- Feature Engineering ---\n    # 1. Normalized remaining capacity: How much space is left relative to the bin's original capacity?\n    #    We assume an \"ideal\" bin has a certain amount of slack.\n    #    For this heuristic, let's define a parameter `ideal_gap`.\n    #    A smaller difference (tight fit) is generally good, but we don't want to penalize\n    #    bins that have a *little* more space if it means fitting a very small item.\n    #    Let's consider the \"waste\" or \"slack\" in the bin if the item is placed.\n    #    Slack = bin_remain_cap - item\n\n    slack = potential_bins_remain_cap - item\n\n    # 2. Normalized slack: Divide slack by the bin's original capacity to make it relative.\n    #    This helps compare slack across bins of different initial sizes.\n    #    We need original capacities to normalize. For this function signature,\n    #    we don't have original capacities. A common approach is to normalize by\n    #    a maximum possible capacity or a system-wide average.\n    #    Alternatively, we can normalize slack by the *current* remaining capacity\n    #    before placing the item, which is related to how \"full\" the bin already is.\n    #    Let's try normalizing by the *potential* remaining capacity itself, to see how\n    #    much of the *available space* is consumed. A smaller ratio means a tighter fit.\n    #    (slack / potential_bins_remain_cap) is essentially 1 - (item / potential_bins_remain_cap)\n    #    This ratio represents how much *unused* space remains after placing the item,\n    #    relative to the space that was available.\n    #    A smaller ratio (closer to 0) means a tighter fit.\n\n    # Handle division by zero if potential_bins_remain_cap is 0 (though can_fit_mask should prevent this)\n    # We want to reward tight fits. A tighter fit means slack is small.\n    # A simple approach is to use 1 / (slack + small_epsilon) to give higher scores to smaller slack.\n    # However, this can lead to extremely large values for very tight fits.\n\n    # Let's use a sigmoid-like function to map the \"tightness\" to a priority score.\n    # Tightness can be measured by how close `bin_remain_cap` is to `item`.\n    # We want a high priority when `bin_remain_cap` is just slightly larger than `item`.\n\n    # Define an \"ideal gap\" - the preferred amount of remaining capacity after packing.\n    # This could be a small constant, or related to the item size.\n    # Let's use a constant for simplicity, say `ideal_gap = 0`.\n    # A gap of `g` means `bin_remain_cap = item + g`.\n    # We want to maximize priority when `g` is small.\n\n    # Let's define a \"fit score\" which is higher for tighter fits.\n    # `fit_score = 1 / (slack + epsilon)` is problematic.\n    # Instead, consider a metric that is high when `slack` is low.\n    # Let `metric = item / potential_bins_remain_cap`. This is high for tight fits.\n    # However, it's also high for large items in large bins.\n    # A better metric might be `(potential_bins_remain_cap - item) / potential_bins_remain_cap`\n    # which is `slack / potential_bins_remain_cap`. We want to minimize this ratio.\n\n    # Let's invert and scale: `priority_component = 1.0 - (slack / potential_bins_remain_cap)`\n    # This is equivalent to `item / potential_bins_remain_cap`. High means item is a large\n    # fraction of the available space.\n    # This might over-prioritize large items filling large bins.\n\n    # Let's rethink: we want to favor bins where `bin_remain_cap` is close to `item`.\n    # Consider `similarity = 1 / (1 + abs(bin_remain_cap - item))`. This is high when `bin_remain_cap` is close to `item`.\n    # But we need to ensure `bin_remain_cap >= item`.\n    # So, `similarity = 1 / (1 + (bin_remain_cap - item))` for `bin_remain_cap >= item`.\n    # This gives higher scores to smaller slack.\n\n    # We can use a sigmoid-like function to control the steepness of the preference.\n    # Let `x = slack = bin_remain_cap - item`. We want to give higher scores for smaller `x`.\n    # A function like `1 / (1 + exp(k * x))` where `k` is steepness. Larger `k` means steeper drop-off.\n    # Or, `exp(-k * x)`.\n    # Let's use `exp(-k * slack)`.\n    # `k` controls how aggressively we penalize slack.\n    # A higher `k` means bins with slightly more slack get much lower priority.\n    # A lower `k` means bins with more slack are penalized less.\n\n    # Let's set a steepness parameter.\n    steepness = 5.0  # Tune this: higher means more aggressive preference for tight fits\n\n    # Calculate raw priority based on slack using an exponential decay.\n    # Higher score for smaller slack.\n    raw_priorities = np.exp(-steepness * slack)\n\n    # --- Normalization ---\n    # Normalize these raw priorities so they are between 0 and 1.\n    # This prevents extremely large or small values from dominating.\n    max_raw_priority = np.max(raw_priorities)\n    if max_raw_priority > 0:\n        normalized_priorities = raw_priorities / max_raw_priority\n    else:\n        normalized_priorities = np.zeros_like(raw_priorities) # Should not happen if len > 0\n\n    # --- Applying to original bins ---\n    # Place the normalized priorities back into the full `priorities` array.\n    priorities[can_fit_mask] = normalized_priorities\n\n    # --- Refinement: \"Ideal Gap\" Bonus ---\n    # We might want to slightly penalize bins that are *too* tight, leaving\n    # absolutely no room for error or small future items. Or, conversely,\n    # give a slight boost to bins with a small, non-zero \"ideal\" remaining capacity.\n    # Let's define an `ideal_gap` value. A bin with `remaining_capacity = item + ideal_gap`\n    # could get a bonus.\n    # For simplicity, let's adjust the penalty based on slack. Instead of `exp(-k * slack)`,\n    # we can use a function that peaks when slack is zero, and decreases for positive slack.\n    # Or, we can modify the slack calculation.\n\n    # Alternative approach for priority calculation:\n    # Prioritize bins where `bins_remain_cap` is closest to `item`.\n    # We want to maximize `1 / (1 + (bins_remain_cap - item))` for bins that fit.\n    # Let's try a shifted sigmoid or logistic function.\n    # Let `y = bins_remain_cap - item` (slack).\n    # We want a high value when `y` is small.\n    # Consider a function like `1 / (1 + exp(k * (y - offset)))`.\n    # If `offset = 0`, this peaks at `y=0` (slack=0).\n    # `offset` can represent the \"ideal gap\". A positive offset means we prefer\n    # bins with a small positive slack.\n\n    ideal_gap = 2.0  # This is a tunable parameter: the preferred slack.\n                     # A value of 0 means we prefer bins that fit exactly.\n                     # A small positive value means we prefer bins with a little bit of room left.\n    steepness_fit = 1.0 # Tune this: controls how quickly preference drops off away from ideal_gap.\n\n    # Calculate priority based on deviation from ideal gap.\n    # Higher priority when `slack` is close to `ideal_gap`.\n    # We use `exp(-steepness_fit * abs(slack - ideal_gap))`\n    # This gives a Gaussian-like peak around `ideal_gap`.\n\n    deviation_from_ideal = np.abs(slack - ideal_gap)\n    fit_quality_scores = np.exp(-steepness_fit * deviation_from_ideal)\n\n    # Normalize these scores to be between 0 and 1.\n    max_fit_quality = np.max(fit_quality_scores)\n    if max_fit_quality > 0:\n        normalized_fit_quality = fit_quality_scores / max_fit_quality\n    else:\n        normalized_fit_quality = np.zeros_like(fit_quality_scores)\n\n    # Update priorities with this new metric\n    priorities[can_fit_mask] = normalized_fit_quality\n\n    # --- Stabilization and Robustness ---\n    # To make scores more stable and less sensitive to minor variations,\n    # we can average the current priority with a lagged average, or add a small\n    # constant to avoid zero priorities unless absolutely necessary.\n    # Adding a small constant also helps in scenarios where all bins might have\n    # very similar low priorities.\n    # Let's add a small baseline priority to all bins that can fit.\n    baseline_priority = 0.1\n    priorities[can_fit_mask] = np.maximum(priorities[can_fit_mask], baseline_priority)\n\n    # Re-normalize after adding baseline to maintain the [0, 1] range effectively.\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n    else:\n        # If all were zero, they remain zero.\n        pass\n\n    return priorities",
    "response_id": 1,
    "obj": 4.816513761467886,
    "cyclomatic_complexity": 5.0,
    "halstead": 185.75424759098897,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response2.txt_stdout.txt",
    "code_path": "problem_iter7_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin,\n    prioritizing tight fits with normalized features and tunable parameters.\n\n    This heuristic balances finding a tight fit (minimizing wasted space)\n    with maintaining some flexibility for future items. It uses a sigmoid\n    function to assign higher priorities to bins that are close to being full\n    but can still accommodate the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    # Tunable parameters\n    epsilon_explore = 0.1  # Probability of random exploration\n    steepness = 15.0  # Controls the steepness of the sigmoid for gap preference\n    ideal_gap_ratio = 0.1  # Target ratio of remaining_capacity / bin_capacity for optimal fit\n\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate relevant features for fitting bins\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n\n    if len(fitting_bins_remain_cap) == 0:\n        return priorities  # No bin can fit the item\n\n    # Feature 1: Tightness of fit (how close is the remaining capacity to the item size)\n    # We want to minimize remaining_capacity - item.\n    # Normalize this difference by the item size to get a relative measure.\n    # Add a small epsilon to avoid division by zero if item is 0.\n    relative_waste = (fitting_bins_remain_cap - item) / (item + 1e-6)\n\n    # Feature 2: Current bin fill level (related to flexibility).\n    # We want bins that are relatively full but not too full.\n    # This can be approximated by how much space is left relative to the original bin capacity.\n    # Assuming bins have a maximum capacity, which we don't explicitly know here.\n    # A proxy could be the item size itself relative to the remaining capacity.\n    # Instead, let's focus on the \"ideal gap\" concept from reflection.\n    # If we don't know the original bin capacity, we can use the available remaining capacity\n    # as a reference, and aim for a certain portion of it to be left empty ideally.\n    # Let's normalize remaining capacity by the item size itself as a reference for \"how full\" it is.\n    # A higher value here means more space is available relative to the item.\n    # We want to penalize bins with too much space.\n    space_ratio = fitting_bins_remain_cap / (item + 1e-6)\n\n    # Combine features using a sigmoid function to prioritize bins with a specific \"gap\"\n    # A smaller `relative_waste` is good. A `space_ratio` closer to 1 (meaning remaining_cap is close to item) is good.\n    # The sigmoid will give higher scores to bins that are close to accommodating the item without much excess.\n\n    # Let's define a score based on how close the remaining capacity is to a target gap after packing.\n    # Target remaining capacity = item * (1 + ideal_gap_ratio)\n    target_remaining_cap = item * (1 + ideal_gap_ratio)\n    gap_deviation = np.abs(fitting_bins_remain_cap - target_remaining_cap)\n\n    # We want to minimize gap_deviation. Apply sigmoid to convert deviation to priority.\n    # Smaller deviation should lead to higher priority.\n    # Sigmoid: 1 / (1 + exp(-k * x)) - this gives high values for positive x.\n    # We want high values for small gap_deviation. So we use -gap_deviation.\n    # The steepness parameter controls how sensitive the priority is to the deviation.\n    # We also need to ensure that bins that are too small (deviation is large and negative) get low priority.\n    # Let's use a sigmoid that maps small positive deviations to high priorities, and larger deviations to lower priorities.\n\n    # A score that is high when `fitting_bins_remain_cap` is close to `target_remaining_cap`.\n    # We can use `target_remaining_cap - fitting_bins_remain_cap`.\n    # A value of 0 here is ideal.\n    diff_from_ideal = target_remaining_cap - fitting_bins_remain_cap\n\n    # Now apply sigmoid. We want large positive values of `diff_from_ideal` (meaning bin is too big)\n    # to map to low priorities, and values close to 0 (ideal fit) to map to high priorities.\n    # Using `steepness * (-diff_from_ideal)` in sigmoid will achieve this.\n    # We also want to ensure that if `fitting_bins_remain_cap` is very small (less than item),\n    # `diff_from_ideal` would be positive and large, resulting in low sigmoid output. This is correct.\n    # However, `fitting_bins_remain_cap` are already filtered to be >= item, so `diff_from_ideal` will be <= 0.\n    # Let's rephrase: we want `fitting_bins_remain_cap` to be as close to `item` as possible,\n    # ideally leaving a small gap.\n    # Let's focus on `(fitting_bins_remain_cap - item)` as the actual waste.\n    # We want this waste to be small, around `item * ideal_gap_ratio`.\n    actual_waste = fitting_bins_remain_cap - item\n    ideal_waste = item * ideal_gap_ratio\n\n    # Measure deviation from ideal waste: `actual_waste - ideal_waste`\n    # We want this deviation to be close to 0.\n    deviation_from_ideal_waste = actual_waste - ideal_waste\n\n    # Use sigmoid: `1 / (1 + exp(-steepness * x))`\n    # If `deviation_from_ideal_waste` is small positive (actual_waste > ideal_waste),\n    # `steepness * x` is positive, sigmoid is > 0.5. High priority.\n    # If `deviation_from_ideal_waste` is small negative (actual_waste < ideal_waste),\n    # `steepness * x` is negative, sigmoid is < 0.5. Lower priority.\n    # If `deviation_from_ideal_waste` is large positive, sigmoid is close to 1.\n    # If `deviation_from_ideal_waste` is large negative, sigmoid is close to 0.\n    # This doesn't quite capture the \"best fit\" idea directly.\n\n    # Let's simplify: prioritize bins where `fitting_bins_remain_cap - item` is minimal.\n    # This is the \"best fit\" concept.\n    # To incorporate the \"ideal gap\", we can create a score that is higher when\n    # `fitting_bins_remain_cap` is between `item` and `item * (1 + ideal_gap_ratio)`.\n    # Or, more generally, a function that peaks around `item * (1 + ideal_gap_ratio)`.\n\n    # Let's create a score based on the `relative_waste` (waste / item).\n    # We want this ratio to be small, ideally `ideal_gap_ratio`.\n    # Score = sigmoid(steepness * (ideal_gap_ratio - relative_waste))\n    # If `relative_waste` is close to `ideal_gap_ratio`, `ideal_gap_ratio - relative_waste` is near 0, sigmoid is 0.5.\n    # If `relative_waste` is smaller than `ideal_gap_ratio` (tighter fit), `ideal_gap_ratio - relative_waste` is positive, sigmoid > 0.5. High priority.\n    # If `relative_waste` is larger than `ideal_gap_ratio` (looser fit), `ideal_gap_ratio - relative_waste` is negative, sigmoid < 0.5. Lower priority.\n    # This prioritizes tighter fits.\n\n    # Let's normalize the potential bins by their current remaining capacity for exploration.\n    # Bins with less remaining capacity might be more critical to fill.\n    # Exploration: random choice among fitting bins, with a bias towards bins that are less full (more flexible).\n    # Or, perhaps exploration should explore the \"less optimal\" fits to see if they lead to better overall packing.\n\n    # Let's stick to the reflection's idea of prioritizing tight fits.\n    # The sigmoid function `1 / (1 + exp(-steepness * (ideal_gap_ratio - normalized_waste)))` seems reasonable.\n\n    normalized_waste = (fitting_bins_remain_cap - item) / (np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 1.0)\n    # Using max bin capacity is a guess, better to use a fixed large value or average if available.\n    # For now, let's use item size as a base for normalization if max_cap is not available.\n    # Let's use a more robust normalization for the \"waste\" aspect.\n    # Let's consider the \"gap\" which is `remaining_capacity - item`.\n    # We want this gap to be ideally `item * ideal_gap_ratio`.\n    # Let's define a score that peaks at this ideal gap.\n    # Score = exp(-steepness * (fitting_bins_remain_cap - item - ideal_waste)^2)\n    # This is a Gaussian-like score that peaks when `fitting_bins_remain_cap - item` is `ideal_waste`.\n    # This is closer to prioritizing a specific gap rather than just the tightest fit.\n\n    # Let's go back to the reflection: \"Prioritize tight fits with normalized features. Tune sigmoid steepness and ideal gap for balance between immediate tightness and future flexibility.\"\n    # \"Immediate tightness\": prioritize bins with `remaining_cap - item` being small.\n    # \"Future flexibility\": avoid filling bins completely, so `remaining_cap - item` shouldn't be too small, or rather, `remaining_cap` should not be too close to `item`.\n\n    # Let's define the priority based on the remaining capacity itself, considering the item.\n    # Bins with `remaining_cap` close to `item` are tight fits.\n    # Bins with `remaining_cap` significantly larger than `item` offer flexibility.\n    # We want a sweet spot.\n\n    # Consider the \"slack\" or \"wasted space\" = `remaining_cap - item`.\n    # We want this slack to be small (tight fit) but not zero (future flexibility).\n    # Let's define a target slack `target_slack = item * ideal_gap_ratio`.\n    # The score should be high when `slack` is close to `target_slack`.\n    # Score = Gaussian(slack, mean=target_slack, std=item * (1-ideal_gap_ratio) / steepness)\n    # Or, simply use the sigmoid approach on the deviation from the target slack.\n\n    # Let's normalize the remaining capacity by the item size for better comparison across different item sizes.\n    # `normalized_remaining_cap = fitting_bins_remain_cap / item`\n    # We want `normalized_remaining_cap` to be close to `1 + ideal_gap_ratio`.\n    # Score = sigmoid(steepness * ( (1 + ideal_gap_ratio) - normalized_remaining_cap ))\n    # If `normalized_remaining_cap` is close to `1 + ideal_gap_ratio`, sigmoid output is ~0.5.\n    # If `normalized_remaining_cap` is smaller (tighter fit), argument is positive, sigmoid > 0.5 (higher priority).\n    # If `normalized_remaining_cap` is larger (looser fit), argument is negative, sigmoid < 0.5 (lower priority).\n\n    normalized_remaining_cap = fitting_bins_remain_cap / (item + 1e-6)\n    score_exploitation = 1 / (1 + np.exp(-steepness * ( (1 + ideal_gap_ratio) - normalized_remaining_cap )))\n\n    # Now incorporate exploration. With probability epsilon_explore, choose randomly.\n    # Otherwise, use the exploitation score.\n    # To avoid always picking the exact same bins during exploration, we can add a small random noise.\n    exploration_scores = np.random.rand(len(fitting_bins_remain_cap)) * 0.5 # Lower than max exploitation score (1.0)\n\n    # Combine exploitation and exploration using epsilon-greedy logic\n    # For each fitting bin, decide whether to explore or exploit.\n    use_exploration = np.random.rand(len(fitting_bins_remain_cap)) < epsilon_explore\n    priorities_for_fitting_bins = np.where(use_exploration,\n                                           exploration_scores,\n                                           score_exploitation)\n\n    # Assign these priorities to the original bins array\n    priorities[fitting_bins_indices] = priorities_for_fitting_bins\n\n    # Normalize priorities so that the highest is 1.0, unless no bins fit.\n    if np.max(priorities) > 0:\n        priorities = priorities / np.max(priorities)\n\n    # Add a small random noise to all priorities to break ties and add stochasticity\n    # This also helps with stabilization as mentioned in reflection.\n    # The noise should be small relative to the priority scale.\n    noise = np.random.rand(num_bins) * 0.1 # Small noise between 0 and 0.1\n    priorities = priorities + noise\n\n    # Final normalization after adding noise\n    if np.max(priorities) > 0:\n        priorities = priorities / np.max(priorities)\n    else:\n        # If all are zero or became zero after some operation (unlikely here)\n        # handle to avoid NaNs.\n        pass\n\n    return priorities",
    "response_id": 2,
    "obj": 35.53051455923414,
    "cyclomatic_complexity": 5.0,
    "halstead": 544.3497074644085,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response3.txt_stdout.txt",
    "code_path": "problem_iter7_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a\n    heuristic that prioritizes tight fits with normalized features.\n\n    The heuristic aims to balance immediate tightness with future flexibility by:\n    1. Prioritizing bins with a \"near-perfect\" fit (small remaining capacity after packing).\n    2. Introducing a slight penalty for bins that become nearly empty after packing,\n       to encourage leaving slightly more space for future larger items.\n    3. Normalizing features to ensure consistent behavior across different item sizes and bin capacities.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # --- Heuristic Calculation ---\n\n    # Calculate the remaining capacity after packing the item for bins that can fit it\n    remaining_after_packing = bins_remain_cap[can_fit_mask] - item\n\n    # Define parameters for the priority function\n    # Steepness of the sigmoid-like function, controlling how quickly priority drops\n    # as the gap increases. Higher values mean tighter fits are more strongly preferred.\n    steepness = 10.0\n    # The ideal remaining capacity. Bins closer to this ideal will get higher priority.\n    ideal_gap = 0.05  # A small positive gap to avoid packing too tightly\n\n    # Calculate a score based on closeness to the ideal gap.\n    # We want to penalize large gaps and very small (negative) gaps (which shouldn't happen due to can_fit_mask).\n    # A sigmoid-like function can be useful here to bound scores and provide a smooth transition.\n    # Let's use a modified version of a logistic function, centered around `ideal_gap`.\n    # The score will be high when `remaining_after_packing` is close to `ideal_gap`.\n\n    # Normalize `remaining_after_packing` for a more robust sigmoid application.\n    # We can normalize relative to the maximum possible remaining capacity (original bin capacity)\n    # or relative to the range of remaining capacities of fitting bins.\n    # For simplicity and to capture the \"tightness\" aspect, let's normalize by the item size,\n    # or relative to a plausible range of remaining capacities.\n    # A simple normalization could be by the item size or by the bin capacity.\n    # Let's normalize the remaining capacity relative to a common scale, e.g., the item size itself,\n    # or a fraction of the total bin capacity if that was available.\n    # Let's try normalizing the *difference* (remaining_after_packing) by the item size.\n    # This scales the \"tightness\" relative to the item's size.\n\n    # Handle cases where item size might be zero or very small to avoid division by zero.\n    # If item is 0, remaining_after_packing is just bins_remain_cap[can_fit_mask].\n    # Let's use a small epsilon or the max remaining capacity as a divisor if item is 0.\n    normalizing_factor = np.maximum(item, 1e-6) # Avoid division by zero\n\n    normalized_remaining = remaining_after_packing / normalizing_factor\n\n    # Calculate scores using a sigmoid-like function that peaks around `ideal_gap / normalizing_factor`\n    # score = 1 / (1 + exp(steepness * (x - center)))\n    # We want a peak at `ideal_gap`, so let's transform `normalized_remaining`.\n    # A simple approach: prioritize smaller `remaining_after_packing`.\n    # Let's use a Gaussian-like function centered on `ideal_gap` or a similar approach.\n\n    # Alternative approach: score = exp(-steepness * (remaining_after_packing - ideal_gap)^2)\n    # This gives a Gaussian peak. The larger the `steepness`, the narrower the peak.\n    # We need to ensure the peak is appropriately scaled.\n\n    # Let's try a simple transformation: prioritize smaller remaining capacity, but with a penalty for being too small.\n    # Higher priority for `remaining_after_packing` close to `ideal_gap`.\n    # score = exp(-steepness * (remaining_after_packing - ideal_gap)**2)\n    # We need to be careful with the scale of `remaining_after_packing`.\n    # Normalizing by item size:\n    # score = exp(-steepness * (normalized_remaining - ideal_gap/normalizing_factor)**2)\n\n    # Let's refine the idea of \"tight fit\" vs. \"future flexibility\".\n    # Tight fit: `remaining_after_packing` is small.\n    # Future flexibility: `remaining_after_packing` is not *too* small, to accommodate larger items.\n    # A U-shaped preference curve for `remaining_after_packing` could work.\n    # Prioritize `remaining_after_packing` near `ideal_gap`.\n\n    # Let's use a function that penalizes deviations from `ideal_gap`.\n    # Penalty = abs(remaining_after_packing - ideal_gap)\n    # We want to minimize this penalty. So, priority is inversely related.\n    # priority_component = 1 / (1 + steepness * abs(remaining_after_packing - ideal_gap))\n    # This gives a peak at `ideal_gap`.\n\n    # Let's scale the `ideal_gap` relative to the item size to be more general.\n    # relative_ideal_gap = ideal_gap / item # This is problematic if item is small.\n    # Better: use a fixed ideal gap and scale the remaining capacity by bin capacity or a typical item size.\n    # Assuming bin capacity is implicitly known or can be estimated. For now, let's stick to relative to item size.\n\n    # Revisit the reflection: \"Prioritize tight fits with normalized features.\"\n    # \"Tune sigmoid steepness and ideal gap for balance between immediate tightness and future flexibility.\"\n    # \"Stabilize scores for robust bin packing.\"\n\n    # Normalized features:\n    # Let's consider the \"waste\" as `remaining_after_packing`.\n    # We want to minimize waste, but not to zero.\n    # A function that maps waste to priority:\n    # Higher priority for `waste` close to `ideal_gap`.\n\n    # Let's define the priority for a fitting bin as a function of `remaining_after_packing`.\n    # `f(waste)`.\n    # `f(waste)` should be maximal at `waste = ideal_gap`.\n    # `f(waste)` should decrease as `waste` moves away from `ideal_gap` in either direction.\n    # `steepness` controls how fast it decreases.\n\n    # Example function: Gaussian kernel: `exp(-steepness * (waste - ideal_gap)^2)`\n    # Let's use `ideal_gap = 0.05` (5% of bin capacity, if we knew it) or\n    # `ideal_gap` related to the item size.\n    # If `ideal_gap` is a fraction of the *item size*: `ideal_gap_item = 0.1 * item`.\n    # Then we want `remaining_after_packing` to be around `ideal_gap_item`.\n    # The difference `remaining_after_packing - ideal_gap_item`.\n    # Let's try this:\n\n    ideal_relative_waste = 0.10  # Ideal waste as a fraction of the item size\n    current_relative_waste = remaining_after_packing / np.maximum(item, 1e-6)\n    deviation_from_ideal = current_relative_waste - ideal_relative_waste\n\n    # Use a Gaussian-like function for priority: higher for deviations closer to zero.\n    # steepness controls how sensitive the priority is to deviations.\n    # Let's scale steepness to be effective for typical deviations.\n    # If `deviation_from_ideal` is around 0.5, we want the score to drop significantly.\n    # exp(-steepness * deviation^2)\n    # If deviation is 0.5, exp(-steepness * 0.25). For this to be ~0.1, steepness ~ 9.\n    # Let's try steepness = 8.0.\n\n    priority_scores_fitting = np.exp(-steepness * (deviation_from_ideal**2))\n\n    # Apply a small random perturbation to break ties and add exploration-like behavior\n    # within the exploitation. This helps stabilize scores by making them unique.\n    # We add a small noise term. The range of `priority_scores_fitting` is [0, 1].\n    # Adding noise scaled by a fraction of this range is good.\n    noise_scale = 0.05\n    random_noise = np.random.normal(0, noise_scale, size=priority_scores_fitting.shape)\n    stabilized_scores = priority_scores_fitting + random_noise\n\n    # Clip scores to be within a reasonable range [0, 1] to prevent negative priorities from noise.\n    stabilized_scores = np.clip(stabilized_scores, 0.0, 1.0)\n\n    # Assign these scores to the bins that can fit the item\n    priorities[can_fit_mask] = stabilized_scores\n\n    # Ensure bins that cannot fit have zero priority\n    priorities[~can_fit_mask] = 0.0\n\n    # Normalize priorities to be in the range [0, 1] for consistent interpretation.\n    # This also makes the \"best\" bin have a score of 1.0.\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n    else:\n        # If all priorities are zero (e.g., item too large for all bins),\n        # this case should have been caught by `not np.any(can_fit_mask)`.\n        # But as a fallback, ensure we return zeros.\n        priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    return priorities",
    "response_id": 3,
    "obj": 9.12445153570004,
    "cyclomatic_complexity": 3.0,
    "halstead": 176.64806144190666,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response4.txt_stdout.txt",
    "code_path": "problem_iter7_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin, prioritizing tight fits\n    with normalized features.\n\n    This heuristic aims to balance immediate tightness (best fit) with future flexibility.\n    It uses a sigmoid function to assign priorities, where the \"ideal gap\" influences\n    the steepness of the priority curve around a preferred remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    # Hyperparameters for tuning\n    # steepness controls how quickly the priority drops off as remaining capacity deviates from the ideal gap.\n    # A higher steepness means a more sensitive response to capacity differences.\n    steepness = 5.0\n    # ideal_gap represents the preferred amount of remaining capacity after packing the item.\n    # This is a target for a \"good\" fit, aiming to leave a reasonable amount of space.\n    ideal_gap = 0.1 * np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity after packing the item for bins that can fit\n    potential_remaining_caps = bins_remain_cap[can_fit_mask]\n    if len(potential_remaining_caps) == 0:\n        return priorities  # No bin can fit the item\n\n    # Calculate the \"gap\" for bins that can fit\n    gaps = potential_remaining_caps - item\n\n    # Calculate the difference from the ideal gap\n    diff_from_ideal_gap = gaps - ideal_gap\n\n    # Use a sigmoid-like function to assign priorities.\n    # The sigmoid function will produce values between 0 and 1.\n    # We want higher priority for gaps closer to the ideal_gap.\n    # A simple way to achieve this is to use exp(-|diff_from_ideal_gap| * steepness).\n    # This gives higher values when diff_from_ideal_gap is close to zero.\n    # We add a small epsilon to the denominator of the sigmoid to prevent division by zero\n    # in edge cases and to slightly temper extreme values.\n    epsilon_stability = 1e-6\n    sigmoid_input = -np.abs(diff_from_ideal_gap) * steepness\n    # Using a shifted and scaled sigmoid for potentially better distribution if needed,\n    # or a simpler exponential decay.\n    # Simple exponential decay: higher priority for smaller absolute difference from ideal_gap\n    scores = np.exp(sigmoid_input)\n\n    # Apply the calculated scores to the corresponding bins in the priorities array\n    priorities[can_fit_mask] = scores\n\n    # Normalize priorities to be between 0 and 1 for consistent scale,\n    # focusing on relative importance.\n    max_priority = np.max(priorities)\n    if max_priority > epsilon_stability:\n        priorities = priorities / max_priority\n    else:\n        # If all eligible bins have near-zero priority (e.g., item is very large)\n        # Assign uniform low priority to all eligible bins.\n        priorities[can_fit_mask] = 0.1 # or some small default value\n\n    # Ensure bins that cannot fit have zero priority\n    priorities[~can_fit_mask] = 0.0\n\n    # Add a small random jitter to break ties and provide some exploration\n    # This helps in scenarios where multiple bins have very similar priority scores.\n    # The jitter is scaled by the current priority to affect higher-priority bins more.\n    jitter_strength = 0.05\n    random_jitter = (np.random.rand(num_bins) - 0.5) * jitter_strength * priorities\n    priorities += random_jitter\n\n    # Re-normalize after adding jitter to maintain the 0-1 scale\n    max_priority_after_jitter = np.max(priorities)\n    if max_priority_after_jitter > epsilon_stability:\n        priorities = priorities / max_priority_after_jitter\n    else:\n        priorities[can_fit_mask] = 0.1 # Re-apply if jitter made all zero\n\n\n    return priorities",
    "response_id": 4,
    "obj": 30.6741124850419,
    "cyclomatic_complexity": 5.0,
    "halstead": 247.1753118485642,
    "exec_success": true
  }
]