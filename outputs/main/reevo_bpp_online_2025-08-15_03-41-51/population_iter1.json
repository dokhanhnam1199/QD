[
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit strategy.\n\n    The First Fit strategy prioritizes bins that can accommodate the item and\n    among those, it favors the bins that leave the least remaining capacity after\n    placing the item. This encourages tighter packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n        Among the bins that can fit, the priority is inversely proportional\n        to the remaining capacity after fitting the item (higher priority for less remaining capacity).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate remaining capacity for bins that can fit the item\n    remaining_capacities_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # For bins that can fit, assign priority. Higher priority for less remaining capacity.\n    # We can use the inverse of remaining capacity, but to avoid division by zero\n    # and to ensure higher values are better, we can use a large number minus the remaining capacity.\n    # Or, more simply, sort by remaining capacity and assign decreasing priorities.\n    # A simple heuristic is to use the negative of the remaining capacity, so smaller remaining capacity is a larger negative number,\n    # but we want higher priority score to be selected. So, we can use a large constant minus the remaining capacity.\n    # Or, to reflect the 'first fit' nature where the *first* suitable bin is preferred, we can simply give a higher score\n    # to the *first* bin that fits, or iterate through and give decreasing scores for subsequent fits.\n\n    # Let's implement a common First Fit priority: assign a high score to bins that fit,\n    # and among those, prioritize the one with the smallest remaining capacity.\n    # We can achieve this by sorting the fitting bins by their remaining capacity\n    # and assigning priorities.\n\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n\n    if fitting_bins_indices.size > 0:\n        # Get the remaining capacities of the bins that can fit the item\n        current_fitting_capacities = bins_remain_cap[fitting_bins_indices]\n\n        # Calculate the resulting remaining capacity after placing the item\n        resulting_remaining_capacities = current_fitting_capacities - item\n\n        # Sort the fitting bins by their resulting remaining capacity in ascending order\n        # This means bins that leave less space will come first.\n        sorted_indices_within_fitting = np.argsort(resulting_remaining_capacities)\n\n        # Assign priorities. The bin that leaves the least remaining capacity gets the highest priority.\n        # We can assign priorities from n to 1, where n is the number of fitting bins.\n        num_fitting_bins = len(fitting_bins_indices)\n        for i in range(num_fitting_bins):\n            original_index = fitting_bins_indices[sorted_indices_within_fitting[i]]\n            priorities[original_index] = num_fitting_bins - i # Assign decreasing priority scores\n\n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 51.89147427955947,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit strategy.\n\n    The Best Fit strategy prioritizes bins that have just enough capacity to fit the item,\n    minimizing wasted space. It assigns a higher priority to bins with the smallest\n    remaining capacity that is still greater than or equal to the item size.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher score indicates higher priority.\n    \"\"\"\n    # Initialize priorities to a very low value (e.g., -infinity) for bins that cannot fit the item\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can fit the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    # For suitable bins, calculate the remaining capacity *after* placing the item\n    remaining_capacities_after_placement = bins_remain_cap[suitable_bins_mask] - item\n\n    # The \"best fit\" is the one that leaves the least remaining capacity,\n    # so we want to prioritize bins with the smallest positive difference.\n    # We can use the negative of this difference to make smaller positive differences\n    # result in higher (less negative) priority scores.\n    priorities[suitable_bins_mask] = -remaining_capacities_after_placement\n\n    # Alternatively, a more direct \"best fit\" priority can be the inverse of the\n    # remaining capacity after placement. However, to ensure it's monotonic with\n    # best fit (smaller remaining capacity is better), we can subtract from\n    # a large constant, or simply use the negative of the remaining capacity as above.\n    # Let's refine the logic to give a clear preference for the smallest *positive* remaining space.\n\n    # We want to maximize the inverse of the remaining space *after* placing the item,\n    # but only for bins that can fit it. A smaller remaining space is better.\n    # So, if a bin can fit the item, its \"goodness\" is inversely related to\n    # (bin_remain_cap - item). To avoid division by zero if capacity is exactly item size,\n    # we can add a small epsilon or handle it.\n\n    # Let's re-implement for clarity on the Best Fit principle:\n    # The best bin is the one where `bin_remain_cap - item` is minimized and non-negative.\n    # This means we want to maximize `-(bin_remain_cap - item)` for eligible bins.\n\n    # If no bins can fit the item, all priorities remain -np.inf, indicating no valid placement.\n    # For bins that *can* fit the item, we want to assign a score that is higher for bins\n    # with smaller `bins_remain_cap - item`.\n    # The score should be related to how \"tight\" the fit is.\n\n    # Option 1: Score as negative of remaining space after placement. Higher score is better.\n    # This naturally prioritizes bins with smaller (bin_remain_cap - item).\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    can_fit_indices = np.where(bins_remain_cap >= item)[0]\n    if len(can_fit_indices) > 0:\n        remaining_after_fit = bins_remain_cap[can_fit_indices] - item\n        # We want to assign higher priority to smaller remaining_after_fit.\n        # So, we can use the negative of remaining_after_fit.\n        # This is equivalent to prioritizing the bins that are closest to exactly fitting the item.\n        priorities[can_fit_indices] = -remaining_after_fit\n\n    # If multiple bins have the exact same best fit, their priorities will be the same.\n    # The selection would then depend on the order or an arbitrary tie-breaker,\n    # which is acceptable for this heuristic.\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 85.11011351724513,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response2.txt_stdout.txt",
    "code_path": "problem_iter1_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Worst Fit strategy aims to put the item in the bin with the largest remaining capacity.\n    # This leaves smaller bins for potentially smaller items later.\n    # We want to prioritize bins with more remaining capacity, so the priority is directly\n    # proportional to the remaining capacity, but only for bins that can fit the item.\n\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Create a mask for bins that have enough capacity for the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Assign priority based on remaining capacity, but only for bins that can fit.\n    # The highest priority goes to the bin with the largest remaining capacity.\n    # To ensure we pick the *worst* fit (largest remaining capacity), we can\n    # directly use the remaining capacity as the priority score.\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask]\n    \n    return priorities",
    "response_id": 2,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 1.0,
    "halstead": 4.754887502163469,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response3.txt_stdout.txt",
    "code_path": "problem_iter1_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit.\n\n    Almost Full Fit prioritizes bins that are close to being full but can still\n    accommodate the item. It favors bins with remaining capacity slightly larger\n    than the item size.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Calculate the \"fit\" for each bin. A smaller value means a tighter fit.\n    # We only consider bins that can actually fit the item.\n    can_fit_mask = bins_remain_cap >= item\n    \n    # If no bin can fit the item, return all zeros (or handle as an error/new bin creation)\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # For bins that can fit the item, calculate a priority score.\n    # The strategy is to give a high priority to bins where the remaining capacity\n    # is just enough or slightly more than the item.\n    # We want to maximize `bins_remain_cap - item`, but penalize cases where\n    # the remaining capacity is much larger than the item.\n    \n    # A simple way to achieve \"almost full fit\" is to prioritize bins\n    # where (bins_remain_cap - item) is minimized.\n    # However, we need to differentiate between bins that are almost full\n    # and bins that are very full (and thus not a good fit).\n\n    # Let's try to prioritize bins where `bins_remain_cap - item` is small.\n    # The ideal scenario is when `bins_remain_cap - item` is 0 (perfect fit).\n    # We can use a function that peaks at 0 and decreases as the difference\n    # increases. A negative quadratic might work, but we need to be careful\n    # about the range.\n\n    # Consider the \"slack\" or \"wasted space\" if the item is placed in the bin: `slack = bins_remain_cap - item`\n    # We want to minimize this slack.\n\n    # A common approach in \"best fit\" style heuristics is to give higher priority\n    # to bins with the smallest remaining capacity that can still fit the item.\n    # So, the priority could be inversely related to `bins_remain_cap` among those that fit.\n    # However, \"Almost Full Fit\" suggests we want bins that are *already* relatively full.\n\n    # Let's consider the bins that *can* fit the item.\n    # For these bins, we want to find the one that will be \"almost full\" *after*\n    # the item is placed. This means the remaining capacity *after* placement\n    # should be small, but non-negative.\n\n    # Let's define priority as a value that increases as the remaining capacity\n    # after placing the item (`bins_remain_cap - item`) decreases,\n    # but only for bins that can accommodate the item.\n\n    # To make it \"Almost Full Fit\", we might want to slightly penalize bins\n    # that are *too* empty, and strongly prefer bins that are just right.\n\n    # Let's try: priority = 1 / (1 + (bins_remain_cap - item))\n    # This gives higher priority to smaller (bins_remain_cap - item) values.\n    # It's capped at 1 when bins_remain_cap == item.\n    # It approaches 0 as (bins_remain_cap - item) gets large.\n\n    # We only apply this to bins that can fit the item.\n    \n    # Calculate potential remaining capacity for fitting bins\n    potential_remaining_caps = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate priorities for fitting bins: Higher priority for smaller remaining capacity\n    # Using `1 / (1 + slack)` encourages smaller positive slacks.\n    # If slack is 0 (perfect fit), priority is 1.\n    # If slack is positive, priority is between 0 and 1.\n    # Adding a small epsilon to the denominator to avoid division by zero if `item` exactly fills a bin and `bins_remain_cap` is 0 (though unlikely with positive item sizes).\n    priorities[can_fit_mask] = 1.0 / (1.0 + potential_remaining_caps + 1e-9)\n\n    # To further emphasize \"almost full\", we can also consider how full the bin\n    # is *before* placing the item. A bin that is already very full (low `bins_remain_cap`)\n    # might be preferred.\n    \n    # Let's modify the priority to give a boost to bins that are already more full.\n    # We can multiply the previous priority by a factor that increases with `bins_remain_cap`.\n    # However, this could conflict if a very full bin cannot fit the item.\n\n    # A better approach for \"Almost Full Fit\" might be to prioritize bins\n    # whose remaining capacity is closest to `item`, but still greater than or equal to it.\n    # So, we want to minimize `bins_remain_cap - item`.\n\n    # Let's re-evaluate the scoring:\n    # We are selecting from bins where `bins_remain_cap >= item`.\n    # Among these, we want the one that, after adding the item, leaves the least amount of space.\n    # This means we want to minimize `bins_remain_cap - item`.\n\n    # If we want to assign a *higher* priority to bins that result in *less* remaining space,\n    # we can invert the value of `bins_remain_cap - item`.\n    # A common way to do this is `1 / (bins_remain_cap - item + epsilon)`.\n    # However, this can lead to very large values if `bins_remain_cap - item` is close to 0.\n\n    # Let's use a Gaussian-like function centered at `item` for the bin capacities.\n    # Or more simply, prioritize bins where `bins_remain_cap` is small but sufficient.\n    \n    # Alternative interpretation of \"Almost Full Fit\":\n    # Prioritize bins that are already quite full (high occupancy).\n    # Among these, pick the one that can fit the item.\n\n    # Let's try prioritizing bins by their original remaining capacity, but only\n    # for those that can fit the item. The bins with the smallest `bins_remain_cap`\n    # that are still >= `item` are the best candidates. This is Best Fit.\n\n    # For \"Almost Full Fit\", maybe it's about finding a bin that, *after* the item is added,\n    # becomes \"almost full\" relative to its original capacity or some threshold.\n\n    # Let's consider the \"urgency\" of filling a bin. A bin that is currently at capacity `C`\n    # and can fit an item `i` leaving `C-i` capacity is prioritized if `C-i` is small.\n\n    # Consider the inverse of the remaining capacity *after* adding the item.\n    # Priority = 1 / (bins_remain_cap[i] - item + epsilon)\n    # This favors bins that leave less space.\n    \n    # We need to ensure that if a bin is too small, it gets zero or very low priority.\n    \n    # Let's try a score that peaks when `bins_remain_cap` is just above `item`.\n    # Consider `f(x) = max(0, C - x)` where `x` is the remaining capacity, `C` is the bin capacity, `item` is the item size.\n    # This doesn't directly use `bins_remain_cap`.\n\n    # Let's focus on `bins_remain_cap`. We want to prioritize bins where `bins_remain_cap`\n    # is relatively small, but still greater than or equal to `item`.\n    \n    # A common heuristic for \"Almost Full Fit\" is to give priority proportional\n    # to the remaining capacity of the bin, but only for bins that can fit the item.\n    # This seems counter-intuitive to \"almost full\".\n\n    # Let's use a score that is high when `bins_remain_cap` is slightly larger than `item`.\n    # And rapidly drops off as `bins_remain_cap` increases beyond `item`.\n\n    # Consider the function: `priority = exp(-alpha * (bins_remain_cap - item))` for `bins_remain_cap >= item`.\n    # `alpha` controls how quickly priority drops. Larger `alpha` means stronger preference for tighter fits.\n    # If `bins_remain_cap - item` is 0, priority is `exp(0) = 1`.\n    # If `bins_remain_cap - item` is small and positive, priority is slightly less than 1.\n    # If `bins_remain_cap - item` is large, priority approaches 0.\n\n    # Let's set alpha = 1 for simplicity.\n    alpha = 1.0\n    \n    # Apply the scoring only to bins that can fit the item.\n    # For bins that cannot fit, their priority remains 0.\n    priorities[can_fit_mask] = np.exp(-alpha * (bins_remain_cap[can_fit_mask] - item))\n\n    # This prioritizes bins that are closer to fitting the item.\n    # The 'almost full' aspect comes from the fact that if a bin's remaining capacity is\n    # much larger than `item`, the priority will be significantly lower than for a bin\n    # where `bins_remain_cap` is just `item` or slightly more.\n\n    # To make it more \"almost full\" specifically, we might want to ensure\n    # that bins that are *already* very full (i.e., `bins_remain_cap` is small,\n    # but still fits `item`) get a bonus.\n    # This current formula `exp(-(rem_cap - item))` inherently gives higher scores\n    # to smaller `rem_cap` values (given they are >= item).\n\n    # Let's consider an example: item = 0.3\n    # Bin capacities: [0.5, 0.4, 1.0, 0.35, 0.2]\n    # can_fit_mask: [True, True, True, True, False]\n    # bins_remain_cap[can_fit_mask]: [0.5, 0.4, 1.0, 0.35]\n    # rem_cap - item: [0.2, 0.1, 0.7, 0.05]\n    # priorities[can_fit_mask]: [exp(-0.2), exp(-0.1), exp(-0.7), exp(-0.05)]\n    # Priorities: [0.818, 0.904, 0.496, 0.951]\n\n    # The highest priority goes to the bin with remaining capacity 0.35, which is closest to 0.3.\n    # This seems to align well with \"Almost Full Fit\" \u2013 it finds a bin that will be\n    # \"almost full\" after the item is packed, by choosing the bin that leaves the least\n    # amount of space.\n\n    return priorities",
    "response_id": 3,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 108.04820237218406,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response4.txt_stdout.txt",
    "code_path": "problem_iter1_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    Exact Fit First prioritizes bins that can exactly fit the item, and among those,\n    it prioritizes the one with the least remaining capacity (to leave larger gaps\n    for potentially larger items later).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Create a boolean mask for bins that can exactly fit the item\n    can_fit_exactly_mask = (bins_remain_cap == item)\n\n    # If there are bins that can exactly fit the item, prioritize them.\n    # Among exact fits, prefer those with less remaining capacity (this is implicitly\n    # handled by sorting if we were to use it, but here we assign a higher value).\n    # For simplicity and to ensure higher priority, we can assign a fixed high value\n    # or a value based on some secondary criterion if needed.\n    # Here, we assign a priority based on the negative of the remaining capacity\n    # for exact fits to break ties towards smaller remaining capacity.\n    if np.any(can_fit_exactly_mask):\n        # Assign a base high priority for exact fits\n        exact_fit_priority = 1000.0\n        # Adjust priority based on remaining capacity to break ties (smaller capacity is better)\n        priorities[can_fit_exactly_mask] = exact_fit_priority - bins_remain_cap[can_fit_exactly_mask]\n    else:\n        # If no exact fit, consider bins that can fit the item (i.e., remaining capacity >= item).\n        # The Exact Fit First strategy, in its purest form, only considers exact fits.\n        # However, in a practical implementation where an exact fit might not exist,\n        # one could extend it. The prompt implies returning scores for *all* bins.\n        # If we strictly adhere to \"Exact Fit First\", non-exact fits get zero priority.\n        # But to return scores for all bins, we'll assign a lower priority to bins\n        # that *can* fit, and a zero priority to bins that *cannot* fit.\n        # For bins that can fit but are not an exact fit, we might assign a negative score\n        # or a score lower than any exact fit.\n\n        can_fit_mask = (bins_remain_cap >= item)\n        # Assign a lower priority to bins that can fit but are not an exact fit.\n        # We can make this priority lower than the exact fit priority.\n        # A negative score or a score less than any possible exact fit score works.\n        # Let's assign a priority based on how \"close\" they are to an exact fit,\n        # or simply a low constant if no exact fit is found.\n        # A simple approach is to assign a constant lower priority.\n        # Here, we'll assign 0.0 to bins that can fit but aren't exact fits.\n        # The rationale for 0.0 is that they are considered after all exact fits.\n        priorities[can_fit_mask] = 0.0 # Or some other low value if needed\n\n    # Bins that cannot fit the item will retain their initial priority of 0.0\n\n    return priorities",
    "response_id": 4,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 25.26619429851844,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response5.txt_stdout.txt",
    "code_path": "problem_iter1_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit).\n\n    This heuristic prioritizes bins that have just enough remaining capacity to fit the item,\n    making it a \"Proximity Fit\" approach. Bins with very little remaining capacity or\n    excessive remaining capacity are penalized.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Consider only bins that can fit the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    # Calculate the \"distance\" to fitting. We want bins where bins_remain_cap - item is close to 0.\n    # A smaller \"distance\" means a better fit. We invert this distance to get a priority score.\n    # To avoid division by zero or very small numbers causing extremely high priorities,\n    # we add a small epsilon.\n    epsilon = 1e-6\n    distances = bins_remain_cap[suitable_bins_mask] - item\n    priorities[suitable_bins_mask] = 1.0 / (distances + epsilon)\n\n    # We can also slightly boost bins that are a \"perfect\" fit (distance == 0) to break ties\n    # or further encourage exact fits if desired.\n    perfect_fit_mask = (distances == 0)\n    priorities[suitable_bins_mask][perfect_fit_mask] *= 1.5 # Boost perfect fits\n\n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 72.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response6.txt_stdout.txt",
    "code_path": "problem_iter1_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score prioritizes bins that are closer to fitting the item perfectly,\n    using a sigmoid function to map the \"tightness\" of the fit to a priority score.\n    Bins that can fit the item (remaining capacity >= item size) are considered.\n    The score is higher for bins where (remaining_capacity - item_size) is closer to zero.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    possible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(possible_bins_mask):\n        # No bin can fit the item, return all zeros (or handle as an error/special case)\n        return priorities\n\n    # Calculate the \"gap\" for possible bins (how much space is left after placing the item)\n    # We want to minimize this gap, so a smaller gap is better.\n    gaps = bins_remain_cap[possible_bins_mask] - item\n\n    # We want to use a sigmoid function that maps smaller gaps to higher priorities.\n    # The standard sigmoid function outputs values between 0 and 1.\n    # sigmoid(x) = 1 / (1 + exp(-x))\n    # If x is large positive, sigmoid(x) is close to 1.\n    # If x is large negative, sigmoid(x) is close to 0.\n    # We want a higher score when the gap is small (close to 0).\n    # Let's transform the gap: a smaller gap should result in a larger positive value\n    # fed into the sigmoid.\n    # Consider `scaled_gap = -gaps / scale_factor`. As gap approaches 0, scaled_gap approaches 0.\n    # sigmoid(0) = 0.5. This doesn't quite give us the highest priority for a perfect fit.\n    #\n    # Let's reconsider the goal: prioritize bins that fit the item *tightly*.\n    # This means the remaining capacity is just slightly larger than the item size.\n    # So, (bins_remain_cap[i] - item) should be small and positive.\n    #\n    # A common way to use sigmoid for prioritization is to map the \"goodness\" of a\n    # characteristic to a score.\n    # Let's define \"tightness\" as how close `bins_remain_cap[i]` is to `item`.\n    # Specifically, for bins that can fit the item, we are interested in\n    # `bins_remain_cap[i] - item`.\n    #\n    # We want a function f(diff) where `diff = bins_remain_cap[i] - item` such that:\n    # - f(diff) is high when `diff` is small and positive.\n    # - f(diff) is lower when `diff` is large and positive.\n    # - f(diff) is 0 or very low when `diff` is negative (or item doesn't fit).\n    #\n    # Let's try `sigmoid(k * (max_capacity - bins_remain_cap[i]))`.\n    # If `bins_remain_cap[i]` is close to `item`, then `bins_remain_cap[i] - item` is small.\n    #\n    # Alternative approach: Model the \"cost\" of fitting.\n    # A perfect fit has zero cost. A loose fit has a cost.\n    # Consider the metric `bins_remain_cap[i] - item`.\n    # We want a high priority when this value is close to 0 (and positive).\n    #\n    # Let's use a sigmoid on the *inverse* of the gap.\n    # A small gap is a large inverse gap.\n    #\n    # Consider the difference `diff = bins_remain_cap[i] - item`.\n    # We want a high score when `diff` is small and positive.\n    #\n    # Let's map `diff` to a score using sigmoid:\n    # `sigmoid(A - B * diff)`:\n    # - If `diff` is small positive, `B * diff` is small positive. `A - B * diff` is large positive. Sigmoid is close to 1.\n    # - If `diff` is large positive, `B * diff` is large positive. `A - B * diff` is large negative. Sigmoid is close to 0.\n    #\n    # We need to choose parameters A and B appropriately.\n    # Let's set A to control the center of the sigmoid and B to control the steepness.\n    # A common approach is to center the sigmoid around 0.\n    #\n    # Let's define a score that peaks at 0 difference.\n    # We can use `sigmoid(slope * (optimal_diff - current_diff))`.\n    # `optimal_diff = 0`. So, `sigmoid(slope * (0 - (bins_remain_cap[i] - item)))`\n    # = `sigmoid(slope * (item - bins_remain_cap[i]))`\n    # = `sigmoid(slope * -(bins_remain_cap[i] - item))`\n    #\n    # For bins where `bins_remain_cap[i] < item`, this calculation is not directly applicable.\n    # We've already filtered these out.\n    # For bins where `bins_remain_cap[i] >= item`:\n    # Let `gap = bins_remain_cap[i] - item`. `gap >= 0`.\n    # The score is `sigmoid(-slope * gap)`.\n    # - If `gap = 0` (perfect fit), score = `sigmoid(0)` = 0.5.\n    # - If `gap` is small positive, `sigmoid` is slightly less than 0.5.\n    # - If `gap` is large positive, `sigmoid` is close to 0.\n    # This gives higher priority to bins with larger gaps, which is the opposite of what we want.\n    #\n    # We need a function that *decreases* as `gap` increases.\n    # So, let's use `1 - sigmoid(slope * gap)` or `sigmoid(-slope * gap)`.\n    #\n    # Let's rethink: we want to prioritize bins where `bins_remain_cap[i]` is CLOSEST to `item`.\n    # The difference `d = bins_remain_cap[i] - item`. We want `d` to be small and positive.\n    #\n    # Consider `1 / (1 + exp(-k * (value)))`\n    # If we want the score to be high when `bins_remain_cap[i]` is just above `item`.\n    #\n    # Let `ratio = item / bins_remain_cap[i]`. This is relevant if bin capacity is variable.\n    # Here, bin capacity is fixed, but remaining capacity changes.\n    #\n    # The \"Sigmoid Fit Score\" usually implies fitting the item as snugly as possible.\n    # This means minimizing `bins_remain_cap[i] - item` for `bins_remain_cap[i] >= item`.\n    #\n    # Let's use the negative of the gap as the input to the sigmoid, which will give\n    # values closer to 1 for smaller gaps.\n    # `score = sigmoid(k * (item - bins_remain_cap[i]))`\n    # This is equivalent to `sigmoid(k * -(bins_remain_cap[i] - item))`.\n    #\n    # Let's use `k = 1.0` for simplicity for now, and `sigmoid(x) = 1 / (1 + exp(-x))`.\n    #\n    # We want the score to be high for `bins_remain_cap[i]` just above `item`.\n    #\n    # Let's scale the gap to avoid numerical issues and control sensitivity.\n    # `scaled_gap = (bins_remain_cap[possible_bins_mask] - item) / max(1, bins_remain_cap[possible_bins_mask].max())`\n    # This makes the gap a value between 0 and 1 (if max_cap is 1).\n    #\n    # Consider `score = sigmoid(A - B * (bins_remain_cap[i] - item))`\n    # Let's center the sigmoid such that a difference of `delta` results in a score of 0.5.\n    # If we want a difference of 0 to be ideal, then `sigmoid(0) = 0.5`.\n    #\n    # A common sigmoid fit score in BPP aims to place the item in the bin\n    # where the remaining capacity is the smallest that can still fit the item.\n    # This is equivalent to minimizing `bins_remain_cap[i] - item` for `bins_remain_cap[i] >= item`.\n    #\n    # So, we want a function `f(gap)` where `f(gap)` is high for small `gap >= 0`.\n    #\n    # Let's use `f(gap) = 1 - sigmoid(k * gap)` or `f(gap) = sigmoid(-k * gap)`.\n    # With `sigmoid(x) = 1 / (1 + exp(-x))`:\n    # - `gap = 0`: `f(0) = sigmoid(0) = 0.5`. This is the peak value.\n    # - `gap = small_positive`: `f(small_positive) = sigmoid(-k * small_positive)` is slightly less than 0.5.\n    # - `gap = large_positive`: `f(large_positive) = sigmoid(-k * large_positive)` is close to 0.\n    #\n    # This means `sigmoid(-k * gap)` assigns a score that decreases as the gap increases.\n    # The highest score is 0.5 for a perfect fit.\n\n    # Let's define a \"steepness\" parameter for the sigmoid.\n    # A larger `steepness` will make the score drop faster as the gap increases.\n    steepness = 5.0  # Tune this parameter\n\n    # Calculate the argument for the sigmoid function\n    # We want to prioritize smaller gaps. So, `item - bins_remain_cap[i]` should be maximized (least negative).\n    # Or equivalently, `bins_remain_cap[i] - item` should be minimized.\n    # Using `sigmoid(-steepness * gap)` makes scores higher for smaller gaps.\n    sigmoid_arg = -steepness * gaps\n\n    # Apply the sigmoid function to get priorities\n    # We are only calculating for possible bins, so we need to put these values back.\n    priorities[possible_bins_mask] = 1 / (1 + np.exp(-sigmoid_arg))\n\n    # If `item` is very small, `bins_remain_cap[i] - item` is close to `bins_remain_cap[i]`.\n    # The sigmoid still works, prioritizing bins with less remaining capacity.\n\n    return priorities",
    "response_id": 6,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 87.56842503028855,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response7.txt_stdout.txt",
    "code_path": "problem_iter1_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Random Fit strategy.\n\n    In Random Fit, any bin that can accommodate the item is a candidate.\n    To implement a priority function, we assign a higher priority to bins that are\n    a \"tighter fit\" for the item, meaning they have just enough capacity for the item.\n    This encourages filling bins more completely. For bins that are not suitable,\n    the priority is zero.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    # For suitable bins, calculate priority as the inverse of the remaining capacity\n    # after placing the item. A smaller remaining capacity (tighter fit) gets a higher priority.\n    # We add a small epsilon to avoid division by zero if an item perfectly fills a bin.\n    suitable_bin_capacities = bins_remain_cap[suitable_bins_mask]\n    # A smaller remaining capacity after fitting means a better fit.\n    # To prioritize better fits, we can use the negative of the remaining capacity.\n    # Or, to make it a positive score, we can use 1 / (remaining_capacity + epsilon)\n    # or something like (max_capacity - item_size) / (current_remaining_capacity - item_size).\n    # Let's go with a simple approach: higher priority for bins with less excess capacity.\n    # This can be achieved by giving a score based on how close the remaining capacity\n    # is to the item size.\n\n    # Option 1: Prioritize bins that have the least excess capacity (closest fit)\n    # If a bin has remaining_capacity, and we add 'item', the new remaining_capacity is remaining_capacity - item.\n    # A tighter fit means remaining_capacity - item is small.\n    # So, higher priority for smaller (remaining_capacity - item).\n    # We can assign priority as - (remaining_capacity - item).\n    priorities[suitable_bins_mask] = - (suitable_bin_capacities - item)\n\n    # Option 2: Random Fit doesn't explicitly define a \"best\" bin,\n    # it picks *any* suitable bin randomly.\n    # If we need a priority function that leads to random selection among suitable bins:\n    # 1. Assign a uniform positive score to all suitable bins.\n    # 2. The selection process (outside this function) would then randomly pick\n    #    from the bins with the highest priority.\n    # This function, however, must *return* priorities, implying a deterministic choice.\n    # The \"Random Fit\" often refers to the *algorithm's decision rule* rather than\n    # a strict priority score for a heuristic.\n    # For this specific task of returning priorities, and given the prompt's wording\n    # \"The bin with the highest priority score will be selected for the item,\"\n    # we need to provide scores that guide selection.\n\n    # Let's refine Option 1 to be more aligned with a heuristic's goal of good packing.\n    # The spirit of \"Random Fit\" is often about not overthinking the choice if many are available.\n    # However, if we MUST assign priorities, a reasonable heuristic priority would be\n    # to favor bins that are *just big enough*, minimizing wasted space in that specific bin.\n    # This is captured by `bins_remain_cap - item` being small.\n    # So, negative of that value is a good proxy for priority.\n\n    # Ensure priorities are not positive if we are trying to find \"least\" of something,\n    # but here higher priority score means better.\n    # A smaller `bins_remain_cap - item` implies a better fit.\n    # Let's use `1.0 / (bins_remain_cap - item + epsilon)` to favor smaller gaps,\n    # making the gap as the basis of priority.\n    epsilon = 1e-9 # To avoid division by zero\n    priorities[suitable_bins_mask] = 1.0 / (suitable_bin_capacities - item + epsilon)\n\n\n    # Another interpretation for \"Random Fit priority\" could be:\n    # give a score to all suitable bins, but the actual selection might still be random among the top-scoring ones.\n    # Or, a simple priority: just 1 for all suitable bins.\n    # This would make them all equally prioritized, and the external selection mechanism would pick one randomly.\n    # Let's try that for a more literal \"Random Fit\" interpretation where the priority doesn't enforce a specific sub-strategy.\n\n    # Alternative approach: Assign a uniform positive priority to all suitable bins.\n    # This allows the external selection mechanism (if it exists) to pick randomly.\n    # If the selection mechanism is simply \"pick highest priority\", and multiple have the same\n    # highest priority, it implies a form of random tie-breaking.\n\n    # Let's provide a priority that encourages a slightly tighter fit, which is usually beneficial,\n    # while acknowledging the spirit of \"randomness\" means not being overly rigid.\n    # The inverse of the gap `(remaining_capacity - item)` works well for this.\n\n    # Re-implementing using a clearer approach that directly reflects favoring tighter fits:\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            # Prioritize bins that leave less remaining space after placing the item.\n            # A smaller value of `bins_remain_cap[i] - item` is better.\n            # We want a *higher* priority score for a *smaller* `bins_remain_cap[i] - item`.\n            # So, use the negative of this difference.\n            priorities[i] = - (bins_remain_cap[i] - item)\n            # Adding a small constant if you want all priorities to be positive,\n            # but the relative ordering is what matters.\n            # priorities[i] = 1.0 / (bins_remain_cap[i] - item + epsilon)\n\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 100.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response8.txt_stdout.txt",
    "code_path": "problem_iter1_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    The Epsilon-Greedy strategy balances exploration and exploitation.\n    - With probability epsilon, it explores by choosing a random bin.\n    - With probability 1-epsilon, it exploits by choosing the bin that offers the\n      best \"fit\" for the current item.\n\n    Here, \"best fit\" is defined as the bin with the smallest remaining capacity\n    that can still accommodate the item. This is a common heuristic for bin packing\n    (e.g., First Fit Decreasing, Best Fit).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    epsilon = 0.2  # Exploration rate\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Determine which bins can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, assign a very low priority to all\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Exploitation: Find the best fit bin(s)\n    # Best fit means the bin with the smallest remaining capacity that still fits the item.\n    # We calculate (remaining_capacity - item) and find the minimum of this difference\n    # among the bins that can fit the item.\n    potential_bins_remaining_cap = bins_remain_cap[can_fit_mask]\n    if len(potential_bins_remaining_cap) > 0:\n        differences = potential_bins_remaining_cap - item\n        min_diff = np.min(differences)\n\n        # Bins with the minimum difference get a high priority (exploitation)\n        # We can assign a base high priority, e.g., 1.0\n        best_fit_indices_in_mask = np.where(differences == min_diff)[0]\n        original_indices_of_best_fit = np.where(can_fit_mask)[0][best_fit_indices_in_mask]\n        priorities[original_indices_of_best_fit] = 1.0\n\n    # Exploration: Assign a smaller priority to some bins randomly\n    # Identify bins that are candidates for exploration (can fit the item)\n    candidate_indices_for_exploration = np.where(can_fit_mask)[0]\n\n    # If there are candidate bins, randomly pick some to give a slightly lower\n    # exploration priority. This ensures exploration doesn't always pick the best.\n    if len(candidate_indices_for_exploration) > 0:\n        num_to_explore = int(np.floor(epsilon * len(candidate_indices_for_exploration)))\n        if num_to_explore > 0:\n            # Choose which of the candidate bins to give an exploration priority\n            explore_indices = np.random.choice(candidate_indices_for_exploration, size=num_to_explore, replace=False)\n            # Assign a priority lower than the best fit, but still positive\n            # This exploration priority should be lower than the exploitation priority (1.0)\n            exploration_priority_value = 0.5\n            priorities[explore_indices] = exploration_priority_value\n\n    # Ensure that bins that cannot fit the item have a priority of 0 (or negative if preferred)\n    priorities[~can_fit_mask] = 0.0\n\n    # Normalize priorities to avoid issues with very large/small numbers,\n    # though for selection, relative values are more important.\n    # This step is optional but can be good practice if priorities are used in other contexts.\n    # If all priorities are 0 (no bin can fit), this will result in NaNs, so handle that.\n    if np.max(priorities) > 0:\n        priorities = priorities / np.max(priorities)\n    else:\n        # If no bin could fit, and priorities are all 0, this is handled.\n        pass\n\n    # Further refinement: Add a small random component to the \"best fit\" bins\n    # to make the greedy choice less deterministic if multiple bins are tied for best fit.\n    # This can be considered a micro-exploration within the exploitation phase.\n    best_fit_indices_refined = np.where(priorities == 1.0)[0]\n    if len(best_fit_indices_refined) > 0:\n        random_boost = np.random.rand(len(best_fit_indices_refined)) * 0.1 # Small boost\n        priorities[best_fit_indices_refined] += random_boost\n\n    # Re-normalize after the boost if needed\n    if np.max(priorities) > 0:\n        priorities = priorities / np.max(priorities)\n\n\n    return priorities",
    "response_id": 8,
    "obj": 3.9289988033506273,
    "cyclomatic_complexity": 8.0,
    "halstead": 247.1753118485642,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response9.txt_stdout.txt",
    "code_path": "problem_iter1_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.\n\n    The priority is calculated based on the remaining capacity of the bins.\n    Bins with more remaining capacity are generally preferred, but the softmax\n    function introduces a probabilistic preference, meaning even bins with less\n    capacity have a chance of being chosen, especially if the differences in\n    capacity are not large.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We want to prioritize bins that can fit the item.\n    # A higher remaining capacity is generally better, as it leaves more room for future items.\n    # The Softmax function allows us to convert these capacities into probabilities (priorities).\n\n    # Calculate a 'fit score' for each bin.\n    # We only consider bins that can actually fit the item.\n    # For bins that cannot fit, their priority will be 0.\n    fit_scores = np.zeros_like(bins_remain_cap)\n    can_fit_mask = bins_remain_cap >= item\n    fit_scores[can_fit_mask] = bins_remain_cap[can_fit_mask]\n\n    # Apply softmax to the fit scores.\n    # Softmax converts scores into probabilities, where higher scores get higher probabilities.\n    # Adding a small epsilon to prevent issues with all scores being zero or identical.\n    epsilon = 1e-9\n    try:\n        # Softmax: exp(score) / sum(exp(scores))\n        exp_scores = np.exp(fit_scores - np.max(fit_scores) + epsilon) # Stabilization for large values\n        priorities = exp_scores / np.sum(exp_scores)\n    except ZeroDivisionError:\n        # If all scores are effectively zero (e.g., no bins can fit the item), return uniform probabilities.\n        # In a real online BPP, this scenario would mean creating a new bin.\n        # Here, we'll just distribute the priority evenly among all bins as a fallback.\n        priorities = np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n\n    return priorities",
    "response_id": 9,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 2.0,
    "halstead": 57.110323830864054,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response10.txt_stdout.txt",
    "code_path": "problem_iter1_code10.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This priority function implements the First Fit Decreasing strategy.\n    It prioritizes bins that have just enough space for the item,\n    but also gives a small bonus to bins that have significantly more space\n    to encourage packing larger items into larger remaining capacities\n    for potential future efficiency gains (a form of lookahead).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate the difference between remaining capacity and item size\n    # A difference close to zero is good (tight fit)\n    tight_fit_score = -np.abs(bins_remain_cap - item)\n\n    # Calculate a bonus for bins with large remaining capacity\n    # This encourages using larger bins when a tight fit isn't available or preferred.\n    # We can normalize this or use a logarithmic scale to avoid\n    # extremely large values for very large capacities.\n    # Here, we use a simple division by a factor to reduce the impact.\n    # Add a small epsilon to avoid division by zero.\n    large_capacity_bonus = bins_remain_cap / (np.max(bins_remain_cap) + 1e-9) * 0.1\n\n    # Combine the scores. The tight fit has a stronger influence.\n    # Bins that can fit the item get a non-zero priority.\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = tight_fit_score[can_fit_mask] + large_capacity_bonus[can_fit_mask]\n\n    # Ensure that bins that cannot fit the item have a very low priority (e.g., negative infinity)\n    # or simply leave them as 0, as they won't be chosen anyway by a max operation.\n    # For clarity, we can explicitly set them to a very low value.\n    priorities[~can_fit_mask] = -np.inf\n\n    return priorities",
    "response_id": 10,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 101.95026032264605,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response11.txt_stdout.txt",
    "code_path": "problem_iter1_code11.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit.\n\n    The Best Fit strategy prioritizes bins that have just enough capacity to\n    accommodate the item, aiming to minimize wasted space. It also favors\n    bins that are already quite full, as these are less likely to be usable\n    for future larger items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Create a mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities to a very low value for bins that cannot fit the item\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # For bins that can fit the item, calculate priority\n    # We want to prioritize bins with the smallest remaining capacity that still fits the item.\n    # This is achieved by maximizing the \"negative difference\" between the remaining capacity\n    # and the item size. A smaller positive difference (closer to zero) gives a higher priority.\n    #\n    # Additionally, we want to give a slight preference to bins that are already relatively full\n    # (i.e., have a smaller remaining capacity even before fitting the item).\n    # This can be incorporated by adding a term that is inversely proportional to\n    # the remaining capacity (but only for bins that can fit the item).\n    # A simple way to do this is to add a small value related to 1 / bins_remain_cap.\n    # To avoid division by zero, we can add a small epsilon or handle it.\n    # A more numerically stable approach is to use the negative of the capacity itself,\n    # as maximizing -capacity is equivalent to minimizing capacity.\n\n    # Calculate the difference if the item is placed in the bin\n    # A smaller difference is better for Best Fit\n    difference = bins_remain_cap - item\n\n    # The core of Best Fit is to find the bin with the minimum difference (closest fit).\n    # We can use the negative of this difference to create a maximization problem.\n    # We also add a small penalty inversely related to the bin's current capacity.\n    # This encourages using bins that are already somewhat full.\n    # Using `np.exp(-bins_remain_cap)` is one way to give higher priority to fuller bins.\n    # We add a small epsilon to `bins_remain_cap` before division to prevent potential\n    # division by zero, although in a real scenario, bins with zero capacity should\n    # already be excluded or handled.\n\n    # To combine the \"closest fit\" and \"prefer fuller bins\" objectives:\n    # The primary goal is the closest fit. The secondary goal is to use fuller bins.\n    # We can prioritize bins with the smallest `difference`. Maximizing `-difference` achieves this.\n    # To incorporate the \"prefer fuller bins\", we can add a term that is larger for smaller `bins_remain_cap`.\n    # `1 / (bins_remain_cap[can_fit_mask] + 1e-9)` would work, but it might be sensitive to scaling.\n    # A simpler heuristic that achieves a similar effect is to use the negative of the remaining capacity.\n    # Maximizing `-bins_remain_cap` is equivalent to minimizing `bins_remain_cap`.\n\n    # So, the priority can be a combination of `-difference` and `-bins_remain_cap`.\n    # We can weight them. Let's prioritize the closest fit (smallest difference) more heavily.\n    # Priority = -difference - weight * bins_remain_cap\n    # Maximizing this means minimizing difference and minimizing bins_remain_cap.\n\n    # A common heuristic for Best Fit is to simply prioritize based on the *smallest* remaining capacity\n    # that *can* fit the item. So we want to maximize the negative of the remaining capacity\n    # among those that fit.\n\n    # Let's refine:\n    # We want to select the bin `j` such that `bins_remain_cap[j] - item` is minimized,\n    # subject to `bins_remain_cap[j] >= item`.\n    # This means we want to maximize `- (bins_remain_cap[j] - item)` among the valid bins.\n\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # To add the secondary heuristic of preferring fuller bins among the \"best fits\",\n    # we can consider bins with smaller remaining capacity first.\n    # If multiple bins offer the same \"best fit\" (same difference), the one\n    # that is already more full (smaller `bins_remain_cap`) should be preferred.\n    # We can achieve this by adding a term that is inversely related to `bins_remain_cap`\n    # to the priority. However, this can be tricky.\n\n    # A simpler interpretation of Best Fit often just prioritizes the minimum remaining capacity\n    # among the fitting bins.\n    # Let's assign a priority of `-bins_remain_cap[j]` for fitting bins, then selecting\n    # the maximum. This directly picks the fullest among the fitting bins.\n    # However, the prompt implies a \"best fit\" which is minimal waste, i.e., minimal `bins_remain_cap[j] - item`.\n\n    # Let's stick to the core Best Fit idea: minimize remaining capacity after placement.\n    # So we maximize `-(bins_remain_cap[j] - item)` for fitting bins.\n    # This implicitly favors fuller bins among those that can fit.\n\n    # Consider bins that can fit the item.\n    # The priority is how \"tightly\" the item fits. The tighter the fit (smaller remaining capacity),\n    # the higher the priority.\n    # So, priority should be high when `bins_remain_cap[j] - item` is small.\n    # Maximizing `-(bins_remain_cap[j] - item)` achieves this.\n\n    # If two bins have the same minimal waste, which one to prefer?\n    # The Best Fit strategy doesn't strictly define this. A common tie-breaker\n    # is to prefer the bin with the smaller overall capacity (i.e., the one that was\n    # already fuller before adding the item).\n    # We can incorporate this by adding a secondary term to the priority.\n    # Let's add a term proportional to `-bins_remain_cap[j]` (to prefer smaller capacities).\n\n    # A common way to implement this is:\n    # 1. Calculate the \"fit score\" for each bin: `fit_score = bins_remain_cap[j] - item`. We want to minimize this.\n    # 2. For bins where `fit_score >= 0`, we can assign a priority.\n    # To make it a maximization problem, we can use `-fit_score`.\n    # To incorporate the \"prefer fuller bins\" tie-breaker, we can subtract a small amount\n    # proportional to the bin's original capacity.\n\n    # Let's try a simple priority that directly encodes Best Fit's goal:\n    # For fitting bins, prioritize those where `bins_remain_cap` is minimized.\n    # So, we want to maximize `-bins_remain_cap[j]` among `bins_remain_cap[j] >= item`.\n    # This isn't strictly \"best fit\" in terms of minimal *waste*, but it's a common interpretation for online scenarios.\n\n    # Let's refine again for \"Best Fit\" in the sense of minimizing waste (`remaining_capacity - item`).\n    # The priority should be high for small `remaining_capacity - item`.\n    # So, the priority is `- (remaining_capacity - item)`.\n\n    # If there are multiple bins with the same minimum waste, how do we choose?\n    # The problem statement doesn't specify a tie-breaker. A good heuristic would be to pick\n    # the bin that is already most full (smallest `remaining_capacity` overall).\n    # This can be achieved by adding a term proportional to `-remaining_capacity`.\n\n    # Combined priority: `- (bins_remain_cap[j] - item) - epsilon * bins_remain_cap[j]`\n    # where epsilon is a small positive value.\n    # Maximizing this priority will prioritize minimal waste, and then prioritize smaller capacity bins.\n\n    # Using a small epsilon can be numerically tricky. A safer approach is to assign priorities\n    # in layers or use lexicographical sorting.\n    # For this function, let's prioritize the minimal difference.\n    # To favor fuller bins as a secondary criterion, we can make the priority more negative\n    # for larger remaining capacities.\n\n    # Priority = - (difference) - (some value that increases with bins_remain_cap)\n    # Let's use `- bins_remain_cap[j]` itself as the penalty.\n\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item) - bins_remain_cap[can_fit_mask]\n\n    # This heuristic prioritizes bins that have a small remaining capacity *after* adding the item\n    # (minimizing `bins_remain_cap - item`), and among those, it prioritizes bins that\n    # *started* with a smaller remaining capacity (`bins_remain_cap`).\n\n    return priorities",
    "response_id": 11,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 75.28421251514429,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response12.txt_stdout.txt",
    "code_path": "problem_iter1_code12.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    The Worst Fit strategy prioritizes bins that have the most remaining capacity,\n    as this leaves more space for potentially larger future items. If an item\n    does not fit in a bin, its priority is set to negative infinity to exclude it.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities with a very low value to represent infeasible bins\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    # For bins where the item fits, the priority is the remaining capacity (Worst Fit)\n    priorities[fit_mask] = bins_remain_cap[fit_mask]\n\n    return priorities",
    "response_id": 12,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 1.0,
    "halstead": 11.60964047443681,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response13.txt_stdout.txt",
    "code_path": "problem_iter1_code13.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    The Almost Full Fit strategy prioritizes bins that are \"almost full\" but can still\n    accommodate the current item. This aims to leave more space in other bins for\n    potentially larger future items, or to consolidate items into fewer bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to a very low value (negative infinity) for bins that cannot fit the item.\n    # This ensures they are never chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate priority.\n    # The strategy prioritizes bins with remaining capacity *closest* to the item size\n    # (but still greater than or equal to it). This is achieved by maximizing\n    # the remaining capacity minus the item size.\n    # A smaller remaining capacity after placing the item means the bin was \"more full\"\n    # before placing the item.\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask] - item\n\n    return priorities",
    "response_id": 13,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 1.0,
    "halstead": 22.458839376460833,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response14.txt_stdout.txt",
    "code_path": "problem_iter1_code14.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    Exact Fit First prioritizes bins that can perfectly fit the item.\n    Among bins that can perfectly fit, it prefers those with less remaining capacity\n    (to encourage tighter packing).\n    Bins that cannot fit the item are given a low priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can perfectly fit the item\n    perfect_fit_mask = (bins_remain_cap == item)\n    if np.any(perfect_fit_mask):\n        # For perfect fits, the priority is the negative of remaining capacity (to pick the smallest capacity bin that fits perfectly)\n        # We multiply by 1000 to give higher precedence to perfect fits over non-perfect fits\n        priorities[perfect_fit_mask] = 1000 - bins_remain_cap[perfect_fit_mask]\n    else:\n        # If no perfect fit, consider bins that can fit the item\n        can_fit_mask = (bins_remain_cap >= item)\n        if np.any(can_fit_mask):\n            # For bins that can fit, prioritize those with the least remaining capacity\n            # This is a common heuristic in BPP variants to avoid leaving small, unusable gaps.\n            # The priority is (1 / remaining_capacity) which is higher for smaller capacities.\n            # Add a small epsilon to avoid division by zero if capacity is 0, though `can_fit_mask` should prevent this.\n            priorities[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] + 1e-9)\n        # Bins that cannot fit the item will have a priority of 0 by default.\n\n    return priorities",
    "response_id": 14,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 55.506595772116384,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response15.txt_stdout.txt",
    "code_path": "problem_iter1_code15.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit) strategy.\n\n    The strategy prioritizes bins that are \"closer\" to fitting the item.\n    This is achieved by assigning higher priority to bins with remaining capacity\n    that is just enough (or slightly more) to accommodate the item.\n    Bins with significantly larger remaining capacity are given lower priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Only consider bins that can actually fit the item\n    possible_bins_mask = bins_remain_cap >= item\n    \n    # Calculate the \"gap\" or \"wasted space\" if the item is placed in a bin\n    # A smaller gap is preferred.\n    gaps = bins_remain_cap[possible_bins_mask] - item\n\n    # The priority is inversely proportional to the gap.\n    # To avoid division by zero if a bin fits the item perfectly (gap is 0),\n    # we add a small epsilon.\n    epsilon = 1e-9\n    priorities = 1.0 / (gaps + epsilon)\n\n    # Initialize the result array with zeros\n    result_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Place the calculated priorities into the correct positions in the result array\n    result_priorities[possible_bins_mask] = priorities\n\n    return result_priorities",
    "response_id": 15,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 41.51317942364757,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response16.txt_stdout.txt",
    "code_path": "problem_iter1_code16.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a Sigmoid Fit Score strategy.\n    The strategy prioritizes bins that are \"close\" to fitting the item perfectly,\n    using a sigmoid function to smooth the transition.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the \"difference\" for each bin: remaining capacity - item size\n    # A negative difference means the item doesn't fit.\n    differences = bins_remain_cap - item\n\n    # We want to prioritize bins where the difference is close to zero (perfect fit).\n    # A large positive difference (bin is much larger than item) is less desirable\n    # than a small positive difference.\n    # A negative difference (item doesn't fit) should have the lowest priority.\n\n    # Sigmoid function to map differences to a priority score between 0 and 1.\n    # We want the peak of the sigmoid to be at a difference of 0.\n    # A common sigmoid is 1 / (1 + exp(-x)). This peaks at x=0.\n    # We will use exp(-(difference)^2) or a variation to achieve a similar effect,\n    # as a simple sigmoid might not penalize items that *barely* don't fit enough.\n    # Let's try a Gaussian-like approach or a shifted sigmoid.\n\n    # Option 1: Gaussian-like penalty for deviation from perfect fit.\n    # We want the highest score when difference is 0, and scores decrease\n    # as the absolute difference increases.\n    # Gaussian function: exp(- (x - mu)^2 / (2 * sigma^2))\n    # Here, mu = 0 (perfect fit) and we can adjust sigma.\n    # Larger sigma means more tolerance for non-perfect fits.\n    # We need to handle cases where item doesn't fit (difference < 0).\n    # Let's only apply the sigmoid to bins where the item fits (differences >= 0).\n    # For bins where the item doesn't fit, assign a very low priority (e.g., 0).\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fit_mask = differences >= 0\n\n    # For bins where the item fits, calculate priority\n    if np.any(fit_mask):\n        fitting_differences = differences[fit_mask]\n\n        # Using exp(-(difference)^2 / sigma^2) as a base.\n        # A smaller sigma makes the function more \"peaky\" around 0 difference.\n        # A larger sigma makes it flatter, giving higher priority to more bins.\n        # Let's choose sigma to control how \"close\" the fit needs to be.\n        # For bin packing, we often prefer fits that leave minimal remaining capacity.\n        # So, a small positive difference is good.\n        # Let's consider the inverse of the difference for positive values,\n        # and a sigmoid-like behavior.\n\n        # Strategy: Prioritize bins where remaining_capacity is slightly larger than item.\n        # Let 'd' be the remaining capacity. We want to maximize f(d, item)\n        # where f peaks when d - item is small and positive.\n        # Consider 'gap' = bins_remain_cap - item.\n        # We want to maximize a function g(gap) where g(0) is high, and g(small_positive) is high,\n        # and g(large_positive) is lower, and g(negative) is 0.\n\n        # Sigmoid function centered around 0: 1 / (1 + exp(-x))\n        # We want to shift and scale this.\n        # Let's try sigmoid(x) = 1 / (1 + exp(-k*x)) where k > 0.\n        # If we want peak at 0, then sigmoid(0) = 0.5.\n        # We want the highest score for smallest positive difference.\n        # Consider `score = sigmoid(a - b * difference)` where we want to tune 'a' and 'b'.\n        # Alternatively, we can use a function that directly maps small positive differences to high scores.\n\n        # Let's use a function that is high for small positive 'differences'\n        # and decreases as 'differences' grow, and is zero for negative 'differences'.\n        # A scaled and shifted sigmoid: `sigmoid_scaled(x) = 1 / (1 + exp(-k * (x - shift)))`\n        # We want the highest value when difference is close to 0.\n\n        # Consider a modified sigmoid:\n        # If `bins_remain_cap[i] >= item`, let `gain = bins_remain_cap[i] - item`\n        # We want to prioritize small 'gain'.\n        # A function like `1 / (1 + gain)` would work but isn't sigmoid.\n        # Let's use a sigmoid on the \"lack of perfect fit\": `penalty = max(0, bins_remain_cap[i] - item)`\n        # Higher penalty is worse.\n        # A sigmoid mapping penalty to priority: `priority = 1 - sigmoid(penalty_strength * penalty)`\n        # This gives low priority for high penalties and high priority for low penalties.\n\n        # Let's define a sigmoid function `sig(x) = 1 / (1 + np.exp(-x))`\n        # We want to map `bins_remain_cap[i]` to a priority for item `item`.\n        # Let's focus on the `bins_remain_cap[i] - item` which is the \"slack\" or \"gap\".\n        # If slack < 0, priority = 0.\n        # If slack >= 0, we want high priority for small slack.\n\n        # Modified sigmoid approach:\n        # `priority = sigmoid(slope * (target_slack - slack))`\n        # Where `target_slack` is the ideal slack (e.g., 0 for a perfect fit).\n        # Let `slack = bins_remain_cap - item`\n        # `priority = 1 / (1 + np.exp(-(slope * (0 - slack))))`\n        # `priority = 1 / (1 + np.exp(-slope * slack))`\n        # If slack = 0, priority = 0.5.\n        # If slack is small positive, exp(-slope*slack) is < 1, priority > 0.5.\n        # If slack is large positive, exp(-slope*slack) is near 0, priority near 1. This is NOT what we want.\n        # We want highest priority for smallest POSITIVE slack.\n\n        # Let's re-evaluate: we want to maximize priority when `bins_remain_cap[i]` is slightly larger than `item`.\n        # Consider the value `k = bins_remain_cap[i] / item` if `item > 0`.\n        # We want `k` to be close to 1, but slightly greater than 1.\n        # This means `bins_remain_cap[i]` should be slightly larger than `item`.\n\n        # Let's consider the \"fit quality\" directly.\n        # For `bins_remain_cap[i] >= item`:\n        # We want a score that is high when `bins_remain_cap[i]` is just above `item`.\n        # A function like `exp(-(bins_remain_cap[i] - item) / scale)` for small positive differences?\n        # Or, transform the difference `d = bins_remain_cap[i] - item`.\n        # We want to maximize `f(d)` for `d >= 0`.\n        # `f(d) = exp(-d^2 / sigma^2)` is a Gaussian peak at `d=0`.\n\n        # Let's try a sigmoid that peaks at a small positive value, or is monotonically decreasing from 0.\n        # Consider `1 / (1 + exp(k * (bins_remain_cap[i] - item)))`.\n        # If `bins_remain_cap[i] = item`, the exponent is 0, sigmoid is 0.5.\n        # If `bins_remain_cap[i] > item` (positive difference), exponent is positive, sigmoid < 0.5. This is decreasing priority with increasing bin capacity.\n        # If `bins_remain_cap[i] < item` (negative difference), exponent is negative, sigmoid > 0.5. This would prioritize bins that *don't* fit.\n\n        # Let's invert the sigmoid and shift it.\n        # We want a function that is high for `bins_remain_cap[i]` slightly > `item`.\n        # Let `target_capacity = item * (1 + epsilon)` for some small epsilon.\n        # Prioritize bins closest to this `target_capacity`.\n\n        # Simpler approach: Focus on the \"best fit\" concept.\n        # Among bins where the item fits, choose the one with the smallest remaining capacity.\n        # This is the \"Best Fit\" heuristic. The request is for \"Sigmoid Fit Score\".\n\n        # Let's map the remaining capacity `r` to a priority `P(r)`.\n        # We want `P(r)` to be high when `r` is just above `item`.\n        # Consider the difference `d = r - item`.\n        # We want a high score for `d` near 0.\n        # Let's use `score = sigmoid_like(d)`.\n        # A function like `exp(-d)` would work for `d >= 0`, but isn't sigmoid.\n        # A Gaussian-like function `exp(-(d/sigma)**2)` for `d >= 0` would be good.\n\n        # Let's use a sigmoid that maps `bins_remain_cap` to a score, and then\n        # penalize bins where `bins_remain_cap < item`.\n        #\n        # We can use `sigmoid(x) = 1 / (1 + exp(-x))`.\n        # If we want a higher score for smaller remaining capacities (closer to item),\n        # we can consider `priority = sigmoid(scale * (max_capacity - bins_remain_cap))`.\n        # This prioritizes bins with less capacity, but still requires them to fit.\n        #\n        # Let's try to model the \"desirability\" as a function of `bins_remain_cap[i]` relative to `item`.\n        # The ideal bin has `bins_remain_cap[i] = item`.\n        # The second best would have `bins_remain_cap[i] = item + delta`, where delta is small.\n        # So we want to be close to `item`, but greater than or equal to `item`.\n        #\n        # Let's define a target capacity `C_target = item`.\n        # The \"closeness\" metric is `abs(bins_remain_cap[i] - C_target)`.\n        # We want to prioritize small positive differences.\n\n        # Let's use a scaled sigmoid function that peaks when `bins_remain_cap[i] - item` is close to 0.\n        # `priority = sigmoid(a * (b - (bins_remain_cap[i] - item)))`\n        # Let `b` be the \"ideal gap\" we want, maybe 0 or a very small positive number.\n        # If we set `b = 0` and `a > 0`, we get `sigmoid(a * (-slack)) = sigmoid(-a * slack)`.\n        # This peaks when `slack` is small and positive.\n        # Let `slack = bins_remain_cap[i] - item`.\n        # `priority = 1 / (1 + exp(-a * slack))`\n        # If `slack = 0`, priority = 0.5.\n        # If `slack = epsilon` (small positive), `exp(-a*epsilon)` is slightly less than 1, so priority is > 0.5.\n        # If `slack = large_positive`, `exp(-a*slack)` is near 0, priority is near 1. This is backwards.\n\n        # We need the sigmoid to have its \"sweet spot\" in the range `bins_remain_cap[i] >= item`.\n        # Let's consider the transformed value `x = bins_remain_cap[i] - item`.\n        # We want a function that is high for `x` near 0 (small positive), and decreases as `x` increases.\n        # Let's try a sigmoid that is high for small values.\n        # Consider `1 / (1 + exp(-k * (some_reference - x)))`.\n        # If `some_reference` is a small positive value, say `epsilon`.\n        # `1 / (1 + exp(-k * (epsilon - (bins_remain_cap[i] - item))))`\n        # `1 / (1 + exp(-k * (epsilon - bins_remain_cap[i] + item)))`\n        # Let `y = epsilon - bins_remain_cap[i] + item`.\n        # `1 / (1 + exp(-k * y))`\n        # When `bins_remain_cap[i] = item + epsilon`, `y = 0`, priority = 0.5.\n        # When `bins_remain_cap[i] = item`, `y = epsilon`, priority > 0.5.\n        # When `bins_remain_cap[i] = item + 2*epsilon`, `y = -epsilon`, priority < 0.5.\n\n        # This suggests that by tuning `epsilon`, we can shift the peak.\n        # Let's set `epsilon` to be a small positive value. This makes the highest priority for bins\n        # that are slightly larger than the item.\n        # Let `epsilon = 0.01` (or a small fraction of the bin capacity/item size).\n\n        epsilon = 0.05  # Small margin to prefer bins that are slightly larger\n        # Scale to control the steepness of the sigmoid. Larger scale -> sharper peak.\n        scale = 50\n\n        # Calculate the input to the sigmoid. We want to center the sigmoid\n        # around a point where `bins_remain_cap` is `item + epsilon`.\n        # So, `input_to_sigmoid = scale * ( (item + epsilon) - bins_remain_cap[i] )`\n        # `priority = 1 / (1 + exp(-input_to_sigmoid))`\n\n        sig_input = scale * ((item + epsilon) - bins_remain_cap)\n\n        # Apply sigmoid only to bins where the item fits.\n        # For bins where item does not fit, priority is 0.\n        priorities[fit_mask] = 1 / (1 + np.exp(-sig_input[fit_mask]))\n\n    return priorities",
    "response_id": 16,
    "obj": 4.038691663342641,
    "cyclomatic_complexity": 2.0,
    "halstead": 95.90827503317318,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response17.txt_stdout.txt",
    "code_path": "problem_iter1_code17.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Random Fit strategy.\n\n    In Random Fit, we prioritize bins that can fit the item. Among those that can fit,\n    we introduce a random element to avoid deterministic behavior that might lead\n    to suboptimal packing in the long run. Specifically, we assign a higher priority\n    to bins that leave less remaining space after fitting the item, as this aligns\n    with the First Fit Decreasing or Best Fit Decreasing heuristics in their spirit,\n    but here applied online and with randomness.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate a priority score.\n    # We want to prioritize bins that, after packing, will have less remaining capacity.\n    # This encourages tighter packing.\n    # We add a small random component to introduce variation and break ties.\n    if np.any(can_fit_mask):\n        remaining_capacities_if_fit = bins_remain_cap[can_fit_mask] - item\n        # A lower remaining capacity is better (tighter fit). So we want to assign higher priority\n        # to smaller remaining capacities. Inverting them works for higher priority.\n        # A simple inversion might lead to very large numbers if remaining capacity is close to zero.\n        # A common approach is to use 1/(remaining_capacity + epsilon) or a large constant - remaining_capacity.\n        # Let's use a large constant minus the remaining capacity.\n        # The range of remaining capacities is [0, max_bin_capacity - item].\n        # Let's assume max_bin_capacity is some value, say 1.0 for normalized problems.\n        # So remaining capacity is in [0, 1-item].\n        # We want to map this to a high priority.\n        # A good mapping could be (max_possible_remaining_capacity - actual_remaining_capacity)\n        # where max_possible_remaining_capacity is roughly max(bins_remain_cap) - item, or a fixed upper bound.\n        # For simplicity, let's use a fixed large number.\n        max_priority_base = 1000.0\n        priorities[can_fit_mask] = max_priority_base - remaining_capacities_if_fit\n\n        # Add a small random jitter to break ties and introduce randomness\n        # The jitter should be small enough not to override the deterministic preference\n        # but large enough to create variation.\n        random_jitter = np.random.rand(np.sum(can_fit_mask)) * 0.1 # Jitter between 0 and 0.1\n        priorities[can_fit_mask] += random_jitter\n    else:\n        # If no bin can fit the item, all priorities remain 0. This implies a new bin is needed.\n        pass\n\n    return priorities",
    "response_id": 17,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 55.506595772116384,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response18.txt_stdout.txt",
    "code_path": "problem_iter1_code18.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    This heuristic prioritizes bins that are a \"tight fit\" for the item (i.e.,\n    have remaining capacity close to the item size) but also explores other\n    options to avoid getting stuck in local optima.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of choosing a random bin\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate a \"tight fit\" score for bins that can accommodate the item\n    # Higher score for bins where remaining capacity is closer to the item size\n    tight_fit_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins_mask = bins_remain_cap >= item\n    if np.any(valid_bins_mask):\n        # Calculate the absolute difference between remaining capacity and item size\n        diff = bins_remain_cap[valid_bins_mask] - item\n        # Invert the difference so smaller differences (tighter fits) get higher scores\n        # Add a small epsilon to avoid division by zero if remaining capacity equals item size\n        tight_fit_scores[valid_bins_mask] = 1.0 / (np.abs(diff) + 1e-9)\n        # Normalize scores to be between 0 and 1\n        if np.max(tight_fit_scores) > 0:\n            tight_fit_scores /= np.max(tight_fit_scores)\n        else:\n            tight_fit_scores[valid_bins_mask] = 1.0 # All remaining valid bins are perfect fits\n\n    # Apply Epsilon-Greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Assign equal probability to all bins\n        priorities = np.ones(num_bins) / num_bins\n    else:\n        # Exploit: Prioritize bins with tight fit scores, but still consider others\n        # A simple way to combine is to add a small random noise to tight fit scores\n        # and also give a baseline probability to all bins.\n        priorities = tight_fit_scores + np.random.rand(num_bins) * 0.1\n        # Ensure that bins that cannot fit the item have a very low priority\n        priorities[~valid_bins_mask] = -1.0\n\n    # Normalize priorities to sum to 1 (optional but good practice for distribution)\n    # However, for a direct selection heuristic, absolute values are more important.\n    # We will simply return the computed priorities.\n    return priorities",
    "response_id": 18,
    "obj": 4.01874750698045,
    "cyclomatic_complexity": 4.0,
    "halstead": 163.45006734995852,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    The priority is calculated based on how well an item fits into a bin,\n    favoring bins that leave minimal remaining capacity after packing the item,\n    while also considering bins that have more remaining capacity. The softmax\n    function is used to convert these 'fitness' scores into probabilities,\n    where bins with higher 'fitness' (i.e., better fits) get higher priorities.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the 'fitness' of each bin for the current item.\n    # A good fit is one where the remaining capacity is small after adding the item.\n    # We only consider bins that can actually fit the item.\n    # For bins that cannot fit the item, we assign a very low 'fitness' (-infinity)\n    # so they get a priority of 0 after softmax.\n    fitness_scores = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins that can fit, a higher priority is given to bins where\n    # the remaining capacity after adding the item is smaller.\n    # We add a small epsilon to avoid division by zero if remaining capacity is 0.\n    # The score is inversely proportional to the remaining capacity after packing.\n    # Adding the item size to the capacity and then taking the reciprocal might\n    # be a simple way to represent \"how much space is left\". A smaller value\n    # is better. We can also consider the proportion of space used.\n    \n    # Let's try a simple approach: higher priority for bins that leave *less*\n    # remaining capacity. This is the \"best fit\" idea.\n    # remaining_after_packing = bins_remain_cap[can_fit_mask] - item\n    # To convert to a score where larger is better, we can take the negative of\n    # remaining capacity, or a function like exp(alpha * (-remaining_capacity)).\n    \n    # A common strategy for softmax-based selection is to have scores that\n    # represent desirability. For bin packing, fitting an item into a bin\n    # without leaving too much wasted space is desirable.\n    \n    # Let's define a score that is high for bins that have just enough space\n    # and lower for bins with much more space.\n    # A simple score could be 1 / (bins_remain_cap - item + epsilon)\n    # or simply the negative of the remaining capacity: -(bins_remain_cap - item)\n    # Let's try a score that represents how much capacity is *used* relative to available space.\n    # For bins that can fit: score = item / bins_remain_cap (if we want to fill bins)\n    # Or, we want to minimize wasted space, so minimize (bins_remain_cap - item).\n    # For softmax, we want higher values for better bins.\n    # So, we can use - (bins_remain_cap - item) or (item - bins_remain_cap)\n    # Let's use a score that is the negative of the leftover space.\n    # Lower (more negative) values mean more leftover space, so less desirable.\n    # Thus, a bin with *less* leftover space is more desirable.\n    # For example, if item=3, bins_remain_cap=[10, 5, 4].\n    # Bin 1: remaining=7, score=-7\n    # Bin 2: remaining=2, score=-2\n    # Bin 3: remaining=1, score=-1\n    # Softmax of [-7, -2, -1] will give higher probability to -1.\n    \n    # To avoid issues with exactly zero remaining capacity if we were to use reciprocal,\n    # and to ensure that bins that are nearly full get higher scores, we can use\n    # the negative of the remaining capacity after packing.\n    \n    # If remaining capacity is R, and item is I, new remaining capacity is R-I.\n    # We want to prioritize bins where R-I is small.\n    # So, we can assign a score of -(R-I) or equivalently I-R.\n    \n    # For bins that can fit:\n    effective_remaining_caps = bins_remain_cap[can_fit_mask] - item\n    # We want smaller effective_remaining_caps to be better.\n    # For softmax, higher values are better. So, we invert the desirability.\n    # A simple inversion is 1 / (effective_remaining_caps + epsilon) or\n    # -(effective_remaining_caps)\n    \n    # Let's use -(effective_remaining_caps). This means bins with less remaining\n    # space (after packing) get higher scores.\n    # Example: bins_remain_cap = [10, 5, 4], item = 3\n    # Bin 1: can fit, 10-3=7, score = -7\n    # Bin 2: can fit, 5-3=2, score = -2\n    # Bin 3: can fit, 4-3=1, score = -1\n    # Softmax of [-7, -2, -1] would give higher weights to the bins with smaller remaining capacity.\n    \n    scores = -(bins_remain_cap[can_fit_mask] - item)\n    fitness_scores[can_fit_mask] = scores\n\n    # Apply softmax function to get probabilities (priorities)\n    # Ensure numerical stability for softmax by subtracting the maximum score\n    if np.all(fitness_scores == -np.inf): # No bins can fit the item\n        return np.zeros_like(bins_remain_cap)\n        \n    # Subtracting the max score before exponentiation for numerical stability.\n    # The relative differences in scores are preserved.\n    max_score = np.max(fitness_scores)\n    exp_scores = np.exp(fitness_scores - max_score)\n    \n    # Avoid division by zero if all exp_scores are effectively zero\n    sum_exp_scores = np.sum(exp_scores)\n    if sum_exp_scores == 0:\n        # If all scores are very negative, resulting in near-zero exponentials,\n        # distribute probability equally among bins that *can* fit.\n        # This case is unlikely with typical capacities and item sizes.\n        # However, for robustness, if sum is zero, we can return uniform probabilities\n        # for fitting bins, and zero for non-fitting bins.\n        priorities = np.zeros_like(bins_remain_cap)\n        num_fitting_bins = np.sum(can_fit_mask)\n        if num_fitting_bins > 0:\n            priorities[can_fit_mask] = 1.0 / num_fitting_bins\n        return priorities\n        \n    priorities = exp_scores / sum_exp_scores\n\n    return priorities",
    "response_id": 19,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 142.62362713128297,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response20.txt_stdout.txt",
    "code_path": "problem_iter1_code20.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a First Fit Decreasing-like strategy.\n\n    The strategy prioritizes bins that have just enough remaining capacity to fit the item,\n    aiming to leave larger remaining capacities in other bins for potentially larger future items.\n    If multiple bins have just enough capacity, the one that fits the item most snugly is preferred.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Find bins that can fit the item\n    fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate a priority score.\n    # We want to prioritize bins that have *just enough* capacity.\n    # The difference (bins_remain_cap - item) represents the 'slack' or remaining capacity\n    # after fitting the item. A smaller slack is better, as it means a tighter fit.\n    # We use 1.0 / (difference + epsilon) to give higher priority to smaller differences.\n    # Adding a small epsilon (e.g., 1e-9) prevents division by zero if a bin has exactly\n    # the remaining capacity as the item size.\n    # A large negative number is assigned to bins that cannot fit the item.\n\n    epsilon = 1e-9\n    priorities[fit_mask] = 1.0 / (bins_remain_cap[fit_mask] - item + epsilon)\n\n    # Assign a very low priority to bins that cannot fit the item\n    priorities[~fit_mask] = -np.inf\n\n    return priorities",
    "response_id": 20,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 35, in priority_v2\nOverflowError: cannot convert float infinity to integer\n1\n62.5102495297363\n"
  },
  {
    "stdout_filepath": "problem_iter1_response21.txt_stdout.txt",
    "code_path": "problem_iter1_code21.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit.\n\n    The Best Fit strategy prioritizes bins that have just enough capacity to hold the item.\n    This is achieved by calculating the difference between the bin's remaining capacity\n    and the item's size. Smaller differences indicate a better fit.\n    We also want to avoid bins that are too small for the item, so we assign a very low\n    priority to such bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to a very low value (effectively making them ineligible)\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can actually accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    # For suitable bins, calculate the \"waste\" if the item is placed in it.\n    # The best fit is the one that minimizes this waste (remaining_capacity - item).\n    # We want to maximize the negative waste, so smaller waste means higher priority.\n    waste = bins_remain_cap[suitable_bins_mask] - item\n    priorities[suitable_bins_mask] = -waste\n\n    return priorities",
    "response_id": 21,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 30.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response22.txt_stdout.txt",
    "code_path": "problem_iter1_code22.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit.\n\n    Worst Fit strategy prioritizes bins that have the most remaining capacity,\n    aiming to leave smaller, more constrained bins for potentially larger future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate a preferred bin.\n    \"\"\"\n    # In Worst Fit, we want to choose the bin with the LARGEST remaining capacity.\n    # The priority should be directly proportional to the remaining capacity,\n    # provided the item fits. Bins where the item doesn't fit get a priority of 0.\n\n    # Create a mask for bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities to 0.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # For bins where the item fits, assign a priority equal to their remaining capacity.\n    # This makes bins with more space more \"prioritized\".\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask]\n\n    return priorities",
    "response_id": 22,
    "obj": 149.30195452732352,
    "cyclomatic_complexity": 1.0,
    "halstead": 4.754887502163469,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response23.txt_stdout.txt",
    "code_path": "problem_iter1_code23.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit.\n\n    The Almost Full Fit strategy prioritizes bins that have just enough remaining capacity\n    to fit the item, but not much more. This aims to leave larger remaining capacities\n    in other bins for potentially larger future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to a very low value (negative infinity effectively)\n    # for bins that cannot fit the item.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate a priority score.\n    # The score is higher for bins with less remaining capacity (closer to 'item').\n    # We want to penalize bins that have a lot of 'slack' after placing the item.\n    # A simple approach is to prioritize bins where (remaining_capacity - item) is small.\n    # To make higher values mean higher priority, we can invert this difference.\n    # A large negative value indicates a lot of slack, thus a low priority.\n    # A value close to zero indicates a good fit.\n    if np.any(can_fit_mask):\n        remaining_capacities_for_fit = bins_remain_cap[can_fit_mask]\n        # Calculate the 'slack' or unused space if the item is placed\n        slack = remaining_capacities_for_fit - item\n        # The priority is inversely related to the slack.\n        # We can use a transformation that maps smaller slack to higher priority.\n        # Example: higher priority for smaller slack.\n        # We can use the negative slack itself, but it might be too sensitive.\n        # A common approach for 'almost full' is to prioritize bins with remaining capacity\n        # just above the item size.\n        # Let's define priority as:\n        # 1 / (slack + epsilon)  where epsilon is a small number to avoid division by zero\n        # Or, more directly for \"almost full\": prioritize bins with remaining capacity `r`\n        # such that `r - item` is minimal.\n        # Let's try assigning a higher score when `bins_remain_cap[i] - item` is small.\n        # To make it higher priority, we can use `1 / (bins_remain_cap[i] - item + epsilon)`\n        # or `-(bins_remain_cap[i] - item)`.\n        # The prompt implies \"highest priority score for each bin\".\n        # \"Almost Full Fit\" suggests we prefer bins that are almost full AFTER the item is placed.\n        # This means we prefer bins with `bins_remain_cap[i] - item` to be small.\n        # To make smaller `bins_remain_cap[i] - item` yield higher priority, we can use\n        # `- (bins_remain_cap[i] - item)`.\n        # So, if bins_remain_cap[i] = 10, item = 3, then slack = 7. Priority = -7.\n        # If bins_remain_cap[j] = 5, item = 3, then slack = 2. Priority = -2.\n        # -2 > -7, so bin j is preferred. This aligns with Almost Full Fit.\n\n        # We can also consider the original remaining capacity: we want bins that are\n        # somewhat large, but not excessively large.\n        # Let's define priority as `remaining_capacity` minus a penalty for being too small\n        # or too large.\n        # A simpler interpretation of \"Almost Full Fit\" is to prioritize bins with the\n        # smallest remaining capacity that can still fit the item.\n\n        # Let's try prioritizing bins with the smallest `bins_remain_cap` that is >= `item`.\n        # This means sorting the suitable bins by their remaining capacity and assigning\n        # higher priority to smaller capacities.\n\n        # Calculate priorities for bins that can fit the item.\n        # A higher score means a better fit according to \"Almost Full Fit\".\n        # We want the bin with the minimum `bins_remain_cap` that is still >= `item`.\n        # So, let's assign a priority that is inversely related to the remaining capacity.\n        # Or, assign higher values to smaller remaining capacities.\n        # `max_possible_capacity - bins_remain_cap[i]` would prioritize bins that are fuller.\n        # But we want bins that are *almost full after placing the item*.\n        # So, `(max_possible_capacity - item) - bins_remain_cap[i]` is another way to look at it.\n        # Let's stick to a simpler and more direct interpretation:\n        # Prioritize bins where `bins_remain_cap[i] - item` is minimized.\n        # So, `- (bins_remain_cap[i] - item)` is a good candidate for higher priority.\n        # Let's set the priorities for the bins that can fit the item.\n        # The maximum priority will come from the bin with the smallest remaining capacity\n        # that is still sufficient.\n        priorities[can_fit_mask] = - (remaining_capacities_for_fit - item)\n\n        # To ensure that the *least* remaining capacity is prioritized,\n        # we can normalize these scores or use a large number for the most preferred bin.\n        # If multiple bins have the same minimal remaining capacity, they will have the same priority.\n        # This is acceptable.\n\n        # Alternative for \"Almost Full Fit\": penalize bins that have a lot of remaining capacity *after* placing the item.\n        # If `bins_remain_cap[i]` is the remaining capacity, and we place `item`,\n        # the new remaining capacity will be `bins_remain_cap[i] - item`.\n        # We want this new remaining capacity to be as small as possible (but non-negative).\n        # So, we want to maximize `-(bins_remain_cap[i] - item)` for bins where `bins_remain_cap[i] >= item`.\n        # This is exactly what was calculated.\n\n    return priorities",
    "response_id": 23,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 39.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response24.txt_stdout.txt",
    "code_path": "problem_iter1_code24.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First strategy.\n\n    The Exact Fit First strategy prioritizes bins that can accommodate the item exactly.\n    If no bin can fit the item exactly, it prioritizes bins that can fit the item\n    with the smallest remaining capacity (i.e., the \"tightest fit\").\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Find bins that can exactly fit the item\n    exact_fit_mask = (bins_remain_cap == item)\n    if np.any(exact_fit_mask):\n        priorities[exact_fit_mask] = 1.0  # Highest priority for exact fits\n    else:\n        # If no exact fit, find bins that can fit the item and prioritize the tightest fit\n        can_fit_mask = (bins_remain_cap >= item)\n        if np.any(can_fit_mask):\n            # For bins that can fit, the priority is inversely related to the remaining capacity\n            # minus the item size. Smaller difference means tighter fit.\n            # We use a large constant to ensure these have lower priority than exact fits\n            # and then scale based on the difference to rank them.\n            fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n            differences = fitting_bins_capacities - item\n            # To make smaller differences have higher priority, we take the negative or use a large constant minus the difference\n            priorities[can_fit_mask] = 1000.0 - differences\n        # Bins that cannot fit the item will have a priority of 0, as initialized.\n\n    return priorities",
    "response_id": 24,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 36.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response25.txt_stdout.txt",
    "code_path": "problem_iter1_code25.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance strategy.\n\n    The Inverse Distance (Proximity Fit) strategy prioritizes bins that have\n    just enough remaining capacity to fit the item, encouraging tighter packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Only consider bins that can actually fit the item\n    valid_bins_mask = bins_remain_cap >= item\n    \n    # If no bin can fit the item, return zeros (no priority)\n    if not np.any(valid_bins_mask):\n        return np.zeros_like(bins_remain_cap)\n        \n    # Calculate the \"distance\" to fit. A smaller distance means a tighter fit.\n    # We are using remaining capacity - item_size as the \"waste\" or \"gap\" after packing.\n    # A smaller gap is better.\n    gaps = bins_remain_cap[valid_bins_mask] - item\n    \n    # We want to prioritize bins with the smallest gap (closest fit).\n    # The inverse distance is typically 1 / distance.\n    # However, to avoid division by zero if the gap is exactly zero (perfect fit),\n    # we can add a small epsilon or directly use the inverse of a positive value.\n    # A common approach for \"proximity\" is to use the inverse of the difference,\n    # but we want the *smallest* difference to have the *highest* priority.\n    # So, we can use 1 / (gap + epsilon) or simply sort the gaps and assign priorities\n    # based on rank.\n    \n    # For simplicity and direct interpretation as \"priority score\":\n    # We'll assign a higher score to bins with smaller gaps.\n    # Let's invert the gap: higher score for smaller gaps.\n    # We can use 1.0 / (gap + a small epsilon) to avoid division by zero and\n    # ensure that smaller gaps get higher scores. A small epsilon also\n    # ensures that bins with exactly zero gap get a very high priority.\n    epsilon = 1e-9\n    priorities_for_valid_bins = 1.0 / (gaps + epsilon)\n    \n    # Now, create the full priorities array and populate it\n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bins_mask] = priorities_for_valid_bins\n    \n    return priorities",
    "response_id": 25,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 51.80615605397529,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response26.txt_stdout.txt",
    "code_path": "problem_iter1_code26.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score strategy.\n\n    The Sigmoid Fit Score strategy prioritizes bins that, after adding the item,\n    would have a remaining capacity that is \"close\" to zero. This is achieved\n    by mapping the remaining capacity to a sigmoid function, where values near\n    zero are amplified towards 1, and larger values are suppressed towards 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity if the item is placed in each bin\n    remaining_capacities_after_placement = bins_remain_cap - item\n\n    # Filter out bins where the item cannot fit\n    valid_bins_mask = remaining_capacities_after_placement >= 0\n    \n    # Initialize priorities to a very low value (or negative infinity for practical purposes)\n    # for bins where the item doesn't fit. This ensures they are never chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf) \n    \n    # Apply the sigmoid function to the remaining capacities of valid bins\n    # We use the remaining capacity itself as the input to the sigmoid.\n    # A smaller remaining capacity (closer to 0) will result in a higher sigmoid output.\n    # The sigmoid function `1 / (1 + exp(-x))` maps values to (0, 1).\n    # We can also scale the input to the sigmoid to control the steepness of the curve.\n    # Here, we'll use a simple sigmoid without scaling, as the objective is to favor\n    # bins with smaller remaining capacity *after* placing the item.\n    \n    # To make sure we don't have issues with exp(very large positive number),\n    # we can apply the sigmoid to a transformation of the remaining capacity.\n    # For example, `1 / (1 + exp(x))` maps values to (0, 1) but in reverse (smaller x -> larger output).\n    # So we want to apply it to something like `-remaining_capacity` or `k * remaining_capacity`\n    # where we want to minimize remaining capacity.\n    \n    # Let's use `exp(-remaining_capacity)` for a decaying function.\n    # Or `1 / (1 + exp(remaining_capacity))` which is `exp(-remaining_capacity) / (exp(-remaining_capacity) + 1)`\n    # This will give higher scores to smaller remaining capacities.\n    \n    # A simple sigmoid transformation that maps smaller remaining capacities to higher scores:\n    # `1 / (1 + exp(remaining_capacities_after_placement))`\n    \n    # To avoid numerical instability with large positive remaining capacities, we can\n    # shift the values. Or, we can simply define the sigmoid argument such that smaller\n    # remaining capacities lead to larger values for the sigmoid argument.\n    # For example, let sigmoid input be `C - remaining_capacity` where C is some large constant.\n    # A more standard approach is to transform the value `x` such that you want to minimize it,\n    # into a value `y` that you want to maximize, and then apply a sigmoid.\n    # If we want to minimize `r` (remaining capacity), we can maximize `-r`.\n    # Then apply sigmoid: `1 / (1 + exp(-(-r))) = 1 / (1 + exp(r))`\n    # This would give higher scores to smaller `r`.\n\n    # Let's use a transformation that maps smaller remaining capacities to higher values before sigmoid.\n    # For example, `exp(-remaining_capacity_after_placement * scale)`\n    # If we want the highest priority for remaining_capacity_after_placement = 0, we can use `exp(-remaining_capacity_after_placement)`.\n    # This value will be 1 for 0 remaining capacity and decay towards 0.\n    # We can use `1 / (1 + exp(remaining_capacity_after_placement))` which maps [0, inf) to (0, 0.5]\n    # Or `1 / (1 + exp(-remaining_capacity_after_placement))` which maps [0, inf) to (0.5, 1]\n\n    # A common Sigmoid Fit Score for BPP aims to put the item into a bin\n    # such that the remaining capacity is as small as possible, but not negative.\n    # This means we want to favor bins where `bins_remain_cap - item` is close to 0.\n    #\n    # We can define a score function that is high when `bins_remain_cap - item` is small and non-negative.\n    # A sigmoid function `1 / (1 + exp(-x))` can be used. If we use `x = C - (bins_remain_cap - item)`,\n    # we want `C` to be large enough such that `C - 0` is a large positive number.\n    # Let's use `x = (MaxCapacity - (bins_remain_cap - item))`. This maps the ideal scenario (0 remaining capacity)\n    # to a high value.\n    #\n    # However, a simpler interpretation is to directly map remaining capacity to a priority.\n    # Smaller remaining capacity should yield higher priority.\n    # Let's use `f(x) = 1 / (1 + exp(x))` where `x = remaining_capacity_after_placement`.\n    # This function maps smaller `x` to higher values.\n\n    # Let's ensure that the input to exp is not too large to avoid overflow.\n    # We can limit the remaining capacity, or scale it.\n    # A common sigmoid approach uses a transformation like `-(remaining_capacity_after_placement)`\n    # scaled by a factor. Let's use `k * (bins_remain_cap - item)`\n    # We want to minimize `bins_remain_cap - item`, so we want to maximize `- (bins_remain_cap - item)`.\n    # A good sigmoid input for maximizing smaller values would be `C - value_to_minimize`.\n    # So, `C - (bins_remain_cap - item)`.\n\n    # Let's consider the difference: `diff = bins_remain_cap - item`.\n    # We want to minimize `diff` for `diff >= 0`.\n    # A common way to use sigmoid for minimization is to transform `x` to `k*x`\n    # and then use `1 / (1 + exp(-k*x))`. This results in values closer to 1 for smaller `x`.\n    # Or `1 / (1 + exp(k*x))` which results in values closer to 1 for larger `x`.\n    # So, we want to use the first one, but with `-k*x` if we want to maximize.\n\n    # Let's define a scaled remaining capacity for the sigmoid input.\n    # We want to maximize the priority for remaining capacity close to 0.\n    # So, let the input to sigmoid be proportional to `-remaining_capacity_after_placement`.\n    # `scaled_remaining_capacity = -remaining_capacities_after_placement * scale_factor`\n    # Then priority = `1 / (1 + exp(-scaled_remaining_capacity))`\n    # This will be high when `scaled_remaining_capacity` is high, meaning `remaining_capacities_after_placement` is low.\n\n    # Let's use a reasonable scale factor. If we want to emphasize values\n    # around 0 remaining capacity, we can choose a scale factor.\n    # A simple approach is to use the item size itself or a related value for scaling.\n    # For instance, if bin capacity is B, an item size s, and remaining capacity r = B - s.\n    # We want high priority for small r.\n\n    # A direct mapping where smaller positive remaining capacity gets higher score:\n    # `sigmoid(remaining_capacity) = 1 / (1 + exp(remaining_capacity))`\n    # This maps [0, inf) to (0, 0.5].\n\n    # Let's try a sigmoid where values close to 0 are mapped to near 1.\n    # We want to minimize `r = bins_remain_cap - item`.\n    # So, we can use `sigmoid(k * r)` where `k` is negative, or `sigmoid(-k * r)` where `k` is positive.\n    # Let's use `k = 1` and apply sigmoid to `-remaining_capacities_after_placement`.\n    \n    # Be cautious about potential overflow with large positive `remaining_capacities_after_placement`.\n    # `exp(very_large_number)` will overflow.\n    # `exp(-very_large_number)` will underflow to 0.\n\n    # Let's use `priority = exp(-remaining_capacities_after_placement)`.\n    # This is simple and maps 0 to 1, and positive values to [0, 1).\n    # It's not strictly a sigmoid, but achieves the goal of prioritizing smaller remaining capacities.\n\n    # Alternatively, to stay closer to the \"Sigmoid Fit Score\" idea:\n    # Consider the function `f(x) = 1 / (1 + exp(-x))` which maps `x` to (0, 1).\n    # We want smaller `remaining_capacities_after_placement` (let's call it `rem_cap`) to have higher priority.\n    # So, we can map `rem_cap` to an argument `arg` such that `sigmoid(arg)` is higher for smaller `rem_cap`.\n    # This means `arg` should be higher for smaller `rem_cap`.\n    # So, `arg = k * (C - rem_cap)` for some constants `k > 0` and `C`.\n    # A simple choice is `arg = -rem_cap`.\n    # Then `priority = 1 / (1 + exp(-(-rem_cap))) = 1 / (1 + exp(rem_cap))`\n    # This function maps `rem_cap = 0` to `1/2` and `rem_cap = inf` to `0`.\n    # This indeed prioritizes smaller remaining capacities.\n\n    # Let's use this formulation: `priorities[valid_bins_mask] = 1 / (1 + np.exp(remaining_capacities_after_placement[valid_bins_mask]))`\n    # However, if `remaining_capacities_after_placement` is large, `exp()` might overflow.\n    # If `bins_remain_cap` can be very large, this is a concern. Assuming standard bin capacities, this might be fine.\n\n    # Let's consider a practical implementation for stability.\n    # If `remaining_capacities_after_placement` is large and positive, `exp()` can overflow.\n    # If `remaining_capacities_after_placement` is very negative, `exp()` can underflow to 0.\n    # For our purpose (minimizing remaining capacity), we only care about `remaining_capacities_after_placement >= 0`.\n    # So, if `remaining_capacities_after_placement` becomes very large positive, its priority should be very low.\n    # The function `1 / (1 + exp(x))` does this.\n\n    # To handle potential overflow with `exp(x)` when `x` is large positive, we can check for it.\n    # `np.exp` can raise OverflowError.\n    # A robust way is to clip the argument to `exp`.\n\n    # Let's calculate `r = remaining_capacities_after_placement` for valid bins.\n    r_valid = remaining_capacities_after_placement[valid_bins_mask]\n\n    # We want to compute `1 / (1 + exp(r_valid))`.\n    # If `r_valid` is very large, `exp(r_valid)` will be infinity. `1 / (1 + inf)` is 0.\n    # If `r_valid` is very small (close to 0), `exp(r_valid)` is close to 1. `1 / (1 + 1)` is 0.5.\n    # This means this sigmoid formulation might not prioritize near-zero remaining capacity as strongly as desired.\n\n    # Let's re-think. We want to maximize a function `S(r)` where `r = remaining_capacity`.\n    # `S(r)` should be high for small `r >= 0`.\n    # Example: `S(r) = exp(-k * r)`. For `k > 0`.\n    # If `k=1`, `S(0)=1`, `S(1)=1/e`, `S(large)=small`. This looks good.\n    \n    # Let's consider a slightly modified sigmoid approach:\n    # `sigmoid_score = 1 / (1 + exp(k * (rem_cap - target)))`\n    # Where `target` is the ideal remaining capacity (0 in this case).\n    # So, `sigmoid_score = 1 / (1 + exp(k * rem_cap))` for `k > 0`.\n    # This maps `rem_cap=0` to `0.5`. Smaller `rem_cap` gives priority approaching `0.5`.\n    # Larger `rem_cap` gives priority approaching `0`.\n    # This might not be what we want. We want near-zero to be highest.\n\n    # What if we use `sigmoid(k * (C - rem_cap))`?\n    # Let `C` be a capacity such that if `rem_cap = C`, we are in the middle of sigmoid range.\n    # Or simply: we want to maximize priority as `rem_cap` approaches 0.\n    # Let's use `priority = 1 - sigmoid(k * rem_cap)` where `k > 0`.\n    # `priority = 1 - (1 / (1 + exp(k * rem_cap)))`\n    # `priority = (1 + exp(k * rem_cap) - 1) / (1 + exp(k * rem_cap))`\n    # `priority = exp(k * rem_cap) / (1 + exp(k * rem_cap))`\n    # This is `1 / (1 + exp(-k * rem_cap))`.\n    # This maps `rem_cap = 0` to `1 / (1 + exp(0)) = 1 / 2`.\n    # As `rem_cap` increases, `-k * rem_cap` decreases, so `exp()` decreases, and priority decreases towards 0.\n    # This looks like a standard way to map a minimization problem to a priority where higher is better.\n\n    # Let's use this: `priorities[valid_bins_mask] = 1 / (1 + np.exp(-k * r_valid))`\n    # We need to choose a `k`. A higher `k` means the priority drops faster as `rem_cap` increases.\n    # Let's start with `k = 1`.\n    \n    k = 1.0  # Scaling factor for the sigmoid. Higher k means sharper transition.\n\n    # Calculate the sigmoid score for valid bins\n    # Use `np.clip` to avoid potential overflow/underflow in `np.exp` arguments.\n    # If `-k * r_valid` is very large positive, `exp` can overflow.\n    # If `-k * r_valid` is very large negative, `exp` can underflow to 0.\n    \n    # For `1 / (1 + exp(-k * r_valid))`:\n    # If `r_valid` is very small positive (e.g., 0), `-k * r_valid` is very small negative, `exp()` near 1, priority near 0.5.\n    # If `r_valid` is very large positive, `-k * r_valid` is very large negative, `exp()` near 0, priority near 1.\n    # THIS IS THE OPPOSITE of what we want. We want small `r_valid` to have HIGH priority.\n\n    # Let's go back to the basic idea: we want to maximize `f(r)` where `f(r)` is high for small `r >= 0`.\n    # Options:\n    # 1. `f(r) = exp(-k * r)` for `k > 0`.\n    # 2. Use a sigmoid that maps smaller `r` to values closer to 1.\n    #    `1 / (1 + exp(k * r))` for `k > 0`.\n    #    This maps `r=0` to `0.5`, `r=inf` to `0`. Still not prioritizing 0 best.\n\n    # Let's use the simple decreasing function `exp(-r)` as a proxy for sigmoid-like behavior,\n    # or `exp(-k*r)`. This gives the highest value at `r=0`.\n    \n    # Option A: Simple decaying exponential\n    # priorities[valid_bins_mask] = np.exp(-k * r_valid)\n\n    # Option B: Sigmoid where argument is chosen to favor small `r`.\n    # We want a function `g(x)` that maps `r` to `x` such that `sigmoid(x)` is high for small `r`.\n    # This means `x` should be high for small `r`.\n    # So, `x = C - r`.\n    # Priority = `1 / (1 + exp(-(C - r)))`\n    # Priority = `1 / (1 + exp(r - C))`\n    # If we choose `C` to be some reference capacity. Let's say the average remaining capacity, or the bin capacity.\n    # Let's use `C` as a parameter. A larger `C` means we tolerate larger remaining capacities for higher priority.\n    \n    # For a strict \"Sigmoid Fit Score\", we aim for the remaining capacity to be as close to zero as possible *without going negative*.\n    # This is often framed as fitting the item into a bin where the remaining space is minimized.\n    # Consider the function: `score(r) = 1 / (1 + exp(k * r))`\n    # This gives higher scores for smaller `r`.\n    # `score(0) = 0.5`\n    # `score(very_large) = 0`\n    # `score(very_small_negative) = approaching 1`\n    # Since we filter for `r >= 0`, the scores will be in `[0, 0.5]`.\n    # This also might not be ideal.\n\n    # What if we consider `bins_remain_cap` and `item`?\n    # The \"fit\" is `bins_remain_cap - item`.\n    # The goal is to minimize `bins_remain_cap - item` for `bins_remain_cap - item >= 0`.\n    # Let `remaining = bins_remain_cap - item`.\n    # We want to maximize `f(remaining)` where `f` decreases as `remaining` increases.\n    # A standard sigmoid approach for this: map `remaining` to `argument` such that `sigmoid(argument)` is maximized for small `remaining`.\n    # Argument should be large for small `remaining`.\n    # So, `argument = -remaining * scale_factor` where `scale_factor > 0`.\n    # Priority = `1 / (1 + exp(-(-remaining * scale_factor)))`\n    # Priority = `1 / (1 + exp(remaining * scale_factor))`\n\n    # Let's use this formulation. `scale_factor` can be adjusted.\n    # A scale factor related to the average item size or bin capacity might be good.\n    # For now, let's use `k=1.0`.\n\n    scale_factor = 1.0\n    # Arguments for the sigmoid. We only care about where item fits.\n    # `np.exp` can overflow for very large positive arguments.\n    # If `r_valid` is large, `exp(r_valid * scale_factor)` is huge, and `1/(1+huge)` is close to 0.\n    # If `r_valid` is close to 0, `exp(...)` is close to 1, `1/(1+1)` is 0.5.\n    # This still prioritizes values away from 0 for *this specific sigmoid formulation*.\n\n    # Let's try the other sigmoid form `1 / (1 + exp(-argument))`.\n    # We want `argument` to be high for small `r_valid`.\n    # `argument = k * (C - r_valid)`\n    # Simple choice: `argument = -k * r_valid`.\n    # Priority = `1 / (1 + exp(-(-k * r_valid)))`\n    # Priority = `1 / (1 + exp(k * r_valid))` -- This is what we had.\n\n    # Let's use a common strategy for \"best fit\" in online BPP, which is to minimize remaining capacity.\n    # A simple way to get a priority score where smaller remaining capacity is better:\n    # - Calculate `remaining_cap = bins_remain_cap - item` for valid bins.\n    # - Use `score = 1 / (1 + remaining_cap)` for valid bins. This is not sigmoid but simple.\n    # - Or `score = exp(-remaining_cap)`.\n\n    # The prompt specifically asks for Sigmoid Fit Score strategy.\n    # A robust Sigmoid Fit Score for minimization (like remaining capacity):\n    # `score = sigmoid(k * (max_value - value_to_minimize))`\n    # `value_to_minimize = remaining_capacities_after_placement[valid_bins_mask]`\n    # Let `max_value` be something like the maximum possible remaining capacity.\n    # If we don't have a fixed max capacity, let's use `bins_remain_cap` itself for scaling.\n\n    # Let's try a different sigmoid argument transformation that focuses on making `r=0` map to a high value.\n    # If `r_valid` are the remaining capacities.\n    # We want a function that peaks at `r_valid = 0`.\n    # Consider the argument `x` for `sigmoid(x) = 1 / (1 + exp(-x))`.\n    # For `sigmoid(x)` to be high, `x` should be high.\n    # So we need a function that maps `r_valid` to a high value when `r_valid` is small.\n    # This means the function itself should be decreasing with `r_valid`.\n    # Let `f(r) = -k * r` for `k > 0`.\n    # So, `argument = -k * r_valid`.\n    # `priorities[valid_bins_mask] = 1 / (1 + np.exp(-(-k * r_valid)))`\n    # `priorities[valid_bins_mask] = 1 / (1 + np.exp(k * r_valid))` -- This is where we started and got confused.\n\n    # Let's re-evaluate: `f(x) = 1 / (1 + exp(-x))`\n    # If `x` is large positive, `f(x)` approaches 1.\n    # If `x` is large negative, `f(x)` approaches 0.\n    # We want high priority for small `r_valid`.\n    # So, we want to map `r_valid` to `x` such that `x` is large positive for small `r_valid`.\n    # This means `x` must be a decreasing function of `r_valid`.\n    # Let `x = C - k * r_valid` where `k > 0`.\n    # `priorities[valid_bins_mask] = 1 / (1 + np.exp(-(C - k * r_valid)))`\n    # `priorities[valid_bins_mask] = 1 / (1 + np.exp(k * r_valid - C))`\n    # Let `C = k * reference_value`.\n    # `priorities[valid_bins_mask] = 1 / (1 + np.exp(k * (r_valid - reference_value)))`\n    # If `reference_value = 0`, then `priorities[valid_bins_mask] = 1 / (1 + np.exp(k * r_valid))`.\n    # This indeed maps `r_valid=0` to `0.5`, and higher `r_valid` to lower priorities.\n    # The issue is that `0.5` isn't the highest priority.\n\n    # The core idea of Sigmoid Fit Score is often to penalize bins that are too large or too small.\n    # For online BPP, \"too small\" remaining capacity after placing the item means a better fit.\n    # So we want to maximize the priority for `remaining_capacity_after_placement` close to 0.\n    # The function `1 / (1 + exp(k * r))` maps `r=0` to `0.5` and `r -> inf` to `0`.\n    # The function `1 / (1 + exp(-k * r))` maps `r=0` to `0.5` and `r -> inf` to `1`. This is also not ideal.\n\n    # Let's use a slightly different sigmoid: `1 - sigmoid(k * r) = 1 - 1/(1+exp(-kr)) = exp(kr)/(1+exp(kr))`.\n    # This is `1 / (1 + exp(-kr))`.\n    # It maps `r=0` to `0.5`. `r -> inf` to `1`. `r -> -inf` to `0`.\n    # This means it assigns higher scores to larger remaining capacities.\n\n    # A more fitting sigmoid for minimization would be one that is high for small positive values.\n    # Consider the function `g(x) = a * exp(-k*x)`. This isn't sigmoid.\n\n    # Let's re-read common Sigmoid Fit strategies for BPP.\n    # Often, they try to fit into bins where `remaining_capacity - item_size` is small.\n    # So, the quantity to minimize is `bins_remain_cap - item_size`.\n\n    # Consider the objective: Minimize `r = bins_remain_cap - item`.\n    # Let's use the sigmoid to \"smoothly\" penalize larger `r`.\n    # A common transformation: `score = sigmoid(k * (target - value))`\n    # Here, `value = r`, `target = 0`.\n    # `score = sigmoid(k * (0 - r)) = sigmoid(-k * r)`\n    # `score = 1 / (1 + exp(-(-k * r))) = 1 / (1 + exp(k * r))`\n    # This maps `r=0` to `0.5`, and larger `r` to smaller scores.\n    # So the valid bins would get scores in `(0, 0.5]`.\n    # The bin with the highest score would be the one with the smallest `r >= 0`. This seems correct.\n\n    # Let's implement `score = 1 / (1 + np.exp(k * r_valid))`\n    # To avoid overflow for `exp(k * r_valid)` when `r_valid` is large and positive,\n    # and underflow for `exp(k * r_valid)` when `r_valid` is very negative (though we filter for >= 0).\n    # If `k * r_valid` becomes very large positive, `exp()` overflows.\n    # We can clamp the argument.\n    # Let's clamp the argument to `exp` to avoid overflow.\n    # `argument = k * r_valid`\n    # If `argument` is too large, say `> 700`, `exp` overflows.\n    # We can set `argument = min(argument, 700.0)`\n\n    # Calculate remaining capacities for valid bins.\n    r_valid = remaining_capacities_after_placement[valid_bins_mask]\n    \n    # Apply sigmoid: 1 / (1 + exp(k * r))\n    # where k is a scaling factor. Smaller r should give higher score.\n    # For r=0, score is 0.5. For r>0, score < 0.5.\n    # For r<0, score > 0.5.\n    # This is precisely what we want if we consider ALL bins (including invalid ones which would map to higher scores for negative r).\n    # However, we've filtered out invalid bins.\n\n    # Let's try to adjust the sigmoid argument or form to make small positive `r` get highest priority.\n    # Consider `score = 1 - (1 / (1 + exp(-k * r)))` for `k > 0`. This is `1 / (1 + exp(k * r))`.\n    #\n    # What if we want `r=0` to be the peak of the sigmoid?\n    # The function `exp(-k * (r - target)^2)` is Gaussian, not sigmoid.\n\n    # Let's re-evaluate: The goal is to pack items. A good fit leaves minimal remaining space.\n    # So, we want to prioritize bins where `bins_remain_cap - item` is as small as possible, and non-negative.\n    \n    # A popular interpretation for \"sigmoid fit\" in this context:\n    # Maximize `P(bin) = sigmoid(k * (capacity - item))` for bins where `capacity >= item`.\n    # Let `rem_cap = capacity - item`.\n    # Maximize `P(bin) = sigmoid(k * rem_cap)`? No, this maximizes for large rem_cap.\n    # Maximize `P(bin) = sigmoid(-k * rem_cap)`? This maps `rem_cap=0` to `0.5`, and larger `rem_cap` to lower priority. This is what we want.\n    #\n    # So the formulation `priorities[valid_bins_mask] = 1 / (1 + np.exp(-k * r_valid))` with `k=1.0` seems incorrect as it leads to higher priority for larger `r_valid` in `[0, inf)`.\n    #\n    # Let's use `priorities[valid_bins_mask] = 1 / (1 + np.exp(k * r_valid))` with `k=1.0`.\n    # This maps `r_valid=0` to `0.5`, and larger `r_valid` to values less than `0.5`.\n    # This means the maximum value of the priority will be `0.5`.\n    # If multiple bins have `r_valid = 0`, they all get `0.5`.\n    # This should work as a priority.\n\n    k = 1.0  # Scaling factor for the sigmoid.\n\n    # Ensure we don't compute exp of very large numbers which can cause overflow.\n    # Clamp `k * r_valid` to a reasonable upper bound. For exp(x), if x > ~700, it overflows.\n    # We are computing `exp(k * r_valid)`. If `r_valid` is large, `k * r_valid` is large.\n    # If `k * r_valid` is large positive, `exp()` grows very large. `1/(1+large)` becomes near 0.\n    # If `k * r_valid` is large negative, `exp()` approaches 0. `1/(1+0)` becomes 1.\n    # We want small `r_valid` to have high priority.\n    # So we want to map small `r_valid` to large scores.\n    # With `sigmoid(k*r)`, small `r` gives small `k*r`, sigmoid gives ~0.5.\n    # With `sigmoid(-k*r)`, small `r` gives small `-k*r`, sigmoid gives ~0.5.\n    #\n    # The most common sigmoid score for \"minimizing remaining capacity\" (best fit) is achieved by using a sigmoid\n    # where the input is proportional to the negative of the remaining capacity.\n    # Let `remaining_cap = bins_remain_cap - item`.\n    # We want to maximize `f(remaining_cap)` where `f` is decreasing for `remaining_cap >= 0`.\n    # Consider `sigmoid(x) = 1 / (1 + exp(-x))`. This increases with `x`.\n    # To get a decreasing priority, we can use `sigmoid(-x) = 1 / (1 + exp(x))`.\n    # Or we can use `1 - sigmoid(x) = exp(x) / (1 + exp(x)) = 1 / (1 + exp(-x))`.\n    #\n    # Let's use `score = 1 / (1 + exp(k * r_valid))`. This yields scores in (0, 0.5].\n    # The issue might be that the absolute values are not high.\n    # But if all valid bins get scores < 0.5, then the highest one (closest to 0.5) is prioritized.\n\n    # A potential problem with `1 / (1 + exp(k * r_valid))` is that when `r_valid` is 0, score is 0.5.\n    # If the item is very small, many bins might have `r_valid` very close to 0.\n    #\n    # Consider a scenario where we want a steep drop-off in priority as remaining capacity increases.\n    # This implies a larger `k`.\n    # Let's think about scaling `r_valid`. If bin capacities are large, `r_valid` can be large.\n    # If we use `k=1.0`, `exp(100)` overflows.\n    #\n    # Robust approach for `1 / (1 + exp(x))`:\n    # If `x` is very large positive, score is near 0.\n    # If `x` is very large negative, score is near 1.\n    # We want small `r_valid`. So we want `k * r_valid` to be negative.\n    # This requires `k < 0` or `r_valid < 0`. But we filter `r_valid >= 0`.\n    #\n    # The strategy is to put items into bins such that the remaining space is minimized.\n    # A way to achieve this with sigmoid is to model the \"badness\" of a bin.\n    # \"Badness\" is increasing with `r_valid`.\n    # Let `Badness(r) = k * r`.\n    # Priority can be `1 - sigmoid(Badness(r))` or `sigmoid(-Badness(r))`.\n    # `Priority = 1 / (1 + exp(k * r_valid))`.\n\n    # Let's use a clamping mechanism for the exponential argument for numerical stability.\n    # If `k * r_valid` is very large, the score approaches 0.\n    # If `k * r_valid` is very small (e.g., negative infinity), the score approaches 1.\n    # Since `r_valid >= 0`, `k * r_valid` will be non-negative if `k>0`.\n    # So the scores will be in (0, 0.5] if `k>0`.\n    \n    # Let's define the clamping bound. For `exp(x)`, a bound around 700 is often used.\n    # `bounded_arg = np.clip(k * r_valid, -700.0, 700.0)`\n    # `priorities[valid_bins_mask] = 1 / (1 + np.exp(bounded_arg))`\n    \n    # This will map `r_valid = 0` to `0.5`.\n    # Larger `r_valid` values will result in larger `k * r_valid`,\n    # thus larger `exp(...)`, and smaller `1/(1+...)`.\n    # So this correctly prioritizes bins with smaller remaining capacities.\n\n    k = 5.0 # A higher k can make the penalty for larger remaining capacity steeper.\n\n    # Calculate remaining capacities for valid bins.\n    r_valid = remaining_capacities_after_placement[valid_bins_mask]\n\n    # Compute the argument for the sigmoid function.\n    # We want to map small remaining capacities to high scores.\n    # Using `sigmoid(k * (C - r))` where `C` is some reference.\n    # A simple transformation for minimizing `r` is `sigmoid(-k * r)`.\n    # `sigmoid(-k * r) = 1 / (1 + exp(-(-k * r))) = 1 / (1 + exp(k * r))`.\n    # This gives scores in (0, 0.5] for `r >= 0`.\n\n    # Let's use a scaled version to ensure the steepness is appropriate.\n    # We want to avoid situations where the priority difference between valid bins is too small or too large.\n    # A common choice for `k` is related to the range of the input values.\n\n    # For robustness with `np.exp(k * r_valid)`:\n    # If `k * r_valid` is large and positive, `exp` can overflow.\n    # If `k * r_valid` is large and negative, `exp` can underflow.\n    #\n    # Let's define the argument: `arg = k * r_valid`.\n    # `priorities[valid_bins_mask] = 1 / (1 + np.exp(arg))`\n    \n    # To make `exp(arg)` more stable:\n    # `exp(x)` can be computed as `exp(min(x, max_val))` if `x > max_val`.\n    # If `x` is very large, `exp(x)` is infinite. `1 / (1 + inf)` is 0.\n    # If `x` is very small (large negative), `exp(x)` is near 0. `1 / (1 + 0)` is 1.\n    \n    # Let's consider the term `k * r_valid`.\n    # If `r_valid` is very large, `k * r_valid` is large positive. `exp` overflows.\n    # We want the score to be very low in this case. So setting it to 0 is fine.\n    #\n    # Let's compute `exp_val = np.exp(k * r_valid)`.\n    # If `exp_val` is infinite due to overflow, the priority will be `1 / (1 + inf)` which is 0.\n    # This means that if `k * r_valid` overflows, the priority correctly becomes 0.\n\n    # However, if `r_valid` is 0, `k * r_valid` is 0, `exp(0)` is 1, priority is `1 / (1 + 1) = 0.5`.\n    # This is the maximum priority for this formulation.\n    # So, `priority_v2` assigns the highest priority to bins that have exactly 0 remaining capacity after placing the item.\n\n    priorities[valid_bins_mask] = 1.0 / (1.0 + np.exp(k * r_valid))\n\n    return priorities",
    "response_id": 26,
    "obj": 4.487435181491823,
    "cyclomatic_complexity": 1.0,
    "halstead": 68.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response27.txt_stdout.txt",
    "code_path": "problem_iter1_code27.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Random Fit.\n\n    In Random Fit, we consider all bins that can accommodate the item and assign\n    them equal priority. Then, a random bin from this subset is chosen.\n    This function assigns a priority of 1 to all bins that can fit the item,\n    and 0 to those that cannot.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    # Assign priority 1 to bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = 1.0\n    return priorities",
    "response_id": 27,
    "obj": 4.487435181491823,
    "cyclomatic_complexity": 1.0,
    "halstead": 4.754887502163469,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response28.txt_stdout.txt",
    "code_path": "problem_iter1_code28.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements an epsilon-greedy priority function for online Bin Packing.\n    The function balances exploration (choosing a random bin) with\n    exploitation (choosing the best-fitting bin).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploring a random bin\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate the 'best fit' priority for each bin\n    # Bins that can fit the item are prioritized.\n    # Among those that can fit, we prefer bins where the remaining capacity\n    # is closest to the item size (to minimize waste).\n    for i in range(num_bins):\n        if bins_remain_cap[i] >= item:\n            # A good heuristic is to maximize the \"tightness\" of the fit.\n            # This means minimizing bins_remain_cap[i] - item.\n            # However, since we are using this as a priority for selection,\n            # a larger value should mean higher priority. So, we can use\n            # a score inversely related to the remaining capacity after packing.\n            # A simple way is to use the inverse of the remaining capacity.\n            # Or, to encourage tighter fits, we can use the capacity minus item size,\n            # but inverted, so smaller (better) differences get higher scores.\n            # Let's use (bins_remain_cap[i] - item) as the \"waste\" and\n            # invert it (or subtract from a large number) to get a priority.\n            # Using a large constant minus waste ensures positive scores and\n            # higher priority for smaller waste.\n            waste = bins_remain_cap[i] - item\n            # We want smaller waste to have higher priority.\n            # If waste is 0, it's a perfect fit, highest priority.\n            # If waste is large, priority is low.\n            priorities[i] = 1.0 / (1.0 + waste) # This is a common approach for \"best fit\" score\n            # Alternative: priorities[i] = 1.0 / (bins_remain_cap[i] - item + 1e-9)\n\n    # Epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Explore: Assign a random, non-zero priority to all bins\n        # to give them a chance to be picked.\n        # For exploration, we want to avoid picking bins that cannot fit.\n        # So we still use the existing `priorities` as a base, but add\n        # a small random noise to all potentially viable bins.\n        # A simple exploration is to give a small uniform boost to all bins.\n        # Or, pick a random bin that can fit and assign it a high priority.\n        # Let's try assigning a slightly randomized priority to all bins that *can* fit.\n        explorable_indices = np.where(bins_remain_cap >= item)[0]\n        if len(explorable_indices) > 0:\n            random_boost = np.random.uniform(0.5, 1.5, size=len(explorable_indices))\n            priorities[explorable_indices] *= random_boost\n        else:\n            # If no bin can fit, we might still want to \"explore\" by just picking one,\n            # but in BPP, this means failing. So, we stick to prioritizing valid bins.\n            pass\n\n    # Normalize priorities to prevent extremely large or small values from dominating\n    # and to make the exploration/exploitation balance more robust.\n    # We only normalize the priorities of bins that can actually fit the item.\n    eligible_priorities = priorities[bins_remain_cap >= item]\n    if len(eligible_priorities) > 0 and np.max(eligible_priorities) > 0:\n        # Find the indices of bins that can fit the item\n        eligible_indices = np.where(bins_remain_cap >= item)[0]\n        # Normalize only the priorities of eligible bins\n        max_priority = np.max(priorities[eligible_indices])\n        if max_priority > 0:\n            priorities[eligible_indices] /= max_priority\n\n    return priorities",
    "response_id": 28,
    "obj": 4.0885520542481055,
    "cyclomatic_complexity": 8.0,
    "halstead": 213.9699375973561,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response29.txt_stdout.txt",
    "code_path": "problem_iter1_code29.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    The priority is calculated by considering how well the item fits into the bin,\n    penalizing bins that are too full or too empty relative to the item's size.\n    A temperature parameter can be used to control the \"softness\" of the softmax.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Filter out bins that cannot accommodate the item\n    available_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap)\n\n    if np.any(available_bins_mask):\n        valid_bins_remain_cap = bins_remain_cap[available_bins_mask]\n\n        # Calculate a \"fit score\" for each available bin.\n        # A good fit is when the remaining capacity is just enough or slightly more than the item.\n        # We can use the inverse of (remaining_capacity - item) as a measure,\n        # but this can lead to division by zero or very large values if remaining_capacity is very close to item.\n        # A better approach might be to look at the ratio or a penalized version.\n\n        # Strategy: Prioritize bins where the remaining capacity is \"close\" to the item size.\n        # This encourages filling bins more completely.\n        # Let's define a \"fitness\" as the proportion of the item that fits into the remaining capacity.\n        # Or, a score that is high when remaining_capacity is close to item, and decreases\n        # as remaining_capacity deviates from item.\n\n        # Consider a score based on the negative difference between remaining capacity and item size,\n        # but this penalizes overfilling significantly and underfilling somewhat.\n        # fitness_scores = -(valid_bins_remain_cap - item)\n\n        # Alternative: Consider the \"tightness\" of the fit.\n        # A tighter fit is when remaining_capacity is slightly larger than item.\n        # Let's use a score that is high when remaining_capacity is close to `item` and decreases as it gets larger.\n        # `1 / (valid_bins_remain_cap - item + epsilon)` can be unstable.\n\n        # Let's use a score that rewards bins that can *just* fit the item.\n        # This could be related to the \"waste\" or the \"slack\" remaining.\n        # We want bins where `bins_remain_cap - item` is small and non-negative.\n        # So, a high score for small non-negative `bins_remain_cap - item`.\n\n        # Let's define a \"proximity\" score: a function that is maximized when `bins_remain_cap - item` is close to 0.\n        # For example, a Gaussian-like function or an inverse quadratic.\n\n        # Let's try a simple approach: the larger the remaining capacity is *relative* to the item,\n        # the less preferred it might be, up to a point.\n        # However, Softmax-Based Fit usually means mapping a desirability score to a probability-like distribution.\n        # A common desirability for \"Best Fit\" is simply `remaining_capacity - item`.\n        # If we want to use Softmax, we need a score that can be exponentiated.\n\n        # Let's try to score bins based on how \"full\" they would become after adding the item.\n        # A more full bin is often preferred to keep smaller bins open.\n        # So, let's consider `item / original_bin_capacity`. But we don't have original capacity here.\n        # We only have `bins_remain_cap`.\n\n        # Let's consider a score where we want to leave as little space as possible after packing.\n        # The space left would be `bins_remain_cap - item`. We want this to be small.\n        # So, we want `bins_remain_cap - item` to be minimized.\n        # However, Softmax usually picks the *best* option, so a larger score is better.\n        # Let's define a score as `-(bins_remain_cap - item)` which is `item - bins_remain_cap`.\n        # This means we prefer bins where the remaining capacity is smaller (closer to the item size).\n        # This is essentially a variation of \"Worst Fit Decreasing\" if used with a greedy strategy.\n        # For Softmax, we need a score.\n\n        # Let's try this: a score that is high when `bins_remain_cap` is just slightly larger than `item`.\n        # Consider `1 / (bins_remain_cap - item + 1e-6)` might work but is prone to large values.\n\n        # A more robust approach for Softmax is to define a desirability.\n        # For example, a desirability score related to how \"good\" the fit is.\n        # A good fit means `bins_remain_cap` is close to `item`.\n        # Let's use a score based on the *proportion* of capacity used by the item.\n        # For available bins, let's calculate `item / (item + bins_remain_cap)`. This is like a \"fill ratio\" if it were empty.\n        # This would prefer bins that are already somewhat full.\n        # However, that's not directly related to `bins_remain_cap` alone.\n\n        # Let's re-evaluate the \"Softmax-Based Fit\" idea. The core is to convert\n        # desirability scores into a probability distribution using softmax.\n        # The desirability should reflect how good it is to place the item in a bin.\n        # For BPP, a good fit is often considered one that leaves minimal waste.\n        # Waste for a bin is `bins_remain_cap - item`. We want to minimize this.\n        # A higher score should correspond to lower waste.\n        # So, let's try a score that is inversely related to the waste, or directly related to how close `bins_remain_cap` is to `item`.\n\n        # Let's try a score that prioritizes bins where `bins_remain_cap` is slightly larger than `item`.\n        # `score = 1.0 / (bins_remain_cap - item + 1.0)` could work, as it's high when the denominator is small.\n        # Adding 1.0 makes it more numerically stable and ensures the denominator is at least 1.\n\n        # More formally for Softmax, we want a utility function u(state).\n        # Here, state could be (item, bin_remaining_capacity).\n        # A possible utility function for placing `item` in `bin` with `remaining_cap`:\n        # utility = -(remaining_cap - item)^2  (penalizes deviation from perfect fit)\n        # utility = -(remaining_cap - item) (prefers bins with less slack)\n        # utility = item / remaining_cap (prefers fuller bins)\n\n        # Let's try a utility that favors bins where `bins_remain_cap` is close to `item`.\n        # We can use a Gaussian-like function centered around `item`.\n        # `score = exp(-alpha * (bins_remain_cap - item)^2)`\n        # Or, for simplicity, a score inversely proportional to the slack:\n        # `score = 1.0 / (bins_remain_cap - item + epsilon)`\n\n        # Let's use a desirability function that's high when remaining_capacity is just slightly larger than item.\n        # This encourages finding a \"tight\" fit.\n        # `desirability = 1.0 / (bins_remain_cap - item + 1.0)`\n\n        # Alternative idea for Softmax: Focus on the *resulting* occupancy.\n        # If the item fits, the new remaining capacity will be `bins_remain_cap - item`.\n        # We might want bins that are \"nearly full\" after packing.\n        # So, we want `bins_remain_cap - item` to be small, but not negative.\n        # The \"fullness\" is `1 - (bins_remain_cap - item) / original_capacity`. We don't have original capacity.\n\n        # Let's simplify: Use a desirability score that's directly related to the quality of the fit.\n        # A good fit means `bins_remain_cap - item` is small.\n        # Let's define a desirability that is high for small non-negative differences.\n        # `desirability = 1.0 / (max(0, bins_remain_cap - item) + 1.0)`\n        # This will give a score of 1.0 for bins where `bins_remain_cap == item`,\n        # and a score close to 1.0 for bins where `bins_remain_cap` is slightly larger than `item`.\n        # For bins where `bins_remain_cap < item`, they are already excluded.\n\n        # Let's try a slightly different approach: penalize bins that leave *too much* space.\n        # A desirable bin is one where `bins_remain_cap` is close to `item`.\n        # Let's define a score based on the \"slack\" or \"waste\". Lower waste is better.\n        # For softmax, higher score is better. So, a score inversely proportional to waste.\n        # Waste = `bins_remain_cap - item`.\n        # Score = `1.0 / (bins_remain_cap - item + epsilon)`\n\n        # Let's use a score that prefers bins that become \"most full\" after packing the item.\n        # This means minimizing `bins_remain_cap - item`.\n        # Let's map this to a positive score for softmax.\n        # Consider `score = exp(-alpha * (bins_remain_cap - item))`\n        # Or, more simply, `score = 1.0 / (bins_remain_cap - item + C)` where C is a small constant.\n        # If `bins_remain_cap` is much larger than `item`, this score becomes small.\n        # If `bins_remain_cap` is just slightly larger than `item`, this score is large.\n        # This is similar to \"Best Fit\" in terms of the ordering, but the softmax transforms it.\n\n        # Let's try `score = 1.0 / (bins_remain_cap - item + 1.0)` for available bins.\n        # This rewards bins that have a small positive slack.\n\n        desirability_scores = 1.0 / (valid_bins_remain_cap - item + 1.0)\n\n        # Apply softmax to the desirability scores\n        # Softmax function: exp(x_i) / sum(exp(x_j))\n        # A temperature parameter can be added to control the sharpness of the distribution.\n        temperature = 1.0  # Can be tuned\n        exp_scores = np.exp(desirability_scores / temperature)\n        softmax_probabilities = exp_scores / np.sum(exp_scores)\n\n        # Assign these probabilities back to the original priority array\n        priorities[available_bins_mask] = softmax_probabilities\n\n    return priorities",
    "response_id": 29,
    "obj": 4.487435181491823,
    "cyclomatic_complexity": 2.0,
    "halstead": 68.53238859703687,
    "exec_success": true
  }
]