[
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-based Best Fit strategy.\n\n    This strategy aims to mimic the \"Best Fit\" heuristic within a softmax framework.\n    It prioritizes bins where placing the item results in the least remaining capacity\n    (i.e., the \"tightest\" fit). Bins that cannot accommodate the item receive zero priority.\n    The priorities are generated using a softmax function on desirability scores,\n    where a higher score indicates a better fit (less remaining capacity after packing).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n        For fitting bins, scores are derived from the resulting remaining capacity,\n        and transformed by softmax to represent probabilities or preferences.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        # Calculate the remaining capacity *after* placing the item for eligible bins\n        valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        resulting_remain_cap = valid_bins_remain_cap - item\n\n        # Define a desirability score for each fitting bin.\n        # For \"Best Fit\", we want to minimize `resulting_remain_cap`.\n        # A higher desirability score should correspond to a lower `resulting_remain_cap`.\n        # We can use a score that is inversely proportional to `resulting_remain_cap`.\n        # To avoid division by zero and ensure positive scores, we can add a small constant\n        # or use a transformation like `1 / (slack + 1)`.\n        # A simple approach is to use `max_possible_slack - slack`, where `max_possible_slack`\n        # is the maximum possible remaining capacity among fitting bins.\n        # Or, more directly, we can use a desirability score that is higher when `resulting_remain_cap` is smaller.\n        # Let's use `-resulting_remain_cap` as a raw desirability, so smaller slack (closer to 0) is better.\n        # For softmax, we want scores that are generally positive for exponentiation.\n        # A good score is `1.0 / (resulting_remain_cap + 1.0)` which is high when `resulting_remain_cap` is small.\n\n        # Let's use a score that emphasizes bins with very little slack.\n        # A slightly larger slack should yield a significantly lower score.\n        # We can use an exponential decay, or a simple inverse relationship.\n        # `score = 1.0 / (resulting_remain_cap + epsilon)` where epsilon is a small positive number.\n        # Let's use `1.0 / (resulting_remain_cap + 1e-6)` to ensure stability and positive values.\n\n        desirability_scores = 1.0 / (resulting_remain_cap + 1e-6)\n\n        # Apply softmax to convert desirability scores into a probability-like distribution.\n        # A temperature parameter can control the \"softness\" of the distribution.\n        # A lower temperature makes the distribution sharper, favoring the best bins more.\n        # A higher temperature makes it flatter, giving more similar probabilities.\n        temperature = 1.0  # Tunable parameter\n\n        # Avoid numerical instability with very large desirability scores by clipping or normalizing first,\n        # or by using a stable softmax implementation if available.\n        # Here, we use `exp(score / temperature)`.\n\n        try:\n            exp_scores = np.exp(desirability_scores / temperature)\n            sum_exp_scores = np.sum(exp_scores)\n            if sum_exp_scores > 0:\n                softmax_probabilities = exp_scores / sum_exp_scores\n            else:\n                # If all desirability scores are extremely small (or negative if we didn't ensure positivity),\n                # softmax might result in zeros. Assign uniform probability in such edge cases.\n                softmax_probabilities = np.ones_like(desirability_scores) / len(desirability_scores)\n        except OverflowError:\n            # Handle potential overflow if desirability_scores are too large\n            # A common approach is to subtract the max score before exponentiation.\n            max_score = np.max(desirability_scores)\n            stable_scores = desirability_scores - max_score\n            exp_scores = np.exp(stable_scores / temperature)\n            sum_exp_scores = np.sum(exp_scores)\n            if sum_exp_scores > 0:\n                softmax_probabilities = exp_scores / sum_exp_scores\n            else:\n                softmax_probabilities = np.ones_like(desirability_scores) / len(desirability_scores)\n\n\n        # Place the calculated probabilities back into the original priorities array\n        priorities[can_fit_mask] = softmax_probabilities\n\n    return priorities",
    "response_id": 0,
    "obj": 5.534503390506582,
    "cyclomatic_complexity": 5.0,
    "halstead": 176.41891628622352,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response1.txt_stdout.txt",
    "code_path": "problem_iter2_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This priority function implements a modified First Fit strategy,\n    often referred to as \"Almost Full Fit\" or \"Best Fit\". It prioritizes\n    bins that can accommodate the item with the least amount of remaining\n    capacity (i.e., the tightest fit). This is because packing an item\n    into a bin that is nearly full leaves less \"wasted\" space in that bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority. Bins that cannot fit the item\n        will have a very low priority.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # We only consider bins that have enough remaining capacity for the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, we want to prioritize those that leave\n    # the least amount of remaining space after the item is placed.\n    # This means minimizing (bins_remain_cap - item).\n    # A common way to turn minimization into maximization for priority is to\n    # use the negative of the difference or an inverse.\n    # Using the negative of the difference directly works well:\n    # A smaller (bins_remain_cap - item) results in a less negative (higher) score.\n    # E.g., if item=0.3, bin_caps=[0.35, 0.4, 0.5]\n    # diffs: [0.05, 0.1, 0.2]\n    # scores: [-0.05, -0.1, -0.2] -> bin_cap=0.35 is prioritized.\n    \n    # To ensure higher priority for tighter fits, we can assign a score that is\n    # inversely proportional to the remaining capacity *after* placing the item.\n    # Specifically, we want to maximize `1 / (bins_remain_cap - item + epsilon)`\n    # or, equivalently, minimize `bins_remain_cap - item`.\n    \n    # A simple and effective way is to use the negative of the slack space:\n    # `slack = bins_remain_cap - item`\n    # Priority = -slack\n    # This means smaller positive slacks (tighter fits) get higher priority.\n\n    # To avoid issues with floating point precision or bins that are exactly full,\n    # we can also consider a score that peaks when bins_remain_cap is exactly item.\n    # A common approach is to use a function that is maximized at 0 for `bins_remain_cap - item`.\n    # For example, `-abs(bins_remain_cap - item)` or `1 / (1 + abs(bins_remain_cap - item))`.\n    # The former `-(bins_remain_cap - item)` is simpler and achieves the goal for\n    # bins that can fit the item.\n\n    # Let's use the negative of the slack space.\n    # We add a small epsilon to the `bins_remain_cap` before calculating slack\n    # to ensure that even if `bins_remain_cap` is exactly `item`, the slack\n    # is a small positive number, leading to a priority close to 0, and\n    # preventing any zero or negative slack values from causing issues if\n    # they were to be inverted directly without care.\n    # However, simply taking `- (bins_remain_cap - item)` is generally sufficient\n    # and more direct for \"Best Fit\" / \"Almost Full Fit\".\n\n    # Prioritize bins where `bins_remain_cap - item` is minimized.\n    # This translates to a score that is maximized for these bins.\n    # We can use `-(bins_remain_cap - item)` which means smaller positive values of `bins_remain_cap - item`\n    # result in higher (less negative) scores.\n    \n    # A small adjustment for \"almost full fit\" could be to slightly penalize bins\n    # that are *too* full (i.e., `bins_remain_cap` is very large), even if they can fit.\n    # However, standard \"Almost Full Fit\" or \"Best Fit\" primarily focuses on minimizing slack.\n\n    # Let's refine the score to be `- (bins_remain_cap - item)` for bins that fit.\n    # This score directly reflects the remaining space after packing.\n    # Higher priority for smaller remaining space.\n\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # For bins that cannot fit the item, their priority should be very low so they are never chosen.\n    # Setting them to negative infinity ensures this.\n    priorities[~can_fit_mask] = -np.inf\n\n    # Note: This heuristic is essentially \"Best Fit\". It prioritizes the bin\n    # that will have the least remaining capacity after the item is placed.\n    # This is often a good strategy for \"Almost Full Fit\" as it aims to\n    # utilize space efficiently in bins that are already somewhat full.\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 39.863137138648355,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response2.txt_stdout.txt",
    "code_path": "problem_iter2_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Minimal Positive Remaining Capacity strategy.\n\n    This strategy prioritizes bins that, after placing the item, will have the smallest\n    positive remaining capacity. This encourages a \"tight fit\" and aims to minimize\n    wasted space in bins that are nearly full.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity if the item is placed in each bin\n    remaining_capacities_after_placement = bins_remain_cap - item\n\n    # Filter out bins where the item cannot fit (remaining capacity would be negative)\n    valid_bins_mask = remaining_capacities_after_placement >= 0\n    \n    # If no bin can fit the item, return zeros (no priority)\n    if not np.any(valid_bins_mask):\n        return np.zeros_like(bins_remain_cap)\n        \n    # Get the remaining capacities for the bins where the item fits\n    valid_remaining_capacities = remaining_capacities_after_placement[valid_bins_mask]\n    \n    # We want to prioritize bins with the smallest positive remaining capacity.\n    # A simple way to do this is to assign a priority score that is inversely\n    # proportional to the remaining capacity. A smaller remaining capacity should\n    # result in a higher priority score.\n    #\n    # To avoid division by zero (if remaining capacity is exactly 0), we add a small epsilon.\n    # The inverse of a small positive number is a large number, effectively giving\n    # the highest priority to bins with the tightest fit.\n    \n    epsilon = 1e-9  # A small value to avoid division by zero\n    \n    # Calculate priorities for the valid bins. Higher value means higher priority.\n    # The smaller the `valid_remaining_capacities`, the larger the priority score.\n    priorities_for_valid_bins = 1.0 / (valid_remaining_capacities + epsilon)\n    \n    # Initialize the full priorities array with zeros\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Assign the calculated priorities to the valid bins\n    priorities[valid_bins_mask] = priorities_for_valid_bins\n    \n    return priorities",
    "response_id": 2,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 53.30296890880645,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements a Sigmoid-based priority function for online Bin Packing.\n\n    This function prioritizes bins that offer the \"tightest fit\" for the item.\n    A tight fit is defined as a bin where the remaining capacity is just\n    slightly larger than the item's size. The priority is calculated using\n    a sigmoid function, which provides a smooth score that peaks when the\n    difference between remaining capacity and item size is minimal (and positive).\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item in the corresponding bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can potentially fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit the item, return all zeros.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the \"gap\" for bins that can fit the item.\n    # The gap is the difference between the bin's remaining capacity and the item's size.\n    # A smaller gap indicates a tighter fit.\n    gaps = bins_remain_cap[can_fit_mask] - item\n\n    # Use a sigmoid function to map the gap to a priority score.\n    # We want to assign higher scores to smaller gaps.\n    # The sigmoid function `1 / (1 + exp(-x))` maps `x` to `(0, 1)`.\n    # To prioritize smaller gaps, we can use `sigmoid(-k * gap)`, where `k` is a steepness factor.\n    # A smaller `gap` results in a less negative argument to the sigmoid, yielding a higher score.\n    # For a gap of 0 (perfect fit), the sigmoid argument is 0, and the score is 0.5.\n    # For larger gaps, the sigmoid argument becomes more negative, and the score approaches 0.\n\n    steepness = 10.0  # This parameter controls how quickly the priority drops as the gap increases.\n                     # A higher value means a stronger preference for very tight fits.\n                     # It can be tuned based on experimental results or problem specifics.\n\n    # Calculate the sigmoid argument: -steepness * gap\n    sigmoid_input = -steepness * gaps\n\n    # Apply the sigmoid function. The output will be in the range (0, 1).\n    # These are the priority scores for the bins that can fit the item.\n    priority_scores = 1 / (1 + np.exp(-sigmoid_input))\n\n    # Place these calculated priority scores back into the full priorities array.\n    priorities[can_fit_mask] = priority_scores\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 87.56842503028855,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response4.txt_stdout.txt",
    "code_path": "problem_iter2_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Best Fit strategy.\n\n    This strategy prioritizes bins that have the smallest remaining capacity *after*\n    packing the item, effectively minimizing wasted space. Bins where the item\n    does not fit are given a very low priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate a preferred bin.\n    \"\"\"\n    # Initialize priorities to a very low value (negative infinity) for all bins.\n    # This ensures that only bins where the item can fit will receive a positive or zero score.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity after placement.\n    # The goal is to minimize this value.\n    remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n    # To prioritize the smallest remaining capacity, we can assign a priority based on the\n    # negative of this value. This makes smaller positive remaining capacities result in\n    # higher (less negative) priority scores.\n    # For example, if remaining capacity after placement is 0, priority is 0.\n    # If remaining capacity after placement is 1, priority is -1.\n    # If remaining capacity after placement is 5, priority is -5.\n    # Thus, 0 > -1 > -5, correctly prioritizing the tightest fit.\n    priorities[can_fit_mask] = -remaining_after_placement\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 30.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response5.txt_stdout.txt",
    "code_path": "problem_iter2_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, prioritizing tighter fits.\n\n    This strategy aims to fill bins as completely as possible. Bins that have just\n    enough remaining capacity to fit the item (i.e., the smallest positive difference\n    between remaining capacity and item size) will receive the highest priority.\n    For bins where the item does not fit, the priority is set to a very low value.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities with a very low value to represent infeasible bins\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins where the item can fit\n    fit_mask = bins_remain_cap >= item\n\n    # For bins where the item fits, calculate the \"gap\" or excess capacity.\n    # A smaller gap means a tighter fit.\n    # We want to prioritize smaller gaps, so we can use the negative of the gap\n    # or an inverse function of the gap. Using negative of the gap directly\n    # makes higher values correspond to tighter fits.\n    gaps = bins_remain_cap[fit_mask] - item\n    priorities[fit_mask] = -gaps\n\n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 30.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response6.txt_stdout.txt",
    "code_path": "problem_iter2_code6.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Tunable Sigmoid Fit Score.\n\n    This version introduces a tunable parameter `steepness` to control how quickly the\n    priority score drops as the gap (remaining capacity - item size) increases.\n    It also adds a small offset to the sigmoid argument to slightly favor bins that\n    are not perfectly filled, potentially leaving more room for future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to a very low value for bins that cannot fit the item.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can accommodate the item\n    possible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(possible_bins_mask):\n        # No bin can fit the item, return all negative infinities.\n        return priorities\n\n    # Calculate the \"gap\" for possible bins (how much space is left after placing the item)\n    gaps = bins_remain_cap[possible_bins_mask] - item\n\n    # Tunable parameter to control the steepness of the sigmoid.\n    # A higher value means the priority drops more sharply as the gap increases.\n    # A value of 0 would make all fitting bins have a score of 0.5 (if offset is 0).\n    steepness = 10.0  # Increased steepness for more distinct prioritization\n\n    # Introduce a small negative offset to the argument.\n    # This means that a gap of 0 (perfect fit) will result in sigmoid(-offset),\n    # which is slightly less than 0.5. Larger gaps will result in scores even further below 0.5.\n    # This can encourage using bins that have a small positive gap, potentially\n    # leaving perfectly filled bins for smaller items if available.\n    # Experimentation is needed to find the optimal offset.\n    # Let's try to make the \"ideal\" gap slightly positive, e.g., `optimal_gap = 0.1 * item`.\n    # We want `steepness * (optimal_gap - gap)` to be around 0.\n    # `steepness * (0.1 * item - gap)`\n    # This means we want `gap` to be slightly less than `0.1 * item` for highest priority.\n    # Or, let's consider the argument `steepness * (item - bins_remain_cap[i])`.\n    # We want this to be high when `bins_remain_cap[i]` is slightly larger than `item`.\n    # Let's scale the negative gap by `steepness`.\n    # `sigmoid_arg = steepness * (item - bins_remain_cap[possible_bins_mask])`\n    # `sigmoid_arg = -steepness * (bins_remain_cap[possible_bins_mask] - item)`\n    # This assigns 0.5 to a perfect fit, lower to larger gaps, and higher to negative gaps (not possible here).\n    #\n    # To slightly prefer bins that are NOT perfectly filled, we can shift the sigmoid.\n    # Let's aim for the peak priority to be at a small positive gap.\n    # Consider `sigmoid(k * (ideal_gap - current_gap))`.\n    # If we want `ideal_gap = 0`, this is `sigmoid(k * -gap)`.\n    #\n    # Let's consider a function that rewards tightness: `f(gap)`.\n    # `f(gap) = exp(-steepness * gap)` could be an alternative, but it's not bounded between 0 and 1.\n    #\n    # Let's stick to the sigmoid but adjust the argument.\n    # We want to prioritize smaller gaps. `sigmoid(-steepness * gap)` does this, with peak at 0.5.\n    # To shift the peak to a small positive gap `g_ideal`, we can use `sigmoid(steepness * (g_ideal - gap))`.\n    # Let's choose `g_ideal` to be a small fraction of the item size, e.g., 10% of item size.\n    # This introduces a dependency on the item size, which might be complex.\n    #\n    # A simpler approach: use `sigmoid(-steepness * gap)` and then scale or offset the output.\n    # Or, slightly modify the input:\n    # Let's aim for the highest priority when `bins_remain_cap[i] - item` is small and positive.\n    # We can use `sigmoid(A - B * gap)` where `A` shifts the curve and `B` controls steepness.\n    # If we want the peak at `gap = 0`, then `A = 0` works (for `sigmoid(B * (-gap))`).\n    #\n    # To reward slightly less full bins, let's try to make the score decrease faster.\n    # Let's modify the sigmoid argument:\n    # Instead of just `-steepness * gap`, consider `-steepness * gap - offset`.\n    # This shifts the entire curve to the left.\n    # For `gap = 0`, the argument becomes `-offset`. `sigmoid(-offset)` is < 0.5.\n    # This means perfect fits get lower scores than before.\n    #\n    # The goal is to rank bins. The absolute values don't matter as much as the relative order.\n    # `sigmoid(-steepness * gap)` already provides a good ranking for \"tightest fit\".\n    #\n    # Let's introduce a small constant to the sigmoid argument to shift the peak.\n    # A small positive constant in `sigmoid(-steepness * gap + const)` will increase scores for all gaps.\n    # A small negative constant `const` will decrease scores.\n    #\n    # Let's reconsider the goal: prioritize bins that are \"almost full\" but can still fit.\n    # This means `bins_remain_cap[i]` should be minimized, subject to `bins_remain_cap[i] >= item`.\n    #\n    # Consider the inverse of the gap, but normalized.\n    # Let `normalized_gap = gap / max_possible_gap`. This is still problematic if max_possible_gap is 0.\n    #\n    # Let's try a simple scaling of the gap and an offset.\n    # `sigmoid_input = steepness * (item - bins_remain_cap[possible_bins_mask])`\n    # `sigmoid_input = -steepness * gaps`\n    #\n    # Let's add a small bias to favor bins that are not completely full.\n    # This is to avoid situations where a bin is filled to absolute capacity, potentially\n    # making it unusable for even the smallest future items.\n    # A small positive value `epsilon` added to the `item` size might simulate this.\n    # `effective_item_size = item + epsilon`\n    # `gaps_adjusted = bins_remain_cap[possible_bins_mask] - effective_item_size`\n    # `sigmoid_arg = -steepness * gaps_adjusted`\n    #\n    # Let's try a simple adjustment to the input of the sigmoid:\n    # Instead of `sigmoid(-steepness * gap)`, let's use `sigmoid(-steepness * gap + adjustment)`.\n    # If `adjustment` is positive, it pushes scores higher for all gaps.\n    # If `adjustment` is negative, it pushes scores lower.\n    #\n    # Let's try to favor bins with a small positive gap, say up to `item * 0.1`.\n    # We want the score to be high in this range.\n    #\n    # Let's use `sigmoid(k * (target_capacity - current_remaining_capacity))`\n    # Target capacity should be `item`. So, `sigmoid(k * (item - bins_remain_cap[i]))`.\n    # This is `sigmoid(-k * gap)`.\n    #\n    # To slightly favor bins with a small positive gap (e.g., `gap = 0.1 * item`),\n    # we want the input to sigmoid to be slightly positive.\n    # `sigmoid(k * (ideal_gap - gap))`. If `ideal_gap = 0.1 * item`, and `gap` is a bit smaller.\n    #\n    # Let's use a simpler approach that's common: scale the gap.\n    # `scaled_gap = gap / item` (if item > 0). This makes it relative.\n    # Then `sigmoid(-steepness * scaled_gap)`.\n    # This way, the \"tightness\" is relative to the item size.\n    #\n    # Let's try to slightly penalize perfect fits (gap=0) to encourage slight overflow.\n    # This means the peak of our priority function should be at a small positive gap.\n    # We can achieve this by shifting the sigmoid input.\n    # `sigmoid(steepness * (ideal_gap - gap))`\n    # Let `ideal_gap = 0.05 * item` (a small fraction of the item size).\n    #\n    # If item is 0, this formula would be problematic.\n    # Let's assume item > 0.\n    #\n    # `ideal_gap = 0.05 * item`\n    # `sigmoid_arg = steepness * (ideal_gap - gaps)`\n    #\n    # Example: item = 10. steepness = 10. ideal_gap = 0.5.\n    # If gap = 0.1 (bin cap = 10.1): sigmoid_arg = 10 * (0.5 - 0.1) = 4.0. Score = sigmoid(4.0) ~ 0.98\n    # If gap = 0.5 (bin cap = 10.5): sigmoid_arg = 10 * (0.5 - 0.5) = 0.0. Score = sigmoid(0.0) = 0.5\n    # If gap = 1.0 (bin cap = 11.0): sigmoid_arg = 10 * (0.5 - 1.0) = -5.0. Score = sigmoid(-5.0) ~ 0.007\n    # If gap = 0.0 (bin cap = 10.0): sigmoid_arg = 10 * (0.5 - 0.0) = 5.0. Score = sigmoid(5.0) ~ 0.993 (Oops, perfect fit is highest!)\n\n    # The goal is usually to fill bins as much as possible. So tightest fit is preferred.\n    # Let's go back to prioritizing the minimum non-negative gap.\n    # `sigmoid(-steepness * gap)` where peak is at gap=0.\n    #\n    # To make `priority_v2` distinct and tunable, let's introduce a parameter\n    # that influences the \"ideal\" tightness.\n    #\n    # `fit_preference`:\n    # - `fit_preference = 0`: Prioritize bins that are exactly full (gap = 0).\n    # - `fit_preference = 1`: Prioritize bins that are somewhat full, but leave a small buffer (e.g., gap = 5% of item size).\n    # - `fit_preference = -1`: Prioritize bins that are less full (larger gaps). This is generally not good.\n\n    # Let's aim for `fit_preference` to control the \"ideal gap\".\n    # `ideal_gap = fit_preference * item * 0.1` (0.1 is a scaling factor for preference strength)\n    # If `fit_preference = 0`, `ideal_gap = 0`.\n    # If `fit_preference = 1`, `ideal_gap = 0.1 * item`.\n    # If `fit_preference = -1`, `ideal_gap = -0.1 * item` (problematic, means item must be smaller than capacity).\n\n    # Let's simplify: use `steepness` and a fixed adjustment that shifts the peak slightly.\n    # `sigmoid(-steepness * gap)` has peak at `gap = 0`.\n    # To shift the peak to `gap = ideal_gap`, use `sigmoid(steepness * (ideal_gap - gap))`.\n    #\n    # Let's define `ideal_gap` as a fraction of the item size.\n    # `ideal_gap_fraction = 0.05`  # Aim for a gap that's 5% of the item's size\n    # `ideal_gap = ideal_gap_fraction * item`\n    #\n    # Ensure `ideal_gap` is not negative and not larger than typical gaps.\n    # `ideal_gap = max(0.0, ideal_gap)` # Ensure non-negative\n\n    # The core idea of Sigmoid Fit Score is to rank by tightness.\n    # `sigmoid(-steepness * gap)` achieves this with peak at `gap=0`.\n    # The \"improvement\" can be in the tuning of `steepness` and potentially\n    # how we normalize or bound the `gap` for the sigmoid input.\n\n    # Let's consider a robust scaling of the gap:\n    # `scaled_gap = gap / max(1, item)` # Avoid division by zero and scale by item size.\n    # This makes the priority sensitive to the relative tightness.\n    # Then `sigmoid(-steepness * scaled_gap)`.\n    #\n    # Let's try a combination: `steepness` for sensitivity and a small `gap_offset`\n    # to slightly shift the priority away from perfect fits.\n\n    # Parameter to control how sensitive the priority is to the gap.\n    # Higher value means smaller gaps are much more preferred than larger gaps.\n    steepness = 15.0\n\n    # Parameter to slightly offset the ideal fit.\n    # A positive `gap_preference_offset` means we slightly prefer bins that are NOT perfectly filled.\n    # A negative offset would strongly prefer perfectly filled bins.\n    # Let's try to prefer bins where the remaining capacity is just slightly larger than the item.\n    # This corresponds to a small positive gap.\n    # If `gap = 0`, we want the input to sigmoid to be low.\n    # If `gap = ideal_gap > 0`, we want the input to sigmoid to be close to 0.\n    # So, `k * (ideal_gap - gap)`.\n    #\n    # Let's use `gap` directly, but adjust the sigmoid mapping.\n    # `sigmoid_arg = -steepness * gaps` -> peak at gap = 0.\n    #\n    # Alternative: `1 - sigmoid(steepness * gaps)` -> peak at gap = 0.\n    #\n    # Consider `max(0, 1 - steepness * gaps)` -- not smooth.\n\n    # Let's stick to sigmoid and tune its input:\n    # `sigmoid(steepness * (B - A * gap))` where B is offset and A is steepness multiplier.\n    # `sigmoid(steepness * (0.1 - gap))` -- this is very dependent on scale.\n    #\n    # A common approach is to normalize the gap:\n    # `normalized_gap = gap / max_capacity_of_bin` (but capacity is fixed, remaining varies)\n    #\n    # Let's use a simple sigmoid where `steepness` controls the curve.\n    # And we can add a small constant to the sigmoid argument to nudge it.\n    # `sigmoid_arg = -steepness * gaps + bias`\n    # `bias = 0.0` means peak at `gap = 0`.\n    # `bias = 1.0` means `sigmoid(-steepness * gap + 1.0)`.\n    # If `gap=0`, arg = 1.0, score = sigmoid(1.0) > 0.5.\n    # If `gap=0.1`, arg = -steepness*0.1 + 1.0. If steepness=10, arg=0.0, score=0.5.\n    # So, `bias` shifts the point where score is 0.5.\n    #\n    # Let's make `steepness` and `bias` tunable.\n    steepness = 20.0  # Higher steepness for better discrimination\n    bias = 0.2        # Shift the sigmoid to favor slightly larger gaps (less than perfect fits)\n\n    # Calculate the argument for the sigmoid function\n    # We want to penalize larger gaps, so `gaps` should be subtracted from something.\n    # `bias - steepness * gaps`\n    # If gap is small positive, `bias - steepness * gap` is positive and large (if gap is much smaller than bias/steepness).\n    # If gap is 0, arg = bias.\n    # If gap is larger than `bias/steepness`, arg becomes negative.\n    sigmoid_arg = bias - steepness * gaps\n\n    # Apply the sigmoid function to get priorities\n    # `1 / (1 + exp(-x))` maps values to [0, 1].\n    # For `gap = 0`, score = `sigmoid(bias)`.\n    # For `gap = bias / steepness`, score = `sigmoid(0) = 0.5`.\n    # For `gap > bias / steepness`, score < 0.5.\n    # This means bins with gaps larger than `bias / steepness` get lower scores.\n    # Higher `bias` means higher scores for all gaps, and a higher gap is needed for score 0.5.\n    priorities[possible_bins_mask] = 1 / (1 + np.exp(-sigmoid_arg))\n\n    return priorities",
    "response_id": 6,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 103.72627427729671,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response7.txt_stdout.txt",
    "code_path": "problem_iter2_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements the 'Almost Full Fit' priority function for online Bin Packing.\n\n    This strategy prioritizes bins that will have the least remaining capacity\n    after the item is packed, provided they can fit the item. This aims to\n    minimize wasted space in each bin.\n\n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: A NumPy array containing the remaining capacities of each bin.\n\n    Returns:\n        A NumPy array of priority scores for each bin. Bins that can fit the item\n        will have higher scores (closer to 0) if their remaining capacity after\n        packing is smaller. Bins that cannot fit the item will have a score of -inf.\n    \"\"\"\n    # Initialize priorities to negative infinity, as bins that cannot fit the item\n    # should have the lowest possible priority.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that have enough remaining capacity to accommodate the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the 'slack' (remaining capacity\n    # after the item is placed). The goal of 'Almost Full Fit' is to minimize this slack.\n    # We want bins with the smallest non-negative slack to have the highest priority.\n    if np.any(can_fit_mask):\n        # Calculate the remaining capacity in the bins *after* the item is placed.\n        remaining_after_packing = bins_remain_cap[can_fit_mask] - item\n\n        # To prioritize bins with the smallest remaining capacity after packing,\n        # we can use the negative of this remaining capacity as the priority.\n        # A smaller remaining_after_packing (e.g., 0) will result in a higher priority (-0).\n        # A larger remaining_after_packing (e.g., 5) will result in a lower priority (-5).\n        priorities[can_fit_mask] = -remaining_after_packing\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 30.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response8.txt_stdout.txt",
    "code_path": "problem_iter2_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using an optimized Sigmoid Fit Score strategy.\n\n    This strategy prioritizes bins that, after adding the item, would have a\n    remaining capacity that is \"close\" to zero. This is achieved by mapping\n    the remaining capacity to a sigmoid function that outputs higher scores\n    for smaller non-negative remaining capacities. A scaling factor `k` is used\n    to control the steepness of the sigmoid curve, making the preference for\n    minimal remaining capacity more pronounced.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins where the item does not fit will have a priority of -np.inf.\n        For bins where the item fits, the priority is calculated using a sigmoid\n        function to favor minimal remaining capacity.\n    \"\"\"\n    # Calculate the remaining capacity if the item is placed in each bin\n    remaining_capacities_after_placement = bins_remain_cap - item\n\n    # Filter out bins where the item cannot fit. Assign them the lowest possible priority.\n    valid_bins_mask = remaining_capacities_after_placement >= 0\n    \n    # Initialize priorities to a very low value for bins where the item doesn't fit.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float) \n    \n    # For valid bins, calculate the priority using a sigmoid function.\n    # We want to maximize priority as remaining_capacities_after_placement approaches 0.\n    # A sigmoid function that achieves this is `1 / (1 + exp(k * r))`, where `r` is remaining capacity and `k > 0`.\n    # This maps r=0 to 0.5, and larger r to values less than 0.5.\n    # To make the priority higher for smaller remaining capacities, we want the sigmoid argument to be more negative\n    # for smaller `r`. This is achieved by mapping `r` to `-k * r`.\n    # The sigmoid form becomes `1 / (1 + exp(-(-k * r))) = 1 / (1 + exp(k * r))`.\n    #\n    # Alternatively, and perhaps more standard for \"minimizing a value\", we can use the sigmoid function\n    # `sigmoid(x) = 1 / (1 + exp(-x))`. To prioritize small `r`, we need the argument `x` to be large\n    # when `r` is small. So, `x = C - k * r` for constants `C` and `k > 0`.\n    # Let's choose `x = -k * r` for simplicity, which means `C=0`.\n    # Priority = `sigmoid(-k * r) = 1 / (1 + exp(-(-k * r))) = 1 / (1 + exp(k * r))`.\n    #\n    # This formulation maps:\n    # r = 0     -> exp(0) = 1   -> priority = 1 / (1 + 1) = 0.5\n    # r = small > 0 -> exp(small positive) > 1 -> priority < 0.5\n    # r = large > 0 -> exp(large positive) >> 1 -> priority near 0\n    #\n    # This correctly assigns the highest priority (0.5) to bins with exactly zero remaining capacity,\n    # and decreasing priorities for bins with larger remaining capacities.\n\n    k = 5.0  # Scaling factor: higher k means a stronger preference for minimal remaining capacity.\n             # This value can be tuned. A higher k makes the priority drop off more sharply as remaining capacity increases.\n\n    r_valid = remaining_capacities_after_placement[valid_bins_mask]\n\n    # Calculate the argument for the sigmoid function: `k * r_valid`.\n    # For numerical stability with `np.exp`, we can bound the argument.\n    # `exp(x)` overflows for `x > ~700`. If `k * r_valid` exceeds this, the priority should be near zero,\n    # which `1 / (1 + infinity)` correctly yields.\n    # If `k * r_valid` is very small (large negative), `exp` underflows to 0, giving priority 1.\n    # Since `r_valid >= 0` and `k > 0`, `k * r_valid >= 0`.\n\n    # We use `np.clip` to prevent potential issues if `k * r_valid` becomes extremely large,\n    # ensuring `exp` doesn't overflow or produce NaNs. The value 700 is a common threshold for `exp(x)`.\n    # If `k * r_valid` is greater than this, `exp` would be very large, and the priority would be close to 0.\n    # If `k * r_valid` is very small (negative), which won't happen here since `r_valid >= 0`,\n    # `exp` would be near 0, giving priority near 1.\n    \n    # The argument for the sigmoid is `k * r_valid`.\n    # To ensure stability, we cap the argument to `exp`.\n    # `bounded_arg = np.clip(k * r_valid, -700.0, 700.0)` is not strictly needed for `1 / (1 + exp(k * r_valid))`\n    # because if `k * r_valid` is very large, `exp` will overflow to infinity, and `1 / (1 + inf)` is 0.\n    # This is the desired behavior (very low priority for large remaining capacity).\n\n    # Directly compute the sigmoid for valid bins.\n    # `np.exp(k * r_valid)` can be very large, causing `1 / (1 + ...)` to be very small.\n    # This is desired for large `r_valid`.\n    priorities[valid_bins_mask] = 1.0 / (1.0 + np.exp(k * r_valid))\n\n    return priorities",
    "response_id": 8,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 68.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Softmax-Based Fit.\n\n    This version refines the priority calculation by directly penalizing remaining\n    capacity. Bins that leave less remaining capacity after packing the item are\n    given higher priority. This encourages tighter fits and aims to minimize\n    wasted space. The scores are transformed using an exponential function\n    (similar to softmax) to ensure that bins with better fits (less remaining capacity)\n    have significantly higher probabilities, while still allowing some probability\n    for less optimal fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        # No bin can fit the item, return all zeros\n        return priorities\n\n    # Calculate the \"desirability\" score for bins that can fit the item.\n    # A higher score means less remaining capacity after packing.\n    # We want to minimize (bins_remain_cap - item).\n    # For a softmax-like score where higher is better, we use the negative\n    # of the remaining capacity, i.e., -(bins_remain_cap[can_fit_mask] - item).\n    # This is equivalent to item - bins_remain_cap[can_fit_mask].\n    # Adding a small constant could help if item sizes are very close to capacities,\n    # but the current formulation directly favors less leftover space.\n    remaining_capacity_after_packing = bins_remain_cap[can_fit_mask] - item\n    \n    # To ensure scores are positive and higher for better fits, we can use:\n    # score = 1.0 / (remaining_capacity_after_packing + 1e-9)  # Small epsilon for stability\n    # Alternatively, and perhaps more robustly for softmax, we can use the negative\n    # of the remaining capacity, as implemented in v1, which favors less leftover space.\n    # Let's stick to the logic of favoring bins with less leftover space:\n    # score = -(remaining_capacity_after_packing)\n    \n    # For softmax, it's often beneficial to have scores that are not too extreme,\n    # or to scale them. A simple approach is to use the negative of the remaining\n    # capacity. Let's use this: higher score means less remaining capacity.\n    scores_for_fitting_bins = -(remaining_capacity_after_packing)\n    \n    # Apply softmax-like transformation.\n    # Subtracting the maximum score before exponentiation for numerical stability.\n    max_score = np.max(scores_for_fitting_bins)\n    exp_scores = np.exp(scores_for_fitting_bins - max_score)\n\n    sum_exp_scores = np.sum(exp_scores)\n\n    if sum_exp_scores > 1e-9:  # Check for numerical stability\n        priorities[can_fit_mask] = exp_scores / sum_exp_scores\n    else:\n        # If all scores are very negative, resulting in near-zero exponentials,\n        # distribute probability equally among bins that *can* fit.\n        num_fitting_bins = np.sum(can_fit_mask)\n        if num_fitting_bins > 0:\n            priorities[can_fit_mask] = 1.0 / num_fitting_bins\n\n    return priorities",
    "response_id": 9,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 106.19818783608963,
    "exec_success": true
  }
]