```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority with which we want to add item to each bin, prioritizing tight fits
    with normalized features and a tunable steepness.

    This heuristic aims to balance fitting an item tightly into a bin (to leave larger
    gaps in other bins for potentially larger future items) while also allowing some
    flexibility.

    The priority for a bin is calculated based on how well it fits the item,
    penalizing bins that are "too large" for the item, but not so much that
    very small items can't find a home.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Returns:
        Array of same size as bins_remain_cap with priority score of each bin.
        Higher scores indicate higher priority.
    """
    num_bins = len(bins_remain_cap)
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins that can fit the item
    can_fit_mask = bins_remain_cap >= item

    # Filter to only consider bins that can fit the item
    potential_bins_remain_cap = bins_remain_cap[can_fit_mask]

    if len(potential_bins_remain_cap) == 0:
        return priorities  # No bin can fit the item

    # --- Feature Engineering ---
    # 1. Normalized remaining capacity: How much space is left relative to the bin's original capacity?
    #    We assume an "ideal" bin has a certain amount of slack.
    #    For this heuristic, let's define a parameter `ideal_gap`.
    #    A smaller difference (tight fit) is generally good, but we don't want to penalize
    #    bins that have a *little* more space if it means fitting a very small item.
    #    Let's consider the "waste" or "slack" in the bin if the item is placed.
    #    Slack = bin_remain_cap - item

    slack = potential_bins_remain_cap - item

    # 2. Normalized slack: Divide slack by the bin's original capacity to make it relative.
    #    This helps compare slack across bins of different initial sizes.
    #    We need original capacities to normalize. For this function signature,
    #    we don't have original capacities. A common approach is to normalize by
    #    a maximum possible capacity or a system-wide average.
    #    Alternatively, we can normalize slack by the *current* remaining capacity
    #    before placing the item, which is related to how "full" the bin already is.
    #    Let's try normalizing by the *potential* remaining capacity itself, to see how
    #    much of the *available space* is consumed. A smaller ratio means a tighter fit.
    #    (slack / potential_bins_remain_cap) is essentially 1 - (item / potential_bins_remain_cap)
    #    This ratio represents how much *unused* space remains after placing the item,
    #    relative to the space that was available.
    #    A smaller ratio (closer to 0) means a tighter fit.

    # Handle division by zero if potential_bins_remain_cap is 0 (though can_fit_mask should prevent this)
    # We want to reward tight fits. A tighter fit means slack is small.
    # A simple approach is to use 1 / (slack + small_epsilon) to give higher scores to smaller slack.
    # However, this can lead to extremely large values for very tight fits.

    # Let's use a sigmoid-like function to map the "tightness" to a priority score.
    # Tightness can be measured by how close `bin_remain_cap` is to `item`.
    # We want a high priority when `bin_remain_cap` is just slightly larger than `item`.

    # Define an "ideal gap" - the preferred amount of remaining capacity after packing.
    # This could be a small constant, or related to the item size.
    # Let's use a constant for simplicity, say `ideal_gap = 0`.
    # A gap of `g` means `bin_remain_cap = item + g`.
    # We want to maximize priority when `g` is small.

    # Let's define a "fit score" which is higher for tighter fits.
    # `fit_score = 1 / (slack + epsilon)` is problematic.
    # Instead, consider a metric that is high when `slack` is low.
    # Let `metric = item / potential_bins_remain_cap`. This is high for tight fits.
    # However, it's also high for large items in large bins.
    # A better metric might be `(potential_bins_remain_cap - item) / potential_bins_remain_cap`
    # which is `slack / potential_bins_remain_cap`. We want to minimize this ratio.

    # Let's invert and scale: `priority_component = 1.0 - (slack / potential_bins_remain_cap)`
    # This is equivalent to `item / potential_bins_remain_cap`. High means item is a large
    # fraction of the available space.
    # This might over-prioritize large items filling large bins.

    # Let's rethink: we want to favor bins where `bin_remain_cap` is close to `item`.
    # Consider `similarity = 1 / (1 + abs(bin_remain_cap - item))`. This is high when `bin_remain_cap` is close to `item`.
    # But we need to ensure `bin_remain_cap >= item`.
    # So, `similarity = 1 / (1 + (bin_remain_cap - item))` for `bin_remain_cap >= item`.
    # This gives higher scores to smaller slack.

    # We can use a sigmoid-like function to control the steepness of the preference.
    # Let `x = slack = bin_remain_cap - item`. We want to give higher scores for smaller `x`.
    # A function like `1 / (1 + exp(k * x))` where `k` is steepness. Larger `k` means steeper drop-off.
    # Or, `exp(-k * x)`.
    # Let's use `exp(-k * slack)`.
    # `k` controls how aggressively we penalize slack.
    # A higher `k` means bins with slightly more slack get much lower priority.
    # A lower `k` means bins with more slack are penalized less.

    # Let's set a steepness parameter.
    steepness = 5.0  # Tune this: higher means more aggressive preference for tight fits

    # Calculate raw priority based on slack using an exponential decay.
    # Higher score for smaller slack.
    raw_priorities = np.exp(-steepness * slack)

    # --- Normalization ---
    # Normalize these raw priorities so they are between 0 and 1.
    # This prevents extremely large or small values from dominating.
    max_raw_priority = np.max(raw_priorities)
    if max_raw_priority > 0:
        normalized_priorities = raw_priorities / max_raw_priority
    else:
        normalized_priorities = np.zeros_like(raw_priorities) # Should not happen if len > 0

    # --- Applying to original bins ---
    # Place the normalized priorities back into the full `priorities` array.
    priorities[can_fit_mask] = normalized_priorities

    # --- Refinement: "Ideal Gap" Bonus ---
    # We might want to slightly penalize bins that are *too* tight, leaving
    # absolutely no room for error or small future items. Or, conversely,
    # give a slight boost to bins with a small, non-zero "ideal" remaining capacity.
    # Let's define an `ideal_gap` value. A bin with `remaining_capacity = item + ideal_gap`
    # could get a bonus.
    # For simplicity, let's adjust the penalty based on slack. Instead of `exp(-k * slack)`,
    # we can use a function that peaks when slack is zero, and decreases for positive slack.
    # Or, we can modify the slack calculation.

    # Alternative approach for priority calculation:
    # Prioritize bins where `bins_remain_cap` is closest to `item`.
    # We want to maximize `1 / (1 + (bins_remain_cap - item))` for bins that fit.
    # Let's try a shifted sigmoid or logistic function.
    # Let `y = bins_remain_cap - item` (slack).
    # We want a high value when `y` is small.
    # Consider a function like `1 / (1 + exp(k * (y - offset)))`.
    # If `offset = 0`, this peaks at `y=0` (slack=0).
    # `offset` can represent the "ideal gap". A positive offset means we prefer
    # bins with a small positive slack.

    ideal_gap = 2.0  # This is a tunable parameter: the preferred slack.
                     # A value of 0 means we prefer bins that fit exactly.
                     # A small positive value means we prefer bins with a little bit of room left.
    steepness_fit = 1.0 # Tune this: controls how quickly preference drops off away from ideal_gap.

    # Calculate priority based on deviation from ideal gap.
    # Higher priority when `slack` is close to `ideal_gap`.
    # We use `exp(-steepness_fit * abs(slack - ideal_gap))`
    # This gives a Gaussian-like peak around `ideal_gap`.

    deviation_from_ideal = np.abs(slack - ideal_gap)
    fit_quality_scores = np.exp(-steepness_fit * deviation_from_ideal)

    # Normalize these scores to be between 0 and 1.
    max_fit_quality = np.max(fit_quality_scores)
    if max_fit_quality > 0:
        normalized_fit_quality = fit_quality_scores / max_fit_quality
    else:
        normalized_fit_quality = np.zeros_like(fit_quality_scores)

    # Update priorities with this new metric
    priorities[can_fit_mask] = normalized_fit_quality

    # --- Stabilization and Robustness ---
    # To make scores more stable and less sensitive to minor variations,
    # we can average the current priority with a lagged average, or add a small
    # constant to avoid zero priorities unless absolutely necessary.
    # Adding a small constant also helps in scenarios where all bins might have
    # very similar low priorities.
    # Let's add a small baseline priority to all bins that can fit.
    baseline_priority = 0.1
    priorities[can_fit_mask] = np.maximum(priorities[can_fit_mask], baseline_priority)

    # Re-normalize after adding baseline to maintain the [0, 1] range effectively.
    max_priority = np.max(priorities)
    if max_priority > 0:
        priorities = priorities / max_priority
    else:
        # If all were zero, they remain zero.
        pass

    return priorities
```
