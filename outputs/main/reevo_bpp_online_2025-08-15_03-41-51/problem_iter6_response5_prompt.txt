{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin.\n\n    This version aims to balance tight fits with future flexibility using a\n    sigmoid-like function. It also incorporates a small bonus for larger\n    remaining capacities to encourage more even distribution.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # --- Heuristic Calculation ---\n    # For bins that can fit the item, calculate a priority score.\n    # The score is a combination of how tightly it fits and a bonus for larger capacities.\n\n    # Calculate the \"tightness\" score: smaller remaining capacity after packing is better.\n    # We want to penalize bins with very large remaining capacity if they are too large.\n    # A sigmoid-like function can map the \"waste\" (remaining_cap - item) to a priority.\n    # Smaller waste -> higher priority. We want to avoid large positive waste values.\n    # Let's consider the inverse: remaining_cap. Higher remaining_cap might be worse for tightness.\n    # A function that is high for small remaining_cap and low for large remaining_cap.\n    # We can use a transformation like 1 / (1 + x) or exp(-x).\n    # Let's use a function that is high when remaining_cap is just above 'item'.\n\n    # Option 1: Focus on minimal remaining capacity (tightest fit)\n    # We want bins where `bins_remain_cap - item` is minimized.\n    # Let's map `bins_remain_cap` directly. High priority for small `bins_remain_cap` if they fit.\n    # Softmax or sigmoid can be good here. Let's use a scaled inverse of remaining capacity\n    # and add a small bonus for larger capacities to encourage distribution.\n\n    # Calculate the actual remaining capacity after packing\n    remaining_after_packing = bins_remain_cap[can_fit_mask] - item\n\n    # We want smaller `remaining_after_packing` to have higher priority (tight fit).\n    # Let's use a scaled inverse relationship.\n    # To avoid division by zero or very small numbers, add a small epsilon.\n    epsilon_small = 1e-6\n    tightness_scores = 1.0 / (remaining_after_packing + epsilon_small)\n\n    # Add a small bonus for larger *original* remaining capacities to encourage spreading.\n    # This bonus should be smaller than the tightness score.\n    # Let's scale the original `bins_remain_cap` for the bonus.\n    # The bonus should be higher for larger capacities.\n    larger_capacity_bonus = bins_remain_cap[can_fit_mask] / np.max(bins_remain_cap[can_fit_mask] + epsilon_small)\n    bonus_weight = 0.2  # Controls how much the bonus affects the score\n\n    # Combine tightness and bonus. The tightness should dominate.\n    # We can use a weighted sum.\n    combined_scores = tightness_scores + bonus_weight * larger_capacity_bonus\n\n    # Normalize combined_scores to be between 0 and 1 for better interpretability/stability\n    if np.max(combined_scores) > 0:\n        normalized_combined_scores = combined_scores / np.max(combined_scores)\n    else:\n        normalized_combined_scores = np.zeros_like(combined_scores)\n\n\n    # Assign these normalized scores to the priorities array for the fitting bins\n    priorities[can_fit_mask] = normalized_combined_scores\n\n    # Ensure bins that cannot fit have zero priority\n    priorities[~can_fit_mask] = 0.0\n\n    # Optional: Add a small random noise to break ties and encourage exploration\n    # This is similar to the epsilon-greedy idea but applied to the output scores.\n    # Add small random noise to all bins that can fit.\n    if np.any(can_fit_mask):\n        noise_magnitude = 0.05  # Small magnitude for noise\n        noise = np.random.uniform(-noise_magnitude, noise_magnitude, size=priorities[can_fit_mask].shape)\n        priorities[can_fit_mask] += noise\n        # Ensure priorities remain non-negative after adding noise\n        priorities[can_fit_mask] = np.maximum(0.0, priorities[can_fit_mask])\n\n    # Final normalization to ensure scores are in a reasonable range and the max is 1\n    if np.max(priorities) > 0:\n        priorities = priorities / np.max(priorities)\n    else:\n        # If all bins are still 0 (e.g., item too large for all), they remain 0.\n        pass\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin,\n    prioritizing tight fits and adding small bonuses for larger capacities.\n\n    The strategy aims to:\n    1. Prioritize bins that offer a \"tight fit\" (minimal remaining capacity after packing).\n    2. Give a small bonus to bins with larger remaining capacities, encouraging their\n       use for potentially larger future items.\n    3. Stabilize scoring and handle un-fittable bins explicitly.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Determine which bins can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Filter to only consider bins that can fit the item\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    fitting_indices = np.where(can_fit_mask)[0]\n\n    # --- Scoring components ---\n\n    # 1. Tight Fit Score: Prioritize bins with minimal remaining capacity after packing.\n    #    We want to minimize (remaining_capacity - item).\n    #    To turn this into a priority (higher is better), we can invert and scale.\n    #    A small value for (remaining_capacity - item) should result in a high score.\n    #    Using (max_possible_difference - diff) or similar.\n    #    Let's define a \"waste\" score: waste = remaining_capacity - item. Lower waste is better.\n    waste = fitting_bins_cap - item\n\n    # Normalize waste to [0, 1] range for consistent scoring.\n    # If all waste is 0, max_waste will be 0, avoid division by zero.\n    max_waste = np.max(waste) if len(waste) > 0 else 0\n    if max_waste > 0:\n        normalized_waste = waste / max_waste\n    else:\n        normalized_waste = np.zeros_like(waste) # All bins perfectly fit or no bins fit\n\n    # Tight fit priority: Higher when normalized_waste is lower (closer to 0).\n    # We can use a function like 1 - normalized_waste or apply a sigmoid-like shape.\n    # Let's try a simple inverted linear score: 1 - normalized_waste.\n    tight_fit_scores = 1.0 - normalized_waste\n\n    # 2. Capacity Bonus Score: Give a small bonus to bins with larger remaining capacities.\n    #    This encourages using bins that might be able to fit larger items later.\n    #    Normalize remaining capacity to [0, 1].\n    max_cap = np.max(bins_remain_cap) if num_bins > 0 else 1 # Avoid division by zero if no bins\n    if max_cap > 0:\n        normalized_caps = bins_remain_cap / max_cap\n    else:\n        normalized_caps = np.zeros_like(bins_remain_cap)\n\n    # Capacity bonus: Add a fraction of the normalized capacity.\n    capacity_bonus_weight = 0.1 # Tunable parameter\n    capacity_bonus_scores = capacity_bonus_weight * normalized_caps[can_fit_mask]\n\n    # --- Combine scores ---\n    # Total score for fitting bins is a weighted sum of tight fit and capacity bonus.\n    # We can use a sigmoid-like transformation to map scores to a [0, 1] range,\n    # ensuring that tight fits dominate but capacity bonus provides a nudge.\n    # Let's combine them linearly first and then apply a scaling/transformation.\n\n    combined_raw_scores = tight_fit_scores + capacity_bonus_scores\n\n    # Apply a sigmoid-like function to map scores to [0, 1] and create a smoother distribution.\n    # A simple approach is to scale and shift, or use np.tanh.\n    # Let's map the combined_raw_scores to a range and then use a function that\n    # emphasizes higher values. For simplicity, let's use a soft ranking.\n    # A softmax-like approach can also work to create relative priorities.\n\n    # Let's use a simple scaling and add noise for exploration.\n    # We want tight fits to be generally higher.\n    # A simple approach: score = tight_fit_score + bonus_for_large_capacity\n    # Let's rescale the tight_fit_scores to be in a higher range, e.g., [0.5, 1]\n    # and bonuses in [0, 0.1].\n\n    # Re-scaling tight fit scores to [0.5, 1.0]\n    scaled_tight_fit = 0.5 + 0.5 * tight_fit_scores\n    # Adding capacity bonus (scaled down)\n    final_fitting_scores = scaled_tight_fit + capacity_bonus_scores * 0.5 # Lower weight for bonus\n\n    # Add a small random component for exploration/stochasticity\n    exploration_noise = np.random.uniform(0, 0.05, size=len(final_fitting_scores))\n    final_fitting_scores += exploration_noise\n\n    # Assign these scores to the appropriate bins\n    priorities[fitting_indices] = final_fitting_scores\n\n    # Ensure non-fitting bins have 0 priority\n    priorities[~can_fit_mask] = 0.0\n\n    # Normalize priorities to [0, 1] for consistency if needed for specific algorithms,\n    # but for selection, relative values are what matter.\n    # If all fitting scores are 0 (which shouldn't happen if can_fit_mask is true and item fits),\n    # avoid division by zero.\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n    else:\n        # This case should ideally not be reached if can_fit_mask has true values.\n        pass\n\n    return priorities\n\n[Reflection]\nBalance tight fits, capacity bonuses, and exploration. Use clear normalization.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}