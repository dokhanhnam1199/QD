```python
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a refined Softmax-Based Fit.

    This version refines the priority calculation by directly penalizing remaining
    capacity. Bins that leave less remaining capacity after packing the item are
    given higher priority. This encourages tighter fits and aims to minimize
    wasted space. The scores are transformed using an exponential function
    (similar to softmax) to ensure that bins with better fits (less remaining capacity)
    have significantly higher probabilities, while still allowing some probability
    for less optimal fits.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        # No bin can fit the item, return all zeros
        return priorities

    # Calculate the "desirability" score for bins that can fit the item.
    # A higher score means less remaining capacity after packing.
    # We want to minimize (bins_remain_cap - item).
    # For a softmax-like score where higher is better, we use the negative
    # of the remaining capacity, i.e., -(bins_remain_cap[can_fit_mask] - item).
    # This is equivalent to item - bins_remain_cap[can_fit_mask].
    # Adding a small constant could help if item sizes are very close to capacities,
    # but the current formulation directly favors less leftover space.
    remaining_capacity_after_packing = bins_remain_cap[can_fit_mask] - item
    
    # To ensure scores are positive and higher for better fits, we can use:
    # score = 1.0 / (remaining_capacity_after_packing + 1e-9)  # Small epsilon for stability
    # Alternatively, and perhaps more robustly for softmax, we can use the negative
    # of the remaining capacity, as implemented in v1, which favors less leftover space.
    # Let's stick to the logic of favoring bins with less leftover space:
    # score = -(remaining_capacity_after_packing)
    
    # For softmax, it's often beneficial to have scores that are not too extreme,
    # or to scale them. A simple approach is to use the negative of the remaining
    # capacity. Let's use this: higher score means less remaining capacity.
    scores_for_fitting_bins = -(remaining_capacity_after_packing)
    
    # Apply softmax-like transformation.
    # Subtracting the maximum score before exponentiation for numerical stability.
    max_score = np.max(scores_for_fitting_bins)
    exp_scores = np.exp(scores_for_fitting_bins - max_score)

    sum_exp_scores = np.sum(exp_scores)

    if sum_exp_scores > 1e-9:  # Check for numerical stability
        priorities[can_fit_mask] = exp_scores / sum_exp_scores
    else:
        # If all scores are very negative, resulting in near-zero exponentials,
        # distribute probability equally among bins that *can* fit.
        num_fitting_bins = np.sum(can_fit_mask)
        if num_fitting_bins > 0:
            priorities[can_fit_mask] = 1.0 / num_fitting_bins

    return priorities
```
