[
  {
    "stdout_filepath": "problem_iter4_response0.txt_stdout.txt",
    "code_path": "problem_iter4_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This priority function implements a \"Best Fit\" strategy. It prioritizes\n    bins that can accommodate the item with the least amount of remaining\n    capacity (i.e., the tightest fit). This is because packing an item\n    into a bin that is nearly full leaves less \"wasted\" space in that bin.\n    Bins that cannot accommodate the item are given a very low priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority. Bins that cannot fit the item\n        will have a very low priority (negative infinity).\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that have enough remaining capacity for the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, we want to prioritize those that leave\n    # the least amount of remaining space after the item is placed.\n    # This means minimizing (bins_remain_cap - item).\n    # A simple way to turn minimization into maximization for priority is to\n    # use the negative of the slack space: `-(bins_remain_cap - item)`.\n    # A smaller positive slack `bins_remain_cap - item` will result in a\n    # less negative (higher) priority score.\n    \n    # Example: item = 0.3, bins_remain_cap = [0.35, 0.4, 0.5, 0.2]\n    # can_fit_mask = [True, True, True, False]\n    # For fitting bins:\n    # slack_bins_remain_cap = [0.35, 0.4, 0.5]\n    # resulting_remain_cap = [0.05, 0.1, 0.2]\n    # priority_scores = [-0.05, -0.1, -0.2]\n    # The bin with remaining capacity 0.35 (slack 0.05) gets the highest priority (-0.05).\n    \n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 30.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response1.txt_stdout.txt",
    "code_path": "problem_iter4_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a tunable Sigmoid Fit Score.\n\n    This version refines the Sigmoid Fit Score by introducing `steepness` to control the\n    sigmoid's slope and `ideal_gap_fraction` to specify the desired positive gap as a\n    fraction of the item size. This allows for fine-tuning the preference for bins\n    that are not perfectly filled, potentially leaving more room for future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of -inf.\n    \"\"\"\n    # Initialize priorities to negative infinity for bins that cannot fit the item.\n    # This ensures they are never selected.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can accommodate the item\n    possible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(possible_bins_mask):\n        # If no bin can fit the item, return the initialized -inf priorities.\n        return priorities\n\n    # Calculate the \"gap\" for bins that can fit the item.\n    # Gap is the remaining capacity after placing the item.\n    gaps = bins_remain_cap[possible_bins_mask] - item\n\n    # --- Tunable Parameters ---\n    # `steepness`: Controls how quickly the priority score decreases as the gap increases.\n    # A higher value leads to a sharper drop-off, making the choice more sensitive to small gaps.\n    steepness = 25.0\n\n    # `ideal_gap_fraction`: Defines the target gap as a fraction of the item's size.\n    # A value of 0.0 means we ideally want a perfect fit (gap=0).\n    # A value of 0.1 means we ideally want a gap of 10% of the item size.\n    # This encourages leaving some buffer space.\n    ideal_gap_fraction = 0.08 # Aim for a gap of ~8% of the item size\n\n    # Calculate the ideal gap based on the item size.\n    # Ensure the ideal gap is non-negative. If item size is 0, ideal_gap is 0.\n    ideal_gap = max(0.0, ideal_gap_fraction * item)\n\n    # --- Sigmoid Argument Calculation ---\n    # We use the sigmoid function `1 / (1 + exp(-x))` which maps x to [0, 1].\n    # To prioritize bins with gaps close to `ideal_gap`, we can map `ideal_gap` to the center\n    # of the sigmoid's steep slope (input close to 0).\n    # The argument `x` for the sigmoid will be `steepness * (ideal_gap - gap)`.\n    #\n    # If `gap < ideal_gap`: The argument is positive, sigmoid output > 0.5. Higher scores for smaller gaps.\n    # If `gap = ideal_gap`: The argument is 0, sigmoid output = 0.5. This is the pivot point.\n    # If `gap > ideal_gap`: The argument is negative, sigmoid output < 0.5. Lower scores for larger gaps.\n    #\n    # By choosing `ideal_gap_fraction`, we are effectively tuning the bias of the sigmoid.\n    # The argument can be seen as: `steepness * ideal_gap - steepness * gap`.\n    # The `steepness * ideal_gap` term acts as an additive bias.\n    sigmoid_arg = steepness * (ideal_gap - gaps)\n\n    # Apply the sigmoid function to compute the priority scores for fitting bins.\n    # Higher values indicate a more desirable bin.\n    priorities[possible_bins_mask] = 1 / (1 + np.exp(-sigmoid_arg))\n\n    return priorities",
    "response_id": 1,
    "obj": 4.068607897885915,
    "cyclomatic_complexity": 2.0,
    "halstead": 118.59257041502654,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response2.txt_stdout.txt",
    "code_path": "problem_iter4_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a balanced Softmax-Fit with flexibility.\n\n    This version aims to balance the \"tightest fit\" heuristic with a consideration\n    for \"future flexibility\". It uses a softmax-like approach to prioritize\n    bins that leave less remaining capacity after packing the item (tightest fit).\n    Additionally, it incorporates a small bonus for bins that, even after fitting\n    the item, still retain a substantial amount of remaining capacity, suggesting\n    they might be more useful for larger future items. This bonus is added to the\n    \"tightness\" score before the softmax transformation to influence the probabilities.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        # No bin can fit the item, return all zeros\n        return priorities\n\n    # --- Heuristic 1: Prioritize tightest fits (minimize remaining capacity) ---\n    # Calculate the remaining capacity after packing the item for fitting bins.\n    # Smaller values of `remaining_capacity_after_packing` are better.\n    remaining_capacity_after_packing = bins_remain_cap[can_fit_mask] - item\n\n    # To use with softmax (where higher input leads to higher probability),\n    # we want to transform this such that smaller remaining capacity yields a higher score.\n    # A simple way is to use the negative of the remaining capacity.\n    # We add a small epsilon to avoid zero if item perfectly fills a bin,\n    # and to slightly penalize perfect fits to allow other options if they exist.\n    # Let's use -(remaining_capacity_after_packing - epsilon) for a slightly\n    # better differentiation among tight fits.\n    # A very small epsilon like 1e-6 can help differentiate between bins with\n    # identical remaining capacities after packing if it's zero.\n    epsilon_tightness = 1e-6\n    tightness_scores = -(remaining_capacity_after_packing - epsilon_tightness)\n\n    # --- Heuristic 2: Bonus for future flexibility (larger remaining capacity) ---\n    # We want to add a bonus to bins that have more remaining capacity *after*\n    # the item is packed. This bonus should be less impactful than the tightness.\n    # A simple linear scaling of the remaining capacity after packing can work.\n    # We want to add a *positive* bonus to bins with *larger* remaining capacity.\n    # Let's scale `bins_remain_cap[can_fit_mask]` directly.\n    # The bonus should be smaller than the tightness score's contribution.\n    bonus_scale_factor = 0.1 # Controls the influence of flexibility\n    flexibility_bonus = bonus_scale_factor * bins_remain_cap[can_fit_mask]\n\n    # Combine the tightness score and flexibility bonus.\n    # The bonus is added to the tightness score.\n    combined_scores = tightness_scores + flexibility_bonus\n\n    # Apply softmax-like transformation to get probabilities.\n    # For numerical stability, subtract the maximum score before exponentiation.\n    max_combined_score = np.max(combined_scores)\n    exp_scores = np.exp(combined_scores - max_combined_score)\n\n    sum_exp_scores = np.sum(exp_scores)\n\n    if sum_exp_scores > 1e-9:  # Check for numerical stability\n        priorities[can_fit_mask] = exp_scores / sum_exp_scores\n    else:\n        # If all scores are extremely negative (or zero), resulting in near-zero\n        # exponentials, distribute probability equally among bins that can fit.\n        num_fitting_bins = np.sum(can_fit_mask)\n        if num_fitting_bins > 0:\n            priorities[can_fit_mask] = 1.0 / num_fitting_bins\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 4.0,
    "halstead": 161.66617507355795,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response3.txt_stdout.txt",
    "code_path": "problem_iter4_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using an adaptive Epsilon-Greedy with Best Fit.\n\n    This heuristic aims to improve upon a basic Epsilon-Greedy by:\n    1. Prioritizing strict fits: Bins where placing the item results in very little\n       remaining capacity are given higher priority. This is a refined \"Best Fit\" aspect.\n    2. Adaptive Epsilon: The exploration rate (epsilon) can be adjusted based on the\n       state of the bins. For simplicity here, we'll use a fixed epsilon but conceptually\n       it could be dynamic.\n    3. Balancing Exploration and Exploitation: With probability (1-epsilon), it exploits\n       the best fit. With probability epsilon, it explores by selecting a bin randomly,\n       but we can modulate this exploration to favor slightly less optimal but still valid fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    epsilon = 0.1  # Exploration rate (can be made adaptive)\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # --- Exploitation Phase: Prioritize \"Best Fit\" and \"Near Best Fit\" ---\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n    fitting_bins_caps = bins_remain_cap[fitting_bins_indices]\n    resulting_remain_cap = fitting_bins_caps - item\n\n    # Define a desirability score for fitting bins.\n    # Lower resulting_remain_cap is better. We want scores to be high for good fits.\n    # Using `1.0 / (resulting_remain_cap + epsilon_stable)` where epsilon_stable is a small positive number\n    # to avoid division by zero and ensure positive scores.\n    epsilon_stable = 1e-6\n    desirability_scores = 1.0 / (resulting_remain_cap + epsilon_stable)\n\n    # Apply a \"soft\" selection based on desirability.\n    # This can be seen as a smoothed Best Fit. Instead of picking only the absolute best,\n    # give higher probabilities to better fits.\n    # We use softmax to get probabilities, then scale them.\n    try:\n        # Stable softmax: subtract max before exp to prevent overflow\n        max_score = np.max(desirability_scores)\n        exp_scores = np.exp((desirability_scores - max_score) / 1.0) # Temperature = 1.0\n        probabilities = exp_scores / np.sum(exp_scores)\n    except OverflowError:\n        # Fallback for extreme cases, though the above subtraction usually helps\n        probabilities = np.ones_like(desirability_scores) / len(desirability_scores) # Uniform\n\n    # Assign these probabilities as the base priority for fitting bins\n    priorities[fitting_bins_indices] = probabilities\n\n    # --- Exploration Phase ---\n    # With probability epsilon, we will select a bin randomly among those that can fit.\n    # Instead of completely random, we can also give a small boost to bins that\n    # are \"good enough\" but not strictly the best fit, allowing exploration of\n    # alternatives. For simplicity, we'll introduce random noise to some bins.\n\n    # Identify a subset of bins to slightly boost for exploration/diversity\n    # We can explore a random subset of the fitting bins.\n    num_bins_to_explore = int(np.floor(epsilon * len(fitting_bins_indices)))\n\n    if num_bins_to_explore > 0:\n        # Select random indices from the fitting bins to apply exploration boost\n        explore_indices_in_fitting = np.random.choice(\n            len(fitting_bins_indices), size=num_bins_to_explore, replace=False\n        )\n        original_explore_indices = fitting_bins_indices[explore_indices_in_fitting]\n\n        # Assign a moderate exploration priority, lower than the peak exploitation priority\n        # but higher than zero. This encourages trying slightly suboptimal bins.\n        exploration_priority_value = np.mean(priorities[fitting_bins_indices]) # Average of current priorities\n        if exploration_priority_value == 0: # Ensure it's positive if all are zero\n            exploration_priority_value = 0.1\n        else:\n            exploration_priority_value *= 0.5 # Make it slightly less than average\n\n        priorities[original_explore_indices] = max(priorities[original_explore_indices], exploration_priority_value)\n\n    # Ensure that bins that cannot fit have zero priority.\n    priorities[~can_fit_mask] = 0.0\n\n    # Normalize priorities to a [0, 1] range for consistency.\n    # This step is important if the absolute magnitude matters or for comparisons.\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n    else:\n        # If all priorities are zero (e.g., no bins could fit), return zeros.\n        pass\n\n    return priorities",
    "response_id": 3,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 88, in priority_v2\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n6\n232.04212949048886\n"
  },
  {
    "stdout_filepath": "problem_iter4_response4.txt_stdout.txt",
    "code_path": "problem_iter4_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements a priority function for online Bin Packing that prioritizes\n    bins with small residual capacities after an item is packed.\n\n    This heuristic aims to find the \"tightest fit\" for the item, which often\n    leads to better packing efficiency by minimizing wasted space in each bin.\n    Bins that cannot accommodate the item are given the lowest possible priority.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the current remaining capacity\n                         of each available bin.\n\n    Returns:\n        A NumPy array of the same shape as bins_remain_cap, where each element\n        is the priority score for the corresponding bin. Higher scores indicate\n        higher priority. Bins that cannot fit the item have a score of -np.inf.\n    \"\"\"\n    # Initialize all priorities to negative infinity. This ensures that bins\n    # that cannot fit the item are never selected.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Create a boolean mask to identify bins that have enough remaining capacity\n    # to fit the current item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, we want to prioritize those that will have\n    # the smallest remaining capacity *after* the item is placed. This is\n    # equivalent to minimizing `bins_remain_cap - item`.\n    # To achieve this, we can assign the negative of the residual capacity as\n    # the priority score. A smaller residual capacity (e.g., 0) will result in\n    # a higher (less negative) priority score.\n    if np.any(can_fit_mask):\n        residual_capacities = bins_remain_cap[can_fit_mask] - item\n        priorities[can_fit_mask] = -residual_capacities\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 30.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response5.txt_stdout.txt",
    "code_path": "problem_iter4_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a tunable sigmoid fit score.\n\n    This version introduces two tunable parameters: `steepness` and `ideal_gap_fraction`.\n    `steepness` controls the sensitivity of the priority score to the deviation from the ideal gap.\n    `ideal_gap_fraction` defines the preferred gap size as a fraction of the item size, aiming to\n    slightly favor bins that are not perfectly filled, potentially leaving more room for future items.\n\n    Args:\n        item: Size of the item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of the same size as bins_remain_cap with priority scores for each bin.\n        Scores range from 0 to 1, with higher scores indicating higher priority.\n    \"\"\"\n    # Initialize priorities to a very low value (or 0) for bins that cannot fit the item.\n    # Using 0 here as scores are normalized to [0, 1]. Negative infinity is also an option\n    # if the selection mechanism explicitly handles it (e.g., `np.argmax` with initial low values).\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    possible_bins_mask = bins_remain_cap >= item\n\n    if not np.any(possible_bins_mask):\n        # No bin can fit the item, return all zeros.\n        return priorities\n\n    # Calculate the \"gap\" for possible bins: remaining capacity minus item size.\n    gaps = bins_remain_cap[possible_bins_mask] - item\n\n    # --- Tunable Parameters ---\n    # `steepness`: Controls how quickly the priority drops as the gap deviates from the ideal gap.\n    # A higher value makes the priority more sensitive to the gap size.\n    steepness = 20.0\n\n    # `ideal_gap_fraction`: Defines the target gap size as a fraction of the item's size.\n    # E.g., 0.0 means prioritizing perfect fits (gap=0).\n    # E.g., 0.05 means slightly favoring bins with a gap around 5% of the item size.\n    # This can help in leaving a small buffer for future items.\n    ideal_gap_fraction = 0.05\n\n    # Calculate the ideal gap based on the item size.\n    # Ensure the ideal gap is non-negative.\n    ideal_gap = max(0.0, ideal_gap_fraction * item)\n\n    # Calculate the argument for the sigmoid function.\n    # We want the sigmoid's input to be close to zero when the `gap` is close to `ideal_gap`.\n    # The sigmoid function is `1 / (1 + exp(-x))`.\n    # To have the peak priority (around 1) when `gap == ideal_gap`, we can use `steepness * (ideal_gap - gap)`.\n    # If `gap < ideal_gap`, the argument is positive, sigmoid -> close to 1.\n    # If `gap = ideal_gap`, the argument is 0, sigmoid -> 0.5.\n    # If `gap > ideal_gap`, the argument is negative, sigmoid -> close to 0.\n    # This prioritizes smaller gaps over larger ones, with a peak preference at `ideal_gap`.\n    sigmoid_arg = steepness * (ideal_gap - gaps)\n\n    # Apply the sigmoid function to get priority scores in the range [0, 1].\n    priorities[possible_bins_mask] = 1 / (1 + np.exp(-sigmoid_arg))\n\n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 108.04820237218406,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response6.txt_stdout.txt",
    "code_path": "problem_iter4_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This heuristic implements a \"Best Fit\" strategy for the online Bin Packing Problem.\n    It prioritizes bins that can accommodate the item and will have the least\n    remaining capacity after the item is packed. This aims to minimize wasted\n    space in each packing decision.\n\n    Args:\n        item: Size of the item to be packed.\n        bins_remain_cap: A numpy array representing the remaining capacity of each bin.\n\n    Returns:\n        A numpy array of priority scores for each bin. Higher scores indicate\n        higher priority. Bins that cannot fit the item are assigned a very low\n        priority (negative infinity).\n    \"\"\"\n    # Initialize priorities to a very low value for all bins.\n    # This ensures that bins that cannot fit the item will never be chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that have enough remaining capacity to fit the current item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, we want to select the one that results\n    # in the smallest remaining capacity after packing. This is equivalent to\n    # minimizing `bins_remain_cap - item`.\n    # To translate minimization into maximization for a priority score, we can\n    # use the negative of this difference: `-(bins_remain_cap - item)`.\n    # A smaller positive slack (`bins_remain_cap - item`) will result in a\n    # less negative (higher) priority score.\n    # For example, if item = 0.3 and bin capacities are [0.35, 0.4, 0.5]:\n    # Slack for fitting bins: [0.05, 0.1, 0.2]\n    # Priorities for fitting bins: [-0.05, -0.1, -0.2]\n    # The bin with remaining capacity 0.35 (slack 0.05) gets the highest priority.\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    priorities[can_fit_mask] = -(fitting_bins_remain_cap - item)\n\n    return priorities",
    "response_id": 6,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 1.0,
    "halstead": 30.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response7.txt_stdout.txt",
    "code_path": "problem_iter4_code7.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin, prioritizing tight fits.\n\n    This version focuses on a \"best fit\" strategy by assigning higher priorities to\n    bins that leave minimal remaining capacity after the item is packed. The\n    remaining capacity after packing is calculated as `bin_capacity - item`.\n    To prioritize bins with smaller remaining capacities (tighter fits), we invert\n    this difference. A sigmoid function is applied to the inverted difference to\n    create a smooth priority score, ensuring that bins with slightly larger leftover\n    space still receive some priority, but significantly less than the tightest fits.\n    This approach aims to greedily fill bins as much as possible.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        # No bin can fit the item, return all zeros\n        return priorities\n\n    valid_bins_cap = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    # Calculate the leftover capacity for bins that can fit the item.\n    # We want to minimize `leftover_capacity = valid_bins_cap - item`.\n    # To transform this into a priority score where *higher* is better for *tighter* fits,\n    # we want to give higher scores to *smaller* `leftover_capacity`.\n    # A good way to do this is to use the negative of the leftover capacity,\n    # or an inverse of `leftover_capacity + epsilon`.\n\n    # Let's use a sigmoid transformation of the negative leftover capacity.\n    # The function `sigmoid(k * (item - valid_bins_cap))` works well:\n    # - When `valid_bins_cap` is just slightly larger than `item`, `item - valid_bins_cap` is small negative.\n    #   Sigmoid of small negative is close to 0.5.\n    # - When `valid_bins_cap` is much larger than `item`, `item - valid_bins_cap` is large negative.\n    #   Sigmoid of large negative is close to 0.\n    # - When `valid_bins_cap` is exactly `item`, `item - valid_bins_cap` is 0. Sigmoid(0) is 0.5.\n    # This isn't quite right. We want high priority for small `valid_bins_cap - item`.\n\n    # Let's re-evaluate: we want to maximize `1 / (1 + (valid_bins_cap - item))`\n    # or equivalently, maximize `sigmoid(k * -(valid_bins_cap - item))`.\n    # Let `diff = valid_bins_cap - item`. We want to maximize `1 / (1 + diff)`.\n    # This can be unstable if `diff` is large.\n    # Let's use `sigmoid(k * (item - valid_bins_cap))`.\n    # If `valid_bins_cap` is slightly larger than `item`, `item - valid_bins_cap` is a small negative number.\n    # `sigmoid(k * small_negative)` -> close to 0.5.\n    # If `valid_bins_cap` is much larger than `item`, `item - valid_bins_cap` is a large negative number.\n    # `sigmoid(k * large_negative)` -> close to 0.\n    # If `valid_bins_cap` is equal to `item`, `item - valid_bins_cap` is 0.\n    # `sigmoid(k * 0)` -> 0.5.\n    # This still doesn't perfectly capture \"highest priority for tightest fit\".\n\n    # Let's directly map the \"cost\" of fitting, which is `valid_bins_cap - item`.\n    # We want to penalize larger costs. A simple approach is to use the negative of the cost,\n    # and then apply an exponential function (like softmax) to get probabilities.\n    # Higher priority for smaller `valid_bins_cap - item`.\n    # Let the \"score\" be `-(valid_bins_cap - item)`.\n    # `score = item - valid_bins_cap`\n\n    scores = item - valid_bins_cap\n\n    # To ensure numerical stability with the exponential function, subtract the maximum score.\n    # This shifts all scores so the maximum is 0, preventing overflow when exponentiating.\n    if len(scores) > 0:\n        max_score = np.max(scores)\n        exp_scores = np.exp(scores - max_score)\n\n        # Normalize scores to sum to 1 (like a probability distribution)\n        sum_exp_scores = np.sum(exp_scores)\n\n        if sum_exp_scores > 1e-9:  # Avoid division by zero or near-zero\n            priorities[can_fit_mask] = exp_scores / sum_exp_scores\n        else:\n            # If all exponential scores are effectively zero (e.g., all scores are very negative),\n            # distribute the priority equally among the fitting bins.\n            num_fitting_bins = np.sum(can_fit_mask)\n            if num_fitting_bins > 0:\n                priorities[can_fit_mask] = 1.0 / num_fitting_bins\n    else:\n        # This case should not be reached if np.any(can_fit_mask) is true,\n        # but as a fallback.\n        pass\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 5.0,
    "halstead": 108.41805003750011,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response8.txt_stdout.txt",
    "code_path": "problem_iter4_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Softmax-Based Best Fit.\n\n    This heuristic prioritizes bins that result in the least remaining capacity\n    after packing the item. It uses a softmax-like transformation on the negative\n    of the remaining capacity to assign probabilities. The scores are stabilized\n    by subtracting the maximum score before exponentiation, which is a common\n    technique to prevent overflow in softmax calculations and emphasize differences.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot fit the item will have a priority of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        # No bin can fit the item, return all zeros\n        return priorities\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    # We want to minimize (bins_remain_cap - item).\n    # For a softmax-like score where higher means better, we use the negative\n    # of this value: -(bins_remain_cap - item) which simplifies to item - bins_remain_cap.\n    # A more intuitive way to represent \"best fit\" is by directly using the\n    # resulting remaining capacity and wanting it to be small.\n    # Let's use the negative of the remaining capacity directly as the score\n    # to reward bins that leave less space.\n    \n    resulting_remain_cap_for_fitting_bins = bins_remain_cap[can_fit_mask] - item\n    \n    # A higher score should indicate a better fit (lower remaining capacity).\n    # So, we can use -resulting_remain_cap_for_fitting_bins.\n    # Or, to make it more robust and avoid negative exponents leading to very small numbers,\n    # we can use a score that is inversely proportional to the remaining capacity,\n    # e.g., 1 / (remaining_capacity + epsilon).\n    # However, following the \"minimal remaining capacity\" idea, let's use\n    # a score that is higher when remaining capacity is smaller.\n    # The direct negative of remaining capacity works well for this purpose.\n    \n    scores_for_fitting_bins = -resulting_remain_cap_for_fitting_bins\n\n    # Apply softmax-like transformation.\n    # To prevent potential numerical overflow with np.exp, we subtract the maximum\n    # score from all scores before exponentiation. This does not change the\n    # relative probabilities computed by softmax but ensures that exp(0) is the max.\n    max_score = np.max(scores_for_fitting_bins)\n    \n    # Ensure we don't get NaN or Inf if scores_for_fitting_bins is empty, though\n    # the `if not np.any(can_fit_mask)` check should prevent this.\n    if scores_for_fitting_bins.size > 0:\n        stable_exp_scores = np.exp(scores_for_fitting_bins - max_score)\n        sum_stable_exp_scores = np.sum(stable_exp_scores)\n\n        if sum_stable_exp_scores > 1e-9:  # Check for numerical stability to avoid division by zero\n            priorities[can_fit_mask] = stable_exp_scores / sum_stable_exp_scores\n        else:\n            # If all scores are extremely small (e.g., due to very large negative\n            # resulting remaining capacities), distribute probability equally.\n            num_fitting_bins = np.sum(can_fit_mask)\n            if num_fitting_bins > 0:\n                priorities[can_fit_mask] = 1.0 / num_fitting_bins\n    \n    return priorities",
    "response_id": 8,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 5.0,
    "halstead": 121.01398665684616,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response9.txt_stdout.txt",
    "code_path": "problem_iter4_code9.py",
    "code": "import numpy as np\nfrom scipy.special import expit  # Sigmoid function\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin,\n    prioritizing tightest fits with a bonus for flexibility, using normalized values\n    and sigmoid for smooth prioritization.\n\n    The priority is a composite score based on:\n    1. Tightness: Prioritizes bins with minimal non-negative remaining capacity\n       that can fit the item. Modeled by a sigmoid function of the negative gap.\n    2. Flexibility: Prioritizes bins with larger remaining capacity.\n       Modeled by a sigmoid function of the normalized remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins, dtype=float)\n\n    # Parameters for tuning:\n    # k_tight: Controls the steepness of the tightness score. Higher k_tight\n    #          means smaller gaps are much more favored.\n    # k_flex: Controls the steepness of the flexibility score. Higher k_flex means\n    #         larger capacities reach saturation faster.\n    # w_tight: Weight for the tightness component.\n    # w_flex: Weight for the flexibility component.\n    # norm_clip_max: Value to clip normalized capacities at to prevent extreme flexibility scores.\n    k_tight = 5.0\n    k_flex = 2.0\n    w_tight = 0.7\n    w_flex = 0.3\n    norm_clip_max = 1.0 # Clip normalized capacity at 1.0 to prevent infinite sigmoid input if max_cap == min_cap\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Process only bins that can fit the item\n    eligible_bins_indices = np.where(can_fit_mask)[0]\n\n    if len(eligible_bins_indices) > 0:\n        eligible_rem_cap = bins_remain_cap[eligible_bins_indices]\n        gaps = eligible_rem_cap - item # These are guaranteed non-negative\n\n        # --- Calculate Tightness Score ---\n        # We want small positive gaps to have high scores.\n        # A measure of tightness could be `1 / (1 + gap)` or `exp(-k * gap)`.\n        # Using `exp(-k * gap)`:\n        # If gap = 0, score = exp(0) = 1.\n        # If gap = small positive, score < 1.\n        # If gap = large positive, score -> 0.\n        # This directly gives higher scores for tighter fits.\n        # We can pass this through a sigmoid to smooth and bound.\n        # Let's use sigmoid(k_tight * (1 - gap)) - this doesn't quite work as intended.\n        # Revisit: sigmoid(x) is high for positive x, low for negative x.\n        # We want high score for small gap. Let input to sigmoid be `k * (-gap)`.\n        # `tightness_score = expit(k_tight * (-gaps))`\n        # If gap = 0, input = 0, score = 0.5\n        # If gap = small positive, input = small negative, score < 0.5\n        # If gap = large positive, input = large negative, score -> 0.\n        # This is still inverted.\n\n        # Correct approach for `sigmoid(input)` to be high for small `gaps`:\n        # Input should be high for small gaps.\n        # Try `k * (1.0 / (1.0 + gaps))`\n        #   If gap = 0, input = k * 1.0. Score ~ 1.0\n        #   If gap = small, input = k * (1 / (1 + small)) < k. Score < 1.0\n        #   If gap = large, input = k * (1 / (1 + large)) -> 0. Score -> 0.5.\n        # This seems correct.\n        # Or use `k * exp(-gaps)`\n        #   If gap = 0, input = k * 1.0. Score ~ 1.0\n        #   If gap = small, input = k * exp(-small) < k. Score < 1.0\n        #   If gap = large, input = k * exp(-large) -> 0. Score -> 0.5.\n        # Let's use the `exp(-k*gap)` directly as it's simpler and then sigmoid.\n        # `tightness_measure = np.exp(-k_tight * gaps)`\n        # `tightness_score = expit(k_tight * (tightness_measure - 0.5) * 2)` # Rescale [0,1] to [-1,1] for sigmoid center at 0.5\n\n        # A more direct way for sigmoid:\n        # We want high score when `gaps` is small.\n        # Let `input = C - gaps`. Choose C large enough.\n        # `tightness_score = expit(k_tight * (1.0 - gaps))` # Assuming gaps are normalized 0-1\n        # Since gaps can be larger, let's normalize them first.\n        max_gap = np.max(gaps) if len(gaps) > 0 else 0\n        min_gap = np.min(gaps) if len(gaps) > 0 else 0\n\n        if max_gap > min_gap:\n            normalized_gaps = (gaps - min_gap) / (max_gap - min_gap)\n        else:\n            normalized_gaps = np.zeros_like(gaps) # All gaps are the same\n\n        # Now, small normalized_gaps should yield high priority.\n        # Use `expit(k_tight * (1.0 - normalized_gaps))`\n        tightness_score = expit(k_tight * (1.0 - normalized_gaps))\n\n        # --- Calculate Flexibility Score ---\n        # We want larger remaining capacities to have higher scores.\n        # Normalize `eligible_rem_cap` to [0, 1].\n        max_cap = np.max(bins_remain_cap) if num_bins > 0 else 0\n        min_cap = np.min(bins_remain_cap) if num_bins > 0 else 0\n\n        if max_cap > min_cap:\n            normalized_caps = (eligible_rem_cap - min_cap) / (max_cap - min_cap)\n            # Clip to avoid extreme values if max_cap == min_cap for eligible bins\n            normalized_caps = np.clip(normalized_caps, 0.0, norm_clip_max)\n        else:\n            # If all bins have the same capacity (or only one bin exists),\n            # assign a neutral or slightly positive normalized capacity.\n            normalized_caps = np.full_like(eligible_rem_cap, 0.5)\n\n        # Use sigmoid on normalized capacities. Higher capacity -> higher score.\n        flexibility_score = expit(k_flex * normalized_caps)\n\n        # --- Combine Scores ---\n        # Weighted sum of tightness and flexibility scores\n        composite_priorities = w_tight * tightness_score + w_flex * flexibility_score\n\n        # Assign the calculated composite priorities back to the main priorities array\n        priorities[eligible_bins_indices] = composite_priorities\n\n        # --- Final Normalization (Optional but good practice) ---\n        # Normalize all priorities to be in the range [0, 1] based on the maximum priority found.\n        # This ensures that the highest priority bin always gets a score of 1.0,\n        # making relative scores clearer.\n        max_priority_value = np.max(priorities)\n        if max_priority_value > 0:\n            priorities = priorities / max_priority_value\n        # If all priorities are zero (e.g., no eligible bins or calculation resulted in zeros),\n        # the array remains all zeros.\n\n    return priorities",
    "response_id": 9,
    "obj": 12.814120462704429,
    "cyclomatic_complexity": 9.0,
    "halstead": 359.45228222839756,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response0.txt_stdout.txt",
    "code_path": "problem_iter5_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin.\n\n    This heuristic prioritizes bins based on a combination of:\n    1. Tightness of fit: Bins that leave minimal remaining space after packing.\n    2. Flexibility: A small bonus for bins with larger remaining capacities.\n    3. Stability: Using a sigmoid function to smoothly differentiate priorities.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Determine which bins can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate scores for bins that can fit the item\n    fittable_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    fittable_indices = np.where(can_fit_mask)[0]\n\n    # Score 1: Tightness (prioritize smaller remaining capacity after fit)\n    # We want to minimize (remaining_capacity - item).\n    # A smaller difference is better. We can map this to a higher score.\n    # Using 1 / (difference + small_epsilon) for a steep increase as difference approaches 0.\n    # Or using a negative of the difference for direct correlation with closeness.\n    tightness_scores = fittable_bins_remain_cap - item\n    # Small epsilon to avoid division by zero if remaining capacity is exactly item size\n    epsilon_small = 1e-6\n    tightness_priority = 1.0 / (tightness_scores + epsilon_small)\n\n\n    # Score 2: Flexibility bonus (prioritize larger remaining capacity)\n    # This encourages keeping some large bins open for potentially larger future items.\n    # We can use a small boost proportional to the remaining capacity.\n    flexibility_bonus = 0.1 * fittable_bins_remain_cap / np.max(fittable_bins_remain_cap + epsilon_small)\n\n    # Combined score before sigmoid\n    # Higher tightness_priority is good, higher flexibility_bonus is good\n    combined_raw_score = tightness_priority + flexibility_bonus\n\n    # Use sigmoid to smooth and bound the scores between 0 and 1.\n    # Sigmoid(x) = 1 / (1 + exp(-x))\n    # We need to scale the input to sigmoid to control its steepness.\n    # Let's map a typical range of combined_raw_score to the sigmoid's sensitive region.\n    # A simple scaling can be done by dividing by an estimate of the typical score.\n    # Or, more directly, tune a parameter 'k' for sigmoid(k * x).\n    # For simplicity, we'll use a direct sigmoid, assuming raw scores are somewhat reasonable.\n    # A higher raw score maps to a higher sigmoid output.\n    k_sigmoid = 0.5 # Steepness parameter for sigmoid\n    sigmoid_scores = 1 / (1 + np.exp(-k_sigmoid * (combined_raw_score - np.median(combined_raw_score))))\n    \n    # Assign these sigmoid scores to the priorities array\n    priorities[fittable_indices] = sigmoid_scores\n\n    # Ensure that bins that cannot fit the item have a priority of 0\n    priorities[~can_fit_mask] = 0.0\n\n    # Normalize priorities to be between 0 and 1 (optional, but good for consistency)\n    if np.max(priorities) > 0:\n        priorities = priorities / np.max(priorities)\n\n    return priorities",
    "response_id": 0,
    "obj": 35.63023534104507,
    "cyclomatic_complexity": 3.0,
    "halstead": 244.19821638001633,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response1.txt_stdout.txt",
    "code_path": "problem_iter5_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority scores for each bin for the online Bin Packing Problem,\n    prioritizing tight fits with a sigmoid-like scoring and a small bonus for larger capacities.\n\n    This heuristic aims to balance the \"best fit\" strategy (tight packing) with\n    a slight preference for bins that still offer more flexibility for future items.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array representing the remaining capacity of each bin.\n\n    Returns:\n        A numpy array of priority scores, where higher scores indicate a higher\n        preference for placing the item in that bin.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins, dtype=float)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate a \"tightness\" score. Lower remaining capacity after placing the item is better.\n    # We want to penalize bins that would have a lot of remaining space.\n    # A small epsilon is added to avoid division by zero if a bin has exactly 'item' capacity.\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Use a sigmoid-like function to map remaining capacity to a priority score.\n    # We want to give higher scores to smaller remaining capacities.\n    # The logistic function: L / (1 + exp(-k*(x-x0)))\n    # Here, x is remaining_after_fit.\n    # We want to map small remaining_after_fit to high scores.\n    # Let's invert the remaining_after_fit to make it directly proportional to priority.\n    # A common transformation for \"goodness\" based on \"badness\" (remaining space)\n    # is 1 / (1 + remaining_space), or using exp for smoother transition.\n    \n    # Factor to control the steepness of the sigmoid (higher means sharper transition)\n    steepness = 5.0\n    # Factor to control the midpoint of the sigmoid. We want to center it around\n    # the average \"waste\" if the item perfectly fits.\n    # A good heuristic might be to center around the expected small waste.\n    # Let's use a small value like 0.1 for x0.\n    midpoint = 0.1 \n\n    # Calculate scores based on remaining capacity after fitting.\n    # Higher scores for smaller remaining capacity (tighter fit).\n    # We use exp(-k*x) which is like a decaying exponential, and then normalize it.\n    # Or directly use logistic function on negative remaining capacity.\n    \n    # Option 1: Use exp decay on negative remaining capacity (higher is better)\n    # This will give higher scores to smaller remaining capacities.\n    # Add a small constant to avoid log(0) or very large numbers if remaining_after_fit is very negative (shouldn't happen with mask)\n    tightness_scores = np.exp(-steepness * (remaining_after_fit - midpoint))\n\n    # Option 2: Use a normalized inverse, e.g., 1 / (1 + remaining_after_fit)\n    # This might be less sensitive than exponential.\n    # tightness_scores = 1.0 / (1.0 + remaining_after_fit)\n    \n    # Add a small bonus for bins that still have significant remaining capacity.\n    # This encourages keeping some bins more open for larger future items.\n    # This bonus should be relatively small compared to the tightness score.\n    # We can use a scaled version of the remaining capacity itself.\n    flexibility_bonus_scale = 0.1 # Scale factor for the bonus\n    flexibility_bonus = flexibility_bonus_scale * (bins_remain_cap[can_fit_mask] - item)\n    \n    # Combine tightness score and flexibility bonus.\n    # The tightness score should dominate.\n    combined_scores = tightness_scores + flexibility_bonus\n\n    # Normalize the combined scores for bins that can fit.\n    # This makes the priorities relative to each other.\n    if np.any(combined_scores > 0):\n        normalized_scores = combined_scores / np.max(combined_scores)\n    else:\n        # If all combined scores are zero or negative (shouldn't happen with the logic above)\n        normalized_scores = np.zeros_like(combined_scores)\n\n    # Assign the calculated priorities to the bins that can fit\n    priorities[can_fit_mask] = normalized_scores\n\n    # Ensure bins that cannot fit have zero priority. This is already handled by initialization.\n    # priorities[~can_fit_mask] = 0.0\n\n    return priorities",
    "response_id": 1,
    "obj": 148.94295971280417,
    "cyclomatic_complexity": 3.0,
    "halstead": 143.95954188301644,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response2.txt_stdout.txt",
    "code_path": "problem_iter5_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin.\n\n    This version aims to balance tight fits with future flexibility using a\n    sigmoid-like function. It also incorporates a small bonus for larger\n    remaining capacities to encourage more even distribution.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # --- Heuristic Calculation ---\n    # For bins that can fit the item, calculate a priority score.\n    # The score is a combination of how tightly it fits and a bonus for larger capacities.\n\n    # Calculate the \"tightness\" score: smaller remaining capacity after packing is better.\n    # We want to penalize bins with very large remaining capacity if they are too large.\n    # A sigmoid-like function can map the \"waste\" (remaining_cap - item) to a priority.\n    # Smaller waste -> higher priority. We want to avoid large positive waste values.\n    # Let's consider the inverse: remaining_cap. Higher remaining_cap might be worse for tightness.\n    # A function that is high for small remaining_cap and low for large remaining_cap.\n    # We can use a transformation like 1 / (1 + x) or exp(-x).\n    # Let's use a function that is high when remaining_cap is just above 'item'.\n\n    # Option 1: Focus on minimal remaining capacity (tightest fit)\n    # We want bins where `bins_remain_cap - item` is minimized.\n    # Let's map `bins_remain_cap` directly. High priority for small `bins_remain_cap` if they fit.\n    # Softmax or sigmoid can be good here. Let's use a scaled inverse of remaining capacity\n    # and add a small bonus for larger capacities to encourage distribution.\n\n    # Calculate the actual remaining capacity after packing\n    remaining_after_packing = bins_remain_cap[can_fit_mask] - item\n\n    # We want smaller `remaining_after_packing` to have higher priority (tight fit).\n    # Let's use a scaled inverse relationship.\n    # To avoid division by zero or very small numbers, add a small epsilon.\n    epsilon_small = 1e-6\n    tightness_scores = 1.0 / (remaining_after_packing + epsilon_small)\n\n    # Add a small bonus for larger *original* remaining capacities to encourage spreading.\n    # This bonus should be smaller than the tightness score.\n    # Let's scale the original `bins_remain_cap` for the bonus.\n    # The bonus should be higher for larger capacities.\n    larger_capacity_bonus = bins_remain_cap[can_fit_mask] / np.max(bins_remain_cap[can_fit_mask] + epsilon_small)\n    bonus_weight = 0.2  # Controls how much the bonus affects the score\n\n    # Combine tightness and bonus. The tightness should dominate.\n    # We can use a weighted sum.\n    combined_scores = tightness_scores + bonus_weight * larger_capacity_bonus\n\n    # Normalize combined_scores to be between 0 and 1 for better interpretability/stability\n    if np.max(combined_scores) > 0:\n        normalized_combined_scores = combined_scores / np.max(combined_scores)\n    else:\n        normalized_combined_scores = np.zeros_like(combined_scores)\n\n\n    # Assign these normalized scores to the priorities array for the fitting bins\n    priorities[can_fit_mask] = normalized_combined_scores\n\n    # Ensure bins that cannot fit have zero priority\n    priorities[~can_fit_mask] = 0.0\n\n    # Optional: Add a small random noise to break ties and encourage exploration\n    # This is similar to the epsilon-greedy idea but applied to the output scores.\n    # Add small random noise to all bins that can fit.\n    if np.any(can_fit_mask):\n        noise_magnitude = 0.05  # Small magnitude for noise\n        noise = np.random.uniform(-noise_magnitude, noise_magnitude, size=priorities[can_fit_mask].shape)\n        priorities[can_fit_mask] += noise\n        # Ensure priorities remain non-negative after adding noise\n        priorities[can_fit_mask] = np.maximum(0.0, priorities[can_fit_mask])\n\n    # Final normalization to ensure scores are in a reasonable range and the max is 1\n    if np.max(priorities) > 0:\n        priorities = priorities / np.max(priorities)\n    else:\n        # If all bins are still 0 (e.g., item too large for all), they remain 0.\n        pass\n\n    return priorities",
    "response_id": 2,
    "obj": 70.25329078579976,
    "cyclomatic_complexity": 5.0,
    "halstead": 230.81773576252348,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response3.txt_stdout.txt",
    "code_path": "problem_iter5_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a\n    heuristic that prioritizes tight fits with some flexibility.\n\n    This heuristic aims to:\n    1. Prioritize bins with a tight fit (smallest remaining capacity that still fits the item).\n    2. Provide a slight advantage to bins with larger remaining capacity to maintain flexibility.\n    3. Explicitly handle bins that cannot fit the item.\n    4. Use a sigmoid-like function to create a smooth priority curve based on fit quality.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # For bins that can fit, calculate a priority score.\n    # We want to prioritize bins where remaining_capacity - item is small (tight fit).\n    # We also want to give a small bonus to bins with larger remaining capacity.\n    # A sigmoid-like function can map the 'fit quality' to a priority.\n    # Let's define fit_score as remaining_capacity - item. Lower is better.\n    # We want to invert this relationship for priority.\n\n    # Calculate the \"fit quality\" for bins that can accommodate the item\n    # This is the leftover space after placing the item. Smaller is better.\n    leftover_space = bins_remain_cap[can_fit_mask] - item\n\n    # To prioritize tight fits, we want to give higher scores to smaller leftover_space.\n    # We can use a transformation like 1 / (1 + leftover_space) or exp(-k * leftover_space).\n    # Let's try a simple approach first: prioritize smaller leftover space directly.\n    # To balance with flexibility, we can add a small bonus for larger original capacities.\n\n    # Normalize leftover_space to a [0, 1] range for a smoother function.\n    # Avoid division by zero if all leftover_spaces are 0.\n    min_leftover = np.min(leftover_space)\n    max_leftover = np.max(leftover_space)\n\n    if max_leftover == min_leftover:\n        normalized_leftover = np.zeros_like(leftover_space)\n    else:\n        normalized_leftover = (leftover_space - min_leftover) / (max_leftover - min_leftover)\n\n    # Priority based on tightness: higher for smaller normalized_leftover.\n    # Use a sigmoid-like function: exp(-k * normalized_leftover)\n    # A higher 'k' will emphasize tighter fits more. Let's set k=5.\n    tightness_priority = np.exp(-5 * normalized_leftover)\n\n    # Add a small bonus for larger remaining capacity to promote flexibility.\n    # Normalize original remaining capacity for bins that can fit.\n    original_capacities_fitting = bins_remain_cap[can_fit_mask]\n    min_cap_fit = np.min(original_capacities_fitting)\n    max_cap_fit = np.max(original_capacities_fitting)\n\n    if max_cap_fit == min_cap_fit:\n        normalized_capacity = np.zeros_like(original_capacities_fitting)\n    else:\n        normalized_capacity = (original_capacities_fitting - min_cap_fit) / (max_cap_fit - min_cap_fit)\n\n    # Flexibility bonus: directly proportional to normalized capacity.\n    # Scale this bonus to be smaller than the tightness priority.\n    flexibility_bonus = 0.2 * normalized_capacity\n\n    # Combine priorities: prioritize tightness but give a small boost for flexibility.\n    combined_priority_values = tightness_priority + flexibility_bonus\n\n    # Assign these priorities to the corresponding bins\n    priorities[can_fit_mask] = combined_priority_values\n\n    # Ensure priorities are non-negative and handle potential NaNs if any calculations failed\n    priorities = np.nan_to_num(priorities, nan=0.0, posinf=0.0, neginf=0.0)\n\n    # Normalize priorities to be between 0 and 1 for consistent comparison.\n    # This step is important for the interpretation of \"higher score = higher priority\".\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n    else:\n        # If all priorities are zero (e.g., no bin fits, or all fit-perfectly with no capacity left),\n        # this ensures we return zeros, not NaNs.\n        priorities = np.zeros_like(priorities)\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 5.0,
    "halstead": 247.1753118485642,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response4.txt_stdout.txt",
    "code_path": "problem_iter5_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin,\n    prioritizing tight fits and adding small bonuses for larger capacities.\n\n    The strategy aims to:\n    1. Prioritize bins that offer a \"tight fit\" (minimal remaining capacity after packing).\n    2. Give a small bonus to bins with larger remaining capacities, encouraging their\n       use for potentially larger future items.\n    3. Stabilize scoring and handle un-fittable bins explicitly.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Determine which bins can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Filter to only consider bins that can fit the item\n    fitting_bins_cap = bins_remain_cap[can_fit_mask]\n    fitting_indices = np.where(can_fit_mask)[0]\n\n    # --- Scoring components ---\n\n    # 1. Tight Fit Score: Prioritize bins with minimal remaining capacity after packing.\n    #    We want to minimize (remaining_capacity - item).\n    #    To turn this into a priority (higher is better), we can invert and scale.\n    #    A small value for (remaining_capacity - item) should result in a high score.\n    #    Using (max_possible_difference - diff) or similar.\n    #    Let's define a \"waste\" score: waste = remaining_capacity - item. Lower waste is better.\n    waste = fitting_bins_cap - item\n\n    # Normalize waste to [0, 1] range for consistent scoring.\n    # If all waste is 0, max_waste will be 0, avoid division by zero.\n    max_waste = np.max(waste) if len(waste) > 0 else 0\n    if max_waste > 0:\n        normalized_waste = waste / max_waste\n    else:\n        normalized_waste = np.zeros_like(waste) # All bins perfectly fit or no bins fit\n\n    # Tight fit priority: Higher when normalized_waste is lower (closer to 0).\n    # We can use a function like 1 - normalized_waste or apply a sigmoid-like shape.\n    # Let's try a simple inverted linear score: 1 - normalized_waste.\n    tight_fit_scores = 1.0 - normalized_waste\n\n    # 2. Capacity Bonus Score: Give a small bonus to bins with larger remaining capacities.\n    #    This encourages using bins that might be able to fit larger items later.\n    #    Normalize remaining capacity to [0, 1].\n    max_cap = np.max(bins_remain_cap) if num_bins > 0 else 1 # Avoid division by zero if no bins\n    if max_cap > 0:\n        normalized_caps = bins_remain_cap / max_cap\n    else:\n        normalized_caps = np.zeros_like(bins_remain_cap)\n\n    # Capacity bonus: Add a fraction of the normalized capacity.\n    capacity_bonus_weight = 0.1 # Tunable parameter\n    capacity_bonus_scores = capacity_bonus_weight * normalized_caps[can_fit_mask]\n\n    # --- Combine scores ---\n    # Total score for fitting bins is a weighted sum of tight fit and capacity bonus.\n    # We can use a sigmoid-like transformation to map scores to a [0, 1] range,\n    # ensuring that tight fits dominate but capacity bonus provides a nudge.\n    # Let's combine them linearly first and then apply a scaling/transformation.\n\n    combined_raw_scores = tight_fit_scores + capacity_bonus_scores\n\n    # Apply a sigmoid-like function to map scores to [0, 1] and create a smoother distribution.\n    # A simple approach is to scale and shift, or use np.tanh.\n    # Let's map the combined_raw_scores to a range and then use a function that\n    # emphasizes higher values. For simplicity, let's use a soft ranking.\n    # A softmax-like approach can also work to create relative priorities.\n\n    # Let's use a simple scaling and add noise for exploration.\n    # We want tight fits to be generally higher.\n    # A simple approach: score = tight_fit_score + bonus_for_large_capacity\n    # Let's rescale the tight_fit_scores to be in a higher range, e.g., [0.5, 1]\n    # and bonuses in [0, 0.1].\n\n    # Re-scaling tight fit scores to [0.5, 1.0]\n    scaled_tight_fit = 0.5 + 0.5 * tight_fit_scores\n    # Adding capacity bonus (scaled down)\n    final_fitting_scores = scaled_tight_fit + capacity_bonus_scores * 0.5 # Lower weight for bonus\n\n    # Add a small random component for exploration/stochasticity\n    exploration_noise = np.random.uniform(0, 0.05, size=len(final_fitting_scores))\n    final_fitting_scores += exploration_noise\n\n    # Assign these scores to the appropriate bins\n    priorities[fitting_indices] = final_fitting_scores\n\n    # Ensure non-fitting bins have 0 priority\n    priorities[~can_fit_mask] = 0.0\n\n    # Normalize priorities to [0, 1] for consistency if needed for specific algorithms,\n    # but for selection, relative values are what matter.\n    # If all fitting scores are 0 (which shouldn't happen if can_fit_mask is true and item fits),\n    # avoid division by zero.\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities = priorities / max_priority\n    else:\n        # This case should ideally not be reached if can_fit_mask has true values.\n        pass\n\n    return priorities",
    "response_id": 4,
    "obj": 4.248105305145606,
    "cyclomatic_complexity": 7.0,
    "halstead": 292.57485892279027,
    "exec_success": true
  }
]