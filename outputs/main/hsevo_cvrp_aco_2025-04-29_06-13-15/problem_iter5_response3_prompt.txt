{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Capacitated Vehicle Routing Problem (CVRP) via stochastic solution sampling. CVRP requires finding the shortest path that visits all given nodes and returns to the starting node. Each node has a demand and each vehicle has a capacity. The total demand of the nodes visited by a vehicle cannot exceed the vehicle capacity. When the total demand exceeds the vehicle capacity, the vehicle must return to the starting node.\nThe `heuristics` function takes as input a distance matrix (shape: n by n), Euclidean coordinates of nodes (shape: n by 2), a vector of customer demands (shape: n), and the integer capacity of vehicle capacity. It returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the distance_matrix. The depot node is indexed by 0.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray, coordinates: np.ndarray, demands: np.ndarray, capacity: int) -> np.ndarray:\n\n    \"\"\"\n    Combines distance, demand, and spatial clustering with adaptive scaling.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Avoid division by zero.\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n\n    # Define weights for combining heuristics\n    alpha = 0.4  # Importance of distance\n    beta = 0.35   # Importance of demand feasibility\n    gamma = 0.25  # Importance of cluster affinity\n\n    # Heuristic based on inverse distance\n    distance_heuristic = 1 / distance_matrix\n\n    # Heuristic based on demand feasibility\n    demand_heuristic = np.ones_like(distance_matrix, dtype=float)\n    for i in range(1, n):  # Skip depot\n        for j in range(1, n): # Skip depot\n            if i != j:\n                if demands[i] + demands[j] > capacity:\n                    demand_heuristic[i, j] = 0.1 # severely penalize\n                else:\n                     demand_heuristic[i, j] = 1 - (demands[i] + demands[j]) / (2*capacity)\n\n    # Heuristic based on spatial clustering\n    cluster_heuristic = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(1, n):\n        for j in range(1, n):\n            if i != j:\n                 # Density based on inverse distance to other nodes\n                 density_i = np.sum(1 / distance_matrix[i, 1:]) - (1/ distance_matrix[i,i]) if np.sum(1 / distance_matrix[i, 1:]) < np.inf else 0 # Avoid division by zero\n                 density_j = np.sum(1 / distance_matrix[j, 1:]) - (1/ distance_matrix[j,j]) if np.sum(1 / distance_matrix[j, 1:]) < np.inf else 0 # Avoid division by zero\n                 # penalize connections to nodes with much lower density\n                 cluster_heuristic[i,j] = min(density_i, density_j) / max(density_i, density_j) if max(density_i, density_j) > 0 else 0 #Between 0 and 1\n                 \n    #Gravitational attraction to the depot\n    depot_attraction = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(1, n):\n        depot_attraction[0, i] = 1/(distance_matrix[0, i])\n        depot_attraction[i, 0] = 1/(distance_matrix[i, 0]) # Symmetric\n\n    # Combine heuristics with weights\n    heuristic_matrix = (alpha * distance_heuristic +\n                        beta * demand_heuristic +\n                        gamma * cluster_heuristic +\n                        0.1 * depot_attraction)\n\n    # Depot edges should be generally favored to return to depot\n    for i in range(1, n):\n        heuristic_matrix[0, i] *= 1.2  # Favor leaving the depot\n        heuristic_matrix[i, 0] *= 1.2  # Favor returning to the depot\n\n    # Normalize the heuristic matrix to [0, 1]\n    max_heuristic = np.max(heuristic_matrix)\n    min_heuristic = np.min(heuristic_matrix)\n    heuristic_matrix = (heuristic_matrix - min_heuristic) / (max_heuristic - min_heuristic) if max_heuristic > min_heuristic else np.zeros_like(heuristic_matrix)\n\n    return heuristic_matrix\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray, coordinates: np.ndarray, demands: np.ndarray, capacity: int) -> np.ndarray:\n\n    \"\"\"\n    Combines distance, demand, and angle considerations, with adaptive weighting, and depot proximity.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros((n, n))\n    scaled_distance = distance_matrix / np.max(distance_matrix)\n\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                continue\n\n            heuristic_matrix[i, j] = 1 / (scaled_distance[i, j] + 0.0001)\n\n            demand_penalty = (demands[i] + demands[j]) / (2 * capacity)\n            heuristic_matrix[i, j] /= (demand_penalty + 0.5)\n\n            if demands[i] > 0.7 * capacity or demands[j] > 0.7 * capacity:\n                heuristic_matrix[i, j] /= 5\n\n            if i == 0 or j == 0:\n                heuristic_matrix[i, j] *= 1.5\n            \n            if 0.15*capacity < demands[i] < 0.45*capacity or 0.15*capacity < demands[j] < 0.45*capacity:\n                heuristic_matrix[i, j] *= 1.1\n    \n            if i != 0 and j != 0:\n                vector_ij = coordinates[j] - coordinates[i]\n                vector_0j = coordinates[j] - coordinates[0]\n                dot_product = np.dot(vector_ij, vector_0j)\n                magnitude_ij = np.linalg.norm(vector_ij)\n                magnitude_0j = np.linalg.norm(vector_0j)\n\n                if magnitude_ij > 0 and magnitude_0j > 0:\n                    cos_angle = dot_product / (magnitude_ij * magnitude_0j)\n                    cos_angle = np.clip(cos_angle, -1.0, 1.0)\n                    angle = np.arccos(cos_angle)\n                    angle_factor = 1 - (angle / np.pi)\n                    heuristic_matrix[i, j] *= (0.5 + angle_factor)/1.5 #angle factor 0.5 to 1.5\n                \n\n    return heuristic_matrix\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), there's no difference.\nComparing (3rd) vs (4th), there's no difference.\nComparing (1st) vs (5th), the former utilizes multiple heuristics (distance, demand feasibility, depot proximity, gravitational attraction, savings, sparsification) with adaptive weights, while the latter combines distance, demand, angle to depot, and angle between nodes with fixed weights and capacity penalty.\nComparing (5th) vs (6th), there's no difference.\nComparing (5th) vs (7th), there's no difference.\nComparing (1st) vs (8th), (1st) uses sparsification based on k-nearest neighbors and a dynamic threshold, (8th) uses simpler sparsification based on a mean threshold. (1st) combines with fixed weights while (8th) uses adaptive weighting.\nComparing (1st) vs (9th), (1st) is more complex, (9th) appears simpler.\nComparing (9th) vs (10th), there's no difference.\nComparing (9th) vs (11th), there's no difference.\nComparing (1st) vs (12th), (1st) uses more diverse heuristics like gravitational attraction and savings, along with k-NN sparsification, while (12th) focuses on distance, demand, spatial clustering and depot attraction. Adaptive scaling is present in both.\nComparing (12th) vs (13th), there's no difference.\nComparing (12th) vs (14th), there's no difference.\nComparing (1st) vs (15th), (1st) uses more heuristics and better sparsification.\nComparing (15th) vs (16th), there's no difference.\nComparing (1st) vs (17th), (1st) has more diverse heuristics and better sparsification. (17th) uses unusual demand scaling with distance.\nComparing (1st) vs (18th), (1st) has more diverse heuristics and better sparsification.\nComparing (1st) vs (19th), (1st) is more complex and includes sparsification while (19th) lacks it.\nComparing (19th) vs (20th), there's no difference.\nComparing (second worst) vs (worst), (19th) and (20th) are identical.\n\nOverall: The better heuristics combine multiple factors (distance, demand, depot proximity, angles, savings) using adaptive weights. Sparsification using k-NN or dynamic thresholds is preferred. Simpler heuristics focusing on fewer factors or fixed weights tend to perform worse.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a refined perspective on self-reflection to guide heuristic design, focusing on actionable insights:\n\n*   **Keywords:** Factor diversity, adaptive weighting, normalization, spatial relationships, sparsification, problem characteristics.\n\n*   **Advice:** Explore clustering techniques, gravitational attraction principles, and rigorous normalization strategies within your heuristics. Adapt weights based on real-time problem characteristics, not just initial assumptions.\n\n*   **Avoid:** Solely relying on distance/demand, static weighting schemes, ignoring spatial context, premature convergence.\n\n*   **Explanation:** Broadening the factor base and adapting to problem nuances are crucial for robust and high-performing heuristics. Normalization stabilizes weighting.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}