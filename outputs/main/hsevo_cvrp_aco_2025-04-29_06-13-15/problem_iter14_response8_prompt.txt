{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Capacitated Vehicle Routing Problem (CVRP) via stochastic solution sampling. CVRP requires finding the shortest path that visits all given nodes and returns to the starting node. Each node has a demand and each vehicle has a capacity. The total demand of the nodes visited by a vehicle cannot exceed the vehicle capacity. When the total demand exceeds the vehicle capacity, the vehicle must return to the starting node.\nThe `heuristics` function takes as input a distance matrix (shape: n by n), Euclidean coordinates of nodes (shape: n by 2), a vector of customer demands (shape: n), and the integer capacity of vehicle capacity. It returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the distance_matrix. The depot node is indexed by 0.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray, coordinates: np.ndarray, demands: np.ndarray, capacity: int) -> np.ndarray:\n\n    \"\"\"CVRP heuristics: Combines distance, demand, depot proximity, savings, clustering, and adaptive weighting.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros((n, n))\n    epsilon = 1e-6\n\n    # 1. Distance component (normalized)\n    distance_heuristic = 1 / (distance_matrix + epsilon)\n    distance_heuristic = (distance_heuristic - np.min(distance_heuristic)) / (np.max(distance_heuristic) - np.min(distance_heuristic) + epsilon)\n\n    # 2. Demand Feasibility\n    demand_heuristic = np.ones((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                demand_heuristic[i, j] = 0\n            if demands[i] + demands[j] > capacity and i == 0:\n                demand_heuristic[i, j] = 0.1\n            elif demands[i] + demands[j] > capacity and j == 0:\n                demand_heuristic[i, j] = 0.1\n            elif i != 0 and demands[i] > capacity or j != 0 and demands[j] > capacity:\n                demand_heuristic[i,j] = 0.05\n\n    # 3. Depot Proximity (normalized and enhanced)\n    depot_heuristic = np.zeros((n, n))\n    for i in range(1, n):\n        avg_distance = np.sum(distance_matrix[i, 1:]) / (n - 2) if n > 2 else distance_matrix[i, 0]\n        depot_preference = (avg_distance / np.max(distance_matrix)) * (demands[i] / capacity)\n        depot_heuristic[i, 0] = depot_preference\n        depot_heuristic[0, i] = depot_preference\n    depot_heuristic = (depot_heuristic - np.min(depot_heuristic)) / (np.max(depot_heuristic) - np.min(depot_heuristic) + epsilon)\n\n\n    # 4. Savings Heuristic (normalized)\n    savings_heuristic = np.zeros((n, n))\n    for i in range(1, n):\n        for j in range(i + 1, n):\n            savings = distance_matrix[i, 0] + distance_matrix[0, j] - distance_matrix[i, j]\n            savings_heuristic[i, j] = savings\n            savings_heuristic[j, i] = savings  # Savings are symmetric\n    savings_heuristic = (savings_heuristic - np.min(savings_heuristic)) / (np.max(savings_heuristic) - np.min(savings_heuristic) + epsilon)\n\n    # 5. Clustering Encouragement (based on spatial proximity)\n    clustering_heuristic = np.zeros((n, n))\n    for i in range(1, n):\n        for j in range(i + 1, n):\n            dist_ij = distance_matrix[i, j]\n            # Encourage connections between nodes that are close relative to their distance to the depot\n            clustering_heuristic[i, j] = clustering_heuristic[j, i] = np.exp(-dist_ij / (distance_matrix[i, 0] + distance_matrix[j, 0] + epsilon))\n\n    # Adaptive Weighting (adjust based on problem size/characteristics)\n    n_customers = n - 1\n    capacity_ratio = np.sum(demands[1:]) / (capacity * (n - 1))  # Approximate tightness\n    alpha = 0.3 #distance\n    beta = 0.15 #depot proximity\n    gamma = 0.20 #savings\n    delta = 0.15 #clustering\n    phi = 0.05 # angle cost\n\n    # Adjust weights (example: emphasize depot proximity more for larger problems)\n    if n_customers > 50:\n        beta += 0.05\n        alpha -= 0.05 #reduce alpha a bit\n    if capacity < np.mean(demands) * 5: # Tighter capacity constraints, increase savings\n        gamma += 0.05\n    if capacity_ratio > 0.75:\n        alpha -= 0.05\n        delta += 0.05\n\n    # 9. Angle Cost\n    angle_heuristic = np.zeros((n, n))\n    for i in range(1, n):\n        for j in range(1, n):\n            if i != j:\n                # Calculate angle between vectors depot->i and depot->j\n                vector_i = coordinates[i] - coordinates[0]\n                vector_j = coordinates[j] - coordinates[0]\n                dot_product = np.dot(vector_i, vector_j)\n                magn_i = np.linalg.norm(vector_i)\n                magn_j = np.linalg.norm(vector_j)\n                if magn_i * magn_j == 0:\n                    angle = 0\n                else:\n                    angle = np.arccos(dot_product / (magn_i * magn_j + epsilon))\n                angle_cost = angle / np.pi  # Normalize angle to [0, 1]\n                angle_heuristic[i, j] = 1 - angle_cost  # Prefer smaller angles\n    angle_heuristic = (angle_heuristic - np.min(angle_heuristic)) / (np.max(angle_heuristic) - np.min(angle_heuristic) + epsilon) # Normalize\n\n\n\n    # Combine the heuristics\n    heuristics = (alpha * distance_heuristic * demand_heuristic +\n                  beta * depot_heuristic +\n                  gamma * savings_heuristic +\n                  delta * clustering_heuristic +\n                  phi * angle_heuristic)\n\n    # 6. Sparsification (adaptive threshold based on heuristic values)\n    threshold = np.percentile(heuristics[heuristics > 0], 40) # Increased sparsity\n\n    k_nearest = 12 #consider k-nearest neighbors, increase it a bit\n\n    for i in range(n):\n        nearest_neighbors = np.argsort(distance_matrix[i, :])[1:k_nearest+1]\n        for j in range(n):\n            if heuristics[i, j] < threshold and j not in nearest_neighbors:\n                heuristics[i, j] = 0\n\n    # Normalize to be between 0 and 1\n    max_val = np.max(heuristics)\n    if max_val > 0:\n        heuristics = heuristics / max_val\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray, coordinates: np.ndarray, demands: np.ndarray, capacity: int) -> np.ndarray:\n\n    \"\"\"CVRP heuristics: Combines distance, demand, savings, depot proximity, and adaptive sparsification.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros((n, n))\n    epsilon = 1e-6\n\n    # Distance component\n    distance_heuristic = 1 / (distance_matrix + epsilon)\n\n    # Demand Feasibility\n    demand_heuristic = np.ones((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                demand_heuristic[i, j] = 0\n            if demands[i] + demands[j] > capacity and i == 0:\n                demand_heuristic[i, j] = 0.1\n            elif demands[i] + demands[j] > capacity and j == 0:\n                demand_heuristic[i, j] = 0.1\n            elif i != 0 and demands[i] > capacity or j != 0 and demands[j] > capacity:\n                demand_heuristic[i,j] = 0.05\n\n    # Depot Proximity\n    depot_heuristic = np.zeros((n, n))\n    for i in range(1, n):\n        avg_distance = np.sum(distance_matrix[i, 1:]) / (n - 2) if n > 2 else distance_matrix[i,0]\n        depot_preference = (avg_distance / np.max(distance_matrix)) * (demands[i] / capacity)\n        depot_heuristic[i, 0] = depot_preference\n        depot_heuristic[0, i] = depot_preference\n\n    # Savings Heuristic\n    savings_heuristic = np.zeros((n, n))\n    for i in range(1, n):\n        for j in range(i + 1, n):\n            savings = distance_matrix[i, 0] + distance_matrix[0, j] - distance_matrix[i, j]\n            savings_heuristic[i, j] = savings\n            savings_heuristic[j, i] = savings  # Savings are symmetric\n\n    # Adaptive Weighting\n    capacity_ratio = np.sum(demands[1:]) / (capacity * (n - 1))  # Approximate tightness\n    alpha = 0.3\n    beta = 0.15\n    delta = 0.1\n\n    alpha += 0.1 * capacity_ratio  # Increase importance of distance if capacity is tight\n    beta -= 0.05 * capacity_ratio  # Decrease importance of going to depot if capacity is tight\n\n    heuristics = alpha * distance_heuristic * demand_heuristic + beta * depot_heuristic + delta * savings_heuristic\n\n    # Adaptive Sparsification: Adjust threshold based on capacity ratio\n    k_nearest = 10  # consider only k-nearest neighbors.\n    threshold_percentile = 30 + 10 * capacity_ratio  # Adjust percentile based on tightness\n    threshold = np.percentile(heuristics[heuristics > 0], threshold_percentile)\n\n    for i in range(n):\n        nearest_neighbors = np.argsort(distance_matrix[i, :])[1:k_nearest+1]\n\n        for j in range(n):\n            if heuristics[i, j] < threshold and j not in nearest_neighbors:\n                heuristics[i, j] = 0\n\n    max_val = np.max(heuristics)\n    if max_val > 0:\n        heuristics = heuristics / max_val\n\n    return heuristics\n\n### Analyze & experience\n- *   **(1st) vs (2nd):** These heuristics are identical.\n*   **(1st/2nd) vs (3rd):** These heuristics are identical.\n*   **(1st/2nd/3rd) vs (4th):** The 4th heuristic introduces several key refinements:\n    *   **Distance:** Uses inverse square distance to emphasize shorter edges more.\n    *   **Demand Feasibility:** Implements stricter penalties for capacity violations and simulates route capacity.\n    *   **Depot Proximity:** Applies depot preference contextually to isolated nodes.\n    *   **Savings:** Aggressively prioritizes high-saving merges.\n    *   **Node Similarity:** Encourages connections based on demand/distance profiles.\n    *   **Adaptive Weighting:** Adjusts weights dynamically based on network connectivity.\n    *   **Sparsification:** Removes edges using percentile thresholding and the triangle inequality.\n*   **(4th) vs (5th):** Heuristic 5 introduces angle-based clustering and normalizes individual heuristic components before combining them. Heuristic 4 simulates routing to estimate the remaining capacity and implements triangle inequality during sparsification.\n*   **(5th) vs (6th):** The heuristics are identical.\n*   **(5th/6th) vs (7th):** Heuristic 7 introduces demand density, adaptively weights based on problem size, and normalizes individual heuristics components before combining them.\n*   **(7th) vs (8th):** The 8th heuristic focuses on normalizing all heuristic components, encouraging clustering based on spatial proximity, and adaptively weighting based on problem size and characteristics.\n*   **(8th) vs (9th):** Heuristic 9 simplifies by removing the angle cost and clustering components.\n*   **(9th) vs (10th):** The heuristics are identical.\n*   **(9th/10th) vs (11th):** The 11th heuristic normalizes the heuristic components before combining them and reintroduces a clustering coefficient approximation.\n*   **(11th) vs (12th):** The heuristics are identical.\n*   **(11th/12th) vs (13th):** Heuristic 13 focuses on angles from the depot, demand-distance interaction, node diversity weighting, and adjusts sparsification criteria.\n*   **(13th) vs (14th):** Heuristic 14 normalizes all components.\n*   **(14th) vs (15th):** The 15th heuristic explicitly includes and normalizes an \"angle cost\" heuristic, prioritizing nodes with smaller angles from the depot.\n*   **(15th) vs (16th):** The heuristics are identical.\n*   **(16th) vs (17th):** The heuristics are identical.\n*   **(16th/17th) vs (18th):** The 18th heuristic adds a node centrality measure and normalizes the clustering heuristic and centrality heuristic, and also adapts the k value used in the clustering.\n*   **(18th) vs (19th):** The heuristics are identical.\n*   **(19th) vs (20th):** The heuristics are identical.\n\nOverall: The best heuristics combine several factors: Distance, Demand, Savings, Depot Proximity, Clustering, Angle Cost, Centrality, Adaptive Weighting, Sparsification and Normalization to ensure components are on the same scale. Demand feasibility and distance are the most significant.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a redefinition of \"Current Self-Reflection\" optimized for designing better heuristics, while avoiding the pitfalls of the \"Ineffective Self-Reflection\":\n\n*   **Keywords:** Problem-specific, Adaptive, Normalization, Sparsification, Diverse Factors, Robustness, Solution Quality Influence, Spatial relationships, Clustering, Gravitational attraction.\n\n*   **Advice:** Actively seek and incorporate problem-specific knowledge, especially spatial relationships, clustering, and gravitational attraction; meticulously normalize individual heuristic components; and implement adaptive weighting schemes based on *multiple* problem characteristics and consider sparsification.\n\n*   **Avoid:** Generic \"consider diverse factors.\" Don't just mention adaptive weighting; focus on making it responsive to multiple problem features.\n\n*   **Explanation:** The goal is to move beyond vague recommendations to actionable strategies by emphasizing the importance of normalization and adaptive weighting schemes to multiple problem characteristics, leading to enhanced robustness and adaptability.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}