{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Capacitated Vehicle Routing Problem (CVRP) via stochastic solution sampling. CVRP requires finding the shortest path that visits all given nodes and returns to the starting node. Each node has a demand and each vehicle has a capacity. The total demand of the nodes visited by a vehicle cannot exceed the vehicle capacity. When the total demand exceeds the vehicle capacity, the vehicle must return to the starting node.\nThe `heuristics` function takes as input a distance matrix (shape: n by n), Euclidean coordinates of nodes (shape: n by 2), a vector of customer demands (shape: n), and the integer capacity of vehicle capacity. It returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the distance_matrix. The depot node is indexed by 0.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray, coordinates: np.ndarray, demands: np.ndarray, capacity: int) -> np.ndarray:\n\n    \"\"\"\n    CVRP heuristic: Combines distance, demand, angle, and depot proximity.\n    Sparsifies the matrix by setting unpromising elements to zero.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros((n, n))\n    epsilon = 1e-6\n\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                continue\n\n            dist_h = 1 / (distance_matrix[i, j] + epsilon)\n            demand_h = 1 - np.clip((demands[i] + demands[j]) / (2 * capacity + epsilon), 0, 1)\n            if i == 0 or j == 0:\n                demand_h = 1.0\n\n            angle_h = 1.0\n            if i != 0 and j != 0:\n                angle_h = 0.0\n                nn_idx = -1; dist = np.inf\n                for k in range(n):\n                    if k != i and k != j:\n                        if distance_matrix[i, k] < dist:\n                            dist = distance_matrix[i, k]\n                            nn_idx = k\n\n                if nn_idx != -1:\n                    a = distance_matrix[i, j]\n                    b = distance_matrix[i, nn_idx]\n                    c = distance_matrix[j, nn_idx]\n                    try:\n                        angle_h = (a**2 + b**2 - c**2) / (2 * a * b + epsilon)\n                        angle_h = np.clip(angle_h, -1.0, 1.0)\n                        angle_h = (angle_h + 1.0) / 2.0\n                    except:\n                        angle_h = 0.5\n\n            depot_prox_h = 1 - np.clip((distance_matrix[0, i] + distance_matrix[0, j]) / (2 * np.max(distance_matrix) + epsilon), 0, 1)\n\n            heuristics[i, j] = (\n                0.5 * dist_h +\n                0.15 * demand_h +\n                0.25 * angle_h +\n                0.1 * depot_prox_h\n            )\n\n    # Sparsify the matrix (optional, but can be beneficial)\n    threshold = np.mean(heuristics) * 0.2  # Dynamic threshold based on the mean\n    heuristics[heuristics < threshold] = 0\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray, coordinates: np.ndarray, demands: np.ndarray, capacity: int) -> np.ndarray:\n\n    \"\"\"Enhanced heuristic combining distance, demand, angle, and depot proximity.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # Distance and demand scaling\n    demand_scaled_distance = distance_matrix * (demands[np.newaxis, :] + demands[:, np.newaxis]) / (2 * capacity)\n    demand_scaled_distance = np.where(demand_scaled_distance == 0, np.inf, demand_scaled_distance)\n\n    # Depot proximity\n    depot_x, depot_y = coordinates[0]\n    for i in range(1, n):\n        for j in range(1, n):\n            if i == j:\n                continue\n            node_i_x, node_i_y = coordinates[i]\n            node_j_x, node_j_y = coordinates[j]\n\n            angle_i = np.arctan2(node_i_y - depot_y, node_i_x - depot_x)\n            angle_j = np.arctan2(node_j_y - depot_y, node_j_x - depot_x)\n            angle_diff = np.abs(angle_i - angle_j)\n            angle_diff = np.minimum(angle_diff, 2 * np.pi - angle_diff)\n            angular_penalty = (angle_diff / np.pi)\n\n            depot_proximity = 1 - 0.9 * max(distance_matrix[0,i], distance_matrix[0,j]) / np.mean(distance_matrix)\n\n            heuristics[i, j] = 1 / (demand_scaled_distance[i, j] + 0.1 * angular_penalty) + 0.5 * depot_proximity # combine, add proximity\n            heuristics[j, i] = heuristics[i, j]\n\n    # Depot edges prioritization\n    for i in range(1, n):\n        heuristics[0, i] = 1.0 / (distance_matrix[0, i] * demands[i] / capacity + 1e-6)\n        heuristics[i, 0] = heuristics[0, i]\n\n    # No self-loops\n    for i in range(n):\n        heuristics[i, i] = 0\n\n    return heuristics\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), there's no difference.\nComparing (3rd) vs (4th), there's no difference.\nComparing (1st) vs (5th), the former utilizes multiple heuristics (distance, demand feasibility, depot proximity, gravitational attraction, savings, sparsification) with adaptive weights, while the latter combines distance, demand, angle to depot, and angle between nodes with fixed weights and capacity penalty.\nComparing (5th) vs (6th), there's no difference.\nComparing (5th) vs (7th), there's no difference.\nComparing (1st) vs (8th), (1st) uses sparsification based on k-nearest neighbors and a dynamic threshold, (8th) uses simpler sparsification based on a mean threshold. (1st) combines with fixed weights while (8th) uses adaptive weighting.\nComparing (1st) vs (9th), (1st) is more complex, (9th) appears simpler.\nComparing (9th) vs (10th), there's no difference.\nComparing (9th) vs (11th), there's no difference.\nComparing (1st) vs (12th), (1st) uses more diverse heuristics like gravitational attraction and savings, along with k-NN sparsification, while (12th) focuses on distance, demand, spatial clustering and depot attraction. Adaptive scaling is present in both.\nComparing (12th) vs (13th), there's no difference.\nComparing (12th) vs (14th), there's no difference.\nComparing (1st) vs (15th), (1st) uses more heuristics and better sparsification.\nComparing (15th) vs (16th), there's no difference.\nComparing (1st) vs (17th), (1st) has more diverse heuristics and better sparsification. (17th) uses unusual demand scaling with distance.\nComparing (1st) vs (18th), (1st) has more diverse heuristics and better sparsification.\nComparing (1st) vs (19th), (1st) is more complex and includes sparsification while (19th) lacks it.\nComparing (19th) vs (20th), there's no difference.\nComparing (second worst) vs (worst), (19th) and (20th) are identical.\n\nOverall: The better heuristics combine multiple factors (distance, demand, depot proximity, angles, savings) using adaptive weights. Sparsification using k-NN or dynamic thresholds is preferred. Simpler heuristics focusing on fewer factors or fixed weights tend to perform worse.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a refined perspective on self-reflection to guide heuristic design, focusing on actionable insights:\n\n*   **Keywords:** Factor diversity, adaptive weighting, normalization, spatial relationships, sparsification, problem characteristics.\n\n*   **Advice:** Explore clustering techniques, gravitational attraction principles, and rigorous normalization strategies within your heuristics. Adapt weights based on real-time problem characteristics, not just initial assumptions.\n\n*   **Avoid:** Solely relying on distance/demand, static weighting schemes, ignoring spatial context, premature convergence.\n\n*   **Explanation:** Broadening the factor base and adapting to problem nuances are crucial for robust and high-performing heuristics. Normalization stabilizes weighting.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}