{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Capacitated Vehicle Routing Problem (CVRP) via stochastic solution sampling. CVRP requires finding the shortest path that visits all given nodes and returns to the starting node. Each node has a demand and each vehicle has a capacity. The total demand of the nodes visited by a vehicle cannot exceed the vehicle capacity. When the total demand exceeds the vehicle capacity, the vehicle must return to the starting node.\nThe `heuristics` function takes as input a distance matrix (shape: n by n), Euclidean coordinates of nodes (shape: n by 2), a vector of customer demands (shape: n), and the integer capacity of vehicle capacity. It returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the distance_matrix. The depot node is indexed by 0.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray, coordinates: np.ndarray, demands: np.ndarray, capacity: int) -> np.ndarray:\n\n    \"\"\"\n    Heuristics for solving Capacitated Vehicle Routing Problem (CVRP) via stochastic solution sampling.\n\n    This version incorporates:\n    1.  Distance: Edges with shorter distances are preferred.\n    2.  Demand Feasibility: Penalizes edges that would immediately violate capacity constraints if used early in a route.\n    3.  Depot Proximity: Prioritizes returning to the depot from nodes that are far from other customers and have high demand.\n    4.  Savings Heuristic Integration: Incorporates the savings heuristic to encourage merging routes.\n    5.  Sparsification: Sets unpromising edges to zero to focus the search.\n    6.  Clustering Coefficient: Encourages connections between nodes within local clusters\n    7.  Adaptive Weighting: Adjusts weights based on problem characteristics (e.g., tightness of capacity) and node characteristics.\n    8.  Orientation: Nodes in similar direction from the depot are more likely to be on the same route\n    9.  Demand-Distance interaction: Penalize long edges between high-demand nodes.\n    Args:\n        distance_matrix (np.ndarray): Distance matrix between nodes (shape: n x n).\n        coordinates (np.ndarray): Euclidean coordinates of nodes (shape: n x 2).\n        demands (np.ndarray): Vector of customer demands (shape: n). The demand of depot is demands[0].\n        capacity (int): Vehicle capacity.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution (shape: n x n).\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros((n, n))\n    epsilon = 1e-6\n\n    # 1. Distance component\n    distance_heuristic = 1 / (distance_matrix + epsilon)\n\n    # 2. Demand Feasibility\n    demand_heuristic = np.ones((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                demand_heuristic[i, j] = 0\n            if demands[i] + demands[j] > capacity and (i == 0 or j == 0):  # Only check at the depot\n                demand_heuristic[i, j] = 0.1 # Lower, as the depot is important.\n            elif i != 0 and demands[i] > capacity or j != 0 and demands[j] > capacity:\n                demand_heuristic[i,j] = 0.05 # Much Lower, avoid overload\n\n    # 3. Depot Proximity\n    depot_heuristic = np.zeros((n, n))\n    for i in range(1, n):\n        avg_distance = np.sum(distance_matrix[i, 1:]) / (n - 2) if n > 2 else distance_matrix[i,0]\n        depot_preference = (avg_distance / np.max(distance_matrix)) * (demands[i] / capacity)\n        depot_heuristic[i, 0] = depot_preference\n        depot_heuristic[0, i] = depot_preference\n\n    # 4. Savings Heuristic\n    savings_heuristic = np.zeros((n, n))\n    for i in range(1, n):\n        for j in range(i + 1, n):\n            savings = distance_matrix[i, 0] + distance_matrix[0, j] - distance_matrix[i, j]\n            savings_heuristic[i, j] = savings\n            savings_heuristic[j, i] = savings  # Savings are symmetric\n\n    # 5. Clustering Coefficient approximation (Local Density)\n    clustering_heuristic = np.zeros((n, n))\n    k_nearest = 6  # Consider only k-nearest neighbors for clustering\n    for i in range(1, n):\n        nearest_neighbors = np.argsort(distance_matrix[i, 1:])[:k_nearest] + 1  # k-nearest neighbors excluding depot, adjusting index\n        for j in range(1, n):\n            if i != j:\n                common_neighbors = 0\n                for neighbor_i in nearest_neighbors:\n                    if neighbor_i in (np.argsort(distance_matrix[j, 1:])[:k_nearest] + 1):\n                        common_neighbors += 1\n                clustering_heuristic[i, j] = common_neighbors / k_nearest\n\n    # 6. Orientation Heuristic\n    orientation_heuristic = np.zeros((n, n))\n    for i in range(1, n):\n        for j in range(1, n):\n            if i != j:\n                # Calculate angles from depot\n                angle_i = np.arctan2(coordinates[i, 1] - coordinates[0, 1], coordinates[i, 0] - coordinates[0, 0])\n                angle_j = np.arctan2(coordinates[j, 1] - coordinates[0, 1], coordinates[j, 0] - coordinates[0, 0])\n\n                # Angle difference\n                angle_diff = np.abs(angle_i - angle_j)\n                angle_diff = np.min([angle_diff, 2 * np.pi - angle_diff])  # Ensure smaller angle\n\n                # Prioritize similar orientations (smaller angle difference)\n                orientation_heuristic[i, j] = np.exp(-angle_diff) # Exponential decay\n\n    # 7. Demand-Distance Interaction\n    demand_distance_heuristic = np.ones((n, n))\n    for i in range(1, n):\n        for j in range(1, n):\n             demand_distance_heuristic[i, j] = np.exp(- (demands[i] + demands[j]) * distance_matrix[i, j] / capacity) # Penalize long edges between high-demand nodes.\n\n    # 8. Adaptive Weighting\n    capacity_ratio = np.sum(demands[1:]) / (capacity * (n - 1))  # Approximate tightness\n    alpha = 0.3\n    beta = 0.15\n    gamma = 0.05\n    delta = 0.15\n    eta = 0.1\n    mu = 0.1\n    zeta = 0.15\n    theta = 0.1\n\n    # Adjust alpha (distance) and beta (depot) based on capacity ratio\n    alpha += 0.1 * capacity_ratio  # Increase importance of distance if capacity is tight\n    beta -= 0.05 * capacity_ratio  # Decrease importance of going to depot if capacity is tight (more local clusters)\n\n    # Node Diversity Weighting\n    demand_std = np.std(demands[1:])\n    distance_to_depot_std = np.std(distance_matrix[0, 1:])\n\n    if demand_std > 0 and distance_to_depot_std > 0:\n        alpha += 0.05 * (demand_std / np.mean(demands[1:]))\n        beta += 0.05 * (distance_to_depot_std / np.mean(distance_matrix[0,1:]))\n\n\n    # Combine the heuristics with adaptive weights. Weights could be tuned by a metaheuristic\n    heuristics = alpha * distance_heuristic * demand_heuristic + beta * depot_heuristic + gamma * savings_heuristic + delta * clustering_heuristic + eta * orientation_heuristic + mu * demand_distance_heuristic\n\n    # 9. Sparsification\n    # Only keep edges with a heuristic value above a certain threshold or that are among the k-nearest neighbors.\n    k_nearest = min(10, n - 1)  # consider only k-nearest neighbors.\n    threshold = np.percentile(heuristics[heuristics > 0], 30)  # Dynamic threshold (e.g., 30th percentile)\n\n    for i in range(n):\n        # Find k-nearest neighbors\n        nearest_neighbors = np.argsort(heuristics[i, :])[::-1][1:k_nearest+1] # excluding self loop\n\n        for j in range(n):\n            if heuristics[i, j] < threshold and j not in nearest_neighbors:\n                heuristics[i, j] = 0\n\n    # Normalize to be between 0 and 1\n    max_val = np.max(heuristics)\n    if max_val > 0:\n        heuristics = heuristics / max_val\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray, coordinates: np.ndarray, demands: np.ndarray, capacity: int) -> np.ndarray:\n\n    \"\"\"CVRP heuristics: Combines distance, demand, depot proximity, savings, clustering, centrality, adaptive weighting, sparsification.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros((n, n))\n    epsilon = 1e-6\n\n    # 1. Distance (normalized)\n    distance_heuristic = 1 / (distance_matrix + epsilon)\n    distance_heuristic = (distance_heuristic - np.min(distance_heuristic)) / (np.max(distance_heuristic) - np.min(distance_heuristic) + epsilon)\n\n    # 2. Demand Feasibility\n    demand_heuristic = np.ones((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                demand_heuristic[i, j] = 0\n            if demands[i] + demands[j] > capacity and i == 0:\n                demand_heuristic[i, j] = 0.1\n            elif demands[i] + demands[j] > capacity and j == 0:\n                demand_heuristic[i, j] = 0.1\n            elif i != 0 and demands[i] > capacity or j != 0 and demands[j] > capacity:\n                demand_heuristic[i,j] = 0.05\n\n    # 3. Depot Proximity (normalized)\n    depot_heuristic = np.zeros((n, n))\n    for i in range(1, n):\n        avg_distance = np.sum(distance_matrix[i, 1:]) / (n - 2) if n > 2 else distance_matrix[i, 0]\n        depot_preference = (avg_distance / np.max(distance_matrix)) * (demands[i] / capacity)\n        depot_heuristic[i, 0] = depot_preference\n        depot_heuristic[0, i] = depot_preference\n    depot_heuristic = (depot_heuristic - np.min(depot_heuristic)) / (np.max(depot_heuristic) - np.min(depot_heuristic) + epsilon)\n\n    # 4. Savings (normalized)\n    savings_heuristic = np.zeros((n, n))\n    for i in range(1, n):\n        for j in range(i + 1, n):\n            savings = distance_matrix[i, 0] + distance_matrix[0, j] - distance_matrix[i, j]\n            savings_heuristic[i, j] = savings\n            savings_heuristic[j, i] = savings\n    savings_heuristic = (savings_heuristic - np.min(savings_heuristic)) / (np.max(savings_heuristic) - np.min(savings_heuristic) + epsilon)\n\n    # 5. Clustering (adaptive k)\n    clustering_heuristic = np.zeros((n, n))\n    k_nearest = min(10, n - 1)\n    for i in range(1, n):\n        nearest_neighbors = np.argsort(distance_matrix[i, 1:])[:k_nearest] + 1\n        for j in range(1, n):\n            if i != j:\n                common_neighbors = 0\n                for neighbor_i in nearest_neighbors:\n                    if neighbor_i in (np.argsort(distance_matrix[j, 1:])[:k_nearest] + 1):\n                        common_neighbors += 1\n                clustering_heuristic[i, j] = common_neighbors / k_nearest\n    clustering_heuristic = clustering_heuristic / np.max(clustering_heuristic)\n\n    # 6. Node Centrality (normalized)\n    centrality = np.zeros(n)\n    for i in range(1, n):\n        centrality[i] = np.sum(distance_matrix[i, 1:])\n    centrality = np.max(centrality) / (centrality + epsilon)\n    centrality[0] = 1\n    centrality_heuristic = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            centrality_heuristic[i, j] = (centrality[i] + centrality[j]) / 2.0\n    centrality_heuristic = centrality_heuristic / np.max(centrality_heuristic)\n\n    # Adaptive Weighting\n    capacity_ratio = np.sum(demands[1:]) / (capacity * (n - 1))\n    num_nodes_factor = n / 100.0\n\n    alpha = 0.2\n    beta = 0.15\n    gamma = 0.1\n    delta = 0.15\n    eta = 0.2\n\n    alpha += 0.05 * capacity_ratio + 0.02 * num_nodes_factor\n    beta -= 0.03 * capacity_ratio - 0.01 * num_nodes_factor\n    gamma += 0.02 * capacity_ratio + 0.03 * num_nodes_factor\n    delta += 0.01 * capacity_ratio + 0.02 * num_nodes_factor\n    eta -= 0.01 * capacity_ratio - 0.02 * num_nodes_factor\n\n    # Combine heuristics\n    heuristics = (alpha * distance_heuristic * demand_heuristic +\n                  beta * depot_heuristic +\n                  gamma * savings_heuristic +\n                  delta * clustering_heuristic +\n                  eta * centrality_heuristic)\n\n    # Sparsification (adaptive threshold)\n    k_nearest = min(12, n - 1)\n    threshold = np.percentile(heuristics[heuristics > 0], 40)\n\n    for i in range(n):\n        nearest_neighbors = np.argsort(distance_matrix[i, :])[1:k_nearest+1]\n        for j in range(n):\n            if heuristics[i, j] < threshold and j not in nearest_neighbors:\n                heuristics[i, j] = 0\n\n    # Normalize\n    max_val = np.max(heuristics)\n    if max_val > 0:\n        heuristics = heuristics / max_val\n\n    return heuristics\n\n### Analyze & experience\n- *   **(1st) vs (2nd):** These heuristics are identical.\n*   **(1st/2nd) vs (3rd):** These heuristics are identical.\n*   **(1st/2nd/3rd) vs (4th):** The 4th heuristic introduces several key refinements:\n    *   **Distance:** Uses inverse square distance to emphasize shorter edges more.\n    *   **Demand Feasibility:** Implements stricter penalties for capacity violations and simulates route capacity.\n    *   **Depot Proximity:** Applies depot preference contextually to isolated nodes.\n    *   **Savings:** Aggressively prioritizes high-saving merges.\n    *   **Node Similarity:** Encourages connections based on demand/distance profiles.\n    *   **Adaptive Weighting:** Adjusts weights dynamically based on network connectivity.\n    *   **Sparsification:** Removes edges using percentile thresholding and the triangle inequality.\n*   **(4th) vs (5th):** Heuristic 5 introduces angle-based clustering and normalizes individual heuristic components before combining them. Heuristic 4 simulates routing to estimate the remaining capacity and implements triangle inequality during sparsification.\n*   **(5th) vs (6th):** The heuristics are identical.\n*   **(5th/6th) vs (7th):** Heuristic 7 introduces demand density, adaptively weights based on problem size, and normalizes individual heuristics components before combining them.\n*   **(7th) vs (8th):** The 8th heuristic focuses on normalizing all heuristic components, encouraging clustering based on spatial proximity, and adaptively weighting based on problem size and characteristics.\n*   **(8th) vs (9th):** Heuristic 9 simplifies by removing the angle cost and clustering components.\n*   **(9th) vs (10th):** The heuristics are identical.\n*   **(9th/10th) vs (11th):** The 11th heuristic normalizes the heuristic components before combining them and reintroduces a clustering coefficient approximation.\n*   **(11th) vs (12th):** The heuristics are identical.\n*   **(11th/12th) vs (13th):** Heuristic 13 focuses on angles from the depot, demand-distance interaction, node diversity weighting, and adjusts sparsification criteria.\n*   **(13th) vs (14th):** Heuristic 14 normalizes all components.\n*   **(14th) vs (15th):** The 15th heuristic explicitly includes and normalizes an \"angle cost\" heuristic, prioritizing nodes with smaller angles from the depot.\n*   **(15th) vs (16th):** The heuristics are identical.\n*   **(16th) vs (17th):** The heuristics are identical.\n*   **(16th/17th) vs (18th):** The 18th heuristic adds a node centrality measure and normalizes the clustering heuristic and centrality heuristic, and also adapts the k value used in the clustering.\n*   **(18th) vs (19th):** The heuristics are identical.\n*   **(19th) vs (20th):** The heuristics are identical.\n\nOverall: The best heuristics combine several factors: Distance, Demand, Savings, Depot Proximity, Clustering, Angle Cost, Centrality, Adaptive Weighting, Sparsification and Normalization to ensure components are on the same scale. Demand feasibility and distance are the most significant.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a redefinition of \"Current Self-Reflection\" optimized for designing better heuristics, while avoiding the pitfalls of the \"Ineffective Self-Reflection\":\n\n*   **Keywords:** Problem-specific, Adaptive, Normalization, Sparsification, Diverse Factors, Robustness, Solution Quality Influence, Spatial relationships, Clustering, Gravitational attraction.\n\n*   **Advice:** Actively seek and incorporate problem-specific knowledge, especially spatial relationships, clustering, and gravitational attraction; meticulously normalize individual heuristic components; and implement adaptive weighting schemes based on *multiple* problem characteristics and consider sparsification.\n\n*   **Avoid:** Generic \"consider diverse factors.\" Don't just mention adaptive weighting; focus on making it responsive to multiple problem features.\n\n*   **Explanation:** The goal is to move beyond vague recommendations to actionable strategies by emphasizing the importance of normalization and adaptive weighting schemes to multiple problem characteristics, leading to enhanced robustness and adaptability.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}