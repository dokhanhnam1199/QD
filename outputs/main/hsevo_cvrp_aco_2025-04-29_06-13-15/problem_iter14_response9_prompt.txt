{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Capacitated Vehicle Routing Problem (CVRP) via stochastic solution sampling. CVRP requires finding the shortest path that visits all given nodes and returns to the starting node. Each node has a demand and each vehicle has a capacity. The total demand of the nodes visited by a vehicle cannot exceed the vehicle capacity. When the total demand exceeds the vehicle capacity, the vehicle must return to the starting node.\nThe `heuristics` function takes as input a distance matrix (shape: n by n), Euclidean coordinates of nodes (shape: n by 2), a vector of customer demands (shape: n), and the integer capacity of vehicle capacity. It returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the distance_matrix. The depot node is indexed by 0.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray, coordinates: np.ndarray, demands: np.ndarray, capacity: int) -> np.ndarray:\n\n    \"\"\"\n    Heuristics for solving Capacitated Vehicle Routing Problem (CVRP) via stochastic solution sampling.\n\n    This version focuses on a refined combination of factors, dynamic parameter adaptation,\n    and more aggressive sparsification.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix between nodes (shape: n x n).\n        coordinates (np.ndarray): Euclidean coordinates of nodes (shape: n x 2).\n        demands (np.ndarray): Vector of customer demands (shape: n).\n        capacity (int): Vehicle capacity.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution (shape: n x n).\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros((n, n))\n    epsilon = 1e-6\n\n    # 1. Distance Component (Enhanced)\n    #   - Inverse square distance emphasizes shorter edges more strongly.\n    distance_heuristic = 1 / (distance_matrix**2 + epsilon)\n\n    # 2. Demand Feasibility (Refined)\n    #   - Stricter penalty for immediate capacity violations.\n    #   - Considers the remaining capacity of the current route (simulated).\n    demand_heuristic = np.ones((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                demand_heuristic[i, j] = 0\n            if demands[i] + demands[j] > capacity and i == 0:\n                demand_heuristic[i, j] = 0.01  # Stronger penalty for depot-initiated violation\n            elif demands[i] + demands[j] > capacity and j == 0:\n                demand_heuristic[i, j] = 0.01  # Stronger penalty for depot-initiated violation\n            elif i != 0 and demands[i] > capacity or j != 0 and demands[j] > capacity:\n                demand_heuristic[i,j] = 0.005\n\n            # Simulate a simple route: depot -> i -> j\n            remaining_capacity = capacity - demands[i] if i != 0 else capacity\n            if i != 0 and j!= 0 and remaining_capacity < demands[j]:\n                 demand_heuristic[i,j] = 0.001 #Very strong penalty\n\n\n    # 3. Depot Proximity (Contextualized)\n    #   - Only apply depot preference to nodes that are \"isolated\" (far from most others).\n    #   - Adjust preference strength based on demand.\n    depot_heuristic = np.zeros((n, n))\n    isolation_threshold = np.mean(distance_matrix)  # Node is \"isolated\" if its avg dist is above this\n    for i in range(1, n):\n        avg_distance = np.sum(distance_matrix[i, 1:]) / (n - 2) if n > 2 else distance_matrix[i,0]\n        if avg_distance > isolation_threshold:\n            depot_preference = (avg_distance / np.max(distance_matrix)) * (demands[i] / capacity)**0.5 # sqrt makes demands less impact\n            depot_heuristic[i, 0] = depot_preference\n            depot_heuristic[0, i] = depot_preference\n\n\n    # 4. Savings Heuristic (Aggressive)\n    #   - Prioritize high-saving merges very aggressively.\n    savings_heuristic = np.zeros((n, n))\n    for i in range(1, n):\n        for j in range(i + 1, n):\n            savings = distance_matrix[i, 0] + distance_matrix[0, j] - distance_matrix[i, j]\n            savings_heuristic[i, j] = savings**2  # Emphasize high savings\n            savings_heuristic[j, i] = savings**2\n\n    # 5. Node Similarity (Demand-Distance Tradeoff)\n    # Encourage edges between nodes with similar demand/distance profiles\n    similarity_heuristic = np.zeros((n, n))\n    demand_normalized = demands / np.max(demands)\n    distance_to_depot_normalized = distance_matrix[:, 0] / np.max(distance_matrix[:, 0])\n\n    for i in range(1, n):\n        for j in range(i + 1, n):\n            demand_diff = abs(demand_normalized[i] - demand_normalized[j])\n            distance_diff = abs(distance_to_depot_normalized[i] - distance_to_depot_normalized[j])\n            similarity_heuristic[i, j] = 1 - (demand_diff + distance_diff) / 2 #High score when demand diff and distance_to_depot_diff are small\n            similarity_heuristic[j, i] = similarity_heuristic[i, j]\n\n    # 6. Adaptive Weighting (Mechanism-Driven)\n    #   - Dynamically adjust weights based on network connectivity.\n    #   - High connectivity -> emphasize savings. Low connectivity -> emphasize distance.\n    connectivity = np.sum(distance_matrix < np.mean(distance_matrix)) / (n * n)  # Ratio of \"close\" edges\n    alpha = 0.4 * (1 - connectivity)  # Distance importance (inversely proportional to connectivity)\n    beta = 0.1 * (1-connectivity) # Depot proximity\n    gamma = 0.4 * connectivity  # Savings importance (proportional to connectivity)\n    eta = 0.1 * connectivity #Similarity heuristic\n\n    heuristics = alpha * distance_heuristic * demand_heuristic + beta * depot_heuristic + gamma * savings_heuristic + eta * similarity_heuristic\n\n    # 7. Sparsification (Aggressive and Dynamic)\n    #   - Remove edges below a percentile threshold AND those that violate \"triangle inequality\".\n    #   - Only keep k-nearest neighbors.\n    k_nearest = 8\n    threshold = np.percentile(heuristics[heuristics > 0], 50)  # Aggressive sparsification\n\n    for i in range(n):\n        nearest_neighbors = np.argsort(distance_matrix[i, :])[1:k_nearest+1]\n\n        for j in range(n):\n            if heuristics[i, j] < threshold and j not in nearest_neighbors:\n                heuristics[i, j] = 0\n            #Triangle inequality check\n            for k in range(n):\n              if distance_matrix[i,j] > distance_matrix[i,k] + distance_matrix[k,j]:\n                heuristics[i,j] = 0\n                break\n\n\n\n    # Normalize\n    max_val = np.max(heuristics)\n    if max_val > 0:\n        heuristics = heuristics / max_val\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray, coordinates: np.ndarray, demands: np.ndarray, capacity: int) -> np.ndarray:\n\n    \"\"\"CVRP heuristics: Combines distance, demand, depot proximity, savings, clustering, centrality, adaptive weighting, sparsification.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros((n, n))\n    epsilon = 1e-6\n\n    # 1. Distance (normalized)\n    distance_heuristic = 1 / (distance_matrix + epsilon)\n    distance_heuristic = (distance_heuristic - np.min(distance_heuristic)) / (np.max(distance_heuristic) - np.min(distance_heuristic) + epsilon)\n\n    # 2. Demand Feasibility\n    demand_heuristic = np.ones((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                demand_heuristic[i, j] = 0\n            if demands[i] + demands[j] > capacity and i == 0:\n                demand_heuristic[i, j] = 0.1\n            elif demands[i] + demands[j] > capacity and j == 0:\n                demand_heuristic[i, j] = 0.1\n            elif i != 0 and demands[i] > capacity or j != 0 and demands[j] > capacity:\n                demand_heuristic[i,j] = 0.05\n\n    # 3. Depot Proximity (normalized)\n    depot_heuristic = np.zeros((n, n))\n    for i in range(1, n):\n        avg_distance = np.sum(distance_matrix[i, 1:]) / (n - 2) if n > 2 else distance_matrix[i, 0]\n        depot_preference = (avg_distance / np.max(distance_matrix)) * (demands[i] / capacity)\n        depot_heuristic[i, 0] = depot_preference\n        depot_heuristic[0, i] = depot_preference\n    depot_heuristic = (depot_heuristic - np.min(depot_heuristic)) / (np.max(depot_heuristic) - np.min(depot_heuristic) + epsilon)\n\n    # 4. Savings (normalized)\n    savings_heuristic = np.zeros((n, n))\n    for i in range(1, n):\n        for j in range(i + 1, n):\n            savings = distance_matrix[i, 0] + distance_matrix[0, j] - distance_matrix[i, j]\n            savings_heuristic[i, j] = savings\n            savings_heuristic[j, i] = savings\n    savings_heuristic = (savings_heuristic - np.min(savings_heuristic)) / (np.max(savings_heuristic) - np.min(savings_heuristic) + epsilon)\n\n    # 5. Clustering (adaptive k)\n    clustering_heuristic = np.zeros((n, n))\n    k_nearest = min(10, n - 1)\n    for i in range(1, n):\n        nearest_neighbors = np.argsort(distance_matrix[i, 1:])[:k_nearest] + 1\n        for j in range(1, n):\n            if i != j:\n                common_neighbors = 0\n                for neighbor_i in nearest_neighbors:\n                    if neighbor_i in (np.argsort(distance_matrix[j, 1:])[:k_nearest] + 1):\n                        common_neighbors += 1\n                clustering_heuristic[i, j] = common_neighbors / k_nearest\n    clustering_heuristic = clustering_heuristic / np.max(clustering_heuristic)\n\n    # 6. Node Centrality (normalized)\n    centrality = np.zeros(n)\n    for i in range(1, n):\n        centrality[i] = np.sum(distance_matrix[i, 1:])\n    centrality = np.max(centrality) / (centrality + epsilon)\n    centrality[0] = 1\n    centrality_heuristic = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            centrality_heuristic[i, j] = (centrality[i] + centrality[j]) / 2.0\n    centrality_heuristic = centrality_heuristic / np.max(centrality_heuristic)\n\n    # Adaptive Weighting\n    capacity_ratio = np.sum(demands[1:]) / (capacity * (n - 1))\n    num_nodes_factor = n / 100.0\n\n    alpha = 0.2\n    beta = 0.15\n    gamma = 0.1\n    delta = 0.15\n    eta = 0.2\n\n    alpha += 0.05 * capacity_ratio + 0.02 * num_nodes_factor\n    beta -= 0.03 * capacity_ratio - 0.01 * num_nodes_factor\n    gamma += 0.02 * capacity_ratio + 0.03 * num_nodes_factor\n    delta += 0.01 * capacity_ratio + 0.02 * num_nodes_factor\n    eta -= 0.01 * capacity_ratio - 0.02 * num_nodes_factor\n\n    # Combine heuristics\n    heuristics = (alpha * distance_heuristic * demand_heuristic +\n                  beta * depot_heuristic +\n                  gamma * savings_heuristic +\n                  delta * clustering_heuristic +\n                  eta * centrality_heuristic)\n\n    # Sparsification (adaptive threshold)\n    k_nearest = min(12, n - 1)\n    threshold = np.percentile(heuristics[heuristics > 0], 40)\n\n    for i in range(n):\n        nearest_neighbors = np.argsort(distance_matrix[i, :])[1:k_nearest+1]\n        for j in range(n):\n            if heuristics[i, j] < threshold and j not in nearest_neighbors:\n                heuristics[i, j] = 0\n\n    # Normalize\n    max_val = np.max(heuristics)\n    if max_val > 0:\n        heuristics = heuristics / max_val\n\n    return heuristics\n\n### Analyze & experience\n- *   **(1st) vs (2nd):** These heuristics are identical.\n*   **(1st/2nd) vs (3rd):** These heuristics are identical.\n*   **(1st/2nd/3rd) vs (4th):** The 4th heuristic introduces several key refinements:\n    *   **Distance:** Uses inverse square distance to emphasize shorter edges more.\n    *   **Demand Feasibility:** Implements stricter penalties for capacity violations and simulates route capacity.\n    *   **Depot Proximity:** Applies depot preference contextually to isolated nodes.\n    *   **Savings:** Aggressively prioritizes high-saving merges.\n    *   **Node Similarity:** Encourages connections based on demand/distance profiles.\n    *   **Adaptive Weighting:** Adjusts weights dynamically based on network connectivity.\n    *   **Sparsification:** Removes edges using percentile thresholding and the triangle inequality.\n*   **(4th) vs (5th):** Heuristic 5 introduces angle-based clustering and normalizes individual heuristic components before combining them. Heuristic 4 simulates routing to estimate the remaining capacity and implements triangle inequality during sparsification.\n*   **(5th) vs (6th):** The heuristics are identical.\n*   **(5th/6th) vs (7th):** Heuristic 7 introduces demand density, adaptively weights based on problem size, and normalizes individual heuristics components before combining them.\n*   **(7th) vs (8th):** The 8th heuristic focuses on normalizing all heuristic components, encouraging clustering based on spatial proximity, and adaptively weighting based on problem size and characteristics.\n*   **(8th) vs (9th):** Heuristic 9 simplifies by removing the angle cost and clustering components.\n*   **(9th) vs (10th):** The heuristics are identical.\n*   **(9th/10th) vs (11th):** The 11th heuristic normalizes the heuristic components before combining them and reintroduces a clustering coefficient approximation.\n*   **(11th) vs (12th):** The heuristics are identical.\n*   **(11th/12th) vs (13th):** Heuristic 13 focuses on angles from the depot, demand-distance interaction, node diversity weighting, and adjusts sparsification criteria.\n*   **(13th) vs (14th):** Heuristic 14 normalizes all components.\n*   **(14th) vs (15th):** The 15th heuristic explicitly includes and normalizes an \"angle cost\" heuristic, prioritizing nodes with smaller angles from the depot.\n*   **(15th) vs (16th):** The heuristics are identical.\n*   **(16th) vs (17th):** The heuristics are identical.\n*   **(16th/17th) vs (18th):** The 18th heuristic adds a node centrality measure and normalizes the clustering heuristic and centrality heuristic, and also adapts the k value used in the clustering.\n*   **(18th) vs (19th):** The heuristics are identical.\n*   **(19th) vs (20th):** The heuristics are identical.\n\nOverall: The best heuristics combine several factors: Distance, Demand, Savings, Depot Proximity, Clustering, Angle Cost, Centrality, Adaptive Weighting, Sparsification and Normalization to ensure components are on the same scale. Demand feasibility and distance are the most significant.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a redefinition of \"Current Self-Reflection\" optimized for designing better heuristics, while avoiding the pitfalls of the \"Ineffective Self-Reflection\":\n\n*   **Keywords:** Problem-specific, Adaptive, Normalization, Sparsification, Diverse Factors, Robustness, Solution Quality Influence, Spatial relationships, Clustering, Gravitational attraction.\n\n*   **Advice:** Actively seek and incorporate problem-specific knowledge, especially spatial relationships, clustering, and gravitational attraction; meticulously normalize individual heuristic components; and implement adaptive weighting schemes based on *multiple* problem characteristics and consider sparsification.\n\n*   **Avoid:** Generic \"consider diverse factors.\" Don't just mention adaptive weighting; focus on making it responsive to multiple problem features.\n\n*   **Explanation:** The goal is to move beyond vague recommendations to actionable strategies by emphasizing the importance of normalization and adaptive weighting schemes to multiple problem characteristics, leading to enhanced robustness and adaptability.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}