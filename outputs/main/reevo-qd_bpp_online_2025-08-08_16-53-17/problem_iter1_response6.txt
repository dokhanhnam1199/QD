```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Sigmoid Fit Score.

    The Sigmoid Fit Score strategy aims to prioritize bins that are "just right"
    for the item, avoiding both bins that are too large (leaving a lot of wasted space)
    and bins that are too small (making a tight fit that might lead to future
    fragmentation or difficulty packing subsequent items).

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Returns:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Calculate the difference between remaining capacity and item size.
    # We are looking for differences close to zero (a good fit).
    # We only consider bins where the item can actually fit.
    fit_differences = bins_remain_cap - item

    # Initialize priorities to a very low value (or 0) for bins that cannot fit the item.
    priorities = np.zeros_like(bins_remain_cap)
    can_fit_mask = bins_remain_cap >= item
    priorities[can_fit_mask] = fit_differences[can_fit_mask]

    # Apply a sigmoid function. The goal is to map differences close to 0 to high values.
    # The sigmoid function (1 / (1 + exp(-x))) maps the range (-inf, inf) to (0, 1).
    # We want the "best" fits (differences near 0) to have the highest priority.
    # A negative scaling factor for the difference `x` in `exp(-x)` will map
    # small negative values (good fits) to larger positive exponents, resulting in
    # smaller `exp` values and thus higher sigmoid outputs.
    # A positive scaling factor will map small positive values (slight waste) to
    # smaller positive exponents, resulting in larger `exp` values and thus lower
    # sigmoid outputs.
    # We'll use a scaling factor (e.g., 1.0 or a tuned value) to control the steepness.
    # A value of 1.0 means that a difference of -1 gets a much higher score than a difference of +1.
    # A larger positive value (e.g., 2.0) would make the sigmoid steeper around 0.
    # Let's use a scaling factor of 1.0 for simplicity, and consider tuning it.
    # Also, to ensure we are giving higher scores to smaller differences, we can
    # consider using -fit_differences to flip the behavior if needed, but the current
    # approach of mapping differences close to 0 to high priority is what we want.
    # The sigmoid output will naturally be highest for values closest to 0.

    # To ensure we prioritize smaller differences, we can either:
    # 1. Use -fit_differences if we want the sigmoid to peak at positive values (i.e., bins with small remaining capacity after fitting).
    # 2. Use fit_differences if we want the sigmoid to peak at negative values (i.e., bins with small "slack" after fitting).

    # The prompt implies we want the bin that "best fits", which typically means
    # minimizing wasted space. So, a bin with remaining capacity of `item + epsilon`
    # is better than `item + 10*epsilon`. This corresponds to `fit_differences`
    # being small and positive. To map these small positive differences to high
    # sigmoid values, we should feed the negative of the differences into the sigmoid.

    # Let's rescale the differences to make the sigmoid more sensitive around zero.
    # A common approach is to normalize or scale the differences.
    # We can scale the differences by a factor that makes the 'ideal' fit difference
    # (close to 0) result in a value close to 0 for the sigmoid input.
    # Consider scaling by -1 to map the small positive differences to negative values for the sigmoid.
    # This will result in higher sigmoid values for bins with smaller positive `fit_differences`.

    # To handle potential overflow/underflow with large values, clip differences or use
    # a scaled version. For simplicity, let's directly apply sigmoid after handling
    # the non-fitting bins.

    # Let's refine the scaling. We want small `fit_differences` (which means remaining_cap is close to item)
    # to yield high scores. `sigmoid(x)` is high for large positive `x`.
    # So we want `x` to be large positive when `fit_difference` is small positive.
    # This implies we should use `-fit_difference` if `fit_difference` represents `remaining_cap - item`.
    # Or, if we want to think about the *waste* which is `remaining_cap - item`, then smaller waste is better.
    # If we input `-(remaining_cap - item)` into sigmoid, then `-(item - item) = 0` becomes the peak.

    # A common sigmoid scaling factor is related to the "noise" or variance expected.
    # Let's use a simple scaling for demonstration. A factor of `1.0` means `exp(-x)` dominates.
    # Small positive differences (good fits) should map to high scores.
    # If `fit_difference = bins_remain_cap - item`, then when `fit_difference` is small positive,
    # we want a high sigmoid output. Sigmoid `1/(1+exp(-k*x))` is high for large positive `k*x`.
    # So, `k*(bins_remain_cap - item)` needs to be large positive for small positive `bins_remain_cap - item`.
    # This requires `k` to be negative. Let's use `k = -2.0` for a steeper curve around 0.

    # Ensure we don't feed inf/-inf or NaN into exp.
    # The 'priorities' array already has 0 for bins that don't fit.
    # So, we are calculating sigmoid for the `priorities` which now only holds
    # `bins_remain_cap - item` for fitting bins.

    # Let's transform the differences so that values close to zero are mapped to a value
    # that yields the highest sigmoid output.
    # If `f = bins_remain_cap - item`, we want `f` near 0 to be good.
    # `sigmoid(x)` is good for large positive `x`.
    # We can use `sigmoid(-f * scale)` to get high values for small positive `f`.
    # Let's scale it by a factor, say 2.0, to make the "sweet spot" more pronounced.

    scale_factor = 2.0  # Tune this parameter to control steepness around the ideal fit.
    # For bins that can fit, calculate the negative difference.
    # A smaller positive difference (e.g., 0.1) results in a smaller negative number (-0.1).
    # `exp(-(-0.1))` is `exp(0.1)`, which is closer to 1 than `exp(-1)` or `exp(-10)`.
    # This results in `1/(1+exp(0.1))` being a higher value than `1/(1+exp(1))` or `1/(1+exp(10))`.

    # Directly compute the sigmoid for all bins (where non-fitting bins have 0 priority)
    # We apply the transformation only to the differences that can fit.
    # `priorities` is currently `fit_differences` for fitting bins and 0 otherwise.

    # We want the best fit (remaining_cap - item closest to 0) to have the highest priority.
    # Let `diff = bins_remain_cap[can_fit_mask] - item`.
    # We want small positive `diff` to map to high scores.
    # Sigmoid(k*x) is high for large positive k*x.
    # So, we want `k * diff` to be large positive when `diff` is small positive.
    # This means `k` should be positive.
    # Let's rethink the goal: a bin that leaves minimal wasted space.
    # This means `bins_remain_cap - item` should be minimized.
    # So, `bins_remain_cap - item` = 0 is ideal.
    # `sigmoid(x)` is highest at `x=0` if we consider `1 - sigmoid(x)` and map 0 to 0.
    # Or `sigmoid(-x)` with `x` being the penalty.
    # If we want the highest score for the smallest *positive* difference, we want to penalize large positive differences more.
    # Consider `sigmoid(- (bins_remain_cap - item) * scale)`
    # If `bins_remain_cap - item = 0.1`, sigmoid(-0.2) ~ 0.45
    # If `bins_remain_cap - item = 1.0`, sigmoid(-2.0) ~ 0.12
    # If `bins_remain_cap - item = 10.0`, sigmoid(-20.0) ~ 0.0

    # This looks correct: small positive differences (good fit, low waste) yield higher sigmoid values.
    # Let's clip the exponent to prevent overflow/underflow issues for very large/small differences.
    # Clip the input to sigmoid to be within a reasonable range.
    # The input is `-fit_difference * scale_factor`.
    # `fit_difference` can range from 0 to `max_bin_capacity - min_item_size`.
    # Let's assume bin capacities and item sizes are reasonable floats.

    # Apply the sigmoid transformation to the differences for fitting bins
    transformed_diffs = -priorities[can_fit_mask] * scale_factor
    # Clip to prevent overflow in exp(-x)
    # A very large positive transformed_diffs (very negative fit_difference)
    # will result in exp(-large_positive) which goes to 0.
    # A very large negative transformed_diffs (very positive fit_difference)
    # will result in exp(-large_negative) which goes to inf.
    # We need to clip values that will result in exp going to inf, which are very
    # large positive fit_differences (meaning very large slack).
    # So, clip `priorities[can_fit_mask]` to be not excessively large.
    # A simple clipping of transformed_diffs might be more robust.
    # `exp(-x)` means if `x` is very negative, `exp` is huge. If `x` is very positive, `exp` is near zero.
    # So we need to prevent `x` from being extremely negative.
    # `x = -priorities[can_fit_mask] * scale_factor`
    # `priorities[can_fit_mask]` is `bins_remain_cap - item`.
    # If `bins_remain_cap - item` is very large positive, then `x` is very large negative.
    # `np.exp(large_positive)` can cause overflow. We need to avoid that.

    # Let's cap the `priorities[can_fit_mask]` (the `bins_remain_cap - item`)
    # A practical upper bound for the remaining capacity would be the maximum bin capacity itself.
    # Let's assume a reasonable maximum slack, say 100 units of capacity.
    # If `bins_remain_cap - item > 100`, we treat it as 100 for the sigmoid calculation.
    # This makes bins with "too much" space similarly penalized.
    # The maximum positive `priorities[can_fit_mask]` value we will consider for `sigmoid(-x*scale)`
    # is when `bins_remain_cap - item` is around 100. This results in `exp(-200)`, which is practically 0.

    # The problematic case is when `bins_remain_cap - item` is very small negative (meaning item is larger than bin)
    # But we already masked those.
    # So, we are concerned about `bins_remain_cap - item` being very large positive.

    # Let's use a robust sigmoid calculation, often `sigmoid(x) = 0.5 * (1 + tanh(x/2))`.
    # Or, directly use `scipy.special.expit` which handles clipping internally.
    # However, the request is to implement using basic numpy, if possible.

    # A simpler approach for robust sigmoid: clip the argument `z = -priorities[can_fit_mask] * scale_factor`
    # such that `exp(z)` is not too large.
    # If `z > 700` (approx where exp overflows), exp(-z) becomes 0.
    # So we want `-z > -700`, which means `z < 700`.
    # `-priorities[can_fit_mask] * scale_factor < 700`
    # `priorities[can_fit_mask] * scale_factor > -700`
    # `bins_remain_cap[can_fit_mask] - item > -700 / scale_factor`
    # This condition is already true for the fitting bins as `bins_remain_cap >= item`.

    # The real issue is when `priorities[can_fit_mask]` is large positive.
    # `-priorities[can_fit_mask] * scale_factor` becomes very negative.
    # e.g., `priorities[can_fit_mask] = 1000`, `scale_factor = 2`. `transformed_diffs = -2000`. `exp(2000)` overflows.
    # This happens when `bins_remain_cap - item` is large positive.

    # To prevent `exp(x)` overflow when `x` is a large negative number:
    # If `transformed_diffs < -700` (approx), we want `exp(transformed_diffs)` to be effectively 0.
    # So `sigmoid` will be `1 / (1 + 0) = 1`.
    # This means if `bins_remain_cap - item` is very large, the sigmoid value should approach 1.
    # Wait, this is opposite. If `bins_remain_cap - item` is very large positive, it's a bad fit.
    # So, the sigmoid output should be low.

    # Let's re-check: `sigmoid(x) = 1 / (1 + exp(-x))`.
    # Goal: highest score for `bins_remain_cap - item` closest to 0 (ideally small positive).
    # We feed `x = - (bins_remain_cap - item) * scale_factor`
    # Case 1: `bins_remain_cap - item = 0.1` (good fit)
    # `x = -0.1 * scale_factor`. If `scale_factor = 2`, `x = -0.2`.
    # `exp(-(-0.2)) = exp(0.2) ≈ 1.22`.
    # `sigmoid(-0.2) = 1 / (1 + 1.22) ≈ 1 / 2.22 ≈ 0.45`.
    # This is NOT high. Highest sigmoid is for `x` large positive.

    # Let's use `sigmoid(x)` where `x` is small positive for good fits.
    # So, `x = (bins_remain_cap - item) * scale_factor`.
    # If `bins_remain_cap - item = 0.1`, `x = 0.2`. `exp(-0.2) ≈ 0.82`. `sigmoid(0.2) ≈ 1 / (1 + 0.82) ≈ 0.55`.
    # If `bins_remain_cap - item = 1.0`, `x = 2.0`. `exp(-2.0) ≈ 0.135`. `sigmoid(2.0) ≈ 1 / (1 + 0.135) ≈ 0.88`.
    # This makes bins with MORE slack have HIGHER priority, which is wrong.

    # We want the "closest fit". This means the difference should be close to zero.
    # Let's consider `1 / (1 + exp(k * (bins_remain_cap - item)))` where `k > 0`.
    # If `bins_remain_cap - item = 0.1` (good fit), `exp(k * 0.1)`. If k=2, exp(0.2) ≈ 1.22. Sigmoid ≈ 0.45.
    # If `bins_remain_cap - item = 1.0` (loose fit), `exp(k * 1.0)`. If k=2, exp(2.0) ≈ 7.39. Sigmoid ≈ 0.11.
    # If `bins_remain_cap - item = -0.1` (item too big, should not happen due to mask), `exp(-k * 0.1)`. If k=2, exp(-0.2) ≈ 0.82. Sigmoid ≈ 0.55.
    # This function prioritizes bins where the item is *just too large* to fit, and penalizes bins that fit well but leave a lot of space.

    # The standard "Sigmoid Fit" heuristic in literature for online BPP typically refers to
    # selecting bins where `remaining_capacity - item` is "close" to zero.
    # If we define "close to zero" as a positive value that's as small as possible,
    # we can map this small positive value to the highest sigmoid score.
    # Consider mapping `f = bins_remain_cap - item` to `sigmoid(a * (1 - f/M))` or similar, where M is max capacity.

    # A more direct interpretation of Sigmoid Fit for BPP is to use it to determine a probability
    # or preference for packing into a bin. Let's stick to the idea that the 'best fit'
    # corresponds to `bins_remain_cap - item` being small and positive.

    # Let's use a robust way to compute `sigmoid(x)`: `0.5 + 0.5 * tanh(x/2)`.
    # Or `1 / (1 + exp(-x))`. For robustness against overflow:
    # `exp(x)` for positive `x` can overflow. `exp(-x)` for positive `x` can underflow to 0.
    # If `x` is large negative, `-x` is large positive, `exp(-x)` overflows.
    # So if `x < -C` for some large C, we can treat `exp(-x)` as infinity, and sigmoid as 0.
    # If `x > C` for some large C, we can treat `exp(-x)` as 0, and sigmoid as 1.

    # Let's try the argument `x = - (bins_remain_cap - item) * scale_factor` again.
    # The problem is when `bins_remain_cap - item` is very large positive.
    # `x` becomes very negative. `exp(-x)` becomes very large positive. Sigmoid ~ 0.
    # This correctly penalizes bins with lots of slack.

    # Let's use a robust sigmoid calculation, ensuring `exp` argument is manageable.
    # `y = -(priorities[can_fit_mask] * scale_factor)`
    # If `y` is very large positive (i.e., `priorities[can_fit_mask]` is very large negative, which won't happen with `can_fit_mask`), then `exp(-y)` is small.
    # If `y` is very large negative (i.e., `priorities[can_fit_mask]` is very large positive), then `exp(-y)` is large, `exp(-y)` overflows.

    # Robust sigmoid implementation for `1 / (1 + exp(-x))`:
    # def robust_sigmoid(x):
    #     if x < -500: return 1.0  # exp(500) is huge, 1/(1+inf) -> 0
    #     if x > 500: return 0.0   # exp(-500) is tiny, 1/(1+0) -> 1
    #     return 1.0 / (1.0 + np.exp(-x))

    # `x` is the argument to the sigmoid. In our case, `x = - (bins_remain_cap - item) * scale_factor`
    # Let `slack = bins_remain_cap[can_fit_mask] - item`. This is always >= 0 for fitting bins.
    # We want small slack to result in high sigmoid scores.
    # Sigmoid(arg) is high for large positive `arg`.
    # So we want `arg` to be large positive when `slack` is small positive.
    # This suggests `arg = -slack * scale`.
    # Example: slack = 0.1, scale=2. arg = -0.2. exp(-arg) = exp(0.2) = 1.22. sigmoid = 1/2.22 ~ 0.45.
    # Example: slack = 1.0, scale=2. arg = -2.0. exp(-arg) = exp(2.0) = 7.39. sigmoid = 1/8.39 ~ 0.12.
    # This means bins with MORE slack get LOWER priority, which is correct!

    # Now, let's consider the range of `slack`. `slack = bins_remain_cap[can_fit_mask] - item`.
    # `slack` can be large. If `slack` is very large, `arg = -slack * scale` becomes very negative.
    # `exp(-arg) = exp(slack * scale)`. If `slack * scale` is large (e.g., 1000), this overflows.
    # So, if `slack * scale > 500` (roughly), `exp(slack * scale)` overflows.
    # This means `slack` should be capped for this calculation.
    # Let's cap `slack` itself. If `slack` > `MAX_SLACK_CONSIDERED`, we can treat it as `MAX_SLACK_CONSIDERED`.
    # MAX_SLACK_CONSIDERED could be related to `scale_factor`. If `MAX_SLACK_CONSIDERED * scale_factor`
    # is about 500, that's a good bound. Let `MAX_SLACK_CONSIDERED = 500 / scale_factor`.

    # If slack is very small, e.g., 0.001, and scale is 2, arg = -0.002. exp(0.002) ~ 1.002. sigmoid ~ 0.499.
    # This means very tight fits get a score near 0.5. This is also not ideal.

    # The common Sigmoid Fit prioritizes bins that have a remaining capacity *close to* the item size.
    # It does not strictly penalize very small positive slack, but wants the slack to be minimal.
    # Let's consider the function `sigmoid(K * (2 * item - remaining_capacity))`
    # Let `f = remaining_capacity - item`. We want `f` to be small and positive.
    # Consider `sigmoid(K * (item - remaining_capacity))`
    # If `rem - item = 0.1`, `item - rem = -0.1`. `sigmoid(K * -0.1)`. With K=10, sigmoid(-1) ~ 0.27. Low.
    # If `rem - item = 1.0`, `item - rem = -1.0`. `sigmoid(K * -1.0)`. With K=10, sigmoid(-10) ~ 0.000045. Very low.
    # If `rem - item = -0.1` (item too big), `item - rem = 1.0`. `sigmoid(K * 1.0)`. With K=10, sigmoid(10) ~ 1.

    # The function `1 / (1 + exp(k * (bins_remain_cap - item)))` with k>0:
    # - If `bins_remain_cap - item = 0.1` (good fit): `exp(k*0.1)`. With k=2, exp(0.2) ≈ 1.22. Sigmoid ≈ 0.45.
    # - If `bins_remain_cap - item = 1.0` (loose fit): `exp(k*1.0)`. With k=2, exp(2.0) ≈ 7.39. Sigmoid ≈ 0.12.
    # - If `bins_remain_cap - item = -0.1` (item too big, not applicable): `exp(-k*0.1)`. Sigmoid > 0.5.

    # This function penalizes bins with larger positive slack, but scores for tight fits (small positive slack)
    # are also not very high (around 0.45). It might be better if tight fits also got high scores.

    # A common approach in practice is to normalize the differences or use a scaled logistic function.
    # Let's try to map the 'gap' `bins_remain_cap - item` to the sigmoid, but map `0` to the highest point.
    # This suggests an argument like `-(bins_remain_cap - item)^2`.
    # If `bins_remain_cap - item = 0.1`, argument is `-0.01`. Sigmoid `1/(1+exp(0.01))` ~ 0.497.
    # If `bins_remain_cap - item = 1.0`, argument is `-1.0`. Sigmoid `1/(1+exp(1))` ~ 0.268.
    # This prioritizes bins closer to `item` capacity.

    # Let's use this: `priorities = sigmoid( - (bins_remain_cap - item)^2 * scale )`
    # where `scale` controls the steepness. A higher scale means we are more sensitive to deviations from perfect fit.
    # If `scale = 100`, `(0.1)^2 * 100 = 0.01 * 100 = 0.1`. sigmoid( -0.1 ) ~ 0.475.
    # If `scale = 100`, `(1.0)^2 * 100 = 1.0 * 100 = 100`. sigmoid( -100 ) ~ 0.
    # This seems like a good approach.

    # Apply to fitting bins only.
    # Calculate `(bins_remain_cap - item)` for fitting bins.
    # These are the `priorities` where `can_fit_mask` is True.
    fit_differences_vals = bins_remain_cap[can_fit_mask] - item

    # Calculate the squared differences.
    squared_diffs = fit_differences_vals ** 2

    # Scale the squared differences. `scale_factor` should be tuned.
    # A higher scale_factor makes the priority drop more sharply as the difference increases.
    # We are looking for values where the exponent `-squared_diffs * scale_factor`
    # does not cause overflow (i.e., `squared_diffs * scale_factor` is not too large negative).
    # The exponent is `-x`. So `x` can't be too negative.
    # `x = squared_diffs * scale_factor`.
    # `squared_diffs` is always >= 0. So `x` is always >= 0.
    # We want `sigmoid(x)` to be highest for `x` closest to 0.
    # Sigmoid(x) is highest for large positive x. We want high scores for small squared diffs.

    # Let's go back to: `sigmoid( K * (target_value - current_value) )`
    # Or `sigmoid( K * -(current_value - target_value) )`
    # `target_value = item`
    # `current_value = bins_remain_cap`
    # `sigmoid( K * -(bins_remain_cap - item))` = `sigmoid( K * (item - bins_remain_cap))`

    # Let `K = 2.0` for sensitivity.
    # `arg = K * (item - bins_remain_cap[can_fit_mask])`
    # For fitting bins: `bins_remain_cap >= item`, so `item - bins_remain_cap <= 0`.
    # Thus `arg` is always <= 0 for fitting bins.

    # If `bins_remain_cap - item = 0.1` (good fit)
    # `item - bins_remain_cap = -0.1`
    # `arg = 2.0 * -0.1 = -0.2`
    # `sigmoid(-0.2) = 1 / (1 + exp(0.2)) ≈ 0.45`

    # If `bins_remain_cap - item = 1.0` (loose fit)
    # `item - bins_remain_cap = -1.0`
    # `arg = 2.0 * -1.0 = -2.0`
    # `sigmoid(-2.0) = 1 / (1 + exp(2.0)) ≈ 0.12`

    # This function assigns lower scores to looser fits, but the highest scores are near 0.5 for tightest fits.
    # This seems reasonable: prioritize bins that don't waste too much space, and among those,
    # prefer the ones that leave less excess capacity.

    # Robust sigmoid for `1 / (1 + exp(x))` where x <= 0 for fitting bins.
    # If `x` becomes very negative (item << remaining_capacity), exp(x) approaches 0. sigmoid approaches 1.
    # This would mean bins with *lots* of slack get the highest score near 1. This is WRONG.

    # The core problem is correctly mapping the "goodness of fit" to a score, where a value close to zero
    # difference is best, and this best value should yield the highest priority.
    # `sigmoid(x)` is high for large POSITIVE `x`.
    # We want `x` to be large positive when `bins_remain_cap - item` is small positive.

    # This means we need `f(bins_remain_cap - item)` such that when `bins_remain_cap - item` is small positive,
    # `f(...)` is large positive.
    # Consider `f(d) = some_constant - d`. If `d` is small positive, `f` is large positive.
    # Let `d = bins_remain_cap - item`. We want `d` close to 0.
    # Consider `sigmoid(Constant - (bins_remain_cap - item))`.

    # Or simply, let the "value" be `- (bins_remain_cap - item)`.
    # When `bins_remain_cap - item = 0.1`, value = -0.1.
    # When `bins_remain_cap - item = 1.0`, value = -1.0.
    # We want to scale these values such that small negative values become large positive for sigmoid.
    # `sigmoid(scale * value)`.
    # `scale * value = scale * (item - bins_remain_cap)`.

    # Let `scale = 5.0`
    # If `bins_remain_cap - item = 0.1`: `item - bins_remain_cap = -0.1`. `scale * value = -0.5`.
    # `sigmoid(-0.5) = 1 / (1 + exp(0.5)) ≈ 0.377`.

    # If `bins_remain_cap - item = 0.01`: `item - bins_remain_cap = -0.01`. `scale * value = -0.05`.
    # `sigmoid(-0.05) = 1 / (1 + exp(0.05)) ≈ 0.487`.
    # This looks good: tighter fits (smaller positive differences) get higher scores, approaching 0.5.
    # If `bins_remain_cap - item = 1.0`: `item - bins_remain_cap = -1.0`. `scale * value = -5.0`.
    # `sigmoid(-5.0) = 1 / (1 + exp(5.0)) ≈ 0.006`. Low score for loose fits.

    # Let's refine this. The scores are capped around 0.5.
    # To get scores potentially higher than 0.5, we can shift the sigmoid or use a different base.
    # The prompt requires "highest priority score".

    # Let's use the "smallest waste" criterion: we want to minimize `bins_remain_cap - item`.
    # This quantity should be mapped to a high value if it's small positive.
    # Consider `f(x) = exp(-x)`. This gives high values for small positive `x`.
    # Let `x = (bins_remain_cap - item)`. Then `exp(-(bins_remain_cap - item))`.
    # If `bins_remain_cap - item = 0.1`, `exp(-0.1) ≈ 0.90`.
    # If `bins_remain_cap - item = 1.0`, `exp(-1.0) ≈ 0.36`.
    # If `bins_remain_cap - item = 10.0`, `exp(-10.0) ≈ 0.000045`.
    # This function also works and can be scaled.
    # Let's use `sigmoid(Constant - (bins_remain_cap - item) * scale)`.
    # The `Constant` term allows us to shift the peak.
    # Or more simply, let's use the original Sigmoid formula but with an argument that peaks at 0.

    # Let `d = bins_remain_cap - item`. We want a function that is max at d=0, decreasing for d>0 and d<0.
    # `-d^2` peaks at 0. `sigmoid(-d^2 * scale)`
    # Let `scale = 10`.
    # `d = 0.1`, `-d^2 * scale = -0.01 * 10 = -0.1`. `sigmoid(-0.1) ≈ 0.475`.
    # `d = 0.01`, `-d^2 * scale = -0.0001 * 10 = -0.001`. `sigmoid(-0.001) ≈ 0.499`.
    # `d = 1.0`, `-d^2 * scale = -1.0 * 10 = -10.0`. `sigmoid(-10.0) ≈ 0.000045`.
    # This correctly assigns higher scores for values of `bins_remain_cap - item` closer to 0.
    # The scores are still around 0.5.

    # Let's adjust the sigmoid to be `0.5 + 0.5 * tanh(x/2)`.
    # If we use `x = - (bins_remain_cap - item) * scale`:
    # `d = 0.1`, `x = -0.1 * scale`. `d=0.01`, `x = -0.001*scale`.
    # For `tanh(y)` to be near 1, `y` must be large positive.
    # `y = x/2`. So `x` must be large positive.
    # `x = -(bins_remain_cap - item) * scale`.
    # For `x` to be large positive, `-(bins_remain_cap - item)` must be large positive, meaning `bins_remain_cap - item` must be large negative.
    # This prioritizes bins where the item is too big. WRONG.

    # Let's try the simple approach with scaling and ensuring numerical stability.
    # Priority = Sigmoid( `scale` * ( `target_capacity` - `remaining_capacity` ) )
    # where `target_capacity` is the ideal capacity for the item.
    # If we assume the ideal scenario is to fill a bin as much as possible without overflow,
    # then `target_capacity` could be related to `item`.
    # Or, we want `remaining_capacity` to be close to `item`.

    # The "Sigmoid Fit" often implies mapping to a value that represents how "close" a bin is to
    # accommodating the item efficiently.
    # Let's define the quality of fit for bin `i` as `q_i = remaining_capacity_i - item`.
    # We want `q_i` to be small and positive.
    # We can map `q_i` using a sigmoid function such that small positive `q_i` yields high output.
    # The function `Sigmoid( C - k * q_i )` where `C` is a bias and `k > 0`.
    # Let `C = 0` and `k = scale`.
    # `Sigmoid( -k * q_i ) = Sigmoid( -k * (bins_remain_cap - item) )`.
    # Let `scale = 2.0`.
    # If `bins_remain_cap - item = 0.1` (good fit): `arg = -0.2`. `Sigmoid(-0.2) ≈ 0.45`.
    # If `bins_remain_cap - item = 1.0` (loose fit): `arg = -2.0`. `Sigmoid(-2.0) ≈ 0.12`.
    # If `bins_remain_cap - item = 10.0` (very loose fit): `arg = -20.0`. `Sigmoid(-20.0) ≈ 0`.

    # The maximum score is < 0.5 here.
    # To get scores closer to 1 for good fits, we can shift the sigmoid or its input.
    # Let's use `sigmoid(C + x)` where `x` is scaled.
    # We want `bins_remain_cap - item` small positive to map to high score.
    # Consider `sigmoid( K * (1 - (bins_remain_cap - item) / MaxCapacity ) )`
    # Or simpler: `sigmoid( K * (IdealFill - ActualFill) )`.

    # Let's simplify to a practical implementation for the heuristic.
    # We want bins that have *just enough* space.
    # We can calculate a "penalty" for each bin: `penalty = max(0, item - remaining_capacity)` for bins that don't fit.
    # For bins that fit: `penalty = max(0, remaining_capacity - item)`.
    # Then apply sigmoid to negative penalty.

    # Final approach: Use the `sigmoid( k * (ideal_remaining - actual_remaining) )` form.
    # Let `ideal_remaining` be the smallest amount of capacity that can accommodate the item, so `item`.
    # Then `ideal_remaining = item`.
    # `arg = scale * (item - bins_remain_cap)`
    # For fitting bins: `bins_remain_cap >= item`. So `item - bins_remain_cap <= 0`.
    # `arg <= 0`.
    # We want the highest score when `item - bins_remain_cap` is closest to 0 (which means `bins_remain_cap - item` is smallest positive).
    # `sigmoid(x)` is highest for large POSITIVE `x`.
    # This means we need `scale * (item - bins_remain_cap)` to be large positive.
    # Which implies `scale` should be negative and `item - bins_remain_cap` should be large negative (meaning `bins_remain_cap - item` is large positive).

    # Let's try `sigmoid( k * (bins_remain_cap - item) )` where `k` is negative.
    # Let `k = -2.0`.
    # `d = bins_remain_cap - item`
    # `d = 0.1` (good fit): `arg = -0.2`. `sigmoid(-0.2) ≈ 0.45`.
    # `d = 1.0` (loose fit): `arg = -2.0`. `sigmoid(-2.0) ≈ 0.12`.
    # `d = 10.0` (very loose fit): `arg = -20.0`. `sigmoid(-20.0) ≈ 0`.
    # This means bins with more slack get lower priority. This is good.
    # The highest scores are still capped below 0.5 for tight fits.

    # To get scores potentially above 0.5, we can bias the input:
    # `sigmoid(Bias + k * (bins_remain_cap - item))`
    # If we want the peak at `bins_remain_cap - item = 0`.
    # Let `k = -2.0`.
    # We want `Bias` such that `sigmoid(Bias + k * 0) = sigmoid(Bias)` is high.
    # Say we want the peak value to be 0.8. `sigmoid(Bias) = 0.8`.
    # `1 / (1 + exp(-Bias)) = 0.8`
    # `1 = 0.8 + 0.8 * exp(-Bias)`
    # `0.2 = 0.8 * exp(-Bias)`
    # `0.25 = exp(-Bias)`
    # `-Bias = ln(0.25) = -ln(4) = -1.386`
    # `Bias = 1.386`

    # So, `sigmoid(1.386 - 2.0 * (bins_remain_cap - item))`
    # Let's check:
    # `d = 0.1`: `arg = 1.386 - 2.0 * 0.1 = 1.386 - 0.2 = 1.186`. `sigmoid(1.186) ≈ 0.766`. Good.
    # `d = 1.0`: `arg = 1.386 - 2.0 * 1.0 = 1.386 - 2.0 = -0.614`. `sigmoid(-0.614) ≈ 0.35`. Lower.
    # `d = 10.0`: `arg = 1.386 - 2.0 * 10.0 = 1.386 - 20.0 = -18.614`. `sigmoid(-18.614) ≈ 0`. Very low.

    # This approach seems to work. The `Bias` value (`~1.386`) can be tuned, as well as `k` (`-2.0`).
    # A common parameter `k` used in literature is around 1 to 5 for the scaled difference.
    # Let's choose `scale_factor = 2.0` and a `bias_shift` to lift the scores.
    # Let's re-evaluate what "Sigmoid Fit Score" truly means.
    # It often refers to mapping the "fit" or "slack" to a preference.
    # A common parameterization might be `sigmoid( (IdealCapacity - CurrentCapacity) / ScalingFactor )`.

    # Let's use a more direct interpretation of "good fit": remaining_capacity is slightly greater than item.
    # Let the "target gap" be 0.
    # We want the gap `g = bins_remain_cap - item` to be close to 0.
    # Consider a score that is high when `g` is small positive.
    # This could be `sigmoid( K * (epsilon - g) )` where `epsilon` is a small positive value.
    # If `epsilon = 0.1`, `K = 2`.
    # `g = 0.1`: `sigmoid(2 * (0.1 - 0.1)) = sigmoid(0) = 0.5`.
    # `g = 0.01`: `sigmoid(2 * (0.1 - 0.01)) = sigmoid(0.18) ≈ 0.545`.
    # `g = 1.0`: `sigmoid(2 * (0.1 - 1.0)) = sigmoid(-1.8) ≈ 0.14`.
    # This looks like a valid Sigmoid Fit heuristic.

    # `scale_factor` here is `K`.
    # `epsilon` is a small constant representing the "ideal slack".

    scale_factor = 3.0  # Controls steepness around epsilon
    ideal_slack = 0.0   # The ideal amount of remaining capacity after fitting an item.
                        # Setting to 0 aims for the tightest possible fit.
                        # Setting to a small positive value can help with future packing.

    # Calculate the argument for the sigmoid function.
    # We want the argument to be high positive when (bins_remain_cap - item) is close to ideal_slack.
    # So, `arg = scale_factor * (ideal_slack - (bins_remain_cap - item))`
    # `arg = scale_factor * (ideal_slack - bins_remain_cap + item)`

    # For bins that cannot fit the item, their priority should be 0.
    # `priorities` is already 0 for non-fitting bins.
    # We only calculate the sigmoid score for fitting bins.

    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]

    # Calculate the 'gap' for fitting bins.
    gap = fitting_bins_remain_cap - item

    # Calculate the sigmoid argument:
    # We want high scores when 'gap' is close to 'ideal_slack'.
    # Sigmoid(x) is high for large positive x.
    # So, we want `arg` to be large positive when `gap` is close to `ideal_slack`.
    # This implies `arg` should be inversely related to `abs(gap - ideal_slack)`.
    # Using `scale_factor * (ideal_slack - gap)` works.

    # If ideal_slack = 0:
    # arg = scale_factor * (0 - gap) = -scale_factor * gap
    # If gap = 0.1, arg = -0.1 * scale_factor. Sigmoid will be < 0.5.
    # If gap = 0.01, arg = -0.01 * scale_factor. Sigmoid will be closer to 0.5.
    # To get scores > 0.5 for good fits, we need to shift the sigmoid.

    # Alternative approach: Prioritize bins that result in the smallest positive remaining capacity.
    # This is often implemented as: For each bin i, if `remaining_capacity_i >= item`,
    # priority is `1.0 / (remaining_capacity_i - item + epsilon)`.
    # Let's use sigmoid to map this inverse relationship.

    # Consider the inverse relationship: the smaller the slack (positive), the higher the priority.
    # `slack = bins_remain_cap - item`.
    # We want `1/slack` to be high for small positive slack.
    # This implies we want to map `1/slack` using sigmoid such that high values of `1/slack`
    # result in high sigmoid outputs.

    # Let `scaled_slack = (bins_remain_cap - item) * scale_factor`.
    # We want small positive `scaled_slack` to give high sigmoid scores.
    # Let's use `sigmoid( K * (Constant - scaled_slack) )`.
    # Let `Constant = 1.0`. `scale_factor = 2.0`.
    # `arg = K * (1.0 - scaled_slack) = K * (1.0 - (bins_remain_cap - item) * scale_factor)`

    # Let's simplify to a commonly used form.
    # Prioritize bins that are "almost full".
    # We can map the remaining capacity itself.
    # A bin with remaining capacity equal to `item` would be ideal.
    # `sigmoid( K * (item - remaining_capacity) )`
    # `arg = K * (item - fitting_bins_remain_cap)`
    # As seen before, for fitting bins `fitting_bins_remain_cap >= item`, so `arg <= 0`.
    # Scores are capped at 0.5.

    # To get scores above 0.5, we can shift the argument or use a different form.
    # Let's consider the inverse of slack, but bounded.
    # Let `slack = bins_remain_cap[can_fit_mask] - item`.
    # If `slack = 0`, priority is high. If `slack` is large positive, priority is low.
    # We can map `1 / (slack + epsilon)` to sigmoid.
    # `1 / (slack + epsilon)` is high when `slack` is small positive.

    # Let `value = 1.0 / (gap + 1e-9)` where `gap = fitting_bins_remain_cap - item`.
    # We want to map this `value` such that large `value` gives high sigmoid.
    # So `sigmoid(scale_factor * value)`.
    # `value` can be very large if `gap` is close to 0.
    # If `gap` is very small (e.g., 1e-10), `value` is large.
    # `scaled_value = scale_factor * value`. This can overflow if `scale_factor` is large and `gap` is small.
    # E.g., `scale_factor=2`, `gap=1e-10`. `value = 1e10`. `scaled_value = 2e10`. Sigmoid argument overflows.

    # Robust sigmoid calculation needed.
    # `sigmoid(x) = 1 / (1 + exp(-x))`
    # We want `x` to be large positive when `gap` is small positive.
    # So `x = - scale_factor * gap` is problematic for small positive gaps.
    # If `gap = 0.01`, `x = -scale_factor * 0.01`. Sigmoid < 0.5.
    # We need `x` to be large positive.

    # Let's try `sigmoid( Constant - scale_factor * gap )`.
    # `Constant = 2.0`, `scale_factor = 2.0`.
    # `gap = 0.01`: `arg = 2.0 - 0.02 = 1.98`. `sigmoid(1.98) ≈ 0.87`. Good.
    # `gap = 0.1`: `arg = 2.0 - 0.2 = 1.8`. `sigmoid(1.8) ≈ 0.86`. Not much difference. Need steeper.
    # Let `scale_factor = 5.0`.
    # `gap = 0.01`: `arg = 2.0 - 0.05 = 1.95`. `sigmoid(1.95) ≈ 0.87`.
    # `gap = 0.1`: `arg = 2.0 - 0.5 = 1.5`. `sigmoid(1.5) ≈ 0.81`. Difference increases.

    # Let `Constant = 3.0`, `scale_factor = 5.0`.
    # `gap = 0.01`: `arg = 3.0 - 0.05 = 2.95`. `sigmoid(2.95) ≈ 0.95`.
    # `gap = 0.1`: `arg = 3.0 - 0.5 = 2.5`. `sigmoid(2.5) ≈ 0.92`.
    # `gap = 0.5`: `arg = 3.0 - 2.5 = 0.5`. `sigmoid(0.5) ≈ 0.62`.
    # `gap = 1.0`: `arg = 3.0 - 5.0 = -2.0`. `sigmoid(-2.0) ≈ 0.12`.

    # This parameterization `sigmoid(Constant - scale_factor * gap)` is reasonable.
    # `Constant` acts as a threshold for the gap. Gaps larger than `Constant/scale_factor`
    # will result in scores below 0.5.
    # `scale_factor` determines how quickly the score drops.

    # Let's implement this.
    # Use `np.clip` on the `bins_remain_cap` or `gap` to avoid issues with `exp` if
    # `Constant - scale_factor * gap` becomes extremely large negative.
    # `Constant - scale_factor * gap` is problematic if it's very negative.
    # This happens when `scale_factor * gap` is very positive.
    # This occurs when `gap` is large positive.
    # If `gap` is large, the score should be low anyway.
    # e.g. `gap = 1000`, `scale = 5`. `arg = C - 5000`. This can be very negative.
    # `exp(-arg)` will be `exp(5000 - C)`, which overflows.
    # We need to handle `exp(-x)` when `x` is very negative.
    # `sigmoid(x) = 1 / (1 + exp(-x))`

    # If `arg = Constant - scale_factor * gap`.
    # If `arg` is very negative: `exp(-arg)` is very positive and can overflow.
    # Let `arg_max = 700`. If `arg > arg_max`, `sigmoid(arg) ≈ 1`.
    # Let `arg_min = -700`. If `arg < arg_min`, `sigmoid(arg) ≈ 0`.

    # We are concerned about `arg` being very negative.
    # `Constant - scale_factor * gap < -700`
    # `Constant + 700 < scale_factor * gap`
    # `gap > (Constant + 700) / scale_factor`
    # This means if `gap` is very large, the argument becomes very negative, and the score goes to 0.
    # This is desired behavior. The overflow happens in `exp(-arg)`.
    # So if `arg` is very negative, `exp(-arg)` is very large positive.
    # `1 / (1 + very_large_positive)` is approximately `1 / very_large_positive` which is near 0.
    # So, `sigmoid` should naturally go to 0.

    # We need to be careful with `gap` values.
    # If `bins_remain_cap` is large and `item` is small, `gap` can be large.
    # Example: `bins_remain_cap = 1000`, `item = 1`. `gap = 999`.
    # `arg = C - 5 * 999 = C - 4995`.
    # If `C=3`, `arg = 3 - 4995 = -4992`.
    # `exp(-arg) = exp(4992)`. This will overflow.

    # A robust sigmoid implementation is needed.
    # `sigmoid(x) = 1 / (1 + exp(-x))`
    # If `x < -X`, `exp(-x)` overflows.
    # If `x > X`, `exp(-x)` underflows to 0.

    # To avoid overflow in `exp(-x)` when `x` is very negative:
    # Let `y = -x`. We need to avoid `exp(y)` overflow.
    # If `y > Y_max`, replace `exp(y)` with something like `np.inf`.
    # `sigmoid(x) = 1 / (1 + np.exp(-x))`
    # `x = Constant - scale_factor * gap`
    # Let `scale_factor = 5.0`. `Constant = 3.0`.
    # If `gap` is large, `x` is very negative.
    # If `gap = 1000`, `x = 3 - 5000 = -4997`.
    # `-x = 4997`. `exp(4997)` overflows.

    # If we clip `gap` before calculating `x`:
    # Let `max_gap = 100`.
    # `clipped_gap = np.clip(gap, 0, max_gap)`.
    # `x = Constant - scale_factor * clipped_gap`.
    # If `gap` was originally 1000, `clipped_gap` is 100.
    # `x = 3 - 5 * 100 = 3 - 500 = -497`.
    # `-x = 497`. Still overflows.

    # We need to scale the argument itself to a range where sigmoid is well-behaved.
    # OR use a different form: `0.5 * (1 + tanh(x/2))`
    # `tanh(y)` can also overflow/underflow.
    # `tanh(y) = (exp(y) - exp(-y)) / (exp(y) + exp(-y))`
    # If `y` is large positive, `tanh(y)` is 1. If `y` is large negative, `tanh(y)` is -1.
    # The `exp(y)` part can overflow if `y` is large positive.
    # `y = x/2 = (Constant - scale_factor * gap) / 2`.
    # This also faces similar issues.

    # A common robust sigmoid:
    # def robust_sigmoid(x):
    #     return np.clip(0.5 * (1 + np.tanh(x / 2.0)), 0.0, 1.0)
    # Let's assume we have this robust version implicitly for now.

    # Let's try the simple form, with reasonable parameters and assume numpy handles intermediate steps or we clip the argument of exp.
    # `Constant = 3.0`, `scale_factor = 5.0`.
    # `gap = bins_remain_cap[can_fit_mask] - item`.
    # `arg = Constant - scale_factor * gap`.
    # We want to prevent `exp(-arg)` from overflowing.
    # This happens if `-arg` is too large positive.
    # So, we want to limit `-arg` from exceeding ~700.
    # `-arg = -Constant + scale_factor * gap`.
    # We need `-Constant + scale_factor * gap < 700`.
    # `scale_factor * gap < 700 + Constant`.
    # `gap < (700 + Constant) / scale_factor`.
    # If `gap` exceeds this, `exp(-arg)` will overflow.
    # For `C=3, S=5`, this threshold is `(700+3)/5 = 703/5 = 140.6`.
    # If `gap` is greater than ~140.6, `exp(-arg)` might overflow.

    # Let's clip the `gap` to a reasonable maximum to prevent extreme values.
    # Let `max_gap_clip = 50.0` (tuned parameter).
    # `clipped_gap = np.clip(gap, 0, max_gap_clip)`.
    # `arg = Constant - scale_factor * clipped_gap`.
    # Max negative `arg` will be `3 - 5 * 50 = 3 - 250 = -247`.
    # `-arg = 247`. `exp(247)` is large but might be manageable. Let's check numpy exp.
    # `np.exp(700)` is `~1.0e304`. `np.exp(710)` is `inf`. So the threshold is around 700-710.
    # Our max negative `arg` is -247. So `-arg` is 247. This should be fine.

    # Parameter values:
    # `Constant`: Influences the "midpoint" of the sigmoid.
    # `scale_factor`: Controls how steep the priority drop-off is.
    # `max_gap_clip`: Prevents extreme gaps from dominating or causing numerical issues.

    constant_bias = 3.0
    scale_factor = 5.0
    max_gap_clip = 50.0  # Represents the maximum "useful" slack. Slack beyond this is equally penalized.

    # Ensure we only process fitting bins.
    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]

    # Calculate the gap (remaining capacity - item size) for fitting bins.
    gap = fitting_bins_remain_cap - item

    # Clip the gap to prevent extreme values and potential numerical issues.
    clipped_gap = np.clip(gap, 0.0, max_gap_clip)

    # Calculate the argument for the sigmoid function.
    # We want higher priority for smaller gaps.
    # The sigmoid function is `1 / (1 + exp(-x))`. It's high for positive `x`.
    # So we want `x` to be large positive when `clipped_gap` is small.
    # We use `Constant - scale_factor * clipped_gap`.
    # If `clipped_gap` is small (e.g., 0), `arg = Constant`.
    # If `clipped_gap` is large (e.g., max_gap_clip), `arg = Constant - scale_factor * max_gap_clip`.
    # This will result in lower scores for larger gaps.
    sigmoid_arg = constant_bias - scale_factor * clipped_gap

    # Apply the sigmoid function to get the priority scores.
    # Use `np.clip` to ensure the argument doesn't cause `exp` overflow/underflow,
    # although the `clipped_gap` should largely handle this.
    # Limiting sigmoid_arg to [-700, 700] is a common robust practice.
    # `-arg = -Constant + scale_factor * clipped_gap`
    # Max negative `arg` is `3 - 5*50 = -247`. Min `-arg` is 247.
    # Min positive `arg` is `3 - 5*0 = 3`. Max `-arg` is -3.
    # The range of `arg` is `[3-250, 3] = [-247, 3]`.
    # So `exp(-arg)` will be in range `[exp(-3), exp(247)]`. `exp(247)` is okay.

    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(-sigmoid_arg))

    # Normalize priorities if needed, or simply return as is.
    # In this context, higher values mean higher priority.

    return priorities
```
