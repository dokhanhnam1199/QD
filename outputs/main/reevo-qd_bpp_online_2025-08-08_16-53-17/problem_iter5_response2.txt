```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a refined Sigmoid Fit Score.

    This heuristic aims to prioritize bins that offer the "best fit" for the item.
    A "best fit" is defined as a bin where the remaining capacity is only slightly
    larger than the item's size. This strategy tries to fill bins as much as possible
    without leaving excessive empty space, thereby minimizing fragmentation.

    The priority is calculated using a sigmoid function. The function is designed
    to peak when the remaining capacity (`bins_remain_cap`) is precisely equal to
    the item's size, and the priority decreases as the remaining capacity deviates
    (either smaller or larger). However, bins where the item doesn't fit at all
    are assigned a zero priority.

    This version refines the priority by:
    1. Directly penalizing bins with very large remaining capacity (Worst Fit Reduction component).
    2. Using softmax to ensure a distribution of probabilities rather than just absolute scores,
       encouraging exploration among good options.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A numpy array where each element is the remaining capacity of a bin.

    Returns:
        A numpy array of the same size as `bins_remain_cap`, containing the priority
        score for each bin. Higher scores indicate a more desirable bin for the item.
    """

    def sigmoid(x, steepness=10.0, center=0.0):
        """A custom sigmoid function that can be shifted and scaled."""
        # Adding a small epsilon to avoid division by zero or log(0) issues if used later
        # though not directly used here, it's good practice for stability.
        return 1 / (1 + np.exp(-steepness * (x - center)))

    # Initialize priorities to a very low negative number for log-sum-exp stability
    # or to 0 for direct softmax if no prior transformation.
    # Let's use a transformation that encourages small gaps and penalizes large gaps.
    # We want to prioritize bins where `bins_remain_cap - item` is close to 0.

    # Initialize raw scores.
    # For bins that don't fit, assign a score that will result in a low probability after softmax.
    # A large negative number is suitable.
    raw_scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)

    # Identify bins where the item can fit.
    fits_mask = bins_remain_cap >= item

    if np.any(fits_mask):
        # Calculate the excess capacity for fitting bins.
        excess_capacities = bins_remain_cap[fits_mask] - item

        # Define parameters for the scoring function.
        # 'ideal_gap' is the desired remaining capacity after fitting the item.
        # Values close to 0 are good (exact fit), but a small positive gap might be even better
        # to avoid packing too tightly and potentially leaving no space for future small items.
        ideal_gap = 0.05  # Target a small positive residual capacity
        steepness_fit = 15.0 # How sensitive the score is to deviations from ideal_gap

        # Component 1: Prioritize near 'ideal_gap' using a shifted sigmoid.
        # We want the peak score (e.g., 0) to be at `excess_capacities = ideal_gap`.
        # `score_fit = -abs(excess_capacities - ideal_gap)` can be used, but sigmoid provides
        # smoother transitions and bounds.
        # `sigmoid(steepness * (ideal_gap - excess_capacities))` centers the sigmoid's inflection
        # point at `ideal_gap`. A higher value means a better fit.
        fit_scores = sigmoid(steepness_fit * (ideal_gap - excess_capacities), steepness=steepness_fit, center=0.0)

        # Component 2: Penalize large remaining capacities (Worst Fit Reduction component).
        # This discourages using bins that are almost empty if a better fit exists.
        # A linear penalty or an inverse relationship could work. Let's use a log-based penalty
        # that grows slowly for very large capacities.
        # We want to assign lower scores to bins with high `bins_remain_cap[fits_mask]`.
        # Use `1 / (1 + excess_capacities)` or `-log(1 + excess_capacities)` as a penalty.
        # A simple inverse relationship for the *remaining* capacity in the bin after fitting:
        # `penalty_large_residual = 1 / (1 + excess_capacities)`
        # Or, to penalize the *original* large capacity:
        # `penalty_large_original = 1 / (1 + bins_remain_cap[fits_mask])`
        # Let's use a penalty on the *excess capacity* to complement the fit score.
        # We want to dampen scores for bins with large `excess_capacities`.
        # A function like `exp(-large_capacity_penalty_factor * excess_capacities)` can work.
        large_capacity_penalty_factor = 0.5
        penalty_scores = np.exp(-large_capacity_penalty_factor * excess_capacities)

        # Combine the scores. A simple multiplication or addition can work.
        # Multiplying preserves the idea that both are important:
        # `combined_scores = fit_scores * penalty_scores`
        # To make it more robust to different scales and use softmax, let's sum weighted terms.
        # Let fit score contribute positively and large capacity negatively.
        # We can map `fit_scores` (0 to 1) to a range, and `penalty_scores` (close to 0 for large excess)
        # to a similar range.

        # A score that combines good fit (excess ~ ideal_gap) and not too much overall waste.
        # Consider the "waste" as `bins_remain_cap[fits_mask] - item`.
        # We want `excess_capacities` to be small and positive.
        # `score = -abs(excess_capacities - ideal_gap)`
        # Let's normalize `excess_capacities` and map them.
        # A simpler approach for raw scores before softmax:
        # Prioritize small `excess_capacities`, especially near `ideal_gap`.
        # `raw_score = -excess_capacities` would prioritize minimal excess.
        # `raw_score = -abs(excess_capacities - ideal_gap)` prioritizes `ideal_gap`.
        # Let's use the latter.

        # Re-evaluate the scoring: we want high score for small `excess_capacities`, and especially near `ideal_gap`.
        # A simple score for "best fit" is `-excess_capacities`.
        # To add the "near ideal_gap" preference: `-abs(excess_capacities - ideal_gap)`
        # Combine: Prioritize small excess (good for overall packing), but also near ideal_gap (avoids tiny gaps).
        # Let's use a scoring function that is high when excess_capacity is low,
        # and has a peak at ideal_gap.
        # `score = exp(-k * excess_capacities)` would prioritize zero excess.
        # `score = exp(-k * abs(excess_capacities - ideal_gap))` peaks at ideal_gap.

        # Let's try a simpler, more direct approach for raw scores:
        # 1. Prioritize exact fits (excess_capacity = 0).
        # 2. Then prioritize small positive excess_capacities.
        # 3. Penalize large excess_capacities.

        # Score for "best fit" (minimal remaining space after packing):
        # `-excess_capacities` naturally prioritizes bins that leave the least space.
        # This is effectively 'Best Fit'.
        best_fit_score = -excess_capacities

        # Score for "worst fit reduction" (penalize bins that are almost empty):
        # We want to down-weight bins where `bins_remain_cap[fits_mask]` is very large.
        # A penalty based on `bins_remain_cap[fits_mask]` itself.
        # `penalty = - bins_remain_cap[fits_mask]` would prioritize smaller bins overall.
        # This could be combined with best fit. For example, `score = -excess_capacities - penalty_factor * bins_remain_cap[fits_mask]`
        # Or, we can have a score that is good for small `excess_capacities` and also for smaller original bin capacities.

        # Let's define a score that balances these.
        # A score that is high for small `excess_capacities` and also for bins that are not excessively large.
        # A common approach for "best fit" is `-excess_capacities`.
        # For "worst fit reduction", we want to penalize bins with high `bins_remain_cap`.
        # Let's try: `score = -excess_capacities - penalty_weight * bins_remain_cap[fits_mask]`
        # This prioritizes minimal excess and smaller original bin sizes.

        penalty_weight = 0.1 # How much to penalize large bins.
        # The combined score:
        combined_scores = -excess_capacities - penalty_weight * bins_remain_cap[fits_mask]

        # Softmax transformation to get probabilities.
        # Softmax requires non-infinite values and usually works better with scaled values.
        # Let's scale `combined_scores` to avoid very large/small exponents.
        # A simple scaling by a factor can help.
        scaling_factor = 1.0 # Adjust as needed.
        scaled_scores = scaling_factor * combined_scores

        # Apply softmax
        # `exp(scaled_scores)` will give relative weights.
        # Normalize these weights to sum to 1.
        raw_scores[fits_mask] = scaled_scores

    # If no bins fit, raw_scores remains [-inf, -inf, ...]
    # Softmax will handle this by assigning 0 probability to all bins if all are -inf.
    # If at least one bin fits, we get probabilities.

    # Use a stable softmax implementation
    # max_score = np.max(raw_scores) # This could be problematic if all are -inf
    # exp_scores = np.exp(raw_scores - max_score) # Subtract max for numerical stability
    # probabilities = exp_scores / np.sum(exp_scores)

    # A more direct way is to use log_softmax if we want log-probabilities,
    # but here we want the relative scores that softmax produces.
    # Let's compute the probabilities directly.
    # If all raw_scores are -inf, np.exp will be 0, sum will be 0, leading to NaNs.
    # Handle the case where no bins fit.
    if not np.any(fits_mask):
        return np.zeros_like(bins_remain_cap) # No bins can fit the item.

    # Ensure we don't have NaN from softmax if all scores are -inf (already handled by checking fits_mask)
    # but also ensure no NaNs from `np.exp(large negative)` if `raw_scores` are very negative.
    # `np.exp` of very negative numbers becomes zero.
    exp_scores = np.exp(raw_scores)
    sum_exp_scores = np.sum(exp_scores)

    if sum_exp_scores == 0:
        # This might happen if all `raw_scores` are extremely negative.
        # Assign uniform probability if scores are indistinguishable due to underflow.
        return np.ones_like(bins_remain_cap) / len(bins_remain_cap)
    else:
        probabilities = exp_scores / sum_exp_scores
        return probabilities

```
