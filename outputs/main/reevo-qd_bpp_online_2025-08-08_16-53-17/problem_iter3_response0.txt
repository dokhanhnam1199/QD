```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a tunable sigmoid and softmax.

    This heuristic aims to prioritize bins that offer the "best fit" for the item,
    defined as bins where the remaining capacity is only slightly larger than the item's size.
    It uses a sigmoid function to assign higher scores to these "tight fits".
    The sigmoid's steepness and ideal gap are tunable.

    To encourage exploration among equally good or near-equally good bins,
    a softmax function is applied to the sigmoid scores. This normalizes scores
    into probabilities, allowing for probabilistic selection and exploration.
    Bins that cannot fit the item are assigned a priority of 0.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A numpy array where each element is the remaining capacity of a bin.

    Returns:
        A numpy array of the same size as `bins_remain_cap`, containing the priority
        score for each bin. Higher scores indicate a more desirable bin for the item.
    """

    def sigmoid(x, steepness=15.0, ideal_gap=0.02):
        """A sigmoid function that peaks at x=0, and can be scaled and shifted.
        Here, x is effectively (bin_capacity - item_size - ideal_gap).
        A smaller positive `ideal_gap` means tighter fits are preferred.
        A larger `steepness` makes the preference for `ideal_gap` more pronounced.
        """
        # We want the sigmoid to peak when (bin_cap - item) is close to ideal_gap.
        # So, we map (bin_cap - item) to the sigmoid's input.
        # Let sigmoid_input = steepness * (ideal_gap - (bin_cap - item))
        # This means when bin_cap - item = ideal_gap, sigmoid_input = 0, and sigmoid output = 0.5.
        # We want higher scores for smaller positive gaps.
        # If bin_cap - item = 0 (perfect fit), sigmoid_input = steepness * ideal_gap > 0, sigmoid output > 0.5.
        # If bin_cap - item = 0.1, sigmoid_input = steepness * (ideal_gap - 0.1). If ideal_gap is small, this is negative.
        return 1 / (1 + np.exp(-x))

    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins where the item can fit.
    fits_mask = bins_remain_cap >= item

    if np.any(fits_mask):
        # Calculate the excess capacity for fitting bins.
        excess_capacities = bins_remain_cap[fits_mask] - item

        # Tunable parameters for the sigmoid function.
        # steepness: controls how quickly the priority drops as excess capacity deviates from ideal_gap.
        # ideal_gap: the preferred small positive residual capacity after packing the item.
        steepness = 15.0
        ideal_gap = 0.02

        # Calculate the argument for the sigmoid.
        # We want the sigmoid to output higher values for smaller, non-negative `excess_capacities`.
        # The sigmoid `1 / (1 + exp(-x))` has its midpoint at x=0.
        # To map `excess_capacities` such that `ideal_gap` gives a good score, we use:
        # `sigmoid_arg = steepness * (ideal_gap - excess_capacities)`
        # If `excess_capacities` is slightly less than `ideal_gap` (a good fit), `sigmoid_arg` is positive, sigmoid > 0.5.
        # If `excess_capacities` is exactly `ideal_gap`, `sigmoid_arg` is 0, sigmoid = 0.5.
        # If `excess_capacities` is larger than `ideal_gap`, `sigmoid_arg` is negative, sigmoid < 0.5.
        sigmoid_arg = steepness * (ideal_gap - excess_capacities)

        # Calculate raw sigmoid scores for fitting bins.
        raw_scores = sigmoid(sigmoid_arg, steepness=steepness, ideal_gap=ideal_gap)

        # Apply softmax to the scores. This normalizes scores into a probability distribution,
        # allowing for probabilistic exploration. Bins with similar high scores will have
        # non-zero probabilities assigned, encouraging trying different "good" bins.
        # Add a small epsilon to avoid issues with exp(very large negative numbers) if all are bad fits.
        # However, since we only calculate for fitting bins, this is less of a concern.
        # We are using the raw scores as inputs to softmax, not probabilities directly.
        # The 'temperature' parameter in softmax can be adjusted to control exploration.
        # A higher temperature leads to more uniform probabilities. A lower temperature
        # leads to probabilities concentrated on the highest score.
        temperature = 1.0 # Tunable exploration parameter

        # Avoid numerical instability with softmax if all `sigmoid_arg` are very large negative (unlikely here)
        # or very large positive.
        # For this application, `sigmoid_arg` will be mostly negative or slightly positive.
        # A simple softmax on the `raw_scores` is usually sufficient.
        exp_scores = np.exp(raw_scores / temperature)
        sum_exp_scores = np.sum(exp_scores)

        if sum_exp_scores > 0:
            priorities[fits_mask] = exp_scores / sum_exp_scores
        else:
            # Fallback: if all exp_scores are zero or NaN (highly unlikely for valid inputs)
            # distribute probability uniformly among fitting bins.
            priorities[fits_mask] = 1.0 / np.sum(fits_mask) if np.sum(fits_mask) > 0 else 0

    # Bins that do not fit have a priority of 0.
    return priorities
```
