[
  {
    "stdout_filepath": "problem_iter1_response10.txt_stdout.txt",
    "code_path": "problem_iter1_code10.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a First Fit Decreasing-like strategy.\n\n    This heuristic prioritizes bins that can accommodate the item and have the least remaining capacity *after*\n    the item is placed. This encourages filling bins more completely. Bins that cannot fit the item are given\n    a very low priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate the priority.\n    # A common strategy is to prioritize bins that will have the *least* remaining capacity after the item is added.\n    # This is like a \"best fit\" approach applied to the priority, aiming to leave less wasted space.\n    # We use the negative of the remaining capacity after placement, so a smaller remaining capacity\n    # results in a higher (less negative) priority score.\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # Bins that cannot fit the item should have a very low priority,\n    # ensuring they are not selected if any fitting bin exists.\n    # np.zeros_like already initializes them to 0, which is a sensible low value\n    # as we expect positive priorities for fitting bins.\n\n    return priorities",
    "response_id": 10,
    "obj": 4.048663741523748,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit.\n\n    The Best Fit strategy aims to place the item into the bin that has the\n    least remaining capacity after the item is placed, such that the item fits.\n    This minimizes wasted space in the bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot accommodate the item will have a priority of 0.\n        Among bins that can accommodate the item, higher priority is given to\n        bins that result in less remaining capacity after placing the item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity if the item is placed in a bin\n    # This is the 'fitness' or 'wasted space' after placement\n    remaining_capacities_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # We want to minimize the remaining capacity after fit (Best Fit).\n    # To translate minimization into a maximization priority score, we can\n    # use the negative of the remaining capacity, or a score that is inversely\n    # proportional to it. A common approach is to use something like:\n    # priority = C - (remaining_capacity_after_fit) where C is a large constant.\n    # A simpler approach that achieves the same ordering is to use the negative\n    # of the remaining capacity if we want to *minimize* it.\n    # Alternatively, we can assign a higher priority to smaller remaining capacities.\n    # Let's use the negative of the remaining capacity as the priority score,\n    # so the smallest positive remaining capacity (least waste) gets the highest score.\n\n    # For bins that can fit the item, the priority is the negative of the\n    # remaining capacity after the item is placed. A smaller positive\n    # remaining capacity means a larger negative priority, which is good.\n    # For example, if remaining capacities after fit are [1, 5, 2],\n    # the negative values are [-1, -5, -2]. The highest priority would be -1.\n    # This correctly prioritizes the bin with remaining capacity 1.\n\n    priorities[can_fit_mask] = -remaining_capacities_after_fit\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response4.txt_stdout.txt",
    "code_path": "problem_iter1_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    Exact Fit First prioritizes bins that can accommodate the item perfectly,\n    meaning the remaining capacity of the bin after placing the item is zero.\n    Among bins that offer an exact fit, we can further refine the priority.\n    Here, we'll prioritize bins that have a smaller remaining capacity *before*\n    fitting the item (but still large enough to fit it). This strategy\n    aims to \"fill up\" bins more effectively with exact fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit exactly\n    exact_fit_mask = (bins_remain_cap - item) == 0\n\n    # For bins that allow an exact fit, assign a high priority based on their\n    # *current* remaining capacity. We want bins that are *almost* full\n    # (but can still fit the item exactly) to be prioritized.\n    # We can achieve this by assigning a priority score that is inversely\n    # related to their current remaining capacity, or by assigning a score\n    # that increases as remaining capacity decreases towards the item size.\n    # A simple approach is to assign a large base score and then add\n    # a bonus related to how close their current capacity is to the item size.\n    # Let's use a base high priority for exact fits and add a term that\n    # rewards smaller remaining capacities.\n\n    # A large base priority to favor exact fits\n    base_exact_fit_priority = 1000.0\n\n    # Calculate priority for exact fit bins: higher score for bins that were\n    # closer to exactly fitting the item (i.e., smaller current remaining_cap).\n    # We add a small value related to (bins_remain_cap - item) to ensure\n    # that among exact fits, those with smaller initial capacity get a slight edge.\n    # Since for exact fits, bins_remain_cap - item is 0, this term is effectively 0.\n    # Let's consider a slightly different approach: prioritize exact fits,\n    # and among those, pick the one that leaves the smallest remaining space\n    # if we consider the *exact* remaining space after fitting.\n    # So, if bins_remain_cap[i] == item, then bins_remain_cap[i] - item is 0.\n    # To favor those closer to the item size, we can use something like\n    # 1 / (epsilon + bins_remain_cap[i]) or a similar inverse relationship.\n    # A simpler heuristic for exact fit is to give a high score to those\n    # where bins_remain_cap == item. We can then add a penalty for\n    # bins that would have excess capacity even with an exact fit.\n\n    # Let's refine the exact fit priority:\n    # Assign a very high priority to bins that can fit the item perfectly.\n    # To break ties among exact fits, we can prioritize the bin that currently\n    # has the least remaining capacity (closest to the item size without being smaller).\n    # If bins_remain_cap[i] == item, then bins_remain_cap[i] - item = 0.\n    # We want higher priority for smaller `bins_remain_cap` among exact fits.\n    # So, `1 / bins_remain_cap[i]` would work, but we need to avoid division by zero\n    # if bin capacity could be 0, or if the item size is 0.\n    # Assuming item > 0 and bin capacities are positive.\n    # A safer approach might be to use the difference `bins_remain_cap[i] - item`.\n    # For exact fits, this is 0. For non-exact fits, it's > 0.\n    # We can assign a score that is inversely proportional to the remaining capacity\n    # *after* fitting the item, for those bins that have an exact fit.\n\n    # Assign a high score to bins that can fit the item exactly.\n    # Among those, give preference to the bin with the smallest remaining capacity *after* fitting.\n    # This means prioritizing bins where `bins_remain_cap[i]` is closest to `item`.\n    # If `bins_remain_cap[i] == item`, the remaining capacity after fit is 0.\n    # We can use `1 / (item + epsilon)` for exact fits.\n\n    # Let's create a priority based on:\n    # 1. A large bonus for exact fits.\n    # 2. Among exact fits, a smaller value for `bins_remain_cap[i]` gets a higher score.\n\n    # For bins that fit the item (i.e., bins_remain_cap >= item):\n    can_fit_mask = bins_remain_cap >= item\n    fittable_capacities = bins_remain_cap[can_fit_mask]\n    fittable_indices = np.where(can_fit_mask)[0]\n\n    # Calculate the remaining capacity after placing the item.\n    remaining_after_fit = fittable_capacities - item\n\n    # Priority for fittable bins:\n    # We want to prioritize bins that result in *zero* remaining capacity (exact fits).\n    # For non-exact fits, we want to prioritize bins that result in *less* remaining capacity.\n    # A common heuristic is to sort by remaining capacity after fitting.\n    # The \"Exact Fit\" strategy specifically looks for `remaining_after_fit == 0`.\n\n    # Let's implement a strict Exact Fit First:\n    # Highest priority to bins where `bins_remain_cap == item`.\n    # For these, the remaining capacity after fitting is 0.\n    # To break ties among exact fits, we can pick the one with the smallest initial capacity.\n    # Or, we can simply assign them all a high uniform priority and then potentially\n    # use a secondary criterion if needed, but the prompt implies a single priority score.\n\n    # Strategy:\n    # 1. Bins with `bins_remain_cap == item` get a very high priority.\n    # 2. Bins with `bins_remain_cap > item` (but not exact fit) get a lower priority.\n    # 3. Bins with `bins_remain_cap < item` get zero priority (cannot fit).\n\n    # Assigning priority:\n    # Use a large multiplier for exact fits.\n    # For bins that are not exact fits but can fit the item, we can assign a score\n    # that decreases as their remaining capacity (after fitting) increases.\n    # The smaller the `bins_remain_cap[i] - item` for non-exact fits, the higher the priority.\n    # This makes it a \"Best Fit\" if no exact fit exists.\n\n    # A high score for exact fits\n    exact_fit_bonus = 100.0\n\n    # For bins that fit, calculate a score that is higher for those with smaller remaining capacity *after* fitting.\n    # We can use a large negative number for non-fitting bins to ensure they are least prioritized.\n    priorities[can_fit_mask] = -remaining_after_fit # Higher score for smaller remaining capacity\n\n    # Now, adjust for exact fits. We want exact fits to be *higher* than any non-exact fit.\n    # If `remaining_after_fit` is 0, we want a score that is > any negative number.\n    # Let's assign a fixed high score to exact fits.\n    # The indices where exact fit happens are `fittable_indices[remaining_after_fit == 0]`.\n    exact_fit_indices = fittable_indices[remaining_after_fit == 0]\n\n    # For exact fits, set a very high priority. If there are multiple exact fits,\n    # their scores here would be the same. If a secondary criterion is needed,\n    # it could be incorporated into this score, e.g., `-bins_remain_cap[exact_fit_indices]`.\n    # For this implementation, let's assign a high base priority to all exact fits.\n    priorities[exact_fit_indices] = exact_fit_bonus\n\n    # For bins that can fit but are not exact fits, their priority is `-remaining_after_fit`.\n    # Since `remaining_after_fit` is positive, these scores will be negative.\n    # The largest negative score (closest to zero) will be for the smallest `remaining_after_fit`.\n\n    # Final priority adjustment:\n    # If an exact fit exists, all exact fits should have the highest priority.\n    # Among exact fits, let's favor those that were closer to the item size initially.\n    # If `bins_remain_cap[i] == item`, its score is `exact_fit_bonus`.\n    # If `bins_remain_cap[i] > item`, its score is `-(bins_remain_cap[i] - item)`.\n\n    # To ensure exact fits are strictly preferred over best-fit (among non-exact fits):\n    # We can scale the exact fit bonus to be significantly larger than the\n    # maximum possible score from the best-fit part.\n    # The best-fit scores range from approximately -(BIN_CAPACITY - ITEM_SIZE) to slightly less than 0.\n    # So, `exact_fit_bonus` should be greater than `BIN_CAPACITY`.\n    # Let's use `np.max(bins_remain_cap)` as a safe upper bound for remaining capacity.\n\n    scaled_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    max_possible_best_fit_score = -np.min(remaining_after_fit) if len(remaining_after_fit) > 0 else 0\n    exact_fit_priority_value = exact_fit_bonus # or something larger like 2 * np.max(bins_remain_cap) if we know capacity limits\n\n    # Apply priorities:\n    # Exact fits get the highest priority\n    exact_fit_locations = (bins_remain_cap - item) == 0\n    scaled_priorities[exact_fit_locations] = exact_fit_priority_value\n\n    # For bins that can fit but not exactly, assign a priority based on minimizing\n    # the remaining space. The smaller the remaining space, the higher the priority.\n    # Since `remaining_after_fit` is always >= 0, and we want higher priority for smaller values,\n    # a negative sign and then the value works for sorting.\n    # For example, if item=3 and capacities are 7, 8, 10:\n    # Remaining after fit: 4, 5, 7.\n    # Scores: -4, -5, -7. Higher score is -4 (bin with capacity 7).\n    # This would mean it's a \"Best Fit\" strategy.\n\n    # To ensure Exact Fit First:\n    # Set exact fits to a high constant value.\n    # Set non-exact fits to a lower range of values.\n    # The best fit criterion can be used to order within non-exact fits.\n\n    # Let's assign a score that is simply `exact_fit_priority_value` for exact fits.\n    # For bins that fit but not exactly, we want them to be strictly lower priority.\n    # We can assign them a priority based on the *negative* of their remaining capacity.\n    # The smaller the remaining capacity, the higher (less negative) the score.\n\n    # Initialize all to a very low priority (unusable bins)\n    scaled_priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    fittable_indices = np.where(can_fit_mask)[0]\n\n    # Calculate remaining capacities for fittable bins\n    remaining_after_fit = bins_remain_cap[fittable_indices] - item\n\n    # Find exact fit indices within the fittable ones\n    exact_fit_fittable_indices = fittable_indices[remaining_after_fit == 0]\n    non_exact_fit_fittable_indices = fittable_indices[remaining_after_fit > 0]\n\n    # Assign highest priority to exact fits.\n    # Among exact fits, we can choose the one that was closest to the item size.\n    # If we simply assign the same high score to all exact fits, then it becomes\n    # a matter of which one is encountered first.\n    # A common tie-breaker is the bin with the minimum remaining capacity before fitting,\n    # so `bins_remain_cap[exact_fit_fittable_indices]` would be the tie-breaker.\n    # A higher score for smaller remaining capacity.\n    if len(exact_fit_fittable_indices) > 0:\n        # Let's assign a base high score and add a term that prioritizes smaller initial capacities.\n        # A good tie-breaker is often to pick the one that was closest to being full.\n        # So, `1 / bins_remain_cap` for the exact fits.\n        # Or, use the negative of their current capacity, then sort.\n        # The problem requires a priority score, so higher means better.\n        # Max score for exact fit: `base_exact_fit_priority + (max_initial_cap - current_initial_cap)`\n        # This gives higher score to smaller `current_initial_cap`.\n        base_exact_fit_priority = 1000.0\n        exact_fit_initial_caps = bins_remain_cap[exact_fit_fittable_indices]\n        # Prioritize smaller initial capacities for exact fits\n        exact_fit_priorities = base_exact_fit_priority + (np.max(exact_fit_initial_caps) - exact_fit_initial_caps)\n        scaled_priorities[exact_fit_fittable_indices] = exact_fit_priorities\n    else:\n        # If there are no exact fits, we might fall back to Best Fit.\n        # However, the strategy is strictly Exact Fit First. If no exact fit, then what?\n        # The prompt implies that we always return a priority for *each* bin.\n        # If no exact fit, these bins still might fit. The priority should reflect\n        # that they are *not* exact fits.\n        pass # This section will handle non-exact fits later.\n\n    # Assign priorities to bins that fit but not exactly (Best Fit heuristic for secondary preference)\n    # These should have lower priorities than exact fits.\n    # Priority is inversely related to remaining capacity after fit.\n    # Higher priority for smaller `remaining_after_fit`.\n    if len(non_exact_fit_fittable_indices) > 0:\n        # Assign priorities that are lower than exact fits.\n        # The value `-remaining_after_fit` would be negative.\n        # We want higher score for smaller `remaining_after_fit`.\n        # Example: if remaining_after_fit are 1, 2, 3 -> scores are -1, -2, -3.\n        # The largest score is -1, which corresponds to the smallest remaining capacity.\n        # We need to shift these values so they are still lower than the exact fit priorities.\n        # We can subtract a large constant to ensure they are always lower.\n        offset_for_non_exact_fits = 500.0 # Must be less than base_exact_fit_priority\n        non_exact_fit_priorities = offset_for_non_exact_fits - remaining_after_fit[remaining_after_fit > 0]\n        scaled_priorities[non_exact_fit_fittable_indices] = non_exact_fit_priorities\n\n    # Bins that cannot fit the item retain their initial -inf priority.\n\n    return scaled_priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "SLOC": 35.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response6.txt_stdout.txt",
    "code_path": "problem_iter1_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score strategy aims to prioritize bins that are \"just right\"\n    for the item, avoiding both bins that are too large (leaving a lot of wasted space)\n    and bins that are too small (making a tight fit that might lead to future\n    fragmentation or difficulty packing subsequent items).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the difference between remaining capacity and item size.\n    # We are looking for differences close to zero (a good fit).\n    # We only consider bins where the item can actually fit.\n    fit_differences = bins_remain_cap - item\n\n    # Initialize priorities to a very low value (or 0) for bins that cannot fit the item.\n    priorities = np.zeros_like(bins_remain_cap)\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = fit_differences[can_fit_mask]\n\n    # Apply a sigmoid function. The goal is to map differences close to 0 to high values.\n    # The sigmoid function (1 / (1 + exp(-x))) maps the range (-inf, inf) to (0, 1).\n    # We want the \"best\" fits (differences near 0) to have the highest priority.\n    # A negative scaling factor for the difference `x` in `exp(-x)` will map\n    # small negative values (good fits) to larger positive exponents, resulting in\n    # smaller `exp` values and thus higher sigmoid outputs.\n    # A positive scaling factor will map small positive values (slight waste) to\n    # smaller positive exponents, resulting in larger `exp` values and thus lower\n    # sigmoid outputs.\n    # We'll use a scaling factor (e.g., 1.0 or a tuned value) to control the steepness.\n    # A value of 1.0 means that a difference of -1 gets a much higher score than a difference of +1.\n    # A larger positive value (e.g., 2.0) would make the sigmoid steeper around 0.\n    # Let's use a scaling factor of 1.0 for simplicity, and consider tuning it.\n    # Also, to ensure we are giving higher scores to smaller differences, we can\n    # consider using -fit_differences to flip the behavior if needed, but the current\n    # approach of mapping differences close to 0 to high priority is what we want.\n    # The sigmoid output will naturally be highest for values closest to 0.\n\n    # To ensure we prioritize smaller differences, we can either:\n    # 1. Use -fit_differences if we want the sigmoid to peak at positive values (i.e., bins with small remaining capacity after fitting).\n    # 2. Use fit_differences if we want the sigmoid to peak at negative values (i.e., bins with small \"slack\" after fitting).\n\n    # The prompt implies we want the bin that \"best fits\", which typically means\n    # minimizing wasted space. So, a bin with remaining capacity of `item + epsilon`\n    # is better than `item + 10*epsilon`. This corresponds to `fit_differences`\n    # being small and positive. To map these small positive differences to high\n    # sigmoid values, we should feed the negative of the differences into the sigmoid.\n\n    # Let's rescale the differences to make the sigmoid more sensitive around zero.\n    # A common approach is to normalize or scale the differences.\n    # We can scale the differences by a factor that makes the 'ideal' fit difference\n    # (close to 0) result in a value close to 0 for the sigmoid input.\n    # Consider scaling by -1 to map the small positive differences to negative values for the sigmoid.\n    # This will result in higher sigmoid values for bins with smaller positive `fit_differences`.\n\n    # To handle potential overflow/underflow with large values, clip differences or use\n    # a scaled version. For simplicity, let's directly apply sigmoid after handling\n    # the non-fitting bins.\n\n    # Let's refine the scaling. We want small `fit_differences` (which means remaining_cap is close to item)\n    # to yield high scores. `sigmoid(x)` is high for large positive `x`.\n    # So we want `x` to be large positive when `fit_difference` is small positive.\n    # This implies we should use `-fit_difference` if `fit_difference` represents `remaining_cap - item`.\n    # Or, if we want to think about the *waste* which is `remaining_cap - item`, then smaller waste is better.\n    # If we input `-(remaining_cap - item)` into sigmoid, then `-(item - item) = 0` becomes the peak.\n\n    # A common sigmoid scaling factor is related to the \"noise\" or variance expected.\n    # Let's use a simple scaling for demonstration. A factor of `1.0` means `exp(-x)` dominates.\n    # Small positive differences (good fits) should map to high scores.\n    # If `fit_difference = bins_remain_cap - item`, then when `fit_difference` is small positive,\n    # we want a high sigmoid output. Sigmoid `1/(1+exp(-k*x))` is high for large positive `k*x`.\n    # So, `k*(bins_remain_cap - item)` needs to be large positive for small positive `bins_remain_cap - item`.\n    # This requires `k` to be negative. Let's use `k = -2.0` for a steeper curve around 0.\n\n    # Ensure we don't feed inf/-inf or NaN into exp.\n    # The 'priorities' array already has 0 for bins that don't fit.\n    # So, we are calculating sigmoid for the `priorities` which now only holds\n    # `bins_remain_cap - item` for fitting bins.\n\n    # Let's transform the differences so that values close to zero are mapped to a value\n    # that yields the highest sigmoid output.\n    # If `f = bins_remain_cap - item`, we want `f` near 0 to be good.\n    # `sigmoid(x)` is good for large positive `x`.\n    # We can use `sigmoid(-f * scale)` to get high values for small positive `f`.\n    # Let's scale it by a factor, say 2.0, to make the \"sweet spot\" more pronounced.\n\n    scale_factor = 2.0  # Tune this parameter to control steepness around the ideal fit.\n    # For bins that can fit, calculate the negative difference.\n    # A smaller positive difference (e.g., 0.1) results in a smaller negative number (-0.1).\n    # `exp(-(-0.1))` is `exp(0.1)`, which is closer to 1 than `exp(-1)` or `exp(-10)`.\n    # This results in `1/(1+exp(0.1))` being a higher value than `1/(1+exp(1))` or `1/(1+exp(10))`.\n\n    # Directly compute the sigmoid for all bins (where non-fitting bins have 0 priority)\n    # We apply the transformation only to the differences that can fit.\n    # `priorities` is currently `fit_differences` for fitting bins and 0 otherwise.\n\n    # We want the best fit (remaining_cap - item closest to 0) to have the highest priority.\n    # Let `diff = bins_remain_cap[can_fit_mask] - item`.\n    # We want small positive `diff` to map to high scores.\n    # Sigmoid(k*x) is high for large positive k*x.\n    # So, we want `k * diff` to be large positive when `diff` is small positive.\n    # This means `k` should be positive.\n    # Let's rethink the goal: a bin that leaves minimal wasted space.\n    # This means `bins_remain_cap - item` should be minimized.\n    # So, `bins_remain_cap - item` = 0 is ideal.\n    # `sigmoid(x)` is highest at `x=0` if we consider `1 - sigmoid(x)` and map 0 to 0.\n    # Or `sigmoid(-x)` with `x` being the penalty.\n    # If we want the highest score for the smallest *positive* difference, we want to penalize large positive differences more.\n    # Consider `sigmoid(- (bins_remain_cap - item) * scale)`\n    # If `bins_remain_cap - item = 0.1`, sigmoid(-0.2) ~ 0.45\n    # If `bins_remain_cap - item = 1.0`, sigmoid(-2.0) ~ 0.12\n    # If `bins_remain_cap - item = 10.0`, sigmoid(-20.0) ~ 0.0\n\n    # This looks correct: small positive differences (good fit, low waste) yield higher sigmoid values.\n    # Let's clip the exponent to prevent overflow/underflow issues for very large/small differences.\n    # Clip the input to sigmoid to be within a reasonable range.\n    # The input is `-fit_difference * scale_factor`.\n    # `fit_difference` can range from 0 to `max_bin_capacity - min_item_size`.\n    # Let's assume bin capacities and item sizes are reasonable floats.\n\n    # Apply the sigmoid transformation to the differences for fitting bins\n    transformed_diffs = -priorities[can_fit_mask] * scale_factor\n    # Clip to prevent overflow in exp(-x)\n    # A very large positive transformed_diffs (very negative fit_difference)\n    # will result in exp(-large_positive) which goes to 0.\n    # A very large negative transformed_diffs (very positive fit_difference)\n    # will result in exp(-large_negative) which goes to inf.\n    # We need to clip values that will result in exp going to inf, which are very\n    # large positive fit_differences (meaning very large slack).\n    # So, clip `priorities[can_fit_mask]` to be not excessively large.\n    # A simple clipping of transformed_diffs might be more robust.\n    # `exp(-x)` means if `x` is very negative, `exp` is huge. If `x` is very positive, `exp` is near zero.\n    # So we need to prevent `x` from being extremely negative.\n    # `x = -priorities[can_fit_mask] * scale_factor`\n    # `priorities[can_fit_mask]` is `bins_remain_cap - item`.\n    # If `bins_remain_cap - item` is very large positive, then `x` is very large negative.\n    # `np.exp(large_positive)` can cause overflow. We need to avoid that.\n\n    # Let's cap the `priorities[can_fit_mask]` (the `bins_remain_cap - item`)\n    # A practical upper bound for the remaining capacity would be the maximum bin capacity itself.\n    # Let's assume a reasonable maximum slack, say 100 units of capacity.\n    # If `bins_remain_cap - item > 100`, we treat it as 100 for the sigmoid calculation.\n    # This makes bins with \"too much\" space similarly penalized.\n    # The maximum positive `priorities[can_fit_mask]` value we will consider for `sigmoid(-x*scale)`\n    # is when `bins_remain_cap - item` is around 100. This results in `exp(-200)`, which is practically 0.\n\n    # The problematic case is when `bins_remain_cap - item` is very small negative (meaning item is larger than bin)\n    # But we already masked those.\n    # So, we are concerned about `bins_remain_cap - item` being very large positive.\n\n    # Let's use a robust sigmoid calculation, often `sigmoid(x) = 0.5 * (1 + tanh(x/2))`.\n    # Or, directly use `scipy.special.expit` which handles clipping internally.\n    # However, the request is to implement using basic numpy, if possible.\n\n    # A simpler approach for robust sigmoid: clip the argument `z = -priorities[can_fit_mask] * scale_factor`\n    # such that `exp(z)` is not too large.\n    # If `z > 700` (approx where exp overflows), exp(-z) becomes 0.\n    # So we want `-z > -700`, which means `z < 700`.\n    # `-priorities[can_fit_mask] * scale_factor < 700`\n    # `priorities[can_fit_mask] * scale_factor > -700`\n    # `bins_remain_cap[can_fit_mask] - item > -700 / scale_factor`\n    # This condition is already true for the fitting bins as `bins_remain_cap >= item`.\n\n    # The real issue is when `priorities[can_fit_mask]` is large positive.\n    # `-priorities[can_fit_mask] * scale_factor` becomes very negative.\n    # e.g., `priorities[can_fit_mask] = 1000`, `scale_factor = 2`. `transformed_diffs = -2000`. `exp(2000)` overflows.\n    # This happens when `bins_remain_cap - item` is large positive.\n\n    # To prevent `exp(x)` overflow when `x` is a large negative number:\n    # If `transformed_diffs < -700` (approx), we want `exp(transformed_diffs)` to be effectively 0.\n    # So `sigmoid` will be `1 / (1 + 0) = 1`.\n    # This means if `bins_remain_cap - item` is very large, the sigmoid value should approach 1.\n    # Wait, this is opposite. If `bins_remain_cap - item` is very large positive, it's a bad fit.\n    # So, the sigmoid output should be low.\n\n    # Let's re-check: `sigmoid(x) = 1 / (1 + exp(-x))`.\n    # Goal: highest score for `bins_remain_cap - item` closest to 0 (ideally small positive).\n    # We feed `x = - (bins_remain_cap - item) * scale_factor`\n    # Case 1: `bins_remain_cap - item = 0.1` (good fit)\n    # `x = -0.1 * scale_factor`. If `scale_factor = 2`, `x = -0.2`.\n    # `exp(-(-0.2)) = exp(0.2) \u2248 1.22`.\n    # `sigmoid(-0.2) = 1 / (1 + 1.22) \u2248 1 / 2.22 \u2248 0.45`.\n    # This is NOT high. Highest sigmoid is for `x` large positive.\n\n    # Let's use `sigmoid(x)` where `x` is small positive for good fits.\n    # So, `x = (bins_remain_cap - item) * scale_factor`.\n    # If `bins_remain_cap - item = 0.1`, `x = 0.2`. `exp(-0.2) \u2248 0.82`. `sigmoid(0.2) \u2248 1 / (1 + 0.82) \u2248 0.55`.\n    # If `bins_remain_cap - item = 1.0`, `x = 2.0`. `exp(-2.0) \u2248 0.135`. `sigmoid(2.0) \u2248 1 / (1 + 0.135) \u2248 0.88`.\n    # This makes bins with MORE slack have HIGHER priority, which is wrong.\n\n    # We want the \"closest fit\". This means the difference should be close to zero.\n    # Let's consider `1 / (1 + exp(k * (bins_remain_cap - item)))` where `k > 0`.\n    # If `bins_remain_cap - item = 0.1` (good fit), `exp(k * 0.1)`. If k=2, exp(0.2) \u2248 1.22. Sigmoid \u2248 0.45.\n    # If `bins_remain_cap - item = 1.0` (loose fit), `exp(k * 1.0)`. If k=2, exp(2.0) \u2248 7.39. Sigmoid \u2248 0.11.\n    # If `bins_remain_cap - item = -0.1` (item too big, should not happen due to mask), `exp(-k * 0.1)`. If k=2, exp(-0.2) \u2248 0.82. Sigmoid \u2248 0.55.\n    # This function prioritizes bins where the item is *just too large* to fit, and penalizes bins that fit well but leave a lot of space.\n\n    # The standard \"Sigmoid Fit\" heuristic in literature for online BPP typically refers to\n    # selecting bins where `remaining_capacity - item` is \"close\" to zero.\n    # If we define \"close to zero\" as a positive value that's as small as possible,\n    # we can map this small positive value to the highest sigmoid score.\n    # Consider mapping `f = bins_remain_cap - item` to `sigmoid(a * (1 - f/M))` or similar, where M is max capacity.\n\n    # A more direct interpretation of Sigmoid Fit for BPP is to use it to determine a probability\n    # or preference for packing into a bin. Let's stick to the idea that the 'best fit'\n    # corresponds to `bins_remain_cap - item` being small and positive.\n\n    # Let's use a robust way to compute `sigmoid(x)`: `0.5 + 0.5 * tanh(x/2)`.\n    # Or `1 / (1 + exp(-x))`. For robustness against overflow:\n    # `exp(x)` for positive `x` can overflow. `exp(-x)` for positive `x` can underflow to 0.\n    # If `x` is large negative, `-x` is large positive, `exp(-x)` overflows.\n    # So if `x < -C` for some large C, we can treat `exp(-x)` as infinity, and sigmoid as 0.\n    # If `x > C` for some large C, we can treat `exp(-x)` as 0, and sigmoid as 1.\n\n    # Let's try the argument `x = - (bins_remain_cap - item) * scale_factor` again.\n    # The problem is when `bins_remain_cap - item` is very large positive.\n    # `x` becomes very negative. `exp(-x)` becomes very large positive. Sigmoid ~ 0.\n    # This correctly penalizes bins with lots of slack.\n\n    # Let's use a robust sigmoid calculation, ensuring `exp` argument is manageable.\n    # `y = -(priorities[can_fit_mask] * scale_factor)`\n    # If `y` is very large positive (i.e., `priorities[can_fit_mask]` is very large negative, which won't happen with `can_fit_mask`), then `exp(-y)` is small.\n    # If `y` is very large negative (i.e., `priorities[can_fit_mask]` is very large positive), then `exp(-y)` is large, `exp(-y)` overflows.\n\n    # Robust sigmoid implementation for `1 / (1 + exp(-x))`:\n    # def robust_sigmoid(x):\n    #     if x < -500: return 1.0  # exp(500) is huge, 1/(1+inf) -> 0\n    #     if x > 500: return 0.0   # exp(-500) is tiny, 1/(1+0) -> 1\n    #     return 1.0 / (1.0 + np.exp(-x))\n\n    # `x` is the argument to the sigmoid. In our case, `x = - (bins_remain_cap - item) * scale_factor`\n    # Let `slack = bins_remain_cap[can_fit_mask] - item`. This is always >= 0 for fitting bins.\n    # We want small slack to result in high sigmoid scores.\n    # Sigmoid(arg) is high for large positive `arg`.\n    # So we want `arg` to be large positive when `slack` is small positive.\n    # This suggests `arg = -slack * scale`.\n    # Example: slack = 0.1, scale=2. arg = -0.2. exp(-arg) = exp(0.2) = 1.22. sigmoid = 1/2.22 ~ 0.45.\n    # Example: slack = 1.0, scale=2. arg = -2.0. exp(-arg) = exp(2.0) = 7.39. sigmoid = 1/8.39 ~ 0.12.\n    # This means bins with MORE slack get LOWER priority, which is correct!\n\n    # Now, let's consider the range of `slack`. `slack = bins_remain_cap[can_fit_mask] - item`.\n    # `slack` can be large. If `slack` is very large, `arg = -slack * scale` becomes very negative.\n    # `exp(-arg) = exp(slack * scale)`. If `slack * scale` is large (e.g., 1000), this overflows.\n    # So, if `slack * scale > 500` (roughly), `exp(slack * scale)` overflows.\n    # This means `slack` should be capped for this calculation.\n    # Let's cap `slack` itself. If `slack` > `MAX_SLACK_CONSIDERED`, we can treat it as `MAX_SLACK_CONSIDERED`.\n    # MAX_SLACK_CONSIDERED could be related to `scale_factor`. If `MAX_SLACK_CONSIDERED * scale_factor`\n    # is about 500, that's a good bound. Let `MAX_SLACK_CONSIDERED = 500 / scale_factor`.\n\n    # If slack is very small, e.g., 0.001, and scale is 2, arg = -0.002. exp(0.002) ~ 1.002. sigmoid ~ 0.499.\n    # This means very tight fits get a score near 0.5. This is also not ideal.\n\n    # The common Sigmoid Fit prioritizes bins that have a remaining capacity *close to* the item size.\n    # It does not strictly penalize very small positive slack, but wants the slack to be minimal.\n    # Let's consider the function `sigmoid(K * (2 * item - remaining_capacity))`\n    # Let `f = remaining_capacity - item`. We want `f` to be small and positive.\n    # Consider `sigmoid(K * (item - remaining_capacity))`\n    # If `rem - item = 0.1`, `item - rem = -0.1`. `sigmoid(K * -0.1)`. With K=10, sigmoid(-1) ~ 0.27. Low.\n    # If `rem - item = 1.0`, `item - rem = -1.0`. `sigmoid(K * -1.0)`. With K=10, sigmoid(-10) ~ 0.000045. Very low.\n    # If `rem - item = -0.1` (item too big), `item - rem = 1.0`. `sigmoid(K * 1.0)`. With K=10, sigmoid(10) ~ 1.\n\n    # The function `1 / (1 + exp(k * (bins_remain_cap - item)))` with k>0:\n    # - If `bins_remain_cap - item = 0.1` (good fit): `exp(k*0.1)`. With k=2, exp(0.2) \u2248 1.22. Sigmoid \u2248 0.45.\n    # - If `bins_remain_cap - item = 1.0` (loose fit): `exp(k*1.0)`. With k=2, exp(2.0) \u2248 7.39. Sigmoid \u2248 0.12.\n    # - If `bins_remain_cap - item = -0.1` (item too big, not applicable): `exp(-k*0.1)`. Sigmoid > 0.5.\n\n    # This function penalizes bins with larger positive slack, but scores for tight fits (small positive slack)\n    # are also not very high (around 0.45). It might be better if tight fits also got high scores.\n\n    # A common approach in practice is to normalize the differences or use a scaled logistic function.\n    # Let's try to map the 'gap' `bins_remain_cap - item` to the sigmoid, but map `0` to the highest point.\n    # This suggests an argument like `-(bins_remain_cap - item)^2`.\n    # If `bins_remain_cap - item = 0.1`, argument is `-0.01`. Sigmoid `1/(1+exp(0.01))` ~ 0.497.\n    # If `bins_remain_cap - item = 1.0`, argument is `-1.0`. Sigmoid `1/(1+exp(1))` ~ 0.268.\n    # This prioritizes bins closer to `item` capacity.\n\n    # Let's use this: `priorities = sigmoid( - (bins_remain_cap - item)^2 * scale )`\n    # where `scale` controls the steepness. A higher scale means we are more sensitive to deviations from perfect fit.\n    # If `scale = 100`, `(0.1)^2 * 100 = 0.01 * 100 = 0.1`. sigmoid( -0.1 ) ~ 0.475.\n    # If `scale = 100`, `(1.0)^2 * 100 = 1.0 * 100 = 100`. sigmoid( -100 ) ~ 0.\n    # This seems like a good approach.\n\n    # Apply to fitting bins only.\n    # Calculate `(bins_remain_cap - item)` for fitting bins.\n    # These are the `priorities` where `can_fit_mask` is True.\n    fit_differences_vals = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate the squared differences.\n    squared_diffs = fit_differences_vals ** 2\n\n    # Scale the squared differences. `scale_factor` should be tuned.\n    # A higher scale_factor makes the priority drop more sharply as the difference increases.\n    # We are looking for values where the exponent `-squared_diffs * scale_factor`\n    # does not cause overflow (i.e., `squared_diffs * scale_factor` is not too large negative).\n    # The exponent is `-x`. So `x` can't be too negative.\n    # `x = squared_diffs * scale_factor`.\n    # `squared_diffs` is always >= 0. So `x` is always >= 0.\n    # We want `sigmoid(x)` to be highest for `x` closest to 0.\n    # Sigmoid(x) is highest for large positive x. We want high scores for small squared diffs.\n\n    # Let's go back to: `sigmoid( K * (target_value - current_value) )`\n    # Or `sigmoid( K * -(current_value - target_value) )`\n    # `target_value = item`\n    # `current_value = bins_remain_cap`\n    # `sigmoid( K * -(bins_remain_cap - item))` = `sigmoid( K * (item - bins_remain_cap))`\n\n    # Let `K = 2.0` for sensitivity.\n    # `arg = K * (item - bins_remain_cap[can_fit_mask])`\n    # For fitting bins: `bins_remain_cap >= item`, so `item - bins_remain_cap <= 0`.\n    # Thus `arg` is always <= 0 for fitting bins.\n\n    # If `bins_remain_cap - item = 0.1` (good fit)\n    # `item - bins_remain_cap = -0.1`\n    # `arg = 2.0 * -0.1 = -0.2`\n    # `sigmoid(-0.2) = 1 / (1 + exp(0.2)) \u2248 0.45`\n\n    # If `bins_remain_cap - item = 1.0` (loose fit)\n    # `item - bins_remain_cap = -1.0`\n    # `arg = 2.0 * -1.0 = -2.0`\n    # `sigmoid(-2.0) = 1 / (1 + exp(2.0)) \u2248 0.12`\n\n    # This function assigns lower scores to looser fits, but the highest scores are near 0.5 for tightest fits.\n    # This seems reasonable: prioritize bins that don't waste too much space, and among those,\n    # prefer the ones that leave less excess capacity.\n\n    # Robust sigmoid for `1 / (1 + exp(x))` where x <= 0 for fitting bins.\n    # If `x` becomes very negative (item << remaining_capacity), exp(x) approaches 0. sigmoid approaches 1.\n    # This would mean bins with *lots* of slack get the highest score near 1. This is WRONG.\n\n    # The core problem is correctly mapping the \"goodness of fit\" to a score, where a value close to zero\n    # difference is best, and this best value should yield the highest priority.\n    # `sigmoid(x)` is high for large POSITIVE `x`.\n    # We want `x` to be large positive when `bins_remain_cap - item` is small positive.\n\n    # This means we need `f(bins_remain_cap - item)` such that when `bins_remain_cap - item` is small positive,\n    # `f(...)` is large positive.\n    # Consider `f(d) = some_constant - d`. If `d` is small positive, `f` is large positive.\n    # Let `d = bins_remain_cap - item`. We want `d` close to 0.\n    # Consider `sigmoid(Constant - (bins_remain_cap - item))`.\n\n    # Or simply, let the \"value\" be `- (bins_remain_cap - item)`.\n    # When `bins_remain_cap - item = 0.1`, value = -0.1.\n    # When `bins_remain_cap - item = 1.0`, value = -1.0.\n    # We want to scale these values such that small negative values become large positive for sigmoid.\n    # `sigmoid(scale * value)`.\n    # `scale * value = scale * (item - bins_remain_cap)`.\n\n    # Let `scale = 5.0`\n    # If `bins_remain_cap - item = 0.1`: `item - bins_remain_cap = -0.1`. `scale * value = -0.5`.\n    # `sigmoid(-0.5) = 1 / (1 + exp(0.5)) \u2248 0.377`.\n\n    # If `bins_remain_cap - item = 0.01`: `item - bins_remain_cap = -0.01`. `scale * value = -0.05`.\n    # `sigmoid(-0.05) = 1 / (1 + exp(0.05)) \u2248 0.487`.\n    # This looks good: tighter fits (smaller positive differences) get higher scores, approaching 0.5.\n    # If `bins_remain_cap - item = 1.0`: `item - bins_remain_cap = -1.0`. `scale * value = -5.0`.\n    # `sigmoid(-5.0) = 1 / (1 + exp(5.0)) \u2248 0.006`. Low score for loose fits.\n\n    # Let's refine this. The scores are capped around 0.5.\n    # To get scores potentially higher than 0.5, we can shift the sigmoid or use a different base.\n    # The prompt requires \"highest priority score\".\n\n    # Let's use the \"smallest waste\" criterion: we want to minimize `bins_remain_cap - item`.\n    # This quantity should be mapped to a high value if it's small positive.\n    # Consider `f(x) = exp(-x)`. This gives high values for small positive `x`.\n    # Let `x = (bins_remain_cap - item)`. Then `exp(-(bins_remain_cap - item))`.\n    # If `bins_remain_cap - item = 0.1`, `exp(-0.1) \u2248 0.90`.\n    # If `bins_remain_cap - item = 1.0`, `exp(-1.0) \u2248 0.36`.\n    # If `bins_remain_cap - item = 10.0`, `exp(-10.0) \u2248 0.000045`.\n    # This function also works and can be scaled.\n    # Let's use `sigmoid(Constant - (bins_remain_cap - item) * scale)`.\n    # The `Constant` term allows us to shift the peak.\n    # Or more simply, let's use the original Sigmoid formula but with an argument that peaks at 0.\n\n    # Let `d = bins_remain_cap - item`. We want a function that is max at d=0, decreasing for d>0 and d<0.\n    # `-d^2` peaks at 0. `sigmoid(-d^2 * scale)`\n    # Let `scale = 10`.\n    # `d = 0.1`, `-d^2 * scale = -0.01 * 10 = -0.1`. `sigmoid(-0.1) \u2248 0.475`.\n    # `d = 0.01`, `-d^2 * scale = -0.0001 * 10 = -0.001`. `sigmoid(-0.001) \u2248 0.499`.\n    # `d = 1.0`, `-d^2 * scale = -1.0 * 10 = -10.0`. `sigmoid(-10.0) \u2248 0.000045`.\n    # This correctly assigns higher scores for values of `bins_remain_cap - item` closer to 0.\n    # The scores are still around 0.5.\n\n    # Let's adjust the sigmoid to be `0.5 + 0.5 * tanh(x/2)`.\n    # If we use `x = - (bins_remain_cap - item) * scale`:\n    # `d = 0.1`, `x = -0.1 * scale`. `d=0.01`, `x = -0.001*scale`.\n    # For `tanh(y)` to be near 1, `y` must be large positive.\n    # `y = x/2`. So `x` must be large positive.\n    # `x = -(bins_remain_cap - item) * scale`.\n    # For `x` to be large positive, `-(bins_remain_cap - item)` must be large positive, meaning `bins_remain_cap - item` must be large negative.\n    # This prioritizes bins where the item is too big. WRONG.\n\n    # Let's try the simple approach with scaling and ensuring numerical stability.\n    # Priority = Sigmoid( `scale` * ( `target_capacity` - `remaining_capacity` ) )\n    # where `target_capacity` is the ideal capacity for the item.\n    # If we assume the ideal scenario is to fill a bin as much as possible without overflow,\n    # then `target_capacity` could be related to `item`.\n    # Or, we want `remaining_capacity` to be close to `item`.\n\n    # The \"Sigmoid Fit\" often implies mapping to a value that represents how \"close\" a bin is to\n    # accommodating the item efficiently.\n    # Let's define the quality of fit for bin `i` as `q_i = remaining_capacity_i - item`.\n    # We want `q_i` to be small and positive.\n    # We can map `q_i` using a sigmoid function such that small positive `q_i` yields high output.\n    # The function `Sigmoid( C - k * q_i )` where `C` is a bias and `k > 0`.\n    # Let `C = 0` and `k = scale`.\n    # `Sigmoid( -k * q_i ) = Sigmoid( -k * (bins_remain_cap - item) )`.\n    # Let `scale = 2.0`.\n    # If `bins_remain_cap - item = 0.1` (good fit): `arg = -0.2`. `Sigmoid(-0.2) \u2248 0.45`.\n    # If `bins_remain_cap - item = 1.0` (loose fit): `arg = -2.0`. `Sigmoid(-2.0) \u2248 0.12`.\n    # If `bins_remain_cap - item = 10.0` (very loose fit): `arg = -20.0`. `Sigmoid(-20.0) \u2248 0`.\n\n    # The maximum score is < 0.5 here.\n    # To get scores closer to 1 for good fits, we can shift the sigmoid or its input.\n    # Let's use `sigmoid(C + x)` where `x` is scaled.\n    # We want `bins_remain_cap - item` small positive to map to high score.\n    # Consider `sigmoid( K * (1 - (bins_remain_cap - item) / MaxCapacity ) )`\n    # Or simpler: `sigmoid( K * (IdealFill - ActualFill) )`.\n\n    # Let's simplify to a practical implementation for the heuristic.\n    # We want bins that have *just enough* space.\n    # We can calculate a \"penalty\" for each bin: `penalty = max(0, item - remaining_capacity)` for bins that don't fit.\n    # For bins that fit: `penalty = max(0, remaining_capacity - item)`.\n    # Then apply sigmoid to negative penalty.\n\n    # Final approach: Use the `sigmoid( k * (ideal_remaining - actual_remaining) )` form.\n    # Let `ideal_remaining` be the smallest amount of capacity that can accommodate the item, so `item`.\n    # Then `ideal_remaining = item`.\n    # `arg = scale * (item - bins_remain_cap)`\n    # For fitting bins: `bins_remain_cap >= item`. So `item - bins_remain_cap <= 0`.\n    # `arg <= 0`.\n    # We want the highest score when `item - bins_remain_cap` is closest to 0 (which means `bins_remain_cap - item` is smallest positive).\n    # `sigmoid(x)` is highest for large POSITIVE `x`.\n    # This means we need `scale * (item - bins_remain_cap)` to be large positive.\n    # Which implies `scale` should be negative and `item - bins_remain_cap` should be large negative (meaning `bins_remain_cap - item` is large positive).\n\n    # Let's try `sigmoid( k * (bins_remain_cap - item) )` where `k` is negative.\n    # Let `k = -2.0`.\n    # `d = bins_remain_cap - item`\n    # `d = 0.1` (good fit): `arg = -0.2`. `sigmoid(-0.2) \u2248 0.45`.\n    # `d = 1.0` (loose fit): `arg = -2.0`. `sigmoid(-2.0) \u2248 0.12`.\n    # `d = 10.0` (very loose fit): `arg = -20.0`. `sigmoid(-20.0) \u2248 0`.\n    # This means bins with more slack get lower priority. This is good.\n    # The highest scores are still capped below 0.5 for tight fits.\n\n    # To get scores potentially above 0.5, we can bias the input:\n    # `sigmoid(Bias + k * (bins_remain_cap - item))`\n    # If we want the peak at `bins_remain_cap - item = 0`.\n    # Let `k = -2.0`.\n    # We want `Bias` such that `sigmoid(Bias + k * 0) = sigmoid(Bias)` is high.\n    # Say we want the peak value to be 0.8. `sigmoid(Bias) = 0.8`.\n    # `1 / (1 + exp(-Bias)) = 0.8`\n    # `1 = 0.8 + 0.8 * exp(-Bias)`\n    # `0.2 = 0.8 * exp(-Bias)`\n    # `0.25 = exp(-Bias)`\n    # `-Bias = ln(0.25) = -ln(4) = -1.386`\n    # `Bias = 1.386`\n\n    # So, `sigmoid(1.386 - 2.0 * (bins_remain_cap - item))`\n    # Let's check:\n    # `d = 0.1`: `arg = 1.386 - 2.0 * 0.1 = 1.386 - 0.2 = 1.186`. `sigmoid(1.186) \u2248 0.766`. Good.\n    # `d = 1.0`: `arg = 1.386 - 2.0 * 1.0 = 1.386 - 2.0 = -0.614`. `sigmoid(-0.614) \u2248 0.35`. Lower.\n    # `d = 10.0`: `arg = 1.386 - 2.0 * 10.0 = 1.386 - 20.0 = -18.614`. `sigmoid(-18.614) \u2248 0`. Very low.\n\n    # This approach seems to work. The `Bias` value (`~1.386`) can be tuned, as well as `k` (`-2.0`).\n    # A common parameter `k` used in literature is around 1 to 5 for the scaled difference.\n    # Let's choose `scale_factor = 2.0` and a `bias_shift` to lift the scores.\n    # Let's re-evaluate what \"Sigmoid Fit Score\" truly means.\n    # It often refers to mapping the \"fit\" or \"slack\" to a preference.\n    # A common parameterization might be `sigmoid( (IdealCapacity - CurrentCapacity) / ScalingFactor )`.\n\n    # Let's use a more direct interpretation of \"good fit\": remaining_capacity is slightly greater than item.\n    # Let the \"target gap\" be 0.\n    # We want the gap `g = bins_remain_cap - item` to be close to 0.\n    # Consider a score that is high when `g` is small positive.\n    # This could be `sigmoid( K * (epsilon - g) )` where `epsilon` is a small positive value.\n    # If `epsilon = 0.1`, `K = 2`.\n    # `g = 0.1`: `sigmoid(2 * (0.1 - 0.1)) = sigmoid(0) = 0.5`.\n    # `g = 0.01`: `sigmoid(2 * (0.1 - 0.01)) = sigmoid(0.18) \u2248 0.545`.\n    # `g = 1.0`: `sigmoid(2 * (0.1 - 1.0)) = sigmoid(-1.8) \u2248 0.14`.\n    # This looks like a valid Sigmoid Fit heuristic.\n\n    # `scale_factor` here is `K`.\n    # `epsilon` is a small constant representing the \"ideal slack\".\n\n    scale_factor = 3.0  # Controls steepness around epsilon\n    ideal_slack = 0.0   # The ideal amount of remaining capacity after fitting an item.\n                        # Setting to 0 aims for the tightest possible fit.\n                        # Setting to a small positive value can help with future packing.\n\n    # Calculate the argument for the sigmoid function.\n    # We want the argument to be high positive when (bins_remain_cap - item) is close to ideal_slack.\n    # So, `arg = scale_factor * (ideal_slack - (bins_remain_cap - item))`\n    # `arg = scale_factor * (ideal_slack - bins_remain_cap + item)`\n\n    # For bins that cannot fit the item, their priority should be 0.\n    # `priorities` is already 0 for non-fitting bins.\n    # We only calculate the sigmoid score for fitting bins.\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # Calculate the 'gap' for fitting bins.\n    gap = fitting_bins_remain_cap - item\n\n    # Calculate the sigmoid argument:\n    # We want high scores when 'gap' is close to 'ideal_slack'.\n    # Sigmoid(x) is high for large positive x.\n    # So, we want `arg` to be large positive when `gap` is close to `ideal_slack`.\n    # This implies `arg` should be inversely related to `abs(gap - ideal_slack)`.\n    # Using `scale_factor * (ideal_slack - gap)` works.\n\n    # If ideal_slack = 0:\n    # arg = scale_factor * (0 - gap) = -scale_factor * gap\n    # If gap = 0.1, arg = -0.1 * scale_factor. Sigmoid will be < 0.5.\n    # If gap = 0.01, arg = -0.01 * scale_factor. Sigmoid will be closer to 0.5.\n    # To get scores > 0.5 for good fits, we need to shift the sigmoid.\n\n    # Alternative approach: Prioritize bins that result in the smallest positive remaining capacity.\n    # This is often implemented as: For each bin i, if `remaining_capacity_i >= item`,\n    # priority is `1.0 / (remaining_capacity_i - item + epsilon)`.\n    # Let's use sigmoid to map this inverse relationship.\n\n    # Consider the inverse relationship: the smaller the slack (positive), the higher the priority.\n    # `slack = bins_remain_cap - item`.\n    # We want `1/slack` to be high for small positive slack.\n    # This implies we want to map `1/slack` using sigmoid such that high values of `1/slack`\n    # result in high sigmoid outputs.\n\n    # Let `scaled_slack = (bins_remain_cap - item) * scale_factor`.\n    # We want small positive `scaled_slack` to give high sigmoid scores.\n    # Let's use `sigmoid( K * (Constant - scaled_slack) )`.\n    # Let `Constant = 1.0`. `scale_factor = 2.0`.\n    # `arg = K * (1.0 - scaled_slack) = K * (1.0 - (bins_remain_cap - item) * scale_factor)`\n\n    # Let's simplify to a commonly used form.\n    # Prioritize bins that are \"almost full\".\n    # We can map the remaining capacity itself.\n    # A bin with remaining capacity equal to `item` would be ideal.\n    # `sigmoid( K * (item - remaining_capacity) )`\n    # `arg = K * (item - fitting_bins_remain_cap)`\n    # As seen before, for fitting bins `fitting_bins_remain_cap >= item`, so `arg <= 0`.\n    # Scores are capped at 0.5.\n\n    # To get scores above 0.5, we can shift the argument or use a different form.\n    # Let's consider the inverse of slack, but bounded.\n    # Let `slack = bins_remain_cap[can_fit_mask] - item`.\n    # If `slack = 0`, priority is high. If `slack` is large positive, priority is low.\n    # We can map `1 / (slack + epsilon)` to sigmoid.\n    # `1 / (slack + epsilon)` is high when `slack` is small positive.\n\n    # Let `value = 1.0 / (gap + 1e-9)` where `gap = fitting_bins_remain_cap - item`.\n    # We want to map this `value` such that large `value` gives high sigmoid.\n    # So `sigmoid(scale_factor * value)`.\n    # `value` can be very large if `gap` is close to 0.\n    # If `gap` is very small (e.g., 1e-10), `value` is large.\n    # `scaled_value = scale_factor * value`. This can overflow if `scale_factor` is large and `gap` is small.\n    # E.g., `scale_factor=2`, `gap=1e-10`. `value = 1e10`. `scaled_value = 2e10`. Sigmoid argument overflows.\n\n    # Robust sigmoid calculation needed.\n    # `sigmoid(x) = 1 / (1 + exp(-x))`\n    # We want `x` to be large positive when `gap` is small positive.\n    # So `x = - scale_factor * gap` is problematic for small positive gaps.\n    # If `gap = 0.01`, `x = -scale_factor * 0.01`. Sigmoid < 0.5.\n    # We need `x` to be large positive.\n\n    # Let's try `sigmoid( Constant - scale_factor * gap )`.\n    # `Constant = 2.0`, `scale_factor = 2.0`.\n    # `gap = 0.01`: `arg = 2.0 - 0.02 = 1.98`. `sigmoid(1.98) \u2248 0.87`. Good.\n    # `gap = 0.1`: `arg = 2.0 - 0.2 = 1.8`. `sigmoid(1.8) \u2248 0.86`. Not much difference. Need steeper.\n    # Let `scale_factor = 5.0`.\n    # `gap = 0.01`: `arg = 2.0 - 0.05 = 1.95`. `sigmoid(1.95) \u2248 0.87`.\n    # `gap = 0.1`: `arg = 2.0 - 0.5 = 1.5`. `sigmoid(1.5) \u2248 0.81`. Difference increases.\n\n    # Let `Constant = 3.0`, `scale_factor = 5.0`.\n    # `gap = 0.01`: `arg = 3.0 - 0.05 = 2.95`. `sigmoid(2.95) \u2248 0.95`.\n    # `gap = 0.1`: `arg = 3.0 - 0.5 = 2.5`. `sigmoid(2.5) \u2248 0.92`.\n    # `gap = 0.5`: `arg = 3.0 - 2.5 = 0.5`. `sigmoid(0.5) \u2248 0.62`.\n    # `gap = 1.0`: `arg = 3.0 - 5.0 = -2.0`. `sigmoid(-2.0) \u2248 0.12`.\n\n    # This parameterization `sigmoid(Constant - scale_factor * gap)` is reasonable.\n    # `Constant` acts as a threshold for the gap. Gaps larger than `Constant/scale_factor`\n    # will result in scores below 0.5.\n    # `scale_factor` determines how quickly the score drops.\n\n    # Let's implement this.\n    # Use `np.clip` on the `bins_remain_cap` or `gap` to avoid issues with `exp` if\n    # `Constant - scale_factor * gap` becomes extremely large negative.\n    # `Constant - scale_factor * gap` is problematic if it's very negative.\n    # This happens when `scale_factor * gap` is very positive.\n    # This occurs when `gap` is large positive.\n    # If `gap` is large, the score should be low anyway.\n    # e.g. `gap = 1000`, `scale = 5`. `arg = C - 5000`. This can be very negative.\n    # `exp(-arg)` will be `exp(5000 - C)`, which overflows.\n    # We need to handle `exp(-x)` when `x` is very negative.\n    # `sigmoid(x) = 1 / (1 + exp(-x))`\n\n    # If `arg = Constant - scale_factor * gap`.\n    # If `arg` is very negative: `exp(-arg)` is very positive and can overflow.\n    # Let `arg_max = 700`. If `arg > arg_max`, `sigmoid(arg) \u2248 1`.\n    # Let `arg_min = -700`. If `arg < arg_min`, `sigmoid(arg) \u2248 0`.\n\n    # We are concerned about `arg` being very negative.\n    # `Constant - scale_factor * gap < -700`\n    # `Constant + 700 < scale_factor * gap`\n    # `gap > (Constant + 700) / scale_factor`\n    # This means if `gap` is very large, the argument becomes very negative, and the score goes to 0.\n    # This is desired behavior. The overflow happens in `exp(-arg)`.\n    # So if `arg` is very negative, `exp(-arg)` is very large positive.\n    # `1 / (1 + very_large_positive)` is approximately `1 / very_large_positive` which is near 0.\n    # So, `sigmoid` should naturally go to 0.\n\n    # We need to be careful with `gap` values.\n    # If `bins_remain_cap` is large and `item` is small, `gap` can be large.\n    # Example: `bins_remain_cap = 1000`, `item = 1`. `gap = 999`.\n    # `arg = C - 5 * 999 = C - 4995`.\n    # If `C=3`, `arg = 3 - 4995 = -4992`.\n    # `exp(-arg) = exp(4992)`. This will overflow.\n\n    # A robust sigmoid implementation is needed.\n    # `sigmoid(x) = 1 / (1 + exp(-x))`\n    # If `x < -X`, `exp(-x)` overflows.\n    # If `x > X`, `exp(-x)` underflows to 0.\n\n    # To avoid overflow in `exp(-x)` when `x` is very negative:\n    # Let `y = -x`. We need to avoid `exp(y)` overflow.\n    # If `y > Y_max`, replace `exp(y)` with something like `np.inf`.\n    # `sigmoid(x) = 1 / (1 + np.exp(-x))`\n    # `x = Constant - scale_factor * gap`\n    # Let `scale_factor = 5.0`. `Constant = 3.0`.\n    # If `gap` is large, `x` is very negative.\n    # If `gap = 1000`, `x = 3 - 5000 = -4997`.\n    # `-x = 4997`. `exp(4997)` overflows.\n\n    # If we clip `gap` before calculating `x`:\n    # Let `max_gap = 100`.\n    # `clipped_gap = np.clip(gap, 0, max_gap)`.\n    # `x = Constant - scale_factor * clipped_gap`.\n    # If `gap` was originally 1000, `clipped_gap` is 100.\n    # `x = 3 - 5 * 100 = 3 - 500 = -497`.\n    # `-x = 497`. Still overflows.\n\n    # We need to scale the argument itself to a range where sigmoid is well-behaved.\n    # OR use a different form: `0.5 * (1 + tanh(x/2))`\n    # `tanh(y)` can also overflow/underflow.\n    # `tanh(y) = (exp(y) - exp(-y)) / (exp(y) + exp(-y))`\n    # If `y` is large positive, `tanh(y)` is 1. If `y` is large negative, `tanh(y)` is -1.\n    # The `exp(y)` part can overflow if `y` is large positive.\n    # `y = x/2 = (Constant - scale_factor * gap) / 2`.\n    # This also faces similar issues.\n\n    # A common robust sigmoid:\n    # def robust_sigmoid(x):\n    #     return np.clip(0.5 * (1 + np.tanh(x / 2.0)), 0.0, 1.0)\n    # Let's assume we have this robust version implicitly for now.\n\n    # Let's try the simple form, with reasonable parameters and assume numpy handles intermediate steps or we clip the argument of exp.\n    # `Constant = 3.0`, `scale_factor = 5.0`.\n    # `gap = bins_remain_cap[can_fit_mask] - item`.\n    # `arg = Constant - scale_factor * gap`.\n    # We want to prevent `exp(-arg)` from overflowing.\n    # This happens if `-arg` is too large positive.\n    # So, we want to limit `-arg` from exceeding ~700.\n    # `-arg = -Constant + scale_factor * gap`.\n    # We need `-Constant + scale_factor * gap < 700`.\n    # `scale_factor * gap < 700 + Constant`.\n    # `gap < (700 + Constant) / scale_factor`.\n    # If `gap` exceeds this, `exp(-arg)` will overflow.\n    # For `C=3, S=5`, this threshold is `(700+3)/5 = 703/5 = 140.6`.\n    # If `gap` is greater than ~140.6, `exp(-arg)` might overflow.\n\n    # Let's clip the `gap` to a reasonable maximum to prevent extreme values.\n    # Let `max_gap_clip = 50.0` (tuned parameter).\n    # `clipped_gap = np.clip(gap, 0, max_gap_clip)`.\n    # `arg = Constant - scale_factor * clipped_gap`.\n    # Max negative `arg` will be `3 - 5 * 50 = 3 - 250 = -247`.\n    # `-arg = 247`. `exp(247)` is large but might be manageable. Let's check numpy exp.\n    # `np.exp(700)` is `~1.0e304`. `np.exp(710)` is `inf`. So the threshold is around 700-710.\n    # Our max negative `arg` is -247. So `-arg` is 247. This should be fine.\n\n    # Parameter values:\n    # `Constant`: Influences the \"midpoint\" of the sigmoid.\n    # `scale_factor`: Controls how steep the priority drop-off is.\n    # `max_gap_clip`: Prevents extreme gaps from dominating or causing numerical issues.\n\n    constant_bias = 3.0\n    scale_factor = 5.0\n    max_gap_clip = 50.0  # Represents the maximum \"useful\" slack. Slack beyond this is equally penalized.\n\n    # Ensure we only process fitting bins.\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # Calculate the gap (remaining capacity - item size) for fitting bins.\n    gap = fitting_bins_remain_cap - item\n\n    # Clip the gap to prevent extreme values and potential numerical issues.\n    clipped_gap = np.clip(gap, 0.0, max_gap_clip)\n\n    # Calculate the argument for the sigmoid function.\n    # We want higher priority for smaller gaps.\n    # The sigmoid function is `1 / (1 + exp(-x))`. It's high for positive `x`.\n    # So we want `x` to be large positive when `clipped_gap` is small.\n    # We use `Constant - scale_factor * clipped_gap`.\n    # If `clipped_gap` is small (e.g., 0), `arg = Constant`.\n    # If `clipped_gap` is large (e.g., max_gap_clip), `arg = Constant - scale_factor * max_gap_clip`.\n    # This will result in lower scores for larger gaps.\n    sigmoid_arg = constant_bias - scale_factor * clipped_gap\n\n    # Apply the sigmoid function to get the priority scores.\n    # Use `np.clip` to ensure the argument doesn't cause `exp` overflow/underflow,\n    # although the `clipped_gap` should largely handle this.\n    # Limiting sigmoid_arg to [-700, 700] is a common robust practice.\n    # `-arg = -Constant + scale_factor * clipped_gap`\n    # Max negative `arg` is `3 - 5*50 = -247`. Min `-arg` is 247.\n    # Min positive `arg` is `3 - 5*0 = 3`. Max `-arg` is -3.\n    # The range of `arg` is `[3-250, 3] = [-247, 3]`.\n    # So `exp(-arg)` will be in range `[exp(-3), exp(247)]`. `exp(247)` is okay.\n\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(-sigmoid_arg))\n\n    # Normalize priorities if needed, or simply return as is.\n    # In this context, higher values mean higher priority.\n\n    return priorities",
    "response_id": 6,
    "obj": 4.487435181491823,
    "SLOC": 22.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response5.txt_stdout.txt",
    "code_path": "problem_iter2_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Sigmoid Fit Score.\n\n    This heuristic aims to prioritize bins that offer the \"best fit\" for the item.\n    A \"best fit\" is defined as a bin where the remaining capacity is only slightly\n    larger than the item's size. This strategy tries to fill bins as much as possible\n    without leaving excessive empty space, thereby minimizing fragmentation.\n\n    The priority is calculated using a sigmoid function. The function is designed\n    to peak when the remaining capacity (`bins_remain_cap`) is precisely equal to\n    the item's size, and the priority decreases as the remaining capacity deviates\n    (either smaller or larger). However, bins where the item doesn't fit at all\n    are assigned a zero priority.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element is the remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as `bins_remain_cap`, containing the priority\n        score for each bin. Higher scores indicate a more desirable bin for the item.\n    \"\"\"\n\n    def sigmoid(x, steepness=10.0, center=0.0):\n        \"\"\"A custom sigmoid function that can be shifted and scaled.\"\"\"\n        return 1 / (1 + np.exp(-steepness * (x - center)))\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit.\n    # The condition is that the bin's remaining capacity must be greater than or equal to the item's size.\n    fits_mask = bins_remain_cap >= item\n\n    # For bins where the item fits, calculate the 'fit difference'.\n    # This is the excess capacity after placing the item.\n    # We want to prioritize bins where this difference is small and non-negative.\n    if np.any(fits_mask):\n        # Calculate the excess capacity for fitting bins.\n        excess_capacities = bins_remain_cap[fits_mask] - item\n\n        # The sigmoid function is used to map these excess capacities to priority scores.\n        # We want the highest priority (e.g., close to 1) when excess_capacity is minimal.\n        # A common formulation for \"good fit\" prioritizes `excess_capacity` close to 0.\n        # To achieve this with a sigmoid, we can use a function that decreases as `excess_capacity` increases.\n        # `sigmoid(k * (ideal_excess - actual_excess))` works well here.\n        # Let `ideal_excess` be 0.\n        # The argument becomes `steepness * (0 - excess_capacities)` which is `-steepness * excess_capacities`.\n        # This means scores will decrease from 0.5 as `excess_capacities` increases.\n        #\n        # To align with \"slightly larger\" being the peak priority (as per problem description interpretation),\n        # we can shift the sigmoid. Let's define the peak priority (score=0.5) at `ideal_gap` (a small positive value).\n        #\n        # The function `sigmoid(steepness * (ideal_gap - excess_capacities))` will have its midpoint at `excess_capacities = ideal_gap`.\n        # With `steepness=10.0` and `ideal_gap=0.05`, the peak priority of 0.5 occurs when `bins_remain_cap - item = 0.05`.\n        # Values of `excess_capacities` smaller than `ideal_gap` (but >= 0) will yield scores > 0.5.\n        # Values of `excess_capacities` larger than `ideal_gap` will yield scores < 0.5.\n\n        ideal_gap = 0.05  # Prefer bins with a small positive remaining capacity after fitting the item.\n        steepness = 10.0  # Controls how sharply the priority drops as excess capacity increases.\n\n        # Calculate the argument for the sigmoid function.\n        # We want the peak of the sigmoid (where the argument is 0) to align with our 'ideal_gap'.\n        # `argument = steepness * (ideal_gap - excess_capacities)`\n        argument_values = steepness * (ideal_gap - excess_capacities)\n\n        # Apply the sigmoid function to get the priority scores for fitting bins.\n        priorities[fits_mask] = sigmoid(argument_values, steepness=steepness, center=0.0)\n\n    # For bins where the item does not fit (fits_mask is False), the priority remains 0,\n    # ensuring they are not selected unless no fitting bins exist.\n\n    return priorities",
    "response_id": 5,
    "obj": 3.9389708815317115,
    "SLOC": 12.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response24.txt_stdout.txt",
    "code_path": "problem_iter1_code24.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    Exact Fit First prioritizes bins that can accommodate the item with the least remaining capacity.\n    This is achieved by assigning a high priority to bins where (remaining_capacity - item_size) is minimized,\n    but only if the item fits. For items that don't fit, a very low priority is assigned.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find bins where the item fits\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins where the item fits, calculate the \"exactness\" score.\n    # A smaller difference means a better fit. We want to prioritize smaller differences.\n    # So, we take the negative of the difference.\n    # The ideal fit would have a difference of 0.\n    if np.any(can_fit_mask):\n        remaining_capacities_for_fitting_bins = bins_remain_cap[can_fit_mask]\n        differences = remaining_capacities_for_fitting_bins - item\n        # We want to prioritize smaller differences, so we use the negative difference.\n        # A difference of 0 (perfect fit) will give a priority of 0.\n        # A small positive difference (e.g., 0.1) will give a priority of -0.1.\n        # A large positive difference (e.g., 2) will give a priority of -2.\n        # This ensures bins with minimal leftover space are preferred.\n        priorities[can_fit_mask] = -differences\n    \n    # Bins where the item does not fit get a very low priority score\n    # to ensure they are never chosen if a fitting bin exists.\n    priorities[~can_fit_mask] = -np.inf\n\n    return priorities",
    "response_id": 24,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response11.txt_stdout.txt",
    "code_path": "problem_iter1_code11.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit strategy.\n\n    The Best Fit strategy aims to place the item into the bin where it fits\n    most snugly, leaving the smallest possible remaining capacity. This often\n    helps in leaving larger spaces available for future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher priority means a better fit. Bins that cannot accommodate the\n        item are given a priority of -1 (or any value indicating they are invalid).\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with -1 for invalid bins\n\n    # Calculate remaining capacity after placing the item in each bin\n    potential_remaining_capacities = bins_remain_cap - item\n\n    # Consider only bins that can accommodate the item\n    valid_indices = np.where(potential_remaining_capacities >= 0)[0]\n\n    if len(valid_indices) > 0:\n        # The priority is the negative of the remaining capacity.\n        # This is because we want to minimize the remaining capacity (best fit).\n        # A smaller positive remaining capacity (e.g., 0.1) is better than a larger one (e.g., 2.0).\n        # By taking the negative, a smaller positive value becomes a larger negative value,\n        # effectively giving it a higher \"priority\" in a maximization context.\n        # Alternatively, we can think of this as assigning a \"waste\" score, and we want to minimize waste.\n        # For Best Fit, we want the bin with the smallest non-negative remaining capacity.\n        # Assigning priorities as the negative remaining capacity:\n        # A bin with remaining_cap = 1.5 after placing item gets priority -1.5\n        # A bin with remaining_cap = 0.2 after placing item gets priority -0.2\n        # -0.2 > -1.5, so the bin with 0.2 remaining capacity has higher priority.\n        priorities[valid_indices] = -potential_remaining_capacities[valid_indices]\n\n    return priorities",
    "response_id": 11,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response14.txt_stdout.txt",
    "code_path": "problem_iter1_code14.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    Exact Fit First strategy prioritizes bins that have a remaining capacity\n    exactly equal to the item's size. Among bins with exact fits, it\n    further prioritizes bins that are \"tighter\" (i.e., have less remaining\n    capacity after the item is placed). If no exact fit is found, it\n    falls back to selecting a bin that can accommodate the item and\n    minimizes the remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Find bins that have an exact fit\n    exact_fit_mask = bins_remain_cap == item\n    if np.any(exact_fit_mask):\n        # Among exact fits, prefer the one with least remaining capacity (which is 0 after fit)\n        # So we can give a high positive score, higher for those that would become exactly full.\n        # Since all exact fits will have 0 remaining capacity after the item,\n        # we can assign a high, uniform score to them to prioritize them.\n        # We add a small penalty to break ties in a deterministic way, although for exact fit,\n        # any exact fit is generally considered equal.\n        priorities[exact_fit_mask] = 1.0\n\n        # To further differentiate among exact fits and prioritize the \"tightest\"\n        # (which in this case, after placing the item, all exact fits leave 0 space),\n        # we can assign a small negative bonus based on their original remaining capacity.\n        # This might seem counterintuitive for \"exact fit\", but if we interpret\n        # \"exact fit\" more broadly as \"closest to item size\", this would matter.\n        # For strict \"exact fit = item size\", all exact fits are equally good.\n        # For this implementation, let's prioritize exact fits and then by how\n        # little extra space is left. For exact fits, this extra space is 0.\n        # So, let's assign a higher priority to exact fits by simply making them positive.\n        # To make them *more* priority, we can give them a higher score.\n        # Let's assign a score that reflects \"best fit\" if it's an exact fit.\n        # We can think of this as `(bin_capacity - item) + penalty_for_exact_fit`.\n        # For exact fits, `bin_capacity - item` is 0.\n\n        # A more sophisticated approach for \"tightest\" exact fit would involve\n        # looking at bins that are *almost* exact fits if we allowed a small epsilon,\n        # but for strict exact fit, we assign a high priority.\n        # Let's assign a score that is higher than any non-exact fit score.\n        priorities[exact_fit_mask] = 1000.0  # High priority for exact fits\n    else:\n        # If no exact fit, find bins that can fit the item\n        can_fit_mask = bins_remain_cap >= item\n        if np.any(can_fit_mask):\n            # For bins that can fit, prioritize those with the smallest remaining capacity\n            # after placing the item (i.e., the \"tightest\" fit).\n            # The score is inversely related to the remaining capacity after placement.\n            # A bin with remaining capacity `c` for item `i` will have `c - i` remaining.\n            # We want to minimize `c - i`.\n            # So, higher priority for smaller `c - i`.\n            # Let's use `-(bins_remain_cap[can_fit_mask] - item)` as a priority score.\n            # Higher values of this (less negative) are better.\n            priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # For any bins that cannot fit the item, their priority remains 0.\n    return priorities",
    "response_id": 14,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response16.txt_stdout.txt",
    "code_path": "problem_iter1_code16.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score prioritizes bins that are a \"good fit\" for the item.\n    A good fit is defined as a bin where the remaining capacity is slightly larger\n    than the item size. This strategy aims to leave larger remaining capacities\n    in bins that can accommodate more items, while using bins that are nearly full\n    for items that fill them up, thus reducing fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the difference between remaining capacity and item size.\n    # We only consider bins where the item can actually fit.\n    fit_differences = bins_remain_cap - item\n\n    # For bins where the item doesn't fit, set the difference to a very small\n    # negative number to ensure they get a low priority.\n    # A very large positive difference would also lead to low priority if not for the sigmoid.\n    # However, the sigmoid will compress large positive differences.\n    # For \"no fit\" bins, we want them to be clearly worse than any \"fit\" bin.\n    # We can achieve this by ensuring their sigmoid output is very low.\n    # A very large negative input to sigmoid results in a value close to 0.\n    # Let's set a threshold that's smaller than any possible valid fit_difference.\n    # The smallest possible fit_difference for a fitting bin could be very close to 0.\n    # So, a large negative number will ensure no-fit bins are penalized.\n    no_fit_penalty = -1e9\n    valid_fits_mask = fit_differences >= 0\n    fit_differences[~valid_fits_mask] = no_fit_penalty\n\n    # Apply the sigmoid function. The sigmoid function maps any real-valued number\n    # into a value between 0 and 1.\n    # f(x) = 1 / (1 + exp(-k * (x - x0)))\n    # Here, x is fit_difference.\n    # k: Steepness parameter. A larger k makes the transition steeper.\n    # x0: Midpoint of the sigmoid. This is the ideal difference for a \"perfect\" fit.\n    # We want bins where the remaining capacity is *just* enough for the item.\n    # So, the ideal difference is 0 (remaining_cap == item_size).\n    # Let's choose a steepness (k) and a midpoint (x0).\n    # A smaller midpoint (x0) means we prefer bins that are closer to being full.\n    # A larger midpoint (x0) means we are more tolerant of larger remaining capacities.\n\n    # Strategy: Prefer bins where `remaining_capacity - item_size` is small and non-negative.\n    # This means the item fits snugly.\n    # A sigmoid function with a midpoint around 0 and a steep slope will achieve this.\n    # A positive `fit_difference` means the bin has more space than needed.\n    # We want the score to be higher for smaller positive `fit_difference`.\n    # The sigmoid function typically increases. So, we need to transform `fit_difference`\n    # such that smaller positive values yield higher scores.\n    # This can be done by passing a negative value to sigmoid, e.g., sigmoid(-x).\n    # Or by shifting the sigmoid function's midpoint.\n\n    # Let's use the form: sigmoid(k * (x0 - x))\n    # where x is `fit_difference`.\n    # x0 = 0: We prefer perfect fits.\n    # k > 0: Steepness.\n\n    # Example parameters:\n    k = 5.0  # Steepness. Higher values mean the transition is sharper around x0.\n    x0 = 0.0 # Midpoint. We want the \"peak\" of the priority score to be when fit_difference is 0.\n\n    # Calculate priorities using the shifted sigmoid function.\n    # We want higher scores for smaller, non-negative fit_differences.\n    # The standard sigmoid (1 / (1 + exp(-z))) increases as z increases.\n    # So, we can pass -(fit_difference) or (x0 - fit_difference) to the sigmoid.\n    # Let's use `x0 - fit_difference` for conceptual clarity that we want to be close to `x0`.\n    # When fit_difference is 0, argument is x0.\n    # When fit_difference is large positive, argument is negative and large.\n    # When fit_difference is large negative (no fit), argument is large positive.\n    # This means we need to be careful with the 'no fit' case and the sigmoid behavior.\n\n    # Let's reconsider:\n    # We want the priority to be high when `fit_difference` is small and positive.\n    # Consider `sigmoid(A - B * x)` where x is `fit_difference`.\n    # If B > 0, `sigmoid(A - B*x)` increases as x decreases.\n    # We want a high priority for small x. So, we should use `B > 0`.\n    # Let's set `B=k`.\n    # For the peak, we want `A - B*x` to be near 0. So, `A = B*x`.\n    # If we want the peak at `fit_difference = 0`, then `A=0`.\n    # This gives `sigmoid(-k * fit_difference)`.\n    # If fit_difference = 0, sigmoid(0) = 0.5.\n    # If fit_difference = positive small, sigmoid(-k * positive) < 0.5.\n    # If fit_difference = negative small (no fit), sigmoid(-k * negative) > 0.5.\n    # This is the opposite of what we want.\n\n    # Let's try `sigmoid(A + B * x)`. For it to be high when x is small positive,\n    # the argument `A + B*x` should be large positive.\n    # If `B > 0`, then `A` needs to be large positive to compensate for any positive x.\n    # If `B < 0`, then `A` needs to be large positive to compensate for negative x.\n\n    # The most intuitive interpretation of \"Sigmoid Fit Score\" in BPP usually means\n    # scoring bins where the remaining capacity is close to the item size.\n    # This is often achieved by scoring `(remaining_capacity - item_size)`\n    # and mapping smaller non-negative values to higher scores.\n    # A sigmoid function that decreases as its input increases can be used,\n    # or a standard sigmoid applied to a transformed value.\n\n    # Let's use `sigmoid(a - b * difference)` where `difference = bins_remain_cap - item`.\n    # If `difference` is small positive, we want a high score.\n    # This means `a - b * difference` should be large. This requires `b > 0` and `a` to be set appropriately.\n    # Example: `a` represents a target \"goodness\" score, `b` the sensitivity.\n    #\n    # Let's try a simpler, common approach:\n    # Prioritize bins where `remaining_capacity` is just enough for the item.\n    # This means `bins_remain_cap - item` is small and non-negative.\n    #\n    # Consider the function `f(diff) = exp(-k * diff)` for `diff >= 0`. This decreases.\n    # For `diff < 0`, we assign 0.\n    # This is not a sigmoid.\n\n    # Let's map `fit_difference` to a score:\n    # If `fit_difference` is negative (item doesn't fit), score = 0.\n    # If `fit_difference` is 0, score = 1 (perfect fit).\n    # If `fit_difference` is small positive, score = high (close to 1).\n    # If `fit_difference` is large positive, score = low (close to 0).\n    #\n    # This behavior is characteristic of `sigmoid(large_positive_number - k * fit_difference)`\n    # or `sigmoid(a - k * fit_difference)` where `a` is sufficiently large and `k > 0`.\n    #\n    # Let's define `x = fit_difference`.\n    # We want a function `score(x)` such that:\n    # score(x) = 0 if x < 0\n    # score(x) approaches 1 as x approaches 0 from positive side.\n    # score(x) approaches 0 as x becomes large positive.\n    #\n    # This is like `sigmoid(-k * x)` but with the x<0 case handled.\n    # Let's use `sigmoid(a - k * x)` for all values.\n    # If we set `k > 0` and choose `a` appropriately, we can achieve the desired behavior.\n    #\n    # Let's test the function: `sigmoid(a - k * x)`\n    # - If `x < 0` (item doesn't fit): `a - k*x` will be `a + k*|x|`. If `a` is large and `k>0`, this argument can be large positive, leading to score close to 1. This is incorrect; no-fit bins should have low scores.\n    #\n    # To handle the \"no fit\" case: explicitly set their scores to 0.\n    # For bins where the item fits (`fit_difference >= 0`):\n    # We want scores to be high for small `fit_difference` and low for large `fit_difference`.\n    # Use `sigmoid(-k * fit_difference)`.\n    #\n    # If `fit_difference = 0`, sigmoid(0) = 0.5\n    # If `fit_difference` is small positive (e.g., 0.1): sigmoid(-k * 0.1) < 0.5\n    # If `fit_difference` is large positive (e.g., 1.0): sigmoid(-k * 1.0) << 0.5\n    # This is the opposite of what we want: higher scores for smaller positive differences.\n    #\n    # So, the argument should be `sigmoid(k * (x0 - x))`.\n    # `x = fit_difference`.\n    # `x0` is the ideal difference. Let `x0 = 0` for a perfect fit.\n    # `k` is steepness.\n    #\n    # `sigmoid(k * (0 - fit_difference)) = sigmoid(-k * fit_difference)`\n    # Still leads to the opposite behavior.\n\n    # Let's use the score as `sigmoid(positive_slope * (target - current_value))`.\n    # `current_value` is `bins_remain_cap`. `target` is `item`.\n    # We want higher score when `bins_remain_cap` is close to `item`.\n    # If `bins_remain_cap < item`, then `target - current_value > 0`.\n    # If `bins_remain_cap > item`, then `target - current_value < 0`.\n    #\n    # If `bins_remain_cap` is slightly larger than `item`: `target - current_value` is small negative.\n    # If `bins_remain_cap` is much larger than `item`: `target - current_value` is large negative.\n    #\n    # Let `arg = a + b * (item - bins_remain_cap)`.\n    # If item fits, `item - bins_remain_cap <= 0`.\n    # We want high score when `item - bins_remain_cap` is close to 0 (from negative side).\n    # This means `a + b * (near_zero_negative)` should be large.\n    # If `b > 0`, then `a` needs to be large to make the argument large.\n    #\n    # Let's try:\n    # `score = sigmoid(k * (item - bins_remain_cap))`\n    # For valid fits (`bins_remain_cap >= item`):\n    # `item - bins_remain_cap <= 0`\n    # If `k > 0`:\n    #   - `bins_remain_cap` slightly > `item` => `item - bins_remain_cap` = small negative => `sigmoid(k * small_negative)` = value slightly < 0.5.\n    #   - `bins_remain_cap` == `item` => `item - bins_remain_cap` = 0 => `sigmoid(0)` = 0.5.\n    #   - `bins_remain_cap` much > `item` => `item - bins_remain_cap` = large negative => `sigmoid(k * large_negative)` = value close to 0.\n    # This gives higher priority to bins that are closer to full.\n\n    # To prioritize bins that are 'good fits' (remaining capacity is close to item size,\n    # and the item actually fits), we need a function that peaks when `remaining_capacity - item`\n    # is small and non-negative.\n    #\n    # The function `sigmoid(a - k*x)` where `x = remaining_capacity - item`\n    # with `k > 0` and `a` chosen well, can work if we manage the 'no fit' case.\n    #\n    # Let `x = bins_remain_cap - item`.\n    # We want high scores when `x` is close to 0 and `x >= 0`.\n    #\n    # Let's consider `f(x) = sigmoid(slope * (peak_x - x))`.\n    # If `peak_x = 0` and `slope > 0`: `f(x) = sigmoid(-slope * x)`.\n    # For `x >= 0`, this function decreases from 0.5.\n    # To get higher scores for smaller `x`, we want the argument to be larger.\n    #\n    # Try `sigmoid(k * (central_point - (bins_remain_cap - item)))`\n    # Let `central_point` be the ideal gap, typically 0.\n    # `arg = k * (0 - (bins_remain_cap - item)) = k * (item - bins_remain_cap)`\n    #\n    # For `bins_remain_cap >= item`:\n    # `item - bins_remain_cap` is 0 or negative.\n    # If `k > 0`:\n    #   - `item - bins_remain_cap = 0` (perfect fit) => `sigmoid(0) = 0.5`\n    #   - `item - bins_remain_cap = small negative` => `sigmoid(k * small_negative)` < 0.5\n    #   - `item - bins_remain_cap = large negative` => `sigmoid(k * large_negative)` -> 0\n    #\n    # This prioritizes bins that are *closer* to being full, but not perfectly full.\n    # A small remaining gap is penalized.\n\n    # A better approach for \"good fit\" could be to penalize bins that are \"too empty\"\n    # or \"too full\" (just fitting).\n    #\n    # For the Sigmoid Fit Score, the common interpretation is to assign a high score\n    # to bins whose remaining capacity `R` is such that `R - item_size` is small and non-negative.\n    #\n    # This can be modeled by a sigmoid function where the input is decreasing as `R - item_size` increases.\n    # So, `sigmoid(a - b * (R - item_size))` where `a > 0, b > 0`.\n    #\n    # Let's use `k` as the steepness and `x0` as the optimal gap (`R - item_size`).\n    # We want optimal gap `x0 = 0`.\n    # The function is `sigmoid(k * (x0 - (bins_remain_cap - item)))`\n    # = `sigmoid(k * (item - bins_remain_cap))`\n    #\n    # This function's output for `bins_remain_cap >= item` decreases from 0.5.\n    # The peak priority for a fitting bin is 0.5, achieved at `bins_remain_cap = item`.\n    # For bins where `bins_remain_cap > item`, the priority is < 0.5.\n    #\n    # To ensure bins that are *just* a fit get highest priority, we can shift the sigmoid.\n    # Or use a formulation that peaks.\n    #\n    # Consider `sigmoid(k * x)` where x is adjusted.\n    #\n    # A common Sigmoid Fit strategy in literature is:\n    # `Score = 1 / (1 + exp(-k * (ideal_capacity - current_remaining_capacity)))`\n    # Here, `ideal_capacity` is the capacity of the bin if it were to be perfectly filled\n    # with the current item. So, `ideal_capacity = item`.\n    # `current_remaining_capacity` is `bins_remain_cap`.\n    #\n    # `Score = sigmoid(k * (item - bins_remain_cap))`\n    #\n    # For valid fits (`bins_remain_cap >= item`):\n    # `item - bins_remain_cap` is `0` or negative.\n    # If `k > 0`:\n    #   - `bins_remain_cap == item`: `item - bins_remain_cap = 0`. Score = `sigmoid(0) = 0.5`.\n    #   - `bins_remain_cap = item + epsilon` (small positive gap): `item - bins_remain_cap = -epsilon`. Score = `sigmoid(-k*epsilon)` < 0.5.\n    #   - `bins_remain_cap = item + large_delta`: `item - bins_remain_cap = -large_delta`. Score = `sigmoid(-k*large_delta)` -> 0.\n    #\n    # This means bins that are closest to being full (smallest positive remaining capacity after fitting the item) get lower scores.\n    # This seems counter-intuitive for \"good fit\".\n    #\n    # Let's redefine \"good fit\" as a bin that can accommodate the item without too much excess space.\n    # This means `bins_remain_cap - item` should be small and non-negative.\n    #\n    # We need a score that:\n    # 1. Is 0 if `bins_remain_cap < item`.\n    # 2. Peaks when `bins_remain_cap = item`.\n    # 3. Decreases as `bins_remain_cap` increases beyond `item`.\n    #\n    # Let's use `sigmoid(k * (target - value))`.\n    # `target` = `item`. `value` = `bins_remain_cap`.\n    # `sigmoid(k * (item - bins_remain_cap))`.\n    #\n    # We want the argument to be large when `item - bins_remain_cap` is large (meaning `bins_remain_cap` is much smaller than `item`).\n    #\n    # Alternative: Focus on the *slack*. Slack = `bins_remain_cap - item`.\n    # We want small slack.\n    #\n    # Let's use `sigmoid(a - k * slack)` with `a` large, `k > 0`.\n    # `slack = bins_remain_cap - item`.\n    #\n    # Consider the function `sigmoid(a - k * (bins_remain_cap - item))`.\n    # If `bins_remain_cap < item` (no fit):\n    #   `bins_remain_cap - item` is negative.\n    #   `a - k * (negative)` = `a + k * abs(negative)`. If `a` is large, this is large positive.\n    #   So, score is close to 1. This is WRONG for no-fit bins.\n    #\n    # Let's enforce the \"no fit\" rule first.\n    # For bins where `bins_remain_cap < item`, the priority is 0.\n    # For bins where `bins_remain_cap >= item`:\n    # We want to maximize `score` when `bins_remain_cap - item` is small.\n    #\n    # Use `sigmoid(k * (ideal_fit - current_fit))`.\n    # `current_fit_diff = bins_remain_cap - item`.\n    # `ideal_fit_diff = 0`.\n    # So, `sigmoid(k * (0 - (bins_remain_cap - item))) = sigmoid(k * (item - bins_remain_cap))`.\n    #\n    # We saw this decreases for positive `bins_remain_cap - item`.\n    #\n    # To achieve higher score for smaller positive `bins_remain_cap - item`:\n    # We need the argument to `sigmoid` to be smaller when `bins_remain_cap - item` is larger.\n    # So, `sigmoid(k * (some_constant - (bins_remain_cap - item)))`.\n    # The constant is the `x0` where `k * (x0 - diff)` is 0.\n    #\n    # Let's set the \"ideal\" scenario as `bins_remain_cap = item`.\n    # `bins_remain_cap - item = 0`.\n    # We want a high score then.\n    #\n    # Consider `sigmoid(a + b * x)` where `x = bins_remain_cap`.\n    # We want score to be high when `bins_remain_cap` is `item`.\n    #\n    # The \"Sigmoid Fit\" heuristic typically implies that the priority is determined by how well an item fits into a bin, often favoring bins that are close to being full but still accommodate the item.\n    #\n    # Let's use the interpretation that the priority is proportional to `sigmoid(k * (b - item))`, where `b` is bin remaining capacity.\n    # We want to penalize bins where `b` is too large or too small.\n    #\n    # A common sigmoid fit aims to select bins where `b` is 'close' to `item`.\n    # Specifically, `b - item` is small and non-negative.\n    #\n    # A possible formulation for this \"closeness\":\n    #\n    # `priority = sigmoid(k * (center - (bins_remain_cap - item)))`\n    #\n    # where `center` is the ideal gap (e.g., 0) and `k` is steepness.\n    #\n    # If `center = 0` and `k > 0`:\n    # `priority = sigmoid(k * (0 - (bins_remain_cap - item)))`\n    # `priority = sigmoid(k * (item - bins_remain_cap))`\n    #\n    # For valid fits (`bins_remain_cap >= item`):\n    # `item - bins_remain_cap <= 0`\n    #   - `bins_remain_cap = item` => `item - bins_remain_cap = 0`. Score = `sigmoid(0) = 0.5`.\n    #   - `bins_remain_cap = item + epsilon` => `item - bins_remain_cap = -epsilon`. Score = `sigmoid(-k*epsilon)` < 0.5.\n    #   - `bins_remain_cap = item + large_delta` => `item - bins_remain_cap = -large_delta`. Score = `sigmoid(-k*large_delta)` -> 0.\n    #\n    # This gives higher priority to bins that are closer to being *completely full* after the item is placed. This is a valid strategy (trying to leave larger capacities).\n\n    # Let's define the sigmoid function.\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    # Parameters for the sigmoid.\n    # `k`: Steepness. Higher values mean the priority changes more rapidly around the midpoint.\n    # `x0`: The point where the sigmoid is 0.5. We want the \"ideal fit\" to be prioritized.\n    #\n    # The priority for a bin with remaining capacity `R` and item size `I` is usually designed to be high when `R` is close to `I` and `R >= I`.\n    #\n    # Consider the quantity `gap = R - I`. We want small, non-negative gaps to have high priority.\n    # A sigmoid function of the form `sigmoid(A - B * gap)` with `B > 0` will decrease as `gap` increases.\n    # To make it peak, we can introduce a bias or adjust the formulation.\n    #\n    # Let's re-evaluate the common \"Sigmoid Fit\" formulation:\n    # Priority(bin, item) = sigmoid(k * (capacity(bin) - size(item)))\n    #\n    # Using `bins_remain_cap` for `capacity(bin)` and `item` for `size(item)`.\n    # `priorities = sigmoid(k * (bins_remain_cap - item))`\n    #\n    # Let's analyze `sigmoid(k * (bins_remain_cap - item))`:\n    # If `k > 0`:\n    #   - `bins_remain_cap = item`: `arg = 0`. `sigmoid(0) = 0.5`.\n    #   - `bins_remain_cap = item + epsilon` (small positive gap): `arg = k*epsilon`. `sigmoid(k*epsilon)` > 0.5. Higher priority.\n    #   - `bins_remain_cap = item + large_delta`: `arg = k*large_delta`. `sigmoid(k*large_delta)` -> 1. Highest priority.\n    #   - `bins_remain_cap < item`: `arg` is negative and large. `sigmoid(negative)` -> 0. Lowest priority.\n    #\n    # This formulation prioritizes bins that have *ample* space remaining. This is the \"Worst Fit\" idea applied via sigmoid.\n    #\n    # The goal is usually to find a \"good fit\", meaning the remaining capacity is *just enough*.\n    # So we want `bins_remain_cap - item` to be small and non-negative.\n    #\n    # To achieve this, we need a function that decreases as `bins_remain_cap - item` increases.\n    # Let's use `sigmoid(k * (ideal - actual))`.\n    # `ideal = item`. `actual = bins_remain_cap`.\n    # `priority = sigmoid(k * (item - bins_remain_cap))`.\n    #\n    # With `k > 0`:\n    #   - `bins_remain_cap = item`: `arg = 0`. `sigmoid(0) = 0.5`.\n    #   - `bins_remain_cap = item + epsilon` (small positive gap): `arg = -k*epsilon`. `sigmoid(-k*epsilon)` < 0.5. Lower priority.\n    #   - `bins_remain_cap = item + large_delta`: `arg = -k*large_delta`. `sigmoid(-k*large_delta)` -> 0. Lowest priority.\n    #   - `bins_remain_cap < item`: `arg` is positive and large. `sigmoid(positive)` -> 1. Highest priority for bins that don't fit? This is wrong.\n\n    # Let's try a sigmoid centered on the \"good fit\" condition.\n    # We want `bins_remain_cap` to be `item`.\n    # Let's define `diff = bins_remain_cap - item`. We want small `diff >= 0`.\n    #\n    # Consider `sigmoid(a - k * diff)`.\n    # If `diff` is small positive, we want a high score. This means `a - k*diff` should be large. Requires `k > 0` and `a` to be sufficiently large to counter positive `diff`.\n    # If `diff` is large positive, we want a low score. This means `a - k*diff` should be small.\n    #\n    # For the \"no fit\" case (`diff < 0`), we need the score to be 0.\n    #\n    # Let's refine the parameters for `sigmoid(a - k * (bins_remain_cap - item))`.\n    #\n    # We need `a - k * (bins_remain_cap - item)` to be large for small positive `bins_remain_cap - item`.\n    # Let's set `a` to offset the negative impact of `bins_remain_cap - item`.\n    #\n    # Consider the behavior:\n    # 1. Bins where `bins_remain_cap < item`: priority should be 0.\n    # 2. Bins where `bins_remain_cap == item`: priority should be high (e.g., 1).\n    # 3. Bins where `bins_remain_cap = item + epsilon` (small excess capacity): priority should be high but less than 1 (e.g., 0.75).\n    # 4. Bins where `bins_remain_cap = item + large_delta` (large excess capacity): priority should be low (e.g., 0.1).\n    #\n    # Let's use the form: `sigmoid(k * (x_center - x_value))`.\n    # We want to prioritize when `bins_remain_cap` is close to `item`.\n    # Let `x_value = bins_remain_cap`.\n    # Let `x_center = item`.\n    # `sigmoid(k * (item - bins_remain_cap))`\n    #\n    # For valid fits (`bins_remain_cap >= item`):\n    # `item - bins_remain_cap` is non-positive.\n    #\n    # To get high scores for small non-positive values of `item - bins_remain_cap`:\n    # We need `k * (item - bins_remain_cap)` to be large positive.\n    # This implies `k < 0` if `item - bins_remain_cap` is negative.\n    #\n    # Let's reverse the formulation to use positive exponents for sigmoid argument to get higher scores for small positive differences.\n    #\n    # `priority = sigmoid(k * (bins_remain_cap - item))` where `k` is chosen such that higher `bins_remain_cap` values are preferred IF they fit.\n    #\n    # This is effectively the \"Worst Fit\" strategy transformed into a sigmoid.\n    # If the aim is \"Best Fit\" using sigmoid:\n    # We want bins where `bins_remain_cap - item` is small and non-negative.\n    #\n    # Let `delta = bins_remain_cap - item`.\n    # If `delta < 0`, priority = 0.\n    # If `delta >= 0`:\n    #   We want priority to be high for small `delta`.\n    #   Consider `f(delta) = exp(-k * delta)` for `delta >= 0`. This decreases.\n    #   Transforming to sigmoid:\n    #   `sigmoid(A - B * delta)`.\n    #   If `B > 0`, this decreases. We need `A` large.\n    #   Let `A = k * X_ideal`, where `X_ideal` is the target `delta`. We want `X_ideal = 0`.\n    #   So, `sigmoid(k * (0 - delta)) = sigmoid(-k * delta)`.\n    #   For `delta >= 0`, this gives values less than 0.5, decreasing towards 0.\n    #   This means bins that are closer to being full after placement get lower priority.\n    #   This is again \"Worst Fit\" kind of logic.\n    #\n    # The problem statement implies prioritizing \"good fits\".\n    # \"A good fit is defined as a bin where the remaining capacity is slightly larger than the item size.\"\n    # This means `bins_remain_cap` should be slightly larger than `item`.\n    # So, `bins_remain_cap - item = epsilon > 0` (small positive).\n    #\n    # Let `delta = bins_remain_cap - item`.\n    # We want high scores when `delta` is small and positive.\n    #\n    # Use `sigmoid(k * (target_delta - actual_delta))`.\n    # `target_delta = epsilon_small` (e.g., 0.1).\n    # `actual_delta = bins_remain_cap - item`.\n    #\n    # `priority = sigmoid(k * (epsilon_small - (bins_remain_cap - item)))`\n    #\n    # For valid fits (`bins_remain_cap >= item`):\n    #   `epsilon_small - (bins_remain_cap - item)`\n    #   - If `bins_remain_cap = item + epsilon_small`: arg = 0. sigmoid(0) = 0.5. Peak priority.\n    #   - If `bins_remain_cap = item + epsilon_small + small_positive`: arg = negative small. sigmoid(negative small) < 0.5. Lower priority.\n    #   - If `bins_remain_cap = item + epsilon_small + large_positive`: arg = negative large. sigmoid(negative large) -> 0. Lowest priority.\n    #   - If `bins_remain_cap = item - epsilon_small_negative` (effectively `bins_remain_cap < item`):\n    #     `epsilon_small - (-epsilon_small_negative) = epsilon_small + epsilon_small_negative`.\n    #     If `epsilon_small_negative` is such that `bins_remain_cap < item` then `bins_remain_cap - item < 0`.\n    #     So `epsilon_small - (bins_remain_cap - item)` will be `epsilon_small - (negative number)`, which is `epsilon_small + positive_number`, hence large positive.\n    #     `sigmoid(large_positive)` -> 1. This gives high priority to bins that don't fit. This is still problematic.\n\n    # Let's stick to the core \"Sigmoid Fit Score\" idea as commonly implemented, which focuses on\n    # prioritizing bins based on the relationship between remaining capacity and item size.\n    # A typical strategy is to score based on the 'fit difference'.\n    #\n    # Let `fit_diff = bins_remain_cap - item`.\n    # We are interested in the case where `fit_diff >= 0`.\n    # We want smaller `fit_diff` to result in higher scores.\n    #\n    # Let's use `sigmoid(k * (X - Y))` where `Y` is the 'actual' value and `X` is the 'ideal' value.\n    # Our 'ideal' for `bins_remain_cap` would be `item`.\n    #\n    # Consider `sigmoid(k * (item - bins_remain_cap))`.\n    # `k > 0`.\n    # - `bins_remain_cap < item`: `item - bins_remain_cap > 0`. Score -> 1. (Problematic)\n    # - `bins_remain_cap == item`: `item - bins_remain_cap = 0`. Score = 0.5.\n    # - `bins_remain_cap > item`: `item - bins_remain_cap < 0`. Score < 0.5 (decreases towards 0).\n    #\n    # This formulation gives high scores to bins that don't fit and low scores to bins with excess capacity.\n\n    # A standard \"Sigmoid Fit\" heuristic often seeks bins where `bins_remain_cap` is *just enough* for the item.\n    # This means prioritizing `bins_remain_cap` values that are slightly larger than `item`.\n    #\n    # Let's use a formulation that ensures no-fit bins get zero priority.\n    # For bins that fit (`bins_remain_cap >= item`):\n    #   Calculate `excess_capacity = bins_remain_cap - item`.\n    #   We want high priority for small `excess_capacity`.\n    #   This behavior is captured by a decreasing sigmoid.\n    #   `priority = sigmoid(k * (target_excess - actual_excess))`\n    #   Where `target_excess` is the ideal excess, ideally 0 or a small positive value.\n    #   Let's pick `target_excess = 0`.\n    #   `priority = sigmoid(k * (0 - (bins_remain_cap - item)))`\n    #   `priority = sigmoid(k * (item - bins_remain_cap))`\n    #\n    # To avoid issues with `bins_remain_cap < item`, we explicitly set their priority to 0.\n    #\n    # Let's set parameters:\n    # `k`: steepness. A value around 3-10 is common.\n    # `center`: where the priority is 0.5. For \"Sigmoid Fit\", we want this to be where `bins_remain_cap` is close to `item`.\n    #\n    # The formulation that prioritizes bins where `bins_remain_cap` is slightly larger than `item` implies that `bins_remain_cap - item` should be small and positive.\n    #\n    # Let `x = bins_remain_cap - item`.\n    # We want a score that is high for small `x >= 0`.\n    # This can be modeled by `sigmoid(a - k * x)` with `a` large and `k > 0`.\n    #\n    # Let `k = 5.0`. Let `a = k * ideal_excess`. We want `ideal_excess` to be a small positive number, say 0.05.\n    # So `a = 5.0 * 0.05 = 0.25`.\n    # `priority = sigmoid(0.25 - 5.0 * (bins_remain_cap - item))`\n    #\n    # Let's check:\n    # `bins_remain_cap = item + 0.05` (ideal fit):\n    #   `arg = 0.25 - 5.0 * (0.05) = 0.25 - 0.25 = 0`. `sigmoid(0) = 0.5`. Peak.\n    # `bins_remain_cap = item + 0.05 + 0.01` (slightly more excess):\n    #   `arg = 0.25 - 5.0 * (0.06) = 0.25 - 0.30 = -0.05`. `sigmoid(-0.05) < 0.5`. Lower.\n    # `bins_remain_cap = item + 0.05 - 0.01` (slightly less excess):\n    #   `arg = 0.25 - 5.0 * (0.04) = 0.25 - 0.20 = 0.05`. `sigmoid(0.05) > 0.5`. Higher.\n    # `bins_remain_cap = item + 0.05 + 0.5` (much more excess):\n    #   `arg = 0.25 - 5.0 * (0.55) = 0.25 - 2.75 = -2.5`. `sigmoid(-2.5)` -> 0. Low.\n    #\n    # This prioritizes bins where the remaining capacity is slightly larger than the item.\n    #\n    # What about the `bins_remain_cap < item` case?\n    # `bins_remain_cap = item - 0.05` (doesn't fit)\n    #   `arg = 0.25 - 5.0 * (-0.05) = 0.25 + 0.25 = 0.5`. `sigmoid(0.5) > 0.5`.\n    #\n    # To enforce zero priority for non-fitting bins:\n    # Mask the non-fitting bins.\n\n    # Parameters for the sigmoid function.\n    # `k_steepness`: Controls how quickly the priority score changes around the 'ideal fit'.\n    # `ideal_gap`: The preferred difference between remaining capacity and item size.\n    #              A small positive value here means we prefer bins that are 'almost full'.\n    k_steepness = 5.0\n    ideal_gap = 0.05  # Prefer bins with a remaining capacity that is slightly larger than the item size.\n\n    # Calculate the 'argument' for the sigmoid function.\n    # The core idea is `sigmoid(k * (ideal_value - actual_value))`.\n    # Here, the 'value' we care about is `bins_remain_cap - item`.\n    # The 'ideal value' for this difference is `ideal_gap`.\n    #\n    # So, the argument is `k_steepness * (ideal_gap - (bins_remain_cap - item))`.\n    # This simplifies to `k_steepness * (ideal_gap + item - bins_remain_cap)`.\n\n    # Initialize priorities to zero (for bins where the item doesn't fit).\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins where the item can actually fit.\n    fits_mask = bins_remain_cap >= item\n\n    # For bins that fit, calculate the priority score using the sigmoid function.\n    # The argument to sigmoid is designed such that:\n    # - When `bins_remain_cap - item == ideal_gap`, the argument is 0, sigmoid(0) = 0.5 (peak priority).\n    # - When `bins_remain_cap - item` is larger than `ideal_gap`, the argument is negative, leading to scores < 0.5.\n    # - When `bins_remain_cap - item` is smaller than `ideal_gap` (but still >= 0), the argument is positive, leading to scores > 0.5.\n    #   This part might be counter-intuitive if we strictly want \"slightly larger\" to be peak.\n    #\n    # Let's reconsider the function form to match the definition:\n    # \"A good fit is defined as a bin where the remaining capacity is slightly larger than the item size.\"\n    # This means we want to maximize score when `bins_remain_cap - item` is small and positive.\n    #\n    # The function `sigmoid(a - k*x)` where `x = bins_remain_cap - item` with `k>0` works.\n    # The argument `a - k*x` should be large for small positive `x`.\n    #\n    # Let the 'peak' occur when `bins_remain_cap - item = ideal_gap`.\n    # Argument at peak: `a - k * ideal_gap`. This should correspond to the 'center' of the sigmoid for peak value.\n    # Let's set `a - k * ideal_gap = 0`. So `a = k * ideal_gap`.\n    #\n    # The formula becomes: `sigmoid(k * ideal_gap - k * (bins_remain_cap - item))`\n    # `sigmoid(k * (ideal_gap - (bins_remain_cap - item)))`\n    # This is what we had before.\n\n    # Let's use the 'a' value to shift the sigmoid curve so the peak (0.5 score) is at `ideal_gap`.\n    # We'll calculate `argument = k_steepness * (ideal_gap - (bins_remain_cap - item))` only for fitting bins.\n    #\n    # `bins_remain_cap` in `fits_mask` array.\n    # `excess_capacities = bins_remain_cap[fits_mask] - item`\n\n    # The argument: `k_steepness * (ideal_gap - excess_capacities)`\n    # Let's shift it so that at `excess_capacities == ideal_gap`, the argument is 0.\n    # This is already the case with `ideal_gap` being the `ideal_value`.\n\n    # A more robust way to achieve peaking at `ideal_gap`:\n    # Map `excess_capacities` such that `ideal_gap` maps to 0.\n    # `mapped_value = excess_capacities - ideal_gap`.\n    # Then apply `sigmoid(-k * mapped_value)` to get higher scores for smaller `mapped_value`.\n    # `sigmoid(-k * (excess_capacities - ideal_gap))`\n    # `= sigmoid(k * (ideal_gap - excess_capacities))`\n    # This is indeed the same form.\n\n    # Apply the sigmoid to the calculated arguments for the fitting bins.\n    argument_for_sigmoid = k_steepness * (ideal_gap - (bins_remain_cap[fits_mask] - item))\n    priorities[fits_mask] = sigmoid(argument_for_sigmoid)\n\n    # Ensure priorities are within [0, 1] range, though sigmoid inherently does this.\n    # priorities = np.clip(priorities, 0, 1) # Not strictly necessary with sigmoid.\n\n    return priorities",
    "response_id": 16,
    "obj": 4.487435181491823,
    "SLOC": 16.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response18.txt_stdout.txt",
    "code_path": "problem_iter1_code18.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Exploration rate\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Determine which bins can potentially fit the item\n    valid_bins_indices = np.where(bins_remain_cap >= item)[0]\n\n    if len(valid_bins_indices) == 0:\n        # No bin can fit the item, return all zeros (or handle appropriately)\n        return np.zeros(num_bins)\n\n    # Greedy part: prioritize bins that leave the least remaining capacity\n    # This is the \"best fit\" heuristic within the greedy selection.\n    greedy_scores = bins_remain_cap[valid_bins_indices] - item\n\n    # Apply epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: randomly choose one of the valid bins\n        chosen_bin_index_in_valid = np.random.randint(0, len(valid_bins_indices))\n        chosen_bin_global_index = valid_bins_indices[chosen_bin_index_in_valid]\n        priorities[chosen_bin_global_index] = 1.0  # Assign highest priority to the chosen bin\n    else:\n        # Exploitation: choose the bin that minimizes remaining capacity (best fit)\n        best_fit_index_in_valid = np.argmin(greedy_scores)\n        best_fit_global_index = valid_bins_indices[best_fit_index_in_valid]\n        priorities[best_fit_global_index] = 1.0 # Assign highest priority to the best fit bin\n\n    # For bins that *can* fit the item but are not chosen by epsilon-greedy,\n    # we can assign a small non-zero priority to still allow them a chance,\n    # or simply keep them at zero if we only want one bin to have the highest score.\n    # For simplicity here, we only give priority to the chosen bin.\n\n    return priorities",
    "response_id": 18,
    "obj": 4.148384523334677,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response29.txt_stdout.txt",
    "code_path": "problem_iter1_code29.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    The Softmax-Based Fit strategy assigns higher priorities to bins that can\n    accommodate the item and are \"closer\" to being full after the item is packed.\n    This encourages tighter packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to negative infinity for bins that cannot fit the item\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can accommodate the item\n    fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate a score.\n    # The score is based on the remaining capacity *after* the item is packed.\n    # We want to prioritize bins where remaining_capacity - item is small (tighter fit).\n    # A simple approach is to use the negative of the remaining capacity after packing.\n    # Softmax will then turn these scores into probabilities.\n    if np.any(fit_mask):\n        remaining_after_fit = bins_remain_cap[fit_mask] - item\n        # We want smaller remaining capacity to have higher priority.\n        # So, we use the negative of the remaining capacity.\n        # Adding a small epsilon to avoid log(0) issues if remaining capacity becomes exactly 0\n        # or for numerical stability if values are very close to zero.\n        priorities[fit_mask] = -remaining_after_fit + 1e-9\n\n    # Apply softmax to convert scores into probabilities (priorities)\n    # Ensure the exponentiation is stable by subtracting the maximum score\n    # before applying exp, which doesn't change the relative probabilities.\n    if np.any(priorities > -np.inf):\n        max_priority = np.max(priorities[priorities > -np.inf])\n        exp_priorities = np.exp(priorities - max_priority)\n        # Normalize to get probabilities summing to 1 (for the fitting bins)\n        sum_exp_priorities = np.sum(exp_priorities[priorities > -np.inf])\n        if sum_exp_priorities > 0:\n            priorities = exp_priorities / sum_exp_priorities\n        else:\n            # If for some reason sum is zero (e.g., all fitting priorities were -inf),\n            # reset to uniform probability for fitting bins.\n            priorities[priorities > -np.inf] = 1.0 / np.sum(fit_mask)\n    else:\n        # If no bins can fit the item, all priorities remain -inf, or can be set to 0.\n        # For consistency with Softmax, let's keep them as they are.\n        pass\n\n    return priorities",
    "response_id": 29,
    "obj": 4.11846828879138,
    "SLOC": 17.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority for packing an item into bins using an Exact Fit First strategy\n    with Best Fit as a tie-breaker for non-exact fits, and an exploration component.\n\n    The strategy prioritizes bins that perfectly fit the item (Exact Fit).\n    Among bins that do not offer an exact fit but can accommodate the item,\n    it prioritizes those that leave the least remaining capacity (Best Fit).\n    An epsilon-greedy approach is incorporated to allow for exploration of non-optimal bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    epsilon = 0.05  # Exploration rate\n    num_bins = len(bins_remain_cap)\n    priorities = np.full(num_bins, -np.inf, dtype=float)  # Initialize with a very low priority\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    fittable_indices = np.where(can_fit_mask)[0]\n\n    if len(fittable_indices) == 0:\n        return priorities  # No bin can fit the item\n\n    # Calculate remaining capacities for fittable bins\n    remaining_after_fit = bins_remain_cap[fittable_indices] - item\n\n    # Separate exact fits and non-exact fits\n    exact_fit_fittable_indices = fittable_indices[remaining_after_fit == 0]\n    non_exact_fit_fittable_indices = fittable_indices[remaining_after_fit > 0]\n\n    # --- Assign priorities ---\n\n    # 1. Exact Fits: Highest priority.\n    #    Among exact fits, prioritize bins that were initially closer to fitting the item\n    #    (i.e., smaller `bins_remain_cap` before placing the item).\n    #    A large base priority is used to ensure they are favored.\n    base_exact_fit_priority = 1000.0\n    if len(exact_fit_fittable_indices) > 0:\n        exact_fit_initial_caps = bins_remain_cap[exact_fit_fittable_indices]\n        # Score = Base Priority + (Max Initial Capacity among exact fits - Current Initial Capacity)\n        # This assigns higher scores to bins that were smaller but still exact fits.\n        max_initial_cap_exact_fits = np.max(exact_fit_initial_caps)\n        exact_fit_priorities = base_exact_fit_priority + (max_initial_cap_exact_fits - exact_fit_initial_caps)\n        priorities[exact_fit_fittable_indices] = exact_fit_priorities\n\n    # 2. Non-Exact Fits (Best Fit): Lower priority than exact fits.\n    #    Prioritize bins that leave the least remaining capacity after fitting the item.\n    #    The score is derived from `-(remaining_after_fit)` so that smaller remaining capacities\n    #    result in higher (less negative) scores. We shift these scores down to be strictly\n    #    less than the exact fit priorities.\n    offset_for_non_exact_fits = 500.0 # Ensures non-exact fits are lower priority than exact fits\n    if len(non_exact_fit_fittable_indices) > 0:\n        # Score = Offset - Remaining Capacity after fit\n        # Smaller remaining capacity yields a higher score.\n        non_exact_fit_priorities = offset_for_non_exact_fits - remaining_after_fit[remaining_after_fit > 0]\n        priorities[non_exact_fit_fittable_indices] = non_exact_fit_priorities\n\n    # --- Epsilon-Greedy Exploration ---\n    # With probability epsilon, choose a random fittable bin.\n    # Otherwise, choose the bin with the highest calculated priority (exploitation).\n\n    if np.random.rand() < epsilon and len(fittable_indices) > 0:\n        # Exploration: Choose a random bin among those that can fit the item.\n        # Assign it the highest possible priority to ensure it's picked if chosen.\n        exploration_bin_index = np.random.choice(fittable_indices)\n        # Assign a priority that is guaranteed to be the highest if this bin is chosen\n        # A value higher than any possible deterministic priority.\n        highest_priority_for_exploration = np.max(priorities[fittable_indices]) + 1.0\n        priorities = np.full(num_bins, -np.inf, dtype=float) # Reset priorities for exploration\n        priorities[exploration_bin_index] = highest_priority_for_exploration\n    else:\n        # Exploitation: The priorities are already set based on Exact Fit / Best Fit.\n        # If no bin can fit, priorities remain -inf.\n        pass\n\n    return priorities",
    "response_id": 0,
    "obj": 4.098524132429212,
    "SLOC": 29.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response7.txt_stdout.txt",
    "code_path": "problem_iter2_code7.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a combination of Best Fit and a novel 'Worst Fit Reduction' strategy.\n\n    This heuristic aims to:\n    1. Favor bins that result in a tight fit (similar to Best Fit).\n    2. Introduce a secondary preference for bins that, after packing, still have substantial remaining capacity,\n       thereby \"reserving\" tighter-fitting bins for potentially smaller future items. This is a form of\n       \"Worst Fit Reduction\" in that we reduce the \"waste\" by not over-filling bins that could\n       accommodate more.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate scores for bins that can fit the item\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n    fitting_bins_remain_cap = bins_remain_cap[fitting_bins_indices]\n\n    # Score 1: Best Fit component - prioritize bins that leave less remaining capacity\n    # We use the negative of remaining capacity to make larger negative values (smaller remaining capacity) higher.\n    best_fit_scores = -(fitting_bins_remain_cap - item)\n\n    # Score 2: Worst Fit Reduction component - penalize bins that are too \"full\" after packing,\n    # giving a slight preference to bins that have more remaining capacity.\n    # This can be achieved by adding a small factor of the remaining capacity.\n    # We add a small constant to avoid division by zero if remaining capacity is 0.\n    # This term should be smaller than the best_fit_scores to ensure best fit is dominant.\n    # The scale of this term needs careful tuning; a small positive value is used here.\n    worst_fit_reduction_scores = (fitting_bins_remain_cap - item) * 0.1\n\n    # Combine scores. We want to maximize the combined score.\n    # Prioritize bins that are a good fit AND don't become too full.\n    # Higher scores mean higher priority.\n    combined_scores = best_fit_scores + worst_fit_reduction_scores\n\n    # Normalize scores to a probability distribution using softmax.\n    # Subtract max to prevent overflow during exponentiation and ensure numerical stability.\n    max_score = np.max(combined_scores)\n    exp_scores = np.exp(combined_scores - max_score)\n    sum_exp_scores = np.sum(exp_scores)\n\n    if sum_exp_scores > 0:\n        normalized_priorities = exp_scores / sum_exp_scores\n    else:\n        # If all scores are effectively -inf after subtractions, assign uniform probability.\n        normalized_priorities = np.ones_like(combined_scores) / len(combined_scores)\n\n    # Assign the calculated priorities to the original bins array\n    priorities[fitting_bins_indices] = normalized_priorities\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "SLOC": 20.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority score for each bin using a hybrid approach prioritizing good fits and balancing exploration.\n\n    This heuristic prioritizes bins that offer a \"good fit\" for the item, meaning\n    their remaining capacity is slightly larger than the item size. It also\n    incorporates an exploration component to occasionally consider less-full bins,\n    balancing the \"best fit\" tendency with exploration of potential future packing\n    opportunities.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array containing the remaining capacity of each bin.\n\n    Returns:\n        A numpy array of priority scores for each bin. Higher scores indicate a\n        higher preference for placing the item in that bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploration (choosing a bin less greedily)\n    k_steepness = 5.0  # Steepness parameter for the sigmoid function\n    ideal_gap = 0.05  # The preferred remaining capacity beyond the item size\n\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Identify bins where the item can fit\n    fits_mask = bins_remain_cap >= item\n    fitting_bins_indices = np.where(fits_mask)[0]\n\n    # If no bins can fit the item, return all zeros.\n    if len(fitting_bins_indices) == 0:\n        return priorities\n\n    # --- Exploitation Component: Prioritize Good Fits ---\n    # Calculate the \"excess capacity\" for bins where the item fits.\n    # A good fit has a small, positive excess capacity.\n    excess_capacities = bins_remain_cap[fitting_bins_indices] - item\n\n    # Use a sigmoid function to score the \"goodness\" of the fit.\n    # We want the score to peak when `excess_capacities` is close to `ideal_gap`.\n    # The argument for the sigmoid is designed so that:\n    # - When `excess_capacities == ideal_gap`, the argument is 0, sigmoid(0) = 0.5 (peak of the \"good fit\" score).\n    # - When `excess_capacities` deviates from `ideal_gap` (either smaller or larger), the score decreases.\n    # This formulation penalizes bins that are too empty or too full relative to the ideal gap.\n    sigmoid_argument = k_steepness * (ideal_gap - excess_capacities)\n    exploitation_scores = 1 / (1 + np.exp(-sigmoid_argument))\n\n    # Normalize exploitation scores so they sum to 1 among fitting bins.\n    # This helps in combining with exploration probabilities later if needed, or for clearer interpretation.\n    # However, for direct preference scores, normalization might not be strictly required if scaled appropriately.\n    # Let's scale them to be within a reasonable range for prioritization.\n    # We can scale the 0.5 peak to a higher value for better differentiation if needed.\n    # For now, let's use the raw sigmoid output (0 to 1).\n\n    # --- Exploration Component ---\n    # With probability epsilon, we want to explore other options.\n    # We can assign a uniform, lower priority to all fitting bins to represent exploration.\n    # The probability of choosing an exploratory bin is epsilon.\n    # If we explore, we pick one of the fitting bins uniformly.\n    # So, each fitting bin gets an additional 'exploration boost'.\n    exploration_boost = epsilon / len(fitting_bins_indices)\n\n    # --- Combine Exploitation and Exploration ---\n    # The final priority for fitting bins is a weighted sum:\n    # (1 - epsilon) * exploitation_score + epsilon * uniform_exploration_score\n    # Here, the 'uniform_exploration_score' is implicitly handled by adding\n    # the exploration boost.\n\n    # Apply the combined strategy:\n    # For fitting bins: priority = (1-epsilon) * normalized_exploitation_score + epsilon * (uniform score)\n    # A simpler way to think is that the final score is a mix.\n    # Let's consider the scores as probabilities of selection for a simplified view.\n    # We want to assign scores reflecting preference.\n\n    # Let's use the exploitation scores directly, but add an exploration bonus.\n    # The exploration bonus makes less-preferred bins (by exploitation score) more competitive.\n    # We add a constant exploration value to all fitting bins to ensure some randomness.\n    # The value of exploration_boost is small, relative to the peak exploitation score (0.5).\n    # We can add a scaled exploration value.\n    exploration_value = epsilon * 0.5 # A smaller constant exploration value\n\n    # The final priorities are a mix. We can think of it as:\n    # A base score derived from exploitation, with a small random jitter or uniform boost for exploration.\n    priorities[fitting_bins_indices] = exploitation_scores + exploration_value\n\n    # Ensure scores are non-negative.\n    priorities = np.maximum(priorities, 0)\n\n    # Optional: Normalize priorities to sum to 1 if they represent probabilities of selection,\n    # or just return them as preference scores. For selection, normalization is common.\n    # Let's return them as preference scores, allowing the caller to normalize if needed.\n    # A common strategy is to use these scores with a softmax-like selection mechanism.\n\n    # For simplicity and direct preference, we can scale the exploitation scores\n    # and add a smaller exploration component.\n    # Let's re-scale exploitation_scores to have a max value higher than 0.5, e.g., 1.\n    # Max exploitation score is 1 (when sigmoid_argument is large positive).\n    # Min exploitation score is 0 (when sigmoid_argument is large negative).\n    # Peak is 0.5.\n\n    # Let's try a different combination:\n    # Prioritize bins based on exploitation score, but ensure some chance for others.\n    # If random draw < epsilon, pick a random fitting bin. Otherwise, pick based on exploitation.\n    # This is more of a selection logic. For generating priority scores:\n    # We want to combine the 'good fit' score with an exploration factor.\n\n    # A common way to represent this combination is:\n    # priority = (1-epsilon) * exploitation_score + epsilon * uniform_score\n    # Where uniform_score is 1/num_fitting_bins.\n    # Let's re-normalize exploitation scores first to sum to 1 for clarity in combination.\n    if np.sum(exploitation_scores) > 0:\n        normalized_exploitation_scores = exploitation_scores / np.sum(exploitation_scores)\n    else:\n        normalized_exploitation_scores = np.zeros_like(exploitation_scores) # Should not happen if fitting_bins exist\n\n    # Uniform score for exploration among fitting bins\n    uniform_exploration_score = 1.0 / len(fitting_bins_indices) if len(fitting_bins_indices) > 0 else 0.0\n\n    # Combine them:\n    combined_priorities = (1 - epsilon) * normalized_exploitation_scores + epsilon * uniform_exploration_score\n\n    # Place these combined priorities back into the main priorities array\n    priorities[fitting_bins_indices] = combined_priorities\n\n    # Ensure all priorities are non-negative (should already be true)\n    priorities = np.maximum(priorities, 0)\n\n    return priorities",
    "response_id": 9,
    "obj": 4.048663741523748,
    "SLOC": 26.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response0.txt_stdout.txt",
    "code_path": "problem_iter3_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a tunable sigmoid and softmax.\n\n    This heuristic aims to prioritize bins that offer the \"best fit\" for the item,\n    defined as bins where the remaining capacity is only slightly larger than the item's size.\n    It uses a sigmoid function to assign higher scores to these \"tight fits\".\n    The sigmoid's steepness and ideal gap are tunable.\n\n    To encourage exploration among equally good or near-equally good bins,\n    a softmax function is applied to the sigmoid scores. This normalizes scores\n    into probabilities, allowing for probabilistic selection and exploration.\n    Bins that cannot fit the item are assigned a priority of 0.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element is the remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as `bins_remain_cap`, containing the priority\n        score for each bin. Higher scores indicate a more desirable bin for the item.\n    \"\"\"\n\n    def sigmoid(x, steepness=15.0, ideal_gap=0.02):\n        \"\"\"A sigmoid function that peaks at x=0, and can be scaled and shifted.\n        Here, x is effectively (bin_capacity - item_size - ideal_gap).\n        A smaller positive `ideal_gap` means tighter fits are preferred.\n        A larger `steepness` makes the preference for `ideal_gap` more pronounced.\n        \"\"\"\n        # We want the sigmoid to peak when (bin_cap - item) is close to ideal_gap.\n        # So, we map (bin_cap - item) to the sigmoid's input.\n        # Let sigmoid_input = steepness * (ideal_gap - (bin_cap - item))\n        # This means when bin_cap - item = ideal_gap, sigmoid_input = 0, and sigmoid output = 0.5.\n        # We want higher scores for smaller positive gaps.\n        # If bin_cap - item = 0 (perfect fit), sigmoid_input = steepness * ideal_gap > 0, sigmoid output > 0.5.\n        # If bin_cap - item = 0.1, sigmoid_input = steepness * (ideal_gap - 0.1). If ideal_gap is small, this is negative.\n        return 1 / (1 + np.exp(-x))\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit.\n    fits_mask = bins_remain_cap >= item\n\n    if np.any(fits_mask):\n        # Calculate the excess capacity for fitting bins.\n        excess_capacities = bins_remain_cap[fits_mask] - item\n\n        # Tunable parameters for the sigmoid function.\n        # steepness: controls how quickly the priority drops as excess capacity deviates from ideal_gap.\n        # ideal_gap: the preferred small positive residual capacity after packing the item.\n        steepness = 15.0\n        ideal_gap = 0.02\n\n        # Calculate the argument for the sigmoid.\n        # We want the sigmoid to output higher values for smaller, non-negative `excess_capacities`.\n        # The sigmoid `1 / (1 + exp(-x))` has its midpoint at x=0.\n        # To map `excess_capacities` such that `ideal_gap` gives a good score, we use:\n        # `sigmoid_arg = steepness * (ideal_gap - excess_capacities)`\n        # If `excess_capacities` is slightly less than `ideal_gap` (a good fit), `sigmoid_arg` is positive, sigmoid > 0.5.\n        # If `excess_capacities` is exactly `ideal_gap`, `sigmoid_arg` is 0, sigmoid = 0.5.\n        # If `excess_capacities` is larger than `ideal_gap`, `sigmoid_arg` is negative, sigmoid < 0.5.\n        sigmoid_arg = steepness * (ideal_gap - excess_capacities)\n\n        # Calculate raw sigmoid scores for fitting bins.\n        raw_scores = sigmoid(sigmoid_arg, steepness=steepness, ideal_gap=ideal_gap)\n\n        # Apply softmax to the scores. This normalizes scores into a probability distribution,\n        # allowing for probabilistic exploration. Bins with similar high scores will have\n        # non-zero probabilities assigned, encouraging trying different \"good\" bins.\n        # Add a small epsilon to avoid issues with exp(very large negative numbers) if all are bad fits.\n        # However, since we only calculate for fitting bins, this is less of a concern.\n        # We are using the raw scores as inputs to softmax, not probabilities directly.\n        # The 'temperature' parameter in softmax can be adjusted to control exploration.\n        # A higher temperature leads to more uniform probabilities. A lower temperature\n        # leads to probabilities concentrated on the highest score.\n        temperature = 1.0 # Tunable exploration parameter\n\n        # Avoid numerical instability with softmax if all `sigmoid_arg` are very large negative (unlikely here)\n        # or very large positive.\n        # For this application, `sigmoid_arg` will be mostly negative or slightly positive.\n        # A simple softmax on the `raw_scores` is usually sufficient.\n        exp_scores = np.exp(raw_scores / temperature)\n        sum_exp_scores = np.sum(exp_scores)\n\n        if sum_exp_scores > 0:\n            priorities[fits_mask] = exp_scores / sum_exp_scores\n        else:\n            # Fallback: if all exp_scores are zero or NaN (highly unlikely for valid inputs)\n            # distribute probability uniformly among fitting bins.\n            priorities[fits_mask] = 1.0 / np.sum(fits_mask) if np.sum(fits_mask) > 0 else 0\n\n    # Bins that do not fit have a priority of 0.\n    return priorities",
    "response_id": 0,
    "obj": 4.108496210610296,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response3.txt_stdout.txt",
    "code_path": "problem_iter3_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a tunable sigmoid and varied exploration.\n\n    This heuristic aims to prioritize bins that offer the \"best fit\" for the item.\n    A \"best fit\" is defined as a bin where the remaining capacity is only slightly\n    larger than the item's size. This strategy tries to fill bins as much as possible\n    without leaving excessive empty space, thereby minimizing fragmentation.\n\n    The priority is calculated using a sigmoid function. The function is designed\n    to peak when the remaining capacity (`bins_remain_cap`) is precisely equal to\n    the item's size, and the priority decreases as the remaining capacity deviates\n    (either smaller or larger). However, bins where the item doesn't fit at all\n    are assigned a zero priority.\n\n    Tie-breaking is handled using a softmax-like approach on top of the sigmoid scores.\n    This introduces a probabilistic element, favoring bins with higher scores more\n    often but not exclusively, allowing for exploration of less ideal fits.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element is the remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as `bins_remain_cap`, containing the priority\n        score for each bin. Higher scores indicate a more desirable bin for the item.\n    \"\"\"\n\n    def sigmoid(x, steepness=10.0, center=0.0):\n        \"\"\"A custom sigmoid function that can be shifted and scaled.\"\"\"\n        # Ensure numerical stability for large negative exponents\n        exponent = -steepness * (x - center)\n        # Clip exponent to avoid overflow in exp, then compute sigmoid\n        # A large negative exponent approaches 0, large positive approaches 1.\n        # Clipping at -700 is a common practice for exp(-700) ~ 1e-304\n        clipped_exponent = np.clip(exponent, -700, 700)\n        return 1 / (1 + np.exp(clipped_exponent))\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit.\n    fits_mask = bins_remain_cap >= item\n\n    if np.any(fits_mask):\n        # Calculate the excess capacity for fitting bins.\n        excess_capacities = bins_remain_cap[fits_mask] - item\n\n        # Tunable parameters for the sigmoid function\n        # `ideal_gap` defines the target excess capacity for the best fit.\n        # A smaller `ideal_gap` means we prefer bins that leave very little space.\n        # `steepness` controls how quickly the priority drops as excess capacity deviates from `ideal_gap`.\n        ideal_gap = 0.05  # Target for minimal positive residual space\n        steepness = 15.0  # Increased steepness for sharper preference\n\n        # Calculate the argument for the sigmoid function.\n        # We want the peak of the sigmoid (where the argument is 0) to align with our 'ideal_gap'.\n        # The argument is `steepness * (ideal_gap - excess_capacities)`.\n        # This means when `excess_capacities` is close to `ideal_gap`, the argument is close to 0,\n        # resulting in a sigmoid output close to 0.5 (midpoint).\n        # Scores > 0.5 for `excess_capacities` < `ideal_gap`\n        # Scores < 0.5 for `excess_capacities` > `ideal_gap`\n        argument_values = steepness * (ideal_gap - excess_capacities)\n\n        # Apply the sigmoid function to get raw scores for fitting bins.\n        raw_scores = sigmoid(argument_values, steepness=steepness, center=0.0)\n\n        # Normalize scores using a softmax-like approach for exploration.\n        # This converts scores into probabilities, allowing for probabilistic selection\n        # and thus exploration of bins that are not strictly the \"best\" fit.\n        # A small epsilon is added to prevent issues with all scores being identical.\n        # A temperature parameter could be introduced here for more control over exploration.\n        # For simplicity, we'll use the direct softmax on the scaled sigmoid outputs.\n        \n        # We want higher sigmoid scores to translate to higher probabilities.\n        # A simple softmax transformation: exp(score) / sum(exp(scores))\n        # However, direct softmax on scores that might be close to 0 or 1 can lead to extreme probabilities.\n        # A common approach in exploration is to add noise or use a temperature parameter.\n        # Let's scale the sigmoid output to a range that is more suitable for softmax, e.g., [0, 10]\n        # or simply use the sigmoid output directly if it's already in a reasonable range.\n        \n        # For this implementation, let's directly use the sigmoid output as \"desirability\".\n        # Higher desirability means a higher chance of selection.\n        # A simple way to implement \"varied exploration\" without explicit softmax is to\n        # slightly perturb the scores or use a mechanism like epsilon-greedy on these scores.\n        # However, the prompt implies a more direct score-based exploration.\n        # The \"softmax-like\" part can be interpreted as ensuring relative ordering is maintained\n        # and higher scores are disproportionately favored.\n        #\n        # Let's refine the \"softmax-like\" to mean we are generating relative weights.\n        # The sigmoid already provides a relative measure of \"goodness of fit\".\n        # The primary refinement for \"varied exploration\" beyond standard greedy would be:\n        # 1. Add small random noise to the scores.\n        # 2. Use the scores as weights in a weighted random choice.\n        #\n        # The reflection mentioned \"softmax for score normalization\".\n        # If we consider the raw sigmoid scores (0 to 1), softmax on these would yield\n        # probabilities summing to 1 across the *fitting* bins.\n        \n        # Option: Softmax on raw_scores\n        # exp_scores = np.exp(raw_scores)\n        # probabilities = exp_scores / np.sum(exp_scores)\n        # priorities[fits_mask] = probabilities\n\n        # Option: Scaled scores for softmax (e.g., if sigmoid scores are too clustered)\n        # scaled_scores = raw_scores * 5.0 # Scale to a range like [0, 5]\n        # exp_scaled_scores = np.exp(scaled_scores)\n        # probabilities = exp_scaled_scores / np.sum(exp_scaled_scores)\n        # priorities[fits_mask] = probabilities\n\n        # The current sigmoid output is already designed to be a priority score.\n        # The \"varied exploration\" might simply mean that the sigmoid output itself\n        # provides a graded preference, allowing a weighted selection mechanism\n        # (not implemented here but implied by \"priority score\") to explore.\n        # If we interpret \"softmax-like\" as creating relative probabilities, we can do that.\n        # However, a simpler interpretation for \"priority score\" is to just return\n        # the modulated sigmoid scores, and let the selection algorithm handle the exploration.\n\n        # Let's stick to the interpretation that the `priority_v2` function *outputs*\n        # scores that can be used for exploration. The sigmoid already provides a nuanced score.\n        # For tie-breaking and varied exploration, we can enhance the score or rely on the selection mechanism.\n        #\n        # A simple way to introduce variation without full softmax: add a small, scaled random component.\n        # This is akin to adding noise for exploration.\n        #\n        # Let's ensure the scores are in a somewhat predictable range and then use them.\n        # The sigmoid output is [0, 1].\n        \n        # For \"varied exploration\", we can make the scores slightly more distinct or add noise.\n        # Adding a small random noise:\n        # noise = np.random.normal(0, 0.05, raw_scores.shape) # Mean 0, std dev 0.05\n        # noisy_scores = raw_scores + noise\n        # Ensure scores remain within a valid range (e.g., > 0) for weighted selection.\n        # We can clamp or re-normalize.\n\n        # Let's interpret \"varied exploration\" as generating scores that are\n        # not purely greedy, but represent a soft preference. The sigmoid itself does this.\n        # The \"softmax for score normalization\" could mean scaling these priorities\n        # so they sum to a constant (e.g., 1 if using weighted random choice).\n        #\n        # The most direct interpretation is to simply return the refined sigmoid scores.\n        # The \"varied exploration\" aspect is then handled by how these scores are *used* by the calling algorithm\n        # (e.g., weighted random choice vs. simple argmax).\n        #\n        # If the intent is for this function itself to produce probabilities for a direct selection,\n        # then softmax is appropriate. Given the wording \"returns a priority score for each bin\",\n        # it suggests the output is a score, not necessarily a final probability.\n        #\n        # Let's refine the sigmoid output to ensure higher scores are more distinct,\n        # making the preference clearer for exploration.\n        # We can amplify the difference between scores.\n        \n        # A simple amplification of the sigmoid output might be:\n        # amplified_scores = raw_scores ** power_factor  (where power_factor > 1)\n        # Or a linear scaling and shifting that maintains order but increases separation.\n        \n        # Let's use the original sigmoid scores, as they are already graded.\n        # If \"varied exploration\" means probabilistic selection based on scores,\n        # the scores themselves are the input to that mechanism.\n        priorities[fits_mask] = raw_scores\n\n    # For bins where the item does not fit, the priority remains 0.\n    # This correctly implies they have the lowest priority.\n    return priorities",
    "response_id": 3,
    "obj": 4.078579976067022,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response2.txt_stdout.txt",
    "code_path": "problem_iter4_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First and then Best Fit.\n\n    This strategy prioritizes bins that have a remaining capacity exactly equal\n    to the item's size (exact fit). Among these exact fits, it assigns a high,\n    uniform priority. If no exact fit is found, it then selects bins that can\n    accommodate the item and prioritizes those that minimize the remaining\n    capacity after the item is placed (best fit). Bins that cannot fit the\n    item are assigned a priority of zero.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n        # Identify exact fits among the fitting bins\n        exact_fit_mask_subset = fitting_bins_remain_cap == item\n\n        if np.any(exact_fit_mask_subset):\n            # Assign a very high priority to exact fits.\n            # We can assign a uniform high score, or slightly differentiate if needed.\n            # For simplicity and to strongly prioritize exact fits, we use a large value.\n            # The original indices are needed to update the `priorities` array.\n            original_indices_exact_fit = np.where(can_fit_mask)[0][exact_fit_mask_subset]\n            priorities[original_indices_exact_fit] = 1e9  # High priority for exact fits\n        else:\n            # If no exact fit, apply Best Fit logic to the bins that can fit.\n            # Prioritize bins that leave the minimum remaining capacity after placement.\n            # The remaining capacity after placing the item is `bin_cap - item`.\n            # We want to minimize this value. So, we use `-(bin_cap - item)`\n            # as the priority score, because higher scores are preferred.\n            # This means bins with smaller `bin_cap - item` get larger (less negative) scores.\n            \n            # Calculate the remaining capacity after placing the item for fitting bins.\n            remaining_after_fit = fitting_bins_remain_cap - item\n            \n            # Higher priority for smaller remaining capacity.\n            # We use the negative of remaining capacity. For example, if remaining is 0.1, score is -0.1.\n            # If remaining is 0.05, score is -0.05. -0.05 is greater than -0.1, so it's prioritized.\n            # This creates a descending order of priority based on remaining capacity.\n            # We can also shift this by a constant or scale it to ensure it's positive if desired,\n            # but a negative score is fine as long as argmax is used.\n            # Let's make them positive for clarity, e.g., max_possible_remaining - remaining_after_fit.\n            # A simpler approach is to just use the negative value as a score, and rely on max.\n            \n            # Let's add a small constant to ensure scores are positive and maintain order.\n            # The largest possible `remaining_after_fit` would be `max(bins_remain_cap) - item`.\n            # A simple way is to use `1 - remaining_after_fit` if `remaining_after_fit` is in [0, 1].\n            # A more robust way is to shift by a value larger than `max(remaining_after_fit)`.\n            \n            # The current `remaining_after_fit` values are non-negative.\n            # We want to maximize `-(remaining_after_fit)`.\n            # This is equivalent to minimizing `remaining_after_fit`.\n            # Let's use `1 / (remaining_after_fit + epsilon)` or similar, but that can cause issues.\n            # A safe approach is to use `constant - remaining_after_fit`.\n            \n            # For simplicity, we can use `-(remaining_after_fit)` and trust that `argmax` will pick the largest (least negative).\n            # Let's shift it to be positive, making higher values clearly better.\n            # We want the smallest `remaining_after_fit` to have the highest score.\n            # Let `score = -remaining_after_fit`.\n            # We can use `MaxCapacity - (bin_cap - item)` if we want scores to be related to total capacity.\n            # A simple approach: `score = MaxPossibleRemaining - remaining_after_fit`.\n            # Let's use `1.0 - remaining_after_fit` as a heuristic score, assuming `remaining_after_fit` is typically small.\n            # A more robust score would be to invert the rank.\n            \n            # Let's use the negative of the remaining capacity.\n            # This means a bin that results in 0.01 remaining gets a score of -0.01.\n            # A bin that results in 0.05 remaining gets a score of -0.05.\n            # -0.01 > -0.05, so the tighter fit is preferred.\n            \n            # To ensure positive scores and strong preference for tighter fits:\n            # We can use `MaxPossibleRemaining - remaining_after_fit`.\n            # A simpler approach that maintains order and positivity is to use a large constant minus the penalty.\n            # Let's use a constant like 1.0 and subtract the `remaining_after_fit`.\n            # This assumes `remaining_after_fit` is not excessively large.\n            # A better approach is to rank them.\n            \n            # Let's use a score that directly represents \"how much space is left\".\n            # We want to minimize `bins_remain_cap[i] - item`.\n            # So, priority should be inversely related to this.\n            # A simple transformation: `max(0, MAX_REMAINING_CAPACITY - (bins_remain_cap[i] - item))`\n            # Or simpler: `1.0 - (bins_remain_cap[i] - item)` if we assume values are within a reasonable range.\n            \n            # Let's use the negative of the remaining capacity, as it correctly orders them and `argmax` works.\n            # The actual magnitude isn't critical as long as the order is correct.\n            # We need to map these scores back to the original `priorities` array.\n            original_indices_non_exact_fit = np.where(can_fit_mask)[0][~exact_fit_mask_subset]\n            priorities[original_indices_non_exact_fit] = -remaining_after_fit\n            \n            # To make the scores more distinct and potentially avoid floating point issues with `argmax`\n            # if many scores are very close, we can add a small penalty to their index if we were\n            # sorting, but here `argmax` handles it.\n            # The scores are already ordered correctly: -0.01 > -0.05.\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response7.txt_stdout.txt",
    "code_path": "problem_iter4_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Sigmoid-based Best Fit strategy.\n\n    This strategy aims to prioritize bins that leave the smallest positive remaining\n    capacity after the item is placed (Best Fit). It uses a sigmoid function to\n    map this \"goodness of fit\" to a priority score, where tighter fits (smaller\n    positive remaining capacity) receive higher scores.\n\n    The function `sigmoid(Constant - scale_factor * gap)` is used, where `gap` is\n    the remaining capacity after placing the item (`bins_remain_cap - item`).\n    - `Constant` biases the sigmoid, allowing scores above 0.5 for good fits.\n    - `scale_factor` controls the steepness of the priority drop-off as the gap increases.\n    - A clipping mechanism (`max_gap_clip`) is used to prevent extreme gaps from\n      causing numerical instability and to treat very large gaps similarly.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate a higher priority for that bin. Bins that cannot\n        accommodate the item are given a priority score of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)  # Initialize with 0 for non-fitting bins\n\n    # Calculate the potential remaining capacity if the item is placed in each bin.\n    # We are interested in cases where the item fits, so remaining capacity >= 0.\n    potential_remaining_capacities = bins_remain_cap - item\n\n    # Identify bins where the item can fit.\n    can_fit_mask = potential_remaining_capacities >= 0\n\n    # Get the remaining capacities for only the bins that can fit the item.\n    fitting_bins_remain_cap = potential_remaining_capacities[can_fit_mask]\n\n    # Calculate the 'gap' or 'slack' for fitting bins: remaining_capacity - item_size.\n    # We want this gap to be as small and positive as possible.\n    gap = fitting_bins_remain_cap\n\n    # --- Heuristic Parameters ---\n    # These parameters can be tuned for performance.\n    constant_bias = 3.0  # Shifts the sigmoid curve; higher values mean higher priority for tighter fits.\n    scale_factor = 5.0   # Controls the steepness of the priority drop-off for larger gaps.\n    max_gap_clip = 50.0  # Caps the gap to prevent extreme values from dominating or causing numerical issues.\n                         # Gaps larger than this are treated equally poorly.\n    # --------------------------\n\n    # Clip the gap values to the maximum defined clip value.\n    # This ensures that extremely large gaps don't cause numerical instability\n    # in the sigmoid calculation and are penalized consistently.\n    clipped_gap = np.clip(gap, 0.0, max_gap_clip)\n\n    # Calculate the argument for the sigmoid function.\n    # We want higher priority (higher sigmoid output) for smaller gaps.\n    # Sigmoid(x) = 1 / (1 + exp(-x)) is highest for large positive 'x'.\n    # So, we want 'x' to be large positive when 'clipped_gap' is small.\n    # The form `Constant - scale_factor * clipped_gap` achieves this:\n    # - If clipped_gap is small (near 0), arg is ~Constant (positive, large sigmoid output).\n    # - If clipped_gap is large, arg is smaller (negative, smaller sigmoid output).\n    sigmoid_arg = constant_bias - scale_factor * clipped_gap\n\n    # Apply the sigmoid function to calculate priorities for fitting bins.\n    # np.exp() can overflow if the argument to `exp` is too large positive.\n    # Here, the argument to `exp` is `-sigmoid_arg`.\n    # `-sigmoid_arg = -constant_bias + scale_factor * clipped_gap`.\n    # Maximum value of `clipped_gap` is `max_gap_clip`.\n    # Max value of `-sigmoid_arg` is `-constant_bias + scale_factor * max_gap_clip`.\n    # With parameters: `-3.0 + 5.0 * 50.0 = -3.0 + 250.0 = 247.0`.\n    # np.exp(247.0) is large but generally within numerical limits for standard floats.\n    # If `sigmoid_arg` becomes extremely negative (due to very large original gaps that were clipped),\n    # `-sigmoid_arg` will be very positive, `exp(-sigmoid_arg)` might approach infinity,\n    # making the sigmoid value approach 0, which is the desired behavior for bad fits.\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(-sigmoid_arg))\n\n    return priorities",
    "response_id": 7,
    "obj": 4.487435181491823,
    "SLOC": 13.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response9.txt_stdout.txt",
    "code_path": "problem_iter4_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority score for each bin using a hybrid approach prioritizing good fits and balancing exploration.\n\n    This heuristic prioritizes bins that offer a \"good fit\" for the item, meaning\n    their remaining capacity is slightly larger than the item size. It also\n    incorporates an exploration component to occasionally consider less-full bins,\n    balancing the \"best fit\" tendency with exploration of potential future packing\n    opportunities.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array containing the remaining capacity of each bin.\n\n    Returns:\n        A numpy array of priority scores for each bin. Higher scores indicate a\n        higher preference for placing the item in that bin.\n    \"\"\"\n    # Tunable parameters\n    epsilon = 0.1  # Probability of exploration (choosing a bin less greedily)\n    k_steepness = 10.0  # Steepness parameter for the sigmoid function, higher means more emphasis on ideal_gap\n    ideal_gap = 0.05  # The preferred remaining capacity beyond the item size\n\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Identify bins where the item can fit.\n    fits_mask = bins_remain_cap >= item\n    fitting_bins_indices = np.where(fits_mask)[0]\n\n    # If no bins can fit the item, return all zeros.\n    if len(fitting_bins_indices) == 0:\n        return priorities\n\n    # --- Exploitation Component: Prioritize Good Fits ---\n    # Calculate the \"excess capacity\" for bins where the item fits.\n    excess_capacities = bins_remain_cap[fitting_bins_indices] - item\n\n    # Use a sigmoid function to score the \"goodness\" of the fit.\n    # The argument is designed so that `ideal_gap` results in a score of 0.5.\n    # Scores are higher for `excess_capacities` closer to `ideal_gap`.\n    sigmoid_argument = k_steepness * (ideal_gap - excess_capacities)\n    exploitation_scores = 1 / (1 + np.exp(-sigmoid_argument))\n\n    # Normalize exploitation scores so they sum to 1 among fitting bins.\n    # This makes combining with exploration probability more straightforward.\n    sum_exploitation_scores = np.sum(exploitation_scores)\n    if sum_exploitation_scores > 0:\n        normalized_exploitation_scores = exploitation_scores / sum_exploitation_scores\n    else:\n        # If all exploitation scores are zero (unlikely for valid inputs),\n        # assign uniform probability among fitting bins.\n        normalized_exploitation_scores = np.ones(len(fitting_bins_indices)) / len(fitting_bins_indices)\n\n    # --- Exploration Component ---\n    # Assign a uniform exploration score to all fitting bins.\n    # This ensures that even bins with low exploitation scores have a chance.\n    uniform_exploration_score = 1.0 / len(fitting_bins_indices)\n\n    # --- Combine Exploitation and Exploration ---\n    # The final priority is a weighted average:\n    # (1 - epsilon) * exploitation_score + epsilon * uniform_exploration_score\n    combined_priorities = (1 - epsilon) * normalized_exploitation_scores + epsilon * uniform_exploration_score\n\n    # Place these combined priorities back into the main priorities array\n    priorities[fitting_bins_indices] = combined_priorities\n\n    # Ensure all priorities are non-negative (should already be true due to combination)\n    priorities = np.maximum(priorities, 0)\n\n    return priorities",
    "response_id": 9,
    "obj": 4.048663741523748,
    "SLOC": 23.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response0.txt_stdout.txt",
    "code_path": "problem_iter5_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a refined scoring mechanism.\n\n    This heuristic aims to prioritize bins that offer the \"best fit\" for the item,\n    meaning the remaining capacity is just enough or slightly more than the item's size.\n    It also considers a \"worst fit reduction\" aspect by giving a slight preference\n    to bins that would have a larger remaining capacity if the item were placed,\n    but only if the fit is still reasonably good. This helps in potentially\n    leaving smaller gaps for future small items.\n\n    The scoring function combines a sigmoid for \"best fit\" around a small gap,\n    and a linear component for \"worst fit reduction\" for bins with larger capacities,\n    while ensuring items that don't fit get zero priority.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element is the remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as `bins_remain_cap`, containing the priority\n        score for each bin. Higher scores indicate a more desirable bin for the item.\n    \"\"\"\n\n    def sigmoid(x, steepness=10.0, center=0.0):\n        \"\"\"A custom sigmoid function that can be shifted and scaled.\"\"\"\n        # Ensure we don't overflow with large negative exponents\n        safe_x = np.clip(x, -700, 700)\n        return 1 / (1 + np.exp(-steepness * (safe_x - center)))\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit.\n    fits_mask = bins_remain_cap >= item\n\n    if np.any(fits_mask):\n        fitting_capacities = bins_remain_cap[fits_mask]\n        excess_capacities = fitting_capacities - item\n\n        # --- Best Fit Component ---\n        # Prioritize bins where the excess capacity is small.\n        # Use sigmoid to create a peak around a small positive excess capacity (ideal_gap).\n        ideal_gap = 0.05  # Target for minimal wasted space\n        steepness_bf = 10.0\n\n        # The argument to sigmoid is `steepness * (ideal_gap - excess_capacities)`.\n        # This maps `excess_capacities = ideal_gap` to an argument of 0 (sigmoid output 0.5).\n        # Smaller excess_capacities get higher scores.\n        best_fit_scores = sigmoid(excess_capacities, steepness=steepness_bf, center=ideal_gap)\n\n        # --- Worst Fit Reduction Component ---\n        # For bins that have significantly more space than needed, we can consider\n        # how much \"worst fit\" we are creating. Bins with larger excess capacities\n        # could be considered for \"worst fit reduction\" if we want to keep smaller bins\n        # more available. However, the problem statement implies prioritizing \"good fits\".\n        # A simple way to incorporate \"worst fit reduction\" into prioritization without\n        # drastically altering the \"best fit\" goal is to give a slight boost to bins\n        # that have larger remaining capacity *after* packing, if they are still a reasonable fit.\n        # This is somewhat counter-intuitive for typical 'worst fit' heuristics which\n        # aim to pack into the bin with the largest residual capacity first.\n        # Here, we are prioritizing bins to pack *into*, and we want to avoid\n        # creating many small residual capacities.\n        #\n        # Let's re-interpret \"worst fit reduction\" as avoiding situations where placing\n        # the item creates a *very small* residual capacity that is unusable.\n        # Alternatively, we can boost bins that leave a *larger* residual capacity\n        # (but still fit the item) as this might be beneficial for future large items.\n        # This aligns with \"maximal remaining capacity\" among those that fit.\n\n        # For bins that are not perfect fits, we might prefer those that leave more space.\n        # This can be modeled by a linear or slightly concave function of excess capacity.\n        # Let's consider a linear scaling for `excess_capacities > ideal_gap`.\n        # A simple approach is to add a small bonus proportional to `excess_capacities`.\n        # We need to normalize this bonus. A simple scaling factor can be used.\n        # Let's define a maximum \"good\" excess capacity beyond which the bonus diminishes or becomes negative.\n\n        steepness_wfr = 5.0 # Controls how quickly the WFR bonus drops off\n        max_wfr_bonus_at_excess = 0.2 # Maximum bonus for WFR, scaled from 0 to 1\n\n        # Calculate a bonus that increases with excess_capacity up to a point, then decreases.\n        # We can use a shifted sigmoid or a clipped linear function.\n        # Let's try a linear component for excess_capacity > ideal_gap, and a penalty for very large gaps.\n        # A simpler approach: give a small boost to bins with larger remaining capacity, but cap it.\n        # Let's scale `excess_capacities` linearly, but only for values significantly larger than `ideal_gap`.\n        # We want to avoid large penalties for bins that are just slightly over the ideal gap.\n\n        # Consider `excess_capacities` relative to the item size or total capacity.\n        # Let's use a bonus that is proportional to `excess_capacities`, but scaled down.\n        # The idea is to give a slight advantage to bins that have more remaining space,\n        # as long as they fit the item.\n        # The bonus should be smaller than the best-fit score for tight fits.\n\n        # Let's try a simple linear bonus for excess_capacities, scaled by a factor.\n        # We want to blend this with the best_fit_scores.\n        # `bonus = bonus_weight * excess_capacities`.\n        # The `bonus_weight` should be small.\n\n        bonus_weight = 0.1 # Adjust this to control the influence of the WFR component\n        wfr_bonus = bonus_weight * excess_capacities\n\n        # Combine scores: weighted sum.\n        # We want the BF score to dominate for tight fits, and WFR to add a boost for looser fits.\n        # Ensure scores remain within a reasonable range (e.g., [0, 1] for BF, bonus might go slightly over).\n        # A simple sum might exceed 1. Normalization or careful blending is needed.\n\n        # Let's re-think: prioritize exact fits, then minimal residual space. Combine 'best fit' with 'worst fit reduction'.\n        # 'Worst fit reduction' could mean, among the 'best fits', prefer the one that leaves slightly more space.\n        # Or, if many bins are \"good fits\", pick one that has a bit more residual capacity.\n        #\n        # Let's use a composite score.\n        # Primary objective: minimize `excess_capacity` (best fit).\n        # Secondary objective: if `excess_capacity` is small, prefer slightly larger `excess_capacity` (WFR).\n        #\n        # This suggests a function that is high for `excess_capacity` near 0, and then slightly increases before decreasing.\n        # A Gaussian-like function centered slightly above 0 could work, or a combination of sigmoid terms.\n        #\n        # Let's use the `best_fit_scores` as the primary score.\n        # For bins where `excess_capacity > ideal_gap`, the `best_fit_scores` are less than 0.5.\n        # We can add a bonus proportional to `excess_capacity` in this region.\n\n        # A simpler approach:\n        # 1. Score for 'best fit' (sigmoid peak at `ideal_gap`).\n        # 2. Score for 'worst fit reduction' (linear increase with `excess_capacity` beyond `ideal_gap`).\n        # The challenge is blending these so they make sense.\n\n        # Let's define two criteria:\n        # C1: Closeness to ideal_gap. Score = sigmoid(ideal_gap - excess)\n        # C2: Amount of excess capacity (for WFR). Score = excess_capacity (scaled)\n        # Combined score = w1 * C1 + w2 * C2.\n        #\n        # If we want to prioritize exact fit and then leave slightly more space,\n        # a function like `f(x) = sigmoid(k1 * (ideal - x)) + k2 * x` for `x >= 0`.\n        # where `ideal` is our `ideal_gap`.\n        #\n        # Let's use `ideal_gap = 0.05`.\n        #\n        # Score for `excess_capacity`\n        #\n        # For `excess_capacity` close to 0: High score (from sigmoid)\n        # For `excess_capacity` = `ideal_gap`: Mid score (0.5 from sigmoid)\n        # For `excess_capacity` > `ideal_gap`: Lower score from sigmoid, but potentially higher from WFR.\n        #\n        # Let's define two components:\n        # `score_bf`: Prioritizes `excess_capacity` near `ideal_gap`.\n        # `score_wfr`: Prioritizes larger `excess_capacity` (for values that fit).\n        #\n        # We need to ensure that even with WFR, the best fits are still preferred.\n        #\n        # Let's try:\n        # `priority = sigmoid(steepness * (ideal_gap - excess)) + scale * excess`\n        # This directly combines the two. `scale` needs to be small enough.\n\n        steepness = 10.0\n        scale = 0.1 # Controls the influence of the linear \"worst fit reduction\" component.\n                    # Larger `scale` means larger remaining capacities are more preferred.\n\n        # Calculate the composite score directly.\n        # `sigmoid(steepness * (ideal_gap - excess_capacities))` gives higher scores for smaller `excess_capacities`.\n        # `scale * excess_capacities` gives higher scores for larger `excess_capacities`.\n        # The sum will have a peak somewhere between 0 and the `excess_capacity` that maximizes the sum.\n        # This structure aligns with \"prioritize exact fits, then minimal residual space\" and then considering WFR.\n        # The \"minimal residual space\" part is handled by the sigmoid's steep slope near 0.\n        # The \"worst fit reduction\" aspect is handled by the linear term for larger `excess_capacities`.\n\n        combined_scores = sigmoid(excess_capacities, steepness=steepness, center=ideal_gap) + scale * excess_capacities\n\n        # Ensure scores don't exceed a reasonable maximum, though they can go above 1.\n        # The relative ordering is what matters most.\n\n        priorities[fits_mask] = combined_scores\n\n    return priorities",
    "response_id": 0,
    "obj": 149.30195452732352,
    "SLOC": 21.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response1.txt_stdout.txt",
    "code_path": "problem_iter5_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This heuristic aims to prioritize bins based on a multi-objective approach:\n    1. Exact fits (remaining capacity == item size) are given the highest priority.\n    2. Among bins where the item fits, those with minimal residual space (after packing)\n       are preferred, but with a preference for slightly positive residual space over zero.\n       This helps in avoiding fragmentation while still trying to fill bins well.\n    3. Bins that would leave a very large residual space are penalized.\n    4. Exploration is encouraged by using a softmax-like scaling on good fits,\n       giving a boost to slightly less optimal but still good choices.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element is the remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as `bins_remain_cap`, containing the priority\n        score for each bin. Higher scores indicate a more desirable bin for the item.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit.\n    fits_mask = bins_remain_cap >= item\n\n    if np.any(fits_mask):\n        # Calculate the residual capacity after packing the item for fitting bins.\n        residual_capacities = bins_remain_cap[fits_mask] - item\n\n        # --- Objective 1: Exact Fits ---\n        # Assign a very high base priority to exact fits.\n        exact_fit_mask = (residual_capacities == 0)\n        if np.any(exact_fit_mask):\n            priorities[fits_mask][exact_fit_mask] = 100.0 # High score for exact fits\n\n        # --- Objective 2 & 3: Minimal Residual Space & Penalize Large Residuals ---\n        # Use a function that peaks at a small positive residual and decreases for larger ones.\n        # A Gaussian-like shape or a modified sigmoid can work.\n        # Let's use a function where the priority is high for small residuals (0 to ~0.2)\n        # and drops off as residuals increase.\n\n        # Penalize bins where the residual is excessively large.\n        # For example, if residual > bin_capacity / 2, assign low priority.\n        # We can model this by taking `1 / (1 + penalty_factor * max(0, residual - threshold))`\n\n        # Define a \"good\" range for residual capacity, say [0, 0.2].\n        # We want to reward residuals within this range.\n        # Let's use a function that increases up to a point and then decreases.\n        # A simple approach:\n        # score = exp(-k * residual) for residuals near 0.\n        # For residuals > 0, let's try to incentivize residuals that are small but positive.\n        # `gaussian_like_score = exp(-(residual / std_dev)**2)` where std_dev is small.\n        # To make it peak at a small positive value, we can shift it.\n        # Let's try: score = exp(-k * (residual - target_residual)**2)\n        # For target_residual = 0.05, std_dev = 0.1.\n\n        target_residual = 0.05\n        std_dev = 0.1\n        steepness_factor = 5.0 # Controls how quickly priority drops off\n\n        # Calculate scores for bins that are not exact fits\n        non_exact_fit_mask = ~exact_fit_mask\n        if np.any(non_exact_fit_mask):\n            non_exact_residuals = residual_capacities[non_exact_fit_mask]\n\n            # Calculate a score based on how close the residual is to the target residual.\n            # Higher score for residuals closer to target_residual.\n            # We use a Gaussian-like function centered around target_residual.\n            # Add a small base score to avoid zero for all non-exact fits unless residual is huge.\n            gaussian_scores = np.exp(-steepness_factor * ((non_exact_residuals - target_residual) / std_dev)**2)\n\n            # Ensure residuals much larger than target are penalized.\n            # If residual is > target_residual + 2*std_dev, the score is already low.\n            # We can further cap or reduce scores for very large residuals.\n            large_residual_threshold = target_residual + 2 * std_dev\n            penalty_mask = (non_exact_residuals > large_residual_threshold)\n            gaussian_scores[penalty_mask] *= 0.1 # Heavily penalize very large residuals\n\n            # Add a small base priority to all fitting bins (excluding exact fits which have 100)\n            # to ensure they are considered if no exact fits exist.\n            base_priority_for_fitting = 0.1\n            priorities[fits_mask][non_exact_fit_mask] = base_priority_for_fitting + gaussian_scores\n\n        # --- Objective 4: Softmax-like Exploration ---\n        # Apply a scaling to the priorities of fitting bins to encourage exploration\n        # among the better options. A simple way is to exponentiate and normalize,\n        # or just rescale. Let's scale the non-100 priorities to be within a range,\n        # e.g., [0.1, 0.9].\n\n        # Find the indices of non-exact fits within the original bins_remain_cap array.\n        fitting_indices_in_original = np.where(fits_mask)[0]\n        non_exact_fit_indices_in_original = fitting_indices_in_original[non_exact_fit_mask]\n\n        # Re-calculate priorities for non-exact fits, making sure they are relative.\n        if np.any(non_exact_fit_mask):\n            current_non_exact_priorities = priorities[fits_mask][non_exact_fit_mask]\n            # Softmax-like scaling: exp(priority) and then normalize or simply scale.\n            # Let's scale them relative to each other.\n            scaled_priorities = np.exp(current_non_exact_priorities / np.max(current_non_exact_priorities))\n            # Normalize to a range, e.g., [0.1, 0.9] if max is not 1.\n            min_scaled = np.min(scaled_priorities)\n            max_scaled = np.max(scaled_priorities)\n\n            if max_scaled > min_scaled: # Avoid division by zero if all are same\n                normalized_scaled = 0.1 + 0.8 * (scaled_priorities - min_scaled) / (max_scaled - min_scaled)\n            else:\n                normalized_scaled = np.full_like(scaled_priorities, 0.5) # Middle value if all same\n\n            priorities[fits_mask][non_exact_fit_mask] = normalized_scaled\n\n        # Ensure exact fits still have the highest priority\n        if np.any(exact_fit_mask):\n            priorities[fits_mask][exact_fit_mask] = 100.0\n\n        # Ensure that even the \"best\" non-exact fit is less than exact fits.\n        # If the scaled priorities for non-exact fits accidentally exceed exact fits (due to initialization), cap them.\n        fitting_indices = np.where(fits_mask)[0]\n        if np.any(fitting_indices):\n            max_priority_non_exact = np.max(priorities[fits_mask]) if not np.all(priorities[fits_mask] == 100.0) else 0\n            if max_priority_non_exact > 0 and max_priority_non_exact < 100.0:\n                 priorities[fits_mask] = np.clip(priorities[fits_mask], 0, 100.0)\n            elif max_priority_non_exact == 100.0 and np.any(priorities[fits_mask] == 100.0):\n                 # If there are exact fits, ensure other fitting bins are significantly lower\n                 priorities[fits_mask][~exact_fit_mask[fits_mask]] = np.clip(priorities[fits_mask][~exact_fit_mask[fits_mask]], 0, 10.0)\n\n\n    return priorities",
    "response_id": 1,
    "obj": 4.487435181491823,
    "SLOC": 42.0,
    "cyclomatic_complexity": 13.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response2.txt_stdout.txt",
    "code_path": "problem_iter5_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Sigmoid Fit Score.\n\n    This heuristic aims to prioritize bins that offer the \"best fit\" for the item.\n    A \"best fit\" is defined as a bin where the remaining capacity is only slightly\n    larger than the item's size. This strategy tries to fill bins as much as possible\n    without leaving excessive empty space, thereby minimizing fragmentation.\n\n    The priority is calculated using a sigmoid function. The function is designed\n    to peak when the remaining capacity (`bins_remain_cap`) is precisely equal to\n    the item's size, and the priority decreases as the remaining capacity deviates\n    (either smaller or larger). However, bins where the item doesn't fit at all\n    are assigned a zero priority.\n\n    This version refines the priority by:\n    1. Directly penalizing bins with very large remaining capacity (Worst Fit Reduction component).\n    2. Using softmax to ensure a distribution of probabilities rather than just absolute scores,\n       encouraging exploration among good options.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element is the remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as `bins_remain_cap`, containing the priority\n        score for each bin. Higher scores indicate a more desirable bin for the item.\n    \"\"\"\n\n    def sigmoid(x, steepness=10.0, center=0.0):\n        \"\"\"A custom sigmoid function that can be shifted and scaled.\"\"\"\n        # Adding a small epsilon to avoid division by zero or log(0) issues if used later\n        # though not directly used here, it's good practice for stability.\n        return 1 / (1 + np.exp(-steepness * (x - center)))\n\n    # Initialize priorities to a very low negative number for log-sum-exp stability\n    # or to 0 for direct softmax if no prior transformation.\n    # Let's use a transformation that encourages small gaps and penalizes large gaps.\n    # We want to prioritize bins where `bins_remain_cap - item` is close to 0.\n\n    # Initialize raw scores.\n    # For bins that don't fit, assign a score that will result in a low probability after softmax.\n    # A large negative number is suitable.\n    raw_scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins where the item can fit.\n    fits_mask = bins_remain_cap >= item\n\n    if np.any(fits_mask):\n        # Calculate the excess capacity for fitting bins.\n        excess_capacities = bins_remain_cap[fits_mask] - item\n\n        # Define parameters for the scoring function.\n        # 'ideal_gap' is the desired remaining capacity after fitting the item.\n        # Values close to 0 are good (exact fit), but a small positive gap might be even better\n        # to avoid packing too tightly and potentially leaving no space for future small items.\n        ideal_gap = 0.05  # Target a small positive residual capacity\n        steepness_fit = 15.0 # How sensitive the score is to deviations from ideal_gap\n\n        # Component 1: Prioritize near 'ideal_gap' using a shifted sigmoid.\n        # We want the peak score (e.g., 0) to be at `excess_capacities = ideal_gap`.\n        # `score_fit = -abs(excess_capacities - ideal_gap)` can be used, but sigmoid provides\n        # smoother transitions and bounds.\n        # `sigmoid(steepness * (ideal_gap - excess_capacities))` centers the sigmoid's inflection\n        # point at `ideal_gap`. A higher value means a better fit.\n        fit_scores = sigmoid(steepness_fit * (ideal_gap - excess_capacities), steepness=steepness_fit, center=0.0)\n\n        # Component 2: Penalize large remaining capacities (Worst Fit Reduction component).\n        # This discourages using bins that are almost empty if a better fit exists.\n        # A linear penalty or an inverse relationship could work. Let's use a log-based penalty\n        # that grows slowly for very large capacities.\n        # We want to assign lower scores to bins with high `bins_remain_cap[fits_mask]`.\n        # Use `1 / (1 + excess_capacities)` or `-log(1 + excess_capacities)` as a penalty.\n        # A simple inverse relationship for the *remaining* capacity in the bin after fitting:\n        # `penalty_large_residual = 1 / (1 + excess_capacities)`\n        # Or, to penalize the *original* large capacity:\n        # `penalty_large_original = 1 / (1 + bins_remain_cap[fits_mask])`\n        # Let's use a penalty on the *excess capacity* to complement the fit score.\n        # We want to dampen scores for bins with large `excess_capacities`.\n        # A function like `exp(-large_capacity_penalty_factor * excess_capacities)` can work.\n        large_capacity_penalty_factor = 0.5\n        penalty_scores = np.exp(-large_capacity_penalty_factor * excess_capacities)\n\n        # Combine the scores. A simple multiplication or addition can work.\n        # Multiplying preserves the idea that both are important:\n        # `combined_scores = fit_scores * penalty_scores`\n        # To make it more robust to different scales and use softmax, let's sum weighted terms.\n        # Let fit score contribute positively and large capacity negatively.\n        # We can map `fit_scores` (0 to 1) to a range, and `penalty_scores` (close to 0 for large excess)\n        # to a similar range.\n\n        # A score that combines good fit (excess ~ ideal_gap) and not too much overall waste.\n        # Consider the \"waste\" as `bins_remain_cap[fits_mask] - item`.\n        # We want `excess_capacities` to be small and positive.\n        # `score = -abs(excess_capacities - ideal_gap)`\n        # Let's normalize `excess_capacities` and map them.\n        # A simpler approach for raw scores before softmax:\n        # Prioritize small `excess_capacities`, especially near `ideal_gap`.\n        # `raw_score = -excess_capacities` would prioritize minimal excess.\n        # `raw_score = -abs(excess_capacities - ideal_gap)` prioritizes `ideal_gap`.\n        # Let's use the latter.\n\n        # Re-evaluate the scoring: we want high score for small `excess_capacities`, and especially near `ideal_gap`.\n        # A simple score for \"best fit\" is `-excess_capacities`.\n        # To add the \"near ideal_gap\" preference: `-abs(excess_capacities - ideal_gap)`\n        # Combine: Prioritize small excess (good for overall packing), but also near ideal_gap (avoids tiny gaps).\n        # Let's use a scoring function that is high when excess_capacity is low,\n        # and has a peak at ideal_gap.\n        # `score = exp(-k * excess_capacities)` would prioritize zero excess.\n        # `score = exp(-k * abs(excess_capacities - ideal_gap))` peaks at ideal_gap.\n\n        # Let's try a simpler, more direct approach for raw scores:\n        # 1. Prioritize exact fits (excess_capacity = 0).\n        # 2. Then prioritize small positive excess_capacities.\n        # 3. Penalize large excess_capacities.\n\n        # Score for \"best fit\" (minimal remaining space after packing):\n        # `-excess_capacities` naturally prioritizes bins that leave the least space.\n        # This is effectively 'Best Fit'.\n        best_fit_score = -excess_capacities\n\n        # Score for \"worst fit reduction\" (penalize bins that are almost empty):\n        # We want to down-weight bins where `bins_remain_cap[fits_mask]` is very large.\n        # A penalty based on `bins_remain_cap[fits_mask]` itself.\n        # `penalty = - bins_remain_cap[fits_mask]` would prioritize smaller bins overall.\n        # This could be combined with best fit. For example, `score = -excess_capacities - penalty_factor * bins_remain_cap[fits_mask]`\n        # Or, we can have a score that is good for small `excess_capacities` and also for smaller original bin capacities.\n\n        # Let's define a score that balances these.\n        # A score that is high for small `excess_capacities` and also for bins that are not excessively large.\n        # A common approach for \"best fit\" is `-excess_capacities`.\n        # For \"worst fit reduction\", we want to penalize bins with high `bins_remain_cap`.\n        # Let's try: `score = -excess_capacities - penalty_weight * bins_remain_cap[fits_mask]`\n        # This prioritizes minimal excess and smaller original bin sizes.\n\n        penalty_weight = 0.1 # How much to penalize large bins.\n        # The combined score:\n        combined_scores = -excess_capacities - penalty_weight * bins_remain_cap[fits_mask]\n\n        # Softmax transformation to get probabilities.\n        # Softmax requires non-infinite values and usually works better with scaled values.\n        # Let's scale `combined_scores` to avoid very large/small exponents.\n        # A simple scaling by a factor can help.\n        scaling_factor = 1.0 # Adjust as needed.\n        scaled_scores = scaling_factor * combined_scores\n\n        # Apply softmax\n        # `exp(scaled_scores)` will give relative weights.\n        # Normalize these weights to sum to 1.\n        raw_scores[fits_mask] = scaled_scores\n\n    # If no bins fit, raw_scores remains [-inf, -inf, ...]\n    # Softmax will handle this by assigning 0 probability to all bins if all are -inf.\n    # If at least one bin fits, we get probabilities.\n\n    # Use a stable softmax implementation\n    # max_score = np.max(raw_scores) # This could be problematic if all are -inf\n    # exp_scores = np.exp(raw_scores - max_score) # Subtract max for numerical stability\n    # probabilities = exp_scores / np.sum(exp_scores)\n\n    # A more direct way is to use log_softmax if we want log-probabilities,\n    # but here we want the relative scores that softmax produces.\n    # Let's compute the probabilities directly.\n    # If all raw_scores are -inf, np.exp will be 0, sum will be 0, leading to NaNs.\n    # Handle the case where no bins fit.\n    if not np.any(fits_mask):\n        return np.zeros_like(bins_remain_cap) # No bins can fit the item.\n\n    # Ensure we don't have NaN from softmax if all scores are -inf (already handled by checking fits_mask)\n    # but also ensure no NaNs from `np.exp(large negative)` if `raw_scores` are very negative.\n    # `np.exp` of very negative numbers becomes zero.\n    exp_scores = np.exp(raw_scores)\n    sum_exp_scores = np.sum(exp_scores)\n\n    if sum_exp_scores == 0:\n        # This might happen if all `raw_scores` are extremely negative.\n        # Assign uniform probability if scores are indistinguishable due to underflow.\n        return np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n    else:\n        probabilities = exp_scores / sum_exp_scores\n        return probabilities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "SLOC": 27.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response4.txt_stdout.txt",
    "code_path": "problem_iter5_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Sigmoid Fit Score.\n\n    This heuristic aims to prioritize bins that offer the \"best fit\" for the item,\n    defined as a bin where the remaining capacity is only slightly larger than the\n    item's size. This strategy tries to fill bins as much as possible without leaving\n    excessive empty space, thereby minimizing fragmentation.\n\n    Additionally, it incorporates a \"worst fit reduction\" concept by slightly penalizing\n    bins that have a very large remaining capacity, pushing towards using bins that\n    are closer to being full if a good fit isn't available, but still prioritizing\n    exact or near-exact fits.\n\n    The priority is calculated using a sigmoid function for near-exact fits and a\n    linear decay for larger excesses to slightly favor smaller remaining capacities\n    among the non-ideal fits.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element is the remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as `bins_remain_cap`, containing the priority\n        score for each bin. Higher scores indicate a more desirable bin for the item.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit.\n    fits_mask = bins_remain_cap >= item\n\n    if np.any(fits_mask):\n        fitting_caps = bins_remain_cap[fits_mask]\n        excess_capacities = fitting_caps - item\n\n        # --- Refined Priority Calculation ---\n        # Objective 1: Prioritize exact or near-exact fits (minimal positive excess capacity)\n        # We use a sigmoid centered slightly above zero to give a boost to bins with a small positive gap.\n        ideal_gap = 0.05  # Target for the best fit (small positive remaining capacity)\n        sigmoid_steepness = 20.0 # Higher steepness for a sharper peak around ideal_gap\n\n        # Calculate scores for the \"best fit\" aspect.\n        # sigmoid(steepness * (ideal_gap - excess_capacity)) -> peak at ideal_gap\n        best_fit_scores = 1 / (1 + np.exp(-sigmoid_steepness * (ideal_gap - excess_capacities)))\n\n        # Objective 2: Worst Fit Reduction - slightly penalize very large remaining capacities\n        # This encourages using bins that are closer to full if an exact fit isn't found.\n        # We can achieve this by slightly lowering the priority of bins with large excess_capacities.\n        # A simple linear decay or another sigmoid-like function can work.\n        # Let's use a transformation that gives higher scores to smaller excess capacities.\n        # We want to reduce the advantage of bins with very large excess.\n        # A simple approach is to invert the excess capacity (scaled) and add it.\n        # We need to be careful not to let this override the best-fit priority.\n\n        # Normalize excess capacities to a reasonable range for scaling.\n        # Clamp to avoid issues with extremely large or negative (already handled by fits_mask) capacities.\n        max_relevant_capacity = 5.0 # Consider capacities up to this as relevant for this penalty\n        scaled_excess = np.clip(excess_capacities, 0, max_relevant_capacity) / max_relevant_capacity\n\n        # Penalty term: higher score for smaller scaled_excess.\n        # Using 1 - scaled_excess means bins with excess close to 0 get a score of 1,\n        # and bins with excess close to max_relevant_capacity get a score of 0.\n        # We want this to be a *reduction* from the best_fit_score, so we can add a small negative component,\n        # or modulate the best_fit_score.\n        #\n        # A simpler approach: linearly map excess capacity to a penalty factor.\n        # Excess 0 -> penalty 1 (no reduction)\n        # Excess max_relevant_capacity -> penalty 0 (max reduction)\n        penalty_factor = np.maximum(0, 1 - scaled_excess) # ensures it's between 0 and 1\n\n        # Combine best-fit score with a penalty reduction.\n        # The penalty factor reduces the score when excess capacity is large.\n        # We can think of this as: priority = best_fit_score * (1 - penalty_weight * penalty_factor)\n        # Or more directly: priority = best_fit_score - penalty_weight * scaled_excess\n        # Let's try a combination that ensures smaller excess is generally better,\n        # while still prioritizing near-zero excess.\n\n        # Option: Summation with weights, adjusted for the desired behavior.\n        # Higher weight for best-fit.\n        best_fit_weight = 1.0\n        least_waste_weight = 0.5 # Encourages smaller waste, but less than perfect fit\n\n        # Score based on how close to zero the excess is.\n        # We can use `1 / (1 + excess_capacity)` for a decay.\n        # Or a normalized inverse.\n        # Let's try to make priority decrease as excess_capacity increases.\n        # We want to avoid the extreme penalty of a simple inverse for large excesses.\n        # Let's use a sigmoid for the \"least waste\" aspect too, but centered differently or scaled.\n        #\n        # Simpler strategy: Combine the best_fit_score with a term that favors smaller waste.\n        # A bin with excess 0 is better than excess 0.01, which is better than excess 0.1.\n        # The sigmoid already handles the \"peak\" at ideal_gap. For excess > ideal_gap,\n        # the sigmoid score drops. We can amplify this drop slightly.\n\n        # Let's define a score that directly favors smaller excess, but capped.\n        # For excess > ideal_gap, we want a smooth decrease.\n        # Consider `1 / (1 + excess_capacity * scale_factor)`.\n        # If excess_capacity is 0, score is 1. If excess_capacity is 1, score is 0.5.\n        # If excess_capacity is 0.1, score is 1/(1+0.1) = 0.9.\n        # This might be too strong.\n\n        # Let's try a direct combination: Best fit + a bonus for minimal waste.\n        # The best_fit_scores are already high for small excess.\n        # We can introduce a \"least waste\" bonus.\n        # Minimal waste means excess_capacity close to 0.\n        # Let's add a term that is high for small excess_capacity and decays.\n        # Use another sigmoid, but one that peaks at 0 excess.\n        least_waste_steepness = 5.0\n        least_waste_scores = 1 / (1 + np.exp(least_waste_steepness * excess_capacities))\n        # This gives 1 for 0 excess, ~0.5 for excess=0, and decays.\n\n        # Combine: Prioritize the best fit, and add a bonus for minimal waste.\n        # The best_fit_scores already give high values to excess ~ ideal_gap.\n        # We want to ensure that excess = 0 (if it exists) is also highly rated, and potentially higher.\n        #\n        # Let's refine `best_fit_scores` to be more peaky at `ideal_gap` and then decay.\n        # The current sigmoid `1/(1+exp(-steepness*(ideal_gap - excess)))` means:\n        # excess < ideal_gap -> score > 0.5\n        # excess = ideal_gap -> score = 0.5\n        # excess > ideal_gap -> score < 0.5\n        # This isn't quite right for \"peak priority at ideal_gap\".\n        # For peak priority at `ideal_gap`, the argument should be 0.\n        # `f(x) = 1 / (1 + exp(-k * (x - c)))` has midpoint at `c`.\n        # To peak at `ideal_gap`, we want `ideal_gap` to map to the maximum.\n        # If we invert the argument or shift it:\n        # `sigmoid(steepness * (excess_capacities - ideal_gap))` has midpoint at `ideal_gap`.\n        # This will give scores that *increase* as excess increases. Not good.\n        #\n        # Correct sigmoid for peak at `ideal_gap` for `excess_capacities`:\n        # Consider a Gaussian-like shape, or a different function.\n        # Or adjust the sigmoid's range.\n        # If we want scores to be highest at `ideal_gap`, we want the argument to `exp` to be minimal (most negative) at `ideal_gap`.\n        # Let the argument be `-steepness * abs(excess_capacities - ideal_gap)`.\n        # This is not a sigmoid.\n        #\n        # Let's stick to the idea of `sigmoid(steepness * (center - x))`.\n        # If `center = ideal_gap`, scores increase with `excess`.\n        # If we want scores to *decrease* from a peak at `ideal_gap`:\n        # Consider `1 - sigmoid(...)`. `1 - sigmoid(x)` is a sigmoid itself, just inverted.\n        # `1 - sigmoid(steepness * (excess_capacities - ideal_gap))`\n        # This peaks at 1 when `excess_capacities = ideal_gap`.\n\n        # Re-evaluate the sigmoid for best fit:\n        # We want a high score when `excess_capacity` is close to `ideal_gap`.\n        # Let `score = exp(-k * (excess_capacity - ideal_gap)^2)` - Gaussian, but not a sigmoid.\n        #\n        # Let's use the sigmoid for a smooth transition.\n        # A common way to get a peak is to use a logistic function centered on the desired value.\n        # `sigmoid(steepness * (ideal_gap - excess_capacity))`\n        # This gives priority values that decrease from 1 (at `excess_capacity << ideal_gap`) to 0 (at `excess_capacity >> ideal_gap`), with a transition around `ideal_gap`.\n        # This means it prioritizes bins with *less* excess capacity.\n        #\n        # To prioritize `ideal_gap` specifically:\n        # We can use a \"bell curve\" shape, or combine two sigmoids.\n        # For simplicity and staying within sigmoid framework, let's modify the `sigmoid` to map values.\n        # `sigmoid(x)` ranges from 0 to 1. We want a function `g(excess_capacity)` that is max at `ideal_gap`.\n        #\n        # Let's define a priority score that captures:\n        # 1. High priority for `excess_capacity` near `ideal_gap`.\n        # 2. Decent priority for `excess_capacity == 0` (exact fit).\n        # 3. Priority decreases as `excess_capacity` increases.\n\n        # Re-thinking: Prioritize \"best fit\" = minimal positive remainder.\n        # Then, prioritize \"least waste\" = minimal non-negative remainder.\n        # So, `excess_capacity = 0` is good, `excess_capacity = 0.01` is better, `excess_capacity = 0.1` is okay, `excess_capacity = 1` is bad.\n\n        # Let's assign scores based on the magnitude of `excess_capacity`:\n        # Small excess_capacity (near 0) should be penalized slightly less than larger ones.\n        # This sounds like a decreasing function of `excess_capacity`.\n\n        # Priority = 1 / (1 + `excess_capacity`^p) or similar decay.\n        # `excess_capacity` itself is a strong indicator.\n        # `sigmoid(k * (ideal_gap - excess_capacity))` already makes it decrease.\n        # Let's use `ideal_gap = 0` for absolute minimal waste, and then consider the \"slightly larger\" aspect.\n\n        # --- Revised Strategy ---\n        # 1. Exact fit (`excess_capacity == 0`) is best.\n        # 2. Near-exact fit (`excess_capacity` small positive) is also very good.\n        # 3. Larger excess is progressively worse.\n\n        # We can use a function that maps `excess_capacity` to priority.\n        # Let `p(x)` be the priority for excess `x`.\n        # `p(0)` should be high.\n        # `p(small_positive)` should be high, potentially even higher than `p(0)`.\n        # `p(large_positive)` should be low.\n\n        # Let's use a function `f(x) = exp(-k * x)`.\n        # `f(0) = 1`. `f(small_positive)` < 1. This favors exact fits.\n        # To favor `ideal_gap`: `f(x) = exp(-k * abs(x - ideal_gap))`.\n        # This gives a peak at `ideal_gap`, but the function decays on both sides.\n        # This is closer to the original description's intent (\"slightly larger\").\n\n        # Let's use `sigmoid(steepness * (center - x))` where `center` is `ideal_gap`.\n        # `priority = sigmoid(steepness * (ideal_gap - excess_capacities))`\n        # This gives high scores for `excess_capacities < ideal_gap` and low for `excess_capacities > ideal_gap`.\n        # The peak of this function is not at `ideal_gap` itself, it's where the argument is 0.\n        # The *midpoint* of the sigmoid is at `ideal_gap`.\n        #\n        # If we want the highest score to be associated with `ideal_gap`, we can use the sigmoid's output as a base.\n        #\n        # Let's consider `excess_capacity` directly:\n        # Score = 1 / (1 + `excess_capacity`**power)\n        # If power=1, Score = 1 / (1 + excess_capacity). Decays from 1 to 0.\n        # If power=2, Score = 1 / (1 + excess_capacity**2). Decays faster.\n        # This favors smaller excesses.\n\n        # Let's try a combination that specifically targets \"minimal excess\" and then \"least waste\".\n        # Minimal excess = `excess_capacity` near 0.\n        # Least waste = `excess_capacity` is small.\n\n        # We can use a two-part function or a single function that behaves piecewise.\n        # `priority(excess_capacity)`:\n        # If `excess_capacity` < `ideal_gap`: `sigmoid(steepness_early * (ideal_gap - excess_capacity))` (favoring smaller gaps)\n        # If `excess_capacity` >= `ideal_gap`: `sigmoid(steepness_late * (excess_capacity - ideal_gap))` (penalizing larger gaps)\n        # This creates a peak at `ideal_gap`.\n\n        # Let's use `ideal_gap = 0.01` (a very small positive value).\n        ideal_gap = 0.01\n        steepness_early = 20.0 # Steep decline for small excesses below ideal_gap\n        steepness_late = 10.0  # Gradual decline for excesses above ideal_gap\n\n        # For excess < ideal_gap, we want higher scores as excess approaches 0.\n        # `sigmoid(steepness * (ideal_gap - excess))` gives scores that DECREASE as excess increases.\n        # This is correct for \"preferring less excess\".\n        # So, for `excess < ideal_gap`, `sigmoid(steepness_early * (ideal_gap - excess))` is good.\n\n        # For excess >= ideal_gap, we want scores to decrease as excess increases.\n        # `sigmoid(steepness * (ideal_gap - excess))` still works for this part.\n        # The transition is what matters.\n\n        # Let's use a single sigmoid that peaks appropriately.\n        # Consider a function that is high for `excess = 0` and `excess = ideal_gap`, and lower in between or for larger values.\n        #\n        # How about:\n        # Priority = `(1 - sigmoid(steepness_1 * excess))`  # Prioritizes exact fits (score 1 at excess=0)\n        #            + `sigmoid(steepness_2 * (ideal_gap - excess))` # Bonus for near-exact fits\n        # Need to normalize weights.\n\n        # Let's try a different perspective: the \"worst fit reduction\" aims to pick the bin\n        # that *least* increases the total waste.\n        # If we have bins with capacities [10, 5, 2] and item is 3:\n        # Bin 1: remains [7, 5, 2]. Waste = 7.\n        # Bin 2: remains [10, 2, 2]. Waste = 10.\n        # Bin 3: remains [10, 5, -1]. Invalid.\n        # Worst fit would pick bin 2.\n\n        # For online BPP with the goal of minimizing bins, we want to fill bins.\n        # Prioritize \"best fit\" for tight packing.\n        # If no good fit, then \"first fit\" or \"worst fit\" among valid bins.\n        #\n        # The reflection says: \"Prioritize exact fits, then minimal residual space. Combine 'best fit' with 'worst fit reduction'.\"\n        #\n        # Minimal residual space = `excess_capacity` close to 0.\n        # \"Worst fit reduction\" can mean: among bins that are NOT near-perfect fits, pick the one that is \"least empty\".\n        # This means picking the bin with largest remaining capacity among those that aren't \"best fits\".\n        # This sounds contradictory to minimizing total waste.\n\n        # Let's interpret \"worst fit reduction\" as: \"Don't leave *too much* excess if a better option exists\".\n        # OR: \"Among bins that aren't perfect/near-perfect fits, pick the one that is relatively fullest.\"\n        #\n        # The current `priority_v1` is essentially a \"best fit\" heuristic.\n        #\n        # Let's focus on the \"refined Sigmoid Fit Score\".\n        # The description for v1 implies a peak at a slight positive excess.\n        # `sigmoid(steepness * (ideal_gap - excess_capacity))`\n        # This function maps `ideal_gap` to 0.5.\n        # For `excess < ideal_gap`, scores > 0.5.\n        # For `excess > ideal_gap`, scores < 0.5.\n        # So, it favors LESS excess capacity.\n\n        # To make it peak at `ideal_gap`:\n        # We need a function that is maximal at `ideal_gap`.\n        # Let's scale the `excess_capacities` and use a Sigmoid that peaks.\n        # A common way to model this is using a modified logistic function or a bell curve.\n        #\n        # Consider a function `f(x)` where `x = excess_capacity`.\n        # Peak at `ideal_gap`.\n        # `f(x) = exp(-k * (x - ideal_gap)**2)` (Gaussian)\n        #\n        # Alternative sigmoid approach:\n        # Map `excess_capacity` to a range, then apply sigmoid.\n        # `priorities[fits_mask] = sigmoid(steepness, center=ideal_gap)` applied to `excess_capacities`.\n        # The `sigmoid` function is `1 / (1 + exp(-steepness * (x - center)))`.\n        # This means `center` is where the sigmoid value is 0.5.\n        #\n        # If we want peak at `ideal_gap`, we can use `1 - sigmoid(steepness * (excess - ideal_gap))`\n        # `1 - (1 / (1 + exp(-steepness * (excess - ideal_gap))))`\n        # `= exp(-steepness * (excess - ideal_gap)) / (1 + exp(-steepness * (excess - ideal_gap)))`\n        # This is the logistic function `logistic(steepness * (ideal_gap - excess))`.\n        # This function peaks at `ideal_gap`.\n\n        # Let's use `logistic(k * (ideal_gap - x))`\n        def logistic(x, steepness=10.0):\n            return 1 / (1 + np.exp(-steepness * x))\n\n        # Priority score where the peak is at `ideal_gap`.\n        # Scores are higher for `excess_capacity` closer to `ideal_gap`.\n        # For `excess_capacity` < `ideal_gap`, scores will be > 0.5.\n        # For `excess_capacity` > `ideal_gap`, scores will be < 0.5.\n        # This aligns with \"minimal residual space\" and prioritizing slight gaps.\n        ideal_gap = 0.01 # Very small positive remainder is ideal.\n        steepness = 25.0\n\n        # We want priority to be high when excess_capacity is close to ideal_gap.\n        # Function: logistic(steepness * (ideal_gap - excess_capacity))\n        # This peaks at 1 when excess_capacity = ideal_gap.\n        # When excess_capacity = 0, score = logistic(steepness * ideal_gap) (slightly less than 1)\n        # When excess_capacity = 2*ideal_gap, score = logistic(-steepness * ideal_gap) (less than 0.5)\n        # This favors small positive gaps over exact fits, which is what \"slightly larger\" implies.\n\n        priority_scores = logistic(steepness * (ideal_gap - excess_capacities))\n\n        # Now, incorporate \"worst fit reduction\".\n        # This part is tricky to interpret as a *reduction* while also prioritizing *minimal* residual space.\n        # If \"worst fit reduction\" means: don't pick a bin with excessively large remaining capacity if other good options exist.\n        # The current scores already decay for larger excesses.\n        #\n        # Let's consider a slight adjustment:\n        # For bins with `excess_capacity > some_threshold`, slightly reduce their score to favor bins that are generally fuller.\n        #\n        # If we want to also capture \"least waste\" as a secondary objective:\n        # Use a combined score.\n        # Score = w1 * (priority for ideal_gap) + w2 * (priority for minimal excess)\n        #\n        # Let's re-read: \"Prioritize exact fits, then minimal residual space. Combine 'best fit' with 'worst fit reduction'.\"\n        # Minimal residual space = minimal `excess_capacity`.\n        # Best fit = `excess_capacity` near 0 or `ideal_gap`.\n        # Worst fit reduction: This usually means choosing the bin with the largest remaining capacity.\n        # Applying a \"reduction\" to this could mean making it less attractive, or capping its benefit.\n        #\n        # If we want to combine:\n        # 1. High score for `excess_capacity` near 0.\n        # 2. High score for `excess_capacity` near `ideal_gap`.\n        # 3. Lower scores for larger `excess_capacity`.\n        #\n        # Let's try a score that is a weighted sum of two components:\n        # Component A: Favoring `ideal_gap` (slight positive remainder).\n        # Component B: Favoring exact fits (zero remainder).\n        # Component C: Penalizing large remainders (worst fit reduction).\n\n        # Component A: Use the logistic function as defined above.\n        # `score_A = logistic(steepness_ideal * (ideal_gap - excess_capacities))`\n        ideal_gap_for_peak = 0.01\n        steepness_peak = 30.0\n        score_A = logistic(steepness_peak * (ideal_gap_for_peak - excess_capacities))\n\n        # Component B: Favoring exact fits (zero remainder).\n        # Use a steep sigmoid for excess=0.\n        # `score_B = sigmoid(steepness_zero * (0 - excess_capacities))`\n        # `score_B = sigmoid(-steepness_zero * excess_capacities)`\n        steepness_zero = 20.0\n        score_B = logistic(steepness_zero * (0 - excess_capacities)) # same as sigmoid(steepness_zero * excess) but inverted\n\n        # Wait, `logistic(x)` increases. `logistic(-x)` decreases.\n        # We want score_B to be high for excess=0, decrease for excess>0.\n        # So `logistic(steepness * (0 - excess))` or `sigmoid(steepness * (0 - excess))`.\n        # `score_B = logistic(-steepness_zero * excess_capacities)` is good.\n        # This gives 0.5 at excess=0 and decays.\n\n        # Combine A and B:\n        # We want `ideal_gap` to be the absolute peak, followed by 0 excess, then decay.\n        # `score_A` peaks at `ideal_gap`. `score_B` peaks at 0.\n        # If `ideal_gap` is small and positive, `score_A` will be higher than `score_B` at `ideal_gap`.\n        # At `excess = 0`: `score_A = logistic(steepness_peak * (ideal_gap))` (slightly < 1)\n        #                `score_B = logistic(0)` = 0.5\n        # This means `score_A` will be higher at 0 as well if `ideal_gap` is small.\n        #\n        # Let's give `score_A` more weight.\n        weight_A = 1.0\n        weight_B = 0.5 # Give less weight to exact fit bonus\n\n        combined_score = weight_A * score_A + weight_B * score_B\n\n        # Component C: \"Worst fit reduction\".\n        # This implies that if there are many bins with large excess capacities,\n        # we might want to pick the one with the largest capacity to keep smaller capacities for later small items.\n        # This is the core of \"worst fit\".\n        # \"Worst fit reduction\" might mean: DO NOT pick the bin with the largest excess if a near-perfect fit exists.\n        # Or: Slightly penalize bins with large excess to encourage fuller bins overall.\n\n        # Let's adjust the combined score to penalize large excesses more.\n        # If `excess_capacity` is large, the current scores already drop.\n        # We can add a term that aggressively reduces scores for large excesses.\n        #\n        # Let's define a \"penalty\" for large excess.\n        # `penalty = sigmoid(steepness_penalty * (excess_capacities - large_threshold))`\n        # This penalty is 0.5 at `large_threshold`, and increases.\n        # We want to SUBTRACT this penalty.\n        #\n        # Example: Item = 1, bins = [10, 10, 1.1]\n        # Excess: [9, 9, 0.1]\n        # score_A for 0.1: logistic(30 * (0.01 - 0.1)) = logistic(-2.7) ~ 0.06\n        # score_B for 0.1: logistic(-20 * 0.1) = logistic(-2.0) ~ 0.12\n        # Combined: 1.0 * 0.06 + 0.5 * 0.12 = 0.06 + 0.06 = 0.12. This is low.\n\n        # Let's flip the logistic usage:\n        # Prioritize minimal residual space: favor `excess_capacity` close to 0.\n        # `logistic(-steepness * excess_capacity)` -> Peaks at 0.5 for excess=0, decays.\n        # Prioritize slight positive gap: favor `excess_capacity` close to `ideal_gap`.\n        # `logistic(steepness * (ideal_gap - excess_capacity))` -> Peaks at 1 for `ideal_gap`.\n\n        # Let's adjust the structure:\n        # Bin remaining capacity `c`. Item size `i`. Bin capacity `C`. `excess = c - i`.\n        #\n        # Heuristic:\n        # 1. Try to find bins where `c - i` is small and positive (e.g., `0 < c - i < 0.1`).\n        # 2. If not found, try to find bins where `c - i = 0`.\n        # 3. If not found, consider other bins.\n\n        # Score function `S(excess)`:\n        # S(0) = high\n        # S(small_positive) = very high (peak)\n        # S(larger_positive) = medium, decreasing\n        # S(very_large_positive) = low\n\n        # Let's refine the \"peak at `ideal_gap`\" part.\n        # `logistic(steepness * (ideal_gap - excess))` gives scores that are:\n        # `ideal_gap`: 1.0\n        # `0`: logistic(steepness * ideal_gap) (near 1 for small ideal_gap)\n        # `very large`: close to 0.\n        # This doesn't quite distinguish between 0 and ideal_gap as well as we might like.\n\n        # Let's try a different combination using sigmoids:\n        # Score = `w1 * sigmoid(s1 * (ideal_gap - excess))`  # Favors excess < ideal_gap\n        #       + `w2 * sigmoid(s2 * (0 - excess))`        # Favors excess == 0\n\n        # Let's use the `priority_v1` structure but enhance it for \"worst fit reduction\".\n        # The `priority_v1` uses `sigmoid(steepness * (ideal_gap - excess_capacities))`.\n        # This already penalizes large excesses.\n        # \"Worst fit reduction\" in this context might mean: if there are multiple bins with similar \"good fit\" scores,\n        # prefer the one that is \"least empty\" among them, or penalize those that are \"most empty\" among the good fits.\n        #\n        # Consider bins with `excess_capacity` within a certain range around `ideal_gap`.\n        # Among these, pick the one with minimal `excess_capacity`.\n\n        # Let's apply a bonus for minimal waste (excess=0) and a penalty for large waste.\n        # Base priority = `logistic(steepness * (ideal_gap - excess_capacities))` (peaks at ideal_gap)\n        # Bonus for exact fit = `logistic(-steepness_zero * excess_capacities)` (peaks at 0)\n        # Penalty for large excess = `sigmoid(steepness_penalty * (excess_capacities - penalty_threshold))`\n\n        # Let's try a simpler approach inspired by Best Fit Decreasing (BFD) for online.\n        # The \"residual space\" matters.\n        # Consider the ratio `item_size / bin_capacity`.\n        # Or `bin_remaining_cap / bin_capacity`.\n        #\n        # The goal is to minimize the number of bins.\n        # So, we want to fill bins as much as possible.\n        # This means minimizing `excess_capacity`.\n        #\n        # So, a priority function that decreases with `excess_capacity` is good.\n        # `f(excess) = 1 / (1 + excess)`\n        # `f(excess) = exp(-k * excess)`\n        #\n        # Let's return to the interpretation of \"best fit\" as minimal excess.\n        # We want to prioritize bins where `excess_capacity` is smallest.\n        #\n        # \"Prioritize exact fits, then minimal residual space.\"\n        # This implies:\n        # 1. `excess_capacity = 0` is best.\n        # 2. `excess_capacity` slightly > 0 is next best.\n        # 3. `excess_capacity` larger is worse.\n        #\n        # This is what `logistic(-k * excess)` combined with `logistic(k * (ideal_gap - excess))` could achieve.\n\n        # Let's define the score `S(excess)` as:\n        # `S(excess) = a * logistic(-s1 * excess) + b * logistic(s2 * (ideal_gap - excess))`\n        # `a` and `b` are weights, `s1` and `s2` are steepness.\n        # `ideal_gap = 0.01`\n\n        steepness_exact = 20.0\n        steepness_gap = 25.0\n        ideal_gap = 0.01\n\n        # Score component for exact fit (peaks at excess=0)\n        score_exact = logistic(-steepness_exact * excess_capacities)\n\n        # Score component for slight gap (peaks at excess=ideal_gap)\n        score_gap = logistic(steepness_gap * (ideal_gap - excess_capacities))\n\n        # Combine them. We want exact fit to be good, slight gap to be even better.\n        # If we want `ideal_gap` to be the absolute maximum score:\n        # This requires careful weighting.\n        # If `excess = ideal_gap`: `score_exact` = logistic(-s1 * ideal_gap) ( < 0.5)\n        #                         `score_gap` = logistic(0) = 0.5.\n        # This doesn't work as intended. The logistic function with `(ideal_gap - excess)` peaks at 1.\n        # The logistic function with `-steepness * excess` peaks at 0.5.\n\n        # Let's use the `priority_v1` structure and modify it to ensure the peak is indeed at `ideal_gap`.\n        # The function `f(x) = sigmoid(steepness * (center - x))` has its midpoint at `center`.\n        # If we want peak priority AT `ideal_gap`, then `ideal_gap` should map to a high value.\n        #\n        # Let's use the original `priority_v1` concept: prioritizing bins where remaining capacity is *slightly* larger.\n        # `excess_capacities = bins_remain_cap[fits_mask] - item`\n        # We want to maximize a function `P(excess_capacity)`.\n        # `P(0)` should be high.\n        # `P(ideal_gap)` should be the highest.\n        # `P(large_excess)` should be low.\n\n        # Let's define `P(x)` as:\n        # `P(x) = 1 / (1 + exp(-k * (x - ideal_gap)^2))`  -- Gaussian-like\n        # This isn't directly a sigmoid.\n\n        # Let's try to combine two sigmoids carefully.\n        # 1. `sigmoid_exact = sigmoid(steepness_exact * (0 - excess_capacities))` -> Favors excess close to 0.\n        # 2. `sigmoid_gap = sigmoid(steepness_gap * (ideal_gap - excess_capacities))` -> Favors excess close to `ideal_gap`.\n        #\n        # The original `priority_v1` function IS using `sigmoid(steepness * (ideal_gap - excess_capacities))`.\n        # Let's analyze that:\n        # `ideal_gap = 0.05`, `steepness = 10`.\n        # `excess = 0`: `sigmoid(10 * (0.05 - 0)) = sigmoid(0.5) \u2248 0.62`\n        # `excess = 0.05`: `sigmoid(10 * (0.05 - 0.05)) = sigmoid(0) = 0.5`\n        # `excess = 0.1`: `sigmoid(10 * (0.05 - 0.1)) = sigmoid(-0.5) \u2248 0.38`\n        # `excess = 0.02`: `sigmoid(10 * (0.05 - 0.02)) = sigmoid(0.3) \u2248 0.57`\n        #\n        # This function *decreases* from 0.62 at excess=0 to 0.5 at excess=0.05 and further down.\n        # It does NOT peak at `ideal_gap`. It prioritizes smaller excesses, with a slight bias.\n\n        # To peak at `ideal_gap`:\n        # We need a function that is high for `ideal_gap`, and lower for `0` and `larger_excess`.\n        # A \"bell curve\" shape.\n        # `exp(-(excess - ideal_gap)^2 / sigma^2)`\n        #\n        # Using sigmoids to approximate a peak:\n        # Priority = `sigmoid(k1 * (ideal_gap - excess)) * sigmoid(k2 * (excess - 0))`\n        # This gives `sigmoid(k1*ideal_gap) * sigmoid(0)` at `excess=0`.\n        # And `sigmoid(0) * sigmoid(k2 * ideal_gap)` at `excess=ideal_gap`.\n        # This doesn't directly yield a peak.\n\n        # Let's reconsider the \"worst fit reduction\".\n        # If we have bins [10, 10, 10, 10, 10] and item = 1.\n        # Excesses are all 9.\n        # If we have bins [5, 5, 5, 5, 5] and item = 4.\n        # Excesses are all 1.\n        #\n        # \"Worst fit reduction\": Amongst bins that are *not* best fits, pick the one with the largest remaining capacity.\n        # This means that if `ideal_gap` fit is not found, and exact fit is not found, then among the remaining,\n        # pick the one that is \"least full\". This is counter-intuitive for minimizing bins.\n        #\n        # Alternative interpretation: Prioritize small excesses, and if there are multiple bins with similar small excesses,\n        # prefer the one that has a larger capacity (to potentially save smaller bins for smaller items later).\n        #\n        # Let's try to model:\n        # Score = `f(excess_capacity)`\n        # `f(0)` is good.\n        # `f(ideal_gap)` is best.\n        # `f(excess)` decreases as `excess` increases.\n        #\n        # Function: `1 / (1 + (excess - ideal_gap)^2)` ? Not sigmoid.\n        #\n        # Let's refine the sigmoid approach.\n        # We want a function that:\n        # - Has a maximum value.\n        # - The maximum value occurs at `ideal_gap`.\n        # - Values decrease as `excess_capacity` moves away from `ideal_gap` in either direction (but especially for larger positive excess).\n        #\n        # Let `score = sigmoid(steepness * (ideal_gap - excess))` for `excess <= ideal_gap`.\n        # And `score = sigmoid(steepness * (excess - ideal_gap))` for `excess > ideal_gap`, but inverted.\n        #\n        # Consider the function: `f(x) = sigmoid(k * (ideal_gap - x))`\n        # At `x = ideal_gap`, `f(ideal_gap) = sigmoid(0) = 0.5`.\n        # This is the center of the sigmoid.\n        #\n        # To get a peak AT `ideal_gap`, we need the function to be maximal there.\n        #\n        # Final attempt at interpretation:\n        # Priority = `WeightBestFit * ScoreBestFit` + `WeightLeastWaste * ScoreLeastWaste`\n        # `ScoreBestFit`: High for `excess` near `ideal_gap`.\n        # `ScoreLeastWaste`: High for `excess` near `0`.\n        #\n        # `ScoreBestFit = logistic(steepness_ideal * (ideal_gap - excess))`\n        # `ScoreLeastWaste = logistic(-steepness_zero * excess)`\n        #\n        # If `ideal_gap = 0.01`:\n        # excess=0: SF=logistic(-s_z*0) = 0.5; SB=logistic(s_i*0.01) (near 1)\n        # excess=0.01: SF=logistic(-s_z*0.01) (near 0.5); SB=logistic(0) = 0.5\n        #\n        # This still doesn't make `ideal_gap` the absolute peak if `s_i` is not chosen carefully relative to `s_z`.\n        #\n        # Let's assume \"minimal residual space\" is the MOST important factor.\n        # This means `excess_capacity` should be as small as possible.\n        #\n        # Priority = `f(excess_capacity)` where `f` is decreasing.\n        # `f(x) = 1 / (1 + x)`\n        # `f(x) = exp(-k*x)`\n        #\n        # The `priority_v1` is already quite good for this. It penalizes larger excesses.\n        #\n        # How to incorporate \"worst fit reduction\"?\n        # If we have bins [10, 20, 30] and item=5:\n        # Excesses: [5, 15, 25].\n        # Priority: `sigmoid(10 * (0.05 - excess))`\n        # excess=5: sigmoid(-4.95) ~ 0\n        # excess=15: sigmoid(-14.95) ~ 0\n        # excess=25: sigmoid(-24.95) ~ 0\n        #\n        # If we have bins [5, 6, 7] and item=4:\n        # Excesses: [1, 2, 3]\n        # Priority:\n        # excess=1: sigmoid(10 * (0.05 - 1)) = sigmoid(-9.5) ~ 0\n        # excess=2: sigmoid(10 * (0.05 - 2)) = sigmoid(-19.5) ~ 0\n        # excess=3: sigmoid(10 * (0.05 - 3)) = sigmoid(-29.5) ~ 0\n        #\n        # The steepness needs to be adjusted based on the expected range of `excess_capacities`.\n        # If `excess_capacities` can be large, `steepness` should be adjusted or the function modified.\n        #\n        # Let's consider the \"worst fit reduction\" as a way to break ties or modify scores for less ideal fits.\n        # If multiple bins have similar \"good\" scores, pick the one that's \"least empty\".\n        #\n        # Modified score: `priority = base_score + alpha * (bins_remain_cap[fits_mask] / MAX_CAPACITY)`\n        # Where `base_score` is the sigmoid score.\n        #\n        # Let's try:\n        # Base score: `logistic(-steepness_least_waste * excess_capacities)` -> Favors minimal waste (excess near 0)\n        # Bonus for \"least empty\" among these: `bonus = bins_remain_cap[fits_mask]`. Normalize this.\n        #\n        # Final approach based on reflection:\n        # 1. Prioritize exact fits (excess=0) => High score for excess=0.\n        # 2. Then minimal residual space (excess near 0) => High score for small positive excess.\n        # 3. Combine 'best fit' (minimal excess) with 'worst fit reduction' (prefer fuller bins among non-ideal).\n\n        # Let's prioritize minimal excess first.\n        # Score = `logistic(-steepness_minimal_waste * excess_capacities)`\n        # This function peaks at 0.5 for excess=0 and decreases.\n        #\n        # To make it \"prioritize exact fits, then minimal residual space\":\n        # We need the value at excess=0 to be higher than for slightly positive excess.\n        # But the reflection says \"then minimal residual space\".\n        # Let's assume \"exact fits\" is the primary goal.\n        #\n        # If `excess_capacity = 0`, assign a high score.\n        # If `0 < excess_capacity < threshold`, assign good scores.\n        # If `excess_capacity >= threshold`, assign lower scores.\n        #\n        # Let's try to model this with two components, weighted.\n        # Component 1: Exact fit bonus.\n        #   `score_exact = 1.0` if `excess == 0`, else `0.0`. (Hard threshold)\n        #   Or `score_exact = exp(-steepness_e * excess)` (Soft threshold)\n        # Component 2: Minimal residual space / Best fit.\n        #   `score_bestfit = logistic(steepness_bf * (ideal_gap - excess))`\n        #\n        # Combining them:\n        # `priority = w_exact * score_exact + w_bestfit * score_bestfit`\n        #\n        # Let's use soft thresholds for all.\n        steepness_exact_fit = 50.0 # Very steep near 0\n        steepness_best_fit = 20.0\n        ideal_gap = 0.02 # Small positive remainder\n\n        # Score for exact fit (high for excess=0, drops rapidly)\n        score_exact = logistic(steepness_exact_fit * (0 - excess_capacities))\n\n        # Score for best fit (high for excess near ideal_gap, decreases)\n        score_best_fit = logistic(steepness_best_fit * (ideal_gap - excess_capacities))\n\n        # Combine: Exact fit is paramount, followed by best fit.\n        # Give exact fit a higher weight.\n        weight_exact = 1.0\n        weight_bestfit = 0.7\n\n        # The combined score will be:\n        # At excess=0: `1.0*logistic(0) + 0.7*logistic(20*0.02)` = `1.0*0.5 + 0.7*logistic(0.4)`\n        #             `0.5 + 0.7 * (1 / (1 + exp(-0.4)))` = `0.5 + 0.7 * (1 / (1 + 0.67))`\n        #             `0.5 + 0.7 * (1 / 1.67)` = `0.5 + 0.7 * 0.598` = `0.5 + 0.418` = 0.918\n        # At excess=ideal_gap=0.02: `1.0*logistic(-50*0.02) + 0.7*logistic(0)`\n        #                         `1.0*logistic(-1.0) + 0.7*0.5`\n        #                         `0.268 + 0.35` = 0.618\n        # This gives higher priority to exact fits.\n\n        # \"Worst fit reduction\": among the \"best fit\" candidates, prefer fuller bins.\n        # If `score_best_fit` is similar, we want to boost bins with smaller `excess_capacities`.\n        # This is implicitly handled by `score_exact` and the way `score_best_fit` decays from `ideal_gap`.\n        #\n        # Let's add a term that specifically boosts smaller `excess_capacities` when `score_best_fit` is relevant.\n        # Consider the ratio `excess_capacities / bins_remain_cap[fits_mask]`. This isn't right.\n        #\n        # The reflection stated: \"Combine 'best fit' with 'worst fit reduction'.\"\n        # If \"best fit\" is minimal excess, then \"worst fit reduction\" might mean:\n        # Among bins that are NOT best fits, prefer the one that has the largest remaining capacity.\n        # This seems to imply a secondary sorting criteria or a blended score.\n\n        # Let's use a score that prioritizes minimal excess, and then adds a bonus for larger remaining capacity among those that are similar.\n        #\n        # Final Score = `f(excess_capacity, remaining_capacity)`\n        # Prioritize `excess_capacity` being small.\n        # If `excess_capacity` values are close, boost based on `remaining_capacity` (larger is better).\n\n        # Let `f(excess) = logistic(-steepness_minimal_waste * excess)`.\n        # This gives higher scores for smaller excesses.\n        # Let `g(rem_cap) = rem_cap / MAX_BIN_CAPACITY`. Add this as bonus.\n        # `priority = f(excess) + bonus_weight * g(remaining_capacity)`\n        #\n        # Let's pick `MAX_BIN_CAPACITY` dynamically or assume a constant.\n\n        # Let's assume max bin capacity is 1.0 for simplicity, or we need it passed in.\n        # For now, we can normalize by the maximum *available* capacity, but that's unstable.\n        # Let's normalize by a fixed expected max capacity if known, otherwise by a representative value.\n        # Assume max capacity is 1.0 for normalization scale.\n\n        # `priorities[fits_mask] = logistic(-steepness_minimal_waste * excess_capacities) \\\n        #                         + 0.3 * (bins_remain_cap[fits_mask] / 1.0)` # Normalize remaining capacity\n\n        steepness_minimal_waste = 25.0\n        bonus_weight = 0.3\n        assumed_max_capacity = 1.0 # Normalize remaining capacity by this value.\n\n        # Minimal waste score\n        score_minimal_waste = logistic(-steepness_minimal_waste * excess_capacities)\n\n        # Worst fit bonus (prefer fuller bins among those with similar minimal waste scores)\n        # If excess is small, remaining capacity is also small (close to item size).\n        # If remaining capacity is larger, it's less \"wasteful\" among options with small excess.\n        # This is confusing. \"Worst fit reduction\" implies penalizing large capacities.\n        #\n        # Let's assume the reflection means:\n        # 1. Prioritize bins where `excess_capacity` is minimal.\n        # 2. If there's a tie in minimal excess, pick the bin with largest `remaining_capacity`.\n        #\n        # This can be achieved by sorting by `(excess_capacity, -remaining_capacity)`.\n        # Our priority function should reflect this ordering.\n        #\n        # Let the score be dominated by `excess_capacity`.\n        # Use `1 / (1 + excess_capacity)` as a base.\n        # Then add a bonus for larger remaining capacity.\n        #\n        # Base score based on minimal waste:\n        # `base_score = 1 / (1 + excess_capacities)`\n        # Bonus for fuller bins:\n        # `bonus = bins_remain_cap[fits_mask]`\n        # Normalize bonus: `bonus_normalized = bonus / np.max(bonus)` (careful if only one bin)\n        # A fixed `MAX_CAPACITY` is better.\n\n        # Let's use the logistic function for minimal waste, and add the bonus.\n        # Minimal waste score: `logistic(-steepness * excess)`\n        # Bonus: Normalize remaining capacity. Higher is better.\n        # We want to penalize large remaining capacity for the \"reduction\" part.\n        #\n        # So, prioritize small excess, AND penalize large remaining capacity.\n        # This implies `excess` is the primary factor, and `remaining_capacity` is secondary,\n        # where higher `remaining_capacity` leads to LOWER priority (if we interpret \"reduction\" as penalty).\n\n        # Let's try to directly fulfill the interpretation:\n        # \"Prioritize exact fits, then minimal residual space.\"\n        # => Maximize `f(excess)` where `f(0)` is highest, and `f(x)` decreases for `x > 0`.\n        #\n        # \"Combine 'best fit' (minimal excess) with 'worst fit reduction'.\"\n        # If \"worst fit reduction\" means preferring fuller bins among equally-good fits.\n        #\n        # Let's take the minimal waste score `logistic(-steepness * excess_capacity)`.\n        # This handles minimal waste well.\n        # How to add \"worst fit reduction\" meaning \"prefer fuller bins\"?\n        # It means, if two bins have similar small `excess_capacity`, pick the one that is \"less empty\".\n        # This means picking the bin with the largest `remaining_capacity`.\n        #\n        # So, a score that uses `excess_capacity` as primary and `remaining_capacity` as secondary (descending).\n        # `score = Base(excess_capacity) + bonus(remaining_capacity)`\n        #\n        # `Base(excess) = logistic(-steepness * excess)`\n        # `bonus(rem_cap) = weight * (rem_cap / MAX_CAP)`\n        # This boosts bins with larger remaining capacity.\n        #\n        # `excess = rem_cap - item`. So `rem_cap = excess + item`.\n        # Maximizing `rem_cap` is equivalent to maximizing `excess` (if item is constant).\n        # This contradicts minimal waste.\n\n        # Re-reading again: \"Combine 'best fit' with 'worst fit reduction'\".\n        # This often means:\n        # - Best Fit: minimal `excess_capacity`.\n        # - Worst Fit: maximal `remaining_capacity`.\n        # - Reduction: Avoid the *worst* aspect of worst fit. The worst aspect is having large `remaining_capacity`.\n        #   So, if we *must* pick a bin that is not a \"best fit\", we should avoid picking one with *extremely* large `remaining_capacity`.\n        #   This implies that among non-best-fit bins, we prefer those with smaller `remaining_capacity` (i.e., less empty).\n\n        # So, prioritize:\n        # 1. Minimal `excess_capacity` (best fit).\n        # 2. Among bins with similar minimal `excess_capacity`, prefer smaller `remaining_capacity`.\n        # This is effectively prioritizing minimal `excess_capacity`.\n\n        # Let's implement the minimal waste priority with a bonus for NOT being overly empty.\n        # `excess_capacity` is primary.\n        # `remaining_capacity` is secondary for tie-breaking or small adjustments.\n        #\n        # `priority_score = logistic(-steepness * excess_capacity)`\n        #\n        # If two bins have very similar `excess_capacity` (e.g., 0.01 and 0.011), the logistic function will give similar scores.\n        # To break ties or differentiate slightly: add a term based on `remaining_capacity`.\n        # If we want to prefer bins that are \"less empty\" among these, we would add a term that increases with `remaining_capacity`.\n        # BUT, this contradicts the goal of minimal residual space.\n\n        # Let's stick to the idea of prioritizing minimal excess, and if \"worst fit reduction\" means\n        # \"don't leave *too much* empty space if a better option exists\", then our decaying function is sufficient.\n        # The key is that the score should be high for small `excess_capacity`.\n\n        # Using `priority_v1` function's core idea:\n        # `sigmoid(steepness * (ideal_gap - excess))`\n        # This prioritizes bins where `excess_capacity < ideal_gap`.\n        # It decays for larger excesses.\n        # Let's make `ideal_gap` smaller and steepness higher.\n        #\n        # `ideal_gap = 0.01`\n        # `steepness = 25`\n        # `priority = sigmoid(25 * (0.01 - excess))`\n        #\n        # excess=0: sigmoid(25*0.01) = sigmoid(0.25) = 0.56\n        # excess=0.01: sigmoid(0) = 0.5\n        # excess=0.02: sigmoid(25*(-0.01)) = sigmoid(-0.25) = 0.44\n        #\n        # This prioritizes smaller excesses, but not strictly minimal excess.\n        # It also doesn't guarantee a peak at `ideal_gap`.\n\n        # Let's try the combination:\n        # Score = `w1 * logistic(-s1 * excess)` (favors minimal waste)\n        #       + `w2 * logistic(s2 * (ideal_gap - excess))` (favors slight gap)\n\n        steepness_min_waste = 20.0\n        steepness_slight_gap = 25.0\n        ideal_gap_val = 0.01\n\n        score_min_waste = logistic(-steepness_min_waste * excess_capacities)\n        score_slight_gap = logistic(steepness_slight_gap * (ideal_gap_val - excess_capacities))\n\n        # Combine them, perhaps giving more weight to minimal waste.\n        weight_min_waste = 1.0\n        weight_slight_gap = 0.6\n\n        # Final score calculation\n        priorities[fits_mask] = weight_min_waste * score_min_waste + weight_slight_gap * score_slight_gap\n\n        # What about \"worst fit reduction\"?\n        # If `excess_capacities` is large, scores will be low.\n        # The `score_slight_gap` will be low. The `score_min_waste` will be low.\n        # This inherently reduces priority for bins with large excess.\n        #\n        # Let's add a small penalty for very large remaining capacities IF they are not \"best fits\".\n        # This is tricky to implement cleanly with a single score.\n        # The current combined score already penalizes large excesses.\n        #\n        # Let's consider the \"worst fit reduction\" as a modifier to the score.\n        # If a bin has a large `excess_capacity`, we might want to reduce its score further.\n        # The current score already does this.\n        #\n        # Let's assume \"worst fit reduction\" in context of \"best fit\" means:\n        # Among bins that are *not* best fits (i.e., `excess_capacity` > `ideal_gap`), pick the one that's least wasteful (smallest `excess_capacity`).\n        # And if they are equal, pick the one with larger `remaining_capacity`.\n        #\n        # The current combined score:\n        # At excess=0: `1.0*logistic(0) + 0.6*logistic(25*0.01)` = `0.5 + 0.6*logistic(0.25)` = `0.5 + 0.6*0.56` = `0.5 + 0.336` = 0.836\n        # At excess=0.01: `1.0*logistic(-20*0.01) + 0.6*logistic(0)` = `1.0*logistic(-0.2) + 0.6*0.5` = `0.452 + 0.3` = 0.752\n        # At excess=0.02: `1.0*logistic(-20*0.02) + 0.6*logistic(25*(-0.01))` = `1.0*logistic(-0.4) + 0.6*logistic(-0.25)`\n        #              `0.401 + 0.6*0.44 = 0.401 + 0.264` = 0.665\n        #\n        # This combination prioritizes minimal waste (excess=0) and then slightly prefers the ideal gap.\n        # The scores decrease as `excess_capacity` increases.\n        # This seems to capture the main goals.\n\n    return priorities",
    "response_id": 4,
    "obj": 4.078579976067022,
    "SLOC": 57.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  }
]