```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Sigmoid Fit Score.

    The Sigmoid Fit Score strategy prioritizes bins that leave a smaller remaining capacity
    after packing the item. This is achieved by mapping the remaining capacity to a
    sigmoid function. Bins that result in a smaller remaining capacity (closer to 0)
    will have a higher sigmoid output (closer to 1), indicating higher priority.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Calculate the remaining capacity if the item were placed in each bin
    potential_remaining_cap = bins_remain_cap - item

    # We only consider bins where the item can actually fit
    # For bins where it doesn't fit, the priority is effectively 0 (or very low)
    valid_bins_mask = potential_remaining_cap >= 0

    # Initialize priorities for all bins to a low value (e.g., 0)
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Apply the sigmoid function to the potential remaining capacities of valid bins.
    # The sigmoid function is generally 1 / (1 + exp(-x)).
    # To prioritize smaller remaining capacities, we want the input to the sigmoid
    # to be negative for smaller remaining capacities.
    # So, we can use -(potential_remaining_cap) or similar transformations.
    # A common approach for "best fit" like strategies is to prioritize bins
    # that result in the *smallest* remaining capacity.
    #
    # Let's define the sigmoid function: sigmoid(x) = 1 / (1 + exp(-x))
    # We want to map a *small* remaining capacity (e.g., close to 0) to a *high* priority (close to 1).
    # If `potential_remaining_cap` is small and positive, we want a high sigmoid value.
    # If `potential_remaining_cap` is large and positive, we want a low sigmoid value.
    #
    # Consider a transformation: - (potential_remaining_cap * scaling_factor)
    # If potential_remaining_cap is small (e.g., 0.1), then -0.1 * sf will be a small negative number, sigmoid ~ 0.47
    # If potential_remaining_cap is larger (e.g., 5), then -5 * sf will be a large negative number, sigmoid ~ 0
    # This seems reversed. We want small remaining capacity -> high priority.
    #
    # Let's re-think: we want to reward bins that leave *less* empty space.
    # If remaining_cap = 0.1, priority should be high.
    # If remaining_cap = 5, priority should be low.
    #
    # Try transformation: `k * (1 - potential_remaining_cap / BIN_CAPACITY)` for bins that can fit
    # This would give higher priority to bins that are almost full.
    #
    # For Sigmoid Fit Score, it's common to use a formula that maps the "fitness" of a bin.
    # Fitness can be defined as the negative of the remaining capacity (to maximize fitness)
    # or simply the remaining capacity itself, and then use a sigmoid that maps smaller values to higher outputs.
    #
    # Let's consider the remaining capacity after fitting `item`. We want `potential_remaining_cap` to be small.
    # The input to the sigmoid should ideally be such that small `potential_remaining_cap` maps to a large sigmoid output.
    # A common sigmoid application is `1 / (1 + exp(-k * (x - c)))` where `x` is the value being evaluated.
    # To make smaller `x` values result in higher output, `k` should be positive, and we might need to invert the term inside `exp`.
    #
    # Alternative interpretation: we want the *tightest* fit.
    # The tighter the fit, the smaller `potential_remaining_cap`.
    # Let's use a transformation of `potential_remaining_cap` such that smaller values are mapped to larger positive inputs for `exp`.
    # Or, more simply, we want to transform `potential_remaining_cap` into a value where a standard sigmoid `1 / (1 + exp(-x))` applied to it yields higher values for smaller `potential_remaining_cap`.
    #
    # Consider transforming `potential_remaining_cap` to `-(potential_remaining_cap)`.
    # sigmoid(-(potential_remaining_cap)):
    # If potential_remaining_cap = 0.1, sigmoid(-0.1) is ~0.477
    # If potential_remaining_cap = 5, sigmoid(-5) is ~0.0067
    # This maps smaller remaining capacities to higher sigmoid outputs.
    #
    # To fine-tune this, we can introduce a steepness parameter `k` and an offset `c`.
    # sigmoid(k * (c - potential_remaining_cap))
    # Let's choose c as a typical bin capacity or a representative "good" remaining capacity (e.g., 0).
    # Let's pick a steepness factor `k`. A higher `k` makes the transition sharper.
    #
    # Example: k=5, c=0.
    # sigmoid(5 * (0 - potential_remaining_cap)) = sigmoid(-5 * potential_remaining_cap)
    # This is what we reasoned above.
    #
    # Let's define the maximum possible capacity from the input for scaling.
    # If no bins, or all bins are full, the array might be empty or contain zeros.
    # Avoid division by zero if all capacities are zero.
    if bins_remain_cap.size > 0 and np.any(bins_remain_cap > 0):
        # Choose a scaling factor. A good heuristic might be related to the average bin capacity or a fixed value.
        # Let's try a scaling factor that's a bit aggressive to push scores towards 0 or 1.
        # A common approach is `k * (max_cap - remaining_cap)` and then sigmoid.
        # Or `1 / (1 + exp(-k * remaining_cap))` which maps smaller `remaining_cap` to higher output.
        
        # Let's use a simplified sigmoid form, focusing on the "best fit" idea:
        # Prioritize bins with smaller `potential_remaining_cap`.
        # We can transform `potential_remaining_cap` such that smaller values become more positive inputs to exp.
        # sigmoid(x) = 1 / (1 + exp(-x))
        # We want to maximize `1 / (1 + exp(-(-potential_remaining_cap)))` = `1 / (1 + exp(potential_remaining_cap))`
        # This sigmoid maps larger positive inputs to lower outputs.
        # We need smaller `potential_remaining_cap` to map to HIGHER outputs.
        # So, `1 / (1 + exp(-k * (some_function(potential_remaining_cap))))`
        # If `some_function(potential_remaining_cap)` is `(constant - potential_remaining_cap)`, then smaller `potential_remaining_cap` yields larger input to exp.
        #
        # Let's try a simple form that rewards closeness to zero for remaining capacity.
        # For the sake of simplicity and a "Sigmoid Fit Score" interpretation,
        # we can use the sigmoid function directly on a transformed `potential_remaining_cap`
        # such that smaller `potential_remaining_cap` values lead to a higher output score.

        # A standard approach: map the range of `potential_remaining_cap` to an activation range.
        # Let's use the sigmoid `1 / (1 + exp(-x))` where `x` is related to `-potential_remaining_cap`.
        # To control the "steepness" of the decision, we can multiply by a factor `k`.
        # `sigmoid(k * (-potential_remaining_cap))`
        # If `k` is large, the scores will be very close to 0 or 1.
        #
        # Let's select `k` based on the context, or use a reasonable fixed value.
        # A `k` value around 2-5 often provides a good balance. Let's try `k=3`.
        
        k = 5.0  # Steepness factor
        
        # Calculate the exponent argument. We want smaller remaining capacity to result in a larger (less negative) number
        # inside exp, so that the overall sigmoid output is higher.
        # We use -(potential_remaining_cap) effectively.
        exponent_arg = -k * potential_remaining_cap[valid_bins_mask]

        # Calculate sigmoid scores for valid bins
        sigmoid_scores = 1 / (1 + np.exp(exponent_arg))

        # Assign these scores to the priority array for valid bins
        priorities[valid_bins_mask] = sigmoid_scores

        # Ensure we don't have NaN values if any computation goes wrong (e.g., if potential_remaining_cap becomes infinity due to overflow)
        priorities = np.nan_to_num(priorities, nan=0.0, posinf=0.0, neginf=0.0)
        
    return priorities

```
