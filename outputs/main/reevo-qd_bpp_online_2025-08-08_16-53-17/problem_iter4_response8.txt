```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority scores for packing an item into bins using a Sigmoid Fit heuristic with exploration.

    This heuristic prioritizes bins that offer a "best fit" for the item,
    defined as bins where the remaining capacity is only slightly larger than the item's size.
    It uses a sigmoid function to assign higher scores to these "tight fits".
    To encourage exploration among equally good or near-equally good bins,
    the raw sigmoid scores are converted into probabilities using a softmax-like approach.
    Bins that cannot fit the item are assigned a priority of 0.

    Args:
        item: The size of the item to be packed.
        bins_remain_cap: A numpy array where each element is the remaining capacity of a bin.

    Returns:
        A numpy array of the same size as `bins_remain_cap`, containing the priority
        score (probability) for each bin. Higher scores indicate a more desirable bin.
    """

    # Initialize priorities to 0 for all bins.
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    # Identify bins where the item can fit.
    fits_mask = bins_remain_cap >= item

    # Only consider bins that can fit the item.
    fitting_bins_indices = np.where(fits_mask)[0]
    fitting_bins_remain_cap = bins_remain_cap[fits_mask]

    if fitting_bins_indices.size > 0:
        # Calculate the "gap" or excess capacity for fitting bins.
        # This is the amount of space left after packing the item.
        # We prefer smaller positive gaps.
        gap = fitting_bins_remain_cap - item

        # --- Tunable Parameters ---
        # steepness: Controls the steepness of the sigmoid curve. Higher values make the
        #            preference for the ideal_gap more pronounced.
        # ideal_gap: The preferred small positive residual capacity. A value close to 0
        #            encourages tighter packing.
        # exploration_temp: Temperature parameter for softmax. Higher values lead to more
        #                   uniform probabilities (more exploration). Lower values concentrate
        #                   probability on the best-scoring bins.
        steepness = 15.0
        ideal_gap = 0.02
        exploration_temp = 1.0

        # Calculate the argument for the sigmoid function.
        # The sigmoid function `1 / (1 + exp(-x))` is highest for positive `x`.
        # We want high scores when `gap` is small and positive, ideally near `ideal_gap`.
        # The argument `steepness * (ideal_gap - gap)` achieves this:
        # - If `gap < ideal_gap` (good fit): argument is positive, sigmoid score > 0.5.
        # - If `gap == ideal_gap`: argument is 0, sigmoid score = 0.5.
        # - If `gap > ideal_gap` (loose fit): argument is negative, sigmoid score < 0.5.
        sigmoid_arg = steepness * (ideal_gap - gap)

        # Apply sigmoid to get raw desirability scores for fitting bins.
        # Using a robust sigmoid implementation might be necessary for extreme `sigmoid_arg` values,
        # but numpy's `exp` handles large negative arguments gracefully (approaching 0).
        # For large positive `sigmoid_arg`, `exp` can overflow, but this is less common here.
        # If `sigmoid_arg` is very large positive, score approaches 1.0.
        # If `sigmoid_arg` is very large negative, score approaches 0.0.
        raw_desirability_scores = 1.0 / (1.0 + np.exp(-sigmoid_arg))

        # Apply a softmax-like transformation to convert scores into probabilities.
        # This encourages exploration by assigning non-zero probabilities to multiple good bins.
        # Protect against all-zero or extreme values to avoid NaN in softmax.
        # Add a small constant to the scores before exponentiation to prevent issues
        # if all raw scores are very low (e.g., close to 0).
        # A common practice is to subtract the maximum score before exponentiating,
        # but simply using the scores with temperature is often sufficient.
        
        # Ensure we don't have NaNs or infinities in raw scores, though unlikely with sigmoid.
        raw_desirability_scores = np.nan_to_num(raw_desirability_scores, nan=0.0, posinf=1.0, neginf=0.0)

        # Apply temperature scaling to control exploration.
        scaled_scores = raw_desirability_scores / exploration_temp

        # Compute probabilities using the exponential function.
        # Add a small epsilon to the denominator to avoid division by zero if all scaled_scores are -inf.
        # Ensure numerators are not producing NaN/inf.
        exp_scaled_scores = np.exp(scaled_scores)
        
        # Handle cases where all scaled_scores might be extremely negative, leading to exp_scaled_scores = 0.
        sum_exp_scaled_scores = np.sum(exp_scaled_scores)

        if sum_exp_scaled_scores > 0:
            probabilities = exp_scaled_scores / sum_exp_scaled_scores
        else:
            # If all raw scores were very bad (resulting in ~0 desirability),
            # distribute probability equally among the fitting bins.
            probabilities = np.ones_like(fitting_bins_indices, dtype=float) / fitting_bins_indices.size

        # Assign the calculated probabilities to the priorities array.
        priorities[fits_mask] = probabilities

    return priorities
```
