```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a refined combination of Best Fit and Worst Fit Reduction.

    This heuristic aims to:
    1. Favor bins that result in a tight fit (Best Fit component).
    2. Favor bins that, after packing, leave a larger remaining capacity (Worst Fit Reduction component),
       to potentially accommodate future larger items or reduce fragmentation.

    The priority is calculated as:
    priority = (BinRemainingCapacity - ItemSize) - alpha * (BinRemainingCapacity - ItemSize)^2
    where alpha is a tuning parameter. This formulation prioritizes bins where `BinRemainingCapacity - ItemSize` is small (good fit)
    but penalizes bins where this difference is extremely small (too tight fit, potentially wasting space if a slightly larger item comes).
    It also slightly favors bins with a larger `BinRemainingCapacity - ItemSize` to a lesser extent.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    num_bins = len(bins_remain_cap)
    priorities = np.zeros(num_bins)

    # Identify bins that can fit the item
    can_fit_mask = bins_remain_cap >= item

    # If no bins can fit the item, return all zeros
    if not np.any(can_fit_mask):
        return priorities

    fitting_bins_indices = np.where(can_fit_mask)[0]
    fitting_bins_remain_cap = bins_remain_cap[fitting_bins_indices]

    # Calculate the remaining capacity after packing
    remaining_capacities_after_packing = fitting_bins_remain_cap - item

    # Tuning parameter: controls the trade-off between best fit and worst fit reduction.
    # A smaller alpha emphasizes best fit. A larger alpha emphasizes worst fit reduction (leaving more space).
    # Alpha = 0.1 is a starting point.
    alpha = 0.1

    # Combined score:
    # The first term (remaining_capacities_after_packing) represents the "Worst Fit Reduction" aspect (higher is better).
    # The second term (remaining_capacities_after_packing**2) penalizes very tight fits (lower is better).
    # Subtracting the squared term makes it a penalty.
    # We want to maximize this combined score.
    combined_scores = remaining_capacities_after_packing - alpha * (remaining_capacities_after_packing ** 2)

    # Normalize scores to a probability distribution using softmax.
    # Subtract max to prevent overflow and ensure numerical stability.
    # Add a small epsilon to the denominator to avoid division by zero if all scores are identical.
    max_score = np.max(combined_scores)
    exp_scores = np.exp(combined_scores - max_score)
    sum_exp_scores = np.sum(exp_scores)

    if sum_exp_scores > 0:
        normalized_priorities = exp_scores / sum_exp_scores
    else:
        # If all scores are effectively -inf after subtractions, assign uniform probability.
        normalized_priorities = np.ones_like(combined_scores) / len(combined_scores)

    # Assign the calculated priorities to the original bins array
    priorities[fitting_bins_indices] = normalized_priorities

    return priorities
```
