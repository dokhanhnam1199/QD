[
  {
    "stdout_filepath": "problem_iter1_response10.txt_stdout.txt",
    "code_path": "problem_iter1_code10.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a First Fit Decreasing-like strategy.\n\n    This heuristic prioritizes bins that can accommodate the item and have the least remaining capacity *after*\n    the item is placed. This encourages filling bins more completely. Bins that cannot fit the item are given\n    a very low priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate the priority.\n    # A common strategy is to prioritize bins that will have the *least* remaining capacity after the item is added.\n    # This is like a \"best fit\" approach applied to the priority, aiming to leave less wasted space.\n    # We use the negative of the remaining capacity after placement, so a smaller remaining capacity\n    # results in a higher (less negative) priority score.\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # Bins that cannot fit the item should have a very low priority,\n    # ensuring they are not selected if any fitting bin exists.\n    # np.zeros_like already initializes them to 0, which is a sensible low value\n    # as we expect positive priorities for fitting bins.\n\n    return priorities",
    "response_id": 10,
    "obj": 4.048663741523748,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit.\n\n    The Best Fit strategy aims to place the item into the bin that has the\n    least remaining capacity after the item is placed, such that the item fits.\n    This minimizes wasted space in the bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins that cannot accommodate the item will have a priority of 0.\n        Among bins that can accommodate the item, higher priority is given to\n        bins that result in less remaining capacity after placing the item.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity if the item is placed in a bin\n    # This is the 'fitness' or 'wasted space' after placement\n    remaining_capacities_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # We want to minimize the remaining capacity after fit (Best Fit).\n    # To translate minimization into a maximization priority score, we can\n    # use the negative of the remaining capacity, or a score that is inversely\n    # proportional to it. A common approach is to use something like:\n    # priority = C - (remaining_capacity_after_fit) where C is a large constant.\n    # A simpler approach that achieves the same ordering is to use the negative\n    # of the remaining capacity if we want to *minimize* it.\n    # Alternatively, we can assign a higher priority to smaller remaining capacities.\n    # Let's use the negative of the remaining capacity as the priority score,\n    # so the smallest positive remaining capacity (least waste) gets the highest score.\n\n    # For bins that can fit the item, the priority is the negative of the\n    # remaining capacity after the item is placed. A smaller positive\n    # remaining capacity means a larger negative priority, which is good.\n    # For example, if remaining capacities after fit are [1, 5, 2],\n    # the negative values are [-1, -5, -2]. The highest priority would be -1.\n    # This correctly prioritizes the bin with remaining capacity 1.\n\n    priorities[can_fit_mask] = -remaining_capacities_after_fit\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response4.txt_stdout.txt",
    "code_path": "problem_iter1_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    Exact Fit First prioritizes bins that can accommodate the item perfectly,\n    meaning the remaining capacity of the bin after placing the item is zero.\n    Among bins that offer an exact fit, we can further refine the priority.\n    Here, we'll prioritize bins that have a smaller remaining capacity *before*\n    fitting the item (but still large enough to fit it). This strategy\n    aims to \"fill up\" bins more effectively with exact fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit exactly\n    exact_fit_mask = (bins_remain_cap - item) == 0\n\n    # For bins that allow an exact fit, assign a high priority based on their\n    # *current* remaining capacity. We want bins that are *almost* full\n    # (but can still fit the item exactly) to be prioritized.\n    # We can achieve this by assigning a priority score that is inversely\n    # related to their current remaining capacity, or by assigning a score\n    # that increases as remaining capacity decreases towards the item size.\n    # A simple approach is to assign a large base score and then add\n    # a bonus related to how close their current capacity is to the item size.\n    # Let's use a base high priority for exact fits and add a term that\n    # rewards smaller remaining capacities.\n\n    # A large base priority to favor exact fits\n    base_exact_fit_priority = 1000.0\n\n    # Calculate priority for exact fit bins: higher score for bins that were\n    # closer to exactly fitting the item (i.e., smaller current remaining_cap).\n    # We add a small value related to (bins_remain_cap - item) to ensure\n    # that among exact fits, those with smaller initial capacity get a slight edge.\n    # Since for exact fits, bins_remain_cap - item is 0, this term is effectively 0.\n    # Let's consider a slightly different approach: prioritize exact fits,\n    # and among those, pick the one that leaves the smallest remaining space\n    # if we consider the *exact* remaining space after fitting.\n    # So, if bins_remain_cap[i] == item, then bins_remain_cap[i] - item is 0.\n    # To favor those closer to the item size, we can use something like\n    # 1 / (epsilon + bins_remain_cap[i]) or a similar inverse relationship.\n    # A simpler heuristic for exact fit is to give a high score to those\n    # where bins_remain_cap == item. We can then add a penalty for\n    # bins that would have excess capacity even with an exact fit.\n\n    # Let's refine the exact fit priority:\n    # Assign a very high priority to bins that can fit the item perfectly.\n    # To break ties among exact fits, we can prioritize the bin that currently\n    # has the least remaining capacity (closest to the item size without being smaller).\n    # If bins_remain_cap[i] == item, then bins_remain_cap[i] - item = 0.\n    # We want higher priority for smaller `bins_remain_cap` among exact fits.\n    # So, `1 / bins_remain_cap[i]` would work, but we need to avoid division by zero\n    # if bin capacity could be 0, or if the item size is 0.\n    # Assuming item > 0 and bin capacities are positive.\n    # A safer approach might be to use the difference `bins_remain_cap[i] - item`.\n    # For exact fits, this is 0. For non-exact fits, it's > 0.\n    # We can assign a score that is inversely proportional to the remaining capacity\n    # *after* fitting the item, for those bins that have an exact fit.\n\n    # Assign a high score to bins that can fit the item exactly.\n    # Among those, give preference to the bin with the smallest remaining capacity *after* fitting.\n    # This means prioritizing bins where `bins_remain_cap[i]` is closest to `item`.\n    # If `bins_remain_cap[i] == item`, the remaining capacity after fit is 0.\n    # We can use `1 / (item + epsilon)` for exact fits.\n\n    # Let's create a priority based on:\n    # 1. A large bonus for exact fits.\n    # 2. Among exact fits, a smaller value for `bins_remain_cap[i]` gets a higher score.\n\n    # For bins that fit the item (i.e., bins_remain_cap >= item):\n    can_fit_mask = bins_remain_cap >= item\n    fittable_capacities = bins_remain_cap[can_fit_mask]\n    fittable_indices = np.where(can_fit_mask)[0]\n\n    # Calculate the remaining capacity after placing the item.\n    remaining_after_fit = fittable_capacities - item\n\n    # Priority for fittable bins:\n    # We want to prioritize bins that result in *zero* remaining capacity (exact fits).\n    # For non-exact fits, we want to prioritize bins that result in *less* remaining capacity.\n    # A common heuristic is to sort by remaining capacity after fitting.\n    # The \"Exact Fit\" strategy specifically looks for `remaining_after_fit == 0`.\n\n    # Let's implement a strict Exact Fit First:\n    # Highest priority to bins where `bins_remain_cap == item`.\n    # For these, the remaining capacity after fitting is 0.\n    # To break ties among exact fits, we can pick the one with the smallest initial capacity.\n    # Or, we can simply assign them all a high uniform priority and then potentially\n    # use a secondary criterion if needed, but the prompt implies a single priority score.\n\n    # Strategy:\n    # 1. Bins with `bins_remain_cap == item` get a very high priority.\n    # 2. Bins with `bins_remain_cap > item` (but not exact fit) get a lower priority.\n    # 3. Bins with `bins_remain_cap < item` get zero priority (cannot fit).\n\n    # Assigning priority:\n    # Use a large multiplier for exact fits.\n    # For bins that are not exact fits but can fit the item, we can assign a score\n    # that decreases as their remaining capacity (after fitting) increases.\n    # The smaller the `bins_remain_cap[i] - item` for non-exact fits, the higher the priority.\n    # This makes it a \"Best Fit\" if no exact fit exists.\n\n    # A high score for exact fits\n    exact_fit_bonus = 100.0\n\n    # For bins that fit, calculate a score that is higher for those with smaller remaining capacity *after* fitting.\n    # We can use a large negative number for non-fitting bins to ensure they are least prioritized.\n    priorities[can_fit_mask] = -remaining_after_fit # Higher score for smaller remaining capacity\n\n    # Now, adjust for exact fits. We want exact fits to be *higher* than any non-exact fit.\n    # If `remaining_after_fit` is 0, we want a score that is > any negative number.\n    # Let's assign a fixed high score to exact fits.\n    # The indices where exact fit happens are `fittable_indices[remaining_after_fit == 0]`.\n    exact_fit_indices = fittable_indices[remaining_after_fit == 0]\n\n    # For exact fits, set a very high priority. If there are multiple exact fits,\n    # their scores here would be the same. If a secondary criterion is needed,\n    # it could be incorporated into this score, e.g., `-bins_remain_cap[exact_fit_indices]`.\n    # For this implementation, let's assign a high base priority to all exact fits.\n    priorities[exact_fit_indices] = exact_fit_bonus\n\n    # For bins that can fit but are not exact fits, their priority is `-remaining_after_fit`.\n    # Since `remaining_after_fit` is positive, these scores will be negative.\n    # The largest negative score (closest to zero) will be for the smallest `remaining_after_fit`.\n\n    # Final priority adjustment:\n    # If an exact fit exists, all exact fits should have the highest priority.\n    # Among exact fits, let's favor those that were closer to the item size initially.\n    # If `bins_remain_cap[i] == item`, its score is `exact_fit_bonus`.\n    # If `bins_remain_cap[i] > item`, its score is `-(bins_remain_cap[i] - item)`.\n\n    # To ensure exact fits are strictly preferred over best-fit (among non-exact fits):\n    # We can scale the exact fit bonus to be significantly larger than the\n    # maximum possible score from the best-fit part.\n    # The best-fit scores range from approximately -(BIN_CAPACITY - ITEM_SIZE) to slightly less than 0.\n    # So, `exact_fit_bonus` should be greater than `BIN_CAPACITY`.\n    # Let's use `np.max(bins_remain_cap)` as a safe upper bound for remaining capacity.\n\n    scaled_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    max_possible_best_fit_score = -np.min(remaining_after_fit) if len(remaining_after_fit) > 0 else 0\n    exact_fit_priority_value = exact_fit_bonus # or something larger like 2 * np.max(bins_remain_cap) if we know capacity limits\n\n    # Apply priorities:\n    # Exact fits get the highest priority\n    exact_fit_locations = (bins_remain_cap - item) == 0\n    scaled_priorities[exact_fit_locations] = exact_fit_priority_value\n\n    # For bins that can fit but not exactly, assign a priority based on minimizing\n    # the remaining space. The smaller the remaining space, the higher the priority.\n    # Since `remaining_after_fit` is always >= 0, and we want higher priority for smaller values,\n    # a negative sign and then the value works for sorting.\n    # For example, if item=3 and capacities are 7, 8, 10:\n    # Remaining after fit: 4, 5, 7.\n    # Scores: -4, -5, -7. Higher score is -4 (bin with capacity 7).\n    # This would mean it's a \"Best Fit\" strategy.\n\n    # To ensure Exact Fit First:\n    # Set exact fits to a high constant value.\n    # Set non-exact fits to a lower range of values.\n    # The best fit criterion can be used to order within non-exact fits.\n\n    # Let's assign a score that is simply `exact_fit_priority_value` for exact fits.\n    # For bins that fit but not exactly, we want them to be strictly lower priority.\n    # We can assign them a priority based on the *negative* of their remaining capacity.\n    # The smaller the remaining capacity, the higher (less negative) the score.\n\n    # Initialize all to a very low priority (unusable bins)\n    scaled_priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    fittable_indices = np.where(can_fit_mask)[0]\n\n    # Calculate remaining capacities for fittable bins\n    remaining_after_fit = bins_remain_cap[fittable_indices] - item\n\n    # Find exact fit indices within the fittable ones\n    exact_fit_fittable_indices = fittable_indices[remaining_after_fit == 0]\n    non_exact_fit_fittable_indices = fittable_indices[remaining_after_fit > 0]\n\n    # Assign highest priority to exact fits.\n    # Among exact fits, we can choose the one that was closest to the item size.\n    # If we simply assign the same high score to all exact fits, then it becomes\n    # a matter of which one is encountered first.\n    # A common tie-breaker is the bin with the minimum remaining capacity before fitting,\n    # so `bins_remain_cap[exact_fit_fittable_indices]` would be the tie-breaker.\n    # A higher score for smaller remaining capacity.\n    if len(exact_fit_fittable_indices) > 0:\n        # Let's assign a base high score and add a term that prioritizes smaller initial capacities.\n        # A good tie-breaker is often to pick the one that was closest to being full.\n        # So, `1 / bins_remain_cap` for the exact fits.\n        # Or, use the negative of their current capacity, then sort.\n        # The problem requires a priority score, so higher means better.\n        # Max score for exact fit: `base_exact_fit_priority + (max_initial_cap - current_initial_cap)`\n        # This gives higher score to smaller `current_initial_cap`.\n        base_exact_fit_priority = 1000.0\n        exact_fit_initial_caps = bins_remain_cap[exact_fit_fittable_indices]\n        # Prioritize smaller initial capacities for exact fits\n        exact_fit_priorities = base_exact_fit_priority + (np.max(exact_fit_initial_caps) - exact_fit_initial_caps)\n        scaled_priorities[exact_fit_fittable_indices] = exact_fit_priorities\n    else:\n        # If there are no exact fits, we might fall back to Best Fit.\n        # However, the strategy is strictly Exact Fit First. If no exact fit, then what?\n        # The prompt implies that we always return a priority for *each* bin.\n        # If no exact fit, these bins still might fit. The priority should reflect\n        # that they are *not* exact fits.\n        pass # This section will handle non-exact fits later.\n\n    # Assign priorities to bins that fit but not exactly (Best Fit heuristic for secondary preference)\n    # These should have lower priorities than exact fits.\n    # Priority is inversely related to remaining capacity after fit.\n    # Higher priority for smaller `remaining_after_fit`.\n    if len(non_exact_fit_fittable_indices) > 0:\n        # Assign priorities that are lower than exact fits.\n        # The value `-remaining_after_fit` would be negative.\n        # We want higher score for smaller `remaining_after_fit`.\n        # Example: if remaining_after_fit are 1, 2, 3 -> scores are -1, -2, -3.\n        # The largest score is -1, which corresponds to the smallest remaining capacity.\n        # We need to shift these values so they are still lower than the exact fit priorities.\n        # We can subtract a large constant to ensure they are always lower.\n        offset_for_non_exact_fits = 500.0 # Must be less than base_exact_fit_priority\n        non_exact_fit_priorities = offset_for_non_exact_fits - remaining_after_fit[remaining_after_fit > 0]\n        scaled_priorities[non_exact_fit_fittable_indices] = non_exact_fit_priorities\n\n    # Bins that cannot fit the item retain their initial -inf priority.\n\n    return scaled_priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "SLOC": 35.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response6.txt_stdout.txt",
    "code_path": "problem_iter1_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score strategy aims to prioritize bins that are \"just right\"\n    for the item, avoiding both bins that are too large (leaving a lot of wasted space)\n    and bins that are too small (making a tight fit that might lead to future\n    fragmentation or difficulty packing subsequent items).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the difference between remaining capacity and item size.\n    # We are looking for differences close to zero (a good fit).\n    # We only consider bins where the item can actually fit.\n    fit_differences = bins_remain_cap - item\n\n    # Initialize priorities to a very low value (or 0) for bins that cannot fit the item.\n    priorities = np.zeros_like(bins_remain_cap)\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = fit_differences[can_fit_mask]\n\n    # Apply a sigmoid function. The goal is to map differences close to 0 to high values.\n    # The sigmoid function (1 / (1 + exp(-x))) maps the range (-inf, inf) to (0, 1).\n    # We want the \"best\" fits (differences near 0) to have the highest priority.\n    # A negative scaling factor for the difference `x` in `exp(-x)` will map\n    # small negative values (good fits) to larger positive exponents, resulting in\n    # smaller `exp` values and thus higher sigmoid outputs.\n    # A positive scaling factor will map small positive values (slight waste) to\n    # smaller positive exponents, resulting in larger `exp` values and thus lower\n    # sigmoid outputs.\n    # We'll use a scaling factor (e.g., 1.0 or a tuned value) to control the steepness.\n    # A value of 1.0 means that a difference of -1 gets a much higher score than a difference of +1.\n    # A larger positive value (e.g., 2.0) would make the sigmoid steeper around 0.\n    # Let's use a scaling factor of 1.0 for simplicity, and consider tuning it.\n    # Also, to ensure we are giving higher scores to smaller differences, we can\n    # consider using -fit_differences to flip the behavior if needed, but the current\n    # approach of mapping differences close to 0 to high priority is what we want.\n    # The sigmoid output will naturally be highest for values closest to 0.\n\n    # To ensure we prioritize smaller differences, we can either:\n    # 1. Use -fit_differences if we want the sigmoid to peak at positive values (i.e., bins with small remaining capacity after fitting).\n    # 2. Use fit_differences if we want the sigmoid to peak at negative values (i.e., bins with small \"slack\" after fitting).\n\n    # The prompt implies we want the bin that \"best fits\", which typically means\n    # minimizing wasted space. So, a bin with remaining capacity of `item + epsilon`\n    # is better than `item + 10*epsilon`. This corresponds to `fit_differences`\n    # being small and positive. To map these small positive differences to high\n    # sigmoid values, we should feed the negative of the differences into the sigmoid.\n\n    # Let's rescale the differences to make the sigmoid more sensitive around zero.\n    # A common approach is to normalize or scale the differences.\n    # We can scale the differences by a factor that makes the 'ideal' fit difference\n    # (close to 0) result in a value close to 0 for the sigmoid input.\n    # Consider scaling by -1 to map the small positive differences to negative values for the sigmoid.\n    # This will result in higher sigmoid values for bins with smaller positive `fit_differences`.\n\n    # To handle potential overflow/underflow with large values, clip differences or use\n    # a scaled version. For simplicity, let's directly apply sigmoid after handling\n    # the non-fitting bins.\n\n    # Let's refine the scaling. We want small `fit_differences` (which means remaining_cap is close to item)\n    # to yield high scores. `sigmoid(x)` is high for large positive `x`.\n    # So we want `x` to be large positive when `fit_difference` is small positive.\n    # This implies we should use `-fit_difference` if `fit_difference` represents `remaining_cap - item`.\n    # Or, if we want to think about the *waste* which is `remaining_cap - item`, then smaller waste is better.\n    # If we input `-(remaining_cap - item)` into sigmoid, then `-(item - item) = 0` becomes the peak.\n\n    # A common sigmoid scaling factor is related to the \"noise\" or variance expected.\n    # Let's use a simple scaling for demonstration. A factor of `1.0` means `exp(-x)` dominates.\n    # Small positive differences (good fits) should map to high scores.\n    # If `fit_difference = bins_remain_cap - item`, then when `fit_difference` is small positive,\n    # we want a high sigmoid output. Sigmoid `1/(1+exp(-k*x))` is high for large positive `k*x`.\n    # So, `k*(bins_remain_cap - item)` needs to be large positive for small positive `bins_remain_cap - item`.\n    # This requires `k` to be negative. Let's use `k = -2.0` for a steeper curve around 0.\n\n    # Ensure we don't feed inf/-inf or NaN into exp.\n    # The 'priorities' array already has 0 for bins that don't fit.\n    # So, we are calculating sigmoid for the `priorities` which now only holds\n    # `bins_remain_cap - item` for fitting bins.\n\n    # Let's transform the differences so that values close to zero are mapped to a value\n    # that yields the highest sigmoid output.\n    # If `f = bins_remain_cap - item`, we want `f` near 0 to be good.\n    # `sigmoid(x)` is good for large positive `x`.\n    # We can use `sigmoid(-f * scale)` to get high values for small positive `f`.\n    # Let's scale it by a factor, say 2.0, to make the \"sweet spot\" more pronounced.\n\n    scale_factor = 2.0  # Tune this parameter to control steepness around the ideal fit.\n    # For bins that can fit, calculate the negative difference.\n    # A smaller positive difference (e.g., 0.1) results in a smaller negative number (-0.1).\n    # `exp(-(-0.1))` is `exp(0.1)`, which is closer to 1 than `exp(-1)` or `exp(-10)`.\n    # This results in `1/(1+exp(0.1))` being a higher value than `1/(1+exp(1))` or `1/(1+exp(10))`.\n\n    # Directly compute the sigmoid for all bins (where non-fitting bins have 0 priority)\n    # We apply the transformation only to the differences that can fit.\n    # `priorities` is currently `fit_differences` for fitting bins and 0 otherwise.\n\n    # We want the best fit (remaining_cap - item closest to 0) to have the highest priority.\n    # Let `diff = bins_remain_cap[can_fit_mask] - item`.\n    # We want small positive `diff` to map to high scores.\n    # Sigmoid(k*x) is high for large positive k*x.\n    # So, we want `k * diff` to be large positive when `diff` is small positive.\n    # This means `k` should be positive.\n    # Let's rethink the goal: a bin that leaves minimal wasted space.\n    # This means `bins_remain_cap - item` should be minimized.\n    # So, `bins_remain_cap - item` = 0 is ideal.\n    # `sigmoid(x)` is highest at `x=0` if we consider `1 - sigmoid(x)` and map 0 to 0.\n    # Or `sigmoid(-x)` with `x` being the penalty.\n    # If we want the highest score for the smallest *positive* difference, we want to penalize large positive differences more.\n    # Consider `sigmoid(- (bins_remain_cap - item) * scale)`\n    # If `bins_remain_cap - item = 0.1`, sigmoid(-0.2) ~ 0.45\n    # If `bins_remain_cap - item = 1.0`, sigmoid(-2.0) ~ 0.12\n    # If `bins_remain_cap - item = 10.0`, sigmoid(-20.0) ~ 0.0\n\n    # This looks correct: small positive differences (good fit, low waste) yield higher sigmoid values.\n    # Let's clip the exponent to prevent overflow/underflow issues for very large/small differences.\n    # Clip the input to sigmoid to be within a reasonable range.\n    # The input is `-fit_difference * scale_factor`.\n    # `fit_difference` can range from 0 to `max_bin_capacity - min_item_size`.\n    # Let's assume bin capacities and item sizes are reasonable floats.\n\n    # Apply the sigmoid transformation to the differences for fitting bins\n    transformed_diffs = -priorities[can_fit_mask] * scale_factor\n    # Clip to prevent overflow in exp(-x)\n    # A very large positive transformed_diffs (very negative fit_difference)\n    # will result in exp(-large_positive) which goes to 0.\n    # A very large negative transformed_diffs (very positive fit_difference)\n    # will result in exp(-large_negative) which goes to inf.\n    # We need to clip values that will result in exp going to inf, which are very\n    # large positive fit_differences (meaning very large slack).\n    # So, clip `priorities[can_fit_mask]` to be not excessively large.\n    # A simple clipping of transformed_diffs might be more robust.\n    # `exp(-x)` means if `x` is very negative, `exp` is huge. If `x` is very positive, `exp` is near zero.\n    # So we need to prevent `x` from being extremely negative.\n    # `x = -priorities[can_fit_mask] * scale_factor`\n    # `priorities[can_fit_mask]` is `bins_remain_cap - item`.\n    # If `bins_remain_cap - item` is very large positive, then `x` is very large negative.\n    # `np.exp(large_positive)` can cause overflow. We need to avoid that.\n\n    # Let's cap the `priorities[can_fit_mask]` (the `bins_remain_cap - item`)\n    # A practical upper bound for the remaining capacity would be the maximum bin capacity itself.\n    # Let's assume a reasonable maximum slack, say 100 units of capacity.\n    # If `bins_remain_cap - item > 100`, we treat it as 100 for the sigmoid calculation.\n    # This makes bins with \"too much\" space similarly penalized.\n    # The maximum positive `priorities[can_fit_mask]` value we will consider for `sigmoid(-x*scale)`\n    # is when `bins_remain_cap - item` is around 100. This results in `exp(-200)`, which is practically 0.\n\n    # The problematic case is when `bins_remain_cap - item` is very small negative (meaning item is larger than bin)\n    # But we already masked those.\n    # So, we are concerned about `bins_remain_cap - item` being very large positive.\n\n    # Let's use a robust sigmoid calculation, often `sigmoid(x) = 0.5 * (1 + tanh(x/2))`.\n    # Or, directly use `scipy.special.expit` which handles clipping internally.\n    # However, the request is to implement using basic numpy, if possible.\n\n    # A simpler approach for robust sigmoid: clip the argument `z = -priorities[can_fit_mask] * scale_factor`\n    # such that `exp(z)` is not too large.\n    # If `z > 700` (approx where exp overflows), exp(-z) becomes 0.\n    # So we want `-z > -700`, which means `z < 700`.\n    # `-priorities[can_fit_mask] * scale_factor < 700`\n    # `priorities[can_fit_mask] * scale_factor > -700`\n    # `bins_remain_cap[can_fit_mask] - item > -700 / scale_factor`\n    # This condition is already true for the fitting bins as `bins_remain_cap >= item`.\n\n    # The real issue is when `priorities[can_fit_mask]` is large positive.\n    # `-priorities[can_fit_mask] * scale_factor` becomes very negative.\n    # e.g., `priorities[can_fit_mask] = 1000`, `scale_factor = 2`. `transformed_diffs = -2000`. `exp(2000)` overflows.\n    # This happens when `bins_remain_cap - item` is large positive.\n\n    # To prevent `exp(x)` overflow when `x` is a large negative number:\n    # If `transformed_diffs < -700` (approx), we want `exp(transformed_diffs)` to be effectively 0.\n    # So `sigmoid` will be `1 / (1 + 0) = 1`.\n    # This means if `bins_remain_cap - item` is very large, the sigmoid value should approach 1.\n    # Wait, this is opposite. If `bins_remain_cap - item` is very large positive, it's a bad fit.\n    # So, the sigmoid output should be low.\n\n    # Let's re-check: `sigmoid(x) = 1 / (1 + exp(-x))`.\n    # Goal: highest score for `bins_remain_cap - item` closest to 0 (ideally small positive).\n    # We feed `x = - (bins_remain_cap - item) * scale_factor`\n    # Case 1: `bins_remain_cap - item = 0.1` (good fit)\n    # `x = -0.1 * scale_factor`. If `scale_factor = 2`, `x = -0.2`.\n    # `exp(-(-0.2)) = exp(0.2) \u2248 1.22`.\n    # `sigmoid(-0.2) = 1 / (1 + 1.22) \u2248 1 / 2.22 \u2248 0.45`.\n    # This is NOT high. Highest sigmoid is for `x` large positive.\n\n    # Let's use `sigmoid(x)` where `x` is small positive for good fits.\n    # So, `x = (bins_remain_cap - item) * scale_factor`.\n    # If `bins_remain_cap - item = 0.1`, `x = 0.2`. `exp(-0.2) \u2248 0.82`. `sigmoid(0.2) \u2248 1 / (1 + 0.82) \u2248 0.55`.\n    # If `bins_remain_cap - item = 1.0`, `x = 2.0`. `exp(-2.0) \u2248 0.135`. `sigmoid(2.0) \u2248 1 / (1 + 0.135) \u2248 0.88`.\n    # This makes bins with MORE slack have HIGHER priority, which is wrong.\n\n    # We want the \"closest fit\". This means the difference should be close to zero.\n    # Let's consider `1 / (1 + exp(k * (bins_remain_cap - item)))` where `k > 0`.\n    # If `bins_remain_cap - item = 0.1` (good fit), `exp(k * 0.1)`. If k=2, exp(0.2) \u2248 1.22. Sigmoid \u2248 0.45.\n    # If `bins_remain_cap - item = 1.0` (loose fit), `exp(k * 1.0)`. If k=2, exp(2.0) \u2248 7.39. Sigmoid \u2248 0.11.\n    # If `bins_remain_cap - item = -0.1` (item too big, should not happen due to mask), `exp(-k * 0.1)`. If k=2, exp(-0.2) \u2248 0.82. Sigmoid \u2248 0.55.\n    # This function prioritizes bins where the item is *just too large* to fit, and penalizes bins that fit well but leave a lot of space.\n\n    # The standard \"Sigmoid Fit\" heuristic in literature for online BPP typically refers to\n    # selecting bins where `remaining_capacity - item` is \"close\" to zero.\n    # If we define \"close to zero\" as a positive value that's as small as possible,\n    # we can map this small positive value to the highest sigmoid score.\n    # Consider mapping `f = bins_remain_cap - item` to `sigmoid(a * (1 - f/M))` or similar, where M is max capacity.\n\n    # A more direct interpretation of Sigmoid Fit for BPP is to use it to determine a probability\n    # or preference for packing into a bin. Let's stick to the idea that the 'best fit'\n    # corresponds to `bins_remain_cap - item` being small and positive.\n\n    # Let's use a robust way to compute `sigmoid(x)`: `0.5 + 0.5 * tanh(x/2)`.\n    # Or `1 / (1 + exp(-x))`. For robustness against overflow:\n    # `exp(x)` for positive `x` can overflow. `exp(-x)` for positive `x` can underflow to 0.\n    # If `x` is large negative, `-x` is large positive, `exp(-x)` overflows.\n    # So if `x < -C` for some large C, we can treat `exp(-x)` as infinity, and sigmoid as 0.\n    # If `x > C` for some large C, we can treat `exp(-x)` as 0, and sigmoid as 1.\n\n    # Let's try the argument `x = - (bins_remain_cap - item) * scale_factor` again.\n    # The problem is when `bins_remain_cap - item` is very large positive.\n    # `x` becomes very negative. `exp(-x)` becomes very large positive. Sigmoid ~ 0.\n    # This correctly penalizes bins with lots of slack.\n\n    # Let's use a robust sigmoid calculation, ensuring `exp` argument is manageable.\n    # `y = -(priorities[can_fit_mask] * scale_factor)`\n    # If `y` is very large positive (i.e., `priorities[can_fit_mask]` is very large negative, which won't happen with `can_fit_mask`), then `exp(-y)` is small.\n    # If `y` is very large negative (i.e., `priorities[can_fit_mask]` is very large positive), then `exp(-y)` is large, `exp(-y)` overflows.\n\n    # Robust sigmoid implementation for `1 / (1 + exp(-x))`:\n    # def robust_sigmoid(x):\n    #     if x < -500: return 1.0  # exp(500) is huge, 1/(1+inf) -> 0\n    #     if x > 500: return 0.0   # exp(-500) is tiny, 1/(1+0) -> 1\n    #     return 1.0 / (1.0 + np.exp(-x))\n\n    # `x` is the argument to the sigmoid. In our case, `x = - (bins_remain_cap - item) * scale_factor`\n    # Let `slack = bins_remain_cap[can_fit_mask] - item`. This is always >= 0 for fitting bins.\n    # We want small slack to result in high sigmoid scores.\n    # Sigmoid(arg) is high for large positive `arg`.\n    # So we want `arg` to be large positive when `slack` is small positive.\n    # This suggests `arg = -slack * scale`.\n    # Example: slack = 0.1, scale=2. arg = -0.2. exp(-arg) = exp(0.2) = 1.22. sigmoid = 1/2.22 ~ 0.45.\n    # Example: slack = 1.0, scale=2. arg = -2.0. exp(-arg) = exp(2.0) = 7.39. sigmoid = 1/8.39 ~ 0.12.\n    # This means bins with MORE slack get LOWER priority, which is correct!\n\n    # Now, let's consider the range of `slack`. `slack = bins_remain_cap[can_fit_mask] - item`.\n    # `slack` can be large. If `slack` is very large, `arg = -slack * scale` becomes very negative.\n    # `exp(-arg) = exp(slack * scale)`. If `slack * scale` is large (e.g., 1000), this overflows.\n    # So, if `slack * scale > 500` (roughly), `exp(slack * scale)` overflows.\n    # This means `slack` should be capped for this calculation.\n    # Let's cap `slack` itself. If `slack` > `MAX_SLACK_CONSIDERED`, we can treat it as `MAX_SLACK_CONSIDERED`.\n    # MAX_SLACK_CONSIDERED could be related to `scale_factor`. If `MAX_SLACK_CONSIDERED * scale_factor`\n    # is about 500, that's a good bound. Let `MAX_SLACK_CONSIDERED = 500 / scale_factor`.\n\n    # If slack is very small, e.g., 0.001, and scale is 2, arg = -0.002. exp(0.002) ~ 1.002. sigmoid ~ 0.499.\n    # This means very tight fits get a score near 0.5. This is also not ideal.\n\n    # The common Sigmoid Fit prioritizes bins that have a remaining capacity *close to* the item size.\n    # It does not strictly penalize very small positive slack, but wants the slack to be minimal.\n    # Let's consider the function `sigmoid(K * (2 * item - remaining_capacity))`\n    # Let `f = remaining_capacity - item`. We want `f` to be small and positive.\n    # Consider `sigmoid(K * (item - remaining_capacity))`\n    # If `rem - item = 0.1`, `item - rem = -0.1`. `sigmoid(K * -0.1)`. With K=10, sigmoid(-1) ~ 0.27. Low.\n    # If `rem - item = 1.0`, `item - rem = -1.0`. `sigmoid(K * -1.0)`. With K=10, sigmoid(-10) ~ 0.000045. Very low.\n    # If `rem - item = -0.1` (item too big), `item - rem = 1.0`. `sigmoid(K * 1.0)`. With K=10, sigmoid(10) ~ 1.\n\n    # The function `1 / (1 + exp(k * (bins_remain_cap - item)))` with k>0:\n    # - If `bins_remain_cap - item = 0.1` (good fit): `exp(k*0.1)`. With k=2, exp(0.2) \u2248 1.22. Sigmoid \u2248 0.45.\n    # - If `bins_remain_cap - item = 1.0` (loose fit): `exp(k*1.0)`. With k=2, exp(2.0) \u2248 7.39. Sigmoid \u2248 0.12.\n    # - If `bins_remain_cap - item = -0.1` (item too big, not applicable): `exp(-k*0.1)`. Sigmoid > 0.5.\n\n    # This function penalizes bins with larger positive slack, but scores for tight fits (small positive slack)\n    # are also not very high (around 0.45). It might be better if tight fits also got high scores.\n\n    # A common approach in practice is to normalize the differences or use a scaled logistic function.\n    # Let's try to map the 'gap' `bins_remain_cap - item` to the sigmoid, but map `0` to the highest point.\n    # This suggests an argument like `-(bins_remain_cap - item)^2`.\n    # If `bins_remain_cap - item = 0.1`, argument is `-0.01`. Sigmoid `1/(1+exp(0.01))` ~ 0.497.\n    # If `bins_remain_cap - item = 1.0`, argument is `-1.0`. Sigmoid `1/(1+exp(1))` ~ 0.268.\n    # This prioritizes bins closer to `item` capacity.\n\n    # Let's use this: `priorities = sigmoid( - (bins_remain_cap - item)^2 * scale )`\n    # where `scale` controls the steepness. A higher scale means we are more sensitive to deviations from perfect fit.\n    # If `scale = 100`, `(0.1)^2 * 100 = 0.01 * 100 = 0.1`. sigmoid( -0.1 ) ~ 0.475.\n    # If `scale = 100`, `(1.0)^2 * 100 = 1.0 * 100 = 100`. sigmoid( -100 ) ~ 0.\n    # This seems like a good approach.\n\n    # Apply to fitting bins only.\n    # Calculate `(bins_remain_cap - item)` for fitting bins.\n    # These are the `priorities` where `can_fit_mask` is True.\n    fit_differences_vals = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate the squared differences.\n    squared_diffs = fit_differences_vals ** 2\n\n    # Scale the squared differences. `scale_factor` should be tuned.\n    # A higher scale_factor makes the priority drop more sharply as the difference increases.\n    # We are looking for values where the exponent `-squared_diffs * scale_factor`\n    # does not cause overflow (i.e., `squared_diffs * scale_factor` is not too large negative).\n    # The exponent is `-x`. So `x` can't be too negative.\n    # `x = squared_diffs * scale_factor`.\n    # `squared_diffs` is always >= 0. So `x` is always >= 0.\n    # We want `sigmoid(x)` to be highest for `x` closest to 0.\n    # Sigmoid(x) is highest for large positive x. We want high scores for small squared diffs.\n\n    # Let's go back to: `sigmoid( K * (target_value - current_value) )`\n    # Or `sigmoid( K * -(current_value - target_value) )`\n    # `target_value = item`\n    # `current_value = bins_remain_cap`\n    # `sigmoid( K * -(bins_remain_cap - item))` = `sigmoid( K * (item - bins_remain_cap))`\n\n    # Let `K = 2.0` for sensitivity.\n    # `arg = K * (item - bins_remain_cap[can_fit_mask])`\n    # For fitting bins: `bins_remain_cap >= item`, so `item - bins_remain_cap <= 0`.\n    # Thus `arg` is always <= 0 for fitting bins.\n\n    # If `bins_remain_cap - item = 0.1` (good fit)\n    # `item - bins_remain_cap = -0.1`\n    # `arg = 2.0 * -0.1 = -0.2`\n    # `sigmoid(-0.2) = 1 / (1 + exp(0.2)) \u2248 0.45`\n\n    # If `bins_remain_cap - item = 1.0` (loose fit)\n    # `item - bins_remain_cap = -1.0`\n    # `arg = 2.0 * -1.0 = -2.0`\n    # `sigmoid(-2.0) = 1 / (1 + exp(2.0)) \u2248 0.12`\n\n    # This function assigns lower scores to looser fits, but the highest scores are near 0.5 for tightest fits.\n    # This seems reasonable: prioritize bins that don't waste too much space, and among those,\n    # prefer the ones that leave less excess capacity.\n\n    # Robust sigmoid for `1 / (1 + exp(x))` where x <= 0 for fitting bins.\n    # If `x` becomes very negative (item << remaining_capacity), exp(x) approaches 0. sigmoid approaches 1.\n    # This would mean bins with *lots* of slack get the highest score near 1. This is WRONG.\n\n    # The core problem is correctly mapping the \"goodness of fit\" to a score, where a value close to zero\n    # difference is best, and this best value should yield the highest priority.\n    # `sigmoid(x)` is high for large POSITIVE `x`.\n    # We want `x` to be large positive when `bins_remain_cap - item` is small positive.\n\n    # This means we need `f(bins_remain_cap - item)` such that when `bins_remain_cap - item` is small positive,\n    # `f(...)` is large positive.\n    # Consider `f(d) = some_constant - d`. If `d` is small positive, `f` is large positive.\n    # Let `d = bins_remain_cap - item`. We want `d` close to 0.\n    # Consider `sigmoid(Constant - (bins_remain_cap - item))`.\n\n    # Or simply, let the \"value\" be `- (bins_remain_cap - item)`.\n    # When `bins_remain_cap - item = 0.1`, value = -0.1.\n    # When `bins_remain_cap - item = 1.0`, value = -1.0.\n    # We want to scale these values such that small negative values become large positive for sigmoid.\n    # `sigmoid(scale * value)`.\n    # `scale * value = scale * (item - bins_remain_cap)`.\n\n    # Let `scale = 5.0`\n    # If `bins_remain_cap - item = 0.1`: `item - bins_remain_cap = -0.1`. `scale * value = -0.5`.\n    # `sigmoid(-0.5) = 1 / (1 + exp(0.5)) \u2248 0.377`.\n\n    # If `bins_remain_cap - item = 0.01`: `item - bins_remain_cap = -0.01`. `scale * value = -0.05`.\n    # `sigmoid(-0.05) = 1 / (1 + exp(0.05)) \u2248 0.487`.\n    # This looks good: tighter fits (smaller positive differences) get higher scores, approaching 0.5.\n    # If `bins_remain_cap - item = 1.0`: `item - bins_remain_cap = -1.0`. `scale * value = -5.0`.\n    # `sigmoid(-5.0) = 1 / (1 + exp(5.0)) \u2248 0.006`. Low score for loose fits.\n\n    # Let's refine this. The scores are capped around 0.5.\n    # To get scores potentially higher than 0.5, we can shift the sigmoid or use a different base.\n    # The prompt requires \"highest priority score\".\n\n    # Let's use the \"smallest waste\" criterion: we want to minimize `bins_remain_cap - item`.\n    # This quantity should be mapped to a high value if it's small positive.\n    # Consider `f(x) = exp(-x)`. This gives high values for small positive `x`.\n    # Let `x = (bins_remain_cap - item)`. Then `exp(-(bins_remain_cap - item))`.\n    # If `bins_remain_cap - item = 0.1`, `exp(-0.1) \u2248 0.90`.\n    # If `bins_remain_cap - item = 1.0`, `exp(-1.0) \u2248 0.36`.\n    # If `bins_remain_cap - item = 10.0`, `exp(-10.0) \u2248 0.000045`.\n    # This function also works and can be scaled.\n    # Let's use `sigmoid(Constant - (bins_remain_cap - item) * scale)`.\n    # The `Constant` term allows us to shift the peak.\n    # Or more simply, let's use the original Sigmoid formula but with an argument that peaks at 0.\n\n    # Let `d = bins_remain_cap - item`. We want a function that is max at d=0, decreasing for d>0 and d<0.\n    # `-d^2` peaks at 0. `sigmoid(-d^2 * scale)`\n    # Let `scale = 10`.\n    # `d = 0.1`, `-d^2 * scale = -0.01 * 10 = -0.1`. `sigmoid(-0.1) \u2248 0.475`.\n    # `d = 0.01`, `-d^2 * scale = -0.0001 * 10 = -0.001`. `sigmoid(-0.001) \u2248 0.499`.\n    # `d = 1.0`, `-d^2 * scale = -1.0 * 10 = -10.0`. `sigmoid(-10.0) \u2248 0.000045`.\n    # This correctly assigns higher scores for values of `bins_remain_cap - item` closer to 0.\n    # The scores are still around 0.5.\n\n    # Let's adjust the sigmoid to be `0.5 + 0.5 * tanh(x/2)`.\n    # If we use `x = - (bins_remain_cap - item) * scale`:\n    # `d = 0.1`, `x = -0.1 * scale`. `d=0.01`, `x = -0.001*scale`.\n    # For `tanh(y)` to be near 1, `y` must be large positive.\n    # `y = x/2`. So `x` must be large positive.\n    # `x = -(bins_remain_cap - item) * scale`.\n    # For `x` to be large positive, `-(bins_remain_cap - item)` must be large positive, meaning `bins_remain_cap - item` must be large negative.\n    # This prioritizes bins where the item is too big. WRONG.\n\n    # Let's try the simple approach with scaling and ensuring numerical stability.\n    # Priority = Sigmoid( `scale` * ( `target_capacity` - `remaining_capacity` ) )\n    # where `target_capacity` is the ideal capacity for the item.\n    # If we assume the ideal scenario is to fill a bin as much as possible without overflow,\n    # then `target_capacity` could be related to `item`.\n    # Or, we want `remaining_capacity` to be close to `item`.\n\n    # The \"Sigmoid Fit\" often implies mapping to a value that represents how \"close\" a bin is to\n    # accommodating the item efficiently.\n    # Let's define the quality of fit for bin `i` as `q_i = remaining_capacity_i - item`.\n    # We want `q_i` to be small and positive.\n    # We can map `q_i` using a sigmoid function such that small positive `q_i` yields high output.\n    # The function `Sigmoid( C - k * q_i )` where `C` is a bias and `k > 0`.\n    # Let `C = 0` and `k = scale`.\n    # `Sigmoid( -k * q_i ) = Sigmoid( -k * (bins_remain_cap - item) )`.\n    # Let `scale = 2.0`.\n    # If `bins_remain_cap - item = 0.1` (good fit): `arg = -0.2`. `Sigmoid(-0.2) \u2248 0.45`.\n    # If `bins_remain_cap - item = 1.0` (loose fit): `arg = -2.0`. `Sigmoid(-2.0) \u2248 0.12`.\n    # If `bins_remain_cap - item = 10.0` (very loose fit): `arg = -20.0`. `Sigmoid(-20.0) \u2248 0`.\n\n    # The maximum score is < 0.5 here.\n    # To get scores closer to 1 for good fits, we can shift the sigmoid or its input.\n    # Let's use `sigmoid(C + x)` where `x` is scaled.\n    # We want `bins_remain_cap - item` small positive to map to high score.\n    # Consider `sigmoid( K * (1 - (bins_remain_cap - item) / MaxCapacity ) )`\n    # Or simpler: `sigmoid( K * (IdealFill - ActualFill) )`.\n\n    # Let's simplify to a practical implementation for the heuristic.\n    # We want bins that have *just enough* space.\n    # We can calculate a \"penalty\" for each bin: `penalty = max(0, item - remaining_capacity)` for bins that don't fit.\n    # For bins that fit: `penalty = max(0, remaining_capacity - item)`.\n    # Then apply sigmoid to negative penalty.\n\n    # Final approach: Use the `sigmoid( k * (ideal_remaining - actual_remaining) )` form.\n    # Let `ideal_remaining` be the smallest amount of capacity that can accommodate the item, so `item`.\n    # Then `ideal_remaining = item`.\n    # `arg = scale * (item - bins_remain_cap)`\n    # For fitting bins: `bins_remain_cap >= item`. So `item - bins_remain_cap <= 0`.\n    # `arg <= 0`.\n    # We want the highest score when `item - bins_remain_cap` is closest to 0 (which means `bins_remain_cap - item` is smallest positive).\n    # `sigmoid(x)` is highest for large POSITIVE `x`.\n    # This means we need `scale * (item - bins_remain_cap)` to be large positive.\n    # Which implies `scale` should be negative and `item - bins_remain_cap` should be large negative (meaning `bins_remain_cap - item` is large positive).\n\n    # Let's try `sigmoid( k * (bins_remain_cap - item) )` where `k` is negative.\n    # Let `k = -2.0`.\n    # `d = bins_remain_cap - item`\n    # `d = 0.1` (good fit): `arg = -0.2`. `sigmoid(-0.2) \u2248 0.45`.\n    # `d = 1.0` (loose fit): `arg = -2.0`. `sigmoid(-2.0) \u2248 0.12`.\n    # `d = 10.0` (very loose fit): `arg = -20.0`. `sigmoid(-20.0) \u2248 0`.\n    # This means bins with more slack get lower priority. This is good.\n    # The highest scores are still capped below 0.5 for tight fits.\n\n    # To get scores potentially above 0.5, we can bias the input:\n    # `sigmoid(Bias + k * (bins_remain_cap - item))`\n    # If we want the peak at `bins_remain_cap - item = 0`.\n    # Let `k = -2.0`.\n    # We want `Bias` such that `sigmoid(Bias + k * 0) = sigmoid(Bias)` is high.\n    # Say we want the peak value to be 0.8. `sigmoid(Bias) = 0.8`.\n    # `1 / (1 + exp(-Bias)) = 0.8`\n    # `1 = 0.8 + 0.8 * exp(-Bias)`\n    # `0.2 = 0.8 * exp(-Bias)`\n    # `0.25 = exp(-Bias)`\n    # `-Bias = ln(0.25) = -ln(4) = -1.386`\n    # `Bias = 1.386`\n\n    # So, `sigmoid(1.386 - 2.0 * (bins_remain_cap - item))`\n    # Let's check:\n    # `d = 0.1`: `arg = 1.386 - 2.0 * 0.1 = 1.386 - 0.2 = 1.186`. `sigmoid(1.186) \u2248 0.766`. Good.\n    # `d = 1.0`: `arg = 1.386 - 2.0 * 1.0 = 1.386 - 2.0 = -0.614`. `sigmoid(-0.614) \u2248 0.35`. Lower.\n    # `d = 10.0`: `arg = 1.386 - 2.0 * 10.0 = 1.386 - 20.0 = -18.614`. `sigmoid(-18.614) \u2248 0`. Very low.\n\n    # This approach seems to work. The `Bias` value (`~1.386`) can be tuned, as well as `k` (`-2.0`).\n    # A common parameter `k` used in literature is around 1 to 5 for the scaled difference.\n    # Let's choose `scale_factor = 2.0` and a `bias_shift` to lift the scores.\n    # Let's re-evaluate what \"Sigmoid Fit Score\" truly means.\n    # It often refers to mapping the \"fit\" or \"slack\" to a preference.\n    # A common parameterization might be `sigmoid( (IdealCapacity - CurrentCapacity) / ScalingFactor )`.\n\n    # Let's use a more direct interpretation of \"good fit\": remaining_capacity is slightly greater than item.\n    # Let the \"target gap\" be 0.\n    # We want the gap `g = bins_remain_cap - item` to be close to 0.\n    # Consider a score that is high when `g` is small positive.\n    # This could be `sigmoid( K * (epsilon - g) )` where `epsilon` is a small positive value.\n    # If `epsilon = 0.1`, `K = 2`.\n    # `g = 0.1`: `sigmoid(2 * (0.1 - 0.1)) = sigmoid(0) = 0.5`.\n    # `g = 0.01`: `sigmoid(2 * (0.1 - 0.01)) = sigmoid(0.18) \u2248 0.545`.\n    # `g = 1.0`: `sigmoid(2 * (0.1 - 1.0)) = sigmoid(-1.8) \u2248 0.14`.\n    # This looks like a valid Sigmoid Fit heuristic.\n\n    # `scale_factor` here is `K`.\n    # `epsilon` is a small constant representing the \"ideal slack\".\n\n    scale_factor = 3.0  # Controls steepness around epsilon\n    ideal_slack = 0.0   # The ideal amount of remaining capacity after fitting an item.\n                        # Setting to 0 aims for the tightest possible fit.\n                        # Setting to a small positive value can help with future packing.\n\n    # Calculate the argument for the sigmoid function.\n    # We want the argument to be high positive when (bins_remain_cap - item) is close to ideal_slack.\n    # So, `arg = scale_factor * (ideal_slack - (bins_remain_cap - item))`\n    # `arg = scale_factor * (ideal_slack - bins_remain_cap + item)`\n\n    # For bins that cannot fit the item, their priority should be 0.\n    # `priorities` is already 0 for non-fitting bins.\n    # We only calculate the sigmoid score for fitting bins.\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # Calculate the 'gap' for fitting bins.\n    gap = fitting_bins_remain_cap - item\n\n    # Calculate the sigmoid argument:\n    # We want high scores when 'gap' is close to 'ideal_slack'.\n    # Sigmoid(x) is high for large positive x.\n    # So, we want `arg` to be large positive when `gap` is close to `ideal_slack`.\n    # This implies `arg` should be inversely related to `abs(gap - ideal_slack)`.\n    # Using `scale_factor * (ideal_slack - gap)` works.\n\n    # If ideal_slack = 0:\n    # arg = scale_factor * (0 - gap) = -scale_factor * gap\n    # If gap = 0.1, arg = -0.1 * scale_factor. Sigmoid will be < 0.5.\n    # If gap = 0.01, arg = -0.01 * scale_factor. Sigmoid will be closer to 0.5.\n    # To get scores > 0.5 for good fits, we need to shift the sigmoid.\n\n    # Alternative approach: Prioritize bins that result in the smallest positive remaining capacity.\n    # This is often implemented as: For each bin i, if `remaining_capacity_i >= item`,\n    # priority is `1.0 / (remaining_capacity_i - item + epsilon)`.\n    # Let's use sigmoid to map this inverse relationship.\n\n    # Consider the inverse relationship: the smaller the slack (positive), the higher the priority.\n    # `slack = bins_remain_cap - item`.\n    # We want `1/slack` to be high for small positive slack.\n    # This implies we want to map `1/slack` using sigmoid such that high values of `1/slack`\n    # result in high sigmoid outputs.\n\n    # Let `scaled_slack = (bins_remain_cap - item) * scale_factor`.\n    # We want small positive `scaled_slack` to give high sigmoid scores.\n    # Let's use `sigmoid( K * (Constant - scaled_slack) )`.\n    # Let `Constant = 1.0`. `scale_factor = 2.0`.\n    # `arg = K * (1.0 - scaled_slack) = K * (1.0 - (bins_remain_cap - item) * scale_factor)`\n\n    # Let's simplify to a commonly used form.\n    # Prioritize bins that are \"almost full\".\n    # We can map the remaining capacity itself.\n    # A bin with remaining capacity equal to `item` would be ideal.\n    # `sigmoid( K * (item - remaining_capacity) )`\n    # `arg = K * (item - fitting_bins_remain_cap)`\n    # As seen before, for fitting bins `fitting_bins_remain_cap >= item`, so `arg <= 0`.\n    # Scores are capped at 0.5.\n\n    # To get scores above 0.5, we can shift the argument or use a different form.\n    # Let's consider the inverse of slack, but bounded.\n    # Let `slack = bins_remain_cap[can_fit_mask] - item`.\n    # If `slack = 0`, priority is high. If `slack` is large positive, priority is low.\n    # We can map `1 / (slack + epsilon)` to sigmoid.\n    # `1 / (slack + epsilon)` is high when `slack` is small positive.\n\n    # Let `value = 1.0 / (gap + 1e-9)` where `gap = fitting_bins_remain_cap - item`.\n    # We want to map this `value` such that large `value` gives high sigmoid.\n    # So `sigmoid(scale_factor * value)`.\n    # `value` can be very large if `gap` is close to 0.\n    # If `gap` is very small (e.g., 1e-10), `value` is large.\n    # `scaled_value = scale_factor * value`. This can overflow if `scale_factor` is large and `gap` is small.\n    # E.g., `scale_factor=2`, `gap=1e-10`. `value = 1e10`. `scaled_value = 2e10`. Sigmoid argument overflows.\n\n    # Robust sigmoid calculation needed.\n    # `sigmoid(x) = 1 / (1 + exp(-x))`\n    # We want `x` to be large positive when `gap` is small positive.\n    # So `x = - scale_factor * gap` is problematic for small positive gaps.\n    # If `gap = 0.01`, `x = -scale_factor * 0.01`. Sigmoid < 0.5.\n    # We need `x` to be large positive.\n\n    # Let's try `sigmoid( Constant - scale_factor * gap )`.\n    # `Constant = 2.0`, `scale_factor = 2.0`.\n    # `gap = 0.01`: `arg = 2.0 - 0.02 = 1.98`. `sigmoid(1.98) \u2248 0.87`. Good.\n    # `gap = 0.1`: `arg = 2.0 - 0.2 = 1.8`. `sigmoid(1.8) \u2248 0.86`. Not much difference. Need steeper.\n    # Let `scale_factor = 5.0`.\n    # `gap = 0.01`: `arg = 2.0 - 0.05 = 1.95`. `sigmoid(1.95) \u2248 0.87`.\n    # `gap = 0.1`: `arg = 2.0 - 0.5 = 1.5`. `sigmoid(1.5) \u2248 0.81`. Difference increases.\n\n    # Let `Constant = 3.0`, `scale_factor = 5.0`.\n    # `gap = 0.01`: `arg = 3.0 - 0.05 = 2.95`. `sigmoid(2.95) \u2248 0.95`.\n    # `gap = 0.1`: `arg = 3.0 - 0.5 = 2.5`. `sigmoid(2.5) \u2248 0.92`.\n    # `gap = 0.5`: `arg = 3.0 - 2.5 = 0.5`. `sigmoid(0.5) \u2248 0.62`.\n    # `gap = 1.0`: `arg = 3.0 - 5.0 = -2.0`. `sigmoid(-2.0) \u2248 0.12`.\n\n    # This parameterization `sigmoid(Constant - scale_factor * gap)` is reasonable.\n    # `Constant` acts as a threshold for the gap. Gaps larger than `Constant/scale_factor`\n    # will result in scores below 0.5.\n    # `scale_factor` determines how quickly the score drops.\n\n    # Let's implement this.\n    # Use `np.clip` on the `bins_remain_cap` or `gap` to avoid issues with `exp` if\n    # `Constant - scale_factor * gap` becomes extremely large negative.\n    # `Constant - scale_factor * gap` is problematic if it's very negative.\n    # This happens when `scale_factor * gap` is very positive.\n    # This occurs when `gap` is large positive.\n    # If `gap` is large, the score should be low anyway.\n    # e.g. `gap = 1000`, `scale = 5`. `arg = C - 5000`. This can be very negative.\n    # `exp(-arg)` will be `exp(5000 - C)`, which overflows.\n    # We need to handle `exp(-x)` when `x` is very negative.\n    # `sigmoid(x) = 1 / (1 + exp(-x))`\n\n    # If `arg = Constant - scale_factor * gap`.\n    # If `arg` is very negative: `exp(-arg)` is very positive and can overflow.\n    # Let `arg_max = 700`. If `arg > arg_max`, `sigmoid(arg) \u2248 1`.\n    # Let `arg_min = -700`. If `arg < arg_min`, `sigmoid(arg) \u2248 0`.\n\n    # We are concerned about `arg` being very negative.\n    # `Constant - scale_factor * gap < -700`\n    # `Constant + 700 < scale_factor * gap`\n    # `gap > (Constant + 700) / scale_factor`\n    # This means if `gap` is very large, the argument becomes very negative, and the score goes to 0.\n    # This is desired behavior. The overflow happens in `exp(-arg)`.\n    # So if `arg` is very negative, `exp(-arg)` is very large positive.\n    # `1 / (1 + very_large_positive)` is approximately `1 / very_large_positive` which is near 0.\n    # So, `sigmoid` should naturally go to 0.\n\n    # We need to be careful with `gap` values.\n    # If `bins_remain_cap` is large and `item` is small, `gap` can be large.\n    # Example: `bins_remain_cap = 1000`, `item = 1`. `gap = 999`.\n    # `arg = C - 5 * 999 = C - 4995`.\n    # If `C=3`, `arg = 3 - 4995 = -4992`.\n    # `exp(-arg) = exp(4992)`. This will overflow.\n\n    # A robust sigmoid implementation is needed.\n    # `sigmoid(x) = 1 / (1 + exp(-x))`\n    # If `x < -X`, `exp(-x)` overflows.\n    # If `x > X`, `exp(-x)` underflows to 0.\n\n    # To avoid overflow in `exp(-x)` when `x` is very negative:\n    # Let `y = -x`. We need to avoid `exp(y)` overflow.\n    # If `y > Y_max`, replace `exp(y)` with something like `np.inf`.\n    # `sigmoid(x) = 1 / (1 + np.exp(-x))`\n    # `x = Constant - scale_factor * gap`\n    # Let `scale_factor = 5.0`. `Constant = 3.0`.\n    # If `gap` is large, `x` is very negative.\n    # If `gap = 1000`, `x = 3 - 5000 = -4997`.\n    # `-x = 4997`. `exp(4997)` overflows.\n\n    # If we clip `gap` before calculating `x`:\n    # Let `max_gap = 100`.\n    # `clipped_gap = np.clip(gap, 0, max_gap)`.\n    # `x = Constant - scale_factor * clipped_gap`.\n    # If `gap` was originally 1000, `clipped_gap` is 100.\n    # `x = 3 - 5 * 100 = 3 - 500 = -497`.\n    # `-x = 497`. Still overflows.\n\n    # We need to scale the argument itself to a range where sigmoid is well-behaved.\n    # OR use a different form: `0.5 * (1 + tanh(x/2))`\n    # `tanh(y)` can also overflow/underflow.\n    # `tanh(y) = (exp(y) - exp(-y)) / (exp(y) + exp(-y))`\n    # If `y` is large positive, `tanh(y)` is 1. If `y` is large negative, `tanh(y)` is -1.\n    # The `exp(y)` part can overflow if `y` is large positive.\n    # `y = x/2 = (Constant - scale_factor * gap) / 2`.\n    # This also faces similar issues.\n\n    # A common robust sigmoid:\n    # def robust_sigmoid(x):\n    #     return np.clip(0.5 * (1 + np.tanh(x / 2.0)), 0.0, 1.0)\n    # Let's assume we have this robust version implicitly for now.\n\n    # Let's try the simple form, with reasonable parameters and assume numpy handles intermediate steps or we clip the argument of exp.\n    # `Constant = 3.0`, `scale_factor = 5.0`.\n    # `gap = bins_remain_cap[can_fit_mask] - item`.\n    # `arg = Constant - scale_factor * gap`.\n    # We want to prevent `exp(-arg)` from overflowing.\n    # This happens if `-arg` is too large positive.\n    # So, we want to limit `-arg` from exceeding ~700.\n    # `-arg = -Constant + scale_factor * gap`.\n    # We need `-Constant + scale_factor * gap < 700`.\n    # `scale_factor * gap < 700 + Constant`.\n    # `gap < (700 + Constant) / scale_factor`.\n    # If `gap` exceeds this, `exp(-arg)` will overflow.\n    # For `C=3, S=5`, this threshold is `(700+3)/5 = 703/5 = 140.6`.\n    # If `gap` is greater than ~140.6, `exp(-arg)` might overflow.\n\n    # Let's clip the `gap` to a reasonable maximum to prevent extreme values.\n    # Let `max_gap_clip = 50.0` (tuned parameter).\n    # `clipped_gap = np.clip(gap, 0, max_gap_clip)`.\n    # `arg = Constant - scale_factor * clipped_gap`.\n    # Max negative `arg` will be `3 - 5 * 50 = 3 - 250 = -247`.\n    # `-arg = 247`. `exp(247)` is large but might be manageable. Let's check numpy exp.\n    # `np.exp(700)` is `~1.0e304`. `np.exp(710)` is `inf`. So the threshold is around 700-710.\n    # Our max negative `arg` is -247. So `-arg` is 247. This should be fine.\n\n    # Parameter values:\n    # `Constant`: Influences the \"midpoint\" of the sigmoid.\n    # `scale_factor`: Controls how steep the priority drop-off is.\n    # `max_gap_clip`: Prevents extreme gaps from dominating or causing numerical issues.\n\n    constant_bias = 3.0\n    scale_factor = 5.0\n    max_gap_clip = 50.0  # Represents the maximum \"useful\" slack. Slack beyond this is equally penalized.\n\n    # Ensure we only process fitting bins.\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # Calculate the gap (remaining capacity - item size) for fitting bins.\n    gap = fitting_bins_remain_cap - item\n\n    # Clip the gap to prevent extreme values and potential numerical issues.\n    clipped_gap = np.clip(gap, 0.0, max_gap_clip)\n\n    # Calculate the argument for the sigmoid function.\n    # We want higher priority for smaller gaps.\n    # The sigmoid function is `1 / (1 + exp(-x))`. It's high for positive `x`.\n    # So we want `x` to be large positive when `clipped_gap` is small.\n    # We use `Constant - scale_factor * clipped_gap`.\n    # If `clipped_gap` is small (e.g., 0), `arg = Constant`.\n    # If `clipped_gap` is large (e.g., max_gap_clip), `arg = Constant - scale_factor * max_gap_clip`.\n    # This will result in lower scores for larger gaps.\n    sigmoid_arg = constant_bias - scale_factor * clipped_gap\n\n    # Apply the sigmoid function to get the priority scores.\n    # Use `np.clip` to ensure the argument doesn't cause `exp` overflow/underflow,\n    # although the `clipped_gap` should largely handle this.\n    # Limiting sigmoid_arg to [-700, 700] is a common robust practice.\n    # `-arg = -Constant + scale_factor * clipped_gap`\n    # Max negative `arg` is `3 - 5*50 = -247`. Min `-arg` is 247.\n    # Min positive `arg` is `3 - 5*0 = 3`. Max `-arg` is -3.\n    # The range of `arg` is `[3-250, 3] = [-247, 3]`.\n    # So `exp(-arg)` will be in range `[exp(-3), exp(247)]`. `exp(247)` is okay.\n\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(-sigmoid_arg))\n\n    # Normalize priorities if needed, or simply return as is.\n    # In this context, higher values mean higher priority.\n\n    return priorities",
    "response_id": 6,
    "obj": 4.487435181491823,
    "SLOC": 22.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response5.txt_stdout.txt",
    "code_path": "problem_iter2_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Sigmoid Fit Score.\n\n    This heuristic aims to prioritize bins that offer the \"best fit\" for the item.\n    A \"best fit\" is defined as a bin where the remaining capacity is only slightly\n    larger than the item's size. This strategy tries to fill bins as much as possible\n    without leaving excessive empty space, thereby minimizing fragmentation.\n\n    The priority is calculated using a sigmoid function. The function is designed\n    to peak when the remaining capacity (`bins_remain_cap`) is precisely equal to\n    the item's size, and the priority decreases as the remaining capacity deviates\n    (either smaller or larger). However, bins where the item doesn't fit at all\n    are assigned a zero priority.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element is the remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as `bins_remain_cap`, containing the priority\n        score for each bin. Higher scores indicate a more desirable bin for the item.\n    \"\"\"\n\n    def sigmoid(x, steepness=10.0, center=0.0):\n        \"\"\"A custom sigmoid function that can be shifted and scaled.\"\"\"\n        return 1 / (1 + np.exp(-steepness * (x - center)))\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit.\n    # The condition is that the bin's remaining capacity must be greater than or equal to the item's size.\n    fits_mask = bins_remain_cap >= item\n\n    # For bins where the item fits, calculate the 'fit difference'.\n    # This is the excess capacity after placing the item.\n    # We want to prioritize bins where this difference is small and non-negative.\n    if np.any(fits_mask):\n        # Calculate the excess capacity for fitting bins.\n        excess_capacities = bins_remain_cap[fits_mask] - item\n\n        # The sigmoid function is used to map these excess capacities to priority scores.\n        # We want the highest priority (e.g., close to 1) when excess_capacity is minimal.\n        # A common formulation for \"good fit\" prioritizes `excess_capacity` close to 0.\n        # To achieve this with a sigmoid, we can use a function that decreases as `excess_capacity` increases.\n        # `sigmoid(k * (ideal_excess - actual_excess))` works well here.\n        # Let `ideal_excess` be 0.\n        # The argument becomes `steepness * (0 - excess_capacities)` which is `-steepness * excess_capacities`.\n        # This means scores will decrease from 0.5 as `excess_capacities` increases.\n        #\n        # To align with \"slightly larger\" being the peak priority (as per problem description interpretation),\n        # we can shift the sigmoid. Let's define the peak priority (score=0.5) at `ideal_gap` (a small positive value).\n        #\n        # The function `sigmoid(steepness * (ideal_gap - excess_capacities))` will have its midpoint at `excess_capacities = ideal_gap`.\n        # With `steepness=10.0` and `ideal_gap=0.05`, the peak priority of 0.5 occurs when `bins_remain_cap - item = 0.05`.\n        # Values of `excess_capacities` smaller than `ideal_gap` (but >= 0) will yield scores > 0.5.\n        # Values of `excess_capacities` larger than `ideal_gap` will yield scores < 0.5.\n\n        ideal_gap = 0.05  # Prefer bins with a small positive remaining capacity after fitting the item.\n        steepness = 10.0  # Controls how sharply the priority drops as excess capacity increases.\n\n        # Calculate the argument for the sigmoid function.\n        # We want the peak of the sigmoid (where the argument is 0) to align with our 'ideal_gap'.\n        # `argument = steepness * (ideal_gap - excess_capacities)`\n        argument_values = steepness * (ideal_gap - excess_capacities)\n\n        # Apply the sigmoid function to get the priority scores for fitting bins.\n        priorities[fits_mask] = sigmoid(argument_values, steepness=steepness, center=0.0)\n\n    # For bins where the item does not fit (fits_mask is False), the priority remains 0,\n    # ensuring they are not selected unless no fitting bins exist.\n\n    return priorities",
    "response_id": 5,
    "obj": 3.9389708815317115,
    "SLOC": 12.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response24.txt_stdout.txt",
    "code_path": "problem_iter1_code24.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    Exact Fit First prioritizes bins that can accommodate the item with the least remaining capacity.\n    This is achieved by assigning a high priority to bins where (remaining_capacity - item_size) is minimized,\n    but only if the item fits. For items that don't fit, a very low priority is assigned.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find bins where the item fits\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins where the item fits, calculate the \"exactness\" score.\n    # A smaller difference means a better fit. We want to prioritize smaller differences.\n    # So, we take the negative of the difference.\n    # The ideal fit would have a difference of 0.\n    if np.any(can_fit_mask):\n        remaining_capacities_for_fitting_bins = bins_remain_cap[can_fit_mask]\n        differences = remaining_capacities_for_fitting_bins - item\n        # We want to prioritize smaller differences, so we use the negative difference.\n        # A difference of 0 (perfect fit) will give a priority of 0.\n        # A small positive difference (e.g., 0.1) will give a priority of -0.1.\n        # A large positive difference (e.g., 2) will give a priority of -2.\n        # This ensures bins with minimal leftover space are preferred.\n        priorities[can_fit_mask] = -differences\n    \n    # Bins where the item does not fit get a very low priority score\n    # to ensure they are never chosen if a fitting bin exists.\n    priorities[~can_fit_mask] = -np.inf\n\n    return priorities",
    "response_id": 24,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response11.txt_stdout.txt",
    "code_path": "problem_iter1_code11.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit strategy.\n\n    The Best Fit strategy aims to place the item into the bin where it fits\n    most snugly, leaving the smallest possible remaining capacity. This often\n    helps in leaving larger spaces available for future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher priority means a better fit. Bins that cannot accommodate the\n        item are given a priority of -1 (or any value indicating they are invalid).\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)  # Initialize with -1 for invalid bins\n\n    # Calculate remaining capacity after placing the item in each bin\n    potential_remaining_capacities = bins_remain_cap - item\n\n    # Consider only bins that can accommodate the item\n    valid_indices = np.where(potential_remaining_capacities >= 0)[0]\n\n    if len(valid_indices) > 0:\n        # The priority is the negative of the remaining capacity.\n        # This is because we want to minimize the remaining capacity (best fit).\n        # A smaller positive remaining capacity (e.g., 0.1) is better than a larger one (e.g., 2.0).\n        # By taking the negative, a smaller positive value becomes a larger negative value,\n        # effectively giving it a higher \"priority\" in a maximization context.\n        # Alternatively, we can think of this as assigning a \"waste\" score, and we want to minimize waste.\n        # For Best Fit, we want the bin with the smallest non-negative remaining capacity.\n        # Assigning priorities as the negative remaining capacity:\n        # A bin with remaining_cap = 1.5 after placing item gets priority -1.5\n        # A bin with remaining_cap = 0.2 after placing item gets priority -0.2\n        # -0.2 > -1.5, so the bin with 0.2 remaining capacity has higher priority.\n        priorities[valid_indices] = -potential_remaining_capacities[valid_indices]\n\n    return priorities",
    "response_id": 11,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response14.txt_stdout.txt",
    "code_path": "problem_iter1_code14.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    Exact Fit First strategy prioritizes bins that have a remaining capacity\n    exactly equal to the item's size. Among bins with exact fits, it\n    further prioritizes bins that are \"tighter\" (i.e., have less remaining\n    capacity after the item is placed). If no exact fit is found, it\n    falls back to selecting a bin that can accommodate the item and\n    minimizes the remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Find bins that have an exact fit\n    exact_fit_mask = bins_remain_cap == item\n    if np.any(exact_fit_mask):\n        # Among exact fits, prefer the one with least remaining capacity (which is 0 after fit)\n        # So we can give a high positive score, higher for those that would become exactly full.\n        # Since all exact fits will have 0 remaining capacity after the item,\n        # we can assign a high, uniform score to them to prioritize them.\n        # We add a small penalty to break ties in a deterministic way, although for exact fit,\n        # any exact fit is generally considered equal.\n        priorities[exact_fit_mask] = 1.0\n\n        # To further differentiate among exact fits and prioritize the \"tightest\"\n        # (which in this case, after placing the item, all exact fits leave 0 space),\n        # we can assign a small negative bonus based on their original remaining capacity.\n        # This might seem counterintuitive for \"exact fit\", but if we interpret\n        # \"exact fit\" more broadly as \"closest to item size\", this would matter.\n        # For strict \"exact fit = item size\", all exact fits are equally good.\n        # For this implementation, let's prioritize exact fits and then by how\n        # little extra space is left. For exact fits, this extra space is 0.\n        # So, let's assign a higher priority to exact fits by simply making them positive.\n        # To make them *more* priority, we can give them a higher score.\n        # Let's assign a score that reflects \"best fit\" if it's an exact fit.\n        # We can think of this as `(bin_capacity - item) + penalty_for_exact_fit`.\n        # For exact fits, `bin_capacity - item` is 0.\n\n        # A more sophisticated approach for \"tightest\" exact fit would involve\n        # looking at bins that are *almost* exact fits if we allowed a small epsilon,\n        # but for strict exact fit, we assign a high priority.\n        # Let's assign a score that is higher than any non-exact fit score.\n        priorities[exact_fit_mask] = 1000.0  # High priority for exact fits\n    else:\n        # If no exact fit, find bins that can fit the item\n        can_fit_mask = bins_remain_cap >= item\n        if np.any(can_fit_mask):\n            # For bins that can fit, prioritize those with the smallest remaining capacity\n            # after placing the item (i.e., the \"tightest\" fit).\n            # The score is inversely related to the remaining capacity after placement.\n            # A bin with remaining capacity `c` for item `i` will have `c - i` remaining.\n            # We want to minimize `c - i`.\n            # So, higher priority for smaller `c - i`.\n            # Let's use `-(bins_remain_cap[can_fit_mask] - item)` as a priority score.\n            # Higher values of this (less negative) are better.\n            priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # For any bins that cannot fit the item, their priority remains 0.\n    return priorities",
    "response_id": 14,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response16.txt_stdout.txt",
    "code_path": "problem_iter1_code16.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score prioritizes bins that are a \"good fit\" for the item.\n    A good fit is defined as a bin where the remaining capacity is slightly larger\n    than the item size. This strategy aims to leave larger remaining capacities\n    in bins that can accommodate more items, while using bins that are nearly full\n    for items that fill them up, thus reducing fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the difference between remaining capacity and item size.\n    # We only consider bins where the item can actually fit.\n    fit_differences = bins_remain_cap - item\n\n    # For bins where the item doesn't fit, set the difference to a very small\n    # negative number to ensure they get a low priority.\n    # A very large positive difference would also lead to low priority if not for the sigmoid.\n    # However, the sigmoid will compress large positive differences.\n    # For \"no fit\" bins, we want them to be clearly worse than any \"fit\" bin.\n    # We can achieve this by ensuring their sigmoid output is very low.\n    # A very large negative input to sigmoid results in a value close to 0.\n    # Let's set a threshold that's smaller than any possible valid fit_difference.\n    # The smallest possible fit_difference for a fitting bin could be very close to 0.\n    # So, a large negative number will ensure no-fit bins are penalized.\n    no_fit_penalty = -1e9\n    valid_fits_mask = fit_differences >= 0\n    fit_differences[~valid_fits_mask] = no_fit_penalty\n\n    # Apply the sigmoid function. The sigmoid function maps any real-valued number\n    # into a value between 0 and 1.\n    # f(x) = 1 / (1 + exp(-k * (x - x0)))\n    # Here, x is fit_difference.\n    # k: Steepness parameter. A larger k makes the transition steeper.\n    # x0: Midpoint of the sigmoid. This is the ideal difference for a \"perfect\" fit.\n    # We want bins where the remaining capacity is *just* enough for the item.\n    # So, the ideal difference is 0 (remaining_cap == item_size).\n    # Let's choose a steepness (k) and a midpoint (x0).\n    # A smaller midpoint (x0) means we prefer bins that are closer to being full.\n    # A larger midpoint (x0) means we are more tolerant of larger remaining capacities.\n\n    # Strategy: Prefer bins where `remaining_capacity - item_size` is small and non-negative.\n    # This means the item fits snugly.\n    # A sigmoid function with a midpoint around 0 and a steep slope will achieve this.\n    # A positive `fit_difference` means the bin has more space than needed.\n    # We want the score to be higher for smaller positive `fit_difference`.\n    # The sigmoid function typically increases. So, we need to transform `fit_difference`\n    # such that smaller positive values yield higher scores.\n    # This can be done by passing a negative value to sigmoid, e.g., sigmoid(-x).\n    # Or by shifting the sigmoid function's midpoint.\n\n    # Let's use the form: sigmoid(k * (x0 - x))\n    # where x is `fit_difference`.\n    # x0 = 0: We prefer perfect fits.\n    # k > 0: Steepness.\n\n    # Example parameters:\n    k = 5.0  # Steepness. Higher values mean the transition is sharper around x0.\n    x0 = 0.0 # Midpoint. We want the \"peak\" of the priority score to be when fit_difference is 0.\n\n    # Calculate priorities using the shifted sigmoid function.\n    # We want higher scores for smaller, non-negative fit_differences.\n    # The standard sigmoid (1 / (1 + exp(-z))) increases as z increases.\n    # So, we can pass -(fit_difference) or (x0 - fit_difference) to the sigmoid.\n    # Let's use `x0 - fit_difference` for conceptual clarity that we want to be close to `x0`.\n    # When fit_difference is 0, argument is x0.\n    # When fit_difference is large positive, argument is negative and large.\n    # When fit_difference is large negative (no fit), argument is large positive.\n    # This means we need to be careful with the 'no fit' case and the sigmoid behavior.\n\n    # Let's reconsider:\n    # We want the priority to be high when `fit_difference` is small and positive.\n    # Consider `sigmoid(A - B * x)` where x is `fit_difference`.\n    # If B > 0, `sigmoid(A - B*x)` increases as x decreases.\n    # We want a high priority for small x. So, we should use `B > 0`.\n    # Let's set `B=k`.\n    # For the peak, we want `A - B*x` to be near 0. So, `A = B*x`.\n    # If we want the peak at `fit_difference = 0`, then `A=0`.\n    # This gives `sigmoid(-k * fit_difference)`.\n    # If fit_difference = 0, sigmoid(0) = 0.5.\n    # If fit_difference = positive small, sigmoid(-k * positive) < 0.5.\n    # If fit_difference = negative small (no fit), sigmoid(-k * negative) > 0.5.\n    # This is the opposite of what we want.\n\n    # Let's try `sigmoid(A + B * x)`. For it to be high when x is small positive,\n    # the argument `A + B*x` should be large positive.\n    # If `B > 0`, then `A` needs to be large positive to compensate for any positive x.\n    # If `B < 0`, then `A` needs to be large positive to compensate for negative x.\n\n    # The most intuitive interpretation of \"Sigmoid Fit Score\" in BPP usually means\n    # scoring bins where the remaining capacity is close to the item size.\n    # This is often achieved by scoring `(remaining_capacity - item_size)`\n    # and mapping smaller non-negative values to higher scores.\n    # A sigmoid function that decreases as its input increases can be used,\n    # or a standard sigmoid applied to a transformed value.\n\n    # Let's use `sigmoid(a - b * difference)` where `difference = bins_remain_cap - item`.\n    # If `difference` is small positive, we want a high score.\n    # This means `a - b * difference` should be large. This requires `b > 0` and `a` to be set appropriately.\n    # Example: `a` represents a target \"goodness\" score, `b` the sensitivity.\n    #\n    # Let's try a simpler, common approach:\n    # Prioritize bins where `remaining_capacity` is just enough for the item.\n    # This means `bins_remain_cap - item` is small and non-negative.\n    #\n    # Consider the function `f(diff) = exp(-k * diff)` for `diff >= 0`. This decreases.\n    # For `diff < 0`, we assign 0.\n    # This is not a sigmoid.\n\n    # Let's map `fit_difference` to a score:\n    # If `fit_difference` is negative (item doesn't fit), score = 0.\n    # If `fit_difference` is 0, score = 1 (perfect fit).\n    # If `fit_difference` is small positive, score = high (close to 1).\n    # If `fit_difference` is large positive, score = low (close to 0).\n    #\n    # This behavior is characteristic of `sigmoid(large_positive_number - k * fit_difference)`\n    # or `sigmoid(a - k * fit_difference)` where `a` is sufficiently large and `k > 0`.\n    #\n    # Let's define `x = fit_difference`.\n    # We want a function `score(x)` such that:\n    # score(x) = 0 if x < 0\n    # score(x) approaches 1 as x approaches 0 from positive side.\n    # score(x) approaches 0 as x becomes large positive.\n    #\n    # This is like `sigmoid(-k * x)` but with the x<0 case handled.\n    # Let's use `sigmoid(a - k * x)` for all values.\n    # If we set `k > 0` and choose `a` appropriately, we can achieve the desired behavior.\n    #\n    # Let's test the function: `sigmoid(a - k * x)`\n    # - If `x < 0` (item doesn't fit): `a - k*x` will be `a + k*|x|`. If `a` is large and `k>0`, this argument can be large positive, leading to score close to 1. This is incorrect; no-fit bins should have low scores.\n    #\n    # To handle the \"no fit\" case: explicitly set their scores to 0.\n    # For bins where the item fits (`fit_difference >= 0`):\n    # We want scores to be high for small `fit_difference` and low for large `fit_difference`.\n    # Use `sigmoid(-k * fit_difference)`.\n    #\n    # If `fit_difference = 0`, sigmoid(0) = 0.5\n    # If `fit_difference` is small positive (e.g., 0.1): sigmoid(-k * 0.1) < 0.5\n    # If `fit_difference` is large positive (e.g., 1.0): sigmoid(-k * 1.0) << 0.5\n    # This is the opposite of what we want: higher scores for smaller positive differences.\n    #\n    # So, the argument should be `sigmoid(k * (x0 - x))`.\n    # `x = fit_difference`.\n    # `x0` is the ideal difference. Let `x0 = 0` for a perfect fit.\n    # `k` is steepness.\n    #\n    # `sigmoid(k * (0 - fit_difference)) = sigmoid(-k * fit_difference)`\n    # Still leads to the opposite behavior.\n\n    # Let's use the score as `sigmoid(positive_slope * (target - current_value))`.\n    # `current_value` is `bins_remain_cap`. `target` is `item`.\n    # We want higher score when `bins_remain_cap` is close to `item`.\n    # If `bins_remain_cap < item`, then `target - current_value > 0`.\n    # If `bins_remain_cap > item`, then `target - current_value < 0`.\n    #\n    # If `bins_remain_cap` is slightly larger than `item`: `target - current_value` is small negative.\n    # If `bins_remain_cap` is much larger than `item`: `target - current_value` is large negative.\n    #\n    # Let `arg = a + b * (item - bins_remain_cap)`.\n    # If item fits, `item - bins_remain_cap <= 0`.\n    # We want high score when `item - bins_remain_cap` is close to 0 (from negative side).\n    # This means `a + b * (near_zero_negative)` should be large.\n    # If `b > 0`, then `a` needs to be large to make the argument large.\n    #\n    # Let's try:\n    # `score = sigmoid(k * (item - bins_remain_cap))`\n    # For valid fits (`bins_remain_cap >= item`):\n    # `item - bins_remain_cap <= 0`\n    # If `k > 0`:\n    #   - `bins_remain_cap` slightly > `item` => `item - bins_remain_cap` = small negative => `sigmoid(k * small_negative)` = value slightly < 0.5.\n    #   - `bins_remain_cap` == `item` => `item - bins_remain_cap` = 0 => `sigmoid(0)` = 0.5.\n    #   - `bins_remain_cap` much > `item` => `item - bins_remain_cap` = large negative => `sigmoid(k * large_negative)` = value close to 0.\n    # This gives higher priority to bins that are closer to full.\n\n    # To prioritize bins that are 'good fits' (remaining capacity is close to item size,\n    # and the item actually fits), we need a function that peaks when `remaining_capacity - item`\n    # is small and non-negative.\n    #\n    # The function `sigmoid(a - k*x)` where `x = remaining_capacity - item`\n    # with `k > 0` and `a` chosen well, can work if we manage the 'no fit' case.\n    #\n    # Let `x = bins_remain_cap - item`.\n    # We want high scores when `x` is close to 0 and `x >= 0`.\n    #\n    # Let's consider `f(x) = sigmoid(slope * (peak_x - x))`.\n    # If `peak_x = 0` and `slope > 0`: `f(x) = sigmoid(-slope * x)`.\n    # For `x >= 0`, this function decreases from 0.5.\n    # To get higher scores for smaller `x`, we want the argument to be larger.\n    #\n    # Try `sigmoid(k * (central_point - (bins_remain_cap - item)))`\n    # Let `central_point` be the ideal gap, typically 0.\n    # `arg = k * (0 - (bins_remain_cap - item)) = k * (item - bins_remain_cap)`\n    #\n    # For `bins_remain_cap >= item`:\n    # `item - bins_remain_cap` is 0 or negative.\n    # If `k > 0`:\n    #   - `item - bins_remain_cap = 0` (perfect fit) => `sigmoid(0) = 0.5`\n    #   - `item - bins_remain_cap = small negative` => `sigmoid(k * small_negative)` < 0.5\n    #   - `item - bins_remain_cap = large negative` => `sigmoid(k * large_negative)` -> 0\n    #\n    # This prioritizes bins that are *closer* to being full, but not perfectly full.\n    # A small remaining gap is penalized.\n\n    # A better approach for \"good fit\" could be to penalize bins that are \"too empty\"\n    # or \"too full\" (just fitting).\n    #\n    # For the Sigmoid Fit Score, the common interpretation is to assign a high score\n    # to bins whose remaining capacity `R` is such that `R - item_size` is small and non-negative.\n    #\n    # This can be modeled by a sigmoid function where the input is decreasing as `R - item_size` increases.\n    # So, `sigmoid(a - b * (R - item_size))` where `a > 0, b > 0`.\n    #\n    # Let's use `k` as the steepness and `x0` as the optimal gap (`R - item_size`).\n    # We want optimal gap `x0 = 0`.\n    # The function is `sigmoid(k * (x0 - (bins_remain_cap - item)))`\n    # = `sigmoid(k * (item - bins_remain_cap))`\n    #\n    # This function's output for `bins_remain_cap >= item` decreases from 0.5.\n    # The peak priority for a fitting bin is 0.5, achieved at `bins_remain_cap = item`.\n    # For bins where `bins_remain_cap > item`, the priority is < 0.5.\n    #\n    # To ensure bins that are *just* a fit get highest priority, we can shift the sigmoid.\n    # Or use a formulation that peaks.\n    #\n    # Consider `sigmoid(k * x)` where x is adjusted.\n    #\n    # A common Sigmoid Fit strategy in literature is:\n    # `Score = 1 / (1 + exp(-k * (ideal_capacity - current_remaining_capacity)))`\n    # Here, `ideal_capacity` is the capacity of the bin if it were to be perfectly filled\n    # with the current item. So, `ideal_capacity = item`.\n    # `current_remaining_capacity` is `bins_remain_cap`.\n    #\n    # `Score = sigmoid(k * (item - bins_remain_cap))`\n    #\n    # For valid fits (`bins_remain_cap >= item`):\n    # `item - bins_remain_cap` is `0` or negative.\n    # If `k > 0`:\n    #   - `bins_remain_cap == item`: `item - bins_remain_cap = 0`. Score = `sigmoid(0) = 0.5`.\n    #   - `bins_remain_cap = item + epsilon` (small positive gap): `item - bins_remain_cap = -epsilon`. Score = `sigmoid(-k*epsilon)` < 0.5.\n    #   - `bins_remain_cap = item + large_delta`: `item - bins_remain_cap = -large_delta`. Score = `sigmoid(-k*large_delta)` -> 0.\n    #\n    # This means bins that are closest to being full (smallest positive remaining capacity after fitting the item) get lower scores.\n    # This seems counter-intuitive for \"good fit\".\n    #\n    # Let's redefine \"good fit\" as a bin that can accommodate the item without too much excess space.\n    # This means `bins_remain_cap - item` should be small and non-negative.\n    #\n    # We need a score that:\n    # 1. Is 0 if `bins_remain_cap < item`.\n    # 2. Peaks when `bins_remain_cap = item`.\n    # 3. Decreases as `bins_remain_cap` increases beyond `item`.\n    #\n    # Let's use `sigmoid(k * (target - value))`.\n    # `target` = `item`. `value` = `bins_remain_cap`.\n    # `sigmoid(k * (item - bins_remain_cap))`.\n    #\n    # We want the argument to be large when `item - bins_remain_cap` is large (meaning `bins_remain_cap` is much smaller than `item`).\n    #\n    # Alternative: Focus on the *slack*. Slack = `bins_remain_cap - item`.\n    # We want small slack.\n    #\n    # Let's use `sigmoid(a - k * slack)` with `a` large, `k > 0`.\n    # `slack = bins_remain_cap - item`.\n    #\n    # Consider the function `sigmoid(a - k * (bins_remain_cap - item))`.\n    # If `bins_remain_cap < item` (no fit):\n    #   `bins_remain_cap - item` is negative.\n    #   `a - k * (negative)` = `a + k * abs(negative)`. If `a` is large, this is large positive.\n    #   So, score is close to 1. This is WRONG for no-fit bins.\n    #\n    # Let's enforce the \"no fit\" rule first.\n    # For bins where `bins_remain_cap < item`, the priority is 0.\n    # For bins where `bins_remain_cap >= item`:\n    # We want to maximize `score` when `bins_remain_cap - item` is small.\n    #\n    # Use `sigmoid(k * (ideal_fit - current_fit))`.\n    # `current_fit_diff = bins_remain_cap - item`.\n    # `ideal_fit_diff = 0`.\n    # So, `sigmoid(k * (0 - (bins_remain_cap - item))) = sigmoid(k * (item - bins_remain_cap))`.\n    #\n    # We saw this decreases for positive `bins_remain_cap - item`.\n    #\n    # To achieve higher score for smaller positive `bins_remain_cap - item`:\n    # We need the argument to `sigmoid` to be smaller when `bins_remain_cap - item` is larger.\n    # So, `sigmoid(k * (some_constant - (bins_remain_cap - item)))`.\n    # The constant is the `x0` where `k * (x0 - diff)` is 0.\n    #\n    # Let's set the \"ideal\" scenario as `bins_remain_cap = item`.\n    # `bins_remain_cap - item = 0`.\n    # We want a high score then.\n    #\n    # Consider `sigmoid(a + b * x)` where `x = bins_remain_cap`.\n    # We want score to be high when `bins_remain_cap` is `item`.\n    #\n    # The \"Sigmoid Fit\" heuristic typically implies that the priority is determined by how well an item fits into a bin, often favoring bins that are close to being full but still accommodate the item.\n    #\n    # Let's use the interpretation that the priority is proportional to `sigmoid(k * (b - item))`, where `b` is bin remaining capacity.\n    # We want to penalize bins where `b` is too large or too small.\n    #\n    # A common sigmoid fit aims to select bins where `b` is 'close' to `item`.\n    # Specifically, `b - item` is small and non-negative.\n    #\n    # A possible formulation for this \"closeness\":\n    #\n    # `priority = sigmoid(k * (center - (bins_remain_cap - item)))`\n    #\n    # where `center` is the ideal gap (e.g., 0) and `k` is steepness.\n    #\n    # If `center = 0` and `k > 0`:\n    # `priority = sigmoid(k * (0 - (bins_remain_cap - item)))`\n    # `priority = sigmoid(k * (item - bins_remain_cap))`\n    #\n    # For valid fits (`bins_remain_cap >= item`):\n    # `item - bins_remain_cap <= 0`\n    #   - `bins_remain_cap = item` => `item - bins_remain_cap = 0`. Score = `sigmoid(0) = 0.5`.\n    #   - `bins_remain_cap = item + epsilon` => `item - bins_remain_cap = -epsilon`. Score = `sigmoid(-k*epsilon)` < 0.5.\n    #   - `bins_remain_cap = item + large_delta` => `item - bins_remain_cap = -large_delta`. Score = `sigmoid(-k*large_delta)` -> 0.\n    #\n    # This gives higher priority to bins that are closer to being *completely full* after the item is placed. This is a valid strategy (trying to leave larger capacities).\n\n    # Let's define the sigmoid function.\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    # Parameters for the sigmoid.\n    # `k`: Steepness. Higher values mean the priority changes more rapidly around the midpoint.\n    # `x0`: The point where the sigmoid is 0.5. We want the \"ideal fit\" to be prioritized.\n    #\n    # The priority for a bin with remaining capacity `R` and item size `I` is usually designed to be high when `R` is close to `I` and `R >= I`.\n    #\n    # Consider the quantity `gap = R - I`. We want small, non-negative gaps to have high priority.\n    # A sigmoid function of the form `sigmoid(A - B * gap)` with `B > 0` will decrease as `gap` increases.\n    # To make it peak, we can introduce a bias or adjust the formulation.\n    #\n    # Let's re-evaluate the common \"Sigmoid Fit\" formulation:\n    # Priority(bin, item) = sigmoid(k * (capacity(bin) - size(item)))\n    #\n    # Using `bins_remain_cap` for `capacity(bin)` and `item` for `size(item)`.\n    # `priorities = sigmoid(k * (bins_remain_cap - item))`\n    #\n    # Let's analyze `sigmoid(k * (bins_remain_cap - item))`:\n    # If `k > 0`:\n    #   - `bins_remain_cap = item`: `arg = 0`. `sigmoid(0) = 0.5`.\n    #   - `bins_remain_cap = item + epsilon` (small positive gap): `arg = k*epsilon`. `sigmoid(k*epsilon)` > 0.5. Higher priority.\n    #   - `bins_remain_cap = item + large_delta`: `arg = k*large_delta`. `sigmoid(k*large_delta)` -> 1. Highest priority.\n    #   - `bins_remain_cap < item`: `arg` is negative and large. `sigmoid(negative)` -> 0. Lowest priority.\n    #\n    # This formulation prioritizes bins that have *ample* space remaining. This is the \"Worst Fit\" idea applied via sigmoid.\n    #\n    # The goal is usually to find a \"good fit\", meaning the remaining capacity is *just enough*.\n    # So we want `bins_remain_cap - item` to be small and non-negative.\n    #\n    # To achieve this, we need a function that decreases as `bins_remain_cap - item` increases.\n    # Let's use `sigmoid(k * (ideal - actual))`.\n    # `ideal = item`. `actual = bins_remain_cap`.\n    # `priority = sigmoid(k * (item - bins_remain_cap))`.\n    #\n    # With `k > 0`:\n    #   - `bins_remain_cap = item`: `arg = 0`. `sigmoid(0) = 0.5`.\n    #   - `bins_remain_cap = item + epsilon` (small positive gap): `arg = -k*epsilon`. `sigmoid(-k*epsilon)` < 0.5. Lower priority.\n    #   - `bins_remain_cap = item + large_delta`: `arg = -k*large_delta`. `sigmoid(-k*large_delta)` -> 0. Lowest priority.\n    #   - `bins_remain_cap < item`: `arg` is positive and large. `sigmoid(positive)` -> 1. Highest priority for bins that don't fit? This is wrong.\n\n    # Let's try a sigmoid centered on the \"good fit\" condition.\n    # We want `bins_remain_cap` to be `item`.\n    # Let's define `diff = bins_remain_cap - item`. We want small `diff >= 0`.\n    #\n    # Consider `sigmoid(a - k * diff)`.\n    # If `diff` is small positive, we want a high score. This means `a - k*diff` should be large. Requires `k > 0` and `a` to be sufficiently large to counter positive `diff`.\n    # If `diff` is large positive, we want a low score. This means `a - k*diff` should be small.\n    #\n    # For the \"no fit\" case (`diff < 0`), we need the score to be 0.\n    #\n    # Let's refine the parameters for `sigmoid(a - k * (bins_remain_cap - item))`.\n    #\n    # We need `a - k * (bins_remain_cap - item)` to be large for small positive `bins_remain_cap - item`.\n    # Let's set `a` to offset the negative impact of `bins_remain_cap - item`.\n    #\n    # Consider the behavior:\n    # 1. Bins where `bins_remain_cap < item`: priority should be 0.\n    # 2. Bins where `bins_remain_cap == item`: priority should be high (e.g., 1).\n    # 3. Bins where `bins_remain_cap = item + epsilon` (small excess capacity): priority should be high but less than 1 (e.g., 0.75).\n    # 4. Bins where `bins_remain_cap = item + large_delta` (large excess capacity): priority should be low (e.g., 0.1).\n    #\n    # Let's use the form: `sigmoid(k * (x_center - x_value))`.\n    # We want to prioritize when `bins_remain_cap` is close to `item`.\n    # Let `x_value = bins_remain_cap`.\n    # Let `x_center = item`.\n    # `sigmoid(k * (item - bins_remain_cap))`\n    #\n    # For valid fits (`bins_remain_cap >= item`):\n    # `item - bins_remain_cap` is non-positive.\n    #\n    # To get high scores for small non-positive values of `item - bins_remain_cap`:\n    # We need `k * (item - bins_remain_cap)` to be large positive.\n    # This implies `k < 0` if `item - bins_remain_cap` is negative.\n    #\n    # Let's reverse the formulation to use positive exponents for sigmoid argument to get higher scores for small positive differences.\n    #\n    # `priority = sigmoid(k * (bins_remain_cap - item))` where `k` is chosen such that higher `bins_remain_cap` values are preferred IF they fit.\n    #\n    # This is effectively the \"Worst Fit\" strategy transformed into a sigmoid.\n    # If the aim is \"Best Fit\" using sigmoid:\n    # We want bins where `bins_remain_cap - item` is small and non-negative.\n    #\n    # Let `delta = bins_remain_cap - item`.\n    # If `delta < 0`, priority = 0.\n    # If `delta >= 0`:\n    #   We want priority to be high for small `delta`.\n    #   Consider `f(delta) = exp(-k * delta)` for `delta >= 0`. This decreases.\n    #   Transforming to sigmoid:\n    #   `sigmoid(A - B * delta)`.\n    #   If `B > 0`, this decreases. We need `A` large.\n    #   Let `A = k * X_ideal`, where `X_ideal` is the target `delta`. We want `X_ideal = 0`.\n    #   So, `sigmoid(k * (0 - delta)) = sigmoid(-k * delta)`.\n    #   For `delta >= 0`, this gives values less than 0.5, decreasing towards 0.\n    #   This means bins that are closer to being full after placement get lower priority.\n    #   This is again \"Worst Fit\" kind of logic.\n    #\n    # The problem statement implies prioritizing \"good fits\".\n    # \"A good fit is defined as a bin where the remaining capacity is slightly larger than the item size.\"\n    # This means `bins_remain_cap` should be slightly larger than `item`.\n    # So, `bins_remain_cap - item = epsilon > 0` (small positive).\n    #\n    # Let `delta = bins_remain_cap - item`.\n    # We want high scores when `delta` is small and positive.\n    #\n    # Use `sigmoid(k * (target_delta - actual_delta))`.\n    # `target_delta = epsilon_small` (e.g., 0.1).\n    # `actual_delta = bins_remain_cap - item`.\n    #\n    # `priority = sigmoid(k * (epsilon_small - (bins_remain_cap - item)))`\n    #\n    # For valid fits (`bins_remain_cap >= item`):\n    #   `epsilon_small - (bins_remain_cap - item)`\n    #   - If `bins_remain_cap = item + epsilon_small`: arg = 0. sigmoid(0) = 0.5. Peak priority.\n    #   - If `bins_remain_cap = item + epsilon_small + small_positive`: arg = negative small. sigmoid(negative small) < 0.5. Lower priority.\n    #   - If `bins_remain_cap = item + epsilon_small + large_positive`: arg = negative large. sigmoid(negative large) -> 0. Lowest priority.\n    #   - If `bins_remain_cap = item - epsilon_small_negative` (effectively `bins_remain_cap < item`):\n    #     `epsilon_small - (-epsilon_small_negative) = epsilon_small + epsilon_small_negative`.\n    #     If `epsilon_small_negative` is such that `bins_remain_cap < item` then `bins_remain_cap - item < 0`.\n    #     So `epsilon_small - (bins_remain_cap - item)` will be `epsilon_small - (negative number)`, which is `epsilon_small + positive_number`, hence large positive.\n    #     `sigmoid(large_positive)` -> 1. This gives high priority to bins that don't fit. This is still problematic.\n\n    # Let's stick to the core \"Sigmoid Fit Score\" idea as commonly implemented, which focuses on\n    # prioritizing bins based on the relationship between remaining capacity and item size.\n    # A typical strategy is to score based on the 'fit difference'.\n    #\n    # Let `fit_diff = bins_remain_cap - item`.\n    # We are interested in the case where `fit_diff >= 0`.\n    # We want smaller `fit_diff` to result in higher scores.\n    #\n    # Let's use `sigmoid(k * (X - Y))` where `Y` is the 'actual' value and `X` is the 'ideal' value.\n    # Our 'ideal' for `bins_remain_cap` would be `item`.\n    #\n    # Consider `sigmoid(k * (item - bins_remain_cap))`.\n    # `k > 0`.\n    # - `bins_remain_cap < item`: `item - bins_remain_cap > 0`. Score -> 1. (Problematic)\n    # - `bins_remain_cap == item`: `item - bins_remain_cap = 0`. Score = 0.5.\n    # - `bins_remain_cap > item`: `item - bins_remain_cap < 0`. Score < 0.5 (decreases towards 0).\n    #\n    # This formulation gives high scores to bins that don't fit and low scores to bins with excess capacity.\n\n    # A standard \"Sigmoid Fit\" heuristic often seeks bins where `bins_remain_cap` is *just enough* for the item.\n    # This means prioritizing `bins_remain_cap` values that are slightly larger than `item`.\n    #\n    # Let's use a formulation that ensures no-fit bins get zero priority.\n    # For bins that fit (`bins_remain_cap >= item`):\n    #   Calculate `excess_capacity = bins_remain_cap - item`.\n    #   We want high priority for small `excess_capacity`.\n    #   This behavior is captured by a decreasing sigmoid.\n    #   `priority = sigmoid(k * (target_excess - actual_excess))`\n    #   Where `target_excess` is the ideal excess, ideally 0 or a small positive value.\n    #   Let's pick `target_excess = 0`.\n    #   `priority = sigmoid(k * (0 - (bins_remain_cap - item)))`\n    #   `priority = sigmoid(k * (item - bins_remain_cap))`\n    #\n    # To avoid issues with `bins_remain_cap < item`, we explicitly set their priority to 0.\n    #\n    # Let's set parameters:\n    # `k`: steepness. A value around 3-10 is common.\n    # `center`: where the priority is 0.5. For \"Sigmoid Fit\", we want this to be where `bins_remain_cap` is close to `item`.\n    #\n    # The formulation that prioritizes bins where `bins_remain_cap` is slightly larger than `item` implies that `bins_remain_cap - item` should be small and positive.\n    #\n    # Let `x = bins_remain_cap - item`.\n    # We want a score that is high for small `x >= 0`.\n    # This can be modeled by `sigmoid(a - k * x)` with `a` large and `k > 0`.\n    #\n    # Let `k = 5.0`. Let `a = k * ideal_excess`. We want `ideal_excess` to be a small positive number, say 0.05.\n    # So `a = 5.0 * 0.05 = 0.25`.\n    # `priority = sigmoid(0.25 - 5.0 * (bins_remain_cap - item))`\n    #\n    # Let's check:\n    # `bins_remain_cap = item + 0.05` (ideal fit):\n    #   `arg = 0.25 - 5.0 * (0.05) = 0.25 - 0.25 = 0`. `sigmoid(0) = 0.5`. Peak.\n    # `bins_remain_cap = item + 0.05 + 0.01` (slightly more excess):\n    #   `arg = 0.25 - 5.0 * (0.06) = 0.25 - 0.30 = -0.05`. `sigmoid(-0.05) < 0.5`. Lower.\n    # `bins_remain_cap = item + 0.05 - 0.01` (slightly less excess):\n    #   `arg = 0.25 - 5.0 * (0.04) = 0.25 - 0.20 = 0.05`. `sigmoid(0.05) > 0.5`. Higher.\n    # `bins_remain_cap = item + 0.05 + 0.5` (much more excess):\n    #   `arg = 0.25 - 5.0 * (0.55) = 0.25 - 2.75 = -2.5`. `sigmoid(-2.5)` -> 0. Low.\n    #\n    # This prioritizes bins where the remaining capacity is slightly larger than the item.\n    #\n    # What about the `bins_remain_cap < item` case?\n    # `bins_remain_cap = item - 0.05` (doesn't fit)\n    #   `arg = 0.25 - 5.0 * (-0.05) = 0.25 + 0.25 = 0.5`. `sigmoid(0.5) > 0.5`.\n    #\n    # To enforce zero priority for non-fitting bins:\n    # Mask the non-fitting bins.\n\n    # Parameters for the sigmoid function.\n    # `k_steepness`: Controls how quickly the priority score changes around the 'ideal fit'.\n    # `ideal_gap`: The preferred difference between remaining capacity and item size.\n    #              A small positive value here means we prefer bins that are 'almost full'.\n    k_steepness = 5.0\n    ideal_gap = 0.05  # Prefer bins with a remaining capacity that is slightly larger than the item size.\n\n    # Calculate the 'argument' for the sigmoid function.\n    # The core idea is `sigmoid(k * (ideal_value - actual_value))`.\n    # Here, the 'value' we care about is `bins_remain_cap - item`.\n    # The 'ideal value' for this difference is `ideal_gap`.\n    #\n    # So, the argument is `k_steepness * (ideal_gap - (bins_remain_cap - item))`.\n    # This simplifies to `k_steepness * (ideal_gap + item - bins_remain_cap)`.\n\n    # Initialize priorities to zero (for bins where the item doesn't fit).\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins where the item can actually fit.\n    fits_mask = bins_remain_cap >= item\n\n    # For bins that fit, calculate the priority score using the sigmoid function.\n    # The argument to sigmoid is designed such that:\n    # - When `bins_remain_cap - item == ideal_gap`, the argument is 0, sigmoid(0) = 0.5 (peak priority).\n    # - When `bins_remain_cap - item` is larger than `ideal_gap`, the argument is negative, leading to scores < 0.5.\n    # - When `bins_remain_cap - item` is smaller than `ideal_gap` (but still >= 0), the argument is positive, leading to scores > 0.5.\n    #   This part might be counter-intuitive if we strictly want \"slightly larger\" to be peak.\n    #\n    # Let's reconsider the function form to match the definition:\n    # \"A good fit is defined as a bin where the remaining capacity is slightly larger than the item size.\"\n    # This means we want to maximize score when `bins_remain_cap - item` is small and positive.\n    #\n    # The function `sigmoid(a - k*x)` where `x = bins_remain_cap - item` with `k>0` works.\n    # The argument `a - k*x` should be large for small positive `x`.\n    #\n    # Let the 'peak' occur when `bins_remain_cap - item = ideal_gap`.\n    # Argument at peak: `a - k * ideal_gap`. This should correspond to the 'center' of the sigmoid for peak value.\n    # Let's set `a - k * ideal_gap = 0`. So `a = k * ideal_gap`.\n    #\n    # The formula becomes: `sigmoid(k * ideal_gap - k * (bins_remain_cap - item))`\n    # `sigmoid(k * (ideal_gap - (bins_remain_cap - item)))`\n    # This is what we had before.\n\n    # Let's use the 'a' value to shift the sigmoid curve so the peak (0.5 score) is at `ideal_gap`.\n    # We'll calculate `argument = k_steepness * (ideal_gap - (bins_remain_cap - item))` only for fitting bins.\n    #\n    # `bins_remain_cap` in `fits_mask` array.\n    # `excess_capacities = bins_remain_cap[fits_mask] - item`\n\n    # The argument: `k_steepness * (ideal_gap - excess_capacities)`\n    # Let's shift it so that at `excess_capacities == ideal_gap`, the argument is 0.\n    # This is already the case with `ideal_gap` being the `ideal_value`.\n\n    # A more robust way to achieve peaking at `ideal_gap`:\n    # Map `excess_capacities` such that `ideal_gap` maps to 0.\n    # `mapped_value = excess_capacities - ideal_gap`.\n    # Then apply `sigmoid(-k * mapped_value)` to get higher scores for smaller `mapped_value`.\n    # `sigmoid(-k * (excess_capacities - ideal_gap))`\n    # `= sigmoid(k * (ideal_gap - excess_capacities))`\n    # This is indeed the same form.\n\n    # Apply the sigmoid to the calculated arguments for the fitting bins.\n    argument_for_sigmoid = k_steepness * (ideal_gap - (bins_remain_cap[fits_mask] - item))\n    priorities[fits_mask] = sigmoid(argument_for_sigmoid)\n\n    # Ensure priorities are within [0, 1] range, though sigmoid inherently does this.\n    # priorities = np.clip(priorities, 0, 1) # Not strictly necessary with sigmoid.\n\n    return priorities",
    "response_id": 16,
    "obj": 4.487435181491823,
    "SLOC": 16.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response18.txt_stdout.txt",
    "code_path": "problem_iter1_code18.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Exploration rate\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Determine which bins can potentially fit the item\n    valid_bins_indices = np.where(bins_remain_cap >= item)[0]\n\n    if len(valid_bins_indices) == 0:\n        # No bin can fit the item, return all zeros (or handle appropriately)\n        return np.zeros(num_bins)\n\n    # Greedy part: prioritize bins that leave the least remaining capacity\n    # This is the \"best fit\" heuristic within the greedy selection.\n    greedy_scores = bins_remain_cap[valid_bins_indices] - item\n\n    # Apply epsilon-greedy strategy\n    if np.random.rand() < epsilon:\n        # Exploration: randomly choose one of the valid bins\n        chosen_bin_index_in_valid = np.random.randint(0, len(valid_bins_indices))\n        chosen_bin_global_index = valid_bins_indices[chosen_bin_index_in_valid]\n        priorities[chosen_bin_global_index] = 1.0  # Assign highest priority to the chosen bin\n    else:\n        # Exploitation: choose the bin that minimizes remaining capacity (best fit)\n        best_fit_index_in_valid = np.argmin(greedy_scores)\n        best_fit_global_index = valid_bins_indices[best_fit_index_in_valid]\n        priorities[best_fit_global_index] = 1.0 # Assign highest priority to the best fit bin\n\n    # For bins that *can* fit the item but are not chosen by epsilon-greedy,\n    # we can assign a small non-zero priority to still allow them a chance,\n    # or simply keep them at zero if we only want one bin to have the highest score.\n    # For simplicity here, we only give priority to the chosen bin.\n\n    return priorities",
    "response_id": 18,
    "obj": 4.148384523334677,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response29.txt_stdout.txt",
    "code_path": "problem_iter1_code29.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    The Softmax-Based Fit strategy assigns higher priorities to bins that can\n    accommodate the item and are \"closer\" to being full after the item is packed.\n    This encourages tighter packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to negative infinity for bins that cannot fit the item\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can accommodate the item\n    fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate a score.\n    # The score is based on the remaining capacity *after* the item is packed.\n    # We want to prioritize bins where remaining_capacity - item is small (tighter fit).\n    # A simple approach is to use the negative of the remaining capacity after packing.\n    # Softmax will then turn these scores into probabilities.\n    if np.any(fit_mask):\n        remaining_after_fit = bins_remain_cap[fit_mask] - item\n        # We want smaller remaining capacity to have higher priority.\n        # So, we use the negative of the remaining capacity.\n        # Adding a small epsilon to avoid log(0) issues if remaining capacity becomes exactly 0\n        # or for numerical stability if values are very close to zero.\n        priorities[fit_mask] = -remaining_after_fit + 1e-9\n\n    # Apply softmax to convert scores into probabilities (priorities)\n    # Ensure the exponentiation is stable by subtracting the maximum score\n    # before applying exp, which doesn't change the relative probabilities.\n    if np.any(priorities > -np.inf):\n        max_priority = np.max(priorities[priorities > -np.inf])\n        exp_priorities = np.exp(priorities - max_priority)\n        # Normalize to get probabilities summing to 1 (for the fitting bins)\n        sum_exp_priorities = np.sum(exp_priorities[priorities > -np.inf])\n        if sum_exp_priorities > 0:\n            priorities = exp_priorities / sum_exp_priorities\n        else:\n            # If for some reason sum is zero (e.g., all fitting priorities were -inf),\n            # reset to uniform probability for fitting bins.\n            priorities[priorities > -np.inf] = 1.0 / np.sum(fit_mask)\n    else:\n        # If no bins can fit the item, all priorities remain -inf, or can be set to 0.\n        # For consistency with Softmax, let's keep them as they are.\n        pass\n\n    return priorities",
    "response_id": 29,
    "obj": 4.11846828879138,
    "SLOC": 17.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority for packing an item into bins using an Exact Fit First strategy\n    with Best Fit as a tie-breaker for non-exact fits, and an exploration component.\n\n    The strategy prioritizes bins that perfectly fit the item (Exact Fit).\n    Among bins that do not offer an exact fit but can accommodate the item,\n    it prioritizes those that leave the least remaining capacity (Best Fit).\n    An epsilon-greedy approach is incorporated to allow for exploration of non-optimal bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    epsilon = 0.05  # Exploration rate\n    num_bins = len(bins_remain_cap)\n    priorities = np.full(num_bins, -np.inf, dtype=float)  # Initialize with a very low priority\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    fittable_indices = np.where(can_fit_mask)[0]\n\n    if len(fittable_indices) == 0:\n        return priorities  # No bin can fit the item\n\n    # Calculate remaining capacities for fittable bins\n    remaining_after_fit = bins_remain_cap[fittable_indices] - item\n\n    # Separate exact fits and non-exact fits\n    exact_fit_fittable_indices = fittable_indices[remaining_after_fit == 0]\n    non_exact_fit_fittable_indices = fittable_indices[remaining_after_fit > 0]\n\n    # --- Assign priorities ---\n\n    # 1. Exact Fits: Highest priority.\n    #    Among exact fits, prioritize bins that were initially closer to fitting the item\n    #    (i.e., smaller `bins_remain_cap` before placing the item).\n    #    A large base priority is used to ensure they are favored.\n    base_exact_fit_priority = 1000.0\n    if len(exact_fit_fittable_indices) > 0:\n        exact_fit_initial_caps = bins_remain_cap[exact_fit_fittable_indices]\n        # Score = Base Priority + (Max Initial Capacity among exact fits - Current Initial Capacity)\n        # This assigns higher scores to bins that were smaller but still exact fits.\n        max_initial_cap_exact_fits = np.max(exact_fit_initial_caps)\n        exact_fit_priorities = base_exact_fit_priority + (max_initial_cap_exact_fits - exact_fit_initial_caps)\n        priorities[exact_fit_fittable_indices] = exact_fit_priorities\n\n    # 2. Non-Exact Fits (Best Fit): Lower priority than exact fits.\n    #    Prioritize bins that leave the least remaining capacity after fitting the item.\n    #    The score is derived from `-(remaining_after_fit)` so that smaller remaining capacities\n    #    result in higher (less negative) scores. We shift these scores down to be strictly\n    #    less than the exact fit priorities.\n    offset_for_non_exact_fits = 500.0 # Ensures non-exact fits are lower priority than exact fits\n    if len(non_exact_fit_fittable_indices) > 0:\n        # Score = Offset - Remaining Capacity after fit\n        # Smaller remaining capacity yields a higher score.\n        non_exact_fit_priorities = offset_for_non_exact_fits - remaining_after_fit[remaining_after_fit > 0]\n        priorities[non_exact_fit_fittable_indices] = non_exact_fit_priorities\n\n    # --- Epsilon-Greedy Exploration ---\n    # With probability epsilon, choose a random fittable bin.\n    # Otherwise, choose the bin with the highest calculated priority (exploitation).\n\n    if np.random.rand() < epsilon and len(fittable_indices) > 0:\n        # Exploration: Choose a random bin among those that can fit the item.\n        # Assign it the highest possible priority to ensure it's picked if chosen.\n        exploration_bin_index = np.random.choice(fittable_indices)\n        # Assign a priority that is guaranteed to be the highest if this bin is chosen\n        # A value higher than any possible deterministic priority.\n        highest_priority_for_exploration = np.max(priorities[fittable_indices]) + 1.0\n        priorities = np.full(num_bins, -np.inf, dtype=float) # Reset priorities for exploration\n        priorities[exploration_bin_index] = highest_priority_for_exploration\n    else:\n        # Exploitation: The priorities are already set based on Exact Fit / Best Fit.\n        # If no bin can fit, priorities remain -inf.\n        pass\n\n    return priorities",
    "response_id": 0,
    "obj": 4.098524132429212,
    "SLOC": 29.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response7.txt_stdout.txt",
    "code_path": "problem_iter2_code7.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a combination of Best Fit and a novel 'Worst Fit Reduction' strategy.\n\n    This heuristic aims to:\n    1. Favor bins that result in a tight fit (similar to Best Fit).\n    2. Introduce a secondary preference for bins that, after packing, still have substantial remaining capacity,\n       thereby \"reserving\" tighter-fitting bins for potentially smaller future items. This is a form of\n       \"Worst Fit Reduction\" in that we reduce the \"waste\" by not over-filling bins that could\n       accommodate more.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate scores for bins that can fit the item\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n    fitting_bins_remain_cap = bins_remain_cap[fitting_bins_indices]\n\n    # Score 1: Best Fit component - prioritize bins that leave less remaining capacity\n    # We use the negative of remaining capacity to make larger negative values (smaller remaining capacity) higher.\n    best_fit_scores = -(fitting_bins_remain_cap - item)\n\n    # Score 2: Worst Fit Reduction component - penalize bins that are too \"full\" after packing,\n    # giving a slight preference to bins that have more remaining capacity.\n    # This can be achieved by adding a small factor of the remaining capacity.\n    # We add a small constant to avoid division by zero if remaining capacity is 0.\n    # This term should be smaller than the best_fit_scores to ensure best fit is dominant.\n    # The scale of this term needs careful tuning; a small positive value is used here.\n    worst_fit_reduction_scores = (fitting_bins_remain_cap - item) * 0.1\n\n    # Combine scores. We want to maximize the combined score.\n    # Prioritize bins that are a good fit AND don't become too full.\n    # Higher scores mean higher priority.\n    combined_scores = best_fit_scores + worst_fit_reduction_scores\n\n    # Normalize scores to a probability distribution using softmax.\n    # Subtract max to prevent overflow during exponentiation and ensure numerical stability.\n    max_score = np.max(combined_scores)\n    exp_scores = np.exp(combined_scores - max_score)\n    sum_exp_scores = np.sum(exp_scores)\n\n    if sum_exp_scores > 0:\n        normalized_priorities = exp_scores / sum_exp_scores\n    else:\n        # If all scores are effectively -inf after subtractions, assign uniform probability.\n        normalized_priorities = np.ones_like(combined_scores) / len(combined_scores)\n\n    # Assign the calculated priorities to the original bins array\n    priorities[fitting_bins_indices] = normalized_priorities\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "SLOC": 20.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority score for each bin using a hybrid approach prioritizing good fits and balancing exploration.\n\n    This heuristic prioritizes bins that offer a \"good fit\" for the item, meaning\n    their remaining capacity is slightly larger than the item size. It also\n    incorporates an exploration component to occasionally consider less-full bins,\n    balancing the \"best fit\" tendency with exploration of potential future packing\n    opportunities.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array containing the remaining capacity of each bin.\n\n    Returns:\n        A numpy array of priority scores for each bin. Higher scores indicate a\n        higher preference for placing the item in that bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploration (choosing a bin less greedily)\n    k_steepness = 5.0  # Steepness parameter for the sigmoid function\n    ideal_gap = 0.05  # The preferred remaining capacity beyond the item size\n\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins)\n\n    # Identify bins where the item can fit\n    fits_mask = bins_remain_cap >= item\n    fitting_bins_indices = np.where(fits_mask)[0]\n\n    # If no bins can fit the item, return all zeros.\n    if len(fitting_bins_indices) == 0:\n        return priorities\n\n    # --- Exploitation Component: Prioritize Good Fits ---\n    # Calculate the \"excess capacity\" for bins where the item fits.\n    # A good fit has a small, positive excess capacity.\n    excess_capacities = bins_remain_cap[fitting_bins_indices] - item\n\n    # Use a sigmoid function to score the \"goodness\" of the fit.\n    # We want the score to peak when `excess_capacities` is close to `ideal_gap`.\n    # The argument for the sigmoid is designed so that:\n    # - When `excess_capacities == ideal_gap`, the argument is 0, sigmoid(0) = 0.5 (peak of the \"good fit\" score).\n    # - When `excess_capacities` deviates from `ideal_gap` (either smaller or larger), the score decreases.\n    # This formulation penalizes bins that are too empty or too full relative to the ideal gap.\n    sigmoid_argument = k_steepness * (ideal_gap - excess_capacities)\n    exploitation_scores = 1 / (1 + np.exp(-sigmoid_argument))\n\n    # Normalize exploitation scores so they sum to 1 among fitting bins.\n    # This helps in combining with exploration probabilities later if needed, or for clearer interpretation.\n    # However, for direct preference scores, normalization might not be strictly required if scaled appropriately.\n    # Let's scale them to be within a reasonable range for prioritization.\n    # We can scale the 0.5 peak to a higher value for better differentiation if needed.\n    # For now, let's use the raw sigmoid output (0 to 1).\n\n    # --- Exploration Component ---\n    # With probability epsilon, we want to explore other options.\n    # We can assign a uniform, lower priority to all fitting bins to represent exploration.\n    # The probability of choosing an exploratory bin is epsilon.\n    # If we explore, we pick one of the fitting bins uniformly.\n    # So, each fitting bin gets an additional 'exploration boost'.\n    exploration_boost = epsilon / len(fitting_bins_indices)\n\n    # --- Combine Exploitation and Exploration ---\n    # The final priority for fitting bins is a weighted sum:\n    # (1 - epsilon) * exploitation_score + epsilon * uniform_exploration_score\n    # Here, the 'uniform_exploration_score' is implicitly handled by adding\n    # the exploration boost.\n\n    # Apply the combined strategy:\n    # For fitting bins: priority = (1-epsilon) * normalized_exploitation_score + epsilon * (uniform score)\n    # A simpler way to think is that the final score is a mix.\n    # Let's consider the scores as probabilities of selection for a simplified view.\n    # We want to assign scores reflecting preference.\n\n    # Let's use the exploitation scores directly, but add an exploration bonus.\n    # The exploration bonus makes less-preferred bins (by exploitation score) more competitive.\n    # We add a constant exploration value to all fitting bins to ensure some randomness.\n    # The value of exploration_boost is small, relative to the peak exploitation score (0.5).\n    # We can add a scaled exploration value.\n    exploration_value = epsilon * 0.5 # A smaller constant exploration value\n\n    # The final priorities are a mix. We can think of it as:\n    # A base score derived from exploitation, with a small random jitter or uniform boost for exploration.\n    priorities[fitting_bins_indices] = exploitation_scores + exploration_value\n\n    # Ensure scores are non-negative.\n    priorities = np.maximum(priorities, 0)\n\n    # Optional: Normalize priorities to sum to 1 if they represent probabilities of selection,\n    # or just return them as preference scores. For selection, normalization is common.\n    # Let's return them as preference scores, allowing the caller to normalize if needed.\n    # A common strategy is to use these scores with a softmax-like selection mechanism.\n\n    # For simplicity and direct preference, we can scale the exploitation scores\n    # and add a smaller exploration component.\n    # Let's re-scale exploitation_scores to have a max value higher than 0.5, e.g., 1.\n    # Max exploitation score is 1 (when sigmoid_argument is large positive).\n    # Min exploitation score is 0 (when sigmoid_argument is large negative).\n    # Peak is 0.5.\n\n    # Let's try a different combination:\n    # Prioritize bins based on exploitation score, but ensure some chance for others.\n    # If random draw < epsilon, pick a random fitting bin. Otherwise, pick based on exploitation.\n    # This is more of a selection logic. For generating priority scores:\n    # We want to combine the 'good fit' score with an exploration factor.\n\n    # A common way to represent this combination is:\n    # priority = (1-epsilon) * exploitation_score + epsilon * uniform_score\n    # Where uniform_score is 1/num_fitting_bins.\n    # Let's re-normalize exploitation scores first to sum to 1 for clarity in combination.\n    if np.sum(exploitation_scores) > 0:\n        normalized_exploitation_scores = exploitation_scores / np.sum(exploitation_scores)\n    else:\n        normalized_exploitation_scores = np.zeros_like(exploitation_scores) # Should not happen if fitting_bins exist\n\n    # Uniform score for exploration among fitting bins\n    uniform_exploration_score = 1.0 / len(fitting_bins_indices) if len(fitting_bins_indices) > 0 else 0.0\n\n    # Combine them:\n    combined_priorities = (1 - epsilon) * normalized_exploitation_scores + epsilon * uniform_exploration_score\n\n    # Place these combined priorities back into the main priorities array\n    priorities[fitting_bins_indices] = combined_priorities\n\n    # Ensure all priorities are non-negative (should already be true)\n    priorities = np.maximum(priorities, 0)\n\n    return priorities",
    "response_id": 9,
    "obj": 4.048663741523748,
    "SLOC": 26.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response0.txt_stdout.txt",
    "code_path": "problem_iter3_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a tunable sigmoid and softmax.\n\n    This heuristic aims to prioritize bins that offer the \"best fit\" for the item,\n    defined as bins where the remaining capacity is only slightly larger than the item's size.\n    It uses a sigmoid function to assign higher scores to these \"tight fits\".\n    The sigmoid's steepness and ideal gap are tunable.\n\n    To encourage exploration among equally good or near-equally good bins,\n    a softmax function is applied to the sigmoid scores. This normalizes scores\n    into probabilities, allowing for probabilistic selection and exploration.\n    Bins that cannot fit the item are assigned a priority of 0.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element is the remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as `bins_remain_cap`, containing the priority\n        score for each bin. Higher scores indicate a more desirable bin for the item.\n    \"\"\"\n\n    def sigmoid(x, steepness=15.0, ideal_gap=0.02):\n        \"\"\"A sigmoid function that peaks at x=0, and can be scaled and shifted.\n        Here, x is effectively (bin_capacity - item_size - ideal_gap).\n        A smaller positive `ideal_gap` means tighter fits are preferred.\n        A larger `steepness` makes the preference for `ideal_gap` more pronounced.\n        \"\"\"\n        # We want the sigmoid to peak when (bin_cap - item) is close to ideal_gap.\n        # So, we map (bin_cap - item) to the sigmoid's input.\n        # Let sigmoid_input = steepness * (ideal_gap - (bin_cap - item))\n        # This means when bin_cap - item = ideal_gap, sigmoid_input = 0, and sigmoid output = 0.5.\n        # We want higher scores for smaller positive gaps.\n        # If bin_cap - item = 0 (perfect fit), sigmoid_input = steepness * ideal_gap > 0, sigmoid output > 0.5.\n        # If bin_cap - item = 0.1, sigmoid_input = steepness * (ideal_gap - 0.1). If ideal_gap is small, this is negative.\n        return 1 / (1 + np.exp(-x))\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit.\n    fits_mask = bins_remain_cap >= item\n\n    if np.any(fits_mask):\n        # Calculate the excess capacity for fitting bins.\n        excess_capacities = bins_remain_cap[fits_mask] - item\n\n        # Tunable parameters for the sigmoid function.\n        # steepness: controls how quickly the priority drops as excess capacity deviates from ideal_gap.\n        # ideal_gap: the preferred small positive residual capacity after packing the item.\n        steepness = 15.0\n        ideal_gap = 0.02\n\n        # Calculate the argument for the sigmoid.\n        # We want the sigmoid to output higher values for smaller, non-negative `excess_capacities`.\n        # The sigmoid `1 / (1 + exp(-x))` has its midpoint at x=0.\n        # To map `excess_capacities` such that `ideal_gap` gives a good score, we use:\n        # `sigmoid_arg = steepness * (ideal_gap - excess_capacities)`\n        # If `excess_capacities` is slightly less than `ideal_gap` (a good fit), `sigmoid_arg` is positive, sigmoid > 0.5.\n        # If `excess_capacities` is exactly `ideal_gap`, `sigmoid_arg` is 0, sigmoid = 0.5.\n        # If `excess_capacities` is larger than `ideal_gap`, `sigmoid_arg` is negative, sigmoid < 0.5.\n        sigmoid_arg = steepness * (ideal_gap - excess_capacities)\n\n        # Calculate raw sigmoid scores for fitting bins.\n        raw_scores = sigmoid(sigmoid_arg, steepness=steepness, ideal_gap=ideal_gap)\n\n        # Apply softmax to the scores. This normalizes scores into a probability distribution,\n        # allowing for probabilistic exploration. Bins with similar high scores will have\n        # non-zero probabilities assigned, encouraging trying different \"good\" bins.\n        # Add a small epsilon to avoid issues with exp(very large negative numbers) if all are bad fits.\n        # However, since we only calculate for fitting bins, this is less of a concern.\n        # We are using the raw scores as inputs to softmax, not probabilities directly.\n        # The 'temperature' parameter in softmax can be adjusted to control exploration.\n        # A higher temperature leads to more uniform probabilities. A lower temperature\n        # leads to probabilities concentrated on the highest score.\n        temperature = 1.0 # Tunable exploration parameter\n\n        # Avoid numerical instability with softmax if all `sigmoid_arg` are very large negative (unlikely here)\n        # or very large positive.\n        # For this application, `sigmoid_arg` will be mostly negative or slightly positive.\n        # A simple softmax on the `raw_scores` is usually sufficient.\n        exp_scores = np.exp(raw_scores / temperature)\n        sum_exp_scores = np.sum(exp_scores)\n\n        if sum_exp_scores > 0:\n            priorities[fits_mask] = exp_scores / sum_exp_scores\n        else:\n            # Fallback: if all exp_scores are zero or NaN (highly unlikely for valid inputs)\n            # distribute probability uniformly among fitting bins.\n            priorities[fits_mask] = 1.0 / np.sum(fits_mask) if np.sum(fits_mask) > 0 else 0\n\n    # Bins that do not fit have a priority of 0.\n    return priorities",
    "response_id": 0,
    "obj": 4.108496210610296,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response3.txt_stdout.txt",
    "code_path": "problem_iter3_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a tunable sigmoid and varied exploration.\n\n    This heuristic aims to prioritize bins that offer the \"best fit\" for the item.\n    A \"best fit\" is defined as a bin where the remaining capacity is only slightly\n    larger than the item's size. This strategy tries to fill bins as much as possible\n    without leaving excessive empty space, thereby minimizing fragmentation.\n\n    The priority is calculated using a sigmoid function. The function is designed\n    to peak when the remaining capacity (`bins_remain_cap`) is precisely equal to\n    the item's size, and the priority decreases as the remaining capacity deviates\n    (either smaller or larger). However, bins where the item doesn't fit at all\n    are assigned a zero priority.\n\n    Tie-breaking is handled using a softmax-like approach on top of the sigmoid scores.\n    This introduces a probabilistic element, favoring bins with higher scores more\n    often but not exclusively, allowing for exploration of less ideal fits.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array where each element is the remaining capacity of a bin.\n\n    Returns:\n        A numpy array of the same size as `bins_remain_cap`, containing the priority\n        score for each bin. Higher scores indicate a more desirable bin for the item.\n    \"\"\"\n\n    def sigmoid(x, steepness=10.0, center=0.0):\n        \"\"\"A custom sigmoid function that can be shifted and scaled.\"\"\"\n        # Ensure numerical stability for large negative exponents\n        exponent = -steepness * (x - center)\n        # Clip exponent to avoid overflow in exp, then compute sigmoid\n        # A large negative exponent approaches 0, large positive approaches 1.\n        # Clipping at -700 is a common practice for exp(-700) ~ 1e-304\n        clipped_exponent = np.clip(exponent, -700, 700)\n        return 1 / (1 + np.exp(clipped_exponent))\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit.\n    fits_mask = bins_remain_cap >= item\n\n    if np.any(fits_mask):\n        # Calculate the excess capacity for fitting bins.\n        excess_capacities = bins_remain_cap[fits_mask] - item\n\n        # Tunable parameters for the sigmoid function\n        # `ideal_gap` defines the target excess capacity for the best fit.\n        # A smaller `ideal_gap` means we prefer bins that leave very little space.\n        # `steepness` controls how quickly the priority drops as excess capacity deviates from `ideal_gap`.\n        ideal_gap = 0.05  # Target for minimal positive residual space\n        steepness = 15.0  # Increased steepness for sharper preference\n\n        # Calculate the argument for the sigmoid function.\n        # We want the peak of the sigmoid (where the argument is 0) to align with our 'ideal_gap'.\n        # The argument is `steepness * (ideal_gap - excess_capacities)`.\n        # This means when `excess_capacities` is close to `ideal_gap`, the argument is close to 0,\n        # resulting in a sigmoid output close to 0.5 (midpoint).\n        # Scores > 0.5 for `excess_capacities` < `ideal_gap`\n        # Scores < 0.5 for `excess_capacities` > `ideal_gap`\n        argument_values = steepness * (ideal_gap - excess_capacities)\n\n        # Apply the sigmoid function to get raw scores for fitting bins.\n        raw_scores = sigmoid(argument_values, steepness=steepness, center=0.0)\n\n        # Normalize scores using a softmax-like approach for exploration.\n        # This converts scores into probabilities, allowing for probabilistic selection\n        # and thus exploration of bins that are not strictly the \"best\" fit.\n        # A small epsilon is added to prevent issues with all scores being identical.\n        # A temperature parameter could be introduced here for more control over exploration.\n        # For simplicity, we'll use the direct softmax on the scaled sigmoid outputs.\n        \n        # We want higher sigmoid scores to translate to higher probabilities.\n        # A simple softmax transformation: exp(score) / sum(exp(scores))\n        # However, direct softmax on scores that might be close to 0 or 1 can lead to extreme probabilities.\n        # A common approach in exploration is to add noise or use a temperature parameter.\n        # Let's scale the sigmoid output to a range that is more suitable for softmax, e.g., [0, 10]\n        # or simply use the sigmoid output directly if it's already in a reasonable range.\n        \n        # For this implementation, let's directly use the sigmoid output as \"desirability\".\n        # Higher desirability means a higher chance of selection.\n        # A simple way to implement \"varied exploration\" without explicit softmax is to\n        # slightly perturb the scores or use a mechanism like epsilon-greedy on these scores.\n        # However, the prompt implies a more direct score-based exploration.\n        # The \"softmax-like\" part can be interpreted as ensuring relative ordering is maintained\n        # and higher scores are disproportionately favored.\n        #\n        # Let's refine the \"softmax-like\" to mean we are generating relative weights.\n        # The sigmoid already provides a relative measure of \"goodness of fit\".\n        # The primary refinement for \"varied exploration\" beyond standard greedy would be:\n        # 1. Add small random noise to the scores.\n        # 2. Use the scores as weights in a weighted random choice.\n        #\n        # The reflection mentioned \"softmax for score normalization\".\n        # If we consider the raw sigmoid scores (0 to 1), softmax on these would yield\n        # probabilities summing to 1 across the *fitting* bins.\n        \n        # Option: Softmax on raw_scores\n        # exp_scores = np.exp(raw_scores)\n        # probabilities = exp_scores / np.sum(exp_scores)\n        # priorities[fits_mask] = probabilities\n\n        # Option: Scaled scores for softmax (e.g., if sigmoid scores are too clustered)\n        # scaled_scores = raw_scores * 5.0 # Scale to a range like [0, 5]\n        # exp_scaled_scores = np.exp(scaled_scores)\n        # probabilities = exp_scaled_scores / np.sum(exp_scaled_scores)\n        # priorities[fits_mask] = probabilities\n\n        # The current sigmoid output is already designed to be a priority score.\n        # The \"varied exploration\" might simply mean that the sigmoid output itself\n        # provides a graded preference, allowing a weighted selection mechanism\n        # (not implemented here but implied by \"priority score\") to explore.\n        # If we interpret \"softmax-like\" as creating relative probabilities, we can do that.\n        # However, a simpler interpretation for \"priority score\" is to just return\n        # the modulated sigmoid scores, and let the selection algorithm handle the exploration.\n\n        # Let's stick to the interpretation that the `priority_v2` function *outputs*\n        # scores that can be used for exploration. The sigmoid already provides a nuanced score.\n        # For tie-breaking and varied exploration, we can enhance the score or rely on the selection mechanism.\n        #\n        # A simple way to introduce variation without full softmax: add a small, scaled random component.\n        # This is akin to adding noise for exploration.\n        #\n        # Let's ensure the scores are in a somewhat predictable range and then use them.\n        # The sigmoid output is [0, 1].\n        \n        # For \"varied exploration\", we can make the scores slightly more distinct or add noise.\n        # Adding a small random noise:\n        # noise = np.random.normal(0, 0.05, raw_scores.shape) # Mean 0, std dev 0.05\n        # noisy_scores = raw_scores + noise\n        # Ensure scores remain within a valid range (e.g., > 0) for weighted selection.\n        # We can clamp or re-normalize.\n\n        # Let's interpret \"varied exploration\" as generating scores that are\n        # not purely greedy, but represent a soft preference. The sigmoid itself does this.\n        # The \"softmax for score normalization\" could mean scaling these priorities\n        # so they sum to a constant (e.g., 1 if using weighted random choice).\n        #\n        # The most direct interpretation is to simply return the refined sigmoid scores.\n        # The \"varied exploration\" aspect is then handled by how these scores are *used* by the calling algorithm\n        # (e.g., weighted random choice vs. simple argmax).\n        #\n        # If the intent is for this function itself to produce probabilities for a direct selection,\n        # then softmax is appropriate. Given the wording \"returns a priority score for each bin\",\n        # it suggests the output is a score, not necessarily a final probability.\n        #\n        # Let's refine the sigmoid output to ensure higher scores are more distinct,\n        # making the preference clearer for exploration.\n        # We can amplify the difference between scores.\n        \n        # A simple amplification of the sigmoid output might be:\n        # amplified_scores = raw_scores ** power_factor  (where power_factor > 1)\n        # Or a linear scaling and shifting that maintains order but increases separation.\n        \n        # Let's use the original sigmoid scores, as they are already graded.\n        # If \"varied exploration\" means probabilistic selection based on scores,\n        # the scores themselves are the input to that mechanism.\n        priorities[fits_mask] = raw_scores\n\n    # For bins where the item does not fit, the priority remains 0.\n    # This correctly implies they have the lowest priority.\n    return priorities",
    "response_id": 3,
    "obj": 4.078579976067022,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  }
]