```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Softmax-Based Fit.

    This heuristic prioritizes bins that have a remaining capacity close to the item's size,
    while also considering bins that can accommodate the item. A higher priority is given
    to bins where the item fits snugly (remaining capacity is slightly larger than item size)
    and also to bins with substantial remaining capacity as a secondary consideration.
    The Softmax function is used to convert these scores into probabilities, where the
    bin with the best "fit" (or closest remaining capacity to the item) gets the highest
    probability.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap)
    
    # Calculate a "fit score" for each bin:
    # - If the item fits, we want to favor bins where remaining_capacity - item is small
    #   (meaning a snug fit). We can use the negative of this difference to make larger
    #   negative values (better fits) translate to larger positive scores after softmax.
    # - If the item doesn't fit, assign a very low score (e.g., negative infinity or a large negative number)
    #   so it has zero probability of being selected.
    
    fit_scores = np.full_like(bins_remain_cap, -np.inf) # Initialize with a very low score
    
    # Consider only bins where the item can fit
    can_fit_mask = bins_remain_cap >= item
    
    # For bins where the item fits, calculate the "snugness" score.
    # A smaller positive difference (remaining_capacity - item) is better.
    # We want to maximize this value.
    # To make it work with softmax (which favors larger values), we can use:
    #   a) remaining_capacity - item  (smaller positive values are better fits)
    #   b) -(remaining_capacity - item) (larger values are better fits)
    #   c) A function that peaks when remaining_capacity is close to item.
    
    # Let's try approach (b): maximize the negative difference.
    # This prioritizes bins with less wasted space after fitting the item.
    snugness_scores = bins_remain_cap[can_fit_mask] - item
    
    # Softmax works best with values that are not too skewed or too close to zero if we want
    # to differentiate. For "snugness", a direct difference might lead to values
    # close to zero for many bins.
    
    # Let's create a score that is high for bins where remaining capacity is just enough
    # and also consider bins with more space as a secondary option, but less so.
    # We can use a Gaussian-like function centered around `item` if `bins_remain_cap` is the variable.
    # Or more simply, consider the 'waste' when placing the item.
    
    # Score based on minimizing wasted space:
    # `bins_remain_cap - item` represents wasted space. Lower is better.
    # We want higher priority for smaller waste. So, `- (bins_remain_cap - item)` is better.
    # If remaining capacity is exactly item size, waste is 0, score is 0.
    # If remaining capacity is slightly larger, waste is small positive, score is small negative.
    # If remaining capacity is much larger, waste is large positive, score is large negative.
    
    # To make scores generally positive for softmax, let's adjust.
    # Option 1: Prioritize bins with exactly `item` remaining capacity (score 0), then slightly larger (small positive), then larger (more positive). This is inverse of snugness.
    # Option 2: Prioritize bins where `bins_remain_cap - item` is smallest (snug fit).
    
    # Let's aim for prioritizing snug fits using the 'waste' concept.
    # Higher score = better fit (less waste)
    # If item fits: score = 1 / (1 + (bins_remain_cap - item))  -- hyperbolic, approaches 1 for waste=0
    # Or, let's use a Gaussian-like weighting where peak is at remaining_capacity = item.
    # `np.exp(-(bins_remain_cap[can_fit_mask] - item)**2 / (2 * sigma**2))`
    # This requires a sigma. A simple linear scaling might be sufficient.
    
    # Let's use the concept of "filling potential" and "snugness".
    # A bin is good if it has capacity *close* to item size, but also if it has
    # *enough* capacity to fit the item without excessive waste.
    
    # A common approach for Softmax BPP heuristics involves a combination of factors.
    # Let's prioritize bins that have remaining capacity *just slightly larger* than the item.
    # This means we want `bins_remain_cap - item` to be small and positive.
    #
    # A possible score for a fitting bin could be:
    #   `1 / (1 + (bins_remain_cap[i] - item))`  -- this prioritizes minimal positive waste
    #
    # Alternatively, we can prioritize bins that are "nearly full" with respect to the item.
    #
    # Let's try a score that's high when `bins_remain_cap` is slightly larger than `item`.
    
    # We'll assign a positive "score" to bins that can fit the item.
    # For bins that cannot fit, their score will be negative infinity (handled by `fit_scores` initialization).
    
    # Calculate how "good" each fitting bin is by looking at the remaining capacity relative to the item.
    # We want bins where `bins_remain_cap[i] - item` is minimized (but still >= 0).
    # A large negative value for `bins_remain_cap[i] - item` is also "good" in the sense of ample space,
    # but perhaps not the primary goal of "snugness".
    
    # Let's focus on "snugness" primarily.
    # We want `bins_remain_cap[i]` to be close to `item`.
    # Let's use a score that decreases as `bins_remain_cap[i] - item` increases beyond 0.
    # A simple linear inverse relationship could work:
    # `score = 1.0 / (1.0 + (bins_remain_cap[i] - item))`
    # This gives a score of 1.0 if `bins_remain_cap[i] == item`.
    # Scores decrease as `bins_remain_cap[i]` increases.
    
    for i in range(len(bins_remain_cap)):
        if bins_remain_cap[i] >= item:
            # Prioritize bins that have just enough capacity or slightly more.
            # We want to minimize `bins_remain_cap[i] - item`.
            # Using `1.0 / (1.0 + diff)` gives higher scores for smaller positive `diff`.
            diff = bins_remain_cap[i] - item
            priorities[i] = 1.0 / (1.0 + diff) # Higher score for smaller `diff` (snugger fit)
        else:
            # Item does not fit, assign a very low score (effectively 0 after softmax)
            priorities[i] = -np.inf 

    # The Softmax function: exp(score) / sum(exp(scores))
    # This converts scores into probabilities. Bins with higher scores get higher probabilities.
    
    # If all priorities are -inf (no bins can fit the item), return uniform probabilities or zeros.
    if np.all(priorities == -np.inf):
        return np.ones_like(priorities) / len(priorities) # Or np.zeros_like(priorities)
        
    # Ensure we don't have issues with softmax on all -inf values by adding a small offset
    # or handling it. If `priorities` contains `-np.inf`, `np.exp` will become 0 for those.
    
    # Calculate exponential of priorities
    exp_priorities = np.exp(priorities)
    
    # Normalize using Softmax to get probabilities
    sum_exp_priorities = np.sum(exp_priorities)
    
    # Avoid division by zero if sum_exp_priorities is 0 (e.g., if all initial priorities were -inf)
    if sum_exp_priorities == 0:
        return np.zeros_like(priorities) # Or handle as error or uniform distribution
    
    probabilities = exp_priorities / sum_exp_priorities
    
    return probabilities
```
