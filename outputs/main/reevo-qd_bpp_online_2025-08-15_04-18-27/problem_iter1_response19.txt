```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.

    The strategy prioritizes bins that have a remaining capacity that is "close" to the item size.
    "Closeness" is determined by a Gaussian-like function, effectively giving higher
    priority to bins that can accommodate the item with minimal wasted space.
    A temperature parameter controls the "softness" of the softmax, influencing
    how aggressively the best bin is chosen.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Filter out bins that cannot fit the item
    valid_bins_mask = bins_remain_cap >= item
    valid_bins_remain_cap = bins_remain_cap[valid_bins_mask]

    if valid_bins_remain_cap.size == 0:
        # If no bin can fit the item, return all zeros
        return np.zeros_like(bins_remain_cap)

    # Calculate the "fit score" for each valid bin.
    # We want to prioritize bins where remaining capacity is close to item size.
    # A simple way is to consider the difference. Lower difference is better.
    # To use softmax, we need values that represent preference, so we invert the difference
    # or use a transformation that maps smaller differences to higher values.
    # Let's use a Gaussian-like function centered at 0 difference (perfect fit).
    # The parameter 'sigma' controls the width of the Gaussian. A smaller sigma
    # makes the function more peaked around the perfect fit.
    sigma = 2.0  # Tunable parameter: Controls sensitivity to fit
    fit_scores = np.exp(-((valid_bins_remain_cap - item)**2) / (2 * sigma**2))

    # Apply a temperature parameter to the softmax to control the "sharpness" of preferences.
    # A higher temperature leads to more uniform probabilities (softer choices),
    # while a lower temperature leads to more concentrated probabilities (harder choices).
    temperature = 1.0  # Tunable parameter: Controls softmax steepness
    softmax_scores = np.exp(fit_scores / temperature)
    probabilities = softmax_scores / np.sum(softmax_scores)

    # Assign priorities. In this strategy, we can directly use the probabilities
    # as priorities for the valid bins.
    priorities = np.zeros_like(bins_remain_cap)
    priorities[valid_bins_mask] = probabilities

    return priorities
```
