```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using a Softmax-Based Fit strategy.

    This strategy prioritizes bins that are "almost full" but can still accommodate the item,
    while also giving some consideration to bins that have ample space. The Softmax function
    is used to translate these preferences into a probability-like score for each bin.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # We only consider bins that can fit the current item.
    # For bins that cannot fit, assign a very low "priority" (effectively zero).
    can_fit_mask = bins_remain_cap >= item
    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]

    if valid_bins_remain_cap.size == 0:
        # If no bin can fit the item, return zeros.
        return np.zeros_like(bins_remain_cap)

    # Calculate a "fit score" for each valid bin.
    # We want to favor bins that are nearly full but can still fit the item.
    # A simple approach is to use the remaining capacity directly.
    # A higher remaining capacity means it's less "tightly packed".
    # To encourage "almost full" bins, we can invert the remaining capacity relative to the item size.
    # This makes bins with remaining capacity just slightly larger than 'item' have higher scores.
    # We add a small epsilon to avoid division by zero if remaining capacity is exactly 'item'.
    fit_scores = (valid_bins_remain_cap - item + 1e-6) / (valid_bins_remain_cap + 1e-6)

    # Apply a penalty for bins that have significantly more space than needed.
    # This can be achieved by making the 'fit_scores' decrease faster for larger remaining capacities.
    # For example, we can scale the fit_scores by a factor that reduces with remaining capacity.
    # Or, more simply, we can add a small constant to 'item' before subtracting,
    # which slightly penalizes bins with very large remaining capacities.
    # Let's consider a more nuanced approach:
    # We want to prioritize bins where (remaining_capacity - item) is small.
    # If remaining_capacity is much larger than item, it's less desirable.

    # Alternative approach: prioritize bins where remaining capacity is "closest" to item size.
    # We can use the absolute difference, but then we want smaller differences to be better.
    # Or, we can use a measure that is high when remaining capacity is just enough.
    # Let's try a score that is high when (remaining_capacity - item) is close to zero,
    # and decreases as (remaining_capacity - item) increases.

    # Score based on how "tightly" the item fits:
    # We want smaller (remaining_capacity - item) to be better.
    # Let's try a transformation:
    # If remaining_capacity = item, score is high.
    # If remaining_capacity >> item, score is low.
    # Consider score proportional to 1 / (remaining_capacity - item + epsilon) if we want tighter fits.
    # However, this can lead to extreme values.

    # Let's go back to a Softmax strategy focusing on "almost full" bins.
    # We want a higher value for bins where `remaining_capacity - item` is small,
    # but non-negative.
    # Let's create a score that is inversely related to `remaining_capacity - item`.
    # To prevent very large values when `remaining_capacity - item` is close to zero,
    # we can use an exponential function.

    # We want higher scores for bins where `valid_bins_remain_cap - item` is small.
    # Let `slack = valid_bins_remain_cap - item`.
    # We want scores to be high for small slack, and lower for large slack.
    # Using `np.exp(-slack)` would achieve this. However, if slack is very large,
    # the value becomes close to zero.

    # Let's try to model this as a desirability score.
    # Desirability increases as remaining capacity gets closer to item size.
    # A potential score could be based on the "waste" created: waste = remaining_capacity - item.
    # We want to minimize waste. So, high priority for low waste.

    # Let's create a "preference" value for each valid bin.
    # We want to prioritize bins that have a small amount of remaining capacity *after*
    # placing the item, but not so small that it cannot accommodate the item.
    # A score could be related to 1 / (remaining_capacity - item + epsilon).
    # To smooth this and use with Softmax, we can use an exponential.
    # `np.exp(-(valid_bins_remain_cap - item))` would work, higher for smaller (remaining_capacity - item).

    # Let's try a score that emphasizes bins that are almost full.
    # Consider the inverse of the available slack.
    # `slack = valid_bins_remain_cap - item`.
    # `score = 1.0 / (slack + 1e-9)`. This can be very sensitive.

    # Let's refine: We want to give priority to bins where the remaining capacity
    # is "just enough" for the item. This means `remaining_capacity - item` is small.
    # A Gaussian-like function centered around a small slack value?
    # Or simpler: A function that decreases as slack increases.

    # Let's use a form inspired by Softmax, where we have a "quality" for each bin.
    # The quality should be higher for bins with less "waste".
    # `waste = valid_bins_remain_cap - item`
    # Higher quality for smaller `waste`.

    # Let's try a score that uses the reciprocal of remaining capacity to give
    # a slight bias towards fuller bins, but also penalizes very empty bins.
    # `1.0 / (valid_bins_remain_cap + 1e-6)` -> Higher for less remaining capacity.
    # This would favor bins that are already somewhat full.

    # To combine "can fit" with a "desirability" of being almost full:
    # A value that is high when `valid_bins_remain_cap` is just slightly greater than `item`.
    # Consider `remaining_capacity / bin_capacity` (if bin_capacity was known).
    # Without bin_capacity, we can consider the ratio of item size to remaining capacity.
    # `item / valid_bins_remain_cap`. This is high for smaller remaining capacities,
    # which is what we want if the bin is "almost full".

    # Let's try a score based on `remaining_capacity - item`.
    # We want this value to be small.
    # Score = `exp(-k * (remaining_capacity - item))` where `k` is a scaling factor.
    # A larger `k` makes the preference for smaller slack stronger.

    # Let's set a moderate `k` to balance preference for "almost full" vs. "more space".
    # We can also make the preference sharper or flatter by adjusting the scaling.
    # For simplicity, let's use a simple inverse relationship with a floor.

    # Let's try to create scores that are high for bins where `remaining_capacity` is
    # just above `item`.

    # A simple score: the difference between the remaining capacity and the item size.
    # We want smaller differences to be prioritized. So, we invert this or subtract from a constant.
    # `priorities_valid = 1.0 / (valid_bins_remain_cap - item + 1e-9)`
    # This would give very high priority to bins where `remaining_capacity - item` is tiny.

    # Softmax-Based Fit: the idea is to assign probabilities.
    # We want to put the item in a bin that results in the least "slack" (remaining_capacity - item).
    # So, higher priority for smaller slack.
    # A common transformation for Softmax is `exp(score)`.
    # Let the score be `-(valid_bins_remain_cap - item)`. This means higher scores for smaller slack.
    # Then, we normalize these scores using Softmax.

    scores = -(valid_bins_remain_cap - item)

    # Apply Softmax to the scores of valid bins.
    # The Softmax function converts scores into probabilities.
    # We add a small value to `valid_bins_remain_cap - item` to avoid division by zero in exp.
    # Let's use `exp(-slack)` where `slack = valid_bins_remain_cap - item`.
    # This gives higher values for smaller slack.
    # We need to be careful about the scale of these scores before exp to avoid overflow/underflow.
    # A common technique is to subtract the maximum score before exp.

    # Let `value = valid_bins_remain_cap - item`
    # We want higher priority when `value` is small.
    # Consider `transformed_value = 1.0 / (value + 1e-9)` - this might be too extreme.
    # Consider `transformed_value = -value` - this means smaller `value` gives larger score.
    # Using Softmax: `exp(-value)`

    # Let's try a more direct approach related to the "goodness" of a fit.
    # A good fit is when `remaining_capacity` is close to `item`.
    # We can define a "fit quality" for each valid bin.
    # Let `quality = 1.0 / (valid_bins_remain_cap - item + 1e-9)`.
    # This makes bins with very little slack have very high quality.

    # Alternative: Consider a Gaussian-like function peaking at zero slack.
    # `quality = np.exp(-(valid_bins_remain_cap - item)**2 / (2 * sigma**2))` for some sigma.
    # However, this penalizes bins that are too full (negative slack) which are not allowed.

    # Let's stick with a simpler score that emphasizes bins with small residual capacity.
    # Score for valid bins: `1.0 / (valid_bins_remain_cap - item + 1e-6)`
    # This gives higher scores to bins with less remaining capacity after fitting the item.

    # Using Softmax:
    # `logits = 1.0 / (valid_bins_remain_cap - item + 1e-6)`
    # `exp_logits = np.exp(logits - np.max(logits))`
    # `probabilities = exp_logits / np.sum(exp_logits)`

    # This approach might overemphasize bins with near-zero slack.
    # Let's try a score that is higher when the remaining capacity is just slightly larger than item.
    # A more controlled "almost full" preference.
    # Score: `np.exp(-(valid_bins_remain_cap - item))` would work well with softmax.
    # Smaller `(valid_bins_remain_cap - item)` -> higher score.

    scores_for_softmax = np.exp(-(valid_bins_remain_cap - item))

    # To prevent overflow with large negative exponents, or underflow with large positive ones,
    # it's good practice to shift the scores by subtracting the maximum score.
    shifted_scores = scores_for_softmax - np.max(scores_for_softmax)

    # Calculate the probabilities using Softmax.
    exp_shifted_scores = np.exp(shifted_scores)
    probabilities = exp_shifted_scores / np.sum(exp_shifted_scores)

    # Create the final priorities array, filling in probabilities for valid bins
    # and zeros for invalid bins.
    priorities = np.zeros_like(bins_remain_cap)
    priorities[can_fit_mask] = probabilities

    return priorities
```
