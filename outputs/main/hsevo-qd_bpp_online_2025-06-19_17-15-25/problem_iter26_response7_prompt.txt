{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins based on multiple factors including fit, fullness, remaining capacity, and a refined exploration strategy.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Fullness factor: Prioritize bins that are already relatively full\n        fullness = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / bins_remain_cap[fit_indices].max()\n        fullness_priority = fullness\n\n        # Remaining capacity factor: Prefer bins with tighter fits but avoid nearly full bins\n        remaining_cap_priority = 1.0 / (remaining_capacity[fit_indices] + 0.01)  # Avoid division by zero. small remaining get high priority\n        \n        # Combine fullness and remaining capacity.\n        combined_priority = fullness_priority + remaining_cap_priority\n\n        #Adaptive scaling based on item size\n        scale = np.mean(bins_remain_cap[fit_indices])\n        priorities[fit_indices] = combined_priority / scale\n\n\n        # Refined exploration strategy: Item-size aware and decaying randomness\n        exploration_strength = min(0.1, item)  # Smaller items get more exploration\n        exploration_bonus = np.random.rand(np.sum(fit_indices)) * exploration_strength\n        priorities[fit_indices] += exploration_bonus\n\n\n    # Penalize bins where the item doesn't fit heavily\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities, handling edge cases\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        min_priority = np.min(priorities)\n        priorities = priorities - min_priority\n        priorities = priorities / np.sum(priorities) if np.sum(priorities) > 0 else np.zeros_like(priorities) # Handle case where all priorities are very negative after shift\n\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins based on a combination of factors, including best fit,\n    fullness, and adaptive scaling, with a focus on balancing exploration and exploitation.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Best-Fit component: Prioritize bins where the remaining capacity is smallest.\n        best_fit_priority = 1 / (remaining_capacity[fit_indices] + 1e-9)\n\n        # Fullness component: Prioritize bins that are already relatively full.\n        fullness_priority = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / (bins_remain_cap[fit_indices].max()+1e-9)\n\n        # Adaptive Scaling: Scale based on item size.  Larger items favor tighter fits.\n        scale = item / bins_remain_cap[fit_indices].mean() if bins_remain_cap[fit_indices].mean() > 0 else item\n        adaptive_priority = scale * best_fit_priority  # Scale best-fit\n\n        # Combine priorities. Use a weighted average to balance fit and fullness.\n        priorities[fit_indices] = 0.6 * adaptive_priority + 0.4 * fullness_priority\n\n        # Introduce more strategic randomness (exploration) proportional to remaining space.\n        exploration_factor = np.random.rand(np.sum(fit_indices)) * (remaining_capacity[fit_indices] / bins_remain_cap[fit_indices].max()) * 0.1\n        priorities[fit_indices] += exploration_factor\n\n    # Bins where the item doesn't fit get a very low priority.\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize only if positive priorities exist\n    if np.any(priorities > 0):\n        priorities[priorities > 0] /= np.sum(priorities[priorities > 0])\n    elif np.any(priorities < 0) and np.all(priorities <=0):\n        priorities = priorities - np.min(priorities)\n        if np.sum(priorities) > 0:\n            priorities = priorities / np.sum(priorities)\n    #Else do nothing: all zeroes or all negative\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic incorporates fullness, remaining capacity, adaptive scaling based on item size, and a refined exploration strategy using randomness, along with normalization. The worst heuristic only calculates a ratio of item size to remaining capacity and takes the negative logarithm. (2nd best) vs (second worst), the second worst only use log ratios. Comparing (1st) vs (2nd), we see the code is identical, implying their performance is extremely close, maybe identical. (3rd) vs (4th), these are also identical, suggesting the ranking might be based on subtle differences not apparent in the code itself or is incorrect. Comparing (second worst) vs (worst), both use log ratios, but the worst lacks any handling of edge cases or normalization. Comparing (1st) vs (11th), the 1st one uses fullness factor, Remaining capacity factor, combined priority, Adaptive scaling based on item size and Refined exploration strategy. The 11th one just calculates the remaining capacity and applies a simple priority based on it. Overall: The better heuristics prioritize a combination of factors like fullness, remaining capacity, and adaptive scaling, use a bit of randomness for exploration and normalization of priorities, while penalizing invalid placements. The weaker heuristics rely on simpler metrics or fail to normalize, potentially leading to suboptimal solutions.\n- \nOkay, let's refine \"Current Self-Reflection\" to be more effective in guiding heuristic design.\n\n*   **Keywords:** Multi-factor, adaptive, robust, experimental.\n*   **Advice:** Focus on creating heuristics that are adaptable to specific problem instance characteristics. Experiment to find the best parameters.\n*   **Avoid:** Oversimplification and over-complication. Avoid prematurely fixing parameters.\n*   **Explanation:** Prioritize a balanced approach to heuristic design that considers fit, fullness, randomness, normalization, and penalty. Make your heuristics more dynamic.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}