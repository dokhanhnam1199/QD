[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    ratios = item / bins_remain_cap\n    log_ratios = np.log(ratios)\n    priorities = -log_ratios\n    return priorities",
    "response_id": 0,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 11.60964047443681,
    "mi": 94.04446327225541,
    "token_count": 47.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate remaining capacity after adding the item.\n    remaining_capacity = bins_remain_cap - item\n    \n    # Give high priority to bins where the item fits and leaves minimal waste.\n    fit_indices = remaining_capacity >= 0\n    if np.any(fit_indices):\n        priorities[fit_indices] = 1 / (remaining_capacity[fit_indices] + 1e-9)  # Avoid division by zero\n    \n    # Give a very low priority (or negative) to bins where the item doesn't fit.\n    priorities[remaining_capacity < 0] = -1e9  # Large negative value\n        \n    # Normalize the priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n      priorities = priorities - np.min(priorities)\n      priorities = priorities / np.sum(priorities)\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 6.0,
    "halstead": 439.44362512259653,
    "mi": 78.54110725816716,
    "token_count": 200.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on fit and remaining capacity, with normalization.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    \n    fit_indices = remaining_capacity >= 0\n    if np.any(fit_indices):\n        priorities[fit_indices] = 1 / (remaining_capacity[fit_indices] + 1e-9)\n        \n    priorities[remaining_capacity < 0] = -1e9\n\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n        \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 142.7018117963935,
    "mi": 83.9064421464444,
    "token_count": 158.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response0.txt_stdout.txt",
    "code_path": "problem_iter3_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version prioritizes bins that are already relatively full, to reduce fragmentation.\n    It also includes a small random component to encourage exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate remaining capacity after adding the item.\n    remaining_capacity = bins_remain_cap - item\n    \n    # Give high priority to bins where the item fits and leaves minimal waste.\n    fit_indices = remaining_capacity >= 0\n    if np.any(fit_indices):\n        # Prioritize bins that are already relatively full.  We want to minimize fragmentation.\n        # The smaller the remaining capacity after adding the item, the higher the priority.\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] - remaining_capacity[fit_indices]) / bins_remain_cap[fit_indices] + 1 / (remaining_capacity[fit_indices] + 1e-9) # (bins_remain_cap[fit_indices] - remaining_capacity[fit_indices]) / bins_remain_cap[fit_indices]   # Avoid division by zero\n        \n        # Add a small random component to encourage exploration and escape local optima\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * 0.01\n    \n    # Give a very low priority (or negative) to bins where the item doesn't fit.\n    priorities[remaining_capacity < 0] = -1e9  # Large negative value\n    \n    # Normalize the priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n    \n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 5.0,
    "halstead": 289.5158000807695,
    "mi": 78.10593116293605,
    "token_count": 224.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response0.txt_stdout.txt",
    "code_path": "problem_iter4_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                fit_priority_scale: float = 6.967182577916326,\n                no_fit_priority: float = -9635982018.188837,\n                avoid_zero_division: float = 8.518156183015935e-09) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        fit_priority_scale: Scaling factor for the priority of bins where the item fits.\n        no_fit_priority: Priority given to bins where the item doesn't fit.\n        avoid_zero_division: Small value to avoid division by zero.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate remaining capacity after adding the item.\n    remaining_capacity = bins_remain_cap - item\n    \n    # Give high priority to bins where the item fits and leaves minimal waste.\n    fit_indices = remaining_capacity >= 0\n    if np.any(fit_indices):\n        priorities[fit_indices] = fit_priority_scale / (remaining_capacity[fit_indices] + avoid_zero_division)  # Avoid division by zero\n    \n    # Give a very low priority (or negative) to bins where the item doesn't fit.\n    priorities[remaining_capacity < 0] = no_fit_priority  # Large negative value\n        \n    # Normalize the priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n      priorities = priorities - np.min(priorities)\n      priorities = priorities / np.sum(priorities)\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 17.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 129.65784284662087,
    "mi": 86.12855289617025,
    "token_count": 185.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response5.txt_stdout.txt",
    "code_path": "problem_iter5_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on fullness, fit, and adaptive scaling.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on remaining capacity\n        scale = np.mean(bins_remain_cap[fit_indices]) # Dynamic scale \n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + 1e-9)\n\n        # Incorporate some randomness for exploration\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * 0.01\n\n    # Very low priority to bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 3.9788591942560925,
    "SLOC": 15.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 211.52361657053456,
    "mi": 84.12654317825582,
    "token_count": 183.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response3.txt_stdout.txt",
    "code_path": "problem_iter6_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on remaining capacity, fit, item size, and bin occupancy, with adaptive scaling and exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # 1. Prioritize bins with tighter fit, encouraging better space utilization\n        fit_priority = 1 / (remaining_capacity[fit_indices] + 1e-9)\n\n        # 2. Prioritize bins with higher occupancy (more items already packed), aiming to consolidate packings\n        occupancy_priority = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / (bins_remain_cap[fit_indices].max()+ 1e-9)\n\n        # 3. Adaptive scaling based on the item size relative to average remaining capacity.  Larger items favor almost full bins\n        scale = np.mean(bins_remain_cap[fit_indices])\n        item_size_priority = (item / (scale + 1e-9)) \n\n        #4. Combine priorities with weights. Dynamically adjust weights based on performance characteristics (omitted for simplicity but crucial in a real-world scenario).\n        alpha, beta, gamma = 0.6, 0.3, 0.1 # Weights can be dynamically tuned. Experiment with different values to improve performance.\n        priorities[fit_indices] = alpha * fit_priority + beta * occupancy_priority + gamma * item_size_priority\n        \n        # 5. Constrained Random Exploration: Add small noise only to promising bins\n        exploration_noise = np.random.rand(np.sum(fit_indices)) * 0.01\n        priorities[fit_indices] += exploration_noise\n\n    # Very low priority to bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n    \n    # Normalize priorities to a probability distribution\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n          priorities = priorities - np.min(priorities)\n          priorities = priorities / np.sum(priorities)\n\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 27.0,
    "cyclomatic_complexity": 7.0,
    "halstead": 286.51993510959227,
    "mi": 78.93427044953592,
    "token_count": 322.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_hs3.txt_stdout.txt",
    "code_path": "problem_iter7_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, small_random_component_weight: float = 0.06363480534447948, not_fit_priority: float = -3127673057.0031824, division_eps: float = 2.258132558622918e-09) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version prioritizes bins that are already relatively full, to reduce fragmentation.\n    It also includes a small random component to encourage exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        small_random_component_weight: Weight of the small random component. Default is 0.01.\n        not_fit_priority: Priority given to bins where the item doesn't fit. Default is -1e9.\n        division_eps: Epsilon value to avoid division by zero. Default is 1e-9.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate remaining capacity after adding the item.\n    remaining_capacity = bins_remain_cap - item\n    \n    # Give high priority to bins where the item fits and leaves minimal waste.\n    fit_indices = remaining_capacity >= 0\n    if np.any(fit_indices):\n        # Prioritize bins that are already relatively full.  We want to minimize fragmentation.\n        # The smaller the remaining capacity after adding the item, the higher the priority.\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] - remaining_capacity[fit_indices]) / bins_remain_cap[fit_indices] + 1 / (remaining_capacity[fit_indices] + division_eps) # (bins_remain_cap[fit_indices] - remaining_capacity[fit_indices]) / bins_remain_cap[fit_indices]   # Avoid division by zero\n        \n        # Add a small random component to encourage exploration and escape local optima\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * small_random_component_weight\n    \n    # Give a very low priority (or negative) to bins where the item doesn't fit.\n    priorities[remaining_capacity < 0] = not_fit_priority  # Large negative value\n    \n    # Normalize the priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n    \n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.008775428799367,
    "SLOC": 14.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 222.9388339674094,
    "mi": 73.96694735309869,
    "token_count": 226.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response6.txt_stdout.txt",
    "code_path": "problem_iter8_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on fullness, fit, adaptive scaling, and randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on average remaining capacity of bins where the item fits.\n        scale = np.mean(bins_remain_cap[fit_indices])\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + 1e-9)\n\n        # Introduce randomness for exploration.\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * 0.01\n\n    # Very low priority to bins where item doesn't fit.\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities to ensure they sum to 1 (or handle negative priorities).\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 3.948942959712818,
    "SLOC": 13.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 180.68572508221183,
    "mi": 79.89045254875423,
    "token_count": 172.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response0.txt_stdout.txt",
    "code_path": "problem_iter9_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on a combination of factors, focusing on best fit,\n    penalizing near misses, and adaptively scaling based on item size.  Includes a\n    dynamically adjusted exploration factor.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n    n_bins = len(bins_remain_cap)\n\n    if np.any(fit_indices):\n        # 1. Best Fit Emphasis: Prioritize bins where the item fits best (smallest waste).\n        priorities[fit_indices] = 1.0 / (remaining_capacity[fit_indices] + 1e-9)\n\n        # 2. Penalize Near Misses (Bins that are just too small): Soft constraint handling.\n        near_miss_indices = (remaining_capacity < 0) & (bins_remain_cap >= (item - 0.1 * item)) # within 10% of fitting\n        priorities[near_miss_indices] = -0.5 # Small negative priority; consider if other options exist\n\n        # 3. Adaptive Scaling Based on Item Size: Adjust priority scaling based on the item's relative size.\n        item_size_ratio = item / np.mean(bins_remain_cap[fit_indices]) if np.any(fit_indices) else 0.5  #Ratio of the current item size to average bin capacity.\n        scale_factor = max(0.1, min(1.0, item_size_ratio))  # Clamp to [0.1, 1] to avoid extreme scaling\n        priorities[fit_indices] *= scale_factor\n\n        # 4. Dynamic Exploration:  Introduce randomness that *decreases* as more bins become suitable.\n        #    Fewer suitable bins = more exploration needed.\n        exploration_factor = 0.1 * (1 - (np.sum(fit_indices) / n_bins))  # Scale randomness based on fit ratio\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * exploration_factor\n\n    else:\n        # If NO bins fit, give a very small non-zero chance to bins which are closest to the required size.\n        closest_bin_index = np.argmin(bins_remain_cap)\n        priorities[closest_bin_index] = 0.0001\n\n    # 5. Bins where the item doesn't fit at all get a very negative priority (unless we're *forced* to use them).\n    priorities[remaining_capacity < 0] = np.where(priorities[remaining_capacity < 0] != -0.5, -1e9, priorities[remaining_capacity < 0])\n\n    # Normalize priorities to ensure they sum to 1 (or handle negative priorities).\n    priority_sum = np.sum(priorities)\n    if priority_sum > 0:\n        priorities = priorities / priority_sum\n    elif priority_sum < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities) if np.sum(priorities) > 0 else np.full_like(priorities, 1/len(priorities))\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 24.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 400.90527603206624,
    "mi": 79.76664761002536,
    "token_count": 328.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter10_response0.txt_stdout.txt",
    "code_path": "problem_iter10_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                fit_priority_scale: float = 7.682424551077083,\n                no_fit_priority: float = -5901544632.397996,\n                avoid_zero_division: float = 7.329912700727891e-09,\n                priority_initial_value: float = 0.40093602976369525) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        fit_priority_scale: Scaling factor for the priority of bins where the item fits.\n        no_fit_priority: Priority given to bins where the item doesn't fit.\n        avoid_zero_division: Small value to avoid division by zero.\n        priority_initial_value: Initial value for the priority array.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, priority_initial_value, dtype=float)\n    \n    # Calculate remaining capacity after adding the item.\n    remaining_capacity = bins_remain_cap - item\n    \n    # Give high priority to bins where the item fits and leaves minimal waste.\n    fit_indices = remaining_capacity >= 0\n    if np.any(fit_indices):\n        priorities[fit_indices] = fit_priority_scale / (remaining_capacity[fit_indices] + avoid_zero_division)  # Avoid division by zero\n    \n    # Give a very low priority (or negative) to bins where the item doesn't fit.\n    priorities[remaining_capacity < 0] = no_fit_priority  # Large negative value\n        \n    # Normalize the priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n      priorities = priorities - np.min(priorities)\n      priorities = priorities / np.sum(priorities)\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 129.65784284662087,
    "mi": 86.08501858897519,
    "token_count": 195.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response1.txt_stdout.txt",
    "code_path": "problem_iter11_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on fit, fullness, and adaptive scaling.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on remaining capacity\n        scale = np.mean(bins_remain_cap[fit_indices])\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + 1e-9)\n\n        # Incorporate some randomness for exploration\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * 0.01\n\n    # Very low priority to bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.038691663342641,
    "SLOC": 10.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 115.94522208456974,
    "mi": 90.8149361149426,
    "token_count": 136.0,
    "exec_success": true
  }
]