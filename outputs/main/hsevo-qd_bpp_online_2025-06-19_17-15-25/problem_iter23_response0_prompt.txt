{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins based on multiple factors including fit, fullness, remaining capacity, and a refined exploration strategy.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Fullness factor: Prioritize bins that are already relatively full\n        fullness = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / bins_remain_cap[fit_indices].max()\n        fullness_priority = fullness\n\n        # Remaining capacity factor: Prefer bins with tighter fits but avoid nearly full bins\n        remaining_cap_priority = 1.0 / (remaining_capacity[fit_indices] + 0.01)  # Avoid division by zero. small remaining get high priority\n        \n        # Combine fullness and remaining capacity.\n        combined_priority = fullness_priority + remaining_cap_priority\n\n        #Adaptive scaling based on item size\n        scale = np.mean(bins_remain_cap[fit_indices])\n        priorities[fit_indices] = combined_priority / scale\n\n\n        # Refined exploration strategy: Item-size aware and decaying randomness\n        exploration_strength = min(0.1, item)  # Smaller items get more exploration\n        exploration_bonus = np.random.rand(np.sum(fit_indices)) * exploration_strength\n        priorities[fit_indices] += exploration_bonus\n\n\n    # Penalize bins where the item doesn't fit heavily\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities, handling edge cases\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        min_priority = np.min(priorities)\n        priorities = priorities - min_priority\n        priorities = priorities / np.sum(priorities) if np.sum(priorities) > 0 else np.zeros_like(priorities) # Handle case where all priorities are very negative after shift\n\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins using a combination of remaining capacity, item fit,\n    normalized fullness, and adaptive exploration. Aims to improve upon v1.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Calculate fullness (normalized remaining capacity)\n        fullness = 1 - (remaining_capacity[fit_indices] / bins_remain_cap[fit_indices])\n        fullness = np.clip(fullness, 0, 1)  # Ensure fullness is within [0, 1]\n\n        # Priority based on fullness, favoring bins that are already somewhat full\n        priorities[fit_indices] += fullness\n\n        # Fit score: Higher if the item fits well (less remaining space)\n        fit_score = (item / bins_remain_cap[fit_indices])\n        priorities[fit_indices] += fit_score\n\n        # Adaptive scaling based on the average item size and bin capacity\n        scale = np.mean(bins_remain_cap[fit_indices]) + item\n        adaptive_scale = item / scale\n        priorities[fit_indices] += adaptive_scale\n\n        # Exploration factor, adaptively scaled based on the number of available bins\n        exploration_factor = min(0.1, 0.05 / (np.sum(fit_indices) + 1e-9))\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * exploration_factor\n\n    # Penalize bins where the item doesn't fit harshly\n    priorities[remaining_capacity < 0] = -1e9\n\n    # If no bins can accept the item return equal priority\n    if np.all(remaining_capacity < 0):\n        priorities = np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n    else:\n        # Normalize priorities to ensure they sum to 1 (if possible)\n        if np.sum(priorities) > 0:\n            priorities = priorities / np.sum(priorities)\n        elif np.sum(priorities) < 0:\n            priorities = priorities - np.min(priorities)\n            priorities = priorities / np.sum(priorities)\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic uses a combination of fullness, remaining capacity, adaptive scaling, and a refined exploration strategy, while the worst heavily relies on pre-defined constants and lacks dynamic adaptation. (2nd best) vs (second worst), adaptive scaling is present in both, but the weighting and randomness are different. Comparing (1st) vs (2nd), the first incorporates a more refined exploration strategy that is item-size aware and uses decaying randomness. Comparing (3rd) vs (4th), these heuristics are nearly identical, highlighting the sensitivity of the ranking and potential for minor variations to have a significant impact. Comparing (second worst) vs (worst), both use predefined constants, but the 19th uses slightly more descriptive variable names. Overall: The best heuristics prioritize a nuanced combination of factors including best fit, fullness, adaptive scaling based on item size and remaining capacity, and strategic exploration. They dynamically adjust parameters based on item characteristics and available bin capacities, and handle edge cases robustly. Worse heuristics over-rely on constants, are less adaptive, and have less sophisticated exploration. Normalization methods are consistently applied to ensure probabilities sum to 1.\n- \nHere's a redefined approach to self-reflection for heuristic design, focusing on actionable improvements:\n\n*   **Keywords:** Data-driven adaptation, contextual scaling, robust handling, calibrated randomness.\n*   **Advice:** Design heuristics that learn from problem instance data, dynamically adjusting parameters based on context (item sizes, bin capacities, constraints) rather than relying on static values.\n*   **Avoid:** Generic considerations of \"all constraints\" or \"normalization.\" Instead, focus on specific, measurable improvements derived from problem data.\n*   **Explanation:** Move beyond simply acknowledging factors to implementing data-informed adjustments. Shift from static weighting to dynamic, context-aware scaling. Prioritize measurable robustness enhancements.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}