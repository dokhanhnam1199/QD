{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins based on multiple factors including fit, fullness, remaining capacity, and a refined exploration strategy.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Fullness factor: Prioritize bins that are already relatively full\n        fullness = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / bins_remain_cap[fit_indices].max()\n        fullness_priority = fullness\n\n        # Remaining capacity factor: Prefer bins with tighter fits but avoid nearly full bins\n        remaining_cap_priority = 1.0 / (remaining_capacity[fit_indices] + 0.01)  # Avoid division by zero. small remaining get high priority\n        \n        # Combine fullness and remaining capacity.\n        combined_priority = fullness_priority + remaining_cap_priority\n\n        #Adaptive scaling based on item size\n        scale = np.mean(bins_remain_cap[fit_indices])\n        priorities[fit_indices] = combined_priority / scale\n\n\n        # Refined exploration strategy: Item-size aware and decaying randomness\n        exploration_strength = min(0.1, item)  # Smaller items get more exploration\n        exploration_bonus = np.random.rand(np.sum(fit_indices)) * exploration_strength\n        priorities[fit_indices] += exploration_bonus\n\n\n    # Penalize bins where the item doesn't fit heavily\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities, handling edge cases\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        min_priority = np.min(priorities)\n        priorities = priorities - min_priority\n        priorities = priorities / np.sum(priorities) if np.sum(priorities) > 0 else np.zeros_like(priorities) # Handle case where all priorities are very negative after shift\n\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins based on fullness, fit, adaptive scaling, and item-aware exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Best-Fit priority\n        best_fit_priority = 1 / (remaining_capacity[fit_indices] + 1e-9)\n\n        # Fullness priority\n        fullness_priority = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / (bins_remain_cap[fit_indices].max()+1e-9)\n\n        # Adaptive scaling based on item size and bin capacity.\n        scale = item / bins_remain_cap[fit_indices].mean() if bins_remain_cap[fit_indices].mean() > 0 else item\n        adaptive_priority = scale * best_fit_priority\n\n        # Combine priorities with adaptive weights.\n        weight_fit = 0.5 + 0.5 * (item / np.max(bins_remain_cap)) # Larger items favor fit more\n        priorities[fit_indices] = weight_fit * adaptive_priority + (1 - weight_fit) * fullness_priority\n\n        # Strategic exploration based on remaining capacity and item size.\n        exploration_factor = np.random.rand(np.sum(fit_indices)) * (remaining_capacity[fit_indices] / bins_remain_cap[fit_indices].max()) * (item / np.max(bins_remain_cap)) * 0.1\n        priorities[fit_indices] += exploration_factor\n\n    # Very low priority for bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities\n    if np.any(priorities > 0):\n        priorities[priorities > 0] /= np.sum(priorities[priorities > 0])\n    elif np.any(priorities < 0) and np.all(priorities <=0):\n        priorities = priorities - np.min(priorities)\n        if np.sum(priorities) > 0:\n            priorities = priorities / np.sum(priorities)\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic uses a combination of fullness, remaining capacity, adaptive scaling, and exploration, while the worst primarily focuses on scaling based on remaining capacity with a fixed exploration. The best includes adaptive weighting of fullness and remaining capacity based on item size, encouraging larger items to favor fuller bins. The worst only considers fit and penalizes non-fit.\n\nComparing (2nd) vs (19th), we see (2nd) uses a combination of fullness and remaining capacity, and adaptive scaling based on item size, while (19th) prioritizes bins based on remaining capacity, fit, item size, and bin occupancy, with adaptive scaling and exploration. The difference is (19th) incorporates occupancy priority and dynamic weight adjustment, making it more sophisticated.\n\nComparing (1st) vs (2nd), we see the first one adaptively scales weights for fullness and remaining capacity based on item size, giving larger items higher weight to fullness.  It also has exploration strength that scales with item size, while the second scales exploration with the inverse.\n\nComparing (3rd) vs (4th), we see (4th) introduces data-driven scaling based on item and bin characteristics which is a significant improvement, while (3rd) uses adaptive scaling based on item size. (4th) also has calibrated randomness using bin capacity awareness.\n\nComparing (second worst) vs (worst), we see (second worst) uses slightly more contextual information, but both are relatively basic. (20th) is the worst as it only uses scaling factor, fit priority and a no-fit penalty.\n\nOverall: The best heuristics incorporate multiple factors such as fullness, remaining capacity, adaptive scaling, and exploration. They adapt weights and scaling based on item size and bin characteristics. Also, they include more sophisticated exploration strategies. Normalization is also crucial.\n- \nOkay, here's a redefined \"Current Self-Reflection\" distilled into actionable advice for heuristic design, along with what to avoid, and a rationale:\n\n*   **Keywords:** Multi-factor, Adaptive Scaling, Exploration, Normalization, Edge Cases, Validation.\n*   **Advice:** Integrate multiple relevant factors using adaptive scaling to dynamically adjust priorities, incorporate controlled randomness for exploration, and rigorously handle edge cases. Validate through testing.\n*   **Avoid:** Oversimplification (raw ratios), over-complication (too many unvalidated dynamic weights), fixed constants, ignoring constraints/edge cases.\n*   **Explanation:** Combine diverse criteria for a comprehensive evaluation. Adaptive scaling adjusts to problem characteristics. Exploration prevents getting stuck in local optima. Normalization ensures consistent probability-based selection. Robust edge case handling guarantees reliability. Thorough validation is critical for effectiveness.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}